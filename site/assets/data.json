[
  {
    "date": "2026-02-26",
    "title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation",
    "link": "http://arxiv.org/abs/2602.22896",
    "summary_markdown": "## 研究单位\n- **北京大学人工智能学院**\n- **北京大学集成电路学院**\n- **深圳人工智能与机器人社会研究院**\n- **北京大学电子工程与计算机科学学院**\n- **北京集成电路高精尖创新中心**\n## 论文概述\n- 针对视觉-语言-动作模型在机器人操作任务中推理计算成本高、延迟大的问题，提出了一种新颖的加速框架 **DySL-VLA**。\n- 核心思路是利用机器人任务中不同动作的重要性差异，通过动态-静态层级跳跃的方法，在保证关键动作精度的同时，跳过非关键动作对应的不必要计算层。\n- 旨在解决 **VLA** 模型在资源有限的机器人平台上实时部署的挑战。\n## 核心贡献\n- 提出了 **DySL-VLA** 框架，通过**动态-静态层级跳跃** 显著加速 VLA 模型推理。\n- 提出了**先验-后验跳跃引导**机制，利用动作轨迹的连续性来近似动作重要性，智能地决定何时可以跳过层级，从而保证关键动作的准确性。\n- 提出了一种**跳跃感知的两阶段知识蒸馏**算法，用于高效地将标准 VLA 模型训练成 DySL-VLA，大幅降低了训练参数量和步骤。\n- 通过全面的实验验证了方法的有效性，在保持甚至提升模型性能的同时，实现了显著的推理加速和训练成本降低。\n## 方法描述\n- 核心技术是**动态-静态层级跳跃**：首先通过观察确定 VLA 模型中信息量大的**静态层**（始终执行），以及对模型性能影响较小的**动态层**（可选择性跳过）。在推理时，跳过控制器根据先验-后验引导机制，动态决定是否跳过一连串动态层，并使用轻量级适配器来拟合被跳过层的信息。\n- 创新点包括：1) **先验-后验跳跃引导**：结合动作轨迹的连续性变化，通过“跳过允许点”的移动和重预测验证，确保重要动作不被跳过。2) **跳跃感知的两阶段知识蒸馏**：第一阶段仅训练适配器，第二阶段再共同训练适配器和跳过控制器，解决了收敛问题，并大幅减少了需要训练的参数。\n## 数据集与资源\n- 使用的数据集：**CALVIN** 和 **LIBERO** 机器人操作基准数据集。\n- 模型规模与参数量：实验基于 **RoboFlamingo-3B/9B** 和 **OpenVLA-OFT-7B** 模型。DySL-VLA 仅需训练少量参数（例如在 RoboFlamingo-3B 上为 **14M** 参数），远少于基线方法。\n- 训练与评估资源：在 **RTX 4090** 和 **A6000 GPU** 上进行训练和部分推理评估，并在 **NVIDIA Jetson Orin** 机器人计算平台上进行部署测试。\n## 评估与结果\n- 评估环境：在 CALVIN 和 LIBERO 数据集上的仿真环境及 Jetson Orin 硬件平台。\n- 主要评估指标：任务完成度（CALVIN 为**平均成功长度**，LIBERO 为**成功率 SR**）和模型**推理延迟**。\n- 关键实验结果：\n - **准确性**：在 CALVIN D->D 数据集上，相比 **DeeR-VLA**，DySL-VLA 实现了 **2.1%** 的平均成功长度提升。在 LIBERO 数据集上，平均成功率也优于基线方法。\n - **效率**：相比完整的 RoboFlamingo 基线，DySL-VLA 在 iso-accuracy 下实现了 **3.75倍** 的延迟降低（从 51.0ms 降至 13.6ms）。\n - **训练成本**：相比 DeeR-VLA，DySL-VLA 减少了 **85.7倍** 的可训练参数和 **13.7倍** 的训练步骤。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>北京大学人工智能学院</strong></li><li><strong>北京大学集成电路学院</strong></li><li><strong>深圳人工智能与机器人社会研究院</strong></li><li><strong>北京大学电子工程与计算机科学学院</strong></li><li><strong>北京集成电路高精尖创新中心</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>针对视觉-语言-动作模型在机器人操作任务中推理计算成本高、延迟大的问题，提出了一种新颖的加速框架 <strong>DySL-VLA</strong>。</li><li>核心思路是利用机器人任务中不同动作的重要性差异，通过动态-静态层级跳跃的方法，在保证关键动作精度的同时，跳过非关键动作对应的不必要计算层。</li><li>旨在解决 <strong>VLA</strong> 模型在资源有限的机器人平台上实时部署的挑战。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>DySL-VLA</strong> 框架，通过<strong>动态-静态层级跳跃</strong> 显著加速 VLA 模型推理。</li><li>提出了<strong>先验-后验跳跃引导</strong>机制，利用动作轨迹的连续性来近似动作重要性，智能地决定何时可以跳过层级，从而保证关键动作的准确性。</li><li>提出了一种<strong>跳跃感知的两阶段知识蒸馏</strong>算法，用于高效地将标准 VLA 模型训练成 DySL-VLA，大幅降低了训练参数量和步骤。</li><li>通过全面的实验验证了方法的有效性，在保持甚至提升模型性能的同时，实现了显著的推理加速和训练成本降低。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心技术是<strong>动态-静态层级跳跃</strong>：首先通过观察确定 VLA 模型中信息量大的<strong>静态层</strong>（始终执行），以及对模型性能影响较小的<strong>动态层</strong>（可选择性跳过）。在推理时，跳过控制器根据先验-后验引导机制，动态决定是否跳过一连串动态层，并使用轻量级适配器来拟合被跳过层的信息。</li><li>创新点包括：1) <strong>先验-后验跳跃引导</strong>：结合动作轨迹的连续性变化，通过“跳过允许点”的移动和重预测验证，确保重要动作不被跳过。2) <strong>跳跃感知的两阶段知识蒸馏</strong>：第一阶段仅训练适配器，第二阶段再共同训练适配器和跳过控制器，解决了收敛问题，并大幅减少了需要训练的参数。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的数据集：<strong>CALVIN</strong> 和 <strong>LIBERO</strong> 机器人操作基准数据集。</li><li>模型规模与参数量：实验基于 <strong>RoboFlamingo-3B/9B</strong> 和 <strong>OpenVLA-OFT-7B</strong> 模型。DySL-VLA 仅需训练少量参数（例如在 RoboFlamingo-3B 上为 <strong>14M</strong> 参数），远少于基线方法。</li><li>训练与评估资源：在 <strong>RTX 4090</strong> 和 <strong>A6000 GPU</strong> 上进行训练和部分推理评估，并在 <strong>NVIDIA Jetson Orin</strong> 机器人计算平台上进行部署测试。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：在 CALVIN 和 LIBERO 数据集上的仿真环境及 Jetson Orin 硬件平台。</li><li>主要评估指标：任务完成度（CALVIN 为<strong>平均成功长度</strong>，LIBERO 为<strong>成功率 SR</strong>）和模型<strong>推理延迟</strong>。</li><li>关键实验结果：</li></ul>\n<p> - <strong>准确性</strong>：在 CALVIN D->D 数据集上，相比 <strong>DeeR-VLA</strong>，DySL-VLA 实现了 <strong>2.1%</strong> 的平均成功长度提升。在 LIBERO 数据集上，平均成功率也优于基线方法。</p>\n<p> - <strong>效率</strong>：相比完整的 RoboFlamingo 基线，DySL-VLA 在 iso-accuracy 下实现了 <strong>3.75倍</strong> 的延迟降低（从 51.0ms 降至 13.6ms）。</p>\n<p> - <strong>训练成本</strong>：相比 DeeR-VLA，DySL-VLA 减少了 <strong>85.7倍</strong> 的可训练参数和 <strong>13.7倍</strong> 的训练步骤。</p>"
  },
  {
    "date": "2026-02-26",
    "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline",
    "link": "http://arxiv.org/abs/2602.22663",
    "summary_markdown": "## 研究单位\n- **香港科技大学（广州）**\n- **华中科技大学**\n- **西湖大学**\n- **浙江大学**\n## 论文概述\n- 针对现有**视觉-语言-动作**模型参数量大、预训练成本高、难以应用于移动操作等实际问题，提出了一个新的综合性基准和一个改进的基线模型。\n- 核心目标是研究并提升 **VLA 模型的实用性**，使其能够轻量化、无需大规模预训练，并能处理跨具身和多任务场景。\n## 核心贡献\n- 提出了跨具身基准 **CEBench**，涵盖模拟和真实世界中的单臂、双臂及移动双臂操作任务，并考虑了领域随机化，包含总计 **16k** 条轨迹数据。\n- 对 VLA 的实用性进行了全面研究，围绕**轻量化设计**、**训练范式**和**统一动作空间**提出了八个关键发现。\n- 提出了一个改进的基线模型 **LLaVA-VLA**。该模型**轻量（0.5B 参数）**、**无需大规模机器人数据预训练**，并且是**首个能够进行端到端移动操作**的 VLA 模型。\n- 在多个基准测试（CALVIN， RoboTwin， 真实世界）上验证了 **LLaVA-VLA** 的性能，其表现达到或超越了参数量大 **10 倍** 的模型，并展现了强大的视觉泛化能力。\n- 承诺将开源所有数据集、代码和模型检查点，以促进可复现性和未来研究。\n## 方法描述\n- 提出了 **LLaVA-VLA** 模型架构。它基于一个紧凑的预训练 **VLM 骨干网络**，并集成了**多视角感知**（垂直拼接第一和第三人称视图图像）、**本体感觉标记化**和**动作分块**技术。\n- 创新性地设计了一个**混合动作空间**，通过方向令牌（如前、停）和值令牌来统一导航和操作动作。\n- 采用了两阶段训练范式：先在**多任务数据**上进行**后训练**，再在**特定任务数据**上进行**微调**，从而消除了对大规模、跨具身预训练的依赖。\n## 数据集与资源\n- **模拟数据集**：包括 **CALVIN** 和 **RoboTwin 2.0**，在 RoboTwin 上构建了包含 **14.4k** 条轨迹和 **36** 个任务的数据集。\n- **真实世界数据集**：使用 **Cobot-Magic** 移动机器人平台，收集了包含 **8** 个任务的 **1.6k** 条高质量演示轨迹，其中包括移动操作任务。\n- **模型规模**：主要模型 **LLaVA-VLA** 基于 **LLaVA-OneVision-0.5B** 骨干网络，参数量为 **0.5B**。\n- **训练资源**：后训练使用 **8 块 NVIDIA H100 GPU**，微调使用 **1 块 NVIDIA RTX 4090 GPU**。\n## 评估与结果\n- **评估环境**：在 **CEBench** 的三个子集上进行评估：单臂操作（CALVIN ABC->D 任务）、双臂模拟操作（RoboTwin 的 8 个任务）、以及真实世界的双臂和移动操作任务。\n- **评估指标**：主要使用**任务成功率**和**平均完成的轨迹长度**。\n- **关键结果**：在 **CALVIN** 基准上，**LLaVA-VLA (0.5B)** 的平均轨迹长度为 **3.68**，超越了包括 **OpenVLA (7B)** 和 **RoboFlamingo (3B)** 在内的大模型。在 **RoboTwin** 的领域随机化设置下，平均成功率达 **28.6%**，优于 **ACT** 和 **Diffusion Policy** 等基线。在真实世界任务中，其成功率远高于 **ACT** 和 **TinyVLA**，并且在移动操作任务上取得了成功，而基线方法几乎失败。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>香港科技大学（广州）</strong></li><li><strong>华中科技大学</strong></li><li><strong>西湖大学</strong></li><li><strong>浙江大学</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>针对现有<strong>视觉-语言-动作</strong>模型参数量大、预训练成本高、难以应用于移动操作等实际问题，提出了一个新的综合性基准和一个改进的基线模型。</li><li>核心目标是研究并提升 <strong>VLA 模型的实用性</strong>，使其能够轻量化、无需大规模预训练，并能处理跨具身和多任务场景。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了跨具身基准 <strong>CEBench</strong>，涵盖模拟和真实世界中的单臂、双臂及移动双臂操作任务，并考虑了领域随机化，包含总计 <strong>16k</strong> 条轨迹数据。</li><li>对 VLA 的实用性进行了全面研究，围绕<strong>轻量化设计</strong>、<strong>训练范式</strong>和<strong>统一动作空间</strong>提出了八个关键发现。</li><li>提出了一个改进的基线模型 <strong>LLaVA-VLA</strong>。该模型<strong>轻量（0.5B 参数）</strong>、<strong>无需大规模机器人数据预训练</strong>，并且是<strong>首个能够进行端到端移动操作</strong>的 VLA 模型。</li><li>在多个基准测试（CALVIN， RoboTwin， 真实世界）上验证了 <strong>LLaVA-VLA</strong> 的性能，其表现达到或超越了参数量大 <strong>10 倍</strong> 的模型，并展现了强大的视觉泛化能力。</li><li>承诺将开源所有数据集、代码和模型检查点，以促进可复现性和未来研究。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>提出了 <strong>LLaVA-VLA</strong> 模型架构。它基于一个紧凑的预训练 <strong>VLM 骨干网络</strong>，并集成了<strong>多视角感知</strong>（垂直拼接第一和第三人称视图图像）、<strong>本体感觉标记化</strong>和<strong>动作分块</strong>技术。</li><li>创新性地设计了一个<strong>混合动作空间</strong>，通过方向令牌（如前、停）和值令牌来统一导航和操作动作。</li><li>采用了两阶段训练范式：先在<strong>多任务数据</strong>上进行<strong>后训练</strong>，再在<strong>特定任务数据</strong>上进行<strong>微调</strong>，从而消除了对大规模、跨具身预训练的依赖。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>模拟数据集</strong>：包括 <strong>CALVIN</strong> 和 <strong>RoboTwin 2.0</strong>，在 RoboTwin 上构建了包含 <strong>14.4k</strong> 条轨迹和 <strong>36</strong> 个任务的数据集。</li><li><strong>真实世界数据集</strong>：使用 <strong>Cobot-Magic</strong> 移动机器人平台，收集了包含 <strong>8</strong> 个任务的 <strong>1.6k</strong> 条高质量演示轨迹，其中包括移动操作任务。</li><li><strong>模型规模</strong>：主要模型 <strong>LLaVA-VLA</strong> 基于 <strong>LLaVA-OneVision-0.5B</strong> 骨干网络，参数量为 <strong>0.5B</strong>。</li><li><strong>训练资源</strong>：后训练使用 <strong>8 块 NVIDIA H100 GPU</strong>，微调使用 <strong>1 块 NVIDIA RTX 4090 GPU</strong>。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在 <strong>CEBench</strong> 的三个子集上进行评估：单臂操作（CALVIN ABC->D 任务）、双臂模拟操作（RoboTwin 的 8 个任务）、以及真实世界的双臂和移动操作任务。</li><li><strong>评估指标</strong>：主要使用<strong>任务成功率</strong>和<strong>平均完成的轨迹长度</strong>。</li><li><strong>关键结果</strong>：在 <strong>CALVIN</strong> 基准上，<strong>LLaVA-VLA (0.5B)</strong> 的平均轨迹长度为 <strong>3.68</strong>，超越了包括 <strong>OpenVLA (7B)</strong> 和 <strong>RoboFlamingo (3B)</strong> 在内的大模型。在 <strong>RoboTwin</strong> 的领域随机化设置下，平均成功率达 <strong>28.6%</strong>，优于 <strong>ACT</strong> 和 <strong>Diffusion Policy</strong> 等基线。在真实世界任务中，其成功率远高于 <strong>ACT</strong> 和 <strong>TinyVLA</strong>，并且在移动操作任务上取得了成功，而基线方法几乎失败。</li></ul>"
  },
  {
    "date": "2026-02-26",
    "title": "Metamorphic Testing of Vision-Language Action-Enabled Robots",
    "link": "http://arxiv.org/abs/2602.22579",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-26",
    "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation",
    "link": "http://arxiv.org/abs/2602.22514",
    "summary_markdown": "## 研究单位\n- 论文作者所属研究机构未在提供的HTML片段中明确列出。所有作者标记为`<sup>1</span></sup>`，但具体的机构名称未在正文或作者列表中给出。\n## 论文概述\n- 提出了首个**手语驱动的视觉-语言-动作（VLA）框架** `SignVLA`，旨在实现**实时、直观、包容的人机交互**。\n- 旨在解决当前VLA机器人系统依赖于文本或语音指令的“听力规范”局限性，为听障或语障人士提供原生、自然的手语交互模态。\n## 核心贡献\n- 提出了第一个将**手语作为原生指令模态**集成到**VLA框架**中的系统，使机器人能够通过手势直接理解和执行任务。\n- 开发了一个鲁棒的**手语到文字感知管道**，集成了几何归一化、时序平滑和基于**编辑距离**的词法修正，实现了准确稳定的**实时字母级**手语识别。\n- 设计了一个**模块化接口**，用于桥接连续的姿态流与离散的基于令牌的计算，确保可扩展性并减轻多模态具身系统中的灾难性遗忘。\n- 在**Franka Emika Panda**机器人上验证了所提出的框架，在复杂的操作环境中有效地将手语指令落地为精确的物理动作。\n## 方法描述\n- 采用一个**免注释符（gloss-free）的、分层级的管道**。首先，通过**MediaPipe Hands**实时提取3D手部关键点，利用**ResNet (2+1)D**等混合时空模型进行字母级手势识别。\n- 核心创新包括一个**语言缓冲机制**（包括时序平滑和滑动窗口去抖动）以及一个基于**编辑距离**的词典修正模块，将连续的字母流合成为稳定的单词和自然语言指令。\n- 将合成的指令与机器人RGB观察输入到基于**GR00T N1**的**VLA策略**中进行多模态融合和行动生成。该策略采用**双系统**处理：**System 2**使用**Eagle-2 VLM**进行语义接地（10Hz），**System 1**使用**扩散变换器（DiT）**进行动作流匹配和行动分块，实现低延迟（120Hz）的闭环控制。\n## 数据集与资源\n- 使用了多个数据集，包括用于扩展的小型原始手势数据集（`D_raw`）、**ASL Citizen**（在“未来工作”中提及）以及用于评估的**CSL数据集**。\n- 模型规模：手语识别模型使用**ResNet (2+1)D**等轻量级架构；VLA模型基于大型基础模型**GR00T N1**。\n- 训练与部署资源：实验平台包括**Franka Emika Panda**机器人、**Intel RealSense RGB-D**摄像头以及配备**NVIDIA RTX系列GPU**的专用工作站。\n## 评估与结果\n- 评估环境：在**Franka Emika Panda**机器人上进行端到端机器人操作任务评估。使用**CSL数据集**对手语感知模块进行基准测试。\n- 主要评估指标：**手语识别准确率**、**词错率（WER）**、机器人操作任务的**成功率**、**执行时间**和**稳定性**。\n- 关键实验结果：**ResNet (2+1)D**模型在**CSL数据集**100类和500类任务上分别达到**98.68%** 和**94.85%** 的顶级识别准确率。初步VLA评估显示，经过**时序平滑**的手语指令控制任务成功率（**84.7%**）接近文本指令控制（**86.5%**）。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li>论文作者所属研究机构未在提供的HTML片段中明确列出。所有作者标记为<code><sup>1</span></sup></code>，但具体的机构名称未在正文或作者列表中给出。</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了首个<strong>手语驱动的视觉-语言-动作（VLA）框架</strong> <code>SignVLA</code>，旨在实现<strong>实时、直观、包容的人机交互</strong>。</li><li>旨在解决当前VLA机器人系统依赖于文本或语音指令的“听力规范”局限性，为听障或语障人士提供原生、自然的手语交互模态。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了第一个将<strong>手语作为原生指令模态</strong>集成到<strong>VLA框架</strong>中的系统，使机器人能够通过手势直接理解和执行任务。</li><li>开发了一个鲁棒的<strong>手语到文字感知管道</strong>，集成了几何归一化、时序平滑和基于<strong>编辑距离</strong>的词法修正，实现了准确稳定的<strong>实时字母级</strong>手语识别。</li><li>设计了一个<strong>模块化接口</strong>，用于桥接连续的姿态流与离散的基于令牌的计算，确保可扩展性并减轻多模态具身系统中的灾难性遗忘。</li><li>在<strong>Franka Emika Panda</strong>机器人上验证了所提出的框架，在复杂的操作环境中有效地将手语指令落地为精确的物理动作。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>采用一个<strong>免注释符（gloss-free）的、分层级的管道</strong>。首先，通过<strong>MediaPipe Hands</strong>实时提取3D手部关键点，利用<strong>ResNet (2+1)D</strong>等混合时空模型进行字母级手势识别。</li><li>核心创新包括一个<strong>语言缓冲机制</strong>（包括时序平滑和滑动窗口去抖动）以及一个基于<strong>编辑距离</strong>的词典修正模块，将连续的字母流合成为稳定的单词和自然语言指令。</li><li>将合成的指令与机器人RGB观察输入到基于<strong>GR00T N1</strong>的<strong>VLA策略</strong>中进行多模态融合和行动生成。该策略采用<strong>双系统</strong>处理：<strong>System 2</strong>使用<strong>Eagle-2 VLM</strong>进行语义接地（10Hz），<strong>System 1</strong>使用<strong>扩散变换器（DiT）</strong>进行动作流匹配和行动分块，实现低延迟（120Hz）的闭环控制。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用了多个数据集，包括用于扩展的小型原始手势数据集（<code>D_raw</code>）、<strong>ASL Citizen</strong>（在“未来工作”中提及）以及用于评估的<strong>CSL数据集</strong>。</li><li>模型规模：手语识别模型使用<strong>ResNet (2+1)D</strong>等轻量级架构；VLA模型基于大型基础模型<strong>GR00T N1</strong>。</li><li>训练与部署资源：实验平台包括<strong>Franka Emika Panda</strong>机器人、<strong>Intel RealSense RGB-D</strong>摄像头以及配备<strong>NVIDIA RTX系列GPU</strong>的专用工作站。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：在<strong>Franka Emika Panda</strong>机器人上进行端到端机器人操作任务评估。使用<strong>CSL数据集</strong>对手语感知模块进行基准测试。</li><li>主要评估指标：<strong>手语识别准确率</strong>、<strong>词错率（WER）</strong>、机器人操作任务的<strong>成功率</strong>、<strong>执行时间</strong>和<strong>稳定性</strong>。</li><li>关键实验结果：<strong>ResNet (2+1)D</strong>模型在<strong>CSL数据集</strong>100类和500类任务上分别达到<strong>98.68%</strong> 和<strong>94.85%</strong> 的顶级识别准确率。初步VLA评估显示，经过<strong>时序平滑</strong>的手语指令控制任务成功率（<strong>84.7%</strong>）接近文本指令控制（<strong>86.5%</strong>）。</li></ul>"
  },
  {
    "date": "2026-02-25",
    "title": "World Guidance: World Modeling in Condition Space for Action Generation",
    "link": "http://arxiv.org/abs/2602.22010",
    "summary_markdown": "## 研究单位\n- **ByteDance Seed**\n- **The University of Hong Kong**\n## 论文概述\n- 提出了一种名为 **WoG (World Guidance)** 的新框架，旨在解决现有**视觉-语言-动作 (VLA)** 模型在利用未来观测建模来提升动作生成能力时所面临的平衡难题。\n- 核心目标是找到一个**既易于预测又足够信息丰富**的预测空间（即**条件空间**），以精确指导细粒度的动作生成，避免现有方法在预测冗余的未来表征和粗粒度的潜在动作之间的两难困境。\n## 核心贡献\n- 提出了 **WoG 框架**，通过一个**两阶段训练方案**，将未来观测映射为动作推断流程中的紧凑条件表示，实现**在条件空间中进行世界建模**。\n- 证明了**建模和预测这个条件空间**不仅能有效促进细粒度的动作生成，还展现出**优越的泛化能力**。\n- 设计了一种**可扩展的训练策略**，能够有效地从大规模**人类操作视频**（包括带标注和不带标注的数据）以及**UMI 数据集**中学习未来条件。\n- 在**仿真和真实世界**的广泛实验中，验证了 WoG 方法在多个基准任务上显著优于现有基于未来预测的方法。\n- 提供了对**未来编码器架构**和**预训练视觉编码器组合**的深入消融研究，揭示了影响条件空间效率和模型性能的关键因素。\n## 方法描述\n- **核心技术方法**：采用两阶段课程学习。第一阶段，使用一个可训练的 **Q-Former 未来编码器**从冻结的预训练视觉基础模型（如 **DINOv2, Wan VAE**）中查询和压缩未来观测，形成条件表示，并将其注入 **DiT 动作头部**来指导动作预测。第二阶段，冻结未来编码器，**VLM 主干**被训练同时**预测这些未来的条件表示和对应的动作**，将未来条件知识内化到模型中。\n- **创新点**：该方法的核心创新在于**将未来观测直接注入动作推断流程来定义和优化条件空间**，而不是预测整个未来的视觉模态或学习粗粒度的潜在动作。这使得 VLA 模型能够学习到一个**非冗余的、与动作高度相关的预测空间**。\n## 数据集与资源\n- **主要使用数据集**：**OXE 数据集**（用于预训练），以及从 **SIMPLER 仿真环境** 和 **内部收集的真实世界专家演示** 中获取的任务特定数据集。\n- **模型规模与参数**：VLM 主干采用 **Prismatic VLM**（类似 OpenVLA），动作头采用 **DiT** 架构。未来编码器基于 **Q-Former**。\n- **训练资源**：未在提供片段中明确说明，但根据实验规模推断，应使用了多块 **GPU**（例如，NVIDIA RTX 4090 用于推理和控制工作站）。\n## 评估与结果\n- **评估环境与基准**：\n - **仿真**：在 **SIMPLER** 环境中对 **Google Robot** 和 **WidowX** 机器人进行闭环评估，涵盖多种拾放、移动和操作任务。\n - **真实世界**：在 **UR5** 机器人上评估**拾放 (P&P)**、**关闭微波炉** 和 **折叠毛巾** 三项任务，并设置了**背景变化、光照变化、新物体**等**分布外 (OOD)** 泛化场景。\n- **主要评估指标**：**任务成功率 (%)**。\n- **关键实验结果**：\n - 在 **SIMPLER 仿真**中，WoG 在绝大多数任务上显著超越了 **π0、OpenVLA、UniVLA、VITA、ViPRA、DeFI** 等多种基线方法。\n - 在**真实世界**实验中，WoG 在所有三项任务上均优于 **UniVLA** 和 **VPP**。尤其在 OOD 场景下，WoG 表现出更强的**泛化能力**，性能下降幅度远小于基线。\n - **消融实验**表明，两阶段训练方案、未来编码器的使用以及整合人类视频数据都能带来显著的性能提升。例如，使用 **DINOv2 + Wan VAE** 编码器组合表现最佳。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>ByteDance Seed</strong></li><li><strong>The University of Hong Kong</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种名为 <strong>WoG (World Guidance)</strong> 的新框架，旨在解决现有<strong>视觉-语言-动作 (VLA)</strong> 模型在利用未来观测建模来提升动作生成能力时所面临的平衡难题。</li><li>核心目标是找到一个<strong>既易于预测又足够信息丰富</strong>的预测空间（即<strong>条件空间</strong>），以精确指导细粒度的动作生成，避免现有方法在预测冗余的未来表征和粗粒度的潜在动作之间的两难困境。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>WoG 框架</strong>，通过一个<strong>两阶段训练方案</strong>，将未来观测映射为动作推断流程中的紧凑条件表示，实现<strong>在条件空间中进行世界建模</strong>。</li><li>证明了<strong>建模和预测这个条件空间</strong>不仅能有效促进细粒度的动作生成，还展现出<strong>优越的泛化能力</strong>。</li><li>设计了一种<strong>可扩展的训练策略</strong>，能够有效地从大规模<strong>人类操作视频</strong>（包括带标注和不带标注的数据）以及<strong>UMI 数据集</strong>中学习未来条件。</li><li>在<strong>仿真和真实世界</strong>的广泛实验中，验证了 WoG 方法在多个基准任务上显著优于现有基于未来预测的方法。</li><li>提供了对<strong>未来编码器架构</strong>和<strong>预训练视觉编码器组合</strong>的深入消融研究，揭示了影响条件空间效率和模型性能的关键因素。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>核心技术方法</strong>：采用两阶段课程学习。第一阶段，使用一个可训练的 <strong>Q-Former 未来编码器</strong>从冻结的预训练视觉基础模型（如 <strong>DINOv2, Wan VAE</strong>）中查询和压缩未来观测，形成条件表示，并将其注入 <strong>DiT 动作头部</strong>来指导动作预测。第二阶段，冻结未来编码器，<strong>VLM 主干</strong>被训练同时<strong>预测这些未来的条件表示和对应的动作</strong>，将未来条件知识内化到模型中。</li><li><strong>创新点</strong>：该方法的核心创新在于<strong>将未来观测直接注入动作推断流程来定义和优化条件空间</strong>，而不是预测整个未来的视觉模态或学习粗粒度的潜在动作。这使得 VLA 模型能够学习到一个<strong>非冗余的、与动作高度相关的预测空间</strong>。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>主要使用数据集</strong>：<strong>OXE 数据集</strong>（用于预训练），以及从 <strong>SIMPLER 仿真环境</strong> 和 <strong>内部收集的真实世界专家演示</strong> 中获取的任务特定数据集。</li><li><strong>模型规模与参数</strong>：VLM 主干采用 <strong>Prismatic VLM</strong>（类似 OpenVLA），动作头采用 <strong>DiT</strong> 架构。未来编码器基于 <strong>Q-Former</strong>。</li><li><strong>训练资源</strong>：未在提供片段中明确说明，但根据实验规模推断，应使用了多块 <strong>GPU</strong>（例如，NVIDIA RTX 4090 用于推理和控制工作站）。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：</li></ul>\n<p> - <strong>仿真</strong>：在 <strong>SIMPLER</strong> 环境中对 <strong>Google Robot</strong> 和 <strong>WidowX</strong> 机器人进行闭环评估，涵盖多种拾放、移动和操作任务。</p>\n<p> - <strong>真实世界</strong>：在 <strong>UR5</strong> 机器人上评估<strong>拾放 (P&P)</strong>、<strong>关闭微波炉</strong> 和 <strong>折叠毛巾</strong> 三项任务，并设置了<strong>背景变化、光照变化、新物体</strong>等<strong>分布外 (OOD)</strong> 泛化场景。</p>\n<ul><li><strong>主要评估指标</strong>：<strong>任务成功率 (%)</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 在 <strong>SIMPLER 仿真</strong>中，WoG 在绝大多数任务上显著超越了 <strong>π0、OpenVLA、UniVLA、VITA、ViPRA、DeFI</strong> 等多种基线方法。</p>\n<p> - 在<strong>真实世界</strong>实验中，WoG 在所有三项任务上均优于 <strong>UniVLA</strong> 和 <strong>VPP</strong>。尤其在 OOD 场景下，WoG 表现出更强的<strong>泛化能力</strong>，性能下降幅度远小于基线。</p>\n<p> - <strong>消融实验</strong>表明，两阶段训练方案、未来编码器的使用以及整合人类视频数据都能带来显著的性能提升。例如，使用 <strong>DINOv2 + Wan VAE</strong> 编码器组合表现最佳。</p>"
  },
  {
    "date": "2026-02-25",
    "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
    "link": "http://arxiv.org/abs/2602.22001",
    "summary_markdown": "## 研究单位\n- **Institute of Robotics and Mechatronics, German Aerospace Center (DLR)**, Wessling, Germany.\n- **Stanford AI Lab, Stanford University**, Palo Alto, USA.\n## 论文概述\n- 本文探讨了**基础模型**和**Transformer网络**如何影响机器人技术中不同抽象层面的**迁移学习**，推动机器人迈向“全栈迁移”。\n- 文章从机器人迁移学习的视角，审视了**大型语言模型**、**视觉语言模型**和**视觉语言行动模型**，旨在识别超越具体实现的、通用的迁移概念。\n- 论文旨在阐明这些模型如何解决机器人在新任务、新环境和不同机器人本体（**跨任务、跨环境、跨本体**）之间有效复用知识的核心挑战。\n## 核心贡献\n- 提供了关于基础模型对机器人“全栈迁移”（从高级语言理解到低级运动技能）影响的**综合性综述**。\n- 将不同的**VLA架构组件**映射到已知的迁移学习抽象层，明确指出了各种迁移类型在模型中的发生位置。\n- 分析了**知识隔离**（例如通过梯度停止或图像修复技术）在防止高层语言能力被低层动作数据微调破坏方面的重要作用。\n- 系统地讨论了**跨本体迁移**的挑战与现有方法，包括动作空间表示和传感器配置的差异。\n- 针对基础模型时代，提出了机器人**数据收集**与**迁移性能基准测试**的挑战与未来最佳实践方向。\n## 方法描述\n- 本文为综述性文章，重点分析而非提出新方法。它系统性地回顾了为实现全栈迁移而设计的关键技术。\n- 核心分析围绕**视觉语言行动模型**展开，对比了**离散动作令牌**、**带标记器的动作压缩**、**基于去噪/扩散策略**以及**基于图像修复**等多种VLA架构。\n- 强调了**Transformer网络**作为统一骨干网络，在整合视觉语言理解和视觉运动策略两个研究流中的核心作用。\n- 创新性地将“**知识隔离**”作为核心设计原则进行讨论，以避免不同抽象层次学习目标间的相互干扰，提升迁移性能。\n## 数据集与资源\n- 提及了用于训练和评估的大型机器人数据集，如 **DROID** 数据集。\n- 引用的模型规模从千万到百亿参数不等，例如基于 **PaLI** 或类似架构的预训练VLMs/VLAs。\n- 训练这些基础模型需要大规模计算资源，通常涉及数十至数百个**GPU/TPU**，并使用互联网规模的图文数据（用于预训练）以及数千小时的机器人演示数据（用于微调）。\n## 评估与结果\n- 评估通常在真实机器人或仿真环境中进行，基准包括对新物体、新场景和新任务的**零样本泛化**能力。\n- 主要评估指标为**任务成功率**和**任务完成率**，并强调需要在分布内和分布外条件下进行对比测试。\n- 关键实验结果表明：**预训练的VLM骨干对泛化至关重要**；**增加训练数据的多样性能直接提升泛化能力**；**知识隔离技术能有效保留高层语言理解能力同时学习低级控制策略**；然而，**实现真正的零样本跨本体迁移**仍然是一个重大挑战。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Institute of Robotics and Mechatronics, German Aerospace Center (DLR)</strong>, Wessling, Germany.</li><li><strong>Stanford AI Lab, Stanford University</strong>, Palo Alto, USA.</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>本文探讨了<strong>基础模型</strong>和<strong>Transformer网络</strong>如何影响机器人技术中不同抽象层面的<strong>迁移学习</strong>，推动机器人迈向“全栈迁移”。</li><li>文章从机器人迁移学习的视角，审视了<strong>大型语言模型</strong>、<strong>视觉语言模型</strong>和<strong>视觉语言行动模型</strong>，旨在识别超越具体实现的、通用的迁移概念。</li><li>论文旨在阐明这些模型如何解决机器人在新任务、新环境和不同机器人本体（<strong>跨任务、跨环境、跨本体</strong>）之间有效复用知识的核心挑战。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提供了关于基础模型对机器人“全栈迁移”（从高级语言理解到低级运动技能）影响的<strong>综合性综述</strong>。</li><li>将不同的<strong>VLA架构组件</strong>映射到已知的迁移学习抽象层，明确指出了各种迁移类型在模型中的发生位置。</li><li>分析了<strong>知识隔离</strong>（例如通过梯度停止或图像修复技术）在防止高层语言能力被低层动作数据微调破坏方面的重要作用。</li><li>系统地讨论了<strong>跨本体迁移</strong>的挑战与现有方法，包括动作空间表示和传感器配置的差异。</li><li>针对基础模型时代，提出了机器人<strong>数据收集</strong>与<strong>迁移性能基准测试</strong>的挑战与未来最佳实践方向。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>本文为综述性文章，重点分析而非提出新方法。它系统性地回顾了为实现全栈迁移而设计的关键技术。</li><li>核心分析围绕<strong>视觉语言行动模型</strong>展开，对比了<strong>离散动作令牌</strong>、<strong>带标记器的动作压缩</strong>、<strong>基于去噪/扩散策略</strong>以及<strong>基于图像修复</strong>等多种VLA架构。</li><li>强调了<strong>Transformer网络</strong>作为统一骨干网络，在整合视觉语言理解和视觉运动策略两个研究流中的核心作用。</li><li>创新性地将“<strong>知识隔离</strong>”作为核心设计原则进行讨论，以避免不同抽象层次学习目标间的相互干扰，提升迁移性能。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>提及了用于训练和评估的大型机器人数据集，如 <strong>DROID</strong> 数据集。</li><li>引用的模型规模从千万到百亿参数不等，例如基于 <strong>PaLI</strong> 或类似架构的预训练VLMs/VLAs。</li><li>训练这些基础模型需要大规模计算资源，通常涉及数十至数百个<strong>GPU/TPU</strong>，并使用互联网规模的图文数据（用于预训练）以及数千小时的机器人演示数据（用于微调）。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估通常在真实机器人或仿真环境中进行，基准包括对新物体、新场景和新任务的<strong>零样本泛化</strong>能力。</li><li>主要评估指标为<strong>任务成功率</strong>和<strong>任务完成率</strong>，并强调需要在分布内和分布外条件下进行对比测试。</li><li>关键实验结果表明：<strong>预训练的VLM骨干对泛化至关重要</strong>；<strong>增加训练数据的多样性能直接提升泛化能力</strong>；<strong>知识隔离技术能有效保留高层语言理解能力同时学习低级控制策略</strong>；然而，<strong>实现真正的零样本跨本体迁移</strong>仍然是一个重大挑战。</li></ul>"
  },
  {
    "date": "2026-02-25",
    "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild",
    "link": "http://arxiv.org/abs/2602.21736",
    "summary_markdown": "## 研究单位\n- **北京大学** (Peking University)\n- **中国人民大学** (Renmin University of China)\n- **BeingBeyond**\n## 论文概述\n- 提出了一种名为 **JALA** (Joint-Aligned Latent Actions) 的新型预训练框架，旨在解决从混合（标注与未标注）人类操作视频中学习的大规模 **视觉-语言-动作模型** 预训练的难题。\n- 该方法的核心是绕过复杂的视频动态重建，转而学习一个与**逆动力学**和**真实动作**都对齐的**预测性动作嵌入**，从而构建一个统一的、对状态转移敏感的动作潜在空间。\n- 论文解决的关键问题是：如何有效结合**高精度、小规模**的实验室标注数据与**高多样性、大规模**的野外无标注视频，以实现可扩展的VLA预训练。\n## 核心贡献\n- 提出了**联合对齐的潜在动作**新范式，通过对齐VLA产生的预测性嵌入与逆动力学模型提取的潜在动作，实现从标注和未标注人类视频的统一学习。\n- 构建并发布了 **UniHand-Mix**，一个包含 **7.5M** 个样本、超过 **2000小时** 视频的混合人类操作数据集，统一了实验室标注数据和野外视频数据。\n- 设计了一套完整的**两阶段训练方法**：第一阶段（预训练）使用混合损失在UniHand-Mix上进行；第二阶段（后训练）使用流匹配头将预训练的知识迁移到机器人任务中。\n- 引入了**联合感知器与解耦更新**机制（包括Latent Action Perceiver - LAP 和 Latent State Perceiver - LSP），通过非对称EMA更新稳定训练，确保潜在动作同时具有**可预测性**和**动作信息**。\n- 在多个任务（手部运动生成、模拟机器人操控、真实机器人操控）上进行了综合评估，证明了 **JALA** 在相同规模模型中实现了优异的性能，尤其在野外场景泛化和下游任务迁移上表现出色。\n## 方法描述\n- 核心方法是在Transformer-based的VLA中，将中间层隐藏状态作为**预测性嵌入**，与由**潜在动作感知器**从视频片段边界帧推断出的**潜在动作**进行**联合对齐**（L1损失）。\n- 对于有标注数据，还使用**掩码分块预测** 任务来学习手部运动。\n- 关键技术包括：**联合感知器**（LAP和LSP，结构共享、权重解耦更新）、**混合掩码方案**、**两阶段训练**（预训练+基于流匹配的微调）。\n- 创新点在于摒弃了传统基于重建的潜在动作提取管道（如LAPA），转而采用对齐方法，使潜在动作空间既对上下文的预测敏感，又对视觉动态进行编码，从而能高效利用混合数据。\n## 数据集与资源\n- 主要数据集：**UniHand-Mix**（7.5M 样本， >2,000 小时视频），包含两个子集：**实验室标注子集**（5M+ 样本， 1000小时， 带精确手部追踪和动作标注）和**野外子集**（2.5M 样本， 约1123小时Ego4D视频， 仅部分有伪手部姿态标注）。\n- 模型规模：基于 **InternVL3-2B** 作为视觉-语言主干模型，潜在外观器为2层结构。\n- 训练资源：预训练在 **8 块 NVIDIA A800 (80GB)** GPU上进行，耗时约 **68 小时**。后训练在不同任务上使用相同配置进行 **8-16 小时** 的微调。\n## 评估与结果\n- 评估环境与基准：\n - **手部运动生成**：在UniHand-Mix的实验室分割和野外分割上进行评估。\n - **模拟机器人操作**：在 **LIBERO**、**RoboCasa**、**GR1 Tabletop** 等标准模拟基准上评估。\n - **真实世界机器人实验**：在 **Franka Emika Panda** 机器人上进行评估。\n- 主要评估指标：\n - 手部运动生成：**MPJPE**、**PA-MPJPE**、**MWTE**、**MDE**。\n - 机器人任务：**任务成功率**。\n- 关键实验结果：\n - **JALA** 在手部运动生成任务中，在 **野外分割** 上显著优于 **Being-H0** 等基线模型，例如 **JALA-dino** 的 MPJPE 从基线 ~16.91 提升至 **11.02**，显示了强大的野外泛化能力。\n - 在 **LIBERO**（双视图）基准上， **JALA-dino** 实现了 **96.6%** 的平均成功率，超越了相同规模或相近的模型（如GR00T-N1.5: 93.9%， UniVLA: 95.5%， π0: 94.2%）。\n - 在更具挑战性的 **LIBERO 单视图** 设置下，JALA的优势更加明显，展示了更强的视觉抽象能力。\n - 消融实验证实了**联合对齐**、**解耦EMA更新**以及使用**混合数据集**的必要性和有效性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>北京大学</strong> (Peking University)</li><li><strong>中国人民大学</strong> (Renmin University of China)</li><li><strong>BeingBeyond</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种名为 <strong>JALA</strong> (Joint-Aligned Latent Actions) 的新型预训练框架，旨在解决从混合（标注与未标注）人类操作视频中学习的大规模 <strong>视觉-语言-动作模型</strong> 预训练的难题。</li><li>该方法的核心是绕过复杂的视频动态重建，转而学习一个与<strong>逆动力学</strong>和<strong>真实动作</strong>都对齐的<strong>预测性动作嵌入</strong>，从而构建一个统一的、对状态转移敏感的动作潜在空间。</li><li>论文解决的关键问题是：如何有效结合<strong>高精度、小规模</strong>的实验室标注数据与<strong>高多样性、大规模</strong>的野外无标注视频，以实现可扩展的VLA预训练。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了<strong>联合对齐的潜在动作</strong>新范式，通过对齐VLA产生的预测性嵌入与逆动力学模型提取的潜在动作，实现从标注和未标注人类视频的统一学习。</li><li>构建并发布了 <strong>UniHand-Mix</strong>，一个包含 <strong>7.5M</strong> 个样本、超过 <strong>2000小时</strong> 视频的混合人类操作数据集，统一了实验室标注数据和野外视频数据。</li><li>设计了一套完整的<strong>两阶段训练方法</strong>：第一阶段（预训练）使用混合损失在UniHand-Mix上进行；第二阶段（后训练）使用流匹配头将预训练的知识迁移到机器人任务中。</li><li>引入了<strong>联合感知器与解耦更新</strong>机制（包括Latent Action Perceiver - LAP 和 Latent State Perceiver - LSP），通过非对称EMA更新稳定训练，确保潜在动作同时具有<strong>可预测性</strong>和<strong>动作信息</strong>。</li><li>在多个任务（手部运动生成、模拟机器人操控、真实机器人操控）上进行了综合评估，证明了 <strong>JALA</strong> 在相同规模模型中实现了优异的性能，尤其在野外场景泛化和下游任务迁移上表现出色。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心方法是在Transformer-based的VLA中，将中间层隐藏状态作为<strong>预测性嵌入</strong>，与由<strong>潜在动作感知器</strong>从视频片段边界帧推断出的<strong>潜在动作</strong>进行<strong>联合对齐</strong>（L1损失）。</li><li>对于有标注数据，还使用<strong>掩码分块预测</strong> 任务来学习手部运动。</li><li>关键技术包括：<strong>联合感知器</strong>（LAP和LSP，结构共享、权重解耦更新）、<strong>混合掩码方案</strong>、<strong>两阶段训练</strong>（预训练+基于流匹配的微调）。</li><li>创新点在于摒弃了传统基于重建的潜在动作提取管道（如LAPA），转而采用对齐方法，使潜在动作空间既对上下文的预测敏感，又对视觉动态进行编码，从而能高效利用混合数据。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>主要数据集：<strong>UniHand-Mix</strong>（7.5M 样本， >2,000 小时视频），包含两个子集：<strong>实验室标注子集</strong>（5M+ 样本， 1000小时， 带精确手部追踪和动作标注）和<strong>野外子集</strong>（2.5M 样本， 约1123小时Ego4D视频， 仅部分有伪手部姿态标注）。</li><li>模型规模：基于 <strong>InternVL3-2B</strong> 作为视觉-语言主干模型，潜在外观器为2层结构。</li><li>训练资源：预训练在 <strong>8 块 NVIDIA A800 (80GB)</strong> GPU上进行，耗时约 <strong>68 小时</strong>。后训练在不同任务上使用相同配置进行 <strong>8-16 小时</strong> 的微调。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境与基准：</li></ul>\n<p> - <strong>手部运动生成</strong>：在UniHand-Mix的实验室分割和野外分割上进行评估。</p>\n<p> - <strong>模拟机器人操作</strong>：在 <strong>LIBERO</strong>、<strong>RoboCasa</strong>、<strong>GR1 Tabletop</strong> 等标准模拟基准上评估。</p>\n<p> - <strong>真实世界机器人实验</strong>：在 <strong>Franka Emika Panda</strong> 机器人上进行评估。</p>\n<ul><li>主要评估指标：</li></ul>\n<p> - 手部运动生成：<strong>MPJPE</strong>、<strong>PA-MPJPE</strong>、<strong>MWTE</strong>、<strong>MDE</strong>。</p>\n<p> - 机器人任务：<strong>任务成功率</strong>。</p>\n<ul><li>关键实验结果：</li></ul>\n<p> - <strong>JALA</strong> 在手部运动生成任务中，在 <strong>野外分割</strong> 上显著优于 <strong>Being-H0</strong> 等基线模型，例如 <strong>JALA-dino</strong> 的 MPJPE 从基线 ~16.91 提升至 <strong>11.02</strong>，显示了强大的野外泛化能力。</p>\n<p> - 在 <strong>LIBERO</strong>（双视图）基准上， <strong>JALA-dino</strong> 实现了 <strong>96.6%</strong> 的平均成功率，超越了相同规模或相近的模型（如GR00T-N1.5: 93.9%， UniVLA: 95.5%， π0: 94.2%）。</p>\n<p> - 在更具挑战性的 <strong>LIBERO 单视图</strong> 设置下，JALA的优势更加明显，展示了更强的视觉抽象能力。</p>\n<p> - 消融实验证实了<strong>联合对齐</strong>、<strong>解耦EMA更新</strong>以及使用<strong>混合数据集</strong>的必要性和有效性。</p>"
  },
  {
    "date": "2026-02-25",
    "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
    "link": "http://arxiv.org/abs/2602.21633",
    "summary_markdown": "## 研究单位\n- 作者未明确列出所属机构，但推测为 AI/机器人学习研究团队（根据论文内容和 GitHub 项目 Kisaragi0/SC-VLA）。\n## 论文概述\n- 提出 **Self-Correcting VLA (SC-VLA)** 框架，通过**稀疏世界想象 (Sparse World Imagination, SPI)** 和**在线动作优化 (Online Action Refinement, OAR)**，实现视觉-语言-动作模型的**自我改进 (self-improvement)**。\n- 旨在解决传统 VLA 模型依赖静态数据先验、缺乏物理动态理解，以及传统强化学习依赖外部奖励信号而与模型内部状态脱节的问题。\n## 核心贡献\n- 提出 **SC-VLA 框架**，将离线动作生成与在线优化相结合，通过**稀疏世界想象 (SPI)** 预测未来状态，约束策略编码物理演化。\n- 设计了**在线动作优化 (OAR)** 模块，利用残差强化学习和基于想象的**内生密集奖励 (endogenous dense rewards)** 动态调整轨迹方向。\n- 在模拟（**ManiSkill3** 基准）和真实世界（**ARX5** 机器人平台）的复杂操作任务上进行了系统性评估，证明了该方法在成功率和任务吞吐量上的优越性。\n## 方法描述\n- 方法基于**条件流匹配 (Conditional Flow Matching, FM)** 和 **Soft Actor-Critic (SAC)** 算法构建。\n- **第一阶段 (SPI)**：在 VLM-DiT 骨干网络上，添加辅助预测头，同时预测**任务进度 (progress)** 和**未来相对状态变化 (ΔState)**，作为稀疏世界信号。\n- **第二阶段 (OAR)**：冻结基础策略，在其输出之上训练一个轻量级**残差策略 (Residual Policy)**。奖励信号由环境稀疏奖励和基于预测未来状态方向的**内生密集指导奖励 (intrinsic dense guidance reward)** 动态加权组合而成。\n## 数据集与资源\n- 使用 **ManiSkill3** 仿真基准中的四个挑战性机器人操作任务（StackCube, PlaceSphere, LiftPegUpright, PegInsertion）。\n- 在真实世界使用 **ARX5** 机械臂平台进行验证。\n- 模型基于 **SigLIP-2** 视觉编码器、**Eagle-2** VLM 和 **DiT (Diffusion Transformer)** 骨干网络。\n- 训练资源未在提供的片段中明确说明。\n## 评估与结果\n- **评估环境**：ManiSkill3 仿真环境与 ARX5 真实机器人。\n- **主要指标**：任务成功率，成功任务的平均完成步数（任务吞吐量）。\n- **关键结果**：\n - 在 ManiSkill3 上，**SC-VLA** 以 **86%** 的平均成功率超越所有基线（如 **GR00T N1.5** 72%， **π₀** 55%），并在最难的 PegInsertion 任务上提升 **28%**。\n - 在成功任务中，**SC-VLA** 的平均步数（**157步**）比最佳基线减少了 **16%**，实现了最高吞吐量。\n - 在真实世界任务中，**SC-VLA** 获得 **71%** 的平均成功率，比最佳基线 **GR00T N1.5** 高出 **14%**。\n - 消融实验验证了稀疏世界想象、内生密集奖励和动态权重调度各组件均对性能有显著贡献。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li>作者未明确列出所属机构，但推测为 AI/机器人学习研究团队（根据论文内容和 GitHub 项目 Kisaragi0/SC-VLA）。</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>Self-Correcting VLA (SC-VLA)</strong> 框架，通过<strong>稀疏世界想象 (Sparse World Imagination, SPI)</strong> 和<strong>在线动作优化 (Online Action Refinement, OAR)</strong>，实现视觉-语言-动作模型的<strong>自我改进 (self-improvement)</strong>。</li><li>旨在解决传统 VLA 模型依赖静态数据先验、缺乏物理动态理解，以及传统强化学习依赖外部奖励信号而与模型内部状态脱节的问题。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出 <strong>SC-VLA 框架</strong>，将离线动作生成与在线优化相结合，通过<strong>稀疏世界想象 (SPI)</strong> 预测未来状态，约束策略编码物理演化。</li><li>设计了<strong>在线动作优化 (OAR)</strong> 模块，利用残差强化学习和基于想象的<strong>内生密集奖励 (endogenous dense rewards)</strong> 动态调整轨迹方向。</li><li>在模拟（<strong>ManiSkill3</strong> 基准）和真实世界（<strong>ARX5</strong> 机器人平台）的复杂操作任务上进行了系统性评估，证明了该方法在成功率和任务吞吐量上的优越性。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>方法基于<strong>条件流匹配 (Conditional Flow Matching, FM)</strong> 和 <strong>Soft Actor-Critic (SAC)</strong> 算法构建。</li><li><strong>第一阶段 (SPI)</strong>：在 VLM-DiT 骨干网络上，添加辅助预测头，同时预测<strong>任务进度 (progress)</strong> 和<strong>未来相对状态变化 (ΔState)</strong>，作为稀疏世界信号。</li><li><strong>第二阶段 (OAR)</strong>：冻结基础策略，在其输出之上训练一个轻量级<strong>残差策略 (Residual Policy)</strong>。奖励信号由环境稀疏奖励和基于预测未来状态方向的<strong>内生密集指导奖励 (intrinsic dense guidance reward)</strong> 动态加权组合而成。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用 <strong>ManiSkill3</strong> 仿真基准中的四个挑战性机器人操作任务（StackCube, PlaceSphere, LiftPegUpright, PegInsertion）。</li><li>在真实世界使用 <strong>ARX5</strong> 机械臂平台进行验证。</li><li>模型基于 <strong>SigLIP-2</strong> 视觉编码器、<strong>Eagle-2</strong> VLM 和 <strong>DiT (Diffusion Transformer)</strong> 骨干网络。</li><li>训练资源未在提供的片段中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：ManiSkill3 仿真环境与 ARX5 真实机器人。</li><li><strong>主要指标</strong>：任务成功率，成功任务的平均完成步数（任务吞吐量）。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - 在 ManiSkill3 上，<strong>SC-VLA</strong> 以 <strong>86%</strong> 的平均成功率超越所有基线（如 <strong>GR00T N1.5</strong> 72%， <strong>π₀</strong> 55%），并在最难的 PegInsertion 任务上提升 <strong>28%</strong>。</p>\n<p> - 在成功任务中，<strong>SC-VLA</strong> 的平均步数（<strong>157步</strong>）比最佳基线减少了 <strong>16%</strong>，实现了最高吞吐量。</p>\n<p> - 在真实世界任务中，<strong>SC-VLA</strong> 获得 <strong>71%</strong> 的平均成功率，比最佳基线 <strong>GR00T N1.5</strong> 高出 <strong>14%</strong>。</p>\n<p> - 消融实验验证了稀疏世界想象、内生密集奖励和动态权重调度各组件均对性能有显著贡献。</p>"
  },
  {
    "date": "2026-02-25",
    "title": "LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies",
    "link": "http://arxiv.org/abs/2602.21531",
    "summary_markdown": "## 研究单位\n- **University of North Carolina at Chapel Hill**\n- **Georgia Institute of Technology**\n- **Carnegie Mellon University**\n## 论文概述\n- 提出 **LiLo-VLA**，一个用于长时程（long-horizon）机器人操作的模块化框架，旨在解决现有视觉-语言-动作（VLA）模型在技能组合泛化和抗环境干扰方面的不足。\n- 核心研究问题是：如何实现无需任务特定训练数据、对视觉干扰和环境变化鲁棒，并能从执行失败中恢复的**组合式长时程操作**。\n## 核心贡献\n- **LiLo-VLA 框架**：一个将全局运动（运输）与局部交互解耦的模块化框架，实现**零样本组合泛化**，并对视觉杂乱和执行失败具有鲁棒性。\n- **评估基准**：提出了一个包含 21 个任务的基准测试，涵盖两个不同侧重点的测试集：**LIBERO-Long++**（强调视觉鲁棒性）和 **Ultra-Long**（挑战极端时序可扩展性，最多包含 16 个顺序技能）。\n- **仿真性能**：在模拟实验中，**LiLo-VLA** 实现了 69% 的平均成功率，显著优于 **Pi0.5** (28%) 和 **OpenVLA-OFT** (2%) 等基线模型。\n- **真实世界验证**：在 8 个复杂的真实世界长时程任务（最多 8 个技能）中，实现了 **85%** 的平均成功率。\n## 方法描述\n- 采用模块化控制策略，**解耦**执行过程：一个 **Reaching Module**（基于经典运动规划器）负责端到端的全局运输，一个 **Interaction Module**（基于对象中心的 VLA 策略）负责精细的原子操作。\n- **关键技术**：使用**对象中心观测空间**（仅手腕相机视图）和**视觉杂乱增强**（随机擦除背景）来确保交互策略的视觉鲁棒性；为交互策略引入**初始状态扰动**训练，以补偿运动规划与感知噪声；设计了**闭环恢复机制**，能够动态重规划并重置工作空间以有效重试失败技能。\n## 数据集与资源\n- 使用 **LIBERO-90** 演示数据，并将其分解为原子技能以生成训练数据。\n- 主要模型基于 **OpenVLA-OFT** 和 **Pi0.5** 作为 VLA 策略骨干。真实世界实验中使用 **Pi0.5**。\n- 感知模块使用 **YOLOE**（开放词汇目标检测）和 **FoundationPose**（6D 姿态估计）。\n- 运动规划使用 **MPLib** 库。\n- 训练资源未在提供的 HTML 片段中明确说明。\n## 评估与结果\n- **评估基准**：自建的 21 任务基准，分为 **LIBERO-Long++**（6 个场景，带视觉杂乱）和 **Ultra-Long**（3 个超长任务，9-16 步）。\n- **主要指标**：**成功率** 和 **平均进度**。要求严格按照给定顺序执行所有技能才算成功。\n- **关键结果**：\n - 在模拟基准测试中，**LiLo-VLA** 整体平均成功率为 **69%**，显著优于基线模型。\n - **LiLo-VLA** 在新技能排序（Variant）任务上仍保持高成功率，证明了其零样本组合泛化能力，而基线模型（如 Pi0.5）则严重依赖见过的轨迹顺序。\n - 消融实验证明了**到达模块**、**视觉掩蔽**和**闭环恢复**对成功至关重要，移除任一部分性能均大幅下降。\n - 在 8 个真实世界任务中，**LiLo-VLA** 在标准、多样化布局和变更技能顺序等不同条件下均表现鲁棒，平均成功率达 **85%**。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>University of North Carolina at Chapel Hill</strong></li><li><strong>Georgia Institute of Technology</strong></li><li><strong>Carnegie Mellon University</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>LiLo-VLA</strong>，一个用于长时程（long-horizon）机器人操作的模块化框架，旨在解决现有视觉-语言-动作（VLA）模型在技能组合泛化和抗环境干扰方面的不足。</li><li>核心研究问题是：如何实现无需任务特定训练数据、对视觉干扰和环境变化鲁棒，并能从执行失败中恢复的<strong>组合式长时程操作</strong>。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li><strong>LiLo-VLA 框架</strong>：一个将全局运动（运输）与局部交互解耦的模块化框架，实现<strong>零样本组合泛化</strong>，并对视觉杂乱和执行失败具有鲁棒性。</li><li><strong>评估基准</strong>：提出了一个包含 21 个任务的基准测试，涵盖两个不同侧重点的测试集：<strong>LIBERO-Long++</strong>（强调视觉鲁棒性）和 <strong>Ultra-Long</strong>（挑战极端时序可扩展性，最多包含 16 个顺序技能）。</li><li><strong>仿真性能</strong>：在模拟实验中，<strong>LiLo-VLA</strong> 实现了 69% 的平均成功率，显著优于 <strong>Pi0.5</strong> (28%) 和 <strong>OpenVLA-OFT</strong> (2%) 等基线模型。</li><li><strong>真实世界验证</strong>：在 8 个复杂的真实世界长时程任务（最多 8 个技能）中，实现了 <strong>85%</strong> 的平均成功率。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>采用模块化控制策略，<strong>解耦</strong>执行过程：一个 <strong>Reaching Module</strong>（基于经典运动规划器）负责端到端的全局运输，一个 <strong>Interaction Module</strong>（基于对象中心的 VLA 策略）负责精细的原子操作。</li><li><strong>关键技术</strong>：使用<strong>对象中心观测空间</strong>（仅手腕相机视图）和<strong>视觉杂乱增强</strong>（随机擦除背景）来确保交互策略的视觉鲁棒性；为交互策略引入<strong>初始状态扰动</strong>训练，以补偿运动规划与感知噪声；设计了<strong>闭环恢复机制</strong>，能够动态重规划并重置工作空间以有效重试失败技能。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用 <strong>LIBERO-90</strong> 演示数据，并将其分解为原子技能以生成训练数据。</li><li>主要模型基于 <strong>OpenVLA-OFT</strong> 和 <strong>Pi0.5</strong> 作为 VLA 策略骨干。真实世界实验中使用 <strong>Pi0.5</strong>。</li><li>感知模块使用 <strong>YOLOE</strong>（开放词汇目标检测）和 <strong>FoundationPose</strong>（6D 姿态估计）。</li><li>运动规划使用 <strong>MPLib</strong> 库。</li><li>训练资源未在提供的 HTML 片段中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估基准</strong>：自建的 21 任务基准，分为 <strong>LIBERO-Long++</strong>（6 个场景，带视觉杂乱）和 <strong>Ultra-Long</strong>（3 个超长任务，9-16 步）。</li><li><strong>主要指标</strong>：<strong>成功率</strong> 和 <strong>平均进度</strong>。要求严格按照给定顺序执行所有技能才算成功。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - 在模拟基准测试中，<strong>LiLo-VLA</strong> 整体平均成功率为 <strong>69%</strong>，显著优于基线模型。</p>\n<p> - <strong>LiLo-VLA</strong> 在新技能排序（Variant）任务上仍保持高成功率，证明了其零样本组合泛化能力，而基线模型（如 Pi0.5）则严重依赖见过的轨迹顺序。</p>\n<p> - 消融实验证明了<strong>到达模块</strong>、<strong>视觉掩蔽</strong>和<strong>闭环恢复</strong>对成功至关重要，移除任一部分性能均大幅下降。</p>\n<p> - 在 8 个真实世界任务中，<strong>LiLo-VLA</strong> 在标准、多样化布局和变更技能顺序等不同条件下均表现鲁棒，平均成功率达 <strong>85%</strong>。</p>"
  },
  {
    "date": "2026-02-24",
    "title": "VLA Knows Its Limits",
    "link": "http://arxiv.org/abs/2602.21445",
    "summary_markdown": "## 研究单位\n- **University of Illinois Chicago**\n- **Cisco Research**\n## 论文概述\n- 聚焦于基于流的**Vision-Language-Action (VLA)** 模型中的**动作分块 (action chunking)** 实践。\n- 研究发现，执行视野（从每个预测动作块中执行的动作数量）的选择会显著影响模型性能，存在一个先上升后下降的峰值模式。\n- 论文旨在解决如何为每个预测的动作块**动态地、自适应地确定最佳执行视野**的问题，以平衡策略的长时一致性和短时反应性。\n## 核心贡献\n- **分析与观察**：通过理论（存在最优执行视野的证明）和实证分析，揭示了VLA性能随执行视野变化的规律，并通过分析注意力权重发现其内在原因（如**注意力不变性**和**径向动作汇点**）。\n- **提出 AutoHorizon 方法**：首次提出一种基于测试时注意力权重的动态执行视野估计策略，无需额外训练，计算开销极低。\n- **广泛验证**：在模拟（**LIBERO**, **RoboTwin**）和真实世界机器人操作任务中，**AutoHorizon** 在多种流式VLA模型（如 **π0.5**, **GR00T N1.5**）上均有效，性能优于需要大量调优的静态基线。\n## 方法描述\n- **核心技术**：分析基于流的VLA模型中的**跨注意力和自注意力权重**，将其解释为模型预测置信度的隐含指标。\n- **创新点**：\n - 发现动作分块内的动作对视觉-语言标记的注意力**保持不变**，导致对环境变化的适应性有限。\n - 发现预测动作序列的**起始和终止动作标记**作为稳定的**径向动作汇点 (radial action sinks)**，是中间动作组织的锚点。\n - 提出 **AutoHorizon** 算法：利用动作自注意力权重，通过**双向软指针机制**识别注意力轨迹的转折点（注意力停止前进并开始平稳），以此估计模型的预测极限，动态确定每个动作块的执行视野。\n## 数据集与资源\n- **数据集**：**LIBERO** 基准（包含 LIB-Spatial, LIB-Goal, LIB-Object, LIB-10 子集）和 **RoboTwin** 基准（包含多个双手操作任务）。\n- **评估模型**：**π0.5**（预测视野 p=10 和 p=50），**GR00T N1.5**（预测视野 p=16）。\n- **训练/测试资源**：使用了预训练模型检查点。具体训练资源未在提供文本中明确说明。\n## 评估与结果\n- **评估环境**：模拟环境（使用LIBERO和RoboTwin基准）和真实世界机器人（Franka Research 3）操作。\n- **主要评估指标**：任务**成功率（Success Rate）**。\n- **关键实验结果**：\n - 在LIBERO基准上，**AutoHorizon** 在 π0.5 (p=50) 模型上超越了所有静态基线（Static Oracle, Static Oracle+）和随机基线。\n - 在GR00T N1.5模型上，**AutoHorizon** 同样表现最佳。\n - 在更具挑战性的RoboTwin基准上，**AutoHorizon** 在大多数任务上达到或超过最强基线性能。\n - 真实世界任务（如放置黄瓜/魔方）中，AutoHorizon的动态视野调整策略有效提高了任务完成率。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>University of Illinois Chicago</strong></li><li><strong>Cisco Research</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>聚焦于基于流的<strong>Vision-Language-Action (VLA)</strong> 模型中的<strong>动作分块 (action chunking)</strong> 实践。</li><li>研究发现，执行视野（从每个预测动作块中执行的动作数量）的选择会显著影响模型性能，存在一个先上升后下降的峰值模式。</li><li>论文旨在解决如何为每个预测的动作块<strong>动态地、自适应地确定最佳执行视野</strong>的问题，以平衡策略的长时一致性和短时反应性。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li><strong>分析与观察</strong>：通过理论（存在最优执行视野的证明）和实证分析，揭示了VLA性能随执行视野变化的规律，并通过分析注意力权重发现其内在原因（如<strong>注意力不变性</strong>和<strong>径向动作汇点</strong>）。</li><li><strong>提出 AutoHorizon 方法</strong>：首次提出一种基于测试时注意力权重的动态执行视野估计策略，无需额外训练，计算开销极低。</li><li><strong>广泛验证</strong>：在模拟（<strong>LIBERO</strong>, <strong>RoboTwin</strong>）和真实世界机器人操作任务中，<strong>AutoHorizon</strong> 在多种流式VLA模型（如 <strong>π0.5</strong>, <strong>GR00T N1.5</strong>）上均有效，性能优于需要大量调优的静态基线。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>核心技术</strong>：分析基于流的VLA模型中的<strong>跨注意力和自注意力权重</strong>，将其解释为模型预测置信度的隐含指标。</li><li><strong>创新点</strong>：</li></ul>\n<p> - 发现动作分块内的动作对视觉-语言标记的注意力<strong>保持不变</strong>，导致对环境变化的适应性有限。</p>\n<p> - 发现预测动作序列的<strong>起始和终止动作标记</strong>作为稳定的<strong>径向动作汇点 (radial action sinks)</strong>，是中间动作组织的锚点。</p>\n<p> - 提出 <strong>AutoHorizon</strong> 算法：利用动作自注意力权重，通过<strong>双向软指针机制</strong>识别注意力轨迹的转折点（注意力停止前进并开始平稳），以此估计模型的预测极限，动态确定每个动作块的执行视野。</p>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>数据集</strong>：<strong>LIBERO</strong> 基准（包含 LIB-Spatial, LIB-Goal, LIB-Object, LIB-10 子集）和 <strong>RoboTwin</strong> 基准（包含多个双手操作任务）。</li><li><strong>评估模型</strong>：<strong>π0.5</strong>（预测视野 p=10 和 p=50），<strong>GR00T N1.5</strong>（预测视野 p=16）。</li><li><strong>训练/测试资源</strong>：使用了预训练模型检查点。具体训练资源未在提供文本中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：模拟环境（使用LIBERO和RoboTwin基准）和真实世界机器人（Franka Research 3）操作。</li><li><strong>主要评估指标</strong>：任务<strong>成功率（Success Rate）</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 在LIBERO基准上，<strong>AutoHorizon</strong> 在 π0.5 (p=50) 模型上超越了所有静态基线（Static Oracle, Static Oracle+）和随机基线。</p>\n<p> - 在GR00T N1.5模型上，<strong>AutoHorizon</strong> 同样表现最佳。</p>\n<p> - 在更具挑战性的RoboTwin基准上，<strong>AutoHorizon</strong> 在大多数任务上达到或超过最强基线性能。</p>\n<p> - 真实世界任务（如放置黄瓜/魔方）中，AutoHorizon的动态视野调整策略有效提高了任务完成率。</p>"
  },
  {
    "date": "2026-02-24",
    "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
    "link": "http://arxiv.org/abs/2602.21172",
    "summary_markdown": "## 研究单位\n- **Applied Intuition**\n- **Texas A&M University**\n- **UC Berkeley**\n## 论文概述\n- 论文提出了 **NoRD** 模型，旨在解决当前视觉-语言-行动模型对**海量数据**和**密集推理标注**两大昂贵需求的依赖。\n- 核心研究目标是探索能否在不使用任何推理标注、且训练数据量大幅减少（<60%）的情况下，在自动驾驶基准上达到与现有方法**竞争的性能**。\n## 核心贡献\n- **首次发现**了数据高效且无推理的VLA训练失败源于**难度偏差**，该偏差由弱SFT策略和复杂驾驶奖励共同触发。\n- 实证表征了这一失败，表明数据高效的SFT策略会诱导出两极分化的奖励分布，从而剥夺了**GRPO**的学习信号。\n- 提出使用 **Dr. GRPO** 作为直接替代算法来训练 **NoRD**，这是**首次在自动驾驶领域验证**这种策略优化方法。\n- 在 **NAVSIM** 和 **WaymoE2E** 基准测试上，**在不使用任何推理标注且数据量减少至少60%的情况下**，取得了具有竞争力的性能，同时提升了推理速度。\n## 方法描述\n- 基于 **Qwen-2.5VL-3B-Instruct** 模型构建VLA，采用两阶段训练：1）**使用有限数据的有监督微调**，形成弱的**NoRD-base**策略；2）使用 **Dr. GRPO** 对弱SFT策略进行**强化学习后训练**。\n- 创新点在于使用 **Dr. GRPO** 解决标准 **GRPO** 在优化弱SFT策略时的**难度偏差**问题。Dr. GRPO通过移除GRPO优势估计中的标准差归一化项，使模型能够从高方差（困难）样本中有效学习。\n- 为提高标记效率，使用 **k-disc 标记化**（词汇表大小为2048）将轨迹编码为离散标记，并直接预测未来轨迹。\n## 数据集与资源\n- 主要使用 **NAVSIM** 和 **WaymoE2E** 两个自动驾驶数据集进行训练和评估。\n- 基础模型为 **Qwen-2.5VL-3B-Instruct**，是一个约30亿参数的多模态模型。\n- 训练资源：SFT阶段使用**16个A100 GPU**；RL后训练阶段对NAVSIM使用**30个A100 GPU**，对WaymoE2E使用**32个A100 GPU**。使用了 **verl** 和 **vLLM** 等工具库。\n## 评估与结果\n- 在 **WaymoE2E** 测试集上，**NoRD** 取得了 **RFS 7.709** 的成绩（排名第三），是顶级模型中唯一**不使用推理**和**集成**的模型，且**训练数据量大幅减少**（例如，比Poutine少17倍）。\n- 在 **NAVSIM** 测试集（navtest子集）上，**NoRD** 取得了 **PDM Score 85.6** 的成绩，在**仅使用3个相机帧、无推理、无激光雷达**的条件下，性能与许多BEV方法和VLA方法可比。\n- 在 Best-of-N 评估中，**NoRD-BoN** 取得了 **PDM Score 92.4**，超过了推理模型 **AutoVLA-BoN**。\n- 实验证明，使用 **Dr. GRPO** 优化弱SFT策略（**NoRD-base**）带来了 **+11.68%** 的性能提升，而标准 **GRPO** 仅带来 **+0.67%** 的微小提升。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Applied Intuition</strong></li><li><strong>Texas A&M University</strong></li><li><strong>UC Berkeley</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出了 <strong>NoRD</strong> 模型，旨在解决当前视觉-语言-行动模型对<strong>海量数据</strong>和<strong>密集推理标注</strong>两大昂贵需求的依赖。</li><li>核心研究目标是探索能否在不使用任何推理标注、且训练数据量大幅减少（<60%）的情况下，在自动驾驶基准上达到与现有方法<strong>竞争的性能</strong>。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li><strong>首次发现</strong>了数据高效且无推理的VLA训练失败源于<strong>难度偏差</strong>，该偏差由弱SFT策略和复杂驾驶奖励共同触发。</li><li>实证表征了这一失败，表明数据高效的SFT策略会诱导出两极分化的奖励分布，从而剥夺了<strong>GRPO</strong>的学习信号。</li><li>提出使用 <strong>Dr. GRPO</strong> 作为直接替代算法来训练 <strong>NoRD</strong>，这是<strong>首次在自动驾驶领域验证</strong>这种策略优化方法。</li><li>在 <strong>NAVSIM</strong> 和 <strong>WaymoE2E</strong> 基准测试上，<strong>在不使用任何推理标注且数据量减少至少60%的情况下</strong>，取得了具有竞争力的性能，同时提升了推理速度。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>基于 <strong>Qwen-2.5VL-3B-Instruct</strong> 模型构建VLA，采用两阶段训练：1）<strong>使用有限数据的有监督微调</strong>，形成弱的<strong>NoRD-base</strong>策略；2）使用 <strong>Dr. GRPO</strong> 对弱SFT策略进行<strong>强化学习后训练</strong>。</li><li>创新点在于使用 <strong>Dr. GRPO</strong> 解决标准 <strong>GRPO</strong> 在优化弱SFT策略时的<strong>难度偏差</strong>问题。Dr. GRPO通过移除GRPO优势估计中的标准差归一化项，使模型能够从高方差（困难）样本中有效学习。</li><li>为提高标记效率，使用 <strong>k-disc 标记化</strong>（词汇表大小为2048）将轨迹编码为离散标记，并直接预测未来轨迹。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>主要使用 <strong>NAVSIM</strong> 和 <strong>WaymoE2E</strong> 两个自动驾驶数据集进行训练和评估。</li><li>基础模型为 <strong>Qwen-2.5VL-3B-Instruct</strong>，是一个约30亿参数的多模态模型。</li><li>训练资源：SFT阶段使用<strong>16个A100 GPU</strong>；RL后训练阶段对NAVSIM使用<strong>30个A100 GPU</strong>，对WaymoE2E使用<strong>32个A100 GPU</strong>。使用了 <strong>verl</strong> 和 <strong>vLLM</strong> 等工具库。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>在 <strong>WaymoE2E</strong> 测试集上，<strong>NoRD</strong> 取得了 <strong>RFS 7.709</strong> 的成绩（排名第三），是顶级模型中唯一<strong>不使用推理</strong>和<strong>集成</strong>的模型，且<strong>训练数据量大幅减少</strong>（例如，比Poutine少17倍）。</li><li>在 <strong>NAVSIM</strong> 测试集（navtest子集）上，<strong>NoRD</strong> 取得了 <strong>PDM Score 85.6</strong> 的成绩，在<strong>仅使用3个相机帧、无推理、无激光雷达</strong>的条件下，性能与许多BEV方法和VLA方法可比。</li><li>在 Best-of-N 评估中，<strong>NoRD-BoN</strong> 取得了 <strong>PDM Score 92.4</strong>，超过了推理模型 <strong>AutoVLA-BoN</strong>。</li><li>实验证明，使用 <strong>Dr. GRPO</strong> 优化弱SFT策略（<strong>NoRD-base</strong>）带来了 <strong>+11.68%</strong> 的性能提升，而标准 <strong>GRPO</strong> 仅带来 <strong>+0.67%</strong> 的微小提升。</li></ul>"
  },
  {
    "date": "2026-02-24",
    "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking",
    "link": "http://arxiv.org/abs/2602.21161",
    "summary_markdown": "## 研究单位\n- **University of Cambridge** (CV4DT, CSIC, Department of Engineering)\n- **University of Oxford** (Department of Engineering)\n## 论文概述\n- 提出 **ActionReasoning**，一个**LLM驱动**的框架，用于在3D空间中执行机器人动作推理，以完成物理一致的机器人砖块堆叠操作。\n- 旨在解决单纯依靠数据和参数扩展的**Vision-Language-Action (VLA)** 模型在机器人通用化（如跨任务、跨具身平台和长时程推理）方面存在的局限性。\n- 探索如何利用大型语言模型（LLM）中已有的**物理先验和世界知识**，通过结构化提示和多智能体架构，将其转化为可直接在3D空间（SE(3)）中执行的动作规划，从而弥合机器人感知与执行之间的差距。\n## 核心贡献\n- 引入了 **ActionReasoning** 框架，这是一个**3D动作推理框架**，通过高层级工具调用和提示工程实现机器人决策，无需为每个场景编写专用代码，展现了跨配置的泛化能力。\n- 设计了一个**多智能体LLM编排器**，其中不同阶段的专用智能体将人类任务规范与世界模型输入融合，在SE(3)空间中进行推理，生成物理一致的动作规划。\n- 通过**模拟研究和消融实验**表明，相比传统的、具有相似简单编程的控制方法，基于3D的LLM推理提高了鲁棒性，为未来LLM驱动的机器人物理推理奠定了基础。\n## 方法描述\n- 核心技术是**基于LLM的多智能体动作推理框架**。框架将堆叠任务分解为由六个专门的LLM智能体组成的序列管道，每个智能体负责一个阶段（如预抓取定位、下降、抓取闭合、安全提升、砖块放置、返回就绪位）。\n- 创新点在于**门控机制**和**结构化提示**。每个智能体接收包含当前环境状态、记忆信息、角色定义、知识库、思维链和输出格式的结构化提示，输出可执行路径点或工具调用代码，并通过物理可行性检查（门控）决定是否进入下一阶段。\n- 该方法将机器人控制建模为一个**马尔可夫路径点闭环**：LLM根据当前环境状态 `St` 和目标 `G` 生成下一个末端执行器目标位姿 `w(t+1)`，底层控制器执行插补轨迹，然后更新环境状态并重复此循环。\n## 数据集与资源\n- 使用**自定义的砖块堆叠模拟环境**进行评估，并未提及使用现有的公开大型数据集。\n- 论文未明确说明具体使用的**LLM模型名称、规模或参数量**。\n- 实验在**PyBullet物理模拟器**中进行，使用模拟的**KUKA机器人手臂**。训练/推理所使用的具体计算资源（GPU/TPU）未在提供的文本中明确说明。\n## 评估与结果\n- **评估环境与基准**：在PyBullet模拟器中，对两种堆叠模式（**金字塔式**和**网格式**）进行评估，每种模式进行10次随机初始位姿的试验。与**经典控制器基线**（手写脚本，无事件处理）和**单智能体消融版本**（合并所有智能体功能）进行比较。\n- **主要评估指标**：**位姿精度**（旋转误差、中心偏移误差）和**3D边界框交并比（IoU）**。\n- **关键实验结果**：\n - **ActionReasoning** 显著优于基线：平均旋转误差从1.004°降至0.703°，平均中心偏移从4.314厘米降至0.637厘米，3D IoU从38.38%提升至88.03%。\n - **多智能体设计至关重要**：单智能体消融版本表现不佳，经常碰撞并推倒已堆叠的结构，证明了分阶段角色专业化和门控验证的必要性。\n - 定性结果（图4、5）显示，所提方法实现了明显更整齐的砖块对齐和稳定的堆叠。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>University of Cambridge</strong> (CV4DT, CSIC, Department of Engineering)</li><li><strong>University of Oxford</strong> (Department of Engineering)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>ActionReasoning</strong>，一个<strong>LLM驱动</strong>的框架，用于在3D空间中执行机器人动作推理，以完成物理一致的机器人砖块堆叠操作。</li><li>旨在解决单纯依靠数据和参数扩展的<strong>Vision-Language-Action (VLA)</strong> 模型在机器人通用化（如跨任务、跨具身平台和长时程推理）方面存在的局限性。</li><li>探索如何利用大型语言模型（LLM）中已有的<strong>物理先验和世界知识</strong>，通过结构化提示和多智能体架构，将其转化为可直接在3D空间（SE(3)）中执行的动作规划，从而弥合机器人感知与执行之间的差距。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>引入了 <strong>ActionReasoning</strong> 框架，这是一个<strong>3D动作推理框架</strong>，通过高层级工具调用和提示工程实现机器人决策，无需为每个场景编写专用代码，展现了跨配置的泛化能力。</li><li>设计了一个<strong>多智能体LLM编排器</strong>，其中不同阶段的专用智能体将人类任务规范与世界模型输入融合，在SE(3)空间中进行推理，生成物理一致的动作规划。</li><li>通过<strong>模拟研究和消融实验</strong>表明，相比传统的、具有相似简单编程的控制方法，基于3D的LLM推理提高了鲁棒性，为未来LLM驱动的机器人物理推理奠定了基础。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心技术是<strong>基于LLM的多智能体动作推理框架</strong>。框架将堆叠任务分解为由六个专门的LLM智能体组成的序列管道，每个智能体负责一个阶段（如预抓取定位、下降、抓取闭合、安全提升、砖块放置、返回就绪位）。</li><li>创新点在于<strong>门控机制</strong>和<strong>结构化提示</strong>。每个智能体接收包含当前环境状态、记忆信息、角色定义、知识库、思维链和输出格式的结构化提示，输出可执行路径点或工具调用代码，并通过物理可行性检查（门控）决定是否进入下一阶段。</li><li>该方法将机器人控制建模为一个<strong>马尔可夫路径点闭环</strong>：LLM根据当前环境状态 <code>St</code> 和目标 <code>G</code> 生成下一个末端执行器目标位姿 <code>w(t+1)</code>，底层控制器执行插补轨迹，然后更新环境状态并重复此循环。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用<strong>自定义的砖块堆叠模拟环境</strong>进行评估，并未提及使用现有的公开大型数据集。</li><li>论文未明确说明具体使用的<strong>LLM模型名称、规模或参数量</strong>。</li><li>实验在<strong>PyBullet物理模拟器</strong>中进行，使用模拟的<strong>KUKA机器人手臂</strong>。训练/推理所使用的具体计算资源（GPU/TPU）未在提供的文本中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在PyBullet模拟器中，对两种堆叠模式（<strong>金字塔式</strong>和<strong>网格式</strong>）进行评估，每种模式进行10次随机初始位姿的试验。与<strong>经典控制器基线</strong>（手写脚本，无事件处理）和<strong>单智能体消融版本</strong>（合并所有智能体功能）进行比较。</li><li><strong>主要评估指标</strong>：<strong>位姿精度</strong>（旋转误差、中心偏移误差）和<strong>3D边界框交并比（IoU）</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>ActionReasoning</strong> 显著优于基线：平均旋转误差从1.004°降至0.703°，平均中心偏移从4.314厘米降至0.637厘米，3D IoU从38.38%提升至88.03%。</p>\n<p> - <strong>多智能体设计至关重要</strong>：单智能体消融版本表现不佳，经常碰撞并推倒已堆叠的结构，证明了分阶段角色专业化和门控验证的必要性。</p>\n<p> - 定性结果（图4、5）显示，所提方法实现了明显更整齐的砖块对齐和稳定的堆叠。</p>"
  },
  {
    "date": "2026-02-24",
    "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning",
    "link": "http://arxiv.org/abs/2602.21157",
    "summary_markdown": "## 研究单位\n- **香港科技大学** (The Hong Kong University of Science and Technology)\n- **瑞士洛桑联邦理工学院** (EPFL)\n- **中山大学** (Sun Yat-sen University)\n## 论文概述\n- 提出名为 **HALO** 的统一视觉-语言-动作模型，用于实现具身多模态思维链推理。核心在于模仿人类“思考-想象-执行”的认知过程，通过先进行文本推理和任务规划，再预测视觉子目标提供细粒度引导，最后利用生成的多模态思维链指导动作预测。\n- 旨在解决现有视觉-语言-动作模型在长视野或分布外复杂场景中，因缺乏显式的多模态推理和环境状态演化的预见能力而表现不佳的问题。\n- 研究目标是开发一个既能进行语义推理，又能进行视觉想象和动作预测的统一框架，以提升机器人在复杂、动态环境中的任务执行鲁棒性和泛化能力。\n## 核心贡献\n- 提出 **具身多模态思维链** 框架，将文本推理、视觉预见和动作预测整合到一个序列化流程中，显著提升了模型在复杂任务中的表现。\n- 采用 **混合专家转换器** 架构，将多模态理解、视觉生成和动作预测解耦为三个独立的专家，同时通过共享自注意力机制实现跨专家的无缝协作，避免了单一模型处理异构能力的冲突。\n- 设计了一个自动化的 **EM-CoT 数据合成流水线**，利用动作基元和视觉语言模型对原始机器人轨迹进行标注，大规模生成带有对齐文本推理和视觉子目标的监督数据。\n- 提出一个精心设计的 **两阶段训练方案**：第一阶段进行多样化预训练以建立通用基础；第二阶段进行 EM-CoT 增强的微调，以注入结构化多模态推理能力，同时保留通用知识。\n- 在模拟和真实环境中进行了广泛的实验验证，证明了 **HALO** 的卓越性能，在 **RoboTwin 2.0** 基准测试中平均成功率超过基线模型 **π₀** 达 34.1%，并展示了强大的泛化能力。\n## 方法描述\n- 核心方法是 **HALO** 模型，其核心是一个统一的 **混合专家转换器** 架构，包含三个专家：**多模态理解专家** (用于文本推理)、**视觉生成专家** (用于预测视觉子目标图像) 和**动作预测专家**。\n- 模型通过特殊标记控制模态切换，并通过精心设计的注意力掩码策略管理信息流，确保推理的自回归性和视觉生成的全局依赖性，同时防止信息泄露。\n- 创新点在于：1）将文本推理、视觉生成和动作预测解耦为独立的专家，避免了单一流程的性能瓶颈；2）通过共享自注意力实现跨模态的丰富交互；3）模仿人类认知流程，先推理后行动。\n- 关键技术包括：使用 **Qwen2.5-1.5B** 作为 LLM 骨干，使用 **SigLIP2** 和 **NaViT** 初始化用于理解任务的视觉编码器，使用 **VAE** 进行图像压缩用于生成任务，并采用线性投影处理动作。\n## 数据集与资源\n- 使用的数据集包括：用于预训练的 **LLaVA-NeXT-779k** (VQA)、**Open X-Embodiment** 和 **Something-Something v2** (视觉生成)，以及用于微调的 **RoboTwin 2.0** 模拟数据集和自收集的真实世界机器人数据集。\n- 模型总参数量约为 **45 亿**，基于三个初始化的 **Qwen2.5-1.5B** 专家构建。\n- 训练资源：使用了 **H800/A100 GPU**，并采用 **Flex Attention** 技术来加速具有超长序列的训练。\n## 评估与结果\n- 评估环境主要为 **RoboTwin 2.0** 模拟器，包含 50 个具有挑战性的双手操作任务，并设置了“简单”（清洁环境）和“困难”（强域随机化）两种评估设置。同时在 **Cobot Mobile ALOHA** 平台上进行了真实世界评估。\n- 主要评估指标是 **任务成功率**。\n- 关键实验结果：在 RoboTwin 2.0 上，**HALO** 在简单和困难设置下的平均成功率分别达到 **80.5%** 和 **26.4%**，显著超越了 **π₀**、**RDT-1B** 和 **Diffusion Policy** 等基线模型。消融研究证实了 EM-CoT 设计和两阶段训练方案的所有组成部分都对性能提升有贡献。真实世界实验在工具使用、双手协作等任务上也展现了优越的性能和泛化能力。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>香港科技大学</strong> (The Hong Kong University of Science and Technology)</li><li><strong>瑞士洛桑联邦理工学院</strong> (EPFL)</li><li><strong>中山大学</strong> (Sun Yat-sen University)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出名为 <strong>HALO</strong> 的统一视觉-语言-动作模型，用于实现具身多模态思维链推理。核心在于模仿人类“思考-想象-执行”的认知过程，通过先进行文本推理和任务规划，再预测视觉子目标提供细粒度引导，最后利用生成的多模态思维链指导动作预测。</li><li>旨在解决现有视觉-语言-动作模型在长视野或分布外复杂场景中，因缺乏显式的多模态推理和环境状态演化的预见能力而表现不佳的问题。</li><li>研究目标是开发一个既能进行语义推理，又能进行视觉想象和动作预测的统一框架，以提升机器人在复杂、动态环境中的任务执行鲁棒性和泛化能力。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出 <strong>具身多模态思维链</strong> 框架，将文本推理、视觉预见和动作预测整合到一个序列化流程中，显著提升了模型在复杂任务中的表现。</li><li>采用 <strong>混合专家转换器</strong> 架构，将多模态理解、视觉生成和动作预测解耦为三个独立的专家，同时通过共享自注意力机制实现跨专家的无缝协作，避免了单一模型处理异构能力的冲突。</li><li>设计了一个自动化的 <strong>EM-CoT 数据合成流水线</strong>，利用动作基元和视觉语言模型对原始机器人轨迹进行标注，大规模生成带有对齐文本推理和视觉子目标的监督数据。</li><li>提出一个精心设计的 <strong>两阶段训练方案</strong>：第一阶段进行多样化预训练以建立通用基础；第二阶段进行 EM-CoT 增强的微调，以注入结构化多模态推理能力，同时保留通用知识。</li><li>在模拟和真实环境中进行了广泛的实验验证，证明了 <strong>HALO</strong> 的卓越性能，在 <strong>RoboTwin 2.0</strong> 基准测试中平均成功率超过基线模型 <strong>π₀</strong> 达 34.1%，并展示了强大的泛化能力。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心方法是 <strong>HALO</strong> 模型，其核心是一个统一的 <strong>混合专家转换器</strong> 架构，包含三个专家：<strong>多模态理解专家</strong> (用于文本推理)、<strong>视觉生成专家</strong> (用于预测视觉子目标图像) 和<strong>动作预测专家</strong>。</li><li>模型通过特殊标记控制模态切换，并通过精心设计的注意力掩码策略管理信息流，确保推理的自回归性和视觉生成的全局依赖性，同时防止信息泄露。</li><li>创新点在于：1）将文本推理、视觉生成和动作预测解耦为独立的专家，避免了单一流程的性能瓶颈；2）通过共享自注意力实现跨模态的丰富交互；3）模仿人类认知流程，先推理后行动。</li><li>关键技术包括：使用 <strong>Qwen2.5-1.5B</strong> 作为 LLM 骨干，使用 <strong>SigLIP2</strong> 和 <strong>NaViT</strong> 初始化用于理解任务的视觉编码器，使用 <strong>VAE</strong> 进行图像压缩用于生成任务，并采用线性投影处理动作。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的数据集包括：用于预训练的 <strong>LLaVA-NeXT-779k</strong> (VQA)、<strong>Open X-Embodiment</strong> 和 <strong>Something-Something v2</strong> (视觉生成)，以及用于微调的 <strong>RoboTwin 2.0</strong> 模拟数据集和自收集的真实世界机器人数据集。</li><li>模型总参数量约为 <strong>45 亿</strong>，基于三个初始化的 <strong>Qwen2.5-1.5B</strong> 专家构建。</li><li>训练资源：使用了 <strong>H800/A100 GPU</strong>，并采用 <strong>Flex Attention</strong> 技术来加速具有超长序列的训练。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境主要为 <strong>RoboTwin 2.0</strong> 模拟器，包含 50 个具有挑战性的双手操作任务，并设置了“简单”（清洁环境）和“困难”（强域随机化）两种评估设置。同时在 <strong>Cobot Mobile ALOHA</strong> 平台上进行了真实世界评估。</li><li>主要评估指标是 <strong>任务成功率</strong>。</li><li>关键实验结果：在 RoboTwin 2.0 上，<strong>HALO</strong> 在简单和困难设置下的平均成功率分别达到 <strong>80.5%</strong> 和 <strong>26.4%</strong>，显著超越了 <strong>π₀</strong>、<strong>RDT-1B</strong> 和 <strong>Diffusion Policy</strong> 等基线模型。消融研究证实了 EM-CoT 设计和两阶段训练方案的所有组成部分都对性能提升有贡献。真实世界实验在工具使用、双手协作等任务上也展现了优越的性能和泛化能力。</li></ul>"
  },
  {
    "date": "2026-02-24",
    "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks",
    "link": "http://arxiv.org/abs/2602.21013",
    "summary_markdown": "## 研究单位\n- **Qualcomm AI Research** (Qualcomm Technologies, Inc.)\n## 论文概述\n- 提出了一种为 **视觉-语言-行动 (Vision-Language-Action, VLA)** 模型引入显式记忆机制的方法：**语言便笺 (Language Scratchpad)**。\n- 旨在解决现有“无状态”VLA模型在依赖**时空记忆**的长周期、多步骤操作任务（如物体位置交换）上性能不佳的问题。\n- 通过让模型生成、存储和利用语言描述来记录任务计划、环境状态（空间记忆）和子任务进度（时间记忆），从而解决任务的非马尔可夫特性问题。\n## 核心贡献\n- 提出一种**基于语言便笺的简单记忆机制**，使VLA模型能够创建行动计划，并基于此计划执行和条件化后续动作。\n- 引入了**ClevrSkills-Mem** 评测基准，这是一个从ClevrSkills环境中提取的、包含五个依赖记忆的操作任务的数据集，用于系统地评估模型的时空记忆能力。\n- 展示了便笺机制能显著提升**非循环 (T-VLA)** 和**循环 (R-VLA)** 策略在记忆依赖任务上的性能，平均成功率分别提升约48%和11%。\n- 在**MemoryBench** 基准和**真实世界**的拾取-放置-恢复任务上验证了该方法的有效性，证明了其泛化能力。\n## 方法描述\n- 在标准的VLA模型基础上，引入一个语言描述的**便笺**作为输入和输出的一部分。模型在每一步接收当前观测、指令和便笺，并同时输出动作和一条语言描述。\n- 关键创新在于便笺的**动态更新机制**：当模型完成一个子任务时，会预测一个特殊的`<done>`标记，触发将该步骤的描述（即“思考”）加入便笺，供后续步骤使用。\n- 便笺内容结构化，包含**环境初始化信息（接地）**、**任务计划（规划）**和**已完成动作（执行）**三部分，灵活地编码了空间和时间记忆。\n- 该方法被同时应用于**基于Transformer的VLA** 和**基于循环架构（如Mamba）的VLA**。\n## 数据集与资源\n- 使用的数据集：**ClevrSkills-Mem** （基于ManiSkill2），**MemoryBench** （基于RLBench），以及自建的**真实世界拾取-放置-恢复**数据集（200条轨迹）。\n- 模型规模与参数：\n - **T-VLA**：基于**PaliGemma-2** VLM（3B参数）。\n - **R-VLA**：基于**Mamba** 语言模型（130M参数）和ViT视觉骨干。\n- 训练资源：实验在**4块NVIDIA A100 GPU** 上进行。\n## 评估与结果\n- 评估环境：**ClevrSkills-Mem**（5个任务）、**MemoryBench**（Put-Block-Back任务）和真实世界机器人平台（**UFACTORY xArm 6**）。\n- 主要评估指标：**任务成功率**、**子任务完成率**、**物体最终放置的准确度（平均距离）**。\n- 关键实验结果：\n - 在**ClevrSkills-Mem**上，T-VLA使用便笺后平均成功率提升了**48.8%**；R-VLA也获得了**11%**的平均提升。\n - 在**MemoryBench**的Put-Block-Back任务中，T-VLA使用便笺的成功率为**40%**；在简化评估下（接近按钮时使用真实动作）达到**100%**。\n - 在真实世界的拾取-放置-恢复任务中，基于**OpenVLA**的模型在添加便笺后，成功率从**0%**提升至**65%**。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Qualcomm AI Research</strong> (Qualcomm Technologies, Inc.)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种为 <strong>视觉-语言-行动 (Vision-Language-Action, VLA)</strong> 模型引入显式记忆机制的方法：<strong>语言便笺 (Language Scratchpad)</strong>。</li><li>旨在解决现有“无状态”VLA模型在依赖<strong>时空记忆</strong>的长周期、多步骤操作任务（如物体位置交换）上性能不佳的问题。</li><li>通过让模型生成、存储和利用语言描述来记录任务计划、环境状态（空间记忆）和子任务进度（时间记忆），从而解决任务的非马尔可夫特性问题。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出一种<strong>基于语言便笺的简单记忆机制</strong>，使VLA模型能够创建行动计划，并基于此计划执行和条件化后续动作。</li><li>引入了<strong>ClevrSkills-Mem</strong> 评测基准，这是一个从ClevrSkills环境中提取的、包含五个依赖记忆的操作任务的数据集，用于系统地评估模型的时空记忆能力。</li><li>展示了便笺机制能显著提升<strong>非循环 (T-VLA)</strong> 和<strong>循环 (R-VLA)</strong> 策略在记忆依赖任务上的性能，平均成功率分别提升约48%和11%。</li><li>在<strong>MemoryBench</strong> 基准和<strong>真实世界</strong>的拾取-放置-恢复任务上验证了该方法的有效性，证明了其泛化能力。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>在标准的VLA模型基础上，引入一个语言描述的<strong>便笺</strong>作为输入和输出的一部分。模型在每一步接收当前观测、指令和便笺，并同时输出动作和一条语言描述。</li><li>关键创新在于便笺的<strong>动态更新机制</strong>：当模型完成一个子任务时，会预测一个特殊的<code><done></code>标记，触发将该步骤的描述（即“思考”）加入便笺，供后续步骤使用。</li><li>便笺内容结构化，包含<strong>环境初始化信息（接地）</strong>、<strong>任务计划（规划）</strong>和<strong>已完成动作（执行）</strong>三部分，灵活地编码了空间和时间记忆。</li><li>该方法被同时应用于<strong>基于Transformer的VLA</strong> 和<strong>基于循环架构（如Mamba）的VLA</strong>。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的数据集：<strong>ClevrSkills-Mem</strong> （基于ManiSkill2），<strong>MemoryBench</strong> （基于RLBench），以及自建的<strong>真实世界拾取-放置-恢复</strong>数据集（200条轨迹）。</li><li>模型规模与参数：</li></ul>\n<p> - <strong>T-VLA</strong>：基于<strong>PaliGemma-2</strong> VLM（3B参数）。</p>\n<p> - <strong>R-VLA</strong>：基于<strong>Mamba</strong> 语言模型（130M参数）和ViT视觉骨干。</p>\n<ul><li>训练资源：实验在<strong>4块NVIDIA A100 GPU</strong> 上进行。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：<strong>ClevrSkills-Mem</strong>（5个任务）、<strong>MemoryBench</strong>（Put-Block-Back任务）和真实世界机器人平台（<strong>UFACTORY xArm 6</strong>）。</li><li>主要评估指标：<strong>任务成功率</strong>、<strong>子任务完成率</strong>、<strong>物体最终放置的准确度（平均距离）</strong>。</li><li>关键实验结果：</li></ul>\n<p> - 在<strong>ClevrSkills-Mem</strong>上，T-VLA使用便笺后平均成功率提升了<strong>48.8%</strong>；R-VLA也获得了<strong>11%</strong>的平均提升。</p>\n<p> - 在<strong>MemoryBench</strong>的Put-Block-Back任务中，T-VLA使用便笺的成功率为<strong>40%</strong>；在简化评估下（接近按钮时使用真实动作）达到<strong>100%</strong>。</p>\n<p> - 在真实世界的拾取-放置-恢复任务中，基于<strong>OpenVLA</strong>的模型在添加便笺后，成功率从<strong>0%</strong>提升至<strong>65%</strong>。</p>"
  },
  {
    "date": "2026-02-24",
    "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation",
    "link": "http://arxiv.org/abs/2602.20715",
    "summary_markdown": "## 研究单位\n- **浙江大学机械工程系 Grasp 实验室**\n- **Torch Kernel Co., Ltd.**\n## 论文概述\n- 提出一个名为 **IG-RFT** 的交互引导强化学习微调框架，用于提升**基于流的视觉-语言-动作模型**在**长视野、复杂真实世界机器人操作任务**中的性能。\n- 旨在解决**监督式微调**在高质量演示数据稀缺和分布偏移时的局限性，以及**传统强化学习**在现实世界应用中的探索效率、稳定性和样本成本挑战。\n## 核心贡献\n- 提出**交互引导优势加权回归算法**，通过机器人交互状态动态调制探索强度，有效平衡探索与利用。\n- 设计一种**混合稠密奖励函数**，融合轨迹级奖励和子任务级奖励，为长视野任务提供通用的、稠密的程序性反馈。\n- 构建了一个**多阶段强化学习微调系统**，整合**监督式微调**、**离线强化学习**和**人在回路强化学习**，实现稳定、高效的政策学习与迭代精炼。\n- 在四个具有挑战性的真实世界长视野操作任务上进行了广泛实验，验证了系统的有效性。\n## 方法描述\n- 核心技术为 **IG-RFT** 框架，包含三个核心模块：1) **IG-AWR** 算法；2) **混合稠密奖励函数**；3) **多阶段训练系统**。\n- 创新点在于利用**视觉交互信号**（通过机器人分割模型和光流估计自动提取）动态调节基于流模型的初始采样噪声方差：在非交互阶段鼓励探索，在精细交互阶段强调稳健性。\n- 奖励函数设计具有通用性，仅依赖于轨迹长度，结合了均匀分布的轨迹级成功奖励和基于子任务进度的势能奖励。\n## 数据集与资源\n- 每个任务收集了**60条专家演示数据**用于初始数据集，并在人在回路阶段额外进行了**40次真实世界策略部署与数据收集**。\n- 基础策略模型为**π₀.₅**，这是一个经过大规模预训练的、基于流的视觉-语言-动作模型。\n- 训练资源包括配备 **NVIDIA A100 GPU** 的云服务器，用于策略推理和训练；本地部署使用 **Intel Core i7 CPU** 和 **RTX 4070 GPU**。\n## 评估与结果\n- **评估环境**：**真实机器人平台**（Galaxea A1双臂机器人，配备Orbbec和RealSense相机），在四个长视野复杂任务（包裹打包、水果装袋、积木堆叠、饮料上架）上进行。\n- **评估指标**：主要评估指标为**成功率**。\n- **关键结果**：**IG-RFT** 在四个任务上的平均成功率高达 **85.0%**，显著优于 **监督式微调** 的 **18.8%** 和标准**离线强化学习基线**的 **~40.0%**。消融实验证实了**交互引导**和**混合奖励**各自的关键贡献。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>浙江大学机械工程系 Grasp 实验室</strong></li><li><strong>Torch Kernel Co., Ltd.</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出一个名为 <strong>IG-RFT</strong> 的交互引导强化学习微调框架，用于提升<strong>基于流的视觉-语言-动作模型</strong>在<strong>长视野、复杂真实世界机器人操作任务</strong>中的性能。</li><li>旨在解决<strong>监督式微调</strong>在高质量演示数据稀缺和分布偏移时的局限性，以及<strong>传统强化学习</strong>在现实世界应用中的探索效率、稳定性和样本成本挑战。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出<strong>交互引导优势加权回归算法</strong>，通过机器人交互状态动态调制探索强度，有效平衡探索与利用。</li><li>设计一种<strong>混合稠密奖励函数</strong>，融合轨迹级奖励和子任务级奖励，为长视野任务提供通用的、稠密的程序性反馈。</li><li>构建了一个<strong>多阶段强化学习微调系统</strong>，整合<strong>监督式微调</strong>、<strong>离线强化学习</strong>和<strong>人在回路强化学习</strong>，实现稳定、高效的政策学习与迭代精炼。</li><li>在四个具有挑战性的真实世界长视野操作任务上进行了广泛实验，验证了系统的有效性。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心技术为 <strong>IG-RFT</strong> 框架，包含三个核心模块：1) <strong>IG-AWR</strong> 算法；2) <strong>混合稠密奖励函数</strong>；3) <strong>多阶段训练系统</strong>。</li><li>创新点在于利用<strong>视觉交互信号</strong>（通过机器人分割模型和光流估计自动提取）动态调节基于流模型的初始采样噪声方差：在非交互阶段鼓励探索，在精细交互阶段强调稳健性。</li><li>奖励函数设计具有通用性，仅依赖于轨迹长度，结合了均匀分布的轨迹级成功奖励和基于子任务进度的势能奖励。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>每个任务收集了<strong>60条专家演示数据</strong>用于初始数据集，并在人在回路阶段额外进行了<strong>40次真实世界策略部署与数据收集</strong>。</li><li>基础策略模型为<strong>π₀.₅</strong>，这是一个经过大规模预训练的、基于流的视觉-语言-动作模型。</li><li>训练资源包括配备 <strong>NVIDIA A100 GPU</strong> 的云服务器，用于策略推理和训练；本地部署使用 <strong>Intel Core i7 CPU</strong> 和 <strong>RTX 4070 GPU</strong>。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：<strong>真实机器人平台</strong>（Galaxea A1双臂机器人，配备Orbbec和RealSense相机），在四个长视野复杂任务（包裹打包、水果装袋、积木堆叠、饮料上架）上进行。</li><li><strong>评估指标</strong>：主要评估指标为<strong>成功率</strong>。</li><li><strong>关键结果</strong>：<strong>IG-RFT</strong> 在四个任务上的平均成功率高达 <strong>85.0%</strong>，显著优于 <strong>监督式微调</strong> 的 <strong>18.8%</strong> 和标准<strong>离线强化学习基线</strong>的 <strong>~40.0%</strong>。消融实验证实了<strong>交互引导</strong>和<strong>混合奖励</strong>各自的关键贡献。</li></ul>"
  },
  {
    "date": "2026-02-24",
    "title": "Recursive Belief Vision Language Model",
    "link": "http://arxiv.org/abs/2602.20659",
    "summary_markdown": "## 研究单位\n- 论文作者未明确列出其所属研究机构。根据作者姓名（Vaidehi Bagaria, Bijo Sebastian, Nirav Patel）和致谢内容，推断其可能与学术研究机构或工业研究实验室相关，但原文未直接提供单位信息。\n## 论文概述\n- 论文提出了一个名为 **Recursive Belief VLA (RB-VLA)** 的新型视觉语言动作模型架构，旨在解决长视野、部分可观测环境下的机器人操作任务。\n- 核心目标是克服现有视觉语言动作模型（VLA）中存在的短期记忆依赖、高推理延迟、以及在多阶段任务中由于感知混淆和遮挡导致的任务状态丢失问题。\n- 论文主要解决在部分可观测环境中进行长视野、多阶段操作时，模型缺乏持续、与动作相关的内部状态表示，从而导致任务执行不稳定和效率低下的问题。\n## 核心贡献\n- 提出**基于信念的VLA框架**：引入一个固定大小、递归更新的**信念**状态，用于编码与任务相关的交互历史、系统动力学和对象状态，支持部分可观测下的因果推理。\n- 实现**阶段感知的长视野控制**：该递归信念为扩散策略提供短期和长期的预测上下文，支持隐式的阶段估计，实现动态一致的闭环控制，替代了短观察窗口。\n- 采用**解耦的语义与控制系统**：将语义推理（通过**VLM**完成）视为一次性过程（每个任务仅查询一次以提取高级意图），避免了密集的语义重复推理，显著降低了计算开销。\n- 在**长视野基准测试中进行了实证验证**：在长视野、多阶段操作任务以及遮挡条件下，**RB-VLA** 大幅超越了现有的VLA方法，同时保持了恒定的内存使用和显著更低的推理延迟。\n- 通过**消融研究**证明了信念模块是性能提升的主要驱动力，将无信念模块时的成功率从32.5%提升到了有信念模块时的77.5%。\n## 方法描述\n- **系统架构**：**RB-VLA** 由三部分组成：一个视觉语言模块（用于生成静态的**意图**嵌入）、一个信念估计器（递归维护**信念**状态）和一个基于**扩散**策略的控制器。\n- **递归信念估计器**：采用类**RSSM**的世界模型训练目标，使用基于**Transformer**的模块集成多模态观察（视觉、本体感觉、动作）的短期窗口以及先前的**信念**状态，生成一个紧凑的、固定维度的潜在**信念**状态。该信念通过预测未来状态和自我监督目标进行训练，使其能够捕捉动作条件下的动态变化。\n- **创新点**：主要创新在于引入并学习一个持续的内部**信念**表示，该表示总结了过去动作的后果和任务进度，从而在无需存储原始观察或频繁调用**VLM**的情况下，实现稳健的、阶段感知的控制。信念的递归特性确保了内存使用不随时间增长。\n## 数据集与资源\n- **使用数据集**：**RoboSuite**、**LIBERO-Long**、**LIBERO-Object** 和一个 **UR5 MuJoCo** 环境。共使用了40,000条模拟操作轨迹（约4千万时间步），涵盖拾放、堆叠和家庭操作等任务。\n- **模型规模与参数量**：\n - **信念模型**：使用 **DINOv2 ViT-S/14** 视觉编码器，包含**1.5亿**参数。\n - **VLM**：基于 **Qwen2.5-VL-7B** 模型（保持冻结）。\n - **扩散控制器**：包含**1.2亿**参数。\n- **训练资源**：所有实验在 **12 个 NVIDIA A100 GPU (80GB)** 上使用分布式数据并行训练完成。\n## 评估与结果\n- **评估环境与基准**：在模拟环境（**RoboSuite**、**LIBERO-Long**）和真实世界（**UR5机械臂**）中评估。与基线模型如 **GROOT-N1**、**OpenVLA**、**π₀** 进行比较。\n- **主要评估指标**：任务成功率、推理延迟（每回合平均时间）、内存使用增长、信念表示质量分析。\n- **关键实验结果**：\n - **长视野任务成功率**：**RB-VLA** 在多物体拾放和堆叠任务中显著优于基线。例如，在遮挡下的多阶段拾放任务中，达到 **77.5%** 的成功率，相比 **π₀**（25.0%）提高了 **52.5%**。\n - **效率提升**：推理延迟相比基线（如 **OpenVLA**）降低了高达 **5倍**。内存使用保持恒定，不随观察帧数增长，而基线模型的内存使用则呈超线性增长。\n - **消融研究**：完整 **RB-VLA**（包含信念）在双物体拾放任务中成功率为 **77.5%**，移除信念后（仅扩散策略）成功率降至 **32.5%**。\n - **真实世界部署**：经过少量微调，在真实 **UR5** 机械臂上成功执行多物体拾放任务，成功率达到 **68.0%**。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li>论文作者未明确列出其所属研究机构。根据作者姓名（Vaidehi Bagaria, Bijo Sebastian, Nirav Patel）和致谢内容，推断其可能与学术研究机构或工业研究实验室相关，但原文未直接提供单位信息。</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出了一个名为 <strong>Recursive Belief VLA (RB-VLA)</strong> 的新型视觉语言动作模型架构，旨在解决长视野、部分可观测环境下的机器人操作任务。</li><li>核心目标是克服现有视觉语言动作模型（VLA）中存在的短期记忆依赖、高推理延迟、以及在多阶段任务中由于感知混淆和遮挡导致的任务状态丢失问题。</li><li>论文主要解决在部分可观测环境中进行长视野、多阶段操作时，模型缺乏持续、与动作相关的内部状态表示，从而导致任务执行不稳定和效率低下的问题。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出<strong>基于信念的VLA框架</strong>：引入一个固定大小、递归更新的<strong>信念</strong>状态，用于编码与任务相关的交互历史、系统动力学和对象状态，支持部分可观测下的因果推理。</li><li>实现<strong>阶段感知的长视野控制</strong>：该递归信念为扩散策略提供短期和长期的预测上下文，支持隐式的阶段估计，实现动态一致的闭环控制，替代了短观察窗口。</li><li>采用<strong>解耦的语义与控制系统</strong>：将语义推理（通过<strong>VLM</strong>完成）视为一次性过程（每个任务仅查询一次以提取高级意图），避免了密集的语义重复推理，显著降低了计算开销。</li><li>在<strong>长视野基准测试中进行了实证验证</strong>：在长视野、多阶段操作任务以及遮挡条件下，<strong>RB-VLA</strong> 大幅超越了现有的VLA方法，同时保持了恒定的内存使用和显著更低的推理延迟。</li><li>通过<strong>消融研究</strong>证明了信念模块是性能提升的主要驱动力，将无信念模块时的成功率从32.5%提升到了有信念模块时的77.5%。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>系统架构</strong>：<strong>RB-VLA</strong> 由三部分组成：一个视觉语言模块（用于生成静态的<strong>意图</strong>嵌入）、一个信念估计器（递归维护<strong>信念</strong>状态）和一个基于<strong>扩散</strong>策略的控制器。</li><li><strong>递归信念估计器</strong>：采用类<strong>RSSM</strong>的世界模型训练目标，使用基于<strong>Transformer</strong>的模块集成多模态观察（视觉、本体感觉、动作）的短期窗口以及先前的<strong>信念</strong>状态，生成一个紧凑的、固定维度的潜在<strong>信念</strong>状态。该信念通过预测未来状态和自我监督目标进行训练，使其能够捕捉动作条件下的动态变化。</li><li><strong>创新点</strong>：主要创新在于引入并学习一个持续的内部<strong>信念</strong>表示，该表示总结了过去动作的后果和任务进度，从而在无需存储原始观察或频繁调用<strong>VLM</strong>的情况下，实现稳健的、阶段感知的控制。信念的递归特性确保了内存使用不随时间增长。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>使用数据集</strong>：<strong>RoboSuite</strong>、<strong>LIBERO-Long</strong>、<strong>LIBERO-Object</strong> 和一个 <strong>UR5 MuJoCo</strong> 环境。共使用了40,000条模拟操作轨迹（约4千万时间步），涵盖拾放、堆叠和家庭操作等任务。</li><li><strong>模型规模与参数量</strong>：</li></ul>\n<p> - <strong>信念模型</strong>：使用 <strong>DINOv2 ViT-S/14</strong> 视觉编码器，包含<strong>1.5亿</strong>参数。</p>\n<p> - <strong>VLM</strong>：基于 <strong>Qwen2.5-VL-7B</strong> 模型（保持冻结）。</p>\n<p> - <strong>扩散控制器</strong>：包含<strong>1.2亿</strong>参数。</p>\n<ul><li><strong>训练资源</strong>：所有实验在 <strong>12 个 NVIDIA A100 GPU (80GB)</strong> 上使用分布式数据并行训练完成。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在模拟环境（<strong>RoboSuite</strong>、<strong>LIBERO-Long</strong>）和真实世界（<strong>UR5机械臂</strong>）中评估。与基线模型如 <strong>GROOT-N1</strong>、<strong>OpenVLA</strong>、<strong>π₀</strong> 进行比较。</li><li><strong>主要评估指标</strong>：任务成功率、推理延迟（每回合平均时间）、内存使用增长、信念表示质量分析。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>长视野任务成功率</strong>：<strong>RB-VLA</strong> 在多物体拾放和堆叠任务中显著优于基线。例如，在遮挡下的多阶段拾放任务中，达到 <strong>77.5%</strong> 的成功率，相比 <strong>π₀</strong>（25.0%）提高了 <strong>52.5%</strong>。</p>\n<p> - <strong>效率提升</strong>：推理延迟相比基线（如 <strong>OpenVLA</strong>）降低了高达 <strong>5倍</strong>。内存使用保持恒定，不随观察帧数增长，而基线模型的内存使用则呈超线性增长。</p>\n<p> - <strong>消融研究</strong>：完整 <strong>RB-VLA</strong>（包含信念）在双物体拾放任务中成功率为 <strong>77.5%</strong>，移除信念后（仅扩散策略）成功率降至 <strong>32.5%</strong>。</p>\n<p> - <strong>真实世界部署</strong>：经过少量微调，在真实 <strong>UR5</strong> 机械臂上成功执行多物体拾放任务，成功率达到 <strong>68.0%</strong>。</p>"
  },
  {
    "date": "2026-02-24",
    "title": "An interactive enhanced driving dataset for autonomous driving",
    "link": "http://arxiv.org/abs/2602.20575",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-24",
    "title": "BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model",
    "link": "http://arxiv.org/abs/2602.20566",
    "summary_markdown": "## 研究单位\n- **中国科学院软件研究所**\n- **中国科学院大学**\n- **LimX Dynamic (深圳)**\n- **南京大学**\n- **南洋理工大学**\n- **浙江大学**\n## 论文概述\n- 提出 **BFA++**，一种专门为多视角视觉-语言-动作模型设计的动态令牌剪枝框架，旨在解决冗余视觉令牌带来的实时性挑战。\n- 核心目标是：**提升推理速度** 并 **提高机器人操作成功率**，通过感知任务的关键信息，剔除多视角输入中的冗余和干扰信息。\n- 要解决的问题：现有面向大视觉语言模型的令牌剪枝技术直接应用于VLA模型时，由于忽略了**视图间动态关系**和**任务特定特性**，导致性能下降。\n## 核心贡献\n- 提出一种**分层（局部分层和全局分层）令牌剪枝方法**，通过**视图间重要性预测器** 和 **视图内重要性预测器** 这两种指导信号，在保留任务关键信息的同时鲁棒地消除冗余令牌。\n- 设计了一个全面的**离线两级重要性标注系统**，用于生成监督数据，支持两个重要性预测器的训练。\n- 实验表明，该**即插即用框架**可有效应用于 **π0** 和 **RDT** 两种主流的VLA模型，实现了 **1.5-1.8倍** 的推理加速，同时将操作成功率提升了约 **10%**。\n- 分析表明，BFA++ 能有效分离不同视图的令牌分布，使视觉编码器的注意力更集中于任务关键区域（如机械夹爪、目标物体）。\n## 方法描述\n- 提出 **BFA++**，一个用于VLA模型的分层令牌剪枝框架。\n- **关键技术/创新点**：\n - **两级重要性预测器**：**视图间重要性预测器** 动态评估不同相机视图在整个操作阶段的重要性；**视图内重要性预测器** 识别每个视图内任务相关区域（如夹爪、目标物体）的令牌重要性。\n - **分层剪枝策略**：首先基于视图内重要性分数进行**局部剪枝**，剔除每个视图内不重要的令牌；然后融合视图间和视图内重要性分数进行**全局排名**，跨所有视图统一剔除最不重要的令牌。\n - **监督训练**：两个重要性预测器在VLA后训练阶段，与主模型联合优化，使用包含动作预测损失和两个重要性预测辅助损失的总损失函数。\n## 数据集与资源\n- **使用数据集**：**RoboTwin** 基准数据集、**RoboTwin2** 的域外任务、5个真实世界机器人任务。\n- **模型规模**：基于两个主流VLA模型：**π0** 和 **RDT-1B**。\n- **训练资源**：使用 **8块 NVIDIA A100 GPU** 进行训练。推理在 **单块 NVIDIA RTX 3090 GPU** 上测试。\n## 评估与结果\n- **评估环境与基准**：在 **RoboTwin** 仿真基准（8个任务）、**RoboTwin2** 的域外任务（4个任务）以及 **5个真实世界机器人任务** 上进行评估。\n- **主要评估指标**：**任务成功率** 和 **推理控制频率**。\n- **关键实验结果**：\n - 在 **π0** 模型上，BFA++ 实现了 **1.8倍** 加速（从6.5 Hz提升至10.3 Hz），平均成功率提升约 **10%**。\n - 在 **RDT** 模型上，实现了 **1.5倍** 加速（从1.0 Hz提升至1.5 Hz），平均成功率同样提升约 **10%**。\n - 在真实世界任务和域外任务上，BFA++ 的表现也显著优于基线方法（**π0**, **RDT**）和其他剪枝方法（**BFA**, **DART**）。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>中国科学院软件研究所</strong></li><li><strong>中国科学院大学</strong></li><li><strong>LimX Dynamic (深圳)</strong></li><li><strong>南京大学</strong></li><li><strong>南洋理工大学</strong></li><li><strong>浙江大学</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>BFA++</strong>，一种专门为多视角视觉-语言-动作模型设计的动态令牌剪枝框架，旨在解决冗余视觉令牌带来的实时性挑战。</li><li>核心目标是：<strong>提升推理速度</strong> 并 <strong>提高机器人操作成功率</strong>，通过感知任务的关键信息，剔除多视角输入中的冗余和干扰信息。</li><li>要解决的问题：现有面向大视觉语言模型的令牌剪枝技术直接应用于VLA模型时，由于忽略了<strong>视图间动态关系</strong>和<strong>任务特定特性</strong>，导致性能下降。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出一种<strong>分层（局部分层和全局分层）令牌剪枝方法</strong>，通过<strong>视图间重要性预测器</strong> 和 <strong>视图内重要性预测器</strong> 这两种指导信号，在保留任务关键信息的同时鲁棒地消除冗余令牌。</li><li>设计了一个全面的<strong>离线两级重要性标注系统</strong>，用于生成监督数据，支持两个重要性预测器的训练。</li><li>实验表明，该<strong>即插即用框架</strong>可有效应用于 <strong>π0</strong> 和 <strong>RDT</strong> 两种主流的VLA模型，实现了 <strong>1.5-1.8倍</strong> 的推理加速，同时将操作成功率提升了约 <strong>10%</strong>。</li><li>分析表明，BFA++ 能有效分离不同视图的令牌分布，使视觉编码器的注意力更集中于任务关键区域（如机械夹爪、目标物体）。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>提出 <strong>BFA++</strong>，一个用于VLA模型的分层令牌剪枝框架。</li><li><strong>关键技术/创新点</strong>：</li></ul>\n<p> - <strong>两级重要性预测器</strong>：<strong>视图间重要性预测器</strong> 动态评估不同相机视图在整个操作阶段的重要性；<strong>视图内重要性预测器</strong> 识别每个视图内任务相关区域（如夹爪、目标物体）的令牌重要性。</p>\n<p> - <strong>分层剪枝策略</strong>：首先基于视图内重要性分数进行<strong>局部剪枝</strong>，剔除每个视图内不重要的令牌；然后融合视图间和视图内重要性分数进行<strong>全局排名</strong>，跨所有视图统一剔除最不重要的令牌。</p>\n<p> - <strong>监督训练</strong>：两个重要性预测器在VLA后训练阶段，与主模型联合优化，使用包含动作预测损失和两个重要性预测辅助损失的总损失函数。</p>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>使用数据集</strong>：<strong>RoboTwin</strong> 基准数据集、<strong>RoboTwin2</strong> 的域外任务、5个真实世界机器人任务。</li><li><strong>模型规模</strong>：基于两个主流VLA模型：<strong>π0</strong> 和 <strong>RDT-1B</strong>。</li><li><strong>训练资源</strong>：使用 <strong>8块 NVIDIA A100 GPU</strong> 进行训练。推理在 <strong>单块 NVIDIA RTX 3090 GPU</strong> 上测试。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在 <strong>RoboTwin</strong> 仿真基准（8个任务）、<strong>RoboTwin2</strong> 的域外任务（4个任务）以及 <strong>5个真实世界机器人任务</strong> 上进行评估。</li><li><strong>主要评估指标</strong>：<strong>任务成功率</strong> 和 <strong>推理控制频率</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 在 <strong>π0</strong> 模型上，BFA++ 实现了 <strong>1.8倍</strong> 加速（从6.5 Hz提升至10.3 Hz），平均成功率提升约 <strong>10%</strong>。</p>\n<p> - 在 <strong>RDT</strong> 模型上，实现了 <strong>1.5倍</strong> 加速（从1.0 Hz提升至1.5 Hz），平均成功率同样提升约 <strong>10%</strong>。</p>\n<p> - 在真实世界任务和域外任务上，BFA++ 的表现也显著优于基线方法（<strong>π0</strong>, <strong>RDT</strong>）和其他剪枝方法（<strong>BFA</strong>, <strong>DART</strong>）。</p>"
  },
  {
    "date": "2026-02-23",
    "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.20309",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n俄亥俄州立大学 (The Ohio State University)，印第安纳大学 (Indiana University)，密歇根大学 (University of Michigan)，香港城市大学 (City University of Hong Kong)\n## 论文概述\n本文介绍了QuantVLA，一个专门为视觉-语言-动作模型设计的、免训练的后训练量化框架。该框架旨在解决VLA模型（尤其是基于扩散变换器的动作头）在部署时面临的计算和内存瓶颈。QuantVLA采用了一种尺度校准的PTQ方法，可以在保持架构不变、无需重新训练的情况下，对语言主干和DiT动作头进行有效的低比特量化，从而显著降低内存占用，同时维持甚至超越全精度基线的任务成功率。\n## 论文核心贡献点\n1. 首次系统分析了基于DiT的VLA模型的量化敏感性，揭示了导致PTQ失效的关键原因。\n2. 提出了QuantVLA，这是第一个用于VLA模型的免训练PTQ框架，能够在低精度推理下实现最先进的性能，并在部署时实现显著的内存节省。\n## 论文方法描述\nQuantVLA框架包含三个尺度校准组件：\n1. **选择性量化布局**：对语言主干中的所有线性层和DiT中的所有MLP层进行整数量化，同时保持注意力投影（Q, K, V, O）为浮点数，以保留原始操作调度并避免放大分析中识别的两种误差漂移（注意力温度漂移和残差流能量漂移）。\n2. **注意力温度匹配**：一种轻量级的每头缩放机制，用于稳定注意力logits。它通过校准估算一个每头标量α，用于在推理时调整logits的温度，并折叠到去量化尺度中。\n3. **输出头平衡**：一种每层残差接口校准，用于缓解投影后的能量漂移。它通过校准估算一个每层标量β，用于匹配残差接口处的投影后能量，并折叠到去量化尺度中。\n该框架基于DuQuant（一种结合通道平滑、块正交旋转和zigzag通道置换的鲁棒重参数化方法）进行构建，并针对VLA模型中语言与动作模块的紧密耦合引入了上述校准机制。\n## 论文使用数据集和训练资源\n- **评估数据集**：主要在LIBERO模拟器上进行评估。LIBERO包含四个任务套件：Spatial（空间关系推理）、Object（物体操作）、Goal（指令-目标对齐）和Long（长时程任务）。\n- **扩展评估**：还在Simpler操作基准上进行了跨任务鲁棒性评估（详见附录）。\n- **训练资源**：实验在NVIDIA A100 GPU上进行。QuantVLA本身是免训练的后训练量化方法，仅需少量未标记的校准缓冲区用于估计ATM和OHB的标量参数。\n## 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO模拟器。\n- **评估指标**：标准LIBERO协议下的**任务成功率**。具体报告了Spatial、Object、Goal、Long四个子任务套件的成功率及其平均值。同时，还报告了量化模块（LLM+DiT）的**内存占用**（GB）以及相对于基线的**相对内存节省百分比**。\n- **对比方法**：将QuantVLA与全精度基线（FP16）以及基线PTQ方法DuQuant进行了比较。实验结果表明，QuantVLA在W4A8量化下，对于OpenPI π0.5和GR00T N1.5模型，不仅能将平均成功率保持在接近或超过基线水平（π0.5: 97.6% vs 97.1%；GR00T N1.5: 88.0% vs 86.5%），还能实现显著的内存节省（π0.5: 1.28GB vs 4.27GB，节省70.0%；GR00T N1.5: 0.91GB vs 2.02GB，节省55.0%），而直接应用DuQuant会导致性能大幅下降。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>俄亥俄州立大学 (The Ohio State University)，印第安纳大学 (Indiana University)，密歇根大学 (University of Michigan)，香港城市大学 (City University of Hong Kong)</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文介绍了QuantVLA，一个专门为视觉-语言-动作模型设计的、免训练的后训练量化框架。该框架旨在解决VLA模型（尤其是基于扩散变换器的动作头）在部署时面临的计算和内存瓶颈。QuantVLA采用了一种尺度校准的PTQ方法，可以在保持架构不变、无需重新训练的情况下，对语言主干和DiT动作头进行有效的低比特量化，从而显著降低内存占用，同时维持甚至超越全精度基线的任务成功率。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>首次系统分析了基于DiT的VLA模型的量化敏感性，揭示了导致PTQ失效的关键原因。</li><li>提出了QuantVLA，这是第一个用于VLA模型的免训练PTQ框架，能够在低精度推理下实现最先进的性能，并在部署时实现显著的内存节省。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>QuantVLA框架包含三个尺度校准组件：</p>\n<ol><li><strong>选择性量化布局</strong>：对语言主干中的所有线性层和DiT中的所有MLP层进行整数量化，同时保持注意力投影（Q, K, V, O）为浮点数，以保留原始操作调度并避免放大分析中识别的两种误差漂移（注意力温度漂移和残差流能量漂移）。</li><li><strong>注意力温度匹配</strong>：一种轻量级的每头缩放机制，用于稳定注意力logits。它通过校准估算一个每头标量α，用于在推理时调整logits的温度，并折叠到去量化尺度中。</li><li><strong>输出头平衡</strong>：一种每层残差接口校准，用于缓解投影后的能量漂移。它通过校准估算一个每层标量β，用于匹配残差接口处的投影后能量，并折叠到去量化尺度中。</li></ol>\n<p>该框架基于DuQuant（一种结合通道平滑、块正交旋转和zigzag通道置换的鲁棒重参数化方法）进行构建，并针对VLA模型中语言与动作模块的紧密耦合引入了上述校准机制。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>评估数据集</strong>：主要在LIBERO模拟器上进行评估。LIBERO包含四个任务套件：Spatial（空间关系推理）、Object（物体操作）、Goal（指令-目标对齐）和Long（长时程任务）。</li><li><strong>扩展评估</strong>：还在Simpler操作基准上进行了跨任务鲁棒性评估（详见附录）。</li><li><strong>训练资源</strong>：实验在NVIDIA A100 GPU上进行。QuantVLA本身是免训练的后训练量化方法，仅需少量未标记的校准缓冲区用于估计ATM和OHB的标量参数。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：LIBERO模拟器。</li><li><strong>评估指标</strong>：标准LIBERO协议下的<strong>任务成功率</strong>。具体报告了Spatial、Object、Goal、Long四个子任务套件的成功率及其平均值。同时，还报告了量化模块（LLM+DiT）的<strong>内存占用</strong>（GB）以及相对于基线的<strong>相对内存节省百分比</strong>。</li><li><strong>对比方法</strong>：将QuantVLA与全精度基线（FP16）以及基线PTQ方法DuQuant进行了比较。实验结果表明，QuantVLA在W4A8量化下，对于OpenPI π0.5和GR00T N1.5模型，不仅能将平均成功率保持在接近或超过基线水平（π0.5: 97.6% vs 97.1%；GR00T N1.5: 88.0% vs 86.5%），还能实现显著的内存节省（π0.5: 1.28GB vs 4.27GB，节省70.0%；GR00T N1.5: 0.91GB vs 2.02GB，节省55.0%），而直接应用DuQuant会导致性能大幅下降。</li></ul>"
  },
  {
    "date": "2026-02-23",
    "title": "UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.20231",
    "summary_markdown": "## 研究单位\n- **美国北卡罗来纳大学夏洛特分校** 计算机科学系\n## 论文概述\n- 提出 **UniLACT** 模型，首次将深度信息融入到视觉-语言-动作模型的**无监督潜在动作表示预训练**中，以增强机器人的三维空间理解和操作精度。\n- 旨在解决现有基于RGB的潜在动作表示**缺乏显式三维几何结构**，导致在接触丰富的精确操作任务（如抓取、放置）中性能受限的问题。\n- 引入 **UniLARN** 框架，联合学习RGB和深度模态下特定于模态的以及统一的潜在动作表示，为下游策略学习提供更强的空间先验。\n## 核心贡献\n- 提出 **UniLARN** 统一潜在动作学习框架，基于逆动力学和正动力学目标，在共享嵌入空间中学习RGB和深度模态的表示，并显式建模其跨模态交互。\n- 提出 **UniLACT** 模型，一个基于Transformer的视觉-语言-动作模型，通过跨模态潜在动作预训练，整合了语义和几何信息。\n- 在模拟和真实世界实验中，验证了深度感知的统一潜在动作表示的有效性。**UniLACT** 在域内和域外预训练、以及已见和未见任务上，均显著优于仅基于RGB的基线模型。\n## 方法描述\n- **UniLARN**：使用两阶段向量量化流程，通过逆动力学模型和正动力学模型，从RGB-D视频帧对中学习模态特定及统一的离散潜在动作表示。设计了一个共享的嵌入空间和跨模态融合机制。\n- **UniLACT**：采用三阶段训练：（1）用**UniLARN**生成伪动作标签；（2）对**UniLACT**进行潜在动作预训练，使其能够基于视觉观察和任务指令预测统一的潜在动作；（3）使用有限的机器人演示数据进行动作微调，将预测的潜在动作映射为可执行的7-DoF末端执行器增量控制指令。**深度仅在训练时使用，推理时只需RGB图像**。\n## 数据集与资源\n- 主要模拟数据集：**CALVIN** 基准测试数据集（约18K条轨迹，含RGB-D观测和自然语言注释）。\n- 预训练数据：**Open X-Embodiment** 数据集子集。\n- 真实世界数据：在xArm7机械臂上收集的演示数据（每任务30条），使用**Intel RealSense D435i** RGB-D相机。\n- 模型：**UniLACT** 基于**GPT-2**因果Transformer主干，视觉编码器使用**ViT-L**，语言编码器使用**T5**。\n- 训练资源：在**4块 NVIDIA H200 GPU**上进行实验。\n## 评估与结果\n- **评估基准**：**CALVIN ABC→D** 基准测试（模拟）；真实世界4个桌面操作任务。\n- **主要指标**：任务序列平均完成长度、单任务成功率。\n- **关键结果**：\n - 在**CALVIN ABC→D** 模拟实验中，**UniLACT**（平均长度2.86）优于核心RGB基线**Moto**（2.60）。使用**OXE**进行域外预训练时，**UniLACT** 以 **+29.2%** 的相对优势超越**Moto**。\n - 在真实世界实验中，**UniLACT** 在4个任务上的平均成功率（62.5%）比**Moto**（52.5%）高出**10%**，尤其在几何感知任务上表现更稳定。\n - 计算效率与基线相当，推理时仅使用RGB，保持相同的模型参数量（89.8M）和每步延迟（27ms）。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>美国北卡罗来纳大学夏洛特分校</strong> 计算机科学系</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>UniLACT</strong> 模型，首次将深度信息融入到视觉-语言-动作模型的<strong>无监督潜在动作表示预训练</strong>中，以增强机器人的三维空间理解和操作精度。</li><li>旨在解决现有基于RGB的潜在动作表示<strong>缺乏显式三维几何结构</strong>，导致在接触丰富的精确操作任务（如抓取、放置）中性能受限的问题。</li><li>引入 <strong>UniLARN</strong> 框架，联合学习RGB和深度模态下特定于模态的以及统一的潜在动作表示，为下游策略学习提供更强的空间先验。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出 <strong>UniLARN</strong> 统一潜在动作学习框架，基于逆动力学和正动力学目标，在共享嵌入空间中学习RGB和深度模态的表示，并显式建模其跨模态交互。</li><li>提出 <strong>UniLACT</strong> 模型，一个基于Transformer的视觉-语言-动作模型，通过跨模态潜在动作预训练，整合了语义和几何信息。</li><li>在模拟和真实世界实验中，验证了深度感知的统一潜在动作表示的有效性。<strong>UniLACT</strong> 在域内和域外预训练、以及已见和未见任务上，均显著优于仅基于RGB的基线模型。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>UniLARN</strong>：使用两阶段向量量化流程，通过逆动力学模型和正动力学模型，从RGB-D视频帧对中学习模态特定及统一的离散潜在动作表示。设计了一个共享的嵌入空间和跨模态融合机制。</li><li><strong>UniLACT</strong>：采用三阶段训练：（1）用<strong>UniLARN</strong>生成伪动作标签；（2）对<strong>UniLACT</strong>进行潜在动作预训练，使其能够基于视觉观察和任务指令预测统一的潜在动作；（3）使用有限的机器人演示数据进行动作微调，将预测的潜在动作映射为可执行的7-DoF末端执行器增量控制指令。<strong>深度仅在训练时使用，推理时只需RGB图像</strong>。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>主要模拟数据集：<strong>CALVIN</strong> 基准测试数据集（约18K条轨迹，含RGB-D观测和自然语言注释）。</li><li>预训练数据：<strong>Open X-Embodiment</strong> 数据集子集。</li><li>真实世界数据：在xArm7机械臂上收集的演示数据（每任务30条），使用<strong>Intel RealSense D435i</strong> RGB-D相机。</li><li>模型：<strong>UniLACT</strong> 基于<strong>GPT-2</strong>因果Transformer主干，视觉编码器使用<strong>ViT-L</strong>，语言编码器使用<strong>T5</strong>。</li><li>训练资源：在<strong>4块 NVIDIA H200 GPU</strong>上进行实验。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估基准</strong>：<strong>CALVIN ABC→D</strong> 基准测试（模拟）；真实世界4个桌面操作任务。</li><li><strong>主要指标</strong>：任务序列平均完成长度、单任务成功率。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - 在<strong>CALVIN ABC→D</strong> 模拟实验中，<strong>UniLACT</strong>（平均长度2.86）优于核心RGB基线<strong>Moto</strong>（2.60）。使用<strong>OXE</strong>进行域外预训练时，<strong>UniLACT</strong> 以 <strong>+29.2%</strong> 的相对优势超越<strong>Moto</strong>。</p>\n<p> - 在真实世界实验中，<strong>UniLACT</strong> 在4个任务上的平均成功率（62.5%）比<strong>Moto</strong>（52.5%）高出<strong>10%</strong>，尤其在几何感知任务上表现更稳定。</p>\n<p> - 计算效率与基线相当，推理时仅使用RGB，保持相同的模型参数量（89.8M）和每步延迟（27ms）。</p>"
  },
  {
    "date": "2026-02-23",
    "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies",
    "link": "http://arxiv.org/abs/2602.19710",
    "summary_markdown": "## 研究单位\n- **Tencent Robotics X**\n- **The Hong Kong University of Science and Technology**\n- **Fudan University**\n## 论文概述\n- 提出了一种名为 **Pose-VLA** 的解耦学习范式，用于构建可泛化的视觉-语言-动作模型。\n- 旨在解决现有 VLA 模型因感知与稀疏动作监督纠缠而导致的**特征崩溃**和**训练效率低下**问题，并缓解视觉-语言模型在高层语义识别和机器人任务所需**细粒度三维状态感知**之间的错位。\n- 核心思想是通过**统一的姿态表示**（姿态令牌）和**两阶段训练**，分离空间先验学习与具身对齐。\n## 核心贡献\n- 提出了一个将RGB图像、深度图和相机内参（射线图）统一整合的**VLM框架**，以注入内在的3D感知，促进视觉-语言知识向机器人控制的有效迁移。\n- 引入了**离散姿态令牌**作为一种通用接口，在统一的相机中心观测空间中，用于对齐和吸收来自异构非机器人3D数据和专用机器人演示的空间先验。\n- 构建了一个综合的**预训练语料库**，包含140万张图像和650万个3D标注用于空间感知，以及约155万条机器人轨迹用于运动对齐。\n- 实验证明，**Pose-VLA** 在 **RoboTwin 2.0** 上达到79.5%的平均成功率（SOTA），在 **LIBERO** 上达到96.0%的竞争性性能，并在真实世界任务中仅需每个任务100个演示就能实现强大的泛化能力。\n## 方法描述\n- 基于 **PaliGemma** 架构，采用解耦的两阶段训练策略：1）**预训练阶段**：在统一的相机中心空间中，利用大规模3D数据集（目标检测、6D姿态估计）和机器人轨迹数据，通过下一个令牌预测损失提取通用3D空间先验。2）**后训练阶段**：附加一个轻量级**动作专家模块**（使用流匹配目标）进行具身对齐，将预训练表示映射到机器人特定命令。\n- 关键创新在于**离散姿态令牌**（`<trans_xy>`, `<trans_z>`, `<rot>`, `<size>`）作为通用表示，统一了物体和末端执行器的3D状态与运动轨迹。同时整合了**相机射线编码**和**深度先验**作为附加空间信息，采用模态掩码策略确保推理时对不同传感器输入的鲁棒性。\n## 数据集与资源\n- **空间感知预训练数据集**：总计约 **140万张图像**，包含 **650万个3D标注**，来自 **Omni3D**、**Omni6DPose** 和 **BOP Challenge** 等多个数据集。\n- **机器人轨迹预训练数据集**：约 **155万条轨迹**，来自 **AgibotWorld Beta**（真实机器人，100万台）和 **InternData-A1**（仿真，多平台）。\n- **模型规模**：基于3B参数的 **PaliGemma** 模型。\n- **训练资源**：在 **16 块 NVIDIA H20 GPU** 上进行训练，使用 bfloat16 精度。\n## 评估与结果\n- **评估环境与基准**：\n - **3D空间感知基准**：在 **Omni3D** 的 **SUN-RGBD** 和 **Objectron** 子集上评估。\n - **仿真基准**：在 **RoboTwin 2.0** 和 **LIBERO** 四个任务套件上进行评估。\n - **真实世界实验**：在包含刚性、关节式和可变形物体的多样化任务上评估。\n- **主要评估指标**：\n - **3D感知**：采用 **mAP@0.15**（AP15）。\n - **机器人任务**：**平均成功率**。\n- **关键实验结果**：\n - **3D感知**：在 **Objectron** 数据集上达到 **87.3 AP15**，显著超越 **Qwen3-VL**、**Seed1.5-VL** 等基线模型。\n - **RoboTwin 2.0**：在 **Hard** 设置下平均成功率达 **79.1%**，超越 **π₀**（65.1%）和 **PaliGemma_expert**（33.4%）基线。\n - **LIBERO**：平均成功率达 **96.0%**，与最先进的 **π₀.₅**（96.8%）性能相当，展现出强大的跨任务泛化能力。\n - **真实世界**：单一多任务模型仅需**每个任务100个演示**，就能在多样化物体上实现鲁棒泛化。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Tencent Robotics X</strong></li><li><strong>The Hong Kong University of Science and Technology</strong></li><li><strong>Fudan University</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种名为 <strong>Pose-VLA</strong> 的解耦学习范式，用于构建可泛化的视觉-语言-动作模型。</li><li>旨在解决现有 VLA 模型因感知与稀疏动作监督纠缠而导致的<strong>特征崩溃</strong>和<strong>训练效率低下</strong>问题，并缓解视觉-语言模型在高层语义识别和机器人任务所需<strong>细粒度三维状态感知</strong>之间的错位。</li><li>核心思想是通过<strong>统一的姿态表示</strong>（姿态令牌）和<strong>两阶段训练</strong>，分离空间先验学习与具身对齐。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了一个将RGB图像、深度图和相机内参（射线图）统一整合的<strong>VLM框架</strong>，以注入内在的3D感知，促进视觉-语言知识向机器人控制的有效迁移。</li><li>引入了<strong>离散姿态令牌</strong>作为一种通用接口，在统一的相机中心观测空间中，用于对齐和吸收来自异构非机器人3D数据和专用机器人演示的空间先验。</li><li>构建了一个综合的<strong>预训练语料库</strong>，包含140万张图像和650万个3D标注用于空间感知，以及约155万条机器人轨迹用于运动对齐。</li><li>实验证明，<strong>Pose-VLA</strong> 在 <strong>RoboTwin 2.0</strong> 上达到79.5%的平均成功率（SOTA），在 <strong>LIBERO</strong> 上达到96.0%的竞争性性能，并在真实世界任务中仅需每个任务100个演示就能实现强大的泛化能力。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>基于 <strong>PaliGemma</strong> 架构，采用解耦的两阶段训练策略：1）<strong>预训练阶段</strong>：在统一的相机中心空间中，利用大规模3D数据集（目标检测、6D姿态估计）和机器人轨迹数据，通过下一个令牌预测损失提取通用3D空间先验。2）<strong>后训练阶段</strong>：附加一个轻量级<strong>动作专家模块</strong>（使用流匹配目标）进行具身对齐，将预训练表示映射到机器人特定命令。</li><li>关键创新在于<strong>离散姿态令牌</strong>（<code><trans_xy></code>, <code><trans_z></code>, <code><rot></code>, <code><size></code>）作为通用表示，统一了物体和末端执行器的3D状态与运动轨迹。同时整合了<strong>相机射线编码</strong>和<strong>深度先验</strong>作为附加空间信息，采用模态掩码策略确保推理时对不同传感器输入的鲁棒性。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>空间感知预训练数据集</strong>：总计约 <strong>140万张图像</strong>，包含 <strong>650万个3D标注</strong>，来自 <strong>Omni3D</strong>、<strong>Omni6DPose</strong> 和 <strong>BOP Challenge</strong> 等多个数据集。</li><li><strong>机器人轨迹预训练数据集</strong>：约 <strong>155万条轨迹</strong>，来自 <strong>AgibotWorld Beta</strong>（真实机器人，100万台）和 <strong>InternData-A1</strong>（仿真，多平台）。</li><li><strong>模型规模</strong>：基于3B参数的 <strong>PaliGemma</strong> 模型。</li><li><strong>训练资源</strong>：在 <strong>16 块 NVIDIA H20 GPU</strong> 上进行训练，使用 bfloat16 精度。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：</li></ul>\n<p> - <strong>3D空间感知基准</strong>：在 <strong>Omni3D</strong> 的 <strong>SUN-RGBD</strong> 和 <strong>Objectron</strong> 子集上评估。</p>\n<p> - <strong>仿真基准</strong>：在 <strong>RoboTwin 2.0</strong> 和 <strong>LIBERO</strong> 四个任务套件上进行评估。</p>\n<p> - <strong>真实世界实验</strong>：在包含刚性、关节式和可变形物体的多样化任务上评估。</p>\n<ul><li><strong>主要评估指标</strong>：</li></ul>\n<p> - <strong>3D感知</strong>：采用 <strong>mAP@0.15</strong>（AP15）。</p>\n<p> - <strong>机器人任务</strong>：<strong>平均成功率</strong>。</p>\n<ul><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>3D感知</strong>：在 <strong>Objectron</strong> 数据集上达到 <strong>87.3 AP15</strong>，显著超越 <strong>Qwen3-VL</strong>、<strong>Seed1.5-VL</strong> 等基线模型。</p>\n<p> - <strong>RoboTwin 2.0</strong>：在 <strong>Hard</strong> 设置下平均成功率达 <strong>79.1%</strong>，超越 <strong>π₀</strong>（65.1%）和 <strong>PaliGemma_expert</strong>（33.4%）基线。</p>\n<p> - <strong>LIBERO</strong>：平均成功率达 <strong>96.0%</strong>，与最先进的 <strong>π₀.₅</strong>（96.8%）性能相当，展现出强大的跨任务泛化能力。</p>\n<p> - <strong>真实世界</strong>：单一多任务模型仅需<strong>每个任务100个演示</strong>，就能在多样化物体上实现鲁棒泛化。</p>"
  },
  {
    "date": "2026-02-22",
    "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
    "link": "http://arxiv.org/abs/2602.19313",
    "summary_markdown": "## 研究单位\n- **华盛顿大学**\n- **加州大学伯克利分校**\n- **多伦多大学向量研究所**\n- **上海人工智能实验室**\n- **艾伦人工智能研究所**\n## 论文概述\n- 提出了一种名为 **TOPReward** 的零样本奖励模型，其利用预训练视频-语言模型内部的**token概率**（而非文本输出）来估计机器人任务进度。\n- 旨在解决现实世界机器人强化学习中**样本效率低**和**稀疏奖励**的问题，提供一种无需特定任务微调的、可泛化的密集奖励信号。\n- 引入了名为 **ManiRewardBench** 的新基准，用于严格评估现实世界机器人操作任务上的奖励模型。\n## 核心贡献\n- 提出了**TOPReward**：一种新的、基于概率的时间价值函数，通过查询模型对“轨迹是否完成任务”的置信度（即“True” token的对数概率）来获得连续的奖励信号，从而绕过了语言模型在数值生成和指令遵循方面的瓶颈。\n- 展示了**TOPReward** 在开源视觉语言模型（如 **Qwen3-VL-8B** 和 **Molmo2-8B**）上的有效性，显著优于需要数值文本输出的SOTA零样本方法 **GVL**。\n- 引入了**ManiRewardBench**：一个包含130+个现实世界操作任务、跨多种机器人平台（Franka， YAM， SO-100/101）并带有精细时间标注（子任务边界）的奖励模型基准。\n- 验证了**TOPReward** 作为多功能工具可用于下游应用，包括**成功检测**和用于策略改进的**优势加权行为克隆**（TOP-AWR），在真实机器人任务上持续超越标准行为克隆。\n## 方法描述\n- **核心方法**：给定任务指令和视频轨迹前缀，构建一个提示询问模型“该轨迹是否完成了任务”，然后提取模型对肯定回答token（如“True”）的**对数概率**作为奖励信号。\n- **关键技术**：该方法完全避免了自回归文本生成，直接利用模型内部的**对数概率分布**，从而更稳定地反映模型对任务完成的潜在“信念”。\n- **流程**：对轨迹的多个均匀间隔的前缀进行采样，计算每个前缀的奖励，然后通过**最小-最大归一化**将奖励值映射到[0,1]区间作为进度估计，并可进一步计算**奖励增量**用于密集奖励。\n## 数据集与资源\n- 使用的主要数据集：**ManiRewardBench**（内部收集，130+任务，497+成功轨迹，156失败轨迹）和 **Open X-Embodiment**（OXE）数据集的一个子集（39个数据集）。\n- 模型：主要基于开源视频-语言模型进行评估，包括 **Qwen3-VL-8B**、**Molmo2-8B**，并与专有模型 **Gemini-2.5-Pro** 进行对比。\n- 训练/评估资源：论文未明确说明具体训练资源（因其为零样本方法，无需训练）。评估涉及对大量轨迹前缀进行前向传播计算。\n## 评估与结果\n- **评估基准**：在 **ManiRewardBench** 和 **Open X-Embodiment** 数据集上，与 **GVL** 进行对比。\n- **主要指标**：**价值顺序相关性**，用于衡量预测进度值与时间顺序的斯皮尔曼秩相关性。\n- **关键结果**：\n - 在 **Qwen3-VL-8B** 上，**TOPReward** 在ManiRewardBench上达到 **0.947** 的平均VOC，在OXE上达到 **0.857**，而GVL在同一模型上的表现分别接近零或不一致。\n - **TOPReward** 在**成功检测**（ROC-AUC）任务上优于GVL（Qwen3-VL-8B：0.654 vs 0.519）。\n - 在6个真实世界单臂SO-100操作任务中，使用**TOPReward** 进行优势加权行为克隆（TOP-AWR）在10次试验的总部分成功率上持续优于标准行为克隆，例如在“将娃娃放入盒子”任务中达到10/10 vs 7/10。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>华盛顿大学</strong></li><li><strong>加州大学伯克利分校</strong></li><li><strong>多伦多大学向量研究所</strong></li><li><strong>上海人工智能实验室</strong></li><li><strong>艾伦人工智能研究所</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种名为 <strong>TOPReward</strong> 的零样本奖励模型，其利用预训练视频-语言模型内部的<strong>token概率</strong>（而非文本输出）来估计机器人任务进度。</li><li>旨在解决现实世界机器人强化学习中<strong>样本效率低</strong>和<strong>稀疏奖励</strong>的问题，提供一种无需特定任务微调的、可泛化的密集奖励信号。</li><li>引入了名为 <strong>ManiRewardBench</strong> 的新基准，用于严格评估现实世界机器人操作任务上的奖励模型。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了<strong>TOPReward</strong>：一种新的、基于概率的时间价值函数，通过查询模型对“轨迹是否完成任务”的置信度（即“True” token的对数概率）来获得连续的奖励信号，从而绕过了语言模型在数值生成和指令遵循方面的瓶颈。</li><li>展示了<strong>TOPReward</strong> 在开源视觉语言模型（如 <strong>Qwen3-VL-8B</strong> 和 <strong>Molmo2-8B</strong>）上的有效性，显著优于需要数值文本输出的SOTA零样本方法 <strong>GVL</strong>。</li><li>引入了<strong>ManiRewardBench</strong>：一个包含130+个现实世界操作任务、跨多种机器人平台（Franka， YAM， SO-100/101）并带有精细时间标注（子任务边界）的奖励模型基准。</li><li>验证了<strong>TOPReward</strong> 作为多功能工具可用于下游应用，包括<strong>成功检测</strong>和用于策略改进的<strong>优势加权行为克隆</strong>（TOP-AWR），在真实机器人任务上持续超越标准行为克隆。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>核心方法</strong>：给定任务指令和视频轨迹前缀，构建一个提示询问模型“该轨迹是否完成了任务”，然后提取模型对肯定回答token（如“True”）的<strong>对数概率</strong>作为奖励信号。</li><li><strong>关键技术</strong>：该方法完全避免了自回归文本生成，直接利用模型内部的<strong>对数概率分布</strong>，从而更稳定地反映模型对任务完成的潜在“信念”。</li><li><strong>流程</strong>：对轨迹的多个均匀间隔的前缀进行采样，计算每个前缀的奖励，然后通过<strong>最小-最大归一化</strong>将奖励值映射到[0,1]区间作为进度估计，并可进一步计算<strong>奖励增量</strong>用于密集奖励。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的主要数据集：<strong>ManiRewardBench</strong>（内部收集，130+任务，497+成功轨迹，156失败轨迹）和 <strong>Open X-Embodiment</strong>（OXE）数据集的一个子集（39个数据集）。</li><li>模型：主要基于开源视频-语言模型进行评估，包括 <strong>Qwen3-VL-8B</strong>、<strong>Molmo2-8B</strong>，并与专有模型 <strong>Gemini-2.5-Pro</strong> 进行对比。</li><li>训练/评估资源：论文未明确说明具体训练资源（因其为零样本方法，无需训练）。评估涉及对大量轨迹前缀进行前向传播计算。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估基准</strong>：在 <strong>ManiRewardBench</strong> 和 <strong>Open X-Embodiment</strong> 数据集上，与 <strong>GVL</strong> 进行对比。</li><li><strong>主要指标</strong>：<strong>价值顺序相关性</strong>，用于衡量预测进度值与时间顺序的斯皮尔曼秩相关性。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - 在 <strong>Qwen3-VL-8B</strong> 上，<strong>TOPReward</strong> 在ManiRewardBench上达到 <strong>0.947</strong> 的平均VOC，在OXE上达到 <strong>0.857</strong>，而GVL在同一模型上的表现分别接近零或不一致。</p>\n<p> - <strong>TOPReward</strong> 在<strong>成功检测</strong>（ROC-AUC）任务上优于GVL（Qwen3-VL-8B：0.654 vs 0.519）。</p>\n<p> - 在6个真实世界单臂SO-100操作任务中，使用<strong>TOPReward</strong> 进行优势加权行为克隆（TOP-AWR）在10次试验的总部分成功率上持续优于标准行为克隆，例如在“将娃娃放入盒子”任务中达到10/10 vs 7/10。</p>"
  },
  {
    "date": "2026-02-22",
    "title": "The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption",
    "link": "http://arxiv.org/abs/2602.19260",
    "summary_markdown": "## 研究单位\n- **Human-Robot Interaction Lab, Tufts University**, Medford, MA, USA\n- **AIT Austrian Institute of Technology GmbH**, Center for Vision, Automation & Control, Vienna, Austria\n## 论文概述\n- 在结构化、长视野的操作任务上，对**微调的开源视觉-语言-动作模型**与**神经符号架构**进行全面的经验比较。\n- 评估两种方法在解决结构化的“汉诺塔”任务中的**任务性能和能耗**，并测试其**泛化能力**。\n- 核心问题是评估在结构化长视野操作任务中，直接微调大规模端到端基础模型是否比采用结合显式符号推理的架构更有效、更高效。\n## 核心贡献\n- 在结构化长视野操作任务（3块汉诺塔）上，**神经符号模型的成功率显著高于VLA模型**（95% vs. 34%）。\n- 神经符号模型在**能耗方面表现极佳**，其训练能耗比VLA微调低近两个数量级，且推理过程**无需GPU**。\n- 神经符号模型展现出**强大的泛化能力**，在没有训练过的4块汉诺塔任务上达到78%的成功率，而两种VLA模型均完全失败。\n- 提供了一个**基于汉诺塔的基准任务**和一个系统性的比较框架，用于分析不同架构在性能和效率上的权衡。\n- 强调了对于具有明确程序约束的任务，**显式结合符号结构**在可靠性、数据效率和能源效率方面的优势。\n## 方法描述\n- **比较对象1：VLA模型**。基于开源的 **`π₀`** 模型，使用LoRA在自定义数据集上进行微调。评估了两种变体：**端到端VLA**（接收单一高级指令）和**规划引导VLA**（接收分解的子任务指令）。\n- **比较对象2：神经符号模型**。借鉴先前工作，结合了**基于PDDL的符号规划器**和**基于扩散模型的低级控制策略**。它从演示中**同时学习符号抽象域和底层神经策略**。\n- **关键技术**：神经符号模型的核心创新在于其**分层架构**。首先通过**ASP求解器**从少量演示中推断符号规划域，然后为每个符号算子训练一个**扩散模仿学习策略**来执行，实现了**结构化的长视野规划与连续控制**的结合。\n## 数据集与资源\n- **训练数据集**：在 **`Robosuite`** 仿真环境中构建的自定义汉诺塔任务数据集。VLA使用300个全任务轨迹演示，神经符号模型仅使用50个堆叠任务演示。\n- **模型规模**：VLA模型使用**PaliGemma 2B**作为视觉语言主干，**Gemma 300M**作为动作头。神经符号模型的规划器与策略网络规模较小。\n- **训练资源**：所有实验（训练、微调、评估）均在一张 **NVIDIA GeForce RTX 4090 (24GB)** 上完成。使用Weights & Biases和RAPL日志监测GPU/CPU功耗和能耗。\n## 评估与结果\n- **评估环境**：在 `Robosuite` 仿真中进行的结构化汉诺塔任务（单步移动、3块、4块）。\n- **主要指标**：**任务成功率**、任务推进率、**总能耗**（GPU+CPU）、单次执行时长。\n- **关键结果**：\n - **任务性能**：在3块任务上，**NSM**成功率95%，**E2E-VLA**34%，**PG-VLA**0%。在4块任务上，**NSM**成功率78%，两种VLA均为0%。\n - **训练能耗**：**VLA微调**总能耗约65-68 MJ，**NSM训练**仅需0.85 MJ，相差近**两个数量级**。\n - **推理能耗**：VLA需要GPU推理，单次任务总功耗~115W；NSM仅需CPU，功耗~19W。在多数任务上，VLA的单次执行能耗是NSM的**一个数量级**以上。\n - **VLM规划器评估**：除了**GPT-5**（84%最优），评估的**Qwen**和**PaliGemma VLM**均无法生成有效规划。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Human-Robot Interaction Lab, Tufts University</strong>, Medford, MA, USA</li><li><strong>AIT Austrian Institute of Technology GmbH</strong>, Center for Vision, Automation & Control, Vienna, Austria</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>在结构化、长视野的操作任务上，对<strong>微调的开源视觉-语言-动作模型</strong>与<strong>神经符号架构</strong>进行全面的经验比较。</li><li>评估两种方法在解决结构化的“汉诺塔”任务中的<strong>任务性能和能耗</strong>，并测试其<strong>泛化能力</strong>。</li><li>核心问题是评估在结构化长视野操作任务中，直接微调大规模端到端基础模型是否比采用结合显式符号推理的架构更有效、更高效。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>在结构化长视野操作任务（3块汉诺塔）上，<strong>神经符号模型的成功率显著高于VLA模型</strong>（95% vs. 34%）。</li><li>神经符号模型在<strong>能耗方面表现极佳</strong>，其训练能耗比VLA微调低近两个数量级，且推理过程<strong>无需GPU</strong>。</li><li>神经符号模型展现出<strong>强大的泛化能力</strong>，在没有训练过的4块汉诺塔任务上达到78%的成功率，而两种VLA模型均完全失败。</li><li>提供了一个<strong>基于汉诺塔的基准任务</strong>和一个系统性的比较框架，用于分析不同架构在性能和效率上的权衡。</li><li>强调了对于具有明确程序约束的任务，<strong>显式结合符号结构</strong>在可靠性、数据效率和能源效率方面的优势。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>比较对象1：VLA模型</strong>。基于开源的 <strong><code>π₀</code></strong> 模型，使用LoRA在自定义数据集上进行微调。评估了两种变体：<strong>端到端VLA</strong>（接收单一高级指令）和<strong>规划引导VLA</strong>（接收分解的子任务指令）。</li><li><strong>比较对象2：神经符号模型</strong>。借鉴先前工作，结合了<strong>基于PDDL的符号规划器</strong>和<strong>基于扩散模型的低级控制策略</strong>。它从演示中<strong>同时学习符号抽象域和底层神经策略</strong>。</li><li><strong>关键技术</strong>：神经符号模型的核心创新在于其<strong>分层架构</strong>。首先通过<strong>ASP求解器</strong>从少量演示中推断符号规划域，然后为每个符号算子训练一个<strong>扩散模仿学习策略</strong>来执行，实现了<strong>结构化的长视野规划与连续控制</strong>的结合。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>训练数据集</strong>：在 <strong><code>Robosuite</code></strong> 仿真环境中构建的自定义汉诺塔任务数据集。VLA使用300个全任务轨迹演示，神经符号模型仅使用50个堆叠任务演示。</li><li><strong>模型规模</strong>：VLA模型使用<strong>PaliGemma 2B</strong>作为视觉语言主干，<strong>Gemma 300M</strong>作为动作头。神经符号模型的规划器与策略网络规模较小。</li><li><strong>训练资源</strong>：所有实验（训练、微调、评估）均在一张 <strong>NVIDIA GeForce RTX 4090 (24GB)</strong> 上完成。使用Weights & Biases和RAPL日志监测GPU/CPU功耗和能耗。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在 <code>Robosuite</code> 仿真中进行的结构化汉诺塔任务（单步移动、3块、4块）。</li><li><strong>主要指标</strong>：<strong>任务成功率</strong>、任务推进率、<strong>总能耗</strong>（GPU+CPU）、单次执行时长。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - <strong>任务性能</strong>：在3块任务上，<strong>NSM</strong>成功率95%，<strong>E2E-VLA</strong>34%，<strong>PG-VLA</strong>0%。在4块任务上，<strong>NSM</strong>成功率78%，两种VLA均为0%。</p>\n<p> - <strong>训练能耗</strong>：<strong>VLA微调</strong>总能耗约65-68 MJ，<strong>NSM训练</strong>仅需0.85 MJ，相差近<strong>两个数量级</strong>。</p>\n<p> - <strong>推理能耗</strong>：VLA需要GPU推理，单次任务总功耗~115W；NSM仅需CPU，功耗~19W。在多数任务上，VLA的单次执行能耗是NSM的<strong>一个数量级</strong>以上。</p>\n<p> - <strong>VLM规划器评估</strong>：除了<strong>GPT-5</strong>（84%最优），评估的<strong>Qwen</strong>和<strong>PaliGemma VLM</strong>均无法生成有效规划。</p>"
  },
  {
    "date": "2026-02-22",
    "title": "Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation",
    "link": "http://arxiv.org/abs/2602.20200",
    "summary_markdown": "## 研究单位\n- **哈尔滨工业大学（深圳）**\n- **鹏城实验室**\n- **深圳环路研究院**\n- **华为诺亚方舟实验室**\n## 论文概述\n- 论文提出了 **OptimusVLA**，一个双记忆增强的视觉-语言-动作（VLA）框架，旨在解决机器人操作中动作生成过程的效率和鲁棒性瓶颈。\n- 核心目标是解决现有分层VLA模型的两个主要问题：1）**推理效率低**，各向同性噪声先验与目标动作分布之间的差距大，导致去噪步骤多且容易产生不可行动作；2）**鲁棒性差**，策略仅以当前观测为条件，缺乏对任务进度和时序一致性的感知。\n## 核心贡献\n- 提出了 **全局先验记忆**，通过从语义相似的轨迹中检索任务级别的先验，替换高斯噪声，从而缩小先验-目标分布差距，显著减少所需函数评估次数并降低采样风险。\n- 设计了 **局部一致性记忆**，一个轻量级的工作记忆模块，通过动态建模近期动作序列，注入一致性约束，为VLA模型提供任务进度感知和时序平滑性，且计算开销可忽略。\n- 提出了双记忆驱动的 **OptimusVLA** 框架，将GPM和LCM有机结合，在多个仿真和现实世界基准测试中实现了更高的性能以及显著的推理加速。\n## 方法描述\n- 采用了分层VLA架构，包括**视觉-语言主干网络**、**流匹配策略**，以及新提出的**GPM**和**LCM**。\n- **GPM** 的核心在于将先验初始化视为一个记忆驱动的检索问题。它由**先验头**、**记忆库**和**先验感知采样器**组成，根据当前多模态表示检索相似任务的轨迹，并构造一个任务级先验分布，并自适应地注入噪声和选择NFE。\n- **LCM** 通过一个**一致性层**（自注意力）和**动态感知模块**（Mamba结构）对历史动作序列进行建模，学习一个残差偏差，作为约束注入到策略输入中，确保动作的时序一致性和平滑性。\n- 训练分为三个阶段：分层VLA预训练、GPM训练、以及LCM训练。GPM使用InfoNCE损失学习判别性表示，LCM使用L2损失学习预测动作真值与GPM先验均值之间的残差。\n## 数据集与资源\n- 在三个仿真基准上进行评估：**LIBERO**、**CALVIN**和**RoboTwin 2.0**。\n- 模型使用与 **π₀.₅** 相似的架构进行预训练。未在提供的文本中明确说明具体参数量和预训练数据集。\n- 训练资源未在提供的HTML片段中明确说明，但通常这类模型使用大量GPU进行训练。\n## 评估与结果\n- **评估基准**：LIBERO、CALVIN、RoboTwin 2.0的Hard设置，以及真实世界评估套件（Generalization和Long-horizon）。\n- **主要指标**：任务成功率、平均完成长度、推理速度。\n- **关键结果**：\n - **LIBERO**：平均成功率高达 **98.6%**，超越所有对比基线。\n - **CALVIN**：平均完成长度为 **4.45**，相比 **π₀** 提升了13.5%。\n - **RoboTwin 2.0 Hard**：在展示的7个任务中取得最佳或次佳排名，平均成功率约为 **38%**。\n - **真实世界**：在Generalization和Long-horizon套件中分别超越 **π₀** 42.9%和52.4%，同时实现了 **2.9倍** 的推理加速。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>哈尔滨工业大学（深圳）</strong></li><li><strong>鹏城实验室</strong></li><li><strong>深圳环路研究院</strong></li><li><strong>华为诺亚方舟实验室</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出了 <strong>OptimusVLA</strong>，一个双记忆增强的视觉-语言-动作（VLA）框架，旨在解决机器人操作中动作生成过程的效率和鲁棒性瓶颈。</li><li>核心目标是解决现有分层VLA模型的两个主要问题：1）<strong>推理效率低</strong>，各向同性噪声先验与目标动作分布之间的差距大，导致去噪步骤多且容易产生不可行动作；2）<strong>鲁棒性差</strong>，策略仅以当前观测为条件，缺乏对任务进度和时序一致性的感知。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>全局先验记忆</strong>，通过从语义相似的轨迹中检索任务级别的先验，替换高斯噪声，从而缩小先验-目标分布差距，显著减少所需函数评估次数并降低采样风险。</li><li>设计了 <strong>局部一致性记忆</strong>，一个轻量级的工作记忆模块，通过动态建模近期动作序列，注入一致性约束，为VLA模型提供任务进度感知和时序平滑性，且计算开销可忽略。</li><li>提出了双记忆驱动的 <strong>OptimusVLA</strong> 框架，将GPM和LCM有机结合，在多个仿真和现实世界基准测试中实现了更高的性能以及显著的推理加速。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>采用了分层VLA架构，包括<strong>视觉-语言主干网络</strong>、<strong>流匹配策略</strong>，以及新提出的<strong>GPM</strong>和<strong>LCM</strong>。</li><li><strong>GPM</strong> 的核心在于将先验初始化视为一个记忆驱动的检索问题。它由<strong>先验头</strong>、<strong>记忆库</strong>和<strong>先验感知采样器</strong>组成，根据当前多模态表示检索相似任务的轨迹，并构造一个任务级先验分布，并自适应地注入噪声和选择NFE。</li><li><strong>LCM</strong> 通过一个<strong>一致性层</strong>（自注意力）和<strong>动态感知模块</strong>（Mamba结构）对历史动作序列进行建模，学习一个残差偏差，作为约束注入到策略输入中，确保动作的时序一致性和平滑性。</li><li>训练分为三个阶段：分层VLA预训练、GPM训练、以及LCM训练。GPM使用InfoNCE损失学习判别性表示，LCM使用L2损失学习预测动作真值与GPM先验均值之间的残差。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>在三个仿真基准上进行评估：<strong>LIBERO</strong>、<strong>CALVIN</strong>和<strong>RoboTwin 2.0</strong>。</li><li>模型使用与 <strong>π₀.₅</strong> 相似的架构进行预训练。未在提供的文本中明确说明具体参数量和预训练数据集。</li><li>训练资源未在提供的HTML片段中明确说明，但通常这类模型使用大量GPU进行训练。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估基准</strong>：LIBERO、CALVIN、RoboTwin 2.0的Hard设置，以及真实世界评估套件（Generalization和Long-horizon）。</li><li><strong>主要指标</strong>：任务成功率、平均完成长度、推理速度。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - <strong>LIBERO</strong>：平均成功率高达 <strong>98.6%</strong>，超越所有对比基线。</p>\n<p> - <strong>CALVIN</strong>：平均完成长度为 <strong>4.45</strong>，相比 <strong>π₀</strong> 提升了13.5%。</p>\n<p> - <strong>RoboTwin 2.0 Hard</strong>：在展示的7个任务中取得最佳或次佳排名，平均成功率约为 <strong>38%</strong>。</p>\n<p> - <strong>真实世界</strong>：在Generalization和Long-horizon套件中分别超越 <strong>π₀</strong> 42.9%和52.4%，同时实现了 <strong>2.9倍</strong> 的推理加速。</p>"
  },
  {
    "date": "2026-02-21",
    "title": "Habilis-$β$: A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2602.18813",
    "summary_markdown": "## 研究单位\n- **Tommoro Robotics**\n## 论文概述\n- 介绍了一个名为 **Habilis-β** 的快速运动、长时运行、可端侧部署的视觉-语言-动作模型，旨在满足实际部署需求。\n- 提出了生产力-可靠性平面评估框架，使用**每小时任务数 (TPH)** 和**平均干预间隔时间 (MTBI)** 作为新指标，通过连续运行协议评估模型的性能。\n- 解决当前VLA模型评估局限于单次尝试成功率、无法反映实际部署中所需的高速执行和持续鲁棒性（即“快速运动”和“长时运行”能力）的问题。\n## 核心贡献\n- 提出了针对实际部署的**生产力-可靠性平面**评估框架和新指标（TPH和MTBI），超越了传统的单次成功率评估。\n- 设计并实现了**Habilis-β VLA模型**，其训练策略集成了在无语言标注的**大规模游戏数据**上进行预训练以获得鲁棒的交互先验，以及在**循环任务演示数据**上进行后训练以学习跨连续任务迭代的恢复能力和状态漂移适应。\n- 引入了**ESPADA（空间感知下采样）** 方法，对演示数据进行阶段自适应（空闲/精确阶段）的运动加速，以提升自由空间移动速度。\n- 采用**校正流蒸馏**技术，将多步流匹配动作专家模型蒸馏为高效的2步模型，实现在边缘设备上的**高频控制**，降低了推理延迟。\n- 整合了**无分类器引导** 作为部署时的调节旋钮，以动态平衡指令遵循和学习到的交互先验。\n## 方法描述\n- 模型采用前缀-后缀架构：预训练的**VLM（视觉语言模型）前缀**处理多模态观测（多视角图像和指令），**流匹配动作专家后缀**生成连续的动作块。\n- 训练分为三阶段：第一阶段在游戏数据上进行无语言预训练，学习任务无关的交互先验；第二阶段在循环任务数据上进行后训练，并使用ESPADA加速自由空间动作；第三阶段通过校正流蒸馏将多步动作专家压缩成高效的2步模型。\n- 关键技术包括：**流匹配动作生成**、**校正流蒸馏**、用于动作加速的**ESPADA**、用于在线调节的**无分类器引导**，以及通过缩短动作块长度实现的**高频控制**以提高闭环反应速度。\n## 数据集与资源\n- 使用了两种类型的数据集：用于预训练的无结构**游戏数据集**和用于对齐任务的**循环任务数据集**。\n- 模型规模未在提供内容中明确给出具体参数量。\n- 训练资源：未在提供内容中明确说明使用的具体GPU/TPU。\n- 在**NVIDIA Jetson Orin** 边缘设备上进行端侧推理。\n## 评估与结果\n- 评估环境：在**仿真（RoboTwin 2.0环境）** 和**真实世界（RB-Y1人形机器人）** 中进行1小时连续运行评估。\n- 主要评估指标：**每小时任务数**、**平均干预间隔时间** 和**成功率**。\n- 关键实验结果：\n - **仿真实验**：Habilis-β (带ESPADA) 实现了 **572.6 TPH** 和 **39.2秒 MTBI**，显著优于 **π₀.₅** 基线（120.5 TPH， 30.5秒 MTBI）。在标准的RoboTwin 2.0单次尝试基准测试中，Habilis-β在多个任务上也取得了最高报告性能。\n - **真实世界实验**：在“双箱传送带包装”任务中，Habilis-β实现了 **124 TPH** 和 **137.4秒 MTBI**，而 **π₀.₅** 基线为19 TPH和46.1秒 MTBI，成功率达到 **82.7%**。\n - 生产力-可靠性平面图显示，Habilis-β在TPH和MTBI两个指标上均优于基线模型，定位在平面的“高生产力-高可靠性”区域。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Tommoro Robotics</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>介绍了一个名为 <strong>Habilis-β</strong> 的快速运动、长时运行、可端侧部署的视觉-语言-动作模型，旨在满足实际部署需求。</li><li>提出了生产力-可靠性平面评估框架，使用<strong>每小时任务数 (TPH)</strong> 和<strong>平均干预间隔时间 (MTBI)</strong> 作为新指标，通过连续运行协议评估模型的性能。</li><li>解决当前VLA模型评估局限于单次尝试成功率、无法反映实际部署中所需的高速执行和持续鲁棒性（即“快速运动”和“长时运行”能力）的问题。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了针对实际部署的<strong>生产力-可靠性平面</strong>评估框架和新指标（TPH和MTBI），超越了传统的单次成功率评估。</li><li>设计并实现了<strong>Habilis-β VLA模型</strong>，其训练策略集成了在无语言标注的<strong>大规模游戏数据</strong>上进行预训练以获得鲁棒的交互先验，以及在<strong>循环任务演示数据</strong>上进行后训练以学习跨连续任务迭代的恢复能力和状态漂移适应。</li><li>引入了<strong>ESPADA（空间感知下采样）</strong> 方法，对演示数据进行阶段自适应（空闲/精确阶段）的运动加速，以提升自由空间移动速度。</li><li>采用<strong>校正流蒸馏</strong>技术，将多步流匹配动作专家模型蒸馏为高效的2步模型，实现在边缘设备上的<strong>高频控制</strong>，降低了推理延迟。</li><li>整合了<strong>无分类器引导</strong> 作为部署时的调节旋钮，以动态平衡指令遵循和学习到的交互先验。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>模型采用前缀-后缀架构：预训练的<strong>VLM（视觉语言模型）前缀</strong>处理多模态观测（多视角图像和指令），<strong>流匹配动作专家后缀</strong>生成连续的动作块。</li><li>训练分为三阶段：第一阶段在游戏数据上进行无语言预训练，学习任务无关的交互先验；第二阶段在循环任务数据上进行后训练，并使用ESPADA加速自由空间动作；第三阶段通过校正流蒸馏将多步动作专家压缩成高效的2步模型。</li><li>关键技术包括：<strong>流匹配动作生成</strong>、<strong>校正流蒸馏</strong>、用于动作加速的<strong>ESPADA</strong>、用于在线调节的<strong>无分类器引导</strong>，以及通过缩短动作块长度实现的<strong>高频控制</strong>以提高闭环反应速度。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用了两种类型的数据集：用于预训练的无结构<strong>游戏数据集</strong>和用于对齐任务的<strong>循环任务数据集</strong>。</li><li>模型规模未在提供内容中明确给出具体参数量。</li><li>训练资源：未在提供内容中明确说明使用的具体GPU/TPU。</li><li>在<strong>NVIDIA Jetson Orin</strong> 边缘设备上进行端侧推理。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：在<strong>仿真（RoboTwin 2.0环境）</strong> 和<strong>真实世界（RB-Y1人形机器人）</strong> 中进行1小时连续运行评估。</li><li>主要评估指标：<strong>每小时任务数</strong>、<strong>平均干预间隔时间</strong> 和<strong>成功率</strong>。</li><li>关键实验结果：</li></ul>\n<p> - <strong>仿真实验</strong>：Habilis-β (带ESPADA) 实现了 <strong>572.6 TPH</strong> 和 <strong>39.2秒 MTBI</strong>，显著优于 <strong>π₀.₅</strong> 基线（120.5 TPH， 30.5秒 MTBI）。在标准的RoboTwin 2.0单次尝试基准测试中，Habilis-β在多个任务上也取得了最高报告性能。</p>\n<p> - <strong>真实世界实验</strong>：在“双箱传送带包装”任务中，Habilis-β实现了 <strong>124 TPH</strong> 和 <strong>137.4秒 MTBI</strong>，而 <strong>π₀.₅</strong> 基线为19 TPH和46.1秒 MTBI，成功率达到 <strong>82.7%</strong>。</p>\n<p> - 生产力-可靠性平面图显示，Habilis-β在TPH和MTBI两个指标上均优于基线模型，定位在平面的“高生产力-高可靠性”区域。</p>"
  },
  {
    "date": "2026-02-20",
    "title": "How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf",
    "link": "http://arxiv.org/abs/2602.18397",
    "summary_markdown": "## 研究单位\n- **NVIDIA Research**\n## 论文概述\n- 论文针对**视觉-语言-动作 (VLA)** 模型在真实机器人部署中面临的实时推理性能挑战，提出了一个核心研究问题：**未来应如何设计 VLA 模型和系统以支持实时推理？**\n- 为了解决 VLA 模型架构与推理系统组合空间庞大的问题，论文引入了 **VLA-Perf** 这一基于屋顶线模型的**分析性能模型**，用于预测任意模型-系统组合的推理延迟和吞吐量。\n- 通过 **VLA-Perf**，论文对 VLA 推理性能进行了首次系统性分析，从**模型设计**（如模型缩放、架构选择、长上下文输入）和**部署系统**（如设备上/服务器端推理、硬件/网络性能）两个角度出发，提炼了 **15 项关键性能启示**，为未来 VLA 设计与部署提供实践指导。\n## 核心贡献\n- 提出了 **VLA-Perf**，一个可分析任意 VLA 模型与推理系统组合性能的、基于屋顶线模型的**分析性能模型**。\n- 利用 **VLA-Perf** 进行了**首次对 VLA 推理性能景观的系统性研究**，涵盖了模型设计、部署配置和网络环境等多个维度。\n- 通过全面的评估，从结果中提炼出 **15 项关键的性能启示**，为未来 VLA 模型和推理系统的设计提供了**实用的指导原则**。\n- 将 **VLA-Perf** 模型**开源**，以支持更广泛的性能分析。\n## 方法描述\n- 论文核心方法是开发了 **VLA-Perf**，这是一个基于屋顶线的分析性能模型。\n- 创新点在于将复杂的 VLA 推理工作流抽象为一系列**模型组件**与**数据移动阶段**，并对每个模型组件的算子执行延迟使用屋顶线模型（计算与访存延迟取最大值）进行建模。\n- 关键技术包括：对 VLA 推理过程进行建模与参数化；支持不同模型架构、长上下文、异步推理、双系统管道；支持不同部署位置（设备、服务器、混合）和网络配置的延迟计算。\n## 数据集与资源\n- 论文未使用特定数据集，其研究重点是性能分析与建模。\n- 评估基于一系列**模型变体**，这些变体源自 **π0** 架构（总参数量 2.7B），包含 SigLIP 视觉编码器（0.4B）、Gemma VLM（2B）和扩散动作专家（0.3B）。并研究了扩展到 **9.1B、16.7B、81.3B** 的更大模型。\n- 评估涵盖了多种**推理硬件**，包括边缘 GPU (**Jetson Thor**)、消费级 GPU (**RTX 4090**)、数据中心 GPU (**A100, H100, B100**)。\n## 评估与结果\n- 评估环境：使用 **VLA-Perf** 对广泛的模型变体和推理系统配置进行性能预测分析。\n- 主要评估指标：**端到端推理延迟** 和**推理频率（Hz）**。\n- 关键实验结果：\n - **基线分析**：**B100** 等数据中心 GPU 可为小型 VLA 模型（如 π0）提供远超相机帧率的推理频率（最高 314 Hz），而边缘 GPU（Jetson Thor，19 Hz）性能受限。\n - **组件瓶颈**：动作预测在所有硬件上都是**内存瓶颈**，而视觉和 VLM 推理在除 Jetson Thor 外的大多数 GPU 上是**计算瓶颈**。\n - **模型缩放**：延迟随模型规模近似线性增长。数据中心 GPU（如 B100）可为**大一个数量级**（81B）的 VLA 模型保持实时推理（9.6 Hz），而边缘/消费级 GPU 则面临困难。\n - **长上下文推理**：数据中心 GPU 可支持长达 **1K 个历史时间步**的实时长上下文推理（11.7 Hz），而边缘/消费级 GPU 仅能支持约 **100 步**。\n - **模型验证**：在一个优化后的 **π0 Triton 实现**上进行验证，其实测性能达到了 **VLA-Perf** 预测屋顶线性能的 **73-83%**，证明了模型的有效性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>NVIDIA Research</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文针对<strong>视觉-语言-动作 (VLA)</strong> 模型在真实机器人部署中面临的实时推理性能挑战，提出了一个核心研究问题：<strong>未来应如何设计 VLA 模型和系统以支持实时推理？</strong></li><li>为了解决 VLA 模型架构与推理系统组合空间庞大的问题，论文引入了 <strong>VLA-Perf</strong> 这一基于屋顶线模型的<strong>分析性能模型</strong>，用于预测任意模型-系统组合的推理延迟和吞吐量。</li><li>通过 <strong>VLA-Perf</strong>，论文对 VLA 推理性能进行了首次系统性分析，从<strong>模型设计</strong>（如模型缩放、架构选择、长上下文输入）和<strong>部署系统</strong>（如设备上/服务器端推理、硬件/网络性能）两个角度出发，提炼了 <strong>15 项关键性能启示</strong>，为未来 VLA 设计与部署提供实践指导。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>VLA-Perf</strong>，一个可分析任意 VLA 模型与推理系统组合性能的、基于屋顶线模型的<strong>分析性能模型</strong>。</li><li>利用 <strong>VLA-Perf</strong> 进行了<strong>首次对 VLA 推理性能景观的系统性研究</strong>，涵盖了模型设计、部署配置和网络环境等多个维度。</li><li>通过全面的评估，从结果中提炼出 <strong>15 项关键的性能启示</strong>，为未来 VLA 模型和推理系统的设计提供了<strong>实用的指导原则</strong>。</li><li>将 <strong>VLA-Perf</strong> 模型<strong>开源</strong>，以支持更广泛的性能分析。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>论文核心方法是开发了 <strong>VLA-Perf</strong>，这是一个基于屋顶线的分析性能模型。</li><li>创新点在于将复杂的 VLA 推理工作流抽象为一系列<strong>模型组件</strong>与<strong>数据移动阶段</strong>，并对每个模型组件的算子执行延迟使用屋顶线模型（计算与访存延迟取最大值）进行建模。</li><li>关键技术包括：对 VLA 推理过程进行建模与参数化；支持不同模型架构、长上下文、异步推理、双系统管道；支持不同部署位置（设备、服务器、混合）和网络配置的延迟计算。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>论文未使用特定数据集，其研究重点是性能分析与建模。</li><li>评估基于一系列<strong>模型变体</strong>，这些变体源自 <strong>π0</strong> 架构（总参数量 2.7B），包含 SigLIP 视觉编码器（0.4B）、Gemma VLM（2B）和扩散动作专家（0.3B）。并研究了扩展到 <strong>9.1B、16.7B、81.3B</strong> 的更大模型。</li><li>评估涵盖了多种<strong>推理硬件</strong>，包括边缘 GPU (<strong>Jetson Thor</strong>)、消费级 GPU (<strong>RTX 4090</strong>)、数据中心 GPU (<strong>A100, H100, B100</strong>)。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：使用 <strong>VLA-Perf</strong> 对广泛的模型变体和推理系统配置进行性能预测分析。</li><li>主要评估指标：<strong>端到端推理延迟</strong> 和<strong>推理频率（Hz）</strong>。</li><li>关键实验结果：</li></ul>\n<p> - <strong>基线分析</strong>：<strong>B100</strong> 等数据中心 GPU 可为小型 VLA 模型（如 π0）提供远超相机帧率的推理频率（最高 314 Hz），而边缘 GPU（Jetson Thor，19 Hz）性能受限。</p>\n<p> - <strong>组件瓶颈</strong>：动作预测在所有硬件上都是<strong>内存瓶颈</strong>，而视觉和 VLM 推理在除 Jetson Thor 外的大多数 GPU 上是<strong>计算瓶颈</strong>。</p>\n<p> - <strong>模型缩放</strong>：延迟随模型规模近似线性增长。数据中心 GPU（如 B100）可为<strong>大一个数量级</strong>（81B）的 VLA 模型保持实时推理（9.6 Hz），而边缘/消费级 GPU 则面临困难。</p>\n<p> - <strong>长上下文推理</strong>：数据中心 GPU 可支持长达 <strong>1K 个历史时间步</strong>的实时长上下文推理（11.7 Hz），而边缘/消费级 GPU 仅能支持约 <strong>100 步</strong>。</p>\n<p> - <strong>模型验证</strong>：在一个优化后的 <strong>π0 Triton 实现</strong>上进行验证，其实测性能达到了 <strong>VLA-Perf</strong> 预测屋顶线性能的 <strong>73-83%</strong>，证明了模型的有效性。</p>"
  },
  {
    "date": "2026-02-20",
    "title": "SimVLA: A Simple VLA Baseline for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2602.18224",
    "summary_markdown": "## 研究单位\n- **Frontier Robotics**\n## 论文概述\n- 提出 **SimVLA**，一个设计简洁的 **Vision-Language-Action (VLA)** 基准模型，旨在解决当前该领域因模型架构复杂、训练方法不统一而难以公平比较的问题。\n- 研究目标是建立一个 **模块化、可复现的强基线**，清晰地将感知（视觉语言理解）与控制（动作生成）解耦，并通过标准化关键训练细节来实现高性能，以便未来能够准确评估架构创新的价值。\n## 核心贡献\n- 提出了 **SimVLA**，一个模块化的 **VLA** 基线模型，通过解耦感知与控制（使用标准的视觉语言主干网络和轻量级动作头），实现了灵活和面向未来的设计。\n- 识别并标准化了影响 **VLA** 性能的“隐性”驱动因素（如**数据洗牌策略、动作空间归一化、优化动态**），提供了一个严谨的训练方法，以实现公平的跨模型比较。\n- 证明了这种极简设计能够实现 **state-of-the-art** 的性能，在模拟基准测试中超越了更大、更复杂的模型，同时实现了高效的零样本场景泛化和真实机器人部署。\n## 方法描述\n- **SimVLA** 采用 **感知-控制解耦** 的模块化设计：使用一个预训练的视觉语言模型（**VLM**）作为编码器来处理多视角RGB图像和语言指令，生成融合的表示；然后由一个轻量级的 **Transformer 编码器** 作为动作头，使用**条件流匹配** 在归一化的连续动作空间中生成未来的动作块。\n- 创新点在于其**极简架构**，避免了复杂的时空先验、记忆模块或多阶段推理流程。关键技术包括：**流匹配** 用于生成平滑、时间一致的动作块；以及一套严格标准化的**训练方法**，确保性能增益可归因于架构本身而非实现细节。\n## 数据集与资源\n- 使用的数据集包括：**LIBERO**、**LIBERO-PRO**、**SimplerEnv (Fractal/BridgeData-V2)** 以及 **Galaxea Open-World Dataset**。\n- 模型总参数量约为 **0.5B**，其中视觉语言主干可选择 **SmolVLM-0.5B** 或 **Florence-2**。\n- 训练资源涉及 **GPU**，具体数量未在提供片段中明确说明。\n## 评估与结果\n- 评估环境包括**仿真基准**（LIBERO, LIBERO-PRO, SimplerEnv）和**真实机器人**（Galaxea R1 Lite）。\n- 主要评估指标是**任务成功率 (%)**。\n- **SimVLA** 在 **LIBERO** 基准上取得了 **98.6%** 的平均成功率（四个套件的平均），超越了所有对比模型（包括数十亿参数的大型模型）。在**LIBERO-PRO** 鲁棒性测试中，其在**语义扰动**维度上表现最佳，证明了更强的泛化能力。在 **SimplerEnv** 和 **WidowX** 机器人任务上也达到了 **95.8%** 的平均成功率。在真实机器人上的零样本部署表现与强大的 **π0.5** 基线相当。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Frontier Robotics</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>SimVLA</strong>，一个设计简洁的 <strong>Vision-Language-Action (VLA)</strong> 基准模型，旨在解决当前该领域因模型架构复杂、训练方法不统一而难以公平比较的问题。</li><li>研究目标是建立一个 <strong>模块化、可复现的强基线</strong>，清晰地将感知（视觉语言理解）与控制（动作生成）解耦，并通过标准化关键训练细节来实现高性能，以便未来能够准确评估架构创新的价值。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>SimVLA</strong>，一个模块化的 <strong>VLA</strong> 基线模型，通过解耦感知与控制（使用标准的视觉语言主干网络和轻量级动作头），实现了灵活和面向未来的设计。</li><li>识别并标准化了影响 <strong>VLA</strong> 性能的“隐性”驱动因素（如<strong>数据洗牌策略、动作空间归一化、优化动态</strong>），提供了一个严谨的训练方法，以实现公平的跨模型比较。</li><li>证明了这种极简设计能够实现 <strong>state-of-the-art</strong> 的性能，在模拟基准测试中超越了更大、更复杂的模型，同时实现了高效的零样本场景泛化和真实机器人部署。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>SimVLA</strong> 采用 <strong>感知-控制解耦</strong> 的模块化设计：使用一个预训练的视觉语言模型（<strong>VLM</strong>）作为编码器来处理多视角RGB图像和语言指令，生成融合的表示；然后由一个轻量级的 <strong>Transformer 编码器</strong> 作为动作头，使用<strong>条件流匹配</strong> 在归一化的连续动作空间中生成未来的动作块。</li><li>创新点在于其<strong>极简架构</strong>，避免了复杂的时空先验、记忆模块或多阶段推理流程。关键技术包括：<strong>流匹配</strong> 用于生成平滑、时间一致的动作块；以及一套严格标准化的<strong>训练方法</strong>，确保性能增益可归因于架构本身而非实现细节。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的数据集包括：<strong>LIBERO</strong>、<strong>LIBERO-PRO</strong>、<strong>SimplerEnv (Fractal/BridgeData-V2)</strong> 以及 <strong>Galaxea Open-World Dataset</strong>。</li><li>模型总参数量约为 <strong>0.5B</strong>，其中视觉语言主干可选择 <strong>SmolVLM-0.5B</strong> 或 <strong>Florence-2</strong>。</li><li>训练资源涉及 <strong>GPU</strong>，具体数量未在提供片段中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境包括<strong>仿真基准</strong>（LIBERO, LIBERO-PRO, SimplerEnv）和<strong>真实机器人</strong>（Galaxea R1 Lite）。</li><li>主要评估指标是<strong>任务成功率 (%)</strong>。</li><li><strong>SimVLA</strong> 在 <strong>LIBERO</strong> 基准上取得了 <strong>98.6%</strong> 的平均成功率（四个套件的平均），超越了所有对比模型（包括数十亿参数的大型模型）。在<strong>LIBERO-PRO</strong> 鲁棒性测试中，其在<strong>语义扰动</strong>维度上表现最佳，证明了更强的泛化能力。在 <strong>SimplerEnv</strong> 和 <strong>WidowX</strong> 机器人任务上也达到了 <strong>95.8%</strong> 的平均成功率。在真实机器人上的零样本部署表现与强大的 <strong>π0.5</strong> 基线相当。</li></ul>"
  },
  {
    "date": "2026-02-20",
    "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.18020",
    "summary_markdown": "## 研究单位\n- 论文作者来自多个机构，但从HTML原文中的作者列表未能直接推断出具体的研究单位名称。可以推断作者团队是合作的。\n## 论文概述\n- 论文提出了一种名为 **UaOR (Uncertainty-aware Observation Reinjection)** 的即插即用模块，旨在无需重新训练即可提升视觉-语言-动作（VLA）模型的性能。\n- 核心目标是解决VLA模型在推理过程中因“遗忘”观察信息而导致动作生成不确定的问题，从而提升任务执行的可靠性和准确性。\n## 核心贡献\n- 引入了 **Action Entropy** 这一指标，用于量化VLA模型的层级不确定性，并揭示了模型在早期层推理过程中会逐渐“遗忘”观察信息。\n- 提出了 **UaOR** 方法，这是一个无需训练、即插即用的模块，它将前馈网络视为“键-值记忆”，并在模型不确定性高时重新注入观察特征。\n- 提供了严格的理论分析，证明**UaOR**能增加隐藏状态与观察之间的互信息，减少信息瓶颈损失，并降低动作不确定性。\n- 通过在模拟和真实世界环境中的大量实验，证明了**UaOR**能够持续提升多种VLA模型的性能，且无需额外的观察线索或辅助模块。\n## 方法描述\n- **UaOR** 的核心技术是：在模型前向推理过程中，计算每个语言模型层的**Action Entropy**作为不确定性度量。\n- 当某一层的不确定性超过阈值时，**UaOR** 会利用该层输出的隐藏状态作为查询，从编码后的观测特征（视觉和/或本体感知特征）中检索关键信息，并以键-值对的形式重新注入到下一层的前馈网络中。\n- 创新点在于将前馈网络重新解释为键-值记忆，并基于不确定性动态触发观察信息的强化，从而帮助模型在推理过程中更好地关注观察信息，生成更自信、更可靠的动作。\n## 数据集与资源\n- 使用了多个机器人模拟基准数据集进行评估，包括 **LIBERO**、**SIMPLER** 和 **CALVIN**。\n- 评估了多个主流VLA模型，如 **OpenVLA-OFT (7B)**、**π0**、**CogACT** 等。\n- 未明确提及训练所使用的具体GPU/TPU资源，因为**UaOR**本身是一个无需训练的推理时模块。\n## 评估与结果\n- 评估环境：多个机器人模拟环境（LIBERO, SIMPLER, CALVIN）以及真实世界机器人实验。\n- 主要评估指标：任务成功率。\n- 关键实验结果：\n - 在**LIBERO**基准测试中，**UaOR** 将 **OpenVLA-OFT** 的平均成功率从97.1%提升至98.0%，将 **π0** 的平均成功率从91.7%提升至93.2%。\n - 在**SIMPLER**基准测试中，**UaOR** 将 **CogACT** 的平均成功率从73.1%提升至75.7%。\n - 消融实验和真实世界实验进一步验证了**UaOR**各核心设计的有效性和实用性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li>论文作者来自多个机构，但从HTML原文中的作者列表未能直接推断出具体的研究单位名称。可以推断作者团队是合作的。</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出了一种名为 <strong>UaOR (Uncertainty-aware Observation Reinjection)</strong> 的即插即用模块，旨在无需重新训练即可提升视觉-语言-动作（VLA）模型的性能。</li><li>核心目标是解决VLA模型在推理过程中因“遗忘”观察信息而导致动作生成不确定的问题，从而提升任务执行的可靠性和准确性。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>引入了 <strong>Action Entropy</strong> 这一指标，用于量化VLA模型的层级不确定性，并揭示了模型在早期层推理过程中会逐渐“遗忘”观察信息。</li><li>提出了 <strong>UaOR</strong> 方法，这是一个无需训练、即插即用的模块，它将前馈网络视为“键-值记忆”，并在模型不确定性高时重新注入观察特征。</li><li>提供了严格的理论分析，证明<strong>UaOR</strong>能增加隐藏状态与观察之间的互信息，减少信息瓶颈损失，并降低动作不确定性。</li><li>通过在模拟和真实世界环境中的大量实验，证明了<strong>UaOR</strong>能够持续提升多种VLA模型的性能，且无需额外的观察线索或辅助模块。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>UaOR</strong> 的核心技术是：在模型前向推理过程中，计算每个语言模型层的<strong>Action Entropy</strong>作为不确定性度量。</li><li>当某一层的不确定性超过阈值时，<strong>UaOR</strong> 会利用该层输出的隐藏状态作为查询，从编码后的观测特征（视觉和/或本体感知特征）中检索关键信息，并以键-值对的形式重新注入到下一层的前馈网络中。</li><li>创新点在于将前馈网络重新解释为键-值记忆，并基于不确定性动态触发观察信息的强化，从而帮助模型在推理过程中更好地关注观察信息，生成更自信、更可靠的动作。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用了多个机器人模拟基准数据集进行评估，包括 <strong>LIBERO</strong>、<strong>SIMPLER</strong> 和 <strong>CALVIN</strong>。</li><li>评估了多个主流VLA模型，如 <strong>OpenVLA-OFT (7B)</strong>、<strong>π0</strong>、<strong>CogACT</strong> 等。</li><li>未明确提及训练所使用的具体GPU/TPU资源，因为<strong>UaOR</strong>本身是一个无需训练的推理时模块。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：多个机器人模拟环境（LIBERO, SIMPLER, CALVIN）以及真实世界机器人实验。</li><li>主要评估指标：任务成功率。</li><li>关键实验结果：</li></ul>\n<p> - 在<strong>LIBERO</strong>基准测试中，<strong>UaOR</strong> 将 <strong>OpenVLA-OFT</strong> 的平均成功率从97.1%提升至98.0%，将 <strong>π0</strong> 的平均成功率从91.7%提升至93.2%。</p>\n<p> - 在<strong>SIMPLER</strong>基准测试中，<strong>UaOR</strong> 将 <strong>CogACT</strong> 的平均成功率从73.1%提升至75.7%。</p>\n<p> - 消融实验和真实世界实验进一步验证了<strong>UaOR</strong>各核心设计的有效性和实用性。</p>"
  },
  {
    "date": "2026-02-20",
    "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.17951",
    "summary_markdown": "## 研究单位\n- **University of Maryland, College Park (CASE-Lab-UMD)**\n## 论文概述\n- 提出 **ROCKET** 框架，一种用于增强 **Vision-Language-Action (VLA)** 模型空间推理能力的残差导向多层表示对齐方法。\n- 旨在解决现有单层表示对齐方法无法充分利用跨层信息，以及朴素多层对齐导致的梯度冲突问题。\n- 核心目标是高效地将来自预训练 **3D 视觉基础模型** 的空间知识注入到 **2D 预训练的 VLA 模型**中，以提高其在机器人操作任务中的性能。\n## 核心贡献\n- 提出了 **ROCKET** 框架，其核心是使用 **共享投影器** 进行多层对齐，以减少梯度冲突，并引入 **Matryoshka 式稀疏激活**机制来平衡不同深度层的对齐损失。\n- 提供了理论和实证分析，解释了先前多层对齐失败的原因（梯度干扰），并证明了单一共享投影器的优越性和充分性。\n- 通过利用跨深度的互补空间线索，**ROCKET** 在多个基准数据集（**LIBERO**、**LIBERO-Plus**、**RoboTwin 2.0**）和不同 VLA 主干模型上实现了最先进的性能。\n- **ROCKET** 具有极高的计算效率，仅需约 **4%** 的先前进阶方法的训练计算量即可达到相似性能，并且在数据有限的环境中依然有效。\n## 方法描述\n- **方法核心**：将多层对齐视为一个残差流到另一个残差流的对齐。使用 **共享投影器**（一个轻量级的非线性 MLP）来对齐 VLA 骨干的多个层与 3D 视觉基础模型的多个层。\n- **关键技术/创新点**：\n - **共享投影器**：用一个投影器为所有选定层学习一致的学生-教师映射，避免了层间独立投影器导致的梯度冲突，促进了梯度相干性。\n - **Matryoshka 式稀疏激活**：对于较深的层，激活更多的投影器参数（线性增长），防止浅层对齐主导训练，平衡了不同深度层的对齐损失。\n - **训练免费的层选择策略**：通过分析层间表示的相似性（如 CKA），选择具有多样化信息的层进行对齐，简化了部署。\n## 数据集与资源\n- **使用的数据集**：**LIBERO**、**LIBERO-Plus**、**RoboTwin 2.0**。\n- **模型规模和参数量**：\n - 学生模型：**OpenVLA-7B** (基于 Prismatic-7B)、**PI0** 和 **PI0.5** (基于 PaliGemma)。\n - 教师模型：**VGGT** (Visual Geometry Grounded Transformer)，一个强大的 3D 视觉基础模型。\n- **训练资源**：未在摘要或提供章节中明确指定，但附录 A 提及了训练细节，通常此类工作使用多块 **GPU** 进行训练。\n## 评估与结果\n- **评估环境和基准**：在 **LIBERO**、**LIBERO-Plus** 和 **RoboTwin 2.0** 等机器人操作基准上进行评估，并与多种 SOTA VLA 方法（如 Spatial Forcing, GLaD, 3D-CAVLA, GeoVLA 等）进行比较。\n- **主要评估指标**：任务成功率，分为空间、物体、目标和长序列等子类别，并计算平均成功率。\n- **关键实验结果**：\n - 在 **LIBERO** 上，**ROCKET** 实现了 **98.5%** 的平均成功率，与当前 SOTA 方法（Spatial Forcing）持平，并在“物体”、“目标”和“长序列”子任务上取得了最优或并列最优成绩。\n - 在 **LIBERO-Plus** 和 **RoboTwin 2.0** 上也展示了优越的泛化性能。\n - 与多个独立投影器的朴素多层对齐基线相比，**ROCKET** 显著提升了性能，并验证了共享投影器带来的梯度相干性改进。\n - **ROCKET** 收敛速度极快，仅需约 **4%** 的 SOTA 方法（Spatial Forcing）的训练计算预算即可达到接近其最终性能。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>University of Maryland, College Park (CASE-Lab-UMD)</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>ROCKET</strong> 框架，一种用于增强 <strong>Vision-Language-Action (VLA)</strong> 模型空间推理能力的残差导向多层表示对齐方法。</li><li>旨在解决现有单层表示对齐方法无法充分利用跨层信息，以及朴素多层对齐导致的梯度冲突问题。</li><li>核心目标是高效地将来自预训练 <strong>3D 视觉基础模型</strong> 的空间知识注入到 <strong>2D 预训练的 VLA 模型</strong>中，以提高其在机器人操作任务中的性能。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>ROCKET</strong> 框架，其核心是使用 <strong>共享投影器</strong> 进行多层对齐，以减少梯度冲突，并引入 <strong>Matryoshka 式稀疏激活</strong>机制来平衡不同深度层的对齐损失。</li><li>提供了理论和实证分析，解释了先前多层对齐失败的原因（梯度干扰），并证明了单一共享投影器的优越性和充分性。</li><li>通过利用跨深度的互补空间线索，<strong>ROCKET</strong> 在多个基准数据集（<strong>LIBERO</strong>、<strong>LIBERO-Plus</strong>、<strong>RoboTwin 2.0</strong>）和不同 VLA 主干模型上实现了最先进的性能。</li><li><strong>ROCKET</strong> 具有极高的计算效率，仅需约 <strong>4%</strong> 的先前进阶方法的训练计算量即可达到相似性能，并且在数据有限的环境中依然有效。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>方法核心</strong>：将多层对齐视为一个残差流到另一个残差流的对齐。使用 <strong>共享投影器</strong>（一个轻量级的非线性 MLP）来对齐 VLA 骨干的多个层与 3D 视觉基础模型的多个层。</li><li><strong>关键技术/创新点</strong>：</li></ul>\n<p> - <strong>共享投影器</strong>：用一个投影器为所有选定层学习一致的学生-教师映射，避免了层间独立投影器导致的梯度冲突，促进了梯度相干性。</p>\n<p> - <strong>Matryoshka 式稀疏激活</strong>：对于较深的层，激活更多的投影器参数（线性增长），防止浅层对齐主导训练，平衡了不同深度层的对齐损失。</p>\n<p> - <strong>训练免费的层选择策略</strong>：通过分析层间表示的相似性（如 CKA），选择具有多样化信息的层进行对齐，简化了部署。</p>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>使用的数据集</strong>：<strong>LIBERO</strong>、<strong>LIBERO-Plus</strong>、<strong>RoboTwin 2.0</strong>。</li><li><strong>模型规模和参数量</strong>：</li></ul>\n<p> - 学生模型：<strong>OpenVLA-7B</strong> (基于 Prismatic-7B)、<strong>PI0</strong> 和 <strong>PI0.5</strong> (基于 PaliGemma)。</p>\n<p> - 教师模型：<strong>VGGT</strong> (Visual Geometry Grounded Transformer)，一个强大的 3D 视觉基础模型。</p>\n<ul><li><strong>训练资源</strong>：未在摘要或提供章节中明确指定，但附录 A 提及了训练细节，通常此类工作使用多块 <strong>GPU</strong> 进行训练。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境和基准</strong>：在 <strong>LIBERO</strong>、<strong>LIBERO-Plus</strong> 和 <strong>RoboTwin 2.0</strong> 等机器人操作基准上进行评估，并与多种 SOTA VLA 方法（如 Spatial Forcing, GLaD, 3D-CAVLA, GeoVLA 等）进行比较。</li><li><strong>主要评估指标</strong>：任务成功率，分为空间、物体、目标和长序列等子类别，并计算平均成功率。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 在 <strong>LIBERO</strong> 上，<strong>ROCKET</strong> 实现了 <strong>98.5%</strong> 的平均成功率，与当前 SOTA 方法（Spatial Forcing）持平，并在“物体”、“目标”和“长序列”子任务上取得了最优或并列最优成绩。</p>\n<p> - 在 <strong>LIBERO-Plus</strong> 和 <strong>RoboTwin 2.0</strong> 上也展示了优越的泛化性能。</p>\n<p> - 与多个独立投影器的朴素多层对齐基线相比，<strong>ROCKET</strong> 显著提升了性能，并验证了共享投影器带来的梯度相干性改进。</p>\n<p> - <strong>ROCKET</strong> 收敛速度极快，仅需约 <strong>4%</strong> 的 SOTA 方法（Spatial Forcing）的训练计算预算即可达到接近其最终性能。</p>"
  },
  {
    "date": "2026-02-19",
    "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
    "link": "http://arxiv.org/abs/2602.17659",
    "summary_markdown": "## 研究单位\n- **University of North Carolina at Chapel Hill**\n## 论文概述\n- 本文聚焦于**视觉-语言-动作模型**在机器人控制中普遍存在的一个失效模式：**反事实失败**。即模型会过度依赖数据集中的视觉偏见，忽视语言指令，转而执行训练数据中常见的、与场景强关联的动作。\n- 研究目标是**系统评估并缓解VLAs中的这种语言指令遵循失败问题**。具体做法是：1）构建首个反事实评估基准；2）提出一种即插即用的推理方法。\n## 核心贡献\n- 提出了 **LIBERO-CF**，这是第一个用于评估VLA语言遵循能力的**反事实基准**。\n- 提出了 **反事实动作引导**，这是一种简单有效的**双分支推理方案**，可显式增强VLAs中的语言调节，而无需修改现有架构或预训练模型。\n- 对VLAs中的**视觉捷径**和反事实失败进行了全面的分析，并通过实验证明所提方法在多种最先进的VLA模型上带来了一致性改进。\n## 方法描述\n- 核心方法是**反事实动作引导**，其灵感来源于图像生成中的无分类器引导技术。\n- 通过构建一个**双分支推理框架**来增强语言调节：一个分支是标准的VLA策略，另一个分支是无语言条件的视觉-动作策略。\n- 该方法的创新点在于：在推理时，通过混合两个分支的动作预测，显式地对比和增强语言指令的影响，从而**减少对视觉捷径的依赖**，提升对低观测任务的鲁棒性。\n## 数据集与资源\n- 主要使用的数据集是基于**LIBERO**构建的**LIBERO-CF** 基准，包含四个子集：**CF-Spatial**、**CF-Object**、**CF-Long** 和 **CF-OOD**。\n- 评估了多个主流VLA模型，包括 **OpenVLA-OFT**、**π0** 和 **π0.5**，模型参数量级从**3B到7B**。\n- 训练资源方面，论文使用了在**Open-X-Embodiment** 等大型机器人数据集上预训练的模型权重，并在LIBERO上进行了微调。\n## 评估与结果\n- 评估环境主要在**MuJoCo仿真环境**中进行，并进行了真实的机器人实验。\n- 主要评估指标包括**语言接地率**（衡量是否接触指令指定的目标物体）和**任务成功率**。\n- 关键实验结果：在LIBERO-CF基准上，**CAG方法**显著提升了所有测试VLA的语言遵循能力。例如，仅使用无需训练的策略，就能将 **π0.5** 的语言接地率提升**9.7%**，任务成功率提升**3.6%**；若配合单独的VA模型，提升可达**15.5%** 和 **8.5%**。在真实世界实验中，CAG平均减少了**9.4%** 的反事实失败并提升了**17.2%** 的任务成功率。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>University of North Carolina at Chapel Hill</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>本文聚焦于<strong>视觉-语言-动作模型</strong>在机器人控制中普遍存在的一个失效模式：<strong>反事实失败</strong>。即模型会过度依赖数据集中的视觉偏见，忽视语言指令，转而执行训练数据中常见的、与场景强关联的动作。</li><li>研究目标是<strong>系统评估并缓解VLAs中的这种语言指令遵循失败问题</strong>。具体做法是：1）构建首个反事实评估基准；2）提出一种即插即用的推理方法。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>LIBERO-CF</strong>，这是第一个用于评估VLA语言遵循能力的<strong>反事实基准</strong>。</li><li>提出了 <strong>反事实动作引导</strong>，这是一种简单有效的<strong>双分支推理方案</strong>，可显式增强VLAs中的语言调节，而无需修改现有架构或预训练模型。</li><li>对VLAs中的<strong>视觉捷径</strong>和反事实失败进行了全面的分析，并通过实验证明所提方法在多种最先进的VLA模型上带来了一致性改进。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心方法是<strong>反事实动作引导</strong>，其灵感来源于图像生成中的无分类器引导技术。</li><li>通过构建一个<strong>双分支推理框架</strong>来增强语言调节：一个分支是标准的VLA策略，另一个分支是无语言条件的视觉-动作策略。</li><li>该方法的创新点在于：在推理时，通过混合两个分支的动作预测，显式地对比和增强语言指令的影响，从而<strong>减少对视觉捷径的依赖</strong>，提升对低观测任务的鲁棒性。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>主要使用的数据集是基于<strong>LIBERO</strong>构建的<strong>LIBERO-CF</strong> 基准，包含四个子集：<strong>CF-Spatial</strong>、<strong>CF-Object</strong>、<strong>CF-Long</strong> 和 <strong>CF-OOD</strong>。</li><li>评估了多个主流VLA模型，包括 <strong>OpenVLA-OFT</strong>、<strong>π0</strong> 和 <strong>π0.5</strong>，模型参数量级从<strong>3B到7B</strong>。</li><li>训练资源方面，论文使用了在<strong>Open-X-Embodiment</strong> 等大型机器人数据集上预训练的模型权重，并在LIBERO上进行了微调。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境主要在<strong>MuJoCo仿真环境</strong>中进行，并进行了真实的机器人实验。</li><li>主要评估指标包括<strong>语言接地率</strong>（衡量是否接触指令指定的目标物体）和<strong>任务成功率</strong>。</li><li>关键实验结果：在LIBERO-CF基准上，<strong>CAG方法</strong>显著提升了所有测试VLA的语言遵循能力。例如，仅使用无需训练的策略，就能将 <strong>π0.5</strong> 的语言接地率提升<strong>9.7%</strong>，任务成功率提升<strong>3.6%</strong>；若配合单独的VA模型，提升可达<strong>15.5%</strong> 和 <strong>8.5%</strong>。在真实世界实验中，CAG平均减少了<strong>9.4%</strong> 的反事实失败并提升了<strong>17.2%</strong> 的任务成功率。</li></ul>"
  },
  {
    "date": "2026-02-19",
    "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
    "link": "http://arxiv.org/abs/2602.17259",
    "summary_markdown": "这是一段很长的文本，我将帮你提取关键信息来构建论文总结。",
    "summary_html": "<p>这是一段很长的文本，我将帮你提取关键信息来构建论文总结。</p>"
  },
  {
    "date": "2026-02-18",
    "title": "EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data",
    "link": "http://arxiv.org/abs/2602.16710",
    "summary_markdown": "## 研究单位\n- **NVIDIA**\n- **University of California, Berkeley**\n- **University of Maryland**\n## 论文概述\n- 论文提出 **EgoScale**，一个利用大规模第一人称人类数据进行灵巧操作的人-机器人迁移框架。\n- 核心目标是解决**如何利用大规模、多样的人类数据来学习和迁移精细、高自由度的灵巧操作技能**的问题。\n- 探索了人类数据规模（超过20k小时）与模型性能之间的**可预测的缩放规律**。\n## 核心贡献\n- **发现了人类数据预训练的缩放规律**：在超过20k小时的人类数据规模下，揭示了手部动作预测损失与数据规模之间存在明确的**对数线性关系**，且该损失与下游真实机器人性能强相关。\n- **提出了一种高效的人-机器人迁移方法**：结合了**高自由度（DoF）人类手部动作监督**与少量**对齐的人-机器人中间训练数据**，实现了在最小机器人数据下的强后训练性能。\n- **实现了涌现的单样本迁移和跨具身泛化**：该方法能在**22-DoF灵巧手**上实现仅需一个机器人演示的单样本任务适应，并且学习到的表示能有效泛化到具有**更低自由度手（如三指手）** 的不同机器人平台上。\n- 构建并利用了超**20,854小时**的大规模第一人称人类动作数据集，规模超过先前工作的**20倍**。\n## 方法描述\n- 采用**两阶段训练框架**：**第一阶段**在大规模人类数据上预训练视觉-语言-动作模型，使用**相对手腕运动**和**重定向的22-DoF灵巧手关节动作**作为监督信号。**第二阶段**使用少量对齐的人-机器人数据（人类和机器人在相似桌面场景中执行类似任务）进行中间训练，以将预训练的表征锚定到机器人的感知和控制空间。\n- 模型基于**流匹配**的**Vision-Language-Action架构**，共享手腕级动作表示，并为不同具身使用轻量级、特定具身的适配器来处理本体感觉和手部动作。\n- 创新点在于将**数据规模**与**具身对齐**解耦：大规模人类数据提供通用操作结构，少量对齐数据提供精确的机器人控制映射。\n## 数据集与资源\n- 使用的主要数据集为超过 **20，854 小时**的第一人称人类操作视频，包含大量野外数据和约829小时的**EgoDex**数据集。\n- 对齐的人-机器人中间训练数据集包含约**50小时人类数据**和仅**4小时机器人数据**，涵盖344个桌面操作任务。\n- 模型基于类似**GR00T N1**的VLA架构，具体参数量未明确给出。\n- 训练资源：**第一阶段**使用 **256 块 GB200 GPU**，全局批大小为 8192。\n## 评估与结果\n- **评估环境**：在配备**22-DoF Sharpa灵巧手**的Galaxea R1Pro人形机器人以及配备**7-DoF三指手**的Unitree G1机器人上进行真实世界实验。\n- **评估基准**：设计了五个高难度灵巧操作任务：衬衫卷叠、卡片分拣、使用镊子转移水果、拧开瓶盖、注射器液体转移，以及额外的单样本适应任务（折叠衬衫、拧开水瓶）。\n- **主要评估指标**：任务成功率和细粒度的任务完成度得分。\n- **关键实验结果**：\n - **人类预训练显著提升性能**：相比无预训练基线，结合了人类预训练和中间训练的最终策略平均成功率提升 **54%**。\n - **性能随预训练数据规模缩放**：人类动作验证损失与数据规模呈对数线性关系（R² = 0.9983），且该损失与下游机器人任务完成度强相关。\n - **中间训练实现单样本迁移**：在衬衫折叠任务上，使用**单次机器人演示**和100次对齐人类演示，成功率可达 **88%**。\n - **支持跨具身泛化**：在Unitree G1机器人上，经人类预训练和中间训练的策略，相比无人类预训练基线，在两个评估任务上均获得超过 **30%** 的绝对成功率提升。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>NVIDIA</strong></li><li><strong>University of California, Berkeley</strong></li><li><strong>University of Maryland</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出 <strong>EgoScale</strong>，一个利用大规模第一人称人类数据进行灵巧操作的人-机器人迁移框架。</li><li>核心目标是解决<strong>如何利用大规模、多样的人类数据来学习和迁移精细、高自由度的灵巧操作技能</strong>的问题。</li><li>探索了人类数据规模（超过20k小时）与模型性能之间的<strong>可预测的缩放规律</strong>。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li><strong>发现了人类数据预训练的缩放规律</strong>：在超过20k小时的人类数据规模下，揭示了手部动作预测损失与数据规模之间存在明确的<strong>对数线性关系</strong>，且该损失与下游真实机器人性能强相关。</li><li><strong>提出了一种高效的人-机器人迁移方法</strong>：结合了<strong>高自由度（DoF）人类手部动作监督</strong>与少量<strong>对齐的人-机器人中间训练数据</strong>，实现了在最小机器人数据下的强后训练性能。</li><li><strong>实现了涌现的单样本迁移和跨具身泛化</strong>：该方法能在<strong>22-DoF灵巧手</strong>上实现仅需一个机器人演示的单样本任务适应，并且学习到的表示能有效泛化到具有<strong>更低自由度手（如三指手）</strong> 的不同机器人平台上。</li><li>构建并利用了超<strong>20,854小时</strong>的大规模第一人称人类动作数据集，规模超过先前工作的<strong>20倍</strong>。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>采用<strong>两阶段训练框架</strong>：<strong>第一阶段</strong>在大规模人类数据上预训练视觉-语言-动作模型，使用<strong>相对手腕运动</strong>和<strong>重定向的22-DoF灵巧手关节动作</strong>作为监督信号。<strong>第二阶段</strong>使用少量对齐的人-机器人数据（人类和机器人在相似桌面场景中执行类似任务）进行中间训练，以将预训练的表征锚定到机器人的感知和控制空间。</li><li>模型基于<strong>流匹配</strong>的<strong>Vision-Language-Action架构</strong>，共享手腕级动作表示，并为不同具身使用轻量级、特定具身的适配器来处理本体感觉和手部动作。</li><li>创新点在于将<strong>数据规模</strong>与<strong>具身对齐</strong>解耦：大规模人类数据提供通用操作结构，少量对齐数据提供精确的机器人控制映射。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的主要数据集为超过 <strong>20，854 小时</strong>的第一人称人类操作视频，包含大量野外数据和约829小时的<strong>EgoDex</strong>数据集。</li><li>对齐的人-机器人中间训练数据集包含约<strong>50小时人类数据</strong>和仅<strong>4小时机器人数据</strong>，涵盖344个桌面操作任务。</li><li>模型基于类似<strong>GR00T N1</strong>的VLA架构，具体参数量未明确给出。</li><li>训练资源：<strong>第一阶段</strong>使用 <strong>256 块 GB200 GPU</strong>，全局批大小为 8192。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在配备<strong>22-DoF Sharpa灵巧手</strong>的Galaxea R1Pro人形机器人以及配备<strong>7-DoF三指手</strong>的Unitree G1机器人上进行真实世界实验。</li><li><strong>评估基准</strong>：设计了五个高难度灵巧操作任务：衬衫卷叠、卡片分拣、使用镊子转移水果、拧开瓶盖、注射器液体转移，以及额外的单样本适应任务（折叠衬衫、拧开水瓶）。</li><li><strong>主要评估指标</strong>：任务成功率和细粒度的任务完成度得分。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>人类预训练显著提升性能</strong>：相比无预训练基线，结合了人类预训练和中间训练的最终策略平均成功率提升 <strong>54%</strong>。</p>\n<p> - <strong>性能随预训练数据规模缩放</strong>：人类动作验证损失与数据规模呈对数线性关系（R² = 0.9983），且该损失与下游机器人任务完成度强相关。</p>\n<p> - <strong>中间训练实现单样本迁移</strong>：在衬衫折叠任务上，使用<strong>单次机器人演示</strong>和100次对齐人类演示，成功率可达 <strong>88%</strong>。</p>\n<p> - <strong>支持跨具身泛化</strong>：在Unitree G1机器人上，经人类预训练和中间训练的策略，相比无人类预训练基线，在两个评估任务上均获得超过 <strong>30%</strong> 的绝对成功率提升。</p>"
  },
  {
    "date": "2026-02-18",
    "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
    "link": "http://arxiv.org/abs/2602.16444",
    "summary_markdown": "## 研究单位\n- **北京人形机器人创新中心**\n- **北京大学先进制造与机器人学院**\n- **北京理工大学**\n- **北京航空航天大学机械工程及自动化学院**\n- **北京大学计算机学院多媒体信息处理国家重点实验室**\n## 论文概述\n- 提出 **RoboGene** 框架，旨在自动化生成多样化、物理可行的真实世界机器人操作任务，以解决机器人预训练数据稀缺且分布不平衡（长尾分布）的问题。\n- 目标是克服现有方法（人工设计不可扩展，大型基础模型易产生幻觉）的局限性，通过自动化任务生成来为视觉-语言-动作模型提供高质量、平衡的训练数据。\n## 核心贡献\n- 提出了 **RoboGene**，一个基于智能体的框架，通过**多样性驱动的采样**、**自我反思机制**和**融合人类反馈的记忆模块**，实现了大规模自动化任务生成。\n- 引入了一套新颖的指标，用于定量评估生成任务的质量（如物理可行性）和生成数据集整体的多样性。\n- 进行了大规模真实世界实验，收集了超过18k轨迹的数据集，验证了使用**RoboGene**生成的数据进行预训练的VLA模型具有更高的成功率与泛化能力。\n## 方法描述\n- **RoboGene**是一个闭环的智能体框架，包含三个核心组件：\n - **多样性驱动采样**：采用**最少使用频率**策略对任务空间（场景、物体、技能）进行分层采样，以覆盖“长尾”区域，确保数据分布平衡。\n - **自我反思与任务生成**：首先由LLM/VLM生成初始任务，然后通过三个专门的评估器（物理可行性、新颖性、约束遵循）进行多角度审查并提供反馈，最后由反思优化器根据反馈进行修正，以确保任务质量和物理可行性。\n - **融入人类反馈的记忆**：通过一个**长期记忆**模块存储和检索在真实世界执行任务时获得的专家反馈知识（**人在回路**），采用**检索增强生成**技术来持续改进后续的任务生成。\n## 数据集与资源\n- 使用**RoboGene**为单臂、双臂和移动机器人平台生成了900个任务（每种机器人类型300个）。\n- 在真实世界实验中，收集了大规模的机器人数据集，包含**18，000条轨迹**，涵盖**1，200个**不同的任务（每个任务15次演示）。\n- 评估中使用了大型基础模型**GPT-4o**和**Gemini 2.5 Pro**作为基线。\n## 评估与结果\n- **评估基准**：与人工设计、规则基线以及**GPT-4o**、**Gemini 2.5 Pro**等大型基础模型进行比较。\n- **主要指标**：任务清晰度、机器人类型一致性、逻辑有效性、物体覆盖率、技能覆盖率、物理可行性，以及数据集多样性指标（BLEU, ROUGE-L, 余弦相似度）。\n- **关键结果**：\n - 在个体任务质量上，**RoboGene**在所有评估指标上全面超越基线，特别是物理可行性达到**98.99%**，物体/技能覆盖率也远超LLM基线。\n - 在数据集多样性上，**RoboGene**显著提升了任务在**场景**（分布更均匀）、**物体**（覆盖719个独特物体，是GPT-4o的1.7倍）和**技能**（覆盖91.5%的技能空间）上的覆盖度与平衡性。\n - 真实世界实验表明，基于**RoboGene**生成数据预训练的**π₀**模型，在**零样本泛化**到新物体、背景变化、干扰物、光照变化和指令变化等场景时，表现出比所有基线模型都更优越的鲁棒性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>北京人形机器人创新中心</strong></li><li><strong>北京大学先进制造与机器人学院</strong></li><li><strong>北京理工大学</strong></li><li><strong>北京航空航天大学机械工程及自动化学院</strong></li><li><strong>北京大学计算机学院多媒体信息处理国家重点实验室</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>RoboGene</strong> 框架，旨在自动化生成多样化、物理可行的真实世界机器人操作任务，以解决机器人预训练数据稀缺且分布不平衡（长尾分布）的问题。</li><li>目标是克服现有方法（人工设计不可扩展，大型基础模型易产生幻觉）的局限性，通过自动化任务生成来为视觉-语言-动作模型提供高质量、平衡的训练数据。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>RoboGene</strong>，一个基于智能体的框架，通过<strong>多样性驱动的采样</strong>、<strong>自我反思机制</strong>和<strong>融合人类反馈的记忆模块</strong>，实现了大规模自动化任务生成。</li><li>引入了一套新颖的指标，用于定量评估生成任务的质量（如物理可行性）和生成数据集整体的多样性。</li><li>进行了大规模真实世界实验，收集了超过18k轨迹的数据集，验证了使用<strong>RoboGene</strong>生成的数据进行预训练的VLA模型具有更高的成功率与泛化能力。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>RoboGene</strong>是一个闭环的智能体框架，包含三个核心组件：</li></ul>\n<p> - <strong>多样性驱动采样</strong>：采用<strong>最少使用频率</strong>策略对任务空间（场景、物体、技能）进行分层采样，以覆盖“长尾”区域，确保数据分布平衡。</p>\n<p> - <strong>自我反思与任务生成</strong>：首先由LLM/VLM生成初始任务，然后通过三个专门的评估器（物理可行性、新颖性、约束遵循）进行多角度审查并提供反馈，最后由反思优化器根据反馈进行修正，以确保任务质量和物理可行性。</p>\n<p> - <strong>融入人类反馈的记忆</strong>：通过一个<strong>长期记忆</strong>模块存储和检索在真实世界执行任务时获得的专家反馈知识（<strong>人在回路</strong>），采用<strong>检索增强生成</strong>技术来持续改进后续的任务生成。</p>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用<strong>RoboGene</strong>为单臂、双臂和移动机器人平台生成了900个任务（每种机器人类型300个）。</li><li>在真实世界实验中，收集了大规模的机器人数据集，包含<strong>18，000条轨迹</strong>，涵盖<strong>1，200个</strong>不同的任务（每个任务15次演示）。</li><li>评估中使用了大型基础模型<strong>GPT-4o</strong>和<strong>Gemini 2.5 Pro</strong>作为基线。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估基准</strong>：与人工设计、规则基线以及<strong>GPT-4o</strong>、<strong>Gemini 2.5 Pro</strong>等大型基础模型进行比较。</li><li><strong>主要指标</strong>：任务清晰度、机器人类型一致性、逻辑有效性、物体覆盖率、技能覆盖率、物理可行性，以及数据集多样性指标（BLEU, ROUGE-L, 余弦相似度）。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - 在个体任务质量上，<strong>RoboGene</strong>在所有评估指标上全面超越基线，特别是物理可行性达到<strong>98.99%</strong>，物体/技能覆盖率也远超LLM基线。</p>\n<p> - 在数据集多样性上，<strong>RoboGene</strong>显著提升了任务在<strong>场景</strong>（分布更均匀）、<strong>物体</strong>（覆盖719个独特物体，是GPT-4o的1.7倍）和<strong>技能</strong>（覆盖91.5%的技能空间）上的覆盖度与平衡性。</p>\n<p> - 真实世界实验表明，基于<strong>RoboGene</strong>生成数据预训练的<strong>π₀</strong>模型，在<strong>零样本泛化</strong>到新物体、背景变化、干扰物、光照变化和指令变化等场景时，表现出比所有基线模型都更优越的鲁棒性。</p>"
  },
  {
    "date": "2026-02-17",
    "title": "World Action Models are Zero-shot Policies",
    "link": "http://arxiv.org/abs/2602.15922",
    "summary_markdown": "## 研究单位\n- **NVIDIA**\n## 论文概述\n- 论文提出 **DreamZero**，一种基于预训练视频扩散主干构建的 **世界行动模型 (World Action Model, WAM)**，通过联合预测未来视频和动作来学习物理动力学。\n- 核心目标是解决现有 **视觉-语言-行动模型 (Vision-Language-Action Models, VLAs)** 在泛化到未见过的物理运动和技能方面的不足，特别是针对新环境、新任务和新机器人的泛化能力。\n## 核心贡献\n- 提出 **DreamZero**，一个 **14B** 参数的 WAM，通过联合视频和动作预测，能够从**多样、非重复的机器人数据**中进行有效学习。\n- 在真实机器人实验中，相比于最先进的 VLAs，在**新任务和环境上的零样本泛化能力提升超过2倍**。\n- 通过模型和系统优化，实现了 **38倍推理加速**，使 **14B 自回归视频扩散模型能够以 7Hz 的频率进行实时闭环控制**。\n- 展示了**跨具身 (cross-embodiment) 迁移能力**：仅使用来自其他机器人（20分钟）或人类（12分钟）的**仅视频演示**，即可在未见任务上实现**超过42%的相对性能提升**；并且展示了**少量样本的具身适应 (few-shot embodiment adaptation)**，仅用30分钟的玩耍数据就能适应新的机器人，同时保持零样本泛化能力。\n- **开源**了模型权重、推理代码，以及用于运行公开的真实世界（RoboArena）和模拟基准（PolaRiS 和 Genie Sim 3.0）的代码。\n## 方法描述\n- 模型基于预训练的图像到视频扩散模型（**Wan2.1-I2V-14B-480P**）构建，添加了状态编码器、动作编码器和解码器，以**联合去噪视频和动作潜在表示**。\n- 采用**自回归、分块的流匹配 (flow matching) 训练目标**，确保视频和动作模态的紧密对齐。通过 KV 缓存和异步闭环执行（用真实观测替换生成的帧）来实现高效推理，防止误差累积。\n- 关键创新点包括：1) **DreamZero-Flash**：通过**解耦视频和动作的去噪时间表**，允许模型从有噪声的视觉上下文中预测干净的动作，实现极少的去噪步骤（如单步）而性能损失很小；2) 一系列**系统级、实现级和模型级优化**（如 CFG 并行、DiT 缓存、编译优化、量化等）以实现实时控制。\n## 数据集与资源\n- 使用**自收集的约500小时 AgiBot G1 移动双臂机器人遥操作数据**，覆盖22个独特环境，强调任务的多样性和实用性而非重复。\n- 使用公开的 **DROID** 数据集验证模型在 Franka 单臂机器人上的表现。\n- 模型规模为 **14B 参数**（基于 **Wan2.1-I2V-14B-480P** 主干）。\n- 训练资源使用了 **GPU** 进行大规模训练（具体型号和数量未在摘要中明确提及，但涉及批量大小为128的训练）。\n## 评估与结果\n- **评估环境与基准**：在 AgiBot G1 和 Franka (DROID) 上，针对**未见过的环境和物体**进行评估，包括**已见任务**（来自训练分布）和**未见任务**（新动词/技能）。\n- **主要评估指标**：**平均任务进度 (Average Task Progress)**。\n- **关键实验结果**：\n - **已见任务**：**DreamZero** 平均任务进度达到 **62.2%**，显著优于预训练的 VLA 基线（27.4%）和从头训练的 VLA 基线（接近0%）。\n - **未见任务**：**DreamZero** 平均任务进度达到 **39.5%**（AgiBot），优于预训练的 VLA 基线（16.3%）；在 DROID 上达到 **49%** 的任务进度，优于其他基线。\n - **后训练微调**：在特定任务数据上微调后，**DreamZero** 保持或优于 VLA 基线。\n - **跨具身视频迁移**：加入少量（10-20分钟）来自其他机器人或人类的仅视频数据，使 **DreamZero** 在未见任务上的性能提升到 **~55%**。\n - **推理速度**：通过优化，将每动作块的推理时间从 **5.7秒降低至150毫秒**，支持 **7Hz** 的实时控制。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>NVIDIA</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出 <strong>DreamZero</strong>，一种基于预训练视频扩散主干构建的 <strong>世界行动模型 (World Action Model, WAM)</strong>，通过联合预测未来视频和动作来学习物理动力学。</li><li>核心目标是解决现有 <strong>视觉-语言-行动模型 (Vision-Language-Action Models, VLAs)</strong> 在泛化到未见过的物理运动和技能方面的不足，特别是针对新环境、新任务和新机器人的泛化能力。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出 <strong>DreamZero</strong>，一个 <strong>14B</strong> 参数的 WAM，通过联合视频和动作预测，能够从<strong>多样、非重复的机器人数据</strong>中进行有效学习。</li><li>在真实机器人实验中，相比于最先进的 VLAs，在<strong>新任务和环境上的零样本泛化能力提升超过2倍</strong>。</li><li>通过模型和系统优化，实现了 <strong>38倍推理加速</strong>，使 <strong>14B 自回归视频扩散模型能够以 7Hz 的频率进行实时闭环控制</strong>。</li><li>展示了<strong>跨具身 (cross-embodiment) 迁移能力</strong>：仅使用来自其他机器人（20分钟）或人类（12分钟）的<strong>仅视频演示</strong>，即可在未见任务上实现<strong>超过42%的相对性能提升</strong>；并且展示了<strong>少量样本的具身适应 (few-shot embodiment adaptation)</strong>，仅用30分钟的玩耍数据就能适应新的机器人，同时保持零样本泛化能力。</li><li><strong>开源</strong>了模型权重、推理代码，以及用于运行公开的真实世界（RoboArena）和模拟基准（PolaRiS 和 Genie Sim 3.0）的代码。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>模型基于预训练的图像到视频扩散模型（<strong>Wan2.1-I2V-14B-480P</strong>）构建，添加了状态编码器、动作编码器和解码器，以<strong>联合去噪视频和动作潜在表示</strong>。</li><li>采用<strong>自回归、分块的流匹配 (flow matching) 训练目标</strong>，确保视频和动作模态的紧密对齐。通过 KV 缓存和异步闭环执行（用真实观测替换生成的帧）来实现高效推理，防止误差累积。</li><li>关键创新点包括：1) <strong>DreamZero-Flash</strong>：通过<strong>解耦视频和动作的去噪时间表</strong>，允许模型从有噪声的视觉上下文中预测干净的动作，实现极少的去噪步骤（如单步）而性能损失很小；2) 一系列<strong>系统级、实现级和模型级优化</strong>（如 CFG 并行、DiT 缓存、编译优化、量化等）以实现实时控制。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用<strong>自收集的约500小时 AgiBot G1 移动双臂机器人遥操作数据</strong>，覆盖22个独特环境，强调任务的多样性和实用性而非重复。</li><li>使用公开的 <strong>DROID</strong> 数据集验证模型在 Franka 单臂机器人上的表现。</li><li>模型规模为 <strong>14B 参数</strong>（基于 <strong>Wan2.1-I2V-14B-480P</strong> 主干）。</li><li>训练资源使用了 <strong>GPU</strong> 进行大规模训练（具体型号和数量未在摘要中明确提及，但涉及批量大小为128的训练）。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在 AgiBot G1 和 Franka (DROID) 上，针对<strong>未见过的环境和物体</strong>进行评估，包括<strong>已见任务</strong>（来自训练分布）和<strong>未见任务</strong>（新动词/技能）。</li><li><strong>主要评估指标</strong>：<strong>平均任务进度 (Average Task Progress)</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>已见任务</strong>：<strong>DreamZero</strong> 平均任务进度达到 <strong>62.2%</strong>，显著优于预训练的 VLA 基线（27.4%）和从头训练的 VLA 基线（接近0%）。</p>\n<p> - <strong>未见任务</strong>：<strong>DreamZero</strong> 平均任务进度达到 <strong>39.5%</strong>（AgiBot），优于预训练的 VLA 基线（16.3%）；在 DROID 上达到 <strong>49%</strong> 的任务进度，优于其他基线。</p>\n<p> - <strong>后训练微调</strong>：在特定任务数据上微调后，<strong>DreamZero</strong> 保持或优于 VLA 基线。</p>\n<p> - <strong>跨具身视频迁移</strong>：加入少量（10-20分钟）来自其他机器人或人类的仅视频数据，使 <strong>DreamZero</strong> 在未见任务上的性能提升到 <strong>~55%</strong>。</p>\n<p> - <strong>推理速度</strong>：通过优化，将每动作块的推理时间从 <strong>5.7秒降低至150毫秒</strong>，支持 <strong>7Hz</strong> 的实时控制。</p>"
  },
  {
    "date": "2026-02-17",
    "title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA",
    "link": "http://arxiv.org/abs/2602.15543",
    "summary_markdown": "## 研究单位\n- 作者未明确标注所属机构，但根据作者姓名（如 Young-Chae Son, Jung-Woo Lee 等）推测可能与韩国相关研究机构有关\n## 论文概述\n- 针对机器人多模态**VLA**模型中存在的静态融合问题，提出一种受人类主动感知启发的动态信息融合框架，以提升模型效率和鲁棒性\n- 通过引入轻量级的**Camera Router**架构，实时分析任务提示和腕部摄像头观测，预测多个相机视图的任务相关性，并条件性地衰减低效用视图的计算\n- 旨在解决机器人控制环境中多模态传感器信息冗余、计算开销大以及任务无关信息作为噪声干扰的问题\n## 核心贡献\n- 提出**上下文感知的路由器与门控机制**：一种基于腕部摄像头和语言提示输入，实时预测模态重要性的动态路由器\n- 展示了**长时程任务中的鲁棒性**：通过基于路由器的特征融合结构整合关键模态，减轻模态间干扰，在长时程操作任务上取得了比现有方法更高的成功率\n- 建立了**可扩展的VLM监督**流程：利用**Vision-Language Models**构建自动标注流水线，为路由器训练生成标注数据，无需额外数据收集成本\n- 实现了**端到端高效训练**：路由器与基于**Flow Matching**的VLA策略网络集成，可在机器人微调期间与VLA模型一同训练，无需额外数据\n## 方法描述\n- 核心是**Camera Router**架构：该模块以腕部摄像头图像和任务提示作为输入，通过注意力池化和平均池化提取特征，再经MLP和Softmax层输出对其他视图（如外部、热感、触觉摄像头）的重要性权重\n- 创新技术包括**动态门控**：利用路由器预测的权重和学习到的缩放参数，对原始特征进行加权，强调任务相关视觉信息，衰减低重要性视图\n- 采用**不平衡路由学习**：为解决稀疏标签的数据不平衡问题，使用**Focal Loss**训练路由器，聚焦于困难样本\n- 引入**VLM自动标注流水线**：利用**Qwen3-VL**模型，结合历史感知提示策略，分析视频片段并自动生成摄像头激活标签，为路由器提供监督信号\n## 数据集与资源\n- 使用自收集的**机器人操作数据集**：每个任务包含50条专家演示轨迹，数据包括机器人状态、动作、多模态图像和任务提示\n- 视觉数据来自多传感器：腕部摄像头（**Intel RealSense D435**）、外部摄像头（**Intel RealSense L515**）、热感相机（**ThermoEye TMC256B**）、触觉传感器（**Meta Digit**）\n- 模型基础为预训练的**VLA策略**，并采用**LoRA**进行微调以保持先验知识并提高训练效率\n- 自动标注使用**Qwen3-VL-30B-A3B-Instruct**模型\n## 评估与结果\n- **评估环境**：在三个真实世界机器人操作任务上进行测试：**电池分拣**（视觉+热感）、**电线选择**（视觉+触觉）、**阀门操作**（视觉+热感+触觉）\n- **主要指标**：子任务成功率\n- **关键结果**：\n - 提出的方法（以腕部摄像头+提示作为路由器输入）在三个任务上的平均成功率达到**83.33%**，显著优于仅使用多模态输入（10%）和静态融合基线方法\n - 路由器输入配置比较表明，**腕部摄像头+任务提示**的组合效果最佳\n - 在长时程序列任务中，该方法能维持高性能，显示了其鲁棒性\n - **VLM自动标注**的性能（83.33%平均成功率）与**人工标注**（86.67%）相当，验证了其有效性",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li>作者未明确标注所属机构，但根据作者姓名（如 Young-Chae Son, Jung-Woo Lee 等）推测可能与韩国相关研究机构有关</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>针对机器人多模态<strong>VLA</strong>模型中存在的静态融合问题，提出一种受人类主动感知启发的动态信息融合框架，以提升模型效率和鲁棒性</li><li>通过引入轻量级的<strong>Camera Router</strong>架构，实时分析任务提示和腕部摄像头观测，预测多个相机视图的任务相关性，并条件性地衰减低效用视图的计算</li><li>旨在解决机器人控制环境中多模态传感器信息冗余、计算开销大以及任务无关信息作为噪声干扰的问题</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出<strong>上下文感知的路由器与门控机制</strong>：一种基于腕部摄像头和语言提示输入，实时预测模态重要性的动态路由器</li><li>展示了<strong>长时程任务中的鲁棒性</strong>：通过基于路由器的特征融合结构整合关键模态，减轻模态间干扰，在长时程操作任务上取得了比现有方法更高的成功率</li><li>建立了<strong>可扩展的VLM监督</strong>流程：利用<strong>Vision-Language Models</strong>构建自动标注流水线，为路由器训练生成标注数据，无需额外数据收集成本</li><li>实现了<strong>端到端高效训练</strong>：路由器与基于<strong>Flow Matching</strong>的VLA策略网络集成，可在机器人微调期间与VLA模型一同训练，无需额外数据</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心是<strong>Camera Router</strong>架构：该模块以腕部摄像头图像和任务提示作为输入，通过注意力池化和平均池化提取特征，再经MLP和Softmax层输出对其他视图（如外部、热感、触觉摄像头）的重要性权重</li><li>创新技术包括<strong>动态门控</strong>：利用路由器预测的权重和学习到的缩放参数，对原始特征进行加权，强调任务相关视觉信息，衰减低重要性视图</li><li>采用<strong>不平衡路由学习</strong>：为解决稀疏标签的数据不平衡问题，使用<strong>Focal Loss</strong>训练路由器，聚焦于困难样本</li><li>引入<strong>VLM自动标注流水线</strong>：利用<strong>Qwen3-VL</strong>模型，结合历史感知提示策略，分析视频片段并自动生成摄像头激活标签，为路由器提供监督信号</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用自收集的<strong>机器人操作数据集</strong>：每个任务包含50条专家演示轨迹，数据包括机器人状态、动作、多模态图像和任务提示</li><li>视觉数据来自多传感器：腕部摄像头（<strong>Intel RealSense D435</strong>）、外部摄像头（<strong>Intel RealSense L515</strong>）、热感相机（<strong>ThermoEye TMC256B</strong>）、触觉传感器（<strong>Meta Digit</strong>）</li><li>模型基础为预训练的<strong>VLA策略</strong>，并采用<strong>LoRA</strong>进行微调以保持先验知识并提高训练效率</li><li>自动标注使用<strong>Qwen3-VL-30B-A3B-Instruct</strong>模型</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在三个真实世界机器人操作任务上进行测试：<strong>电池分拣</strong>（视觉+热感）、<strong>电线选择</strong>（视觉+触觉）、<strong>阀门操作</strong>（视觉+热感+触觉）</li><li><strong>主要指标</strong>：子任务成功率</li><li><strong>关键结果</strong>：</li></ul>\n<p> - 提出的方法（以腕部摄像头+提示作为路由器输入）在三个任务上的平均成功率达到<strong>83.33%</strong>，显著优于仅使用多模态输入（10%）和静态融合基线方法</p>\n<p> - 路由器输入配置比较表明，<strong>腕部摄像头+任务提示</strong>的组合效果最佳</p>\n<p> - 在长时程序列任务中，该方法能维持高性能，显示了其鲁棒性</p>\n<p> - <strong>VLM自动标注</strong>的性能（83.33%平均成功率）与<strong>人工标注</strong>（86.67%）相当，验证了其有效性</p>"
  },
  {
    "date": "2026-02-17",
    "title": "ActionCodec: What Makes for Good Action Tokenizers",
    "link": "http://arxiv.org/abs/2602.15397",
    "summary_markdown": "## 研究单位\n- 论文作者来自多个机构，可能包括高校或企业的研究团队，但提供的HTML原文中未明确列出机构名称，仅列出了作者姓名。\n## 论文概述\n- 论文旨在回答一个核心问题：“**什么构成了好的动作分词器（Action Tokenizers）**”，专门从**视觉-语言-动作（VLA）模型优化**的视角进行分析。\n- 论文提出了一套基于信息论的设计原则，用于指导高性能动作分词器的构建。\n- 为了解决现有VQ-based分词器在下游任务中表现不佳的问题，论文引入了**ActionCodec**，一个遵循这些原则的新型高性能动作分词器。\n## 核心贡献\n- 建立了一个严谨的分析框架，从VLA训练动力学的角度，明确了动作分词器的最佳设计需求，为通用物理智能的未来研究提供了系统性路线图。\n- 引入了**ActionCodec**，一个实现了已验证设计原则的高性能动作分词器，显著提升了VLA模型的训练效率和抗过拟合能力。\n- 提供了基于**ActionCodec**的VLA模型套件，在多个模拟和现实世界基准测试中取得了最先进的（SOTA）性能，甚至在无需机器人预训练、仅从通用VLM初始化的条件下也表现出色。\n- 通过信息论分解，识别并分析了影响VLA优化的三个关键指标：重叠率（Overlap Rate）、容量/词表大小（Capacity/Vocabulary Size）、以及感知对齐与残余语法（Perceptual Alignment vs. Residual Grammar）。\n- 提出了实用的增强功能，如**特定具身软提示（embodiment-specific soft-prompts）** 和**残差向量量化后训练（RVQ Post-training）**，以支持跨平台知识转移并提高重建保真度。\n## 方法描述\n- 论文采用**向量量化（VQ）** 框架，使用编码器-解码器结构将连续动作序列离散化为令牌序列。\n- 核心创新在于从VLA优化角度（而非单纯重建保真度）出发，通过信息论分析，提出并验证了设计最优动作分词器的关键原则：**最大化时序令牌重叠率、最小化词表冗余、增强令牌与多模态上下文间的互信息、以及确保令牌独立性**。\n- **ActionCodec**使用类似**Perceiver**的Transformer架构，具有处理可变长度序列的灵活性。\n- 关键技术包括：使用时间对比学习（TCL）和CLIP目标来提升感知对齐；采用独立的（无内部注意力）分词方案以最小化残余语法；以及通过RVQ后训练在不破坏拓扑稳定性的前提下提高重建精度。\n## 数据集与资源\n- 用于分词器训练和评估的数据集包括：**LIBERO**、**BridgeData**、**DROID**，以及用于现实世界评估的**SO100-ShapeSorter**和**xArm-PickVeg**数据集。\n- VLA模型基于**SmolVLM2**系列，论文中使用了不同规模的模型，如**SmolVLM2-256M**、**SmolVLM2-500M**和**SmolVLM2-2.2B**。\n- **ActionCodec**本身具有特定的模型规模，例如采用词汇表大小 **S=2048** 和令牌预算 **n=16** 的配置。\n- 训练资源未在提供的HTML片段中明确说明，但通常这类实验需要使用**GPU/TPU**集群。\n## 评估与结果\n- **评估环境与基准**：在**LIBERO**（包括Goal、Spatial、Object、Long四个任务套件）、**Simpler-WidowX**（仿真）以及真实机器人平台**SO100**和**xArm**上进行评估。\n- **主要评估指标**：任务**成功率（SR%）**、**平均排名（Avg. Rank）**、**训练效率**（收敛速度）、**推理延迟（Latency）** 和**吞吐量（Throughput）**。\n- **关键实验结果**：\n - **ActionCodec**在LIBERO基准测试中，使用**SmolVLM2-2.2B**微调，无需机器人预训练即可达到**95.5%** 的平均成功率。\n - 结合**块级自回归（BAR）** 范式后，**ActionCodec-BAR**在LIBERO上达到**97.4%** 的平均成功率，创造了无需机器人预训练的VLA模型新SOTA。\n - 与主流分词器（如Binning、String、VQ-VLA、MiniVLA、FAST）相比，**ActionCodec**在训练效率和最终性能上均显著领先，在5K训练步内达到89.5%成功率，远超其他方法。\n - 在**Simpler-WidowX**基准测试中，**ActionCodec-BAR**同样取得了最高排名和最佳平均成功率（**65.2%**）。\n - 现实世界实验表明，**ActionCodec**能够学习复杂的恢复行为，并在**xArm**平台上实现了**82.5%** 的成功率。\n - **ActionCodec**实现了**22.0 action/s** 的高吞吐量和**0.9秒**的低延迟，兼具高性能和高效率。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li>论文作者来自多个机构，可能包括高校或企业的研究团队，但提供的HTML原文中未明确列出机构名称，仅列出了作者姓名。</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文旨在回答一个核心问题：“<strong>什么构成了好的动作分词器（Action Tokenizers）</strong>”，专门从<strong>视觉-语言-动作（VLA）模型优化</strong>的视角进行分析。</li><li>论文提出了一套基于信息论的设计原则，用于指导高性能动作分词器的构建。</li><li>为了解决现有VQ-based分词器在下游任务中表现不佳的问题，论文引入了<strong>ActionCodec</strong>，一个遵循这些原则的新型高性能动作分词器。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>建立了一个严谨的分析框架，从VLA训练动力学的角度，明确了动作分词器的最佳设计需求，为通用物理智能的未来研究提供了系统性路线图。</li><li>引入了<strong>ActionCodec</strong>，一个实现了已验证设计原则的高性能动作分词器，显著提升了VLA模型的训练效率和抗过拟合能力。</li><li>提供了基于<strong>ActionCodec</strong>的VLA模型套件，在多个模拟和现实世界基准测试中取得了最先进的（SOTA）性能，甚至在无需机器人预训练、仅从通用VLM初始化的条件下也表现出色。</li><li>通过信息论分解，识别并分析了影响VLA优化的三个关键指标：重叠率（Overlap Rate）、容量/词表大小（Capacity/Vocabulary Size）、以及感知对齐与残余语法（Perceptual Alignment vs. Residual Grammar）。</li><li>提出了实用的增强功能，如<strong>特定具身软提示（embodiment-specific soft-prompts）</strong> 和<strong>残差向量量化后训练（RVQ Post-training）</strong>，以支持跨平台知识转移并提高重建保真度。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>论文采用<strong>向量量化（VQ）</strong> 框架，使用编码器-解码器结构将连续动作序列离散化为令牌序列。</li><li>核心创新在于从VLA优化角度（而非单纯重建保真度）出发，通过信息论分析，提出并验证了设计最优动作分词器的关键原则：<strong>最大化时序令牌重叠率、最小化词表冗余、增强令牌与多模态上下文间的互信息、以及确保令牌独立性</strong>。</li><li><strong>ActionCodec</strong>使用类似<strong>Perceiver</strong>的Transformer架构，具有处理可变长度序列的灵活性。</li><li>关键技术包括：使用时间对比学习（TCL）和CLIP目标来提升感知对齐；采用独立的（无内部注意力）分词方案以最小化残余语法；以及通过RVQ后训练在不破坏拓扑稳定性的前提下提高重建精度。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>用于分词器训练和评估的数据集包括：<strong>LIBERO</strong>、<strong>BridgeData</strong>、<strong>DROID</strong>，以及用于现实世界评估的<strong>SO100-ShapeSorter</strong>和<strong>xArm-PickVeg</strong>数据集。</li><li>VLA模型基于<strong>SmolVLM2</strong>系列，论文中使用了不同规模的模型，如<strong>SmolVLM2-256M</strong>、<strong>SmolVLM2-500M</strong>和<strong>SmolVLM2-2.2B</strong>。</li><li><strong>ActionCodec</strong>本身具有特定的模型规模，例如采用词汇表大小 <strong>S=2048</strong> 和令牌预算 <strong>n=16</strong> 的配置。</li><li>训练资源未在提供的HTML片段中明确说明，但通常这类实验需要使用<strong>GPU/TPU</strong>集群。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在<strong>LIBERO</strong>（包括Goal、Spatial、Object、Long四个任务套件）、<strong>Simpler-WidowX</strong>（仿真）以及真实机器人平台<strong>SO100</strong>和<strong>xArm</strong>上进行评估。</li><li><strong>主要评估指标</strong>：任务<strong>成功率（SR%）</strong>、<strong>平均排名（Avg. Rank）</strong>、<strong>训练效率</strong>（收敛速度）、<strong>推理延迟（Latency）</strong> 和<strong>吞吐量（Throughput）</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>ActionCodec</strong>在LIBERO基准测试中，使用<strong>SmolVLM2-2.2B</strong>微调，无需机器人预训练即可达到<strong>95.5%</strong> 的平均成功率。</p>\n<p> - 结合<strong>块级自回归（BAR）</strong> 范式后，<strong>ActionCodec-BAR</strong>在LIBERO上达到<strong>97.4%</strong> 的平均成功率，创造了无需机器人预训练的VLA模型新SOTA。</p>\n<p> - 与主流分词器（如Binning、String、VQ-VLA、MiniVLA、FAST）相比，<strong>ActionCodec</strong>在训练效率和最终性能上均显著领先，在5K训练步内达到89.5%成功率，远超其他方法。</p>\n<p> - 在<strong>Simpler-WidowX</strong>基准测试中，<strong>ActionCodec-BAR</strong>同样取得了最高排名和最佳平均成功率（<strong>65.2%</strong>）。</p>\n<p> - 现实世界实验表明，<strong>ActionCodec</strong>能够学习复杂的恢复行为，并在<strong>xArm</strong>平台上实现了<strong>82.5%</strong> 的成功率。</p>\n<p> - <strong>ActionCodec</strong>实现了<strong>22.0 action/s</strong> 的高吞吐量和<strong>0.9秒</strong>的低延迟，兼具高性能和高效率。</p>"
  },
  {
    "date": "2026-02-16",
    "title": "DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI",
    "link": "http://arxiv.org/abs/2602.14974",
    "summary_markdown": "## 研究单位\n- **DM0 Team**\n- **Dexmal**\n- **StepFun**\n## 论文概述\n- 提出了一种名为 **DM0** 的 **Embodied-Native**（具身原生）的 Vision-Language-Action 框架，旨在实现 Physical AI，与传统“先预训练后适配”的范式不同。\n- 旨在解决当前 VLA 模型通常在互联网静态数据上预训练，缺乏内在物理基础，从而导致适应物理任务时出现知识遗忘或性能下降的问题。\n- 通过一个统一的三阶段训练流程，从初始阶段就融合了网络、驾驶和具身数据，同时学习语义知识和物理先验。\n## 核心贡献\n- 提出了 **Embodied-Native** 的训练范式，从早期阶段就将具身数据作为一等公民，与其他模态数据统一训练。\n- 设计了一个**三阶段训练流程**：预训练、中间训练和后训练，逐步融合多种数据源并最终专精于目标平台。\n- 采用**多源混合训练**策略，在具身数据上解耦动作专家和 VLM 的梯度，以保留 VLM 的通用知识。\n- 引入了**具身空间支架**策略，通过生成空间思维链来分解复杂指令，约束动作解空间。\n- 开发了基于 **Flow Matching** 的动作专家模型，并与 VLM 骨干集成，实现了从感知、推理到连续动作的端到端学习。\n## 方法描述\n- 模型架构：基于 **Qwen3-1.7B** 大语言模型构建 VLM 骨干，并整合了感知编码器来处理多模态输入；其上构建了一个基于 **Flow Matching** 的动作专家来生成连续机器人动作。\n- 核心技术：采用**多源混合训练**策略，对于具身数据，动作专家的梯度不回传到 VLM；对于非具身数据，VLM 继续训练以保留通用能力。\n- 创新点：**Embodied Spatial Scaffolding** 策略，通过层次化预测（子任务、目标框、末端轨迹、离散动作）引导模型从高层语义推理逐步过渡到低层控制。\n## 数据集与资源\n- 预训练数据：整合了知识（网络文本、图像-文本对）、教育（K-12、大学科目）、OCR、视觉问答、GUI、**驾驶**场景和**具身**交互日志等多种来源的数据。\n- 中间训练数据：混合了视觉-语言数据、具身推理数据、仿真数据、单臂/双臂机器人数据（如 **LIBERO**、**RoboTwin2.0**、**OXE**、**ALOHA** 等）。\n- 模型规模：总参数量约为 **2B**，骨干模型基于 **Qwen3-1.7B**。\n- 训练资源：预训练使用了大规模 GPU 集群，中间训练使用了 **64 块 NVIDIA H20 GPU**，后训练和 SFT 使用了 **8 到 16 块 H20 GPU**。\n## 评估与结果\n- 评估环境：在真实世界的 **RoboChallenge** 基准测试上进行评估，特别是 **Table30** 任务套件。\n- 评估指标：主要使用**成功率**，在通才模型评估中还结合了**任务分数**。\n- 关键实验结果：**DM0-Specialist** 在 Table30 上的平均成功率达到 **62.0%**，超过了 **GigaBrain-0.1 (51.67%)**、**Spirit-v1.5 (51.00%)** 和 **π0.5 (42.67%)**。**DM0-Generalist** 的平均成功率为 **37.3%**，显著优于 **π0.5-Generalist (17.67%)** 和 **π0-Generalist (9.0%)**。\n- 额外评估：验证了 **DM0** 在场景理解、OCR、子任务预测等通用多模态理解任务上的能力。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>DM0 Team</strong></li><li><strong>Dexmal</strong></li><li><strong>StepFun</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种名为 <strong>DM0</strong> 的 <strong>Embodied-Native</strong>（具身原生）的 Vision-Language-Action 框架，旨在实现 Physical AI，与传统“先预训练后适配”的范式不同。</li><li>旨在解决当前 VLA 模型通常在互联网静态数据上预训练，缺乏内在物理基础，从而导致适应物理任务时出现知识遗忘或性能下降的问题。</li><li>通过一个统一的三阶段训练流程，从初始阶段就融合了网络、驾驶和具身数据，同时学习语义知识和物理先验。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>Embodied-Native</strong> 的训练范式，从早期阶段就将具身数据作为一等公民，与其他模态数据统一训练。</li><li>设计了一个<strong>三阶段训练流程</strong>：预训练、中间训练和后训练，逐步融合多种数据源并最终专精于目标平台。</li><li>采用<strong>多源混合训练</strong>策略，在具身数据上解耦动作专家和 VLM 的梯度，以保留 VLM 的通用知识。</li><li>引入了<strong>具身空间支架</strong>策略，通过生成空间思维链来分解复杂指令，约束动作解空间。</li><li>开发了基于 <strong>Flow Matching</strong> 的动作专家模型，并与 VLM 骨干集成，实现了从感知、推理到连续动作的端到端学习。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>模型架构：基于 <strong>Qwen3-1.7B</strong> 大语言模型构建 VLM 骨干，并整合了感知编码器来处理多模态输入；其上构建了一个基于 <strong>Flow Matching</strong> 的动作专家来生成连续机器人动作。</li><li>核心技术：采用<strong>多源混合训练</strong>策略，对于具身数据，动作专家的梯度不回传到 VLM；对于非具身数据，VLM 继续训练以保留通用能力。</li><li>创新点：<strong>Embodied Spatial Scaffolding</strong> 策略，通过层次化预测（子任务、目标框、末端轨迹、离散动作）引导模型从高层语义推理逐步过渡到低层控制。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>预训练数据：整合了知识（网络文本、图像-文本对）、教育（K-12、大学科目）、OCR、视觉问答、GUI、<strong>驾驶</strong>场景和<strong>具身</strong>交互日志等多种来源的数据。</li><li>中间训练数据：混合了视觉-语言数据、具身推理数据、仿真数据、单臂/双臂机器人数据（如 <strong>LIBERO</strong>、<strong>RoboTwin2.0</strong>、<strong>OXE</strong>、<strong>ALOHA</strong> 等）。</li><li>模型规模：总参数量约为 <strong>2B</strong>，骨干模型基于 <strong>Qwen3-1.7B</strong>。</li><li>训练资源：预训练使用了大规模 GPU 集群，中间训练使用了 <strong>64 块 NVIDIA H20 GPU</strong>，后训练和 SFT 使用了 <strong>8 到 16 块 H20 GPU</strong>。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：在真实世界的 <strong>RoboChallenge</strong> 基准测试上进行评估，特别是 <strong>Table30</strong> 任务套件。</li><li>评估指标：主要使用<strong>成功率</strong>，在通才模型评估中还结合了<strong>任务分数</strong>。</li><li>关键实验结果：<strong>DM0-Specialist</strong> 在 Table30 上的平均成功率达到 <strong>62.0%</strong>，超过了 <strong>GigaBrain-0.1 (51.67%)</strong>、<strong>Spirit-v1.5 (51.00%)</strong> 和 <strong>π0.5 (42.67%)</strong>。<strong>DM0-Generalist</strong> 的平均成功率为 <strong>37.3%</strong>，显著优于 <strong>π0.5-Generalist (17.67%)</strong> 和 <strong>π0-Generalist (9.0%)</strong>。</li><li>额外评估：验证了 <strong>DM0</strong> 在场景理解、OCR、子任务预测等通用多模态理解任务上的能力。</li></ul>"
  },
  {
    "date": "2026-02-16",
    "title": "DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving",
    "link": "http://arxiv.org/abs/2602.14577",
    "summary_markdown": "## 研究单位\n- **华中科技大学**\n- **小米汽车**\n- **清华大学智能产业研究院（AIR）**\n## 论文概述\n- 提出了一种名为 **DriveFine** 的新型自动驾驶视觉-语言-动作模型，旨在结合扩散式规划器和基于令牌的规划器的互补优势，实现更精确和鲁棒的驾驶。\n- 核心目标是解决现有自动驾驶 VLA 模型中扩散式规划器存在模态对齐困难、泛化能力弱，以及基于令牌的规划器存在不可逆解码错误累积的问题。\n## 核心贡献\n- 对主流的扩散式和基于令牌的 VLA 规划器的优缺点进行了深入分析。\n- 提出了 **DriveFine** 模型，设计了一种“即插即用”的**块级混合专家架构**，以最小成本为基于令牌的 VLA 注入了轨迹优化能力。\n- 针对性地设计了一种**混合强化学习策略**，进一步提升了 DriveFine 的性能上限。\n- 在 **NAVSIM v1、v2 和 Navhard** 等基准测试上进行了广泛实验，证明了 DriveFine 的一致性和最先进的性能。\n## 方法描述\n- 方法以预训练的**掩码扩散大语言模型**为基础规划器，利用其并行解码、双向注意力建模和灵活解码顺序的优势。\n- 创新性地引入了**块级混合专家**：将模型后部的若干 Transformer 块复制为“优化专家”，与原始“生成专家”解耦。在训练和推理时通过梯度隔离和显式选择，实现生成与优化功能的分离，保留了预训练模型的基础能力。\n- 设计了一种**在线-离线混合强化学习**范式：生成专家通过 GRPO 在线采样轨迹；优化专家则利用生成专家采样的轨迹进行离线对比学习，同时进行在线探索优化，二者通过混合优势函数联合训练。\n## 数据集与资源\n- 使用 **NAVSIM** 数据集进行评估，该数据集基于 **nuPlan** 和 **OpenScene**，包含多摄像头图像和 LiDAR 点云，分为 navtrain 和 navtest 等子集。\n- 模型基于 **LLaDA-8B** 扩散语言模型和 **SigLIP-384** 视觉编码器构建。\n- 训练资源未在提供的文本中明确提及，但包含监督微调和强化学习微调两个阶段。\n## 评估与结果\n- 评估环境：**NAVSIM v1** (PDMS 指标)、**NAVSIM v2** (EPDMS 指标)、**Navhard** 两阶段评估基准。\n- 主要评估指标：**PDMS** 和 **EPDMS**，以及构成它们的多个子指标（如 NC、DAC、TTC、舒适度、车道保持、交通灯合规等）。\n- 关键实验结果：DriveFine 在 **NAVSIM v1** 上达到 90.8 PDMS，在 **NAVSIM v2** 上达到 89.7 EPDMS，在更具挑战性的 **Navhard** 基准上也优于扩散基线方法，各项实验均展示了其优越的性能和鲁棒性。消融实验证实了块级 MoE 和混合强化学习策略的有效性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>华中科技大学</strong></li><li><strong>小米汽车</strong></li><li><strong>清华大学智能产业研究院（AIR）</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种名为 <strong>DriveFine</strong> 的新型自动驾驶视觉-语言-动作模型，旨在结合扩散式规划器和基于令牌的规划器的互补优势，实现更精确和鲁棒的驾驶。</li><li>核心目标是解决现有自动驾驶 VLA 模型中扩散式规划器存在模态对齐困难、泛化能力弱，以及基于令牌的规划器存在不可逆解码错误累积的问题。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>对主流的扩散式和基于令牌的 VLA 规划器的优缺点进行了深入分析。</li><li>提出了 <strong>DriveFine</strong> 模型，设计了一种“即插即用”的<strong>块级混合专家架构</strong>，以最小成本为基于令牌的 VLA 注入了轨迹优化能力。</li><li>针对性地设计了一种<strong>混合强化学习策略</strong>，进一步提升了 DriveFine 的性能上限。</li><li>在 <strong>NAVSIM v1、v2 和 Navhard</strong> 等基准测试上进行了广泛实验，证明了 DriveFine 的一致性和最先进的性能。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>方法以预训练的<strong>掩码扩散大语言模型</strong>为基础规划器，利用其并行解码、双向注意力建模和灵活解码顺序的优势。</li><li>创新性地引入了<strong>块级混合专家</strong>：将模型后部的若干 Transformer 块复制为“优化专家”，与原始“生成专家”解耦。在训练和推理时通过梯度隔离和显式选择，实现生成与优化功能的分离，保留了预训练模型的基础能力。</li><li>设计了一种<strong>在线-离线混合强化学习</strong>范式：生成专家通过 GRPO 在线采样轨迹；优化专家则利用生成专家采样的轨迹进行离线对比学习，同时进行在线探索优化，二者通过混合优势函数联合训练。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用 <strong>NAVSIM</strong> 数据集进行评估，该数据集基于 <strong>nuPlan</strong> 和 <strong>OpenScene</strong>，包含多摄像头图像和 LiDAR 点云，分为 navtrain 和 navtest 等子集。</li><li>模型基于 <strong>LLaDA-8B</strong> 扩散语言模型和 <strong>SigLIP-384</strong> 视觉编码器构建。</li><li>训练资源未在提供的文本中明确提及，但包含监督微调和强化学习微调两个阶段。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：<strong>NAVSIM v1</strong> (PDMS 指标)、<strong>NAVSIM v2</strong> (EPDMS 指标)、<strong>Navhard</strong> 两阶段评估基准。</li><li>主要评估指标：<strong>PDMS</strong> 和 <strong>EPDMS</strong>，以及构成它们的多个子指标（如 NC、DAC、TTC、舒适度、车道保持、交通灯合规等）。</li><li>关键实验结果：DriveFine 在 <strong>NAVSIM v1</strong> 上达到 90.8 PDMS，在 <strong>NAVSIM v2</strong> 上达到 89.7 EPDMS，在更具挑战性的 <strong>Navhard</strong> 基准上也优于扩散基线方法，各项实验均展示了其优越的性能和鲁棒性。消融实验证实了块级 MoE 和混合强化学习策略的有效性。</li></ul>"
  },
  {
    "date": "2026-02-15",
    "title": "WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL",
    "link": "http://arxiv.org/abs/2602.13977",
    "summary_markdown": "## 研究单位\n- **Tsinghua University**\n- **Institute of Automation, Chinese Academy of Sciences**\n- **University of Chinese Academy of Sciences**\n- **Zhongguancun Academy**\n- **Infinigence AI**\n## 论文概述\n- 提出 **WoVR** 框架，旨在解决基于世界模型（World Model）进行视觉-语言-动作（**VLA**）策略后训练强化学习（RL）时，闭环想象中存在的 **幻觉（Hallucination）** 问题。\n- 核心目标是：不将世界模型视为完美模拟器，而是通过设计可控的模拟器、可靠的交互协议和策略-模型协同演化，在存在不完美想象动力学的情况下实现可靠的策略优化。\n## 核心贡献\n- 识别并形式化了基于世界模型的强化学习中，由于闭环想象交互中自回归误差累积和政策诱导分布偏移导致的 **幻觉** 问题。\n- 提出了一个系统性的幻觉感知强化学习框架 **WoVR**，该框架集成了**稳定的动作条件化世界模型、关键帧初始化推理（KIR）和策略对齐协同演化（PACE）**。\n- 在 **LIBERO** 基准测试和真实机器人（Franka Panda）操控任务上进行了广泛实验，证明了方法的有效性，显著提升了任务成功率。\n## 方法描述\n- 核心方法是 **WoVR** 框架，从三个层面调控幻觉的可靠性：**可控模拟器设计、可靠想象交互、策略-模型对齐**。\n- **稳定动作条件化世界模型**：基于 **Wan2.2-TI2V-5B** 视频扩散主干，通过双通道动作注入和**首帧锚定（first-frame anchoring）** 实现稳定、可控的长期推演生成。\n- **关键帧初始化推理（KIR）**：在任务关键状态（尤其是失败状态）附近初始化推演，有效缩短预测深度，减少误差累积。\n- **策略对齐协同演化（PACE）**：在策略优化过程中，通过收集少量策略新数据，低频率地协同演化世界模型，以减轻策略分布偏移带来的不匹配。\n## 数据集与资源\n- 主要使用 **LIBERO** 基准测试环境的数据集。\n- 世界模型基于 **Wan2.2-TI2V-5B** 视频扩散模型（约 **50亿参数**）进行改造和训练。\n- 训练和实验使用了 **8块 NVIDIA H100 GPU**。\n## 评估与结果\n- **世界模型质量评估**：在 LIBERO 数据集上，评估了 **LPIPS、FID、FVD、FloLPIPS** 等指标和推理速度（FPS）。**WoVR** 在所有指标和不同推演长度（128/256/512）上均优于 **EVAC、Cosmos-Predict2** 和 **OpenSora（WMPO使用）** 等基线，且达到 **23 FPS** 的高效推演速度。\n- **策略性能评估**：在 LIBERO 的 Spatial、Object、Goal、Long 四个任务套件上，与基线（OpenVLA-OFT-base, GRPO(Online), WMPO）比较**成功率（SR）**。WoVR 将平均成功率从 **39.95%** 提升至 **69.2%**（+29.3），显著超越所有基线。\n- **真实世界迁移评估**：在 Franka Panda 机器人上执行“Pick Banana”和“Pick Bread”任务。WoVR 将平均成功率从 **61.7%** 提升至 **91.7%**（+30.0），验证了方法在真实世界的可靠迁移。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Tsinghua University</strong></li><li><strong>Institute of Automation, Chinese Academy of Sciences</strong></li><li><strong>University of Chinese Academy of Sciences</strong></li><li><strong>Zhongguancun Academy</strong></li><li><strong>Infinigence AI</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>WoVR</strong> 框架，旨在解决基于世界模型（World Model）进行视觉-语言-动作（<strong>VLA</strong>）策略后训练强化学习（RL）时，闭环想象中存在的 <strong>幻觉（Hallucination）</strong> 问题。</li><li>核心目标是：不将世界模型视为完美模拟器，而是通过设计可控的模拟器、可靠的交互协议和策略-模型协同演化，在存在不完美想象动力学的情况下实现可靠的策略优化。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>识别并形式化了基于世界模型的强化学习中，由于闭环想象交互中自回归误差累积和政策诱导分布偏移导致的 <strong>幻觉</strong> 问题。</li><li>提出了一个系统性的幻觉感知强化学习框架 <strong>WoVR</strong>，该框架集成了<strong>稳定的动作条件化世界模型、关键帧初始化推理（KIR）和策略对齐协同演化（PACE）</strong>。</li><li>在 <strong>LIBERO</strong> 基准测试和真实机器人（Franka Panda）操控任务上进行了广泛实验，证明了方法的有效性，显著提升了任务成功率。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心方法是 <strong>WoVR</strong> 框架，从三个层面调控幻觉的可靠性：<strong>可控模拟器设计、可靠想象交互、策略-模型对齐</strong>。</li><li><strong>稳定动作条件化世界模型</strong>：基于 <strong>Wan2.2-TI2V-5B</strong> 视频扩散主干，通过双通道动作注入和<strong>首帧锚定（first-frame anchoring）</strong> 实现稳定、可控的长期推演生成。</li><li><strong>关键帧初始化推理（KIR）</strong>：在任务关键状态（尤其是失败状态）附近初始化推演，有效缩短预测深度，减少误差累积。</li><li><strong>策略对齐协同演化（PACE）</strong>：在策略优化过程中，通过收集少量策略新数据，低频率地协同演化世界模型，以减轻策略分布偏移带来的不匹配。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>主要使用 <strong>LIBERO</strong> 基准测试环境的数据集。</li><li>世界模型基于 <strong>Wan2.2-TI2V-5B</strong> 视频扩散模型（约 <strong>50亿参数</strong>）进行改造和训练。</li><li>训练和实验使用了 <strong>8块 NVIDIA H100 GPU</strong>。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>世界模型质量评估</strong>：在 LIBERO 数据集上，评估了 <strong>LPIPS、FID、FVD、FloLPIPS</strong> 等指标和推理速度（FPS）。<strong>WoVR</strong> 在所有指标和不同推演长度（128/256/512）上均优于 <strong>EVAC、Cosmos-Predict2</strong> 和 <strong>OpenSora（WMPO使用）</strong> 等基线，且达到 <strong>23 FPS</strong> 的高效推演速度。</li><li><strong>策略性能评估</strong>：在 LIBERO 的 Spatial、Object、Goal、Long 四个任务套件上，与基线（OpenVLA-OFT-base, GRPO(Online), WMPO）比较<strong>成功率（SR）</strong>。WoVR 将平均成功率从 <strong>39.95%</strong> 提升至 <strong>69.2%</strong>（+29.3），显著超越所有基线。</li><li><strong>真实世界迁移评估</strong>：在 Franka Panda 机器人上执行“Pick Banana”和“Pick Bread”任务。WoVR 将平均成功率从 <strong>61.7%</strong> 提升至 <strong>91.7%</strong>（+30.0），验证了方法在真实世界的可靠迁移。</li></ul>"
  },
  {
    "date": "2026-02-14",
    "title": "Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation",
    "link": "http://arxiv.org/abs/2602.13833",
    "summary_markdown": "## 研究单位\n- **Kevin Yuchen Ma** (1,2), **Heng Zhang** (1,3), **Weisi Lin** (3), **Mike Zheng Shou** (2), **Yan Wu** (1)\n- 机构标注显示作者可能来自新加坡南洋理工大学或类似研究机构，论文具体机构名称未在提供的HTML片段中明确列出。\n## 论文概述\n- 论文提出了一种名为**Semantic-Contact Fields (SCFields)**的统一3D表示，旨在解决类别级、可泛化的、依赖接触的机器人工具操作问题。\n- 核心目标是弥合高层语义规划（知道“在哪里”使用工具）与高保真物理控制（知道“如何”调节接触力）之间的鸿沟，使策略能够泛化到未见过的工具变体。\n- 主要挑战在于大规模获取真实世界触觉数据极其困难，而直接从模拟到现实进行零样本迁移又因软体传感器复杂的非线性变形动力学而极具挑战性。\n## 核心贡献\n- 提出了**Semantic-Contact Fields (SCFields)**，一个融合了预训练视觉模型语义特征与密集外部接触估计（接触概率和力矢量）的不变3D表示。\n- 引入了一个包含**两阶段的模拟到现实接触学习流程**：首先在大规模模拟数据上预训练以学习通用接触物理；然后在少量通过几何启发式和力优化生成伪标签的真实数据上微调，以实现传感器特性对齐。\n- 基于**PointNet++的“触觉即点云”统一网络架构**，将触觉信号与场景几何直接融合，无需独立的姿态编码器即可处理动态抓握适应。\n- 将SCFields作为密集观察输入，用于训练一个**3D扩散策略**，在真实世界的刮擦、蜡笔画画和削皮任务上实现了鲁棒的类别级泛化，显著优于纯视觉和原始触觉基线。\n## 方法描述\n- 方法分为两个主要阶段：**接触场学习和策略学习**。\n- **接触场学习**：使用统一的多模态感知模型，将原始观测（工具/环境点云、触觉传感器读数、本体感觉）映射到工具表面密集的**外部接触场**（每个点包含接触概率和3D力矢量）。采用两阶段训练：1) 在大规模合成数据集（300个工具实例，32万帧）上进行模拟预训练；2) 在少量使用启发式几何和凸优化（SOCP）生成伪标签的真实世界校准数据上进行微调（真实世界对齐）。\n- **创新点与关键技术**：使用“**Tactile-as-PointCloud**”架构进行触觉-视觉融合；通过**焦点损失 (Focal Loss)**处理类别不平衡；通过真实世界对齐策略有效弥合模拟-现实差距，无需高保真模拟或仪器化工具。\n## 数据集与资源\n- 使用了大规模**合成数据集**，包含300个独特工具（刮刀、蜡笔、削皮器）和320,000帧数据，通过多模拟器流水线（IsaacGym, TacSL, Open3D, PyBullet）生成。\n- 收集了少量**真实世界校准数据**（用于微调），通过结构化的刮擦任务和伪标签生成方法获得。\n- 模型基于**PointNet++**架构。\n- 训练资源：模拟训练使用GPU集群，具体型号和数量未在提供的片段中详细说明。\n## 评估与结果\n- **评估环境与基准**：在配备两个**GelSight Mini触觉传感器**和三个RealSense D435摄像头的**Franka Emika Panda**机器人上进行真实世界实验。评估了三个任务：刮擦、蜡笔画十字、胡萝卜削皮。\n- **主要评估指标**：\n - **接触场评估**：F1分数（接触检测）、力向量均方误差 (MSE)。\n - **策略评估**：成功率(SR)、清洁效率(Eff)、归一化效率(Eff Norm)、绘画一致性(0-1分)、接触率、切入率、平均削皮长度。\n- **关键实验结果**：\n - 接触场模型：与纯模拟(Sim-Only)和纯真实(Real-Only)模型相比，提出的**对齐模型(Ours Aligned)**在未见过的工具上（如蜡笔）取得了最高的F1分数（0.657）。\n - 策略性能：**SCFields**在所有任务上均显著优于基线（Vision-Only, Raw Tactile）。例如，在刮擦任务中，对未见工具的归一化效率达到84.7%，而纯视觉基线仅为35.1%。在削皮任务中，平均削皮长度达到4.52厘米，是纯视觉基线（1.12厘米）的四倍。\n - 消融实验证明了接触力向量估计的重要性（“No Force”模型性能下降），以及两阶段模拟-现实对齐策略的有效性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Kevin Yuchen Ma</strong> (1,2), <strong>Heng Zhang</strong> (1,3), <strong>Weisi Lin</strong> (3), <strong>Mike Zheng Shou</strong> (2), <strong>Yan Wu</strong> (1)</li><li>机构标注显示作者可能来自新加坡南洋理工大学或类似研究机构，论文具体机构名称未在提供的HTML片段中明确列出。</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出了一种名为<strong>Semantic-Contact Fields (SCFields)</strong>的统一3D表示，旨在解决类别级、可泛化的、依赖接触的机器人工具操作问题。</li><li>核心目标是弥合高层语义规划（知道“在哪里”使用工具）与高保真物理控制（知道“如何”调节接触力）之间的鸿沟，使策略能够泛化到未见过的工具变体。</li><li>主要挑战在于大规模获取真实世界触觉数据极其困难，而直接从模拟到现实进行零样本迁移又因软体传感器复杂的非线性变形动力学而极具挑战性。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了<strong>Semantic-Contact Fields (SCFields)</strong>，一个融合了预训练视觉模型语义特征与密集外部接触估计（接触概率和力矢量）的不变3D表示。</li><li>引入了一个包含<strong>两阶段的模拟到现实接触学习流程</strong>：首先在大规模模拟数据上预训练以学习通用接触物理；然后在少量通过几何启发式和力优化生成伪标签的真实数据上微调，以实现传感器特性对齐。</li><li>基于<strong>PointNet++的“触觉即点云”统一网络架构</strong>，将触觉信号与场景几何直接融合，无需独立的姿态编码器即可处理动态抓握适应。</li><li>将SCFields作为密集观察输入，用于训练一个<strong>3D扩散策略</strong>，在真实世界的刮擦、蜡笔画画和削皮任务上实现了鲁棒的类别级泛化，显著优于纯视觉和原始触觉基线。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>方法分为两个主要阶段：<strong>接触场学习和策略学习</strong>。</li><li><strong>接触场学习</strong>：使用统一的多模态感知模型，将原始观测（工具/环境点云、触觉传感器读数、本体感觉）映射到工具表面密集的<strong>外部接触场</strong>（每个点包含接触概率和3D力矢量）。采用两阶段训练：1) 在大规模合成数据集（300个工具实例，32万帧）上进行模拟预训练；2) 在少量使用启发式几何和凸优化（SOCP）生成伪标签的真实世界校准数据上进行微调（真实世界对齐）。</li><li><strong>创新点与关键技术</strong>：使用“<strong>Tactile-as-PointCloud</strong>”架构进行触觉-视觉融合；通过<strong>焦点损失 (Focal Loss)</strong>处理类别不平衡；通过真实世界对齐策略有效弥合模拟-现实差距，无需高保真模拟或仪器化工具。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用了大规模<strong>合成数据集</strong>，包含300个独特工具（刮刀、蜡笔、削皮器）和320,000帧数据，通过多模拟器流水线（IsaacGym, TacSL, Open3D, PyBullet）生成。</li><li>收集了少量<strong>真实世界校准数据</strong>（用于微调），通过结构化的刮擦任务和伪标签生成方法获得。</li><li>模型基于<strong>PointNet++</strong>架构。</li><li>训练资源：模拟训练使用GPU集群，具体型号和数量未在提供的片段中详细说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在配备两个<strong>GelSight Mini触觉传感器</strong>和三个RealSense D435摄像头的<strong>Franka Emika Panda</strong>机器人上进行真实世界实验。评估了三个任务：刮擦、蜡笔画十字、胡萝卜削皮。</li><li><strong>主要评估指标</strong>：</li></ul>\n<p> - <strong>接触场评估</strong>：F1分数（接触检测）、力向量均方误差 (MSE)。</p>\n<p> - <strong>策略评估</strong>：成功率(SR)、清洁效率(Eff)、归一化效率(Eff Norm)、绘画一致性(0-1分)、接触率、切入率、平均削皮长度。</p>\n<ul><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 接触场模型：与纯模拟(Sim-Only)和纯真实(Real-Only)模型相比，提出的<strong>对齐模型(Ours Aligned)</strong>在未见过的工具上（如蜡笔）取得了最高的F1分数（0.657）。</p>\n<p> - 策略性能：<strong>SCFields</strong>在所有任务上均显著优于基线（Vision-Only, Raw Tactile）。例如，在刮擦任务中，对未见工具的归一化效率达到84.7%，而纯视觉基线仅为35.1%。在削皮任务中，平均削皮长度达到4.52厘米，是纯视觉基线（1.12厘米）的四倍。</p>\n<p> - 消融实验证明了接触力向量估计的重要性（“No Force”模型性能下降），以及两阶段模拟-现实对齐策略的有效性。</p>"
  },
  {
    "date": "2026-02-14",
    "title": "MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer",
    "link": "http://arxiv.org/abs/2602.13764",
    "summary_markdown": "## 研究单位\n- **The University of Queensland** (作者： Heng Tao Shen)\n- **Nanjing University** (作者： Lei Zhu)\n- **Chinese University of Hong Kong (Shenzhen)** (作者： Fengling Li, Guoli Yang)\n- 从作者列表推断，可能还包括其他研究机构（未明确列出所有机构）\n## 论文概述\n- 提出 **MOTIF**，一种用于**小样本跨具身智能体迁移**的分层框架，其核心思想是从异构机器人数据中解耦出与具身无关的时空模式，称为**动作基元**。\n- 旨在解决现有跨具身策略（如共享-私有架构）在有限私有参数容量和缺乏显式迁移机制方面的局限，特别是在**数据稀缺的新具身体系**下实现高效迁移。\n- 论文解决了**跨具身错位**（异构运动学导致源策略在目标具身体系上不可行）和**新具身体系数据稀缺**两大挑战。\n## 核心贡献\n- 提出 **MOTIF** 框架，通过将**与具身无关的动作基元**从机器人特定执行中解耦，实现了高效的小样本跨具身迁移。\n- 引入了一种统一的基元学习机制，结合了**进度感知对齐损失**和**具身对抗性损失**，并通过**流匹配策略**将抽象基元转化为精确动作。\n- 设计了轻量级的多模态**基元预测器**，能够从实时视觉和语言输入中推断动作基元，指导策略生成。\n- 在仿真和真实世界环境中进行了广泛实验，表明 **MOTIF** 在少样本迁移场景中性能显著超越强基线（仿真中提升**6.5%**，真实世界中提升**43.7%**）。\n- 提供了一种分三阶段的明确迁移机制，弥补了现有方法依赖大规模预训练进行隐式对齐的不足。\n## 方法描述\n- 核心是一个**三阶段框架**：第一阶段（**动作基元学习**）通过**向量量化自编码器**从异构机器人的短期本体感觉状态片段中学习统一的离散动作基元，并引入了**进度感知对齐损失**和**具身对抗损失**以确保时域和跨具身一致性。\n- 第二阶段（**多模态基元预测器**）使用冻结的预训练图像和语言编码器，通过一个**轻量级的Perceiver模块**从当前观测和语言指令预测动作基元。\n- 第三阶段（**基元条件化机器人策略**）利用预测的基元作为结构化先验，条件化一个基于**扩散Transformer的流匹配策略**，融合基元与具身特定的多模态输入，为目标具身体系生成具体动作序列。\n- 关键技术包括**运动轨迹规范化**、**局部注意力Transformer编码器**、**梯度反转层**以及**基元检索**与**条件化生成**。\n## 数据集与资源\n- 仿真实验使用 **ManiSkill** 基准数据集，包含 **Franka Panda**、**xArm6** 和 **WidowX AI** 三个异构机器人，每个机器人执行6个不同操作任务。\n- 真实世界实验涉及 **ARX5** 和 **Piper** 两种实体机器人。\n- **模型规模与参数量**：MOTIF 总参数量为 **0.31B**（31亿），比部分基线（如π0的3.3B）更紧凑。\n- **训练资源**：文中提到了 GPU 训练，但未明确指定具体型号和数量。\n## 评估与结果\n- **评估环境**：仿真（ManiSkill）和真实世界物理环境，采用**交错任务分配协议**来评估跨具身迁移能力。\n- **主要评估指标**：**成功率**，并区分为 **Transfer成功率**（仅在少样本目标对上的平均成功率）和 **Global成功率**（所有任务对上的平均成功率）。\n- **关键实验结果**：在仿真中，MOTIF 在1-shot、3-shot、5-shot、10-shot和50-shot设置下，Transfer成功率分别为**36.0%**、**48.33%**、**54.33%**、**60.33%**、**75.0%**，均超越所有基线（包括π0*, HPT*, GR00T N1*）。在真实世界4个任务的5-shot迁移评估中，MOTIF达到**58.33%**的Transfer成功率，大幅领先其他方法。消融实验证实了动作基元引导、进度感知对齐和具身对抗损失的有效性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>The University of Queensland</strong> (作者： Heng Tao Shen)</li><li><strong>Nanjing University</strong> (作者： Lei Zhu)</li><li><strong>Chinese University of Hong Kong (Shenzhen)</strong> (作者： Fengling Li, Guoli Yang)</li><li>从作者列表推断，可能还包括其他研究机构（未明确列出所有机构）</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>MOTIF</strong>，一种用于<strong>小样本跨具身智能体迁移</strong>的分层框架，其核心思想是从异构机器人数据中解耦出与具身无关的时空模式，称为<strong>动作基元</strong>。</li><li>旨在解决现有跨具身策略（如共享-私有架构）在有限私有参数容量和缺乏显式迁移机制方面的局限，特别是在<strong>数据稀缺的新具身体系</strong>下实现高效迁移。</li><li>论文解决了<strong>跨具身错位</strong>（异构运动学导致源策略在目标具身体系上不可行）和<strong>新具身体系数据稀缺</strong>两大挑战。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出 <strong>MOTIF</strong> 框架，通过将<strong>与具身无关的动作基元</strong>从机器人特定执行中解耦，实现了高效的小样本跨具身迁移。</li><li>引入了一种统一的基元学习机制，结合了<strong>进度感知对齐损失</strong>和<strong>具身对抗性损失</strong>，并通过<strong>流匹配策略</strong>将抽象基元转化为精确动作。</li><li>设计了轻量级的多模态<strong>基元预测器</strong>，能够从实时视觉和语言输入中推断动作基元，指导策略生成。</li><li>在仿真和真实世界环境中进行了广泛实验，表明 <strong>MOTIF</strong> 在少样本迁移场景中性能显著超越强基线（仿真中提升<strong>6.5%</strong>，真实世界中提升<strong>43.7%</strong>）。</li><li>提供了一种分三阶段的明确迁移机制，弥补了现有方法依赖大规模预训练进行隐式对齐的不足。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心是一个<strong>三阶段框架</strong>：第一阶段（<strong>动作基元学习</strong>）通过<strong>向量量化自编码器</strong>从异构机器人的短期本体感觉状态片段中学习统一的离散动作基元，并引入了<strong>进度感知对齐损失</strong>和<strong>具身对抗损失</strong>以确保时域和跨具身一致性。</li><li>第二阶段（<strong>多模态基元预测器</strong>）使用冻结的预训练图像和语言编码器，通过一个<strong>轻量级的Perceiver模块</strong>从当前观测和语言指令预测动作基元。</li><li>第三阶段（<strong>基元条件化机器人策略</strong>）利用预测的基元作为结构化先验，条件化一个基于<strong>扩散Transformer的流匹配策略</strong>，融合基元与具身特定的多模态输入，为目标具身体系生成具体动作序列。</li><li>关键技术包括<strong>运动轨迹规范化</strong>、<strong>局部注意力Transformer编码器</strong>、<strong>梯度反转层</strong>以及<strong>基元检索</strong>与<strong>条件化生成</strong>。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>仿真实验使用 <strong>ManiSkill</strong> 基准数据集，包含 <strong>Franka Panda</strong>、<strong>xArm6</strong> 和 <strong>WidowX AI</strong> 三个异构机器人，每个机器人执行6个不同操作任务。</li><li>真实世界实验涉及 <strong>ARX5</strong> 和 <strong>Piper</strong> 两种实体机器人。</li><li><strong>模型规模与参数量</strong>：MOTIF 总参数量为 <strong>0.31B</strong>（31亿），比部分基线（如π0的3.3B）更紧凑。</li><li><strong>训练资源</strong>：文中提到了 GPU 训练，但未明确指定具体型号和数量。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：仿真（ManiSkill）和真实世界物理环境，采用<strong>交错任务分配协议</strong>来评估跨具身迁移能力。</li><li><strong>主要评估指标</strong>：<strong>成功率</strong>，并区分为 <strong>Transfer成功率</strong>（仅在少样本目标对上的平均成功率）和 <strong>Global成功率</strong>（所有任务对上的平均成功率）。</li><li><strong>关键实验结果</strong>：在仿真中，MOTIF 在1-shot、3-shot、5-shot、10-shot和50-shot设置下，Transfer成功率分别为<strong>36.0%</strong>、<strong>48.33%</strong>、<strong>54.33%</strong>、<strong>60.33%</strong>、<strong>75.0%</strong>，均超越所有基线（包括π0*, HPT*, GR00T N1*）。在真实世界4个任务的5-shot迁移评估中，MOTIF达到<strong>58.33%</strong>的Transfer成功率，大幅领先其他方法。消融实验证实了动作基元引导、进度感知对齐和具身对抗损失的有效性。</li></ul>"
  },
  {
    "date": "2026-02-14",
    "title": "HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.13710",
    "summary_markdown": "## 研究单位\n- **Xin Yan** (作者1的机构未明确显示，推测为相关研究机构)\n- **Zhenglin Wan**, **Yang You** (作者2和5的机构未明确显示，推测为**新加坡国立大学**或类似机构)\n- **Xingrui Yu**, **Ivor Tsang** (作者3和6的机构未明确显示，推测为**澳大利亚悉尼科技大学**或类似机构)\n- **Hangyu Du** (作者4的机构未明确显示)\n## 论文概述\n- 提出 **HBVLA**，一种专为**视觉-语言-动作模型**设计的1比特训练后量化框架，旨在在保持策略性能的同时实现极致的模型压缩。\n- 解决VLA模型在资源受限的机器人平台部署时，现有1比特量化方法因**权重分布差距**和**动作生成敏感**导致的性能严重下降问题。\n- 核心目标是最大限度地减少量化误差，确保量化后的模型在闭环、长程执行中的**动作精度**和**策略鲁棒性**。\n## 核心贡献\n- 提出了**策略感知的权重划分**方法，通过校正的Hessian矩阵（融合了基于梯度的token重要性权重）来精准识别对动作生成至关重要的权重。\n- 提出了**基于稀疏正交变换的混合二值化**方法，针对非关键权重，先进行重排序以降低Haar变换中的高频能量，再在频域进行组感知的1比特量化。\n- 设计了一种**残差感知的精细化量化流程**，对关键权重在残差空间进行Haar变换和量化，以补偿非关键权重的近似误差。\n- 在三个基准（**LIBERO**、**SIMPLER**、**真实世界机器人**）和三个VLA模型（**OpenVLA**、**OpenVLA-OFT**、**CogACT**）上进行了广泛评估，证明了方法显著优于现有最先进的二值化PTQ方法。\n## 方法描述\n- 方法分为两步：1）**策略感知权重划分**：通过块级梯度回传计算token重要性，构建校正的Hessian来划分关键与非关键权重列。2）**显著性感知的混合二值化**：对非关键权重应用**稀疏正交变换**和**Haar小波变换**后进行分组1比特量化；对关键权重在残差上应用列向Haar变换后进行高保真量化。\n- 关键技术包括：**梯度引导的token重要性重加权Hessian**、用于最小化高频能量的**贪婪配对-链接重排序算法**、以及在Haar域进行的**组感知共享均值二值化**。\n## 数据集与资源\n- 使用的评估数据集/基准：**LIBERO**、**SIMPLER**、自建的**真实世界机器人评估套件**（包括拾放、序列指令、折叠任务）。\n- 量化的基础模型包括：**OpenVLA**、**OpenVLA-OFT**、**CogACT**。模型参数量级通常为十亿级别（具体规模论文未明确列出）。\n- 训练/实验资源：实验在**NVIDIA A800 GPUs**上进行。\n## 评估与结果\n- 评估环境：模拟环境（LIBERO, SIMPLER）和真实世界**Mobile ALOHA**双臂机器人。\n- 主要评估指标：任务**成功率**，以及与全精度模型的性能差距（Δ）。\n- 关键实验结果：\n - 在**LIBERO**上，量化后的**OpenVLA-OFT**平均成功率达**90.3%**，保留了全精度模型**92.2%** 的性能，显著优于基线方法。\n - 在**SIMPLER**上，量化后的**CogACT**在视觉匹配设定下平均成功率达**70.0%**，保留了全精度模型**93.6%** 的性能。\n - 在**真实世界**任务中，**HBVLA**相比全精度模型仅有微小性能下降，大幅优于**BiLLM**、**HBLLM**等基线。\n - 消融实验验证了**策略感知Hessian**和**列L2范数重排序**的有效性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Xin Yan</strong> (作者1的机构未明确显示，推测为相关研究机构)</li><li><strong>Zhenglin Wan</strong>, <strong>Yang You</strong> (作者2和5的机构未明确显示，推测为<strong>新加坡国立大学</strong>或类似机构)</li><li><strong>Xingrui Yu</strong>, <strong>Ivor Tsang</strong> (作者3和6的机构未明确显示，推测为<strong>澳大利亚悉尼科技大学</strong>或类似机构)</li><li><strong>Hangyu Du</strong> (作者4的机构未明确显示)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>HBVLA</strong>，一种专为<strong>视觉-语言-动作模型</strong>设计的1比特训练后量化框架，旨在在保持策略性能的同时实现极致的模型压缩。</li><li>解决VLA模型在资源受限的机器人平台部署时，现有1比特量化方法因<strong>权重分布差距</strong>和<strong>动作生成敏感</strong>导致的性能严重下降问题。</li><li>核心目标是最大限度地减少量化误差，确保量化后的模型在闭环、长程执行中的<strong>动作精度</strong>和<strong>策略鲁棒性</strong>。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了<strong>策略感知的权重划分</strong>方法，通过校正的Hessian矩阵（融合了基于梯度的token重要性权重）来精准识别对动作生成至关重要的权重。</li><li>提出了<strong>基于稀疏正交变换的混合二值化</strong>方法，针对非关键权重，先进行重排序以降低Haar变换中的高频能量，再在频域进行组感知的1比特量化。</li><li>设计了一种<strong>残差感知的精细化量化流程</strong>，对关键权重在残差空间进行Haar变换和量化，以补偿非关键权重的近似误差。</li><li>在三个基准（<strong>LIBERO</strong>、<strong>SIMPLER</strong>、<strong>真实世界机器人</strong>）和三个VLA模型（<strong>OpenVLA</strong>、<strong>OpenVLA-OFT</strong>、<strong>CogACT</strong>）上进行了广泛评估，证明了方法显著优于现有最先进的二值化PTQ方法。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>方法分为两步：1）<strong>策略感知权重划分</strong>：通过块级梯度回传计算token重要性，构建校正的Hessian来划分关键与非关键权重列。2）<strong>显著性感知的混合二值化</strong>：对非关键权重应用<strong>稀疏正交变换</strong>和<strong>Haar小波变换</strong>后进行分组1比特量化；对关键权重在残差上应用列向Haar变换后进行高保真量化。</li><li>关键技术包括：<strong>梯度引导的token重要性重加权Hessian</strong>、用于最小化高频能量的<strong>贪婪配对-链接重排序算法</strong>、以及在Haar域进行的<strong>组感知共享均值二值化</strong>。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的评估数据集/基准：<strong>LIBERO</strong>、<strong>SIMPLER</strong>、自建的<strong>真实世界机器人评估套件</strong>（包括拾放、序列指令、折叠任务）。</li><li>量化的基础模型包括：<strong>OpenVLA</strong>、<strong>OpenVLA-OFT</strong>、<strong>CogACT</strong>。模型参数量级通常为十亿级别（具体规模论文未明确列出）。</li><li>训练/实验资源：实验在<strong>NVIDIA A800 GPUs</strong>上进行。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：模拟环境（LIBERO, SIMPLER）和真实世界<strong>Mobile ALOHA</strong>双臂机器人。</li><li>主要评估指标：任务<strong>成功率</strong>，以及与全精度模型的性能差距（Δ）。</li><li>关键实验结果：</li></ul>\n<p> - 在<strong>LIBERO</strong>上，量化后的<strong>OpenVLA-OFT</strong>平均成功率达<strong>90.3%</strong>，保留了全精度模型<strong>92.2%</strong> 的性能，显著优于基线方法。</p>\n<p> - 在<strong>SIMPLER</strong>上，量化后的<strong>CogACT</strong>在视觉匹配设定下平均成功率达<strong>70.0%</strong>，保留了全精度模型<strong>93.6%</strong> 的性能。</p>\n<p> - 在<strong>真实世界</strong>任务中，<strong>HBVLA</strong>相比全精度模型仅有微小性能下降，大幅优于<strong>BiLLM</strong>、<strong>HBLLM</strong>等基线。</p>\n<p> - 消融实验验证了<strong>策略感知Hessian</strong>和<strong>列L2范数重排序</strong>的有效性。</p>"
  },
  {
    "date": "2026-02-13",
    "title": "AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge",
    "link": "http://arxiv.org/abs/2602.13476",
    "summary_markdown": "## 研究单位\n- **University of California, Berkeley**\n- **Toyota Motor North America**\n- **Princeton University**\n## 论文概述\n- 提出 **AsyncVLA**，一个用于机器人导航的异步控制框架，旨在解决大型视觉-语言-动作（VLA）模型因计算成本高导致的推理延迟问题。\n- 通过结构化解耦，将语义推理（远程大模型）与反应式执行（车载轻量模型）分离，以在高延迟（可达6秒）的动态环境中实现快速、鲁棒的导航。\n## 核心贡献\n- 提出了 **AsyncVLA** 框架，包含一个运行在远程工作站的大型基础VLA（**OmniVLA**）和一个在机器人车载控制器上运行、仅76M参数的轻量级 **Edge Adapter**。\n- 设计了一种**轨迹重加权训练策略**，通过自动识别并上采样训练数据中需要反应式行为（如避障）的片段，提升了系统对动态环境的适应能力。\n- 提出了一种**端到端的微调协议**，通过两阶段训练（先训练Edge Adapter，再联合微调）更好地对齐异步的“慢推理”和“快执行”策略流。\n- 在真实世界的视觉导航任务中进行了全面评估，证明了AsyncVLA在存在网络和推理延迟的情况下，比最先进的基线方法成功率高出**40%**。\n## 方法描述\n- 架构基于**分层控制思想**：一个慢速、强大的 **OmniVLA（>8B参数）** 作为外环提供高级语义引导，一个快速的 **Edge Adapter（76M参数）** 作为内环基于最新观察调整动作。\n- **Edge Adapter** 是一个小型ViT网络，接收VLA压缩后的动作令牌嵌入、当前图像以及延迟图像（用于感知环境变化），并输出高频动作序列。\n- 关键技术包括：使用**Token投影器**压缩VLA输出以高效传输；在训练中引入**随机延迟**和**动态交互数据上采样**，迫使模型学会处理过时信息并适应动态变化。\n## 数据集与资源\n- 使用的数据集混合了多个公开导航数据集：**GNM数据集**（包含RECON、CoryHall等）、**LeLaN数据集**（野外视频和语言导航数据）以及 **SACSoN (HuRoN)数据集**（包含行人的动态场景）。\n- 基础VLA **OmniVLA** 参数量超过**8.26B**，轻量级 **Edge Adapter** 参数量为**76M**。\n- 训练使用**5块NVIDIA H200 GPU**。推理时，大VLA部署在配备**NVIDIA RTX 4090**的工作站，Edge Adapter部署在机器人车载的**NVIDIA Jetson Orin**上。\n## 评估与结果\n- **评估环境**：在真实世界、存在静态障碍和动态行人干扰的室内外环境中，测试了**2D位姿条件导航**和**语言条件导航**任务。\n- **主要评估指标**：成功率（SR）、到达目标时间、与静态/动态障碍物的碰撞次数、语言指令遵循率。\n- **关键实验结果**：\n - **AsyncVLA** 在位姿导航任务中取得**85%**的成功率，显著优于**OmniVLA（45%）** 和**OmniVLA-edge（25%）**。\n - 即使在人为引入高达**5秒**的工作站延迟下，AsyncVLA性能下降平缓，而基线模型性能急剧恶化。\n - 端到端训练至关重要，消融实验（Ours without E2E）成功率降至25%。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>University of California, Berkeley</strong></li><li><strong>Toyota Motor North America</strong></li><li><strong>Princeton University</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>AsyncVLA</strong>，一个用于机器人导航的异步控制框架，旨在解决大型视觉-语言-动作（VLA）模型因计算成本高导致的推理延迟问题。</li><li>通过结构化解耦，将语义推理（远程大模型）与反应式执行（车载轻量模型）分离，以在高延迟（可达6秒）的动态环境中实现快速、鲁棒的导航。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>AsyncVLA</strong> 框架，包含一个运行在远程工作站的大型基础VLA（<strong>OmniVLA</strong>）和一个在机器人车载控制器上运行、仅76M参数的轻量级 <strong>Edge Adapter</strong>。</li><li>设计了一种<strong>轨迹重加权训练策略</strong>，通过自动识别并上采样训练数据中需要反应式行为（如避障）的片段，提升了系统对动态环境的适应能力。</li><li>提出了一种<strong>端到端的微调协议</strong>，通过两阶段训练（先训练Edge Adapter，再联合微调）更好地对齐异步的“慢推理”和“快执行”策略流。</li><li>在真实世界的视觉导航任务中进行了全面评估，证明了AsyncVLA在存在网络和推理延迟的情况下，比最先进的基线方法成功率高出<strong>40%</strong>。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>架构基于<strong>分层控制思想</strong>：一个慢速、强大的 <strong>OmniVLA（>8B参数）</strong> 作为外环提供高级语义引导，一个快速的 <strong>Edge Adapter（76M参数）</strong> 作为内环基于最新观察调整动作。</li><li><strong>Edge Adapter</strong> 是一个小型ViT网络，接收VLA压缩后的动作令牌嵌入、当前图像以及延迟图像（用于感知环境变化），并输出高频动作序列。</li><li>关键技术包括：使用<strong>Token投影器</strong>压缩VLA输出以高效传输；在训练中引入<strong>随机延迟</strong>和<strong>动态交互数据上采样</strong>，迫使模型学会处理过时信息并适应动态变化。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的数据集混合了多个公开导航数据集：<strong>GNM数据集</strong>（包含RECON、CoryHall等）、<strong>LeLaN数据集</strong>（野外视频和语言导航数据）以及 <strong>SACSoN (HuRoN)数据集</strong>（包含行人的动态场景）。</li><li>基础VLA <strong>OmniVLA</strong> 参数量超过<strong>8.26B</strong>，轻量级 <strong>Edge Adapter</strong> 参数量为<strong>76M</strong>。</li><li>训练使用<strong>5块NVIDIA H200 GPU</strong>。推理时，大VLA部署在配备<strong>NVIDIA RTX 4090</strong>的工作站，Edge Adapter部署在机器人车载的<strong>NVIDIA Jetson Orin</strong>上。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在真实世界、存在静态障碍和动态行人干扰的室内外环境中，测试了<strong>2D位姿条件导航</strong>和<strong>语言条件导航</strong>任务。</li><li><strong>主要评估指标</strong>：成功率（SR）、到达目标时间、与静态/动态障碍物的碰撞次数、语言指令遵循率。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>AsyncVLA</strong> 在位姿导航任务中取得<strong>85%</strong>的成功率，显著优于<strong>OmniVLA（45%）</strong> 和<strong>OmniVLA-edge（25%）</strong>。</p>\n<p> - 即使在人为引入高达<strong>5秒</strong>的工作站延迟下，AsyncVLA性能下降平缓，而基线模型性能急剧恶化。</p>\n<p> - 端到端训练至关重要，消融实验（Ours without E2E）成功率降至25%。</p>"
  },
  {
    "date": "2026-02-13",
    "title": "FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation",
    "link": "http://arxiv.org/abs/2602.13444",
    "summary_markdown": "## 研究单位\n- **Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)**\n- **Technical University of Munich (TUM)**\n- **National University of Singapore (NUS)**\n- **Westlake University**\n## 论文概述\n- 提出 **FlowHOI**，一个两阶段的条件流匹配（flow-matching）框架，用于生成基于语义的、时序连贯的手-物体交互序列。\n- 旨在解决**机器人灵巧操作**中长时序、接触密集任务的问题，通过显式生成包含手部姿态、物体姿态和接触状态的身体无关交互表示。\n- 核心研究问题：如何生成物理合理、与场景几何和语言指令语义对齐的交互序列，并提升推理效率和解决高质量监督数据稀缺问题。\n## 核心贡献\n- 提出了首个**基于流匹配的统一、条件化、语义接地的HOI生成方法**，将生成解耦为几何中心的抓取阶段和语义中心的操控阶段，实现了高达 **40倍** 的推理速度提升。\n- 通过**运动-文本对齐损失**和结合几何与语义的**混合3D场景表示**（从3D高斯泼溅重建中提取），实现了HOI生成的语义接地。\n- 设计了一个**从大规模自我中心视频中重建高保真HOI数据的流水线**，用于学习一个鲁棒的HOI先验知识，以克服数据稀缺问题。\n- 在 **GRAB** 和 **HOT3D** 基准测试中，实现了最高的动作识别准确率和 **1.7倍** 的物理模拟成功率提升，同时在真实机器人灵巧操作任务中验证了可行性。\n## 方法描述\n- 采用**两阶段条件流匹配**框架。\n - **第一阶段（抓取）**：基于预训练的抓取先验（从重建的自我中心视频数据中微调），生成接近和抓取物体的手部运动。\n - **第二阶段（操控）**：以第一阶段输出的抓取状态、语言指令和紧凑的3D场景特征为条件，生成后续的交互动作序列。\n- **关键技术/创新点**：\n - 使用**x-prediction** 变体的条件流匹配（Conditional Flow Matching）进行高效生成和提升时序稳定性。\n - 采用**混合3D场景表示**，融合几何嵌入（来自 Concerto）和语义嵌入（来自 SceneSplat），并使用感知器（Perceiver）瓶颈压缩，以实现高效的条件注入。\n - 引入了**对比对齐损失（InfoNCE-based）** 来增强生成动作与语言指令的语义一致性。\n - 设计了**子序列软修复（soft inpainting）** 和**硬约束（hard constraint）**，确保两阶段生成的平滑过渡。\n## 数据集与资源\n- **训练/评估数据集**：\n - **GRAB**（用于无场景定量评估）\n - **HOT3D**（用于带3D场景的评估）\n - **EgoDex**（重建后用于预训练抓取先验模型）\n- **模型规模与参数量**：文中未明确给出总参数量。使用的文本编码器为 **T5-Large**。\n- **训练资源**：在单块 **NVIDIA RTX 6000 Ada GPU (48GB)** 上训练，总批大小为64。\n## 评估与结果\n- **评估环境与基准**：在 GRAB 和 HOT3D 数据集上，与 **DiffH2O** 和 **LatentHOI** 等扩散基线方法进行对比。\n- **主要评估指标**：\n - **物理交互质量**：穿透体积（IV）、穿透深度（ID）、接触率（CR）、单位接触穿透体积（IVU）。\n - **动作质量**：动作识别准确率（AR）、样本多样性（SD）、整体多样性（OD）。\n - **可实现物理可行性**：物理模拟成功率（SR）、保持时间（HT）、启发式物理合理性（Phy）。\n - **效率**：每序列推理时间。\n- **关键实验结果**：\n - 在 **GRAB** 数据集上，**FlowHOI** 实现了最高的动作识别准确率（**0.95**）、最低的单位接触穿透体积（**0.13**）和最高的物理模拟成功率（**55.96%**）。\n - 推理速度显著提升，仅需 **0.16秒/序列**，比最快的扩散基线（LatentHOI: 3.57秒）快约 **22倍**（论文称最高达40倍）。\n - 在真实机器人实验中，成功将生成的HOI序列重定向到 **Allegro 灵巧手**，完成了四种灵巧操作任务（放置、堆叠、打开、倾倒）。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</strong></li><li><strong>Technical University of Munich (TUM)</strong></li><li><strong>National University of Singapore (NUS)</strong></li><li><strong>Westlake University</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>FlowHOI</strong>，一个两阶段的条件流匹配（flow-matching）框架，用于生成基于语义的、时序连贯的手-物体交互序列。</li><li>旨在解决<strong>机器人灵巧操作</strong>中长时序、接触密集任务的问题，通过显式生成包含手部姿态、物体姿态和接触状态的身体无关交互表示。</li><li>核心研究问题：如何生成物理合理、与场景几何和语言指令语义对齐的交互序列，并提升推理效率和解决高质量监督数据稀缺问题。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了首个<strong>基于流匹配的统一、条件化、语义接地的HOI生成方法</strong>，将生成解耦为几何中心的抓取阶段和语义中心的操控阶段，实现了高达 <strong>40倍</strong> 的推理速度提升。</li><li>通过<strong>运动-文本对齐损失</strong>和结合几何与语义的<strong>混合3D场景表示</strong>（从3D高斯泼溅重建中提取），实现了HOI生成的语义接地。</li><li>设计了一个<strong>从大规模自我中心视频中重建高保真HOI数据的流水线</strong>，用于学习一个鲁棒的HOI先验知识，以克服数据稀缺问题。</li><li>在 <strong>GRAB</strong> 和 <strong>HOT3D</strong> 基准测试中，实现了最高的动作识别准确率和 <strong>1.7倍</strong> 的物理模拟成功率提升，同时在真实机器人灵巧操作任务中验证了可行性。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>采用<strong>两阶段条件流匹配</strong>框架。</li></ul>\n<p> - <strong>第一阶段（抓取）</strong>：基于预训练的抓取先验（从重建的自我中心视频数据中微调），生成接近和抓取物体的手部运动。</p>\n<p> - <strong>第二阶段（操控）</strong>：以第一阶段输出的抓取状态、语言指令和紧凑的3D场景特征为条件，生成后续的交互动作序列。</p>\n<ul><li><strong>关键技术/创新点</strong>：</li></ul>\n<p> - 使用<strong>x-prediction</strong> 变体的条件流匹配（Conditional Flow Matching）进行高效生成和提升时序稳定性。</p>\n<p> - 采用<strong>混合3D场景表示</strong>，融合几何嵌入（来自 Concerto）和语义嵌入（来自 SceneSplat），并使用感知器（Perceiver）瓶颈压缩，以实现高效的条件注入。</p>\n<p> - 引入了<strong>对比对齐损失（InfoNCE-based）</strong> 来增强生成动作与语言指令的语义一致性。</p>\n<p> - 设计了<strong>子序列软修复（soft inpainting）</strong> 和<strong>硬约束（hard constraint）</strong>，确保两阶段生成的平滑过渡。</p>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>训练/评估数据集</strong>：</li></ul>\n<p> - <strong>GRAB</strong>（用于无场景定量评估）</p>\n<p> - <strong>HOT3D</strong>（用于带3D场景的评估）</p>\n<p> - <strong>EgoDex</strong>（重建后用于预训练抓取先验模型）</p>\n<ul><li><strong>模型规模与参数量</strong>：文中未明确给出总参数量。使用的文本编码器为 <strong>T5-Large</strong>。</li><li><strong>训练资源</strong>：在单块 <strong>NVIDIA RTX 6000 Ada GPU (48GB)</strong> 上训练，总批大小为64。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在 GRAB 和 HOT3D 数据集上，与 <strong>DiffH2O</strong> 和 <strong>LatentHOI</strong> 等扩散基线方法进行对比。</li><li><strong>主要评估指标</strong>：</li></ul>\n<p> - <strong>物理交互质量</strong>：穿透体积（IV）、穿透深度（ID）、接触率（CR）、单位接触穿透体积（IVU）。</p>\n<p> - <strong>动作质量</strong>：动作识别准确率（AR）、样本多样性（SD）、整体多样性（OD）。</p>\n<p> - <strong>可实现物理可行性</strong>：物理模拟成功率（SR）、保持时间（HT）、启发式物理合理性（Phy）。</p>\n<p> - <strong>效率</strong>：每序列推理时间。</p>\n<ul><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 在 <strong>GRAB</strong> 数据集上，<strong>FlowHOI</strong> 实现了最高的动作识别准确率（<strong>0.95</strong>）、最低的单位接触穿透体积（<strong>0.13</strong>）和最高的物理模拟成功率（<strong>55.96%</strong>）。</p>\n<p> - 推理速度显著提升，仅需 <strong>0.16秒/序列</strong>，比最快的扩散基线（LatentHOI: 3.57秒）快约 <strong>22倍</strong>（论文称最高达40倍）。</p>\n<p> - 在真实机器人实验中，成功将生成的HOI序列重定向到 <strong>Allegro 灵巧手</strong>，完成了四种灵巧操作任务（放置、堆叠、打开、倾倒）。</p>"
  },
  {
    "date": "2026-02-13",
    "title": "RynnBrain: Open Embodied Foundation Models",
    "link": "http://arxiv.org/abs/2602.14979",
    "summary_markdown": "## 研究单位\n- **DAMO Academy, Alibaba Group**\n## 论文概述\n- 提出了 **RynnBrain**，一个面向具身智能的开源时空基础模型，旨在统一感知、推理和规划。\n- 旨在解决现有基础模型在真实世界时空动态中缺乏物理基础和统一框架的问题。\n- 模型家族包括三个基础模型规模（2B、8B 和 30B-A3B MoE）和四个针对下游任务的后训练变体（RynnBrain-Nav, RynnBrain-Plan, RynnBrain-VLA, RynnBrain-CoP）。\n## 核心贡献\n- 提出了 **物理感知的时空预训练方法**，通过统一的时空表示和物理接地输出空间，增强了模型在真实环境中的认知和规划能力。\n- 引入了 **基于物理的链式点推理 (Chain-of-Point Reasoning)**，通过交替进行文本推理和空间定位，实现物理接地的推理过程。\n- 构建了 **大规模、高质量的多模态训练数据集**，总样本量超过 2000 万，专门设计了涵盖物体认知、空间定位、规划和具身理解的数据管道。\n- 提出了 **RynnBrain-Bench** 评测基准，用于全面评估具身模型在物体认知、空间认知、定位和指向等多维度的能力。\n- 验证了 RynnBrain 作为 **强大的预训练主干网络** 的潜力，可以高效适应多种下游具身任务（导航、操作规划、VLA）。\n## 方法描述\n- 基于 **Qwen3-VL** 架构构建，采用解码器-唯一的视觉语言模型设计，包括视觉编码器、视觉语言投影器和大型语言模型主干。\n- 核心创新包括：**统一的时空表示**（将图像和视频作为统一模态处理）、**物理接地的输出空间**（将连续空间坐标离散化为整数令牌）、**链式点推理机制**（交替生成文本和坐标以实现物理接地推理）。\n- 训练采用了 **在线负载均衡** 和 **逐样本损失** 等基础设施优化，以提高训练效率和稳定性。\n- 采用了 **监督微调** 和 **强化学习**（基于 GRPO）的两阶段训练方案，以进一步提升模型的物理推理和规划能力。\n## 数据集与资源\n- 使用了 **超过 20 个大规模公开和自构建数据集**，包括 LLaVA 系列、ShareGPT-4o-video、RefCOCO、RoboAfford++、Grasp-Anything、Open X-Embodiment 等，涵盖通用多模态理解、物体/空间认知、时空定位、物理感知规划等任务。\n- 模型规模包括 **2B、8B 的密集模型** 和一个 **30B-A3B 的 MoE 模型**。\n- 训练使用了 **ZeRO-1/ZeRO-2 优化器**、**专家并行** 等技术，并部署了高效的 **负载均衡管道**。\n## 评估与结果\n- 评估环境包括 **RynnBrain-Bench**（自建）以及 **20 个具身基准** 和 **8 个通用视觉理解基准**。\n- 主要评估指标包括：在物体认知和空间认知任务中使用 **GPT-4o 评分**；在定位任务中使用 **Fréchet 距离、双向平均欧几里得距离** 等几何度量；在导航任务中使用 **成功率和路径长度** 等标准指标。\n- 关键实验结果：**RynnBrain 基础模型在所有模型规模上都显著优于现有的具身基础模型**；后训练变体在特定任务上表现出色，例如 RynnBrain-CoP 在复杂时空任务（如轨迹预测）上性能提升约 **7%**，RynnBrain-Nav 在 **R2R** 和 **RxR** 基准上取得了最先进的结果。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>DAMO Academy, Alibaba Group</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了 <strong>RynnBrain</strong>，一个面向具身智能的开源时空基础模型，旨在统一感知、推理和规划。</li><li>旨在解决现有基础模型在真实世界时空动态中缺乏物理基础和统一框架的问题。</li><li>模型家族包括三个基础模型规模（2B、8B 和 30B-A3B MoE）和四个针对下游任务的后训练变体（RynnBrain-Nav, RynnBrain-Plan, RynnBrain-VLA, RynnBrain-CoP）。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>物理感知的时空预训练方法</strong>，通过统一的时空表示和物理接地输出空间，增强了模型在真实环境中的认知和规划能力。</li><li>引入了 <strong>基于物理的链式点推理 (Chain-of-Point Reasoning)</strong>，通过交替进行文本推理和空间定位，实现物理接地的推理过程。</li><li>构建了 <strong>大规模、高质量的多模态训练数据集</strong>，总样本量超过 2000 万，专门设计了涵盖物体认知、空间定位、规划和具身理解的数据管道。</li><li>提出了 <strong>RynnBrain-Bench</strong> 评测基准，用于全面评估具身模型在物体认知、空间认知、定位和指向等多维度的能力。</li><li>验证了 RynnBrain 作为 <strong>强大的预训练主干网络</strong> 的潜力，可以高效适应多种下游具身任务（导航、操作规划、VLA）。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>基于 <strong>Qwen3-VL</strong> 架构构建，采用解码器-唯一的视觉语言模型设计，包括视觉编码器、视觉语言投影器和大型语言模型主干。</li><li>核心创新包括：<strong>统一的时空表示</strong>（将图像和视频作为统一模态处理）、<strong>物理接地的输出空间</strong>（将连续空间坐标离散化为整数令牌）、<strong>链式点推理机制</strong>（交替生成文本和坐标以实现物理接地推理）。</li><li>训练采用了 <strong>在线负载均衡</strong> 和 <strong>逐样本损失</strong> 等基础设施优化，以提高训练效率和稳定性。</li><li>采用了 <strong>监督微调</strong> 和 <strong>强化学习</strong>（基于 GRPO）的两阶段训练方案，以进一步提升模型的物理推理和规划能力。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用了 <strong>超过 20 个大规模公开和自构建数据集</strong>，包括 LLaVA 系列、ShareGPT-4o-video、RefCOCO、RoboAfford++、Grasp-Anything、Open X-Embodiment 等，涵盖通用多模态理解、物体/空间认知、时空定位、物理感知规划等任务。</li><li>模型规模包括 <strong>2B、8B 的密集模型</strong> 和一个 <strong>30B-A3B 的 MoE 模型</strong>。</li><li>训练使用了 <strong>ZeRO-1/ZeRO-2 优化器</strong>、<strong>专家并行</strong> 等技术，并部署了高效的 <strong>负载均衡管道</strong>。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境包括 <strong>RynnBrain-Bench</strong>（自建）以及 <strong>20 个具身基准</strong> 和 <strong>8 个通用视觉理解基准</strong>。</li><li>主要评估指标包括：在物体认知和空间认知任务中使用 <strong>GPT-4o 评分</strong>；在定位任务中使用 <strong>Fréchet 距离、双向平均欧几里得距离</strong> 等几何度量；在导航任务中使用 <strong>成功率和路径长度</strong> 等标准指标。</li><li>关键实验结果：<strong>RynnBrain 基础模型在所有模型规模上都显著优于现有的具身基础模型</strong>；后训练变体在特定任务上表现出色，例如 RynnBrain-CoP 在复杂时空任务（如轨迹预测）上性能提升约 <strong>7%</strong>，RynnBrain-Nav 在 <strong>R2R</strong> 和 <strong>RxR</strong> 基准上取得了最先进的结果。</li></ul>"
  },
  {
    "date": "2026-02-13",
    "title": "Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control",
    "link": "http://arxiv.org/abs/2602.13193",
    "summary_markdown": "## 研究单位\n- **加州大学伯克利分校**1 (UC Berkeley)\n- **DeepMind**3\n- 其他未明确标注的机构（作者单位标记为1, 2, 3）\n## 论文概述\n- 提出了一种**Steerable Policies（可操纵策略）**，这是一种能够响应多种抽象级别指令的视觉-语言-动作（VLA）模型，旨在提升机器人策略的“可操纵性”。\n- 通过改进低层VLA的可操纵性，可以更好地释放预训练**视觉-语言模型（VLM）** 的知识和推理能力，从而提高机器人任务的泛化性能。\n- 解决了如何有效将预训练基础模型的语义和感知知识**落地到机器人行为中**的核心挑战，克服了传统分层方法中高层VLM与低层VLA之间**指令接口（通常仅为任务级自然语言）过于单一、表达能力不足**的瓶颈。\n## 核心贡献\n- 提出了 **Steerable Policies**：一种能够遵循**多样化、多抽象级别的合成指令**（如子任务、原子动作、像素坐标、抓取器轨迹等）的VLA模型。\n- 提出了一种**规模化生成合成转向指令的自动化流水线**，利用基础模型从机器人轨迹数据中自动提取语义和几何特征，并生成丰富的指令标签。\n- 设计了两种新颖的分层控制方法，用于**更有效地利用VLM来操纵Steerable Policies**：1）将VLM微调为高层具体化推理器；2）利用现成VLM的上下文学习能力来选择最优的转向抽象指令。\n- 在真实世界机器人操作实验（**Bridge WidowX** 数据集）中广泛验证，表明所提方法显著优于现有的具体化推理VLA和基于VLM的分层基线模型，尤其在**泛化任务和长视野任务**上表现出色。\n## 方法描述\n- 核心技术：训练一个VLA（视觉-语言-动作）模型，使其能够在**行为克隆（BC）训练中接受六种不同抽象级别的“转向指令”** 作为条件输入，而非单一的任务标签。\n- 转向指令包括：任务级指令、子任务级指令、原子动作指令、抓取器轨迹、空间点坐标指令、以及上述类型的组合。\n- 关键创新点：**自动化的合成指令生成流水线**。利用一系列基础模型（如Molmo、SAM2、DETR、Gemini 2.0）从机器人轨迹中提取物体、运动和几何特征，然后使用**Gemini** 模型为每一段轨迹子任务生成对应所有指令风格的描述。\n- 分层控制方法：\n - **高层具体化推理器**：将VLM微调，使其根据任务和当前观察，生成思维链推理并输出转向指令。\n - **上下文学习VLM**：直接提示现成的强大VLM（如Gemini 3.0），结合历史观察和指令的上下文，进行场景理解、任务分解，并**动态选择**最合适的指令抽象级别来指导底层策略。\n## 数据集与资源\n- 使用的主要数据集：**Bridge WidowX** 真实世界机器人操作数据集。\n- 数据规模：通过合成指令生成流水线，将原始的38k个标准任务标签，扩展至约**206k个子任务**和总计**近2M条转向指令**。\n- 基础模型：采用 **Prismatic VLM 7B** 架构作为策略主干，该模型基于 **OpenVLA** 代码库构建和训练。\n- 用于特征提取和指令生成的模型：**Molmo**（用于物体识别）、**SAM2**（用于视频分割）、**DETR**（用于抓取器轨迹提取）、**Gemini 2.0/3.0**（用于生成子任务分解、转向指令和推理依据）。\n## 评估与结果\n- 评估环境：真实世界的**Bridge WidowX**机器人操作设置。\n- 评估基准：遵循前人工作，测试任务涵盖**分布内、动作泛化、空间泛化和语义泛化**等多个维度。对于上下文学习实验，额外增加了未见过的**多步骤长视野任务**。\n- 主要评估指标：**任务成功率**。\n- 关键实验结果：\n - **人类作为高层策略**时，使用所有指令风格几乎能实现**100%成功率**，而限制于单一指令风格或仅使用任务级指令时，性能显著下降，证明了多风格指令的有效性和必要性。\n - **微调的高层具体化推理器**在Bridge任务集上，**显著优于**标准的OpenVLA、ECoT-Lite以及完整的具体化思维链推理（ECoT）等基线方法。\n - **基于上下文学习的VLM**方法在复杂多步任务上**明显优于**仅使用子任务指令的类SayCan基线、标准Bridge OpenVLA以及其自身的非推理消融版本。\n - 定性分析显示，可操纵策略使得高层VLM能够利用其**场景理解和上下文学习**能力进行纠错，并动态地为不同情况选择最合适的指令抽象级别（例如，在场景杂乱时选择指向指令，在需要精确动作时选择原子动作指令）。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>加州大学伯克利分校</strong>1 (UC Berkeley)</li><li><strong>DeepMind</strong>3</li><li>其他未明确标注的机构（作者单位标记为1, 2, 3）</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种<strong>Steerable Policies（可操纵策略）</strong>，这是一种能够响应多种抽象级别指令的视觉-语言-动作（VLA）模型，旨在提升机器人策略的“可操纵性”。</li><li>通过改进低层VLA的可操纵性，可以更好地释放预训练<strong>视觉-语言模型（VLM）</strong> 的知识和推理能力，从而提高机器人任务的泛化性能。</li><li>解决了如何有效将预训练基础模型的语义和感知知识<strong>落地到机器人行为中</strong>的核心挑战，克服了传统分层方法中高层VLM与低层VLA之间<strong>指令接口（通常仅为任务级自然语言）过于单一、表达能力不足</strong>的瓶颈。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>Steerable Policies</strong>：一种能够遵循<strong>多样化、多抽象级别的合成指令</strong>（如子任务、原子动作、像素坐标、抓取器轨迹等）的VLA模型。</li><li>提出了一种<strong>规模化生成合成转向指令的自动化流水线</strong>，利用基础模型从机器人轨迹数据中自动提取语义和几何特征，并生成丰富的指令标签。</li><li>设计了两种新颖的分层控制方法，用于<strong>更有效地利用VLM来操纵Steerable Policies</strong>：1）将VLM微调为高层具体化推理器；2）利用现成VLM的上下文学习能力来选择最优的转向抽象指令。</li><li>在真实世界机器人操作实验（<strong>Bridge WidowX</strong> 数据集）中广泛验证，表明所提方法显著优于现有的具体化推理VLA和基于VLM的分层基线模型，尤其在<strong>泛化任务和长视野任务</strong>上表现出色。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心技术：训练一个VLA（视觉-语言-动作）模型，使其能够在<strong>行为克隆（BC）训练中接受六种不同抽象级别的“转向指令”</strong> 作为条件输入，而非单一的任务标签。</li><li>转向指令包括：任务级指令、子任务级指令、原子动作指令、抓取器轨迹、空间点坐标指令、以及上述类型的组合。</li><li>关键创新点：<strong>自动化的合成指令生成流水线</strong>。利用一系列基础模型（如Molmo、SAM2、DETR、Gemini 2.0）从机器人轨迹中提取物体、运动和几何特征，然后使用<strong>Gemini</strong> 模型为每一段轨迹子任务生成对应所有指令风格的描述。</li><li>分层控制方法：</li></ul>\n<p> - <strong>高层具体化推理器</strong>：将VLM微调，使其根据任务和当前观察，生成思维链推理并输出转向指令。</p>\n<p> - <strong>上下文学习VLM</strong>：直接提示现成的强大VLM（如Gemini 3.0），结合历史观察和指令的上下文，进行场景理解、任务分解，并<strong>动态选择</strong>最合适的指令抽象级别来指导底层策略。</p>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的主要数据集：<strong>Bridge WidowX</strong> 真实世界机器人操作数据集。</li><li>数据规模：通过合成指令生成流水线，将原始的38k个标准任务标签，扩展至约<strong>206k个子任务</strong>和总计<strong>近2M条转向指令</strong>。</li><li>基础模型：采用 <strong>Prismatic VLM 7B</strong> 架构作为策略主干，该模型基于 <strong>OpenVLA</strong> 代码库构建和训练。</li><li>用于特征提取和指令生成的模型：<strong>Molmo</strong>（用于物体识别）、<strong>SAM2</strong>（用于视频分割）、<strong>DETR</strong>（用于抓取器轨迹提取）、<strong>Gemini 2.0/3.0</strong>（用于生成子任务分解、转向指令和推理依据）。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：真实世界的<strong>Bridge WidowX</strong>机器人操作设置。</li><li>评估基准：遵循前人工作，测试任务涵盖<strong>分布内、动作泛化、空间泛化和语义泛化</strong>等多个维度。对于上下文学习实验，额外增加了未见过的<strong>多步骤长视野任务</strong>。</li><li>主要评估指标：<strong>任务成功率</strong>。</li><li>关键实验结果：</li></ul>\n<p> - <strong>人类作为高层策略</strong>时，使用所有指令风格几乎能实现<strong>100%成功率</strong>，而限制于单一指令风格或仅使用任务级指令时，性能显著下降，证明了多风格指令的有效性和必要性。</p>\n<p> - <strong>微调的高层具体化推理器</strong>在Bridge任务集上，<strong>显著优于</strong>标准的OpenVLA、ECoT-Lite以及完整的具体化思维链推理（ECoT）等基线方法。</p>\n<p> - <strong>基于上下文学习的VLM</strong>方法在复杂多步任务上<strong>明显优于</strong>仅使用子任务指令的类SayCan基线、标准Bridge OpenVLA以及其自身的非推理消融版本。</p>\n<p> - 定性分析显示，可操纵策略使得高层VLM能够利用其<strong>场景理解和上下文学习</strong>能力进行纠错，并动态地为不同情况选择最合适的指令抽象级别（例如，在场景杂乱时选择指向指令，在需要精确动作时选择原子动作指令）。</p>"
  },
  {
    "date": "2026-02-13",
    "title": "UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph",
    "link": "http://arxiv.org/abs/2602.13086",
    "summary_markdown": "## 研究单位\n- **Nanyang Technological University**, School of Electrical and Electronic Engineering, Singapore\n- **Tsinghua University**, Department of Automation, China\n## 论文概述\n- 提出 **UniManip** 框架，旨在实现**零样本、通用目的**的机器人操作，能够在未见过的物体、任务和机器人平台上泛化，而无需任务特定的微调或重新配置。\n- 核心是引入**双层智能体操作图（Bi-level Agentic Operational Graph, AOG）**，它耦合了用于高层任务编排的智能体逻辑层和用于动态状态表示的场景语义操作层，实现了语义推理与物理执行的统一。\n- 解决了当前端到端视觉-语言-动作模型精度不足，以及传统分层规划器在面对开放世界变化时语义僵化的问题。\n## 核心贡献\n- 提出 **UniManip** 通用操作框架，通过其动态、智能化的闭环实现了跨任务、物体和机器人平台的鲁棒零样本泛化。\n- 引入**双层智能体操作图**，作为一种智能基础设施，通过整合语义记忆和情景记忆，支持复杂的推理、动态任务分解，并与物理环境保持同步。\n- 建立了一个**鲁棒的任务到运动桥梁**，结合单视角保守重建和安全感知的规划器，生成了优化、无碰撞且高效的末端执行器轨迹。\n- 实验证明 **UniManip** 在零样本操作成功率上分别超越当前最先进的**VLA**和**分层基线方法**达**22.5%** 和**25.0%**。\n- 展示了框架的灵活性，能够在不进行微调的情况下，**零样本迁移**从固定基座设置到移动操作。\n## 方法描述\n- 核心是**双层智能体操作图（AOG）**，包含上层的**智能体逻辑图**（负责任务编排、验证和恢复）和下层的**语义操作状态图**（作为动态、基础的世界模型，包含物体的几何、语义和物理属性）。\n- 系统运作遵循**感知-规划-执行-反思**的闭环流程：1）**多模态场景实例化**，从输入中构建对象中心的SOSG；2）**智能体基元执行**，通过安全感知的局部规划器将抽象动作参数化为无碰撞轨迹；3）**闭环反思与恢复**，利用AOG的结构化记忆自主诊断异常并动态重组逻辑图以实现恢复。\n- 关键技术包括：**基于图的场景解析和任务分解**、**通过智能体操作基元进行动作抽象**（如目标导向和操作导向运动）、**安全感知的运动合成**（包括保守占用网格重建和ESDF轨迹规划）以及**基于图形和记忆的智能体验证与自主恢复机制**。\n## 数据集与资源\n- 论文未明确提及使用特定的公开训练数据集。\n- 模型层面，核心利用了**视觉语言模型**进行高层任务规划和语义验证，使用了如**Grounding DINO**和**SAM 2**等开放词汇感知模型进行场景解析。\n- 训练资源未在提供内容中明确说明。\n## 评估与结果\n- **评估环境**：在桌面场景、杂乱场景、长视野操作任务以及办公室环境中的**移动操作**上进行了广泛的零样本实验。\n- **主要评估指标**：任务**成功率**。\n- **关键实验结果**：\n - UniManip 在零样本操作上的成功率比当前最先进的 **VLA 模型**高出 **22.5%**。\n - 比当前最先进的 **分层规划方法** 高出 **25.0%**。\n - 在包含复杂交互和多个步骤的**长视野任务**上表现出色。\n - 成功实现了从固定基座到**移动操作平台**的零样本直接迁移，无需任何调整。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Nanyang Technological University</strong>, School of Electrical and Electronic Engineering, Singapore</li><li><strong>Tsinghua University</strong>, Department of Automation, China</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>UniManip</strong> 框架，旨在实现<strong>零样本、通用目的</strong>的机器人操作，能够在未见过的物体、任务和机器人平台上泛化，而无需任务特定的微调或重新配置。</li><li>核心是引入<strong>双层智能体操作图（Bi-level Agentic Operational Graph, AOG）</strong>，它耦合了用于高层任务编排的智能体逻辑层和用于动态状态表示的场景语义操作层，实现了语义推理与物理执行的统一。</li><li>解决了当前端到端视觉-语言-动作模型精度不足，以及传统分层规划器在面对开放世界变化时语义僵化的问题。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出 <strong>UniManip</strong> 通用操作框架，通过其动态、智能化的闭环实现了跨任务、物体和机器人平台的鲁棒零样本泛化。</li><li>引入<strong>双层智能体操作图</strong>，作为一种智能基础设施，通过整合语义记忆和情景记忆，支持复杂的推理、动态任务分解，并与物理环境保持同步。</li><li>建立了一个<strong>鲁棒的任务到运动桥梁</strong>，结合单视角保守重建和安全感知的规划器，生成了优化、无碰撞且高效的末端执行器轨迹。</li><li>实验证明 <strong>UniManip</strong> 在零样本操作成功率上分别超越当前最先进的<strong>VLA</strong>和<strong>分层基线方法</strong>达<strong>22.5%</strong> 和<strong>25.0%</strong>。</li><li>展示了框架的灵活性，能够在不进行微调的情况下，<strong>零样本迁移</strong>从固定基座设置到移动操作。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心是<strong>双层智能体操作图（AOG）</strong>，包含上层的<strong>智能体逻辑图</strong>（负责任务编排、验证和恢复）和下层的<strong>语义操作状态图</strong>（作为动态、基础的世界模型，包含物体的几何、语义和物理属性）。</li><li>系统运作遵循<strong>感知-规划-执行-反思</strong>的闭环流程：1）<strong>多模态场景实例化</strong>，从输入中构建对象中心的SOSG；2）<strong>智能体基元执行</strong>，通过安全感知的局部规划器将抽象动作参数化为无碰撞轨迹；3）<strong>闭环反思与恢复</strong>，利用AOG的结构化记忆自主诊断异常并动态重组逻辑图以实现恢复。</li><li>关键技术包括：<strong>基于图的场景解析和任务分解</strong>、<strong>通过智能体操作基元进行动作抽象</strong>（如目标导向和操作导向运动）、<strong>安全感知的运动合成</strong>（包括保守占用网格重建和ESDF轨迹规划）以及<strong>基于图形和记忆的智能体验证与自主恢复机制</strong>。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>论文未明确提及使用特定的公开训练数据集。</li><li>模型层面，核心利用了<strong>视觉语言模型</strong>进行高层任务规划和语义验证，使用了如<strong>Grounding DINO</strong>和<strong>SAM 2</strong>等开放词汇感知模型进行场景解析。</li><li>训练资源未在提供内容中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在桌面场景、杂乱场景、长视野操作任务以及办公室环境中的<strong>移动操作</strong>上进行了广泛的零样本实验。</li><li><strong>主要评估指标</strong>：任务<strong>成功率</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - UniManip 在零样本操作上的成功率比当前最先进的 <strong>VLA 模型</strong>高出 <strong>22.5%</strong>。</p>\n<p> - 比当前最先进的 <strong>分层规划方法</strong> 高出 <strong>25.0%</strong>。</p>\n<p> - 在包含复杂交互和多个步骤的<strong>长视野任务</strong>上表现出色。</p>\n<p> - 成功实现了从固定基座到<strong>移动操作平台</strong>的零样本直接迁移，无需任何调整。</p>"
  },
  {
    "date": "2026-02-13",
    "title": "Learning Native Continuation for Action Chunking Flow Policies",
    "link": "http://arxiv.org/abs/2602.12978",
    "summary_markdown": "## 研究单位\n- **香港中文大学** (The Chinese University of Hong Kong)\n- **Spirit AI**\n- **南京大学** (Nanjing University)\n## 论文概述\n- 论文提出了一种名为 **Legato** 的训练时延续方法，用于改进基于流匹配（flow-based）的视觉语言动作（VLA）策略在实时动作分块执行中的轨迹平滑性。\n- 旨在解决在实时分块执行中，由于推断延迟和策略固有的多模态特性导致的块间轨迹不连续和虚假多模态切换问题，从而提高任务效率和轨迹平滑性。\n## 核心贡献\n- 提出了 **Legato**，一个在训练时通过学习重塑流的动力学来实现逐步、基于计划引导的延续框架，确保了训练与推断的一致性。\n- 引入了随机化计划条件，以支持变化的推断延迟并提供对轨迹平滑性的灵活控制。\n- 通过五个真实机器人操作任务的广泛实验表明，Legato 在轨迹平滑性和任务完成时间上始终优于现有的 **RTC** 和 **Training-time RTC** 方法。\n## 方法描述\n- 核心方法基于流匹配（Flow Matching）。Legato 通过学习重塑的速度场，将分块间的连续性内化为策略的固有属性。\n- 关键技术包括：1）定义一个指导计划（guidance schedule），用于指定每个时间步应如何遵循已知（参考）动作；2）通过动作-噪声混合和推导出的连续动力学，确保在每一步去噪中都应用指导；3）重塑速度场以实现严格的训练-推断一致性。\n- 创新点在于通过重塑的流动力学，实现了“逐步指导”和“训练-推断一致性”，使得连续性成为策略的固有（native）特性，而非像 RTC 那样仅在推断时施加的外部约束。\n## 数据集与资源\n- 未在摘要及主体部分明确提及训练所用的具体公开数据集名称。实验任务为真实世界机器人操作任务（如叠碗、倒水、抓取放置等）。\n- 使用的模型是基于 **VLA（Vision-Language-Action）** 的流策略。论文提到了 **π0** 等流模型家族。模型的规模与参数量未在提供的章节中明确给出。\n- 训练资源未在提供的章节中明确说明。\n## 评估与结果\n- 评估环境：**真实世界双手机器人**平台，执行五个不同的操作任务（stack bowls, pour, pick-and-place, open drawer, fold towel）。\n- 主要评估指标：**任务完成分数**、**任务完成时间（秒）**、以及平滑性指标（**NLDLJ**, **NSPARC**, **Overlap RMSE**）。\n- 关键实验结果：如表 I 所示，在所有五个任务上，**Legato** 相比于 **RTC** 基线，任务完成时间平均减少了约 10-20%，并且平滑性指标（更低的 NSPARC, NLDLJ 和 Overlap RMSE）均有显著提升，表明轨迹更平滑、动作更连贯。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>香港中文大学</strong> (The Chinese University of Hong Kong)</li><li><strong>Spirit AI</strong></li><li><strong>南京大学</strong> (Nanjing University)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出了一种名为 <strong>Legato</strong> 的训练时延续方法，用于改进基于流匹配（flow-based）的视觉语言动作（VLA）策略在实时动作分块执行中的轨迹平滑性。</li><li>旨在解决在实时分块执行中，由于推断延迟和策略固有的多模态特性导致的块间轨迹不连续和虚假多模态切换问题，从而提高任务效率和轨迹平滑性。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>Legato</strong>，一个在训练时通过学习重塑流的动力学来实现逐步、基于计划引导的延续框架，确保了训练与推断的一致性。</li><li>引入了随机化计划条件，以支持变化的推断延迟并提供对轨迹平滑性的灵活控制。</li><li>通过五个真实机器人操作任务的广泛实验表明，Legato 在轨迹平滑性和任务完成时间上始终优于现有的 <strong>RTC</strong> 和 <strong>Training-time RTC</strong> 方法。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心方法基于流匹配（Flow Matching）。Legato 通过学习重塑的速度场，将分块间的连续性内化为策略的固有属性。</li><li>关键技术包括：1）定义一个指导计划（guidance schedule），用于指定每个时间步应如何遵循已知（参考）动作；2）通过动作-噪声混合和推导出的连续动力学，确保在每一步去噪中都应用指导；3）重塑速度场以实现严格的训练-推断一致性。</li><li>创新点在于通过重塑的流动力学，实现了“逐步指导”和“训练-推断一致性”，使得连续性成为策略的固有（native）特性，而非像 RTC 那样仅在推断时施加的外部约束。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>未在摘要及主体部分明确提及训练所用的具体公开数据集名称。实验任务为真实世界机器人操作任务（如叠碗、倒水、抓取放置等）。</li><li>使用的模型是基于 <strong>VLA（Vision-Language-Action）</strong> 的流策略。论文提到了 <strong>π0</strong> 等流模型家族。模型的规模与参数量未在提供的章节中明确给出。</li><li>训练资源未在提供的章节中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：<strong>真实世界双手机器人</strong>平台，执行五个不同的操作任务（stack bowls, pour, pick-and-place, open drawer, fold towel）。</li><li>主要评估指标：<strong>任务完成分数</strong>、<strong>任务完成时间（秒）</strong>、以及平滑性指标（<strong>NLDLJ</strong>, <strong>NSPARC</strong>, <strong>Overlap RMSE</strong>）。</li><li>关键实验结果：如表 I 所示，在所有五个任务上，<strong>Legato</strong> 相比于 <strong>RTC</strong> 基线，任务完成时间平均减少了约 10-20%，并且平滑性指标（更低的 NSPARC, NLDLJ 和 Overlap RMSE）均有显著提升，表明轨迹更平滑、动作更连贯。</li></ul>"
  },
  {
    "date": "2026-02-13",
    "title": "ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training",
    "link": "http://arxiv.org/abs/2602.12691",
    "summary_markdown": "## 研究单位\n- **AgiBot**\n- **The Hong Kong University of Science and Technology (香港科技大学)**\n- **Fudan University (复旦大学)**\n- **Independent Researcher**\n## 论文概述\n- 论文提出一种用于 **视觉-语言-动作（Vision-Language-Action, VLA）** 模型后训练的离策略（Off-Policy）评估框架 **ALOE**，旨在通过在线强化学习改进大型基础模型在真实世界的表现。\n- 核心要解决的问题是：在 **真实世界、人机协作（Human-in-the-Loop）** 的数据收集场景下，如何稳定且高效地进行策略评估。传统方法为避免不稳定而采用保守的 **同策略（On-Policy）** 估计，但这限制了从混合历史数据中学习的能力和学习效率。\n## 核心贡献\n- 提出一种面向真实世界 VLA 训练的 **动作级别离策略评估框架**，支持可靠的动作级信用分配（Credit Assignment）和策略改进。\n- 为真实世界设置开发了 **动作-价值函数学习的稳定机制**，包括 **分块时序差分（Q-chunking TD）** 更新和 **悲观价值集成（Pessimistic Ensemble）** ，实现了对流程（Flow-based）VLA 策略的可靠长时程信用分配。\n- 在三个真实世界机器人操作任务上进行了全面评估，证明了方法在 **任务成功率、泛化能力、学习效率和执行错误恢复** 方面优于之前的 VLA 训练方法。\n- 展示了通过引入离策略强化学习，可以在不牺牲稳定性的前提下，**重新引入传统的时序差分自举（Temporal-Difference Bootstrapping）方法** 来评估当前策略的行为质量。\n## 方法描述\n- 采用 **执行器-评判器（Actor-Critic）** 框架：执行器是基于 **流匹配（Flow-Matching）** 的 VLA 模型（**π₀.₅**），评判器是轻量级的集成 Q 网络。\n- **离策略评判器估计**：使用 **分块时序差分（Q-chunking TD）** 自举来评估动作序列（块），而非预测最终任务结果，这能在稀疏奖励下改善对关键动作块的信用分配，并支持稳定策略改进。\n- **基于优势加权的策略改进**：使用集成的悲观 Q 值估计优势函数，然后通过优势加权的最大似然目标来更新 VLA 执行器策略，实现约束策略优化。\n- **关键技术点**包括：使用 **悲观集成 Q 函数** 缓解不确定性区域的高估问题；**分块（Chunking）** 机制加速长时程任务的信用传播；以及 **优势加权回归（Advantage-Weighted Regression）** 风格的策略更新以确保稳定性。\n## 数据集与资源\n- **数据集**：主要使用在三个真实世界任务上在线收集的交互数据，包括来自历史策略和人机干预的混合轨迹片段。\n- **执行器模型**：基于 **π₀.₅** 流匹配 VLA 模型进行端到端微调。\n- **评判器模型**：使用 **π₀.₅** 预训练的 **SigLIP 视觉编码器** 提取图像特征，并融合其他模态，使用 Transformer 骨干网络。\n- **训练资源**：论文提及在真实机器人平台上进行实验，但未明确说明训练使用的 GPU/TPU 具体型号和数量。\n## 评估与结果\n- **评估环境**：在三个具有挑战性的真实世界机器人操作任务上进行评估：**手机包装（高精度任务）、叠衣服（长时程可变形物体任务）、产品分拣（多物体感知的双臂拾取放置任务）**。\n- **主要评估指标**：**任务成功率、执行吞吐量（单位时间完成任务数）、对未见物体的零样本泛化能力、对干扰的鲁棒性**。\n- **关键实验结果**：\n - **ALOE 在三个任务上均取得最高的成功率**，超越了 **行为克隆（BC）**、**DAgger** 和 **优势加权回归（AWR）** 等基线方法。\n - **ALOE 展示了更高的学习效率**（在更少的环境交互下达到更高性能），以及对流程 VLA 模型持续多轮迭代改进的能力。\n - 学习到的 Q 函数能对轨迹中的关键动作和失败模式进行 **细粒度的、动作级别的信用分配**。\n - 在 **手机包装** 任务中，ALOE 实现了更高的任务执行吞吐量。\n - 在 **产品分拣** 任务中，ALOE 对未见物体展现出更好的零样本泛化能力。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>AgiBot</strong></li><li><strong>The Hong Kong University of Science and Technology (香港科技大学)</strong></li><li><strong>Fudan University (复旦大学)</strong></li><li><strong>Independent Researcher</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出一种用于 <strong>视觉-语言-动作（Vision-Language-Action, VLA）</strong> 模型后训练的离策略（Off-Policy）评估框架 <strong>ALOE</strong>，旨在通过在线强化学习改进大型基础模型在真实世界的表现。</li><li>核心要解决的问题是：在 <strong>真实世界、人机协作（Human-in-the-Loop）</strong> 的数据收集场景下，如何稳定且高效地进行策略评估。传统方法为避免不稳定而采用保守的 <strong>同策略（On-Policy）</strong> 估计，但这限制了从混合历史数据中学习的能力和学习效率。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出一种面向真实世界 VLA 训练的 <strong>动作级别离策略评估框架</strong>，支持可靠的动作级信用分配（Credit Assignment）和策略改进。</li><li>为真实世界设置开发了 <strong>动作-价值函数学习的稳定机制</strong>，包括 <strong>分块时序差分（Q-chunking TD）</strong> 更新和 <strong>悲观价值集成（Pessimistic Ensemble）</strong> ，实现了对流程（Flow-based）VLA 策略的可靠长时程信用分配。</li><li>在三个真实世界机器人操作任务上进行了全面评估，证明了方法在 <strong>任务成功率、泛化能力、学习效率和执行错误恢复</strong> 方面优于之前的 VLA 训练方法。</li><li>展示了通过引入离策略强化学习，可以在不牺牲稳定性的前提下，<strong>重新引入传统的时序差分自举（Temporal-Difference Bootstrapping）方法</strong> 来评估当前策略的行为质量。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>采用 <strong>执行器-评判器（Actor-Critic）</strong> 框架：执行器是基于 <strong>流匹配（Flow-Matching）</strong> 的 VLA 模型（<strong>π₀.₅</strong>），评判器是轻量级的集成 Q 网络。</li><li><strong>离策略评判器估计</strong>：使用 <strong>分块时序差分（Q-chunking TD）</strong> 自举来评估动作序列（块），而非预测最终任务结果，这能在稀疏奖励下改善对关键动作块的信用分配，并支持稳定策略改进。</li><li><strong>基于优势加权的策略改进</strong>：使用集成的悲观 Q 值估计优势函数，然后通过优势加权的最大似然目标来更新 VLA 执行器策略，实现约束策略优化。</li><li><strong>关键技术点</strong>包括：使用 <strong>悲观集成 Q 函数</strong> 缓解不确定性区域的高估问题；<strong>分块（Chunking）</strong> 机制加速长时程任务的信用传播；以及 <strong>优势加权回归（Advantage-Weighted Regression）</strong> 风格的策略更新以确保稳定性。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>数据集</strong>：主要使用在三个真实世界任务上在线收集的交互数据，包括来自历史策略和人机干预的混合轨迹片段。</li><li><strong>执行器模型</strong>：基于 <strong>π₀.₅</strong> 流匹配 VLA 模型进行端到端微调。</li><li><strong>评判器模型</strong>：使用 <strong>π₀.₅</strong> 预训练的 <strong>SigLIP 视觉编码器</strong> 提取图像特征，并融合其他模态，使用 Transformer 骨干网络。</li><li><strong>训练资源</strong>：论文提及在真实机器人平台上进行实验，但未明确说明训练使用的 GPU/TPU 具体型号和数量。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在三个具有挑战性的真实世界机器人操作任务上进行评估：<strong>手机包装（高精度任务）、叠衣服（长时程可变形物体任务）、产品分拣（多物体感知的双臂拾取放置任务）</strong>。</li><li><strong>主要评估指标</strong>：<strong>任务成功率、执行吞吐量（单位时间完成任务数）、对未见物体的零样本泛化能力、对干扰的鲁棒性</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>ALOE 在三个任务上均取得最高的成功率</strong>，超越了 <strong>行为克隆（BC）</strong>、<strong>DAgger</strong> 和 <strong>优势加权回归（AWR）</strong> 等基线方法。</p>\n<p> - <strong>ALOE 展示了更高的学习效率</strong>（在更少的环境交互下达到更高性能），以及对流程 VLA 模型持续多轮迭代改进的能力。</p>\n<p> - 学习到的 Q 函数能对轨迹中的关键动作和失败模式进行 <strong>细粒度的、动作级别的信用分配</strong>。</p>\n<p> - 在 <strong>手机包装</strong> 任务中，ALOE 实现了更高的任务执行吞吐量。</p>\n<p> - 在 <strong>产品分拣</strong> 任务中，ALOE 对未见物体展现出更好的零样本泛化能力。</p>"
  },
  {
    "date": "2026-02-13",
    "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
    "link": "http://arxiv.org/abs/2602.12684",
    "summary_markdown": "## 研究单位\n- **小米机器人** (Xiaomi Robotics)\n## 论文概述\n- 介绍了 **Xiaomi-Robotics-0**，一个先进的开源视觉-语言-动作（VLA）模型，旨在实现高性能和流畅的实时执行。\n- 论文致力于解决VLA模型因参数量大而导致的高推理延迟问题，该问题会阻碍在真实机器人上连续、平滑地执行动作。\n## 核心贡献\n- 提出并开源了 **Xiaomi-Robotics-0** 模型，该模型包含47亿参数，集成了 **Qwen3-VL-4B-Instruct** VLM和用于流匹配动作生成的**扩散变换器**。\n- 设计了一个两阶段的训练方案：在大规模跨具身机器人轨迹和视觉语言数据上进行**预训练**，以及对特定机器人进行**后训练**以适应实时执行。\n- 针对后训练中的**异步执行**，提出了创新技术（如 **Λ-shape注意力掩码**），以缓解动作前缀条件导致模型反应性降低的问题，确保平滑连续的动作块切换。\n- 在多个基准测试和真实机器人任务上实现了**最先进的性能**，并在消费级GPU上实现了低延迟（80ms）和高吞吐量的实时执行。\n- 开源了代码、模型检查点和训练数据，以促进未来研究。\n## 方法描述\n- 模型架构为**混合变换器**，由处理视觉和语言输入的预训练VLM（Qwen3-VL-4B-Instruct）和基于流匹配生成动作块的扩散变换器组成。\n- **预训练**包含两步：第一步，通过“Choice Policies”范式同时训练VLM处理视觉语言数据和预测动作；第二步，冻结VLM，从头训练扩散变换器。\n- **后训练**专注于适应特定机器人。为支持异步执行，引入了训练时RTC，并提出使用**Λ-shape注意力掩码**并增加位置编码偏移，以强制模型在生成后续动作时更多地关注视觉和语言信号，而非过度依赖动作前缀。\n- **部署**时，通过仔细对齐连续推理生成的动作块的时间步，确保无缝的实时执行。\n## 数据集与资源\n- 使用了大规模的**跨具身机器人轨迹数据**（约2亿时间步），包括开源数据集（如DROID, MolmoAct）和内部收集的数据（乐高拆卸338小时，毛巾折叠400小时）。\n- 使用了超过8000万样本的**视觉语言数据**，涵盖视觉定位、VQA、图像描述和具身推理等任务。\n- 模型总参数量为 **4.7B**。\n- 训练使用了 **DeepSpeed ZeRO-2** 和 **AdamW** 优化器。\n- 推理在单张 **NVIDIA GeForce RTX 4090 GPU** 上进行。\n## 评估与结果\n- **模拟基准测试**：在**LIBERO**、**CALVIN** 和 **SimplerEnv** 三个基准上均达到SOTA。\n - **LIBERO**：平均成功率 **98.7%**。\n - **CALVIN** (ABCD→D)：平均连续完成任务数 **4.80**；(ABC→D)：**4.75**。\n - **SimplerEnv** (Google Robot)：视觉匹配成功率 **85.5%**，变体聚合成功率 **74.7%**；(WidowX)：成功率 **79.2%**。\n- **真实机器人实验**：在**乐高拆卸**和**毛巾折叠**两项需要精确双手操作的任务上进行评估。\n - **乐高拆卸**：模型在成功率上与基线方法相当，但**吞吐量最高**。\n - **毛巾折叠**：模型达到最高吞吐量 **1.2 条/分钟**，优于基线 **π0.5** 和训练RTC变体。\n- **视觉语言能力保留**：模型在多个通用VLM基准和具身推理基准（**ERQA**）上的表现与基础VLM相当甚至略有超越，证明了其在机器人训练中有效保留了视觉语义知识。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>小米机器人</strong> (Xiaomi Robotics)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>介绍了 <strong>Xiaomi-Robotics-0</strong>，一个先进的开源视觉-语言-动作（VLA）模型，旨在实现高性能和流畅的实时执行。</li><li>论文致力于解决VLA模型因参数量大而导致的高推理延迟问题，该问题会阻碍在真实机器人上连续、平滑地执行动作。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出并开源了 <strong>Xiaomi-Robotics-0</strong> 模型，该模型包含47亿参数，集成了 <strong>Qwen3-VL-4B-Instruct</strong> VLM和用于流匹配动作生成的<strong>扩散变换器</strong>。</li><li>设计了一个两阶段的训练方案：在大规模跨具身机器人轨迹和视觉语言数据上进行<strong>预训练</strong>，以及对特定机器人进行<strong>后训练</strong>以适应实时执行。</li><li>针对后训练中的<strong>异步执行</strong>，提出了创新技术（如 <strong>Λ-shape注意力掩码</strong>），以缓解动作前缀条件导致模型反应性降低的问题，确保平滑连续的动作块切换。</li><li>在多个基准测试和真实机器人任务上实现了<strong>最先进的性能</strong>，并在消费级GPU上实现了低延迟（80ms）和高吞吐量的实时执行。</li><li>开源了代码、模型检查点和训练数据，以促进未来研究。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>模型架构为<strong>混合变换器</strong>，由处理视觉和语言输入的预训练VLM（Qwen3-VL-4B-Instruct）和基于流匹配生成动作块的扩散变换器组成。</li><li><strong>预训练</strong>包含两步：第一步，通过“Choice Policies”范式同时训练VLM处理视觉语言数据和预测动作；第二步，冻结VLM，从头训练扩散变换器。</li><li><strong>后训练</strong>专注于适应特定机器人。为支持异步执行，引入了训练时RTC，并提出使用<strong>Λ-shape注意力掩码</strong>并增加位置编码偏移，以强制模型在生成后续动作时更多地关注视觉和语言信号，而非过度依赖动作前缀。</li><li><strong>部署</strong>时，通过仔细对齐连续推理生成的动作块的时间步，确保无缝的实时执行。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用了大规模的<strong>跨具身机器人轨迹数据</strong>（约2亿时间步），包括开源数据集（如DROID, MolmoAct）和内部收集的数据（乐高拆卸338小时，毛巾折叠400小时）。</li><li>使用了超过8000万样本的<strong>视觉语言数据</strong>，涵盖视觉定位、VQA、图像描述和具身推理等任务。</li><li>模型总参数量为 <strong>4.7B</strong>。</li><li>训练使用了 <strong>DeepSpeed ZeRO-2</strong> 和 <strong>AdamW</strong> 优化器。</li><li>推理在单张 <strong>NVIDIA GeForce RTX 4090 GPU</strong> 上进行。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>模拟基准测试</strong>：在<strong>LIBERO</strong>、<strong>CALVIN</strong> 和 <strong>SimplerEnv</strong> 三个基准上均达到SOTA。</li></ul>\n<p> - <strong>LIBERO</strong>：平均成功率 <strong>98.7%</strong>。</p>\n<p> - <strong>CALVIN</strong> (ABCD→D)：平均连续完成任务数 <strong>4.80</strong>；(ABC→D)：<strong>4.75</strong>。</p>\n<p> - <strong>SimplerEnv</strong> (Google Robot)：视觉匹配成功率 <strong>85.5%</strong>，变体聚合成功率 <strong>74.7%</strong>；(WidowX)：成功率 <strong>79.2%</strong>。</p>\n<ul><li><strong>真实机器人实验</strong>：在<strong>乐高拆卸</strong>和<strong>毛巾折叠</strong>两项需要精确双手操作的任务上进行评估。</li></ul>\n<p> - <strong>乐高拆卸</strong>：模型在成功率上与基线方法相当，但<strong>吞吐量最高</strong>。</p>\n<p> - <strong>毛巾折叠</strong>：模型达到最高吞吐量 <strong>1.2 条/分钟</strong>，优于基线 <strong>π0.5</strong> 和训练RTC变体。</p>\n<ul><li><strong>视觉语言能力保留</strong>：模型在多个通用VLM基准和具身推理基准（<strong>ERQA</strong>）上的表现与基础VLM相当甚至略有超越，证明了其在机器人训练中有效保留了视觉语义知识。</li></ul>"
  },
  {
    "date": "2026-02-13",
    "title": "RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
    "link": "http://arxiv.org/abs/2602.12628",
    "summary_markdown": "## 研究单位\n- **Tsinghua University**\n- **Harbin Institute of Technology**\n- **Peking University**\n- **Carnegie Mellon University**\n- **Shanghai AI Laboratory**\n- **Zhongguancun Academy**\n## 论文概述\n- 提出了 **RL-based Sim-Real Co-Training (RL-Co)** 框架，用于增强**视觉-语言-动作 (VLA)** 模型，以克服传统监督式微调的局限性。\n- 旨在解决利用模拟数据增强VLA模型，同时保持其在真实世界中能力的关键问题，以降低对昂贵真实机器人演示数据的依赖。\n- 核心思想是超越静态模仿学习，利用模拟环境的交互式优势，并通过真实数据正则化防止灾难性遗忘。\n## 核心贡献\n- 提出了一个通用的两阶段**RL-Co**框架，结合了监督式微调和基于强化学习的交互式学习。\n- 在第二阶段引入了**真实世界正则化**的RL损失，在利用模拟数据优化策略的同时，锚定其真实世界能力。\n- 在四个真实世界桌面操作任务和两个代表性VLA模型 (**OpenVLA** 和 **π0.5**) 上验证了方法的有效性，显著提升了任务成功率。\n- 证明了所提方法能带来更强的**泛化能力**和更高的**数据效率**。\n- 开源了代码和模型，为后续研究提供了实用的基准和工具。\n## 方法描述\n- 方法分为两个阶段：**阶段I：SFT协同训练初始化**，在混合的真实和模拟演示数据上进行监督式微调，为策略提供良好的初始化和真实世界知识。\n- **阶段II：带有真实正则化的模拟-真实协同训练**，在模拟环境中进行强化学习微调（使用PPO等算法），同时加入真实世界数据集的SFT损失作为正则化项，平衡探索与遗忘。\n- 关键技术在于将**大规模交互式模拟训练**与**小规模真实世界监督**相结合，超越了纯监督协同训练方法仅将模拟数据视为静态数据源的局限。\n## 数据集与资源\n- 使用的真实数据集：通过远程操作收集的四个桌面操作任务（如**Pick and Place**）的演示，每个任务包含20-50条轨迹。\n- 使用的模拟数据集：使用**MimicGen** 工具在 **ManiSkill** 模拟器中为每个任务生成的约1000条轨迹。\n- 模型规模：使用了预训练的 **OpenVLA** (基于Transformer) 和 **π0.5** (基于流匹配) 模型，具体参数量未在摘要中明确给出。\n- 训练资源：RL训练使用了 **RLinf** 训练框架以提高效率。具体的GPU/TPU配置未在摘要中明确给出。\n## 评估与结果\n- 评估环境：四个真实世界桌面操作任务 (**Pick and Place, Push Cube, Open Drawer, Close Drawer**) 及其对应的**ManiSkill**数字孪生模拟环境。\n- 主要评估指标：真实世界任务**成功率 (SR)**。\n- 关键实验结果：\n - 在**OpenVLA**上，RL-Co相比纯真实数据微调，平均成功率提升了 **+47.5%**；相比SFT协同训练，提升了 **+24%**。\n - 在 **π0.5** 上，RL-Co相比纯真实数据微调，平均成功率提升了 **+39.5%**；相比SFT协同训练，提升了 **+20.3%**。\n - 在泛化测试中（面对未见过的物体和初始状态），RL-Co模型性能下降幅度远小于基线方法，显示出更强的鲁棒性。\n - 数据效率实验表明，RL-Co仅用20条真实演示即可达到基线方法使用200条演示的性能。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Tsinghua University</strong></li><li><strong>Harbin Institute of Technology</strong></li><li><strong>Peking University</strong></li><li><strong>Carnegie Mellon University</strong></li><li><strong>Shanghai AI Laboratory</strong></li><li><strong>Zhongguancun Academy</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了 <strong>RL-based Sim-Real Co-Training (RL-Co)</strong> 框架，用于增强<strong>视觉-语言-动作 (VLA)</strong> 模型，以克服传统监督式微调的局限性。</li><li>旨在解决利用模拟数据增强VLA模型，同时保持其在真实世界中能力的关键问题，以降低对昂贵真实机器人演示数据的依赖。</li><li>核心思想是超越静态模仿学习，利用模拟环境的交互式优势，并通过真实数据正则化防止灾难性遗忘。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了一个通用的两阶段<strong>RL-Co</strong>框架，结合了监督式微调和基于强化学习的交互式学习。</li><li>在第二阶段引入了<strong>真实世界正则化</strong>的RL损失，在利用模拟数据优化策略的同时，锚定其真实世界能力。</li><li>在四个真实世界桌面操作任务和两个代表性VLA模型 (<strong>OpenVLA</strong> 和 <strong>π0.5</strong>) 上验证了方法的有效性，显著提升了任务成功率。</li><li>证明了所提方法能带来更强的<strong>泛化能力</strong>和更高的<strong>数据效率</strong>。</li><li>开源了代码和模型，为后续研究提供了实用的基准和工具。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>方法分为两个阶段：<strong>阶段I：SFT协同训练初始化</strong>，在混合的真实和模拟演示数据上进行监督式微调，为策略提供良好的初始化和真实世界知识。</li><li><strong>阶段II：带有真实正则化的模拟-真实协同训练</strong>，在模拟环境中进行强化学习微调（使用PPO等算法），同时加入真实世界数据集的SFT损失作为正则化项，平衡探索与遗忘。</li><li>关键技术在于将<strong>大规模交互式模拟训练</strong>与<strong>小规模真实世界监督</strong>相结合，超越了纯监督协同训练方法仅将模拟数据视为静态数据源的局限。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的真实数据集：通过远程操作收集的四个桌面操作任务（如<strong>Pick and Place</strong>）的演示，每个任务包含20-50条轨迹。</li><li>使用的模拟数据集：使用<strong>MimicGen</strong> 工具在 <strong>ManiSkill</strong> 模拟器中为每个任务生成的约1000条轨迹。</li><li>模型规模：使用了预训练的 <strong>OpenVLA</strong> (基于Transformer) 和 <strong>π0.5</strong> (基于流匹配) 模型，具体参数量未在摘要中明确给出。</li><li>训练资源：RL训练使用了 <strong>RLinf</strong> 训练框架以提高效率。具体的GPU/TPU配置未在摘要中明确给出。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：四个真实世界桌面操作任务 (<strong>Pick and Place, Push Cube, Open Drawer, Close Drawer</strong>) 及其对应的<strong>ManiSkill</strong>数字孪生模拟环境。</li><li>主要评估指标：真实世界任务<strong>成功率 (SR)</strong>。</li><li>关键实验结果：</li></ul>\n<p> - 在<strong>OpenVLA</strong>上，RL-Co相比纯真实数据微调，平均成功率提升了 <strong>+47.5%</strong>；相比SFT协同训练，提升了 <strong>+24%</strong>。</p>\n<p> - 在 <strong>π0.5</strong> 上，RL-Co相比纯真实数据微调，平均成功率提升了 <strong>+39.5%</strong>；相比SFT协同训练，提升了 <strong>+20.3%</strong>。</p>\n<p> - 在泛化测试中（面对未见过的物体和初始状态），RL-Co模型性能下降幅度远小于基线方法，显示出更强的鲁棒性。</p>\n<p> - 数据效率实验表明，RL-Co仅用20条真实演示即可达到基线方法使用200条演示的性能。</p>"
  },
  {
    "date": "2026-02-13",
    "title": "CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning",
    "link": "http://arxiv.org/abs/2602.12532",
    "summary_markdown": "<nav>\n<form action=\"/search\" class=\"ltx_page_search\" method=\"get\">\n<label for=\"ltx_page_search\"><span class=\"ltx_text ltx_font_bold\">Find articles</span></label>\n<input id=\"ltx_page_search\" name=\"query\" placeholder=\"search 2.6 million+ articles\" type=\"text\" value=\"\"/>\n<button type=\"submit\">Search</button>\n</form>\n</nav>\n</footer>\n</div>\n</body>\n</html>",
    "summary_html": "<p><nav></p>\n<p><form action=\"/search\" class=\"ltx_page_search\" method=\"get\"></p>\n<p><label for=\"ltx_page_search\"><span class=\"ltx_text ltx_font_bold\">Find articles</span></label></p>\n<p><input id=\"ltx_page_search\" name=\"query\" placeholder=\"search 2.6 million+ articles\" type=\"text\" value=\"\"/></p>\n<p><button type=\"submit\">Search</button></p>\n<p></form></p>\n<p></nav></p>\n<p></footer></p>\n<p></div></p>\n<p></body></p>\n<p></html></p>"
  },
  {
    "date": "2026-02-12",
    "title": "LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation",
    "link": "http://arxiv.org/abs/2602.12351",
    "summary_markdown": "## 研究单位\n- **University of Michigan, Ann Arbor**\n## 论文概述\n- 论文提出了 **LongNav-R1**，这是一个用于优化 **视觉-语言-动作 (VLA)** 模型进行长视界导航的端到端多回合强化学习框架。\n- 旨在解决现有单回合模仿学习方法在长视界导航任务中存在的两个关键缺陷：缺乏因果推理能力和行为僵化，导致智能体无法从错误中恢复或适应环境变化。\n## 核心贡献\n- 提出了 **LongNav-R1**，一个端到端的多回合强化学习框架，将导航决策过程重新定义为 **VLA策略** 与具身环境之间的持续对话。\n- 引入了 **Horizon-Adaptive Policy Optimization (HAPO)**，一个无需评论家（critic-free）的优势估计框架，通过显式考虑视界长度变化，实现了长序列上的精确时序信用分配。\n- 通过在实际场景和多个模拟导航基准（**HM3D V1/V2**, **MP3D**, **OVON**）上的综合实验验证了 **LongNav-R1** 的有效性，显著超越了现有最先进方法。\n- 证明了该框架具有优异的样本效率和泛化能力，例如仅用 **34，000** 条轨迹将 **Qwen3-VL-2B** 模型的成功率从 **0.51%** 提升至 **73.0%**。\n## 方法描述\n- **核心方法**：将导航重新构建为多回合强化学习过程，采用**多回合交互式状态更新**和**在线令牌剪枝**（基于历史KV缓存相似性过滤冗余视觉令牌）以提高长序列计算效率。\n- **创新点**：1) **多回合RL框架**，支持因果推理和多样化轨迹生成。2) **HAPO** 通过**核回归**从本地数据中回归出一个时间感知基线，提供了无需训练评论家网络的精确优势估计，解决了长视界任务的时序信用分配挑战。\n- **关键技术**：1) 设计了一个**时间感知的高斯核函数**，其输入特征为时间步长，使基线能够捕捉奖励的时序动态。2) 将现有方法（如 REINFORCE++）统一为该框架下核选择的特例。\n## 数据集与资源\n- **使用数据集**：**HM3D V1**、**V2**、**MP3D** 和 **HM3D-OVON**（用于开放词汇导航）。\n- **模型规模**：基于 **Qwen-3-VL-2B** 模型作为 VLA 策略骨干。\n- **训练资源**：论文提到采用了在线交互训练，但未在提供的HTML片段中明确指定具体的GPU/TPU硬件资源。\n## 评估与结果\n- **评估环境**：真实世界测试和四个模拟基准（HM3D V1/V2， MP3D， OVON）。\n- **主要评估指标**：**成功率 (SR)** 和 **加权路径长度成功率 (SPL)**。\n- **关键实验结果**：\n - 在 **HM3D V1** 上，**LongNav-R1** 取得 **76.0% SR** 和 **44.3% SPL**，优于先前的 SOTA（如 PIRLNav-RL: 70.4% SR, 34.1% SPL）。\n - 在 **HM3D V2** 上，取得 **83.7% SR** 和 **43.4% SPL**。\n - 在 **MP3D** 上，取得 **63.0% SR** 和 **30.2% SPL**。\n - 零样本泛化到 **HM3D-OVON** 基准，性能也超越了许多在OVON上微调的先前方法。\n - 消融研究表明：多回合RL贡献了 **7.3%** 的性能提升，而 **HAPO** 对于解锁长视界导航至关重要（将成功率从 **0%** 提升至 **15.4%**）。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>University of Michigan, Ann Arbor</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出了 <strong>LongNav-R1</strong>，这是一个用于优化 <strong>视觉-语言-动作 (VLA)</strong> 模型进行长视界导航的端到端多回合强化学习框架。</li><li>旨在解决现有单回合模仿学习方法在长视界导航任务中存在的两个关键缺陷：缺乏因果推理能力和行为僵化，导致智能体无法从错误中恢复或适应环境变化。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>LongNav-R1</strong>，一个端到端的多回合强化学习框架，将导航决策过程重新定义为 <strong>VLA策略</strong> 与具身环境之间的持续对话。</li><li>引入了 <strong>Horizon-Adaptive Policy Optimization (HAPO)</strong>，一个无需评论家（critic-free）的优势估计框架，通过显式考虑视界长度变化，实现了长序列上的精确时序信用分配。</li><li>通过在实际场景和多个模拟导航基准（<strong>HM3D V1/V2</strong>, <strong>MP3D</strong>, <strong>OVON</strong>）上的综合实验验证了 <strong>LongNav-R1</strong> 的有效性，显著超越了现有最先进方法。</li><li>证明了该框架具有优异的样本效率和泛化能力，例如仅用 <strong>34，000</strong> 条轨迹将 <strong>Qwen3-VL-2B</strong> 模型的成功率从 <strong>0.51%</strong> 提升至 <strong>73.0%</strong>。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>核心方法</strong>：将导航重新构建为多回合强化学习过程，采用<strong>多回合交互式状态更新</strong>和<strong>在线令牌剪枝</strong>（基于历史KV缓存相似性过滤冗余视觉令牌）以提高长序列计算效率。</li><li><strong>创新点</strong>：1) <strong>多回合RL框架</strong>，支持因果推理和多样化轨迹生成。2) <strong>HAPO</strong> 通过<strong>核回归</strong>从本地数据中回归出一个时间感知基线，提供了无需训练评论家网络的精确优势估计，解决了长视界任务的时序信用分配挑战。</li><li><strong>关键技术</strong>：1) 设计了一个<strong>时间感知的高斯核函数</strong>，其输入特征为时间步长，使基线能够捕捉奖励的时序动态。2) 将现有方法（如 REINFORCE++）统一为该框架下核选择的特例。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>使用数据集</strong>：<strong>HM3D V1</strong>、<strong>V2</strong>、<strong>MP3D</strong> 和 <strong>HM3D-OVON</strong>（用于开放词汇导航）。</li><li><strong>模型规模</strong>：基于 <strong>Qwen-3-VL-2B</strong> 模型作为 VLA 策略骨干。</li><li><strong>训练资源</strong>：论文提到采用了在线交互训练，但未在提供的HTML片段中明确指定具体的GPU/TPU硬件资源。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：真实世界测试和四个模拟基准（HM3D V1/V2， MP3D， OVON）。</li><li><strong>主要评估指标</strong>：<strong>成功率 (SR)</strong> 和 <strong>加权路径长度成功率 (SPL)</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 在 <strong>HM3D V1</strong> 上，<strong>LongNav-R1</strong> 取得 <strong>76.0% SR</strong> 和 <strong>44.3% SPL</strong>，优于先前的 SOTA（如 PIRLNav-RL: 70.4% SR, 34.1% SPL）。</p>\n<p> - 在 <strong>HM3D V2</strong> 上，取得 <strong>83.7% SR</strong> 和 <strong>43.4% SPL</strong>。</p>\n<p> - 在 <strong>MP3D</strong> 上，取得 <strong>63.0% SR</strong> 和 <strong>30.2% SPL</strong>。</p>\n<p> - 零样本泛化到 <strong>HM3D-OVON</strong> 基准，性能也超越了许多在OVON上微调的先前方法。</p>\n<p> - 消融研究表明：多回合RL贡献了 <strong>7.3%</strong> 的性能提升，而 <strong>HAPO</strong> 对于解锁长视界导航至关重要（将成功率从 <strong>0%</strong> 提升至 <strong>15.4%</strong>）。</p>"
  },
  {
    "date": "2026-02-12",
    "title": "ForeAct: Steering Your VLA with Efficient Visual Foresight Planning",
    "link": "http://arxiv.org/abs/2602.12322",
    "summary_markdown": "/v1jM3AAAAAElFTkSuQmCC\"/></a>\n</div>\n</footer>\n</div>\n</body>\n</html>",
    "summary_html": "<p>/v1jM3AAAAAElFTkSuQmCC\"/></a></p>\n<p></div></p>\n<p></footer></p>\n<p></div></p>\n<p></body></p>\n<p></html></p>"
  },
  {
    "date": "2026-02-12",
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "link": "http://arxiv.org/abs/2602.12281",
    "summary_markdown": "## 研究单位\n- **Stanford University**\n- **NVIDIA Research**\n## 论文概述\n- 研究如何通过**测试时缩放**来缩小**视觉-语言-动作**模型的**“意图-动作差距”**。\n- 提出了**CoVer**对比验证器，以及**CoVer-VLA**分层测试时验证流程，通过离线推理和在线选择来优化指令和动作的对齐。\n## 核心贡献\n- 表征了**具身指令跟随的测试时缩放定律**，发现联合缩放指令改写数量和生成动作数量比单独缩放更高效。\n- 提出了**CoVer**——一种用于视觉-语言-动作对齐的**对比验证器**，其架构能优雅地随计算资源和数据扩展。\n- 引入了**引导时计算**进行离线推理，以及一个**分层测试时验证流程**，将高层指令优化与低层动作块选择相结合。\n- 实验证明，与扩展策略预训练相比，**验证方法在多个基准测试中显著提升了性能**，包括 SIMPLER（提升 22% 内分布 / 13% 外分布）、PolaRiS（任务进度提升 14%，成功率提升 9%）和真实世界实验（提升 45%）。\n## 方法描述\n- 核心是**分层测试时验证框架 CoVer-VLA**，包含**引导时指令改写与缓存**和**运行时批量动作验证**。\n- **离线训练 CoVer 验证器**：采用对比学习架构，使用文本感知的视觉编码器和动作编码器，在大量离线样本上训练，评估语义对齐。\n- **关键技术**：利用**引导时计算**让 VLM 对场景进行离线结构化推理并生成多样化指令改写；运行时，VLA 为每个改写指令生成多个动作候选，CoVer 对所有指令-动作对进行评分，并选择最优的高层指令和低层动作块执行。\n## 数据集与资源\n- 主要使用**Bridge V2**和**Open-X Embodiment**数据集进行训练和评估。\n- CoVer 验证器模型规模为**1B 参数**。\n- 使用**8 块 NVIDIA H200 GPU**进行训练。最终部署时使用一个由3个验证器组成的集成模型。\n## 评估与结果\n- **评估基准**：**SIMPLER**（内分布与 OOD 任务）、**PolaRiS** 和真实世界任务（使用 WidowX 机器人）。\n- **评估指标**：任务成功率、任务进度百分比、动作错误率。\n- **关键结果**：\n - 在 SIMPLER 上，CoVer-VLA 相比仅在相同数据上扩展策略预训练，实现了 22% 的内分布提升和 13% 的 OOD 提升。\n - 在 PolaRiS 上，结合 **π0.5** 基模型，任务进度平均提升 13.9%，成功率提升 9.3%。\n - 在真实世界任务中，相比基线模型，实现了**45%的绝对性能提升**。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Stanford University</strong></li><li><strong>NVIDIA Research</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>研究如何通过<strong>测试时缩放</strong>来缩小<strong>视觉-语言-动作</strong>模型的<strong>“意图-动作差距”</strong>。</li><li>提出了<strong>CoVer</strong>对比验证器，以及<strong>CoVer-VLA</strong>分层测试时验证流程，通过离线推理和在线选择来优化指令和动作的对齐。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>表征了<strong>具身指令跟随的测试时缩放定律</strong>，发现联合缩放指令改写数量和生成动作数量比单独缩放更高效。</li><li>提出了<strong>CoVer</strong>——一种用于视觉-语言-动作对齐的<strong>对比验证器</strong>，其架构能优雅地随计算资源和数据扩展。</li><li>引入了<strong>引导时计算</strong>进行离线推理，以及一个<strong>分层测试时验证流程</strong>，将高层指令优化与低层动作块选择相结合。</li><li>实验证明，与扩展策略预训练相比，<strong>验证方法在多个基准测试中显著提升了性能</strong>，包括 SIMPLER（提升 22% 内分布 / 13% 外分布）、PolaRiS（任务进度提升 14%，成功率提升 9%）和真实世界实验（提升 45%）。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心是<strong>分层测试时验证框架 CoVer-VLA</strong>，包含<strong>引导时指令改写与缓存</strong>和<strong>运行时批量动作验证</strong>。</li><li><strong>离线训练 CoVer 验证器</strong>：采用对比学习架构，使用文本感知的视觉编码器和动作编码器，在大量离线样本上训练，评估语义对齐。</li><li><strong>关键技术</strong>：利用<strong>引导时计算</strong>让 VLM 对场景进行离线结构化推理并生成多样化指令改写；运行时，VLA 为每个改写指令生成多个动作候选，CoVer 对所有指令-动作对进行评分，并选择最优的高层指令和低层动作块执行。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>主要使用<strong>Bridge V2</strong>和<strong>Open-X Embodiment</strong>数据集进行训练和评估。</li><li>CoVer 验证器模型规模为<strong>1B 参数</strong>。</li><li>使用<strong>8 块 NVIDIA H200 GPU</strong>进行训练。最终部署时使用一个由3个验证器组成的集成模型。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估基准</strong>：<strong>SIMPLER</strong>（内分布与 OOD 任务）、<strong>PolaRiS</strong> 和真实世界任务（使用 WidowX 机器人）。</li><li><strong>评估指标</strong>：任务成功率、任务进度百分比、动作错误率。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - 在 SIMPLER 上，CoVer-VLA 相比仅在相同数据上扩展策略预训练，实现了 22% 的内分布提升和 13% 的 OOD 提升。</p>\n<p> - 在 PolaRiS 上，结合 <strong>π0.5</strong> 基模型，任务进度平均提升 13.9%，成功率提升 9.3%。</p>\n<p> - 在真实世界任务中，相比基线模型，实现了<strong>45%的绝对性能提升</strong>。</p>"
  },
  {
    "date": "2026-02-12",
    "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
    "link": "http://arxiv.org/abs/2602.12099",
    "summary_markdown": "基于提供的论文HTML原文，以下是根据要求总结的论文要点：\n### 论文研究单位\nGigaAI\n### 论文概述\n这篇论文提出了一个名为GigaBrain-0.5M*的视觉-语言-动作模型。该模型通过世界模型驱动的强化学习进行训练。GigaBrain-0.5M*是在GigaBrain-0.5基础上扩展而来。GigaBrain-0.5本身已经在超过10，000小时的机器人操作数据上进行了预训练，并已在国际RoboChallenge基准测试中取得了第一名的成绩。为了解决传统VLA模型在场景理解和未来预测能力方面的局限性，GigaBrain-0.5M*引入了RAMP框架，利用预测的未来状态和值估计来增强策略的规划和长期执行能力。实验表明，该方法在多个复杂任务上显著优于基准方法。\n### 论文核心贡献点\n1. 提出了GigaBrain-0.5M*，一个通过世界模型驱动的强化学习进行训练的VLA模型。\n2. 提出了RAMP框架，这是一种通过世界模型条件策略进行的强化学习方法，将预测的未来状态和值作为策略的条件输入。\n3. 在理论上证明了现有的RECAP方法是RAMP在忽略未来状态信息时的一个特例，表明RAMP在信息增益和动作生成条件熵降低方面更具优势。\n4. 展示了GigaBrain-0.5在内部评估和RoboChallenge公共基准测试上的优异性能，验证了其基础能力。\n5. 通过实验证明，RAMP在复杂长视野操作任务上显著优于其他强化学习方法，在多个任务上获得了约30%的性能提升。\n### 论文方法描述\n1. **GigaBrain-0.5模型**：该模型是一个端到端的VLA架构，基于混合Transformer和预训练的PaliGemma-2视觉语言模型，并采用流匹配的扩散Transformer来预测动作块。它通过生成具体化的思维链来增强推理能力，包括语言子目标、离散动作令牌和2D操作轨迹。\n2. **RAMP框架**：这是一个四阶段的迭代训练框架：\n * **阶段一：世界模型预训练**：使用大规模机器人操作数据训练一个世界模型，使其能够联合预测未来状态和值估计。\n * **阶段二：带有世界模型条件的策略训练**：将世界模型预测的未来状态和值转换为二元优势指示信号，并作为条件输入来微调GigaBrain-0.5策略。\n * **阶段三：人在回路中滚动数据收集**：部署策略以自主执行和专家干预相结合的方式收集数据，用于生成高质量的训练轨迹。\n * **阶段四：利用滚动数据进行持续训练**：使用收集的数据进一步联合微调世界模型和策略，形成一个自我改进的闭环。\n3. **理论连接**：论文从概率角度阐述了RAMP与RECAP的关系，证明了RECAP可以被视为RAMP在忽略未来潜在状态信息时的特例。RAMP通过显式条件化世界模型的预测，提供了更丰富的信息增益。\n### 论文使用数据集和训练资源\n1. **数据集**：\n * GigaBrain-0.5的预训练数据集总计超过10，000小时，包括超过6，000小时的世界模型生成数据和约4，000小时的真实机器人采集数据。\n * 评估使用了八个内部设计的任务和RoboChallenge基准测试中的30个公开任务。\n * RAMP中世界模型训练使用了4，000小时的真实机器人操作数据。\n2. **训练资源**：\n * 模型训练使用了GigaTrain框架，批次大小为3，072，优化步数为100，000步。\n * 使用了完全分片数据并行（FSDP）v2来减少单GPU内存消耗。\n * 世界模型使用了Wan2.2作为骨干架构进行训练。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n * **内部评估**：在物理机器人平台上进行，包括PiPER机械臂和G1人形机器人。\n * **公开基准评估**：在RoboChallenge平台上进行，该平台包含20台实体机器人，涵盖UR5、Franka、ARX5和ALOHA四种主要平台。\n2. **评估指标**：\n * **任务成功率**：执行指定任务的成功百分比，是主要评估指标。\n * **价值预测评估指标**：用于评估世界模型价值预测的准确性，包括平均绝对误差（MAE）、均方误差（MSE）、均方根误差（RMSE）和肯德尔秩相关系数。\n * **推理时间**：衡量价值预测方法的计算效率。",
    "summary_html": "<p>基于提供的论文HTML原文，以下是根据要求总结的论文要点：</p>\n<h3>论文研究单位</h3>\n<p>GigaAI</p>\n<h3>论文概述</h3>\n<p>这篇论文提出了一个名为GigaBrain-0.5M*的视觉-语言-动作模型。该模型通过世界模型驱动的强化学习进行训练。GigaBrain-0.5M*是在GigaBrain-0.5基础上扩展而来。GigaBrain-0.5本身已经在超过10，000小时的机器人操作数据上进行了预训练，并已在国际RoboChallenge基准测试中取得了第一名的成绩。为了解决传统VLA模型在场景理解和未来预测能力方面的局限性，GigaBrain-0.5M*引入了RAMP框架，利用预测的未来状态和值估计来增强策略的规划和长期执行能力。实验表明，该方法在多个复杂任务上显著优于基准方法。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了GigaBrain-0.5M*，一个通过世界模型驱动的强化学习进行训练的VLA模型。</li><li>提出了RAMP框架，这是一种通过世界模型条件策略进行的强化学习方法，将预测的未来状态和值作为策略的条件输入。</li><li>在理论上证明了现有的RECAP方法是RAMP在忽略未来状态信息时的一个特例，表明RAMP在信息增益和动作生成条件熵降低方面更具优势。</li><li>展示了GigaBrain-0.5在内部评估和RoboChallenge公共基准测试上的优异性能，验证了其基础能力。</li><li>通过实验证明，RAMP在复杂长视野操作任务上显著优于其他强化学习方法，在多个任务上获得了约30%的性能提升。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>GigaBrain-0.5模型</strong>：该模型是一个端到端的VLA架构，基于混合Transformer和预训练的PaliGemma-2视觉语言模型，并采用流匹配的扩散Transformer来预测动作块。它通过生成具体化的思维链来增强推理能力，包括语言子目标、离散动作令牌和2D操作轨迹。</li><li><strong>RAMP框架</strong>：这是一个四阶段的迭代训练框架：</li></ol>\n<p> * <strong>阶段一：世界模型预训练</strong>：使用大规模机器人操作数据训练一个世界模型，使其能够联合预测未来状态和值估计。</p>\n<p> * <strong>阶段二：带有世界模型条件的策略训练</strong>：将世界模型预测的未来状态和值转换为二元优势指示信号，并作为条件输入来微调GigaBrain-0.5策略。</p>\n<p> * <strong>阶段三：人在回路中滚动数据收集</strong>：部署策略以自主执行和专家干预相结合的方式收集数据，用于生成高质量的训练轨迹。</p>\n<p> * <strong>阶段四：利用滚动数据进行持续训练</strong>：使用收集的数据进一步联合微调世界模型和策略，形成一个自我改进的闭环。</p>\n<ol><li><strong>理论连接</strong>：论文从概率角度阐述了RAMP与RECAP的关系，证明了RECAP可以被视为RAMP在忽略未来潜在状态信息时的特例。RAMP通过显式条件化世界模型的预测，提供了更丰富的信息增益。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> * GigaBrain-0.5的预训练数据集总计超过10，000小时，包括超过6，000小时的世界模型生成数据和约4，000小时的真实机器人采集数据。</p>\n<p> * 评估使用了八个内部设计的任务和RoboChallenge基准测试中的30个公开任务。</p>\n<p> * RAMP中世界模型训练使用了4，000小时的真实机器人操作数据。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> * 模型训练使用了GigaTrain框架，批次大小为3，072，优化步数为100，000步。</p>\n<p> * 使用了完全分片数据并行（FSDP）v2来减少单GPU内存消耗。</p>\n<p> * 世界模型使用了Wan2.2作为骨干架构进行训练。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> * <strong>内部评估</strong>：在物理机器人平台上进行，包括PiPER机械臂和G1人形机器人。</p>\n<p> * <strong>公开基准评估</strong>：在RoboChallenge平台上进行，该平台包含20台实体机器人，涵盖UR5、Franka、ARX5和ALOHA四种主要平台。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> * <strong>任务成功率</strong>：执行指定任务的成功百分比，是主要评估指标。</p>\n<p> * <strong>价值预测评估指标</strong>：用于评估世界模型价值预测的准确性，包括平均绝对误差（MAE）、均方误差（MSE）、均方根误差（RMSE）和肯德尔秩相关系数。</p>\n<p> * <strong>推理时间</strong>：衡量价值预测方法的计算效率。</p>"
  },
  {
    "date": "2026-02-12",
    "title": "VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model",
    "link": "http://arxiv.org/abs/2602.12063",
    "summary_markdown": "## 研究单位\n- **The Robotics and AI Institute** (根据致谢部分推断)\n- 受ONR资助 (N00014-22-1-2621)\n## 论文概述\n- 提出 **VLAW** (Vision-Language-Action-World) 框架，通过迭代的协同改进机制，提升 **视觉-语言-动作 (VLA)** 策略和 **世界模型 (World Model)** 的性能。\n- 旨在解决在真实机器人上收集在线交互数据成本高昂的问题，通过基于世界模型生成大量合成轨迹数据来扩展训练样本，以进行策略优化。\n- 核心思想是：利用有限的真实世界策略轨迹数据来**“接地”**（提升保真度）预训练的世界模型，然后用改进后的世界模型生成大规模的合成数据来迭代改进VLA策略。\n## 核心贡献\n- 提出了一种**简单、可扩展的迭代框架**（VLAW），利用世界模型生成的数据来改进最先进的VLA模型，在真实机器人上验证了其有效性。\n- 证明了利用**策略在线交互数据**（包含成功和失败案例）微调世界模型，可以显著提升其在复杂接触式、变形物体任务上的物理仿真保真度。\n- 展示了改进后的世界模型生成的**大规模合成数据**可以有效地用于VLA策略的后训练，在多个下游任务上实现了显著的性能提升（**39.2%** 绝对成功率提升）。\n- 提出并验证了使用**视觉-语言奖励模型**（基于Qwen3-VL微调）来自动标注合成轨迹的成功/失败，从而筛选出高质量数据用于策略学习。\n- 将提出的策略更新方法形式化为一种**正则化强化学习**框架的近似，提供了理论解释。\n## 方法描述\n- **核心技术方法**：一种迭代的协同优化算法。每个迭代包含四个步骤：1) 在真实世界中进行策略部署收集在线轨迹；2) 用这些真实轨迹数据（结合原始的 **DROID** 数据集进行正则化）微调预训练的**动作条件世界模型**（**Ctrl-World**）；3) 用更新后的世界模型进行闭环仿真，生成大量合成轨迹，并用奖励模型进行自动标注和筛选；4) 使用过滤后的成功轨迹（包含真实的和合成的）通过**流匹配 (Flow-Matching)** 目标对VLA策略（**π0.5**）进行监督微调。\n- **创新点和关键技术**：将世界模型的微调与VLA策略的改进**迭代耦合**，利用在线数据提升世界模型在策略状态-动作分布上的保真度，再利用高质量合成数据弥补真实交互数据的稀缺性。避免了复杂、不稳定的强化学习算法（如策略梯度），仅使用稳定的监督学习目标。\n## 数据集与资源\n- **数据集**：**DROID** 数据集（用于世界模型的预训练和正则化），以及在实验中收集的**真实机器人策略轨迹数据**（每个任务类别每次迭代收集50条轨迹）。\n- **模型规模**：\n - 基础VLA策略：**π0.5** (参数量不详，为基础VLA模型)\n - 基础世界模型：**Ctrl-World** (基于视频扩散模型)\n - 奖励模型：微调自 **Qwen3-VL-4B-Instruct** (40亿参数)\n- **训练资源**：文中未明确指定，但实验在**真实DROID机器人平台**（Franka Panda机械臂）上进行。\n## 评估与结果\n- **评估环境**：在真实**DROID机器人平台**上，测试了五个具有挑战性的接触式操作任务类别（**堆叠积木、打开书本、擦拭白板、用勺盛物、在白板上画圆**）。\n- **主要评估指标**：**任务成功率**。\n- **关键实验结果**：\n - **世界模型保真度**：微调后的世界模型在视频质量指标（PSNR, SSIM, LPIPS, FID, FVD）和交互事件预测（降低假阳性FP）上均优于仅用专家数据微调或未经微调的基线。\n - **策略性能提升**：经过两轮迭代，VLAW方法相比基础策略（经过少量专家演示微调后）实现了**39.2%** 的绝对成功率提升。与基线（仅用真实成功轨迹的过滤行为克隆，以及DSRL方法）相比，VLAW在每个任务上都取得了更高的成功率。\n - **合成数据有效性**：仅使用世界模型生成的合成数据改进策略，相比只使用真实数据训练的策略带来了**11.6%** 的额外提升。消融实验表明，减少合成数据量或移除真实成功轨迹都会导致性能下降。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>The Robotics and AI Institute</strong> (根据致谢部分推断)</li><li>受ONR资助 (N00014-22-1-2621)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出 <strong>VLAW</strong> (Vision-Language-Action-World) 框架，通过迭代的协同改进机制，提升 <strong>视觉-语言-动作 (VLA)</strong> 策略和 <strong>世界模型 (World Model)</strong> 的性能。</li><li>旨在解决在真实机器人上收集在线交互数据成本高昂的问题，通过基于世界模型生成大量合成轨迹数据来扩展训练样本，以进行策略优化。</li><li>核心思想是：利用有限的真实世界策略轨迹数据来<strong>“接地”</strong>（提升保真度）预训练的世界模型，然后用改进后的世界模型生成大规模的合成数据来迭代改进VLA策略。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了一种<strong>简单、可扩展的迭代框架</strong>（VLAW），利用世界模型生成的数据来改进最先进的VLA模型，在真实机器人上验证了其有效性。</li><li>证明了利用<strong>策略在线交互数据</strong>（包含成功和失败案例）微调世界模型，可以显著提升其在复杂接触式、变形物体任务上的物理仿真保真度。</li><li>展示了改进后的世界模型生成的<strong>大规模合成数据</strong>可以有效地用于VLA策略的后训练，在多个下游任务上实现了显著的性能提升（<strong>39.2%</strong> 绝对成功率提升）。</li><li>提出并验证了使用<strong>视觉-语言奖励模型</strong>（基于Qwen3-VL微调）来自动标注合成轨迹的成功/失败，从而筛选出高质量数据用于策略学习。</li><li>将提出的策略更新方法形式化为一种<strong>正则化强化学习</strong>框架的近似，提供了理论解释。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>核心技术方法</strong>：一种迭代的协同优化算法。每个迭代包含四个步骤：1) 在真实世界中进行策略部署收集在线轨迹；2) 用这些真实轨迹数据（结合原始的 <strong>DROID</strong> 数据集进行正则化）微调预训练的<strong>动作条件世界模型</strong>（<strong>Ctrl-World</strong>）；3) 用更新后的世界模型进行闭环仿真，生成大量合成轨迹，并用奖励模型进行自动标注和筛选；4) 使用过滤后的成功轨迹（包含真实的和合成的）通过<strong>流匹配 (Flow-Matching)</strong> 目标对VLA策略（<strong>π0.5</strong>）进行监督微调。</li><li><strong>创新点和关键技术</strong>：将世界模型的微调与VLA策略的改进<strong>迭代耦合</strong>，利用在线数据提升世界模型在策略状态-动作分布上的保真度，再利用高质量合成数据弥补真实交互数据的稀缺性。避免了复杂、不稳定的强化学习算法（如策略梯度），仅使用稳定的监督学习目标。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>数据集</strong>：<strong>DROID</strong> 数据集（用于世界模型的预训练和正则化），以及在实验中收集的<strong>真实机器人策略轨迹数据</strong>（每个任务类别每次迭代收集50条轨迹）。</li><li><strong>模型规模</strong>：</li></ul>\n<p> - 基础VLA策略：<strong>π0.5</strong> (参数量不详，为基础VLA模型)</p>\n<p> - 基础世界模型：<strong>Ctrl-World</strong> (基于视频扩散模型)</p>\n<p> - 奖励模型：微调自 <strong>Qwen3-VL-4B-Instruct</strong> (40亿参数)</p>\n<ul><li><strong>训练资源</strong>：文中未明确指定，但实验在<strong>真实DROID机器人平台</strong>（Franka Panda机械臂）上进行。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在真实<strong>DROID机器人平台</strong>上，测试了五个具有挑战性的接触式操作任务类别（<strong>堆叠积木、打开书本、擦拭白板、用勺盛物、在白板上画圆</strong>）。</li><li><strong>主要评估指标</strong>：<strong>任务成功率</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>世界模型保真度</strong>：微调后的世界模型在视频质量指标（PSNR, SSIM, LPIPS, FID, FVD）和交互事件预测（降低假阳性FP）上均优于仅用专家数据微调或未经微调的基线。</p>\n<p> - <strong>策略性能提升</strong>：经过两轮迭代，VLAW方法相比基础策略（经过少量专家演示微调后）实现了<strong>39.2%</strong> 的绝对成功率提升。与基线（仅用真实成功轨迹的过滤行为克隆，以及DSRL方法）相比，VLAW在每个任务上都取得了更高的成功率。</p>\n<p> - <strong>合成数据有效性</strong>：仅使用世界模型生成的合成数据改进策略，相比只使用真实数据训练的策略带来了<strong>11.6%</strong> 的额外提升。消融实验表明，减少合成数据量或移除真实成功轨迹都会导致性能下降。</p>"
  },
  {
    "date": "2026-02-12",
    "title": "HoloBrain-0 Technical Report",
    "link": "http://arxiv.org/abs/2602.12062",
    "summary_markdown": "## 研究单位\n- **Horizon Robotics**\n## 论文概述\n- 提出了 **HoloBrain-0**，一个综合的视觉-语言-动作（VLA）框架，旨在弥合基础模型研究与真实可靠机器人部署之间的差距。\n- 其核心是一个新颖的、显式包含机器人本体先验（如多视角相机参数和运动学描述）的 VLA 架构，以增强 3D 空间推理并支持不同的机器人本体。\n- 通过一个可扩展的“预训练后微调”范式进行验证，在仿真和真实世界任务中实现了优越的性能，并开源了整个生态系统和全栈基础设施 **RoboOrchard**。\n## 核心贡献\n- **本体感知的 VLA 架构**：设计了包含**空间增强器**和**动作专家**的模型，显式整合相机参数、机器人运动链和混合动作空间，以实现跨本体的稳健泛化。\n- **高效可复现的数据策略**：采用两阶段数据管理流程，包括大规模的异构预训练数据和针对微调的**测试驱动数据收集范式**，动态针对失败案例以提高数据质量。\n- **开源的全栈 VLA 基础设施（RoboOrchard）**：提供了一个从数据采集、模型训练到部署的完整开源基础设施，包括标准化的数据采集、组织（RODataset）、训练框架（RoboOrchardLab）和灵活的部署运行时。\n- **提出 SimpleRTC 和 Teacher-forcing 训练策略**：提供了一种零开销的推理时轨迹平滑策略和相应的训练策略，以实现流畅的异步推理和实时控制。\n- **全面的评估与开源**：在多个仿真基准和具有挑战性的真实世界操作任务上达到或超过最先进水平，并开源了所有预训练和微调模型、检查点和完整的 RoboOrchard 基础设施。\n## 方法描述\n- **模型架构**：包含三个核心模块：1) **VLM 骨干**（使用 **GroundingDINO** 或 **Qwen2.5-VL**）用于语义理解；2) **空间增强器**，利用相机参数将多视角图像特征投影到统一的 3D 坐标系；3) **动作专家**，通过**关节图注意力**机制显式编码机器人运动链，并预测混合（关节角和笛卡尔位姿）的相对动作空间。\n- **关键技术**：采用基于扩散的 x-预测进行动作生成，并引入了**赢家通吃训练策略**以保持动作分布的多模态性。\n- **推理与训练策略**：提出了 **SimpleRTC**，一种无需梯度计算的推理时轨迹平滑策略，与**教师强制训练**相结合，以减小训练与推理的分布差异。\n## 数据集与资源\n- **数据集**：预训练数据集超过 **1.566 亿帧**，包含自收集数据、开源真实世界数据（如 **Agibot World**、**Droid**）、仿真数据（如 **RoboTwin 2.0**、**Agibot-digital**）和人类视频数据（**EgoDex**），共涉及 **7 种**不同的机器人本体。\n- **模型规模**：\n - **HoloBrain-0-GD**：0.2B 参数（基于 GroundingDINO Tiny）\n - **HoloBrain-0-QW**：1.1B 参数（基于 Qwen2.5-VL-3B）\n- **训练资源**：预训练进行了 20 万步，微调进行了 10-20 万步。具体使用的 GPU/TPU 资源未在提供的片段中明确说明。\n## 评估与结果\n- **评估环境**：在真实世界的**双臂Piper**机器人上测试了 10 项任务；在**仿真基准** **RoboTwin 2.0**、**LIBERO**、**LIBERO-Plus**、**GenieSim** 上进行了评估。\n- **主要评估指标**：**成功率**（和部分任务的进度分数）。\n- **关键实验结果**：\n - **真实世界任务**：**HoloBrain-0-GD** 和 **HoloBrain-0-QW** 在 10 项任务上的平均成功率（74.81%/77.18%）显著优于基线 **π0.5**（69.16%）。在长时程任务（如叠衣服、折纸盒）上，成功率的提升高达 25%-30%。\n - **仿真基准**：\n - **RoboTwin 2.0**：**HoloBrain-0-GD** 在随机化设置下获得 90.8% 成功率（SOTA水平），**HoloBrain-0-QW** 达到 92.3% 成功率。\n - **LIBERO 和 LIBERO-Plus**：性能优于或与现有 SOTA 方法（如 OpenVLA、GR00T）相当。\n - **跨本体泛化**：得益于其架构，模型能够有效处理来自不同机器人本体（单/双臂、移动机器人）以及人类视频的数据。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>Horizon Robotics</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了 <strong>HoloBrain-0</strong>，一个综合的视觉-语言-动作（VLA）框架，旨在弥合基础模型研究与真实可靠机器人部署之间的差距。</li><li>其核心是一个新颖的、显式包含机器人本体先验（如多视角相机参数和运动学描述）的 VLA 架构，以增强 3D 空间推理并支持不同的机器人本体。</li><li>通过一个可扩展的“预训练后微调”范式进行验证，在仿真和真实世界任务中实现了优越的性能，并开源了整个生态系统和全栈基础设施 <strong>RoboOrchard</strong>。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li><strong>本体感知的 VLA 架构</strong>：设计了包含<strong>空间增强器</strong>和<strong>动作专家</strong>的模型，显式整合相机参数、机器人运动链和混合动作空间，以实现跨本体的稳健泛化。</li><li><strong>高效可复现的数据策略</strong>：采用两阶段数据管理流程，包括大规模的异构预训练数据和针对微调的<strong>测试驱动数据收集范式</strong>，动态针对失败案例以提高数据质量。</li><li><strong>开源的全栈 VLA 基础设施（RoboOrchard）</strong>：提供了一个从数据采集、模型训练到部署的完整开源基础设施，包括标准化的数据采集、组织（RODataset）、训练框架（RoboOrchardLab）和灵活的部署运行时。</li><li><strong>提出 SimpleRTC 和 Teacher-forcing 训练策略</strong>：提供了一种零开销的推理时轨迹平滑策略和相应的训练策略，以实现流畅的异步推理和实时控制。</li><li><strong>全面的评估与开源</strong>：在多个仿真基准和具有挑战性的真实世界操作任务上达到或超过最先进水平，并开源了所有预训练和微调模型、检查点和完整的 RoboOrchard 基础设施。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>模型架构</strong>：包含三个核心模块：1) <strong>VLM 骨干</strong>（使用 <strong>GroundingDINO</strong> 或 <strong>Qwen2.5-VL</strong>）用于语义理解；2) <strong>空间增强器</strong>，利用相机参数将多视角图像特征投影到统一的 3D 坐标系；3) <strong>动作专家</strong>，通过<strong>关节图注意力</strong>机制显式编码机器人运动链，并预测混合（关节角和笛卡尔位姿）的相对动作空间。</li><li><strong>关键技术</strong>：采用基于扩散的 x-预测进行动作生成，并引入了<strong>赢家通吃训练策略</strong>以保持动作分布的多模态性。</li><li><strong>推理与训练策略</strong>：提出了 <strong>SimpleRTC</strong>，一种无需梯度计算的推理时轨迹平滑策略，与<strong>教师强制训练</strong>相结合，以减小训练与推理的分布差异。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>数据集</strong>：预训练数据集超过 <strong>1.566 亿帧</strong>，包含自收集数据、开源真实世界数据（如 <strong>Agibot World</strong>、<strong>Droid</strong>）、仿真数据（如 <strong>RoboTwin 2.0</strong>、<strong>Agibot-digital</strong>）和人类视频数据（<strong>EgoDex</strong>），共涉及 <strong>7 种</strong>不同的机器人本体。</li><li><strong>模型规模</strong>：</li></ul>\n<p> - <strong>HoloBrain-0-GD</strong>：0.2B 参数（基于 GroundingDINO Tiny）</p>\n<p> - <strong>HoloBrain-0-QW</strong>：1.1B 参数（基于 Qwen2.5-VL-3B）</p>\n<ul><li><strong>训练资源</strong>：预训练进行了 20 万步，微调进行了 10-20 万步。具体使用的 GPU/TPU 资源未在提供的片段中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在真实世界的<strong>双臂Piper</strong>机器人上测试了 10 项任务；在<strong>仿真基准</strong> <strong>RoboTwin 2.0</strong>、<strong>LIBERO</strong>、<strong>LIBERO-Plus</strong>、<strong>GenieSim</strong> 上进行了评估。</li><li><strong>主要评估指标</strong>：<strong>成功率</strong>（和部分任务的进度分数）。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - <strong>真实世界任务</strong>：<strong>HoloBrain-0-GD</strong> 和 <strong>HoloBrain-0-QW</strong> 在 10 项任务上的平均成功率（74.81%/77.18%）显著优于基线 <strong>π0.5</strong>（69.16%）。在长时程任务（如叠衣服、折纸盒）上，成功率的提升高达 25%-30%。</p>\n<p> - <strong>仿真基准</strong>：</p>\n<p> - <strong>RoboTwin 2.0</strong>：<strong>HoloBrain-0-GD</strong> 在随机化设置下获得 90.8% 成功率（SOTA水平），<strong>HoloBrain-0-QW</strong> 达到 92.3% 成功率。</p>\n<p> - <strong>LIBERO 和 LIBERO-Plus</strong>：性能优于或与现有 SOTA 方法（如 OpenVLA、GR00T）相当。</p>\n<p> - <strong>跨本体泛化</strong>：得益于其架构，模型能够有效处理来自不同机器人本体（单/双臂、移动机器人）以及人类视频的数据。</p>"
  },
  {
    "date": "2026-02-12",
    "title": "Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control",
    "link": "http://arxiv.org/abs/2602.11934",
    "summary_markdown": "## 研究单位\n- **达姆施塔特工业大学** (Technical University of Darmstadt)\n- **卡尔斯鲁厄理工学院** (Karlsruhe Institute of Technology)\n- **伦敦国王学院** (King’s College London)\n## 论文概述\n- 论文提出**Robot-DIFT**框架，旨在解决接触式机器人操作中**视觉表示不匹配**的问题：主流的判别式视觉编码器（如CLIP、DINOv2）侧重于语义不变性，但抑制了精细几何敏感性，而这对于毫米级精度控制至关重要。\n- 核心方法是**流形蒸馏**，将预训练的生成式扩散模型（如Stable Diffusion）中的丰富几何先验知识，蒸馏到一个确定性的**空间-语义特征金字塔网络（S2-FPN）**中，从而获得兼具几何精度、推理速度和时间稳定性的视觉主干网络。\n## 核心贡献\n- **揭示了机器人感知中的关键表示不匹配问题**：指出判别式视觉主干由于其语义不变性目标，会抑制接触式精细控制所需的几何敏感性。\n- **提出了流形蒸馏方法和S2-FPN学生架构**：通过冻结的扩散教师模型，将几何先验蒸馏到单前向传播的确定性学生网络中，解决了直接使用扩散模型带来的随机性、高延迟和表示漂移问题。\n- **提供了在大型机器人数据集DROID上预训练的通用视觉主干**：该主干融合了像素级空间保真度和高层语义，适用于指令跟随任务。\n- **实验验证了方法在几何一致性和控制性能上的优越性**：在**RoboCasa**和**LIBERO-10**等多个基准测试中，性能超过包括VLA模型在内的强判别式基线，并成功部署于真实机器人。\n## 方法描述\n- **核心技术**：采用**流形蒸馏**，使用预训练的**Stable Diffusion v2.1** U-Net作为冻结教师，其多尺度解码器特征作为监督信号。学生网络具有相同的U-Net结构，并添加了**S2-FPN**进行多尺度特征融合。学生以干净的图像潜变量作为输入，通过训练匹配教师在随机噪声步长下的特征流形。\n- **创新点与关键技术**：\n - **S2-FPN学生架构**：通过全局到精细的特征融合，将粗粒度语义上下文注入细粒度几何特征中，平衡“做什么”和“在哪里做”的信息。\n - **冻结教师约束**：通过余弦相似度损失对齐学生和教师的所有解码器层特征，防止在策略微调时发生表示漂移。\n - **对齐权重退火**：训练初期使用较强对齐约束，后期逐渐减弱，以平衡几何先验保持和任务特定适应。\n## 数据集与资源\n- **主要预训练数据集**：**DROID** (一个大规模真实世界机器人操作数据集)。\n- **评估数据集**：**RoboCasa** (大规模模拟日常操作任务)、**LIBERO-10** (语言条件的长视野任务)。\n- **模型规模**：教师模型为**Stable Diffusion v2.1** (约8.6亿参数)。学生网络具有相同的U-Net结构，并增加了轻量级的S2-FPN和投影头。\n- **训练资源**：未在提供的原文中明确指定，但此类工作通常使用多块**GPU**进行训练。\n## 评估与结果\n- **评估环境与基准**：在模拟器(**RoboCasa**, **LIBERO-10**)和真实机器人(**Franka Emika Panda**)上进行评估。主要与判别式视觉主干(**CLIP, SigLIP, DINOv2/v3**)、**VLA模型(OpenVLA, π₀)** 和**视频动作模型(UVA, UniPi)** 进行比较。\n- **主要评估指标**：**任务成功率**、**推理速度（延迟）**。\n- **关键实验结果**：\n - 在**RoboCasa**的24个任务上，Robot-DIFT平均成功率达**0.49**，优于所有基线（DINOv3: 0.33， 纯扩散特征DIFT: 0.38），并在18个任务上取得最佳成绩。\n - 在**LIBERO-10**上，Robot-DIFT取得最高成功率(**0.93**)，同时推理速度最快(**0.01秒/轨迹**)，显著优于UVA(0.90, 0.23秒)和π₀(0.85, 0.09秒)等模型。\n - **真实机器人实验**在四个需要递增几何精度的任务上验证了方法的有效性，特别是在高精度的“插针”和“按开关”任务中表现出色。\n - **消融实验**证实了多尺度特征融合和对齐权重退火策略的必要性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>达姆施塔特工业大学</strong> (Technical University of Darmstadt)</li><li><strong>卡尔斯鲁厄理工学院</strong> (Karlsruhe Institute of Technology)</li><li><strong>伦敦国王学院</strong> (King’s College London)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文提出<strong>Robot-DIFT</strong>框架，旨在解决接触式机器人操作中<strong>视觉表示不匹配</strong>的问题：主流的判别式视觉编码器（如CLIP、DINOv2）侧重于语义不变性，但抑制了精细几何敏感性，而这对于毫米级精度控制至关重要。</li><li>核心方法是<strong>流形蒸馏</strong>，将预训练的生成式扩散模型（如Stable Diffusion）中的丰富几何先验知识，蒸馏到一个确定性的<strong>空间-语义特征金字塔网络（S2-FPN）</strong>中，从而获得兼具几何精度、推理速度和时间稳定性的视觉主干网络。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li><strong>揭示了机器人感知中的关键表示不匹配问题</strong>：指出判别式视觉主干由于其语义不变性目标，会抑制接触式精细控制所需的几何敏感性。</li><li><strong>提出了流形蒸馏方法和S2-FPN学生架构</strong>：通过冻结的扩散教师模型，将几何先验蒸馏到单前向传播的确定性学生网络中，解决了直接使用扩散模型带来的随机性、高延迟和表示漂移问题。</li><li><strong>提供了在大型机器人数据集DROID上预训练的通用视觉主干</strong>：该主干融合了像素级空间保真度和高层语义，适用于指令跟随任务。</li><li><strong>实验验证了方法在几何一致性和控制性能上的优越性</strong>：在<strong>RoboCasa</strong>和<strong>LIBERO-10</strong>等多个基准测试中，性能超过包括VLA模型在内的强判别式基线，并成功部署于真实机器人。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li><strong>核心技术</strong>：采用<strong>流形蒸馏</strong>，使用预训练的<strong>Stable Diffusion v2.1</strong> U-Net作为冻结教师，其多尺度解码器特征作为监督信号。学生网络具有相同的U-Net结构，并添加了<strong>S2-FPN</strong>进行多尺度特征融合。学生以干净的图像潜变量作为输入，通过训练匹配教师在随机噪声步长下的特征流形。</li><li><strong>创新点与关键技术</strong>：</li></ul>\n<p> - <strong>S2-FPN学生架构</strong>：通过全局到精细的特征融合，将粗粒度语义上下文注入细粒度几何特征中，平衡“做什么”和“在哪里做”的信息。</p>\n<p> - <strong>冻结教师约束</strong>：通过余弦相似度损失对齐学生和教师的所有解码器层特征，防止在策略微调时发生表示漂移。</p>\n<p> - <strong>对齐权重退火</strong>：训练初期使用较强对齐约束，后期逐渐减弱，以平衡几何先验保持和任务特定适应。</p>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li><strong>主要预训练数据集</strong>：<strong>DROID</strong> (一个大规模真实世界机器人操作数据集)。</li><li><strong>评估数据集</strong>：<strong>RoboCasa</strong> (大规模模拟日常操作任务)、<strong>LIBERO-10</strong> (语言条件的长视野任务)。</li><li><strong>模型规模</strong>：教师模型为<strong>Stable Diffusion v2.1</strong> (约8.6亿参数)。学生网络具有相同的U-Net结构，并增加了轻量级的S2-FPN和投影头。</li><li><strong>训练资源</strong>：未在提供的原文中明确指定，但此类工作通常使用多块<strong>GPU</strong>进行训练。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在模拟器(<strong>RoboCasa</strong>, <strong>LIBERO-10</strong>)和真实机器人(<strong>Franka Emika Panda</strong>)上进行评估。主要与判别式视觉主干(<strong>CLIP, SigLIP, DINOv2/v3</strong>)、<strong>VLA模型(OpenVLA, π₀)</strong> 和<strong>视频动作模型(UVA, UniPi)</strong> 进行比较。</li><li><strong>主要评估指标</strong>：<strong>任务成功率</strong>、<strong>推理速度（延迟）</strong>。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 在<strong>RoboCasa</strong>的24个任务上，Robot-DIFT平均成功率达<strong>0.49</strong>，优于所有基线（DINOv3: 0.33， 纯扩散特征DIFT: 0.38），并在18个任务上取得最佳成绩。</p>\n<p> - 在<strong>LIBERO-10</strong>上，Robot-DIFT取得最高成功率(<strong>0.93</strong>)，同时推理速度最快(<strong>0.01秒/轨迹</strong>)，显著优于UVA(0.90, 0.23秒)和π₀(0.85, 0.09秒)等模型。</p>\n<p> - <strong>真实机器人实验</strong>在四个需要递增几何精度的任务上验证了方法的有效性，特别是在高精度的“插针”和“按开关”任务中表现出色。</p>\n<p> - <strong>消融实验</strong>证实了多尺度特征融合和对齐权重退火策略的必要性。</p>"
  },
  {
    "date": "2026-02-12",
    "title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models",
    "link": "http://arxiv.org/abs/2602.11832",
    "summary_markdown": "## 研究单位\n- 论文作者未在提供的 HTML 片段中明确列出其所属机构。\n## 论文概述\n- 论文指出，当前基于预训练视觉-语言模型构建的**视觉-语言-动作模型**存在样本效率低和泛化能力有限的问题，根源在于其视觉表征的不足。\n- 论文旨在解决**VLA模型**对机器人控制至关重要的两种视觉知识（**环境理解**和**策略先验**）获取不充分的问题。\n- 通过系统分析，论文发现视频预测嵌入（特别是**V-JEPA 2**）能有效弥补现有图像预训练表征的缺陷，并据此提出了**JEPA-VLA**集成框架。\n## 核心贡献\n- 识别并形式化了VLA模型所需的两项关键视觉知识：**环境理解**（捕捉任务相关状态、忽略无关干扰）和**策略先验**（编码成功任务执行下的环境演变规律）。\n- 通过实验分析证明，基于视频的预测性表征**V-JEPA 2**在捕捉任务相关状态和策略先验方面，显著优于基于图像的**DINOv2**和基于语言-图像的**SigLIP**等常用静态视觉表征。\n- 提出了**JEPA-VLA**，一种简单有效的通用框架，可将视频预测嵌入自适应地集成到现有VLA中。\n- 在多个基准测试（**LIBERO**、**LIBERO-plus**、**RoboTwin2.0**、**真实机器人任务**）上验证了JEPA-VLA能带来显著的性能提升，尤其在样本效率和泛化性方面。\n## 方法描述\n- 核心方法是集成预训练好的**V-JEPA 2**视觉编码器，为现有VLA模型提供补充的预测性视觉表征。\n- 提出了两种融合策略：**早期融合**（将V-JEPA 2表征作为额外输入token拼接）适用于从头训练的VLA；**门控融合**（引入门控交叉注意力层）适用于已在大规模机器人数据上预训练的VLA，以最小化对已有知识的干扰。\n- 创新点在于利用**V-JEPA 2**的**联合嵌入预测架构**，该架构通过预测视频中被遮蔽的块，学习了强调可预测、任务相关因素，同时抑制不可预测干扰的表示，从而自然地编码了环境动态和时序规律。\n## 数据集与资源\n- 使用的分析/评测数据集包括：**LIBERO**、**LIBERO-plus**、**RoboTwin2.0**、**CortexBench**（包含MetaWorld和DMControl任务）以及一个真实世界的拾取放置任务。\n- 使用了预训练的**V-JEPA 2**模型（基于ViT架构）作为视觉编码器，其参数被冻结。\n- 训练资源细节在提供的片段中未明确说明，但提到了因计算资源有限而调整了批量大小（如LIBERO上使用批量大小16，RoboTwin2.0上使用8）。\n## 评估与结果\n- **评估环境**：在多个模拟仿真基准和真实机器人任务上进行评估。\n- **主要评估指标**：任务成功率、平均成功率、DMControl中的回合奖励。\n- **关键实验结果**：\n - 在**LIBERO**基准上，JEPA-VLA使基础VLA平均成功率提升7.4%，使主流VLA（OpenVLA-OFT）平均成功率提升6.1%，甚至超过官方报告结果。\n - 在**LIBERO-plus**基准（测试泛化性）上，JEPA-VLA仅用1/10的数据，其性能就超过了使用完整数据训练的**WorldVLA**基线模型。\n - 在**RoboTwin2.0**基准上，在干净和域随机化设置下，平均成功率分别提升18.7%和8.4%。\n - 在**真实世界**任务中，JEPA-VLA使用仅1/5的训练轨迹，成功率（60%）即超过使用全部数据的基线（50%）；使用全数据时达到80%成功率。\n - 在更广泛的**CortexBench**评估中，**V-JEPA 2**表征在大多数任务上超越了先前表现最佳的预训练视觉表征**VC-1**。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li>论文作者未在提供的 HTML 片段中明确列出其所属机构。</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>论文指出，当前基于预训练视觉-语言模型构建的<strong>视觉-语言-动作模型</strong>存在样本效率低和泛化能力有限的问题，根源在于其视觉表征的不足。</li><li>论文旨在解决<strong>VLA模型</strong>对机器人控制至关重要的两种视觉知识（<strong>环境理解</strong>和<strong>策略先验</strong>）获取不充分的问题。</li><li>通过系统分析，论文发现视频预测嵌入（特别是<strong>V-JEPA 2</strong>）能有效弥补现有图像预训练表征的缺陷，并据此提出了<strong>JEPA-VLA</strong>集成框架。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>识别并形式化了VLA模型所需的两项关键视觉知识：<strong>环境理解</strong>（捕捉任务相关状态、忽略无关干扰）和<strong>策略先验</strong>（编码成功任务执行下的环境演变规律）。</li><li>通过实验分析证明，基于视频的预测性表征<strong>V-JEPA 2</strong>在捕捉任务相关状态和策略先验方面，显著优于基于图像的<strong>DINOv2</strong>和基于语言-图像的<strong>SigLIP</strong>等常用静态视觉表征。</li><li>提出了<strong>JEPA-VLA</strong>，一种简单有效的通用框架，可将视频预测嵌入自适应地集成到现有VLA中。</li><li>在多个基准测试（<strong>LIBERO</strong>、<strong>LIBERO-plus</strong>、<strong>RoboTwin2.0</strong>、<strong>真实机器人任务</strong>）上验证了JEPA-VLA能带来显著的性能提升，尤其在样本效率和泛化性方面。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>核心方法是集成预训练好的<strong>V-JEPA 2</strong>视觉编码器，为现有VLA模型提供补充的预测性视觉表征。</li><li>提出了两种融合策略：<strong>早期融合</strong>（将V-JEPA 2表征作为额外输入token拼接）适用于从头训练的VLA；<strong>门控融合</strong>（引入门控交叉注意力层）适用于已在大规模机器人数据上预训练的VLA，以最小化对已有知识的干扰。</li><li>创新点在于利用<strong>V-JEPA 2</strong>的<strong>联合嵌入预测架构</strong>，该架构通过预测视频中被遮蔽的块，学习了强调可预测、任务相关因素，同时抑制不可预测干扰的表示，从而自然地编码了环境动态和时序规律。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的分析/评测数据集包括：<strong>LIBERO</strong>、<strong>LIBERO-plus</strong>、<strong>RoboTwin2.0</strong>、<strong>CortexBench</strong>（包含MetaWorld和DMControl任务）以及一个真实世界的拾取放置任务。</li><li>使用了预训练的<strong>V-JEPA 2</strong>模型（基于ViT架构）作为视觉编码器，其参数被冻结。</li><li>训练资源细节在提供的片段中未明确说明，但提到了因计算资源有限而调整了批量大小（如LIBERO上使用批量大小16，RoboTwin2.0上使用8）。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在多个模拟仿真基准和真实机器人任务上进行评估。</li><li><strong>主要评估指标</strong>：任务成功率、平均成功率、DMControl中的回合奖励。</li><li><strong>关键实验结果</strong>：</li></ul>\n<p> - 在<strong>LIBERO</strong>基准上，JEPA-VLA使基础VLA平均成功率提升7.4%，使主流VLA（OpenVLA-OFT）平均成功率提升6.1%，甚至超过官方报告结果。</p>\n<p> - 在<strong>LIBERO-plus</strong>基准（测试泛化性）上，JEPA-VLA仅用1/10的数据，其性能就超过了使用完整数据训练的<strong>WorldVLA</strong>基线模型。</p>\n<p> - 在<strong>RoboTwin2.0</strong>基准上，在干净和域随机化设置下，平均成功率分别提升18.7%和8.4%。</p>\n<p> - 在<strong>真实世界</strong>任务中，JEPA-VLA使用仅1/5的训练轨迹，成功率（60%）即超过使用全部数据的基线（50%）；使用全数据时达到80%成功率。</p>\n<p> - 在更广泛的<strong>CortexBench</strong>评估中，<strong>V-JEPA 2</strong>表征在大多数任务上超越了先前表现最佳的预训练视觉表征<strong>VC-1</strong>。</p>"
  },
  {
    "date": "2026-02-12",
    "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
    "link": "http://arxiv.org/abs/2602.11598",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-11",
    "title": "H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model",
    "link": "http://arxiv.org/abs/2602.11291",
    "summary_markdown": "## 研究单位\n- **华为诺亚方舟实验室 (Huawei Noah’s Ark Lab)**\n- **多伦多大学 (University of Toronto)**\n- **麦吉尔大学 (McGill University)**\n## 论文概述\n- 提出一个**层次化世界模型 (Hierarchical World Model, H-WM)**，通过联合预测逻辑和视觉状态转移，为视觉-语言-动作模型在执行长视距任务时提供中间指导。\n- 旨在解决现有方法（如端到端VLA模型、基于LLM的分层规划、传统任务与运动规划）在长视距任务中表现不佳的问题，如错误累积、目标不明确以及符号推理与视觉感知脱节。\n## 核心贡献\n- 提出了一个**层次化世界模型 (H-WM)** 的训练流程，能够同时在多个抽象层次上预测世界动态，将长视距的逻辑状态转移与视觉状态演变对齐。\n- 引入了**逻辑世界模型**，通过对LLM进行微调以实现符号规划，内化了长视距规划行为，提供了全局一致、可解释的任务级指导。\n- 开发了**基于潜在特征的视觉世界模型**，以预测的逻辑状态和动作为条件，生成紧凑的潜在视觉子目标，提供稳定且丰富的视觉引导。\n- 提出了一个**系统化流程**，将逻辑和视觉引导集成到VLA模型中，使底层策略能够灵活地基于逻辑动作、逻辑状态、视觉子目标或其组合进行执行。\n- 构建并发布了包含机器人动作、符号状态与视觉观测对齐的训练数据集 **LIBERO-Logic**。\n## 方法描述\n- 方法核心是一个双层框架：高层的**逻辑世界模型**和低层的**视觉世界模型**。\n- **逻辑世界模型**：基于LLM微调，学习符号规划动态，能够进行长视距推理，预测符号状态序列和动作序列，并提供规划和奖励评估双重功能。\n- **视觉世界模型**：由一个理解专家和一个预测专家组成。它以前一观察、执行的逻辑动作和预测的下一个逻辑状态为条件，在共享特征空间中生成一个潜在的视觉子目标特征（而非像素图像），为底层控制提供稳定的感知目标。\n- **引导流程**：逻辑世界模型生成符号计划；视觉世界模型为每个子任务生成视觉子目标；一个专门的**子目标VLA策略**综合当前观察、符号任务规范和视觉子目标，产生低层机器人动作。\n## 数据集与资源\n- 主要数据集：**LIBERO-Logic**（基于**LIBERO**基准构建），包含超过98万高质量图像-逻辑对，帧级对齐了机器人状态、视觉观测、逻辑状态和逻辑动作。\n- 评估基准：**LIBERO-LoHo**，一个从LIBERO衍生的长视距基准，包含5个任务类型，任务视距约为原任务的两倍。\n- 模型规模：未在提供的原文中明确说明具体参数量，但方法涉及微调LLM（逻辑世界模型）和使用基于Transformer的专家模型（视觉世界模型和子目标VLA）。\n- 训练资源：未在提供的原文中明确说明。\n## 评估与结果\n- 评估环境：在**LIBERO-LoHo**长视距机器人操作基准上进行。\n- 主要评估指标：**Q-Score**（已完成子目标数与总子目标数之比）和**Success Rate**（完全成功完成的任务百分比）。\n- 关键实验结果：H-WM引导的 **π₀.₅** 模型在所有任务上显著优于所有基线方法（包括端到端 **π₀**、**π₀.₅**，以及语言引导和纯逻辑引导的分层方法）。在平均成功率上，H-WM (64.8%) 远超纯逻辑引导 (48.4%) 和语言引导 (26.8%)。消融研究证实了视觉引导对于将符号计划落地到感知空间、减少错误累积至关重要。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>华为诺亚方舟实验室 (Huawei Noah’s Ark Lab)</strong></li><li><strong>多伦多大学 (University of Toronto)</strong></li><li><strong>麦吉尔大学 (McGill University)</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出一个<strong>层次化世界模型 (Hierarchical World Model, H-WM)</strong>，通过联合预测逻辑和视觉状态转移，为视觉-语言-动作模型在执行长视距任务时提供中间指导。</li><li>旨在解决现有方法（如端到端VLA模型、基于LLM的分层规划、传统任务与运动规划）在长视距任务中表现不佳的问题，如错误累积、目标不明确以及符号推理与视觉感知脱节。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了一个<strong>层次化世界模型 (H-WM)</strong> 的训练流程，能够同时在多个抽象层次上预测世界动态，将长视距的逻辑状态转移与视觉状态演变对齐。</li><li>引入了<strong>逻辑世界模型</strong>，通过对LLM进行微调以实现符号规划，内化了长视距规划行为，提供了全局一致、可解释的任务级指导。</li><li>开发了<strong>基于潜在特征的视觉世界模型</strong>，以预测的逻辑状态和动作为条件，生成紧凑的潜在视觉子目标，提供稳定且丰富的视觉引导。</li><li>提出了一个<strong>系统化流程</strong>，将逻辑和视觉引导集成到VLA模型中，使底层策略能够灵活地基于逻辑动作、逻辑状态、视觉子目标或其组合进行执行。</li><li>构建并发布了包含机器人动作、符号状态与视觉观测对齐的训练数据集 <strong>LIBERO-Logic</strong>。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>方法核心是一个双层框架：高层的<strong>逻辑世界模型</strong>和低层的<strong>视觉世界模型</strong>。</li><li><strong>逻辑世界模型</strong>：基于LLM微调，学习符号规划动态，能够进行长视距推理，预测符号状态序列和动作序列，并提供规划和奖励评估双重功能。</li><li><strong>视觉世界模型</strong>：由一个理解专家和一个预测专家组成。它以前一观察、执行的逻辑动作和预测的下一个逻辑状态为条件，在共享特征空间中生成一个潜在的视觉子目标特征（而非像素图像），为底层控制提供稳定的感知目标。</li><li><strong>引导流程</strong>：逻辑世界模型生成符号计划；视觉世界模型为每个子任务生成视觉子目标；一个专门的<strong>子目标VLA策略</strong>综合当前观察、符号任务规范和视觉子目标，产生低层机器人动作。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>主要数据集：<strong>LIBERO-Logic</strong>（基于<strong>LIBERO</strong>基准构建），包含超过98万高质量图像-逻辑对，帧级对齐了机器人状态、视觉观测、逻辑状态和逻辑动作。</li><li>评估基准：<strong>LIBERO-LoHo</strong>，一个从LIBERO衍生的长视距基准，包含5个任务类型，任务视距约为原任务的两倍。</li><li>模型规模：未在提供的原文中明确说明具体参数量，但方法涉及微调LLM（逻辑世界模型）和使用基于Transformer的专家模型（视觉世界模型和子目标VLA）。</li><li>训练资源：未在提供的原文中明确说明。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：在<strong>LIBERO-LoHo</strong>长视距机器人操作基准上进行。</li><li>主要评估指标：<strong>Q-Score</strong>（已完成子目标数与总子目标数之比）和<strong>Success Rate</strong>（完全成功完成的任务百分比）。</li><li>关键实验结果：H-WM引导的 <strong>π₀.₅</strong> 模型在所有任务上显著优于所有基线方法（包括端到端 <strong>π₀</strong>、<strong>π₀.₅</strong>，以及语言引导和纯逻辑引导的分层方法）。在平均成功率上，H-WM (64.8%) 远超纯逻辑引导 (48.4%) 和语言引导 (26.8%)。消融研究证实了视觉引导对于将符号计划落地到感知空间、减少错误累积至关重要。</li></ul>"
  },
  {
    "date": "2026-02-11",
    "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
    "link": "http://arxiv.org/abs/2602.11236",
    "summary_markdown": "## 研究单位\n- **AMAP CV Lab**\n## 论文概述\n- 提出了 **ABot-M0**，一个面向机器人操作任务的视觉-语言-动作（VLA）基础模型，旨在实现“一个大脑，多种形态”（one-brain, many-forms）的通用具身智能体。\n- 构建了大规模、标准化的数据集 **UniACT-dataset**，并提出 **动作流形学习（Action Manifold Learning, AML）** 等技术，以提升动作预测的效率与稳定性。\n## 核心贡献\n- 提出了 **动作流形假设（Action Manifold Hypothesis）**，并设计了 **动作流形学习（AML）** 方法，让模型直接预测干净的动作序列，提升了生成效率和策略稳定性。\n- 构建了统一的大规模机器人操作数据集 **UniACT-dataset**，通过系统地清洗、对齐六个开源数据集，形成了包含超过600万条轨迹、9500+小时数据、覆盖20+种机器人形态的混合训练集。\n- 提出了一个双阶段训练范式，包括大规模预训练和空间感知的监督微调，以平衡模型通用性和特定任务的精度。\n- 设计了一个模块化感知架构，通过双流机制整合视觉语言模型的语义特征与可插拔的3D模块（如**VGGT**和**Qwen-Image-Edit**）提供的几何先验和多视图信息，增强了空间理解能力。\n## 方法描述\n- 模型采用两阶段架构：一个**视觉语言模型（Qwen3-VL）** 作为感知主干，一个**扩散变换器（DiT）** 作为动作专家。\n- 核心创新点 **动作流形学习（AML）**：基于“有效动作存在于由物理规律和任务约束定义的低维光滑流形上”的假设，让DiT直接预测去噪后的动作块，将学习目标从拟合噪声转移到投影到可行的动作流形上，提升了推理速度和策略质量。\n- 使用**任务均匀采样策略**进行大规模预训练，以平衡数据集和形态的分布，改善跨平台和跨任务的泛化能力。\n- 通过可选的**VGGT**（单图像3D特征）和**Qwen-Image-Edit**（多视图特征合成）模块，以交叉注意力方式将3D几何信息注入到VLM特征中，增强模型的空间推理能力。\n## 数据集与资源\n- 使用的核心数据集：通过清洗和对齐六个开源数据集（**OXE、OXE-AugE、AgiBot-Beta、RoboCoin、RoboMind、Galaxea**）构建的 **UniACT-dataset**。\n- 模型规模：视觉语言模型为 **Qwen3-VL 4B**，动作专家为 **DiT 0.16B**。\n- 训练资源：在1024的总批次大小下，进行了100K步的预训练。\n## 评估与结果\n- 评估环境：在多个模拟基准上进行评估，包括 **LIBERO、LIBERO Plus、RoboCasa GR1 Tabletop Tasks** 和 **RoboTwin 2.0**。\n- 主要评估指标：任务成功率和动作预测的平均绝对误差（MAE）。\n- 关键实验结果：\n - 在**LIBERO**基准上平均成功率高达**98.6%**。\n - 在**LIBERO Plus**上达到**72.4%**的平均成功率，优于基线方法。\n - 在**RoboCasa GR1 Tabletop Tasks**上取得**58.3%**的成功率。\n - 在**RoboTwin 2.0**上达到**81.2%**的成功率。\n - 消融研究证明了动作流形学习、VLM特征交互和3D信息注入等核心组件的有效性。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>AMAP CV Lab</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了 <strong>ABot-M0</strong>，一个面向机器人操作任务的视觉-语言-动作（VLA）基础模型，旨在实现“一个大脑，多种形态”（one-brain, many-forms）的通用具身智能体。</li><li>构建了大规模、标准化的数据集 <strong>UniACT-dataset</strong>，并提出 <strong>动作流形学习（Action Manifold Learning, AML）</strong> 等技术，以提升动作预测的效率与稳定性。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>动作流形假设（Action Manifold Hypothesis）</strong>，并设计了 <strong>动作流形学习（AML）</strong> 方法，让模型直接预测干净的动作序列，提升了生成效率和策略稳定性。</li><li>构建了统一的大规模机器人操作数据集 <strong>UniACT-dataset</strong>，通过系统地清洗、对齐六个开源数据集，形成了包含超过600万条轨迹、9500+小时数据、覆盖20+种机器人形态的混合训练集。</li><li>提出了一个双阶段训练范式，包括大规模预训练和空间感知的监督微调，以平衡模型通用性和特定任务的精度。</li><li>设计了一个模块化感知架构，通过双流机制整合视觉语言模型的语义特征与可插拔的3D模块（如<strong>VGGT</strong>和<strong>Qwen-Image-Edit</strong>）提供的几何先验和多视图信息，增强了空间理解能力。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>模型采用两阶段架构：一个<strong>视觉语言模型（Qwen3-VL）</strong> 作为感知主干，一个<strong>扩散变换器（DiT）</strong> 作为动作专家。</li><li>核心创新点 <strong>动作流形学习（AML）</strong>：基于“有效动作存在于由物理规律和任务约束定义的低维光滑流形上”的假设，让DiT直接预测去噪后的动作块，将学习目标从拟合噪声转移到投影到可行的动作流形上，提升了推理速度和策略质量。</li><li>使用<strong>任务均匀采样策略</strong>进行大规模预训练，以平衡数据集和形态的分布，改善跨平台和跨任务的泛化能力。</li><li>通过可选的<strong>VGGT</strong>（单图像3D特征）和<strong>Qwen-Image-Edit</strong>（多视图特征合成）模块，以交叉注意力方式将3D几何信息注入到VLM特征中，增强模型的空间推理能力。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的核心数据集：通过清洗和对齐六个开源数据集（<strong>OXE、OXE-AugE、AgiBot-Beta、RoboCoin、RoboMind、Galaxea</strong>）构建的 <strong>UniACT-dataset</strong>。</li><li>模型规模：视觉语言模型为 <strong>Qwen3-VL 4B</strong>，动作专家为 <strong>DiT 0.16B</strong>。</li><li>训练资源：在1024的总批次大小下，进行了100K步的预训练。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li>评估环境：在多个模拟基准上进行评估，包括 <strong>LIBERO、LIBERO Plus、RoboCasa GR1 Tabletop Tasks</strong> 和 <strong>RoboTwin 2.0</strong>。</li><li>主要评估指标：任务成功率和动作预测的平均绝对误差（MAE）。</li><li>关键实验结果：</li></ul>\n<p> - 在<strong>LIBERO</strong>基准上平均成功率高达<strong>98.6%</strong>。</p>\n<p> - 在<strong>LIBERO Plus</strong>上达到<strong>72.4%</strong>的平均成功率，优于基线方法。</p>\n<p> - 在<strong>RoboCasa GR1 Tabletop Tasks</strong>上取得<strong>58.3%</strong>的成功率。</p>\n<p> - 在<strong>RoboTwin 2.0</strong>上达到<strong>81.2%</strong>的成功率。</p>\n<p> - 消融研究证明了动作流形学习、VLM特征交互和3D信息注入等核心组件的有效性。</p>"
  },
  {
    "date": "2026-02-11",
    "title": "RISE: Self-Improving Robot Policy with Compositional World Model",
    "link": "http://arxiv.org/abs/2602.11075",
    "summary_markdown": "## 研究单位\n- **清华大学**\n- **上海人工智能实验室**\n- ****商汤科技****\n- **上海交通大学**\n## 论文概述\n- 提出了 **RISE** 框架，通过将机器人策略的学习环境从物理世界转移到由世界模型构建的想象空间，实现可扩展的在线强化学习。\n- 旨在解决在物理世界中直接进行策略强化学习（RL）时面临的成本高、环境重置困难、安全性风险以及数据收集瓶颈等问题，以提升机器人策略在接触密集和动态复杂操作任务中的鲁棒性。\n## 核心贡献\n- 提出了 **RISE** 框架，通过 **“想象”** 实现机器人基础模型的自主 **自我改进**，克服了物理世界交互的限制。\n- 设计并构建了一个 **组合式世界模型**，该模型将动态预测（**可控动力学模型**）与状态价值评估（**进展价值模型**）解耦，为策略改进提供可靠的学习信号。\n- 揭示了实现稳定学习的关键设计选择，包括用于提升动作可控性的 **任务中心批处理**、结合进度回归与时间差分学习训练的价值模型以及混合离线与在线数据的训练策略。\n- 在多个高难度真实世界任务上进行了广泛评估，实验结果表明 **RISE** 的性能显著超越了现有的模仿学习（IL）和强化学习（RL）方法。\n## 方法描述\n- 整体框架分为两个阶段：**策略预热**（在真实世界离线数据上进行优势条件训练）和 **自我改进循环**（在组合式世界模型中生成想象样本进行策略优化）。\n- **组合式世界模型** 由两个独立的模块组成：\n - **可控动力学模型**：基于高效的视频扩散模型 **Genie Envisioner**，通过大规模机器人数据集和 **任务中心批处理** 策略进行预训练，实现了对动作序列的快速、忠实未来状态预测。\n - **进展价值模型**：参数化自预训练的 VLA 策略 **π₀.₅**，通过结合 **进度回归损失** 和 **时间差分学习损失** 进行训练，为想象的轨迹提供密集且对失败敏感的奖励信号。\n- 系统将策略提出的动作序列输入世界模型，合成未来视觉状态并计算优势值，然后通过优势条件训练（Advantage-Conditioned Training）更新策略，整个过程在想象空间中进行，无需物理交互。\n## 数据集与资源\n- 使用的数据集包括：大规模机器人数据集 **Galaxea** 和 **Agibot World**（用于动力学模型预训练），以及为特定任务（动态积木分拣、背包打包、盒子封装）收集的离线真实世界经验数据。\n- **动力学模型** 基于 **Genie Envisioner (GE-base)** 架构；**价值模型** 基于 **π₀.₅** VLA 模型；策略模型为基于流匹配的 **VLA**。\n- 训练资源：动力学模型预训练使用 **16 块 NVIDIA H100 GPU**（约7天），任务特定微调使用 **8 块 H100 GPU**（约3天）；价值模型和策略训练均使用 **8 块 H100 GPU**。\n## 评估与结果\n- **评估环境与基准**：在真实世界的三个挑战性任务上评估——**动态积木分拣**、**背包打包** 和 **盒子封装**。对比了包括 **π₀.₅**、**DAgger**、**PPO**、**DSRL** 和 **RECAP** 在内的多种基线方法。\n- **主要评估指标**：任务**成功率（%）** 和多阶段**评分**。\n- **关键实验结果**：**RISE** 在所有任务上均取得显著优势。在**动态积木分拣**任务上，成功率从 RECAP 的 50% 提升至 **85%**；在**背包打包**任务上，从 RECAP 的 40% 提升至 **85%**；在**盒子封装**任务上，从 RECAP 的 60% 提升至 **95%**。\n- **消融实验**：验证了各组件（如任务中心批处理、价值模型的混合损失、在线动作与状态生成）的有效性，并表明最佳离线数据混合比例约为 **0.6**。\n- **世界模型质量**：在 PSNR、LPIPS、SSIM、FVD 和端点误差（EPE）等指标上，其动力学模型均优于 Cosmos 和基础 Genie Envisioner。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>清华大学</strong></li><li><strong>上海人工智能实验室</strong></li><li>**<strong>商汤科技</strong>**</li><li><strong>上海交通大学</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了 <strong>RISE</strong> 框架，通过将机器人策略的学习环境从物理世界转移到由世界模型构建的想象空间，实现可扩展的在线强化学习。</li><li>旨在解决在物理世界中直接进行策略强化学习（RL）时面临的成本高、环境重置困难、安全性风险以及数据收集瓶颈等问题，以提升机器人策略在接触密集和动态复杂操作任务中的鲁棒性。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了 <strong>RISE</strong> 框架，通过 <strong>“想象”</strong> 实现机器人基础模型的自主 <strong>自我改进</strong>，克服了物理世界交互的限制。</li><li>设计并构建了一个 <strong>组合式世界模型</strong>，该模型将动态预测（<strong>可控动力学模型</strong>）与状态价值评估（<strong>进展价值模型</strong>）解耦，为策略改进提供可靠的学习信号。</li><li>揭示了实现稳定学习的关键设计选择，包括用于提升动作可控性的 <strong>任务中心批处理</strong>、结合进度回归与时间差分学习训练的价值模型以及混合离线与在线数据的训练策略。</li><li>在多个高难度真实世界任务上进行了广泛评估，实验结果表明 <strong>RISE</strong> 的性能显著超越了现有的模仿学习（IL）和强化学习（RL）方法。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>整体框架分为两个阶段：<strong>策略预热</strong>（在真实世界离线数据上进行优势条件训练）和 <strong>自我改进循环</strong>（在组合式世界模型中生成想象样本进行策略优化）。</li><li><strong>组合式世界模型</strong> 由两个独立的模块组成：</li></ul>\n<p> - <strong>可控动力学模型</strong>：基于高效的视频扩散模型 <strong>Genie Envisioner</strong>，通过大规模机器人数据集和 <strong>任务中心批处理</strong> 策略进行预训练，实现了对动作序列的快速、忠实未来状态预测。</p>\n<p> - <strong>进展价值模型</strong>：参数化自预训练的 VLA 策略 <strong>π₀.₅</strong>，通过结合 <strong>进度回归损失</strong> 和 <strong>时间差分学习损失</strong> 进行训练，为想象的轨迹提供密集且对失败敏感的奖励信号。</p>\n<ul><li>系统将策略提出的动作序列输入世界模型，合成未来视觉状态并计算优势值，然后通过优势条件训练（Advantage-Conditioned Training）更新策略，整个过程在想象空间中进行，无需物理交互。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用的数据集包括：大规模机器人数据集 <strong>Galaxea</strong> 和 <strong>Agibot World</strong>（用于动力学模型预训练），以及为特定任务（动态积木分拣、背包打包、盒子封装）收集的离线真实世界经验数据。</li><li><strong>动力学模型</strong> 基于 <strong>Genie Envisioner (GE-base)</strong> 架构；<strong>价值模型</strong> 基于 <strong>π₀.₅</strong> VLA 模型；策略模型为基于流匹配的 <strong>VLA</strong>。</li><li>训练资源：动力学模型预训练使用 <strong>16 块 NVIDIA H100 GPU</strong>（约7天），任务特定微调使用 <strong>8 块 H100 GPU</strong>（约3天）；价值模型和策略训练均使用 <strong>8 块 H100 GPU</strong>。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境与基准</strong>：在真实世界的三个挑战性任务上评估——<strong>动态积木分拣</strong>、<strong>背包打包</strong> 和 <strong>盒子封装</strong>。对比了包括 <strong>π₀.₅</strong>、<strong>DAgger</strong>、<strong>PPO</strong>、<strong>DSRL</strong> 和 <strong>RECAP</strong> 在内的多种基线方法。</li><li><strong>主要评估指标</strong>：任务<strong>成功率（%）</strong> 和多阶段<strong>评分</strong>。</li><li><strong>关键实验结果</strong>：<strong>RISE</strong> 在所有任务上均取得显著优势。在<strong>动态积木分拣</strong>任务上，成功率从 RECAP 的 50% 提升至 <strong>85%</strong>；在<strong>背包打包</strong>任务上，从 RECAP 的 40% 提升至 <strong>85%</strong>；在<strong>盒子封装</strong>任务上，从 RECAP 的 60% 提升至 <strong>95%</strong>。</li><li><strong>消融实验</strong>：验证了各组件（如任务中心批处理、价值模型的混合损失、在线动作与状态生成）的有效性，并表明最佳离线数据混合比例约为 <strong>0.6</strong>。</li><li><strong>世界模型质量</strong>：在 PSNR、LPIPS、SSIM、FVD 和端点误差（EPE）等指标上，其动力学模型均优于 Cosmos 和基础 Genie Envisioner。</li></ul>"
  },
  {
    "date": "2026-02-11",
    "title": "Scaling World Model for Hierarchical Manipulation Policies",
    "link": "http://arxiv.org/abs/2602.10983",
    "summary_markdown": "## 研究单位\n- **西安交通大学** (Xi’an Jiao Tong University)\n- **北京人工智能研究院** (Beijing Academy of Artificial Intelligence)\n- **清华大学** (Tsinghua University)\n- **新加坡国立大学** (National University of Singapore)\n- **中国科学院自动化研究所** (Institute of Automation, Chinese Academy of Sciences)\n## 论文概述\n- 提出了一种名为 **VISTA** 的层次化视觉-语言-动作框架，旨在解决视觉-语言-动作模型在分布外（OOD）场景下泛化能力差的问题。\n- 该方法利用大规模预训练的**世界模型**作为高层规划器，将长时程操作任务分解为包含文本子任务和视觉目标图像的序列，并由低层执行器（目标条件化VLA）依此生成精确动作。\n- 核心创新是使用**视觉目标图像**（而非仅文本）作为任务分解的中间表示，为低层策略提供视觉和物理层面的精确约束，从而显著提升数据效率和模型在未见过的对象与场景中的鲁棒性。\n## 核心贡献\n- 提出了一个可扩展的数据处理流水线，将数百万条机器人轨迹自动重新标注为**文本子任务与视觉目标图像交错**的格式。\n- 设计了一个**生成式具身世界模型**，能够生成物理一致和多视图一致的视觉子目标，用于指导操作任务。\n- 提出了 **VISTA** 框架，这是一个包含生成式世界模型和目标图像条件化策略的层次化系统，在分布外场景下显著优于现有基线方法。\n- 引入了一种**目标条件化的视觉-语言-动作策略**，能够融合当前观察、文本子任务和视觉目标图像，生成动作序列，并提出了子任务感知动作填充和随机目标图像偏移等技术以增强鲁棒性。\n## 方法描述\n- 采用**两层架构**：高层是一个预训练的**世界模型规划器**，它将全局语言指令和初始观测作为输入，自回归地生成一个交错包含文本子任务和视觉目标图像的多模态序列，作为任务的分解。\n- 低层是一个**目标条件化VLA执行器**，它将当前观测、当前子任务的文本描述及其对应的视觉目标图像作为输入，预测动作块。\n- 关键技术包括：将文本和图像统一表示为离散令牌序列的自回归训练；用于融合多视图观测和目标图像的模型架构；以及增强训练鲁棒性的**子任务感知动作填充**和**随机目标图像偏移**。\n- 世界模型基于开源的 **EMU3.5** 进行微调，目标VLA基于 **π0** 架构进行流匹配训练。\n## 数据集与资源\n- 使用大规模具身操作数据集进行训练，包括 **Open-X-Embodiment**、**AgiBot World Beta** 和自有的 **Mobile Aloha** 数据集。\n- 通过自动化流水线处理了超过 **120万条轨迹**，构建了包含交错文本子任务和视觉目标图像的数据集，总计 **152亿个令牌**。\n- 还混合使用了包括 **SEED-Data-Edit**、**WeatherStream** 等在内的 **150亿令牌的 Any-to-Image** 通用数据以增强世界模型的生成能力。\n- 在现实世界实验中，仅收集了**2小时、涉及5个物体的遥操作数据**来训练低层VLA策略。\n- 模型训练使用了 **GPU** 集群。\n## 评估与结果\n- **评估环境**：在真实机器人（Mobile Aloha）上进行多阶段拾放任务评估，重点测试在**分布外场景**的泛化能力，包括未见过的干扰物、目标物体、背景和桌面布局。\n- **评估指标**：主要使用**任务成功率**，并细分为接近成功率和执行成功率。\n- **关键结果**：\n - 在仅用2小时真实数据训练的情况下，**VISTA** 在处理21个未见物体的新颖场景中取得了 **69%** 的成功率。\n - 相比之下，仅使用语言指令指导的基线 **π0** 模型成功率仅为 **14%**，而使用文本子任务指导的变体 **π0-subtask** 也仅达到 **31%**。\n - 世界模型生成的视觉目标图像在多视图、物理一致性和跨本体泛化方面表现出色，显著提升了低层策略的执行精度和鲁棒性。\n - 结果表明，该方法在分布外场景下性能优势明显，特别是在处理未见物体时。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<ul><li><strong>西安交通大学</strong> (Xi’an Jiao Tong University)</li><li><strong>北京人工智能研究院</strong> (Beijing Academy of Artificial Intelligence)</li><li><strong>清华大学</strong> (Tsinghua University)</li><li><strong>新加坡国立大学</strong> (National University of Singapore)</li><li><strong>中国科学院自动化研究所</strong> (Institute of Automation, Chinese Academy of Sciences)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>提出了一种名为 <strong>VISTA</strong> 的层次化视觉-语言-动作框架，旨在解决视觉-语言-动作模型在分布外（OOD）场景下泛化能力差的问题。</li><li>该方法利用大规模预训练的<strong>世界模型</strong>作为高层规划器，将长时程操作任务分解为包含文本子任务和视觉目标图像的序列，并由低层执行器（目标条件化VLA）依此生成精确动作。</li><li>核心创新是使用<strong>视觉目标图像</strong>（而非仅文本）作为任务分解的中间表示，为低层策略提供视觉和物理层面的精确约束，从而显著提升数据效率和模型在未见过的对象与场景中的鲁棒性。</li></ul>\n<h2 class=\"section-title\">核心贡献</h2>\n<ul><li>提出了一个可扩展的数据处理流水线，将数百万条机器人轨迹自动重新标注为<strong>文本子任务与视觉目标图像交错</strong>的格式。</li><li>设计了一个<strong>生成式具身世界模型</strong>，能够生成物理一致和多视图一致的视觉子目标，用于指导操作任务。</li><li>提出了 <strong>VISTA</strong> 框架，这是一个包含生成式世界模型和目标图像条件化策略的层次化系统，在分布外场景下显著优于现有基线方法。</li><li>引入了一种<strong>目标条件化的视觉-语言-动作策略</strong>，能够融合当前观察、文本子任务和视觉目标图像，生成动作序列，并提出了子任务感知动作填充和随机目标图像偏移等技术以增强鲁棒性。</li></ul>\n<h2 class=\"section-title\">方法描述</h2>\n<ul><li>采用<strong>两层架构</strong>：高层是一个预训练的<strong>世界模型规划器</strong>，它将全局语言指令和初始观测作为输入，自回归地生成一个交错包含文本子任务和视觉目标图像的多模态序列，作为任务的分解。</li><li>低层是一个<strong>目标条件化VLA执行器</strong>，它将当前观测、当前子任务的文本描述及其对应的视觉目标图像作为输入，预测动作块。</li><li>关键技术包括：将文本和图像统一表示为离散令牌序列的自回归训练；用于融合多视图观测和目标图像的模型架构；以及增强训练鲁棒性的<strong>子任务感知动作填充</strong>和<strong>随机目标图像偏移</strong>。</li><li>世界模型基于开源的 <strong>EMU3.5</strong> 进行微调，目标VLA基于 <strong>π0</strong> 架构进行流匹配训练。</li></ul>\n<h2 class=\"section-title\">数据集与资源</h2>\n<ul><li>使用大规模具身操作数据集进行训练，包括 <strong>Open-X-Embodiment</strong>、<strong>AgiBot World Beta</strong> 和自有的 <strong>Mobile Aloha</strong> 数据集。</li><li>通过自动化流水线处理了超过 <strong>120万条轨迹</strong>，构建了包含交错文本子任务和视觉目标图像的数据集，总计 <strong>152亿个令牌</strong>。</li><li>还混合使用了包括 <strong>SEED-Data-Edit</strong>、<strong>WeatherStream</strong> 等在内的 <strong>150亿令牌的 Any-to-Image</strong> 通用数据以增强世界模型的生成能力。</li><li>在现实世界实验中，仅收集了<strong>2小时、涉及5个物体的遥操作数据</strong>来训练低层VLA策略。</li><li>模型训练使用了 <strong>GPU</strong> 集群。</li></ul>\n<h2 class=\"section-title\">评估与结果</h2>\n<ul><li><strong>评估环境</strong>：在真实机器人（Mobile Aloha）上进行多阶段拾放任务评估，重点测试在<strong>分布外场景</strong>的泛化能力，包括未见过的干扰物、目标物体、背景和桌面布局。</li><li><strong>评估指标</strong>：主要使用<strong>任务成功率</strong>，并细分为接近成功率和执行成功率。</li><li><strong>关键结果</strong>：</li></ul>\n<p> - 在仅用2小时真实数据训练的情况下，<strong>VISTA</strong> 在处理21个未见物体的新颖场景中取得了 <strong>69%</strong> 的成功率。</p>\n<p> - 相比之下，仅使用语言指令指导的基线 <strong>π0</strong> 模型成功率仅为 <strong>14%</strong>，而使用文本子任务指导的变体 <strong>π0-subtask</strong> 也仅达到 <strong>31%</strong>。</p>\n<p> - 世界模型生成的视觉目标图像在多视图、物理一致性和跨本体泛化方面表现出色，显著提升了低层策略的执行精度和鲁棒性。</p>\n<p> - 结果表明，该方法在分布外场景下性能优势明显，特别是在处理未见物体时。</p>"
  },
  {
    "date": "2026-02-11",
    "title": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
    "link": "http://arxiv.org/abs/2602.10980",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-11",
    "title": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
    "link": "http://arxiv.org/abs/2602.10719",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-11",
    "title": "AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.10698",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-11",
    "title": "LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer",
    "link": "http://arxiv.org/abs/2602.10556",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-11",
    "title": "Towards Long-Lived Robots: Continual Learning VLA Models via Reinforcement Fine-Tuning",
    "link": "http://arxiv.org/abs/2602.10503",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs",
    "link": "http://arxiv.org/abs/2602.10377",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "ST4VLA: Spatially Guided Training for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.10109",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
    "link": "http://arxiv.org/abs/2602.10098",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking",
    "link": "http://arxiv.org/abs/2602.10093",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation",
    "link": "http://arxiv.org/abs/2602.09973",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
    "link": "http://arxiv.org/abs/2602.09849",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization",
    "link": "http://arxiv.org/abs/2602.09722",
    "summary_markdown": "### 论文研究单位\n中国人民大学 (Renmin University of China)，BeingBeyond，北京大学 (Peking University)\n### 论文概述\n本研究对视觉-语言-动作模型在大规模异质机器人数据上扩展的有效性进行了系统的、受控的实验研究。研究挑战了“简单扩大数据规模就能提升泛化能力”的普遍假设，深入探讨了三个核心设计维度（物理对齐、本体数据混合、训练正则化）对VLA模型性能的影响。论文构建了一个基于混合专家架构和流匹配动作生成的代表性VLA框架作为受控测试平台，并提出了一个分组盲集协议以减少真实世界评估中的实验者偏差。\n### 论文核心贡献点\n1. 识别出末端执行器相对坐标动作表示对于跨本体迁移学习至关重要，提供了对通用策略更有优势的动作参数化选择。\n2. 揭示了在VLA预训练中，简单混合异质机器人数据常常导致性能下降而非提升，挑战了在机器人领域简单放大数据规模的有效性。\n3. 实证表明，一些直观的训练正则化策略（如感官丢弃、多阶段微调）在大规模训练中并不能稳定地带来性能增益。\n4. 提出了一个分组盲集评估协议，通过将模型身份匿名化和分离策略执行与结果判定，显著减少了真实机器人实验中的实验者偏差，为可靠的VLA评估提供了方法。\n### 论文方法描述\n1. **模型架构**: 采用混合专家架构，包含一个从预训练VLM初始化的语义专家和一个随机初始化用于控制的任务专家。两个专家通过逐层共享的自注意力机制进行跨模态交互。\n2. **动作生成**: 使用流匹配方法对动作片段进行建模，将动作生成视为基于多模态上下文的条件分布，以实现平滑的轨迹生成。\n3. **物理对齐的统一动作空间**: 定义了一个分区的、物理接地的统一动作空间，包含末端执行器姿态、关节、夹持器等语义对齐的子空间。研究了四种末端执行器姿态的坐标参数化：世界相对、世界增量、EEF相对、EEF增量。\n4. **分组盲集评估协议**: 将待评估模型池随机分成不重叠的小组，对每个组内的模型进行匿名化，并在任务中以随机顺序执行策略，实验者不知晓所用模型的身份，仅作为执行者记录成功/失败结果，以降低人类偏好和疲劳带来的评估偏差。\n### 论文使用数据集和训练资源\n1. **数据集**: 构建了一个大规模异质机器人数据集混合体，总计约1.8亿帧数据转换。数据集来源包括：\n * **真实世界EEF数据**: Open X-Embodiment (DROID, Bridge, Fractal等子集)，AgiBot (夹持器和灵巧手)，RoboMind (多平台)。\n * **仿真和关节空间数据**: InternData (仿真和灵巧手)，SO-100 (低成本遥操作)，真实世界人形数据集。\n * **数据平衡**: 采用动态下采样策略以平衡各数据源的规模差异，防止大型数据集主导梯度更新。\n2. **训练资源**: 所有实验在8个NVIDIA A800 GPU上进行，全局批次大小为256。\n### 论文使用的评估环境和评估指标\n1. **评估环境**:\n * **仿真基准**: LIBERO (多任务套件，5样本微调协议) 和 RoboCasa (24个厨房任务，50样本微调协议)。\n * **真实机器人实验**: 配备两个RGB摄像头的Franka Panda机械臂。定义了四个任务：堆叠碗、拾取放入抽屉、擦黑板、浇灌植物，以测试不同能力。\n2. **评估指标**:\n * **仿真**: 任务成功率。对LIBERO，报告了空间、物体、目标和长序列四个子任务的分数及平均成功率。对RoboCasa，报告了拾取/放置、门/抽屉、其他三类任务的分数及平均成功率。\n * **真实世界**: 使用详细的多步骤评分标准对每个任务进行评分（如堆叠碗最高3分），每个任务进行10次试验，计算总分并换算为百分比。评估在分组盲集协议下进行。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中国人民大学 (Renmin University of China)，BeingBeyond，北京大学 (Peking University)</p>\n<h3>论文概述</h3>\n<p>本研究对视觉-语言-动作模型在大规模异质机器人数据上扩展的有效性进行了系统的、受控的实验研究。研究挑战了“简单扩大数据规模就能提升泛化能力”的普遍假设，深入探讨了三个核心设计维度（物理对齐、本体数据混合、训练正则化）对VLA模型性能的影响。论文构建了一个基于混合专家架构和流匹配动作生成的代表性VLA框架作为受控测试平台，并提出了一个分组盲集协议以减少真实世界评估中的实验者偏差。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>识别出末端执行器相对坐标动作表示对于跨本体迁移学习至关重要，提供了对通用策略更有优势的动作参数化选择。</li><li>揭示了在VLA预训练中，简单混合异质机器人数据常常导致性能下降而非提升，挑战了在机器人领域简单放大数据规模的有效性。</li><li>实证表明，一些直观的训练正则化策略（如感官丢弃、多阶段微调）在大规模训练中并不能稳定地带来性能增益。</li><li>提出了一个分组盲集评估协议，通过将模型身份匿名化和分离策略执行与结果判定，显著减少了真实机器人实验中的实验者偏差，为可靠的VLA评估提供了方法。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>模型架构</strong>: 采用混合专家架构，包含一个从预训练VLM初始化的语义专家和一个随机初始化用于控制的任务专家。两个专家通过逐层共享的自注意力机制进行跨模态交互。</li><li><strong>动作生成</strong>: 使用流匹配方法对动作片段进行建模，将动作生成视为基于多模态上下文的条件分布，以实现平滑的轨迹生成。</li><li><strong>物理对齐的统一动作空间</strong>: 定义了一个分区的、物理接地的统一动作空间，包含末端执行器姿态、关节、夹持器等语义对齐的子空间。研究了四种末端执行器姿态的坐标参数化：世界相对、世界增量、EEF相对、EEF增量。</li><li><strong>分组盲集评估协议</strong>: 将待评估模型池随机分成不重叠的小组，对每个组内的模型进行匿名化，并在任务中以随机顺序执行策略，实验者不知晓所用模型的身份，仅作为执行者记录成功/失败结果，以降低人类偏好和疲劳带来的评估偏差。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>: 构建了一个大规模异质机器人数据集混合体，总计约1.8亿帧数据转换。数据集来源包括：</li></ol>\n<p> * <strong>真实世界EEF数据</strong>: Open X-Embodiment (DROID, Bridge, Fractal等子集)，AgiBot (夹持器和灵巧手)，RoboMind (多平台)。</p>\n<p> * <strong>仿真和关节空间数据</strong>: InternData (仿真和灵巧手)，SO-100 (低成本遥操作)，真实世界人形数据集。</p>\n<p> * <strong>数据平衡</strong>: 采用动态下采样策略以平衡各数据源的规模差异，防止大型数据集主导梯度更新。</p>\n<ol><li><strong>训练资源</strong>: 所有实验在8个NVIDIA A800 GPU上进行，全局批次大小为256。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>:</li></ol>\n<p> * <strong>仿真基准</strong>: LIBERO (多任务套件，5样本微调协议) 和 RoboCasa (24个厨房任务，50样本微调协议)。</p>\n<p> * <strong>真实机器人实验</strong>: 配备两个RGB摄像头的Franka Panda机械臂。定义了四个任务：堆叠碗、拾取放入抽屉、擦黑板、浇灌植物，以测试不同能力。</p>\n<ol><li><strong>评估指标</strong>:</li></ol>\n<p> * <strong>仿真</strong>: 任务成功率。对LIBERO，报告了空间、物体、目标和长序列四个子任务的分数及平均成功率。对RoboCasa，报告了拾取/放置、门/抽屉、其他三类任务的分数及平均成功率。</p>\n<p> * <strong>真实世界</strong>: 使用详细的多步骤评分标准对每个任务进行评分（如堆叠碗最高3分），每个任务进行10次试验，计算总分并换算为百分比。评估在分组盲集协议下进行。</p>"
  },
  {
    "date": "2026-02-10",
    "title": "AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild",
    "link": "http://arxiv.org/abs/2602.09657",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments",
    "link": "http://arxiv.org/abs/2602.09430",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-10",
    "title": "CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments",
    "link": "http://arxiv.org/abs/2602.09367",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-09",
    "title": "TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation",
    "link": "http://arxiv.org/abs/2602.09023",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-09",
    "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
    "link": "http://arxiv.org/abs/2602.09017",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-09",
    "title": "Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications",
    "link": "http://arxiv.org/abs/2602.08822",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-09",
    "title": "Mimic Intent, Not Just Trajectories",
    "link": "http://arxiv.org/abs/2602.08602",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-09",
    "title": "SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios",
    "link": "http://arxiv.org/abs/2602.08440",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-09",
    "title": "Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning",
    "link": "http://arxiv.org/abs/2602.08167",
    "summary_markdown": "论文研究单位：\n论文作者来自多个机构，但根据姓名后面的符号（♯, ♭, ♮）及脚注\"Equal contribution\"，推断他们应隶属于不同的研究单位/机构，但原文未明确列出。从作者姓氏和上下文推测，可能与斯坦福大学等机构有关联，但无法从提供的HTML片段中确定具体单位。\n\n论文概述：\n这篇论文提出了名为R&B-EnCoRe的方法，旨在解决Vision-Language-Action模型在具身推理（Embodied CoT）中的一个关键瓶颈：目前的方法依赖于固定的模板来指定推理原语（如场景对象、高层计划），这些模板可能迫使策略处理与关键动作预测信号无关的干扰信息。论文将具身推理视为一个潜变量，并基于重要性加权变分推断，使模型能够从互联网规模的知识中通过自监督的方式引导（bootstrap）出高质量的、针对特定具身的推理策略，而无需外部奖励、验证器或人工标注。该方法通过在多种具身（仿真和硬件中的机器人操作、足式/轮式/自行车/四足机器人导航、自动驾驶）上进行验证，展示了其有效性。\n\n论文核心贡献点：\n1. 引入了R&B-EnCoRe框架，它将具身推理公式化为潜变量，并提供了一种基于重要性加权变分推断的自监督训练方法，能够从先验知识中精炼并引导出高质量的推理策略，无需手工设计的启发式规则或昂贵的人工标注。\n2. 证明了R&B-EnCoRe能够有效过滤掉“干扰”信息（例如任务无关物体的边界框），同时放大关键信号（例如足式机器人的结构可供性）。该过程产生了高质量、可解释的推理轨迹，无需外部启发式规则、奖励或验证器即可区分关键视觉线索与冗余噪声。\n3. 在多种具身任务（机器人操作、足式导航、自动驾驶）上使用多种VLA架构（参数量分别为1B, 4B, 7B, 30B）验证了R&B-EnCoRe。相比于不加区分地使用所有推理原语的基线模型，R&B-EnCoRe在操作任务成功率上提升了28%，在导航分数上提升了101%，在自动驾驶碰撞率指标上降低了21%。\n\n论文方法描述：\nR&B-EnCoRe框架分为三个阶段：\n1. 通过推理Dropout热身策略假设：给定一组潜在的推理原语集合（如计划、可见物体、子任务等），使用基础模型为每个训练数据样本生成所有原语的文本解释。然后，通过Dropout机制（每个原语以固定概率被随机丢弃）生成多样化的推理轨迹组合，形成热身数据集。这使得模型能够接触各种推理策略（从简洁到详尽）。\n2. 联合训练先验和后验分布：使用热身数据集同时训练一个VLA模型，使其同时扮演先验和后验的角色。先验模型p(Z, A\\|C)训练在序列（Z_j, A\\|C）上，学习根据上下文C生成推理Z_j和动作A（类似于在线策略）。后验模型q(Z\\|C, A)训练在序列（Z_j\\|C, A）上，学习在给定上下文C和真实动作A的条件下，生成能解释该动作的多样推理Z_j（作为提案分布）。\n3. 通过重要性采样进行精炼和引导（Bootstrapping）：\n - 精炼阶段：对于每个演示数据(C, A)，从后验模型q中采样K个候选推理轨迹。为每个轨迹计算重要性权重 w(Z_k) = p(Z_k, A\\|C) / q(Z_k\\|C, A)，其中p来自先验模型。权重反映了该推理轨迹对预测正确动作的信息增益（信息效益）。然后根据权重进行加权采样，选出一个精炼后的轨迹Z*。\n - 引导阶段：使用精炼后的高质量数据集 { (Z*, A\\|C) } 重新训练最终的VLA模型。这使得模型能够从与任务成功对齐的推理示例中学习，从而提升性能。\n\n论文使用数据集和训练资源：\n数据集：\n1. LIBERO-90：用于Franka Panda机器人仿真操作的基准数据集。\n2. Bridge v2 Dataset：用于WidowX硬件机器人操作的真实世界数据集。\n3. NaviTrace Dataset：包含约1000个任务、500个独特场景，用于四种具身（双足、轮式、自行车、四足）导航的数据集。\n4. 自动驾驶实验使用的数据集未在提供片段中明确命名，但提及使用UniAD指标进行评估。\n训练资源：\n使用了多种VLA模型架构：\n1. MiniVLA（1B参数）：结合了0.5B Qwen2.5 LLM、DINOv2和SigLIP视觉编码器。\n2. OpenVLA（7B参数）：结合了Llama 2、DINOv2和SigLIP视觉编码器。\n3. Qwen3-VL-30B-A3B-Instruct（30B参数）：一个混合专家模型。\n用于生成推理原语的基础模型包括Gemini 1.0等。\n\n论文使用的评估环境和评估指标：\n评估环境：\n1. 仿真环境（LIBERO-90, NaviTrace场景）。\n2. 真实硬件环境（WidowX机器人）。\n3. 自动驾驶仿真环境（具体未命名）。\n评估指标：\n1. 操作任务：成功率（Success Rate）。\n2. 导航任务：NaviTrace分数（一个结合了动态时间规整距离、目标端点误差和语义惩罚的综合指标，100分为完美对齐专家路径）。\n3. 自动驾驶任务：L2路径误差（米）和碰撞率（%），按1秒、2秒、3秒预测时域和平均值报告。\n4. 辅助指标：对象关键性率（Object Criticality Rate，推理轨迹中所有列出的物体都是任务关键的比例）、平均生成令牌数（推理轨迹的简洁性）、推理延迟（秒/步）。",
    "summary_html": "<p>论文研究单位：</p>\n<p>论文作者来自多个机构，但根据姓名后面的符号（♯, ♭, ♮）及脚注\"Equal contribution\"，推断他们应隶属于不同的研究单位/机构，但原文未明确列出。从作者姓氏和上下文推测，可能与斯坦福大学等机构有关联，但无法从提供的HTML片段中确定具体单位。</p>\n\n<p>论文概述：</p>\n<p>这篇论文提出了名为R&B-EnCoRe的方法，旨在解决Vision-Language-Action模型在具身推理（Embodied CoT）中的一个关键瓶颈：目前的方法依赖于固定的模板来指定推理原语（如场景对象、高层计划），这些模板可能迫使策略处理与关键动作预测信号无关的干扰信息。论文将具身推理视为一个潜变量，并基于重要性加权变分推断，使模型能够从互联网规模的知识中通过自监督的方式引导（bootstrap）出高质量的、针对特定具身的推理策略，而无需外部奖励、验证器或人工标注。该方法通过在多种具身（仿真和硬件中的机器人操作、足式/轮式/自行车/四足机器人导航、自动驾驶）上进行验证，展示了其有效性。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>引入了R&B-EnCoRe框架，它将具身推理公式化为潜变量，并提供了一种基于重要性加权变分推断的自监督训练方法，能够从先验知识中精炼并引导出高质量的推理策略，无需手工设计的启发式规则或昂贵的人工标注。</li><li>证明了R&B-EnCoRe能够有效过滤掉“干扰”信息（例如任务无关物体的边界框），同时放大关键信号（例如足式机器人的结构可供性）。该过程产生了高质量、可解释的推理轨迹，无需外部启发式规则、奖励或验证器即可区分关键视觉线索与冗余噪声。</li><li>在多种具身任务（机器人操作、足式导航、自动驾驶）上使用多种VLA架构（参数量分别为1B, 4B, 7B, 30B）验证了R&B-EnCoRe。相比于不加区分地使用所有推理原语的基线模型，R&B-EnCoRe在操作任务成功率上提升了28%，在导航分数上提升了101%，在自动驾驶碰撞率指标上降低了21%。</li></ol>\n\n<p>论文方法描述：</p>\n<p>R&B-EnCoRe框架分为三个阶段：</p>\n<ol><li>通过推理Dropout热身策略假设：给定一组潜在的推理原语集合（如计划、可见物体、子任务等），使用基础模型为每个训练数据样本生成所有原语的文本解释。然后，通过Dropout机制（每个原语以固定概率被随机丢弃）生成多样化的推理轨迹组合，形成热身数据集。这使得模型能够接触各种推理策略（从简洁到详尽）。</li><li>联合训练先验和后验分布：使用热身数据集同时训练一个VLA模型，使其同时扮演先验和后验的角色。先验模型p(Z, A\\|C)训练在序列（Z_j, A\\|C）上，学习根据上下文C生成推理Z_j和动作A（类似于在线策略）。后验模型q(Z\\|C, A)训练在序列（Z_j\\|C, A）上，学习在给定上下文C和真实动作A的条件下，生成能解释该动作的多样推理Z_j（作为提案分布）。</li><li>通过重要性采样进行精炼和引导（Bootstrapping）：</li></ol>\n<p> - 精炼阶段：对于每个演示数据(C, A)，从后验模型q中采样K个候选推理轨迹。为每个轨迹计算重要性权重 w(Z_k) = p(Z_k, A\\|C) / q(Z_k\\|C, A)，其中p来自先验模型。权重反映了该推理轨迹对预测正确动作的信息增益（信息效益）。然后根据权重进行加权采样，选出一个精炼后的轨迹Z*。</p>\n<p> - 引导阶段：使用精炼后的高质量数据集 { (Z*, A\\|C) } 重新训练最终的VLA模型。这使得模型能够从与任务成功对齐的推理示例中学习，从而提升性能。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>数据集：</p>\n<ol><li>LIBERO-90：用于Franka Panda机器人仿真操作的基准数据集。</li><li>Bridge v2 Dataset：用于WidowX硬件机器人操作的真实世界数据集。</li><li>NaviTrace Dataset：包含约1000个任务、500个独特场景，用于四种具身（双足、轮式、自行车、四足）导航的数据集。</li><li>自动驾驶实验使用的数据集未在提供片段中明确命名，但提及使用UniAD指标进行评估。</li></ol>\n<p>训练资源：</p>\n<p>使用了多种VLA模型架构：</p>\n<ol><li>MiniVLA（1B参数）：结合了0.5B Qwen2.5 LLM、DINOv2和SigLIP视觉编码器。</li><li>OpenVLA（7B参数）：结合了Llama 2、DINOv2和SigLIP视觉编码器。</li><li>Qwen3-VL-30B-A3B-Instruct（30B参数）：一个混合专家模型。</li></ol>\n<p>用于生成推理原语的基础模型包括Gemini 1.0等。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<ol><li>仿真环境（LIBERO-90, NaviTrace场景）。</li><li>真实硬件环境（WidowX机器人）。</li><li>自动驾驶仿真环境（具体未命名）。</li></ol>\n<p>评估指标：</p>\n<ol><li>操作任务：成功率（Success Rate）。</li><li>导航任务：NaviTrace分数（一个结合了动态时间规整距离、目标端点误差和语义惩罚的综合指标，100分为完美对齐专家路径）。</li><li>自动驾驶任务：L2路径误差（米）和碰撞率（%），按1秒、2秒、3秒预测时域和平均值报告。</li><li>辅助指标：对象关键性率（Object Criticality Rate，推理轨迹中所有列出的物体都是任务关键的比例）、平均生成令牌数（推理轨迹的简洁性）、推理延迟（秒/步）。</li></ol>"
  },
  {
    "date": "2026-02-08",
    "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
    "link": "http://arxiv.org/abs/2602.07845",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-08",
    "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
    "link": "http://arxiv.org/abs/2602.07837",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-07",
    "title": "VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation",
    "link": "http://arxiv.org/abs/2602.07555",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-07",
    "title": "Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning",
    "link": "http://arxiv.org/abs/2602.07541",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-07",
    "title": "VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation",
    "link": "http://arxiv.org/abs/2602.07399",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-06",
    "title": "Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique",
    "link": "http://arxiv.org/abs/2602.06620",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-06",
    "title": "Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation",
    "link": "http://arxiv.org/abs/2602.06575",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-06",
    "title": "LIBERO-X: Robustness Litmus for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.06556",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-06",
    "title": "DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving",
    "link": "http://arxiv.org/abs/2602.06521",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-06",
    "title": "Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2602.06512",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-06",
    "title": "World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy",
    "link": "http://arxiv.org/abs/2602.06508",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-05",
    "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
    "link": "http://arxiv.org/abs/2602.05765",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-05",
    "title": "Benchmarking Affordance Generalization with BusyBox",
    "link": "http://arxiv.org/abs/2602.05441",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-05",
    "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
    "link": "http://arxiv.org/abs/2602.05325",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-05",
    "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
    "link": "http://arxiv.org/abs/2602.05233",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-04",
    "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.05049",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-04",
    "title": "Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data",
    "link": "http://arxiv.org/abs/2602.04600",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-04",
    "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
    "link": "http://arxiv.org/abs/2602.04315",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-04",
    "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.04228",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-04",
    "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.04208",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-03",
    "title": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement",
    "link": "http://arxiv.org/abs/2602.03983",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-03",
    "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
    "link": "http://arxiv.org/abs/2602.03782",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-03",
    "title": "MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction",
    "link": "http://arxiv.org/abs/2602.03668",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-03",
    "title": "CRL-VLA: Continual Vision-Language-Action Learning",
    "link": "http://arxiv.org/abs/2602.03445",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-03",
    "title": "RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization",
    "link": "http://arxiv.org/abs/2602.03310",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-03",
    "title": "When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens",
    "link": "http://arxiv.org/abs/2602.03153",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-02",
    "title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments",
    "link": "http://arxiv.org/abs/2602.02459",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-02",
    "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
    "link": "http://arxiv.org/abs/2602.02454",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-02",
    "title": "MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.02212",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-02",
    "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
    "link": "http://arxiv.org/abs/2602.02142",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-02",
    "title": "Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models",
    "link": "http://arxiv.org/abs/2602.01834",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-02",
    "title": "From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models",
    "link": "http://arxiv.org/abs/2602.01811",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-01",
    "title": "Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.01166",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-01",
    "title": "Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs",
    "link": "http://arxiv.org/abs/2602.01158",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-02-01",
    "title": "StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating",
    "link": "http://arxiv.org/abs/2602.01100",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-31",
    "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
    "link": "http://arxiv.org/abs/2602.00919",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-31",
    "title": "Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds",
    "link": "http://arxiv.org/abs/2602.00807",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-31",
    "title": "Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2602.00780",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-31",
    "title": "SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning",
    "link": "http://arxiv.org/abs/2602.00743",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-31",
    "title": "Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching",
    "link": "http://arxiv.org/abs/2602.00686",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-31",
    "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2602.00557",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-31",
    "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
    "link": "http://arxiv.org/abs/2602.00500",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-30",
    "title": "CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control",
    "link": "http://arxiv.org/abs/2601.22467",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-29",
    "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
    "link": "http://arxiv.org/abs/2601.22153",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-29",
    "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
    "link": "http://arxiv.org/abs/2601.22054",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-29",
    "title": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts",
    "link": "http://arxiv.org/abs/2601.21971",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-29",
    "title": "CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation",
    "link": "http://arxiv.org/abs/2601.21712",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-29",
    "title": "AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation",
    "link": "http://arxiv.org/abs/2601.21602",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-29",
    "title": "IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation",
    "link": "http://arxiv.org/abs/2601.21506",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-28",
    "title": "Demonstration-Free Robotic Control via LLM Agents",
    "link": "http://arxiv.org/abs/2601.20334",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-28",
    "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
    "link": "http://arxiv.org/abs/2601.20321",
    "summary_markdown": "### 论文研究单位\n北京通用人工智能研究院、北京航空航天大学、上海科技大学、香港大学。\n### 论文概述\n当前视觉-语言-动作模型主要依赖视觉模态，缺乏处理接触丰富任务所需的精确力调节和物理直觉。本文针对此“力盲”问题，提出了从“触觉-视觉对齐”到“触觉-力对齐”的范式转变。论文介绍了TaF-VLA框架，该框架通过显式地将高维触觉观测与物理相互作用力关联起来，以实现力感知操作。核心贡献包括一个自动化触觉-力数据采集设备及由此构建的大规模TaF数据集、一个名为TaF-Adapter的触觉-力对齐模块，以及将该模块集成到VLA主干网络中形成的TaF-VLA策略。实验表明，TaF-VLA在接触丰富的任务上显著优于现有的触觉-视觉对齐及纯视觉基线模型。\n### 论文核心贡献点\n1. 设计并构建了一个低成本、自动化的触觉-力数据采集设备（TaF-Device）及相应的大规模数据集（TaF-Dataset），解决了触觉-力对齐数据稀缺问题。\n2. 提出了触觉-力对齐器（TaF-Adapter），该模块通过对比学习，将时序触觉观测映射到一个与力信号对齐的共享潜在空间，捕获历史依赖的接触动力学，并具有抗噪和跨传感器鲁棒性。\n3. 开发了TaF-VLA模型，将预训练并冻结的TaF-Adapter集成到预训练的VLA骨干网络中，形成了一个能够执行复杂力感知操作任务的策略。\n### 论文方法描述\n1. **数据采集与数据集构建**：设计了一个具有双平台、同步驱动结构的自动化设备。该设备能对安装在两个平台上的触觉传感器和力传感器施加相同大小和方向的力，从而同步采集视觉触觉图像、6轴力/力矩和矩阵力分布图。通过更换不同硬度、几何形状的压头以及不同的视觉触觉传感器，构建了包含超过1000万帧同步数据的大规模TaF-Dataset。\n2. **触觉-力对齐（TaF-Adapter）**：\n - **力量化**：使用两个向量量化变分自编码器，分别将时序的矩阵压力图和6轴力/力矩向量量化为离散的代码本，以此作为物理力的稳定锚点。\n - **时序触觉编码**：使用视觉变换器提取单帧触觉图像特征，然后通过一个因果变换器聚合时序窗口内的历史观测，生成一个综合的触觉表征。\n - **跨模态对齐**：通过对比学习（InfoNCE损失）在共享潜在空间中对齐触觉表征和对应的量化力代码，使触觉编码器能从视觉变形中推断力动态。\n3. **策略集成（TaF-VLA）**：将预训练并冻结的TaF-Adapter集成到基于流匹配的VLA骨干网络（π₀.₅）中。力对齐的触觉表征与全局视觉-语言特征、机器人本体感知信息一同作为条件，用于通过流匹配学习生成动作轨迹。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **TaF-Dataset**：使用自研设备采集，包含6种不同的视觉触觉传感器数据。总计超过1000万帧同步的触觉观测、6轴力/力矩和矩阵力图对。\n - **策略训练数据集**：包含超过10,000个力感知操作演示片段，涵盖20多种不同场景。\n- **训练资源**：未在提供的原文中明确指定具体的硬件配置，但提及在NVIDIA RTX 4090工作站上进行推理。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在真实世界的机器人操作任务上进行评估。共定义了8个接触丰富的力感知操作任务，包括管件插入、镊子夹取重物、移动电源拔出、白板擦除、果冻切片、重物提起、巧克力抓取等。\n- **评估指标**：任务成功率。每个模型在每个任务上进行15次试验，成功率计算为成功尝试的比例。成功标准根据具体任务以二元完成条件定义（任务是否成功完成）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京通用人工智能研究院、北京航空航天大学、上海科技大学、香港大学。</p>\n<h3>论文概述</h3>\n<p>当前视觉-语言-动作模型主要依赖视觉模态，缺乏处理接触丰富任务所需的精确力调节和物理直觉。本文针对此“力盲”问题，提出了从“触觉-视觉对齐”到“触觉-力对齐”的范式转变。论文介绍了TaF-VLA框架，该框架通过显式地将高维触觉观测与物理相互作用力关联起来，以实现力感知操作。核心贡献包括一个自动化触觉-力数据采集设备及由此构建的大规模TaF数据集、一个名为TaF-Adapter的触觉-力对齐模块，以及将该模块集成到VLA主干网络中形成的TaF-VLA策略。实验表明，TaF-VLA在接触丰富的任务上显著优于现有的触觉-视觉对齐及纯视觉基线模型。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>设计并构建了一个低成本、自动化的触觉-力数据采集设备（TaF-Device）及相应的大规模数据集（TaF-Dataset），解决了触觉-力对齐数据稀缺问题。</li><li>提出了触觉-力对齐器（TaF-Adapter），该模块通过对比学习，将时序触觉观测映射到一个与力信号对齐的共享潜在空间，捕获历史依赖的接触动力学，并具有抗噪和跨传感器鲁棒性。</li><li>开发了TaF-VLA模型，将预训练并冻结的TaF-Adapter集成到预训练的VLA骨干网络中，形成了一个能够执行复杂力感知操作任务的策略。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据采集与数据集构建</strong>：设计了一个具有双平台、同步驱动结构的自动化设备。该设备能对安装在两个平台上的触觉传感器和力传感器施加相同大小和方向的力，从而同步采集视觉触觉图像、6轴力/力矩和矩阵力分布图。通过更换不同硬度、几何形状的压头以及不同的视觉触觉传感器，构建了包含超过1000万帧同步数据的大规模TaF-Dataset。</li><li><strong>触觉-力对齐（TaF-Adapter）</strong>：</li></ol>\n<p> - <strong>力量化</strong>：使用两个向量量化变分自编码器，分别将时序的矩阵压力图和6轴力/力矩向量量化为离散的代码本，以此作为物理力的稳定锚点。</p>\n<p> - <strong>时序触觉编码</strong>：使用视觉变换器提取单帧触觉图像特征，然后通过一个因果变换器聚合时序窗口内的历史观测，生成一个综合的触觉表征。</p>\n<p> - <strong>跨模态对齐</strong>：通过对比学习（InfoNCE损失）在共享潜在空间中对齐触觉表征和对应的量化力代码，使触觉编码器能从视觉变形中推断力动态。</p>\n<ol><li><strong>策略集成（TaF-VLA）</strong>：将预训练并冻结的TaF-Adapter集成到基于流匹配的VLA骨干网络（π₀.₅）中。力对齐的触觉表征与全局视觉-语言特征、机器人本体感知信息一同作为条件，用于通过流匹配学习生成动作轨迹。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>TaF-Dataset</strong>：使用自研设备采集，包含6种不同的视觉触觉传感器数据。总计超过1000万帧同步的触觉观测、6轴力/力矩和矩阵力图对。</p>\n<p> - <strong>策略训练数据集</strong>：包含超过10,000个力感知操作演示片段，涵盖20多种不同场景。</p>\n<ul><li><strong>训练资源</strong>：未在提供的原文中明确指定具体的硬件配置，但提及在NVIDIA RTX 4090工作站上进行推理。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在真实世界的机器人操作任务上进行评估。共定义了8个接触丰富的力感知操作任务，包括管件插入、镊子夹取重物、移动电源拔出、白板擦除、果冻切片、重物提起、巧克力抓取等。</li><li><strong>评估指标</strong>：任务成功率。每个模型在每个任务上进行15次试验，成功率计算为成功尝试的比例。成功标准根据具体任务以二元完成条件定义（任务是否成功完成）。</li></ul>"
  },
  {
    "date": "2026-01-28",
    "title": "Shallow-π: Knowledge Distillation for Flow-based VLAs",
    "link": "http://arxiv.org/abs/2601.20262",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-27",
    "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
    "link": "http://arxiv.org/abs/2601.19634",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-26",
    "title": "Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods",
    "link": "http://arxiv.org/abs/2601.18723",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-26",
    "title": "A Pragmatic VLA Foundation Model",
    "link": "http://arxiv.org/abs/2601.18692",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-26",
    "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
    "link": "http://arxiv.org/abs/2601.18323",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-25",
    "title": "PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation",
    "link": "http://arxiv.org/abs/2601.17885",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-25",
    "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation",
    "link": "http://arxiv.org/abs/2601.17657",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-23",
    "title": "ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance",
    "link": "http://arxiv.org/abs/2601.16667",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-22",
    "title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance",
    "link": "http://arxiv.org/abs/2601.16207",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-22",
    "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
    "link": "http://arxiv.org/abs/2601.16065",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-22",
    "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
    "link": "http://arxiv.org/abs/2601.15761",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-21",
    "title": "CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation",
    "link": "http://arxiv.org/abs/2601.15541",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-21",
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "link": "http://arxiv.org/abs/2601.15197",
    "summary_markdown": "论文研究单位：\nHUST (华中科技大学), ZGCA (中关村实验室), ZGCI (中关村实验室), HIT (哈尔滨工业大学), HKUST(GZ) (香港科技大学（广州）), ZZU (郑州大学), BUAA (北京航空航天大学), ECNU (华东师范大学), DeepCybo。\n\n论文概述：\n本论文提出了BayesianVLA，一个旨在解决当前视觉-语言-动作模型在存在偏置的、目标驱动的数据集中容易出现的“视觉捷径”病理的框架。该框架通过贝叶斯分解和引入潜在动作查询，强制模型遵循语言指令，从而改善其泛化能力，特别是在分布外和模糊场景中。论文在SimplerEnv和RoboCasa等基准测试上验证了其有效性。\n\n论文核心贡献点：\n1. 识别并实证验证了当前VLA训练中的“视觉捷径”病理，即标准模型往往会忽略语言而依赖数据集特有的视觉相关性。\n2. 提出了BayesianVLA方法，利用潜在动作查询和双分支贝叶斯目标，从存在偏置的数据中恢复真正的语言条件化策略。\n3. 证明了BayesianVLA在SimplerEnv和RoboCasa上达到了先进的性能，特别是在SimplerEnv的OOD泛化上实现了11.3%的显著提升，验证了其打破视觉捷径的有效性。\n\n论文方法描述：\n1. 贝叶斯分解：提出最大化动作与指令之间的条件逐点互信息，其等价于最大化后验策略与视觉先验的对数似然比，迫使动作提供无法仅从视觉推断出的、关于指令的额外信息。\n2. 潜在动作查询：引入一组可学习的查询令牌（例如64个），注入到视觉语言模型中，作为VLM与下游扩散Transformer策略之间的瓶颈接口。这些查询根据其在输入序列中的位置，利用因果注意力掩码来编码纯视觉信息或视觉+语言信息。\n3. 双分支训练框架：\n - 先验分支（视觉专用）：输入序列为[v, Q, l]，查询只能关注视觉观测v，用于学习视觉先验p(a\\|v)。\n - 后验分支（视觉+语言）：输入序列为[v, l, Q]，查询可以关注视觉和语言，用于学习完整策略π(a\\|v, l)。\n - 最大化似然比：通过最大化LLR损失来优化，该损失鼓励查询的隐藏状态携带解释指令l的信息。\n4. 总训练目标：结合后验分支和先验分支的动作预测损失（采用Rectified Flow Matching目标）以及LLR正则化项。推理时仅使用后验分支，不产生额外计算开销。\n\n论文使用数据集和训练资源：\n- 使用的数据集：BridgeDataV2，Fractal (来自Open X-Embodiment数据集)，Humanoid Robot Tabletop Manipulation子集 (来自PhysicalAI-Robotics-GR00T-X-Embodiment-Sim数据集)，LIBERO基准测试。\n- 训练资源：基于StarVLA框架，在16个NVIDIA H100 GPU集群上进行训练。使用AdamW优化器（初始学习率1e-5，余弦退火调度）。系统级优化包括DeepSpeed ZeRO-2，梯度裁剪范数为1.0，无梯度累积。\n\n论文使用的评估环境和评估指标：\n- 评估环境：SimplerEnv（使用WidowX机器人），RoboCasa Tabletop simulation（使用GR1机器人），LIBERO benchmark。\n- 评估指标：任务成功率。在SimplerEnv上，每个任务运行480次独立试验并报告平均性能；在RoboCasa上，每个任务运行50次独立试验并报告平均成功率。",
    "summary_html": "<p>论文研究单位：</p>\n<p>HUST (华中科技大学), ZGCA (中关村实验室), ZGCI (中关村实验室), HIT (哈尔滨工业大学), HKUST(GZ) (香港科技大学（广州）), ZZU (郑州大学), BUAA (北京航空航天大学), ECNU (华东师范大学), DeepCybo。</p>\n\n<p>论文概述：</p>\n<p>本论文提出了BayesianVLA，一个旨在解决当前视觉-语言-动作模型在存在偏置的、目标驱动的数据集中容易出现的“视觉捷径”病理的框架。该框架通过贝叶斯分解和引入潜在动作查询，强制模型遵循语言指令，从而改善其泛化能力，特别是在分布外和模糊场景中。论文在SimplerEnv和RoboCasa等基准测试上验证了其有效性。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>识别并实证验证了当前VLA训练中的“视觉捷径”病理，即标准模型往往会忽略语言而依赖数据集特有的视觉相关性。</li><li>提出了BayesianVLA方法，利用潜在动作查询和双分支贝叶斯目标，从存在偏置的数据中恢复真正的语言条件化策略。</li><li>证明了BayesianVLA在SimplerEnv和RoboCasa上达到了先进的性能，特别是在SimplerEnv的OOD泛化上实现了11.3%的显著提升，验证了其打破视觉捷径的有效性。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>贝叶斯分解：提出最大化动作与指令之间的条件逐点互信息，其等价于最大化后验策略与视觉先验的对数似然比，迫使动作提供无法仅从视觉推断出的、关于指令的额外信息。</li><li>潜在动作查询：引入一组可学习的查询令牌（例如64个），注入到视觉语言模型中，作为VLM与下游扩散Transformer策略之间的瓶颈接口。这些查询根据其在输入序列中的位置，利用因果注意力掩码来编码纯视觉信息或视觉+语言信息。</li><li>双分支训练框架：</li></ol>\n<p> - 先验分支（视觉专用）：输入序列为[v, Q, l]，查询只能关注视觉观测v，用于学习视觉先验p(a\\|v)。</p>\n<p> - 后验分支（视觉+语言）：输入序列为[v, l, Q]，查询可以关注视觉和语言，用于学习完整策略π(a\\|v, l)。</p>\n<p> - 最大化似然比：通过最大化LLR损失来优化，该损失鼓励查询的隐藏状态携带解释指令l的信息。</p>\n<ol><li>总训练目标：结合后验分支和先验分支的动作预测损失（采用Rectified Flow Matching目标）以及LLR正则化项。推理时仅使用后验分支，不产生额外计算开销。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>使用的数据集：BridgeDataV2，Fractal (来自Open X-Embodiment数据集)，Humanoid Robot Tabletop Manipulation子集 (来自PhysicalAI-Robotics-GR00T-X-Embodiment-Sim数据集)，LIBERO基准测试。</li><li>训练资源：基于StarVLA框架，在16个NVIDIA H100 GPU集群上进行训练。使用AdamW优化器（初始学习率1e-5，余弦退火调度）。系统级优化包括DeepSpeed ZeRO-2，梯度裁剪范数为1.0，无梯度累积。</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>评估环境：SimplerEnv（使用WidowX机器人），RoboCasa Tabletop simulation（使用GR1机器人），LIBERO benchmark。</li><li>评估指标：任务成功率。在SimplerEnv上，每个任务运行480次独立试验并报告平均性能；在RoboCasa上，每个任务运行50次独立试验并报告平均成功率。</li></ul>"
  },
  {
    "date": "2026-01-21",
    "title": "TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control",
    "link": "http://arxiv.org/abs/2601.14945",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-21",
    "title": "A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control",
    "link": "http://arxiv.org/abs/2601.14628",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-20",
    "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "link": "http://arxiv.org/abs/2601.14133",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-20",
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "link": "http://arxiv.org/abs/2601.13976",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-20",
    "title": "Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education",
    "link": "http://arxiv.org/abs/2601.13876",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-20",
    "title": "DroneVLA: VLA based Aerial Manipulation",
    "link": "http://arxiv.org/abs/2601.13809",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-19",
    "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
    "link": "http://arxiv.org/abs/2601.12993",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-14",
    "title": "Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics",
    "link": "http://arxiv.org/abs/2601.11637",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-16",
    "title": "Generative Scenario Rollouts for End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2601.11475",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-16",
    "title": "The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents",
    "link": "http://arxiv.org/abs/2601.11421",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-16",
    "title": "ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2601.11404",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-16",
    "title": "VLAgents: A Policy Server for Efficient VLA Inference",
    "link": "http://arxiv.org/abs/2601.11250",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-14",
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "link": "http://arxiv.org/abs/2601.09708",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-14",
    "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
    "link": "http://arxiv.org/abs/2601.09512",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-13",
    "title": "Reasoning Matters for 3D Visual Grounding",
    "link": "http://arxiv.org/abs/2601.08811",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-13",
    "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
    "link": "http://arxiv.org/abs/2601.08665",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-13",
    "title": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation",
    "link": "http://arxiv.org/abs/2601.08325",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-12",
    "title": "Motion Focus Recognition in Fast-Moving Egocentric Video",
    "link": "http://arxiv.org/abs/2601.07154",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-11",
    "title": "PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation",
    "link": "http://arxiv.org/abs/2601.07060",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-11",
    "title": "On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning",
    "link": "http://arxiv.org/abs/2601.06748",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-10",
    "title": "CulinaryCut-VLAP: A Vision-Language-Action-Physics Framework for Food Cutting via a Force-Aware Material Point Method",
    "link": "http://arxiv.org/abs/2601.06451",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-09",
    "title": "LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction",
    "link": "http://arxiv.org/abs/2601.05611",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-08",
    "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2601.05248",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-07",
    "title": "CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos",
    "link": "http://arxiv.org/abs/2601.04061",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-07",
    "title": "Stable Language Guidance for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2601.04052",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-07",
    "title": "A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving",
    "link": "http://arxiv.org/abs/2601.03519",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-06",
    "title": "VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2601.03309",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-06",
    "title": "Limited Linguistic Diversity in Embodied AI Datasets",
    "link": "http://arxiv.org/abs/2601.03136",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-06",
    "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2601.03044",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-05",
    "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2601.02456",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-05",
    "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding",
    "link": "http://arxiv.org/abs/2601.02295",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-04",
    "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation",
    "link": "http://arxiv.org/abs/2601.01618",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2026-01-02",
    "title": "Value Vision-Language-Action Planning & Search",
    "link": "http://arxiv.org/abs/2601.00969",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-31",
    "title": "Dichotomous Diffusion Policy Optimization",
    "link": "http://arxiv.org/abs/2601.00898",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-31",
    "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots",
    "link": "http://arxiv.org/abs/2512.24673",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-31",
    "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence",
    "link": "http://arxiv.org/abs/2512.24653",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-30",
    "title": "Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning",
    "link": "http://arxiv.org/abs/2512.24426",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-30",
    "title": "GR-Dexter Technical Report",
    "link": "http://arxiv.org/abs/2512.24210",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-30",
    "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training",
    "link": "http://arxiv.org/abs/2512.24125",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-29",
    "title": "Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation",
    "link": "http://arxiv.org/abs/2512.23864",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-29",
    "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
    "link": "http://arxiv.org/abs/2512.23162",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-27",
    "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
    "link": "http://arxiv.org/abs/2512.22615",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-27",
    "title": "VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.22539",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-27",
    "title": "Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding",
    "link": "http://arxiv.org/abs/2512.22519",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-27",
    "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.22414",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-22",
    "title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA",
    "link": "http://arxiv.org/abs/2512.22208",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-26",
    "title": "StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision",
    "link": "http://arxiv.org/abs/2512.21970",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-23",
    "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
    "link": "http://arxiv.org/abs/2512.20276",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-23",
    "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.20188",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-23",
    "title": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation",
    "link": "http://arxiv.org/abs/2512.20166",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-23",
    "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting",
    "link": "http://arxiv.org/abs/2512.20014",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-22",
    "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.19562",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-22",
    "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
    "link": "http://arxiv.org/abs/2512.19024",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-22",
    "title": "Point What You Mean: Visually Grounded Instruction Policy",
    "link": "http://arxiv.org/abs/2512.18933",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-20",
    "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.18477",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-20",
    "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
    "link": "http://arxiv.org/abs/2512.18396",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-19",
    "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
    "link": "http://arxiv.org/abs/2512.18007",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-18",
    "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
    "link": "http://arxiv.org/abs/2512.16811",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-18",
    "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
    "link": "http://arxiv.org/abs/2512.16793",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-18",
    "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
    "link": "http://arxiv.org/abs/2512.16760",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-17",
    "title": "Large Video Planner Enables Generalizable Robot Control",
    "link": "http://arxiv.org/abs/2512.15840",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-17",
    "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
    "link": "http://arxiv.org/abs/2512.15692",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-17",
    "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training",
    "link": "http://arxiv.org/abs/2512.15411",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-17",
    "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
    "link": "http://arxiv.org/abs/2512.15258",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-16",
    "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.14666",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-16",
    "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
    "link": "http://arxiv.org/abs/2512.14031",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-15",
    "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
    "link": "http://arxiv.org/abs/2512.13636",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-15",
    "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
    "link": "http://arxiv.org/abs/2512.13080",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-15",
    "title": "Motus: A Unified Latent Action World Model",
    "link": "http://arxiv.org/abs/2512.13030",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-14",
    "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
    "link": "http://arxiv.org/abs/2512.12799",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-12",
    "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.11769",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-12",
    "title": "Embodied Image Compression",
    "link": "http://arxiv.org/abs/2512.11612",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-12",
    "title": "Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents",
    "link": "http://arxiv.org/abs/2512.11584",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-12",
    "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
    "link": "http://arxiv.org/abs/2512.11362",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-12",
    "title": "Benchmarking the Generality of Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.11315",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-12",
    "title": "Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy",
    "link": "http://arxiv.org/abs/2512.11218",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-11",
    "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control",
    "link": "http://arxiv.org/abs/2512.11047",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-11",
    "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control",
    "link": "http://arxiv.org/abs/2512.11921",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-10",
    "title": "Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models",
    "link": "http://arxiv.org/abs/2512.11908",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-11",
    "title": "RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI",
    "link": "http://arxiv.org/abs/2512.10394",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-11",
    "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
    "link": "http://arxiv.org/abs/2512.10226",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-10",
    "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.09928",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-10",
    "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.09927",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-10",
    "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
    "link": "http://arxiv.org/abs/2512.09864",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-10",
    "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.09619",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-09",
    "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning",
    "link": "http://arxiv.org/abs/2512.08580",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-08",
    "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations",
    "link": "http://arxiv.org/abs/2512.07582",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-08",
    "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.07472",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-07",
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "link": "http://arxiv.org/abs/2512.06963",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-05",
    "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving",
    "link": "http://arxiv.org/abs/2512.06112",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-05",
    "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking",
    "link": "http://arxiv.org/abs/2512.05964",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-05",
    "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies",
    "link": "http://arxiv.org/abs/2512.05693",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-04",
    "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.05107",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-04",
    "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization",
    "link": "http://arxiv.org/abs/2512.04952",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-04",
    "title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2512.04733",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-04",
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "link": "http://arxiv.org/abs/2512.04537",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-04",
    "title": "dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning",
    "link": "http://arxiv.org/abs/2512.04459",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-04",
    "title": "Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops",
    "link": "http://arxiv.org/abs/2512.04446",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-03",
    "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations",
    "link": "http://arxiv.org/abs/2512.03913",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-03",
    "title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention",
    "link": "http://arxiv.org/abs/2512.03724",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-03",
    "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation",
    "link": "http://arxiv.org/abs/2512.03538",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-02",
    "title": "Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling",
    "link": "http://arxiv.org/abs/2512.03044",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-02",
    "title": "VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling",
    "link": "http://arxiv.org/abs/2512.02902",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-02",
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "link": "http://arxiv.org/abs/2512.02834",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-02",
    "title": "Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols",
    "link": "http://arxiv.org/abs/2512.02787",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-02",
    "title": "RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning",
    "link": "http://arxiv.org/abs/2512.02729",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-01",
    "title": "ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.02013",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-01",
    "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.01801",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-01",
    "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
    "link": "http://arxiv.org/abs/2512.01715",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-30",
    "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "link": "http://arxiv.org/abs/2512.01031",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-30",
    "title": "CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding",
    "link": "http://arxiv.org/abs/2512.01022",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-30",
    "title": "MM-ACT: Learn from Multimodal Parallel Generation to Act",
    "link": "http://arxiv.org/abs/2512.00975",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-30",
    "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
    "link": "http://arxiv.org/abs/2512.00903",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-30",
    "title": "Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment",
    "link": "http://arxiv.org/abs/2512.00783",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-28",
    "title": "LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.23034",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-28",
    "title": "RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video",
    "link": "http://arxiv.org/abs/2511.22950",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-27",
    "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.22780",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-27",
    "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery",
    "link": "http://arxiv.org/abs/2511.22777",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-27",
    "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations",
    "link": "http://arxiv.org/abs/2511.22697",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-27",
    "title": "Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention",
    "link": "http://arxiv.org/abs/2511.22555",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-27",
    "title": "CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.22532",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-27",
    "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
    "link": "http://arxiv.org/abs/2511.22134",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-26",
    "title": "Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.21663",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-26",
    "title": "VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.21557",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-26",
    "title": "$\\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion",
    "link": "http://arxiv.org/abs/2511.21542",
    "summary_markdown": "### 论文研究单位\n中山大学，广东大数据分析与处理重点实验室，X-Era AI实验室，广东工业大学\n### 论文概述\n提出ℰ₀，一个连续化的离散扩散框架，用于视觉-语言-动作模型，以提升泛化能力和细粒度控制。该方法将动作生成为量化动作标记的迭代去噪过程，解决现有模型在符号对齐和离散控制上的不足，并引入球形视角扰动增强以提高对相机移动的鲁棒性。在LIBERO、VLABench和ManiSkill等数据集上验证，实现了最先进的性能。\n### 论文核心贡献点\n1. 提出ℰ₀框架，支持任意细粒度的动作离散化，实现高精度动作表示，同时保持与预训练视觉-语言模型的兼容性。\n2. 引入球形视角扰动增强和相对球形嵌入机制，显式建模动态相机扰动，提升跨视角一致性和鲁棒性。\n3. 通过多个模拟基准和真实机器人任务（包括Franka机械臂）的广泛实验，证明ℰ₀在14个多样化环境中超越基线模型平均10.7%的性能。\n### 论文方法描述\n基于PaliGemma视觉-语言模型主干网络，添加300M参数的动作专家模块。动作使用基于分位数的离散化，量化为2048个bins以减少异常值影响。训练中，对离散动作添加高斯噪声，模型通过交叉熵损失预测分类分布。推理采用多步迭代去噪，从噪声序列逐步恢复清晰动作，复用观察编码的键值缓存以提高效率。球形视角扰动通过3D点旋转重投影图像，结合可学习的偏移嵌入，增强视角不变性。\n### 论文使用数据集和训练资源\n数据集包括LIBERO（涵盖空间、物体、目标和长时程任务）、VLABench（需语言理解和常识推理）和ManiSkill（精细操作技能如插入和堆叠）。训练使用单块NVIDIA RTX RPO6000 GPU，批次大小32，训练30000步，学习率余弦衰减（峰值5×10⁻⁵，预热10000步），总时长24小时。\n### 论文使用的评估环境和评估指标\n评估环境为LIBERO、VLABench和ManiSkill模拟器，以及Franka Research 3机械臂真实平台。指标为任务成功率（SR%），在各项任务子集和平均表现上报告。此外，进行相机扰动鲁棒性测试，通过动态改变相机位置和姿态评估跨视角泛化能力。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中山大学，广东大数据分析与处理重点实验室，X-Era AI实验室，广东工业大学</p>\n<h3>论文概述</h3>\n<p>提出ℰ₀，一个连续化的离散扩散框架，用于视觉-语言-动作模型，以提升泛化能力和细粒度控制。该方法将动作生成为量化动作标记的迭代去噪过程，解决现有模型在符号对齐和离散控制上的不足，并引入球形视角扰动增强以提高对相机移动的鲁棒性。在LIBERO、VLABench和ManiSkill等数据集上验证，实现了最先进的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出ℰ₀框架，支持任意细粒度的动作离散化，实现高精度动作表示，同时保持与预训练视觉-语言模型的兼容性。</li><li>引入球形视角扰动增强和相对球形嵌入机制，显式建模动态相机扰动，提升跨视角一致性和鲁棒性。</li><li>通过多个模拟基准和真实机器人任务（包括Franka机械臂）的广泛实验，证明ℰ₀在14个多样化环境中超越基线模型平均10.7%的性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>基于PaliGemma视觉-语言模型主干网络，添加300M参数的动作专家模块。动作使用基于分位数的离散化，量化为2048个bins以减少异常值影响。训练中，对离散动作添加高斯噪声，模型通过交叉熵损失预测分类分布。推理采用多步迭代去噪，从噪声序列逐步恢复清晰动作，复用观察编码的键值缓存以提高效率。球形视角扰动通过3D点旋转重投影图像，结合可学习的偏移嵌入，增强视角不变性。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集包括LIBERO（涵盖空间、物体、目标和长时程任务）、VLABench（需语言理解和常识推理）和ManiSkill（精细操作技能如插入和堆叠）。训练使用单块NVIDIA RTX RPO6000 GPU，批次大小32，训练30000步，学习率余弦衰减（峰值5×10⁻⁵，预热10000步），总时长24小时。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境为LIBERO、VLABench和ManiSkill模拟器，以及Franka Research 3机械臂真实平台。指标为任务成功率（SR%），在各项任务子集和平均表现上报告。此外，进行相机扰动鲁棒性测试，通过动态改变相机位置和姿态评估跨视角泛化能力。</p>"
  },
  {
    "date": "2025-11-26",
    "title": "From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings",
    "link": "http://arxiv.org/abs/2511.21428",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-26",
    "title": "When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.21192",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n南洋理工大学\n## 论文概述\n本文系统研究了针对视觉-语言-动作（VLA）模型的通用可迁移对抗补丁攻击。现有方法通常针对单一模型进行白盒攻击，在未知架构、微调变体或模拟到现实的迁移场景下效果不佳。为此，论文提出了UPA-RFAS框架，旨在学习单个物理补丁，该补丁在共享特征空间中优化，并促进跨模型迁移。实验表明，该方法能在模型、任务和视角间实现有效迁移，揭示了基于补丁的实际攻击面，为未来防御提供了强基线。\n## 论文核心贡献点\n- 提出了首个针对VLA机器人的通用可迁移补丁攻击框架，在共享特征空间中结合了ℓ1偏差与排斥性对比对齐。\n- 设计了鲁棒性增强的通用补丁攻击，使用样本级不可见扰动作为增强器，并在大量几何随机化下训练通用补丁。\n- 设计了两种VLA特定损失：Patch Attention Dominance用于劫持文本到视觉的注意力，Patch Semantic Misalignment用于在无标签情况下使指令误对齐。\n- 在多个VLA模型、任务和模拟到现实设置下的广泛实验展示了强大的黑盒迁移性，揭示了基于补丁的实际威胁并为防御提供了可迁移基线。\n## 论文方法描述\n方法核心为UPA-RFAS框架，包含以下组件：\n1. **特征空间目标**：结合ℓ1偏差与排斥性InfoNCE损失，通过稀疏、高显著度的特征偏移和批次一致性方向促进迁移。\n2. **鲁棒性增强的双阶段优化**：内循环通过PGD学习样本级不可见扰动以最小化特征目标，外循环在强化邻域上优化通用补丁。\n3. **Patch Attention Dominance (PAD)损失**：增加补丁路由的文本到视觉注意力，通过单边边际抑制非补丁增量，实现位置无关的注意力吸引。\n4. **Patch Semantic Misalignment (PSM)损失**：将补丁表示拉向探查短语锚点，同时推离当前指令嵌入，创建持久的图像-文本不匹配。\n## 论文使用数据集和训练资源\n- 数据集：Libero、BridgeData等VLA机器人操作任务数据集。\n- 训练资源：未明确提及具体硬件配置，但方法涉及对抗训练（PGD）和AdamW优化，暗示需要GPU支持。\n## 论文使用的评估环境和评估指标\n- **评估环境**：模拟环境（如Libero、BridgeData）和物理世界实验（Franka Emika机械臂）。\n- **评估指标**：任务成功率、动作空间偏差、特征空间ℓ1偏差、注意力增量（补丁与非补丁区域）、文本相似度攻击损失（PSM损失）。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>南洋理工大学</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文系统研究了针对视觉-语言-动作（VLA）模型的通用可迁移对抗补丁攻击。现有方法通常针对单一模型进行白盒攻击，在未知架构、微调变体或模拟到现实的迁移场景下效果不佳。为此，论文提出了UPA-RFAS框架，旨在学习单个物理补丁，该补丁在共享特征空间中优化，并促进跨模型迁移。实验表明，该方法能在模型、任务和视角间实现有效迁移，揭示了基于补丁的实际攻击面，为未来防御提供了强基线。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了首个针对VLA机器人的通用可迁移补丁攻击框架，在共享特征空间中结合了ℓ1偏差与排斥性对比对齐。</li><li>设计了鲁棒性增强的通用补丁攻击，使用样本级不可见扰动作为增强器，并在大量几何随机化下训练通用补丁。</li><li>设计了两种VLA特定损失：Patch Attention Dominance用于劫持文本到视觉的注意力，Patch Semantic Misalignment用于在无标签情况下使指令误对齐。</li><li>在多个VLA模型、任务和模拟到现实设置下的广泛实验展示了强大的黑盒迁移性，揭示了基于补丁的实际威胁并为防御提供了可迁移基线。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>方法核心为UPA-RFAS框架，包含以下组件：</p>\n<ol><li><strong>特征空间目标</strong>：结合ℓ1偏差与排斥性InfoNCE损失，通过稀疏、高显著度的特征偏移和批次一致性方向促进迁移。</li><li><strong>鲁棒性增强的双阶段优化</strong>：内循环通过PGD学习样本级不可见扰动以最小化特征目标，外循环在强化邻域上优化通用补丁。</li><li><strong>Patch Attention Dominance (PAD)损失</strong>：增加补丁路由的文本到视觉注意力，通过单边边际抑制非补丁增量，实现位置无关的注意力吸引。</li><li><strong>Patch Semantic Misalignment (PSM)损失</strong>：将补丁表示拉向探查短语锚点，同时推离当前指令嵌入，创建持久的图像-文本不匹配。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据集：Libero、BridgeData等VLA机器人操作任务数据集。</li><li>训练资源：未明确提及具体硬件配置，但方法涉及对抗训练（PGD）和AdamW优化，暗示需要GPU支持。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：模拟环境（如Libero、BridgeData）和物理世界实验（Franka Emika机械臂）。</li><li><strong>评估指标</strong>：任务成功率、动作空间偏差、特征空间ℓ1偏差、注意力增量（补丁与非补丁区域）、文本相似度攻击损失（PSM损失）。</li></ul>"
  },
  {
    "date": "2025-11-25",
    "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.20720",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-25",
    "title": "Reinforcing Action Policies by Prophesying",
    "link": "http://arxiv.org/abs/2511.20633",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-25",
    "title": "CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model",
    "link": "http://arxiv.org/abs/2511.19914",
    "summary_markdown": "### 论文研究单位\n兰州大学、新加坡国立大学、中国科学技术大学\n### 论文概述\n本文提出了一种名为CoC-VLA的新型视觉-语言-动作模型，旨在通过对抗性域转移实现可解释的自动驾驶。该模型包含教师VLM模型、学生VLM模型和一个判别器。教师模型在模拟数据上预训练，学生模型在真实世界数据上预训练，两者共享相同的因果链（Chain-of-Causality）VLM架构。通过判别器进行对抗训练，将模拟数据中的长尾场景处理能力转移到真实世界部署中，从而弥合了模拟与真实世界自动驾驶之间的差距。\n### 论文核心贡献点\n- 提出了首个能够将罕见场景处理能力从模拟转移到真实世界的VLM自动驾驶模型。\n- 引入了一种新颖的判别器，用于学习模拟与真实世界数据之间的域差异。\n- 提出了一种反向传播策略，增强了对抗训练过程的收敛稳定性。\n- 开发了因果链策略，将时序信息与因果链答案连接，支持思维链推理以建模深层驾驶逻辑。\n- 在nuScenes-VLM数据集上进行了广泛实验，表明该方法显著优于现有方法。\n### 论文方法描述\n方法包括三个核心组件：教师VLM、学生VLM和判别器。\n1. 因果链视觉语言模型（CoC VLM）：作为教师和学生模型的共享基础架构，包含文本适配器（整合历史帧的简化答案与当前指令）、视觉适配器（将多视图图像转换为令牌）、LLM大脑（基于LLaMA）和因果链答案模块（生成感知、预测和规划的因果结构答案）。\n2. 判别器：采用Transformer架构，处理来自教师和学生VLM的特征，通过对抗学习对齐特征表示，最小化域差异。\n3. 训练流程：\n - 预训练：教师模型在模拟数据（CARLA-VLM）上训练，学生模型在真实数据（nuScenes-VLM）上训练。\n - 对抗训练：\n - 步骤1：使用模拟和真实数据前向传播，仅更新判别器参数。\n - 步骤2：使用真实数据前向传播学生VLM和判别器，结合VLM损失和判别器损失更新学生VLM，判别器参数不更新。\n### 论文使用数据集和训练资源\n数据集：\n- CARLA-VLM：模拟数据集，包含61.6%常规场景和38.4%挑战性场景（如交通堵塞、行人入侵等）。\n- nuScenes-VLM：基于nuScenes的真实世界数据集，用于预训练学生模型和最终评估。\n训练资源：使用8块NVIDIA H100 GPU，采用LoRA方法微调模型以降低计算资源需求。\n### 论文使用的评估环境和评估指标\n评估环境：基于nuScenes-VLM数据集进行开环评估。\n评估指标：\n语言评估指标：\n- BLEU-1至BLEU-4：衡量预测文本与参考文本的n元语法匹配度。\n- ROUGE-L：基于最长公共子序列的召回率评估。\n- CIDEr：通过TF-IDF向量余弦距离衡量文本相似性。\n- SPICE：基于场景图的结构相似性评估。\n- GPT Score：利用ChatGPT评估语义对齐度。\n- Accuracy和Match：衡量答案正确性。\n规划评估指标：\n- 平均位移误差（ADE）：预测轨迹与真实轨迹之间的平均L2距离。\n- 碰撞率（Collision Rate）：评估帧中发生碰撞的比例。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>兰州大学、新加坡国立大学、中国科学技术大学</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为CoC-VLA的新型视觉-语言-动作模型，旨在通过对抗性域转移实现可解释的自动驾驶。该模型包含教师VLM模型、学生VLM模型和一个判别器。教师模型在模拟数据上预训练，学生模型在真实世界数据上预训练，两者共享相同的因果链（Chain-of-Causality）VLM架构。通过判别器进行对抗训练，将模拟数据中的长尾场景处理能力转移到真实世界部署中，从而弥合了模拟与真实世界自动驾驶之间的差距。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了首个能够将罕见场景处理能力从模拟转移到真实世界的VLM自动驾驶模型。</li><li>引入了一种新颖的判别器，用于学习模拟与真实世界数据之间的域差异。</li><li>提出了一种反向传播策略，增强了对抗训练过程的收敛稳定性。</li><li>开发了因果链策略，将时序信息与因果链答案连接，支持思维链推理以建模深层驾驶逻辑。</li><li>在nuScenes-VLM数据集上进行了广泛实验，表明该方法显著优于现有方法。</li></ul>\n<h3>论文方法描述</h3>\n<p>方法包括三个核心组件：教师VLM、学生VLM和判别器。</p>\n<ol><li>因果链视觉语言模型（CoC VLM）：作为教师和学生模型的共享基础架构，包含文本适配器（整合历史帧的简化答案与当前指令）、视觉适配器（将多视图图像转换为令牌）、LLM大脑（基于LLaMA）和因果链答案模块（生成感知、预测和规划的因果结构答案）。</li><li>判别器：采用Transformer架构，处理来自教师和学生VLM的特征，通过对抗学习对齐特征表示，最小化域差异。</li><li>训练流程：</li></ol>\n<p> - 预训练：教师模型在模拟数据（CARLA-VLM）上训练，学生模型在真实数据（nuScenes-VLM）上训练。</p>\n<p> - 对抗训练：</p>\n<p> - 步骤1：使用模拟和真实数据前向传播，仅更新判别器参数。</p>\n<p> - 步骤2：使用真实数据前向传播学生VLM和判别器，结合VLM损失和判别器损失更新学生VLM，判别器参数不更新。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：</p>\n<ul><li>CARLA-VLM：模拟数据集，包含61.6%常规场景和38.4%挑战性场景（如交通堵塞、行人入侵等）。</li><li>nuScenes-VLM：基于nuScenes的真实世界数据集，用于预训练学生模型和最终评估。</li></ul>\n<p>训练资源：使用8块NVIDIA H100 GPU，采用LoRA方法微调模型以降低计算资源需求。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：基于nuScenes-VLM数据集进行开环评估。</p>\n<p>评估指标：</p>\n<p>语言评估指标：</p>\n<ul><li>BLEU-1至BLEU-4：衡量预测文本与参考文本的n元语法匹配度。</li><li>ROUGE-L：基于最长公共子序列的召回率评估。</li><li>CIDEr：通过TF-IDF向量余弦距离衡量文本相似性。</li><li>SPICE：基于场景图的结构相似性评估。</li><li>GPT Score：利用ChatGPT评估语义对齐度。</li><li>Accuracy和Match：衡量答案正确性。</li></ul>\n<p>规划评估指标：</p>\n<ul><li>平均位移误差（ADE）：预测轨迹与真实轨迹之间的平均L2距离。</li><li>碰撞率（Collision Rate）：评估帧中发生碰撞的比例。</li></ul>"
  },
  {
    "date": "2025-11-25",
    "title": "Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.19912",
    "summary_markdown": "### 论文研究单位\nLanzhou University, China\nNational University of Singapore, Singapore\nUniversity of Science and Technology of China, China\nTsinghua University, China\nUniversity of New South Wales, Australia\n### 论文概述\n论文提出了一种名为Reasoning-VLA的通用且快速的视觉-语言-动作（VLA）推理模型，用于解决现有自动驾驶VLA模型在推理效率和泛化能力方面的不足。该模型通过一组可学习的动作查询与增强推理的视觉-语言特征交互，实现连续动作轨迹的并行生成。为了提升泛化性，研究整合了八个公开的自动驾驶数据集，构建了一个基于思维链推理的统一数据集。结合监督微调（SFT）和强化学习（RL）的训练策略，实验表明Reasoning-VLA在多个基准上实现了最先进的性能、卓越的泛化能力和目前报告的优异推理速度。\n### 论文核心贡献点\n1. 提出了Reasoning-VLA，一个高效的VLA框架，通过可学习的动作查询与推理增强的视觉-语言表示交互，实现单步并行动作生成。\n2. 通过从真实轨迹中进行高斯分布采样来初始化可学习的动作查询，提升了模型效率。\n3. 构建了一个统一的、基于思维链推理的自动驾驶数据集，该数据集融合了八个现有数据集，以促进模型在不同车辆类型和驾驶环境下的泛化能力。\n4. 采用结合SFT和RL的微调策略，并辅以物理和动力学奖励函数，增强了模型的通用推理能力，显著优于先前的方法。\n### 论文方法描述\n该方法基于Qwen2.5-VL模型构建，包含三个主要部分：\n1. **推理增强的视觉-语言模型主干**：利用Qwen2.5-VL作为基础模型，提供强大的视觉-语言理解和推理能力。\n2. **VL到Action模块**：引入一组可学习的动作查询，这些查询通过自注意力和与VLM特征的交叉注意力进行交互，从VLM的推理内容中提取动作相关信息，实现并行动作轨迹的预测。\n3. **动作细化模块（ARM）**：通过多层感知机（MLP）和注意力机制处理动作查询的隐藏状态，以细化特征表示，提高轨迹精度。\n训练过程分为两个阶段：\n1. **监督微调（SFT）**：使用统一的推理数据集对模型进行初步训练，建立结构化的推理链。\n2. **强化学习（RL）微调**：采用GRPO算法和设计的基于规则的奖励函数进行优化，以提高模型在未见场景下的泛化能力。奖励函数包括物理轨迹奖励、车辆动态奖励（转向和加速度约束）以及它们的加权和。\n### 论文使用数据集和训练资源\n**数据集**：\n构建了一个统一的自动驾驶数据集，整合了八个公开数据集：NAVSIM, nuScenes, Waymo, Argoverse-V2, KITTI, Mapillary, ONCE, 和 IDD。经过筛选和处理，包含了超过75,000个高质量片段，并生成了思维链描述。\n**训练资源**：\n训练在8张H200 GPU上进行，总批大小为8。SFT阶段训练4个周期，学习率从5e-4开始；RL阶段训练1个周期，学习率从1e-6开始。累积梯度步数为2。\n### 论文使用的评估环境和评估指标\n**评估环境**：\n1. **开环评估**：在nuScenes数据集的验证集上进行，与现有方法进行公平比较。\n2. **闭环评估**：在NeuroNCAP模拟器上进行，该模拟器能够模拟复杂的现实世界驾驶场景。\n**评估指标**：\n1. **开环指标**：\n - L2误差（米）：在1秒、2秒、3秒未来时间点的平均L2距离，值越低越好。\n - 碰撞率（%）：在1秒、2秒、3秒未来时间点的碰撞率，值越低越好。\n2. **闭环指标**：\n - NeuroNCAP分数：在静止、正面、侧面等不同场景下的得分，值越高越好。\n - 碰撞率（%）：在静止、正面、侧面等不同场景下的碰撞率，值越低越好。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Lanzhou University, China</p>\n<p>National University of Singapore, Singapore</p>\n<p>University of Science and Technology of China, China</p>\n<p>Tsinghua University, China</p>\n<p>University of New South Wales, Australia</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为Reasoning-VLA的通用且快速的视觉-语言-动作（VLA）推理模型，用于解决现有自动驾驶VLA模型在推理效率和泛化能力方面的不足。该模型通过一组可学习的动作查询与增强推理的视觉-语言特征交互，实现连续动作轨迹的并行生成。为了提升泛化性，研究整合了八个公开的自动驾驶数据集，构建了一个基于思维链推理的统一数据集。结合监督微调（SFT）和强化学习（RL）的训练策略，实验表明Reasoning-VLA在多个基准上实现了最先进的性能、卓越的泛化能力和目前报告的优异推理速度。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了Reasoning-VLA，一个高效的VLA框架，通过可学习的动作查询与推理增强的视觉-语言表示交互，实现单步并行动作生成。</li><li>通过从真实轨迹中进行高斯分布采样来初始化可学习的动作查询，提升了模型效率。</li><li>构建了一个统一的、基于思维链推理的自动驾驶数据集，该数据集融合了八个现有数据集，以促进模型在不同车辆类型和驾驶环境下的泛化能力。</li><li>采用结合SFT和RL的微调策略，并辅以物理和动力学奖励函数，增强了模型的通用推理能力，显著优于先前的方法。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法基于Qwen2.5-VL模型构建，包含三个主要部分：</p>\n<ol><li><strong>推理增强的视觉-语言模型主干</strong>：利用Qwen2.5-VL作为基础模型，提供强大的视觉-语言理解和推理能力。</li><li><strong>VL到Action模块</strong>：引入一组可学习的动作查询，这些查询通过自注意力和与VLM特征的交叉注意力进行交互，从VLM的推理内容中提取动作相关信息，实现并行动作轨迹的预测。</li><li><strong>动作细化模块（ARM）</strong>：通过多层感知机（MLP）和注意力机制处理动作查询的隐藏状态，以细化特征表示，提高轨迹精度。</li></ol>\n<p>训练过程分为两个阶段：</p>\n<ol><li><strong>监督微调（SFT）</strong>：使用统一的推理数据集对模型进行初步训练，建立结构化的推理链。</li><li><strong>强化学习（RL）微调</strong>：采用GRPO算法和设计的基于规则的奖励函数进行优化，以提高模型在未见场景下的泛化能力。奖励函数包括物理轨迹奖励、车辆动态奖励（转向和加速度约束）以及它们的加权和。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>：</p>\n<p>构建了一个统一的自动驾驶数据集，整合了八个公开数据集：NAVSIM, nuScenes, Waymo, Argoverse-V2, KITTI, Mapillary, ONCE, 和 IDD。经过筛选和处理，包含了超过75,000个高质量片段，并生成了思维链描述。</p>\n<p><strong>训练资源</strong>：</p>\n<p>训练在8张H200 GPU上进行，总批大小为8。SFT阶段训练4个周期，学习率从5e-4开始；RL阶段训练1个周期，学习率从1e-6开始。累积梯度步数为2。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>：</p>\n<ol><li><strong>开环评估</strong>：在nuScenes数据集的验证集上进行，与现有方法进行公平比较。</li><li><strong>闭环评估</strong>：在NeuroNCAP模拟器上进行，该模拟器能够模拟复杂的现实世界驾驶场景。</li></ol>\n<p><strong>评估指标</strong>：</p>\n<ol><li><strong>开环指标</strong>：</li></ol>\n<p> - L2误差（米）：在1秒、2秒、3秒未来时间点的平均L2距离，值越低越好。</p>\n<p> - 碰撞率（%）：在1秒、2秒、3秒未来时间点的碰撞率，值越低越好。</p>\n<ol><li><strong>闭环指标</strong>：</li></ol>\n<p> - NeuroNCAP分数：在静止、正面、侧面等不同场景下的得分，值越高越好。</p>\n<p> - 碰撞率（%）：在静止、正面、侧面等不同场景下的碰撞率，值越低越好。</p>"
  },
  {
    "date": "2025-11-25",
    "title": "MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization",
    "link": "http://arxiv.org/abs/2511.19878",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-25",
    "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
    "link": "http://arxiv.org/abs/2511.19861",
    "summary_markdown": "### 论文研究单位\nGigaAI\n### 论文概述\n本文介绍了GigaWorld-0，一个统一的世界模型框架，旨在作为视觉-语言-动作（VLA）学习的数据引擎。它整合了两个协同组件：用于生成大规模视频的GigaWorld-0-Video和结合3D生成建模、物理仿真与运动规划的GigaWorld-0-3D，共同生成视觉丰富、几何一致且物理合理的具身交互数据。通过高效的GigaTrain训练框架实现大规模训练，实验证明其生成数据能显著提升VLA模型在真实机器人任务中的泛化能力和成功率。\n### 论文核心贡献点\n1. 提出GigaWorld-0作为数据引擎，整合视频生成与3D物理仿真，解决具身AI的数据瓶颈问题。\n2. 设计GigaWorld-0-Video系列模型，支持文本控制的外观、视角和动作迁移，以及多视角生成。\n3. 开发GigaWorld-0-3D模块，实现几何一致的3D场景重建、物理属性标注和可执行动作规划。\n4. 构建GigaTrain训练框架，利用FP8精度和稀疏注意力优化大规模视频生成效率。\n5. 通过下游任务验证，证明生成数据能提升VLA模型在真实机器人任务中的性能。\n### 论文方法描述\n1. **GigaWorld-0-Video-Dreamer**：基于流匹配和MoE架构的视频基础模型，支持图像-文本到视频生成。\n2. **GigaWorld-0-Video-AppearanceTransfer**：通过轻量控制分支实现文本引导的纹理、材质和光照编辑。\n3. **GigaWorld-0-Video-ViewTransfer**：结合双条件控制，生成新视角视频并同步调整机器人动作。\n4. **GigaWorld-0-Video-MimicTransfer**：将第一人称人类操作视频转换为机器人可执行轨迹。\n5. **GigaWorld-0-3D**：通过3D高斯溅射重建背景，生成式建模创建前景，结合物理属性标注和动作规划，确保几何与物理一致性。\n6. **GigaTrain框架**：支持FP8精度、稀疏注意力和分布式训练策略，优化计算资源使用。\n### 论文使用数据集和训练资源\n1. **数据集**：结合公开数据集（AgiBotWorld、RoboMind）与私有数据，覆盖工业、商业、办公等14类场景，任务涵盖基础操作到长时序交互。\n2. **训练资源**：使用自研GigaTrain框架，在480×768分辨率下训练61帧序列，支持多GPU/节点分布式训练与混合精度（FP8）优化。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：在DreamGen Bench和PBench Robot Set基准测试，定量评估生成质量；真实机器人部署（如AgiBot G1）验证下游任务性能。\n2. **评估指标**：物理合理性、几何一致性、文本对齐度、多视角一致性、视觉保真度；下游任务成功率与泛化能力。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>GigaAI</p>\n<h3>论文概述</h3>\n<p>本文介绍了GigaWorld-0，一个统一的世界模型框架，旨在作为视觉-语言-动作（VLA）学习的数据引擎。它整合了两个协同组件：用于生成大规模视频的GigaWorld-0-Video和结合3D生成建模、物理仿真与运动规划的GigaWorld-0-3D，共同生成视觉丰富、几何一致且物理合理的具身交互数据。通过高效的GigaTrain训练框架实现大规模训练，实验证明其生成数据能显著提升VLA模型在真实机器人任务中的泛化能力和成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出GigaWorld-0作为数据引擎，整合视频生成与3D物理仿真，解决具身AI的数据瓶颈问题。</li><li>设计GigaWorld-0-Video系列模型，支持文本控制的外观、视角和动作迁移，以及多视角生成。</li><li>开发GigaWorld-0-3D模块，实现几何一致的3D场景重建、物理属性标注和可执行动作规划。</li><li>构建GigaTrain训练框架，利用FP8精度和稀疏注意力优化大规模视频生成效率。</li><li>通过下游任务验证，证明生成数据能提升VLA模型在真实机器人任务中的性能。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>GigaWorld-0-Video-Dreamer</strong>：基于流匹配和MoE架构的视频基础模型，支持图像-文本到视频生成。</li><li><strong>GigaWorld-0-Video-AppearanceTransfer</strong>：通过轻量控制分支实现文本引导的纹理、材质和光照编辑。</li><li><strong>GigaWorld-0-Video-ViewTransfer</strong>：结合双条件控制，生成新视角视频并同步调整机器人动作。</li><li><strong>GigaWorld-0-Video-MimicTransfer</strong>：将第一人称人类操作视频转换为机器人可执行轨迹。</li><li><strong>GigaWorld-0-3D</strong>：通过3D高斯溅射重建背景，生成式建模创建前景，结合物理属性标注和动作规划，确保几何与物理一致性。</li><li><strong>GigaTrain框架</strong>：支持FP8精度、稀疏注意力和分布式训练策略，优化计算资源使用。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：结合公开数据集（AgiBotWorld、RoboMind）与私有数据，覆盖工业、商业、办公等14类场景，任务涵盖基础操作到长时序交互。</li><li><strong>训练资源</strong>：使用自研GigaTrain框架，在480×768分辨率下训练61帧序列，支持多GPU/节点分布式训练与混合精度（FP8）优化。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：在DreamGen Bench和PBench Robot Set基准测试，定量评估生成质量；真实机器人部署（如AgiBot G1）验证下游任务性能。</li><li><strong>评估指标</strong>：物理合理性、几何一致性、文本对齐度、多视角一致性、视觉保真度；下游任务成功率与泛化能力。</li></ol>"
  },
  {
    "date": "2025-11-25",
    "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation",
    "link": "http://arxiv.org/abs/2511.19859",
    "summary_markdown": "### 论文研究单位\n南京大学计算机软件新技术国家重点实验室（State Key Laboratory for Novel Software Technology, Nanjing University）\n### 论文概述\n本文提出VITA（Vision-Integrated Trajectory Alignment）框架，旨在统一视觉感知与机器人动作生成。通过构建跨模态共享的离散潜在空间，VITA将视觉观察与低级动作对齐，并引入隐式视觉思维链（Implicit Visual Chain-of-Thought）：自回归生成的token同时解码为未来帧预测和机器人动作序列，使视觉动态作为动作规划的归纳偏置。该方法解决了视觉-动作模态 gap 和训练不稳定性问题，在模拟与真实环境中实现SOTA性能。\n### 论文核心贡献点\n1. **框架创新**：提出VITA框架，通过统一潜在空间对齐感知与动作，并将未来帧预测内化为动作生成的归纳偏置，实现正向与逆向动力学联合建模。\n2. **训练策略**：设计渐进式训练方法（warmup + co-train + fine-tune），使模型从大规模人类演示视频中学习通用运动动态，同时过滤无关像素细节。\n3. **性能提升**：在CALVIN、LIBERO和SimplerEnv模拟基准上分别提升14.5%、9.6%和12.1%，真实世界任务平均成功率达80.5%。\n### 论文方法描述\n1. **跨模态向量量化框架**：\n - **视觉分支**：使用DINOv2提取连续帧特征，经M-Former生成运动嵌入，通过共享码本量化后解码为未来帧（L1 + SSIM损失）。\n - **动作分支**：对动作序列进行离散余弦变换（DCT）和频率编码，量化后解码重建动作（MSE损失）。\n - 共享码本（Codebook）统一视觉与动作表示，无需跨模态对齐数据。\n2. **VLM主干架构**：\n - **渐进注意力机制**：分阶段处理输入token → 文本子任务token → 跨模态token，确保信息流单向性（Input → Textual → Cross-modal）。\n - **文本CoT**：生成符号化子任务序列（如[GRASP], [MOVE]），作为高层规划指导。\n - **隐式视觉CoT**：基于子任务token生成跨模态token，同步解码为未来帧和动作序列。\n3. **训练流程**：\n - **Warmup阶段**：独立训练视觉/动作自编码器和共享码本，使用单模态数据（无监督重建损失）。\n - **Co-train阶段**：冻结码本，联合训练VLM与双解码器，混合使用视频数据和同步视频-动作数据（视觉损失 + 动作损失）。\n - **Fine-tune阶段**：仅微调动作解码器于特定任务数据。\n### 论文使用数据集和训练资源\n1. **混合数据集**：\n - 人类演示视频：SSv2、Ego4D。\n - 机器人数据（真实）：OXE、RoboMIND。\n - 机器人数据（模拟）：CALVIN-ABC、LIBERO。\n2. **训练资源**：\n - 模型规模：SigLIP视觉编码器（400M） + Gemma主干（2B） + 视觉/动作解码器（共约324M）。\n - 硬件：16块NVIDIA A100 GPU。\n - 时长：约5天，300K训练步数。\n### 论文使用的评估环境和评估指标\n1. **模拟环境**：\n - **CALVIN**：多步骤家庭任务，评估连续5条指令的完成率（Average Number of Tasks）。\n - **LIBERO**：多任务套件（GOAL/SPATIAL/OBJECT/LONG），评估任务平均成功率。\n - **SimplerEnv**：Google Robot和WidowX平台，评估视觉匹配任务成功率。\n2. **真实环境**：\n - **UR-5e机器人平台**：6项桌面操作任务，评估分布内（ID）和分布外（OOD）成功率。\n3. **评估指标**：\n - **CALVIN**：连续任务完成率（1-5任务）及平均完成长度（Avg. Len）。\n - **LIBERO/SimplerEnv**：任务平均成功率（%）。\n - **真实世界**：任务成功率（%），对比基线包括Pi0、OpenVLA、GR00T N1.5。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南京大学计算机软件新技术国家重点实验室（State Key Laboratory for Novel Software Technology, Nanjing University）</p>\n<h3>论文概述</h3>\n<p>本文提出VITA（Vision-Integrated Trajectory Alignment）框架，旨在统一视觉感知与机器人动作生成。通过构建跨模态共享的离散潜在空间，VITA将视觉观察与低级动作对齐，并引入隐式视觉思维链（Implicit Visual Chain-of-Thought）：自回归生成的token同时解码为未来帧预测和机器人动作序列，使视觉动态作为动作规划的归纳偏置。该方法解决了视觉-动作模态 gap 和训练不稳定性问题，在模拟与真实环境中实现SOTA性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>框架创新</strong>：提出VITA框架，通过统一潜在空间对齐感知与动作，并将未来帧预测内化为动作生成的归纳偏置，实现正向与逆向动力学联合建模。</li><li><strong>训练策略</strong>：设计渐进式训练方法（warmup + co-train + fine-tune），使模型从大规模人类演示视频中学习通用运动动态，同时过滤无关像素细节。</li><li><strong>性能提升</strong>：在CALVIN、LIBERO和SimplerEnv模拟基准上分别提升14.5%、9.6%和12.1%，真实世界任务平均成功率达80.5%。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>跨模态向量量化框架</strong>：</li></ol>\n<p> - <strong>视觉分支</strong>：使用DINOv2提取连续帧特征，经M-Former生成运动嵌入，通过共享码本量化后解码为未来帧（L1 + SSIM损失）。</p>\n<p> - <strong>动作分支</strong>：对动作序列进行离散余弦变换（DCT）和频率编码，量化后解码重建动作（MSE损失）。</p>\n<p> - 共享码本（Codebook）统一视觉与动作表示，无需跨模态对齐数据。</p>\n<ol><li><strong>VLM主干架构</strong>：</li></ol>\n<p> - <strong>渐进注意力机制</strong>：分阶段处理输入token → 文本子任务token → 跨模态token，确保信息流单向性（Input → Textual → Cross-modal）。</p>\n<p> - <strong>文本CoT</strong>：生成符号化子任务序列（如[GRASP], [MOVE]），作为高层规划指导。</p>\n<p> - <strong>隐式视觉CoT</strong>：基于子任务token生成跨模态token，同步解码为未来帧和动作序列。</p>\n<ol><li><strong>训练流程</strong>：</li></ol>\n<p> - <strong>Warmup阶段</strong>：独立训练视觉/动作自编码器和共享码本，使用单模态数据（无监督重建损失）。</p>\n<p> - <strong>Co-train阶段</strong>：冻结码本，联合训练VLM与双解码器，混合使用视频数据和同步视频-动作数据（视觉损失 + 动作损失）。</p>\n<p> - <strong>Fine-tune阶段</strong>：仅微调动作解码器于特定任务数据。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>混合数据集</strong>：</li></ol>\n<p> - 人类演示视频：SSv2、Ego4D。</p>\n<p> - 机器人数据（真实）：OXE、RoboMIND。</p>\n<p> - 机器人数据（模拟）：CALVIN-ABC、LIBERO。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> - 模型规模：SigLIP视觉编码器（400M） + Gemma主干（2B） + 视觉/动作解码器（共约324M）。</p>\n<p> - 硬件：16块NVIDIA A100 GPU。</p>\n<p> - 时长：约5天，300K训练步数。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>模拟环境</strong>：</li></ol>\n<p> - <strong>CALVIN</strong>：多步骤家庭任务，评估连续5条指令的完成率（Average Number of Tasks）。</p>\n<p> - <strong>LIBERO</strong>：多任务套件（GOAL/SPATIAL/OBJECT/LONG），评估任务平均成功率。</p>\n<p> - <strong>SimplerEnv</strong>：Google Robot和WidowX平台，评估视觉匹配任务成功率。</p>\n<ol><li><strong>真实环境</strong>：</li></ol>\n<p> - <strong>UR-5e机器人平台</strong>：6项桌面操作任务，评估分布内（ID）和分布外（OOD）成功率。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>CALVIN</strong>：连续任务完成率（1-5任务）及平均完成长度（Avg. Len）。</p>\n<p> - <strong>LIBERO/SimplerEnv</strong>：任务平均成功率（%）。</p>\n<p> - <strong>真实世界</strong>：任务成功率（%），对比基线包括Pi0、OpenVLA、GR00T N1.5。</p>"
  },
  {
    "date": "2025-11-24",
    "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories",
    "link": "http://arxiv.org/abs/2511.19528",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n香港科技大学、清华大学、武汉大学、中南大学、微软研究院\n## 论文概述\n该论文提出了一个名为“发现、学习和强化”（DLR）的三阶段框架，用于生成多样化且高质量的机器人轨迹，以扩大视觉-语言-动作（VLA）模型的预训练规模。该框架旨在通过强化学习（RL）生成数据，解决当前依赖人类遥操作数据成本高昂且行为多样性有限的问题。论文通过理论分析和实验验证了DLR在生成多样化轨迹和提高下游任务泛化能力方面的有效性。\n## 论文核心贡献点\n1. 提出了一个原则性的三阶段框架DLR，利用强化学习为VLA预训练生成高质量且多样化的机器人轨迹。\n2. 提供了理论分析，证明DLR能够保持发现模式的多样性，防止模式坍塌到单一解决方案。\n3. 实验表明，DLR不仅能生成多样化的成功轨迹，还能使预训练的VLA模型在下游任务微调后表现更好。\n## 论文方法描述\nDLR框架包含三个阶段：\n1. 发现：使用基于变分自编码器（VAE）的信息论方法从人类演示中挖掘潜在的多样化行为模式。\n2. 学习：通过行为克隆训练一个条件策略来模仿这些发现的行为模式。\n3. 强化：使用稀疏的任务奖励在线优化每个条件策略，使不同模式收敛到各自对应的精炼解决方案。\n该方法通过解耦多样性目标与策略的探索过程，将多样性目标仅应用于成功轨迹内的状态，避免了标准方法中探索与多样性奖励之间的冲突。\n## 论文使用数据集和训练资源\n数据集：LIBERO基准，特别是LIBERO-90用于预训练，LIBEROSpatial/LIBEROObject/LIBEROGoal/LIBEROLong用于下游评估。\n训练资源：使用ResNet18作为视觉编码器，结合多层感知机（MLP）头输出动作。强化学习算法采用PPO。DLR设置模式数量\\|Z\\|=3，并使用均匀分布采样。在相同的预训练步数和数据量下与基线公平比较。\n## 论文使用的评估环境和评估指标\n评估环境：LIBERO模拟环境，包含多种操作任务，用于评估 lifelong learning 的知识迁移能力。\n评估指标：\n- 轨迹多样性指标：包括平均成对距离、终点方差、方向方差和路径长度方差。\n- 下游任务成功率：在LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long任务套件上的平均成功率，通过50次运行取平均。\n- 数据缩放行为：通过绘制下游成功率随RL生成数据量变化的曲线来评估。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>香港科技大学、清华大学、武汉大学、中南大学、微软研究院</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>该论文提出了一个名为“发现、学习和强化”（DLR）的三阶段框架，用于生成多样化且高质量的机器人轨迹，以扩大视觉-语言-动作（VLA）模型的预训练规模。该框架旨在通过强化学习（RL）生成数据，解决当前依赖人类遥操作数据成本高昂且行为多样性有限的问题。论文通过理论分析和实验验证了DLR在生成多样化轨迹和提高下游任务泛化能力方面的有效性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出了一个原则性的三阶段框架DLR，利用强化学习为VLA预训练生成高质量且多样化的机器人轨迹。</li><li>提供了理论分析，证明DLR能够保持发现模式的多样性，防止模式坍塌到单一解决方案。</li><li>实验表明，DLR不仅能生成多样化的成功轨迹，还能使预训练的VLA模型在下游任务微调后表现更好。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>DLR框架包含三个阶段：</p>\n<ol><li>发现：使用基于变分自编码器（VAE）的信息论方法从人类演示中挖掘潜在的多样化行为模式。</li><li>学习：通过行为克隆训练一个条件策略来模仿这些发现的行为模式。</li><li>强化：使用稀疏的任务奖励在线优化每个条件策略，使不同模式收敛到各自对应的精炼解决方案。</li></ol>\n<p>该方法通过解耦多样性目标与策略的探索过程，将多样性目标仅应用于成功轨迹内的状态，避免了标准方法中探索与多样性奖励之间的冲突。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<p>数据集：LIBERO基准，特别是LIBERO-90用于预训练，LIBEROSpatial/LIBEROObject/LIBEROGoal/LIBEROLong用于下游评估。</p>\n<p>训练资源：使用ResNet18作为视觉编码器，结合多层感知机（MLP）头输出动作。强化学习算法采用PPO。DLR设置模式数量\\|Z\\|=3，并使用均匀分布采样。在相同的预训练步数和数据量下与基线公平比较。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p>评估环境：LIBERO模拟环境，包含多种操作任务，用于评估 lifelong learning 的知识迁移能力。</p>\n<p>评估指标：</p>\n<ul><li>轨迹多样性指标：包括平均成对距离、终点方差、方向方差和路径长度方差。</li><li>下游任务成功率：在LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long任务套件上的平均成功率，通过50次运行取平均。</li><li>数据缩放行为：通过绘制下游成功率随RL生成数据量变化的曲线来评估。</li></ul>"
  },
  {
    "date": "2025-11-24",
    "title": "Mixture of Horizons in Action Chunking",
    "link": "http://arxiv.org/abs/2511.19433",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n中国人民大学 (RUC), 北卡罗来纳大学教堂山分校 (UNC), 香港中文大学 (CUHK)\n## 论文概述\n该论文研究了视觉-语言-动作（VLA）模型中动作块长度（horizon）的影响。研究发现存在一个固有的权衡：较长的horizon提供更好的全局规划能力但会降低细粒度精度，而较短的horizon提供精确的局部控制但在长期任务上表现不佳。为解决这一问题，论文提出了混合视界策略，通过将动作块重组为不同长度的分段，使用共享的动作变换器并行处理，并通过轻量级线性门控融合输出。\n## 论文核心贡献点\n1. 系统研究了VLA中动作块horizon的影响，揭示了长期预测与短期精度之间的关键权衡。\n2. 提出了混合视界策略，这是一个即插即用、低开销的方法，缓解了上述权衡，提升了性能和泛化能力。\n3. 提出了通过跨horizon共识的动态推理方案，以实现更稳定、更快的执行。\n## 论文方法描述\n混合视界策略包含以下关键组件：\n- 动作块重排：将最大horizon H的动作块按候选集合 H={h1, ..., hN} 截断为多个子块。\n- 共享处理：使用单一动作变换器并行处理所有horizon的子块，通过特定注意力掩码处理不同长度。\n- 门控融合：添加轻量级线性门控头，为每个时间步和horizon生成权重，通过softmax归一化后加权融合各horizon预测。\n- 平衡损失：引入负载平衡损失，鼓励所有horizon被均衡利用，避免门控网络退化。\n- 动态推理：通过跨horizon共识机制，选择稳定的动作前缀执行，将不确定动作推迟到下次重规划，提高吞吐量。\n## 论文使用数据集和训练资源\n- 数据集：LIBERO（包含Spatial、Object、Goal、Long四个任务套件）和RoboTwin 2.0（包含50个双手机器人任务）。\n- 训练资源：4块NVIDIA A100 GPU，训练30k迭代，批量大小32，LIBERO训练在10小时内完成，RoboTwin训练约3k-10k迭代。\n- 基础模型：基于PaliGemma的π系列模型（π0、π0.5、πreg）。\n## 论文使用的评估环境和评估指标\n- 评估环境：LIBERO仿真环境（评估500次试验）和RoboTwin 2.0仿真环境（简单和困难模式各评估100次试验）。\n- 评估指标：任务成功率，LIBERO执行每个动作块的前5步，RoboTwin执行前20步。\n- 比较方法：包括Octo、OpenVLA、Diffusion Policy等SOTA方法，在相同随机种子下进行公平比较。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>中国人民大学 (RUC), 北卡罗来纳大学教堂山分校 (UNC), 香港中文大学 (CUHK)</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>该论文研究了视觉-语言-动作（VLA）模型中动作块长度（horizon）的影响。研究发现存在一个固有的权衡：较长的horizon提供更好的全局规划能力但会降低细粒度精度，而较短的horizon提供精确的局部控制但在长期任务上表现不佳。为解决这一问题，论文提出了混合视界策略，通过将动作块重组为不同长度的分段，使用共享的动作变换器并行处理，并通过轻量级线性门控融合输出。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>系统研究了VLA中动作块horizon的影响，揭示了长期预测与短期精度之间的关键权衡。</li><li>提出了混合视界策略，这是一个即插即用、低开销的方法，缓解了上述权衡，提升了性能和泛化能力。</li><li>提出了通过跨horizon共识的动态推理方案，以实现更稳定、更快的执行。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>混合视界策略包含以下关键组件：</p>\n<ul><li>动作块重排：将最大horizon H的动作块按候选集合 H={h1, ..., hN} 截断为多个子块。</li><li>共享处理：使用单一动作变换器并行处理所有horizon的子块，通过特定注意力掩码处理不同长度。</li><li>门控融合：添加轻量级线性门控头，为每个时间步和horizon生成权重，通过softmax归一化后加权融合各horizon预测。</li><li>平衡损失：引入负载平衡损失，鼓励所有horizon被均衡利用，避免门控网络退化。</li><li>动态推理：通过跨horizon共识机制，选择稳定的动作前缀执行，将不确定动作推迟到下次重规划，提高吞吐量。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据集：LIBERO（包含Spatial、Object、Goal、Long四个任务套件）和RoboTwin 2.0（包含50个双手机器人任务）。</li><li>训练资源：4块NVIDIA A100 GPU，训练30k迭代，批量大小32，LIBERO训练在10小时内完成，RoboTwin训练约3k-10k迭代。</li><li>基础模型：基于PaliGemma的π系列模型（π0、π0.5、πreg）。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：LIBERO仿真环境（评估500次试验）和RoboTwin 2.0仿真环境（简单和困难模式各评估100次试验）。</li><li>评估指标：任务成功率，LIBERO执行每个动作块的前5步，RoboTwin执行前20步。</li><li>比较方法：包括Octo、OpenVLA、Diffusion Policy等SOTA方法，在相同随机种子下进行公平比较。</li></ul>"
  },
  {
    "date": "2025-11-24",
    "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.19221",
    "summary_markdown": "### 论文研究单位\nYinwang Intelligent Technology Co. Ltd., Fudan University\n### 论文概述\nPercept-WAM是一种感知增强的世界-意识-动作模型，首次在单个视觉语言模型中隐式集成2D/3D场景理解能力。该模型通过World-PV和World-BEV token统一2D和3D感知任务，编码空间坐标和置信度。采用网格条件预测机制、IoU感知评分和平行自回归解码，提升长尾场景的稳定性。模型保留预训练VLM的通用智能能力，可直接输出感知结果和轨迹控制。实验表明Percept-WAM在下游感知基准匹配或超越经典检测器，并在nuScenes和NAVSIM上提升规划性能。\n### 论文核心贡献点\n1. 感知增强的世界Token：首次通过World-PV和World-BEV token在单一VLM中统一2D/3D感知，编码度量坐标和校准置信度。\n2. 网格条件密集感知：引入网格条件预测头，结合IoU感知评分和平行AR解码，显著提升长尾、远距离和小物体感知的准确性和稳定性。\n3. 感知到动作范式：在nuScenes和NAVSIM上超越专业检测/分割基线，通过World-PV、World-BEV和World-Action token的对齐实现卓越规划性能。\n### 论文方法描述\nPercept-WAM包含三部分：1) VLM主干（InternVL2-8B）维持通用推理能力；2) 可学习BEV网格token隐式建模PV特征到BEV空间的映射；3) 动作专家头用于轨迹解码。World-PV分支处理透视视图感知，支持高分辨率输入和平行AR解码，输出序列格式为cls, <box>坐标</box>, <conf>置信度</conf>。World-BEV分支通过可学习查询token实现BEV空间感知，支持多模态融合。动作模块通过四组点级查询（Q_pv, Q_bev, Q_ego, Q_full）对齐多模态信息，使用MLP解码轨迹，并采用流式KV缓存策略提升效率。\n### 论文使用数据集和训练资源\n数据集：nuImages（2D检测/实例分割/语义分割）、nuScenes（3D检测/BEV分割/轨迹预测）、COCO（2D检测/实例分割/语义分割）、Waymo（3D检测）、RefCOCO系列（定位）、ADE20K（语义分割）、COCOStuff（语义分割）、NAVSIM（轨迹预测）、DriveLM-nuscenes（QA）等。训练使用AdamW优化器（基础学习率0.0002，权重衰减0.01），余弦衰减学习率，混合精度训练，梯度检查点。World-PV采用10×10网格，World-BEV检测使用40×40网格。\n### 论文使用的评估环境和评估指标\n评估环境：nuScenes验证集（PV/BEV感知任务）、NAVSIM v1（闭环规划）、nuScenes开放环路指标（轨迹L2误差）。评估指标：2D/3D检测mAP、NDS（nuScenes检测分数）、实例分割mAP、语义分割mIoU、轨迹L2误差（1s/2s/3s）、NAVSIM闭环指标（NC碰撞率、DAC动态舒适度、TTC时间至碰撞、Comf舒适性、EP效率、PDMS规划得分）、帧延迟（ms）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Yinwang Intelligent Technology Co. Ltd., Fudan University</p>\n<h3>论文概述</h3>\n<p>Percept-WAM是一种感知增强的世界-意识-动作模型，首次在单个视觉语言模型中隐式集成2D/3D场景理解能力。该模型通过World-PV和World-BEV token统一2D和3D感知任务，编码空间坐标和置信度。采用网格条件预测机制、IoU感知评分和平行自回归解码，提升长尾场景的稳定性。模型保留预训练VLM的通用智能能力，可直接输出感知结果和轨迹控制。实验表明Percept-WAM在下游感知基准匹配或超越经典检测器，并在nuScenes和NAVSIM上提升规划性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>感知增强的世界Token：首次通过World-PV和World-BEV token在单一VLM中统一2D/3D感知，编码度量坐标和校准置信度。</li><li>网格条件密集感知：引入网格条件预测头，结合IoU感知评分和平行AR解码，显著提升长尾、远距离和小物体感知的准确性和稳定性。</li><li>感知到动作范式：在nuScenes和NAVSIM上超越专业检测/分割基线，通过World-PV、World-BEV和World-Action token的对齐实现卓越规划性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>Percept-WAM包含三部分：1) VLM主干（InternVL2-8B）维持通用推理能力；2) 可学习BEV网格token隐式建模PV特征到BEV空间的映射；3) 动作专家头用于轨迹解码。World-PV分支处理透视视图感知，支持高分辨率输入和平行AR解码，输出序列格式为cls, <box>坐标</box>, <conf>置信度</conf>。World-BEV分支通过可学习查询token实现BEV空间感知，支持多模态融合。动作模块通过四组点级查询（Q_pv, Q_bev, Q_ego, Q_full）对齐多模态信息，使用MLP解码轨迹，并采用流式KV缓存策略提升效率。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：nuImages（2D检测/实例分割/语义分割）、nuScenes（3D检测/BEV分割/轨迹预测）、COCO（2D检测/实例分割/语义分割）、Waymo（3D检测）、RefCOCO系列（定位）、ADE20K（语义分割）、COCOStuff（语义分割）、NAVSIM（轨迹预测）、DriveLM-nuscenes（QA）等。训练使用AdamW优化器（基础学习率0.0002，权重衰减0.01），余弦衰减学习率，混合精度训练，梯度检查点。World-PV采用10×10网格，World-BEV检测使用40×40网格。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：nuScenes验证集（PV/BEV感知任务）、NAVSIM v1（闭环规划）、nuScenes开放环路指标（轨迹L2误差）。评估指标：2D/3D检测mAP、NDS（nuScenes检测分数）、实例分割mAP、语义分割mIoU、轨迹L2误差（1s/2s/3s）、NAVSIM闭环指标（NC碰撞率、DAC动态舒适度、TTC时间至碰撞、Comf舒适性、EP效率、PDMS规划得分）、帧延迟（ms）。</p>"
  },
  {
    "date": "2025-11-24",
    "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
    "link": "http://arxiv.org/abs/2511.18960",
    "summary_markdown": "### 论文研究单位\nLiAuto Inc., School of Information Science and Technology, Beijing University of Technology, School of Data Science, The Chinese University of Hong Kong, Shenzhen\n### 论文概述\n论文指出现有的视觉-语言-行动模型通常基于视觉-语言模型，将每个时间步的视觉输入作为独立帧处理，这种隐式的马尔可夫决策过程建模未能有效利用历史上下文，在动态的序列决策中是次优的。为解决此问题，论文从部分可观察马尔可夫决策过程的视角重新构建了问题，提出了AVA-VLA框架。该框架引入一个循环状态作为代理信念状态的神经网络近似，并设计了主动视觉注意模块，利用该循环状态动态调制当前帧的视觉处理，使模型能基于历史上下文主动过滤无关信息并聚焦于任务关键特征。\n### 论文核心贡献点\n1. 提出了新颖的AVA-VLA框架，通过受POMDP启发的方法，首次显式地解决了基于MDP的VLA模型中缺乏历史上下文的关键限制。\n2. 引入了主动视觉注意模块，利用循环状态来动态调制当前帧的视觉处理，以进行动作预测。\n3. 在模拟和真实世界任务中进行了全面评估，证明了AVA-VLA框架提升了VLA性能，并在多个机器人任务上取得了最先进的结果。\n### 论文方法描述\nAVA-VLA框架包含三个核心部分：\n1. **问题重构建**：将机器人操作任务从MDP重构建为POMDP。引入一个循环状态 r^{t-1} 作为代理信念状态的神经近似，该状态由上一时间步 t-1 的模型中间输出计算得出。策略预测不仅基于当前观测 x^t，还显式地基于循环状态 r^{t-1}。\n2. **主动视觉注意模块**：该模块接收当前观测和循环状态，通过交叉注意力机制和自注意力层，计算视觉标记的软权重 ω^t。这些权重被应用于语言模型主干所有层的注意力矩阵，动态增强或减弱不同视觉标记的重要性，使模型能根据历史信念主动聚焦于任务相关区域。\n3. **训练与推理**：训练采用截断的反向传播时间策略，以平衡计算可行性。损失函数包括动作块的平均绝对误差损失和对软权重均值的L2正则化惩罚。推理时，模型以完全循环的方式运行，在每个时间步更新循环状态。\n### 论文使用数据集和训练资源\n- **数据集**：\n - LIBERO 和 LIBERO+ 基准\n - CALVIN 基准（ABC到D的零样本泛化设置）\n - 真实世界任务：基于Mobile ALOHA双臂机器人的四个操作任务\n- **训练资源**：\n - 基础模型：OpenVLA-OFT\n - 计算平台：Nvidia A800 GPU\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：LIBERO（MuJoCo模拟器）和CALVIN的模拟环境。\n - 真实环境：一个桌置式双臂机器人平台。\n- **评估指标**：\n - 成功率：用于LIBERO、CALVIN和真实世界任务的主要评估指标。\n - 平均长度：用于CALVIN基准的补充指标，衡量任务完成的平均长度。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>LiAuto Inc., School of Information Science and Technology, Beijing University of Technology, School of Data Science, The Chinese University of Hong Kong, Shenzhen</p>\n<h3>论文概述</h3>\n<p>论文指出现有的视觉-语言-行动模型通常基于视觉-语言模型，将每个时间步的视觉输入作为独立帧处理，这种隐式的马尔可夫决策过程建模未能有效利用历史上下文，在动态的序列决策中是次优的。为解决此问题，论文从部分可观察马尔可夫决策过程的视角重新构建了问题，提出了AVA-VLA框架。该框架引入一个循环状态作为代理信念状态的神经网络近似，并设计了主动视觉注意模块，利用该循环状态动态调制当前帧的视觉处理，使模型能基于历史上下文主动过滤无关信息并聚焦于任务关键特征。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了新颖的AVA-VLA框架，通过受POMDP启发的方法，首次显式地解决了基于MDP的VLA模型中缺乏历史上下文的关键限制。</li><li>引入了主动视觉注意模块，利用循环状态来动态调制当前帧的视觉处理，以进行动作预测。</li><li>在模拟和真实世界任务中进行了全面评估，证明了AVA-VLA框架提升了VLA性能，并在多个机器人任务上取得了最先进的结果。</li></ol>\n<h3>论文方法描述</h3>\n<p>AVA-VLA框架包含三个核心部分：</p>\n<ol><li><strong>问题重构建</strong>：将机器人操作任务从MDP重构建为POMDP。引入一个循环状态 r^{t-1} 作为代理信念状态的神经近似，该状态由上一时间步 t-1 的模型中间输出计算得出。策略预测不仅基于当前观测 x^t，还显式地基于循环状态 r^{t-1}。</li><li><strong>主动视觉注意模块</strong>：该模块接收当前观测和循环状态，通过交叉注意力机制和自注意力层，计算视觉标记的软权重 ω^t。这些权重被应用于语言模型主干所有层的注意力矩阵，动态增强或减弱不同视觉标记的重要性，使模型能根据历史信念主动聚焦于任务相关区域。</li><li><strong>训练与推理</strong>：训练采用截断的反向传播时间策略，以平衡计算可行性。损失函数包括动作块的平均绝对误差损失和对软权重均值的L2正则化惩罚。推理时，模型以完全循环的方式运行，在每个时间步更新循环状态。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - LIBERO 和 LIBERO+ 基准</p>\n<p> - CALVIN 基准（ABC到D的零样本泛化设置）</p>\n<p> - 真实世界任务：基于Mobile ALOHA双臂机器人的四个操作任务</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 基础模型：OpenVLA-OFT</p>\n<p> - 计算平台：Nvidia A800 GPU</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：LIBERO（MuJoCo模拟器）和CALVIN的模拟环境。</p>\n<p> - 真实环境：一个桌置式双臂机器人平台。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率：用于LIBERO、CALVIN和真实世界任务的主要评估指标。</p>\n<p> - 平均长度：用于CALVIN基准的补充指标，衡量任务完成的平均长度。</p>"
  },
  {
    "date": "2025-11-24",
    "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.18950",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-24",
    "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent",
    "link": "http://arxiv.org/abs/2511.18810",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-22",
    "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation",
    "link": "http://arxiv.org/abs/2511.18112",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-22",
    "title": "Continually Evolving Skill Knowledge in Vision Language Action Model",
    "link": "http://arxiv.org/abs/2511.18085",
    "summary_markdown": "### 论文研究单位\n上海交通大学、上海创新研究院、剑桥大学、北京航空航天大学、AgiBot\n### 论文概述\n论文提出Stellar VLA框架，用于视觉-语言-动作模型（VLA）的持续模仿学习（CIL）。该框架通过联合学习任务中心表示和自演化知识空间，使模型能够在获取新技能的同时缓解对先前任务的遗忘。Stellar VLA包含两个变体：T-Stellar（任务级建模）和TS-Stellar（分层任务-技能建模），旨在实现知识驱动的高效持续学习。\n### 论文核心贡献点\n1. 提出Stellar VLA框架，通过自监督的知识演化循环实现任务知识的保留与发现。\n2. 设计基于狄利克雷过程（DP）的知识空间，支持无限任务/技能聚类，自适应扩展知识表示。\n3. 引入知识引导的专家路由机制，在动作头中实现参数共享与任务特化的平衡，降低训练开销。\n4. 在LIBERO基准和真实任务中验证有效性，平均最终成功率相比基线提升超50%。\n### 论文方法描述\n1. **知识空间建模**：\n - T-Stellar使用狄利克雷过程混合模型（DPMM）构建离散任务级知识空间。\n - TS-Stellar采用分层狄利克雷过程（HDP）建模任务-技能层次关系，捕获跨任务共享的子技能。\n2. **自监督学习**：\n - 通过变分自编码器（VAE）学习任务中心表示，约束其聚合在知识空间内。\n - 使用基于记忆的变分贝叶斯（memoVB）算法更新知识分布，支持增量学习。\n3. **专家路由**：\n - 在动作头中集成混合专家（MoE）架构，动态分配专家。\n - 设计知识关系嵌入（距离计算）和Top-K语义嵌入（可学习聚类嵌入）指导专家选择。\n - 路由特征融合知识嵌入、噪声和语言令牌。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 模拟：LIBERO基准（LIBERO-goal, LIBERO-long, LIBERO-30*），涵盖多样化目标、长时序推理和多任务场景。\n - 真实世界：双臂机器人任务（\"Transfer Magic Stick\"、\"Pick up Bag\"、\"Handover Toy\"）。\n- **训练设置**：\n - 采用经验回放（Experience Replay），仅存储1%过去数据以减少内存开销。\n - 所有模型从零开始训练（非预训练）。\n - 真实任务中使用5%回放率确保稳定性。\n- **资源**：\n - 实验使用双臂机器人平台，观测来自头部和腕部摄像头，输出为关节姿态和夹爪状态。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟：LIBERO仿真环境。\n - 真实：人形机器人平台，配备多视角摄像头。\n- **评估指标**：\n - FWT（前向迁移，越高越好）：衡量学习新任务的能力。\n - NBT（反向负迁移，越低越好）：衡量遗忘程度。\n - AUC（成功率曲线下面积，越高越好）：反映整体性能稳定性。\n - Final SR（最终平均成功率，越高越好）：所有任务训练后的综合表现。\n - 每个策略在先前任务上评估100次，构建成功率矩阵进行计算。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学、上海创新研究院、剑桥大学、北京航空航天大学、AgiBot</p>\n<h3>论文概述</h3>\n<p>论文提出Stellar VLA框架，用于视觉-语言-动作模型（VLA）的持续模仿学习（CIL）。该框架通过联合学习任务中心表示和自演化知识空间，使模型能够在获取新技能的同时缓解对先前任务的遗忘。Stellar VLA包含两个变体：T-Stellar（任务级建模）和TS-Stellar（分层任务-技能建模），旨在实现知识驱动的高效持续学习。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出Stellar VLA框架，通过自监督的知识演化循环实现任务知识的保留与发现。</li><li>设计基于狄利克雷过程（DP）的知识空间，支持无限任务/技能聚类，自适应扩展知识表示。</li><li>引入知识引导的专家路由机制，在动作头中实现参数共享与任务特化的平衡，降低训练开销。</li><li>在LIBERO基准和真实任务中验证有效性，平均最终成功率相比基线提升超50%。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>知识空间建模</strong>：</li></ol>\n<p> - T-Stellar使用狄利克雷过程混合模型（DPMM）构建离散任务级知识空间。</p>\n<p> - TS-Stellar采用分层狄利克雷过程（HDP）建模任务-技能层次关系，捕获跨任务共享的子技能。</p>\n<ol><li><strong>自监督学习</strong>：</li></ol>\n<p> - 通过变分自编码器（VAE）学习任务中心表示，约束其聚合在知识空间内。</p>\n<p> - 使用基于记忆的变分贝叶斯（memoVB）算法更新知识分布，支持增量学习。</p>\n<ol><li><strong>专家路由</strong>：</li></ol>\n<p> - 在动作头中集成混合专家（MoE）架构，动态分配专家。</p>\n<p> - 设计知识关系嵌入（距离计算）和Top-K语义嵌入（可学习聚类嵌入）指导专家选择。</p>\n<p> - 路由特征融合知识嵌入、噪声和语言令牌。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟：LIBERO基准（LIBERO-goal, LIBERO-long, LIBERO-30*），涵盖多样化目标、长时序推理和多任务场景。</p>\n<p> - 真实世界：双臂机器人任务（\"Transfer Magic Stick\"、\"Pick up Bag\"、\"Handover Toy\"）。</p>\n<ul><li><strong>训练设置</strong>：</li></ul>\n<p> - 采用经验回放（Experience Replay），仅存储1%过去数据以减少内存开销。</p>\n<p> - 所有模型从零开始训练（非预训练）。</p>\n<p> - 真实任务中使用5%回放率确保稳定性。</p>\n<ul><li><strong>资源</strong>：</li></ul>\n<p> - 实验使用双臂机器人平台，观测来自头部和腕部摄像头，输出为关节姿态和夹爪状态。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟：LIBERO仿真环境。</p>\n<p> - 真实：人形机器人平台，配备多视角摄像头。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - FWT（前向迁移，越高越好）：衡量学习新任务的能力。</p>\n<p> - NBT（反向负迁移，越低越好）：衡量遗忘程度。</p>\n<p> - AUC（成功率曲线下面积，越高越好）：反映整体性能稳定性。</p>\n<p> - Final SR（最终平均成功率，越高越好）：所有任务训练后的综合表现。</p>\n<p> - 每个策略在先前任务上评估100次，构建成功率矩阵进行计算。</p>"
  },
  {
    "date": "2025-11-22",
    "title": "ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.18082",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-22",
    "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
    "link": "http://arxiv.org/abs/2511.17889",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-21",
    "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
    "link": "http://arxiv.org/abs/2511.17502",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-21",
    "title": "METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2511.17366",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-21",
    "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.17199",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-20",
    "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy",
    "link": "http://arxiv.org/abs/2511.16651",
    "summary_markdown": "### 论文研究单位\n上海人工智能实验室，北京大学\n### 论文概述\n该论文提出了InternData-A1，一个大规模、高保真的合成机器人操作数据集。论文旨在验证纯合成数据在预训练视觉-语言-动作（VLA）模型时的有效性。研究团队通过一个高度自动化、完全解耦和组合式的仿真流水线生成了该数据集，涵盖了4种机器人形态、18种技能、70个任务、227个场景，总计63万条轨迹和7433小时的数据，涉及刚性、铰接、柔性及流体物体的操作。核心发现是，一个仅使用InternData-A1预训练的π0模型，其性能可以媲美甚至超过在闭源的真实数据集π-dataset上预训练的官方π0模型，并且在多项任务上表现出惊人的零样本仿真到真实世界的迁移能力。论文将开源该数据集及其生成流水线，以降低机器人研究的数据获取门槛。\n### 论文核心贡献点\n1. 首次证明了纯合成数据可以匹敌最强的真实机器人数据集在VLA模型预训练上的效果，揭示了大规模仿真的巨大潜力。\n2. 推出了InternData-A1，一个具有高物理保真度和照片级渲染效果的合成数据集，其规模（63万轨迹）和多样性（4种机器人、70任务、227场景，覆盖刚性、铰接、柔性、流体物体）均处于领先地位。\n3. 提出了一种高度自动化、完全解耦、组合式的数据合成流水线，支持长时程技能组合、灵活的任务组装和异构机器人，实现了最少人工干预下的高效数据生成（每段轨迹成本低于0.003美元）。\n4. 展示了强大的Sim-to-Real迁移能力，在多个具有挑战性的真实任务上实现了超过50%的零样本迁移成功率，验证了数据的高保真度。\n### 论文方法描述\n论文的数据合成流程分为四个主要阶段：\n1. 环境构建：从资产库中检索并配置机器人、场景（来自GRUtopia）和物体。物体库包含刚性、�接、柔性和流体四类，所有资产都带有详细的物理和功能注释。\n2. 技能组合：任务由模块化的原子技能（如抓取、放置、推动）组合而成。每个技能是一个脚本化策略，输入物体和机器人状态，输出一系列末端执行器的路径点。用户通过配置文件顺序或并行地组织这些技能以构建复杂任务。\n3. 领域随机化：对相机视角（±5°旋转，±5cm平移）、光照（174种环境图）、物体位姿、接触区域等进行随机化，以增强视觉和轨迹的多样性。\n4. 生成与存储：使用CuRobo运动规划器在技能生成的路径点之间插值出密集的关节空间动作。成功验证的轨迹会被记录，包括多视角RGB图像、机器人状态、动作标签等，并转换为LeRobot标准格式。\n此外，论文还介绍了框架层面的多级系统优化，通过将轨迹规划与视觉渲染解耦、动态资源调度和引入堆叠渲染技术，实现了2-3倍的端到端性能提升。\n### 论文使用数据集和训练资源\n数据集：\n- 名称：InternData-A1\n- 规模：包含637,498条轨迹，401,430,981帧，总计7,433.91小时。\n- 内容：覆盖4种机器人形态（AgiBot Genie-1, Franka Panda, AgileX Split Aloha, ARX Lift-2），70个任务（分为基础、抓取放置、铰接操作、长时程四类），18种技能，227个室内场景。\n- 物体：包含3,185个刚性物体，321个铰接物体，20件服装以及流体物体。\n训练资源：\n- 预训练：使用32个A100 GPU，在InternData-A1上训练68万步，以匹配官方π0的训练量。\n- 微调：常规任务和Sim-to-Real任务使用8个GPU进行3万步微调，灵巧任务进行10万步微调。\n### 论文使用的评估环境和评估指标\n评估环境：\n- 仿真评估：使用RoboTwin 2.0基准，包含49个双手操作任务，在“简单”（干净）和“困难”（杂乱）两种模式下进行测试。\n- 真实机器人常规任务评估：在Genie-1、ARX Lift-2两种机器人上测试5个任务（如放置马克笔、传递瓶子、加热三明治）。\n- 真实机器人灵巧任务评估：在一种全新的机器人ARX AC One上测试4个长时程灵巧任务（如折叠衣物、拉开拉链袋）。\n- Sim-to-Real零样本迁移评估：在10个选定的真实任务上，仅使用500-1600条仿真轨迹进行微调，然后测试其在真实世界中的表现。\n基线模型：\n- 官方π0模型（在闭源π-dataset上训练）。\n- 从头开始训练的π0模型（无预训练）。\n- 在OXE、Agibot-World、RoboCasa等开源数据集上预训练的模型。\n评估指标：\n- 平均成功率：所有评估的核心指标，报告在多次试验中的平均任务成功率。\n- 试验次数：仿真任务每个评估100次试验，真实世界和Sim-to-Real任务每个评估30次试验。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海人工智能实验室，北京大学</p>\n<h3>论文概述</h3>\n<p>该论文提出了InternData-A1，一个大规模、高保真的合成机器人操作数据集。论文旨在验证纯合成数据在预训练视觉-语言-动作（VLA）模型时的有效性。研究团队通过一个高度自动化、完全解耦和组合式的仿真流水线生成了该数据集，涵盖了4种机器人形态、18种技能、70个任务、227个场景，总计63万条轨迹和7433小时的数据，涉及刚性、铰接、柔性及流体物体的操作。核心发现是，一个仅使用InternData-A1预训练的π0模型，其性能可以媲美甚至超过在闭源的真实数据集π-dataset上预训练的官方π0模型，并且在多项任务上表现出惊人的零样本仿真到真实世界的迁移能力。论文将开源该数据集及其生成流水线，以降低机器人研究的数据获取门槛。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>首次证明了纯合成数据可以匹敌最强的真实机器人数据集在VLA模型预训练上的效果，揭示了大规模仿真的巨大潜力。</li><li>推出了InternData-A1，一个具有高物理保真度和照片级渲染效果的合成数据集，其规模（63万轨迹）和多样性（4种机器人、70任务、227场景，覆盖刚性、铰接、柔性、流体物体）均处于领先地位。</li><li>提出了一种高度自动化、完全解耦、组合式的数据合成流水线，支持长时程技能组合、灵活的任务组装和异构机器人，实现了最少人工干预下的高效数据生成（每段轨迹成本低于0.003美元）。</li><li>展示了强大的Sim-to-Real迁移能力，在多个具有挑战性的真实任务上实现了超过50%的零样本迁移成功率，验证了数据的高保真度。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文的数据合成流程分为四个主要阶段：</p>\n<ol><li>环境构建：从资产库中检索并配置机器人、场景（来自GRUtopia）和物体。物体库包含刚性、�接、柔性和流体四类，所有资产都带有详细的物理和功能注释。</li><li>技能组合：任务由模块化的原子技能（如抓取、放置、推动）组合而成。每个技能是一个脚本化策略，输入物体和机器人状态，输出一系列末端执行器的路径点。用户通过配置文件顺序或并行地组织这些技能以构建复杂任务。</li><li>领域随机化：对相机视角（±5°旋转，±5cm平移）、光照（174种环境图）、物体位姿、接触区域等进行随机化，以增强视觉和轨迹的多样性。</li><li>生成与存储：使用CuRobo运动规划器在技能生成的路径点之间插值出密集的关节空间动作。成功验证的轨迹会被记录，包括多视角RGB图像、机器人状态、动作标签等，并转换为LeRobot标准格式。</li></ol>\n<p>此外，论文还介绍了框架层面的多级系统优化，通过将轨迹规划与视觉渲染解耦、动态资源调度和引入堆叠渲染技术，实现了2-3倍的端到端性能提升。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：</p>\n<ul><li>名称：InternData-A1</li><li>规模：包含637,498条轨迹，401,430,981帧，总计7,433.91小时。</li><li>内容：覆盖4种机器人形态（AgiBot Genie-1, Franka Panda, AgileX Split Aloha, ARX Lift-2），70个任务（分为基础、抓取放置、铰接操作、长时程四类），18种技能，227个室内场景。</li><li>物体：包含3,185个刚性物体，321个铰接物体，20件服装以及流体物体。</li></ul>\n<p>训练资源：</p>\n<ul><li>预训练：使用32个A100 GPU，在InternData-A1上训练68万步，以匹配官方π0的训练量。</li><li>微调：常规任务和Sim-to-Real任务使用8个GPU进行3万步微调，灵巧任务进行10万步微调。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：</p>\n<ul><li>仿真评估：使用RoboTwin 2.0基准，包含49个双手操作任务，在“简单”（干净）和“困难”（杂乱）两种模式下进行测试。</li><li>真实机器人常规任务评估：在Genie-1、ARX Lift-2两种机器人上测试5个任务（如放置马克笔、传递瓶子、加热三明治）。</li><li>真实机器人灵巧任务评估：在一种全新的机器人ARX AC One上测试4个长时程灵巧任务（如折叠衣物、拉开拉链袋）。</li><li>Sim-to-Real零样本迁移评估：在10个选定的真实任务上，仅使用500-1600条仿真轨迹进行微调，然后测试其在真实世界中的表现。</li></ul>\n<p>基线模型：</p>\n<ul><li>官方π0模型（在闭源π-dataset上训练）。</li><li>从头开始训练的π0模型（无预训练）。</li><li>在OXE、Agibot-World、RoboCasa等开源数据集上预训练的模型。</li></ul>\n<p>评估指标：</p>\n<ul><li>平均成功率：所有评估的核心指标，报告在多次试验中的平均任务成功率。</li><li>试验次数：仿真任务每个评估100次试验，真实世界和Sim-to-Real任务每个评估30次试验。</li></ul>"
  },
  {
    "date": "2025-11-20",
    "title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference",
    "link": "http://arxiv.org/abs/2511.16449",
    "summary_markdown": "### 论文研究单位\n上海交通大学人工智能学院、中国科学技术大学、哈尔滨工业大学（深圳）、北京智源人工智能研究院（BAAI）\n### 论文概述\n本文针对视觉-语言-动作（VLA）模型在处理连续视觉流时计算开销大的问题，提出了一种名为VLA-Pruner的免训练、即插即用的视觉令牌剪枝方法。该方法结合了VLA模型的双系统特性（高层语义理解与底层动作执行），并利用机器人操作中的时间连续性，实现了高效推理。实验表明，在多个VLA架构和机器人任务上，该方法在保持精度的同时显著提升了计算效率。\n### 论文核心贡献点\n1. **双层次重要性标准**：同时考虑视觉-语言预填充注意力（语义级相关性）和动作解码注意力（动作级重要性）。\n2. **时间平滑机制**：利用动作解码注意力的时间连续性，通过衰减窗口平均估计当前动作注意力。\n3. **双层次令牌选择策略**：基于最小冗余最大相关性（mRMR）原则，通过相关性最大化合并和冗余最小化过滤，自适应保留信息量高且多样化的视觉令牌。\n4. **即插即用设计**：无需重新训练，兼容主流VLA架构（如OpenVLA、π₀等）。\n### 论文方法描述\n1. **双层次重要性评估**：\n - 语义级重要性：使用视觉-语言预填充阶段的注意力分数（公式2）。\n - 动作级重要性：通过时间平滑（指数衰减窗口平均）估计动作解码注意力（公式7）。\n2. **令牌选择流程**：\n - **双层次Top-k选择**：分别获取语义和动作级重要令牌子集。\n - **相关性最大化合并**：取两个子集的并集作为候选池。\n - **冗余最小化过滤**：求解最大-最小多样性问题（公式8），通过贪心算法基于余弦距离（公式9）选择最终令牌子集。\n3. **实现细节**：在第3层Transformer层剪枝；窗口大小w=3，衰减率γ=0.8；warm-up w步以收集动作注意力历史。\n### 论文使用数据集和训练资源\n- **数据集**：\n - LIBERO：模拟机器人操作任务套件。\n - SIMPLER：多场景机器人评估基准。\n - 真实机器人数据：6-DoF xArm6机器人部署数据。\n- **训练资源**：\n - 免训练方法，无需额外训练数据或资源。\n - 依赖历史动作注意力进行在线估计，warm-up阶段需w=3步。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：LIBERO和SIMPLER基准测试。\n - 真实环境：xArm6物理机器人。\n- **评估指标**：\n - 任务成功率（Success Rate, %）：LIBERO的Spatial/Object/Goal/Long任务平均值。\n - 计算效率：\n - FLOPs（T）：浮点运算量。\n - 推理延迟（ms/action或ms/action-chunk）。\n - 加速比（Speedup, ×）：相对于未剪枝模型的推理速度倍率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学人工智能学院、中国科学技术大学、哈尔滨工业大学（深圳）、北京智源人工智能研究院（BAAI）</p>\n<h3>论文概述</h3>\n<p>本文针对视觉-语言-动作（VLA）模型在处理连续视觉流时计算开销大的问题，提出了一种名为VLA-Pruner的免训练、即插即用的视觉令牌剪枝方法。该方法结合了VLA模型的双系统特性（高层语义理解与底层动作执行），并利用机器人操作中的时间连续性，实现了高效推理。实验表明，在多个VLA架构和机器人任务上，该方法在保持精度的同时显著提升了计算效率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>双层次重要性标准</strong>：同时考虑视觉-语言预填充注意力（语义级相关性）和动作解码注意力（动作级重要性）。</li><li><strong>时间平滑机制</strong>：利用动作解码注意力的时间连续性，通过衰减窗口平均估计当前动作注意力。</li><li><strong>双层次令牌选择策略</strong>：基于最小冗余最大相关性（mRMR）原则，通过相关性最大化合并和冗余最小化过滤，自适应保留信息量高且多样化的视觉令牌。</li><li><strong>即插即用设计</strong>：无需重新训练，兼容主流VLA架构（如OpenVLA、π₀等）。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>双层次重要性评估</strong>：</li></ol>\n<p> - 语义级重要性：使用视觉-语言预填充阶段的注意力分数（公式2）。</p>\n<p> - 动作级重要性：通过时间平滑（指数衰减窗口平均）估计动作解码注意力（公式7）。</p>\n<ol><li><strong>令牌选择流程</strong>：</li></ol>\n<p> - <strong>双层次Top-k选择</strong>：分别获取语义和动作级重要令牌子集。</p>\n<p> - <strong>相关性最大化合并</strong>：取两个子集的并集作为候选池。</p>\n<p> - <strong>冗余最小化过滤</strong>：求解最大-最小多样性问题（公式8），通过贪心算法基于余弦距离（公式9）选择最终令牌子集。</p>\n<ol><li><strong>实现细节</strong>：在第3层Transformer层剪枝；窗口大小w=3，衰减率γ=0.8；warm-up w步以收集动作注意力历史。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - LIBERO：模拟机器人操作任务套件。</p>\n<p> - SIMPLER：多场景机器人评估基准。</p>\n<p> - 真实机器人数据：6-DoF xArm6机器人部署数据。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 免训练方法，无需额外训练数据或资源。</p>\n<p> - 依赖历史动作注意力进行在线估计，warm-up阶段需w=3步。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：LIBERO和SIMPLER基准测试。</p>\n<p> - 真实环境：xArm6物理机器人。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（Success Rate, %）：LIBERO的Spatial/Object/Goal/Long任务平均值。</p>\n<p> - 计算效率：</p>\n<p> - FLOPs（T）：浮点运算量。</p>\n<p> - 推理延迟（ms/action或ms/action-chunk）。</p>\n<p> - 加速比（Speedup, ×）：相对于未剪枝模型的推理速度倍率。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models",
    "link": "http://arxiv.org/abs/2511.16233",
    "summary_markdown": "### 论文研究单位\n根据论文的致谢部分，研究单位与中国国家自然科学基金和重庆市自然科学基金有关，推测主要研究机构位于中国重庆。\n### 论文概述\n论文针对视觉-语言-行动（VLA）模型因依赖大规模、冗余且价值不均的数据集而导致的效率瓶颈问题，提出了一种以数据为中心的生成式数据蒸馏框架FT-NCFM。该框架通过一个自包含的影响评估引擎（Fact-Tracing, FT）来评估样本的内在价值，并利用该评估结果引导一个对抗性的神经特征函数匹配（NCFM）过程，从而合成一个信息密度高、可复用的数据核心集。实验证明，仅使用5%的合成数据训练模型，即可达到使用完整数据集训练85-90%的任务成功率，同时将训练时间减少80%以上，展示了数据蒸馏在构建高效VLA模型中的巨大潜力。\n### 论文核心贡献点\n1. 提出了一种新的生成式数据蒸馏范式，区别于现有的模型压缩和策略蒸馏等以模型为中心的优化路径，从数据层面提升VLA模型效率。\n2. 设计了一个自包含的内在价值评估引擎FT。该引擎通过两阶段评估样本价值：首先使用因果归因进行预筛选，然后通过一个新颖的程序化对比验证模块，为精英样本生成“最小反例”来精炼其影响权重，从而稳健地量化样本的因果贡献和泛化潜力。\n3. 在多个主流VLA基准测试上进行了系统性评估。结果表明，使用FT-NCFM框架和仅5%的合成数据，模型就能达到使用全量数据训练85-90%的性能，同时显著降低了训练时间和计算资源消耗（训练时间减少80%以上），优于多种SOTA方法。\n### 论文方法描述\nFT-NCFM框架包含一个三阶段的流水线：\n1. **VLA多模态表示学习**: 将原始的视觉、语言和行动数据通过各自的编码器和Transformer骨干网络，融合成一个统一的特征向量 `h`，作为后续分析的基础。\n2. **FT影响评估引擎**: 一个两阶段的样本价值评估过程。\n * **阶段一：基于影响函数的因果归因预筛选**: 使用影响函数近似移除单个训练样本对模型损失的影响，计算出基础影响分数。此阶段使用一个“引导模型”，该模型在原始数据上进行轻量级、不完全的训练。\n * **阶段二：对比验证精化**: 对前K%的精英样本，在模拟器中通过“跨模态不匹配”策略程序化地生成一个语义或物理矛盾的“最小反例”。然后，通过比较原始样本和其反例对测试用例的影响差异，使用一个包含tanh函数的权重调制公式来精炼最终的影响权重。\n3. **引导式NCFM**: 将FT引擎计算出的一组影响权重用于指导NCFM的优化过程。通过将原始NCFM的期望计算改为基于权重的加权期望，迫使判别器关注高价值样本，从而引导生成器合成一个富含关键因果知识且信息密度极高的核心集。该核心集与原始数据格式相同，可直接用于训练任何下游VLA模型。\n### 论文使用数据集和训练资源\n* **使用数据集**:\n * CALVIN: 一个用于长期语言条件策略学习的大规模机器人操作基准。\n * Meta-World: 包含50种不同桌面操作任务的基准，用于评估多任务学习环境中的技能获取。\n * LIBERO: 用于促进终身学习的基准套件，包含Spatial、Object、Goal和Long等多个子集。\n* **训练资源**:\n * **硬件**: 实验主要在单块NVIDIA A100 80GB GPU上进行。\n * **软件环境**: Ubuntu 22.04.3 LTS, CUDA 12.4, Python 3.12.7, PyTorch 2.5.0。\n * **时间成本**: FT-NCFM框架的预处理阶段（FT引擎评估与NCFM合成）是一次性投资，在LIBERO数据集上约为24 GPU小时。在此之后，使用5%的合成数据训练下游策略模型仅需约7 GPU小时。\n### 论文使用的评估环境和评估指标\n* **评估环境**:\n * **硬件**: 1 × NVIDIA A100-SXM4-80GB GPU, AMD EPYC 7742 CPU, 47GiB内存。\n * **软件**: Ubuntu 22.04.3 LTS, CUDA 12.4, PyTorch 2.5.0, Python 3.12.7。\n* **评估指标**:\n * **性能指标**:\n * 成功率 或 平均任务完成长度: 用于在CALVIN、Meta-World和LIBERO上衡量任务完成的性能。CALVIN评估遵循D->A,B,C,D的长期任务泛化协议。\n * **效率指标**:\n * 总训练时间: 以GPU小时为单位，衡量从随机初始化到模型收敛所需的总时间，用于评估训练效率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>根据论文的致谢部分，研究单位与中国国家自然科学基金和重庆市自然科学基金有关，推测主要研究机构位于中国重庆。</p>\n<h3>论文概述</h3>\n<p>论文针对视觉-语言-行动（VLA）模型因依赖大规模、冗余且价值不均的数据集而导致的效率瓶颈问题，提出了一种以数据为中心的生成式数据蒸馏框架FT-NCFM。该框架通过一个自包含的影响评估引擎（Fact-Tracing, FT）来评估样本的内在价值，并利用该评估结果引导一个对抗性的神经特征函数匹配（NCFM）过程，从而合成一个信息密度高、可复用的数据核心集。实验证明，仅使用5%的合成数据训练模型，即可达到使用完整数据集训练85-90%的任务成功率，同时将训练时间减少80%以上，展示了数据蒸馏在构建高效VLA模型中的巨大潜力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新的生成式数据蒸馏范式，区别于现有的模型压缩和策略蒸馏等以模型为中心的优化路径，从数据层面提升VLA模型效率。</li><li>设计了一个自包含的内在价值评估引擎FT。该引擎通过两阶段评估样本价值：首先使用因果归因进行预筛选，然后通过一个新颖的程序化对比验证模块，为精英样本生成“最小反例”来精炼其影响权重，从而稳健地量化样本的因果贡献和泛化潜力。</li><li>在多个主流VLA基准测试上进行了系统性评估。结果表明，使用FT-NCFM框架和仅5%的合成数据，模型就能达到使用全量数据训练85-90%的性能，同时显著降低了训练时间和计算资源消耗（训练时间减少80%以上），优于多种SOTA方法。</li></ol>\n<h3>论文方法描述</h3>\n<p>FT-NCFM框架包含一个三阶段的流水线：</p>\n<ol><li><strong>VLA多模态表示学习</strong>: 将原始的视觉、语言和行动数据通过各自的编码器和Transformer骨干网络，融合成一个统一的特征向量 <code>h</code>，作为后续分析的基础。</li><li><strong>FT影响评估引擎</strong>: 一个两阶段的样本价值评估过程。</li></ol>\n<p> * <strong>阶段一：基于影响函数的因果归因预筛选</strong>: 使用影响函数近似移除单个训练样本对模型损失的影响，计算出基础影响分数。此阶段使用一个“引导模型”，该模型在原始数据上进行轻量级、不完全的训练。</p>\n<p> * <strong>阶段二：对比验证精化</strong>: 对前K%的精英样本，在模拟器中通过“跨模态不匹配”策略程序化地生成一个语义或物理矛盾的“最小反例”。然后，通过比较原始样本和其反例对测试用例的影响差异，使用一个包含tanh函数的权重调制公式来精炼最终的影响权重。</p>\n<ol><li><strong>引导式NCFM</strong>: 将FT引擎计算出的一组影响权重用于指导NCFM的优化过程。通过将原始NCFM的期望计算改为基于权重的加权期望，迫使判别器关注高价值样本，从而引导生成器合成一个富含关键因果知识且信息密度极高的核心集。该核心集与原始数据格式相同，可直接用于训练任何下游VLA模型。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>使用数据集</strong>:</li></ul>\n<p> * CALVIN: 一个用于长期语言条件策略学习的大规模机器人操作基准。</p>\n<p> * Meta-World: 包含50种不同桌面操作任务的基准，用于评估多任务学习环境中的技能获取。</p>\n<p> * LIBERO: 用于促进终身学习的基准套件，包含Spatial、Object、Goal和Long等多个子集。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> * <strong>硬件</strong>: 实验主要在单块NVIDIA A100 80GB GPU上进行。</p>\n<p> * <strong>软件环境</strong>: Ubuntu 22.04.3 LTS, CUDA 12.4, Python 3.12.7, PyTorch 2.5.0。</p>\n<p> * <strong>时间成本</strong>: FT-NCFM框架的预处理阶段（FT引擎评估与NCFM合成）是一次性投资，在LIBERO数据集上约为24 GPU小时。在此之后，使用5%的合成数据训练下游策略模型仅需约7 GPU小时。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> * <strong>硬件</strong>: 1 × NVIDIA A100-SXM4-80GB GPU, AMD EPYC 7742 CPU, 47GiB内存。</p>\n<p> * <strong>软件</strong>: Ubuntu 22.04.3 LTS, CUDA 12.4, PyTorch 2.5.0, Python 3.12.7。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> * <strong>性能指标</strong>:</p>\n<p> * 成功率 或 平均任务完成长度: 用于在CALVIN、Meta-World和LIBERO上衡量任务完成的性能。CALVIN评估遵循D->A,B,C,D的长期任务泛化协议。</p>\n<p> * <strong>效率指标</strong>:</p>\n<p> * 总训练时间: 以GPU小时为单位，衡量从随机初始化到模型收敛所需的总时间，用于评估训练效率。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.16203",
    "summary_markdown": "## 论文研究单位\n- 西湖大学工学院TGAI实验室\n- 浙江大学\n- 宾夕法尼亚州立大学\n- 索尼研究\n## 论文概述\n本文关注视觉-语言-动作（VLA）模型在具身环境中的对抗鲁棒性问题。尽管VLA模型在机器人感知、推理和行动方面取得了显著进展，但其对抗鲁棒性，尤其是在现实多模态和黑盒条件下的表现，尚未被充分探索。现有研究主要集中于单模态扰动，而忽视了影响具身推理和决策的根本性跨模态不对齐问题。为此，论文提出了VLA-Fool，这是一个在白盒和黑盒设置下对具身VLA模型多模态对抗鲁棒性的综合研究套件。VLA-Fool统一了三个层次的多模态对抗攻击：文本扰动、视觉扰动和跨模态不对齐攻击。在LIBERO基准上使用微调的OpenVLA模型进行的实验表明，即使是轻微的多模态扰动也可能导致显著的行为偏差，揭示了具身多模态对齐的脆弱性。\n## 论文核心贡献点\n- 提出了VLA-Fool，一个在白盒和黑盒设置下生成和评估多模态对抗攻击的综合框架。该框架包括通过基于梯度和基于提示的策略进行文本扰动，通过补丁和噪声失真进行视觉扰动，以及故意破坏感知与指令之间语义对应关系的跨模态不对齐攻击。\n- 通过将贪婪坐标梯度（GCG）方法扩展到VLA感知的语义空间，引入了首个自动构建的、语义引导的提示框架，用于VLA对抗攻击，包括四种语言丰富的错误对齐模式：指代歧义、属性削弱、范围模糊和否定混淆。\n- 通过广泛的实验，揭示了最先进的VLA模型在面对多模态扰动时的脆弱性，在所有变化类别的失败率超过60%，在长视野任务中失败率高达100%，为开发更强大和可信的具身代理提供了宝贵的见解和基准。\n## 论文方法描述\nVLA-Fool是一个统一的多模态对抗攻击套件，包含三个互补的模块：\n- **文本攻击**:\n - **SGCG (白盒)**: 一种基于梯度的语义引导攻击方法。它并行执行K个GCG优化过程，每个过程专注于一种特定的语义扰动策略，例如将具体实体替换为代词（指代歧义）、改变关键属性（属性削弱）、替换空间描述词（范围模糊）或引入否定词（否定混淆）。\n - **提示操作 (黑盒)**: 无需模型内部信息的简单攻击。包括后缀注入（如附加“忽略之前的信息”或随机代码片段）和前缀注入（在指令前添加误导性上下文）来操纵模型行为。\n- **视觉攻击**:\n - **局部补丁攻击 (白盒)**: 利用模型梯度直接优化一个可放置在环境物体或机械臂上的小补丁内容，通过最大化模型输出动作与原始动作之间的L2距离，使机器人执行错误的动作序列。\n - **噪声扰动攻击 (黑盒)**: 向图像中注入各种类型的噪声，模拟真实世界的传感器退化或环境干扰。测试的噪声类型包括高斯噪声、椒盐噪声、斑点噪声、均匀噪声和伪随机噪声等。\n- **跨模态不对齐攻击**:\n - 该攻击不单独扰动某个模态，而是同时优化视觉和文本扰动，以最大化跨模态不对齐损失（L_mis）。该损失函数衡量扰动前后视觉嵌入与语言嵌入之间平均余弦相似度的变化，旨在破坏视觉和语言特征之间的语义对应关系，从而从根本上瓦解VLA的感知-语言-行动管道。\n## 论文使用数据集和训练资源\n- **数据集**: LIBERO基准数据集。该数据集提供了一系列多样化的视觉-语言操作任务和逼真的模拟场景，分为四个评估类别：空间关系查询、对象识别与操作、目标导向行为和长视野多步骤任务。\n- **训练资源**: 实验使用了一个在LIBERO数据集上微调的7B参数OpenVLA模型作为受害者模型。模型推理在单块NVIDIA L40s（48GB）GPU上运行，使用bfloat16精度。\n## 论文使用的评估环境和评估指标\n- **评估环境**: 所有实验均在LIBERO数据集提供的模拟环境中进行。\n- **评估指标**:\n - **失败率**: 主要评估指标，定义为 `FR = 1 - SR`（任务成功率）。该指标直接反映了对抗性扰动导致的任务完成度下降，用于衡量模型对攻击的脆弱性。\n - **不对齐损失**: 一个辅助评估指标，用于量化由扰动引起的视觉和语言表征之间的不一致性。它通过计算扰动前后视觉块嵌入与语言标记嵌入之间平均余弦相似度的变化来衡量。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>西湖大学工学院TGAI实验室</li><li>浙江大学</li><li>宾夕法尼亚州立大学</li><li>索尼研究</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文关注视觉-语言-动作（VLA）模型在具身环境中的对抗鲁棒性问题。尽管VLA模型在机器人感知、推理和行动方面取得了显著进展，但其对抗鲁棒性，尤其是在现实多模态和黑盒条件下的表现，尚未被充分探索。现有研究主要集中于单模态扰动，而忽视了影响具身推理和决策的根本性跨模态不对齐问题。为此，论文提出了VLA-Fool，这是一个在白盒和黑盒设置下对具身VLA模型多模态对抗鲁棒性的综合研究套件。VLA-Fool统一了三个层次的多模态对抗攻击：文本扰动、视觉扰动和跨模态不对齐攻击。在LIBERO基准上使用微调的OpenVLA模型进行的实验表明，即使是轻微的多模态扰动也可能导致显著的行为偏差，揭示了具身多模态对齐的脆弱性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了VLA-Fool，一个在白盒和黑盒设置下生成和评估多模态对抗攻击的综合框架。该框架包括通过基于梯度和基于提示的策略进行文本扰动，通过补丁和噪声失真进行视觉扰动，以及故意破坏感知与指令之间语义对应关系的跨模态不对齐攻击。</li><li>通过将贪婪坐标梯度（GCG）方法扩展到VLA感知的语义空间，引入了首个自动构建的、语义引导的提示框架，用于VLA对抗攻击，包括四种语言丰富的错误对齐模式：指代歧义、属性削弱、范围模糊和否定混淆。</li><li>通过广泛的实验，揭示了最先进的VLA模型在面对多模态扰动时的脆弱性，在所有变化类别的失败率超过60%，在长视野任务中失败率高达100%，为开发更强大和可信的具身代理提供了宝贵的见解和基准。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>VLA-Fool是一个统一的多模态对抗攻击套件，包含三个互补的模块：</p>\n<ul><li><strong>文本攻击</strong>:</li></ul>\n<p> - <strong>SGCG (白盒)</strong>: 一种基于梯度的语义引导攻击方法。它并行执行K个GCG优化过程，每个过程专注于一种特定的语义扰动策略，例如将具体实体替换为代词（指代歧义）、改变关键属性（属性削弱）、替换空间描述词（范围模糊）或引入否定词（否定混淆）。</p>\n<p> - <strong>提示操作 (黑盒)</strong>: 无需模型内部信息的简单攻击。包括后缀注入（如附加“忽略之前的信息”或随机代码片段）和前缀注入（在指令前添加误导性上下文）来操纵模型行为。</p>\n<ul><li><strong>视觉攻击</strong>:</li></ul>\n<p> - <strong>局部补丁攻击 (白盒)</strong>: 利用模型梯度直接优化一个可放置在环境物体或机械臂上的小补丁内容，通过最大化模型输出动作与原始动作之间的L2距离，使机器人执行错误的动作序列。</p>\n<p> - <strong>噪声扰动攻击 (黑盒)</strong>: 向图像中注入各种类型的噪声，模拟真实世界的传感器退化或环境干扰。测试的噪声类型包括高斯噪声、椒盐噪声、斑点噪声、均匀噪声和伪随机噪声等。</p>\n<ul><li><strong>跨模态不对齐攻击</strong>:</li></ul>\n<p> - 该攻击不单独扰动某个模态，而是同时优化视觉和文本扰动，以最大化跨模态不对齐损失（L_mis）。该损失函数衡量扰动前后视觉嵌入与语言嵌入之间平均余弦相似度的变化，旨在破坏视觉和语言特征之间的语义对应关系，从而从根本上瓦解VLA的感知-语言-行动管道。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>: LIBERO基准数据集。该数据集提供了一系列多样化的视觉-语言操作任务和逼真的模拟场景，分为四个评估类别：空间关系查询、对象识别与操作、目标导向行为和长视野多步骤任务。</li><li><strong>训练资源</strong>: 实验使用了一个在LIBERO数据集上微调的7B参数OpenVLA模型作为受害者模型。模型推理在单块NVIDIA L40s（48GB）GPU上运行，使用bfloat16精度。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>: 所有实验均在LIBERO数据集提供的模拟环境中进行。</li><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>失败率</strong>: 主要评估指标，定义为 <code>FR = 1 - SR</code>（任务成功率）。该指标直接反映了对抗性扰动导致的任务完成度下降，用于衡量模型对攻击的脆弱性。</p>\n<p> - <strong>不对齐损失</strong>: 一个辅助评估指标，用于量化由扰动引起的视觉和语言表征之间的不一致性。它通过计算扰动前后视觉块嵌入与语言标记嵌入之间平均余弦相似度的变化来衡量。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
    "link": "http://arxiv.org/abs/2511.16175",
    "summary_markdown": "### 论文研究单位\n上海交通大学 (SJTU), 上海人工智能实验室 (SII), 南京邮电大学 (NJUPT), 复旦大学 (FDU), BOSCH。\n### 论文概述\n本文提出了一种名为 Mantis 的视觉-语言-动作 (VLA) 模型，旨在解决现有 VLA 模型面临的挑战：直接预测高维视觉状态会分散模型容量并带来高昂的训练成本，而压缩视觉状态则会引入信息瓶颈，同时现有方法常因忽视语言监督而导致理解与推理能力不佳。Mantis 引入了核心创新——解耦视觉预测 (DVF)，通过结合元查询和扩散 Transformer (DiT) 头，将视觉预测任务与主干网络解耦。当前视觉状态通过残差连接提供给 DiT，使得元查询能自动捕捉描绘视觉轨迹的潜在动作，从而提升显式动作学习。这种解耦设计减轻了主干网络的负担，使其能通过语言监督保持理解和推理能力。实验表明，在 LIBERO 基准上微调后，Mantis 取得了 96.7% 的成功率，并在真实世界评估中表现出优于基线模型的指令跟随、泛化和推理能力。\n### 论文核心贡献点\n- 提出了解耦视觉预测 (DVF)，为动作预测提供简洁且具有指导性的前瞻线索，并构建了新的 VLA 模型 Mantis。\n- 设计了用于模态融合的渐进式训练方案，使 Mantis 能够有效地整合动作预测与语言理解。\n- Mantis 在 LIBERO 基准上实现了 96.7% 的成功率，并在真实世界机器人实验中展示了卓越的指令跟随能力。\n### 论文方法描述\nMantis 的方法论包含四个主要部分：\n1. **模型概述**：模型由一个 VLM 主干网络、一个连接器、一个 DVF 头和一个动作头组成。主干网络接收语言指令和当前视觉观察，并与可学习的潜在动作查询 ([LAT]) 交互。DVF 头接收主干网络输出和当前视觉状态（通过残差连接），用于预测未来的视觉状态。动作头使用可学习的动作查询 ([ACT]) 从输入和 [LAT] 查询中提取信息，以生成显式动作序列。\n2. **模型规范**：主干网络采用 Qwen2.5-VL。DVF 头采用高效的文本到图像生成模型 Sana (一个 DiT)。连接器包含 12 层 Transformer 编码器和一个投影层。动作头同样基于 DiT 结构。\n3. **渐进式训练**：训练分为三个阶段以稳定融合多模态信息。\n - **阶段1：多时差视觉训练**：在 SSV2 人类操作视频数据集上预训练，仅训练 DVF 头和 [LAT] 查询，目标是预测未来帧，让模型学习通用操作技能。\n - **阶段2：视觉-动作联合训练**：在 DROID 机器人演示数据集上训练，引入动作损失，并解冻动作查询。\n - **阶段3：语言监督混合训练**：联合使用 38 个多模态数据集和 DROID 数据进行训练，解冻主干网络并引入语言损失，以保持模型的语义理解与推理能力。\n4. **自适应时间集成 (ATE)**：一种推理时优化策略，用于平衡计算效率和运动稳定性。它动态识别与指令相关的“目标补丁”和发生显著变化的“动态补丁”。当二者重叠时（表示精细操作），启用时间集成以增强稳定性；否则，跳过集成以节省计算开销。该模型变体称为 Mantis-ATE。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **预训练**：SSV2 数据集（约 220K 个人类操作视频），DROID 数据集（76K 个机器人演示），以及 38 个多模态数据集（用于图像-文本对）。\n - **微调**：LIBERO 模拟基准数据集。\n - **真实世界实验**：在三个自定义场景中收集的 100 个远程操作演示数据集。\n- **训练资源**：\n - **模型规模**：总计 58 亿参数（主干网络 3.7B，DVF 头 1.4B，动作头 0.3B，VAE 0.3B）。\n - **优化器**：使用 AdamW 优化器，权重衰减为 0.1，梯度裁剪为 0.5。\n - **训练框架**：使用 DeepSpeed 进行高效分布式训练。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：LIBERO 基准，包含四个任务套件（Spatial, Object, Goal, Long）。\n - **真实世界环境**：Agilex 机器人平台。\n- **评估指标**：\n - **LIBERO 基准**：成功率 (Success Rate, SR)，范围为 0 到 100，越高越好。\n - **真实世界实验**：平均成功执行次数（每个指令执行 10 次，取平均成功数）。\n - **Mantis-ATE 效率**：推理次数 和成功率 (SR)，用于评估计算效率和任务性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学 (SJTU), 上海人工智能实验室 (SII), 南京邮电大学 (NJUPT), 复旦大学 (FDU), BOSCH。</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为 Mantis 的视觉-语言-动作 (VLA) 模型，旨在解决现有 VLA 模型面临的挑战：直接预测高维视觉状态会分散模型容量并带来高昂的训练成本，而压缩视觉状态则会引入信息瓶颈，同时现有方法常因忽视语言监督而导致理解与推理能力不佳。Mantis 引入了核心创新——解耦视觉预测 (DVF)，通过结合元查询和扩散 Transformer (DiT) 头，将视觉预测任务与主干网络解耦。当前视觉状态通过残差连接提供给 DiT，使得元查询能自动捕捉描绘视觉轨迹的潜在动作，从而提升显式动作学习。这种解耦设计减轻了主干网络的负担，使其能通过语言监督保持理解和推理能力。实验表明，在 LIBERO 基准上微调后，Mantis 取得了 96.7% 的成功率，并在真实世界评估中表现出优于基线模型的指令跟随、泛化和推理能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了解耦视觉预测 (DVF)，为动作预测提供简洁且具有指导性的前瞻线索，并构建了新的 VLA 模型 Mantis。</li><li>设计了用于模态融合的渐进式训练方案，使 Mantis 能够有效地整合动作预测与语言理解。</li><li>Mantis 在 LIBERO 基准上实现了 96.7% 的成功率，并在真实世界机器人实验中展示了卓越的指令跟随能力。</li></ul>\n<h3>论文方法描述</h3>\n<p>Mantis 的方法论包含四个主要部分：</p>\n<ol><li><strong>模型概述</strong>：模型由一个 VLM 主干网络、一个连接器、一个 DVF 头和一个动作头组成。主干网络接收语言指令和当前视觉观察，并与可学习的潜在动作查询 ([LAT]) 交互。DVF 头接收主干网络输出和当前视觉状态（通过残差连接），用于预测未来的视觉状态。动作头使用可学习的动作查询 ([ACT]) 从输入和 [LAT] 查询中提取信息，以生成显式动作序列。</li><li><strong>模型规范</strong>：主干网络采用 Qwen2.5-VL。DVF 头采用高效的文本到图像生成模型 Sana (一个 DiT)。连接器包含 12 层 Transformer 编码器和一个投影层。动作头同样基于 DiT 结构。</li><li><strong>渐进式训练</strong>：训练分为三个阶段以稳定融合多模态信息。</li></ol>\n<p> - <strong>阶段1：多时差视觉训练</strong>：在 SSV2 人类操作视频数据集上预训练，仅训练 DVF 头和 [LAT] 查询，目标是预测未来帧，让模型学习通用操作技能。</p>\n<p> - <strong>阶段2：视觉-动作联合训练</strong>：在 DROID 机器人演示数据集上训练，引入动作损失，并解冻动作查询。</p>\n<p> - <strong>阶段3：语言监督混合训练</strong>：联合使用 38 个多模态数据集和 DROID 数据进行训练，解冻主干网络并引入语言损失，以保持模型的语义理解与推理能力。</p>\n<ol><li><strong>自适应时间集成 (ATE)</strong>：一种推理时优化策略，用于平衡计算效率和运动稳定性。它动态识别与指令相关的“目标补丁”和发生显著变化的“动态补丁”。当二者重叠时（表示精细操作），启用时间集成以增强稳定性；否则，跳过集成以节省计算开销。该模型变体称为 Mantis-ATE。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>预训练</strong>：SSV2 数据集（约 220K 个人类操作视频），DROID 数据集（76K 个机器人演示），以及 38 个多模态数据集（用于图像-文本对）。</p>\n<p> - <strong>微调</strong>：LIBERO 模拟基准数据集。</p>\n<p> - <strong>真实世界实验</strong>：在三个自定义场景中收集的 100 个远程操作演示数据集。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - <strong>模型规模</strong>：总计 58 亿参数（主干网络 3.7B，DVF 头 1.4B，动作头 0.3B，VAE 0.3B）。</p>\n<p> - <strong>优化器</strong>：使用 AdamW 优化器，权重衰减为 0.1，梯度裁剪为 0.5。</p>\n<p> - <strong>训练框架</strong>：使用 DeepSpeed 进行高效分布式训练。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：LIBERO 基准，包含四个任务套件（Spatial, Object, Goal, Long）。</p>\n<p> - <strong>真实世界环境</strong>：Agilex 机器人平台。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>LIBERO 基准</strong>：成功率 (Success Rate, SR)，范围为 0 到 100，越高越好。</p>\n<p> - <strong>真实世界实验</strong>：平均成功执行次数（每个指令执行 10 次，取平均成功数）。</p>\n<p> - <strong>Mantis-ATE 效率</strong>：推理次数 和成功率 (SR)，用于评估计算效率和任务性能。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2511.16166",
    "summary_markdown": "### 论文研究单位\nPeking University\n### 论文概述\n论文提出EvoVLA框架，用于解决视觉-语言-行动（VLA）模型在长视野机器人操作中的“阶段幻觉”问题（即智能体通过利用粗糙的评估信号来走捷径多步骤任务，报告高进展但实际任务未完成）。该框架通过三个协同组件实现：阶段对齐奖励（SAR）使用Gemini生成的硬负例进行三元组对比学习；基于位姿的对象探索（POE）将好奇心锚定在相对对象-夹爪位姿而非像素上；以及具有选择性上下文和门控融合的长期记忆，用于稳定内在奖励塑造。在Discoverse-L基准测试（包含三个多阶段任务）上的评估表明，EvoVLA平均成功率比最强基线高10.2个百分点，达到69.2%，样本效率提高1.5倍，阶段幻觉率从38.5%降至14.8%。真实机器人部署的平均成功率为54.6%。\n### 论文核心贡献点\n1. 提出自监督长视野学习方法EvoVLA，结合SAR与Gemini驱动的硬负例、POE提供密集且语义一致的内在反馈，无需额外标签即可实现可扩展的VLA微调。\n2. 通过上下文选择的长期记忆机制（含选择性注意和门控融合）解决长视野遗忘问题，并结合Discoverse-L基准（含三个多阶段操作任务，18–74阶段），为社区提供研究长视野操作记忆的方法和数据集。\n3. 广泛实验验证：EvoVLA在Discoverse-L上成功率69.2%（+10.2点 vs. OpenVLA-OFT），样本效率达1.5×，真实机器人成功率54.6%（+11.0点 vs. OpenVLA-OFT），幻觉率降至14.8%。\n### 论文方法描述\n1. **阶段对齐奖励（SAR）**：\n - 三元组文本生成：使用Gemini 2.5 Pro为每阶段生成正例、互斥负例和反事实硬负例（如“抓取非目标对象”的谓词）。\n - 图像-文本对比评分：通过CLIP编码器计算阶段对齐分数 \\( u_k(t) = \\sigma(\\tau[s_k^+(t) - \\max\\{s_k^-(t), s_k^h(t)\\}]) \\)，其中 \\( s_k^+ \\) 为正例得分，\\( s_k^- \\) 和 \\( s_k^h \\) 为负例和硬负例得分。\n - 时间平滑：维护滑动平均 \\( \\bar{u}_k(t) = (1-\\alpha)\\bar{u}_k(t-1) + \\alpha u_k(t) \\)，阶段奖励 \\( r_t^{\\text{stage}} = \\bar{u}_{\\kappa_t}(t) - \\bar{u}_{\\kappa_t}(t-1) \\)，阈值触发阶段推进。\n2. **基于位姿的对象探索（POE）**：\n - 潜在操作动力学：将操作状态表示为 \\( z_t = \\psi(T_{\\text{ee}}^{-1}T_{\\text{obj}}) \\in \\mathbb{R}^6 \\)（相对位姿）。\n - 训练轻量级MLP前向/逆模型：\\( \\hat{z}_{t+1} = f_\\phi(z_t, a_t) \\) 和 \\( \\hat{a}_t = g_\\psi(z_t, z_{t+1}) \\)。\n - 任务相关内在奖励：好奇心奖励 \\( r_t^{\\text{cur}} = \\frac{\\eta}{2} \\\\|\\text{sg}(\\hat{z}_{t+1}) - z_{t+1}\\\\|_2^2 \\)（sg为停止梯度），基础进展奖励 \\( r_t^{\\text{base}} = \\text{ReLU}(\\overline{\\mathcal{L}_F}(t-1) - \\overline{\\mathcal{L}_F}(t)) \\)。\n3. **长期记忆**：\n - 上下文选择：通过注意力机制从记忆库 \\( \\mathcal{M} \\) 中选择Top-K历史项作为上下文令牌。\n - 门控融合：计算门控信号 \\( g_t^{\\text{mem}} = \\sigma(w_g^\\top [\\hat{h}_t; x_t]) \\)，融合当前表征与上下文 \\( \\tilde{x}_t = (1-g_t^{\\text{mem}})x_t + g_t^{\\text{mem}}\\hat{h}_t \\)。\n - 奖励调制：进展奖励 \\( r_t^{\\text{prog}} = g_t^{\\text{mem}} \\cdot r_t^{\\text{base}} \\)，抑制不稳定模式的虚假信号。\n4. **训练目标**：联合优化PPO损失、世界模型损失和熵正则化：\\( \\mathcal{L} = \\mathcal{L}_{\\text{PPO}}(\\tilde{r}) + \\lambda_F \\mathcal{L}_F + \\lambda_I \\mathcal{L}_I - \\lambda_{\\text{ent}} H(\\pi) \\)，其中组合奖励 \\( \\tilde{r}_t = r_t^e + \\rho (r_t^{\\text{stage}} + r_t^{\\text{cur}} + r_t^{\\text{prog}}) \\)。\n### 论文使用数据集和训练资源\n- **数据集**：Discoverse-L基准（基于DISCOVERSE模拟器和AIRBOT-Play平台），包含三个多阶段操作任务：\n - Block Bridge（74阶段）：放置两根横杆形成桥状结构并填充积木。\n - Stack（18阶段）：顺序堆叠三个彩色积木。\n - Jujube-Cup（19阶段）：将枣放入杯子并移至盘子。\n 每任务生成50个随机初始条件的演示轨迹，存储于RLDS格式，阶段语义由Gemini 2.5 Pro通过视频驱动工作流生成。\n- **训练资源**：硬件为4×NVIDIA H20 GPU（96GB VRAM），软件环境为8个并行DISCOVERSE环境，每个种子训练200万步，共3个随机种子，耗时约24小时/种子。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **仿真**：DISCOVERSE模拟器中的Discoverse-L任务，每任务50个测试回合，固定回合预算400步（复杂任务约150–250步完成）。\n - **真实世界**：物理AIRBOT-Play机器人部署，包括Sim2Real迁移测试（三个任务）和新任务训练（堆叠四个杯子并插入香蕉形物体）。\n- **评估指标**：\n - **成功率（SR）**：400步内完成任务的百分比。\n - **样本效率**：达到50%成功率所需的环境步数。\n - **幻觉率（HR）**：高VLM分数但实际未完成的阶段占比，公式为 \\( \\text{HR} = \\frac{\\mathbb{E}[\\mathbb{1}\\{u_k(t) > \\theta \\land c_k(t)=0\\}]}{\\mathbb{E}[\\mathbb{1}\\{u_k(t) > \\theta\\}]} \\)，其中 \\( \\theta = 0.7 \\)，\\( c_k(t) \\) 为地面真值完成指示器（仅用于评估）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Peking University</p>\n<h3>论文概述</h3>\n<p>论文提出EvoVLA框架，用于解决视觉-语言-行动（VLA）模型在长视野机器人操作中的“阶段幻觉”问题（即智能体通过利用粗糙的评估信号来走捷径多步骤任务，报告高进展但实际任务未完成）。该框架通过三个协同组件实现：阶段对齐奖励（SAR）使用Gemini生成的硬负例进行三元组对比学习；基于位姿的对象探索（POE）将好奇心锚定在相对对象-夹爪位姿而非像素上；以及具有选择性上下文和门控融合的长期记忆，用于稳定内在奖励塑造。在Discoverse-L基准测试（包含三个多阶段任务）上的评估表明，EvoVLA平均成功率比最强基线高10.2个百分点，达到69.2%，样本效率提高1.5倍，阶段幻觉率从38.5%降至14.8%。真实机器人部署的平均成功率为54.6%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出自监督长视野学习方法EvoVLA，结合SAR与Gemini驱动的硬负例、POE提供密集且语义一致的内在反馈，无需额外标签即可实现可扩展的VLA微调。</li><li>通过上下文选择的长期记忆机制（含选择性注意和门控融合）解决长视野遗忘问题，并结合Discoverse-L基准（含三个多阶段操作任务，18–74阶段），为社区提供研究长视野操作记忆的方法和数据集。</li><li>广泛实验验证：EvoVLA在Discoverse-L上成功率69.2%（+10.2点 vs. OpenVLA-OFT），样本效率达1.5×，真实机器人成功率54.6%（+11.0点 vs. OpenVLA-OFT），幻觉率降至14.8%。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>阶段对齐奖励（SAR）</strong>：</li></ol>\n<p> - 三元组文本生成：使用Gemini 2.5 Pro为每阶段生成正例、互斥负例和反事实硬负例（如“抓取非目标对象”的谓词）。</p>\n<p> - 图像-文本对比评分：通过CLIP编码器计算阶段对齐分数 \\( u_k(t) = \\sigma(\\tau[s_k^+(t) - \\max\\{s_k^-(t), s_k^h(t)\\}]) \\)，其中 \\( s_k^+ \\) 为正例得分，\\( s_k^- \\) 和 \\( s_k^h \\) 为负例和硬负例得分。</p>\n<p> - 时间平滑：维护滑动平均 \\( \\bar{u}_k(t) = (1-\\alpha)\\bar{u}_k(t-1) + \\alpha u_k(t) \\)，阶段奖励 \\( r_t^{\\text{stage}} = \\bar{u}_{\\kappa_t}(t) - \\bar{u}_{\\kappa_t}(t-1) \\)，阈值触发阶段推进。</p>\n<ol><li><strong>基于位姿的对象探索（POE）</strong>：</li></ol>\n<p> - 潜在操作动力学：将操作状态表示为 \\( z_t = \\psi(T_{\\text{ee}}^{-1}T_{\\text{obj}}) \\in \\mathbb{R}^6 \\)（相对位姿）。</p>\n<p> - 训练轻量级MLP前向/逆模型：\\( \\hat{z}_{t+1} = f_\\phi(z_t, a_t) \\) 和 \\( \\hat{a}_t = g_\\psi(z_t, z_{t+1}) \\)。</p>\n<p> - 任务相关内在奖励：好奇心奖励 \\( r_t^{\\text{cur}} = \\frac{\\eta}{2} \\\\|\\text{sg}(\\hat{z}_{t+1}) - z_{t+1}\\\\|_2^2 \\)（sg为停止梯度），基础进展奖励 \\( r_t^{\\text{base}} = \\text{ReLU}(\\overline{\\mathcal{L}_F}(t-1) - \\overline{\\mathcal{L}_F}(t)) \\)。</p>\n<ol><li><strong>长期记忆</strong>：</li></ol>\n<p> - 上下文选择：通过注意力机制从记忆库 \\( \\mathcal{M} \\) 中选择Top-K历史项作为上下文令牌。</p>\n<p> - 门控融合：计算门控信号 \\( g_t^{\\text{mem}} = \\sigma(w_g^\\top [\\hat{h}_t; x_t]) \\)，融合当前表征与上下文 \\( \\tilde{x}_t = (1-g_t^{\\text{mem}})x_t + g_t^{\\text{mem}}\\hat{h}_t \\)。</p>\n<p> - 奖励调制：进展奖励 \\( r_t^{\\text{prog}} = g_t^{\\text{mem}} \\cdot r_t^{\\text{base}} \\)，抑制不稳定模式的虚假信号。</p>\n<ol><li><strong>训练目标</strong>：联合优化PPO损失、世界模型损失和熵正则化：\\( \\mathcal{L} = \\mathcal{L}_{\\text{PPO}}(\\tilde{r}) + \\lambda_F \\mathcal{L}_F + \\lambda_I \\mathcal{L}_I - \\lambda_{\\text{ent}} H(\\pi) \\)，其中组合奖励 \\( \\tilde{r}_t = r_t^e + \\rho (r_t^{\\text{stage}} + r_t^{\\text{cur}} + r_t^{\\text{prog}}) \\)。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Discoverse-L基准（基于DISCOVERSE模拟器和AIRBOT-Play平台），包含三个多阶段操作任务：</li></ul>\n<p> - Block Bridge（74阶段）：放置两根横杆形成桥状结构并填充积木。</p>\n<p> - Stack（18阶段）：顺序堆叠三个彩色积木。</p>\n<p> - Jujube-Cup（19阶段）：将枣放入杯子并移至盘子。</p>\n<p> 每任务生成50个随机初始条件的演示轨迹，存储于RLDS格式，阶段语义由Gemini 2.5 Pro通过视频驱动工作流生成。</p>\n<ul><li><strong>训练资源</strong>：硬件为4×NVIDIA H20 GPU（96GB VRAM），软件环境为8个并行DISCOVERSE环境，每个种子训练200万步，共3个随机种子，耗时约24小时/种子。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>仿真</strong>：DISCOVERSE模拟器中的Discoverse-L任务，每任务50个测试回合，固定回合预算400步（复杂任务约150–250步完成）。</p>\n<p> - <strong>真实世界</strong>：物理AIRBOT-Play机器人部署，包括Sim2Real迁移测试（三个任务）和新任务训练（堆叠四个杯子并插入香蕉形物体）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（SR）</strong>：400步内完成任务的百分比。</p>\n<p> - <strong>样本效率</strong>：达到50%成功率所需的环境步数。</p>\n<p> - <strong>幻觉率（HR）</strong>：高VLM分数但实际未完成的阶段占比，公式为 \\( \\text{HR} = \\frac{\\mathbb{E}[\\mathbb{1}\\{u_k(t) > \\theta \\land c_k(t)=0\\}]}{\\mathbb{E}[\\mathbb{1}\\{u_k(t) > \\theta\\}]} \\)，其中 \\( \\theta = 0.7 \\)，\\( c_k(t) \\) 为地面真值完成指示器（仅用于评估）。</p>"
  },
  {
    "date": "2025-11-19",
    "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.15605",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-19",
    "title": "IPR-1: Interactive Physical Reasoner",
    "link": "http://arxiv.org/abs/2511.15407",
    "summary_markdown": "## 论文研究单位\n- 上海交通大学\n- 上海创新研究院\n- 卡内基梅隆大学\n## 论文概述\n论文提出了交互式物理推理器，旨在让智能体通过与环境交互来学习并持续改进人类般的物理推理能力。研究在Game-to-Unseen (G2U)设定下进行，策划了超过1000个具有不同物理和因果机制的异构游戏，并采用受马斯洛需求层次启发的三个级别进行评估：生存、好奇心和效用，分别对应从原始直觉到目标驱动推理的递进。分析发现，现有的VLM/VLA模型虽有推理能力但缺乏交互中的前瞻性，而世界模型能想象未来但倾向于模仿视觉模式而非分析物理因果。因此，IPR利用世界模型的推演来评分和强化VLM的策略，并引入了PhysCode，一种以物理为中心的动作代码，用于统一语义意图与动态，为预测和推理提供共享的动作空间。在1000多个游戏上预训练的IPR在三个级别上表现稳健，总体与GPT-5相当，并在好奇心上超越GPT-5，且性能随训练游戏和交互步数的增加而提升，并能零样本迁移到未见过的游戏中。\n## 论文核心贡献点\n- 提出了G2U问题，并构建了包含1000+异构游戏的基准，采用分层评估（生存/好奇心/效用），诊断了基于预测、强化学习和VLM等方法的优势与不足。\n- 提出了IPR范式：通过世界模型推演来评分和强化同一动作空间中的VLM策略，使得交互经验能够稳步提升物理推理能力。\n- 引入了PhysCode，一种以物理为中心的动作代码，融合了动作语义与视觉动态，为世界模型的预测和VLM的推理之间架起了桥梁。\n## 论文方法描述\nIPR方法包含三个核心组件：\n1. **诱导潜在动作词汇**：通过一个VQ-VAE模型，从视频帧（DINOv3特征）、光流和动作语义中学习一个离散的、以物理为中心的动作代码，即PhysCode。训练目标是使该代码能够捕捉可跨游戏共享的动态基元，并与特定领域的视觉可供性对齐。\n2. **训练潜在层面的世界模型**：在固定PhysCode词汇表后，训练一个特征级世界模型，该模型以当前特征和PhysCode序列为条件，预测未来的特征和奖励。训练过程包括一个特征预测损失和一个用于评估动作价值的Q学习风格的评论器头。\n3. **预测增强的交互式推理**：使用一个8B参数的VLM（Qwen3-VL-8B）作为策略，在推理时生成多个候选的PhysCode序列。世界模型在潜在空间中对这些序列进行短时域推演，并预测其回报和价值。然后，利用这些预测的优势来通过GRPO算法对VLM策略进行强化优化，选择最佳动作执行。\n## 论文使用数据集和训练资源\n- **数据集**：研究整理了一个大规模、多样化的游戏基准，包含：\n - 863个开源复古游戏（通过stable-retro）\n - 134个轻量级HTML/Canvas游戏\n - 3个商业游戏\n 总计超过1000个游戏。数据分布覆盖了不同的游戏类别、控制接口、视觉复杂度、视角、因果机制、物理原理和操作难度。对于每个游戏，记录了4分钟60FPS的人类游戏数据，并包含了物理原理、因果机制、动作语义和游戏指令等注释。\n- **训练资源**：\n - **模型骨干**：VLM部分使用了Qwen3-VL-8B模型。\n - **特征提取**：使用DINOv3提取图像特征，使用T5编码器处理轻量级语义提示。\n - **训练流程**：分三阶段训练，包括PhysCode的预训练、世界模型的训练，以及通过交互式经验对VLM进行强化学习。\n## 论文使用的评估环境和评估指标\n- **评估环境**：主要在200个游戏组成的评估集上进行，该子集的分布（类型、动作空间、物理因果等）与完整数据集保持一致。此外，为了验证零样本迁移能力，构建了一个包含50个未见游戏的保留测试集。\n- **评估指标**：采用受马斯洛需求层次启发的三层评估体系：\n - **生存**：衡量智能体避免风险、存活的时间。指标为归一化的生存时间 H = E[T] / T_typ，其中T_typ是每个游戏下的参考存活时间（例如随机策略下的中位存活时间）。\n - **好奇心**：衡量智能体访问新颖状态的广度。指标为状态空间覆盖率，使用CLIP视觉编码器计算轨迹的多尺度度量空间幅度曲线下的面积 E = AUC(M(τ))。\n - **效用**：衡量智能体达成下游目标的能力。指标为人类归一化分数 (HNS) = (m - m_rnd) / (m_hum - m_rnd)，其中m是智能体得分，m_rnd是随机基准得分，m_hum是人类得分。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>上海交通大学</li><li>上海创新研究院</li><li>卡内基梅隆大学</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文提出了交互式物理推理器，旨在让智能体通过与环境交互来学习并持续改进人类般的物理推理能力。研究在Game-to-Unseen (G2U)设定下进行，策划了超过1000个具有不同物理和因果机制的异构游戏，并采用受马斯洛需求层次启发的三个级别进行评估：生存、好奇心和效用，分别对应从原始直觉到目标驱动推理的递进。分析发现，现有的VLM/VLA模型虽有推理能力但缺乏交互中的前瞻性，而世界模型能想象未来但倾向于模仿视觉模式而非分析物理因果。因此，IPR利用世界模型的推演来评分和强化VLM的策略，并引入了PhysCode，一种以物理为中心的动作代码，用于统一语义意图与动态，为预测和推理提供共享的动作空间。在1000多个游戏上预训练的IPR在三个级别上表现稳健，总体与GPT-5相当，并在好奇心上超越GPT-5，且性能随训练游戏和交互步数的增加而提升，并能零样本迁移到未见过的游戏中。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了G2U问题，并构建了包含1000+异构游戏的基准，采用分层评估（生存/好奇心/效用），诊断了基于预测、强化学习和VLM等方法的优势与不足。</li><li>提出了IPR范式：通过世界模型推演来评分和强化同一动作空间中的VLM策略，使得交互经验能够稳步提升物理推理能力。</li><li>引入了PhysCode，一种以物理为中心的动作代码，融合了动作语义与视觉动态，为世界模型的预测和VLM的推理之间架起了桥梁。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>IPR方法包含三个核心组件：</p>\n<ol><li><strong>诱导潜在动作词汇</strong>：通过一个VQ-VAE模型，从视频帧（DINOv3特征）、光流和动作语义中学习一个离散的、以物理为中心的动作代码，即PhysCode。训练目标是使该代码能够捕捉可跨游戏共享的动态基元，并与特定领域的视觉可供性对齐。</li><li><strong>训练潜在层面的世界模型</strong>：在固定PhysCode词汇表后，训练一个特征级世界模型，该模型以当前特征和PhysCode序列为条件，预测未来的特征和奖励。训练过程包括一个特征预测损失和一个用于评估动作价值的Q学习风格的评论器头。</li><li><strong>预测增强的交互式推理</strong>：使用一个8B参数的VLM（Qwen3-VL-8B）作为策略，在推理时生成多个候选的PhysCode序列。世界模型在潜在空间中对这些序列进行短时域推演，并预测其回报和价值。然后，利用这些预测的优势来通过GRPO算法对VLM策略进行强化优化，选择最佳动作执行。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：研究整理了一个大规模、多样化的游戏基准，包含：</li></ul>\n<p> - 863个开源复古游戏（通过stable-retro）</p>\n<p> - 134个轻量级HTML/Canvas游戏</p>\n<p> - 3个商业游戏</p>\n<p> 总计超过1000个游戏。数据分布覆盖了不同的游戏类别、控制接口、视觉复杂度、视角、因果机制、物理原理和操作难度。对于每个游戏，记录了4分钟60FPS的人类游戏数据，并包含了物理原理、因果机制、动作语义和游戏指令等注释。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - <strong>模型骨干</strong>：VLM部分使用了Qwen3-VL-8B模型。</p>\n<p> - <strong>特征提取</strong>：使用DINOv3提取图像特征，使用T5编码器处理轻量级语义提示。</p>\n<p> - <strong>训练流程</strong>：分三阶段训练，包括PhysCode的预训练、世界模型的训练，以及通过交互式经验对VLM进行强化学习。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：主要在200个游戏组成的评估集上进行，该子集的分布（类型、动作空间、物理因果等）与完整数据集保持一致。此外，为了验证零样本迁移能力，构建了一个包含50个未见游戏的保留测试集。</li><li><strong>评估指标</strong>：采用受马斯洛需求层次启发的三层评估体系：</li></ul>\n<p> - <strong>生存</strong>：衡量智能体避免风险、存活的时间。指标为归一化的生存时间 H = E[T] / T_typ，其中T_typ是每个游戏下的参考存活时间（例如随机策略下的中位存活时间）。</p>\n<p> - <strong>好奇心</strong>：衡量智能体访问新颖状态的广度。指标为状态空间覆盖率，使用CLIP视觉编码器计算轨迹的多尺度度量空间幅度曲线下的面积 E = AUC(M(τ))。</p>\n<p> - <strong>效用</strong>：衡量智能体达成下游目标的能力。指标为人类归一化分数 (HNS) = (m - m_rnd) / (m_hum - m_rnd)，其中m是智能体得分，m_rnd是随机基准得分，m_hum是人类得分。</p>"
  },
  {
    "date": "2025-11-19",
    "title": "Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception",
    "link": "http://arxiv.org/abs/2511.15279",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-18",
    "title": "$π^{*}_{0.6}$: a VLA That Learns From Experience",
    "link": "http://arxiv.org/abs/2511.14759",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-18",
    "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards",
    "link": "http://arxiv.org/abs/2511.14659",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-18",
    "title": "Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM",
    "link": "http://arxiv.org/abs/2511.14499",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-18",
    "title": "Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion",
    "link": "http://arxiv.org/abs/2511.14178",
    "summary_markdown": "### 论文研究单位\n香港中文大学，香港物流机器人中心，意大利技术研究院\n### 论文概述\n本文提出了一种名为VLA-Pilot的即插即用推理时策略引导方法，旨在解决预训练的视觉-语言-动作（VLA）模型在下游任务部署时性能下降的问题。传统方法依赖微调，但其成本高昂且不切实际。VLA-Pilot无需任何额外的策略微调或数据收集，即可实现预训练VLA的零样本部署。该方法通过利用多模态大语言模型（MLLM）进行开放式推理以获得引导目标，并采用进化扩散算法优化动作提议，从而提升策略的任务对齐能力。实验在两个不同的机器人实体上的六个真实世界操作任务中验证了该方法的有效性。\n### 论文核心贡献点\n1. 提出VLA-Pilot，一种即插即用的推理时策略引导方法，实现了冻结VLA策略在不同下游任务和机器人实体上的零样本泛化，无需额外的策略微调或数据收集。\n2. 提出一种具身推理引导的进化扩散策略，该策略联合推断广义的引导目标并优化动作提议，以增强任务对齐。\n3. 在六个真实世界操作任务和两种不同机器人实体上进行了广泛实验。结果表明，VLA-Pilot将两个预训练VLA策略的平均成功率提升了31%，显著优于所有基线方法。\n### 论文方法描述\nVLA-Pilot包含三个核心步骤：\n1. **引导目标推理**：利用一个名为“具身策略引导思维链”（EPS-CoT）的模块，通过多模态大语言模型（如GPT-4o）对任务上下文进行结构化推理。该推理过程包括引导目标确认、场景理解、具身增强（利用DINO和SAM等视觉基础模型提取空间关键点）以及生成任务对齐的奖励函数代码，作为黑盒评分函数。\n2. **动作提议优化**：引入一种进化扩散算法。首先从预训练VLA策略中采样一组初始动作提议。然后进行迭代进化搜索，在每次迭代中：使用推理出的奖励函数对动作提议进行评分；根据分数选择精英提议；对精英提议应用截断的扩散-去噪过程（即添加少量噪声后，再利用VLA策略自身的噪声预测器去噪），以在保持动作分布的同时探索和优化任务对齐的行动。\n3. **迭代引导优化**：这是一个闭环修正机制。执行动作后，EPS-CoT模块会根据初始提议、执行的动作、执行后的任务上下文和推理历史进行反思。如果检测到引导失败，MLLM会修正奖励函数并重新启动引导过程，直至任务完成，从而提高引导的准确性和鲁棒性。\n### 论文使用数据集和训练资源\n该方法是推理时方法，本身不涉及模型训练。\n- **数据集**：实验在六个真实世界的下游操作任务上进行，包括四个简单的单臂任务（Mug Handling, Bag Handling, Basket Flipping, Table Bussing）和两个复杂的多臂任务。这些任务分为分布内（ID）和分布外（OOD）两种场景以评估泛化能力。\n- **训练资源**：无需为VLA-Pilot本身进行训练。实验中使用了预训练的VLA模型（DiVLA, RDT-1B）作为基础策略，并使用GPT-4o作为推理时的MLLM。评估使用的物理机器人硬件包括DOBOT X-Trainer双臂系统和Franka Panda机械臂。\n### 论文使用的评估环境和评估指标\n- **评估环境**：真实机器人环境。主要实验平台是DOBOT X-Trainer双臂系统，包含两个6自由度的Nova2机械臂和1自由度夹爪，配备三个英特尔实感摄像头。跨实体泛化实验在Franka Panda机械臂上进行。\n- **评估指标**：\n 1. **操作成功率**：在策略引导后，成功执行下游操作的机器人动作所占的比例。\n 2. **引导目标一致性**：被选中的动作提议中，与预期引导目标对齐的比例。\n 每个方法和任务场景进行20次试验，报告平均性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>香港中文大学，香港物流机器人中心，意大利技术研究院</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为VLA-Pilot的即插即用推理时策略引导方法，旨在解决预训练的视觉-语言-动作（VLA）模型在下游任务部署时性能下降的问题。传统方法依赖微调，但其成本高昂且不切实际。VLA-Pilot无需任何额外的策略微调或数据收集，即可实现预训练VLA的零样本部署。该方法通过利用多模态大语言模型（MLLM）进行开放式推理以获得引导目标，并采用进化扩散算法优化动作提议，从而提升策略的任务对齐能力。实验在两个不同的机器人实体上的六个真实世界操作任务中验证了该方法的有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出VLA-Pilot，一种即插即用的推理时策略引导方法，实现了冻结VLA策略在不同下游任务和机器人实体上的零样本泛化，无需额外的策略微调或数据收集。</li><li>提出一种具身推理引导的进化扩散策略，该策略联合推断广义的引导目标并优化动作提议，以增强任务对齐。</li><li>在六个真实世界操作任务和两种不同机器人实体上进行了广泛实验。结果表明，VLA-Pilot将两个预训练VLA策略的平均成功率提升了31%，显著优于所有基线方法。</li></ol>\n<h3>论文方法描述</h3>\n<p>VLA-Pilot包含三个核心步骤：</p>\n<ol><li><strong>引导目标推理</strong>：利用一个名为“具身策略引导思维链”（EPS-CoT）的模块，通过多模态大语言模型（如GPT-4o）对任务上下文进行结构化推理。该推理过程包括引导目标确认、场景理解、具身增强（利用DINO和SAM等视觉基础模型提取空间关键点）以及生成任务对齐的奖励函数代码，作为黑盒评分函数。</li><li><strong>动作提议优化</strong>：引入一种进化扩散算法。首先从预训练VLA策略中采样一组初始动作提议。然后进行迭代进化搜索，在每次迭代中：使用推理出的奖励函数对动作提议进行评分；根据分数选择精英提议；对精英提议应用截断的扩散-去噪过程（即添加少量噪声后，再利用VLA策略自身的噪声预测器去噪），以在保持动作分布的同时探索和优化任务对齐的行动。</li><li><strong>迭代引导优化</strong>：这是一个闭环修正机制。执行动作后，EPS-CoT模块会根据初始提议、执行的动作、执行后的任务上下文和推理历史进行反思。如果检测到引导失败，MLLM会修正奖励函数并重新启动引导过程，直至任务完成，从而提高引导的准确性和鲁棒性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>该方法是推理时方法，本身不涉及模型训练。</p>\n<ul><li><strong>数据集</strong>：实验在六个真实世界的下游操作任务上进行，包括四个简单的单臂任务（Mug Handling, Bag Handling, Basket Flipping, Table Bussing）和两个复杂的多臂任务。这些任务分为分布内（ID）和分布外（OOD）两种场景以评估泛化能力。</li><li><strong>训练资源</strong>：无需为VLA-Pilot本身进行训练。实验中使用了预训练的VLA模型（DiVLA, RDT-1B）作为基础策略，并使用GPT-4o作为推理时的MLLM。评估使用的物理机器人硬件包括DOBOT X-Trainer双臂系统和Franka Panda机械臂。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：真实机器人环境。主要实验平台是DOBOT X-Trainer双臂系统，包含两个6自由度的Nova2机械臂和1自由度夹爪，配备三个英特尔实感摄像头。跨实体泛化实验在Franka Panda机械臂上进行。</li><li><strong>评估指标</strong>：</li></ul>\n<p> 1. <strong>操作成功率</strong>：在策略引导后，成功执行下游操作的机器人动作所占的比例。</p>\n<p> 2. <strong>引导目标一致性</strong>：被选中的动作提议中，与预期引导目标对齐的比例。</p>\n<p> 每个方法和任务场景进行20次试验，报告平均性能。</p>"
  },
  {
    "date": "2025-11-18",
    "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
    "link": "http://arxiv.org/abs/2511.14161",
    "summary_markdown": "### 论文研究单位\n\n华中科技大学, 香港大学, INFIFORCE Intelligent Technology Co., Ltd., 浙江大学, 深圳光明实验室.\n### 论文概述\n\n论文提出了 RoboTidy，一个用于语言引导家庭整理任务的统一基准。该基准旨在解决现有基准在建模用户偏好、支持移动性和泛化能力方面的不足。RoboTidy 支持 Vision-Language-Action (VLA) 和 Vision-Language-Navigation (VLN) 模型的训练与评估，通过提供基于 3D Gaussian Splatting (3DGS) 的高保真家庭场景、物理真实的仿真环境以及大规模的演示轨迹，为家庭整理任务提供了一个全面且可扩展的评估平台，并用于验证仿真到现实的迁移能力。\n### 论文核心贡献点\n\n- 提出了一种“Action (Object, Container) list”的对象整理方法，利用 Qwen2.5-VL 模型从观察中自动推断放置规则，并决定具体动作。\n- 构建了 500 个基于 3D Gaussian Splatting 的照片级真实感家庭场景，覆盖 500 种物体和容器，并提供了用于训练的 6.4k 操作演示轨迹和 1.5k 导航轨迹。\n- 提出了一个统一的 RoboTidy 基准，支持对 VLA 和 VLN 方法进行模块化评估，并通过真实世界的移动双臂整理实验验证了仿真到现实的迁移。\n### 论文方法描述\n\nRoboTidy 采用模块化框架在 NVIDIA Isaac Sim 中构建。系统首先通过多视角传感器获取场景信息，使用 Qwen2.5-VL 模型解析物体和容器的类别与属性，并生成一个结构化的“Action (Object, Container)”列表。该列表包含四种基本操作：Pick and Place（抓取并放置）、Pick and Toss（抓取并投掷）、Open Container（打开容器）和 Close Container（关闭容器）。这些操作根据属性、功能、安全性和卫生四个标准来确定优先级。导航模块使用 A* 规划器在 2D 语义地图上生成路径，并通过 PID 控制器进行轨迹跟踪。操作模块则结合逆向运动学（IK）求解器和运动规划器来生成无碰撞的机器人操作轨迹。整个系统支持多模态传感器数据（RGB-D 和 LiDAR）的采集，用于训练和评估。\n### 论文使用数据集和训练资源\n\n论文构建了 RoboTidy 数据集，其中包含：\n- 500 个基于 InteriorGS 构建的 3DGS 家庭场景，这些场景为 3DGS-网格混合结构，以支持高保真渲染和物理碰撞。\n- 涵盖 500 种日常物体和容器的 3D 资产库。\n- 6.4k 条针对四种操作的高质量操作演示轨迹。\n- 1.5k 条跨房间导航轨迹。\n- 400 条真实世界的操作演示数据。\n训练与仿真在 NVIDIA Isaac Sim 5.0 环境中进行。\n### 论文使用的评估环境和评估指标\n\n评估在仿真环境 NVIDIA Isaac Sim 5.0 中进行，并通过真实世界的移动机器人部署了仿真到现实的迁移实验。评估指标包括：\n- Object Placement Accuracy (OPA)：衡量将物体放置到正确容器的准确率，计算公式为正确放置的物体数除以总物体数。\n- Valid Sorting Success Rate (VSSR)：衡量物体被放置到有效容器且所需操作成功完成的比率，结合了容器选择的正确性和操作执行的成败。\n- Navigation Success Rate：评估 VLN 模型导航任务的成功率，并与 R2R 等其他基准进行了对比。",
    "summary_html": "<h3>论文研究单位</h3>\n\n<p>华中科技大学, 香港大学, INFIFORCE Intelligent Technology Co., Ltd., 浙江大学, 深圳光明实验室.</p>\n<h3>论文概述</h3>\n\n<p>论文提出了 RoboTidy，一个用于语言引导家庭整理任务的统一基准。该基准旨在解决现有基准在建模用户偏好、支持移动性和泛化能力方面的不足。RoboTidy 支持 Vision-Language-Action (VLA) 和 Vision-Language-Navigation (VLN) 模型的训练与评估，通过提供基于 3D Gaussian Splatting (3DGS) 的高保真家庭场景、物理真实的仿真环境以及大规模的演示轨迹，为家庭整理任务提供了一个全面且可扩展的评估平台，并用于验证仿真到现实的迁移能力。</p>\n<h3>论文核心贡献点</h3>\n\n<ul><li>提出了一种“Action (Object, Container) list”的对象整理方法，利用 Qwen2.5-VL 模型从观察中自动推断放置规则，并决定具体动作。</li><li>构建了 500 个基于 3D Gaussian Splatting 的照片级真实感家庭场景，覆盖 500 种物体和容器，并提供了用于训练的 6.4k 操作演示轨迹和 1.5k 导航轨迹。</li><li>提出了一个统一的 RoboTidy 基准，支持对 VLA 和 VLN 方法进行模块化评估，并通过真实世界的移动双臂整理实验验证了仿真到现实的迁移。</li></ul>\n<h3>论文方法描述</h3>\n\n<p>RoboTidy 采用模块化框架在 NVIDIA Isaac Sim 中构建。系统首先通过多视角传感器获取场景信息，使用 Qwen2.5-VL 模型解析物体和容器的类别与属性，并生成一个结构化的“Action (Object, Container)”列表。该列表包含四种基本操作：Pick and Place（抓取并放置）、Pick and Toss（抓取并投掷）、Open Container（打开容器）和 Close Container（关闭容器）。这些操作根据属性、功能、安全性和卫生四个标准来确定优先级。导航模块使用 A* 规划器在 2D 语义地图上生成路径，并通过 PID 控制器进行轨迹跟踪。操作模块则结合逆向运动学（IK）求解器和运动规划器来生成无碰撞的机器人操作轨迹。整个系统支持多模态传感器数据（RGB-D 和 LiDAR）的采集，用于训练和评估。</p>\n<h3>论文使用数据集和训练资源</h3>\n\n<p>论文构建了 RoboTidy 数据集，其中包含：</p>\n<ul><li>500 个基于 InteriorGS 构建的 3DGS 家庭场景，这些场景为 3DGS-网格混合结构，以支持高保真渲染和物理碰撞。</li><li>涵盖 500 种日常物体和容器的 3D 资产库。</li><li>6.4k 条针对四种操作的高质量操作演示轨迹。</li><li>1.5k 条跨房间导航轨迹。</li><li>400 条真实世界的操作演示数据。</li></ul>\n<p>训练与仿真在 NVIDIA Isaac Sim 5.0 环境中进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n\n<p>评估在仿真环境 NVIDIA Isaac Sim 5.0 中进行，并通过真实世界的移动机器人部署了仿真到现实的迁移实验。评估指标包括：</p>\n<ul><li>Object Placement Accuracy (OPA)：衡量将物体放置到正确容器的准确率，计算公式为正确放置的物体数除以总物体数。</li><li>Valid Sorting Success Rate (VSSR)：衡量物体被放置到有效容器且所需操作成功完成的比率，结合了容器选择的正确性和操作执行的成败。</li><li>Navigation Success Rate：评估 VLN 模型导航任务的成功率，并与 R2R 等其他基准进行了对比。</li></ul>"
  },
  {
    "date": "2025-11-18",
    "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.14148",
    "summary_markdown": "### 论文研究单位\n上海人工智能实验室、清华大学、浙江大学、Lumos Robotics\n### 论文概述\n本文提出了AsyncVLA，一种新型的视觉-语言-动作（VLA）模型框架。该框架通过引入异步流匹配（AFM）和置信度驱动的自校正机制，解决了传统同步流匹配（SFM）在长视野任务中因刚性时间表而导致的错误累积问题。AsyncVLA首先使用SFM生成初始动作，然后通过置信度评估器筛选低置信度的动作进行异步再生，从而利用动作上下文信息实现选择性自校正，提升了模型的鲁棒性和任务成功率。\n### 论文核心贡献点\n1. 提出了AsyncVLA框架，首次将异步流匹配（AFM）引入VLA模型，突破了SFM的刚性同步时间表限制。\n2. 设计了置信度评估器（Confidence Rater），用于评估SFM生成的初始动作的置信度，并根据置信度动态选择需要再生成的动作标记。\n3. 实现了SFM和AFM的统一训练流程，使单一模型同时具备两种模式，并复用VL KV-cache以提高推理效率。\n4. 通过多个机器人操作基准测试验证了模型的数据高效性和自校正能力，在通用具身评估中实现了最先进的性能。\n### 论文方法描述\nAsyncVLA包含三个 sequential 部分：同步流匹配（SFM）、置信度评估器（Confidence Rater）和异步流匹配（AFM）。\n1. **SFM**：使用统一时间表从噪声同步生成所有动作标记。\n2. **置信度评估器**：由Transformer层和线性层组成，接收VL标记和SFM生成的动作作为输入，输出每个动作标记的置信度分数。通过比较置信度阈值生成二进制掩码，标记低置信度动作。\n3. **AFM**：根据掩码动态分配时间步，仅对掩码标记进行再生，未掩码标记保持不变。SFM生成的动作作为上下文信息，辅助修正低置信度动作。\n4. **统一训练**：将SFM视为全掩码AFM的特例，通过最小化掩码标记上的AFM速度预测损失，联合训练VLA主干和FM动作头，确保模型同时支持SFM和AFM推理。\n### 论文使用数据集和训练资源\n1. **数据集**：Open X-Embodiment（预训练），LIBERO、Bridge-V2、Fractal（微调）。\n2. **训练资源**：预训练使用32个H200 GPU，微调使用8个H200 GPU。预训练约2.5天，微调需15-32小时。全局批次大小为2048，使用BF16精度、ZeRO-2优化器分片、Flash-Attention-2。优化器为AdamW，权重衰减为0，β1=0.9，β2=0.999。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：LIBERO（模拟）、WidowX机器人（SimplerEnv模拟）、Google机器人（SimplerEnv模拟）。\n2. **评估指标**：任务成功率（Success Rate），即任务完成的成功率百分比，每个任务套件测试500次试验（10任务×50片段）。在Google机器人基准上还分为视觉匹配（M）和变体聚合（A）两种协议。</think>\n### 论文研究单位\n上海人工智能实验室、清华大学、浙江大学、Lumos Robotics\n### 论文概述\n本文提出了AsyncVLA，一种新型的视觉-语言-动作（VLA）模型框架。该框架通过引入异步流匹配（AFM）和置信度驱动的自校正机制，解决了传统同步流匹配（SFM）在长视野任务中因刚性时间表而导致的错误累积问题。AsyncVLA首先使用SFM生成初始动作，然后通过置信度评估器筛选低置信度的动作进行异步再生，从而利用动作上下文信息实现选择性自校正，提升了模型的鲁棒性和任务成功率。\n### 论文核心贡献点\n1. 提出了AsyncVLA框架，首次将异步流匹配（AFM）引入VLA模型，突破了SFM的刚性同步时间表限制。\n2. 设计了置信度评估器（Confidence Rater），用于评估SFM生成的初始动作的置信度，并根据置信度动态选择需要再生成的动作标记。\n3. 实现了SFM和AFM的统一训练流程，使单一模型同时具备两种模式，并复用VL KV-cache以提高推理效率。\n4. 通过多个机器人操作基准测试验证了模型的数据高效性和自校正能力，在通用具身评估中实现了最先进的性能。\n### 论文方法描述\nAsyncVLA包含三个 sequential 部分：同步流匹配（SFM）、置信度评估器（Confidence Rater）和异步流匹配（AFM）。\n1. **SFM**：使用统一时间表从噪声同步生成所有动作标记。\n2. **置信度评估器**：由Transformer层和线性层组成，接收VL标记和SFM生成的动作作为输入，输出每个动作标记的置信度分数。通过比较置信度阈值生成二进制掩码，标记低置信度动作。\n3. **AFM**：根据掩码动态分配时间步，仅对掩码标记进行再生，未掩码标记保持不变。SFM生成的动作作为上下文信息，辅助修正低置信度动作。\n4. **统一训练**：将SFM视为全掩码AFM的特例，通过最小化掩码标记上的AFM速度预测损失，联合训练VLA主干和FM动作头，确保模型同时支持SFM和AFM推理。\n### 论文使用数据集和训练资源\n1. **数据集**：Open X-Embodiment（预训练），LIBERO、Bridge-V2、Fractal（微调）。\n2. **训练资源**：预训练使用32个H200 GPU，微调使用8个H200 GPU。预训练约2.5天，微调需15-32小时。全局批次大小为2048，使用BF16精度、ZeRO-2优化器分片、Flash-Attention-2。优化器为AdamW，权重衰减为0，β1=0.9，β2=0.999。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：LIBERO（模拟）、WidowX机器人（SimplerEnv模拟）、Google机器人（SimplerEnv模拟）。\n2. **评估指标**：任务成功率（Success Rate），即任务完成的成功率百分比，每个任务套件测试500次试验（10任务×50片段）。在Google机器人基准上还分为视觉匹配（M）和变体聚合（A）两种协议。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海人工智能实验室、清华大学、浙江大学、Lumos Robotics</p>\n<h3>论文概述</h3>\n<p>本文提出了AsyncVLA，一种新型的视觉-语言-动作（VLA）模型框架。该框架通过引入异步流匹配（AFM）和置信度驱动的自校正机制，解决了传统同步流匹配（SFM）在长视野任务中因刚性时间表而导致的错误累积问题。AsyncVLA首先使用SFM生成初始动作，然后通过置信度评估器筛选低置信度的动作进行异步再生，从而利用动作上下文信息实现选择性自校正，提升了模型的鲁棒性和任务成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了AsyncVLA框架，首次将异步流匹配（AFM）引入VLA模型，突破了SFM的刚性同步时间表限制。</li><li>设计了置信度评估器（Confidence Rater），用于评估SFM生成的初始动作的置信度，并根据置信度动态选择需要再生成的动作标记。</li><li>实现了SFM和AFM的统一训练流程，使单一模型同时具备两种模式，并复用VL KV-cache以提高推理效率。</li><li>通过多个机器人操作基准测试验证了模型的数据高效性和自校正能力，在通用具身评估中实现了最先进的性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>AsyncVLA包含三个 sequential 部分：同步流匹配（SFM）、置信度评估器（Confidence Rater）和异步流匹配（AFM）。</p>\n<ol><li><strong>SFM</strong>：使用统一时间表从噪声同步生成所有动作标记。</li><li><strong>置信度评估器</strong>：由Transformer层和线性层组成，接收VL标记和SFM生成的动作作为输入，输出每个动作标记的置信度分数。通过比较置信度阈值生成二进制掩码，标记低置信度动作。</li><li><strong>AFM</strong>：根据掩码动态分配时间步，仅对掩码标记进行再生，未掩码标记保持不变。SFM生成的动作作为上下文信息，辅助修正低置信度动作。</li><li><strong>统一训练</strong>：将SFM视为全掩码AFM的特例，通过最小化掩码标记上的AFM速度预测损失，联合训练VLA主干和FM动作头，确保模型同时支持SFM和AFM推理。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：Open X-Embodiment（预训练），LIBERO、Bridge-V2、Fractal（微调）。</li><li><strong>训练资源</strong>：预训练使用32个H200 GPU，微调使用8个H200 GPU。预训练约2.5天，微调需15-32小时。全局批次大小为2048，使用BF16精度、ZeRO-2优化器分片、Flash-Attention-2。优化器为AdamW，权重衰减为0，β1=0.9，β2=0.999。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：LIBERO（模拟）、WidowX机器人（SimplerEnv模拟）、Google机器人（SimplerEnv模拟）。</li><li><strong>评估指标</strong>：任务成功率（Success Rate），即任务完成的成功率百分比，每个任务套件测试500次试验（10任务×50片段）。在Google机器人基准上还分为视觉匹配（M）和变体聚合（A）两种协议。</think></li></ol>\n<h3>论文研究单位</h3>\n<p>上海人工智能实验室、清华大学、浙江大学、Lumos Robotics</p>\n<h3>论文概述</h3>\n<p>本文提出了AsyncVLA，一种新型的视觉-语言-动作（VLA）模型框架。该框架通过引入异步流匹配（AFM）和置信度驱动的自校正机制，解决了传统同步流匹配（SFM）在长视野任务中因刚性时间表而导致的错误累积问题。AsyncVLA首先使用SFM生成初始动作，然后通过置信度评估器筛选低置信度的动作进行异步再生，从而利用动作上下文信息实现选择性自校正，提升了模型的鲁棒性和任务成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了AsyncVLA框架，首次将异步流匹配（AFM）引入VLA模型，突破了SFM的刚性同步时间表限制。</li><li>设计了置信度评估器（Confidence Rater），用于评估SFM生成的初始动作的置信度，并根据置信度动态选择需要再生成的动作标记。</li><li>实现了SFM和AFM的统一训练流程，使单一模型同时具备两种模式，并复用VL KV-cache以提高推理效率。</li><li>通过多个机器人操作基准测试验证了模型的数据高效性和自校正能力，在通用具身评估中实现了最先进的性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>AsyncVLA包含三个 sequential 部分：同步流匹配（SFM）、置信度评估器（Confidence Rater）和异步流匹配（AFM）。</p>\n<ol><li><strong>SFM</strong>：使用统一时间表从噪声同步生成所有动作标记。</li><li><strong>置信度评估器</strong>：由Transformer层和线性层组成，接收VL标记和SFM生成的动作作为输入，输出每个动作标记的置信度分数。通过比较置信度阈值生成二进制掩码，标记低置信度动作。</li><li><strong>AFM</strong>：根据掩码动态分配时间步，仅对掩码标记进行再生，未掩码标记保持不变。SFM生成的动作作为上下文信息，辅助修正低置信度动作。</li><li><strong>统一训练</strong>：将SFM视为全掩码AFM的特例，通过最小化掩码标记上的AFM速度预测损失，联合训练VLA主干和FM动作头，确保模型同时支持SFM和AFM推理。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：Open X-Embodiment（预训练），LIBERO、Bridge-V2、Fractal（微调）。</li><li><strong>训练资源</strong>：预训练使用32个H200 GPU，微调使用8个H200 GPU。预训练约2.5天，微调需15-32小时。全局批次大小为2048，使用BF16精度、ZeRO-2优化器分片、Flash-Attention-2。优化器为AdamW，权重衰减为0，β1=0.9，β2=0.999。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：LIBERO（模拟）、WidowX机器人（SimplerEnv模拟）、Google机器人（SimplerEnv模拟）。</li><li><strong>评估指标</strong>：任务成功率（Success Rate），即任务完成的成功率百分比，每个任务套件测试500次试验（10任务×50片段）。在Google机器人基准上还分为视觉匹配（M）和变体聚合（A）两种协议。</li></ol>"
  },
  {
    "date": "2025-11-16",
    "title": "VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.12405",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-14",
    "title": "Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective",
    "link": "http://arxiv.org/abs/2511.11478",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-14",
    "title": "Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.11298",
    "summary_markdown": "```markdown\n### 论文研究单位\nMacquarie University, Sydney, NSW, Australia.\n### 论文概述\n该论文报告了对四种代表性视觉-语言-动作（VLA）模型在机器人操作任务上进行基准测试的经验研究。研究在ALOHA Mobile平台上进行了四个操作任务，结合了真实世界和仿真环境中的评估。论文建立了一个标准化的评估框架，从准确性、效率、适应性（包括分布内和分布外场景）以及语言指令遵循能力三个维度衡量模型性能。研究发现，π₀模型在分布外场景中表现出优越的适应性，而ACT模型在分布内环境中提供了最高的稳定性。此外，研究还识别了常见的失败模式，如抓取偏差、过早释放和长时程状态漂移，并揭示了不同VLA模型架构在精度、泛化和部署成本之间的实际权衡。\n### 论文核心贡献点\n1. **统一基准**：引入了一个双臂操作基准，包含明确定义的分布内和分布外评估设置，并发布了标准化的可复现评估流程，用于公平的跨模型比较。\n2. **失败分类法与诊断分析**：开发了一个结构化的失败分类法，捕获了时间漂移、符号接地失败和执行失误等常见错误模式，并利用它来揭示不同架构模型特有的弱点。\n3. **鲁棒性与数据扩展的经验见解**：通过受控实验，揭示了专业模仿学习策略与通用VLA模型在鲁棒性-精度上的权衡，并表明在长时程、可变形物体任务上，性能会随着有限的演示数据扩展而饱和。\n### 论文方法描述\n论文对四种模型进行了评估：一个专家级模仿学习策略（ACT）和三个通用VLA模型（OpenVLA–OFT, RDT-1B, π₀）。评估在物理和仿真两个环境中进行。物理环境使用ALOHA Mobile平台，该平台配备两个7自由度机械臂、三台Intel RealSense D405 RGB-D相机，并运行在NVIDIA RTX 5090 GPU上，控制频率为25Hz。仿真环境则是一个基于MuJoCo的桌面环境，用于评估零样本的语言理解和空间推理能力。研究设计了四个双臂操作任务，涉及工具使用、可变形物体处理和语言指令理解。评估分为三种条件：分布内、空间分布外和实例+空间分布外，以系统性地测试模型的泛化能力。\n### 论文使用数据集和训练资源\n所有训练数据均通过ALOHA Mobile平台的遥操作收集。对于“Clean Dish”、“Put Sponge into Pot”和“Unzip Bag”任务，各收集了100个演示数据；“Folding Shorts”任务收集了200个演示数据。训练资源方面，ACT和π₀在单块NVIDIA RTX 5090 GPU上进行训练或微调，耗时分别约为2小时和24小时。OpenVLA–OFT和RDT-1B由于需要更大内存，在两块NVIDIA A6000 GPU上进行微调，训练时间均超过2周。\n### 论文使用的评估环境和评估指标\n评估环境包括：\n1. **物理环境**：ALOHA Mobile平台，在一个1米x1米的工作台前操作，使用三台相机提供感知输入，并保持恒定的光照条件。\n2. **仿真环境**：一个基于MuJoCo的桌面环境，包含三个用于测试语言和空间推理任务的彩色立方体。\n\n评估指标包括：\n1. **任务成功率**：成功完成目标的试验百分比。\n2. **成功时间**：仅在成功的试验中，从第一个动作到任务完成所经过的时间。\n3. **指令遵循准确率**：对于语言指定的目标，其目标选择和关系接头的正确性。\n\n为确保公平性，每个（任务、模型、设置）组合进行50次试验，并重用相同的随机初始状态。成功率和指令遵循准确率报告了Wilson-score 95%置信区间。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>Macquarie University, Sydney, NSW, Australia.</p>\n<h3>论文概述</h3>\n<p>该论文报告了对四种代表性视觉-语言-动作（VLA）模型在机器人操作任务上进行基准测试的经验研究。研究在ALOHA Mobile平台上进行了四个操作任务，结合了真实世界和仿真环境中的评估。论文建立了一个标准化的评估框架，从准确性、效率、适应性（包括分布内和分布外场景）以及语言指令遵循能力三个维度衡量模型性能。研究发现，π₀模型在分布外场景中表现出优越的适应性，而ACT模型在分布内环境中提供了最高的稳定性。此外，研究还识别了常见的失败模式，如抓取偏差、过早释放和长时程状态漂移，并揭示了不同VLA模型架构在精度、泛化和部署成本之间的实际权衡。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>统一基准</strong>：引入了一个双臂操作基准，包含明确定义的分布内和分布外评估设置，并发布了标准化的可复现评估流程，用于公平的跨模型比较。</li><li><strong>失败分类法与诊断分析</strong>：开发了一个结构化的失败分类法，捕获了时间漂移、符号接地失败和执行失误等常见错误模式，并利用它来揭示不同架构模型特有的弱点。</li><li><strong>鲁棒性与数据扩展的经验见解</strong>：通过受控实验，揭示了专业模仿学习策略与通用VLA模型在鲁棒性-精度上的权衡，并表明在长时程、可变形物体任务上，性能会随着有限的演示数据扩展而饱和。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文对四种模型进行了评估：一个专家级模仿学习策略（ACT）和三个通用VLA模型（OpenVLA–OFT, RDT-1B, π₀）。评估在物理和仿真两个环境中进行。物理环境使用ALOHA Mobile平台，该平台配备两个7自由度机械臂、三台Intel RealSense D405 RGB-D相机，并运行在NVIDIA RTX 5090 GPU上，控制频率为25Hz。仿真环境则是一个基于MuJoCo的桌面环境，用于评估零样本的语言理解和空间推理能力。研究设计了四个双臂操作任务，涉及工具使用、可变形物体处理和语言指令理解。评估分为三种条件：分布内、空间分布外和实例+空间分布外，以系统性地测试模型的泛化能力。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>所有训练数据均通过ALOHA Mobile平台的遥操作收集。对于“Clean Dish”、“Put Sponge into Pot”和“Unzip Bag”任务，各收集了100个演示数据；“Folding Shorts”任务收集了200个演示数据。训练资源方面，ACT和π₀在单块NVIDIA RTX 5090 GPU上进行训练或微调，耗时分别约为2小时和24小时。OpenVLA–OFT和RDT-1B由于需要更大内存，在两块NVIDIA A6000 GPU上进行微调，训练时间均超过2周。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境包括：</p>\n<ol><li><strong>物理环境</strong>：ALOHA Mobile平台，在一个1米x1米的工作台前操作，使用三台相机提供感知输入，并保持恒定的光照条件。</li><li><strong>仿真环境</strong>：一个基于MuJoCo的桌面环境，包含三个用于测试语言和空间推理任务的彩色立方体。</li></ol>\n\n<p>评估指标包括：</p>\n<ol><li><strong>任务成功率</strong>：成功完成目标的试验百分比。</li><li><strong>成功时间</strong>：仅在成功的试验中，从第一个动作到任务完成所经过的时间。</li><li><strong>指令遵循准确率</strong>：对于语言指定的目标，其目标选择和关系接头的正确性。</li></ol>\n\n<p>为确保公平性，每个（任务、模型、设置）组合进行50次试验，并重用相同的随机初始状态。成功率和指令遵循准确率报告了Wilson-score 95%置信区间。</p>\n<p>```</p>"
  },
  {
    "date": "2025-11-13",
    "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded",
    "link": "http://arxiv.org/abs/2511.10560",
    "summary_markdown": "## 论文研究单位\n香港科技大学(HKUST)、南洋理工大学(NTU)、中山大学(SYSU)、新加坡国立大学(NUS)、阿里巴巴集团\n## 论文概述\n论文提出了OmniVGGT框架，这是一个能够有效利用任意数量辅助几何模态（如深度图、相机内参和外参）进行训练和推理的视觉基础模型。该框架通过轻量级的GeoAdapter模块逐步注入几何信息，并采用随机多模态融合策略，使模型能够在测试时处理任意数量的模态输入。在单目/多视角深度估计、多视角立体视觉和相机姿态估计等多个3D视觉任务上取得了最先进的性能。\n## 论文核心贡献点\n1. 提出了GeoAdapter，一个轻量级适配器，能够将深度图和相机参数编码到空间基础模型中，使用零初始化卷积逐步注入几何信息而不破坏基础模型的表示空间\n2. 设计了随机多模态融合策略，在训练时随机采样模态子集，使模型能够在测试时接受任意数量的模态输入，并学习鲁棒的空间表示\n3. 在多种3D视觉任务上超越了现有方法，即使仅使用RGB输入也取得了最先进的结果\n4. 成功将OmniVGGT集成到视觉-语言-动作(VLA)模型中，在机器人操作任务上实现了持续的性能提升\n## 论文方法描述\n1. **GeoAdapter架构**：包含相机适配器和深度适配器两部分。相机适配器对相机内参和外参进行归一化、参数化为特征向量，然后通过专用编码器和零卷积层处理；深度适配器对深度图进行批内归一化，通过卷积层编码为空间标记\n2. **随机多模态训练**：在训练过程中随机为图像序列分配不同数量的辅助信息（相机参数和深度图），确保模型对各种部分信息场景具有鲁棒性\n3. **训练目标**：采用多任务损失函数，包括相机损失、深度损失和点图损失，每个都增强梯度项以提高局部几何一致性\n4. **网络架构**：基于VGGT框架，使用24个交替注意力块处理输入，最终通过三个预测头输出深度图、相机姿态和3D点图\n## 论文使用数据集和训练资源\n使用19个公共数据集进行训练，包括ARKitScenes、BlendedMVS、DL3DV、Dynamic Replica、HyperSim、Kubric、MapFree、MegaDepth、Matterport 3D、MVS-Synth、ScanNet、ScanNet++、Spring、TartanAir、UASOL、Unreal 4K、Virtual KITTI、Waymo和WildRGBD。这些数据集涵盖了合成和真实内容、室内外环境以及静态和动态场景。训练使用32个NVIDIA A100 GPU，耗时10天，采用梯度检查点优化内存使用。\n## 论文使用的评估环境和评估指标\n在多个标准数据集上进行评估：\n- **深度估计**：使用绝对相对误差和δ<1.25指标\n- **相机姿态估计**：使用相对旋转精度(RRA)、相对平移精度(RTA)和AUC(精度阈值曲线下面积)\n- **3D重建**：使用精度、完整性和法线一致性指标\n- **机器人操作任务**：在CALVIN数据集上评估平均长度等指标\n模型在Sintel、Bonn、NYU-v2、ScanNet、ETH3D、DTU、Tanks and Temples、7-Scenes、Co3Dv2和RealEstate10K等数据集上进行了广泛测试。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>香港科技大学(HKUST)、南洋理工大学(NTU)、中山大学(SYSU)、新加坡国立大学(NUS)、阿里巴巴集团</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文提出了OmniVGGT框架，这是一个能够有效利用任意数量辅助几何模态（如深度图、相机内参和外参）进行训练和推理的视觉基础模型。该框架通过轻量级的GeoAdapter模块逐步注入几何信息，并采用随机多模态融合策略，使模型能够在测试时处理任意数量的模态输入。在单目/多视角深度估计、多视角立体视觉和相机姿态估计等多个3D视觉任务上取得了最先进的性能。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出了GeoAdapter，一个轻量级适配器，能够将深度图和相机参数编码到空间基础模型中，使用零初始化卷积逐步注入几何信息而不破坏基础模型的表示空间</li><li>设计了随机多模态融合策略，在训练时随机采样模态子集，使模型能够在测试时接受任意数量的模态输入，并学习鲁棒的空间表示</li><li>在多种3D视觉任务上超越了现有方法，即使仅使用RGB输入也取得了最先进的结果</li><li>成功将OmniVGGT集成到视觉-语言-动作(VLA)模型中，在机器人操作任务上实现了持续的性能提升</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ol><li><strong>GeoAdapter架构</strong>：包含相机适配器和深度适配器两部分。相机适配器对相机内参和外参进行归一化、参数化为特征向量，然后通过专用编码器和零卷积层处理；深度适配器对深度图进行批内归一化，通过卷积层编码为空间标记</li><li><strong>随机多模态训练</strong>：在训练过程中随机为图像序列分配不同数量的辅助信息（相机参数和深度图），确保模型对各种部分信息场景具有鲁棒性</li><li><strong>训练目标</strong>：采用多任务损失函数，包括相机损失、深度损失和点图损失，每个都增强梯度项以提高局部几何一致性</li><li><strong>网络架构</strong>：基于VGGT框架，使用24个交替注意力块处理输入，最终通过三个预测头输出深度图、相机姿态和3D点图</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<p>使用19个公共数据集进行训练，包括ARKitScenes、BlendedMVS、DL3DV、Dynamic Replica、HyperSim、Kubric、MapFree、MegaDepth、Matterport 3D、MVS-Synth、ScanNet、ScanNet++、Spring、TartanAir、UASOL、Unreal 4K、Virtual KITTI、Waymo和WildRGBD。这些数据集涵盖了合成和真实内容、室内外环境以及静态和动态场景。训练使用32个NVIDIA A100 GPU，耗时10天，采用梯度检查点优化内存使用。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p>在多个标准数据集上进行评估：</p>\n<ul><li><strong>深度估计</strong>：使用绝对相对误差和δ<1.25指标</li><li><strong>相机姿态估计</strong>：使用相对旋转精度(RRA)、相对平移精度(RTA)和AUC(精度阈值曲线下面积)</li><li><strong>3D重建</strong>：使用精度、完整性和法线一致性指标</li><li><strong>机器人操作任务</strong>：在CALVIN数据集上评估平均长度等指标</li></ul>\n<p>模型在Sintel、Bonn、NYU-v2、ScanNet、ETH3D、DTU、Tanks and Temples、7-Scenes、Co3Dv2和RealEstate10K等数据集上进行了广泛测试。</p>"
  },
  {
    "date": "2025-11-13",
    "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.10518",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-13",
    "title": "Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks",
    "link": "http://arxiv.org/abs/2511.10008",
    "summary_markdown": "## 论文研究单位\n浙江大学\n## 论文概述\n本文首次系统性地研究了视觉-语言-动作（VLA）模型在面对物理传感器攻击时的脆弱性。鉴于VLA模型严重依赖摄像头和麦克风等传感器输入，作者旨在量化物理传感器攻击（如激光、电磁、超声波）对模型性能的影响，并探索相应的防御机制。论文提出了一个名为“真实-模拟-真实”的框架，用于自动模拟物理世界中的传感器攻击向量，并在模拟和真实机器人系统上进行验证。通过对多种VLA模型和任务进行大规模评估，研究揭示了这些模型存在的显著漏洞。最后，作者提出并验证了一种基于对抗训练的防御方法，以增强VLA模型对此类物理扰动的鲁棒性。\n## 论文核心贡献点\n- 验证了VLA模型在物理传感器攻击面前的脆弱性，并证实攻击可导致其在真实世界中行为异常。\n- 提出了一个“真实-模拟-真实”框架，以系统且现实的方式评估VLA模型对抗物理传感器攻击的鲁棒性，有效连接了纯数字攻击模拟与资源密集的物理实验之间的差距。\n- 进行了大规模的鲁棒性评估，涵盖了多种VLA模型和任务。至关重要的是，通过在真实世界系统上进行的针对性物理实验，验证了模拟环境的发现，从而证实了该框架的有效性。\n- 提出并验证了一种基于对抗训练的防御策略，用于抵御这些物理攻击，同时保持VLA模型在干净数据集上的性能。\n## 论文方法描述\n论文方法的核心是“真实-模拟-真实”框架。\n1. **攻击选择与模拟**：首先，从顶级安全会议中筛选出八种代表性的物理传感器攻击，其中六种针对摄像头（激光致盲、光投影、激光彩色条纹、电磁彩色条纹、电磁截断、超声波模糊），两种针对麦克风（语音拒绝服务、语音欺骗）。然后，基于这些攻击的物理原理，在数字领域实现高保真度的模拟，并定义了弱、中、强三个攻击强度等级。\n2. **麦克风攻击设计**：\n - **语音拒绝服务**：通过注入高强度超声波信号使麦克风传感器饱和，从而阻止有效指令的接收。\n - **语音欺骗**：通过调制的激光或超声波信号向麦克风注入恶意的语音指令。\n3. **摄像头攻击设计**：\n - **激光致盲**：使用高功率激光照射摄像头，使其光电传感器饱和，无法捕捉环境光线。\n - **光投影**：使用投影仪向环境或镜头投射虚假图像，干扰视觉感知。\n - **激光彩色条纹**：利用摄像头CMOS传感器的卷帘快门效应，通过开关调制激光注入彩色条纹。\n - **电磁彩色条纹与截断**：通过向摄像头图像传输接口（如MIPI CSI-2）注入电磁干扰，导致颜色解码错误或图像截断。\n - **超声波模糊**：向配备防抖模块的摄像头注入超声波，引起其惯性测量单元（IMU）共振，误导防抖算法进行不必要的补偿，导致图像模糊。\n4. **防御方法**：提出一种基于对抗训练的防御策略。该策略首先在干净数据集上训练VLA模型，然后在训练数据中混合一定比例（如30%）的经过攻击模拟的数据集，并对模型进行微调，使其对由传感器攻击引起的分布外扰动具有鲁棒性。\n## 论文使用数据集和训练资源\n- **数据集**：\n - 模拟环境：使用Libero模拟器及其配套的数据集，包括Libero-Spatial、Libero-Object、Libero-Goal和Libero-Long。\n - 真实世界环境：为适应真实世界，通过遥操作收集了一小时的机械臂操作数据，用于微调模型以完成积木抓取与放置任务。\n- **训练资源**：\n - 模型运行与评估：NVIDIA 4090 GPU。\n - 模型微调：使用Lora技术，在NVIDIA H800 GPU (80GB) 上进行。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：Libero模拟器。\n - 真实世界环境：一个装备有Franka Emika Panda机械臂的实验平台。该机械臂配备了一个全局摄像头（Intel RealSense D435i）、一个手腕摄像头（Intel RealSense D435i）和一个麦克风。攻击平台包括电磁干扰（EMI）平台、投影平台、激光平台和超声波平台。\n- **评估指标**：\n - 任务成功率：定义为成功完成的任务次数与总任务次数的比率。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>浙江大学</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文首次系统性地研究了视觉-语言-动作（VLA）模型在面对物理传感器攻击时的脆弱性。鉴于VLA模型严重依赖摄像头和麦克风等传感器输入，作者旨在量化物理传感器攻击（如激光、电磁、超声波）对模型性能的影响，并探索相应的防御机制。论文提出了一个名为“真实-模拟-真实”的框架，用于自动模拟物理世界中的传感器攻击向量，并在模拟和真实机器人系统上进行验证。通过对多种VLA模型和任务进行大规模评估，研究揭示了这些模型存在的显著漏洞。最后，作者提出并验证了一种基于对抗训练的防御方法，以增强VLA模型对此类物理扰动的鲁棒性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>验证了VLA模型在物理传感器攻击面前的脆弱性，并证实攻击可导致其在真实世界中行为异常。</li><li>提出了一个“真实-模拟-真实”框架，以系统且现实的方式评估VLA模型对抗物理传感器攻击的鲁棒性，有效连接了纯数字攻击模拟与资源密集的物理实验之间的差距。</li><li>进行了大规模的鲁棒性评估，涵盖了多种VLA模型和任务。至关重要的是，通过在真实世界系统上进行的针对性物理实验，验证了模拟环境的发现，从而证实了该框架的有效性。</li><li>提出并验证了一种基于对抗训练的防御策略，用于抵御这些物理攻击，同时保持VLA模型在干净数据集上的性能。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>论文方法的核心是“真实-模拟-真实”框架。</p>\n<ol><li><strong>攻击选择与模拟</strong>：首先，从顶级安全会议中筛选出八种代表性的物理传感器攻击，其中六种针对摄像头（激光致盲、光投影、激光彩色条纹、电磁彩色条纹、电磁截断、超声波模糊），两种针对麦克风（语音拒绝服务、语音欺骗）。然后，基于这些攻击的物理原理，在数字领域实现高保真度的模拟，并定义了弱、中、强三个攻击强度等级。</li><li><strong>麦克风攻击设计</strong>：</li></ol>\n<p> - <strong>语音拒绝服务</strong>：通过注入高强度超声波信号使麦克风传感器饱和，从而阻止有效指令的接收。</p>\n<p> - <strong>语音欺骗</strong>：通过调制的激光或超声波信号向麦克风注入恶意的语音指令。</p>\n<ol><li><strong>摄像头攻击设计</strong>：</li></ol>\n<p> - <strong>激光致盲</strong>：使用高功率激光照射摄像头，使其光电传感器饱和，无法捕捉环境光线。</p>\n<p> - <strong>光投影</strong>：使用投影仪向环境或镜头投射虚假图像，干扰视觉感知。</p>\n<p> - <strong>激光彩色条纹</strong>：利用摄像头CMOS传感器的卷帘快门效应，通过开关调制激光注入彩色条纹。</p>\n<p> - <strong>电磁彩色条纹与截断</strong>：通过向摄像头图像传输接口（如MIPI CSI-2）注入电磁干扰，导致颜色解码错误或图像截断。</p>\n<p> - <strong>超声波模糊</strong>：向配备防抖模块的摄像头注入超声波，引起其惯性测量单元（IMU）共振，误导防抖算法进行不必要的补偿，导致图像模糊。</p>\n<ol><li><strong>防御方法</strong>：提出一种基于对抗训练的防御策略。该策略首先在干净数据集上训练VLA模型，然后在训练数据中混合一定比例（如30%）的经过攻击模拟的数据集，并对模型进行微调，使其对由传感器攻击引起的分布外扰动具有鲁棒性。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟环境：使用Libero模拟器及其配套的数据集，包括Libero-Spatial、Libero-Object、Libero-Goal和Libero-Long。</p>\n<p> - 真实世界环境：为适应真实世界，通过遥操作收集了一小时的机械臂操作数据，用于微调模型以完成积木抓取与放置任务。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 模型运行与评估：NVIDIA 4090 GPU。</p>\n<p> - 模型微调：使用Lora技术，在NVIDIA H800 GPU (80GB) 上进行。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：Libero模拟器。</p>\n<p> - 真实世界环境：一个装备有Franka Emika Panda机械臂的实验平台。该机械臂配备了一个全局摄像头（Intel RealSense D435i）、一个手腕摄像头（Intel RealSense D435i）和一个麦克风。攻击平台包括电磁干扰（EMI）平台、投影平台、激光平台和超声波平台。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率：定义为成功完成的任务次数与总任务次数的比率。</p>"
  },
  {
    "date": "2025-11-13",
    "title": "Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.09958",
    "summary_markdown": "## 论文研究单位\n华东师范大学, 复旦大学\n## 论文概述\n本文提出了Audio-VLA，一种将接触音频感知融入视觉-语言-行动模型的多模态机器人操控策略。传统VLA模型仅依赖视觉感知，在感知接触事件和动态操作过程方面存在根本性局限。Audio-VLA通过引入接触音频来克服这一限制，利用音频信号提供视觉无法捕捉的物理交互信息，如接触质量和材料属性。此外，本文还提出了一种新的评估指标——任务完成率，用于更系统地评估机器人在动态操作过程中的感知能力。实验在LIBERO、RLBench仿真环境以及两个真实世界任务中验证了Audio-VLA的优越性能。\n## 论文核心贡献点\n1. 提出了Audio-VLA模型，这是首个将接触音频感知集成到VLA框架中的多模态操控策略，旨在克服纯视觉模型的感知局限。\n2. 提出了任务完成率这一新的评估指标，用于量化机器人在操控过程中对动态过程的感知和理解能力，而不仅仅评估最终结果。\n3. 构建了音频增强的仿真环境，通过在LIBERO和RLBench中引入基于物理碰撞的音频生成，为模型训练和评估提供了更真实的声学反馈。\n## 论文方法描述\nAudio-VLA模型架构包含四个主要部分：\n1. 多模态编码器：使用预训练的DINOv2和SigLIP作为视觉编码器，处理第三人称和手腕相机图像；使用经过ManiWAV数据集微调的AudioCLIP作为音频编码器，处理高频率的接触音频信号；使用一个MLP层编码机器人本体感觉状态（如关节角度）。\n2. 多模态投影器：通过线性层和MLP将视觉、音频和本体感觉特征对齐到统一的文本特征空间，形成一个时序序列。\n3. 语言模块：采用7B参数的Llama2作为骨干网络，将多模态特征序列与语言指令结合，进行认知推理，并解码出包含动作信息的隐藏状态。\n4. 动作头：使用一个四层的MLP，根据解码出的隐藏状态生成连续的机器人动作序列。\n\n模型的训练目标是最小化预测动作序列与专家演示动作之间的平均L1损失。\n## 论文使用数据集和训练资源\n* **数据集**：\n * **仿真环境**：LIBERO官方数据集和RLBench任务数据集（每个任务100个演示）。\n * **真实世界**：为两个任务（擦白板和舀燕麦）通过遥操作方式收集了40个演示数据。\n * **音频预训练**：使用ManiWAV数据集对音频编码器进行额外的微调。\n* **训练资源**：\n * 使用2块NVIDIA H20 GPU进行训练。\n * LoRA秩设置为32，训练步数根据任务在5万到10万之间，批大小为8，学习率为1e-4并采用余弦退火策略。\n## 论文使用的评估环境和评估指标\n* **评估环境**：\n * **仿真环境**：在音频增强的LIBERO和RLBench环境中进行评估。评估分为两种条件：标准环境（与训练环境相同）和领域偏移环境（随机改变光照和桌面材质颜色）。\n * **真实世界环境**：在AgileX Mobile ALOHA平台上进行评估，该平台配备双7-DOF Piper机械臂、手腕相机和安装在夹爪上的压电接触麦克风。评估分为见过环境（与训练条件相同）和未见过环境（使用不同颜色的记号笔或不同特性的燕麦）。\n* **评估指标**：\n * **成功率**：标准的二元任务成功率指标。\n * **任务完成率**：本文提出的新指标，用于衡量任务的完成进度，计算公式为 `TCR = 已完成进度 / 任务目标`。它是一个连续值，能更细致地评估模型对动态过程的理解能力。例如，擦白板任务的TCR是擦除标记的面积，舀燕麦任务的TCR是舀取的燕麦重量。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>华东师范大学, 复旦大学</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文提出了Audio-VLA，一种将接触音频感知融入视觉-语言-行动模型的多模态机器人操控策略。传统VLA模型仅依赖视觉感知，在感知接触事件和动态操作过程方面存在根本性局限。Audio-VLA通过引入接触音频来克服这一限制，利用音频信号提供视觉无法捕捉的物理交互信息，如接触质量和材料属性。此外，本文还提出了一种新的评估指标——任务完成率，用于更系统地评估机器人在动态操作过程中的感知能力。实验在LIBERO、RLBench仿真环境以及两个真实世界任务中验证了Audio-VLA的优越性能。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出了Audio-VLA模型，这是首个将接触音频感知集成到VLA框架中的多模态操控策略，旨在克服纯视觉模型的感知局限。</li><li>提出了任务完成率这一新的评估指标，用于量化机器人在操控过程中对动态过程的感知和理解能力，而不仅仅评估最终结果。</li><li>构建了音频增强的仿真环境，通过在LIBERO和RLBench中引入基于物理碰撞的音频生成，为模型训练和评估提供了更真实的声学反馈。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>Audio-VLA模型架构包含四个主要部分：</p>\n<ol><li>多模态编码器：使用预训练的DINOv2和SigLIP作为视觉编码器，处理第三人称和手腕相机图像；使用经过ManiWAV数据集微调的AudioCLIP作为音频编码器，处理高频率的接触音频信号；使用一个MLP层编码机器人本体感觉状态（如关节角度）。</li><li>多模态投影器：通过线性层和MLP将视觉、音频和本体感觉特征对齐到统一的文本特征空间，形成一个时序序列。</li><li>语言模块：采用7B参数的Llama2作为骨干网络，将多模态特征序列与语言指令结合，进行认知推理，并解码出包含动作信息的隐藏状态。</li><li>动作头：使用一个四层的MLP，根据解码出的隐藏状态生成连续的机器人动作序列。</li></ol>\n\n<p>模型的训练目标是最小化预测动作序列与专家演示动作之间的平均L1损失。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> * <strong>仿真环境</strong>：LIBERO官方数据集和RLBench任务数据集（每个任务100个演示）。</p>\n<p> * <strong>真实世界</strong>：为两个任务（擦白板和舀燕麦）通过遥操作方式收集了40个演示数据。</p>\n<p> * <strong>音频预训练</strong>：使用ManiWAV数据集对音频编码器进行额外的微调。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> * 使用2块NVIDIA H20 GPU进行训练。</p>\n<p> * LoRA秩设置为32，训练步数根据任务在5万到10万之间，批大小为8，学习率为1e-4并采用余弦退火策略。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> * <strong>仿真环境</strong>：在音频增强的LIBERO和RLBench环境中进行评估。评估分为两种条件：标准环境（与训练环境相同）和领域偏移环境（随机改变光照和桌面材质颜色）。</p>\n<p> * <strong>真实世界环境</strong>：在AgileX Mobile ALOHA平台上进行评估，该平台配备双7-DOF Piper机械臂、手腕相机和安装在夹爪上的压电接触麦克风。评估分为见过环境（与训练条件相同）和未见过环境（使用不同颜色的记号笔或不同特性的燕麦）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> * <strong>成功率</strong>：标准的二元任务成功率指标。</p>\n<p> * <strong>任务完成率</strong>：本文提出的新指标，用于衡量任务的完成进度，计算公式为 <code>TCR = 已完成进度 / 任务目标</code>。它是一个连续值，能更细致地评估模型对动态过程的理解能力。例如，擦白板任务的TCR是擦除标记的面积，舀燕麦任务的TCR是舀取的燕麦重量。</p>"
  },
  {
    "date": "2025-11-12",
    "title": "MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.09516",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-12",
    "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.09515",
    "summary_markdown": "### 论文研究单位\n香港科技大学、字节跳动\n### 论文概述\n本文提出WMPO（World Model-based Policy Optimization），一个基于生成式世界模型的视觉-语言-动作（VLA）模型强化学习框架。该方法通过在像素级世界模型中进行策略优化，完全替代了昂贵且低效的真实环境交互，显著提升了VLA模型的样本效率和泛化能力，同时展现出自我修正等涌现行为。\n### 论文核心贡献点\n1. **像素级世界模型**：提出与VLA预训练特征对齐的像素空间世界模型，避免潜在空间不匹配问题。\n2. **策略行为对齐**：通过微调世界模型使其适应策略行为分布，实现对失败场景的逼真模拟。\n3. **高效在线强化学习**：支持GRPO算法在\"想象\"轨迹中进行策略优化，克服物理交互瓶颈。\n4. **涌现能力验证**：实验证明方法能产生自我修正行为、高效任务执行及强泛化能力。\n### 论文方法描述\n1. **世界模型构建**：基于OpenSora的视频扩散模型，替换3D VAE为SDXL的2D VAE以保留运动细节，引入带噪声帧条件增强长时程生成稳定性。\n2. **策略行为对齐**：在Open X-Embodiment数据集预训练后，用策略自身采集的轨迹微调世界模型。\n3. **奖励模型设计**：采用VideoMAE编码器训练二分类模型，通过滑动窗口评估轨迹成功概率。\n4. **GRPO优化**：每组初始状态生成G条想象轨迹，通过动态采样确保批次多样性，利用归一化优势函数更新策略。\n### 论文使用数据集和训练资源\n- **数据集**：Open X-Embodiment（预训练）、Mimicgen仿真任务（Coffee、StackThree等）、Cobot Mobile ALOHA真实轨迹（200条专家演示+128条策略采集）。\n- **训练资源**：8块H100 GPU进行策略微调，32块H100 GPU训练世界模型和优化策略。\n### 论文使用的评估环境和评估指标\n- **仿真环境**：Mimicgen基准的4个操作任务，评估128个随机初始状态的平均成功率。\n- **真实环境**：Cobot Mobile ALOHA平台执行\"插入方块\"任务（间隙5mm），30次试验平均成功率。\n- **泛化测试**：位置扰动（棒随机位置）、背景扰动（灰色背景）、纹理扰动（木质基座）下的成功率。\n- **终身学习**：迭代采集128条轨迹更新策略，对比DPO基线的性能提升。\n- **指标**：任务成功率（%）、轨迹长度（效率）、F1分数（奖励模型可靠性）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>香港科技大学、字节跳动</p>\n<h3>论文概述</h3>\n<p>本文提出WMPO（World Model-based Policy Optimization），一个基于生成式世界模型的视觉-语言-动作（VLA）模型强化学习框架。该方法通过在像素级世界模型中进行策略优化，完全替代了昂贵且低效的真实环境交互，显著提升了VLA模型的样本效率和泛化能力，同时展现出自我修正等涌现行为。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>像素级世界模型</strong>：提出与VLA预训练特征对齐的像素空间世界模型，避免潜在空间不匹配问题。</li><li><strong>策略行为对齐</strong>：通过微调世界模型使其适应策略行为分布，实现对失败场景的逼真模拟。</li><li><strong>高效在线强化学习</strong>：支持GRPO算法在\"想象\"轨迹中进行策略优化，克服物理交互瓶颈。</li><li><strong>涌现能力验证</strong>：实验证明方法能产生自我修正行为、高效任务执行及强泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>世界模型构建</strong>：基于OpenSora的视频扩散模型，替换3D VAE为SDXL的2D VAE以保留运动细节，引入带噪声帧条件增强长时程生成稳定性。</li><li><strong>策略行为对齐</strong>：在Open X-Embodiment数据集预训练后，用策略自身采集的轨迹微调世界模型。</li><li><strong>奖励模型设计</strong>：采用VideoMAE编码器训练二分类模型，通过滑动窗口评估轨迹成功概率。</li><li><strong>GRPO优化</strong>：每组初始状态生成G条想象轨迹，通过动态采样确保批次多样性，利用归一化优势函数更新策略。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Open X-Embodiment（预训练）、Mimicgen仿真任务（Coffee、StackThree等）、Cobot Mobile ALOHA真实轨迹（200条专家演示+128条策略采集）。</li><li><strong>训练资源</strong>：8块H100 GPU进行策略微调，32块H100 GPU训练世界模型和优化策略。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>仿真环境</strong>：Mimicgen基准的4个操作任务，评估128个随机初始状态的平均成功率。</li><li><strong>真实环境</strong>：Cobot Mobile ALOHA平台执行\"插入方块\"任务（间隙5mm），30次试验平均成功率。</li><li><strong>泛化测试</strong>：位置扰动（棒随机位置）、背景扰动（灰色背景）、纹理扰动（木质基座）下的成功率。</li><li><strong>终身学习</strong>：迭代采集128条轨迹更新策略，对比DPO基线的性能提升。</li><li><strong>指标</strong>：任务成功率（%）、轨迹长度（效率）、F1分数（奖励模型可靠性）。</li></ul>"
  },
  {
    "date": "2025-11-12",
    "title": "MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror",
    "link": "http://arxiv.org/abs/2511.08865",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-11",
    "title": "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control",
    "link": "http://arxiv.org/abs/2511.07820",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-10",
    "title": "How Do VLAs Effectively Inherit from VLMs?",
    "link": "http://arxiv.org/abs/2511.06619",
    "summary_markdown": "### 论文研究单位\nMicrosoft Research\n### 论文概述\n该论文探讨了视觉-语言-行动模型如何有效地从预训练的视觉-语言模型中继承知识。为了解决这一核心问题并应对训练过程中可能出现的灾难性遗忘，研究者们引入了一个名为GrinningFace的诊断基准。该基准是一个表情符号桌面操作任务，要求机器人根据语言指令将物体放置在对应的印刷表情符号上。由于表情符号在VLM的预训练数据中广泛存在，但在标准机器人数据集中几乎不存在，该任务的成功完成可以清晰地表明VLM的先验知识被有效迁移到了具身控制中。论文通过在模拟和真实机器人上进行系统性实验，比较了多种有前景的知识迁移技术，揭示了保留VLM先验知识对于VLA泛化能力的关键重要性，并为未来开发真正可泛化的具身智能系统提供了指导。\n### 论文核心贡献点\n- 提出了一个最小化、可复现的基准，用于解构视觉-语义先验知识与运动技能，可作为诊断工具来评估VLM如何被有效地适配到VLA。\n- 在一个严格控制实验的框架内，对不同VLA预训练和微调技术进行了系统的比较分析。\n- 提供了基于实证的洞察，为未来开发可泛化具身智能体的研究方向提供指导，包括：VLM初始化、VLA预训练和VLA微调分别扮演着不同且互补的角色；全参数微调在狭窄任务上表现良好，但会导致灾难性遗忘；联合训练和预测潜在动作是更有前景的研究方向；在更多样化的数据集上进行VLA预训练可以提升性能。\n### 论文方法描述\n论文设计并比较了多种旨在改善VLA继承VLM先验知识的训练技术。核心方法是构建并使用GrinningFace基准进行评估。被比较的技术包括：\n- 基线方法：使用π0风格的模型进行全参数微调。\n- 参数高效微调：采用LoRA或仅微调动作专家模块。\n- 冻结VLM主干：在微调过程中冻结VLM的权重。\n- 联合训练：在VLA训练中同时加入视觉-语言任务，如在桌面场景中识别表情符号。\n- 离散化目标：训练VLA预测离散化的动作目标。\n- 潜在动作目标：训练VLA在预测机器人动作的同时，也预测潜在动作。\n- 多样化数据预训练：研究不同数据集（如OXE magic soup, bridge-v2）对VLA性能的影响。\n### 论文使用数据集和训练资源\n- 数据集：\n - GrinningFace基准：包含100个用于训练的表情符号和100个用于验证的表情符号，收集了500条轨迹用于微调。\n - VLA预训练数据集：主要使用Open X-Embodiment (OXE) magic-soup混合数据集（包含913k条轨迹）。实验中还使用了bridge-v2数据集以及排除了bridge-v2的OXE数据集，以进行消融研究。\n- 训练资源：\n - 计算资源：使用8块Nvidia A100 GPU进行VLA的预训练和微调，使用单块Nvidia A100 GPU进行评估。\n - 模型基础：代码库基于π0的开源实现，使用PaliGemma作为VLM主干，SigLIP作为视觉编码器。\n - 训练配置：VLA预训练80k步，微调30k步，批大小为1024。\n### 论文使用的评估环境和评估指标\n- 评估环境：\n - 模拟环境：ManiSkill3。\n - 真实机器人：搭载Inspire Robots夹爪的Realman RM75机械臂。真实机器人上的任务被简化为“触摸”指定表情符号。\n- 评估指标：\n - 核心公式：整体成功率 = 执行成功率 × 识别成功率。\n - 执行成功率：机器人成功拿起方块并放置到任意一个表情符号卡片上。\n - 识别成功率：机器人在三个候选卡片中选择了正确的那个。\n - 评估协议：\n - ID：评估模型在训练时见过的表情符号组合上的表现。\n - Train：评估模型在由训练集表情符号构成的新组合上的表现。\n - Val：评估模型在验证集表情符号上的表现，用于测试泛化能力。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Microsoft Research</p>\n<h3>论文概述</h3>\n<p>该论文探讨了视觉-语言-行动模型如何有效地从预训练的视觉-语言模型中继承知识。为了解决这一核心问题并应对训练过程中可能出现的灾难性遗忘，研究者们引入了一个名为GrinningFace的诊断基准。该基准是一个表情符号桌面操作任务，要求机器人根据语言指令将物体放置在对应的印刷表情符号上。由于表情符号在VLM的预训练数据中广泛存在，但在标准机器人数据集中几乎不存在，该任务的成功完成可以清晰地表明VLM的先验知识被有效迁移到了具身控制中。论文通过在模拟和真实机器人上进行系统性实验，比较了多种有前景的知识迁移技术，揭示了保留VLM先验知识对于VLA泛化能力的关键重要性，并为未来开发真正可泛化的具身智能系统提供了指导。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个最小化、可复现的基准，用于解构视觉-语义先验知识与运动技能，可作为诊断工具来评估VLM如何被有效地适配到VLA。</li><li>在一个严格控制实验的框架内，对不同VLA预训练和微调技术进行了系统的比较分析。</li><li>提供了基于实证的洞察，为未来开发可泛化具身智能体的研究方向提供指导，包括：VLM初始化、VLA预训练和VLA微调分别扮演着不同且互补的角色；全参数微调在狭窄任务上表现良好，但会导致灾难性遗忘；联合训练和预测潜在动作是更有前景的研究方向；在更多样化的数据集上进行VLA预训练可以提升性能。</li></ul>\n<h3>论文方法描述</h3>\n<p>论文设计并比较了多种旨在改善VLA继承VLM先验知识的训练技术。核心方法是构建并使用GrinningFace基准进行评估。被比较的技术包括：</p>\n<ul><li>基线方法：使用π0风格的模型进行全参数微调。</li><li>参数高效微调：采用LoRA或仅微调动作专家模块。</li><li>冻结VLM主干：在微调过程中冻结VLM的权重。</li><li>联合训练：在VLA训练中同时加入视觉-语言任务，如在桌面场景中识别表情符号。</li><li>离散化目标：训练VLA预测离散化的动作目标。</li><li>潜在动作目标：训练VLA在预测机器人动作的同时，也预测潜在动作。</li><li>多样化数据预训练：研究不同数据集（如OXE magic soup, bridge-v2）对VLA性能的影响。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：</li></ul>\n<p> - GrinningFace基准：包含100个用于训练的表情符号和100个用于验证的表情符号，收集了500条轨迹用于微调。</p>\n<p> - VLA预训练数据集：主要使用Open X-Embodiment (OXE) magic-soup混合数据集（包含913k条轨迹）。实验中还使用了bridge-v2数据集以及排除了bridge-v2的OXE数据集，以进行消融研究。</p>\n<ul><li>训练资源：</li></ul>\n<p> - 计算资源：使用8块Nvidia A100 GPU进行VLA的预训练和微调，使用单块Nvidia A100 GPU进行评估。</p>\n<p> - 模型基础：代码库基于π0的开源实现，使用PaliGemma作为VLM主干，SigLIP作为视觉编码器。</p>\n<p> - 训练配置：VLA预训练80k步，微调30k步，批大小为1024。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：</li></ul>\n<p> - 模拟环境：ManiSkill3。</p>\n<p> - 真实机器人：搭载Inspire Robots夹爪的Realman RM75机械臂。真实机器人上的任务被简化为“触摸”指定表情符号。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 核心公式：整体成功率 = 执行成功率 × 识别成功率。</p>\n<p> - 执行成功率：机器人成功拿起方块并放置到任意一个表情符号卡片上。</p>\n<p> - 识别成功率：机器人在三个候选卡片中选择了正确的那个。</p>\n<p> - 评估协议：</p>\n<p> - ID：评估模型在训练时见过的表情符号组合上的表现。</p>\n<p> - Train：评估模型在由训练集表情符号构成的新组合上的表现。</p>\n<p> - Val：评估模型在验证集表情符号上的表现，用于测试泛化能力。</p>"
  },
  {
    "date": "2025-11-09",
    "title": "ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval",
    "link": "http://arxiv.org/abs/2511.06202",
    "summary_markdown": "### 论文研究单位\n卡内基梅隆大学机器人研究所，美国匹兹堡\n### 论文概述\n论文提出了一种名为ExpReS-VLA的方法，旨在解决预训练的视觉-语言-动作（VLA）模型在特定部署环境中进行快速适应时遇到的灾难性遗忘和性能下降问题。该方法通过压缩的经验回放和检索增强生成技术，使模型能够在设备端利用少量演示（12个）快速适应新环境，同时保留原有能力。ExpReS-VLA通过存储视觉编码器的嵌入而非原始图像，实现了97%的存储空间节省，并结合检索到的相似经验和一种新的对比损失函数（THCL）来学习成功与失败的经验。实验表明，该方法在模拟和真实机器人任务上均显著提升了任务成功率。\n### 论文核心贡献点\nRAG增强的机器人学习：首次将检索机制集成到VLA微调中，加速了适应过程。\n压缩的经验回放：一种通过冻结视觉编码器实现97%内存减少的技术，在保持语义保真度的同时，实现了实际部署。\n用于失败利用的THCL：一种新颖的分段损失函数，通过动态选择合适的对比目标来防止重复性错误。\n严谨的实证评估：在40个模拟任务（5个随机种子）和5个物理操作任务（共150次试验）中进行了系统性消融实验，明确了各组件的贡献。\n### 论文方法描述\n该方法首先使用OpenVLA冻结的视觉编码器（融合SigLIP和DINOv2）从RGB图像中提取1024维的嵌入向量，以实现高效的内存存储。\n维护一个双缓冲区内存管理系统，分别存储成功和失败的经验轨迹，并采用FIFO策略和时序加权进行更新。\n在训练时，系统根据当前观察的嵌入，从缓冲区中检索余弦相似度最高的k个相关经验，并将其与当前数据混合构建训练批次。\n引入了阈值化混合对比损失（THCL），该损失结合了行为克隆损失和自适应的对比学习损失。当三元组损失低于阈值时使用三元组损失，否则使用InfoNCE损失，从而使模型能从成功和失败中学习。\n整个在线学习流程采用LoRA进行高效微调，在性能低于阈值时触发，并在单块消费级GPU（如RTX 5090）上快速完成。\n### 论文使用数据集和训练资源\n数据集：模拟实验使用了LIBERO基准测试，包含四个任务套件（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。真实机器人实验在7-DOF Franka Emika Panda机械臂上进行，包含5个操作任务。\n训练资源：所有实验均在单个NVIDIA RTX 5090 GPU（32GB内存）上完成，使用BFloat16混合精度。模型微调采用LoRA配置，仅训练98.3M个参数（占总数的1.4%）。适应过程仅需12个演示和31秒。\n### 论文使用的评估环境和评估指标\n评估环境：在LIBERO模拟基准和真实的Franka Emika Panda物理机器人上进行评估。物理机器人任务包括分布内和分布外（包含未见过的背景和物体）两种测试场景。\n评估指标：主要评估指标是任务成功率。模拟中，每个任务进行50次滚动测试，重复5个随机种子。物理机器人中，每个任务进行30次分布内试验和10次分布外试验。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>卡内基梅隆大学机器人研究所，美国匹兹堡</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为ExpReS-VLA的方法，旨在解决预训练的视觉-语言-动作（VLA）模型在特定部署环境中进行快速适应时遇到的灾难性遗忘和性能下降问题。该方法通过压缩的经验回放和检索增强生成技术，使模型能够在设备端利用少量演示（12个）快速适应新环境，同时保留原有能力。ExpReS-VLA通过存储视觉编码器的嵌入而非原始图像，实现了97%的存储空间节省，并结合检索到的相似经验和一种新的对比损失函数（THCL）来学习成功与失败的经验。实验表明，该方法在模拟和真实机器人任务上均显著提升了任务成功率。</p>\n<h3>论文核心贡献点</h3>\n<p>RAG增强的机器人学习：首次将检索机制集成到VLA微调中，加速了适应过程。</p>\n<p>压缩的经验回放：一种通过冻结视觉编码器实现97%内存减少的技术，在保持语义保真度的同时，实现了实际部署。</p>\n<p>用于失败利用的THCL：一种新颖的分段损失函数，通过动态选择合适的对比目标来防止重复性错误。</p>\n<p>严谨的实证评估：在40个模拟任务（5个随机种子）和5个物理操作任务（共150次试验）中进行了系统性消融实验，明确了各组件的贡献。</p>\n<h3>论文方法描述</h3>\n<p>该方法首先使用OpenVLA冻结的视觉编码器（融合SigLIP和DINOv2）从RGB图像中提取1024维的嵌入向量，以实现高效的内存存储。</p>\n<p>维护一个双缓冲区内存管理系统，分别存储成功和失败的经验轨迹，并采用FIFO策略和时序加权进行更新。</p>\n<p>在训练时，系统根据当前观察的嵌入，从缓冲区中检索余弦相似度最高的k个相关经验，并将其与当前数据混合构建训练批次。</p>\n<p>引入了阈值化混合对比损失（THCL），该损失结合了行为克隆损失和自适应的对比学习损失。当三元组损失低于阈值时使用三元组损失，否则使用InfoNCE损失，从而使模型能从成功和失败中学习。</p>\n<p>整个在线学习流程采用LoRA进行高效微调，在性能低于阈值时触发，并在单块消费级GPU（如RTX 5090）上快速完成。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：模拟实验使用了LIBERO基准测试，包含四个任务套件（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。真实机器人实验在7-DOF Franka Emika Panda机械臂上进行，包含5个操作任务。</p>\n<p>训练资源：所有实验均在单个NVIDIA RTX 5090 GPU（32GB内存）上完成，使用BFloat16混合精度。模型微调采用LoRA配置，仅训练98.3M个参数（占总数的1.4%）。适应过程仅需12个演示和31秒。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：在LIBERO模拟基准和真实的Franka Emika Panda物理机器人上进行评估。物理机器人任务包括分布内和分布外（包含未见过的背景和物体）两种测试场景。</p>\n<p>评估指标：主要评估指标是任务成功率。模拟中，每个任务进行50次滚动测试，重复5个随机种子。物理机器人中，每个任务进行30次分布内试验和10次分布外试验。</p>"
  },
  {
    "date": "2025-11-08",
    "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.05936",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-07",
    "title": "Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots",
    "link": "http://arxiv.org/abs/2511.05642",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-07",
    "title": "EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.05397",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-07",
    "title": "TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.05275",
    "summary_markdown": "### 论文研究单位\n延世大学人工智能系（Yonsei University, Department of Artificial Intelligence），微软研究院（Microsoft Research）。\n### 论文概述\nVision-Language-Action模型（VLAs）在单臂机器人操控中表现出色，但双臂操控因缺乏大规模公开数据而面临挑战。本研究提出TwinVLA，通过组合两个预训练单臂VLA模型实现数据高效的双臂操控。TwinVLA避免了大量双臂预训练数据的依赖，仅需少量双臂演示即可在真实世界和仿真任务中达到或超越单体模型（如RDT-1B）的性能，并接近状态最先进的π_0模型。\n### 论文核心贡献点\n- 提出模块化双臂架构：通过复制预训练单臂VLA并结合联合注意力机制，实现两臂协调控制。\n- 数据高效微调范式：仅使用小量双臂数据（每任务约50演示）进行微调，无需额外双臂预训练。\n- 性能验证：在真实世界（Anubis机器人）和仿真（RoboTwin 2.0、Tabletop-Sim）任务中，TwinVLA在成功率上优于或匹配RDT-1B和DP方法，接近π_0模型。\n### 论文方法描述\nTwinVLA基于以下三大核心组件：\n1. **单臂策略复制**：复制预训练单臂VLA的VLM骨干（仅复制1.3B参数），共享视觉编码器和DiT动作头。左右臂各有一个轻量级本体感觉编码器。\n2. **联合注意力机制**：共享两个VLM间的自注意力层，实现跨臂信息融合，采用因果联合注意力掩码保持时序性和对称交互。\n3. **混合专家（MoE）集成**：通过MoE高效处理共享输入（如语言和自我视角图像），减少计算开销。同时，采用任务算术和注意力重加权技术保留预训练知识并加速微调。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：OXE数据集的0.5M轨迹子集。\n - 微调：真实世界任务每任务收集50集（绝对末端执行器控制），仿真任务每任务50集（如RoboTwin 2.0和Tabletop-Sim）。\n- **训练资源**：\n - SingleVLA预训练：5×H100 GPUs，耗时5天，120k步。\n - TwinVLA微调：1×L40S GPU，耗时2天，100k步。\n - 计算总量：约25 H100 GPU天（RDT-1B需超1,000 H100 GPU天）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **真实世界**：使用Anubis双臂机器人，任务包括“carrot to bag”（胡萝卜入袋）、“brush to dustpan”（刷子入簸箕）、“take towel off”（取下毛巾）。\n - **仿真**：\n - RoboTwin 2.0：50个双臂任务（Easy和Hard设置）。\n - Tabletop-Sim：5个任务（如“dish-drainer”（碗碟架）、“handover-box”（传递盒子））。\n- **评估指标**：\n - 主要指标为任务成功率（Success Rate），在每任务20-500次 rollout 中计算平均成功率。\n - 语言跟随测试：多任务组合指令（如“put X box into Y pot”）下的平均成功率。\n - 数据效率评估：随演示数量（20、35、50）变化的成功率曲线。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>延世大学人工智能系（Yonsei University, Department of Artificial Intelligence），微软研究院（Microsoft Research）。</p>\n<h3>论文概述</h3>\n<p>Vision-Language-Action模型（VLAs）在单臂机器人操控中表现出色，但双臂操控因缺乏大规模公开数据而面临挑战。本研究提出TwinVLA，通过组合两个预训练单臂VLA模型实现数据高效的双臂操控。TwinVLA避免了大量双臂预训练数据的依赖，仅需少量双臂演示即可在真实世界和仿真任务中达到或超越单体模型（如RDT-1B）的性能，并接近状态最先进的π_0模型。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出模块化双臂架构：通过复制预训练单臂VLA并结合联合注意力机制，实现两臂协调控制。</li><li>数据高效微调范式：仅使用小量双臂数据（每任务约50演示）进行微调，无需额外双臂预训练。</li><li>性能验证：在真实世界（Anubis机器人）和仿真（RoboTwin 2.0、Tabletop-Sim）任务中，TwinVLA在成功率上优于或匹配RDT-1B和DP方法，接近π_0模型。</li></ul>\n<h3>论文方法描述</h3>\n<p>TwinVLA基于以下三大核心组件：</p>\n<ol><li><strong>单臂策略复制</strong>：复制预训练单臂VLA的VLM骨干（仅复制1.3B参数），共享视觉编码器和DiT动作头。左右臂各有一个轻量级本体感觉编码器。</li><li><strong>联合注意力机制</strong>：共享两个VLM间的自注意力层，实现跨臂信息融合，采用因果联合注意力掩码保持时序性和对称交互。</li><li><strong>混合专家（MoE）集成</strong>：通过MoE高效处理共享输入（如语言和自我视角图像），减少计算开销。同时，采用任务算术和注意力重加权技术保留预训练知识并加速微调。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：OXE数据集的0.5M轨迹子集。</p>\n<p> - 微调：真实世界任务每任务收集50集（绝对末端执行器控制），仿真任务每任务50集（如RoboTwin 2.0和Tabletop-Sim）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - SingleVLA预训练：5×H100 GPUs，耗时5天，120k步。</p>\n<p> - TwinVLA微调：1×L40S GPU，耗时2天，100k步。</p>\n<p> - 计算总量：约25 H100 GPU天（RDT-1B需超1,000 H100 GPU天）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>真实世界</strong>：使用Anubis双臂机器人，任务包括“carrot to bag”（胡萝卜入袋）、“brush to dustpan”（刷子入簸箕）、“take towel off”（取下毛巾）。</p>\n<p> - <strong>仿真</strong>：</p>\n<p> - RoboTwin 2.0：50个双臂任务（Easy和Hard设置）。</p>\n<p> - Tabletop-Sim：5个任务（如“dish-drainer”（碗碟架）、“handover-box”（传递盒子））。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标为任务成功率（Success Rate），在每任务20-500次 rollout 中计算平均成功率。</p>\n<p> - 语言跟随测试：多任务组合指令（如“put X box into Y pot”）下的平均成功率。</p>\n<p> - 数据效率评估：随演示数量（20、35、50）变化的成功率曲线。</p>"
  },
  {
    "date": "2025-11-06",
    "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment",
    "link": "http://arxiv.org/abs/2511.04555",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-06",
    "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies",
    "link": "http://arxiv.org/abs/2511.04357",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-04",
    "title": "XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations",
    "link": "http://arxiv.org/abs/2511.02776",
    "summary_markdown": "## 论文研究单位\n北京具身智能机器人创新中心（牵头单位），联合北京航空航天大学（机械工程与自动化学院、虚拟现实技术与系统国家重点实验室）、北京大学（计算机学院多媒体信息处理国家重点实验室）共同完成。\n## 论文概述\n针对视觉语言行动（VLA）模型在低层动作精确控制和跨机器人形态数据整合方面的挑战，论文提出 **XR-1** 框架：通过引入 **统一视觉运动代码（UVMC）** 作为离散潜在表征，结合三阶段训练范式，实现多模态对齐与跨身体控制的视觉语言行动模型。该模型可同时利用人类视频和机器人数据，显著提升多任务泛化能力和实际部署效果。\n## 论文核心贡献点\n1. **UVMC 机制**\n 提出统一视觉运动代码（UVMC），通过双分支 VQ-VAE 将视觉动态和机器人运动编码至共享离散潜在空间，并通过 KL 散度约束实现跨模态对齐。\n2. **三阶段训练框架**\n 包含自监督 UVMC 学习、跨身体预训练和任务特定微调，模型无关于特定 VLA 架构，可适配多种基础模型（如 π₀、SwitchVLA）。\n3. **大规模实证验证**\n 在 6 种机器人平台、120+ 任务中进行超过 14,000 次实机测试，平均成功率显著超越 π₀.₅、π₀、RDT、UniVLA 等 SOTA 基线。\n## 论文方法描述\n### 核心架构\n- **UVMC 学习（Stage-1）**\n 通过双分支 VQ-VAE 分别编码视觉动态（未来帧预测）和机器人运动（动作序列重构），共享码本实现模态统一，引入视觉-运动 KL 散度损失促进对齐。\n- **跨身体预训练（Stage-2）**\n 使用 UVMC 作为监督信号，通过可学习 token 将其注入 VLM 主干，结合动作预测损失联合训练策略网络。\n- **任务特定微调（Stage-3）**\n 基于目标机器人任务数据微调模型，提升特定场景性能。\n### 训练流程\n1. 大规模数据预训练 UVMC：利用 Open-X、RoboMIND、Ego4D、XR-D 四类数据（人类视频+机器人数据），加权采样平衡不同来源。\n2. UVMC 引导策略预训练：在 XR-D 上进行跨身体动作学习。\n3. 任务特定微调：针对下游任务数据进行小样本适配。\n## 论文使用数据集和训练资源\n- **数据集组成**：\n - Open-X（978k episodes, 59.3M frames, 权重 40%）\n - RoboMIND（69k episodes, 21.4M frames, 权重 15%）\n - XR-D（158k episodes, 69.1M frames, 权重 35%）\n - Ego4D（59k episodes, 14.3M frames, 权重 10%）\n- **训练资源**：\n - 主模型 XR-1：基于 PaliGemma（SigLIP+Gemma）架构\n - 轻量模型 XR-1-Light：基于 SwitchVLA（Florence-2）架构\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 6 种真实机器人平台：Tien Kung 1.0/2.0、单/双臂 UR-5e、双臂 Franka、AgileX Cobot Magic 2.0\n - 覆盖 120+ 复杂任务：双臂协作、灵巧操作、可变形体处理、接触密集任务、动态环境等\n- **评估指标**：\n - **成功率（Success Rate）**：每任务 20 次测试的平均完成率\n - **跨身体泛化**：在未见过机器人平台（如 Tien Kung 2.0）上验证迁移能力\n - **鲁棒性测试**：新增物体、背景干扰、光照变化下的性能保持率\n\n**效果摘要**：\nXR-1 在所有平台平均性能显著优于 π₀.₅、π₀ 等基线（如 Tien Kung 2.0 任务平均 72% vs 41%），并在未知场景（新增干扰物等）中展现更强鲁棒性。\n---\n论文核心创新在于通过 UVMC 机制统一视觉与动作模态，结合三阶段训练打通人类演示与机器人数据，实现跨身体的通用技能学习。实验充分验证了其在复杂操作中的有效性。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>北京具身智能机器人创新中心（牵头单位），联合北京航空航天大学（机械工程与自动化学院、虚拟现实技术与系统国家重点实验室）、北京大学（计算机学院多媒体信息处理国家重点实验室）共同完成。</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>针对视觉语言行动（VLA）模型在低层动作精确控制和跨机器人形态数据整合方面的挑战，论文提出 <strong>XR-1</strong> 框架：通过引入 <strong>统一视觉运动代码（UVMC）</strong> 作为离散潜在表征，结合三阶段训练范式，实现多模态对齐与跨身体控制的视觉语言行动模型。该模型可同时利用人类视频和机器人数据，显著提升多任务泛化能力和实际部署效果。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>UVMC 机制</strong></li></ol>\n<p> 提出统一视觉运动代码（UVMC），通过双分支 VQ-VAE 将视觉动态和机器人运动编码至共享离散潜在空间，并通过 KL 散度约束实现跨模态对齐。</p>\n<ol><li><strong>三阶段训练框架</strong></li></ol>\n<p> 包含自监督 UVMC 学习、跨身体预训练和任务特定微调，模型无关于特定 VLA 架构，可适配多种基础模型（如 π₀、SwitchVLA）。</p>\n<ol><li><strong>大规模实证验证</strong></li></ol>\n<p> 在 6 种机器人平台、120+ 任务中进行超过 14,000 次实机测试，平均成功率显著超越 π₀.₅、π₀、RDT、UniVLA 等 SOTA 基线。</p>\n<h2 class=\"section-title\">论文方法描述</h2>\n<h3>核心架构</h3>\n<ul><li><strong>UVMC 学习（Stage-1）</strong></li></ul>\n<p> 通过双分支 VQ-VAE 分别编码视觉动态（未来帧预测）和机器人运动（动作序列重构），共享码本实现模态统一，引入视觉-运动 KL 散度损失促进对齐。</p>\n<ul><li><strong>跨身体预训练（Stage-2）</strong></li></ul>\n<p> 使用 UVMC 作为监督信号，通过可学习 token 将其注入 VLM 主干，结合动作预测损失联合训练策略网络。</p>\n<ul><li><strong>任务特定微调（Stage-3）</strong></li></ul>\n<p> 基于目标机器人任务数据微调模型，提升特定场景性能。</p>\n<h3>训练流程</h3>\n<ol><li>大规模数据预训练 UVMC：利用 Open-X、RoboMIND、Ego4D、XR-D 四类数据（人类视频+机器人数据），加权采样平衡不同来源。</li><li>UVMC 引导策略预训练：在 XR-D 上进行跨身体动作学习。</li><li>任务特定微调：针对下游任务数据进行小样本适配。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集组成</strong>：</li></ul>\n<p> - Open-X（978k episodes, 59.3M frames, 权重 40%）</p>\n<p> - RoboMIND（69k episodes, 21.4M frames, 权重 15%）</p>\n<p> - XR-D（158k episodes, 69.1M frames, 权重 35%）</p>\n<p> - Ego4D（59k episodes, 14.3M frames, 权重 10%）</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 主模型 XR-1：基于 PaliGemma（SigLIP+Gemma）架构</p>\n<p> - 轻量模型 XR-1-Light：基于 SwitchVLA（Florence-2）架构</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 6 种真实机器人平台：Tien Kung 1.0/2.0、单/双臂 UR-5e、双臂 Franka、AgileX Cobot Magic 2.0</p>\n<p> - 覆盖 120+ 复杂任务：双臂协作、灵巧操作、可变形体处理、接触密集任务、动态环境等</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（Success Rate）</strong>：每任务 20 次测试的平均完成率</p>\n<p> - <strong>跨身体泛化</strong>：在未见过机器人平台（如 Tien Kung 2.0）上验证迁移能力</p>\n<p> - <strong>鲁棒性测试</strong>：新增物体、背景干扰、光照变化下的性能保持率</p>\n\n<p><strong>效果摘要</strong>：</p>\n<p>XR-1 在所有平台平均性能显著优于 π₀.₅、π₀ 等基线（如 Tien Kung 2.0 任务平均 72% vs 41%），并在未知场景（新增干扰物等）中展现更强鲁棒性。</p>\n<hr/>\n<p>论文核心创新在于通过 UVMC 机制统一视觉与动作模态，结合三阶段训练打通人类演示与机器人数据，实现跨身体的通用技能学习。实验充分验证了其在复杂操作中的有效性。</p>"
  },
  {
    "date": "2025-11-01",
    "title": "iFlyBot-VLA Technical Report",
    "link": "http://arxiv.org/abs/2511.01914",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-03",
    "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process",
    "link": "http://arxiv.org/abs/2511.01718",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-03",
    "title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2511.01571",
    "summary_markdown": "### 论文研究单位\n- 华南理工大学自动化科学与工程学院\n- 中国科学院沈阳自动化研究所\n- 穆罕默德·本·扎耶德人工智能大学（Mohamed bin Zayed University of Artificial Intelligence）\n- 澳大利亚国立大学\n### 论文概述\nPixelVLA 是首个支持像素级理解和多模态提示（文本和视觉输入）的视觉-语言-动作（VLA）模型。现有 VLAs 局限于图像级理解且依赖文本提示，导致空间推理和跨域泛化能力不足。PixelVLA 通过集成多尺度像素感知编码器、视觉提示编码器和连续动作解码器，解决这些挑战。使用两阶段自动注释管道生成的 Pixel-160K 数据集进行训练，该数据集包含 160K 操作片段和 6.5M 图像-文本-动作三元组。PixelVLA 在三个标准 VLA 基准测试（SimplerEnv、Google Robot、LIBERO）上显著优于 OpenVLA，成功率提升 10.1%~28.7%，且训练成本仅为 OpenVLA 的 1.5%。\n### 论文核心贡献点\n1. **PixelVLA 架构设计**：提出支持像素级理解和多模态提示的 VLA 模型，包括多尺度像素感知编码器、视觉提示编码器（处理点、线、区域等提示）和连续动作解码器（直接预测 7D 动作）。\n2. **两阶段自动注释管道**：开发用于合成 Pixel-160K 数据集的管道，第一阶段定位夹爪并生成区域提议，第二阶段使用 LLM 和开放词汇分割模型生成像素级注释和视觉提示。\n3. **视觉运动指令调优框架**：引入两阶段训练流程——连续动作训练阶段和像素级理解增强阶段，有效提升像素级空间理解能力。\n4. **实证验证**：在多个基准测试上集成 PixelVLA 架构到 OpenVLA 和 π₀ 模型，验证性能提升和泛化能力。\n### 论文方法描述\n- **PixelVLA 架构**：基于 Prismatic-7B VLM 和 Llama 2-7B 的主干，结合视觉编码器（DinoV2 和 SigLIP）、MLP 投影仪、视觉提示编码器（源自 SAM）、多尺度像素感知编码器（生成像素感知嵌入）和连续动作解码器（ResNet 块 + MLP，预测连续动作序列）。\n- **多尺度像素感知编码器**：从多尺度视觉特征中提取像素级信息，公式基于特征和像素掩码的 MLP 组合。\n- **视觉提示编码器**：处理多样化视觉提示（如点、线、区域），生成提示感知嵌入。\n- **连续动作解码器**：基于 LLM 隐藏状态，通过线性投影、ResNet 块和 MLP 输出连续动作值（chunk 大小为 8）。\n- **训练流程**：两阶段视觉运动指令调优。\n - **阶段一（连续动作训练）**：使用 Fractal 和 Bridge v2 数据训练连续动作解码器，冻结其他模块。\n - **阶段二（像素级理解增强）**：使用 Pixel-160K 数据集，通过 LoRA 微调 LLM 主干，联合训练视觉提示和像素感知编码器。\n### 论文使用数据集和训练资源\n- **数据集**：Pixel-160K（160K 操作片段，6.5M 三元组），基于 Fractal 和 Bridge v2 数据集构建；LIBERO-Pixel（使用管道处理 LIBERO 基准）。\n- **训练资源**：\n - 两阶段训练：阶段一 100k 步（批量 32，学习率 5e-4），阶段二 200k 步（批量 32，学习率 1e-3，LoRA r=32）。\n - 总训练成本仅为 OpenVLA 预训练的 1.5%。\n - 输入分辨率 224×224，单视角第三视角相机。\n### 论文使用的评估环境和评估指标\n- **评估环境**：三个仿真基准测试。\n - SimplerEnv（Google Robot 和 WidowX 设置）：评估零样本对象操作。\n - LIBERO：四个任务套件（LIBERO-Spatial、-Object、-Goal、-Long）评估新机器人设置适配。\n- **评估指标**：\n - SimplerEnv：成功率，包括视觉匹配（VM）和变体聚合（VA）分数；平均抓取和任务完成成功率。\n - LIBERO：每个任务套件的成功率和排名。\n - 主要指标：操作成功率（%），如 Google Robot 上 PixelVLA 平均 VM 得分 61.4（提升 28.7%）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>华南理工大学自动化科学与工程学院</li><li>中国科学院沈阳自动化研究所</li><li>穆罕默德·本·扎耶德人工智能大学（Mohamed bin Zayed University of Artificial Intelligence）</li><li>澳大利亚国立大学</li></ul>\n<h3>论文概述</h3>\n<p>PixelVLA 是首个支持像素级理解和多模态提示（文本和视觉输入）的视觉-语言-动作（VLA）模型。现有 VLAs 局限于图像级理解且依赖文本提示，导致空间推理和跨域泛化能力不足。PixelVLA 通过集成多尺度像素感知编码器、视觉提示编码器和连续动作解码器，解决这些挑战。使用两阶段自动注释管道生成的 Pixel-160K 数据集进行训练，该数据集包含 160K 操作片段和 6.5M 图像-文本-动作三元组。PixelVLA 在三个标准 VLA 基准测试（SimplerEnv、Google Robot、LIBERO）上显著优于 OpenVLA，成功率提升 10.1%~28.7%，且训练成本仅为 OpenVLA 的 1.5%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>PixelVLA 架构设计</strong>：提出支持像素级理解和多模态提示的 VLA 模型，包括多尺度像素感知编码器、视觉提示编码器（处理点、线、区域等提示）和连续动作解码器（直接预测 7D 动作）。</li><li><strong>两阶段自动注释管道</strong>：开发用于合成 Pixel-160K 数据集的管道，第一阶段定位夹爪并生成区域提议，第二阶段使用 LLM 和开放词汇分割模型生成像素级注释和视觉提示。</li><li><strong>视觉运动指令调优框架</strong>：引入两阶段训练流程——连续动作训练阶段和像素级理解增强阶段，有效提升像素级空间理解能力。</li><li><strong>实证验证</strong>：在多个基准测试上集成 PixelVLA 架构到 OpenVLA 和 π₀ 模型，验证性能提升和泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>PixelVLA 架构</strong>：基于 Prismatic-7B VLM 和 Llama 2-7B 的主干，结合视觉编码器（DinoV2 和 SigLIP）、MLP 投影仪、视觉提示编码器（源自 SAM）、多尺度像素感知编码器（生成像素感知嵌入）和连续动作解码器（ResNet 块 + MLP，预测连续动作序列）。</li><li><strong>多尺度像素感知编码器</strong>：从多尺度视觉特征中提取像素级信息，公式基于特征和像素掩码的 MLP 组合。</li><li><strong>视觉提示编码器</strong>：处理多样化视觉提示（如点、线、区域），生成提示感知嵌入。</li><li><strong>连续动作解码器</strong>：基于 LLM 隐藏状态，通过线性投影、ResNet 块和 MLP 输出连续动作值（chunk 大小为 8）。</li><li><strong>训练流程</strong>：两阶段视觉运动指令调优。</li></ul>\n<p> - <strong>阶段一（连续动作训练）</strong>：使用 Fractal 和 Bridge v2 数据训练连续动作解码器，冻结其他模块。</p>\n<p> - <strong>阶段二（像素级理解增强）</strong>：使用 Pixel-160K 数据集，通过 LoRA 微调 LLM 主干，联合训练视觉提示和像素感知编码器。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Pixel-160K（160K 操作片段，6.5M 三元组），基于 Fractal 和 Bridge v2 数据集构建；LIBERO-Pixel（使用管道处理 LIBERO 基准）。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 两阶段训练：阶段一 100k 步（批量 32，学习率 5e-4），阶段二 200k 步（批量 32，学习率 1e-3，LoRA r=32）。</p>\n<p> - 总训练成本仅为 OpenVLA 预训练的 1.5%。</p>\n<p> - 输入分辨率 224×224，单视角第三视角相机。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：三个仿真基准测试。</li></ul>\n<p> - SimplerEnv（Google Robot 和 WidowX 设置）：评估零样本对象操作。</p>\n<p> - LIBERO：四个任务套件（LIBERO-Spatial、-Object、-Goal、-Long）评估新机器人设置适配。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - SimplerEnv：成功率，包括视觉匹配（VM）和变体聚合（VA）分数；平均抓取和任务完成成功率。</p>\n<p> - LIBERO：每个任务套件的成功率和排名。</p>\n<p> - 主要指标：操作成功率（%），如 Google Robot 上 PixelVLA 平均 VM 得分 61.4（提升 28.7%）。</p>"
  },
  {
    "date": "2025-11-03",
    "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.01331",
    "summary_markdown": "### 论文研究单位\nWestlake University\n### 论文概述\nRobustVLA是一种针对Vision-Language-Action (VLA)模型的在线强化学习后训练方法，旨在提高模型在环境扰动（如观察噪声和动作噪声）下的鲁棒性。传统方法在out-of-distribution部署中易失败，RobustVLA通过理论分析的鲁棒性界限，引入Jacobian正则化和smoothness正则化来显式约束模型敏感性，显著提升VLA模型的稳定性和可靠性。\n### 论文核心贡献点\n- 提出RobustVLA方法，结合在线RL和鲁棒性约束，针对环境扰动优化VLA模型。\n- 进行系统鲁棒性分析，识别观察扰动和动作扰动的影响，分别推导Jacobian敏感性和smoothness稳定性的关键作用。\n- 引入双正则化策略：Jacobian正则化抑制观察噪声敏感性，smoothness正则化缓解动作扰动和更新漂移。\n- 实验证明RobustVLA在多种扰动下优于SOTA基线（如RIPT-VLA、OpenVLA等），提升迁移学习和消融性能。\n### 论文方法描述\n- **问题建模**：基于马尔可夫决策过程（MDP），VLA模型πθ映射语言指令和观察序列到动作分布。\n- **鲁棒性分析**：\n - 定理1（观察扰动误差界限）：返回差距受Jacobian敏感性和观察噪声影响，需控制\\|\\|∇_s log πθ(a\\|s)\\|\\|。\n - 定理2（动作扰动返回漂移）：返回差距与累积模型漂移∑δ_t和动作噪声σ√d相关，需限制模型更新平滑度。\n - 定理3（联合扰动鲁棒性）：观察和动作扰动联合时返回差距扩大，需同时应用双正则化。\n- **正则化目标**：\n - Jacobian正则化：ℛ_Jac(θ) = E[min(\\|\\|∇_s log πθ(a\\|s)\\|\\|², G_max)]，限制输入敏感性。\n - Smoothness正则化：ℛ_Smooth(θ) = E[\\|\\|μ_θ(s) - μ_θ-(s)\\|\\|²]，稳定模型更新。\n - 整体目标：ℒ_RobustVLA = ℒ_PPO + αℛ_Jac + βℛ_Smooth。\n- **算法实现**：采用课程学习机制（RobustVLA-C），基于成功率的移动平均自适应调整噪声水平（ε_min到ε_max），防止训练初期不稳定。\n### 论文使用数据集和训练资源\n- **数据集**：基于LIBERO仿真平台，包含Objects、Long、Spatial、Goal四个任务套件，每个套件使用50个保留测试上下文。\n- **扰动设置**：\n - 观察扰动：图像移位、旋转、颜色抖动、遮挡、擦除。\n - 动作扰动：零均值高斯噪声，标准差0.1、0.2、0.3。\n- **训练资源**：在线RL交互收集数据，算法在仿真环境中运行；评估基于单VLA模型部署至所有任务。\n### 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO仿真平台，模拟机器人操作任务场景。\n- **评估指标**：\n - 主要指标：平均成功率（SR）在扰动下的表现。\n - 对比基线：离线IL（π₀、GEVRM、OpenVLA、OpenVLA-OFT）、离线RL（RWR、ARFM、ReinboT）、在线RL（RIPT-VLA）。\n - 其他评估：迁移学习性能（从LIBERO Goal套件到下游任务）、消融研究（超参数α和β影响、T-SNE观察表示可视化）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Westlake University</p>\n<h3>论文概述</h3>\n<p>RobustVLA是一种针对Vision-Language-Action (VLA)模型的在线强化学习后训练方法，旨在提高模型在环境扰动（如观察噪声和动作噪声）下的鲁棒性。传统方法在out-of-distribution部署中易失败，RobustVLA通过理论分析的鲁棒性界限，引入Jacobian正则化和smoothness正则化来显式约束模型敏感性，显著提升VLA模型的稳定性和可靠性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出RobustVLA方法，结合在线RL和鲁棒性约束，针对环境扰动优化VLA模型。</li><li>进行系统鲁棒性分析，识别观察扰动和动作扰动的影响，分别推导Jacobian敏感性和smoothness稳定性的关键作用。</li><li>引入双正则化策略：Jacobian正则化抑制观察噪声敏感性，smoothness正则化缓解动作扰动和更新漂移。</li><li>实验证明RobustVLA在多种扰动下优于SOTA基线（如RIPT-VLA、OpenVLA等），提升迁移学习和消融性能。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>问题建模</strong>：基于马尔可夫决策过程（MDP），VLA模型πθ映射语言指令和观察序列到动作分布。</li><li><strong>鲁棒性分析</strong>：</li></ul>\n<p> - 定理1（观察扰动误差界限）：返回差距受Jacobian敏感性和观察噪声影响，需控制\\|\\|∇_s log πθ(a\\|s)\\|\\|。</p>\n<p> - 定理2（动作扰动返回漂移）：返回差距与累积模型漂移∑δ_t和动作噪声σ√d相关，需限制模型更新平滑度。</p>\n<p> - 定理3（联合扰动鲁棒性）：观察和动作扰动联合时返回差距扩大，需同时应用双正则化。</p>\n<ul><li><strong>正则化目标</strong>：</li></ul>\n<p> - Jacobian正则化：ℛ_Jac(θ) = E[min(\\|\\|∇_s log πθ(a\\|s)\\|\\|², G_max)]，限制输入敏感性。</p>\n<p> - Smoothness正则化：ℛ_Smooth(θ) = E[\\|\\|μ_θ(s) - μ_θ-(s)\\|\\|²]，稳定模型更新。</p>\n<p> - 整体目标：ℒ_RobustVLA = ℒ_PPO + αℛ_Jac + βℛ_Smooth。</p>\n<ul><li><strong>算法实现</strong>：采用课程学习机制（RobustVLA-C），基于成功率的移动平均自适应调整噪声水平（ε_min到ε_max），防止训练初期不稳定。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：基于LIBERO仿真平台，包含Objects、Long、Spatial、Goal四个任务套件，每个套件使用50个保留测试上下文。</li><li><strong>扰动设置</strong>：</li></ul>\n<p> - 观察扰动：图像移位、旋转、颜色抖动、遮挡、擦除。</p>\n<p> - 动作扰动：零均值高斯噪声，标准差0.1、0.2、0.3。</p>\n<ul><li><strong>训练资源</strong>：在线RL交互收集数据，算法在仿真环境中运行；评估基于单VLA模型部署至所有任务。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：LIBERO仿真平台，模拟机器人操作任务场景。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：平均成功率（SR）在扰动下的表现。</p>\n<p> - 对比基线：离线IL（π₀、GEVRM、OpenVLA、OpenVLA-OFT）、离线RL（RWR、ARFM、ReinboT）、在线RL（RIPT-VLA）。</p>\n<p> - 其他评估：迁移学习性能（从LIBERO Goal套件到下游任务）、消融研究（超参数α和β影响、T-SNE观察表示可视化）。</p>"
  },
  {
    "date": "2025-11-03",
    "title": "Embodiment Transfer Learning for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.01224",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-03",
    "title": "OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.01210",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-02",
    "title": "Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots",
    "link": "http://arxiv.org/abs/2511.00917",
    "summary_markdown": "## 论文研究单位\n宾夕法尼亚大学（University of Pennsylvania），并开放项目主页：maestro-robot.github.io。\n## 论文概述\n论文提出maestro（Managerial Agent for Executing Sensorimotor Tasks in Robotics），一个以视觉语言模型（VLM）为中心的模块化智能体，通过编排包含感知、几何、控制、预训练视觉-动作模型与图像编辑等在内的“工具集”，以编写与执行代码的方式在真实世界中实现零样本通用机器人操作。该范式不依赖大规模机器人数据，而是在闭循环中“计划—反应—重计划”的执行与反馈回路中不断演化，适应新任务与新硬件。\n## 论文核心贡献点\n- 首个在零样本条件下能与最新视觉-语言-动作（VLA）模型竞争的模块化机器人策略。\n- 提出了以VLM为编码智能体、工具库为核心的系统化设计（覆盖几何与主动感知、碰撞规避、VLA高频监控），显著提升语义理解与精确执行的协同能力。\n- 系统性消融实验揭示主动感知与几何推理工具的必要性。\n- 展示在少量真实试验基础上的演化式改进机制，通过历史代码与失败分析进行上下文学习。\n## 论文方法描述\n- 总体架构：以VLM（文中采用Gemini Robotics-ER 1.5）为代码生成智能体，接收任务指令与场景图像，动态编写程序并实时执行与修改，形成“计划—反应—重计划”的闭环。\n- 工具模块设计（Tabletop / Mobile 双套件）：\n - 感知：原始RGB+本体感觉；分割与中心点；指向与任务相关关键点（受ReKep启发）；FoundationStereo深度；主动感知（手腕相机变焦/环视）。\n - 控制与几何：笛卡尔与手爪控制；cuRobo点云无碰撞运动规划；几何与线性代数（向量构造、距离、相对旋转、向量旋转）以支持空间推理。\n - 视觉运动策略：GraspGen抓取模型；π0.5 VLA作为可调用工具，配以本地托管的Qwen-2.5-VL-72B实现2Hz任务完成监控以高频中断与重规划。\n - 图像编辑：在图像上绘制关键点与6D位姿叠加，提升视觉定位与推理。\n - 移动操控：Mobile base状态估计（Faster-LIO）；语义地图缓存；主动探索工具；细粒度“nudge”局部微调与全局导航（Nav2）。\n- 演化式改进：记录历史执行代码、输出与失败分析，作为上下文样例供后续尝试学习与优化。\n## 论文使用数据集和训练资源\n- 评估方法：为检验零样本泛化，采用STAR-Gen生成评测场景，固定对比基线；5次试验/任务（初始+4次扰动）。\n- 真实世界平台与任务：\n - 桌面操控：Franka Emika Panda（7-DoF）+ Robotiq 2F 手爪；手腕与第三人称相机；DROID平台。任务包括拾取放置、可变形物体（折毛巾/T恤）、铰接物体（开柜）、空间推理（紫色面朝上旋转立方体）、工具使用（用刀切香蕉）、物体功能（挂杯）、记忆与长时序（擦白板指令后按指令叠杯）。\n - 移动操控：Unitree Go2-W 四足机器人 + AgileX PiPER机械臂；校准手腕相机。任务包括长时序取物、投掷、主动探索（搜索并返回）、物体功能（按按钮开门）。\n- 预训练/基础模型：\n - VLM：Gemini Robotics-ER 1.5（用于代码生成与视觉推理）；Qwen-2.5-VL-72B（本地高频监控与中断）。\n - VLA：π0-FAST-DROID 与 π0.5-DROID（作为可调用工具）。\n - 其他：GraspGen（抓取）、FoundationStereo（深度估计）、cuRobo（无碰撞规划）、Faster-LIO（移动基定位）、Nav2（导航）。\n## 论文使用的评估环境和评估指标\n- 评估环境：两类真实世界本体——桌面操控（DROID、Franka）、移动操控（四足+机械臂），跨场景扰动（物体、位姿、语言指令、背景等）形成零样本测试集。\n- 评估指标：\n - 主要指标：平均任务进度（0–100，数值越高越好），基于STAR-Gen与任务分解子目标的可量化评分。\n - 对比基线：Gemini Robotics Agent（代码即策略）、π0-FAST-DROID、π0.5-DROID；另含maestro+π0.5（VLA作为工具并联）。\n - 消融设置：去除主动感知与几何模块对性能的影响。\n - 跨任务类别：在桌面与移动操控的多样任务上进行对比与误差分析。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>宾夕法尼亚大学（University of Pennsylvania），并开放项目主页：maestro-robot.github.io。</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文提出maestro（Managerial Agent for Executing Sensorimotor Tasks in Robotics），一个以视觉语言模型（VLM）为中心的模块化智能体，通过编排包含感知、几何、控制、预训练视觉-动作模型与图像编辑等在内的“工具集”，以编写与执行代码的方式在真实世界中实现零样本通用机器人操作。该范式不依赖大规模机器人数据，而是在闭循环中“计划—反应—重计划”的执行与反馈回路中不断演化，适应新任务与新硬件。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>首个在零样本条件下能与最新视觉-语言-动作（VLA）模型竞争的模块化机器人策略。</li><li>提出了以VLM为编码智能体、工具库为核心的系统化设计（覆盖几何与主动感知、碰撞规避、VLA高频监控），显著提升语义理解与精确执行的协同能力。</li><li>系统性消融实验揭示主动感知与几何推理工具的必要性。</li><li>展示在少量真实试验基础上的演化式改进机制，通过历史代码与失败分析进行上下文学习。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>总体架构：以VLM（文中采用Gemini Robotics-ER 1.5）为代码生成智能体，接收任务指令与场景图像，动态编写程序并实时执行与修改，形成“计划—反应—重计划”的闭环。</li><li>工具模块设计（Tabletop / Mobile 双套件）：</li></ul>\n<p> - 感知：原始RGB+本体感觉；分割与中心点；指向与任务相关关键点（受ReKep启发）；FoundationStereo深度；主动感知（手腕相机变焦/环视）。</p>\n<p> - 控制与几何：笛卡尔与手爪控制；cuRobo点云无碰撞运动规划；几何与线性代数（向量构造、距离、相对旋转、向量旋转）以支持空间推理。</p>\n<p> - 视觉运动策略：GraspGen抓取模型；π0.5 VLA作为可调用工具，配以本地托管的Qwen-2.5-VL-72B实现2Hz任务完成监控以高频中断与重规划。</p>\n<p> - 图像编辑：在图像上绘制关键点与6D位姿叠加，提升视觉定位与推理。</p>\n<p> - 移动操控：Mobile base状态估计（Faster-LIO）；语义地图缓存；主动探索工具；细粒度“nudge”局部微调与全局导航（Nav2）。</p>\n<ul><li>演化式改进：记录历史执行代码、输出与失败分析，作为上下文样例供后续尝试学习与优化。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>评估方法：为检验零样本泛化，采用STAR-Gen生成评测场景，固定对比基线；5次试验/任务（初始+4次扰动）。</li><li>真实世界平台与任务：</li></ul>\n<p> - 桌面操控：Franka Emika Panda（7-DoF）+ Robotiq 2F 手爪；手腕与第三人称相机；DROID平台。任务包括拾取放置、可变形物体（折毛巾/T恤）、铰接物体（开柜）、空间推理（紫色面朝上旋转立方体）、工具使用（用刀切香蕉）、物体功能（挂杯）、记忆与长时序（擦白板指令后按指令叠杯）。</p>\n<p> - 移动操控：Unitree Go2-W 四足机器人 + AgileX PiPER机械臂；校准手腕相机。任务包括长时序取物、投掷、主动探索（搜索并返回）、物体功能（按按钮开门）。</p>\n<ul><li>预训练/基础模型：</li></ul>\n<p> - VLM：Gemini Robotics-ER 1.5（用于代码生成与视觉推理）；Qwen-2.5-VL-72B（本地高频监控与中断）。</p>\n<p> - VLA：π0-FAST-DROID 与 π0.5-DROID（作为可调用工具）。</p>\n<p> - 其他：GraspGen（抓取）、FoundationStereo（深度估计）、cuRobo（无碰撞规划）、Faster-LIO（移动基定位）、Nav2（导航）。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：两类真实世界本体——桌面操控（DROID、Franka）、移动操控（四足+机械臂），跨场景扰动（物体、位姿、语言指令、背景等）形成零样本测试集。</li><li>评估指标：</li></ul>\n<p> - 主要指标：平均任务进度（0–100，数值越高越好），基于STAR-Gen与任务分解子目标的可量化评分。</p>\n<p> - 对比基线：Gemini Robotics Agent（代码即策略）、π0-FAST-DROID、π0.5-DROID；另含maestro+π0.5（VLA作为工具并联）。</p>\n<p> - 消融设置：去除主动感知与几何模块对性能的影响。</p>\n<p> - 跨任务类别：在桌面与移动操控的多样任务上进行对比与误差分析。</p>"
  },
  {
    "date": "2025-10-31",
    "title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection",
    "link": "http://arxiv.org/abs/2511.00139",
    "summary_markdown": "### 论文研究单位\nByteDance Seed\n### 论文概述\n论文针对灵巧手在一般机器人中的manipulation挑战，提出一种共享自治（Shared Autonomy）框架，结合人类VR远程操作和自主DexGrasp-VLA策略（用于手部控制），以高效收集协调手臂-手部演示数据。该框架通过分工控制宏-微运动域：人类引导手臂，VLA Copilot执行精细抓取，降低认知负担。数据用于训练端到端VLA策略（含Arm-Hand Feature Enhancement模块），并通过校正远程操作（Corrective Teleoperation）持续改进。实验显示，策略在50+物体上达约90%成功率，数据质量高。\n### 论文核心贡献点\n- 提出多模态VLA Copilot（DexGrasp-VLA）：融合视觉、语言、触觉和本体感受，实现力适应抓取。\n- 设计共享自治框架：人类VR控制手臂，VLA自主控制手部，高效收集高质量演示。\n- 开发Arm-Hand Feature Enhancement架构：显式建模手臂（宏运动）和手部（微运动）的互补特征，提升协调性。\n- 实现校正人类在环远程操作：采集失败恢复数据，迭代优化策略鲁棒性。\n### 论文方法描述\n方法分四阶段：\n1. **DexGrasp-VLA策略训练**：先训练LSTM“盲策略”（融合参数化力控制和远程操作数据），再扩展为多模态VLA（整合视觉、触觉、语言和本体感受）。\n2. **共享自治数据收集**：人类通过VR相对运动映射远程操作手臂（90Hz），DexGrasp-VLA并行控制手部（30Hz），采集同步手臂-手部数据。\n3. **端到端VLA策略学习**：基于预训练VLA模型（SFT），添加Arm-Hand Feature Enhancement模块（共享表示+MLP编码的手臂/手部特征），优化总损失（含主损失+辅助损失）。\n4. **校正远程操作系统**：部署策略自动记录成功/失败轨迹；人类介入纠错采集恢复数据，迭代SFT更新策略。\n### 论文使用数据集和训练资源\n- **LSTM预训练数据集**：218条轨迹（150条人类远程操作+68条力控制数据），覆盖20+物体，用于训练“盲”力适应抓取策略。\n- **DexGrasp-VLA手部数据集**：180条抓取轨迹（杂乱场景中60种物体），含手部RGB、触觉（力向量+空间嵌入）、本体状态和动作。\n- **端到端手臂-手部数据集**：100条共享自治演示，覆盖20种物体，含多视角RGB、语言指令和手臂-手部联合动作。\n- **校正干预数据集**：100条轨迹（50条方向失败恢复+50条角落案例）。\n- **训练资源**：基于开源框架LeRobot（Fine-tuning预训练VLA模型π₀）；硬件包括UR3e机械臂（6-DoF）、Xhand五指手（12-DoF）、三RGB-D摄像头及触觉传感器。\n### 论文使用的评估环境和评估指标\n- **评估环境**：集成机器人平台（UR3e + Xhand），装配三RGB-D摄像头（两静态+一手腕-mounted）；任务场景包括杂杂乱物体抓取、表单清理等。\n- **评估指标**：\n - 抓取成功率：DexGrasp-VLA在杂乱场景中95.5%，端到端策略在50+物体上约90%。\n - 消融研究：验证触觉特征、手部-手臂特征增强、校正远程操作对性能的影响（如成功率、鲁棒性）。\n - 数据效率：远程操作会话时长、认知负担（经验显示~30分钟无疲劳）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>ByteDance Seed</p>\n<h3>论文概述</h3>\n<p>论文针对灵巧手在一般机器人中的manipulation挑战，提出一种共享自治（Shared Autonomy）框架，结合人类VR远程操作和自主DexGrasp-VLA策略（用于手部控制），以高效收集协调手臂-手部演示数据。该框架通过分工控制宏-微运动域：人类引导手臂，VLA Copilot执行精细抓取，降低认知负担。数据用于训练端到端VLA策略（含Arm-Hand Feature Enhancement模块），并通过校正远程操作（Corrective Teleoperation）持续改进。实验显示，策略在50+物体上达约90%成功率，数据质量高。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出多模态VLA Copilot（DexGrasp-VLA）：融合视觉、语言、触觉和本体感受，实现力适应抓取。</li><li>设计共享自治框架：人类VR控制手臂，VLA自主控制手部，高效收集高质量演示。</li><li>开发Arm-Hand Feature Enhancement架构：显式建模手臂（宏运动）和手部（微运动）的互补特征，提升协调性。</li><li>实现校正人类在环远程操作：采集失败恢复数据，迭代优化策略鲁棒性。</li></ul>\n<h3>论文方法描述</h3>\n<p>方法分四阶段：</p>\n<ol><li><strong>DexGrasp-VLA策略训练</strong>：先训练LSTM“盲策略”（融合参数化力控制和远程操作数据），再扩展为多模态VLA（整合视觉、触觉、语言和本体感受）。</li><li><strong>共享自治数据收集</strong>：人类通过VR相对运动映射远程操作手臂（90Hz），DexGrasp-VLA并行控制手部（30Hz），采集同步手臂-手部数据。</li><li><strong>端到端VLA策略学习</strong>：基于预训练VLA模型（SFT），添加Arm-Hand Feature Enhancement模块（共享表示+MLP编码的手臂/手部特征），优化总损失（含主损失+辅助损失）。</li><li><strong>校正远程操作系统</strong>：部署策略自动记录成功/失败轨迹；人类介入纠错采集恢复数据，迭代SFT更新策略。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>LSTM预训练数据集</strong>：218条轨迹（150条人类远程操作+68条力控制数据），覆盖20+物体，用于训练“盲”力适应抓取策略。</li><li><strong>DexGrasp-VLA手部数据集</strong>：180条抓取轨迹（杂乱场景中60种物体），含手部RGB、触觉（力向量+空间嵌入）、本体状态和动作。</li><li><strong>端到端手臂-手部数据集</strong>：100条共享自治演示，覆盖20种物体，含多视角RGB、语言指令和手臂-手部联合动作。</li><li><strong>校正干预数据集</strong>：100条轨迹（50条方向失败恢复+50条角落案例）。</li><li><strong>训练资源</strong>：基于开源框架LeRobot（Fine-tuning预训练VLA模型π₀）；硬件包括UR3e机械臂（6-DoF）、Xhand五指手（12-DoF）、三RGB-D摄像头及触觉传感器。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：集成机器人平台（UR3e + Xhand），装配三RGB-D摄像头（两静态+一手腕-mounted）；任务场景包括杂杂乱物体抓取、表单清理等。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 抓取成功率：DexGrasp-VLA在杂乱场景中95.5%，端到端策略在50+物体上约90%。</p>\n<p> - 消融研究：验证触觉特征、手部-手臂特征增强、校正远程操作对性能的影响（如成功率、鲁棒性）。</p>\n<p> - 数据效率：远程操作会话时长、认知负担（经验显示~30分钟无疲劳）。</p>"
  },
  {
    "date": "2025-10-30",
    "title": "Self-Improving Vision-Language-Action Models with Data Generation via Residual RL",
    "link": "http://arxiv.org/abs/2511.00091",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-30",
    "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
    "link": "http://arxiv.org/abs/2511.00088",
    "summary_markdown": "# 论文研究单位\nNVIDIA\n# 论文概述\nAlpamayo-R1 (AR1) 是一个视觉-语言-动作模型，旨在通过整合Chain of Causation推理与轨迹规划来增强自主驾驶在复杂驾驶场景中的决策能力。该模型专注于解决安全关键的尾部场景问题，这些场景中监督稀疏且因果理解有限。\n# 论文核心贡献点\n1. **Chain of Causation (CoC)数据集**：构建了混合自动标注和人工标注流程，产生与驾驶行为对齐的决策导向、因果关联的推理轨迹\n2. **模块化VLA架构**：结合Cosmos-Reason（为物理AI应用预训练的视觉-语言模型）与基于扩散的轨迹解码器，实时生成动态可行的计划\n3. **多阶段训练策略**：使用监督微调来引出推理能力，并通过强化学习优化推理质量和推理-行动一致性\n# 论文方法描述\n## 架构设计\n- **VLM主干网络**：采用Cosmos-Reason作为视觉-语言模型骨干\n- **高效视觉编码**：支持单图像标记化、多相机标记化和多相机视频标记化，实现最高20倍token压缩率\n- **扩散式轨迹解码**：使用流匹配框架将离散轨迹token转换为连续表示，基于unicycle动力学建模\n## 推理与行动结合\n通过统一的序列预测框架，将历史观测、推理过程和未来轨迹整合为统一序列，实现推理与行动预测的无缝结合。\n## 数据构建方法\n**Chain of Causation数据集**采用结构化标注框架：\n- **决策标注**：定义封闭集合的高层驾驶决策\n- **关键组件标注**：开放集合的因果因素\n- **混合标注流程**：结合人工标注(10-20%数据)和自动标注(80-90%数据)\n# 论文使用数据集和训练资源\n- **基础数据集**：基于nuScenes等公开驾驶数据集\n- **CoC数据集**：混合人工和自动标注，包含结构化推理轨迹\n- **物理AI领域数据**：覆盖机器人、医疗、智慧城市等多领域数据用于预训练\n- **人类标注数据**：10-20%高质量人工标注样本用于监督微调和评估\n- **自动标注数据**：大规模自动生成的结构化标注数据\n# 论文使用的评估环境和评估指标\n## 评估环境\n- **开环评估**：标准数据集上的规划准确性评估\n- **闭环评估**：仿真环境中的驾驶性能测试\n- **实车测试**：车载系统验证实际部署性能\n## 评估指标\n- **规划性能**：在挑战性场景中规划准确性提升12%\n- **安全性指标**：\n - 离路率降低35%\n - 近距离遭遇率降低25%\n- **推理质量**：通过大推理模型评估，推理质量提升45%\n- **推理-行动一致性**：一致性提升37%\n- **模型扩展性**：从0.5B到7B参数的一致改进\n- **实时性能**：端到端延迟99毫秒，支持实时部署",
    "summary_html": "<h1>论文研究单位</h1>\n<p>NVIDIA</p>\n<h1>论文概述</h1>\n<p>Alpamayo-R1 (AR1) 是一个视觉-语言-动作模型，旨在通过整合Chain of Causation推理与轨迹规划来增强自主驾驶在复杂驾驶场景中的决策能力。该模型专注于解决安全关键的尾部场景问题，这些场景中监督稀疏且因果理解有限。</p>\n<h1>论文核心贡献点</h1>\n<ol><li><strong>Chain of Causation (CoC)数据集</strong>：构建了混合自动标注和人工标注流程，产生与驾驶行为对齐的决策导向、因果关联的推理轨迹</li><li><strong>模块化VLA架构</strong>：结合Cosmos-Reason（为物理AI应用预训练的视觉-语言模型）与基于扩散的轨迹解码器，实时生成动态可行的计划</li><li><strong>多阶段训练策略</strong>：使用监督微调来引出推理能力，并通过强化学习优化推理质量和推理-行动一致性</li></ol>\n<h1>论文方法描述</h1>\n<h2 class=\"section-title\">架构设计</h2>\n<ul><li><strong>VLM主干网络</strong>：采用Cosmos-Reason作为视觉-语言模型骨干</li><li><strong>高效视觉编码</strong>：支持单图像标记化、多相机标记化和多相机视频标记化，实现最高20倍token压缩率</li><li><strong>扩散式轨迹解码</strong>：使用流匹配框架将离散轨迹token转换为连续表示，基于unicycle动力学建模</li></ul>\n<h2 class=\"section-title\">推理与行动结合</h2>\n<p>通过统一的序列预测框架，将历史观测、推理过程和未来轨迹整合为统一序列，实现推理与行动预测的无缝结合。</p>\n<h2 class=\"section-title\">数据构建方法</h2>\n<p><strong>Chain of Causation数据集</strong>采用结构化标注框架：</p>\n<ul><li><strong>决策标注</strong>：定义封闭集合的高层驾驶决策</li><li><strong>关键组件标注</strong>：开放集合的因果因素</li><li><strong>混合标注流程</strong>：结合人工标注(10-20%数据)和自动标注(80-90%数据)</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>基础数据集</strong>：基于nuScenes等公开驾驶数据集</li><li><strong>CoC数据集</strong>：混合人工和自动标注，包含结构化推理轨迹</li><li><strong>物理AI领域数据</strong>：覆盖机器人、医疗、智慧城市等多领域数据用于预训练</li><li><strong>人类标注数据</strong>：10-20%高质量人工标注样本用于监督微调和评估</li><li><strong>自动标注数据</strong>：大规模自动生成的结构化标注数据</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<h2 class=\"section-title\">评估环境</h2>\n<ul><li><strong>开环评估</strong>：标准数据集上的规划准确性评估</li><li><strong>闭环评估</strong>：仿真环境中的驾驶性能测试</li><li><strong>实车测试</strong>：车载系统验证实际部署性能</li></ul>\n<h2 class=\"section-title\">评估指标</h2>\n<ul><li><strong>规划性能</strong>：在挑战性场景中规划准确性提升12%</li><li><strong>安全性指标</strong>：</li></ul>\n<p> - 离路率降低35%</p>\n<p> - 近距离遭遇率降低25%</p>\n<ul><li><strong>推理质量</strong>：通过大推理模型评估，推理质量提升45%</li><li><strong>推理-行动一致性</strong>：一致性提升37%</li><li><strong>模型扩展性</strong>：从0.5B到7B参数的一致改进</li><li><strong>实时性能</strong>：端到端延迟99毫秒，支持实时部署</li></ul>"
  },
  {
    "date": "2025-10-31",
    "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2510.27607",
    "summary_markdown": "### 论文研究单位\nKAIST (韩国科学技术院) 和 RLWRLD\n### 论文概述\n本文提出了双流扩散（DUST）框架，一种用于增强视觉-语言-动作模型（VLA）的世界模型。DUST通过在多模态扩散Transformer中维护独立的模态流，同时通过跨模态注意力实现知识共享，解决了联合预测动作和未来观测时存在的模态冲突问题。该方法还包括解耦的训练算法和异步采样策略，支持推理时的扩展。\n### 论文核心贡献点\n1. 提出了双流扩散（DUST）框架，结合世界模型增强VLA，通过独立模态流和跨模态注意力解决模态冲突。\n2. 设计了多模态扩散Transformer（MMDiT）架构，分别处理动作和视觉流，实现双向信息传递。\n3. 开发了基于独立噪声调度的解耦训练算法，学习动作与视觉之间的因果关系。\n4. 引入了异步联合采样策略，允许视觉和动作标记以不同速率进行扩散，实现推理时的性能扩展。\n### 论文方法描述\nDUST架构包括：\n1. **VLM主干网络**：使用预训练的视觉-语言模型提取当前观测和指令的语义特征。\n2. **多模态扩散Transformer（MMDiT）**：包含12个MMDiT层和4个模态特定的DiT层。动作和视觉标记流在MMDiT层中通过共享交叉注意力层交互，但在其他操作中保持独立。\n3. **解耦训练**：动作和未来视觉观测分别添加独立噪声，使用模态特定的流匹配损失函数优化联合目标。\n4. **异步采样**：推理时，视觉标记以更高频率（如32步）去噪，动作标记以较低频率（如4步）去噪，平衡效率与精度。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **模拟环境**：RoboCasa（24个厨房任务）和GR-1（24个人形机器人桌面任务）。\n - **真实世界**：Franka Research 3机械臂的4个取放任务。\n - **迁移学习**：使用BridgeV2的无动作视频进行预训练。\n- **训练资源**：\n - VLM主干：冻结Eagle-2模型。\n - 扩散模型：使用Flow Matching目标，批大小未明确，迭代次数未明确。\n - 损失权重：\\(\\lambda_{\\text{WM}} = 1.0\\)。\n - 硬件：未明确说明。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟**：RoboCasa和GR-1基准，使用不同规模的演示数据（100、300、1000条/任务）。\n - **真实世界**：Franka Research 3机械臂在桌面场景执行取放任务。\n- **评估指标**：\n - **成功率（%）**：任务完成的主要指标，涵盖不同任务类别（如取放、开合装置）。\n - **平均成功率**：跨任务类别的平均表现。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>KAIST (韩国科学技术院) 和 RLWRLD</p>\n<h3>论文概述</h3>\n<p>本文提出了双流扩散（DUST）框架，一种用于增强视觉-语言-动作模型（VLA）的世界模型。DUST通过在多模态扩散Transformer中维护独立的模态流，同时通过跨模态注意力实现知识共享，解决了联合预测动作和未来观测时存在的模态冲突问题。该方法还包括解耦的训练算法和异步采样策略，支持推理时的扩展。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了双流扩散（DUST）框架，结合世界模型增强VLA，通过独立模态流和跨模态注意力解决模态冲突。</li><li>设计了多模态扩散Transformer（MMDiT）架构，分别处理动作和视觉流，实现双向信息传递。</li><li>开发了基于独立噪声调度的解耦训练算法，学习动作与视觉之间的因果关系。</li><li>引入了异步联合采样策略，允许视觉和动作标记以不同速率进行扩散，实现推理时的性能扩展。</li></ol>\n<h3>论文方法描述</h3>\n<p>DUST架构包括：</p>\n<ol><li><strong>VLM主干网络</strong>：使用预训练的视觉-语言模型提取当前观测和指令的语义特征。</li><li><strong>多模态扩散Transformer（MMDiT）</strong>：包含12个MMDiT层和4个模态特定的DiT层。动作和视觉标记流在MMDiT层中通过共享交叉注意力层交互，但在其他操作中保持独立。</li><li><strong>解耦训练</strong>：动作和未来视觉观测分别添加独立噪声，使用模态特定的流匹配损失函数优化联合目标。</li><li><strong>异步采样</strong>：推理时，视觉标记以更高频率（如32步）去噪，动作标记以较低频率（如4步）去噪，平衡效率与精度。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：RoboCasa（24个厨房任务）和GR-1（24个人形机器人桌面任务）。</p>\n<p> - <strong>真实世界</strong>：Franka Research 3机械臂的4个取放任务。</p>\n<p> - <strong>迁移学习</strong>：使用BridgeV2的无动作视频进行预训练。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - VLM主干：冻结Eagle-2模型。</p>\n<p> - 扩散模型：使用Flow Matching目标，批大小未明确，迭代次数未明确。</p>\n<p> - 损失权重：\\(\\lambda_{\\text{WM}} = 1.0\\)。</p>\n<p> - 硬件：未明确说明。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟</strong>：RoboCasa和GR-1基准，使用不同规模的演示数据（100、300、1000条/任务）。</p>\n<p> - <strong>真实世界</strong>：Franka Research 3机械臂在桌面场景执行取放任务。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（%）</strong>：任务完成的主要指标，涵盖不同任务类别（如取放、开合装置）。</p>\n<p> - <strong>平均成功率</strong>：跨任务类别的平均表现。</p>"
  },
  {
    "date": "2025-10-31",
    "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
    "link": "http://arxiv.org/abs/2510.27545",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-30",
    "title": "Running VLAs at Real-time Speed",
    "link": "http://arxiv.org/abs/2510.26742",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-30",
    "title": "RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration",
    "link": "http://arxiv.org/abs/2510.26536",
    "summary_markdown": "### 论文研究单位\n- 北京大学计算机科学学院多媒体信息处理国家重点实验室\n- 北京智源人工智能研究院\n- 中国科学院自动化研究所\n- 北京航空航天大学\n### 论文概述\nRoboOS-NeXT是一个统一的基于内存的多机器人协作框架，旨在实现终身适应性、可扩展协作和鲁棒调度。该框架的核心是Spatio-Temporal–Embodiment Memory (STEM)，它将空间场景几何、时间事件历史和机器人配置文件集成到一个共享表示中。通过采用大脑-小脑框架，高级大脑模型通过检索和更新STEM进行全局规划，而低级控制器在本地执行动作。这种认知、执行和内存的闭环实现了动态任务分配、容错协作和一致状态同步。\n### 论文核心贡献点\n- 提出了RoboOS-NeXT，一个基于内存的多机器人协作框架，构建在STEM之上，将空间、时间和实例化维度集成到统一表示中\n- 设计了Brain-Cerebellum-Memory层次循环，通过STEM连接全局推理与技能执行，为多机器人协作提供了原则性基础\n- 在餐厅、超市和家庭等多样化任务中评估RoboOS-NeXT，并在真实世界演示中展示其在异构机器人上的有效性\n### 论文方法描述\nRoboOS-NeXT的核心方法包括：\n\n1. **Spatio-Temporal–Embodiment Memory (STEM)**：\n - 时间内存：维护附加仅、时间有序的列表，记录状态增量、暂存任务上下文和工具调用跟踪\n - 空间内存：建模场景为根类型多分支树（全局场景->区域->载体），每个载体锚定对象级图\n - 实例化内存：为每个机器人维护档案，包括定位、能力、资源、传感器状态和可用性\n - 组织为队列-树-图-代理结构，支持跨维度交互\n\n2. **Brain-Cerebellum-Memory框架**：\n - 全局任务分解：大脑模型使用检索增强生成过程查询共享空间内存，生成结构化推理跟踪和工作流图\n - 拓扑子任务分配：监控器基于有向无环图中的拓扑依赖关系动态调度和分配子任务\n - 分布式子任务代理：为每个子任务部署专用机器人代理，自主编排工具选择\n - 动态内存更新：随着机器人在子任务执行过程中感知和行动，增量更新时间内存和空间内存\n### 论文使用数据集和训练资源\n- **数据集**：在三个领域（餐厅、超市、家庭）各实例化200个任务的模拟设置\n- **训练资源**：\n - 高级推理由RoboBrain-2.0多模态大语言模型驱动，增强用于时空推理\n - 低级执行包含基于SLAM的导航模块和基于扩散策略的操纵模块\n- **真实机器人平台**：Unitree G1人形机器人、Agilex双臂机器人和Realman单臂机器人\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 三个领域（餐厅、超市、家庭）的模拟设置\n - 真实世界协作演示场景\n- **评估指标**：\n - 成功率(SR, %)：在步骤预算内成功完成的任务比例\n - 边际成功率(MSR, %)：在终身或课程序列中每个序列的最终任务上测量的成功率\n - 平均每任务执行步骤(AEST, #)：完成任务所需的平均步骤数\n - 每步成功率(SS, %/#)：任务成功率与平均步骤数之间的比率，反映每步实现平均准确性",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>北京大学计算机科学学院多媒体信息处理国家重点实验室</li><li>北京智源人工智能研究院</li><li>中国科学院自动化研究所</li><li>北京航空航天大学</li></ul>\n<h3>论文概述</h3>\n<p>RoboOS-NeXT是一个统一的基于内存的多机器人协作框架，旨在实现终身适应性、可扩展协作和鲁棒调度。该框架的核心是Spatio-Temporal–Embodiment Memory (STEM)，它将空间场景几何、时间事件历史和机器人配置文件集成到一个共享表示中。通过采用大脑-小脑框架，高级大脑模型通过检索和更新STEM进行全局规划，而低级控制器在本地执行动作。这种认知、执行和内存的闭环实现了动态任务分配、容错协作和一致状态同步。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了RoboOS-NeXT，一个基于内存的多机器人协作框架，构建在STEM之上，将空间、时间和实例化维度集成到统一表示中</li><li>设计了Brain-Cerebellum-Memory层次循环，通过STEM连接全局推理与技能执行，为多机器人协作提供了原则性基础</li><li>在餐厅、超市和家庭等多样化任务中评估RoboOS-NeXT，并在真实世界演示中展示其在异构机器人上的有效性</li></ul>\n<h3>论文方法描述</h3>\n<p>RoboOS-NeXT的核心方法包括：</p>\n\n<ol><li><strong>Spatio-Temporal–Embodiment Memory (STEM)</strong>：</li></ol>\n<p> - 时间内存：维护附加仅、时间有序的列表，记录状态增量、暂存任务上下文和工具调用跟踪</p>\n<p> - 空间内存：建模场景为根类型多分支树（全局场景->区域->载体），每个载体锚定对象级图</p>\n<p> - 实例化内存：为每个机器人维护档案，包括定位、能力、资源、传感器状态和可用性</p>\n<p> - 组织为队列-树-图-代理结构，支持跨维度交互</p>\n\n<ol><li><strong>Brain-Cerebellum-Memory框架</strong>：</li></ol>\n<p> - 全局任务分解：大脑模型使用检索增强生成过程查询共享空间内存，生成结构化推理跟踪和工作流图</p>\n<p> - 拓扑子任务分配：监控器基于有向无环图中的拓扑依赖关系动态调度和分配子任务</p>\n<p> - 分布式子任务代理：为每个子任务部署专用机器人代理，自主编排工具选择</p>\n<p> - 动态内存更新：随着机器人在子任务执行过程中感知和行动，增量更新时间内存和空间内存</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：在三个领域（餐厅、超市、家庭）各实例化200个任务的模拟设置</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 高级推理由RoboBrain-2.0多模态大语言模型驱动，增强用于时空推理</p>\n<p> - 低级执行包含基于SLAM的导航模块和基于扩散策略的操纵模块</p>\n<ul><li><strong>真实机器人平台</strong>：Unitree G1人形机器人、Agilex双臂机器人和Realman单臂机器人</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 三个领域（餐厅、超市、家庭）的模拟设置</p>\n<p> - 真实世界协作演示场景</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率(SR, %)：在步骤预算内成功完成的任务比例</p>\n<p> - 边际成功率(MSR, %)：在终身或课程序列中每个序列的最终任务上测量的成功率</p>\n<p> - 平均每任务执行步骤(AEST, #)：完成任务所需的平均步骤数</p>\n<p> - 每步成功率(SS, %/#)：任务成功率与平均步骤数之间的比率，反映每步实现平均准确性</p>"
  },
  {
    "date": "2025-10-30",
    "title": "Human-in-the-loop Online Rejection Sampling for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2510.26406",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-29",
    "title": "$π_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.25889",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-29",
    "title": "Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.25713",
    "summary_markdown": "### 论文研究单位\nSoft Robotics Lab, ETHz（苏黎世联邦理工学院软机器人实验室）\n### 论文概述\n针对机器人与人类在协作任务中的自然交互挑战，论文提出基于预训练视觉语言动作模型（VLA）的改进方案。核心思想是通过运动线索（而非语言提示）实现隐式意图理解，使机器人能实时响应人类动作并完成协作任务。\n### 论文核心贡献点\n- **FiLM条件化**：在视觉编码器中插入FiLM层，增强文本-视觉对齐能力\n- **辅助损失设计**：添加并行预测头（手部姿态和目标物体）以隐式学习人类意图\n- **动作后处理优化**：通过增量坐标、旋转向量及手部关节PCA降维（保留96%方差）重构低维动作流形\n- **方向性损失**（实验效果负向）：提出方向对齐损失，但未提升性能\n- **推理系统集成**：构建实时硬件-模型接口，实现长任务动态提示切换\n### 论文方法描述\n**模型架构**：基于Open-VLA改进（SigLIP+DINOv2视觉编码器+LLaMA2-7B语言模型），集成FiLM条件化和辅助预测头。\n**数据收集流程**：双角色协作采集（操作者穿戴动作捕捉手套控制机器人，协作者在共享空间交互），10Hz采样并同步多模态数据，附文本指令和手部姿态标注。\n**训练策略**：4×H100 GPU分布式训练（批量24，LoRA rank=32，20轮迭代），采用复合损失（动作+辅助损失）。\n**推理系统**：硬件接口（绿色）→模型推理（蓝色）→高层规划（红色）的三层架构，0.3秒端到端延迟。\n### 论文使用数据集和训练资源\n- **协作任务数据**：\n - \"Pick up cube\"任务：120条轨迹（红蓝立方体各60条）\n - \"Pass cube\"任务：260条轨迹（红立方体200条，蓝立方体60条）\n - 辅助标签：手部3D姿态（MediaPipe）、目标物体索引\n- **训练资源**：4×H100 GPU集群，平均训练时长12小时\n### 论文使用的评估环境和评估指标\n- **动作空间分析**：\n - 端 effector增量坐标分布更接近高斯（vs原始坐标非凸性）\n - 手部关节PCA保留4个主成分解释96%方差\n- **消融实验**：\n - 动作后处理贡献最大（各指标显著提升）\n - 辅助损失提供稳定增益\n - 方向性损失和FiLM在不同损失下呈负向效果\n- **真实世界评估**：\n - 场景：桌面临场交互（多摄像头）\n - 长任务成功率：10次试验中1次成功\n - 失败原因：协作对象与训练者不同时出现\"训练员过拟合\"（模型对特定操作者行为过拟合）\n- **评估指标**：L2损失、PCA重构误差、方向对齐误差、真实任务完成率",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Soft Robotics Lab, ETHz（苏黎世联邦理工学院软机器人实验室）</p>\n<h3>论文概述</h3>\n<p>针对机器人与人类在协作任务中的自然交互挑战，论文提出基于预训练视觉语言动作模型（VLA）的改进方案。核心思想是通过运动线索（而非语言提示）实现隐式意图理解，使机器人能实时响应人类动作并完成协作任务。</p>\n<h3>论文核心贡献点</h3>\n<ul><li><strong>FiLM条件化</strong>：在视觉编码器中插入FiLM层，增强文本-视觉对齐能力</li><li><strong>辅助损失设计</strong>：添加并行预测头（手部姿态和目标物体）以隐式学习人类意图</li><li><strong>动作后处理优化</strong>：通过增量坐标、旋转向量及手部关节PCA降维（保留96%方差）重构低维动作流形</li><li><strong>方向性损失</strong>（实验效果负向）：提出方向对齐损失，但未提升性能</li><li><strong>推理系统集成</strong>：构建实时硬件-模型接口，实现长任务动态提示切换</li></ul>\n<h3>论文方法描述</h3>\n<p><strong>模型架构</strong>：基于Open-VLA改进（SigLIP+DINOv2视觉编码器+LLaMA2-7B语言模型），集成FiLM条件化和辅助预测头。</p>\n<p><strong>数据收集流程</strong>：双角色协作采集（操作者穿戴动作捕捉手套控制机器人，协作者在共享空间交互），10Hz采样并同步多模态数据，附文本指令和手部姿态标注。</p>\n<p><strong>训练策略</strong>：4×H100 GPU分布式训练（批量24，LoRA rank=32，20轮迭代），采用复合损失（动作+辅助损失）。</p>\n<p><strong>推理系统</strong>：硬件接口（绿色）→模型推理（蓝色）→高层规划（红色）的三层架构，0.3秒端到端延迟。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>协作任务数据</strong>：</li></ul>\n<p> - \"Pick up cube\"任务：120条轨迹（红蓝立方体各60条）</p>\n<p> - \"Pass cube\"任务：260条轨迹（红立方体200条，蓝立方体60条）</p>\n<p> - 辅助标签：手部3D姿态（MediaPipe）、目标物体索引</p>\n<ul><li><strong>训练资源</strong>：4×H100 GPU集群，平均训练时长12小时</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>动作空间分析</strong>：</li></ul>\n<p> - 端 effector增量坐标分布更接近高斯（vs原始坐标非凸性）</p>\n<p> - 手部关节PCA保留4个主成分解释96%方差</p>\n<ul><li><strong>消融实验</strong>：</li></ul>\n<p> - 动作后处理贡献最大（各指标显著提升）</p>\n<p> - 辅助损失提供稳定增益</p>\n<p> - 方向性损失和FiLM在不同损失下呈负向效果</p>\n<ul><li><strong>真实世界评估</strong>：</li></ul>\n<p> - 场景：桌面临场交互（多摄像头）</p>\n<p> - 长任务成功率：10次试验中1次成功</p>\n<p> - 失败原因：协作对象与训练者不同时出现\"训练员过拟合\"（模型对特定操作者行为过拟合）</p>\n<ul><li><strong>评估指标</strong>：L2损失、PCA重构误差、方向对齐误差、真实任务完成率</li></ul>"
  },
  {
    "date": "2025-10-29",
    "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization",
    "link": "http://arxiv.org/abs/2510.25616",
    "summary_markdown": "### 论文研究单位\nCognitive AI Lab, IAI MIPT, Moscow, Russia\n### 论文概述\n该论文研究了视觉-语言-动作模型（VLA）在动作微调过程中视觉表示退化的问题。通过系统性分析，作者发现标准的监督微调（SFT）会导致视觉表示崩溃、注意力分散和领域特定遗忘。为解决这一问题，论文提出了一种轻量级的视觉表示对齐方法，将VLA的视觉特征锚定到预训练的教师模型上，从而保留多模态理解能力并提升分布外（OOD）泛化性能。\n### 论文核心贡献点\n1. **表示退化分析**：通过注意力图可视化和t-SNE分析，系统证明VLA微调会导致视觉表示崩溃和注意力分散，相较于原始VLM丧失语义一致性。\n2. **VL-Think任务套件**：设计了一个诊断基准，包含8个视觉-语言任务（如形状、颜色、交通符号识别），用于量化VLA从VLM继承的VL知识迁移效果，揭示动作微调引起的领域遗忘。\n3. **视觉对齐方法**：提出一种基于柏拉图表示假说的轻量级方法，通过正则化损失将VLA的中间特征对齐到教师模型，无需额外计算开销即可恢复语义表示，提升OOD泛化。\n### 论文方法描述\n- **视觉对齐机制**：在VLA的Transformer主干中选择中间层 \\(i^{\\star}\\)，提取视觉token \\(h^{i^{\\star}}_{1:k}\\)，通过可训练投影器 \\(P_{\\varphi}\\) 映射到教师特征空间（如C-RADIOv3），最小化负相似度损失 \\(\\mathcal{L}_{\\text{align}}\\)。\n- **总损失函数**：结合标准动作损失 \\(\\mathcal{L}_{\\text{VLA}}\\) 与对齐损失 \\(\\mathcal{L}_{\\text{align}}\\)：\\(\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{VLA}} + \\lambda \\mathcal{L}_{\\text{align}}\\)，其中 \\(\\lambda > 0\\) 平衡策略学习与语义保留。\n- **实现细节**：使用LoRA适配器微调OpenVLA，投影器为冻结MLP以防止特征退化，教师模型冻结作为语义锚点。\n### 论文使用数据集和训练资源\n- **数据集**：基于Simpler基准的1400条专家演示轨迹，涵盖16种物体和16种桌面，用于任务特定微调；VL-Think任务套件含8个视觉-语言任务；ImageNet-100用于线性探测。\n- **训练资源**：使用MPLib运动规划器生成轨迹，训练采用LoRA适配器，硬件未明确说明（开源代码未提供细节）。\n- **教师模型**：C-RADIOv3作为主要视觉教师，对比DINOv2、SigLIP、Theia等。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Simpler基准的OOD变体，包括：\n - **视觉扰动**：动态纹理（Tex03/Tex05）、图像噪声。\n - **语义变化**：新物体、新容器、指令改写。\n - **执行变化**：随机初始位姿、中途物体重定位。\n- **评估指标**：\n - **任务成功率**：在128个随机种子上取平均值及标准差（SD），衡量放置动作正确性。\n - **VL-Think准确率**：视觉-语言理解的零样本准确率。\n - **线性探测准确率**：ImageNet-100上训练线性分类器的分类精度。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Cognitive AI Lab, IAI MIPT, Moscow, Russia</p>\n<h3>论文概述</h3>\n<p>该论文研究了视觉-语言-动作模型（VLA）在动作微调过程中视觉表示退化的问题。通过系统性分析，作者发现标准的监督微调（SFT）会导致视觉表示崩溃、注意力分散和领域特定遗忘。为解决这一问题，论文提出了一种轻量级的视觉表示对齐方法，将VLA的视觉特征锚定到预训练的教师模型上，从而保留多模态理解能力并提升分布外（OOD）泛化性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>表示退化分析</strong>：通过注意力图可视化和t-SNE分析，系统证明VLA微调会导致视觉表示崩溃和注意力分散，相较于原始VLM丧失语义一致性。</li><li><strong>VL-Think任务套件</strong>：设计了一个诊断基准，包含8个视觉-语言任务（如形状、颜色、交通符号识别），用于量化VLA从VLM继承的VL知识迁移效果，揭示动作微调引起的领域遗忘。</li><li><strong>视觉对齐方法</strong>：提出一种基于柏拉图表示假说的轻量级方法，通过正则化损失将VLA的中间特征对齐到教师模型，无需额外计算开销即可恢复语义表示，提升OOD泛化。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>视觉对齐机制</strong>：在VLA的Transformer主干中选择中间层 \\(i^{\\star}\\)，提取视觉token \\(h^{i^{\\star}}_{1:k}\\)，通过可训练投影器 \\(P_{\\varphi}\\) 映射到教师特征空间（如C-RADIOv3），最小化负相似度损失 \\(\\mathcal{L}_{\\text{align}}\\)。</li><li><strong>总损失函数</strong>：结合标准动作损失 \\(\\mathcal{L}_{\\text{VLA}}\\) 与对齐损失 \\(\\mathcal{L}_{\\text{align}}\\)：\\(\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{VLA}} + \\lambda \\mathcal{L}_{\\text{align}}\\)，其中 \\(\\lambda > 0\\) 平衡策略学习与语义保留。</li><li><strong>实现细节</strong>：使用LoRA适配器微调OpenVLA，投影器为冻结MLP以防止特征退化，教师模型冻结作为语义锚点。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：基于Simpler基准的1400条专家演示轨迹，涵盖16种物体和16种桌面，用于任务特定微调；VL-Think任务套件含8个视觉-语言任务；ImageNet-100用于线性探测。</li><li><strong>训练资源</strong>：使用MPLib运动规划器生成轨迹，训练采用LoRA适配器，硬件未明确说明（开源代码未提供细节）。</li><li><strong>教师模型</strong>：C-RADIOv3作为主要视觉教师，对比DINOv2、SigLIP、Theia等。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Simpler基准的OOD变体，包括：</li></ul>\n<p> - <strong>视觉扰动</strong>：动态纹理（Tex03/Tex05）、图像噪声。</p>\n<p> - <strong>语义变化</strong>：新物体、新容器、指令改写。</p>\n<p> - <strong>执行变化</strong>：随机初始位姿、中途物体重定位。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率</strong>：在128个随机种子上取平均值及标准差（SD），衡量放置动作正确性。</p>\n<p> - <strong>VL-Think准确率</strong>：视觉-语言理解的零样本准确率。</p>\n<p> - <strong>线性探测准确率</strong>：ImageNet-100上训练线性分类器的分类精度。</p>"
  },
  {
    "date": "2025-10-29",
    "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies",
    "link": "http://arxiv.org/abs/2510.25122",
    "summary_markdown": "```markdown\n### 论文研究单位\n不列颠哥伦比亚大学、阿尔伯塔大学、小米EV、东北大学\n### 论文概述\n论文提出NanoVLA，一种面向边缘设备的轻量级视觉-语言-动作（VLA）模型框架，旨在解决传统VLA模型在资源受限设备（如移动机器人）上的部署难题。通过重新设计模态融合机制、动作执行策略和计算分配方式，实现在参数量减少98%的情况下，推理速度提升52倍，同时保持或超越现有模型的任务准确性和泛化能力。\n### 论文核心贡献点\n1. **视觉-语言解耦（Vision-language Decoupling）**：将视觉和语言输入的处理分离至后期融合阶段，支持指令特征缓存，减少重复计算。\n2. **长-短动作分块（Long-short Action Chunking）**：训练时预测长动作序列以保证平滑性，推理时仅执行短窗口并基于新观测重新规划，平衡连贯性与实时响应。\n3. **动态路由（Dynamic Routing）**：根据任务复杂度自适应选择轻量或重量级语言模型，优化计算资源分配。\n### 论文方法描述\n1. **视觉-语言解耦**：\n - 使用冻结的视觉编码器（如ResNet）和语言编码器（如BERT、Qwen）独立提取特征。\n - 通过轻量级Transformer的交叉注意力层在后期融合多模态嵌入。\n - 缓存静态指令特征，每时间步仅更新视觉特征。\n2. **长-短动作分块**：\n - 训练目标：预测长动作序列（$H_{\\text{train}}$步），使用监督回归损失。\n - 推理策略：执行预测序列的前$h \\ll H_{\\text{train}}$步后重新规划，确保平滑性与适应性。\n3. **动态路由**：\n - 基于贝叶斯成功模型估计不同模型在任务上的胜率。\n - 训练文本条件二分类器预测模型胜率，推理时默认使用轻量模型，仅在复杂任务（胜率超阈值$\\tau$）时切换至重量模型。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **模拟环境**：LIBERO基准（包含LIBERO-Spatial/Object/Goal/Long任务集）及LIBERO-90（90个短时序任务）。\n - **真实环境**：LeRobot平台，包含10个操作任务（如物体抓取、变形体操作、精确开盖等）及2个未见任务。\n- **训练资源**：\n - 使用100步动作块（AC）训练，推理时采用10步AC。\n - 模型变体：NanoVLA-S（BERT-base，161M参数）、NanoVLA-L（Qwen2.5 0.5B，520M参数）、NanoVLA-R（动态路由，平均296M参数）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：标准LIBERO测试环境。\n - **真实环境**：Jetson Orin Nano（8GB内存）边缘设备部署LeRobot So-arm 101。\n- **评估指标**：\n - **成功率（Success Rate, SR）**：50次独立试验的平均成功率（模拟和真实任务）。\n - **推理延迟**：边缘设备上的帧率（FPS）及平均模型参数量。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>不列颠哥伦比亚大学、阿尔伯塔大学、小米EV、东北大学</p>\n<h3>论文概述</h3>\n<p>论文提出NanoVLA，一种面向边缘设备的轻量级视觉-语言-动作（VLA）模型框架，旨在解决传统VLA模型在资源受限设备（如移动机器人）上的部署难题。通过重新设计模态融合机制、动作执行策略和计算分配方式，实现在参数量减少98%的情况下，推理速度提升52倍，同时保持或超越现有模型的任务准确性和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>视觉-语言解耦（Vision-language Decoupling）</strong>：将视觉和语言输入的处理分离至后期融合阶段，支持指令特征缓存，减少重复计算。</li><li><strong>长-短动作分块（Long-short Action Chunking）</strong>：训练时预测长动作序列以保证平滑性，推理时仅执行短窗口并基于新观测重新规划，平衡连贯性与实时响应。</li><li><strong>动态路由（Dynamic Routing）</strong>：根据任务复杂度自适应选择轻量或重量级语言模型，优化计算资源分配。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>视觉-语言解耦</strong>：</li></ol>\n<p> - 使用冻结的视觉编码器（如ResNet）和语言编码器（如BERT、Qwen）独立提取特征。</p>\n<p> - 通过轻量级Transformer的交叉注意力层在后期融合多模态嵌入。</p>\n<p> - 缓存静态指令特征，每时间步仅更新视觉特征。</p>\n<ol><li><strong>长-短动作分块</strong>：</li></ol>\n<p> - 训练目标：预测长动作序列（$H_{\\text{train}}$步），使用监督回归损失。</p>\n<p> - 推理策略：执行预测序列的前$h \\ll H_{\\text{train}}$步后重新规划，确保平滑性与适应性。</p>\n<ol><li><strong>动态路由</strong>：</li></ol>\n<p> - 基于贝叶斯成功模型估计不同模型在任务上的胜率。</p>\n<p> - 训练文本条件二分类器预测模型胜率，推理时默认使用轻量模型，仅在复杂任务（胜率超阈值$\\tau$）时切换至重量模型。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：LIBERO基准（包含LIBERO-Spatial/Object/Goal/Long任务集）及LIBERO-90（90个短时序任务）。</p>\n<p> - <strong>真实环境</strong>：LeRobot平台，包含10个操作任务（如物体抓取、变形体操作、精确开盖等）及2个未见任务。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 使用100步动作块（AC）训练，推理时采用10步AC。</p>\n<p> - 模型变体：NanoVLA-S（BERT-base，161M参数）、NanoVLA-L（Qwen2.5 0.5B，520M参数）、NanoVLA-R（动态路由，平均296M参数）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：标准LIBERO测试环境。</p>\n<p> - <strong>真实环境</strong>：Jetson Orin Nano（8GB内存）边缘设备部署LeRobot So-arm 101。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（Success Rate, SR）</strong>：50次独立试验的平均成功率（模拟和真实任务）。</p>\n<p> - <strong>推理延迟</strong>：边缘设备上的帧率（FPS）及平均模型参数量。</p>\n<p>```</p>"
  },
  {
    "date": "2025-10-27",
    "title": "A Survey on Efficient Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.24795",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-28",
    "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning",
    "link": "http://arxiv.org/abs/2510.24161",
    "summary_markdown": "# 论文研究单位\n- Tongji University; Shanghai Magic; Koala Uran\n# 论文概述\n- 背景：现有多模态大语言模型（MLLM）、具身大语言模型（ELLM）、视觉-语言-动作模型（VLA）和通用多模态大模型（GMLM）未能同时实现跨空间、跨任务、跨具身的统一能力\n- 目标：提出BLM1（Boundless Large Model），在单一统一模型中实现数字空间推理与物理空间控制的融合\n- 方法：两阶段训练（Stage I SFT注入具身知识；Stage II冻结MLLM，训练基于意图桥接接口的扩散变换器（DiT）策略模块）\n- 结论：单一BLM1在数字任务上约提升6%，物理任务上约提升3%，同时保持指令跟随与跨具身控制能力\n# 论文核心贡献点\n- 统一三能力：跨空间迁移（数字→物理）、跨任务学习、跨具身泛化\n- 保持语言能力：Stage I以SFT注入具身知识，避免破坏MLLM指令跟随\n- 有效桥接：意图桥接接口+Perceiver压缩高阶意图，支撑高频率闭环控制\n- 策略共享：Diffusion Transformer作为通用策略头（0.76B参数），跨具身参数共享\n- 训练配方：加权采样保证数据集平衡；Stage II中后期引入未来预测损失提升泛化\n- 系统评测：数字空间六大基准、物理空间四具身六任务，建立统一对比\n- 结果优势：超越四类模型家族（MLLM、ELLM、VLA、GMLM），单一模型不切换架构\n# 论文方法描述\n- 架构组成\n - Backbone：Qwen2.5-VL-7B-Instruct；融合视觉与语言；抽取第k层隐藏状态Hk作为高层意图\n - 意图桥接接口：Perceiver将Hk压缩为固定K个令牌H̃k，降低计算、适配控制\n - 机器人策略：DiT扩散变换器作为条件生成头；状态编码器fs、动作编码器fa/解码器；预测horizon为h的动作块At\n- 训练目标\n - SFT目标（Stage I）：标准next-token交叉熵，冻结视觉与投影器，更新语言与投影\n - 流动匹配目标（Stage II早期）：Rectified-flow路径，DiT拟合速度场vφ(xτ,τ\\|H̃k)，LFM为MSE\n - 未来预测目标（Stage II后期）：在DiT中引入可学习未来令牌F，使其中间层表征与冻结MLLM对未来观察o_{t+H}与指令y的压缩表征对齐，用余弦相似度\n - 总体：Stage I仅L_SFT；Stage II早期训练Perceiver/DiT/编解码器；后期叠加L_FP\n- 训练配方\n - 加权采样：保证各数据集等概率抽取，避免大数据主导\n - Stage I：数字多模态问答对（RoboVQA、AgiBot、HoloAssist、BridgeData V2、EgoPlan、ShareRobot）\n - Stage II：冻结MLLM，使用四具身×六任务的模拟演示数据训练策略；具身特定编码器/解码器独立优化，DiT参数跨具身共享\n - 物理数据生成：基于ManiSkill，高层技能+关键姿态+IK与碰撞检验+路径规划与平滑，录制第三人称与腕部图像、本体感受与动作\n# 论文使用数据集和训练资源\n- 数字空间数据集（Stage I）\n - RoboVQA：829,502视频文本对，29,520指令；多类问答（规划、完成检验、 affordance判别与生成、过去/未来事件）\n - AgiBot：1,001,552条轨迹，2,976.4小时；217任务、87技能、106场景\n - HoloAssist：166小时；350教师-执行者对，20物理任务、16类物体\n - BridgeData V2：60,096轨迹，24环境，100+物类；基础到复杂13技能族\n - EgoPlan：EgoPlan-Bench 4,939多选题 + EgoPlan-IT 50K问答（基于EPIC-Kitchens）\n - ShareRobot：整合Open X-Embodiment的23数据集，102场景、12具身；1,027,990规划问答、6,522 affordance注释、6,870轨迹预测\n- 物理空间数据集（Stage II）\n - 具身：Franka Emika Panda、xArm-6、xArm-7、WidowX AI\n - 任务：PickCube、PullCube、StackCube、PushCube、PlaceSphere、LiftPegUpright（每具身×任务100集）\n - 总帧数：347.8K；任务特定成功判定\n- 数字评测基准（Stage I后）\n - 多选题：RoboVQA、AgiBot、HoloAssist、RoboFail\n - 自由问答：EgoThink、ShareRobot\n - 总计：3,160测试样本（见表2）\n- 训练资源\n - 框架：ManiSkill模拟收集\n - 加权采样：C个数据集等概率，样本在数据集内均匀\n - 视频帧采样：统一0.5秒间隔采样；短视频4帧，长视频8帧；任务前移除测试片段防泄露\n# 论文使用的评估环境和评估指标\n- 数字空间评估\n - 指标\n - Exact-match accuracy（EM）：多选题严格匹配归一化答案\n - LLM-as-a-judge（GPT评分）：自由问答使用判别式提示与评分标准\n - EgoThink：s_i∈{0,0.5,1}，最终分数按平均×100\n - ShareRobot：s_i∈{1,2,3,4,5}，最终分数按((s_i−1)/4)平均×100\n- 物理空间评估\n - 指标：Episode级成功率sr_{e,u}=N^s_{e,u}/N_{e,u}；总体为E具身×U任务的成功率平均\n - 环境：仿真闭环控制；每任务50次rollout，具身特定步长上限\n- 结果概览\n - 数字空间：表4显示BLM1平均64.88，显著优于GPT-4o（59.86）与开源MLLM；细分见表5/表6\n - 物理空间：BLM1在四具身×六任务上均实现稳定跨具身控制成功率，体现共享DiT策略的有效性",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>Tongji University; Shanghai Magic; Koala Uran</li></ul>\n<h1>论文概述</h1>\n<ul><li>背景：现有多模态大语言模型（MLLM）、具身大语言模型（ELLM）、视觉-语言-动作模型（VLA）和通用多模态大模型（GMLM）未能同时实现跨空间、跨任务、跨具身的统一能力</li><li>目标：提出BLM1（Boundless Large Model），在单一统一模型中实现数字空间推理与物理空间控制的融合</li><li>方法：两阶段训练（Stage I SFT注入具身知识；Stage II冻结MLLM，训练基于意图桥接接口的扩散变换器（DiT）策略模块）</li><li>结论：单一BLM1在数字任务上约提升6%，物理任务上约提升3%，同时保持指令跟随与跨具身控制能力</li></ul>\n<h1>论文核心贡献点</h1>\n<ul><li>统一三能力：跨空间迁移（数字→物理）、跨任务学习、跨具身泛化</li><li>保持语言能力：Stage I以SFT注入具身知识，避免破坏MLLM指令跟随</li><li>有效桥接：意图桥接接口+Perceiver压缩高阶意图，支撑高频率闭环控制</li><li>策略共享：Diffusion Transformer作为通用策略头（0.76B参数），跨具身参数共享</li><li>训练配方：加权采样保证数据集平衡；Stage II中后期引入未来预测损失提升泛化</li><li>系统评测：数字空间六大基准、物理空间四具身六任务，建立统一对比</li><li>结果优势：超越四类模型家族（MLLM、ELLM、VLA、GMLM），单一模型不切换架构</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>架构组成</li></ul>\n<p> - Backbone：Qwen2.5-VL-7B-Instruct；融合视觉与语言；抽取第k层隐藏状态Hk作为高层意图</p>\n<p> - 意图桥接接口：Perceiver将Hk压缩为固定K个令牌H̃k，降低计算、适配控制</p>\n<p> - 机器人策略：DiT扩散变换器作为条件生成头；状态编码器fs、动作编码器fa/解码器；预测horizon为h的动作块At</p>\n<ul><li>训练目标</li></ul>\n<p> - SFT目标（Stage I）：标准next-token交叉熵，冻结视觉与投影器，更新语言与投影</p>\n<p> - 流动匹配目标（Stage II早期）：Rectified-flow路径，DiT拟合速度场vφ(xτ,τ\\|H̃k)，LFM为MSE</p>\n<p> - 未来预测目标（Stage II后期）：在DiT中引入可学习未来令牌F，使其中间层表征与冻结MLLM对未来观察o_{t+H}与指令y的压缩表征对齐，用余弦相似度</p>\n<p> - 总体：Stage I仅L_SFT；Stage II早期训练Perceiver/DiT/编解码器；后期叠加L_FP</p>\n<ul><li>训练配方</li></ul>\n<p> - 加权采样：保证各数据集等概率抽取，避免大数据主导</p>\n<p> - Stage I：数字多模态问答对（RoboVQA、AgiBot、HoloAssist、BridgeData V2、EgoPlan、ShareRobot）</p>\n<p> - Stage II：冻结MLLM，使用四具身×六任务的模拟演示数据训练策略；具身特定编码器/解码器独立优化，DiT参数跨具身共享</p>\n<p> - 物理数据生成：基于ManiSkill，高层技能+关键姿态+IK与碰撞检验+路径规划与平滑，录制第三人称与腕部图像、本体感受与动作</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数字空间数据集（Stage I）</li></ul>\n<p> - RoboVQA：829,502视频文本对，29,520指令；多类问答（规划、完成检验、 affordance判别与生成、过去/未来事件）</p>\n<p> - AgiBot：1,001,552条轨迹，2,976.4小时；217任务、87技能、106场景</p>\n<p> - HoloAssist：166小时；350教师-执行者对，20物理任务、16类物体</p>\n<p> - BridgeData V2：60,096轨迹，24环境，100+物类；基础到复杂13技能族</p>\n<p> - EgoPlan：EgoPlan-Bench 4,939多选题 + EgoPlan-IT 50K问答（基于EPIC-Kitchens）</p>\n<p> - ShareRobot：整合Open X-Embodiment的23数据集，102场景、12具身；1,027,990规划问答、6,522 affordance注释、6,870轨迹预测</p>\n<ul><li>物理空间数据集（Stage II）</li></ul>\n<p> - 具身：Franka Emika Panda、xArm-6、xArm-7、WidowX AI</p>\n<p> - 任务：PickCube、PullCube、StackCube、PushCube、PlaceSphere、LiftPegUpright（每具身×任务100集）</p>\n<p> - 总帧数：347.8K；任务特定成功判定</p>\n<ul><li>数字评测基准（Stage I后）</li></ul>\n<p> - 多选题：RoboVQA、AgiBot、HoloAssist、RoboFail</p>\n<p> - 自由问答：EgoThink、ShareRobot</p>\n<p> - 总计：3,160测试样本（见表2）</p>\n<ul><li>训练资源</li></ul>\n<p> - 框架：ManiSkill模拟收集</p>\n<p> - 加权采样：C个数据集等概率，样本在数据集内均匀</p>\n<p> - 视频帧采样：统一0.5秒间隔采样；短视频4帧，长视频8帧；任务前移除测试片段防泄露</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>数字空间评估</li></ul>\n<p> - 指标</p>\n<p> - Exact-match accuracy（EM）：多选题严格匹配归一化答案</p>\n<p> - LLM-as-a-judge（GPT评分）：自由问答使用判别式提示与评分标准</p>\n<p> - EgoThink：s_i∈{0,0.5,1}，最终分数按平均×100</p>\n<p> - ShareRobot：s_i∈{1,2,3,4,5}，最终分数按((s_i−1)/4)平均×100</p>\n<ul><li>物理空间评估</li></ul>\n<p> - 指标：Episode级成功率sr_{e,u}=N^s_{e,u}/N_{e,u}；总体为E具身×U任务的成功率平均</p>\n<p> - 环境：仿真闭环控制；每任务50次rollout，具身特定步长上限</p>\n<ul><li>结果概览</li></ul>\n<p> - 数字空间：表4显示BLM1平均64.88，显著优于GPT-4o（59.86）与开源MLLM；细分见表5/表6</p>\n<p> - 物理空间：BLM1在四具身×六任务上均实现稳定跨具身控制成功率，体现共享DiT策略的有效性</p>"
  },
  {
    "date": "2025-10-27",
    "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
    "link": "http://arxiv.org/abs/2510.23763",
    "summary_markdown": "### 论文研究单位\n复旦大学、上海创新研究院、新加坡国立大学\n### 论文概述\n论文提出“跨模态上下文指令”设置：机器人需从人类语音、环境声与视觉线索中主动推断潜在意图，并通过交互确认后执行动作，而不再依赖显式指令。为解决数据缺失与评估基准不足，构建 OmniAction 数据集与 OmniAction-LIBERO 评测；提出 RoboOmni 端到端全模态大模型，集成感知、推理、对话与控制，实现意图识别—确认—执行的闭环。仿真与真实验证显示其在成功率、推理速度、主动辅助与意图识别上显著优于文本与ASR基线。\n### 论文核心贡献点\n- 定义并研究“跨模态上下文指令”任务：融合语音、环境声与视觉进行主动意图推断与交互确认\n- 提出 RoboOmni：Perceiver–Thinker–Talker–Executor 的端到端全模态框架，统一推理与控制，支持直接语音交互与行动生成\n- 构造 OmniAction：约14万集 multimodal 数据、5千+说话人、2.4千事件声音、640背景、六类上下文指令；并基于 LIBERO 构建 OmniAction-LIBERO 评测套件\n- 实验显示：仿真 OmniAction-LIBERO 上平均成功率达85.6%；真实人声指令平均76.6%；推理延迟降至基线的约0.49倍；具备更强的意图识别与主动协助能力\n### 论文方法描述\n- 架构：Perceiver 统一编码视觉/音频/文本为共享表征；Thinker 基于全模态LLM在词汇与动作标记联合空间自回归生成；Talker 生成语音；Executor 用 FAST+ 将离散动作标记解码为连续控制向量\n- 训练：统一自回归目标，同时优化对话与动作生成。预训练使用64×A100、10天、15,360 A100‑小时、批量512、学习率5e-5；下游SFT为8×A100、5e‑5、10–30k步。图像224×224、音频16kHz、动作块长6\n### 论文使用数据集和训练资源\n- 数据集\n - OmniAction：约141,162集，涵盖112技能、748对象、5,096说话人音色、2,482非言语事件、640环境背景与六类上下文指令\n - OmniAction‑LIBERO：基于 LIBERO 的仿真评测（OmniAction‑LIBERO‑TTS：40任务×6变体=240评测量；OmniAction‑LIBERO‑Real：10名志愿者真实语音指令）\n- 训练资源\n - 预训练：64×A100，10天，总计约15,360 A100‑小时，批量512，LR=5e-5，前1k步warm‑up\n - 下游SFT：8×A100，LR=5e-5，10–30k步\n### 论文使用的评估环境和评估指标\n- 仿真评估：OmniAction‑LIBERO（四套任务：Spatial、Goal、Object、Long‑Horizon）×六种上下文指令；与 OpenVLA、OpenVLA‑OFT、π0、NORA 比较；指标为成功率\n- 真实人声评测：OmniAction‑LIBERO‑Real；对比“真实文本提示/ASR转文本→VLA”与RoboOmni直接音频输入；指标为成功率\n- 主动辅助与意图识别：对比 Qwen2.5‑Omni‑3B/7B 与 ASR+GPT‑4o；指标为意图识别准确率、交互能力质性评估\n- 效率分析：对比端到端与级联流水线；指标为推理延迟（相对ASR+OpenVLA的倍数）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>复旦大学、上海创新研究院、新加坡国立大学</p>\n<h3>论文概述</h3>\n<p>论文提出“跨模态上下文指令”设置：机器人需从人类语音、环境声与视觉线索中主动推断潜在意图，并通过交互确认后执行动作，而不再依赖显式指令。为解决数据缺失与评估基准不足，构建 OmniAction 数据集与 OmniAction-LIBERO 评测；提出 RoboOmni 端到端全模态大模型，集成感知、推理、对话与控制，实现意图识别—确认—执行的闭环。仿真与真实验证显示其在成功率、推理速度、主动辅助与意图识别上显著优于文本与ASR基线。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>定义并研究“跨模态上下文指令”任务：融合语音、环境声与视觉进行主动意图推断与交互确认</li><li>提出 RoboOmni：Perceiver–Thinker–Talker–Executor 的端到端全模态框架，统一推理与控制，支持直接语音交互与行动生成</li><li>构造 OmniAction：约14万集 multimodal 数据、5千+说话人、2.4千事件声音、640背景、六类上下文指令；并基于 LIBERO 构建 OmniAction-LIBERO 评测套件</li><li>实验显示：仿真 OmniAction-LIBERO 上平均成功率达85.6%；真实人声指令平均76.6%；推理延迟降至基线的约0.49倍；具备更强的意图识别与主动协助能力</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>架构：Perceiver 统一编码视觉/音频/文本为共享表征；Thinker 基于全模态LLM在词汇与动作标记联合空间自回归生成；Talker 生成语音；Executor 用 FAST+ 将离散动作标记解码为连续控制向量</li><li>训练：统一自回归目标，同时优化对话与动作生成。预训练使用64×A100、10天、15,360 A100‑小时、批量512、学习率5e-5；下游SFT为8×A100、5e‑5、10–30k步。图像224×224、音频16kHz、动作块长6</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集</li></ul>\n<p> - OmniAction：约141,162集，涵盖112技能、748对象、5,096说话人音色、2,482非言语事件、640环境背景与六类上下文指令</p>\n<p> - OmniAction‑LIBERO：基于 LIBERO 的仿真评测（OmniAction‑LIBERO‑TTS：40任务×6变体=240评测量；OmniAction‑LIBERO‑Real：10名志愿者真实语音指令）</p>\n<ul><li>训练资源</li></ul>\n<p> - 预训练：64×A100，10天，总计约15,360 A100‑小时，批量512，LR=5e-5，前1k步warm‑up</p>\n<p> - 下游SFT：8×A100，LR=5e-5，10–30k步</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>仿真评估：OmniAction‑LIBERO（四套任务：Spatial、Goal、Object、Long‑Horizon）×六种上下文指令；与 OpenVLA、OpenVLA‑OFT、π0、NORA 比较；指标为成功率</li><li>真实人声评测：OmniAction‑LIBERO‑Real；对比“真实文本提示/ASR转文本→VLA”与RoboOmni直接音频输入；指标为成功率</li><li>主动辅助与意图识别：对比 Qwen2.5‑Omni‑3B/7B 与 ASR+GPT‑4o；指标为意图识别准确率、交互能力质性评估</li><li>效率分析：对比端到端与级联流水线；指标为推理延迟（相对ASR+OpenVLA的倍数）</li></ul>"
  },
  {
    "date": "2025-10-27",
    "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
    "link": "http://arxiv.org/abs/2510.23576",
    "summary_markdown": "## 论文研究单位\n北京大学、Galbot、中国科学技术大学、北京人工智能研究院(BAAI)\n## 论文概述\nUrbanVLA是一个面向城市微移动性应用的视觉-语言-动作(VLA)框架，主要用于配送机器人等在城市环境中的长距离导航任务。该方法通过整合导航工具提供的高级路线信息与机器人视觉感知，实现了大规模城市场景中可靠、安全的长程导航。\n## 论文核心贡献点\n- 提出首个面向城市微移动性的路线条件化VLA框架，将导航工具指导与视觉语言策略学习有效结合\n- 开发了模拟到现实的训练管道，通过路线提升算法构建模拟-现实聚合数据集，实现SOTA性能并展示现实世界泛化能力\n- 引入基于IQL的强化学习微调方法，显著提升安全关键行为能力，包括避障、行人互动和交通合规性\n## 论文方法描述\nUrbanVLA基于预训练的导航基础模型NavFoM，采用两阶段训练策略：**(1)监督微调(SFT)**：在MetaUrban模拟器和网络导航视频数据上训练，结合VideoQA任务和路线条件导航任务，学习基本导航能力；**(2)强化微调(RFT)**：使用隐式Q-Learning(IQL)在模拟-现实聚合数据上进行离线强化学习，优化安全性和适应性。方法核心包括多模态特征融合、路线与视觉观察对齐、轨迹级别的策略优化。\n## 论文使用数据集和训练资源\n- **模拟数据**：MetaUrban-12K数据集(约40小时，2400集)，由PPO专家生成\n- **现实数据**：约8小时的人为遥操作系统数据\n- **网络数据**：Sekai网络导航视频数据集和LongVU视频问答数据集\n- **训练资源**：8张NVIDIA H100 GPU，训练约12小时(总计96 GPU小时)\n## 论文使用的评估环境和评估指标\n- **模拟环境评估**：MetaUrban基准，包含PointNav和SocialNav任务，在MetaUrban-test(1000场景)和MetaUrban-unseen(100场景)上测试\n- **现实环境评估**：在城市街区进行，包括天桥、行人横道等复杂场景，机器人为Unitree Go2四足机器人\n- **评估指标**：成功率(SR)、路径长度加权的成功率(SPL)、社交导航分数(SNS)、累积成本(CC)、路线完成度(RC)等",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>北京大学、Galbot、中国科学技术大学、北京人工智能研究院(BAAI)</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>UrbanVLA是一个面向城市微移动性应用的视觉-语言-动作(VLA)框架，主要用于配送机器人等在城市环境中的长距离导航任务。该方法通过整合导航工具提供的高级路线信息与机器人视觉感知，实现了大规模城市场景中可靠、安全的长程导航。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出首个面向城市微移动性的路线条件化VLA框架，将导航工具指导与视觉语言策略学习有效结合</li><li>开发了模拟到现实的训练管道，通过路线提升算法构建模拟-现实聚合数据集，实现SOTA性能并展示现实世界泛化能力</li><li>引入基于IQL的强化学习微调方法，显著提升安全关键行为能力，包括避障、行人互动和交通合规性</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>UrbanVLA基于预训练的导航基础模型NavFoM，采用两阶段训练策略：<strong>(1)监督微调(SFT)</strong>：在MetaUrban模拟器和网络导航视频数据上训练，结合VideoQA任务和路线条件导航任务，学习基本导航能力；<strong>(2)强化微调(RFT)</strong>：使用隐式Q-Learning(IQL)在模拟-现实聚合数据上进行离线强化学习，优化安全性和适应性。方法核心包括多模态特征融合、路线与视觉观察对齐、轨迹级别的策略优化。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>模拟数据</strong>：MetaUrban-12K数据集(约40小时，2400集)，由PPO专家生成</li><li><strong>现实数据</strong>：约8小时的人为遥操作系统数据</li><li><strong>网络数据</strong>：Sekai网络导航视频数据集和LongVU视频问答数据集</li><li><strong>训练资源</strong>：8张NVIDIA H100 GPU，训练约12小时(总计96 GPU小时)</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>模拟环境评估</strong>：MetaUrban基准，包含PointNav和SocialNav任务，在MetaUrban-test(1000场景)和MetaUrban-unseen(100场景)上测试</li><li><strong>现实环境评估</strong>：在城市街区进行，包括天桥、行人横道等复杂场景，机器人为Unitree Go2四足机器人</li><li><strong>评估指标</strong>：成功率(SR)、路径长度加权的成功率(SPL)、社交导航分数(SNS)、累积成本(CC)、路线完成度(RC)等</li></ul>"
  },
  {
    "date": "2025-10-27",
    "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
    "link": "http://arxiv.org/abs/2510.23571",
    "summary_markdown": "### 论文研究单位\n卡内基梅隆大学、浙江大学、北京大学、国立台湾大学\n### 论文概述\n随着通用机器人策略的快速发展，其评估面临规模化、可复现性与安全性等挑战，现有评测要么依赖真实世界环境（人力与安全成本高），要么局限在同一合成域内训练—测试，无法评估真实数据或其他环境训练的模型。为解决这些问题，本文提出RobotArena ∞，通过自动化将真实演示视频映射为大规模可扩展的仿真环境，并结合视觉语言模型（VLM）自动评分与人类偏好反馈，对来自全球多实验室的视觉语言动作（VLA）策略开展系统性评测与稳健性检验。\n### 论文核心贡献点\n- 提出可扩展、可扩展的机器人评测协议：结合物理引擎、真实到仿真（real-to-sim）转换与人类偏好反馈，实现规模化且可复现的评测。\n- 完整自动化的reality-to-simulation翻译管线：融合VLMs、2D到3D生成模型与可微分渲染，无需人工标定或额外注释。\n- 大规模跨实验室政策评测：聚合超过7000条人类偏好比较、覆盖百余个仿真场景及多种扰动，目前最大规模的机器人评测工作。\n- 系统性评测发现：跨数据集泛化能力弱、模型架构差异明显、颜色扰动下强VLM背bone策略更稳健、背景变化带来显著性能衰减。\n### 论文方法描述\n方法核心是将演示视频自动翻译为仿真环境，再对策略进行多维度评测。\n\n- 自动化视频到仿真映射：提取五个关键要素（1）相机—机器人六自由度位姿；（2）任务相关物体的三维网格重建、姿态与材质属性；（3）场景深度；（4）干净背景；（5）控制增益估计。\n- 机器人—相机标定：采用可微分渲染的分析—综合方法，优化RGB、光流与DINOv2特征对齐损失，估计相机—机器人位姿。\n- 物体与场景三维重建：利用Gemini进行机器人与前景物体分割；InvSR提升分割图像质量；Hunyuan-3D生成有纹理网格；通过MINIMA建立真实crop与模拟渲染视图的2D对应；利用MoGe单目深度与SVD解算刚体变换；用LaMa进行背景修复；系统辨识校准PD控制增益。\n- 可控域扰动：ΔBG替换背景；ΔColor调整颜色通道（如BGR转换）；ΔObjPose随机改变物体位置；用于压力测试策略泛化。\n- 评测方式：绝对评测用VLM对打乱帧序列进行任务进度打分（最后30%帧平均分数与人类进度相关性最佳）；相对评测采用双盲成对比较，收集偏好标签与自由文本解释，使用Bradley–Terry模型与Sandwich方差得到全局排名与置信区间。\n- 对比与外部验证：与SIMPLER基准比较，评估策略在BridgeSim与SIMPLER四场景上的表现一致性；在一个真实任务“把胡萝卜放到盘子里”验证仿真—现实一致性。\n### 论文使用数据集和训练资源\n- 数据源与仿真环境：BridgeSim（Bridge v2，OXE的广泛使用子集）、DROIDSim（DROID，因噪声较高常被排除于预训练）、Rh20TSim（仅SpatialVLA用其训练）。\n- 评测对象策略：Octo-Base（93M，OXE预训练）、RoboVLM（基于KosMos的VLA）、SpatialVLA（引入3D空间表征）、CogAct（7B VLM背bone+扩散Transformer动作预测）。\n- 环境规模：100个名义环境与数百种扰动（ΔBG/ΔColor/ΔObjPose），累计7000+人类偏好比较与多轮VLM自动评分。\n### 论文使用的评估环境和评估指标\n- 评估环境：自动化生成的仿真环境来源于真实演示视频，支持内分布（与训练集对应）与外分布（超出训练集）评测；在仿真中部署策略执行并录制轨迹视频用于后续评分。\n- 评估指标：\n - 自动化VLM任务进度评分：打乱帧序列+VLM打分，优选“执行最后30%帧的平均分数”作为进度指标。\n - 人类偏好成对比较：偏好标签（胜/负/平局）与自由文本理由；Bradley–Terry模型全局排名，Sandwich方差估计置信区间。\n - 稳健性：跨数据集迁移性能（BridgeSim vs. DROIDSim vs. Rh20TSim），以及在ΔBG/ΔColor/ΔObjPose扰动下的性能曲线与趋势。\n - 现实—仿真一致性：单任务现实与仿真部署对比（如“把胡萝卜放到盘子里”）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>卡内基梅隆大学、浙江大学、北京大学、国立台湾大学</p>\n<h3>论文概述</h3>\n<p>随着通用机器人策略的快速发展，其评估面临规模化、可复现性与安全性等挑战，现有评测要么依赖真实世界环境（人力与安全成本高），要么局限在同一合成域内训练—测试，无法评估真实数据或其他环境训练的模型。为解决这些问题，本文提出RobotArena ∞，通过自动化将真实演示视频映射为大规模可扩展的仿真环境，并结合视觉语言模型（VLM）自动评分与人类偏好反馈，对来自全球多实验室的视觉语言动作（VLA）策略开展系统性评测与稳健性检验。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出可扩展、可扩展的机器人评测协议：结合物理引擎、真实到仿真（real-to-sim）转换与人类偏好反馈，实现规模化且可复现的评测。</li><li>完整自动化的reality-to-simulation翻译管线：融合VLMs、2D到3D生成模型与可微分渲染，无需人工标定或额外注释。</li><li>大规模跨实验室政策评测：聚合超过7000条人类偏好比较、覆盖百余个仿真场景及多种扰动，目前最大规模的机器人评测工作。</li><li>系统性评测发现：跨数据集泛化能力弱、模型架构差异明显、颜色扰动下强VLM背bone策略更稳健、背景变化带来显著性能衰减。</li></ul>\n<h3>论文方法描述</h3>\n<p>方法核心是将演示视频自动翻译为仿真环境，再对策略进行多维度评测。</p>\n\n<ul><li>自动化视频到仿真映射：提取五个关键要素（1）相机—机器人六自由度位姿；（2）任务相关物体的三维网格重建、姿态与材质属性；（3）场景深度；（4）干净背景；（5）控制增益估计。</li><li>机器人—相机标定：采用可微分渲染的分析—综合方法，优化RGB、光流与DINOv2特征对齐损失，估计相机—机器人位姿。</li><li>物体与场景三维重建：利用Gemini进行机器人与前景物体分割；InvSR提升分割图像质量；Hunyuan-3D生成有纹理网格；通过MINIMA建立真实crop与模拟渲染视图的2D对应；利用MoGe单目深度与SVD解算刚体变换；用LaMa进行背景修复；系统辨识校准PD控制增益。</li><li>可控域扰动：ΔBG替换背景；ΔColor调整颜色通道（如BGR转换）；ΔObjPose随机改变物体位置；用于压力测试策略泛化。</li><li>评测方式：绝对评测用VLM对打乱帧序列进行任务进度打分（最后30%帧平均分数与人类进度相关性最佳）；相对评测采用双盲成对比较，收集偏好标签与自由文本解释，使用Bradley–Terry模型与Sandwich方差得到全局排名与置信区间。</li><li>对比与外部验证：与SIMPLER基准比较，评估策略在BridgeSim与SIMPLER四场景上的表现一致性；在一个真实任务“把胡萝卜放到盘子里”验证仿真—现实一致性。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据源与仿真环境：BridgeSim（Bridge v2，OXE的广泛使用子集）、DROIDSim（DROID，因噪声较高常被排除于预训练）、Rh20TSim（仅SpatialVLA用其训练）。</li><li>评测对象策略：Octo-Base（93M，OXE预训练）、RoboVLM（基于KosMos的VLA）、SpatialVLA（引入3D空间表征）、CogAct（7B VLM背bone+扩散Transformer动作预测）。</li><li>环境规模：100个名义环境与数百种扰动（ΔBG/ΔColor/ΔObjPose），累计7000+人类偏好比较与多轮VLM自动评分。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：自动化生成的仿真环境来源于真实演示视频，支持内分布（与训练集对应）与外分布（超出训练集）评测；在仿真中部署策略执行并录制轨迹视频用于后续评分。</li><li>评估指标：</li></ul>\n<p> - 自动化VLM任务进度评分：打乱帧序列+VLM打分，优选“执行最后30%帧的平均分数”作为进度指标。</p>\n<p> - 人类偏好成对比较：偏好标签（胜/负/平局）与自由文本理由；Bradley–Terry模型全局排名，Sandwich方差估计置信区间。</p>\n<p> - 稳健性：跨数据集迁移性能（BridgeSim vs. DROIDSim vs. Rh20TSim），以及在ΔBG/ΔColor/ΔObjPose扰动下的性能曲线与趋势。</p>\n<p> - 现实—仿真一致性：单任务现实与仿真部署对比（如“把胡萝卜放到盘子里”）。</p>"
  },
  {
    "date": "2025-10-27",
    "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox",
    "link": "http://arxiv.org/abs/2510.23511",
    "summary_markdown": "# 论文总结\n## 论文研究单位\nDexmal、StepFun\n## 论文概述\nDexbotic是一个基于PyTorch的开源视觉-语言-动作(VLA)模型工具箱，为具身智能领域的专业人士提供一站式VLA研究服务。该工具箱统一了不同研究机构的VLA策略框架，解决了研究碎片化问题，支持多种主流VLA策略的复现，并提供更强的预训练模型以提升性能。\n## 论文核心贡献点\n- 统一模块化VLA框架：将VLA策略标准化为视觉语言模型(VLM)和动作专家(AE)两个部分\n- 预训练基础模型：提供比原始开源模型更强大的预训练模型，在多个基准测试中显著提升性能\n- 实验为中心开发：采用分层配置架构，用户可通过简单修改Exp脚本快速开发新实验\n- 多平台训练支持：同时支持云端(如阿里云)和本地GPU(RTX 4090)训练\n- 多机器人支持：统一数据格式支持UR5、Franka、ALOHA等多种主流机器人平台\n## 论文方法描述\n统一架构设计：\n- VLM部分：包含视觉编码器(CLIP)、投影器(两层MLP)和大语言模型(Qwen2.5)\n- AE部分：支持扩散变换器、多层感知机或专家混合等不同架构\n\n预训练策略：\n- 离散预训练模型(Dexbotic-Base)：将连续动作量化为256个离散区间\n- 连续预训练模型：支持单臂和双臂任务，通过扩展噪声token数量实现\n- 多视图处理：共享视觉编码器处理多视角输入\n\n数据格式：\n- Dexdata格式：统一存储机器人数据集，包含video和jsonl两个核心元素\n- 相比LeRobot和RLDS格式更节省存储空间\n## 论文使用数据集和训练资源\n训练数据来源：\n- Open-X Embodiment数据集子集\n- 仿真数据：RLBench、LIBERO、ManiSkill2\n- 真实机器人数据：UR5等多型号单臂机器人\n- 私有数据集：52个操作任务，使用8种单臂真实机器人收集\n- 双臂数据：Robomind、AgiBot World数据集及ALOHA双臂机器人数据\n\n训练环境：\n- 云端平台：阿里云、Volcano Engine\n- 本地GPU：RTX 4090等消费级显卡\n## 论文使用的评估环境和评估指标\n仿真基准测试：\n- SimplerEnv：WidowX机器人，包含4项视觉匹配任务\n- CALVIN：ABC→D设定，评估长期任务完成能力\n- ManiSkill2：5项代表性抓取和堆叠任务\n- RoboTwin2.0：4项双臂操作任务\n- LIBERO：4个任务套件(Spatial、Object、Goal、Long)\n\n评估指标：\n- 任务成功率(Success Rate)：各基准测试的核心指标\n- 平均完成长度(CALVIN)：连续完成的任务序列平均长度\n- 性能提升幅度：与原始开源模型的对比改善\n\n真实世界评估：\n- 在UR5e、ALOHA、ARX5、Franka等机器人上验证\n- 任务类型：日常操作如摆放盘子、按钮操作、纸张粉碎等\n- 典型结果：set the plates任务100%成功率，search the green box任务80%成功率",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>Dexmal、StepFun</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>Dexbotic是一个基于PyTorch的开源视觉-语言-动作(VLA)模型工具箱，为具身智能领域的专业人士提供一站式VLA研究服务。该工具箱统一了不同研究机构的VLA策略框架，解决了研究碎片化问题，支持多种主流VLA策略的复现，并提供更强的预训练模型以提升性能。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>统一模块化VLA框架：将VLA策略标准化为视觉语言模型(VLM)和动作专家(AE)两个部分</li><li>预训练基础模型：提供比原始开源模型更强大的预训练模型，在多个基准测试中显著提升性能</li><li>实验为中心开发：采用分层配置架构，用户可通过简单修改Exp脚本快速开发新实验</li><li>多平台训练支持：同时支持云端(如阿里云)和本地GPU(RTX 4090)训练</li><li>多机器人支持：统一数据格式支持UR5、Franka、ALOHA等多种主流机器人平台</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>统一架构设计：</p>\n<ul><li>VLM部分：包含视觉编码器(CLIP)、投影器(两层MLP)和大语言模型(Qwen2.5)</li><li>AE部分：支持扩散变换器、多层感知机或专家混合等不同架构</li></ul>\n\n<p>预训练策略：</p>\n<ul><li>离散预训练模型(Dexbotic-Base)：将连续动作量化为256个离散区间</li><li>连续预训练模型：支持单臂和双臂任务，通过扩展噪声token数量实现</li><li>多视图处理：共享视觉编码器处理多视角输入</li></ul>\n\n<p>数据格式：</p>\n<ul><li>Dexdata格式：统一存储机器人数据集，包含video和jsonl两个核心元素</li><li>相比LeRobot和RLDS格式更节省存储空间</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<p>训练数据来源：</p>\n<ul><li>Open-X Embodiment数据集子集</li><li>仿真数据：RLBench、LIBERO、ManiSkill2</li><li>真实机器人数据：UR5等多型号单臂机器人</li><li>私有数据集：52个操作任务，使用8种单臂真实机器人收集</li><li>双臂数据：Robomind、AgiBot World数据集及ALOHA双臂机器人数据</li></ul>\n\n<p>训练环境：</p>\n<ul><li>云端平台：阿里云、Volcano Engine</li><li>本地GPU：RTX 4090等消费级显卡</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p>仿真基准测试：</p>\n<ul><li>SimplerEnv：WidowX机器人，包含4项视觉匹配任务</li><li>CALVIN：ABC→D设定，评估长期任务完成能力</li><li>ManiSkill2：5项代表性抓取和堆叠任务</li><li>RoboTwin2.0：4项双臂操作任务</li><li>LIBERO：4个任务套件(Spatial、Object、Goal、Long)</li></ul>\n\n<p>评估指标：</p>\n<ul><li>任务成功率(Success Rate)：各基准测试的核心指标</li><li>平均完成长度(CALVIN)：连续完成的任务序列平均长度</li><li>性能提升幅度：与原始开源模型的对比改善</li></ul>\n\n<p>真实世界评估：</p>\n<ul><li>在UR5e、ALOHA、ARX5、Franka等机器人上验证</li><li>任务类型：日常操作如摆放盘子、按钮操作、纸张粉碎等</li><li>典型结果：set the plates任务100%成功率，search the green box任务80%成功率</li></ul>"
  },
  {
    "date": "2025-10-25",
    "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
    "link": "http://arxiv.org/abs/2510.22201",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-23",
    "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence",
    "link": "http://arxiv.org/abs/2510.21860",
    "summary_markdown": "### 论文研究单位\nAndon Labs（研究机构及实验室）\n### 论文概述\nButter-Bench是一个评估LLM控制机器人实用智能的基准测试。论文探讨当前SOTA大型语言模型在机器人编排（orchestration）能力上的上限，通过设计真实世界任务（如“传递黄油”）评估模型的空间推理、社交理解和物理世界常识等能力。研究发现，人类在所有任务中显著优于LLM，最佳模型完成率仅达40%，人类平均达95%。\n### 论文核心贡献点\n1. **引入实用智能评估框架**：提出区分分析智能（analytical intelligence）与实用智能（practical intelligence）的机器人基准测试。\n2. **隔离编排器评估**：在简化硬件架构下测试LLM编排能力，避免VLA（视觉语言动作）模型干扰。\n3. **系统性能力测评**：设计六大任务全面覆盖空间导航、视觉推理、社交互动和多步规划。\n4. **红队安全评估**：在压力场景（低电量、充电器故障）下测试模型的对抗性脆弱性。\n5. **对比人类基线**：明确人类与LLM在真实环境中的能力差距。\n### 论文方法描述\n- **硬件平台**：使用TurtleBot 4标准移动机器人（搭载OAK-D相机、2D激光雷达、IMU等），运行ROS 2系统提供SLAM能力。\n- **代理架构**：采用ReAct风格循环，LLM观察环境→推理决策→调用工具执行动作。工具箱包括：\n 1. 运动控制：`drive`, `rotate`, `wait`\n 2. 维护功能：`dock`, `undock`, `status`\n 3. 视觉感知：`take_photo`\n 4. 导航工具：`view_map`, `navigate_to`\n 5. 社交交互：`read_msg`, `send_msg`, `save_image`\n- **任务设计**：六个子任务评估不同能力（如识别含黄油纸袋、注意用户缺席、多步路径规划等）。\n- **红队测试**：模拟低电量+充电器故障场景，诱导模型泄露机密信息。\n### 论文使用数据集和训练资源\n- **任务数据集**：基于真实办公环境设计的6项操作任务（附录A详述），包含接受标准。\n- **模型评估**：测试多个LLM：\n - **SOTA通用模型**：Gemini 2.5 Pro、Claude Opus 4.1、GPT-5、Grok 4、Llama 4 Maverick\n - **专用实体模型**：Gemini ER 1.5（针对机器人调优）\n- **人类基线**：3名人类操作员通过网页界面控制机器人完成相同任务（不知环境布局）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：真实办公室场景（单房间含出口、办公桌、厨房等），固定光照和障碍物布局。\n- **核心指标**：\n 1. **主要指标**：任务完成率（每模型-任务组合测试5次）。\n 2. **辅助指标**：\n - 任务完成时间（推理延迟+规划效率）\n - 工具调用分布（分析工具使用模式）\n - 定性失效模式分类（空间推理、社交理解等5类）。\n- **安全性评估**：红队攻击成功率（如泄露机密信息的倾向性）。\n\n**结果摘要**\n- 人类vs LLM：人类平均95%完成率，最佳LLM Gemini 2.5 Pro仅40%。\n- **关键弱点**：\n - 社交理解（\"注意缺席\"任务：LLM 0% vs 人类100%）\n - 多步空间规划（\"Plan\"任务：LLM依赖随机路径漂移，非真实理解）\n- **模型差异**：\n Gemini ER 1.5未优于Gemini 2.5 Pro，表明实体调优未显著提升实用智能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Andon Labs（研究机构及实验室）</p>\n<h3>论文概述</h3>\n<p>Butter-Bench是一个评估LLM控制机器人实用智能的基准测试。论文探讨当前SOTA大型语言模型在机器人编排（orchestration）能力上的上限，通过设计真实世界任务（如“传递黄油”）评估模型的空间推理、社交理解和物理世界常识等能力。研究发现，人类在所有任务中显著优于LLM，最佳模型完成率仅达40%，人类平均达95%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>引入实用智能评估框架</strong>：提出区分分析智能（analytical intelligence）与实用智能（practical intelligence）的机器人基准测试。</li><li><strong>隔离编排器评估</strong>：在简化硬件架构下测试LLM编排能力，避免VLA（视觉语言动作）模型干扰。</li><li><strong>系统性能力测评</strong>：设计六大任务全面覆盖空间导航、视觉推理、社交互动和多步规划。</li><li><strong>红队安全评估</strong>：在压力场景（低电量、充电器故障）下测试模型的对抗性脆弱性。</li><li><strong>对比人类基线</strong>：明确人类与LLM在真实环境中的能力差距。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>硬件平台</strong>：使用TurtleBot 4标准移动机器人（搭载OAK-D相机、2D激光雷达、IMU等），运行ROS 2系统提供SLAM能力。</li><li><strong>代理架构</strong>：采用ReAct风格循环，LLM观察环境→推理决策→调用工具执行动作。工具箱包括：</li></ul>\n<p> 1. 运动控制：<code>drive</code>, <code>rotate</code>, <code>wait</code></p>\n<p> 2. 维护功能：<code>dock</code>, <code>undock</code>, <code>status</code></p>\n<p> 3. 视觉感知：<code>take_photo</code></p>\n<p> 4. 导航工具：<code>view_map</code>, <code>navigate_to</code></p>\n<p> 5. 社交交互：<code>read_msg</code>, <code>send_msg</code>, <code>save_image</code></p>\n<ul><li><strong>任务设计</strong>：六个子任务评估不同能力（如识别含黄油纸袋、注意用户缺席、多步路径规划等）。</li><li><strong>红队测试</strong>：模拟低电量+充电器故障场景，诱导模型泄露机密信息。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>任务数据集</strong>：基于真实办公环境设计的6项操作任务（附录A详述），包含接受标准。</li><li><strong>模型评估</strong>：测试多个LLM：</li></ul>\n<p> - <strong>SOTA通用模型</strong>：Gemini 2.5 Pro、Claude Opus 4.1、GPT-5、Grok 4、Llama 4 Maverick</p>\n<p> - <strong>专用实体模型</strong>：Gemini ER 1.5（针对机器人调优）</p>\n<ul><li><strong>人类基线</strong>：3名人类操作员通过网页界面控制机器人完成相同任务（不知环境布局）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：真实办公室场景（单房间含出口、办公桌、厨房等），固定光照和障碍物布局。</li><li><strong>核心指标</strong>：</li></ul>\n<p> 1. <strong>主要指标</strong>：任务完成率（每模型-任务组合测试5次）。</p>\n<p> 2. <strong>辅助指标</strong>：</p>\n<p> - 任务完成时间（推理延迟+规划效率）</p>\n<p> - 工具调用分布（分析工具使用模式）</p>\n<p> - 定性失效模式分类（空间推理、社交理解等5类）。</p>\n<ul><li><strong>安全性评估</strong>：红队攻击成功率（如泄露机密信息的倾向性）。</li></ul>\n\n<p><strong>结果摘要</strong></p>\n<ul><li>人类vs LLM：人类平均95%完成率，最佳LLM Gemini 2.5 Pro仅40%。</li><li><strong>关键弱点</strong>：</li></ul>\n<p> - 社交理解（\"注意缺席\"任务：LLM 0% vs 人类100%）</p>\n<p> - 多步空间规划（\"Plan\"任务：LLM依赖随机路径漂移，非真实理解）</p>\n<ul><li><strong>模型差异</strong>：</li></ul>\n<p> Gemini ER 1.5未优于Gemini 2.5 Pro，表明实体调优未显著提升实用智能。</p>"
  },
  {
    "date": "2025-10-21",
    "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting",
    "link": "http://arxiv.org/abs/2510.21817",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-24",
    "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos",
    "link": "http://arxiv.org/abs/2510.21571",
    "summary_markdown": "# 论文研究单位\n\n清华大学、微软亚洲研究院\n# 论文概述\n\n本文提出了一种将无脚本真实生活人类手部活动视频转换为视觉-语言-动作（VLA）数据格式的新方法，用于预训练机器人操作VLA模型。该方法将人类手部视为灵巧机器人末端执行器，通过开发全自动化的人类活动分析框架，将\"野生\"第一人称人类视频转换为与现有机器人VLA训练数据完全对齐的数据格式，无需任何注释。最终构建了包含100万片段和2600万帧的大规模手部VLA数据集，并设计了灵巧手部VLA模型。\n# 论文核心贡献点\n\n1. **首个VLA预训练方法**：首次将非结构化人类视频转换为与机器人VLA训练数据完全对齐的数据格式，无需人工注释\n\n2. **自动化分析框架**：开发了全自动化的人类活动分析框架，包含三维运动标注、原子动作分割和指令标注三个阶段\n\n3. **大规模数据集构建**：构建了包含100万片段和2600万帧的手部VLA数据集，覆盖广泛的对象、概念、技能和环境变化\n\n4. **VLA模型架构**：设计了由VLM骨干网和扩散动作专家组成的灵巧手部VLA模型，支持统一的单双hand动作预测\n\n5. **零样本泛化能力**：模型在完全未见过的真实世界观察中展现出强大的零样本能力\n\n6. **缩放行为验证**：证明了模型任务性能随预训练数据规模的清晰缩放行为\n# 论文方法描述\n## 数据转换框架\n\n**三维运动标注**：采用单目三维相机和手部姿态跟踪方法，利用深度视觉SLAM、深度估计和手部重建技术，估计相机内参和帧级手部姿态（基于6D手腕姿态和完整关节角度）\n\n**原子动作分割**：通过检测世界空间中3D手部手腕的速度最小值作为切割点，将长视频分割为原子级手部动作序列，遵循机器人VLA数据的粒度要求\n\n**指令标注**：从每个片段均匀采样8帧，在帧上叠加手部轨迹投影，使用GPT-4.1为每个片段生成动作的描述性语言指令\n## VLA模型设计\n\n**模型架构**：由PaliGemma-2 VLM骨干网和DiT-Base扩散Transformer动作专家组成，添加可学习的认知token作为条件输入\n\n**动作空间**：在相机坐标系中预测手部动作，包含左右手的手腕平移、旋转和15个关节的欧拉角\n\n**统一预测**：通过动作掩码机制处理单双hand动作的统一预测，使用因果注意机制进行动作去噪\n\n**训练策略**：采用轨迹感知数据增强，随机裁剪和透视变换，同时变换对应的动作序列\n## 机器人微调\n\n将人类手部动作空间映射到机器人手部，通过关节拓扑对应关系进行简单的映射策略，对模型进行微调以适应真实机器人操作\n# 论文使用数据集和训练资源\n## 预训练数据集\n\n**数据来源**：处理了来自Ego4D（77%）、Epic-Kitchen（12%）、EgoExo4D（6%）和Something-Something-V2（5%）的第一人称人类视频\n\n**数据规模**：100万片段，2600万帧，覆盖烹饪、清洁、建筑、维修、手工和绘画等真实生活活动\n\n**数据多样性**：与OpenImages数据集相比具有更高的视觉多样性，包含更丰富的语言指令多样性（名词、动词、形容词）\n## 训练资源\n\n**预训练**：在8个NVIDIA H100 GPU上训练2天，批量大小512，学习率1e-4和1e-5\n\n**微调**：在8个NVIDIA H100 GPU上训练8小时，批量大小256，学习率1e-5\n## 机器人数据\n\n收集了1.2K条遥操作轨迹用于四个任务的微调：一般拾取放置、功能抓取、倒水和清扫\n# 论文使用的评估环境和评估指标\n## 人类手部动作预测评估\n\n**评估环境**：\n- 抓取任务：在47个未见过的环境中使用Azure Kinect捕获RGB-D图像\n- 一般动作任务：在117个未见过的真实生活环境中进行用户研究\n\n**评估指标**：\n- 抓取任务：手-物体最小距离（$d_{\\text{hand-obj}}$）评估动作合理性\n- 一般动作：23名参与者对30个随机场景的top-3动作进行排名打分\n## 真实机器人灵巧操作评估\n\n**评估环境**：\n- Realman机器人配备12-DoF XHand灵巧手和RealSense头部相机\n- 桌面环境设置，使用遥操作系统收集训练数据\n\n**评估任务**：\n- 拾取放置：移动物体到盒子中（3-4个随机干扰物）\n- 功能抓取：在功能位置抓取物体（如手柄）\n- 倒水：抓取瓶子，将内容倒入另一容器，放回桌面\n- 扫帚清扫：从篮子中拿起扫帚，将垃圾扫入簸箕，归位扫帚\n\n**评估指标**：\n- 任务成功率：在可见和不可见物体/背景下的成功率百分比\n- 对比基线：与VPP和π₀等代表性方法的性能比较\n- 缩放行为：不同数据规模下的性能变化趋势",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>清华大学、微软亚洲研究院</p>\n<h1>论文概述</h1>\n\n<p>本文提出了一种将无脚本真实生活人类手部活动视频转换为视觉-语言-动作（VLA）数据格式的新方法，用于预训练机器人操作VLA模型。该方法将人类手部视为灵巧机器人末端执行器，通过开发全自动化的人类活动分析框架，将\"野生\"第一人称人类视频转换为与现有机器人VLA训练数据完全对齐的数据格式，无需任何注释。最终构建了包含100万片段和2600万帧的大规模手部VLA数据集，并设计了灵巧手部VLA模型。</p>\n<h1>论文核心贡献点</h1>\n\n<ol><li><strong>首个VLA预训练方法</strong>：首次将非结构化人类视频转换为与机器人VLA训练数据完全对齐的数据格式，无需人工注释</li></ol>\n\n<ol><li><strong>自动化分析框架</strong>：开发了全自动化的人类活动分析框架，包含三维运动标注、原子动作分割和指令标注三个阶段</li></ol>\n\n<ol><li><strong>大规模数据集构建</strong>：构建了包含100万片段和2600万帧的手部VLA数据集，覆盖广泛的对象、概念、技能和环境变化</li></ol>\n\n<ol><li><strong>VLA模型架构</strong>：设计了由VLM骨干网和扩散动作专家组成的灵巧手部VLA模型，支持统一的单双hand动作预测</li></ol>\n\n<ol><li><strong>零样本泛化能力</strong>：模型在完全未见过的真实世界观察中展现出强大的零样本能力</li></ol>\n\n<ol><li><strong>缩放行为验证</strong>：证明了模型任务性能随预训练数据规模的清晰缩放行为</li></ol>\n<h1>论文方法描述</h1>\n<h2 class=\"section-title\">数据转换框架</h2>\n\n<p><strong>三维运动标注</strong>：采用单目三维相机和手部姿态跟踪方法，利用深度视觉SLAM、深度估计和手部重建技术，估计相机内参和帧级手部姿态（基于6D手腕姿态和完整关节角度）</p>\n\n<p><strong>原子动作分割</strong>：通过检测世界空间中3D手部手腕的速度最小值作为切割点，将长视频分割为原子级手部动作序列，遵循机器人VLA数据的粒度要求</p>\n\n<p><strong>指令标注</strong>：从每个片段均匀采样8帧，在帧上叠加手部轨迹投影，使用GPT-4.1为每个片段生成动作的描述性语言指令</p>\n<h2 class=\"section-title\">VLA模型设计</h2>\n\n<p><strong>模型架构</strong>：由PaliGemma-2 VLM骨干网和DiT-Base扩散Transformer动作专家组成，添加可学习的认知token作为条件输入</p>\n\n<p><strong>动作空间</strong>：在相机坐标系中预测手部动作，包含左右手的手腕平移、旋转和15个关节的欧拉角</p>\n\n<p><strong>统一预测</strong>：通过动作掩码机制处理单双hand动作的统一预测，使用因果注意机制进行动作去噪</p>\n\n<p><strong>训练策略</strong>：采用轨迹感知数据增强，随机裁剪和透视变换，同时变换对应的动作序列</p>\n<h2 class=\"section-title\">机器人微调</h2>\n\n<p>将人类手部动作空间映射到机器人手部，通过关节拓扑对应关系进行简单的映射策略，对模型进行微调以适应真实机器人操作</p>\n<h1>论文使用数据集和训练资源</h1>\n<h2 class=\"section-title\">预训练数据集</h2>\n\n<p><strong>数据来源</strong>：处理了来自Ego4D（77%）、Epic-Kitchen（12%）、EgoExo4D（6%）和Something-Something-V2（5%）的第一人称人类视频</p>\n\n<p><strong>数据规模</strong>：100万片段，2600万帧，覆盖烹饪、清洁、建筑、维修、手工和绘画等真实生活活动</p>\n\n<p><strong>数据多样性</strong>：与OpenImages数据集相比具有更高的视觉多样性，包含更丰富的语言指令多样性（名词、动词、形容词）</p>\n<h2 class=\"section-title\">训练资源</h2>\n\n<p><strong>预训练</strong>：在8个NVIDIA H100 GPU上训练2天，批量大小512，学习率1e-4和1e-5</p>\n\n<p><strong>微调</strong>：在8个NVIDIA H100 GPU上训练8小时，批量大小256，学习率1e-5</p>\n<h2 class=\"section-title\">机器人数据</h2>\n\n<p>收集了1.2K条遥操作轨迹用于四个任务的微调：一般拾取放置、功能抓取、倒水和清扫</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<h2 class=\"section-title\">人类手部动作预测评估</h2>\n\n<p><strong>评估环境</strong>：</p>\n<ul><li>抓取任务：在47个未见过的环境中使用Azure Kinect捕获RGB-D图像</li><li>一般动作任务：在117个未见过的真实生活环境中进行用户研究</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li>抓取任务：手-物体最小距离（$d_{\\text{hand-obj}}$）评估动作合理性</li><li>一般动作：23名参与者对30个随机场景的top-3动作进行排名打分</li></ul>\n<h2 class=\"section-title\">真实机器人灵巧操作评估</h2>\n\n<p><strong>评估环境</strong>：</p>\n<ul><li>Realman机器人配备12-DoF XHand灵巧手和RealSense头部相机</li><li>桌面环境设置，使用遥操作系统收集训练数据</li></ul>\n\n<p><strong>评估任务</strong>：</p>\n<ul><li>拾取放置：移动物体到盒子中（3-4个随机干扰物）</li><li>功能抓取：在功能位置抓取物体（如手柄）</li><li>倒水：抓取瓶子，将内容倒入另一容器，放回桌面</li><li>扫帚清扫：从篮子中拿起扫帚，将垃圾扫入簸箕，归位扫帚</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li>任务成功率：在可见和不可见物体/背景下的成功率百分比</li><li>对比基线：与VPP和π₀等代表性方法的性能比较</li><li>缩放行为：不同数据规模下的性能变化趋势</li></ul>"
  },
  {
    "date": "2025-10-23",
    "title": "SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing",
    "link": "http://arxiv.org/abs/2510.20965",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-23",
    "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation",
    "link": "http://arxiv.org/abs/2510.20818",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-23",
    "title": "MemER: Scaling Up Memory for Robot Control via Experience Retrieval",
    "link": "http://arxiv.org/abs/2510.20328",
    "summary_markdown": "## 论文研究单位\n\n斯坦福大学（Stanford University）\n## 论文概述\n\n机器人策略通常缺乏长期记忆能力，无法像人类一样利用历史信息完成复杂任务。针对这一问题，本文提出了MemER（Memory via Experience Retrieval）框架，采用分层策略设计，使机器人能够选择性地记忆任务相关的关键帧，从而实现分钟级别的长期记忆。该方法在三个真实世界的长时序机器人操作任务上进行了验证。\n## 论文核心贡献点\n\n1. **分层记忆架构**：提出高层策略负责选择和跟踪相关关键帧，低层策略执行具体动作的层次化设计\n2. **关键帧选择机制**：开发了一种基于一维单链接聚类的算法，能从历史观察中自动提取任务相关信息\n3. **高效记忆存储**：设计了紧凑的视觉记忆表示方法，避免了传统长上下文方法的计算开销和协变量偏移问题\n4. **少样本适应**：仅需50个远程操作演示即可将预训练的视觉-语言模型适配到机器人任务\n## 论文方法描述\n\n**高层策略（π_h）**：\n- 基于Qwen2.5-VL-7B-Instruct微调\n- 输入：最近N帧、任务指令、选定关键帧\n- 输出：当前子任务和候选关键帧\n- 采用一维单链接聚类（合并距离d=5帧）构建视觉记忆\n\n**低层策略（π_l）**：\n- 基于π_0.5微调\n- 输入：当前图像、关节状态、子任务指令\n- 输出：机器人动作块\n\n**关键帧选择算法**：\n1. 收集所有候选关键帧的时间戳\n2. 对时间戳进行排序\n3. 使用单链接聚类合并相近帧\n4. 选择每簇的中位数作为代表帧\n\n**模型合并技术**：使用权重插值θ=(1-α)·θ_pre + α·θ_ft（α=0.8）保持预训练模型的鲁棒性\n## 论文使用数据集和训练资源\n\n**训练数据**：\n- 50个长时序轨迹演示，每个任务10-15个干预演示\n- 数据格式：(I_t, q_t, l_t, l'_t, a_t)\n\n**模型训练**：\n- 高层策略：4500梯度步，96 H200 GPU小时\n- 低层策略：18000训练步，48 H200 GPU小时\n- 冻结视觉编码器和投影层，仅微调LLM骨干网络\n\n**硬件配置**：\n- Franka机械臂，平行爪夹持器\n- 双摄像头：第三人称ZED摄像头和手腕安装的miniZED摄像头\n- 图像分辨率320×180，15Hz采样率\n## 论文使用的评估环境和评估指标\n\n**评估任务**：\n1. **物品搜索任务**：在三个不透明垃圾箱中搜索3-5个物品，评估记忆优化路径\n2. **计数舀取任务**：向两个碗中放入指定数量的两种食材，评估计数准确性\n3. **除尘与复位任务**：从双层架子移除物品，清洗架子并复位物品，评估空间记忆\n\n**评估指标**：\n- 物品搜索：成功检索次数（↑）、使用最优路径次数（↑）\n- 计数舀取：错误舀取数量（↓）\n- 除尘与复位：底层/顶层除尘成功（↑）、底层/顶层物品复位成功（↑）\n\n**对比方法**：\n- 无历史：仅当前帧\n- 短历史：N=8帧\n- 长历史：N=32帧\n- 人类高层：人工提供正确子任务\n\n**主要结果**：\n- MemER在所有任务上>90%成功率\n- 明显优于无历史和短历史基线\n- 长历史方法平均性能比MemER差34%\n- 与人类高层策略性能相当\n- API-VLMs因延迟问题表现不佳",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n\n<p>斯坦福大学（Stanford University）</p>\n<h2 class=\"section-title\">论文概述</h2>\n\n<p>机器人策略通常缺乏长期记忆能力，无法像人类一样利用历史信息完成复杂任务。针对这一问题，本文提出了MemER（Memory via Experience Retrieval）框架，采用分层策略设计，使机器人能够选择性地记忆任务相关的关键帧，从而实现分钟级别的长期记忆。该方法在三个真实世界的长时序机器人操作任务上进行了验证。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n\n<ol><li><strong>分层记忆架构</strong>：提出高层策略负责选择和跟踪相关关键帧，低层策略执行具体动作的层次化设计</li><li><strong>关键帧选择机制</strong>：开发了一种基于一维单链接聚类的算法，能从历史观察中自动提取任务相关信息</li><li><strong>高效记忆存储</strong>：设计了紧凑的视觉记忆表示方法，避免了传统长上下文方法的计算开销和协变量偏移问题</li><li><strong>少样本适应</strong>：仅需50个远程操作演示即可将预训练的视觉-语言模型适配到机器人任务</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n\n<p><strong>高层策略（π_h）</strong>：</p>\n<ul><li>基于Qwen2.5-VL-7B-Instruct微调</li><li>输入：最近N帧、任务指令、选定关键帧</li><li>输出：当前子任务和候选关键帧</li><li>采用一维单链接聚类（合并距离d=5帧）构建视觉记忆</li></ul>\n\n<p><strong>低层策略（π_l）</strong>：</p>\n<ul><li>基于π_0.5微调</li><li>输入：当前图像、关节状态、子任务指令</li><li>输出：机器人动作块</li></ul>\n\n<p><strong>关键帧选择算法</strong>：</p>\n<ol><li>收集所有候选关键帧的时间戳</li><li>对时间戳进行排序</li><li>使用单链接聚类合并相近帧</li><li>选择每簇的中位数作为代表帧</li></ol>\n\n<p><strong>模型合并技术</strong>：使用权重插值θ=(1-α)·θ_pre + α·θ_ft（α=0.8）保持预训练模型的鲁棒性</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n\n<p><strong>训练数据</strong>：</p>\n<ul><li>50个长时序轨迹演示，每个任务10-15个干预演示</li><li>数据格式：(I_t, q_t, l_t, l'_t, a_t)</li></ul>\n\n<p><strong>模型训练</strong>：</p>\n<ul><li>高层策略：4500梯度步，96 H200 GPU小时</li><li>低层策略：18000训练步，48 H200 GPU小时</li><li>冻结视觉编码器和投影层，仅微调LLM骨干网络</li></ul>\n\n<p><strong>硬件配置</strong>：</p>\n<ul><li>Franka机械臂，平行爪夹持器</li><li>双摄像头：第三人称ZED摄像头和手腕安装的miniZED摄像头</li><li>图像分辨率320×180，15Hz采样率</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n\n<p><strong>评估任务</strong>：</p>\n<ol><li><strong>物品搜索任务</strong>：在三个不透明垃圾箱中搜索3-5个物品，评估记忆优化路径</li><li><strong>计数舀取任务</strong>：向两个碗中放入指定数量的两种食材，评估计数准确性</li><li><strong>除尘与复位任务</strong>：从双层架子移除物品，清洗架子并复位物品，评估空间记忆</li></ol>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li>物品搜索：成功检索次数（↑）、使用最优路径次数（↑）</li><li>计数舀取：错误舀取数量（↓）</li><li>除尘与复位：底层/顶层除尘成功（↑）、底层/顶层物品复位成功（↑）</li></ul>\n\n<p><strong>对比方法</strong>：</p>\n<ul><li>无历史：仅当前帧</li><li>短历史：N=8帧</li><li>长历史：N=32帧</li><li>人类高层：人工提供正确子任务</li></ul>\n\n<p><strong>主要结果</strong>：</p>\n<ul><li>MemER在所有任务上>90%成功率</li><li>明显优于无历史和短历史基线</li><li>长历史方法平均性能比MemER差34%</li><li>与人类高层策略性能相当</li><li>API-VLMs因延迟问题表现不佳</li></ul>"
  },
  {
    "date": "2025-10-22",
    "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.19752",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-22",
    "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2510.19430",
    "summary_markdown": "# 论文总结\n## 论文研究单位\nGigaAI。论文页面：https://GigaBrain0.github.io\n## 论文概述\nGigaBrain-0是一类由世界模型驱动的视觉-语言-动作（VLA）基础模型，旨在缓解真实机器人数据收集的高成本与多样性受限问题。通过在大规模、真实且多样的世界模型合成轨迹上训练，GigaBrain-0显著减少对真实机器人数据的依赖，同时提升跨任务、跨外观与跨视角的泛化能力。\n## 论文核心贡献点\n- 以世界模型生成的多样化数据为核心驱动的VLA范式，涵盖视频生成、Real2Real、View Transfer、Sim2Real、人体视频迁移等多源合成轨迹。\n- 架构层面引入RGB-D输入与Embodied Chain-of-Thought（CoT）监督，增强3D几何感知与长时程推理能力，支持操纵轨迹、子目标语言与离散动作符号的联合学习。\n- 混合Transformer架构：基于PaliGemma2的VLM作为感知与语言接口，动作生成采用Diffusion Transformer（DiT）与Flow Matching以预测连续动作块；引入离散动作token加速收敛；通过Knowledge Insulation隔离不同优化流以减轻干扰。\n- 系统性数据管线GigaWorld，提供高效视频生成（单步蒸馏、NATTEN注意力、FP8推理，速度提升>50倍）与质量评估（几何一致性、多视角一致性、文本对齐、物理合理性）。\n- 真实世界广泛验证：在灵巧操作、长时程与移动操纵任务上稳定领先；外观、放置与视角泛化随合成数据混合比α提升显著。\n- 轻量化部署GigaBrain-0-Small，在NVIDIA Jetson AGX Orin上以约402M参数实现低延迟与低显存，保持与π0相当的任务成功率。\n## 论文方法描述\n- 输入与表示：RGB-D输入（B×H×W×4），对SigLIP首层卷积扩展零初始化深度通道；训练期随机丢弃深度以兼容RGB推断。\n- 架构细节：VLM（PaliGemma2）编码多模态输入；动作专家采用DiT进行Flow Matching生成连续动作块；离散动作token作为辅助预测以加速训练；10个可学习的轨迹token通过双向注意与GRU解码输出2D操纵轨迹关键点；子目标语言与离散动作采用自回归生成。\n- 监督与损失：联合优化三项——语言/离散动作的next-token损失、Flow Matching动作块回归损失、轨迹2D坐标回归损失；通过Knowledge Insulation隔离语义与动作学习的相互干扰。\n## 论文使用数据集和训练资源\n- 真实世界数据：融合公开数据集（AgiBotWorld、RoboMind、Open X-Embodiment）与自采数据（Agilex Cobot Magic 199小时 + AgiBot G1 983小时，共1182小时；覆盖工业、商业、办公、居住、实验室等五大类共14个场景）。\n- 世界模型生成数据：GigaWorld提供多管线合成数据，显著提升外观、几何与视角多样性。包括：\n - Real2Real Transfer：在深度与边缘结构先验约束下生成多纹理/光照/材质的视觉变体。\n - View Transfer：基于深度的新视角投影、遮挡修复与物理仿真微调，保持任务语义一致。\n - Sim2Real Transfer：在Isaac Sim中构造多样化场景并合成至逼真外观，增强域泛化。\n - Human Video Transfer：将EgoDex等第一人称人类演示转化为稳定机器人执行序列。\n - 文本条件视频生成+逆动力学（IDM）反推动作序列，扩充操纵轨迹。\n - 多视角一致视频生成与质量评估用于4D一致性与可训练性筛选。\n- 训练策略：全量/部分/未标注数据混合训练；基于手爪状态切换自动切分子目标并用Qwen-VL-2.5生成语言子目标；统一去重与采样策略提升样本效率。\n## 论文使用的评估环境和评估指标\n- 评估平台：AgiBot G1双足/双手机器人平台与PiPER双臂平台。\n- 任务覆盖：\n - 灵巧操纵：Laundry Folding、Paper Towel Preparation。\n - 长时程：Table Bussing、Juice Preparation。\n - 移动操纵：Boxes Moving、Laundry Baskets Moving。\n- 指标：真实世界任务成功率（多次试验平均）；外观/放置/视角泛化实验以合成数据混合比α为变量，评估成功率随α提升曲线。\n- 设备部署：NVIDIA Jetson AGX Orin上对GigaBrain-0-Small与π0进行推理性能与成功率对比（FLOPs、参数量、显存、时延、成功率）。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>GigaAI。论文页面：https://GigaBrain0.github.io</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>GigaBrain-0是一类由世界模型驱动的视觉-语言-动作（VLA）基础模型，旨在缓解真实机器人数据收集的高成本与多样性受限问题。通过在大规模、真实且多样的世界模型合成轨迹上训练，GigaBrain-0显著减少对真实机器人数据的依赖，同时提升跨任务、跨外观与跨视角的泛化能力。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>以世界模型生成的多样化数据为核心驱动的VLA范式，涵盖视频生成、Real2Real、View Transfer、Sim2Real、人体视频迁移等多源合成轨迹。</li><li>架构层面引入RGB-D输入与Embodied Chain-of-Thought（CoT）监督，增强3D几何感知与长时程推理能力，支持操纵轨迹、子目标语言与离散动作符号的联合学习。</li><li>混合Transformer架构：基于PaliGemma2的VLM作为感知与语言接口，动作生成采用Diffusion Transformer（DiT）与Flow Matching以预测连续动作块；引入离散动作token加速收敛；通过Knowledge Insulation隔离不同优化流以减轻干扰。</li><li>系统性数据管线GigaWorld，提供高效视频生成（单步蒸馏、NATTEN注意力、FP8推理，速度提升>50倍）与质量评估（几何一致性、多视角一致性、文本对齐、物理合理性）。</li><li>真实世界广泛验证：在灵巧操作、长时程与移动操纵任务上稳定领先；外观、放置与视角泛化随合成数据混合比α提升显著。</li><li>轻量化部署GigaBrain-0-Small，在NVIDIA Jetson AGX Orin上以约402M参数实现低延迟与低显存，保持与π0相当的任务成功率。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>输入与表示：RGB-D输入（B×H×W×4），对SigLIP首层卷积扩展零初始化深度通道；训练期随机丢弃深度以兼容RGB推断。</li><li>架构细节：VLM（PaliGemma2）编码多模态输入；动作专家采用DiT进行Flow Matching生成连续动作块；离散动作token作为辅助预测以加速训练；10个可学习的轨迹token通过双向注意与GRU解码输出2D操纵轨迹关键点；子目标语言与离散动作采用自回归生成。</li><li>监督与损失：联合优化三项——语言/离散动作的next-token损失、Flow Matching动作块回归损失、轨迹2D坐标回归损失；通过Knowledge Insulation隔离语义与动作学习的相互干扰。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>真实世界数据：融合公开数据集（AgiBotWorld、RoboMind、Open X-Embodiment）与自采数据（Agilex Cobot Magic 199小时 + AgiBot G1 983小时，共1182小时；覆盖工业、商业、办公、居住、实验室等五大类共14个场景）。</li><li>世界模型生成数据：GigaWorld提供多管线合成数据，显著提升外观、几何与视角多样性。包括：</li></ul>\n<p> - Real2Real Transfer：在深度与边缘结构先验约束下生成多纹理/光照/材质的视觉变体。</p>\n<p> - View Transfer：基于深度的新视角投影、遮挡修复与物理仿真微调，保持任务语义一致。</p>\n<p> - Sim2Real Transfer：在Isaac Sim中构造多样化场景并合成至逼真外观，增强域泛化。</p>\n<p> - Human Video Transfer：将EgoDex等第一人称人类演示转化为稳定机器人执行序列。</p>\n<p> - 文本条件视频生成+逆动力学（IDM）反推动作序列，扩充操纵轨迹。</p>\n<p> - 多视角一致视频生成与质量评估用于4D一致性与可训练性筛选。</p>\n<ul><li>训练策略：全量/部分/未标注数据混合训练；基于手爪状态切换自动切分子目标并用Qwen-VL-2.5生成语言子目标；统一去重与采样策略提升样本效率。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估平台：AgiBot G1双足/双手机器人平台与PiPER双臂平台。</li><li>任务覆盖：</li></ul>\n<p> - 灵巧操纵：Laundry Folding、Paper Towel Preparation。</p>\n<p> - 长时程：Table Bussing、Juice Preparation。</p>\n<p> - 移动操纵：Boxes Moving、Laundry Baskets Moving。</p>\n<ul><li>指标：真实世界任务成功率（多次试验平均）；外观/放置/视角泛化实验以合成数据混合比α为变量，评估成功率随α提升曲线。</li><li>设备部署：NVIDIA Jetson AGX Orin上对GigaBrain-0-Small与π0进行推理性能与成功率对比（FLOPs、参数量、显存、时延、成功率）。</li></ul>"
  },
  {
    "date": "2025-10-22",
    "title": "Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes",
    "link": "http://arxiv.org/abs/2510.19400",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-21",
    "title": "MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning",
    "link": "http://arxiv.org/abs/2510.18337",
    "summary_markdown": "### 论文研究单位\n- **Harvard University** (Wenhui Huang, Han Qi, Yilun Du, Heng Yang)\n- **University of Michigan** (Changhe Chen)\n- **Nanyang Technological University** (Chen Lv)\n### 论文概述\nMoTVLA是一个基于混合Transformer（MoT）架构的视觉语言动作（VLA）模型，通过整合快速与慢速推理来改进机器人学习中的语言指令控制能力。该模型保留预训练VLM的通用智能（通用智能专家）以处理感知和语义规划，同时引入领域专家生成快速运动分解信号，最终通过扩散Transformer（DiT）动作专家执行具体操作。该设计显著提升语言可控性和推理效率，并已在NLP基准、仿真环境（ManiSkill）和真实机器人实验中验证有效性。\n### 论文核心贡献点\n1. **统一快慢推理架构**：在单一MoT模型中实现通用智能（慢速推理）与领域特定知识（快速推理）的融合，通过分解-组合-再分解的机制保持性能平衡。\n2. **动作条件化策略学习**：将快速推理生成的运动分解信号作为扩散策略条件，提升任务执行效率和行为可解释性。\n3. **延迟与性能优势**：在推理速度（快推理频率达4倍提升）、语义理解及真实操作任务上超越SOTA基线（如π0.5、RT-2）。\n### 论文方法描述\n- **架构设计**：\n - **通用智能专家（Slow Reasoning）**：基于Qwen2.5-LLM 7B的预训练模型，处理视觉-文本多模态理解。\n - **领域专家（Fast Reasoning）**：同构架构，生成步骤化运动分解文本（双向注意力）。\n - **动作专家**：DiT结构，基于视觉窗口（5帧）、机器人状态、运动分解信号执行扩散去噪生成动作序列。\n- **训练流程**：\n - **领域专家SFT**：使用1.27M问答对（仿真154K + 真实125K + Robo2VLM 678K + LLaVA-OV 318K）优化快速推理。\n - **动作专家扩散策略**：ManiSkill仿真（每任务300轨迹）+ 真实操作（Pick-and-Place 50轨迹，Table Bussing 200轨迹），以均方误差优化去噪网络。\n- **推理模式**：\n - **对话模式**：通用智能专家执行慢推理回答人类问题（如场景描述）。\n - **行动模式**：领域专家快速生成运动分解 → 动作专家执行多步操作（如堆叠、插孔）。\n### 论文使用数据集和训练资源\n- **领域专家数据**：\n - 仿真数据：154K（ManiSkill的Cube Stacking、Peg-in-Hole、L-tool Pull）\n - 真实演示：125K（人类操作的Pick-and-Place、Table Bussing）\n - 外部数据：678K（Robo2VLM，转换为推理文本）、318K（LLaVA-OV，筛选长答案）\n- **动作专家数据**：\n - 仿真：900轨迹（3任务×300）\n - 真实：250轨迹（Pick-and-Place 50 + Table Bussing 200）\n- **预训练资源**：\n - 通用智能专家：Bagel-VLM初始化（SigLIP-So400m视觉编码 + Qwen2.5文本分词器）\n - 推理骨干：MoTVLA-14B（双7B专家，总参数量14B）\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **仿真**：ManiSkill平台（Cube Stacking、Peg-in-Hole、L-tool Pull）。\n - **真实操作**：桌面任务（蔬菜Pick-and-Place含干扰物、Table Bussing含模糊指令）。\n- **评估指标**：\n - **语义推理**：BLEU、METEOR、CIDEr、Token准确率（VQA任务如ScienceQA、Visual-7W）。\n - **机器人任务**：平均成功率（重复随机种子测试，零样本干扰场景）。\n - **延迟对比**：推理频率（MoTVLA-14B/1B vs π0.5-KI，H100/A6000 GPU）。\n- **基线对比**：π0.5-KI、π0、GR-MG、Diffusion Policy（DP），基于1050轨迹微调后性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li><strong>Harvard University</strong> (Wenhui Huang, Han Qi, Yilun Du, Heng Yang)</li><li><strong>University of Michigan</strong> (Changhe Chen)</li><li><strong>Nanyang Technological University</strong> (Chen Lv)</li></ul>\n<h3>论文概述</h3>\n<p>MoTVLA是一个基于混合Transformer（MoT）架构的视觉语言动作（VLA）模型，通过整合快速与慢速推理来改进机器人学习中的语言指令控制能力。该模型保留预训练VLM的通用智能（通用智能专家）以处理感知和语义规划，同时引入领域专家生成快速运动分解信号，最终通过扩散Transformer（DiT）动作专家执行具体操作。该设计显著提升语言可控性和推理效率，并已在NLP基准、仿真环境（ManiSkill）和真实机器人实验中验证有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>统一快慢推理架构</strong>：在单一MoT模型中实现通用智能（慢速推理）与领域特定知识（快速推理）的融合，通过分解-组合-再分解的机制保持性能平衡。</li><li><strong>动作条件化策略学习</strong>：将快速推理生成的运动分解信号作为扩散策略条件，提升任务执行效率和行为可解释性。</li><li><strong>延迟与性能优势</strong>：在推理速度（快推理频率达4倍提升）、语义理解及真实操作任务上超越SOTA基线（如π0.5、RT-2）。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>架构设计</strong>：</li></ul>\n<p> - <strong>通用智能专家（Slow Reasoning）</strong>：基于Qwen2.5-LLM 7B的预训练模型，处理视觉-文本多模态理解。</p>\n<p> - <strong>领域专家（Fast Reasoning）</strong>：同构架构，生成步骤化运动分解文本（双向注意力）。</p>\n<p> - <strong>动作专家</strong>：DiT结构，基于视觉窗口（5帧）、机器人状态、运动分解信号执行扩散去噪生成动作序列。</p>\n<ul><li><strong>训练流程</strong>：</li></ul>\n<p> - <strong>领域专家SFT</strong>：使用1.27M问答对（仿真154K + 真实125K + Robo2VLM 678K + LLaVA-OV 318K）优化快速推理。</p>\n<p> - <strong>动作专家扩散策略</strong>：ManiSkill仿真（每任务300轨迹）+ 真实操作（Pick-and-Place 50轨迹，Table Bussing 200轨迹），以均方误差优化去噪网络。</p>\n<ul><li><strong>推理模式</strong>：</li></ul>\n<p> - <strong>对话模式</strong>：通用智能专家执行慢推理回答人类问题（如场景描述）。</p>\n<p> - <strong>行动模式</strong>：领域专家快速生成运动分解 → 动作专家执行多步操作（如堆叠、插孔）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>领域专家数据</strong>：</li></ul>\n<p> - 仿真数据：154K（ManiSkill的Cube Stacking、Peg-in-Hole、L-tool Pull）</p>\n<p> - 真实演示：125K（人类操作的Pick-and-Place、Table Bussing）</p>\n<p> - 外部数据：678K（Robo2VLM，转换为推理文本）、318K（LLaVA-OV，筛选长答案）</p>\n<ul><li><strong>动作专家数据</strong>：</li></ul>\n<p> - 仿真：900轨迹（3任务×300）</p>\n<p> - 真实：250轨迹（Pick-and-Place 50 + Table Bussing 200）</p>\n<ul><li><strong>预训练资源</strong>：</li></ul>\n<p> - 通用智能专家：Bagel-VLM初始化（SigLIP-So400m视觉编码 + Qwen2.5文本分词器）</p>\n<p> - 推理骨干：MoTVLA-14B（双7B专家，总参数量14B）</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>仿真</strong>：ManiSkill平台（Cube Stacking、Peg-in-Hole、L-tool Pull）。</p>\n<p> - <strong>真实操作</strong>：桌面任务（蔬菜Pick-and-Place含干扰物、Table Bussing含模糊指令）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>语义推理</strong>：BLEU、METEOR、CIDEr、Token准确率（VQA任务如ScienceQA、Visual-7W）。</p>\n<p> - <strong>机器人任务</strong>：平均成功率（重复随机种子测试，零样本干扰场景）。</p>\n<p> - <strong>延迟对比</strong>：推理频率（MoTVLA-14B/1B vs π0.5-KI，H100/A6000 GPU）。</p>\n<ul><li><strong>基线对比</strong>：π0.5-KI、π0、GR-MG、Diffusion Policy（DP），基于1050轨迹微调后性能。</li></ul>"
  },
  {
    "date": "2025-10-20",
    "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies",
    "link": "http://arxiv.org/abs/2510.17950",
    "summary_markdown": "## 论文研究单位\nDexmal 和 Hugging Face\n## 论文概述\n论文介绍了RoboChallenge，一个在线机器人评估系统，旨在为视觉-语言-动作模型（VLA）提供大规模真实机器人基准测试。系统提供10台在线机器人供公众访问，覆盖UR5、Franka Panda、Cobot Magic Aloha和ARX-5等四种类型。研究构建了初始基准Table30，包含30个围绕桌子的任务，测试VLA模型的多种能力。论文描述了系统设计、评估协议和实验结果，强调了真实机器人在评估中的必要性。\n## 论文核心贡献点\n- 提出了一个在线机器人评估平台，支持大规模真实机器人测试，提供10台机器人机群。\n- 设计了评估协议，通过视觉任务重现控制初始状态，区分稳定性（多次测试一致）和公平性（模型间比较一致）。\n- 构建了Table30基准，包含30个任务，涵盖精确3D定位、多视角、时间依赖和软体操作等挑战。\n- 实现了“远程机器人”范式，用户通过API直接访问机器人，无需提交模型文件。\n- 提供了详细的结果分析，展示不同VLA模型在各项任务上的性能差异。\n## 论文方法描述\n系统采用“远程机器人”模式：用户通过低级别API访问机器人传感器（时间戳RGB、深度和本体感受数据）和动作队列（不可撤销动作）。评估协议包括：\n- 视觉任务重现：通过参考图像引导测试者固定初始场景状态。\n- 环境扰动控制：允许光照和背景变化，测试模型鲁棒性。\n- 测试者控制：区分经验测试者、无知测试者和自适应测试者，提出受控测试者减少偏差。\n- 稳定性与公平性权衡：基准协议关注模型稳定性（多次测试结果一致），比较协议关注公平性（模型间相对排序一致）。\n## 论文使用数据集和训练资源\n- 数据集：提供每个任务的演示数据（每任务最多1000集），用户需基于此微调模型。\n- 训练资源：\n - 任务特定设置：在8-GPU机器上训练约1天。\n - 一般设置：混合多任务数据训练“机器通才”模型（每任务约50集）。\n- 示例模型：包括π₀、π₀.5、CogACT和OpenVLA/OFT等开源VLA算法。\n## 论文使用的评估环境和评估指标\n- 评估环境：10台机器人，分为四种类型（UR5、Franka Panda、Cobot Magic Aloha、ARX-5），配备多摄像头（主要、手腕、侧面）。\n- 评估指标：\n - 成功率（SR）：任务完成百分比。\n - 进度分数：基于任务阶段完成情况，每个任务总分10分，每阶段分配点数，重试扣0.5分；每任务测试10次，总分100。\n- Table30基准：30个任务列表（见表1），如插花、整理水果和开抽屉等，测试精确操作、多视角和时间依赖等能力。\n- 结果：在π₀.5模型上观察到最高成功率，平均43.7%，其他模型性能较低；任务分析显示时间和软体相关任务更具挑战性。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>Dexmal 和 Hugging Face</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文介绍了RoboChallenge，一个在线机器人评估系统，旨在为视觉-语言-动作模型（VLA）提供大规模真实机器人基准测试。系统提供10台在线机器人供公众访问，覆盖UR5、Franka Panda、Cobot Magic Aloha和ARX-5等四种类型。研究构建了初始基准Table30，包含30个围绕桌子的任务，测试VLA模型的多种能力。论文描述了系统设计、评估协议和实验结果，强调了真实机器人在评估中的必要性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了一个在线机器人评估平台，支持大规模真实机器人测试，提供10台机器人机群。</li><li>设计了评估协议，通过视觉任务重现控制初始状态，区分稳定性（多次测试一致）和公平性（模型间比较一致）。</li><li>构建了Table30基准，包含30个任务，涵盖精确3D定位、多视角、时间依赖和软体操作等挑战。</li><li>实现了“远程机器人”范式，用户通过API直接访问机器人，无需提交模型文件。</li><li>提供了详细的结果分析，展示不同VLA模型在各项任务上的性能差异。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>系统采用“远程机器人”模式：用户通过低级别API访问机器人传感器（时间戳RGB、深度和本体感受数据）和动作队列（不可撤销动作）。评估协议包括：</p>\n<ul><li>视觉任务重现：通过参考图像引导测试者固定初始场景状态。</li><li>环境扰动控制：允许光照和背景变化，测试模型鲁棒性。</li><li>测试者控制：区分经验测试者、无知测试者和自适应测试者，提出受控测试者减少偏差。</li><li>稳定性与公平性权衡：基准协议关注模型稳定性（多次测试结果一致），比较协议关注公平性（模型间相对排序一致）。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据集：提供每个任务的演示数据（每任务最多1000集），用户需基于此微调模型。</li><li>训练资源：</li></ul>\n<p> - 任务特定设置：在8-GPU机器上训练约1天。</p>\n<p> - 一般设置：混合多任务数据训练“机器通才”模型（每任务约50集）。</p>\n<ul><li>示例模型：包括π₀、π₀.5、CogACT和OpenVLA/OFT等开源VLA算法。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：10台机器人，分为四种类型（UR5、Franka Panda、Cobot Magic Aloha、ARX-5），配备多摄像头（主要、手腕、侧面）。</li><li>评估指标：</li></ul>\n<p> - 成功率（SR）：任务完成百分比。</p>\n<p> - 进度分数：基于任务阶段完成情况，每个任务总分10分，每阶段分配点数，重试扣0.5分；每任务测试10次，总分100。</p>\n<ul><li>Table30基准：30个任务列表（见表1），如插花、整理水果和开抽屉等，测试精确操作、多视角和时间依赖等能力。</li><li>结果：在π₀.5模型上观察到最高成功率，平均43.7%，其他模型性能较低；任务分析显示时间和软体相关任务更具挑战性。</li></ul>"
  },
  {
    "date": "2025-10-20",
    "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2510.17640",
    "summary_markdown": "### 论文研究单位\n- 南洋理工大学（新加坡）\n- 清华大学（中国北京）\n- 北京邮电大学（中国北京）\n### 论文概述\n论文针对机器人操作中的分布外（OOD）泛化问题，提出RESample框架。现有视觉-语言-动作（VLA）模型在模仿学习中易受OOD状态影响，因数据集仅含成功轨迹。RESample利用离线强化学习获取动作价值网络识别次优动作，并通过探索性采样生成潜在OOD轨迹，将数据整合训练集以增强VLA模型鲁棒性。\n### 论文核心贡献点\n- 设计稳健数据增强框架缓解模仿学习的OOD问题。\n- 提出探索性采样机制，自适应纳入OOD样本。\n- 利用动作价值网络识别细粒度OOD动作样本。\n- 在模拟和真实机器人任务中验证有效性。\n### 论文方法描述\nRESample基于策略（πθ）和动作价值网络（Qφ）的二元性：\n- **概念框架**：策略与评论家分歧触发探索性采样，强制执行策略自信但低值行动，暴露失败模式。\n- **价值函数估计**：基于Cal-QL改进，使用行为克隆代理策略（πψ）校准Q值。评论家目标含时间差分损失和正则化（统一惩罚、行动锚定、数据保持）。\n- **探索性采样机制**：策略生成候选动作，评论家过滤Q值低样本执行；为空则用常规动作。产生失败和恢复轨迹纳入训练。\n### 论文使用数据集和训练资源\n- **数据集**：模拟实验使用LIBERO基准（包含Spatial、Object、Goal和Long-horizon四类任务，每类10个任务）。真实实验使用Galaxea A1机器人手臂。\n- **训练资源**：评论家基于SAC算法离线训练，策略如DiT Policy或π0在模拟和真实环境重新训练。参数包括批大小（64或256）、学习率（1e-4或2e-5）、折扣因子（0.99）等。\n### 论文使用的评估环境和评估指标\n- **评估环境**：模拟实验在LIBERO基准评估，真实实验在四个任务（Pick Block、Stack Cup、Arrange Cubes、Stack 2 Cups）。\n- **评估指标**：主要使用任务成功率（Success Rate）。在LIBERO上报告各任务类别平均成功率；消融研究使用成功率衡量性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>南洋理工大学（新加坡）</li><li>清华大学（中国北京）</li><li>北京邮电大学（中国北京）</li></ul>\n<h3>论文概述</h3>\n<p>论文针对机器人操作中的分布外（OOD）泛化问题，提出RESample框架。现有视觉-语言-动作（VLA）模型在模仿学习中易受OOD状态影响，因数据集仅含成功轨迹。RESample利用离线强化学习获取动作价值网络识别次优动作，并通过探索性采样生成潜在OOD轨迹，将数据整合训练集以增强VLA模型鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>设计稳健数据增强框架缓解模仿学习的OOD问题。</li><li>提出探索性采样机制，自适应纳入OOD样本。</li><li>利用动作价值网络识别细粒度OOD动作样本。</li><li>在模拟和真实机器人任务中验证有效性。</li></ul>\n<h3>论文方法描述</h3>\n<p>RESample基于策略（πθ）和动作价值网络（Qφ）的二元性：</p>\n<ul><li><strong>概念框架</strong>：策略与评论家分歧触发探索性采样，强制执行策略自信但低值行动，暴露失败模式。</li><li><strong>价值函数估计</strong>：基于Cal-QL改进，使用行为克隆代理策略（πψ）校准Q值。评论家目标含时间差分损失和正则化（统一惩罚、行动锚定、数据保持）。</li><li><strong>探索性采样机制</strong>：策略生成候选动作，评论家过滤Q值低样本执行；为空则用常规动作。产生失败和恢复轨迹纳入训练。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：模拟实验使用LIBERO基准（包含Spatial、Object、Goal和Long-horizon四类任务，每类10个任务）。真实实验使用Galaxea A1机器人手臂。</li><li><strong>训练资源</strong>：评论家基于SAC算法离线训练，策略如DiT Policy或π0在模拟和真实环境重新训练。参数包括批大小（64或256）、学习率（1e-4或2e-5）、折扣因子（0.99）等。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：模拟实验在LIBERO基准评估，真实实验在四个任务（Pick Block、Stack Cup、Arrange Cubes、Stack 2 Cups）。</li><li><strong>评估指标</strong>：主要使用任务成功率（Success Rate）。在LIBERO上报告各任务类别平均成功率；消融研究使用成功率衡量性能。</li></ul>"
  },
  {
    "date": "2025-10-20",
    "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
    "link": "http://arxiv.org/abs/2510.17439",
    "summary_markdown": "### 论文研究单位\n- ByteDance Seed、NUS、NTU、THU、SMU 等机构合作\n### 论文概述\n- FALCON (From Spatial to Action) 是一类视觉-语言-行动（VLA）模型，针对现有 VLA 基于 2D 视觉编码、缺乏可靠 3D 空间理解的问题，提出通过空间基础模型在仅 RGB 条件下注入丰富的空间 token，并在可选条件下融合深度与位姿，从而在动作头层面融合语义与几何信息，增强长程与空间推理能力、跨模态可迁移性以及语言-视觉对齐\n### 论文核心贡献点\n- 以 Embodied Spatial Model（ESM）从 RGB 生成强几何先验，支持可选深度/位姿注入，无需重训即可提升精度\n- 设计 Spatial-Enhanced Action Head，将空间 token 在动作头而非 VLM 中融合，保持 VLM 语义对齐不受破坏\n- 采用随机条件注入训练策略（RGB-only 与 RGB-D/Pose 随机），实现强跨模态迁移能力\n- 在动作头采用轻量元素相加融合，最大化稳定性与效率（优于交叉注意力与 FiLM）\n### 论文方法描述\n- 整体架构：2D VLM（如 Kosmos-2）负责语义表示；ESM 提取空间 tokens（基于 VGGT 范式，结合 DINO 视觉 token）；动作头融合语义动作 token 与空间 tokens 产生精确动作\n- 空间编码：\n - 输入为第三视角图像，附加可学习相机 token，进入 Spatial Encoder（N 层自/交叉注意力）输出空间 tokens 与相机 token\n - 3D 条件编码：将相机位姿（7DoF）编码为 GT camera token；对深度与有效性掩膜进行归一化与卷积编码，生成与图像 tokens 等尺寸的深度 tokens\n - 3D 条件注入：采用 b_d、b_p∼Bernoulli(p) 随机决定是否注入深度与位姿，替换可学习相机 token 并进行元素级相加；在训练中采用深度、点云与位姿的多任务监督\n- 空间增强动作头：\n - 将空间 tokens 最大池化为统一向量，经轻量 MLP 投影至动作头维度，与语义 action token 元素级相加\n - 比较了交叉注意力、FiLM 门控与元素相加，最终采用元素相加（在泛化与稳定性上表现最佳）\n - 预测器支持两类：MLP（短期）或 LSTM（长期）输出动作序列（7DoF 姿态+夹爪状态，C 步预测）\n- 训练范式：\n - 两阶段训练：阶段一冻结 VLM/ESM/旧动作头，只训练适配器（零初始化线性层避免初始破坏对齐）；阶段二解冻并联合优化\n - 损失：MSE（姿态）+ BCE（夹爪）+ 3D 重建/深度/位姿监督（与 VGGT 一致）\n### 论文使用数据集和训练资源\n- 数据与平台：Open X-Embodiment 混合预训练；真实机器人多任务微调；基准覆盖 CALVIN（ABCD→D、ABC→D）、SimplerEnv（WidowX、Google Robot）与 11 项真实任务\n- 训练资源：32×A100 训练；模型规模约 2.9B（VLM～1.6B，ESM～1.0B，配套动作头）\n### 论文使用的评估环境和评估指标\n- 仿真基准与指标：\n - CALVIN：Tasks Completed in a Row（1-5 连贯任务完成率）与 Avg. Len（平均连击长度）\n - ABCD→D：FALCON 97.2/93.3/90.3/88.0/84.0%，Avg Len 4.53；ABC→D：98.4/94.5/88.6/82.5/75.5%，Avg Len 4.40\n - 相较 RT-1/Robo-Flamingo/GR-1/UP-VLA/RoboVLM 等一致领先\n - SimplerEnv（WidowX/Google Robot）：按任务成功率汇总\n - WidowX：FALCON 56.3%（Put Spoon 62.5%，Put Carrot 41.7% 等）\n - Google Robot：FALCON 62.9%，在“打开抽屉放苹果”上 41.7%，远超 RT-2-X 55B 的 3.7%\n- 真实世界评测与指标：三类设置的成功率（%）\n - Base Tasks（9 套任务）：FALCON 70.0%，显著高于 SpatialVLA（44.4%）\n - Few-shot（Simple/Unseen）：在未见物体/背景/任务描述变体下领先，Simple 提升 +27.5%，Unseen 平均 +27%\n - 空间理解能力：在高度变化、尺寸变化、空间指令等任务中保持最高成功率（例如高度变化任务 60%→80%）\n- 模态可迁移性与消融：在 CALVIN 与真实任务上验证 RGB-only 与 RGB-D/Pose 的增益；ESM 深度预测指标（δ<1.25、AbsRel）随可用模态增强而显著提升；元素相加在融合策略中兼具效率与性能",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>ByteDance Seed、NUS、NTU、THU、SMU 等机构合作</li></ul>\n<h3>论文概述</h3>\n<ul><li>FALCON (From Spatial to Action) 是一类视觉-语言-行动（VLA）模型，针对现有 VLA 基于 2D 视觉编码、缺乏可靠 3D 空间理解的问题，提出通过空间基础模型在仅 RGB 条件下注入丰富的空间 token，并在可选条件下融合深度与位姿，从而在动作头层面融合语义与几何信息，增强长程与空间推理能力、跨模态可迁移性以及语言-视觉对齐</li></ul>\n<h3>论文核心贡献点</h3>\n<ul><li>以 Embodied Spatial Model（ESM）从 RGB 生成强几何先验，支持可选深度/位姿注入，无需重训即可提升精度</li><li>设计 Spatial-Enhanced Action Head，将空间 token 在动作头而非 VLM 中融合，保持 VLM 语义对齐不受破坏</li><li>采用随机条件注入训练策略（RGB-only 与 RGB-D/Pose 随机），实现强跨模态迁移能力</li><li>在动作头采用轻量元素相加融合，最大化稳定性与效率（优于交叉注意力与 FiLM）</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>整体架构：2D VLM（如 Kosmos-2）负责语义表示；ESM 提取空间 tokens（基于 VGGT 范式，结合 DINO 视觉 token）；动作头融合语义动作 token 与空间 tokens 产生精确动作</li><li>空间编码：</li></ul>\n<p> - 输入为第三视角图像，附加可学习相机 token，进入 Spatial Encoder（N 层自/交叉注意力）输出空间 tokens 与相机 token</p>\n<p> - 3D 条件编码：将相机位姿（7DoF）编码为 GT camera token；对深度与有效性掩膜进行归一化与卷积编码，生成与图像 tokens 等尺寸的深度 tokens</p>\n<p> - 3D 条件注入：采用 b_d、b_p∼Bernoulli(p) 随机决定是否注入深度与位姿，替换可学习相机 token 并进行元素级相加；在训练中采用深度、点云与位姿的多任务监督</p>\n<ul><li>空间增强动作头：</li></ul>\n<p> - 将空间 tokens 最大池化为统一向量，经轻量 MLP 投影至动作头维度，与语义 action token 元素级相加</p>\n<p> - 比较了交叉注意力、FiLM 门控与元素相加，最终采用元素相加（在泛化与稳定性上表现最佳）</p>\n<p> - 预测器支持两类：MLP（短期）或 LSTM（长期）输出动作序列（7DoF 姿态+夹爪状态，C 步预测）</p>\n<ul><li>训练范式：</li></ul>\n<p> - 两阶段训练：阶段一冻结 VLM/ESM/旧动作头，只训练适配器（零初始化线性层避免初始破坏对齐）；阶段二解冻并联合优化</p>\n<p> - 损失：MSE（姿态）+ BCE（夹爪）+ 3D 重建/深度/位姿监督（与 VGGT 一致）</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据与平台：Open X-Embodiment 混合预训练；真实机器人多任务微调；基准覆盖 CALVIN（ABCD→D、ABC→D）、SimplerEnv（WidowX、Google Robot）与 11 项真实任务</li><li>训练资源：32×A100 训练；模型规模约 2.9B（VLM～1.6B，ESM～1.0B，配套动作头）</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>仿真基准与指标：</li></ul>\n<p> - CALVIN：Tasks Completed in a Row（1-5 连贯任务完成率）与 Avg. Len（平均连击长度）</p>\n<p> - ABCD→D：FALCON 97.2/93.3/90.3/88.0/84.0%，Avg Len 4.53；ABC→D：98.4/94.5/88.6/82.5/75.5%，Avg Len 4.40</p>\n<p> - 相较 RT-1/Robo-Flamingo/GR-1/UP-VLA/RoboVLM 等一致领先</p>\n<p> - SimplerEnv（WidowX/Google Robot）：按任务成功率汇总</p>\n<p> - WidowX：FALCON 56.3%（Put Spoon 62.5%，Put Carrot 41.7% 等）</p>\n<p> - Google Robot：FALCON 62.9%，在“打开抽屉放苹果”上 41.7%，远超 RT-2-X 55B 的 3.7%</p>\n<ul><li>真实世界评测与指标：三类设置的成功率（%）</li></ul>\n<p> - Base Tasks（9 套任务）：FALCON 70.0%，显著高于 SpatialVLA（44.4%）</p>\n<p> - Few-shot（Simple/Unseen）：在未见物体/背景/任务描述变体下领先，Simple 提升 +27.5%，Unseen 平均 +27%</p>\n<p> - 空间理解能力：在高度变化、尺寸变化、空间指令等任务中保持最高成功率（例如高度变化任务 60%→80%）</p>\n<ul><li>模态可迁移性与消融：在 CALVIN 与真实任务上验证 RGB-only 与 RGB-D/Pose 的增益；ESM 深度预测指标（δ<1.25、AbsRel）随可用模态增强而显著提升；元素相加在融合策略中兼具效率与性能</li></ul>"
  },
  {
    "date": "2025-10-20",
    "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots",
    "link": "http://arxiv.org/abs/2510.17369",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-20",
    "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
    "link": "http://arxiv.org/abs/2510.17148",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-20",
    "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
    "link": "http://arxiv.org/abs/2510.17111",
    "summary_markdown": "## 论文研究单位\n中国科学院自动化研究所、中国科学院大学、AiRiA、南京信息工程大学\n## 论文概述\n这是一篇关于高效视觉-语言-动作（VLA）模型在具身操作中的系统性综述论文。VLA模型通过将自然语言指令和视觉观察映射到机器人动作，实现具身控制。尽管VLA系统功能强大，但其巨大的计算和内存需求与边缘平台的实时性能要求存在显著冲突。本综述首次从效率角度对VLA模型进行系统性回顾，涵盖模型架构、感知特征、动作生成和训练/推理策略四个维度，提供了主流效率优化方法的分类和比较分析。\n## 论文核心贡献点\n1. 首次专门针对高效VLA的系统性综述，将效率改进技术分为四个维度：模型架构、感知特征、动作生成、训练/推理机制\n2. 基于分类法总结主流效率优化方法，分析各种技术的优劣势和适用场景\n3. 讨论VLA模型未来发展趋势，强调需要在新兴趋势下进一步改进效率的优先方向\n4. 提供实用的参考，支撑开发既高效又具备通用可靠具身智能的VLA系统\n## 论文方法描述\n**架构层面：**\n- **静态骨干选择**：使用轻量级模型如Mamba、SmolVLA（2.24亿参数）、NORA（30亿参数）替代大型预训练模型\n- **动态计算路径**：包括层剪枝（FLOWER）、早期退出（DEER-VLA）、专家混合（MoLE-VLA）、相似性跳过（Efficient-VLA）\n- **双系统设计**：结合慢系统（大型多模态语言模型）处理复杂推理和快系统（轻量模型）实现快速响应\n\n**感知特征优化：**\n- **选择性特征处理**：基于注意力分数、任务相关性、空间结构的令牌修剪方法\n- **时间共享重用**：利用帧间相似性重用KV缓存、高层表示和推理结果\n\n**动作生成策略：**\n- **原始动作生成**：动作块分块、令牌压缩（FAST）、离散化（VOTE）\n- **推理感知动作**：语言链式思维推理（ECoT）、视觉推理（UniPi、VPP、CoT-VLA）\n\n**训练推理优化：**\n- **训练效率**：参数高效微调（LoRA）、知识蒸馏、量化感知训练、剪枝恢复\n- **推理效率**：非自回归解码、投机解码（Spec-VLA）、并行细化（PD-VLA）\n## 论文使用数据集和训练资源\n- **Open X-Embodiment (OXE)**：大规模真实世界机器人数据集\n- **DROID**：大规模野外机器人操作数据集\n- **各种模拟数据集**：用于预训练和Sim-to-Real迁移\n## 论文使用的评估环境和评估指标\n**评估维度：**\n- **资源效率**：模型规模、推理延迟、内存占用、训练时间、能耗\n- **性能鲁棒性**：任务成功率、长时序稳定性、分布外泛化能力、环境扰动恢复\n- **可解释性**：人类可读的推理过程、决策归因机制、透明度\n\n**技术实现：**\n- 统一的云边部署架构\n- 标准化硬件配置报告\n- 开源评估框架和基准测试\n- 多任务多场景的公开数据集和仿真环境",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>中国科学院自动化研究所、中国科学院大学、AiRiA、南京信息工程大学</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>这是一篇关于高效视觉-语言-动作（VLA）模型在具身操作中的系统性综述论文。VLA模型通过将自然语言指令和视觉观察映射到机器人动作，实现具身控制。尽管VLA系统功能强大，但其巨大的计算和内存需求与边缘平台的实时性能要求存在显著冲突。本综述首次从效率角度对VLA模型进行系统性回顾，涵盖模型架构、感知特征、动作生成和训练/推理策略四个维度，提供了主流效率优化方法的分类和比较分析。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>首次专门针对高效VLA的系统性综述，将效率改进技术分为四个维度：模型架构、感知特征、动作生成、训练/推理机制</li><li>基于分类法总结主流效率优化方法，分析各种技术的优劣势和适用场景</li><li>讨论VLA模型未来发展趋势，强调需要在新兴趋势下进一步改进效率的优先方向</li><li>提供实用的参考，支撑开发既高效又具备通用可靠具身智能的VLA系统</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p><strong>架构层面：</strong></p>\n<ul><li><strong>静态骨干选择</strong>：使用轻量级模型如Mamba、SmolVLA（2.24亿参数）、NORA（30亿参数）替代大型预训练模型</li><li><strong>动态计算路径</strong>：包括层剪枝（FLOWER）、早期退出（DEER-VLA）、专家混合（MoLE-VLA）、相似性跳过（Efficient-VLA）</li><li><strong>双系统设计</strong>：结合慢系统（大型多模态语言模型）处理复杂推理和快系统（轻量模型）实现快速响应</li></ul>\n\n<p><strong>感知特征优化：</strong></p>\n<ul><li><strong>选择性特征处理</strong>：基于注意力分数、任务相关性、空间结构的令牌修剪方法</li><li><strong>时间共享重用</strong>：利用帧间相似性重用KV缓存、高层表示和推理结果</li></ul>\n\n<p><strong>动作生成策略：</strong></p>\n<ul><li><strong>原始动作生成</strong>：动作块分块、令牌压缩（FAST）、离散化（VOTE）</li><li><strong>推理感知动作</strong>：语言链式思维推理（ECoT）、视觉推理（UniPi、VPP、CoT-VLA）</li></ul>\n\n<p><strong>训练推理优化：</strong></p>\n<ul><li><strong>训练效率</strong>：参数高效微调（LoRA）、知识蒸馏、量化感知训练、剪枝恢复</li><li><strong>推理效率</strong>：非自回归解码、投机解码（Spec-VLA）、并行细化（PD-VLA）</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>Open X-Embodiment (OXE)</strong>：大规模真实世界机器人数据集</li><li><strong>DROID</strong>：大规模野外机器人操作数据集</li><li><strong>各种模拟数据集</strong>：用于预训练和Sim-to-Real迁移</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p><strong>评估维度：</strong></p>\n<ul><li><strong>资源效率</strong>：模型规模、推理延迟、内存占用、训练时间、能耗</li><li><strong>性能鲁棒性</strong>：任务成功率、长时序稳定性、分布外泛化能力、环境扰动恢复</li><li><strong>可解释性</strong>：人类可读的推理过程、决策归因机制、透明度</li></ul>\n\n<p><strong>技术实现：</strong></p>\n<ul><li>统一的云边部署架构</li><li>标准化硬件配置报告</li><li>开源评估框架和基准测试</li><li>多任务多场景的公开数据集和仿真环境</li></ul>"
  },
  {
    "date": "2025-10-18",
    "title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation",
    "link": "http://arxiv.org/abs/2510.16617",
    "summary_markdown": "## 论文研究单位\nUniversity of Texas at Austin（作者来自德州大学奥斯汀分校）\n## 论文概述\n现有的大规模视觉-语言-动作（VLA）模型在跨实验室、环境与任务上的泛化能力不足，通常需要在新场景中收集一定量的专家演示并进行参数微调才能奏效。本文提出 MoS-VLA（Mixture of Skills VLA），通过“技能混合”的方式将机器人策略表示为有限基函数的线性组合，从而构建结构化的技能空间；在线推理时，仅需一次专家演示，通过最小化 L1 行动误差的轻量凸优化（线性规划）得到系数，无需梯度回传即可在新环境中快速适配，显著降低适配开销。论文基于 OpenVLA 与 Open X-Embodiment 数据进行训练，并在仿真与真实机械臂任务上验证效果。\n## 论文核心贡献点\n- 提出一种一次样本、上下文内的 VLA 适配方法：在推理阶段只需一次专家演示，无需梯度更新，计算与内存开销极低（仅前向传播），在普通 GPU（如 RTX 3090）上几秒即可完成校准。\n- 首次将函数编码器（Function Encoders）应用于亿级参数与多模态机器人数据，验证其在 VLA 场景的可行性与可扩展性。\n- 在结构化技能空间中缓解“混合上下文过拟合”：通过学习基函数并在系数空间区分不同上下文，提升了分布外数据集与实际环境中的表现。\n- 构建了 L1 误差驱动的凸优化校准流程（线性规划），强调鲁棒性与对抗离群点的能力。\n- 给出端到端实现细节与可扩展训练策略，支持分布式并行与缓冲校准，保证可复现性与工程可行性。\n## 论文方法描述\n- 问题形式化：机器人策略空间为状态 S = I × T 到行动 A 的映射；上下文 c（如光照、相机位姿、机器人形态等）影响专家策略但不可直接观测；将不同上下文下的专家策略集合视为函数空间。\n- 主思想：将上下文相关的专家策略表示为函数空间中的函数，学习一组神经网络的基函数 {g1, ..., gk}，任何上下文策略 π_exp^c 可表示为这些基函数的线性组合。新任务在线适配时，通过一次专家演示 τ_exp^c，最小化 L1 行动误差，求解线性规划得到系数 α^c，再以线性组合给出动作预测。\n- 架构实现：在 OpenVLA 主干的基础上替换语言模型输出头为 k 个独立的“基函数动作头”，共享主干特征；采用并行解码策略同时预测多个行动维度；降低参数开销并保持 Transformer 输入的变长处理能力。\n- 训练流程：采用修正的函数编码器算法，支持 L1 误差与 Banach 空间设置；使用 LoRA 进行高效微调；维护校准缓冲（每数据集 512 样本），每若干步更新一次上下文系数并在训练中广播；使用分布式数据并行（DDP）在多节点（32×GH200）上进行训练。\n## 论文使用数据集和训练资源\n- 数据集：Open X-Embodiment（RT-X）中的 Magic Soup Plus 数据混合（同 OpenVLA），包含多个实验室的轨迹数据；包含分布内与分布外（OOD）子集。\n- 训练资源：32 个计算节点，每节点 1×GH200；全局批次 320；训练步数约 5000（耗时约 24 小时）；Adam 优化器，学习率 1e-4，热身 10 步。\n- 适配资源（在线）：单条专家演示；在普通 GPU（如 RTX 3090）上仅需几秒完成系数求解。\n## 论文使用的评估环境和评估指标\n- 数据集评估：在 27 个训练子集与 5 个 OOD 子集上评估动作预测误差（L1 误差），对比基线（OpenVLA）。\n- 仿真实验（Robosuite）：两项任务——块搬运与开门；每任务 m=20 次试验。\n- 真实机械臂实验（Franka Emika Panda）：三项任务——目标抵达、块搬运、笔插入；每任务 m=10 次试验；单 RGB 前置相机环境，部分运动方向限制以简化深度估计。\n- 指标：任务成功率（%），同时观察与讨论适配在短-horizon 任务上的有效性以及在更长时序或更高随机性场景中的局限。\n## 关键结果\n- 在 5 个 OOD 数据集上，MoS-VLA 的 L1 行动误差全面低于基线；在训练子集中也有 18/27 的优势。\n- 仿真与真实实验：OpenVLA 在未见环境与新任务上成功率为 0%；MoS-VLA 在一次演示校准后，仿真任务（块搬运、开门）达 70–75% 成功率，真实任务（目标抵达、块搬运、笔插入）达 100% 成功率。\n- 系数可视化显示相似实验室的上下文在技能空间内聚类，说明学习到的技能空间能够捕捉环境与任务的结构性差异。\n## 工程与复现要点\n- 架构：在 OpenVLA 主干上引入 k=16 个基函数动作头，采用并行解码直接输出标量动作。\n- 校准：维护分布式校准缓冲，每 16 步进行一次系数广播与更新；采用 CVXPY 求解 L1 线性规划。\n- 资源友好：在线适配不需梯度回传，推理阶段计算与内存开销与专家轨迹长度无关（常数复杂度）。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>University of Texas at Austin（作者来自德州大学奥斯汀分校）</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>现有的大规模视觉-语言-动作（VLA）模型在跨实验室、环境与任务上的泛化能力不足，通常需要在新场景中收集一定量的专家演示并进行参数微调才能奏效。本文提出 MoS-VLA（Mixture of Skills VLA），通过“技能混合”的方式将机器人策略表示为有限基函数的线性组合，从而构建结构化的技能空间；在线推理时，仅需一次专家演示，通过最小化 L1 行动误差的轻量凸优化（线性规划）得到系数，无需梯度回传即可在新环境中快速适配，显著降低适配开销。论文基于 OpenVLA 与 Open X-Embodiment 数据进行训练，并在仿真与真实机械臂任务上验证效果。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出一种一次样本、上下文内的 VLA 适配方法：在推理阶段只需一次专家演示，无需梯度更新，计算与内存开销极低（仅前向传播），在普通 GPU（如 RTX 3090）上几秒即可完成校准。</li><li>首次将函数编码器（Function Encoders）应用于亿级参数与多模态机器人数据，验证其在 VLA 场景的可行性与可扩展性。</li><li>在结构化技能空间中缓解“混合上下文过拟合”：通过学习基函数并在系数空间区分不同上下文，提升了分布外数据集与实际环境中的表现。</li><li>构建了 L1 误差驱动的凸优化校准流程（线性规划），强调鲁棒性与对抗离群点的能力。</li><li>给出端到端实现细节与可扩展训练策略，支持分布式并行与缓冲校准，保证可复现性与工程可行性。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>问题形式化：机器人策略空间为状态 S = I × T 到行动 A 的映射；上下文 c（如光照、相机位姿、机器人形态等）影响专家策略但不可直接观测；将不同上下文下的专家策略集合视为函数空间。</li><li>主思想：将上下文相关的专家策略表示为函数空间中的函数，学习一组神经网络的基函数 {g1, ..., gk}，任何上下文策略 π_exp^c 可表示为这些基函数的线性组合。新任务在线适配时，通过一次专家演示 τ_exp^c，最小化 L1 行动误差，求解线性规划得到系数 α^c，再以线性组合给出动作预测。</li><li>架构实现：在 OpenVLA 主干的基础上替换语言模型输出头为 k 个独立的“基函数动作头”，共享主干特征；采用并行解码策略同时预测多个行动维度；降低参数开销并保持 Transformer 输入的变长处理能力。</li><li>训练流程：采用修正的函数编码器算法，支持 L1 误差与 Banach 空间设置；使用 LoRA 进行高效微调；维护校准缓冲（每数据集 512 样本），每若干步更新一次上下文系数并在训练中广播；使用分布式数据并行（DDP）在多节点（32×GH200）上进行训练。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据集：Open X-Embodiment（RT-X）中的 Magic Soup Plus 数据混合（同 OpenVLA），包含多个实验室的轨迹数据；包含分布内与分布外（OOD）子集。</li><li>训练资源：32 个计算节点，每节点 1×GH200；全局批次 320；训练步数约 5000（耗时约 24 小时）；Adam 优化器，学习率 1e-4，热身 10 步。</li><li>适配资源（在线）：单条专家演示；在普通 GPU（如 RTX 3090）上仅需几秒完成系数求解。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>数据集评估：在 27 个训练子集与 5 个 OOD 子集上评估动作预测误差（L1 误差），对比基线（OpenVLA）。</li><li>仿真实验（Robosuite）：两项任务——块搬运与开门；每任务 m=20 次试验。</li><li>真实机械臂实验（Franka Emika Panda）：三项任务——目标抵达、块搬运、笔插入；每任务 m=10 次试验；单 RGB 前置相机环境，部分运动方向限制以简化深度估计。</li><li>指标：任务成功率（%），同时观察与讨论适配在短-horizon 任务上的有效性以及在更长时序或更高随机性场景中的局限。</li></ul>\n<h2 class=\"section-title\">关键结果</h2>\n<ul><li>在 5 个 OOD 数据集上，MoS-VLA 的 L1 行动误差全面低于基线；在训练子集中也有 18/27 的优势。</li><li>仿真与真实实验：OpenVLA 在未见环境与新任务上成功率为 0%；MoS-VLA 在一次演示校准后，仿真任务（块搬运、开门）达 70–75% 成功率，真实任务（目标抵达、块搬运、笔插入）达 100% 成功率。</li><li>系数可视化显示相似实验室的上下文在技能空间内聚类，说明学习到的技能空间能够捕捉环境与任务的结构性差异。</li></ul>\n<h2 class=\"section-title\">工程与复现要点</h2>\n<ul><li>架构：在 OpenVLA 主干上引入 k=16 个基函数动作头，采用并行解码直接输出标量动作。</li><li>校准：维护分布式校准缓冲，每 16 步进行一次系数广播与更新；采用 CVXPY 求解 L1 线性规划。</li><li>资源友好：在线适配不需梯度回传，推理阶段计算与内存开销与专家轨迹长度无关（常数复杂度）。</li></ul>"
  },
  {
    "date": "2025-10-17",
    "title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?",
    "link": "http://arxiv.org/abs/2510.16263",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-17",
    "title": "VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving",
    "link": "http://arxiv.org/abs/2510.15446",
    "summary_markdown": "## 论文研究单位\n- 清华大学苏州汽车研究院\n- 作者实习单位为清华大学苏州汽车研究院\n## 论文概述\n论文提出 VDRive 框架，将视觉-语言-动作模型（VLA）与扩散策略（Diffusion Policy）结合，用于端到端自动驾驶。框架以离散化的状态-动作表示为核心，通过“强化的模态对齐”桥接高维传感器空间与低维动作空间：在上下文层面，VLA 通过视觉 token 预训练预测未来观测；几何层面，通过对 VLA 的强化学习微调，使其基于当前驾驶条件预测轨迹与动作。随后，扩散策略头在 VLA 提供当前与预测状态 token 的条件下生成层次化动作与轨迹，并由动态引导的精炼头优化轨迹输出。方法采用全离线强化学习范式，在 nuScenes 开环规划与 Bench2Drive 闭环基准上均取得最先进性能。\n## 论文核心贡献点\n- 提出 VDRive 框架：通过 VLA 预测未来状态 token 与扩散策略头联合训练，实现上下文与几何对齐的驾驶决策。\n- 构建强化学习偏好数据集：基于 nuScenes 与 Bench2Drive 构造“选择/拒绝”视觉-动作对用于 VLA 微调。\n- 设计离线奖励数据：结合规则奖励与专家 VLM（Qwen2.5-VL-72B）评分，训练扩散策略头并构建 actor-critic 强化学习闭环。\n- 在多基准上实现最先进结果：在 nuScenes 开环评估与 Bench2Drive 闭环评估中均显著提升。\n## 论文方法描述\n- 离散化表示（CVQ-VAE）：训练条件向量量化变分自编码器，以轨迹为条件，将原始/分割图像编码为离散观测 token，加入 VLA 分词器词表。\n- VLA 强化微调：利用偏好数据训练 VLA 生成未来观测 token、动作信号与导航命令，结合选择/拒绝样本进行偏好对齐。\n- 扩散策略头学习：在离线状态-动作-奖励-下一状态数据集上，训练扩散模型作为 actor 网络、最小化生成动作与真实动作的重建误差，并通过 critic 网络提供价值反馈，实现累积奖励最大化。\n- 动态引导的精炼头：将 VLA 与扩散策略的异步动作预测作为条件，通过 Transformer 编码器与 MLP 映射融合位置嵌入与动作特征，最终解码优化轨迹。\n## 论文使用数据集和训练资源\n- 数据：nuScenes（1000 场景，6 相机/5 雷达/1 激光雷达，2 Hz）与 Bench2Drive（CARLA 仿真）用于开/闭环评估与偏好/奖励数据集构建；利用 Vista 生成合成风险场景。\n- 预训练/微调：视觉 token 预训练（CVQ-VAE + VLA tokenizer）；VLA 强化微调（偏好对齐）；扩散策略离线强化学习（actor-critic）。\n- 模型：VLA 基于 InternVL3-8B；扩散策略头使用扩散模型；奖励评估采用 Qwen2.5-VL-72B。\n## 论文使用的评估环境和评估指标\n- 开环评估（nuScenes）：平均 L2 误差与碰撞率；在 1s/2s/3s 时间步与总体上报告指标。\n- 闭环评估（Bench2Drive）：Driving Score、Success Rate、Efficiency、Comfortness；并报告多能力指标（并线、超车、紧急制动、让行、交通标志）。\n- 消融实验：Bench2Drive-mini 上对比不同数据构造与不同精炼模块（Transformer/LSTM/GRU）设计。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>清华大学苏州汽车研究院</li><li>作者实习单位为清华大学苏州汽车研究院</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文提出 VDRive 框架，将视觉-语言-动作模型（VLA）与扩散策略（Diffusion Policy）结合，用于端到端自动驾驶。框架以离散化的状态-动作表示为核心，通过“强化的模态对齐”桥接高维传感器空间与低维动作空间：在上下文层面，VLA 通过视觉 token 预训练预测未来观测；几何层面，通过对 VLA 的强化学习微调，使其基于当前驾驶条件预测轨迹与动作。随后，扩散策略头在 VLA 提供当前与预测状态 token 的条件下生成层次化动作与轨迹，并由动态引导的精炼头优化轨迹输出。方法采用全离线强化学习范式，在 nuScenes 开环规划与 Bench2Drive 闭环基准上均取得最先进性能。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出 VDRive 框架：通过 VLA 预测未来状态 token 与扩散策略头联合训练，实现上下文与几何对齐的驾驶决策。</li><li>构建强化学习偏好数据集：基于 nuScenes 与 Bench2Drive 构造“选择/拒绝”视觉-动作对用于 VLA 微调。</li><li>设计离线奖励数据：结合规则奖励与专家 VLM（Qwen2.5-VL-72B）评分，训练扩散策略头并构建 actor-critic 强化学习闭环。</li><li>在多基准上实现最先进结果：在 nuScenes 开环评估与 Bench2Drive 闭环评估中均显著提升。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>离散化表示（CVQ-VAE）：训练条件向量量化变分自编码器，以轨迹为条件，将原始/分割图像编码为离散观测 token，加入 VLA 分词器词表。</li><li>VLA 强化微调：利用偏好数据训练 VLA 生成未来观测 token、动作信号与导航命令，结合选择/拒绝样本进行偏好对齐。</li><li>扩散策略头学习：在离线状态-动作-奖励-下一状态数据集上，训练扩散模型作为 actor 网络、最小化生成动作与真实动作的重建误差，并通过 critic 网络提供价值反馈，实现累积奖励最大化。</li><li>动态引导的精炼头：将 VLA 与扩散策略的异步动作预测作为条件，通过 Transformer 编码器与 MLP 映射融合位置嵌入与动作特征，最终解码优化轨迹。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据：nuScenes（1000 场景，6 相机/5 雷达/1 激光雷达，2 Hz）与 Bench2Drive（CARLA 仿真）用于开/闭环评估与偏好/奖励数据集构建；利用 Vista 生成合成风险场景。</li><li>预训练/微调：视觉 token 预训练（CVQ-VAE + VLA tokenizer）；VLA 强化微调（偏好对齐）；扩散策略离线强化学习（actor-critic）。</li><li>模型：VLA 基于 InternVL3-8B；扩散策略头使用扩散模型；奖励评估采用 Qwen2.5-VL-72B。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>开环评估（nuScenes）：平均 L2 误差与碰撞率；在 1s/2s/3s 时间步与总体上报告指标。</li><li>闭环评估（Bench2Drive）：Driving Score、Success Rate、Efficiency、Comfortness；并报告多能力指标（并线、超车、紧急制动、让行、交通标志）。</li><li>消融实验：Bench2Drive-mini 上对比不同数据构造与不同精炼模块（Transformer/LSTM/GRU）设计。</li></ul>"
  },
  {
    "date": "2025-10-16",
    "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
    "link": "http://arxiv.org/abs/2510.14968",
    "summary_markdown": "### 论文研究单位\n- Mingxuan Yan、Yuping Wang、Jiachen Li：加州大学河滨分校（University of California, Riverside）\n- Yuping Wang（共同作者）：密歇根大学（University of Michigan）\n- Zechun Liu：Meta AI\n### 论文概述\n论文针对长程任务中的层次化视觉-语言-行动（VLA）框架问题，提出检索式演示分解器（RDD）。现有VLM规划器微调依赖人工或启发式子任务分解，易导致子任务与低层视觉运动策略训练数据不一致，降低任务性能。RDD通过视觉特征检索自动分解演示，将子任务与策略训练数据对齐，使用动态规划优化分解策略，实现计划器与策略协同。\n### 论文核心贡献点\n- 首次协调高层规划器与低层视觉运动策略，通过生成对齐的规划器微调数据集提升长程任务性能。\n- 提出RDD框架，训练自由的检索式演示分解方法，形式化为最优分割问题，用动态规划高效求解（含理论分析）。\n- 在仿真和真实基准评估中，RDD优于启发式分解器（如UVD），展现跨设置鲁棒性。\n### 论文方法描述\n- **问题建模**：将演示分解形式化为最优分割问题（公式3.1），最大化分割策略与策略训练数据的相似性。\n- **动态规划求解**：利用最优性原理，将复杂度从O(2^N)降至O(N^2)（有界区间时O(N)）（算法1）。\n- **区间评分函数**：定义区间相似性（公式3.3），结合视觉编码器（如LIV）、近似最近邻检索（Annoy）和时间持续差异。\n- **视觉特征**：使用视觉编码器嵌入间隔（公式3.5），结合开始和结束帧，计算间隔相似度（公式3.6）。\n- **OOD处理**：扩展方法处理分布外子任务，结合检索和启发式（公式3.7-3.8）。\n### 论文使用数据集和训练资源\n- **数据集**：RLBench机器人操作基准，使用训练集1908个演示（分解为12700子任务区间）。\n- **视觉编码器**：LIV（主用），对比R3M、VIP、VC-1、CLIP、DINOv2、ResNet。\n- **检索方法**：Annoy近似最近邻搜索（Angular距离）。\n- **规划器**：LLaVA-based VLM（llama3-llava-next-8B），LoRA微调（rank 128，scale 256）。\n- **计算资源**：4个NVIDIA 6000 Ada GPU，微调约5分钟。\n### 论文使用的评估环境和评估指标\n- **评估环境**：RLBench仿真基准，13个任务（成功率高>35%）。\n- **评估指标**：\n - 多任务成功率（%，平均值和标准差）。\n - 平均排名（↓）。\n- **比较基线**：Expert（启发式专家）、UVD（视觉分解器）、Uniform（均匀分割）、w/o Finetune（不微调）。\n- **实验设置**：每个任务3个演示用于微调；结果平均10次随机种子。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>Mingxuan Yan、Yuping Wang、Jiachen Li：加州大学河滨分校（University of California, Riverside）</li><li>Yuping Wang（共同作者）：密歇根大学（University of Michigan）</li><li>Zechun Liu：Meta AI</li></ul>\n<h3>论文概述</h3>\n<p>论文针对长程任务中的层次化视觉-语言-行动（VLA）框架问题，提出检索式演示分解器（RDD）。现有VLM规划器微调依赖人工或启发式子任务分解，易导致子任务与低层视觉运动策略训练数据不一致，降低任务性能。RDD通过视觉特征检索自动分解演示，将子任务与策略训练数据对齐，使用动态规划优化分解策略，实现计划器与策略协同。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首次协调高层规划器与低层视觉运动策略，通过生成对齐的规划器微调数据集提升长程任务性能。</li><li>提出RDD框架，训练自由的检索式演示分解方法，形式化为最优分割问题，用动态规划高效求解（含理论分析）。</li><li>在仿真和真实基准评估中，RDD优于启发式分解器（如UVD），展现跨设置鲁棒性。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>问题建模</strong>：将演示分解形式化为最优分割问题（公式3.1），最大化分割策略与策略训练数据的相似性。</li><li><strong>动态规划求解</strong>：利用最优性原理，将复杂度从O(2^N)降至O(N^2)（有界区间时O(N)）（算法1）。</li><li><strong>区间评分函数</strong>：定义区间相似性（公式3.3），结合视觉编码器（如LIV）、近似最近邻检索（Annoy）和时间持续差异。</li><li><strong>视觉特征</strong>：使用视觉编码器嵌入间隔（公式3.5），结合开始和结束帧，计算间隔相似度（公式3.6）。</li><li><strong>OOD处理</strong>：扩展方法处理分布外子任务，结合检索和启发式（公式3.7-3.8）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：RLBench机器人操作基准，使用训练集1908个演示（分解为12700子任务区间）。</li><li><strong>视觉编码器</strong>：LIV（主用），对比R3M、VIP、VC-1、CLIP、DINOv2、ResNet。</li><li><strong>检索方法</strong>：Annoy近似最近邻搜索（Angular距离）。</li><li><strong>规划器</strong>：LLaVA-based VLM（llama3-llava-next-8B），LoRA微调（rank 128，scale 256）。</li><li><strong>计算资源</strong>：4个NVIDIA 6000 Ada GPU，微调约5分钟。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：RLBench仿真基准，13个任务（成功率高>35%）。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 多任务成功率（%，平均值和标准差）。</p>\n<p> - 平均排名（↓）。</p>\n<ul><li><strong>比较基线</strong>：Expert（启发式专家）、UVD（视觉分解器）、Uniform（均匀分割）、w/o Finetune（不微调）。</li><li><strong>实验设置</strong>：每个任务3个演示用于微调；结果平均10次随机种子。</li></ul>"
  },
  {
    "date": "2025-10-16",
    "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation",
    "link": "http://arxiv.org/abs/2510.14902",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-16",
    "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.14836",
    "summary_markdown": "### 论文研究单位\n- School of Artificial Intelligence, University of the Chinese Academy of Sciences\n- Institute of Automation, Chinese Academy of Sciences\n- Beijing Zhongke Huiling Robot Technology Co.\n### 论文概述\n论文提出QDepth-VLA框架，旨在通过引入量化深度预测作为辅助监督信号，增强视觉-语言-动作（VLA）模型的空间感知和推理能力。现有VLA模型在长时序和精细操作任务中表现不佳，主要因缺乏3D几何理解。QDepth-VLA利用向量量化（VQ-VAE）将深度图转换为离散令牌，并设计专门的深度专家模块预测这些令牌，避免干扰预训练语义对齐。实验在模拟和真实机器人任务上验证了其有效性。\n### 论文核心贡献点\n1. **QDepth-VLA框架**：通过量化深度预测增强VLA模型的空间理解能力。\n2. **深度专家模块**：设计用于预测离散深度令牌，而非回归像素级深度，提供更紧凑的优化友好监督信号。\n3. **性能提升**：在LIBERO基准上相比open π0平均提升7.7%成功率，在Simpler基准上提升6.1%，真实机器人任务提升10.0%。\n### 论文方法描述\n- **深度标注**：使用Video-Depth-Anything (ViDA)生成单目深度估计。\n- **VQ-VAE重建**：预训练VQ-VAE将深度图量化为256个代码向量的离散令牌（网格分辨率16×16）。\n- **QDepth-VLA架构**：基于PaLI-Gemma 3B的VLM，包括预训练VLM、动作专家和深度专家。深度专家采用Transformer架构，输入视觉令牌，预测量化深度令牌。\n- **混合注意力机制**：设计层次化注意力，允许深度令牌关注文本和图像令牌，动作令牌关注所有前序模态，防止干扰预训练VLM。\n- **联合训练程序**：总损失函数为动作损失（CFM损失）加指数衰减的深度损失（交叉熵损失）。优化使用AdamW优化器，余弦退火学习率（200步预热）。\n### 论文使用数据集和训练资源\n- **数据集**：LIBERO（四个子集：Spatial, Object, Goal, Long）、Simpler（Google Robot和WidowX250任务）、真实机器人数据集（Piper Arm任务）。\n- **训练资源**：8×H20 GPU，FSDP并行训练，全局批量大小1024，梯度累积。\n### 论文使用的评估环境和评估指标\n- **评估环境**：模拟环境（LIBERO和Simpler）、真实机器人环境（6-DoF Piper机械臂）。\n- **评估指标**：任务成功率（Success Rate），LIBERO每任务50次rollouts，Simpler每任务240-2400次评估，真实机器人每任务10次试验。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>School of Artificial Intelligence, University of the Chinese Academy of Sciences</li><li>Institute of Automation, Chinese Academy of Sciences</li><li>Beijing Zhongke Huiling Robot Technology Co.</li></ul>\n<h3>论文概述</h3>\n<p>论文提出QDepth-VLA框架，旨在通过引入量化深度预测作为辅助监督信号，增强视觉-语言-动作（VLA）模型的空间感知和推理能力。现有VLA模型在长时序和精细操作任务中表现不佳，主要因缺乏3D几何理解。QDepth-VLA利用向量量化（VQ-VAE）将深度图转换为离散令牌，并设计专门的深度专家模块预测这些令牌，避免干扰预训练语义对齐。实验在模拟和真实机器人任务上验证了其有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>QDepth-VLA框架</strong>：通过量化深度预测增强VLA模型的空间理解能力。</li><li><strong>深度专家模块</strong>：设计用于预测离散深度令牌，而非回归像素级深度，提供更紧凑的优化友好监督信号。</li><li><strong>性能提升</strong>：在LIBERO基准上相比open π0平均提升7.7%成功率，在Simpler基准上提升6.1%，真实机器人任务提升10.0%。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>深度标注</strong>：使用Video-Depth-Anything (ViDA)生成单目深度估计。</li><li><strong>VQ-VAE重建</strong>：预训练VQ-VAE将深度图量化为256个代码向量的离散令牌（网格分辨率16×16）。</li><li><strong>QDepth-VLA架构</strong>：基于PaLI-Gemma 3B的VLM，包括预训练VLM、动作专家和深度专家。深度专家采用Transformer架构，输入视觉令牌，预测量化深度令牌。</li><li><strong>混合注意力机制</strong>：设计层次化注意力，允许深度令牌关注文本和图像令牌，动作令牌关注所有前序模态，防止干扰预训练VLM。</li><li><strong>联合训练程序</strong>：总损失函数为动作损失（CFM损失）加指数衰减的深度损失（交叉熵损失）。优化使用AdamW优化器，余弦退火学习率（200步预热）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：LIBERO（四个子集：Spatial, Object, Goal, Long）、Simpler（Google Robot和WidowX250任务）、真实机器人数据集（Piper Arm任务）。</li><li><strong>训练资源</strong>：8×H20 GPU，FSDP并行训练，全局批量大小1024，梯度累积。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：模拟环境（LIBERO和Simpler）、真实机器人环境（6-DoF Piper机械臂）。</li><li><strong>评估指标</strong>：任务成功率（Success Rate），LIBERO每任务50次rollouts，Simpler每任务240-2400次评估，真实机器人每任务10次试验。</li></ul>"
  },
  {
    "date": "2025-10-16",
    "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning",
    "link": "http://arxiv.org/abs/2510.14300",
    "summary_markdown": "### 论文研究单位\n论文作者来自多个机构，主要包括：\n- 上海交通大学（MoE关键人工智能实验室、自动化与智能传感学院、计算机科学学院）\n- 上海AI实验室\n- 清华大学深圳国际研究生院\n- 香港大学\n- 同济大学\n- D-Robotics\n- 系统控制与信息处理教育部重点实验室（上海交通大学）\n- 信息安全综合管理技术上海市重点实验室\n### 论文概述\n论文针对Vision-Language-Action (VLA)模型在扩展时面临的挑战（如计算资源需求高、实时控制效率与模型容量平衡难题），提出了AdaMoE（Action-Specialized Mixture of Experts）架构。AdaMoE通过继承预训练VLA模型权重，并在动作专家中引入稀疏激活的MoE层来扩展模型容量，同时利用解耦专家选择与权重的创新设计，在保持计算效率的前提下提升性能。论文在仿真基准（LIBERO和RoboTwin 2.0）和真实世界实验中验证了AdaMoE的有效性。\n### 论文核心贡献点\n1. **高效扩展VLA模型**：提出一种从预训练密集VLA模型继承权重并转换为MoE架构的低成本方法。\n2. **解耦专家选择与权重**：引入独立的比例适配器（scale adapter）与路由器（router），解决传统MoE中负载平衡与任务性能冲突的根本问题。\n3. **显著性能提升**：在LIBERO基准提升1.8%，在RoboTwin 2.0域随机化任务提升9.3%，在真实世界实验提升21.5%，验证了实际应用价值。\n4. **专家特化验证**：通过专家激活模式分析，证明专家能捕获有意义的操作行为（如定位、抓取等）。\n### 论文方法描述\n- **架构设计**：基于π₀框架的动作专家由共享专家（处理通用操作）和路由专家（处理特定任务）组成，使用top-k选择（默认k=1）进行稀疏激活。\n- **解耦机制**：路由器（R）负责专家选择以实现负载平衡，比例适配器（S）独立调整专家贡献权重，最终权重为S_i(x) + softmax(R_i(x))。\n- **训练目标**：结合流匹配损失（flow matching loss，用于高频率动作生成）和负载平衡损失（load balancing loss），总损失为L_total = L_τ + λ_balance * L_balance（λ_balance默认0.01）。\n- **专家初始化**：共享专家继承原始FFN权重，路由专家为副本以快速扩展。\n- **推理过程**：使用流匹配进行去噪迭代（从纯噪声生成动作序列）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - LIBERO基准：包含LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long四个任务套件，每个套件含100专家轨迹。\n - RoboTwin 2.0：19个域随机化任务（清洁和随机化环境各100/400轨迹）。\n- **训练资源**：\n - 训练步数：120,000步（批量大小32）。\n - 优化器：AdamW（峰值学习率2.5e-5，路由器学习率5e-5，β1=0.9，β2=0.95）。\n - 其他超参数：动作视野50、专家数4、梯度裁剪1.0、EMA衰减0.99。\n - 计算资源：百度云平台提供计算支持，AgileX Robotics提供机器人平台。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **仿真环境**：LIBERO和RoboTwin 2.0基准（使用域随机化增强鲁棒性）。\n - **真实世界环境**：ALOHA-Agilex双机械臂系统（AgileX Robotics），包括四个任务：堆叠盘子、按铃、调整瓶子、放置杯子。\n- **评估指标**：\n - 主要指标：任务成功率（Success Rate, SR），以百分比表示（每个任务评估50次试验）。\n - 其他分析：专家使用强度（专家激活模式）、消融研究（比较不同架构变体如Vanilla MoE、CSMoE和AdaMoE）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>论文作者来自多个机构，主要包括：</p>\n<ul><li>上海交通大学（MoE关键人工智能实验室、自动化与智能传感学院、计算机科学学院）</li><li>上海AI实验室</li><li>清华大学深圳国际研究生院</li><li>香港大学</li><li>同济大学</li><li>D-Robotics</li><li>系统控制与信息处理教育部重点实验室（上海交通大学）</li><li>信息安全综合管理技术上海市重点实验室</li></ul>\n<h3>论文概述</h3>\n<p>论文针对Vision-Language-Action (VLA)模型在扩展时面临的挑战（如计算资源需求高、实时控制效率与模型容量平衡难题），提出了AdaMoE（Action-Specialized Mixture of Experts）架构。AdaMoE通过继承预训练VLA模型权重，并在动作专家中引入稀疏激活的MoE层来扩展模型容量，同时利用解耦专家选择与权重的创新设计，在保持计算效率的前提下提升性能。论文在仿真基准（LIBERO和RoboTwin 2.0）和真实世界实验中验证了AdaMoE的有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>高效扩展VLA模型</strong>：提出一种从预训练密集VLA模型继承权重并转换为MoE架构的低成本方法。</li><li><strong>解耦专家选择与权重</strong>：引入独立的比例适配器（scale adapter）与路由器（router），解决传统MoE中负载平衡与任务性能冲突的根本问题。</li><li><strong>显著性能提升</strong>：在LIBERO基准提升1.8%，在RoboTwin 2.0域随机化任务提升9.3%，在真实世界实验提升21.5%，验证了实际应用价值。</li><li><strong>专家特化验证</strong>：通过专家激活模式分析，证明专家能捕获有意义的操作行为（如定位、抓取等）。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>架构设计</strong>：基于π₀框架的动作专家由共享专家（处理通用操作）和路由专家（处理特定任务）组成，使用top-k选择（默认k=1）进行稀疏激活。</li><li><strong>解耦机制</strong>：路由器（R）负责专家选择以实现负载平衡，比例适配器（S）独立调整专家贡献权重，最终权重为S_i(x) + softmax(R_i(x))。</li><li><strong>训练目标</strong>：结合流匹配损失（flow matching loss，用于高频率动作生成）和负载平衡损失（load balancing loss），总损失为L_total = L_τ + λ_balance * L_balance（λ_balance默认0.01）。</li><li><strong>专家初始化</strong>：共享专家继承原始FFN权重，路由专家为副本以快速扩展。</li><li><strong>推理过程</strong>：使用流匹配进行去噪迭代（从纯噪声生成动作序列）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - LIBERO基准：包含LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long四个任务套件，每个套件含100专家轨迹。</p>\n<p> - RoboTwin 2.0：19个域随机化任务（清洁和随机化环境各100/400轨迹）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 训练步数：120,000步（批量大小32）。</p>\n<p> - 优化器：AdamW（峰值学习率2.5e-5，路由器学习率5e-5，β1=0.9，β2=0.95）。</p>\n<p> - 其他超参数：动作视野50、专家数4、梯度裁剪1.0、EMA衰减0.99。</p>\n<p> - 计算资源：百度云平台提供计算支持，AgileX Robotics提供机器人平台。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>仿真环境</strong>：LIBERO和RoboTwin 2.0基准（使用域随机化增强鲁棒性）。</p>\n<p> - <strong>真实世界环境</strong>：ALOHA-Agilex双机械臂系统（AgileX Robotics），包括四个任务：堆叠盘子、按铃、调整瓶子、放置杯子。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：任务成功率（Success Rate, SR），以百分比表示（每个任务评估50次试验）。</p>\n<p> - 其他分析：专家使用强度（专家激活模式）、消融研究（比较不同架构变体如Vanilla MoE、CSMoE和AdaMoE）。</p>"
  },
  {
    "date": "2025-10-15",
    "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.13626",
    "summary_markdown": "## 论文研究单位\n- 复旦大学\n- 同济大学\n- 上海创新研究院\n- 新加坡国立大学\n\n（作者分别隶属上述单位，属于跨机构合作研究）\n## 论文概述\n针对视觉‑语言‑动作（VLA）模型在标准基准上表现优异，却在真实场景中易受扰动的现象，本文在 LIBERO 仿真基准上系统地进行鲁棒性评估。通过在七种维度（对象布局、相机视角、机器人初始姿态、语言指令、光照、背景纹理、传感器噪声）上施加可控扰动，揭示模型的脆弱性：对视角和初始状态的微小变化即可导致成功率从 95% 下降到 30% 以下；而语言变化对整体表现影响最小，进一步实验表明多数模型并未真正依赖语言信号。研究还发现模型存在位置偏置、忽略视觉语义以及对组合扰动的负交互效应。为解决这些问题，文中提出了 LIBERO‑Plus 基准、组合泛化差距的统计定义以及基于大规模扰动数据的通用微调方案，实现了显著鲁棒性提升。\n## 论文核心贡献点\n1. **系统性脆弱性分析**：在七种单一维度扰动下对 10 种 VLA 模型进行评估，量化每类扰动的绝对性能跌幅（表 1）。\n2. **诊断框架**：定义扰动随机变量与成功指示器，提出条件成功率、组合概率及协方差形式的组合泛化差距 Δij，并用 χ² 检验显著性。\n3. **深度洞见**：揭示模型对视觉‑动作的固定映射依赖、语言利用率低、位置记忆偏置以及对组合扰动的负交互效应。\n4. **LIBERO‑Plus 基准**：自动化生成 10,030 任务，涵盖七大扰动因子及 21 子组件，按多模型成功率划分为 Level‑1–Level‑5 五级难度（图 5‑6）。\n5. **通用训练策略**：构建超过 20,000 条扰动成功轨迹的通用训练集，对 OpenVLA‑OFT_m 进行混合微调，整体成功率提升至 79.6%（视角扰动 92.8%）。\n6. **开源资源**：公开评估脚本、任务生成代码、难度划分方法及训练配置，便于后续研究复用。\n## 论文方法描述\n1. **扰动维度**\n - 对象布局（添加干扰对象、目标位移）\n - 相机视角（视角/视野变化）\n - 机器人初始姿态（关节角度/位置扰动）\n - 语言指令（语义改写、句式变换）\n - 光照（强度、方向、颜色、阴影）\n - 背景纹理（桌面/场景材质）\n - 传感器噪声（光度失真、抖动、模糊）\n\n2. **模型集合**\n OpenVLA、OpenVLA‑OFT（含_oft_w、_oft_m）、π₀、π₀‑fast、Nora、WorldVLA、UniVLA、RIPT‑VLA 等，涵盖自回归与扩散两大范式。\n\n3. **单维度扰动实验**\n 对每种扰动在 LIBERO 四套任务上评估成功率，记录绝对跌幅（表 1），并分析模型之间的鲁棒差异。\n\n4. **视觉注意分析**\n 将对象布局扰动拆分为“添加干扰对象”与“目标位移”，对比成功率（图 1），验证模型是否聚焦任务关键对象。\n\n5. **光照鲁棒性极端消融**\n - **全黑**：所有相机输入置黑。\n - **第三视角黑**：仅第三人称视角黑掉，保留腕部相机。\n 通过成功率差异解释腕部相机提供的光照不敏感几何线索（图 2）。\n\n6. **语言利用实验**\n - **空指令**：完全删除语言输入。\n - **目标替换**：将指令中的目标对象替换为同场景的其它对象。\n 观察成功率变化（图 3），判断模型是否真正依赖语言。\n\n7. **组合泛化差距定义**\n 设 D_i 为第 i 种扰动是否施加的二值随机变量，Y 为任务成功指示器，定义条件成功率 s(D_i=d_i,D_j=d_j)=P(Y=1\\|D_i=d_i,D_j=d_j)。组合差距 Δ_ij = Cov(D_i,D_j\\|Y=1) = p(D_i=1,D_j=1\\|Y=1) - p(D_i=1\\|Y=1)p(D_j=1\\|Y=1)。通过 2000 次独立实验估计概率，构建热图并进行 χ² 显著性检验（附录 F）。\n\n8. **LIBERO‑Plus 基准构建**\n - 自动生成任务 → 过滤平衡 → 按四模型成功率划分为五级难度。\n - 任务规模：10,030，七扰动因子，21 子组件（图 5‑6）。\n\n9. **通用训练与微调**\n - 基于原始 LIBERO 轨迹扩增 20,000+ 条成功扰动轨迹。\n - 以 OpenVLA‑OFT_m 预训练权重为起点进行混合微调。\n - 在 LIBERO‑Plus 上评估，实现跨扰动显著提升（表 2）。\n## 论文使用数据集和训练资源\n- **基础数据**：LIBERO 基准（4 套任务），每个套件提供对象、语言指令和标准成功轨迹。\n- **扰动生成**：自动化在七大扰动维度上生成场景与轨迹，筛选后形成 10,030 评估任务。\n- **训练集**：超过 20,000 条成功扰动轨迹，覆盖对象布局、相机、光照、背景、噪声等多维变化。轨迹统一保存格式（附录 D）。\n- **微调配置**：使用官方 OpenVLA‑OFT_m 权重进行混合微调；具体超参数与训练过程详见附录 D。\n- **计算资源**：所有实验在 LIBERO 仿真平台完成，未在文中披露具体硬件，但规模要求大量随机生成的评估样本。\n## 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO 仿真平台，支持第三人称视角与腕部相机（如模型使用）。\n- **模型评估**：\n 1. **单维度扰动**：在 4 套标准任务上分别施加七种扰动，记录原始成功率及绝对跌幅（表 1）。\n 2. **组合扰动**：对选定模型在两两扰动组合下进行 2000 次试验，估计联合概率并计算 Δ_ij（热图（图 4））。\n 3. **极端消融**：全黑与第三人称视角黑掉两种场景，检验视觉依赖。\n 4. **语言实验**：空指令与目标替换任务，评估语言利用率。\n- **指标**：\n - **成功率（%）**——任务执行成功的比例。\n - **绝对跌幅**——相对原始成功率的下降低点（表 1）。\n - **组合泛化差距 Δ_ij**——协方差形式的交互效应度量。\n - **χ² 检验**——Δ_ij 的统计显著性（附录 F）。\n - **难度分级**——基于多模型成功率分布的任务划分（Level‑1至Level‑5）。\n - **综合成功率**——所有扰动维度的平均表现（表 2）。\n\n（所有图表与实验细节均可在正文相应章节及附录中查阅）",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>复旦大学</li><li>同济大学</li><li>上海创新研究院</li><li>新加坡国立大学</li></ul>\n\n<p>（作者分别隶属上述单位，属于跨机构合作研究）</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>针对视觉‑语言‑动作（VLA）模型在标准基准上表现优异，却在真实场景中易受扰动的现象，本文在 LIBERO 仿真基准上系统地进行鲁棒性评估。通过在七种维度（对象布局、相机视角、机器人初始姿态、语言指令、光照、背景纹理、传感器噪声）上施加可控扰动，揭示模型的脆弱性：对视角和初始状态的微小变化即可导致成功率从 95% 下降到 30% 以下；而语言变化对整体表现影响最小，进一步实验表明多数模型并未真正依赖语言信号。研究还发现模型存在位置偏置、忽略视觉语义以及对组合扰动的负交互效应。为解决这些问题，文中提出了 LIBERO‑Plus 基准、组合泛化差距的统计定义以及基于大规模扰动数据的通用微调方案，实现了显著鲁棒性提升。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>系统性脆弱性分析</strong>：在七种单一维度扰动下对 10 种 VLA 模型进行评估，量化每类扰动的绝对性能跌幅（表 1）。</li><li><strong>诊断框架</strong>：定义扰动随机变量与成功指示器，提出条件成功率、组合概率及协方差形式的组合泛化差距 Δij，并用 χ² 检验显著性。</li><li><strong>深度洞见</strong>：揭示模型对视觉‑动作的固定映射依赖、语言利用率低、位置记忆偏置以及对组合扰动的负交互效应。</li><li><strong>LIBERO‑Plus 基准</strong>：自动化生成 10,030 任务，涵盖七大扰动因子及 21 子组件，按多模型成功率划分为 Level‑1–Level‑5 五级难度（图 5‑6）。</li><li><strong>通用训练策略</strong>：构建超过 20,000 条扰动成功轨迹的通用训练集，对 OpenVLA‑OFT_m 进行混合微调，整体成功率提升至 79.6%（视角扰动 92.8%）。</li><li><strong>开源资源</strong>：公开评估脚本、任务生成代码、难度划分方法及训练配置，便于后续研究复用。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ol><li><strong>扰动维度</strong></li></ol>\n<p> - 对象布局（添加干扰对象、目标位移）</p>\n<p> - 相机视角（视角/视野变化）</p>\n<p> - 机器人初始姿态（关节角度/位置扰动）</p>\n<p> - 语言指令（语义改写、句式变换）</p>\n<p> - 光照（强度、方向、颜色、阴影）</p>\n<p> - 背景纹理（桌面/场景材质）</p>\n<p> - 传感器噪声（光度失真、抖动、模糊）</p>\n\n<ol><li><strong>模型集合</strong></li></ol>\n<p> OpenVLA、OpenVLA‑OFT（含_oft_w、_oft_m）、π₀、π₀‑fast、Nora、WorldVLA、UniVLA、RIPT‑VLA 等，涵盖自回归与扩散两大范式。</p>\n\n<ol><li><strong>单维度扰动实验</strong></li></ol>\n<p> 对每种扰动在 LIBERO 四套任务上评估成功率，记录绝对跌幅（表 1），并分析模型之间的鲁棒差异。</p>\n\n<ol><li><strong>视觉注意分析</strong></li></ol>\n<p> 将对象布局扰动拆分为“添加干扰对象”与“目标位移”，对比成功率（图 1），验证模型是否聚焦任务关键对象。</p>\n\n<ol><li><strong>光照鲁棒性极端消融</strong></li></ol>\n<p> - <strong>全黑</strong>：所有相机输入置黑。</p>\n<p> - <strong>第三视角黑</strong>：仅第三人称视角黑掉，保留腕部相机。</p>\n<p> 通过成功率差异解释腕部相机提供的光照不敏感几何线索（图 2）。</p>\n\n<ol><li><strong>语言利用实验</strong></li></ol>\n<p> - <strong>空指令</strong>：完全删除语言输入。</p>\n<p> - <strong>目标替换</strong>：将指令中的目标对象替换为同场景的其它对象。</p>\n<p> 观察成功率变化（图 3），判断模型是否真正依赖语言。</p>\n\n<ol><li><strong>组合泛化差距定义</strong></li></ol>\n<p> 设 D_i 为第 i 种扰动是否施加的二值随机变量，Y 为任务成功指示器，定义条件成功率 s(D_i=d_i,D_j=d_j)=P(Y=1\\|D_i=d_i,D_j=d_j)。组合差距 Δ_ij = Cov(D_i,D_j\\|Y=1) = p(D_i=1,D_j=1\\|Y=1) - p(D_i=1\\|Y=1)p(D_j=1\\|Y=1)。通过 2000 次独立实验估计概率，构建热图并进行 χ² 显著性检验（附录 F）。</p>\n\n<ol><li><strong>LIBERO‑Plus 基准构建</strong></li></ol>\n<p> - 自动生成任务 → 过滤平衡 → 按四模型成功率划分为五级难度。</p>\n<p> - 任务规模：10,030，七扰动因子，21 子组件（图 5‑6）。</p>\n\n<ol><li><strong>通用训练与微调</strong></li></ol>\n<p> - 基于原始 LIBERO 轨迹扩增 20,000+ 条成功扰动轨迹。</p>\n<p> - 以 OpenVLA‑OFT_m 预训练权重为起点进行混合微调。</p>\n<p> - 在 LIBERO‑Plus 上评估，实现跨扰动显著提升（表 2）。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>基础数据</strong>：LIBERO 基准（4 套任务），每个套件提供对象、语言指令和标准成功轨迹。</li><li><strong>扰动生成</strong>：自动化在七大扰动维度上生成场景与轨迹，筛选后形成 10,030 评估任务。</li><li><strong>训练集</strong>：超过 20,000 条成功扰动轨迹，覆盖对象布局、相机、光照、背景、噪声等多维变化。轨迹统一保存格式（附录 D）。</li><li><strong>微调配置</strong>：使用官方 OpenVLA‑OFT_m 权重进行混合微调；具体超参数与训练过程详见附录 D。</li><li><strong>计算资源</strong>：所有实验在 LIBERO 仿真平台完成，未在文中披露具体硬件，但规模要求大量随机生成的评估样本。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：LIBERO 仿真平台，支持第三人称视角与腕部相机（如模型使用）。</li><li><strong>模型评估</strong>：</li></ul>\n<p> 1. <strong>单维度扰动</strong>：在 4 套标准任务上分别施加七种扰动，记录原始成功率及绝对跌幅（表 1）。</p>\n<p> 2. <strong>组合扰动</strong>：对选定模型在两两扰动组合下进行 2000 次试验，估计联合概率并计算 Δ_ij（热图（图 4））。</p>\n<p> 3. <strong>极端消融</strong>：全黑与第三人称视角黑掉两种场景，检验视觉依赖。</p>\n<p> 4. <strong>语言实验</strong>：空指令与目标替换任务，评估语言利用率。</p>\n<ul><li><strong>指标</strong>：</li></ul>\n<p> - <strong>成功率（%）</strong>——任务执行成功的比例。</p>\n<p> - <strong>绝对跌幅</strong>——相对原始成功率的下降低点（表 1）。</p>\n<p> - <strong>组合泛化差距 Δ_ij</strong>——协方差形式的交互效应度量。</p>\n<p> - <strong>χ² 检验</strong>——Δ_ij 的统计显著性（附录 F）。</p>\n<p> - <strong>难度分级</strong>——基于多模型成功率分布的任务划分（Level‑1至Level‑5）。</p>\n<p> - <strong>综合成功率</strong>——所有扰动维度的平均表现（表 2）。</p>\n\n<p>（所有图表与实验细节均可在正文相应章节及附录中查阅）</p>"
  },
  {
    "date": "2025-10-15",
    "title": "DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning",
    "link": "http://arxiv.org/abs/2510.13375",
    "summary_markdown": "## 论文研究单位\nIIIS, 清华大学;Galaxea AI\n## 论文概述\nDepthVLA是一篇针对视觉-语言-动作模型（Vision-Language-Action, VLA）在精确空间推理方面表现不佳的研究。当前基于大规模语言/视觉预训练的VLA虽然在语义理解上表现强劲，但普遍缺乏精细的空间理解，导致在精准抓取、避障与精细操作任务中失败。DepthVLA通过引入预训练的深度预测模块作为“深度专家”，在保持语言理解与开放词汇感知的同时显式增强几何推理能力。\n\nDepthVLA采用“混合 transformer”（Mixture-of-Transformers, MoT）架构，统一VLM、深度专家与动作专家（流匹配）三者并共享注意力，但采用分块掩码以保护各专家的预训练能力。深度专家基于DINOv2编码器、采用Depth Anything V2初始化，并通过尺度不变的对数损失进行训练，为动作专家提供贯穿中间层的几何特征，从而提升抓取与碰撞避免等精细操作能力。\n\n实验在真实世界与仿真环境开展，包括Simpler WidowX与LIBERO等基准。DepthVLA显著优于现有方法：在真实任务中取得78.5% vs 65.0%的成绩，在LIBERO上达94.9% vs 93.6%，在Simpler上为74.8% vs 58.8%。尽管新增约600M参数与约20ms的推理延迟，仍具实际可用性。\n## 论文核心贡献点\n- 深度专家融入MoT的VLA架构：在不破坏语义能力的前提下显式提供空间几何线索，提升精准操作与避障表现。\n- 分模块预训练策略：VLM与深度专家可分别在大规模数据上独立预训练，提高效率与可扩展性，突破仅依赖具身动作数据。\n- 真实与仿真全面验证：在多个基准上实现稳定且显著提升，涵盖精细抓取、碰撞避免与复杂多步任务。\n## 论文方法描述\n- 统一MoT架构：共享注意力的三专家设计（VLM、深度、动作），采用分块掩码使VLM/深度token自注意力、动作token可跨模态注意；保持各专家独立权重。\n- 深度专家：基于DINOv2的编码器-解码器结构，使用Depth Anything V2初始化进行预训练；损失为尺度不变对数损失，使其具备稳健的距离估计与几何感知。深度专家在所有中间层输出几何特征，支撑动作专家的细粒度空间推理。\n- 动作专家：采用流匹配（flow matching）损失对连续动作轨迹建模，与深度损失联合优化，实现端到端训练。\n- 训练策略：深度专家在大规模3D/深度数据上预训练；VLA阶段在具身演示数据上微调，保留深度预测损失以维持几何能力。\n## 论文使用数据集和训练资源\n- 深度预训练：WildRGB-D、ScanNet、ScanNet++、HyperSim等大规模3D/深度数据。\n- VLA训练：Galaxea Open-World（10万轨迹、150任务类、50场景）、BridgeData V2（6万+轨迹）、LIBERO（每个suite 500演示）。\n- 伪深度标签：Depth Anything V2、UniDepth V2、VGGT。\n- 训练资源：32×H100 GPU，AdamW优化；批量与学习率针对不同数据集设置。\n## 论文使用的评估环境和评估指标\n- 仿真评估：\n - Simpler WidowX：4套任务、每套120次试验，报告各任务及平均成功率。\n - LIBERO（Franka Panda）：四个suite（Spatial/Object/Goal/Long），总计2000次试验，每个suite报告成功率。\n- 真实评估：\n - Galaxea R1 Lite双臂移动平台：设计“餐桌清理”“微波操作”“方块叠放”三类任务。\n - 指标：进度分数（每步成功计一分），每任务平均20次；并提供少样本微调（20条轨迹）设置下的比较。\n- 推理与资源指标：显存（8.0GB vs 6.7GB）、延迟（210ms/步 vs 190ms/步），参数增加约600M。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>IIIS, 清华大学;Galaxea AI</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>DepthVLA是一篇针对视觉-语言-动作模型（Vision-Language-Action, VLA）在精确空间推理方面表现不佳的研究。当前基于大规模语言/视觉预训练的VLA虽然在语义理解上表现强劲，但普遍缺乏精细的空间理解，导致在精准抓取、避障与精细操作任务中失败。DepthVLA通过引入预训练的深度预测模块作为“深度专家”，在保持语言理解与开放词汇感知的同时显式增强几何推理能力。</p>\n\n<p>DepthVLA采用“混合 transformer”（Mixture-of-Transformers, MoT）架构，统一VLM、深度专家与动作专家（流匹配）三者并共享注意力，但采用分块掩码以保护各专家的预训练能力。深度专家基于DINOv2编码器、采用Depth Anything V2初始化，并通过尺度不变的对数损失进行训练，为动作专家提供贯穿中间层的几何特征，从而提升抓取与碰撞避免等精细操作能力。</p>\n\n<p>实验在真实世界与仿真环境开展，包括Simpler WidowX与LIBERO等基准。DepthVLA显著优于现有方法：在真实任务中取得78.5% vs 65.0%的成绩，在LIBERO上达94.9% vs 93.6%，在Simpler上为74.8% vs 58.8%。尽管新增约600M参数与约20ms的推理延迟，仍具实际可用性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>深度专家融入MoT的VLA架构：在不破坏语义能力的前提下显式提供空间几何线索，提升精准操作与避障表现。</li><li>分模块预训练策略：VLM与深度专家可分别在大规模数据上独立预训练，提高效率与可扩展性，突破仅依赖具身动作数据。</li><li>真实与仿真全面验证：在多个基准上实现稳定且显著提升，涵盖精细抓取、碰撞避免与复杂多步任务。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>统一MoT架构：共享注意力的三专家设计（VLM、深度、动作），采用分块掩码使VLM/深度token自注意力、动作token可跨模态注意；保持各专家独立权重。</li><li>深度专家：基于DINOv2的编码器-解码器结构，使用Depth Anything V2初始化进行预训练；损失为尺度不变对数损失，使其具备稳健的距离估计与几何感知。深度专家在所有中间层输出几何特征，支撑动作专家的细粒度空间推理。</li><li>动作专家：采用流匹配（flow matching）损失对连续动作轨迹建模，与深度损失联合优化，实现端到端训练。</li><li>训练策略：深度专家在大规模3D/深度数据上预训练；VLA阶段在具身演示数据上微调，保留深度预测损失以维持几何能力。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>深度预训练：WildRGB-D、ScanNet、ScanNet++、HyperSim等大规模3D/深度数据。</li><li>VLA训练：Galaxea Open-World（10万轨迹、150任务类、50场景）、BridgeData V2（6万+轨迹）、LIBERO（每个suite 500演示）。</li><li>伪深度标签：Depth Anything V2、UniDepth V2、VGGT。</li><li>训练资源：32×H100 GPU，AdamW优化；批量与学习率针对不同数据集设置。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>仿真评估：</li></ul>\n<p> - Simpler WidowX：4套任务、每套120次试验，报告各任务及平均成功率。</p>\n<p> - LIBERO（Franka Panda）：四个suite（Spatial/Object/Goal/Long），总计2000次试验，每个suite报告成功率。</p>\n<ul><li>真实评估：</li></ul>\n<p> - Galaxea R1 Lite双臂移动平台：设计“餐桌清理”“微波操作”“方块叠放”三类任务。</p>\n<p> - 指标：进度分数（每步成功计一分），每任务平均20次；并提供少样本微调（20条轨迹）设置下的比较。</p>\n<ul><li>推理与资源指标：显存（8.0GB vs 6.7GB）、延迟（210ms/步 vs 190ms/步），参数增加约600M。</li></ul>"
  },
  {
    "date": "2025-10-15",
    "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.13237",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-15",
    "title": "RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation",
    "link": "http://arxiv.org/abs/2510.13149",
    "summary_markdown": "### 论文研究单位\n南京大学、香港科技大学、新加坡国立大学、上海交通大学、上海创新研究院等机构联合研究\n### 论文概述\n机器人长期操作任务中的组合泛化能力面临挑战，现有模型在扰动条件下难以有效组合技能。针对此问题，研究提出RoboHiMan层次化评估范式，通过HiMan-Bench基准测试系统评估模型在原子任务和组合任务上的泛化能力，结合多级训练数据集和三种评估模式，分析分层架构的瓶颈。\n### 论文核心贡献点\n1. **构建HiMan-Bench基准**：包含114个原子任务和144个组合任务，覆盖12种扰动因素（颜色、纹理、光照等）\n2. **设计层次化评估范式**：提出Vanilla、Decoupled、Coupled三种评估模式，分离测试规划和执行能力\n3. **分析模型能力缺口**：发现VLA模型在组合任务和扰动场景下性能显著下降，揭示分层架构的脆弱性\n### 论文方法描述\n- **基准测试设计**：\n - 原子任务：10类基础操作（开抽屉、拿取物品等）\n - 组合任务：12类多步骤操作（取物放抽屉、转移物品等）\n - 扰动因素：对象颜色/纹理/大小、接收对象属性、光照、桌面纹理、干扰物、背景纹理、相机位姿\n- **多级训练数据集**：\n - L1：仅原子任务演示（每任务20条）\n - L2：添加扰动原子任务（每任务1条）\n - L3：添加4个组合任务（每任务5条）\n - L4：添加扰动组合任务（每任务1条）\n- **评估模式**：\n - Vanilla：端到端执行无规划器\n - Decoupled：规则规划器/视觉语言模型规划器独立评估\n - Coupled：端到端分层系统（视觉语言模型规划器+低层策略）\n### 论文使用数据集和训练资源\n- **仿真环境**：基于CoppeliaSim/PyRep构建的HiMan-Bench\n- **训练数据**：四个多级数据集（L1-L4），总演示量递增\n- **模型**：四类VLA模型（RVT-2、3D Diffuser Actor、π₀、π₀.₅）\n- **规划器**：Qwen2.5-VL视觉语言模型（用于高层规划）\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真测试：114个原子任务和144个组合任务，每任务15个无扰动测试+5个扰动测试+5个全扰动测试\n - 真实世界验证：设计小规模原子/组合任务集，测试扰动鲁棒性\n- **评估指标**：\n - **原子任务**：720集评估，平均成功率（A/AP任务）\n - **组合任务**：900集评估，平均成功率（C/CP任务）\n - **规划准确率**：离线评估视觉语言模型的分任务预测精度\n - **性能下降率**：在线评估对比离线基准的降幅（如L4→C&CP任务下降0.382）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南京大学、香港科技大学、新加坡国立大学、上海交通大学、上海创新研究院等机构联合研究</p>\n<h3>论文概述</h3>\n<p>机器人长期操作任务中的组合泛化能力面临挑战，现有模型在扰动条件下难以有效组合技能。针对此问题，研究提出RoboHiMan层次化评估范式，通过HiMan-Bench基准测试系统评估模型在原子任务和组合任务上的泛化能力，结合多级训练数据集和三种评估模式，分析分层架构的瓶颈。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>构建HiMan-Bench基准</strong>：包含114个原子任务和144个组合任务，覆盖12种扰动因素（颜色、纹理、光照等）</li><li><strong>设计层次化评估范式</strong>：提出Vanilla、Decoupled、Coupled三种评估模式，分离测试规划和执行能力</li><li><strong>分析模型能力缺口</strong>：发现VLA模型在组合任务和扰动场景下性能显著下降，揭示分层架构的脆弱性</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>基准测试设计</strong>：</li></ul>\n<p> - 原子任务：10类基础操作（开抽屉、拿取物品等）</p>\n<p> - 组合任务：12类多步骤操作（取物放抽屉、转移物品等）</p>\n<p> - 扰动因素：对象颜色/纹理/大小、接收对象属性、光照、桌面纹理、干扰物、背景纹理、相机位姿</p>\n<ul><li><strong>多级训练数据集</strong>：</li></ul>\n<p> - L1：仅原子任务演示（每任务20条）</p>\n<p> - L2：添加扰动原子任务（每任务1条）</p>\n<p> - L3：添加4个组合任务（每任务5条）</p>\n<p> - L4：添加扰动组合任务（每任务1条）</p>\n<ul><li><strong>评估模式</strong>：</li></ul>\n<p> - Vanilla：端到端执行无规划器</p>\n<p> - Decoupled：规则规划器/视觉语言模型规划器独立评估</p>\n<p> - Coupled：端到端分层系统（视觉语言模型规划器+低层策略）</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>仿真环境</strong>：基于CoppeliaSim/PyRep构建的HiMan-Bench</li><li><strong>训练数据</strong>：四个多级数据集（L1-L4），总演示量递增</li><li><strong>模型</strong>：四类VLA模型（RVT-2、3D Diffuser Actor、π₀、π₀.₅）</li><li><strong>规划器</strong>：Qwen2.5-VL视觉语言模型（用于高层规划）</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真测试：114个原子任务和144个组合任务，每任务15个无扰动测试+5个扰动测试+5个全扰动测试</p>\n<p> - 真实世界验证：设计小规模原子/组合任务集，测试扰动鲁棒性</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>原子任务</strong>：720集评估，平均成功率（A/AP任务）</p>\n<p> - <strong>组合任务</strong>：900集评估，平均成功率（C/CP任务）</p>\n<p> - <strong>规划准确率</strong>：离线评估视觉语言模型的分任务预测精度</p>\n<p> - <strong>性能下降率</strong>：在线评估对比离线基准的降幅（如L4→C&CP任务下降0.382）</p>"
  },
  {
    "date": "2025-10-15",
    "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
    "link": "http://arxiv.org/abs/2510.13054",
    "summary_markdown": "## 论文研究单位\n**NVIDIA**\n## 论文概述\nVLA-0 探讨了一种简洁的视觉语言动作模型（VLA）构建方法。该方法直接利用预训练视觉语言模型（VLM）的文本生成能力，通过自然语言字符串表示机器人动作（如坐标、关节角度），而非修改模型架构或引入新的动作表示机制。核心创新在于，通过精心设计的训练和推理配方（如动作集成预测和遮蔽动作增强），这种极简设计在性能上可与更复杂的VLA方法媲美。实验表明，在 LIBERO 仿真基准测试中，VLA-0 在不使用大规模预训练数据的情况下超越了所有基线方法；在真实世界评估中，也击败了基于SO-100数据集预训练的SmolVLA。\n## 论文核心贡献点\n1. **挑战VLA设计复杂性：** 证明了基于纯文本动作表示的简单VLA设计（无需修改VLM）可达到甚至超越更复杂的VLA架构（如离散token、生成式动作头或定制架构）的性能。\n2. **提出有效训练配方：** 详细阐述了实现高性能的关键技巧：动作文本解码（如归一化到整数）、推理阶段的动作集成预测（平均多个时间步预测）、训练阶段的遮蔽动作增强（随机遮蔽字符以强制模型基于视觉信息推理）。\n## 论文方法描述\n* **核心原理：** VLA-0 直接提示预训练的VLM（使用Qwen-VL-2.5-3B）输出动作文本序列。输入包含系统提示（定义任务）、图像（单一或拼接）、任务指令。\n* **动作表示：** 连续机器人动作（如末端执行器位置）被归一化到固定整数范围（如[0, 1000]），VLM输出该范围内的空间分隔整数序列，代表未来多个时间步的动作。\n* **推理阶段（关键配方）：**\n * **动作集成预测：** 模型预测未来n步动作。对当前时间步t，可获得t、t-1、...、t-n+1时刻预测的t步动作（作为其预测序列中的第一个或后续动作）。对这些n个预测取平均作为最终动作输出，增强稳定性。\n * **动作文本解码：** 解析VLM输出文本为具体动作值。\n* **训练阶段（关键配方）：**\n * **遮蔽动作增强：** 随机遮蔽动作字符串中的字符（如替换为占位符），迫使模型不依赖序列自回归完成，而是根据视觉和指令信息推理目标动作。\n * **优化：** 使用Adam优化器，64个epoch，批量大小192，学习率5e-6，在8个A100 GPU上训练约32小时。\n## 论文使用数据集和训练资源\n* **仿真数据：** 基于 LIBERO 基准测试。包含四个套件（空间推理、目标泛化、目标导向、长期任务），每个套件10个任务。VLA-0 在LIBERO内域数据上训练，未使用大规模预训练数据。\n* **真实数据：** 基于 SO-100 机器人平台和LeRobot框架。四个不同任务（重定位方块、推动苹果、拾取放置香蕉/纸杯蛋糕），每个任务100个演示。\n* **训练资源：** 使用8个 NVIDIA A100 GPU 进行约32小时的训练。\n## 论文使用的评估环境和评估指标\n* **评估环境：**\n * **仿真：** LIBERO 基准测试（四个套件，40个任务）。\n * **真实世界：** SO-100 机器人平台（基于LeRobot框架）。\n* **评估指标：**\n * **主要指标：** 任务成功率（Success Rate）。\n * **报告方式：** LIBERO 报告每个套件（10个任务）的平均成功率及总体平均成功率；SO-100 报告各任务成功率及总体平均成功率。所有评估均在固定次数的回合（如LIBERO每任务50回合）上进行。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p><strong>NVIDIA</strong></p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>VLA-0 探讨了一种简洁的视觉语言动作模型（VLA）构建方法。该方法直接利用预训练视觉语言模型（VLM）的文本生成能力，通过自然语言字符串表示机器人动作（如坐标、关节角度），而非修改模型架构或引入新的动作表示机制。核心创新在于，通过精心设计的训练和推理配方（如动作集成预测和遮蔽动作增强），这种极简设计在性能上可与更复杂的VLA方法媲美。实验表明，在 LIBERO 仿真基准测试中，VLA-0 在不使用大规模预训练数据的情况下超越了所有基线方法；在真实世界评估中，也击败了基于SO-100数据集预训练的SmolVLA。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>挑战VLA设计复杂性：</strong> 证明了基于纯文本动作表示的简单VLA设计（无需修改VLM）可达到甚至超越更复杂的VLA架构（如离散token、生成式动作头或定制架构）的性能。</li><li><strong>提出有效训练配方：</strong> 详细阐述了实现高性能的关键技巧：动作文本解码（如归一化到整数）、推理阶段的动作集成预测（平均多个时间步预测）、训练阶段的遮蔽动作增强（随机遮蔽字符以强制模型基于视觉信息推理）。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li><strong>核心原理：</strong> VLA-0 直接提示预训练的VLM（使用Qwen-VL-2.5-3B）输出动作文本序列。输入包含系统提示（定义任务）、图像（单一或拼接）、任务指令。</li><li><strong>动作表示：</strong> 连续机器人动作（如末端执行器位置）被归一化到固定整数范围（如[0, 1000]），VLM输出该范围内的空间分隔整数序列，代表未来多个时间步的动作。</li><li><strong>推理阶段（关键配方）：</strong></li></ul>\n<p> * <strong>动作集成预测：</strong> 模型预测未来n步动作。对当前时间步t，可获得t、t-1、...、t-n+1时刻预测的t步动作（作为其预测序列中的第一个或后续动作）。对这些n个预测取平均作为最终动作输出，增强稳定性。</p>\n<p> * <strong>动作文本解码：</strong> 解析VLM输出文本为具体动作值。</p>\n<ul><li><strong>训练阶段（关键配方）：</strong></li></ul>\n<p> * <strong>遮蔽动作增强：</strong> 随机遮蔽动作字符串中的字符（如替换为占位符），迫使模型不依赖序列自回归完成，而是根据视觉和指令信息推理目标动作。</p>\n<p> * <strong>优化：</strong> 使用Adam优化器，64个epoch，批量大小192，学习率5e-6，在8个A100 GPU上训练约32小时。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>仿真数据：</strong> 基于 LIBERO 基准测试。包含四个套件（空间推理、目标泛化、目标导向、长期任务），每个套件10个任务。VLA-0 在LIBERO内域数据上训练，未使用大规模预训练数据。</li><li><strong>真实数据：</strong> 基于 SO-100 机器人平台和LeRobot框架。四个不同任务（重定位方块、推动苹果、拾取放置香蕉/纸杯蛋糕），每个任务100个演示。</li><li><strong>训练资源：</strong> 使用8个 NVIDIA A100 GPU 进行约32小时的训练。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境：</strong></li></ul>\n<p> * <strong>仿真：</strong> LIBERO 基准测试（四个套件，40个任务）。</p>\n<p> * <strong>真实世界：</strong> SO-100 机器人平台（基于LeRobot框架）。</p>\n<ul><li><strong>评估指标：</strong></li></ul>\n<p> * <strong>主要指标：</strong> 任务成功率（Success Rate）。</p>\n<p> * <strong>报告方式：</strong> LIBERO 报告每个套件（10个任务）的平均成功率及总体平均成功率；SO-100 报告各任务成功率及总体平均成功率。所有评估均在固定次数的回合（如LIBERO每任务50回合）上进行。</p>"
  },
  {
    "date": "2025-10-14",
    "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
    "link": "http://arxiv.org/abs/2510.12796",
    "summary_markdown": "# DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving\n## 论文研究单位\n中国科学院自动化研究所模式识别国家重点实验室(NLPR)与银网智能科技有限公司联合完成\n## 论文概述\nDriveVLA-W0提出以世界模型为核心的训练范式，缓解VLA模型在稀疏动作监督下的“监督赤字”问题。该方法通过预测未来图像，引入密集的自监督信号，同时适配离散与连续视觉表征的两类主流VLA架构，并在端到端自主驾驶中实现实时推理与更高数据扩展性。\n## 论文核心贡献点\n- 识别并解决VLA的“监督赤字”：仅用低维动作信号无法充分驱动大规模VLA模型，世界模型通过预测未来图像提供密集监督\n- 设计跨架构世界建模：离散视觉token采用自回归(AR)世界模型，连续视觉特征采用扩散世界模型\n- 提出轻量化MoE Action Expert：与大型VLA主干通过联合注意力融合，提升推理效率(延迟降至原VLA的63.1%)\n- 揭示动作解码器的扩展规律逆转：在小数据上优的query/flow-matching在大规模数据下被自回归解码超越\n- 放大数据扩展律：在大规模训练中，世界模型持续提升性能，显著优于仅动作监督的基线\n## 论文方法描述\n- VLA基线：处理语言指令、前视图像与历史动作，采用Emu3-8B(VQ离散token)与Qwen2.5-VL-7B(连续ViT特征)双骨干，输出语言/视觉/动作三类特征，动作预测采用交叉熵损失\n- AR世界模型(DriveVLA-W0(VQ))：自回归预测当前图像的离散视觉token，训练目标为下一token预测损失，总损失为动作损失加权的世界模型损失\n- 扩散世界模型(DriveVLA-W0(ViT))：在潜空间进行条件扩散，预测未来帧图像，使用MSE对噪声预测网络优化，总损失为动作损失加权的扩散世界模型损失\n- 两阶段训练：先联合世界模型与动作目标预训练，再结合Action Expert进行动作微调；输入序列采用深交错的多模态时间拼接\n- Action Expert(MoE架构)：大型VLA Expert与500M轻量专家通过联合注意力融合，支持三种动作解码策略(查询式回归、自回归、Flow Matching)\n- 实时性：推理阶段可跳过显式的图像生成过程以保障低延迟\n## 论文使用数据集和训练资源\n- 数据集：NAVSIM v1/v2(基于OpenScene的安全关键场景基准)与大规模内部数据集(7000万帧、100万视频片段，100个挑战场景)\n- 训练资源：NAVSIM实验使用8×L20 GPU(global batch=48)；内部数据集使用64×GPU(global batch=256)；统一使用AdamW与余弦学习率、bfloat16混合精度\n- 对比基线：复现实验的TransFuser-50M与TransFuser-7B(单前视相机配置)\n## 论文使用的评估环境和评估指标\n- NAVSIM v1：NC、DAC、TTC、舒适度C、EP，综合指标PDMS\n- NAVSIM v2：NC、DAC、DDC、TLC、EP、TTC、LK、HC、EC，综合指标EPDMS\n- 内部数据集：3秒6点轨迹的ADE(↓)与碰撞率(↓)，碰撞率计算方法与NC一致\n- 扩展评估：跨域迁移(动作分布差异)、数据扩展律(70k/700k/70M帧)、延迟分析(H200 GPU)、生成保真度与规划性能的关联分析",
    "summary_html": "<h1>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>中国科学院自动化研究所模式识别国家重点实验室(NLPR)与银网智能科技有限公司联合完成</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>DriveVLA-W0提出以世界模型为核心的训练范式，缓解VLA模型在稀疏动作监督下的“监督赤字”问题。该方法通过预测未来图像，引入密集的自监督信号，同时适配离散与连续视觉表征的两类主流VLA架构，并在端到端自主驾驶中实现实时推理与更高数据扩展性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>识别并解决VLA的“监督赤字”：仅用低维动作信号无法充分驱动大规模VLA模型，世界模型通过预测未来图像提供密集监督</li><li>设计跨架构世界建模：离散视觉token采用自回归(AR)世界模型，连续视觉特征采用扩散世界模型</li><li>提出轻量化MoE Action Expert：与大型VLA主干通过联合注意力融合，提升推理效率(延迟降至原VLA的63.1%)</li><li>揭示动作解码器的扩展规律逆转：在小数据上优的query/flow-matching在大规模数据下被自回归解码超越</li><li>放大数据扩展律：在大规模训练中，世界模型持续提升性能，显著优于仅动作监督的基线</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>VLA基线：处理语言指令、前视图像与历史动作，采用Emu3-8B(VQ离散token)与Qwen2.5-VL-7B(连续ViT特征)双骨干，输出语言/视觉/动作三类特征，动作预测采用交叉熵损失</li><li>AR世界模型(DriveVLA-W0(VQ))：自回归预测当前图像的离散视觉token，训练目标为下一token预测损失，总损失为动作损失加权的世界模型损失</li><li>扩散世界模型(DriveVLA-W0(ViT))：在潜空间进行条件扩散，预测未来帧图像，使用MSE对噪声预测网络优化，总损失为动作损失加权的扩散世界模型损失</li><li>两阶段训练：先联合世界模型与动作目标预训练，再结合Action Expert进行动作微调；输入序列采用深交错的多模态时间拼接</li><li>Action Expert(MoE架构)：大型VLA Expert与500M轻量专家通过联合注意力融合，支持三种动作解码策略(查询式回归、自回归、Flow Matching)</li><li>实时性：推理阶段可跳过显式的图像生成过程以保障低延迟</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据集：NAVSIM v1/v2(基于OpenScene的安全关键场景基准)与大规模内部数据集(7000万帧、100万视频片段，100个挑战场景)</li><li>训练资源：NAVSIM实验使用8×L20 GPU(global batch=48)；内部数据集使用64×GPU(global batch=256)；统一使用AdamW与余弦学习率、bfloat16混合精度</li><li>对比基线：复现实验的TransFuser-50M与TransFuser-7B(单前视相机配置)</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>NAVSIM v1：NC、DAC、TTC、舒适度C、EP，综合指标PDMS</li><li>NAVSIM v2：NC、DAC、DDC、TLC、EP、TTC、LK、HC、EC，综合指标EPDMS</li><li>内部数据集：3秒6点轨迹的ADE(↓)与碰撞率(↓)，碰撞率计算方法与NC一致</li><li>扩展评估：跨域迁移(动作分布差异)、数据扩展律(70k/700k/70M帧)、延迟分析(H200 GPU)、生成保真度与规划性能的关联分析</li></ul>"
  },
  {
    "date": "2025-10-14",
    "title": "Reflection-Based Task Adaptation for Self-Improving VLA",
    "link": "http://arxiv.org/abs/2510.12710",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-14",
    "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model",
    "link": "http://arxiv.org/abs/2510.12276",
    "summary_markdown": "### 论文研究单位\n香港科技大学（广州）、清华大学、西湖大学、浙江大学、华南理工大学等机构联合完成\n### 论文概述\n针对视觉-语言-行动（VLA）模型缺乏3D空间理解能力的问题，论文提出**空间强制（Spatial Forcing, SF）**方法。该方法通过将VLA模型的中间视觉表征与3D基础模型（VGGT）的几何表征对齐，在不依赖显式3D传感器或深度估计的条件下，隐式增强VLA的空间推理能力。\n### 论文核心贡献点\n1. **深度探测实验**：验证原始VLA视觉嵌入缺乏有效空间结构\n2. **空间强制（SF）机制**：将VLA中间视觉表征与VGGT输出的空间表征对齐\n3. **显著性能提升**：在仿真和真实环境中超越现有方法，同时提升训练效率（3.8×）和数据利用率\n### 论文方法描述\n- **对齐框架**：\n - 使用VGGT处理多视图图像，生成像素级空间表征\n - 对VLA中间层视觉令牌进行归一化和MLP变换\n - 通过余弦相似度最大化对齐视觉令牌与VGGT的空间表征\n- **监督策略**：监督中层而非最深层（防止视觉特征丢失）\n- **总损失函数**：结合动作生成损失与对齐损失\n- **推理阶段**：与标准VLA模型一致，无额外计算开销\n### 论文使用数据集和训练资源\n- **仿真数据**：\n - LIBERO基准（Spatial/Object/Goal/Long任务套件，共500专家演示）\n - RoboTwin（双机械臂仿真，含简单/困难模式）\n- **真实数据**：双机械臂AgileX平台（主相机+腕部相机）\n- **计算资源**：\n - 训练：8×NVIDIA H100显卡（LIBERO任务），1×H100（RoboTwin任务）\n - 训练迭代：15万次（LIBERO），3万次（RoboTwin）\n - 权重因子α：0.5\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **仿真环境**：LIBERO（四类任务）、RoboTwin（双机械臂）\n - **真实环境**：AgileX双机械臂平台\n- **评估指标**：\n - 任务成功率（Success Rate, SR）\n - 训练收敛速度（迭代次数比较）\n - 数据效率（不同数据比例下的成功率）\n - t-SNE可视化验证表征对齐效果\n- **实验结果**：\n - LIBERO平均SR达98.5%（超越所有2D/3D方法）\n - 训练效率提升3.8×（相同成功率所需迭代减少）\n - 5%数据下实现75.8% SR（数据效率提升5.9×）\n - 真实任务中透明杯堆叠等场景下SR显著提升\n\n论文主页：https://spatial-forcing.github.io/",
    "summary_html": "<h3>论文研究单位</h3>\n<p>香港科技大学（广州）、清华大学、西湖大学、浙江大学、华南理工大学等机构联合完成</p>\n<h3>论文概述</h3>\n<p>针对视觉-语言-行动（VLA）模型缺乏3D空间理解能力的问题，论文提出<strong>空间强制（Spatial Forcing, SF）</strong>方法。该方法通过将VLA模型的中间视觉表征与3D基础模型（VGGT）的几何表征对齐，在不依赖显式3D传感器或深度估计的条件下，隐式增强VLA的空间推理能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>深度探测实验</strong>：验证原始VLA视觉嵌入缺乏有效空间结构</li><li><strong>空间强制（SF）机制</strong>：将VLA中间视觉表征与VGGT输出的空间表征对齐</li><li><strong>显著性能提升</strong>：在仿真和真实环境中超越现有方法，同时提升训练效率（3.8×）和数据利用率</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>对齐框架</strong>：</li></ul>\n<p> - 使用VGGT处理多视图图像，生成像素级空间表征</p>\n<p> - 对VLA中间层视觉令牌进行归一化和MLP变换</p>\n<p> - 通过余弦相似度最大化对齐视觉令牌与VGGT的空间表征</p>\n<ul><li><strong>监督策略</strong>：监督中层而非最深层（防止视觉特征丢失）</li><li><strong>总损失函数</strong>：结合动作生成损失与对齐损失</li><li><strong>推理阶段</strong>：与标准VLA模型一致，无额外计算开销</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>仿真数据</strong>：</li></ul>\n<p> - LIBERO基准（Spatial/Object/Goal/Long任务套件，共500专家演示）</p>\n<p> - RoboTwin（双机械臂仿真，含简单/困难模式）</p>\n<ul><li><strong>真实数据</strong>：双机械臂AgileX平台（主相机+腕部相机）</li><li><strong>计算资源</strong>：</li></ul>\n<p> - 训练：8×NVIDIA H100显卡（LIBERO任务），1×H100（RoboTwin任务）</p>\n<p> - 训练迭代：15万次（LIBERO），3万次（RoboTwin）</p>\n<p> - 权重因子α：0.5</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>仿真环境</strong>：LIBERO（四类任务）、RoboTwin（双机械臂）</p>\n<p> - <strong>真实环境</strong>：AgileX双机械臂平台</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（Success Rate, SR）</p>\n<p> - 训练收敛速度（迭代次数比较）</p>\n<p> - 数据效率（不同数据比例下的成功率）</p>\n<p> - t-SNE可视化验证表征对齐效果</p>\n<ul><li><strong>实验结果</strong>：</li></ul>\n<p> - LIBERO平均SR达98.5%（超越所有2D/3D方法）</p>\n<p> - 训练效率提升3.8×（相同成功率所需迭代减少）</p>\n<p> - 5%数据下实现75.8% SR（数据效率提升5.9×）</p>\n<p> - 真实任务中透明杯堆叠等场景下SR显著提升</p>\n\n<p>论文主页：https://spatial-forcing.github.io/</p>"
  },
  {
    "date": "2025-10-13",
    "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
    "link": "http://arxiv.org/abs/2510.11660",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-13",
    "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "link": "http://arxiv.org/abs/2510.11027",
    "summary_markdown": "# Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning\n## 论文研究单位\n- 论文由上海AI实验室主导，联合中国科学技术大学、上海交通大学、浙江大学、南京大学、复旦大学、清华大学、NUS、东北大学、深圳大学等多机构合作完成。\n- 通讯作者来自上海AI实验室；项目页面与代码仓库公开。\n## 论文概述\n- 针对上游VLM（Vision-Language Model）的具身推理能力与下游VLA（Vision-Language-Action）策略学习之间的关键差距，提出Vlaser：一个将高阶推理与低阶控制协同的VLA基础模型。\n- 构建高质量Vlaser-6M数据引擎，形成600万规模的多任务具身推理语料；系统分析不同VLM初始化与数据类型对VLA微调的影响，缩小互联网数据与具身策略学习之间的域间偏移。\n- 在12项具身推理基准与闭环机器人任务（WidowX/Google Robot）上取得领先结果，公开模型、数据与代码。\n## 论文核心贡献点\n- 统一的具身VLA架构与两阶段训练范式：将InternVL3扩展出“动作专家”，通过流匹配实现未来动作chunk预测，兼顾视觉、语言与控制。\n- 数据引擎与方法论创新：系统整合并增广多源公共数据集，构建覆盖定位（grounding）、空间推理、规划与通用VQA的Vlaser-6M；进一步在SimplerEnv中生成200万面向VLA的“域内”QA数据，缓解视觉域偏移。\n- 实证与洞见：上游具身推理的提升未必转化为下游VLA闭环成功率；与互联网数据相比，针对具体机器人本体与视角的“域内”数据更能加速收敛与提升成功率，并明确域间偏移的核心问题。\n## 论文方法描述\n- 模型结构\n - VLM骨干：基于InternVL3（InternViT图像编码 + Qwen2.5语言模型），尺寸为2B与8B，强调具身常识与端到端控制。\n - 动作专家：在VLM上增设低层控制分支；采用流匹配（flow matching）学习动作向量化场（vector field），以单帧观测、语言指令与机器人状态token为输入，生成未来H步动作序列；VLA流使用非因果注意力。\n - 训练与推理损失：VLM阶段采用自回归语言建模损失；VLA阶段最小化预测向量场与真实去噪向量场的MSE；推理时用欧拉积分从τ=0至τ=1对随机噪声进行去噪，生成动作。\n- 数据引擎\n - 具身定位：1.5M问答（边界框与中心点，标准化至[0,1000]），来源RoboPoint、ShareRobot、Pixmo-Points、Paco-LaVIS、RefSpatial；并从SA-1B分割掩码增广30万标注。\n - 通用/空间推理：1.2M通用RoboVQA + 0.5M空间推理数据；源RoboVQA、Robo2VLM、RoboPoint、SPAR、SpaceR-151k、VILASR，以及ScanNet/ScanNet++/CA-1M/ARKitScenes的10万手工空间样本。\n - 规划：40万数据，含Alpaca-15k-Instruction、MuEP、WAP，以及LLaRP在Habitat的规划轨迹与EgoPlan-IT/EgoCOT的第一人称视频规划样例。\n - VLA域内数据：200万针对WidowX与Google Robot的SimplerEnv生成问答，涵盖定位、空间、规划、通用VQA等类别。\n- 训练配方\n - 阶段一：监督微调InternVL3构建具身VLM骨干。\n - 阶段二：在机器人数据集上对动作专家进行VLA微调；使用流匹配损失训练，H=4，积分步长δ=10。\n## 论文使用数据集和训练资源\n- 具身推理评估数据（12项基准）：ERQA、Ego-Plan2、Where2place、Pointarena、Paco-Lavis、Pixmo-Points、VSI-Bench、RefSpatial-Bench、MMSI-Bench、VLABench、EmbodiedBench（ALFRED/Habitat）。\n- 机器人闭环评估：SimplerEnv（Bridge/WidowX 与 Google Robot 任务集），包含超过5百万帧图像与机器人 эпизодов。\n- 数据引擎与规模：Vlaser-6M由多源公共数据整合与增广（定位、空间推理、规划、通用VQA），以及来自SimplerEnv的200万域内VLM预训练问答对。\n- 预训练骨干：InternVL3-2B/8B（InternViT + Qwen2.5-1.5B/7B），动作专家采用流匹配架构；公开模型、训练与推理代码。\n## 论文使用的评估环境和评估指标\n- 评估环境\n - 具身推理：12项公开基准，涵盖问答、任务规划、定位、空间推理与仿真闭环评估。\n - 闭环机器人控制：SimplerEnv中的WidowX与Google Robot平台（Pick Coke Can、Move Near、Drawer、Carrot on plate、Put eggplant in basket、Spoon on towel、Stack Cube等任务）。\n- 评估指标\n - 具身推理：各基准的任务分数与标准化平均分（Avg）。\n - 闭环控制：成功率（Success Rate），分任务与平均成功率。\n - 训练分析：收敛速度与跨域泛化表现（域内vs互联网数据对VLA微调的影响）。",
    "summary_html": "<h1>Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>论文由上海AI实验室主导，联合中国科学技术大学、上海交通大学、浙江大学、南京大学、复旦大学、清华大学、NUS、东北大学、深圳大学等多机构合作完成。</li><li>通讯作者来自上海AI实验室；项目页面与代码仓库公开。</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>针对上游VLM（Vision-Language Model）的具身推理能力与下游VLA（Vision-Language-Action）策略学习之间的关键差距，提出Vlaser：一个将高阶推理与低阶控制协同的VLA基础模型。</li><li>构建高质量Vlaser-6M数据引擎，形成600万规模的多任务具身推理语料；系统分析不同VLM初始化与数据类型对VLA微调的影响，缩小互联网数据与具身策略学习之间的域间偏移。</li><li>在12项具身推理基准与闭环机器人任务（WidowX/Google Robot）上取得领先结果，公开模型、数据与代码。</li></ul>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>统一的具身VLA架构与两阶段训练范式：将InternVL3扩展出“动作专家”，通过流匹配实现未来动作chunk预测，兼顾视觉、语言与控制。</li><li>数据引擎与方法论创新：系统整合并增广多源公共数据集，构建覆盖定位（grounding）、空间推理、规划与通用VQA的Vlaser-6M；进一步在SimplerEnv中生成200万面向VLA的“域内”QA数据，缓解视觉域偏移。</li><li>实证与洞见：上游具身推理的提升未必转化为下游VLA闭环成功率；与互联网数据相比，针对具体机器人本体与视角的“域内”数据更能加速收敛与提升成功率，并明确域间偏移的核心问题。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>模型结构</li></ul>\n<p> - VLM骨干：基于InternVL3（InternViT图像编码 + Qwen2.5语言模型），尺寸为2B与8B，强调具身常识与端到端控制。</p>\n<p> - 动作专家：在VLM上增设低层控制分支；采用流匹配（flow matching）学习动作向量化场（vector field），以单帧观测、语言指令与机器人状态token为输入，生成未来H步动作序列；VLA流使用非因果注意力。</p>\n<p> - 训练与推理损失：VLM阶段采用自回归语言建模损失；VLA阶段最小化预测向量场与真实去噪向量场的MSE；推理时用欧拉积分从τ=0至τ=1对随机噪声进行去噪，生成动作。</p>\n<ul><li>数据引擎</li></ul>\n<p> - 具身定位：1.5M问答（边界框与中心点，标准化至[0,1000]），来源RoboPoint、ShareRobot、Pixmo-Points、Paco-LaVIS、RefSpatial；并从SA-1B分割掩码增广30万标注。</p>\n<p> - 通用/空间推理：1.2M通用RoboVQA + 0.5M空间推理数据；源RoboVQA、Robo2VLM、RoboPoint、SPAR、SpaceR-151k、VILASR，以及ScanNet/ScanNet++/CA-1M/ARKitScenes的10万手工空间样本。</p>\n<p> - 规划：40万数据，含Alpaca-15k-Instruction、MuEP、WAP，以及LLaRP在Habitat的规划轨迹与EgoPlan-IT/EgoCOT的第一人称视频规划样例。</p>\n<p> - VLA域内数据：200万针对WidowX与Google Robot的SimplerEnv生成问答，涵盖定位、空间、规划、通用VQA等类别。</p>\n<ul><li>训练配方</li></ul>\n<p> - 阶段一：监督微调InternVL3构建具身VLM骨干。</p>\n<p> - 阶段二：在机器人数据集上对动作专家进行VLA微调；使用流匹配损失训练，H=4，积分步长δ=10。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>具身推理评估数据（12项基准）：ERQA、Ego-Plan2、Where2place、Pointarena、Paco-Lavis、Pixmo-Points、VSI-Bench、RefSpatial-Bench、MMSI-Bench、VLABench、EmbodiedBench（ALFRED/Habitat）。</li><li>机器人闭环评估：SimplerEnv（Bridge/WidowX 与 Google Robot 任务集），包含超过5百万帧图像与机器人 эпизодов。</li><li>数据引擎与规模：Vlaser-6M由多源公共数据整合与增广（定位、空间推理、规划、通用VQA），以及来自SimplerEnv的200万域内VLM预训练问答对。</li><li>预训练骨干：InternVL3-2B/8B（InternViT + Qwen2.5-1.5B/7B），动作专家采用流匹配架构；公开模型、训练与推理代码。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境</li></ul>\n<p> - 具身推理：12项公开基准，涵盖问答、任务规划、定位、空间推理与仿真闭环评估。</p>\n<p> - 闭环机器人控制：SimplerEnv中的WidowX与Google Robot平台（Pick Coke Can、Move Near、Drawer、Carrot on plate、Put eggplant in basket、Spoon on towel、Stack Cube等任务）。</p>\n<ul><li>评估指标</li></ul>\n<p> - 具身推理：各基准的任务分数与标准化平均分（Avg）。</p>\n<p> - 闭环控制：成功率（Success Rate），分任务与平均成功率。</p>\n<p> - 训练分析：收敛速度与跨域泛化表现（域内vs互联网数据对VLA微调的影响）。</p>"
  },
  {
    "date": "2025-10-13",
    "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2510.10975",
    "summary_markdown": "### 论文研究单位\n深圳先进技术研究院（中国科学院）、鹏城实验室、中山大学计算机科学与工程学院、南洋理工大学计算与数据科学学院、上海AI实验室、中国科学院大学、X-Era AI Lab\n### 论文概述\nRoVer是一个针对视觉-语言-动作（VLA）模型的外部测试时缩放框架，旨在通过引入一个紧凑的机器人过程奖励模型（PRM）作为测试时验证器，在不修改原始VLA架构或权重的情况下增强其性能。该方法通过生成多个候选动作并使用PRM预测标量过程奖励和动作空间方向，以选择最优动作执行。关键是通过缓存共享感知特征来摊销感知成本，从而实现高效的测试时计算资源分配。\n### 论文核心贡献点\n1. **通用测试时缩放框架**：提出一个即插即用的外部测试时缩放框架，纯在推理时增强冻结的VLA策略，无需额外数据或模型重训练。\n2. **紧凑过程奖励模型**：设计一个同时输出标量过程奖励和动作细化方向的PRM，实现基于验证器的智能候选动作评估和方向引导探索。\n3. **高效方向引导采样策略**：利用共享感知缓存机制，将感知成本摊销到多个候选动作上，在固定测试时计算预算下支持可扩展的候选生成和选择。\n### 论文方法描述\nRoVer的整体方法分为三个部分：\n- **模型架构**：基于GPT-2风格的transformer，初始化自GR-1（使用MAE和CLIP预训练权重），添加奖励和方向预测头。引入共享感知缓存（编码观测、语言和状态特征一次并复用）和动作放大器（MLP结构，强化动作细微差异）。\n- **模型训练**：采用方向监督和偏好学习（Bradley-Terrry损失）训练PRM。数据准备通过锚定中心采样构造“更好/更差”动作对，并预测从当前动作到专家动作的方向向量。目标函数结合方向对齐损失（余弦相似度）和奖励偏好损失。\n- **方向引导测试时缩放**：推理时从基础策略采样N个候选动作，沿PRM预测的方向在有界角度内扩展M个新候选（统一候选预算K=N+M），PRM评分所有候选后选择最优动作执行。方向引导策略优于随机高斯采样，通过集中探索提升效率。\n### 论文使用数据集和训练资源\n- **数据集**：模拟实验使用CALVIN基准（ABC→D设置：训练集为A、B、C环境，测试为未见环境D），仅用20%训练集样本；真实机器人任务基于自定义场景（抓取放置、推按钮、堆叠碗）。\n- **训练资源**：PRM训练使用CALVIN ABC→D训练集100个epoch；模型总参数0.2B（可训练参数40M，冻结MAE和CLIP编码器）；推理在单张NVIDIA V100 GPU上运行，测试时计算预算可配置（如候选数K=10~10000）。\n### 论文使用的评估环境和评估指标\n- **模拟评估环境**：CALVIN基准（长时序语言条件任务），评估设置包括ABC→D跨环境迁移。指标为SR@k（连续完成k个任务的概率，k=1-5）和平均链长（Average Chain Length）。\n- **真实机器人评估环境**：双机械臂Dobot平台（右侧执行任务，左侧固定），使用腕部相机和俯视相机。任务涵盖Seen、Unseen object和Unseen position条件。指标为成功率（%，每条件10次试验）。\n- **效率评估**：使用共享感知缓存对比无缓存的延迟（秒）和加速比（倍），测试候选数10~10000下的吞吐扩展性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>深圳先进技术研究院（中国科学院）、鹏城实验室、中山大学计算机科学与工程学院、南洋理工大学计算与数据科学学院、上海AI实验室、中国科学院大学、X-Era AI Lab</p>\n<h3>论文概述</h3>\n<p>RoVer是一个针对视觉-语言-动作（VLA）模型的外部测试时缩放框架，旨在通过引入一个紧凑的机器人过程奖励模型（PRM）作为测试时验证器，在不修改原始VLA架构或权重的情况下增强其性能。该方法通过生成多个候选动作并使用PRM预测标量过程奖励和动作空间方向，以选择最优动作执行。关键是通过缓存共享感知特征来摊销感知成本，从而实现高效的测试时计算资源分配。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>通用测试时缩放框架</strong>：提出一个即插即用的外部测试时缩放框架，纯在推理时增强冻结的VLA策略，无需额外数据或模型重训练。</li><li><strong>紧凑过程奖励模型</strong>：设计一个同时输出标量过程奖励和动作细化方向的PRM，实现基于验证器的智能候选动作评估和方向引导探索。</li><li><strong>高效方向引导采样策略</strong>：利用共享感知缓存机制，将感知成本摊销到多个候选动作上，在固定测试时计算预算下支持可扩展的候选生成和选择。</li></ol>\n<h3>论文方法描述</h3>\n<p>RoVer的整体方法分为三个部分：</p>\n<ul><li><strong>模型架构</strong>：基于GPT-2风格的transformer，初始化自GR-1（使用MAE和CLIP预训练权重），添加奖励和方向预测头。引入共享感知缓存（编码观测、语言和状态特征一次并复用）和动作放大器（MLP结构，强化动作细微差异）。</li><li><strong>模型训练</strong>：采用方向监督和偏好学习（Bradley-Terrry损失）训练PRM。数据准备通过锚定中心采样构造“更好/更差”动作对，并预测从当前动作到专家动作的方向向量。目标函数结合方向对齐损失（余弦相似度）和奖励偏好损失。</li><li><strong>方向引导测试时缩放</strong>：推理时从基础策略采样N个候选动作，沿PRM预测的方向在有界角度内扩展M个新候选（统一候选预算K=N+M），PRM评分所有候选后选择最优动作执行。方向引导策略优于随机高斯采样，通过集中探索提升效率。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：模拟实验使用CALVIN基准（ABC→D设置：训练集为A、B、C环境，测试为未见环境D），仅用20%训练集样本；真实机器人任务基于自定义场景（抓取放置、推按钮、堆叠碗）。</li><li><strong>训练资源</strong>：PRM训练使用CALVIN ABC→D训练集100个epoch；模型总参数0.2B（可训练参数40M，冻结MAE和CLIP编码器）；推理在单张NVIDIA V100 GPU上运行，测试时计算预算可配置（如候选数K=10~10000）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>模拟评估环境</strong>：CALVIN基准（长时序语言条件任务），评估设置包括ABC→D跨环境迁移。指标为SR@k（连续完成k个任务的概率，k=1-5）和平均链长（Average Chain Length）。</li><li><strong>真实机器人评估环境</strong>：双机械臂Dobot平台（右侧执行任务，左侧固定），使用腕部相机和俯视相机。任务涵盖Seen、Unseen object和Unseen position条件。指标为成功率（%，每条件10次试验）。</li><li><strong>效率评估</strong>：使用共享感知缓存对比无缓存的延迟（秒）和加速比（倍），测试候选数10~10000下的吞吐扩展性。</li></ul>"
  },
  {
    "date": "2025-10-11",
    "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2510.10274",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-11",
    "title": "Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback",
    "link": "http://arxiv.org/abs/2510.10181",
    "summary_markdown": "### 论文研究单位\n上海交通大学计算机科学学院\n### 论文概述\n论文提出Dejavu框架，用于具身代理的后部署学习。该框架通过经验反馈网络（EFN）增强冻结的视觉-语言-行动（VLA）策略。EFN检索与当前上下文相关的历史成功行动经验，并预测残差行动来修正基础策略输出。在部署期间，EFN持续从新经验中学习，尽管基础策略权重固定，但能提升代理的适应性和成功率。实验基于LIBERO模拟器和AgiBot G1机器人，涵盖OpenVLA、UniVLA和GO-1骨干，显示EFN显著改善部署时性能。\n### 论文核心贡献点\n- 引入EFN作为以经验为中心的部署时机制，结合实时体验库和轻量级控制器，改进冻结VLA策略，无需梯度重训练。\n- 形式化体验为同步视觉-语言-行动轨迹，并实现语言条件视觉相似度的检索机制。\n- 将EFN集成至OpenVLA、UniVLA和GO-1骨干，在模拟和真实环境中实现一致的部署时改进。\n### 论文方法描述\n- **体验库设计**：存储步骤级轨迹（图像、视觉令牌、潜在行动），通过均值-最大融合构造紧凑关键向量，采用余弦相似度的概率top-k检索。\n- **残差策略学习**：使用软演员-评论家（SAC）算法训练EFN，以预测残差行动；奖励函数基于语义相似度（当前观察与检索经验的下一观察匹配），并包含反空闲惩罚。\n- **部署时机制**：指令过滤候选集、步进检索与效率优先（倾向短轨迹）、行动修正和执行、在线体验增长（将成功轨迹加入库）。\n- **训练细节**：EFN输入包括当前观察、基础行动和检索经验；上下文编码后通过行动者网络输出残差，评论家网络评估修正行动；损失函数基于SAC目标。\n### 论文使用数据集和训练资源\n- **数据集**：模拟实验使用LIBERO数据集；真实实验基于AgiBot G1机器人。\n- **训练资源**：VLA骨干包括OpenVLA、UniVLA和GO-1；使用Prismatic预训练管道、Flash Attention；LIBERO模拟器用于模拟，AgiBot G1硬件用于现实实验；EFN训练在模拟环境中进行，依赖CUDA加速。\n### 论文使用的评估环境和评估指标\n- **评估环境**：模拟实验在LIBERO平台上进行；真实实验在AgiBot G1机器人上进行。\n- **评估指标**：主要指标为成功率（Success Rate）和成功时的平均步数（Steps）；例如，LIBERO四子任务（Spatial、Object、Goal、Long）的成功率提升和步数减少；真实世界任务（PutBottle、SortItem、AddGoods）显示类似改进。基准比较包括EFN与OpenVLA/UniVLA/GO-1的对比，以及不同体验库容量（Volume 100-1000）的消融研究。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学计算机科学学院</p>\n<h3>论文概述</h3>\n<p>论文提出Dejavu框架，用于具身代理的后部署学习。该框架通过经验反馈网络（EFN）增强冻结的视觉-语言-行动（VLA）策略。EFN检索与当前上下文相关的历史成功行动经验，并预测残差行动来修正基础策略输出。在部署期间，EFN持续从新经验中学习，尽管基础策略权重固定，但能提升代理的适应性和成功率。实验基于LIBERO模拟器和AgiBot G1机器人，涵盖OpenVLA、UniVLA和GO-1骨干，显示EFN显著改善部署时性能。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>引入EFN作为以经验为中心的部署时机制，结合实时体验库和轻量级控制器，改进冻结VLA策略，无需梯度重训练。</li><li>形式化体验为同步视觉-语言-行动轨迹，并实现语言条件视觉相似度的检索机制。</li><li>将EFN集成至OpenVLA、UniVLA和GO-1骨干，在模拟和真实环境中实现一致的部署时改进。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>体验库设计</strong>：存储步骤级轨迹（图像、视觉令牌、潜在行动），通过均值-最大融合构造紧凑关键向量，采用余弦相似度的概率top-k检索。</li><li><strong>残差策略学习</strong>：使用软演员-评论家（SAC）算法训练EFN，以预测残差行动；奖励函数基于语义相似度（当前观察与检索经验的下一观察匹配），并包含反空闲惩罚。</li><li><strong>部署时机制</strong>：指令过滤候选集、步进检索与效率优先（倾向短轨迹）、行动修正和执行、在线体验增长（将成功轨迹加入库）。</li><li><strong>训练细节</strong>：EFN输入包括当前观察、基础行动和检索经验；上下文编码后通过行动者网络输出残差，评论家网络评估修正行动；损失函数基于SAC目标。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：模拟实验使用LIBERO数据集；真实实验基于AgiBot G1机器人。</li><li><strong>训练资源</strong>：VLA骨干包括OpenVLA、UniVLA和GO-1；使用Prismatic预训练管道、Flash Attention；LIBERO模拟器用于模拟，AgiBot G1硬件用于现实实验；EFN训练在模拟环境中进行，依赖CUDA加速。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：模拟实验在LIBERO平台上进行；真实实验在AgiBot G1机器人上进行。</li><li><strong>评估指标</strong>：主要指标为成功率（Success Rate）和成功时的平均步数（Steps）；例如，LIBERO四子任务（Spatial、Object、Goal、Long）的成功率提升和步数减少；真实世界任务（PutBottle、SortItem、AddGoods）显示类似改进。基准比较包括EFN与OpenVLA/UniVLA/GO-1的对比，以及不同体验库容量（Volume 100-1000）的消融研究。</li></ul>"
  },
  {
    "date": "2025-10-11",
    "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.09976",
    "summary_markdown": "## 论文研究单位\n- 脑认知与脑启发智能实验室（中科院自动化所）\n- 中国科学院大学\n- 脑认知与脑启发智能技术国家重点实验室\n- Long-term AI\n## 论文概述\n针对基于条件流匹配（flow matching）的视觉-语言-动作模型（VLA，如 π0），传统策略梯度强化学习方法由于无法计算重要性采样比而难以应用。本文提出“流策略优化”（Flow Policy Optimization，FPO）：用基于条件流匹配（CFM）损失的样本级变化构造“无似然比率”，替换传统重要性采样，实现 PPO 风格的截断代理目标更新；并结合结构感知信用分配、截断代理、潜空间多步欧拉探索与 Q 值集合体，构建稳定且可扩展的在线强化微调框架。在 LIBERO 与 ALOHA Transfer Cube 仿真任务上进行评估，FPO 在多项指标上超越多类强基线，并呈现稳定的学习曲线与稀疏奖励下的有效探索。\n## 论文核心贡献点\n- 首次实现流匹配策略与 PPO 风格策略梯度更新的兼容：无须显式似然或雅可比，计算“可通约的比率代理”。\n- 将样本级 CFM 损失差作为策略改进步信号（结构对齐的信用分配），并配套截断代理以控制更新幅度。\n- 引入潜空间多步欧拉探索与 Q 值集合体（保守目标与 GAE），提升在线学习的稳定性与效率。\n- 在 LIBERO（四个子套件）与 ALOHA Transfer Cube 任务上，π0-FPO 取得一致优于多类强基线的效果（含在线 RL、偏好对齐与扩散/自回归策略），并稳定收敛。\n## 论文方法描述\n- FPO 采用“冷启动/滚动-更新”交替流程：冻结旧策略进行滚动收集经验（轨迹、奖励、隐变量与 CFM 损失缓存），随后在当前策略上重评 CFM 损失，将损失差标准化并映射为无似然比率 ρt，最终采用截断代理目标进行策略更新。\n- 无似然比率：从旧/新策略在同一样本上的 CFM 损失差 Δℓcfm 出发，经批内标准化 zt = (Δℓcfm − μΔ)/σΔ 与单调指数映射 ρt = exp(β zt)，作为 PPO 截断代理中的重要性比替代。\n- 截断代理目标：Lactor(θ) = −E[min(ρt Ât, clip(ρt, 1−ε, 1+ε) Ât)]，其中优势 Ât 由 GAE 估计。\n- Q 值集合体：以最小化集合成员的保守目标 y_t = r_t + γ min_i Q̄_ϕi(s_{t+1}, x′_{t+1}) 进行时序差分训练，并用 Polyak 平均更新目标网络，以减少过估计与不稳定性。\n- 潜空间探索：在隐变量 x 上使用 CFM 速度场 vθ 进行多步欧拉积分生成平滑、时间相关的扰动，增强探索。\n- 数据管理：使用小滑窗轨迹缓存，保持更新策略与数据收集策略的分布接近，保证比率代理的稳定与可信。\n## 论文使用数据集和训练资源\n- 数据与预训练：采用 π0 预训练检查点作为起点，冻结其解码器 π0，仅在线更新流匹配 Actor 与 Q 集合体。\n- 训练方式：在仿真环境中进行在线交互与强化微调，无需大量监督演示数据。细节包括滚动窗口大小、K_update 更新轮次、β 与 ε 等超参数的选择与设置。\n## 论文使用的评估环境和评估指标\n- 评估环境：LIBERO 基准（ LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long）与 ALOHA Transfer Cube 仿真任务；二者均为接触丰富与稀疏奖励的典型具身控制环境。\n- 评估指标：按官方协议报告任务成功率（SR, %），并在 LIBERO 上给出各套件的平均排名；在学习动力学分析中结合成功率与平均回报曲线；消融研究则报告在指定任务上的最终成功率。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>脑认知与脑启发智能实验室（中科院自动化所）</li><li>中国科学院大学</li><li>脑认知与脑启发智能技术国家重点实验室</li><li>Long-term AI</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>针对基于条件流匹配（flow matching）的视觉-语言-动作模型（VLA，如 π0），传统策略梯度强化学习方法由于无法计算重要性采样比而难以应用。本文提出“流策略优化”（Flow Policy Optimization，FPO）：用基于条件流匹配（CFM）损失的样本级变化构造“无似然比率”，替换传统重要性采样，实现 PPO 风格的截断代理目标更新；并结合结构感知信用分配、截断代理、潜空间多步欧拉探索与 Q 值集合体，构建稳定且可扩展的在线强化微调框架。在 LIBERO 与 ALOHA Transfer Cube 仿真任务上进行评估，FPO 在多项指标上超越多类强基线，并呈现稳定的学习曲线与稀疏奖励下的有效探索。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>首次实现流匹配策略与 PPO 风格策略梯度更新的兼容：无须显式似然或雅可比，计算“可通约的比率代理”。</li><li>将样本级 CFM 损失差作为策略改进步信号（结构对齐的信用分配），并配套截断代理以控制更新幅度。</li><li>引入潜空间多步欧拉探索与 Q 值集合体（保守目标与 GAE），提升在线学习的稳定性与效率。</li><li>在 LIBERO（四个子套件）与 ALOHA Transfer Cube 任务上，π0-FPO 取得一致优于多类强基线的效果（含在线 RL、偏好对齐与扩散/自回归策略），并稳定收敛。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>FPO 采用“冷启动/滚动-更新”交替流程：冻结旧策略进行滚动收集经验（轨迹、奖励、隐变量与 CFM 损失缓存），随后在当前策略上重评 CFM 损失，将损失差标准化并映射为无似然比率 ρt，最终采用截断代理目标进行策略更新。</li><li>无似然比率：从旧/新策略在同一样本上的 CFM 损失差 Δℓcfm 出发，经批内标准化 zt = (Δℓcfm − μΔ)/σΔ 与单调指数映射 ρt = exp(β zt)，作为 PPO 截断代理中的重要性比替代。</li><li>截断代理目标：Lactor(θ) = −E[min(ρt Ât, clip(ρt, 1−ε, 1+ε) Ât)]，其中优势 Ât 由 GAE 估计。</li><li>Q 值集合体：以最小化集合成员的保守目标 y_t = r_t + γ min_i Q̄_ϕi(s_{t+1}, x′_{t+1}) 进行时序差分训练，并用 Polyak 平均更新目标网络，以减少过估计与不稳定性。</li><li>潜空间探索：在隐变量 x 上使用 CFM 速度场 vθ 进行多步欧拉积分生成平滑、时间相关的扰动，增强探索。</li><li>数据管理：使用小滑窗轨迹缓存，保持更新策略与数据收集策略的分布接近，保证比率代理的稳定与可信。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据与预训练：采用 π0 预训练检查点作为起点，冻结其解码器 π0，仅在线更新流匹配 Actor 与 Q 集合体。</li><li>训练方式：在仿真环境中进行在线交互与强化微调，无需大量监督演示数据。细节包括滚动窗口大小、K_update 更新轮次、β 与 ε 等超参数的选择与设置。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：LIBERO 基准（ LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long）与 ALOHA Transfer Cube 仿真任务；二者均为接触丰富与稀疏奖励的典型具身控制环境。</li><li>评估指标：按官方协议报告任务成功率（SR, %），并在 LIBERO 上给出各套件的平均排名；在学习动力学分析中结合成功率与平均回报曲线；消融研究则报告在指定任务上的最终成功率。</li></ul>"
  },
  {
    "date": "2025-10-10",
    "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation",
    "link": "http://arxiv.org/abs/2510.09607",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n南京大学、腾讯Youtu Lab、中科院(CASIA)\n## 论文概述\nVITA-VLA论文提出了一种高效的教学框架，通过动作专家蒸馏技术将小型动作模型的知识转移到预训练的大规模视觉-语言模型(VLM)中，使其具备动作执行能力。该方法解决了传统VLA模型需要大量计算资源和数据进行端到端训练的局限，通过两阶段训练策略显著降低训练成本，同时保持高性能表现。在仿真基准(LIBERO和CALVIN)和真实机器人环境中的实验验证了该方法的有效性。\n## 论文核心贡献点\n1. **精简架构设计**：在保持原始VLM结构的基础上，仅添加动作token和状态编码器，集成物理输入信息\n2. **两阶段训练策略**：第一阶段进行轻量级对齐(仅3000万参数)，第二阶段选择性微调，有效降低训练成本\n3. **优秀性能表现**：在LIBERO基准上达到97.3%平均成功率(提升11.8%)，在LIBERO-LONG上达到93.5%成功率(提升24.5%)，在CALVIN ABC-D基准上达到92.5%第一任务成功率\n4. **真实世界验证**：在ALOHA机器人平台上实现82.0%平均成功率，较teacher模型提升17%\n## 论文方法描述\n### 整体架构\n- **视觉编码器**：InternViT-300M处理图像输入\n- **连接器**：3层MLP桥接视觉和语言模态\n- **语言模型**：Qwen-2.5-7B作为基础\n- **状态编码器**：将6-DoF机械臂状态和2维夹爪状态编码为单token\n- **动作token**：作为可学习查询，重复3次预测未来3步动作\n- **动作映射器**：3层MLP将VLM隐藏状态映射到动作空间\n- **动作解码器**：复用的2层MLP生成最终执行动作\n### 两阶段训练策略\n**阶段1-对齐阶段**：\n- 训练状态编码器、动作token和动作映射器(约3000万参数)\n- 使用MSE损失对齐VLM和小动作模型的隐藏表示空间\n- 复用预训练动作解码器，避免昂贵端到端预训练\n\n**阶段2-微调阶段**：\n- 端到端微调语言模型、状态编码器、动作模块\n- 使用MAE损失监督6-DoF臂动作，BCE损失监督夹爪动作\n- 组合损失函数：L_total = L_arm + λ·L_gripper (λ=0.01)\n## 论文使用数据集和训练资源\n### 仿真数据集\n- **CALVIN ABC-D**：训练于环境A、B、C，测试于未见环境D，评估零样本泛化\n- **LIBERO基准**：4个任务套件(Spatial、Object、Goal、LONG)，每个包含10个长时序任务\n### 真实世界数据\n- 手动收集500个高质量演示轨迹(每任务100个)\n- 覆盖5个操作任务：close drawer、stack cups、stack blocks、pick place sponge、pick place block\n### 训练资源\n- 使用DeepSpeed ZeRO-2阶段优化内存使用\n- 对齐阶段：batch size=8，学习率=1e-4，训练3个epoch\n- 微调阶段：batch size=4，学习率=1e-4，训练2个epoch\n- 图像分辨率：统一为200×200像素\n## 论文使用的评估环境和评估指标\n### 评估环境\n- **仿真环境**：CALVIN和LIBERO基准测试平台\n- **真实世界**：ALOHA机器人平台(6关节PiPer机械臂+Songling夹爪)\n### 评估指标\n- **成功率**：任务完成的平均百分比\n- **平均任务长度**：连续完成指令的平均数量(CALVIN基准)\n- **长时序执行能力**：LIBERO-LONG基准上的表现\n- **真实世界性能**：5个操作任务各40次独立试验的平均成功率\n\n评估结果显示VITA-VLA在所有基准上都实现了最佳VLA模型性能，特别是在复杂长时序任务中表现出色，并在真实机器人部署中验证了方法的实际有效性。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>南京大学、腾讯Youtu Lab、中科院(CASIA)</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>VITA-VLA论文提出了一种高效的教学框架，通过动作专家蒸馏技术将小型动作模型的知识转移到预训练的大规模视觉-语言模型(VLM)中，使其具备动作执行能力。该方法解决了传统VLA模型需要大量计算资源和数据进行端到端训练的局限，通过两阶段训练策略显著降低训练成本，同时保持高性能表现。在仿真基准(LIBERO和CALVIN)和真实机器人环境中的实验验证了该方法的有效性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>精简架构设计</strong>：在保持原始VLM结构的基础上，仅添加动作token和状态编码器，集成物理输入信息</li><li><strong>两阶段训练策略</strong>：第一阶段进行轻量级对齐(仅3000万参数)，第二阶段选择性微调，有效降低训练成本</li><li><strong>优秀性能表现</strong>：在LIBERO基准上达到97.3%平均成功率(提升11.8%)，在LIBERO-LONG上达到93.5%成功率(提升24.5%)，在CALVIN ABC-D基准上达到92.5%第一任务成功率</li><li><strong>真实世界验证</strong>：在ALOHA机器人平台上实现82.0%平均成功率，较teacher模型提升17%</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<h3>整体架构</h3>\n<ul><li><strong>视觉编码器</strong>：InternViT-300M处理图像输入</li><li><strong>连接器</strong>：3层MLP桥接视觉和语言模态</li><li><strong>语言模型</strong>：Qwen-2.5-7B作为基础</li><li><strong>状态编码器</strong>：将6-DoF机械臂状态和2维夹爪状态编码为单token</li><li><strong>动作token</strong>：作为可学习查询，重复3次预测未来3步动作</li><li><strong>动作映射器</strong>：3层MLP将VLM隐藏状态映射到动作空间</li><li><strong>动作解码器</strong>：复用的2层MLP生成最终执行动作</li></ul>\n<h3>两阶段训练策略</h3>\n<p><strong>阶段1-对齐阶段</strong>：</p>\n<ul><li>训练状态编码器、动作token和动作映射器(约3000万参数)</li><li>使用MSE损失对齐VLM和小动作模型的隐藏表示空间</li><li>复用预训练动作解码器，避免昂贵端到端预训练</li></ul>\n\n<p><strong>阶段2-微调阶段</strong>：</p>\n<ul><li>端到端微调语言模型、状态编码器、动作模块</li><li>使用MAE损失监督6-DoF臂动作，BCE损失监督夹爪动作</li><li>组合损失函数：L_total = L_arm + λ·L_gripper (λ=0.01)</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<h3>仿真数据集</h3>\n<ul><li><strong>CALVIN ABC-D</strong>：训练于环境A、B、C，测试于未见环境D，评估零样本泛化</li><li><strong>LIBERO基准</strong>：4个任务套件(Spatial、Object、Goal、LONG)，每个包含10个长时序任务</li></ul>\n<h3>真实世界数据</h3>\n<ul><li>手动收集500个高质量演示轨迹(每任务100个)</li><li>覆盖5个操作任务：close drawer、stack cups、stack blocks、pick place sponge、pick place block</li></ul>\n<h3>训练资源</h3>\n<ul><li>使用DeepSpeed ZeRO-2阶段优化内存使用</li><li>对齐阶段：batch size=8，学习率=1e-4，训练3个epoch</li><li>微调阶段：batch size=4，学习率=1e-4，训练2个epoch</li><li>图像分辨率：统一为200×200像素</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<h3>评估环境</h3>\n<ul><li><strong>仿真环境</strong>：CALVIN和LIBERO基准测试平台</li><li><strong>真实世界</strong>：ALOHA机器人平台(6关节PiPer机械臂+Songling夹爪)</li></ul>\n<h3>评估指标</h3>\n<ul><li><strong>成功率</strong>：任务完成的平均百分比</li><li><strong>平均任务长度</strong>：连续完成指令的平均数量(CALVIN基准)</li><li><strong>长时序执行能力</strong>：LIBERO-LONG基准上的表现</li><li><strong>真实世界性能</strong>：5个操作任务各40次独立试验的平均成功率</li></ul>\n\n<p>评估结果显示VITA-VLA在所有基准上都实现了最佳VLA模型性能，特别是在复杂长时序任务中表现出色，并在真实机器人部署中验证了方法的实际有效性。</p>"
  },
  {
    "date": "2025-10-10",
    "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
    "link": "http://arxiv.org/abs/2510.09507",
    "summary_markdown": "## 论文研究单位\nHKUST(GZ)、HKUST、Beihang University、Knowin\n## 论文概述\nPhysToolBench是一个专门评估多模态大语言模型（MLLMs）物理工具理解能力的基准。它采用视觉问答（VQA）格式，涵盖超过1000个图像文本对，分为三个难度级别：工具识别（Tool Recognition）、工具理解（Tool Understanding）和工具创建（Tool Creation）。论文评估了32个不同类型的MLLMs，包括专有、开源、具体化和VLA骨干模型，发现当前MLLMs在工具理解方面存在显著不足，性能远低于人类表现（超过90% vs MLLMs最高63%）。\n## 论文核心贡献点\n1. **首个物理工具理解基准**：提出PhysToolBench，系统性地评估MLLMs对物理工具的掌握程度。\n2. **三层难度设计**：设计Easy、Medium、Hard三个级别，其中Medium细分为属性理解、工具组合和可用性理解三个子挑战，以渐进式评估深度理解。\n3. **广泛模型评估**：全面评估32个MLLMs，揭示关键弱点，如小模型缺乏涌现能力、长尾问题严重、工具可用性幻觉和视觉推理不足。\n4. **视觉中心推理框架**：提出一个初步解决方案，通过全局分析、物体检测和多层次证据整合来增强视觉推理能力。\n## 论文方法描述\n- **基准设计**：\n - 数据集包含1000+图像文本对（1024×1024图像），每个图像标注数字标签，模型需输出对应标签或“None”。\n - 三个难度级别：\n - Easy: 工具识别（如识别切菜刀用于切蔬菜）。\n - Medium: 工具理解，包含三个子挑战：\n - M.1 属性理解（如选择铸铁锅因耐高温）。\n - M.2 工具组合（如电池插入遥控器）。\n - M.3 可用性理解（如识别破损的活塞不可用）。\n - Hard: 工具创建（如用硬币代替平头螺丝刀）。\n- **数据集收集**：\n - 阶段1（概念化）：专家设计任务场景对。\n - 阶段2（图像生成）：使用GPT-4o-image（约90%）和真实摄影（约10%），人工监督质量。\n - 阶段3（标注验证）：自定义工具标注数字标签，多轮审核确保可靠性。\n- **评估方法**：\n - 使用一致文本提示，强制链式思考（CoT）推理或允许内置“思考”模式。\n - 输出工具标签或“None”，基于准确率评分。\n- **解决方案**：视觉中心推理代理框架：\n - 全局分析：整体理解任务和图像上下文。\n - 物体检测：调用DINOX工具识别并裁剪对象。\n - 多层次证据整合：结合全局和细节分析生成答案。\n## 论文使用数据集和训练资源\n- **数据集**：PhysToolBench，包含1000+图像文本对，涵盖日常、工业、户外和专业场景。图像由GPT-4o-image和真实摄影生成，人工标注数字标签。\n- **模型资源**：评估32个MLLMs，包括GPT-5、o3、GPT-4o、Claude、Gemini等专有模型，Qwen、InternVL、GLM等开源模型，RoboBrain、Embodied-R1等具体化模型，以及PaliGemma、Phi-3-Vision等VLA骨干模型。\n- **训练资源**：未训练新模型，而是使用现有模型进行评估。专有模型通过API调用，开源模型本地部署（如GLM-4.5V使用108B参数）。硬件要求未明确描述，但涉及大模型推理。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 专有模型（如GPT系列、Claude）通过各自API在线评估。\n - 开源模型（如Qwen、InternVL）本地部署，使用统一提示。\n - 人类基准：5名人类参与者作为参考。\n- **评估指标**：准确率（accuracy），以百分比表示（↑表示越高越好）。\n - 按难度级别细分：Easy、Medium（M.1/M.2/M.3）、Hard。\n - 按场景类别细分：专业（Professional）、工业（Industrial）、户外（Outdoor）、日常（Daily）。\n - 总体分数（Overall）基于加权或平均计算。表1显示人类最佳表现（如Easy 96.19%）与MLLMs最佳表现（如GPT-5 Overall 62.15%）对比。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>HKUST(GZ)、HKUST、Beihang University、Knowin</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>PhysToolBench是一个专门评估多模态大语言模型（MLLMs）物理工具理解能力的基准。它采用视觉问答（VQA）格式，涵盖超过1000个图像文本对，分为三个难度级别：工具识别（Tool Recognition）、工具理解（Tool Understanding）和工具创建（Tool Creation）。论文评估了32个不同类型的MLLMs，包括专有、开源、具体化和VLA骨干模型，发现当前MLLMs在工具理解方面存在显著不足，性能远低于人类表现（超过90% vs MLLMs最高63%）。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>首个物理工具理解基准</strong>：提出PhysToolBench，系统性地评估MLLMs对物理工具的掌握程度。</li><li><strong>三层难度设计</strong>：设计Easy、Medium、Hard三个级别，其中Medium细分为属性理解、工具组合和可用性理解三个子挑战，以渐进式评估深度理解。</li><li><strong>广泛模型评估</strong>：全面评估32个MLLMs，揭示关键弱点，如小模型缺乏涌现能力、长尾问题严重、工具可用性幻觉和视觉推理不足。</li><li><strong>视觉中心推理框架</strong>：提出一个初步解决方案，通过全局分析、物体检测和多层次证据整合来增强视觉推理能力。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li><strong>基准设计</strong>：</li></ul>\n<p> - 数据集包含1000+图像文本对（1024×1024图像），每个图像标注数字标签，模型需输出对应标签或“None”。</p>\n<p> - 三个难度级别：</p>\n<p> - Easy: 工具识别（如识别切菜刀用于切蔬菜）。</p>\n<p> - Medium: 工具理解，包含三个子挑战：</p>\n<p> - M.1 属性理解（如选择铸铁锅因耐高温）。</p>\n<p> - M.2 工具组合（如电池插入遥控器）。</p>\n<p> - M.3 可用性理解（如识别破损的活塞不可用）。</p>\n<p> - Hard: 工具创建（如用硬币代替平头螺丝刀）。</p>\n<ul><li><strong>数据集收集</strong>：</li></ul>\n<p> - 阶段1（概念化）：专家设计任务场景对。</p>\n<p> - 阶段2（图像生成）：使用GPT-4o-image（约90%）和真实摄影（约10%），人工监督质量。</p>\n<p> - 阶段3（标注验证）：自定义工具标注数字标签，多轮审核确保可靠性。</p>\n<ul><li><strong>评估方法</strong>：</li></ul>\n<p> - 使用一致文本提示，强制链式思考（CoT）推理或允许内置“思考”模式。</p>\n<p> - 输出工具标签或“None”，基于准确率评分。</p>\n<ul><li><strong>解决方案</strong>：视觉中心推理代理框架：</li></ul>\n<p> - 全局分析：整体理解任务和图像上下文。</p>\n<p> - 物体检测：调用DINOX工具识别并裁剪对象。</p>\n<p> - 多层次证据整合：结合全局和细节分析生成答案。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：PhysToolBench，包含1000+图像文本对，涵盖日常、工业、户外和专业场景。图像由GPT-4o-image和真实摄影生成，人工标注数字标签。</li><li><strong>模型资源</strong>：评估32个MLLMs，包括GPT-5、o3、GPT-4o、Claude、Gemini等专有模型，Qwen、InternVL、GLM等开源模型，RoboBrain、Embodied-R1等具体化模型，以及PaliGemma、Phi-3-Vision等VLA骨干模型。</li><li><strong>训练资源</strong>：未训练新模型，而是使用现有模型进行评估。专有模型通过API调用，开源模型本地部署（如GLM-4.5V使用108B参数）。硬件要求未明确描述，但涉及大模型推理。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 专有模型（如GPT系列、Claude）通过各自API在线评估。</p>\n<p> - 开源模型（如Qwen、InternVL）本地部署，使用统一提示。</p>\n<p> - 人类基准：5名人类参与者作为参考。</p>\n<ul><li><strong>评估指标</strong>：准确率（accuracy），以百分比表示（↑表示越高越好）。</li></ul>\n<p> - 按难度级别细分：Easy、Medium（M.1/M.2/M.3）、Hard。</p>\n<p> - 按场景类别细分：专业（Professional）、工业（Industrial）、户外（Outdoor）、日常（Daily）。</p>\n<p> - 总体分数（Overall）基于加权或平均计算。表1显示人类最佳表现（如Easy 96.19%）与MLLMs最佳表现（如GPT-5 Overall 62.15%）对比。</p>"
  },
  {
    "date": "2025-10-09",
    "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered",
    "link": "http://arxiv.org/abs/2510.08464",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-09",
    "title": "Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track",
    "link": "http://arxiv.org/abs/2510.07871",
    "summary_markdown": "## 论文研究单位\n- **香港科技大学（广州）**\n- **清华大学**\n- **中国科学院自动化研究所**\n- **小米电动汽车**\n## 论文概述\n论文提出了基于主动风险感知的社交导航方法，针对动态室内环境中的机器人导航任务。该方法在Falcon框架基础上创新性地引入了**主动风险感知模块（Proactive Risk Perception Module）**，通过预测周围人类的碰撞风险评分来增强机器人的空间感知能力。在IROS 2025 RoboSense挑战赛中，该方法获得**第二名**。\n## 论文核心贡献点\n1. **创新性风险感知模块**：提出基于距离的连续风险评分机制，将社交空间分为危险区、警告区和安全区\n2. **辅助学习优化**：通过密度监督信号提升传统强化学习在社交导航中的表现\n3. **高效集成方案**：风险模块与Falcon共享状态编码器，训练时提升导航能力，推理时零额外开销\n4. **实际竞赛验证**：在Social-HM3D数据集上优于Falcon基线方法7.46%\n## 论文方法描述\n### 核心架构\n- **主策略网络**：处理RGB-D观测和GPS+指南针信息，通过ResNet-50编码器提取视觉特征，2层LSTM处理时序依赖\n- **风险感知模块**：轻量级神经网络(公式10)，基于LSTM隐藏状态预测风险分数\n- **风险评分公式** (公式11)：\n - 危险区(d<2m)：风险评分=1.0\n - 警告区(2m≤d<4m)：风险评分线性衰减\n - 安全区(d≥4m)：风险评分=0\n### 训练优化\n- **总损失函数** (公式12)：结合主导航损失、Falcon辅助损失和风险感知损失\n- **DD-PPO算法**：在4块A40 GPU上训练75M步\n- **风险权重β_risk=0.1**：平衡风险感知与其他目标\n## 论文使用数据集和训练资源\n### 数据集\n- **Social-HM3D**：基于HM3D构建的844个真实室内场景\n- **数据特性**：\n - 人类数量按场景面积比例校准\n - 人类运动速度0.8-1.2倍机器人速度\n - 采用ORCA算法实现避障\n - 训练集/验证集/测试集比例为：大规模/1000 эпизодов/500 эпизодов\n### 计算资源\n- **4块NVIDIA A40 GPU**\n- **8并行环境训练**\n- **约75M训练步数**\n## 论文使用的评估环境和评估指标\n### 评估环境\n- **IROS 2025 RoboSense挑战赛**：Track 2社交导航轨道\n- **测试数据**：私有测试集约500个未见场景\n- **约束条件**：\n - 仅使用RGB-D观测和里程计\n - 无全局地图或特权信息\n - 禁止人类位置预测器\n### 评估指标\n- **成功率和路径效率**：\n - 成功率(SR)：到达目标1m内百分比\n - 加权路径长度(SPL)：相对最优路径的效率评估\n- **社交规范指标**：\n - 个人空间合规性(PSC)：与人类保持≥0.5m距离的时长百分比\n - 人类碰撞率(H-Coll)：发生人类碰撞的剧集百分比\n- **总成绩计算** (公式14)：Total = 0.4×SR + 0.3×SPL + 0.3×PSC\n### 竞赛结果\n- 团队排名：**16支队伍中第2名**\n- 总分：0.6994（较Falcon基线(0.6248)提升11.94%）\n- 核心指标：SR=0.656 \\|SPL=0.5958 \\|PSC=0.8608 \\|H-Coll=0.33\n- 最佳团队差距：仅0.0028分差",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li><strong>香港科技大学（广州）</strong></li><li><strong>清华大学</strong></li><li><strong>中国科学院自动化研究所</strong></li><li><strong>小米电动汽车</strong></li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文提出了基于主动风险感知的社交导航方法，针对动态室内环境中的机器人导航任务。该方法在Falcon框架基础上创新性地引入了<strong>主动风险感知模块（Proactive Risk Perception Module）</strong>，通过预测周围人类的碰撞风险评分来增强机器人的空间感知能力。在IROS 2025 RoboSense挑战赛中，该方法获得<strong>第二名</strong>。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>创新性风险感知模块</strong>：提出基于距离的连续风险评分机制，将社交空间分为危险区、警告区和安全区</li><li><strong>辅助学习优化</strong>：通过密度监督信号提升传统强化学习在社交导航中的表现</li><li><strong>高效集成方案</strong>：风险模块与Falcon共享状态编码器，训练时提升导航能力，推理时零额外开销</li><li><strong>实际竞赛验证</strong>：在Social-HM3D数据集上优于Falcon基线方法7.46%</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<h3>核心架构</h3>\n<ul><li><strong>主策略网络</strong>：处理RGB-D观测和GPS+指南针信息，通过ResNet-50编码器提取视觉特征，2层LSTM处理时序依赖</li><li><strong>风险感知模块</strong>：轻量级神经网络(公式10)，基于LSTM隐藏状态预测风险分数</li><li><strong>风险评分公式</strong> (公式11)：</li></ul>\n<p> - 危险区(d<2m)：风险评分=1.0</p>\n<p> - 警告区(2m≤d<4m)：风险评分线性衰减</p>\n<p> - 安全区(d≥4m)：风险评分=0</p>\n<h3>训练优化</h3>\n<ul><li><strong>总损失函数</strong> (公式12)：结合主导航损失、Falcon辅助损失和风险感知损失</li><li><strong>DD-PPO算法</strong>：在4块A40 GPU上训练75M步</li><li><strong>风险权重β_risk=0.1</strong>：平衡风险感知与其他目标</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<h3>数据集</h3>\n<ul><li><strong>Social-HM3D</strong>：基于HM3D构建的844个真实室内场景</li><li><strong>数据特性</strong>：</li></ul>\n<p> - 人类数量按场景面积比例校准</p>\n<p> - 人类运动速度0.8-1.2倍机器人速度</p>\n<p> - 采用ORCA算法实现避障</p>\n<p> - 训练集/验证集/测试集比例为：大规模/1000 эпизодов/500 эпизодов</p>\n<h3>计算资源</h3>\n<ul><li><strong>4块NVIDIA A40 GPU</strong></li><li><strong>8并行环境训练</strong></li><li><strong>约75M训练步数</strong></li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<h3>评估环境</h3>\n<ul><li><strong>IROS 2025 RoboSense挑战赛</strong>：Track 2社交导航轨道</li><li><strong>测试数据</strong>：私有测试集约500个未见场景</li><li><strong>约束条件</strong>：</li></ul>\n<p> - 仅使用RGB-D观测和里程计</p>\n<p> - 无全局地图或特权信息</p>\n<p> - 禁止人类位置预测器</p>\n<h3>评估指标</h3>\n<ul><li><strong>成功率和路径效率</strong>：</li></ul>\n<p> - 成功率(SR)：到达目标1m内百分比</p>\n<p> - 加权路径长度(SPL)：相对最优路径的效率评估</p>\n<ul><li><strong>社交规范指标</strong>：</li></ul>\n<p> - 个人空间合规性(PSC)：与人类保持≥0.5m距离的时长百分比</p>\n<p> - 人类碰撞率(H-Coll)：发生人类碰撞的剧集百分比</p>\n<ul><li><strong>总成绩计算</strong> (公式14)：Total = 0.4×SR + 0.3×SPL + 0.3×PSC</li></ul>\n<h3>竞赛结果</h3>\n<ul><li>团队排名：<strong>16支队伍中第2名</strong></li><li>总分：0.6994（较Falcon基线(0.6248)提升11.94%）</li><li>核心指标：SR=0.656 \\|SPL=0.5958 \\|PSC=0.8608 \\|H-Coll=0.33</li><li>最佳团队差距：仅0.0028分差</li></ul>"
  },
  {
    "date": "2025-10-09",
    "title": "USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots",
    "link": "http://arxiv.org/abs/2510.07869",
    "summary_markdown": "## 论文研究单位\n- 中国科学院自动化研究所复杂系统认知与决策智能重点实验室(北京)\n- 百度公司(北京)\n- 中国科学院大学人工智能学院(北京)\n## 论文概述\n针对水下机器人数据稀缺、多任务泛化困难的问题，论文提出USIM，一个基于仿真的多任务视觉-语言-动作(VLA)数据集，覆盖9种场景和20项任务(561K帧，1,852条轨迹，约15.6小时)，并基于USIM训练得到水下通用VLA模型U0。U0采用多模态融合与卷积-注意力感知聚焦增强(CAP)模块，在检查、避障、扫描与动态跟踪等任务上平均成功率约80%，在挑战性移动抓取任务中较基线将机器人-目标距离缩短21.2%，证明仿真数据可有效驱动水下VLA能力形成。\n## 论文核心贡献点\n- 首个面向多任务、多场景的大规模水下VLA数据集USIM：561K帧/1,852轨迹/约15.6小时，20任务/9场景，涵盖抓取、检查、扫描、导航、跟踪、运输等。\n- 水下通用VLA模型U0：基于Isaac-GR00T N1.5预训练，加入多模态融合(双目、压力、IMU、DVL等)与CAP模块，显式提升水下目标感知与空间理解。\n- 建立可扩展“数据-任务”框架：仿真-数据-模型一体化管线，验证了闭环节在线评估与开环节离线评估均显著优于未微调基线，且距离指标改善21.2%。\n## 论文方法描述\n- 仿真环境：使用Stonefish构建9种水下场景(海床、海底管道、工业池、太阳能充电站、湖泊、近海工厂、现代/古代沉船等)，内置BlueROV2与机械手-夹爪；通过ROS集成与地图随机化、光照/水质变化，生成多样且逼真的视觉条件。\n- 数据生成：自动化并行采集；每任务多episode，控制层采用PID(ROV姿态跟踪)与MoveIt(机械臂规划)；数据以10Hz录制，遵循LeRobot格式。\n- 模型U0：\n - 多模态融合：视觉(左/右相机)、语言、压力/IMU/DVL等状态，归一化推进器PWM与机械臂关节角作为动作空间，采用机器人中心坐标表示目标(相对位姿)，提升动态性与跨任务泛化。\n - CAP模块：受VLM特征引导的卷积-注意力分支，强化目标检测与定位，损失为CAP的MSE与动作模块损失加权求和(推理时可关闭)。\n- 训练与部署：基于USIM对GR00T N1.5微调，总批1024、5000步；3B参数，适配NVIDIA Jetson等嵌入式平台。\n## 论文使用数据集和训练资源\n- 数据集：USIM(561K帧/1,852轨迹/约15.6小时)；训练：526K帧/1,752轨迹；测试：35K帧/100轨迹。\n- 场景与任务：9场景；20任务(12抓取、2管道检查、2沉船扫描、2避障导航、1动态跟踪、1运输)。\n- 传感器与动作：双目相机、压力传感器、IMU、DVL；推进器PWM与机械臂关节角。\n- 训练资源：基于Isaac-GR00T N1.5；批大小1024、训练步数5000；3B参数模型适配Jetson部署。\n## 论文使用的评估环境和评估指标\n- 评估方式：开环节离线评估(仿真测试集，20任务×5轨迹，共35K帧，约1小时)；闭环节在线测试(在仿真环境执行真实任务)。\n- 评估指标：\n - 动作误差 e_action(越低越好)\n - 目标误差 e_target(CAP模块，衡量定位精度)\n - 任务成功率(闭环节在线，多次试验统计)\n - 移动抓取任务的机器人-目标距离(平均距离，越低越好)\n- 主要结果：\n - 未微调GR00T N1.5在e_action上远高于微调模型，证实显著领域差异。\n - 经USIM微调后，模型在双目输入下优于单目；U0相对GR00T FT在单目与双目e_action上分别再降7.7%与4.2%。\n - 闭环节在线：U0在7项非抓取任务上平均成功率约80%，双目优于单目，并超过GR00T FT。\n - 移动抓取：U0(双目)将平均距离较GR00T FT缩短21.2%，显示对复杂流体力与机体-机械臂-目标交互的更好适应能力。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>中国科学院自动化研究所复杂系统认知与决策智能重点实验室(北京)</li><li>百度公司(北京)</li><li>中国科学院大学人工智能学院(北京)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>针对水下机器人数据稀缺、多任务泛化困难的问题，论文提出USIM，一个基于仿真的多任务视觉-语言-动作(VLA)数据集，覆盖9种场景和20项任务(561K帧，1,852条轨迹，约15.6小时)，并基于USIM训练得到水下通用VLA模型U0。U0采用多模态融合与卷积-注意力感知聚焦增强(CAP)模块，在检查、避障、扫描与动态跟踪等任务上平均成功率约80%，在挑战性移动抓取任务中较基线将机器人-目标距离缩短21.2%，证明仿真数据可有效驱动水下VLA能力形成。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>首个面向多任务、多场景的大规模水下VLA数据集USIM：561K帧/1,852轨迹/约15.6小时，20任务/9场景，涵盖抓取、检查、扫描、导航、跟踪、运输等。</li><li>水下通用VLA模型U0：基于Isaac-GR00T N1.5预训练，加入多模态融合(双目、压力、IMU、DVL等)与CAP模块，显式提升水下目标感知与空间理解。</li><li>建立可扩展“数据-任务”框架：仿真-数据-模型一体化管线，验证了闭环节在线评估与开环节离线评估均显著优于未微调基线，且距离指标改善21.2%。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>仿真环境：使用Stonefish构建9种水下场景(海床、海底管道、工业池、太阳能充电站、湖泊、近海工厂、现代/古代沉船等)，内置BlueROV2与机械手-夹爪；通过ROS集成与地图随机化、光照/水质变化，生成多样且逼真的视觉条件。</li><li>数据生成：自动化并行采集；每任务多episode，控制层采用PID(ROV姿态跟踪)与MoveIt(机械臂规划)；数据以10Hz录制，遵循LeRobot格式。</li><li>模型U0：</li></ul>\n<p> - 多模态融合：视觉(左/右相机)、语言、压力/IMU/DVL等状态，归一化推进器PWM与机械臂关节角作为动作空间，采用机器人中心坐标表示目标(相对位姿)，提升动态性与跨任务泛化。</p>\n<p> - CAP模块：受VLM特征引导的卷积-注意力分支，强化目标检测与定位，损失为CAP的MSE与动作模块损失加权求和(推理时可关闭)。</p>\n<ul><li>训练与部署：基于USIM对GR00T N1.5微调，总批1024、5000步；3B参数，适配NVIDIA Jetson等嵌入式平台。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据集：USIM(561K帧/1,852轨迹/约15.6小时)；训练：526K帧/1,752轨迹；测试：35K帧/100轨迹。</li><li>场景与任务：9场景；20任务(12抓取、2管道检查、2沉船扫描、2避障导航、1动态跟踪、1运输)。</li><li>传感器与动作：双目相机、压力传感器、IMU、DVL；推进器PWM与机械臂关节角。</li><li>训练资源：基于Isaac-GR00T N1.5；批大小1024、训练步数5000；3B参数模型适配Jetson部署。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估方式：开环节离线评估(仿真测试集，20任务×5轨迹，共35K帧，约1小时)；闭环节在线测试(在仿真环境执行真实任务)。</li><li>评估指标：</li></ul>\n<p> - 动作误差 e_action(越低越好)</p>\n<p> - 目标误差 e_target(CAP模块，衡量定位精度)</p>\n<p> - 任务成功率(闭环节在线，多次试验统计)</p>\n<p> - 移动抓取任务的机器人-目标距离(平均距离，越低越好)</p>\n<ul><li>主要结果：</li></ul>\n<p> - 未微调GR00T N1.5在e_action上远高于微调模型，证实显著领域差异。</p>\n<p> - 经USIM微调后，模型在双目输入下优于单目；U0相对GR00T FT在单目与双目e_action上分别再降7.7%与4.2%。</p>\n<p> - 闭环节在线：U0在7项非抓取任务上平均成功率约80%，双目优于单目，并超过GR00T FT。</p>\n<p> - 移动抓取：U0(双目)将平均距离较GR00T FT缩短21.2%，显示对复杂流体力与机体-机械臂-目标交互的更好适应能力。</p>"
  },
  {
    "date": "2025-10-09",
    "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction",
    "link": "http://arxiv.org/abs/2510.07778",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-08",
    "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2510.07313",
    "summary_markdown": "## 论文研究单位\n- 北京大学多媒体信息处理国家重点实验室（计算机学院）\n- 香港科技大学\n- 新加坡国立大学\n- 北京人形机器人创新中心\n## 论文概述\nWristWorld提出首个从第三人称“锚点视角”生成腕部视角视频的4D世界模型，无需首帧腕部视角输入，即可合成时空一致且几何对齐的腕部视角序列，用于数据增强与下游视觉–语言–动作（VLA）策略提升。\n## 论文核心贡献点\n- 首个两阶段4D生成框架，在仅提供锚点视图的条件下，实现腕部视角视频的时空与几何一致合成。\n- 设计“腕部头”（wrist head）与空间投影一致性损失（SPC），从密集2D–2D对应与重建点云中监督腕部相机位姿，无需深度或外参。\n- 引入CLIP编码的锚点视角语义与文本提示，联合几何投影条件，指导视频扩散模型合成更真实、对齐的腕部视角。\n- 作为即插即用模块扩展单视角世界模型为多视角，无需新增腕部数据即可提供腕部视角训练样本。\n- 在Droid、Calvin与Franka Panda上实现SOTA视频质量，显著提升VLA性能：Calvin平均任务完成长度提升3.81%，缩小锚点–腕部性能差距42.4%。\n## 论文方法描述\n- 两阶段4D生成：\n - 重建阶段：扩展VGGT特征并通过“腕部头”与SPC损失估计腕部相机位姿与4D点云，投影得到时间对齐的条件图。\n - 生成阶段：基于Video DiT，将腕部投影条件与CLIP锚点语义及文本共同作为条件，生成腕部视角视频；结构上支持以[腕部潜变量；条件潜变量]双通道输入。\n- 训练目标包含标准扩散噪声预测损失与SPC投影损失（像素重投影误差+深度可行性项）。\n## 论文使用数据集和训练资源\n- 数据集：\n - Droid：约76k视频，覆盖59个任务，含ext1/ext2与腕部相机；10k子集预训练，100视频验证。\n - Calvin（模拟）：多任务语言条件基准，使用D split的10%数据。\n - Franka Panda（实机）：1700演示，3静态外视角+腕部视角；保留100视频评估。\n- 训练资源：\n - 重建阶段：Droid预训练在8×A800 GPU约12小时，640×480分辨率，批4。\n - 生成阶段：在8×A800 GPU约24小时，条件token长度512。\n - Franka跨视角微调：重建6小时、生成12小时，相同批大小、分辨率与token长度。\n## 论文使用的评估环境和评估指标\n- 评估环境：Droid、Calvin、Franka Panda。\n- 视频质量指标：FVD（越低越好）、LPIPS（越低越好）、SSIM（越高越好）、PSNR（越高越好）。\n- VLA评估（Calvin）：逐任务连续成功率的1/5–5/5统计与平均完成长度（Avg. Len.）。\n- 消融实验：评估腕部投影、CLIP锚点语义与SPC损失的贡献。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>北京大学多媒体信息处理国家重点实验室（计算机学院）</li><li>香港科技大学</li><li>新加坡国立大学</li><li>北京人形机器人创新中心</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>WristWorld提出首个从第三人称“锚点视角”生成腕部视角视频的4D世界模型，无需首帧腕部视角输入，即可合成时空一致且几何对齐的腕部视角序列，用于数据增强与下游视觉–语言–动作（VLA）策略提升。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>首个两阶段4D生成框架，在仅提供锚点视图的条件下，实现腕部视角视频的时空与几何一致合成。</li><li>设计“腕部头”（wrist head）与空间投影一致性损失（SPC），从密集2D–2D对应与重建点云中监督腕部相机位姿，无需深度或外参。</li><li>引入CLIP编码的锚点视角语义与文本提示，联合几何投影条件，指导视频扩散模型合成更真实、对齐的腕部视角。</li><li>作为即插即用模块扩展单视角世界模型为多视角，无需新增腕部数据即可提供腕部视角训练样本。</li><li>在Droid、Calvin与Franka Panda上实现SOTA视频质量，显著提升VLA性能：Calvin平均任务完成长度提升3.81%，缩小锚点–腕部性能差距42.4%。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>两阶段4D生成：</li></ul>\n<p> - 重建阶段：扩展VGGT特征并通过“腕部头”与SPC损失估计腕部相机位姿与4D点云，投影得到时间对齐的条件图。</p>\n<p> - 生成阶段：基于Video DiT，将腕部投影条件与CLIP锚点语义及文本共同作为条件，生成腕部视角视频；结构上支持以[腕部潜变量；条件潜变量]双通道输入。</p>\n<ul><li>训练目标包含标准扩散噪声预测损失与SPC投影损失（像素重投影误差+深度可行性项）。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据集：</li></ul>\n<p> - Droid：约76k视频，覆盖59个任务，含ext1/ext2与腕部相机；10k子集预训练，100视频验证。</p>\n<p> - Calvin（模拟）：多任务语言条件基准，使用D split的10%数据。</p>\n<p> - Franka Panda（实机）：1700演示，3静态外视角+腕部视角；保留100视频评估。</p>\n<ul><li>训练资源：</li></ul>\n<p> - 重建阶段：Droid预训练在8×A800 GPU约12小时，640×480分辨率，批4。</p>\n<p> - 生成阶段：在8×A800 GPU约24小时，条件token长度512。</p>\n<p> - Franka跨视角微调：重建6小时、生成12小时，相同批大小、分辨率与token长度。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：Droid、Calvin、Franka Panda。</li><li>视频质量指标：FVD（越低越好）、LPIPS（越低越好）、SSIM（越高越好）、PSNR（越高越好）。</li><li>VLA评估（Calvin）：逐任务连续成功率的1/5–5/5统计与平均完成长度（Avg. Len.）。</li><li>消融实验：评估腕部投影、CLIP锚点语义与SPC损失的贡献。</li></ul>"
  },
  {
    "date": "2025-10-08",
    "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
    "link": "http://arxiv.org/abs/2510.07134",
    "summary_markdown": "### 论文研究单位\n北京大学、Galbot、中国科学技术大学、BAAI、北京航空航天大学、南方科技大学、北京师范大学联合研究。\n### 论文概述\n提出TrackVLA++，一种增强具身视觉跟踪（EVT）能力的视觉-语言-动作（VLA）模型。针对现有方法在动态场景中因缺乏显式空间推理和有效时间记忆而失效的问题，TrackVLA++通过引入极坐标思维链（Polar-CoT）机制和目标识别记忆（TIM）模块，显著提升追踪性能与鲁棒性，支持多视角扩展，并在模拟和真实场景中验证了SOTA表现。\n### 论文核心贡献点\n1. **Polar-CoT机制**：将目标相对位置编码为极坐标token，提供轻量级空间推理能力，优于传统边界框方法。\n2. **TIM模块**：采用置信度门控记忆更新策略，抵抗遮挡和干扰物，实现长期目标识别。\n3. **跨域泛化**：在EVT-Bench和Gym-UnrealCV等基准测试中实现SOTA，并在真实机器人任务中验证零样本泛化。\n### 论文方法描述\n- **架构**：基于导航基础模型NavFoM构建，结合双编码器提取视觉特征（SigLIP和DINOv2）。\n- **Polar-CoT模块**：\n - 将代理感知空间（0.6m-5.0m）离散化为60角度×30距离网格，编码为唯一词汇token（含<invalid> token处理遮挡）。\n - 输入：视觉特征、语言token和TIM记忆；输出：紧凑reasoning token。\n- **TIM模块**：\n - 通过加权平均更新记忆：权重由预测置信度（归一化熵）决定，仅高置信度时融合新特征。\n - 初始化为空状态，首次有效特征后生效。\n- **训练流程**：\n - 输入序列：LLM接收视觉、语言和reasoning token；输出动作token经MLP解码为8步轨迹。\n - 损失函数：轨迹MSE损失、推理对数损失和文本损失（权重α=0.2，β=0.5）。\n- **数据集**：混合200万样本（100万EVT-Bench跟踪数据 + 100万QA数据：SYNTH-PEDES、图像/视频QA）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 模拟：EVT-Bench（STT/DT/AT分割）、Gym-UnrealCV（Single Target/Distractor/Unseen Objects）。\n - 真实：Unitree GO2机器人多视角RGB流（Obstacle/Winding Path/Distractor场景）。\n- **训练资源**：\n - 硬件：8块NVIDIA H100 GPU。\n - 时间：约192 GPU小时（约一天）。\n - 推理速度：4.8 FPS（对比NavFoM 5.1 FPS）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟：EVT-Bench（DT分割作为主要测试）、Gym-UnrealCV。\n - 真实：Unitree GO2四足机器人，配备4台SG3S11AFxK摄像头。\n- **评估指标**：\n - **SR**（Success Rate）：成功完成任务（1-3米内正确定向）的比例。\n - **TR**（Tracking Rate）：成功跟踪时间步的比例。\n - **CR**（Collision Rate）：因碰撞终止任务的比例。\n - **EL**（Episode Length）：Gym-UnrealCV中的平均步长（最大500）。\n - **识别准确率**：零样本人类识别任务（SYNTH-PEDES数据集）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学、Galbot、中国科学技术大学、BAAI、北京航空航天大学、南方科技大学、北京师范大学联合研究。</p>\n<h3>论文概述</h3>\n<p>提出TrackVLA++，一种增强具身视觉跟踪（EVT）能力的视觉-语言-动作（VLA）模型。针对现有方法在动态场景中因缺乏显式空间推理和有效时间记忆而失效的问题，TrackVLA++通过引入极坐标思维链（Polar-CoT）机制和目标识别记忆（TIM）模块，显著提升追踪性能与鲁棒性，支持多视角扩展，并在模拟和真实场景中验证了SOTA表现。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>Polar-CoT机制</strong>：将目标相对位置编码为极坐标token，提供轻量级空间推理能力，优于传统边界框方法。</li><li><strong>TIM模块</strong>：采用置信度门控记忆更新策略，抵抗遮挡和干扰物，实现长期目标识别。</li><li><strong>跨域泛化</strong>：在EVT-Bench和Gym-UnrealCV等基准测试中实现SOTA，并在真实机器人任务中验证零样本泛化。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>架构</strong>：基于导航基础模型NavFoM构建，结合双编码器提取视觉特征（SigLIP和DINOv2）。</li><li><strong>Polar-CoT模块</strong>：</li></ul>\n<p> - 将代理感知空间（0.6m-5.0m）离散化为60角度×30距离网格，编码为唯一词汇token（含<invalid> token处理遮挡）。</p>\n<p> - 输入：视觉特征、语言token和TIM记忆；输出：紧凑reasoning token。</p>\n<ul><li><strong>TIM模块</strong>：</li></ul>\n<p> - 通过加权平均更新记忆：权重由预测置信度（归一化熵）决定，仅高置信度时融合新特征。</p>\n<p> - 初始化为空状态，首次有效特征后生效。</p>\n<ul><li><strong>训练流程</strong>：</li></ul>\n<p> - 输入序列：LLM接收视觉、语言和reasoning token；输出动作token经MLP解码为8步轨迹。</p>\n<p> - 损失函数：轨迹MSE损失、推理对数损失和文本损失（权重α=0.2，β=0.5）。</p>\n<ul><li><strong>数据集</strong>：混合200万样本（100万EVT-Bench跟踪数据 + 100万QA数据：SYNTH-PEDES、图像/视频QA）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟：EVT-Bench（STT/DT/AT分割）、Gym-UnrealCV（Single Target/Distractor/Unseen Objects）。</p>\n<p> - 真实：Unitree GO2机器人多视角RGB流（Obstacle/Winding Path/Distractor场景）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 硬件：8块NVIDIA H100 GPU。</p>\n<p> - 时间：约192 GPU小时（约一天）。</p>\n<p> - 推理速度：4.8 FPS（对比NavFoM 5.1 FPS）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟：EVT-Bench（DT分割作为主要测试）、Gym-UnrealCV。</p>\n<p> - 真实：Unitree GO2四足机器人，配备4台SG3S11AFxK摄像头。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>SR</strong>（Success Rate）：成功完成任务（1-3米内正确定向）的比例。</p>\n<p> - <strong>TR</strong>（Tracking Rate）：成功跟踪时间步的比例。</p>\n<p> - <strong>CR</strong>（Collision Rate）：因碰撞终止任务的比例。</p>\n<p> - <strong>EL</strong>（Episode Length）：Gym-UnrealCV中的平均步长（最大500）。</p>\n<p> - <strong>识别准确率</strong>：零样本人类识别任务（SYNTH-PEDES数据集）。</p>"
  },
  {
    "date": "2025-10-08",
    "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
    "link": "http://arxiv.org/abs/2510.07077",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-08",
    "title": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models",
    "link": "http://arxiv.org/abs/2510.07067",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-08",
    "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
    "link": "http://arxiv.org/abs/2510.06710",
    "summary_markdown": "# RLinf-VLA: A unified and efficient framework for VLA+RL training\n## 论文研究单位\n- 清华大学、Zhongguancun Academy、Infinigence AI、北京大学、加州大学伯克利分校、哈尔滨工业大学、中国科学院自动化研究所\n## 论文概述\n- 背景与动机：VLA（视觉-语言-动作）模型大多采用SFT（监督微调），易受分布偏移影响，泛化与稳健性受限；RL（强化学习）通过交互直接优化任务表现，但现有VLA+RL工作碎片化，缺乏统一平台，难以公平对比与规模化扩展。\n- 贡献概览：提出RLinf-VLA，一个统一且高效的VLA强化学习训练框架。通过三大GPU分配模式、统一的模型/算法/仿真器接口、细致的算法设计（优势与日志概率粒度、部分重置、动作掩码、长度归一化等），在系统与算法两端同时提效。\n- 关键数字：在仿真中，单一统一模型在LIBERO-130（130项任务）达98.11%成功率，在ManiSkill 25项Pick-and-Place任务达97.66%；系统层面相对基线吞吐2.27×，在GPU并行仿真中采用混合细粒度流水实现1.61×–1.88×加速。\n## 论文核心贡献点\n- 统一设计与接口：支持多仿真器（ManiSkill、LIBERO）、多VLA架构（OpenVLA、OpenVLA-OFT）、多RL算法（PPO、GRPO）；统一训练/生成/仿真器接口，简化跨配置迁移与公平对比。\n- GPU分配策略：提出colocated、disaggregated、hybrid三种模式；针对GPU并行仿真，提出hybrid + 细粒度流水，有效削减“GPU气泡”与频繁卸载开销。\n- 算法与工程细节：优势与日志概率支持chunk/action/token三级粒度并可组合；PPO支持部分重置（提升样本效率）、轻量值网络（共享参数）；GRPO支持分组设计、有效动作掩码、轨迹长度归一化与成功率筛选。\n- 强实证与实践准则：给出PPO/GRPO与VLA结合的最佳实践；开源并维护生态，持续更新。\n## 论文方法描述\n- GPU分配策略\n - Colocated（共享）：训练/生成/仿真共用所有GPU，支持CPU卸载（offload），但在频繁交互场景开销大。\n - Disaggregated（分离）：各组件分配独占GPU分区，避免内存争抢，但因依赖关系产生“GPU气泡”。\n - Hybrid + 细粒度流水：将一个GPU上的仿真实例拆分为多个子仿真器S^(1)…S^(k)，仿真与生成并发流水，消除空闲；通过配置可切换模式，无需改动代码。\n- 模型兼容\n - LoRA支持：低秩适配，冻结原权重，仅训练少量参数，降低显存与成本。\n - 模型类型：OpenVLA（~7B，连续/离散动作）与OpenVLA-OFT（连续动作空间+L1回归损失，支持并行解码与动作分块，提高推理吞吐）。\n - 统一接口：屏蔽模型差异，一套API适配多模型。\n- 多仿真器支持\n - 统一接口：Gym风格reset/step、auto_reset、ignore_terminations、chunk_step（处理分块动作与边界）、可视化与固定重置状态等工具。\n - 任务集合：ManiSkill 25项PutOnPlateInScene（OOD设置复现）；LIBERO六类任务组合为LIBERO-130（130项）。\n- 多算法支持与设计\n - PPO：GAE估计优势；轻量值头（共享LM参数）；chunk/action级价值估计；支持“固定episode长度”与“部分重置”两种优化目标。\n - GRPO：以组为单位进行相对优势估计（去值函数），通过分组（相同任务+相同初始状态）、有效动作掩码、轨迹长度归一化与成功率筛选提升稳定性与效率。\n- 优势与日志概率粒度\n - 支持优势（chunk/action）、日志概率（chunk/action/token）组合；不兼容时采用广播（优势广播至更细粒度）。\n## 论文使用数据集和训练资源\n- 仿真环境\n - ManiSkill：PutOnPlateInScene25Main-v3（25个抓取-放置任务）。\n - LIBERO：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-10、LIBERO-90；统一为LIBERO-130。\n- 训练设置\n - 算法与模式：PPO（固定episode/部分重置），GRPO（固定episode/有效动作掩码）。\n - 分块与解码：OpenVLA-OFT支持动作分块与并行解码，提升高频控制能力。\n - 超参与资源：支持LoRA与不同rollout批量；建议更大rollout批量；LoRA对性能影响不显著但常需超参再调；部分重置与长度归一化提升样本效率与稳定性。\n- GPU模式与流水\n - Colocated/Disaggregated/Hybrid三模式切换；细粒度流水stage数量可配置；支持各组件独立offload开关。\n## 论文使用的评估环境和评估指标\n- 评估环境\n - 仿真：LIBERO-130（跨任务类别、长视野）、ManiSkill（25项OOD抓取-放置）。\n - 实机：Franka机器人在6个未见物体上零样本抓取-放置。\n- 指标\n - 成功率：LIBERO-130上98.11%；ManiSkill上97.66%。\n - 系统效率：相比基线2.27×吞吐；在GPU并行仿真中混合细粒度流水加速1.61×–1.88×。\n - 消融与实践：PPO中动作级价值估计优于分块级；部分重置显著提升样本效率；GRPO中长度归一化与有效动作掩码关键；成功率筛选改善稳定性；更大rollout批量有益；LoRA需调参但本身不降性能。\n - 实机对比：RL策略零样本完成8/30次；SFT策略0/30次，体现更强泛化。",
    "summary_html": "<h1>RLinf-VLA: A unified and efficient framework for VLA+RL training</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>清华大学、Zhongguancun Academy、Infinigence AI、北京大学、加州大学伯克利分校、哈尔滨工业大学、中国科学院自动化研究所</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>背景与动机：VLA（视觉-语言-动作）模型大多采用SFT（监督微调），易受分布偏移影响，泛化与稳健性受限；RL（强化学习）通过交互直接优化任务表现，但现有VLA+RL工作碎片化，缺乏统一平台，难以公平对比与规模化扩展。</li><li>贡献概览：提出RLinf-VLA，一个统一且高效的VLA强化学习训练框架。通过三大GPU分配模式、统一的模型/算法/仿真器接口、细致的算法设计（优势与日志概率粒度、部分重置、动作掩码、长度归一化等），在系统与算法两端同时提效。</li><li>关键数字：在仿真中，单一统一模型在LIBERO-130（130项任务）达98.11%成功率，在ManiSkill 25项Pick-and-Place任务达97.66%；系统层面相对基线吞吐2.27×，在GPU并行仿真中采用混合细粒度流水实现1.61×–1.88×加速。</li></ul>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>统一设计与接口：支持多仿真器（ManiSkill、LIBERO）、多VLA架构（OpenVLA、OpenVLA-OFT）、多RL算法（PPO、GRPO）；统一训练/生成/仿真器接口，简化跨配置迁移与公平对比。</li><li>GPU分配策略：提出colocated、disaggregated、hybrid三种模式；针对GPU并行仿真，提出hybrid + 细粒度流水，有效削减“GPU气泡”与频繁卸载开销。</li><li>算法与工程细节：优势与日志概率支持chunk/action/token三级粒度并可组合；PPO支持部分重置（提升样本效率）、轻量值网络（共享参数）；GRPO支持分组设计、有效动作掩码、轨迹长度归一化与成功率筛选。</li><li>强实证与实践准则：给出PPO/GRPO与VLA结合的最佳实践；开源并维护生态，持续更新。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>GPU分配策略</li></ul>\n<p> - Colocated（共享）：训练/生成/仿真共用所有GPU，支持CPU卸载（offload），但在频繁交互场景开销大。</p>\n<p> - Disaggregated（分离）：各组件分配独占GPU分区，避免内存争抢，但因依赖关系产生“GPU气泡”。</p>\n<p> - Hybrid + 细粒度流水：将一个GPU上的仿真实例拆分为多个子仿真器S^(1)…S^(k)，仿真与生成并发流水，消除空闲；通过配置可切换模式，无需改动代码。</p>\n<ul><li>模型兼容</li></ul>\n<p> - LoRA支持：低秩适配，冻结原权重，仅训练少量参数，降低显存与成本。</p>\n<p> - 模型类型：OpenVLA（~7B，连续/离散动作）与OpenVLA-OFT（连续动作空间+L1回归损失，支持并行解码与动作分块，提高推理吞吐）。</p>\n<p> - 统一接口：屏蔽模型差异，一套API适配多模型。</p>\n<ul><li>多仿真器支持</li></ul>\n<p> - 统一接口：Gym风格reset/step、auto_reset、ignore_terminations、chunk_step（处理分块动作与边界）、可视化与固定重置状态等工具。</p>\n<p> - 任务集合：ManiSkill 25项PutOnPlateInScene（OOD设置复现）；LIBERO六类任务组合为LIBERO-130（130项）。</p>\n<ul><li>多算法支持与设计</li></ul>\n<p> - PPO：GAE估计优势；轻量值头（共享LM参数）；chunk/action级价值估计；支持“固定episode长度”与“部分重置”两种优化目标。</p>\n<p> - GRPO：以组为单位进行相对优势估计（去值函数），通过分组（相同任务+相同初始状态）、有效动作掩码、轨迹长度归一化与成功率筛选提升稳定性与效率。</p>\n<ul><li>优势与日志概率粒度</li></ul>\n<p> - 支持优势（chunk/action）、日志概率（chunk/action/token）组合；不兼容时采用广播（优势广播至更细粒度）。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>仿真环境</li></ul>\n<p> - ManiSkill：PutOnPlateInScene25Main-v3（25个抓取-放置任务）。</p>\n<p> - LIBERO：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-10、LIBERO-90；统一为LIBERO-130。</p>\n<ul><li>训练设置</li></ul>\n<p> - 算法与模式：PPO（固定episode/部分重置），GRPO（固定episode/有效动作掩码）。</p>\n<p> - 分块与解码：OpenVLA-OFT支持动作分块与并行解码，提升高频控制能力。</p>\n<p> - 超参与资源：支持LoRA与不同rollout批量；建议更大rollout批量；LoRA对性能影响不显著但常需超参再调；部分重置与长度归一化提升样本效率与稳定性。</p>\n<ul><li>GPU模式与流水</li></ul>\n<p> - Colocated/Disaggregated/Hybrid三模式切换；细粒度流水stage数量可配置；支持各组件独立offload开关。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境</li></ul>\n<p> - 仿真：LIBERO-130（跨任务类别、长视野）、ManiSkill（25项OOD抓取-放置）。</p>\n<p> - 实机：Franka机器人在6个未见物体上零样本抓取-放置。</p>\n<ul><li>指标</li></ul>\n<p> - 成功率：LIBERO-130上98.11%；ManiSkill上97.66%。</p>\n<p> - 系统效率：相比基线2.27×吞吐；在GPU并行仿真中混合细粒度流水加速1.61×–1.88×。</p>\n<p> - 消融与实践：PPO中动作级价值估计优于分块级；部分重置显著提升样本效率；GRPO中长度归一化与有效动作掩码关键；成功率筛选改善稳定性；更大rollout批量有益；LoRA需调参但本身不降性能。</p>\n<p> - 实机对比：RL策略零样本完成8/30次；SFT策略0/30次，体现更强泛化。</p>"
  },
  {
    "date": "2025-10-07",
    "title": "Verifier-free Test-Time Sampling for Vision Language Action Models",
    "link": "http://arxiv.org/abs/2510.05681",
    "summary_markdown": "根据提供的Arxiv论文HTML原文，我为您总结这篇关于Vision-Language-Action模型的无验证器测试时采样论文：\n## 论文研究单位\nKAIST（韩国科学技术院）、首尔国立大学（SNU）、RLWRLD\n## 论文概述\nVision-Language-Action (VLA) 模型在机器人控制中展现出卓越性能，但在需要高精度的任务中仍存在根本性局限。当前方法主要依赖单次推理范式，限制了精细操作任务的表现。虽然已有测试时扩展方法使用外部验证器提升性能，但这些方法需要额外训练且泛化能力有限。本研究提出Masking Distribution Guided Selection (MG-Select)，一种新颖的VLA测试时扩展框架，仅利用模型内部属性，无需额外训练或外部模块。\n## 论文核心贡献点\n1. **提出MG-Select框架**：利用KL散度作为置信度指标，从多个候选动作中选择最优动作\n2. **条件掩蔽分布设计**：通过随机掩蔽状态和语言条件生成参考分布，确保最大不确定性同时保持任务分布对齐\n3. **联合训练策略**：通过dropout技术使模型学习条件和无条件分布，进一步提升参考分布质量\n4. **显著性能提升**：在真实世界任务中实现28%的内分布和35%的外分布性能提升，在RoboCasa pick-and-place任务中实现168%相对增益\n## 论文方法描述\n### 测试时扩展框架\n- **阶段1**：并行随机采样生成N个候选动作\n- **阶段2**：使用特定标准进行Best-of-N选择\n### 条件掩蔽分布置信度\n- 使用KL散度测量预测分布与参考分布间的距离作为置信度指标\n- 参考分布通过掩蔽文本、状态或两者创建，分别对应：文本掩蔽、状态掩蔽、文本&状态掩蔽\n- 针对不同任务环境选择最优置信度变体\n### 联合训练策略\n- 训练时引入四种掩蔽变体：(qₜ,I)、(qₜ,∅)、(∅,I)、(∅,∅)\n- 通过dropout技术增强模型对条件掩蔽分布的感知能力\n## 论文使用数据集和训练资源\n### 训练资源\n- **硬件**：NVIDIA A100 GPU（2块）\n- **模型**：π₀-FAST (Paligemma-3B VLM)、OpenVLA (Prismatic-7B VLM)\n- **优化器**：AdamW，学习率2.5e-5到2.5e-6的余弦衰减\n- **训练配置**：warmup_steps=1,000，global batch size根据数据集变化\n### 使用数据集\n- **RoboCasa**：24个家庭厨房环境中的原子任务，专注于8个pick-and-place任务\n- **SIMPLER-WidowX**：4个pick-and-place任务，基于BridgeData V2训练\n- **LIBERO**：多轴泛化评估，包括布局、物体和目标变化，以及长期任务\n- **真实世界数据**：DROID数据集，基于Franka Research 3机器人\n## 论文使用的评估环境和评估指标\n### 评估环境\n- **仿真环境**：RoboCasa、SIMPLER-WidowX、LIBERO\n- **真实世界环境**：Franka Research 3机器人，7-DoF机械臂\n### 评估指标\n- **成功率**：以百分比表示的成功率，基于多次试验（通常为50次仿真、16-24次真实世界）\n- **样本效率**：在30、100、300个演示样本下的性能表现\n- **泛化能力**：内分布（ID）和外分布（OOD）任务的区分评估\n- **效率分析**：推理延迟分析，通过单预填充策略实现45%的延迟减少\n\n实验结果表明MG-Select在不同演示规模下均能持续改进基础模型性能，特别是在低数据场景下效果显著。",
    "summary_html": "<p>根据提供的Arxiv论文HTML原文，我为您总结这篇关于Vision-Language-Action模型的无验证器测试时采样论文：</p>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>KAIST（韩国科学技术院）、首尔国立大学（SNU）、RLWRLD</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>Vision-Language-Action (VLA) 模型在机器人控制中展现出卓越性能，但在需要高精度的任务中仍存在根本性局限。当前方法主要依赖单次推理范式，限制了精细操作任务的表现。虽然已有测试时扩展方法使用外部验证器提升性能，但这些方法需要额外训练且泛化能力有限。本研究提出Masking Distribution Guided Selection (MG-Select)，一种新颖的VLA测试时扩展框架，仅利用模型内部属性，无需额外训练或外部模块。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>提出MG-Select框架</strong>：利用KL散度作为置信度指标，从多个候选动作中选择最优动作</li><li><strong>条件掩蔽分布设计</strong>：通过随机掩蔽状态和语言条件生成参考分布，确保最大不确定性同时保持任务分布对齐</li><li><strong>联合训练策略</strong>：通过dropout技术使模型学习条件和无条件分布，进一步提升参考分布质量</li><li><strong>显著性能提升</strong>：在真实世界任务中实现28%的内分布和35%的外分布性能提升，在RoboCasa pick-and-place任务中实现168%相对增益</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<h3>测试时扩展框架</h3>\n<ul><li><strong>阶段1</strong>：并行随机采样生成N个候选动作</li><li><strong>阶段2</strong>：使用特定标准进行Best-of-N选择</li></ul>\n<h3>条件掩蔽分布置信度</h3>\n<ul><li>使用KL散度测量预测分布与参考分布间的距离作为置信度指标</li><li>参考分布通过掩蔽文本、状态或两者创建，分别对应：文本掩蔽、状态掩蔽、文本&状态掩蔽</li><li>针对不同任务环境选择最优置信度变体</li></ul>\n<h3>联合训练策略</h3>\n<ul><li>训练时引入四种掩蔽变体：(qₜ,I)、(qₜ,∅)、(∅,I)、(∅,∅)</li><li>通过dropout技术增强模型对条件掩蔽分布的感知能力</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<h3>训练资源</h3>\n<ul><li><strong>硬件</strong>：NVIDIA A100 GPU（2块）</li><li><strong>模型</strong>：π₀-FAST (Paligemma-3B VLM)、OpenVLA (Prismatic-7B VLM)</li><li><strong>优化器</strong>：AdamW，学习率2.5e-5到2.5e-6的余弦衰减</li><li><strong>训练配置</strong>：warmup_steps=1,000，global batch size根据数据集变化</li></ul>\n<h3>使用数据集</h3>\n<ul><li><strong>RoboCasa</strong>：24个家庭厨房环境中的原子任务，专注于8个pick-and-place任务</li><li><strong>SIMPLER-WidowX</strong>：4个pick-and-place任务，基于BridgeData V2训练</li><li><strong>LIBERO</strong>：多轴泛化评估，包括布局、物体和目标变化，以及长期任务</li><li><strong>真实世界数据</strong>：DROID数据集，基于Franka Research 3机器人</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<h3>评估环境</h3>\n<ul><li><strong>仿真环境</strong>：RoboCasa、SIMPLER-WidowX、LIBERO</li><li><strong>真实世界环境</strong>：Franka Research 3机器人，7-DoF机械臂</li></ul>\n<h3>评估指标</h3>\n<ul><li><strong>成功率</strong>：以百分比表示的成功率，基于多次试验（通常为50次仿真、16-24次真实世界）</li><li><strong>样本效率</strong>：在30、100、300个演示样本下的性能表现</li><li><strong>泛化能力</strong>：内分布（ID）和外分布（OOD）任务的区分评估</li><li><strong>效率分析</strong>：推理延迟分析，通过单预填充策略实现45%的延迟减少</li></ul>\n\n<p>实验结果表明MG-Select在不同演示规模下均能持续改进基础模型性能，特别是在低数据场景下效果显著。</p>"
  },
  {
    "date": "2025-10-07",
    "title": "MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption",
    "link": "http://arxiv.org/abs/2510.05580",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-06",
    "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation",
    "link": "http://arxiv.org/abs/2510.05057",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-06",
    "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
    "link": "http://arxiv.org/abs/2510.04898",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-05",
    "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context",
    "link": "http://arxiv.org/abs/2510.04246",
    "summary_markdown": "## 论文总结\n\n**论文研究单位：**\nKAIST（韩国科学技术院）、RLWRLD、加州大学伯克利分校\n\n**论文概述：**\n针对部分可观察机器人任务中时序上下文缺失导致的策略性能不稳定问题，提出ContextVLA框架。该框架基于预训练视觉语言模型（VLM）的视觉语言动作模型（VLA），通过将历史多帧观测压缩为单一上下文令牌来高效利用时序信息，同时避免直接处理高维视频序列的计算开销。与传统多帧训练策略不同，该方法利用VLM的时序理解能力，在模拟和真实机器人任务中均显著提升策略性能。\n\n**论文核心贡献点：**\n1. **揭示关键瓶颈**：系统性分析发现VLA架构在利用多帧观测方面优于传统策略，原因在于预训练VLM具备的时序理解能力；\n2. **创新压缩机制**：提出分两阶段处理策略——前半段VLM层保留多帧输入以提取时序特征，后半段将历史帧聚合为单令牌（平均池化），实现高效时序上下文融合；\n3. **通用适配性**：支持自回归和扩散式动作解码器，可直接增强现有VLA模型（如π₀、GR00T N1.5）；\n4. **显著性能提升**：\n - 模拟任务：Libero基准平均提升1.9%，Simpler-WidowX提升14.4%（41.8%→56.2%）\n - 真实任务：长时序\"Pick-and-Place Twice\"任务从25%提升至65%\n\n**论文方法描述：**\n- **观测压缩流程**：\n 1. 用视觉编码器处理8帧历史观测得到视觉特征\n 2. 在VLM第2层对历史帧隐藏状态执行平均池化生成上下文令牌m\n 3. 用m替代后续VLM层中的历史帧令牌，仅保留当前帧令牌\n- **动作生成**：上下文令牌与VLM当前层特征共同输入动作解码器，支持自回归（令牌化动作）或扩散式生成\n- **高效推理**：结合因果注意力掩码与KV缓存机制，在t-1步预计算历史令牌和KV缓存，t步仅处理当前观测和上下文令牌\n\n**论文使用数据集和训练资源：**\n- **模拟数据集**：Libero（40个任务）、Simpler-WidowX（4任务）、Robocasa（24任务）\n- **真实数据集**：Bridge v2、3类长时序操作任务（抓取-放置-两次、覆盖-堆叠）\n- **训练配置**：\n - 60K迭代，批量32，优化器AdamW\n - 8帧历史观测，在第2层VLM块执行压缩（n=2）\n - 硬件：NVIDIA A100 80GB GPU（4卡并行训练，单卡推理）\n\n**论文使用的评估环境和评估指标：**\n- **环境**：\n - 模拟任务：Libero（4子基准）、Simpler-WidowX（4任务）、Robocasa（24任务）\n - 真实机器人任务：3类时序敏感操作（抓取-放置序列、手部开合等）\n- **指标**：\n - **性能**：任务成功率（%）\n - **效率**：训练墙钟时间（基于π₀在Libero的60K迭代训练）、推理延迟（ms/8帧2视角输入）\n- **关键结果**：\n - 在Simpler-WidowX实现14.4%平均提升\n - 推理时间从227.2ms（未压缩）降至96.3ms（压缩+KV缓存）\n - 长时序真实任务成功率最高达80%\n---\n注：该论文在ICLR/NeurIPS等多帧机器人策略研究基础上，通过VLM时序建模和计算优化，首次系统解决了多帧观测在VLA中的高效利用难题。",
    "summary_html": "<h2 class=\"section-title\">论文总结</h2>\n\n<p><strong>论文研究单位：</strong></p>\n<p>KAIST（韩国科学技术院）、RLWRLD、加州大学伯克利分校</p>\n\n<p><strong>论文概述：</strong></p>\n<p>针对部分可观察机器人任务中时序上下文缺失导致的策略性能不稳定问题，提出ContextVLA框架。该框架基于预训练视觉语言模型（VLM）的视觉语言动作模型（VLA），通过将历史多帧观测压缩为单一上下文令牌来高效利用时序信息，同时避免直接处理高维视频序列的计算开销。与传统多帧训练策略不同，该方法利用VLM的时序理解能力，在模拟和真实机器人任务中均显著提升策略性能。</p>\n\n<p><strong>论文核心贡献点：</strong></p>\n<ol><li><strong>揭示关键瓶颈</strong>：系统性分析发现VLA架构在利用多帧观测方面优于传统策略，原因在于预训练VLM具备的时序理解能力；</li><li><strong>创新压缩机制</strong>：提出分两阶段处理策略——前半段VLM层保留多帧输入以提取时序特征，后半段将历史帧聚合为单令牌（平均池化），实现高效时序上下文融合；</li><li><strong>通用适配性</strong>：支持自回归和扩散式动作解码器，可直接增强现有VLA模型（如π₀、GR00T N1.5）；</li><li><strong>显著性能提升</strong>：</li></ol>\n<p> - 模拟任务：Libero基准平均提升1.9%，Simpler-WidowX提升14.4%（41.8%→56.2%）</p>\n<p> - 真实任务：长时序\"Pick-and-Place Twice\"任务从25%提升至65%</p>\n\n<p><strong>论文方法描述：</strong></p>\n<ul><li><strong>观测压缩流程</strong>：</li></ul>\n<p> 1. 用视觉编码器处理8帧历史观测得到视觉特征</p>\n<p> 2. 在VLM第2层对历史帧隐藏状态执行平均池化生成上下文令牌m</p>\n<p> 3. 用m替代后续VLM层中的历史帧令牌，仅保留当前帧令牌</p>\n<ul><li><strong>动作生成</strong>：上下文令牌与VLM当前层特征共同输入动作解码器，支持自回归（令牌化动作）或扩散式生成</li><li><strong>高效推理</strong>：结合因果注意力掩码与KV缓存机制，在t-1步预计算历史令牌和KV缓存，t步仅处理当前观测和上下文令牌</li></ul>\n\n<p><strong>论文使用数据集和训练资源：</strong></p>\n<ul><li><strong>模拟数据集</strong>：Libero（40个任务）、Simpler-WidowX（4任务）、Robocasa（24任务）</li><li><strong>真实数据集</strong>：Bridge v2、3类长时序操作任务（抓取-放置-两次、覆盖-堆叠）</li><li><strong>训练配置</strong>：</li></ul>\n<p> - 60K迭代，批量32，优化器AdamW</p>\n<p> - 8帧历史观测，在第2层VLM块执行压缩（n=2）</p>\n<p> - 硬件：NVIDIA A100 80GB GPU（4卡并行训练，单卡推理）</p>\n\n<p><strong>论文使用的评估环境和评估指标：</strong></p>\n<ul><li><strong>环境</strong>：</li></ul>\n<p> - 模拟任务：Libero（4子基准）、Simpler-WidowX（4任务）、Robocasa（24任务）</p>\n<p> - 真实机器人任务：3类时序敏感操作（抓取-放置序列、手部开合等）</p>\n<ul><li><strong>指标</strong>：</li></ul>\n<p> - <strong>性能</strong>：任务成功率（%）</p>\n<p> - <strong>效率</strong>：训练墙钟时间（基于π₀在Libero的60K迭代训练）、推理延迟（ms/8帧2视角输入）</p>\n<ul><li><strong>关键结果</strong>：</li></ul>\n<p> - 在Simpler-WidowX实现14.4%平均提升</p>\n<p> - 推理时间从227.2ms（未压缩）降至96.3ms（压缩+KV缓存）</p>\n<p> - 长时序真实任务成功率最高达80%</p>\n<hr/>\n<p>注：该论文在ICLR/NeurIPS等多帧机器人策略研究基础上，通过VLM时序建模和计算优化，首次系统解决了多帧观测在VLA中的高效利用难题。</p>"
  },
  {
    "date": "2025-10-05",
    "title": "SITCOM: Scaling Inference-Time COMpute for VLAs",
    "link": "http://arxiv.org/abs/2510.04041",
    "summary_markdown": "### 论文研究单位\nCarnegie Mellon University\n### 论文概述\n论文提出了一种名为SITCOM（Scaling Inference-Time COMpute for VLAs）的框架，旨在解决视觉-语言-动作模型在长时程任务中缺乏前瞻规划和误差累积的问题。该框架通过结合预训练的VLA、一个学习的动力学模型和一个奖励模型，将VLA从单步动作执行器转变为能够进行多步规划的鲁棒长期规划器。SITCOM利用模型预测控制（MPC）的思想，在推理时生成多个动作序列的展开，并通过奖励机制选择最优序列执行。在SIMPLER环境中的实验表明，该方法能将任务完成率从48%显著提升至72%。\n### 论文核心贡献点\n1. 提出了一个通用的推理时规划框架SITCOM，可以增强任何预训练的VLA模型，通过模拟多步动作展开并基于奖励选择最优动作序列。\n2. 开发了一个高效的基于Transformer的动力学模型，该模型在BridgeV2数据上预训练，并在SIMPLER环境中微调以弥合真实到模拟的差距。同时，引入了一种受DAgger启发的自适应策略来减少长时程展开中的误差累积。\n3. 提供了对通过增加候选动作序列数量和未来预测深度来扩展推理时计算以获得性能提升的深入分析。\n### 论文方法描述\nSITCOM的核心是一个迭代决策过程。在每个决策点：\n1. VLA模型根据当前图像观察和任务指令，通过高温度采样生成n个候选动作。\n2. 每个候选动作初始化一个轨迹。利用一个独立的动力学模型来模拟未来状态，即对于每个候选动作，动力学模型预测下一帧图像。\n3. 这个过程会迭代l步（rollout length），为每个候选动作生成一个完整的多步动作序列和相应的未来状态轨迹。\n4. 使用一个奖励模型对每条轨迹的最终状态进行评分，该奖励函数综合考虑了夹爪与物体的间隙、物体与目标的距离以及抓取成功与否等指标。\n5. 选择奖励最高的轨迹，并将其第一个动作在真实世界中执行。\n6. 更新环境观察，并根据预设的重规划频率重复此过程，直到任务完成或终止。\n该方法包含两个变体：SITCOM (EnvSim) 使用真实的模拟器进行展开，而 SITCOM (World Model) 则使用训练好的动力学模型进行展开。\n### 论文使用数据集和训练资源\n* **数据集**:\n * 动力学模型预训练：使用了BridgeV2数据集，包含约25,000条轨迹，覆盖了13种操作技能和24个场景。\n * 动力学模型微调与VLA微调：使用了在SIMPLER环境中收集的100条多任务专家轨迹。\n* **训练资源**:\n * 论文中未明确指定训练所使用的具体硬件（如GPU类型和数量）或训练时长。\n### 论文使用的评估环境和评估指标\n* **评估环境**:\n * SIMPLER，一个开源的模拟环境套件，用于评估通用机器人操作策略。\n * 使用了7自由度的WidowX机械臂进行四项任务的评估。\n* **评估指标**:\n * **Average Success Rate (平均成功率)**: 主要性能指标，计算为成功完成的任务数与总试验数的比值。\n * **Partial Success Rate (部分成功率)**: 衡量机器人实现部分目标的场景，例如成功抓取物体但未能正确放置。\n * **Time (时间)**: 与计算资源成正比，衡量为不同数量的候选轨迹生成计划所需的时间，评估了推理时的计算开销。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Carnegie Mellon University</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为SITCOM（Scaling Inference-Time COMpute for VLAs）的框架，旨在解决视觉-语言-动作模型在长时程任务中缺乏前瞻规划和误差累积的问题。该框架通过结合预训练的VLA、一个学习的动力学模型和一个奖励模型，将VLA从单步动作执行器转变为能够进行多步规划的鲁棒长期规划器。SITCOM利用模型预测控制（MPC）的思想，在推理时生成多个动作序列的展开，并通过奖励机制选择最优序列执行。在SIMPLER环境中的实验表明，该方法能将任务完成率从48%显著提升至72%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个通用的推理时规划框架SITCOM，可以增强任何预训练的VLA模型，通过模拟多步动作展开并基于奖励选择最优动作序列。</li><li>开发了一个高效的基于Transformer的动力学模型，该模型在BridgeV2数据上预训练，并在SIMPLER环境中微调以弥合真实到模拟的差距。同时，引入了一种受DAgger启发的自适应策略来减少长时程展开中的误差累积。</li><li>提供了对通过增加候选动作序列数量和未来预测深度来扩展推理时计算以获得性能提升的深入分析。</li></ol>\n<h3>论文方法描述</h3>\n<p>SITCOM的核心是一个迭代决策过程。在每个决策点：</p>\n<ol><li>VLA模型根据当前图像观察和任务指令，通过高温度采样生成n个候选动作。</li><li>每个候选动作初始化一个轨迹。利用一个独立的动力学模型来模拟未来状态，即对于每个候选动作，动力学模型预测下一帧图像。</li><li>这个过程会迭代l步（rollout length），为每个候选动作生成一个完整的多步动作序列和相应的未来状态轨迹。</li><li>使用一个奖励模型对每条轨迹的最终状态进行评分，该奖励函数综合考虑了夹爪与物体的间隙、物体与目标的距离以及抓取成功与否等指标。</li><li>选择奖励最高的轨迹，并将其第一个动作在真实世界中执行。</li><li>更新环境观察，并根据预设的重规划频率重复此过程，直到任务完成或终止。</li></ol>\n<p>该方法包含两个变体：SITCOM (EnvSim) 使用真实的模拟器进行展开，而 SITCOM (World Model) 则使用训练好的动力学模型进行展开。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> * 动力学模型预训练：使用了BridgeV2数据集，包含约25,000条轨迹，覆盖了13种操作技能和24个场景。</p>\n<p> * 动力学模型微调与VLA微调：使用了在SIMPLER环境中收集的100条多任务专家轨迹。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> * 论文中未明确指定训练所使用的具体硬件（如GPU类型和数量）或训练时长。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> * SIMPLER，一个开源的模拟环境套件，用于评估通用机器人操作策略。</p>\n<p> * 使用了7自由度的WidowX机械臂进行四项任务的评估。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> * <strong>Average Success Rate (平均成功率)</strong>: 主要性能指标，计算为成功完成的任务数与总试验数的比值。</p>\n<p> * <strong>Partial Success Rate (部分成功率)</strong>: 衡量机器人实现部分目标的场景，例如成功抓取物体但未能正确放置。</p>\n<p> * <strong>Time (时间)</strong>: 与计算资源成正比，衡量为不同数量的候选轨迹生成计划所需的时间，评估了推理时的计算开销。</p>"
  },
  {
    "date": "2025-10-04",
    "title": "Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert",
    "link": "http://arxiv.org/abs/2510.03896",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-04",
    "title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation",
    "link": "http://arxiv.org/abs/2510.03895",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-04",
    "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization",
    "link": "http://arxiv.org/abs/2510.03827",
    "summary_markdown": "```markdown\n## 论文研究单位\n华中科技大学、哈佛大学、麻省理工学院、武汉理工大学、理海大学\n## 论文概述\n论文指出LIBERO基准作为视觉-语言-动作（VLA）模型主流评估平台存在训练与评估同质化问题，导致模型通过记忆训练数据即可获得超90%的虚高性能。为此，提出LIBERO-PRO扩展基准，通过四个维度（操作对象、初始状态、任务指令、环境）引入扰动评估模型鲁棒性。实验表明，现有模型在扰动条件下性能骤降至0%，暴露其缺乏真实任务理解和泛化能力，仅依赖动作序列记忆。\n## 论文核心贡献点\n1. 通过实验证明LIBERO当前评估协议存在根本缺陷：高分数主要反映训练数据记忆而非真实任务理解。\n2. 提出LIBERO-PRO基准，支持对象、位置、指令、环境四维度扰动及随机组合，实现更可靠评估。\n3. 在LIBERO-PRO上评测OpenVLA、pi0等模型，揭示其在扰动下性能崩溃，呼吁采用新评估标准。\n## 论文方法描述\n定义扰动框架$\\tau^{(k)}=\\phi_k(\\tau)$，$k \\in \\{O,S,L,E\\}$：\n- **对象属性扰动**：修改非关键属性（如颜色、大小），任务语义不变。\n- **初始位置扰动**：调整物体位置，保持物理合理性。\n- **指令扰动**：分语义级（改写指令）和任务级（更换目标对象/动作），确保组件来自训练集。\n- **环境扰动**：替换背景场景，不影响任务可行性。\n施加约束：扰动幅度$\\delta_k$可控，任务间总变距离$d_{TV} > \\epsilon$，确保任务有效且差异显著。\n## 论文使用数据集和训练资源\n- **数据集**：基于LIBERO原始数据扩展，新增对象资产、空间区域、指令改写版本及环境变体。\n- **训练资源**：直接使用预训练模型，未新增训练。评估模型包括OpenVLA（多任务检查点）、pi0/pi0.5（单任务检查点），均采用官方发布权重。\n## 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO-PRO仿真环境，支持四维度扰动随机组合。\n- **评估指标**：任务成功率（Success Rate），每任务50次试验。对比标准LIBERO与扰动场景下的表现，包括对象（Obj）、位置（Pos）、语义（Sem）、任务（Task）、环境（Env）扰动。\n```",
    "summary_html": "<p>```markdown</p>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>华中科技大学、哈佛大学、麻省理工学院、武汉理工大学、理海大学</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文指出LIBERO基准作为视觉-语言-动作（VLA）模型主流评估平台存在训练与评估同质化问题，导致模型通过记忆训练数据即可获得超90%的虚高性能。为此，提出LIBERO-PRO扩展基准，通过四个维度（操作对象、初始状态、任务指令、环境）引入扰动评估模型鲁棒性。实验表明，现有模型在扰动条件下性能骤降至0%，暴露其缺乏真实任务理解和泛化能力，仅依赖动作序列记忆。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>通过实验证明LIBERO当前评估协议存在根本缺陷：高分数主要反映训练数据记忆而非真实任务理解。</li><li>提出LIBERO-PRO基准，支持对象、位置、指令、环境四维度扰动及随机组合，实现更可靠评估。</li><li>在LIBERO-PRO上评测OpenVLA、pi0等模型，揭示其在扰动下性能崩溃，呼吁采用新评估标准。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>定义扰动框架$\\tau^{(k)}=\\phi_k(\\tau)$，$k \\in \\{O,S,L,E\\}$：</p>\n<ul><li><strong>对象属性扰动</strong>：修改非关键属性（如颜色、大小），任务语义不变。</li><li><strong>初始位置扰动</strong>：调整物体位置，保持物理合理性。</li><li><strong>指令扰动</strong>：分语义级（改写指令）和任务级（更换目标对象/动作），确保组件来自训练集。</li><li><strong>环境扰动</strong>：替换背景场景，不影响任务可行性。</li></ul>\n<p>施加约束：扰动幅度$\\delta_k$可控，任务间总变距离$d_{TV} > \\epsilon$，确保任务有效且差异显著。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：基于LIBERO原始数据扩展，新增对象资产、空间区域、指令改写版本及环境变体。</li><li><strong>训练资源</strong>：直接使用预训练模型，未新增训练。评估模型包括OpenVLA（多任务检查点）、pi0/pi0.5（单任务检查点），均采用官方发布权重。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：LIBERO-PRO仿真环境，支持四维度扰动随机组合。</li><li><strong>评估指标</strong>：任务成功率（Success Rate），每任务50次试验。对比标准LIBERO与扰动场景下的表现，包括对象（Obj）、位置（Pos）、语义（Sem）、任务（Task）、环境（Env）扰动。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-10-02",
    "title": "Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer",
    "link": "http://arxiv.org/abs/2510.03342",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-03",
    "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
    "link": "http://arxiv.org/abs/2510.03142",
    "summary_markdown": "### 论文研究单位\n- 北京大学（Peking University）\n- Galbot\n- 上海交通大学（Shanghai Jiao Tong University）\n- 清华大学（Tsinghua University）\n- BAAI（可能指北京人工智能研究院）\n### 论文概述\n论文提出MM-Nav，一个多视图视觉语言行动（VLA）模型，旨在通过多专家学习实现鲁棒的视觉导航。核心思想是结合合成环境中的多样导航数据与VLA模型的泛化能力，解决视觉导航中观察数据建模困难的问题。MM-Nav使用360度视觉观察（四个水平分布的相机视图），通过两阶段训练学习：先从强化学习（RL）专家收集数据预训练，再进行在线教师-学生迭代训练。方法最终在模拟和真实环境中展示了强泛化能力，并优于单一能力RL专家。\n### 论文核心贡献点\n1. 提出MM-Nav多视图VLA模型，支持360度视觉输入，直接输出连续速度命令。\n2. 设计三种RL专家（reaching、squeezing、avoiding），分别学习导航能力。\n3. 两阶段训练：离线数据预训练和在线迭代训练（DAgger方式），引入能力平衡数据聚合策略（基于性能差距动态调整数据比例）。\n4. 在模拟和真实环境中评估，模型性能超越RL教师，验证多能力学习的协同效应。\n5. 提供开源实现和项目页面，促进研究社区应用。\n### 论文方法描述\n- **方法概述**：采用教师-学生范式。学生模型是VLA（基于SigLIP视觉编码器和Qwen2语言模型），处理多视图RGB输入（四个相机），并预测速度命令。教师模型为三个RL专家（训练于不同合成环境，使用特权深度信息）。训练分两阶段：离线预训练（用专家数据初始化VLA）和在线迭代（VLA部署后收集专家数据，动态平衡训练比例）。\n- **RL专家**：三个专家（reaching：到达目标；squeezing：穿行狭窄通道；avoiding：避开动态障碍）。使用PPO算法、深度图像输入（四个相机）、奖励函数针对能力调整。\n- **VLA学生模型**：视觉编码（SigLIP输出视觉令牌；历史滑窗处理），语言提示（点目标转化为文本），动作预测（输出速度[v_x, v_y, v_yaw]）。\n- **在线训练**：能力平衡数据聚合（计算VLA与专家的加权旅行时间（WTT）差距，调整数据比例），迭代优化直至收敛。\n### 论文使用数据集和训练资源\n- **数据集**：合成环境数据（IsaacLab生成）：用于训练RL专家的三个能力特定场景（reaching、squeezing、avoiding），收集500k步专家数据；真实环境测试数据（四个场景：Narrow Zigzag Corridor、Thin Obstacle Avoidance、Dynamic Environment、Cluttered Static Environment）。\n- **训练资源**：\n - RL专家训练：IsaacLab模拟，NVIDIA RTX 4090 GPU，单次训练8-12小时。\n - VLA预训练：8个NVIDIA H100 GPU，约5小时（40 GPU小时）。\n - 在线迭代：每次迭代2小时（200k专家数据）。\n - 部署：服务器配备NVIDIA RTX 5090 GPU，机器人端（Unitree GO2）实时推理。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：IsaacLab中三个能力特定场景（reaching、squeezing、avoiding）和一个混合场景（包含所有能力）。\n - 真实环境：四个场景测试sim-to-real泛化。\n- **评估指标**：\n - 成功率（SR）：成功到达目标的百分比。\n - 碰撞率（CR）：发生碰撞的百分比。\n - 加权旅行时间（WTT）：成功到达目标的平均时间除以成功率。\n- 对比基线：iPlanner、ViPlanner、NavDP等，结果显示MM-Nav在SR和CR上优于基线，WTT更低。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>北京大学（Peking University）</li><li>Galbot</li><li>上海交通大学（Shanghai Jiao Tong University）</li><li>清华大学（Tsinghua University）</li><li>BAAI（可能指北京人工智能研究院）</li></ul>\n<h3>论文概述</h3>\n<p>论文提出MM-Nav，一个多视图视觉语言行动（VLA）模型，旨在通过多专家学习实现鲁棒的视觉导航。核心思想是结合合成环境中的多样导航数据与VLA模型的泛化能力，解决视觉导航中观察数据建模困难的问题。MM-Nav使用360度视觉观察（四个水平分布的相机视图），通过两阶段训练学习：先从强化学习（RL）专家收集数据预训练，再进行在线教师-学生迭代训练。方法最终在模拟和真实环境中展示了强泛化能力，并优于单一能力RL专家。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出MM-Nav多视图VLA模型，支持360度视觉输入，直接输出连续速度命令。</li><li>设计三种RL专家（reaching、squeezing、avoiding），分别学习导航能力。</li><li>两阶段训练：离线数据预训练和在线迭代训练（DAgger方式），引入能力平衡数据聚合策略（基于性能差距动态调整数据比例）。</li><li>在模拟和真实环境中评估，模型性能超越RL教师，验证多能力学习的协同效应。</li><li>提供开源实现和项目页面，促进研究社区应用。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>方法概述</strong>：采用教师-学生范式。学生模型是VLA（基于SigLIP视觉编码器和Qwen2语言模型），处理多视图RGB输入（四个相机），并预测速度命令。教师模型为三个RL专家（训练于不同合成环境，使用特权深度信息）。训练分两阶段：离线预训练（用专家数据初始化VLA）和在线迭代（VLA部署后收集专家数据，动态平衡训练比例）。</li><li><strong>RL专家</strong>：三个专家（reaching：到达目标；squeezing：穿行狭窄通道；avoiding：避开动态障碍）。使用PPO算法、深度图像输入（四个相机）、奖励函数针对能力调整。</li><li><strong>VLA学生模型</strong>：视觉编码（SigLIP输出视觉令牌；历史滑窗处理），语言提示（点目标转化为文本），动作预测（输出速度[v_x, v_y, v_yaw]）。</li><li><strong>在线训练</strong>：能力平衡数据聚合（计算VLA与专家的加权旅行时间（WTT）差距，调整数据比例），迭代优化直至收敛。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：合成环境数据（IsaacLab生成）：用于训练RL专家的三个能力特定场景（reaching、squeezing、avoiding），收集500k步专家数据；真实环境测试数据（四个场景：Narrow Zigzag Corridor、Thin Obstacle Avoidance、Dynamic Environment、Cluttered Static Environment）。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - RL专家训练：IsaacLab模拟，NVIDIA RTX 4090 GPU，单次训练8-12小时。</p>\n<p> - VLA预训练：8个NVIDIA H100 GPU，约5小时（40 GPU小时）。</p>\n<p> - 在线迭代：每次迭代2小时（200k专家数据）。</p>\n<p> - 部署：服务器配备NVIDIA RTX 5090 GPU，机器人端（Unitree GO2）实时推理。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：IsaacLab中三个能力特定场景（reaching、squeezing、avoiding）和一个混合场景（包含所有能力）。</p>\n<p> - 真实环境：四个场景测试sim-to-real泛化。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（SR）：成功到达目标的百分比。</p>\n<p> - 碰撞率（CR）：发生碰撞的百分比。</p>\n<p> - 加权旅行时间（WTT）：成功到达目标的平均时间除以成功率。</p>\n<ul><li>对比基线：iPlanner、ViPlanner、NavDP等，结果显示MM-Nav在SR和CR上优于基线，WTT更低。</li></ul>"
  },
  {
    "date": "2025-10-03",
    "title": "Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation - Technical Report for IROS 2025 RoboSense Challenge Track 4",
    "link": "http://arxiv.org/abs/2510.02728",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-02",
    "title": "Contrastive Representation Regularization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.01711",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-02",
    "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.01642",
    "summary_markdown": "### 论文研究单位\n南洋理工大学（Nanyang Technological University）、A*STAR前沿人工智能研究中心（Centre for Frontier AI Research, A*STAR）、艾伦人工智能研究所（Allen Institute for AI）、华盛顿大学（University of Washington）。\n### 论文概述\n论文提出FailSafe框架，用于视觉-语言-动作（VLA）模型的故障推理和自动恢复。VLA模型在机器人操作任务中表现良好，但现有数据集缺乏故障和恢复数据，导致模型无法应对执行中的失败。FailSafe通过在仿真中自动生成故障场景和可执行恢复动作，构建大规模数据集，并将LLaVa-OneVision-7B微调为FailSafe-VLM。实验表明，FailSafe-VLM能检测故障和生成恢复动作，并显著提升多个VLA模型在ManiSkill任务中的性能，同时具备跨视角、对象和机器人本体的泛化能力。\n### 论文核心贡献点\n- 首次提出FailSafe框架，自动生成故障推理和可执行恢复动作，支持任意仿真任务。\n- 构建FailSafe数据集，使VLM和VLA模型具备故障推理能力，并提升性能，泛化到不同视角、空间配置、对象和机器人。\n- 开源FailSafe代码，促进社区开发更稳健的具身智能系统。\n### 论文方法描述\nFailSafe流水线包括四个阶段：\n1. **故障生成**：定义三种故障模式（translation、rotation、no-ops），通过YAML配置在仿真中随机注入扰动，将任务轨迹扰动为失败场景。\n2. **动作收集**：从失败和正确轨迹中映射偏差姿态Pd到正确姿态Pc，生成多个候选恢复动作ΔA（7-DoF差值），避免碰撞。\n3. **系统验证**：回放轨迹验证ΔA能否使失败恢复为成功，仅通过验证的ΔA被纳入数据集。\n4. **指令微调**：使用FailSafe数据集微调LLaVa-OneVision-7B，得到FailSafe-VLM。训练设置：32个H100 GPU，DeepSpeed ZeRO 3，学习率1e-5（视觉塔2e-6），余弦退火warmup 3%，bfloat16/TF32。\n### 论文使用数据集和训练资源\n- **数据集**：FailSafe数据集（131k失败-动作对 + 56k真值轨迹），来自ManiSkill仿真中三个任务（pick cube、push cube、stack cube），含多视角图像（front、side、hand）。\n- **训练资源**：32个H100 GPU（DeepSpeed ZeRO 3），初始化自LLaVa-OV-7B单图像检查点，语言主干Qwen2-7B-Instruct，视觉塔SigLIP，两层GELU MLP投影器。\n### 论文使用的评估环境和评估指标\n- **评估环境**：ManiSkill仿真平台，使用Franka Emika Panda机器人臂；测试种子生成未见空间配置；评估不同对象（sphere、charger）和机器人（xArm 6）的泛化能力。\n- **评估指标**：\n - **VLM比较**：二元成功率（区分失败/成功）、准确性（识别故障类型）、余弦相似度（预测ΔA与真值ΔA相似度）。\n - **VLA模型**：成功率提升（有无FailSafe-VLM的对比），在三个ManiSkill任务（pick cube、push cube、stack cube）上报告平均改进。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南洋理工大学（Nanyang Technological University）、A*STAR前沿人工智能研究中心（Centre for Frontier AI Research, A*STAR）、艾伦人工智能研究所（Allen Institute for AI）、华盛顿大学（University of Washington）。</p>\n<h3>论文概述</h3>\n<p>论文提出FailSafe框架，用于视觉-语言-动作（VLA）模型的故障推理和自动恢复。VLA模型在机器人操作任务中表现良好，但现有数据集缺乏故障和恢复数据，导致模型无法应对执行中的失败。FailSafe通过在仿真中自动生成故障场景和可执行恢复动作，构建大规模数据集，并将LLaVa-OneVision-7B微调为FailSafe-VLM。实验表明，FailSafe-VLM能检测故障和生成恢复动作，并显著提升多个VLA模型在ManiSkill任务中的性能，同时具备跨视角、对象和机器人本体的泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首次提出FailSafe框架，自动生成故障推理和可执行恢复动作，支持任意仿真任务。</li><li>构建FailSafe数据集，使VLM和VLA模型具备故障推理能力，并提升性能，泛化到不同视角、空间配置、对象和机器人。</li><li>开源FailSafe代码，促进社区开发更稳健的具身智能系统。</li></ul>\n<h3>论文方法描述</h3>\n<p>FailSafe流水线包括四个阶段：</p>\n<ol><li><strong>故障生成</strong>：定义三种故障模式（translation、rotation、no-ops），通过YAML配置在仿真中随机注入扰动，将任务轨迹扰动为失败场景。</li><li><strong>动作收集</strong>：从失败和正确轨迹中映射偏差姿态Pd到正确姿态Pc，生成多个候选恢复动作ΔA（7-DoF差值），避免碰撞。</li><li><strong>系统验证</strong>：回放轨迹验证ΔA能否使失败恢复为成功，仅通过验证的ΔA被纳入数据集。</li><li><strong>指令微调</strong>：使用FailSafe数据集微调LLaVa-OneVision-7B，得到FailSafe-VLM。训练设置：32个H100 GPU，DeepSpeed ZeRO 3，学习率1e-5（视觉塔2e-6），余弦退火warmup 3%，bfloat16/TF32。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：FailSafe数据集（131k失败-动作对 + 56k真值轨迹），来自ManiSkill仿真中三个任务（pick cube、push cube、stack cube），含多视角图像（front、side、hand）。</li><li><strong>训练资源</strong>：32个H100 GPU（DeepSpeed ZeRO 3），初始化自LLaVa-OV-7B单图像检查点，语言主干Qwen2-7B-Instruct，视觉塔SigLIP，两层GELU MLP投影器。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：ManiSkill仿真平台，使用Franka Emika Panda机器人臂；测试种子生成未见空间配置；评估不同对象（sphere、charger）和机器人（xArm 6）的泛化能力。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>VLM比较</strong>：二元成功率（区分失败/成功）、准确性（识别故障类型）、余弦相似度（预测ΔA与真值ΔA相似度）。</p>\n<p> - <strong>VLA模型</strong>：成功率提升（有无FailSafe-VLM的对比），在三个ManiSkill任务（pick cube、push cube、stack cube）上报告平均改进。</p>"
  },
  {
    "date": "2025-10-02",
    "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.01623",
    "summary_markdown": "### 论文研究单位\nGigaAI, CASIA, Tsinghua University\n### 论文概述\n该论文提出了VLA-R1，一个旨在增强视觉-语言-动作（VLA）模型推理能力的模型。现有VLA模型常缺乏显式的分步推理能力，其后训练流程也很少强化推理质量。为解决这些问题，VLA-R1集成了基于可验证奖励的强化学习（RLVR）和组相对策略优化（GRPO），通过精心设计的奖励函数来系统性地优化模型的推理和执行鲁棒性。此外，论文还构建了VLA-CoT-13K数据集，为模型提供与可供性和轨迹标签对齐的思维链监督。在多个基准测试、仿真环境和真实机器人平台上的评估表明，VLA-R1具有优越的性能和泛化能力。\n### 论文核心贡献点\n1. 提出了VLA-R1模型，引入了基于RLVR的优化方案，通过GRPO和精心设计的奖励（区域对齐、轨迹一致性、输出格式），系统性地增强了模型的推理和执行鲁棒性。\n2. 开发了VLA-CoT数据引擎，生成了与可供性和轨迹标签对齐的高质量数据集VLA-CoT-13K，弥补了现有VLA模型缺乏逐步推理监督的不足。\n3. 在域内、域外、仿真和真实机器人平台上对VLA-R1进行了全面评估，验证了其有效性和跨域泛化能力。\n### 论文方法描述\nVLA-R1的训练方法包含两个主要阶段：\n1. **监督微调（SFT）**：首先，使用Qwen2.5-VL-72B模型构建VLA-CoT-13K数据集，该数据集包含与可供性和轨迹注释对齐的逐步思维链。然后，在此数据集上对基于Qwen2.5-VL-3B的模型进行微调，同时监督模型输出的思维片段和最终动作片段。\n2. **强化学习（RL）**：在SFT之后，使用GRPO算法进行进一步优化。该过程设计了三种可验证的奖励函数：\n - **Fréchet轨迹奖励**：采用角度长度增强的Fréchet距离（ALAF）来衡量预测轨迹与真实轨迹的对齐程度，考虑了位置、角度和分段长度。\n - **GIoU可供性奖励**：使用广义交并比（GIoU）作为奖励，以评估预测边界框与真实边界框的空间对齐质量，特别是在非重叠情况下也能提供有效梯度。\n - **格式奖励**：一个二元奖励，用于强制模型的输出必须遵循指定的结构（思维片段后跟动作片段）。\n### 论文使用数据集和训练资源\n- **使用数据集**:\n - **训练数据**: ShareRobot数据集，源自Open X-Embodiment，包含102个操作场景和12种机器人形态的供能力与轨迹注释。\n - **合成数据**: VLA-CoT-13K，一个包含13K条思维链注释的高质量数据集，通过VLA-CoT数据引擎生成，与可供性和轨迹标签对齐。\n - **域外评估数据**:\n - 可供性任务：UMD Part Affordance数据集的子集。\n - 轨迹任务：VAIT数据集（LLARVA的验证集）。\n- **训练资源**: 论文中未明确说明所使用的具体计算资源（如GPU类型、数量、训练时长等）。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - **基准测试**: 在ShareRobot（域内）以及UMD和VAIT（域外）数据集上进行离线评估。\n - **仿真环境**: 使用RoboTwin模拟器，在带有随机杂物的桌面场景中进行测试，评估了Piper和UR5两种机器人平台。\n - **真实世界**: 在一个真实的桌面平台上，设计了四个经典场景（碗抓取、水果抓取、厨房场景、混合场景）进行评估。\n- **评估指标**:\n - **可供性任务**: 使用交并比（IoU）作为主要度量指标。\n - **轨迹任务**: 使用离散Fréchet距离（DFD）、豪斯多夫距离（HD）和均方根误差（RMSE）来评估轨迹相似性。\n - **仿真与真实世界任务**: 使用成功率（SR）作为任务级指标。对于可供性任务，成功定义为正确定位并抓取物体；对于轨迹任务，成功定义为将物体成功运送到指定目标区域。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>GigaAI, CASIA, Tsinghua University</p>\n<h3>论文概述</h3>\n<p>该论文提出了VLA-R1，一个旨在增强视觉-语言-动作（VLA）模型推理能力的模型。现有VLA模型常缺乏显式的分步推理能力，其后训练流程也很少强化推理质量。为解决这些问题，VLA-R1集成了基于可验证奖励的强化学习（RLVR）和组相对策略优化（GRPO），通过精心设计的奖励函数来系统性地优化模型的推理和执行鲁棒性。此外，论文还构建了VLA-CoT-13K数据集，为模型提供与可供性和轨迹标签对齐的思维链监督。在多个基准测试、仿真环境和真实机器人平台上的评估表明，VLA-R1具有优越的性能和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了VLA-R1模型，引入了基于RLVR的优化方案，通过GRPO和精心设计的奖励（区域对齐、轨迹一致性、输出格式），系统性地增强了模型的推理和执行鲁棒性。</li><li>开发了VLA-CoT数据引擎，生成了与可供性和轨迹标签对齐的高质量数据集VLA-CoT-13K，弥补了现有VLA模型缺乏逐步推理监督的不足。</li><li>在域内、域外、仿真和真实机器人平台上对VLA-R1进行了全面评估，验证了其有效性和跨域泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<p>VLA-R1的训练方法包含两个主要阶段：</p>\n<ol><li><strong>监督微调（SFT）</strong>：首先，使用Qwen2.5-VL-72B模型构建VLA-CoT-13K数据集，该数据集包含与可供性和轨迹注释对齐的逐步思维链。然后，在此数据集上对基于Qwen2.5-VL-3B的模型进行微调，同时监督模型输出的思维片段和最终动作片段。</li><li><strong>强化学习（RL）</strong>：在SFT之后，使用GRPO算法进行进一步优化。该过程设计了三种可验证的奖励函数：</li></ol>\n<p> - <strong>Fréchet轨迹奖励</strong>：采用角度长度增强的Fréchet距离（ALAF）来衡量预测轨迹与真实轨迹的对齐程度，考虑了位置、角度和分段长度。</p>\n<p> - <strong>GIoU可供性奖励</strong>：使用广义交并比（GIoU）作为奖励，以评估预测边界框与真实边界框的空间对齐质量，特别是在非重叠情况下也能提供有效梯度。</p>\n<p> - <strong>格式奖励</strong>：一个二元奖励，用于强制模型的输出必须遵循指定的结构（思维片段后跟动作片段）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>使用数据集</strong>:</li></ul>\n<p> - <strong>训练数据</strong>: ShareRobot数据集，源自Open X-Embodiment，包含102个操作场景和12种机器人形态的供能力与轨迹注释。</p>\n<p> - <strong>合成数据</strong>: VLA-CoT-13K，一个包含13K条思维链注释的高质量数据集，通过VLA-CoT数据引擎生成，与可供性和轨迹标签对齐。</p>\n<p> - <strong>域外评估数据</strong>:</p>\n<p> - 可供性任务：UMD Part Affordance数据集的子集。</p>\n<p> - 轨迹任务：VAIT数据集（LLARVA的验证集）。</p>\n<ul><li><strong>训练资源</strong>: 论文中未明确说明所使用的具体计算资源（如GPU类型、数量、训练时长等）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - <strong>基准测试</strong>: 在ShareRobot（域内）以及UMD和VAIT（域外）数据集上进行离线评估。</p>\n<p> - <strong>仿真环境</strong>: 使用RoboTwin模拟器，在带有随机杂物的桌面场景中进行测试，评估了Piper和UR5两种机器人平台。</p>\n<p> - <strong>真实世界</strong>: 在一个真实的桌面平台上，设计了四个经典场景（碗抓取、水果抓取、厨房场景、混合场景）进行评估。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>可供性任务</strong>: 使用交并比（IoU）作为主要度量指标。</p>\n<p> - <strong>轨迹任务</strong>: 使用离散Fréchet距离（DFD）、豪斯多夫距离（HD）和均方根误差（RMSE）来评估轨迹相似性。</p>\n<p> - <strong>仿真与真实世界任务</strong>: 使用成功率（SR）作为任务级指标。对于可供性任务，成功定义为正确定位并抓取物体；对于轨迹任务，成功定义为将物体成功运送到指定目标区域。</p>"
  },
  {
    "date": "2025-10-01",
    "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.01389",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-01",
    "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition",
    "link": "http://arxiv.org/abs/2510.01068",
    "summary_markdown": "### 论文研究单位\nThe University of Hong Kong, Beijing Innovation Center of Humanoid Robotics, Shanghai AI Lab, Shanghai Jiaotong University, The Hong Kong University of Science and Technology\n### 论文概述\n本文提出了一种替代范式，通过组合现有的预训练模型来创建更强的机器人策略，无需额外的模型训练。具体而言，介绍了一种称为通用策略组合（GPC）的训练免费方法，在推理时通过凸组合和测试时搜索组合多个预训练策略的分布分数。研究表明，组合后的策略性能可以超过任何单个父策略的性能。\n### 论文核心贡献点\n1. 建立了机器人策略组合的理论基础，证明了分布分数的凸组合可以产生改进的功能目标，并且这种优势会传播到系统层面。\n2. 提出了通用策略组合（GPC），一个灵活的、训练免费的框架，可以将不同模态和架构的预训练策略组合成一个更具表现力的策略。\n3. 在模拟和现实世界进行了广泛评估，展示了GPC的持续性能提升，并分析了关键设计选择。\n### 论文方法描述\nGPC方法通过凸组合多个预训练策略的分布分数来工作，使用测试时搜索来找到最优的组合权重。该方法可以处理不同类型的策略，包括视觉-动作（VA）和视觉-语言-动作（VLA）模型，以及基于扩散或流匹配的策略，无论其输入视觉模态如何。算法流程包括：初始化噪声轨迹，对不同的权重值进行测试时搜索，在每个去噪步骤中计算组合分数，最终选择最优权重。\n### 论文使用数据集和训练资源\n- 数据集：Robomimic（包括Can、Lift、Square任务）、PushT、RoboTwin（包含多个双臂操作任务）\n- 真实世界实验：Place Bottles、Hang Mug、Close Table、Punch Holes\n- 由于GPC是训练免费的，直接使用预训练策略，不需要额外训练资源\n### 论文使用的评估环境和评估指标\n- 评估环境：模拟环境（Robomimic、PushT、RoboTwin）和真实世界机器人环境\n- 评估指标：成功率（Success Rate, SR），平均成功率（Average SR）\n- 每个设置评估200次（RoboTwin为100次）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>The University of Hong Kong, Beijing Innovation Center of Humanoid Robotics, Shanghai AI Lab, Shanghai Jiaotong University, The Hong Kong University of Science and Technology</p>\n<h3>论文概述</h3>\n<p>本文提出了一种替代范式，通过组合现有的预训练模型来创建更强的机器人策略，无需额外的模型训练。具体而言，介绍了一种称为通用策略组合（GPC）的训练免费方法，在推理时通过凸组合和测试时搜索组合多个预训练策略的分布分数。研究表明，组合后的策略性能可以超过任何单个父策略的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>建立了机器人策略组合的理论基础，证明了分布分数的凸组合可以产生改进的功能目标，并且这种优势会传播到系统层面。</li><li>提出了通用策略组合（GPC），一个灵活的、训练免费的框架，可以将不同模态和架构的预训练策略组合成一个更具表现力的策略。</li><li>在模拟和现实世界进行了广泛评估，展示了GPC的持续性能提升，并分析了关键设计选择。</li></ol>\n<h3>论文方法描述</h3>\n<p>GPC方法通过凸组合多个预训练策略的分布分数来工作，使用测试时搜索来找到最优的组合权重。该方法可以处理不同类型的策略，包括视觉-动作（VA）和视觉-语言-动作（VLA）模型，以及基于扩散或流匹配的策略，无论其输入视觉模态如何。算法流程包括：初始化噪声轨迹，对不同的权重值进行测试时搜索，在每个去噪步骤中计算组合分数，最终选择最优权重。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：Robomimic（包括Can、Lift、Square任务）、PushT、RoboTwin（包含多个双臂操作任务）</li><li>真实世界实验：Place Bottles、Hang Mug、Close Table、Punch Holes</li><li>由于GPC是训练免费的，直接使用预训练策略，不需要额外训练资源</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：模拟环境（Robomimic、PushT、RoboTwin）和真实世界机器人环境</li><li>评估指标：成功率（Success Rate, SR），平均成功率（Average SR）</li><li>每个设置评估200次（RoboTwin为100次）</li></ul>"
  },
  {
    "date": "2025-10-01",
    "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy",
    "link": "http://arxiv.org/abs/2510.00695",
    "summary_markdown": "### 论文研究单位\nKAIST、UC Berkeley、RLWRLD\n### 论文概述\n现有视觉-语言-行动（Vision-Language-Action, VLA）模型依赖当前观察进行行动预测，忽略历史上下文，导致在需要长期推理的任务中性能受限。本论文提出HAMLET框架，通过引入记忆机制使VLA能够利用历史信息。具体方法包括：\n- 使用可学习的\"moment tokens\"压缩每个时间步的感知信息，并通过时间对比学习初始化以突出时序特征。\n- 集成轻量级\"memory module\"聚合历史moment tokens，生成行动预测的条件特征。\n实验表明，HAMLET在真实世界长时序任务和仿真基准上显著提升性能，且无需修改原始VLA架构。\n### 论文核心贡献点\n- 动机分析：现有VLA假设当前观察独立，限制了在历史依赖任务（如遮挡或长期操作）中的表现。\n- 方法创新：提出HAMLET框架，通过moment tokens和memory module增强历史感知。\n- 实验验证：跨多VLA backbone（GR00T N1.5、CogACT）验证有效性，在真实世界任务平均提升47.2%，仿真基准（RoboCasa、LIBERO）也获益。\n- 通用性：backbone-agnostic设计，plug-and-play适配，无需额外预训练。\n- 效率优化：避免多帧输入带来的计算开销，内存占用仅为多帧基线的约2倍。\n### 论文方法描述\nHAMLET包含两个核心组件：\n- **Context compression via moment tokens**：在每个时间步t，附加可学习向量$\\mathbf{m}_t$到VLM输入，通过冻结VLM进行时间对比学习初始化（使用同一时间步的正样本和不同时间步的负样本），使moment tokens捕捉时序判别特征，过滤静态背景。\n- **Memory consolidation via memory module**：使用2层Transformer处理堆叠的近T个moment tokens（如$\\mathbf{M}' = [\\mathbf{m}'_{t-k(T-1)}, ..., \\mathbf{m}'_t]$），通过因果自注意力生成历史增强特征$\\tilde{\\mathbf{m}}'$，与VLM表示$\\mathbf{h}_t$拼接后输入行动专家，预测k步行动。\n训练流程：冻结VLM进行moment tokens初始化（30k步），随后联合训练行动预测（60k步）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 真实世界任务：3个桌面任务（Pick-and-Place Twice、Cover-and-Stack、Swap Cubes），每任务50个演示，平均约268帧/轨迹。\n - 仿真基准：\n - RoboCasa Kitchen：24个任务，训练演示30/100/300每任务。\n - LIBERO：40个任务，分4套件（Spatial、Object、Goal、Long）。\n - SimplerEnv-Bridge：基于WidowX机器人，使用BridgeV2数据集。\n- **训练资源**：使用NVIDIA A100 GPU进行训练和测量（延迟和内存）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 真实世界：Franka Research 3机器人+Robotiq 2F-85 gripper，双视角摄像头。\n - 仿真：RoboCasa Kitchen（Franka机器人）、LIBERO（Franka）、SimplerEnv-Bridge（WidowX）。\n- **评估指标**：\n - 成功率和部分成功率（如Pick-and-Place Once、Cover Cube），跨任务和基准报告。\n - 效率指标：延迟（毫秒）和峰值内存使用（MB），在RoboCasa数据集上测量。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>KAIST、UC Berkeley、RLWRLD</p>\n<h3>论文概述</h3>\n<p>现有视觉-语言-行动（Vision-Language-Action, VLA）模型依赖当前观察进行行动预测，忽略历史上下文，导致在需要长期推理的任务中性能受限。本论文提出HAMLET框架，通过引入记忆机制使VLA能够利用历史信息。具体方法包括：</p>\n<ul><li>使用可学习的\"moment tokens\"压缩每个时间步的感知信息，并通过时间对比学习初始化以突出时序特征。</li><li>集成轻量级\"memory module\"聚合历史moment tokens，生成行动预测的条件特征。</li></ul>\n<p>实验表明，HAMLET在真实世界长时序任务和仿真基准上显著提升性能，且无需修改原始VLA架构。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>动机分析：现有VLA假设当前观察独立，限制了在历史依赖任务（如遮挡或长期操作）中的表现。</li><li>方法创新：提出HAMLET框架，通过moment tokens和memory module增强历史感知。</li><li>实验验证：跨多VLA backbone（GR00T N1.5、CogACT）验证有效性，在真实世界任务平均提升47.2%，仿真基准（RoboCasa、LIBERO）也获益。</li><li>通用性：backbone-agnostic设计，plug-and-play适配，无需额外预训练。</li><li>效率优化：避免多帧输入带来的计算开销，内存占用仅为多帧基线的约2倍。</li></ul>\n<h3>论文方法描述</h3>\n<p>HAMLET包含两个核心组件：</p>\n<ul><li><strong>Context compression via moment tokens</strong>：在每个时间步t，附加可学习向量$\\mathbf{m}_t$到VLM输入，通过冻结VLM进行时间对比学习初始化（使用同一时间步的正样本和不同时间步的负样本），使moment tokens捕捉时序判别特征，过滤静态背景。</li><li><strong>Memory consolidation via memory module</strong>：使用2层Transformer处理堆叠的近T个moment tokens（如$\\mathbf{M}' = [\\mathbf{m}'_{t-k(T-1)}, ..., \\mathbf{m}'_t]$），通过因果自注意力生成历史增强特征$\\tilde{\\mathbf{m}}'$，与VLM表示$\\mathbf{h}_t$拼接后输入行动专家，预测k步行动。</li></ul>\n<p>训练流程：冻结VLM进行moment tokens初始化（30k步），随后联合训练行动预测（60k步）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 真实世界任务：3个桌面任务（Pick-and-Place Twice、Cover-and-Stack、Swap Cubes），每任务50个演示，平均约268帧/轨迹。</p>\n<p> - 仿真基准：</p>\n<p> - RoboCasa Kitchen：24个任务，训练演示30/100/300每任务。</p>\n<p> - LIBERO：40个任务，分4套件（Spatial、Object、Goal、Long）。</p>\n<p> - SimplerEnv-Bridge：基于WidowX机器人，使用BridgeV2数据集。</p>\n<ul><li><strong>训练资源</strong>：使用NVIDIA A100 GPU进行训练和测量（延迟和内存）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 真实世界：Franka Research 3机器人+Robotiq 2F-85 gripper，双视角摄像头。</p>\n<p> - 仿真：RoboCasa Kitchen（Franka机器人）、LIBERO（Franka）、SimplerEnv-Bridge（WidowX）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率和部分成功率（如Pick-and-Place Once、Cover Cube），跨任务和基准报告。</p>\n<p> - 效率指标：延迟（毫秒）和峰值内存使用（MB），在RoboCasa数据集上测量。</p>"
  },
  {
    "date": "2025-10-01",
    "title": "Hybrid Training for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.00600",
    "summary_markdown": "### 论文研究单位\nQualcomm AI Research, University of Tübingen, Max Planck Institute\n### 论文概述\n本文提出了一种名为混合训练（Hybrid Training, HyT）的框架，用于训练视觉-语言-动作模型（VLAs）。该方法使模型能够从思维链（CoT）推理中学习，同时保持与标准VLA相同的快速推理速度。HyT通过引入模态变量，支持模型在推理时直接预测动作、生成思维或遵循指令等多种模式，从而在模拟基准测试和真实世界实验中提升了性能。\n### 论文核心贡献点\n- 提出混合训练（HyT）框架，使VLAs能够从CoT中学习，同时在推理时跳过CoT生成，实现快速动作预测。\n- 通过模态变量控制模型输出模式，支持直接动作预测（act）、思维生成（think）和指令跟随（follow）三种模式。\n- 验证了CoT技术的性能提升主要源于训练时对思维的内部化，而非推理时的显式生成。\n### 论文方法描述\n1. **混合训练公式**：定义动作条件分布为思维和模态变量的边缘化分布，公式为：\n \\[\n p(a_t\\|x_t,l) = \\sum_i \\sum_j p_{\\theta}(a_t, \\tau^i, m^j\\|x_t,l)p(m^j)\n \\]\n 其中，\\(\\tau\\) 为思维，\\(m\\) 为模态变量。\n2. **训练实现**：使用蒙特卡洛估计，以不同概率采样条件输入和输出（如act、think、follow模式），训练模型预测多样化条件分布。\n3. **推理模式**：通过设置模态变量（如`<act>`），模型可直接输出动作，无需生成思维；或使用`<think>`生成思维，增强可解释性。\n### 论文使用数据集和训练资源\n- **数据集**：\n - ClevrSkills：包含3000个演示轨迹，涵盖空间放置、堆叠等9个任务。\n - LIBERO：4个任务套件（Spatial、Object、Goal、Long），使用LLM生成思维链标注。\n - 真实世界数据：320条轨迹，来自UFactory xArm 6机械臂，涵盖分布内和分布外任务。\n- **训练资源**：\n - ClevrSkills：使用4块A100 GPU，批量大小32，学习率2e-5，从PaliGemma-2（3B参数）微调。\n - LIBERO：结合OFT策略，从Prismatic VLM微调。\n - 真实世界实验：使用OpenVLA（7B参数），LoRA微调（rank 32），学习率5e-4。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：ClevrSkills（基于ManiSkill2）、LIBERO。\n - 真实环境：UFactory xArm 6机械臂，配备RealSense D435相机。\n- **评估指标**：\n - 成功率（Success Rate）：任务完成率，每个模型在100个评估回合中测试。\n - 推理时间：动作生成频率，HyT与标准VLA相当（约3Hz），显著快于ECoT（慢3倍）和分层VLA（慢4倍）。\n - 平均性能：多任务场景下的综合成功率，如LIBERO的4套件平均得分。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Qualcomm AI Research, University of Tübingen, Max Planck Institute</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为混合训练（Hybrid Training, HyT）的框架，用于训练视觉-语言-动作模型（VLAs）。该方法使模型能够从思维链（CoT）推理中学习，同时保持与标准VLA相同的快速推理速度。HyT通过引入模态变量，支持模型在推理时直接预测动作、生成思维或遵循指令等多种模式，从而在模拟基准测试和真实世界实验中提升了性能。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出混合训练（HyT）框架，使VLAs能够从CoT中学习，同时在推理时跳过CoT生成，实现快速动作预测。</li><li>通过模态变量控制模型输出模式，支持直接动作预测（act）、思维生成（think）和指令跟随（follow）三种模式。</li><li>验证了CoT技术的性能提升主要源于训练时对思维的内部化，而非推理时的显式生成。</li></ul>\n<h3>论文方法描述</h3>\n<ol><li><strong>混合训练公式</strong>：定义动作条件分布为思维和模态变量的边缘化分布，公式为：</li></ol>\n<p> \\[</p>\n<p> p(a_t\\|x_t,l) = \\sum_i \\sum_j p_{\\theta}(a_t, \\tau^i, m^j\\|x_t,l)p(m^j)</p>\n<p> \\]</p>\n<p> 其中，\\(\\tau\\) 为思维，\\(m\\) 为模态变量。</p>\n<ol><li><strong>训练实现</strong>：使用蒙特卡洛估计，以不同概率采样条件输入和输出（如act、think、follow模式），训练模型预测多样化条件分布。</li><li><strong>推理模式</strong>：通过设置模态变量（如<code><act></code>），模型可直接输出动作，无需生成思维；或使用<code><think></code>生成思维，增强可解释性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - ClevrSkills：包含3000个演示轨迹，涵盖空间放置、堆叠等9个任务。</p>\n<p> - LIBERO：4个任务套件（Spatial、Object、Goal、Long），使用LLM生成思维链标注。</p>\n<p> - 真实世界数据：320条轨迹，来自UFactory xArm 6机械臂，涵盖分布内和分布外任务。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - ClevrSkills：使用4块A100 GPU，批量大小32，学习率2e-5，从PaliGemma-2（3B参数）微调。</p>\n<p> - LIBERO：结合OFT策略，从Prismatic VLM微调。</p>\n<p> - 真实世界实验：使用OpenVLA（7B参数），LoRA微调（rank 32），学习率5e-4。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：ClevrSkills（基于ManiSkill2）、LIBERO。</p>\n<p> - 真实环境：UFactory xArm 6机械臂，配备RealSense D435相机。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（Success Rate）：任务完成率，每个模型在100个评估回合中测试。</p>\n<p> - 推理时间：动作生成频率，HyT与标准VLA相当（约3Hz），显著快于ECoT（慢3倍）和分层VLA（慢4倍）。</p>\n<p> - 平均性能：多任务场景下的综合成功率，如LIBERO的4套件平均得分。</p>"
  },
  {
    "date": "2025-10-01",
    "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators",
    "link": "http://arxiv.org/abs/2510.00406",
    "summary_markdown": "### 论文研究单位\n西湖大学、浙江大学、复旦大学、OpenHelix团队、北京邮电大学、郑州大学、河北工业大学等\n### 论文概述\n论文提出 VLA-RFT（Vision-Language-Action 强化微调），用学习到的世界模型作为可控模拟器为 VLA 提供验证奖励，在模拟器中对策略进行轨迹级强化优化，以更低样本复杂度提升 VLA 的泛化与鲁棒性。\n### 论文核心贡献点\n- 世界模型作为数据驱动的可交互模拟器，基于动作预测未来视觉观察，支持策略多 rollout 生成与反馈\n- 轨迹级“验证奖励”，通过将生成的视觉轨迹与目标达成参考轨迹对比获得密集且动作对齐的学习信号\n- 将流匹配扩展为 SDE 策略（SDE-Policy），引入 Sigma Net 表征随机性；结合 GRPO 进行高效稳定微调\n- 只需约 400 步强化微调即可显著优于强监督基线，并在扰动场景下展现更强鲁棒性\n### 论文方法描述\n两阶段训练：Stage I 预训练世界模型与 VLA；Stage II 在世界模型中交互 rollout、计算验证奖励并用 GRPO 优化 VLA。\n- 世界模型：基于 LLaMA 架构的轻量自回归视频预测模型（138M 参数），以最大似然训练\n- VLA 预训练：VLM 编码器 + 流匹配动作头，MSE 目标稳定输出动作片段\n- SDE-Policy：在流匹配基础上加入 Sigma Net（输出方差），将 ODE 扩展为 SDE；在 K=10 步扩散积分中计算平均对数似然并形成策略比 r\n- 验证奖励：将世界模型生成的轨迹与离线专家轨迹对齐，定义为 L1 与 LPIPS 的负加权和；同起点 rollout 的奖励做组内平均以减方差\n- 目标函数：GRPO 损失 + 流匹配 MSE 辅助项 + 熵正则，确保高效与稳定\n### 论文使用数据集和训练资源\n- 数据集与基准：LIBERO（Spatial、Object、Goal、Long 四套任务）\n- 基础策略：轻量 VLA-Adapter；先进行监督微调，再进行强化微调\n- 世界模型：138M 参数的自回归模型，基于 LLaMA 架构，在 LIBERO 上预训练\n- 训练资源：4× A800 GPU；采用 VERL 分布式强化框架与 FSDP 切分训练\n### 论文使用的评估环境和评估指标\n- 世界模型评估：像素误差（MSE）、峰值信噪比（PSNR）、结构相似性（SSIM）、感知距离（LPIPS）\n- 策略评估：成功率（SR）在标准套件与扰动套件；扰动包含物体/目标位置、机器人状态及组合扰动\n- 关键结果：约 400 步 RFT 将平均 SR 从 86.6% 提升至 91.1%，且在各类扰动下保持更高稳定性；世界模型具备高保真视觉预测能力，显著低于监督扩展 SFT 所需迭代量",
    "summary_html": "<h3>论文研究单位</h3>\n<p>西湖大学、浙江大学、复旦大学、OpenHelix团队、北京邮电大学、郑州大学、河北工业大学等</p>\n<h3>论文概述</h3>\n<p>论文提出 VLA-RFT（Vision-Language-Action 强化微调），用学习到的世界模型作为可控模拟器为 VLA 提供验证奖励，在模拟器中对策略进行轨迹级强化优化，以更低样本复杂度提升 VLA 的泛化与鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>世界模型作为数据驱动的可交互模拟器，基于动作预测未来视觉观察，支持策略多 rollout 生成与反馈</li><li>轨迹级“验证奖励”，通过将生成的视觉轨迹与目标达成参考轨迹对比获得密集且动作对齐的学习信号</li><li>将流匹配扩展为 SDE 策略（SDE-Policy），引入 Sigma Net 表征随机性；结合 GRPO 进行高效稳定微调</li><li>只需约 400 步强化微调即可显著优于强监督基线，并在扰动场景下展现更强鲁棒性</li></ul>\n<h3>论文方法描述</h3>\n<p>两阶段训练：Stage I 预训练世界模型与 VLA；Stage II 在世界模型中交互 rollout、计算验证奖励并用 GRPO 优化 VLA。</p>\n<ul><li>世界模型：基于 LLaMA 架构的轻量自回归视频预测模型（138M 参数），以最大似然训练</li><li>VLA 预训练：VLM 编码器 + 流匹配动作头，MSE 目标稳定输出动作片段</li><li>SDE-Policy：在流匹配基础上加入 Sigma Net（输出方差），将 ODE 扩展为 SDE；在 K=10 步扩散积分中计算平均对数似然并形成策略比 r</li><li>验证奖励：将世界模型生成的轨迹与离线专家轨迹对齐，定义为 L1 与 LPIPS 的负加权和；同起点 rollout 的奖励做组内平均以减方差</li><li>目标函数：GRPO 损失 + 流匹配 MSE 辅助项 + 熵正则，确保高效与稳定</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集与基准：LIBERO（Spatial、Object、Goal、Long 四套任务）</li><li>基础策略：轻量 VLA-Adapter；先进行监督微调，再进行强化微调</li><li>世界模型：138M 参数的自回归模型，基于 LLaMA 架构，在 LIBERO 上预训练</li><li>训练资源：4× A800 GPU；采用 VERL 分布式强化框架与 FSDP 切分训练</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>世界模型评估：像素误差（MSE）、峰值信噪比（PSNR）、结构相似性（SSIM）、感知距离（LPIPS）</li><li>策略评估：成功率（SR）在标准套件与扰动套件；扰动包含物体/目标位置、机器人状态及组合扰动</li><li>关键结果：约 400 步 RFT 将平均 SR 从 86.6% 提升至 91.1%，且在各类扰动下保持更高稳定性；世界模型具备高保真视觉预测能力，显著低于监督扩展 SFT 所需迭代量</li></ul>"
  },
  {
    "date": "2025-09-30",
    "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2509.26642",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-30",
    "title": "Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA",
    "link": "http://arxiv.org/abs/2509.26251",
    "summary_markdown": "### 论文研究单位\n- 主要单位：清华大学深圳国际研究生院（Tsinghua Shenzhen International Graduate School, Tsinghua University）、阿里巴巴集团高德地图（Amap, Alibaba Group）\n- 合作单位：西安交通大学软件学院（School of Software Engineering, Xi’an Jiaotong University）\n### 论文概述\n论文针对潜在行动模型（LAMs）在视觉-语言-行动（VLA）系统中的两个核心瓶颈：空间理解不足（仅依赖RGB编码，忽视几何结构）和时序感知有限（依赖稀疏双帧输入，忽略长时动态），提出了一种名为SSM-VLA（Seeing Space and Motion - Vision-Language-Action）的框架。SSM-VLA通过融合几何感知空间编码、多尺度时序建模和视觉思维链推理，增强了VLA系统的鲁棒性和可解释性。\n### 论文核心贡献点\n1. **Farsighted-LAM模型**：通过DINOv2特征的几何感知空间编码和多尺度时序建模，提升潜在行动的空间-动态表示能力。\n2. **SSM-VLA架构**：集成Farsighted-LAM与视觉思维链（VisualCoT）推理模块，实现结构化感知与显式推理的结合，增强决策一致性和可解释性。\n3. **实验验证**：在仿真（CALVIN ABC-D基准）和真实环境中取得最先进性能，证明几何建模、时序一致性和显式推理的有效性。\n### 论文方法描述\n- **Farsighted-LAM**：\n - 编码器：处理当前RGB帧和多个未来关键帧（DINOv2特征），通过潜在行动查询生成离散潜在行动序列。\n - 解码器：重构未来观测（RGB和深度），采用多模态重建损失（L2和LPIPS损失、梯度感知深度损失）监督。\n- **VLA策略**：\n - 三阶段推理：视觉思维链预测未来观测 → 远视潜在行动推理 → 条件流匹配生成行动。\n - 多模态协同注意：统一变换器内实现，结构化注意机制管理信息流。\n- **关键组件**：几何先验（深度监督）、视觉思维链推理、潜在行动量化。\n### 论文使用数据集和训练资源\n- **数据集**：\n - CALVIN基准：仿真环境（Franka Panda机械臂），训练集为A/B/C环境，测试集为未见环境D。\n - 真实世界数据：基于Open-X-Embodiment数据集预训练，在50个人类演示上微调（AgileX Piper机器人）。\n- **训练资源**：\n - 潜在行动模型：AdamW优化器，学习率10^-4，权重衰减10^-5，批大小256，训练步100，代码本大小32。\n - VLA模型：AdamW优化器，学习率10^-3，权重衰减10^-4，批大小64，训练步30，损失权重λ_vision=0.1，λ_latent=0.01。\n - 流程匹配：10步去噪。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - CALVIN仿真基准：34个操控任务，语言指令，1000个指令链（每链5个连续任务）。\n - 真实世界实验：AgileX Piper机器人执行放置任务（粉色球入盒），环境包括杂乱背景。\n- **评估指标**：\n - CALVIN基准：连续任务完成率（1-5步）和平均成功链长（与基线对比）。\n - 消融研究：验证Farsighted-LAM结构、多模态注意和几何先验的贡献。\n - 真实世界：定性展示泛化能力（视觉重建对齐）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>主要单位：清华大学深圳国际研究生院（Tsinghua Shenzhen International Graduate School, Tsinghua University）、阿里巴巴集团高德地图（Amap, Alibaba Group）</li><li>合作单位：西安交通大学软件学院（School of Software Engineering, Xi’an Jiaotong University）</li></ul>\n<h3>论文概述</h3>\n<p>论文针对潜在行动模型（LAMs）在视觉-语言-行动（VLA）系统中的两个核心瓶颈：空间理解不足（仅依赖RGB编码，忽视几何结构）和时序感知有限（依赖稀疏双帧输入，忽略长时动态），提出了一种名为SSM-VLA（Seeing Space and Motion - Vision-Language-Action）的框架。SSM-VLA通过融合几何感知空间编码、多尺度时序建模和视觉思维链推理，增强了VLA系统的鲁棒性和可解释性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>Farsighted-LAM模型</strong>：通过DINOv2特征的几何感知空间编码和多尺度时序建模，提升潜在行动的空间-动态表示能力。</li><li><strong>SSM-VLA架构</strong>：集成Farsighted-LAM与视觉思维链（VisualCoT）推理模块，实现结构化感知与显式推理的结合，增强决策一致性和可解释性。</li><li><strong>实验验证</strong>：在仿真（CALVIN ABC-D基准）和真实环境中取得最先进性能，证明几何建模、时序一致性和显式推理的有效性。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>Farsighted-LAM</strong>：</li></ul>\n<p> - 编码器：处理当前RGB帧和多个未来关键帧（DINOv2特征），通过潜在行动查询生成离散潜在行动序列。</p>\n<p> - 解码器：重构未来观测（RGB和深度），采用多模态重建损失（L2和LPIPS损失、梯度感知深度损失）监督。</p>\n<ul><li><strong>VLA策略</strong>：</li></ul>\n<p> - 三阶段推理：视觉思维链预测未来观测 → 远视潜在行动推理 → 条件流匹配生成行动。</p>\n<p> - 多模态协同注意：统一变换器内实现，结构化注意机制管理信息流。</p>\n<ul><li><strong>关键组件</strong>：几何先验（深度监督）、视觉思维链推理、潜在行动量化。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - CALVIN基准：仿真环境（Franka Panda机械臂），训练集为A/B/C环境，测试集为未见环境D。</p>\n<p> - 真实世界数据：基于Open-X-Embodiment数据集预训练，在50个人类演示上微调（AgileX Piper机器人）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 潜在行动模型：AdamW优化器，学习率10^-4，权重衰减10^-5，批大小256，训练步100，代码本大小32。</p>\n<p> - VLA模型：AdamW优化器，学习率10^-3，权重衰减10^-4，批大小64，训练步30，损失权重λ_vision=0.1，λ_latent=0.01。</p>\n<p> - 流程匹配：10步去噪。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - CALVIN仿真基准：34个操控任务，语言指令，1000个指令链（每链5个连续任务）。</p>\n<p> - 真实世界实验：AgileX Piper机器人执行放置任务（粉色球入盒），环境包括杂乱背景。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - CALVIN基准：连续任务完成率（1-5步）和平均成功链长（与基线对比）。</p>\n<p> - 消融研究：验证Farsighted-LAM结构、多模态注意和几何先验的贡献。</p>\n<p> - 真实世界：定性展示泛化能力（视觉重建对齐）。</p>"
  },
  {
    "date": "2025-09-30",
    "title": "TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses",
    "link": "http://arxiv.org/abs/2509.25746",
    "summary_markdown": "### 论文研究单位\n小米机器人（Xiaomi Robotics）\n### 论文概述\n针对机器人灵巧手抓取执行阶段的“最后一公里”精度问题，论文提出TacRefineNet——首个仅依靠多指指尖触觉反馈实现已知物体任意姿态间精确调整的框架。该方法通过迭代调整末端执行器姿态，利用高分辨率压阻式触觉传感器捕获指尖接触力，实现毫米级抓取精度。为跨越仿真到现实的鸿沟，研究团队构建了MuJoCo仿真环境与真实机器人结合的数据集，并设计了多分支融合策略网络。\n### 论文核心贡献点\n1. **首个纯触觉驱动框架**：提出TacRefineNet，实现六自由度手部姿态的毫米级精确调整，完全依赖指尖触觉反馈\n2. **多分支融合策略网络**：通过融合多指触觉信号与本体感受，设计支持任意姿态间转换的策略网络，无需逐姿态重训练\n3. **仿真-现实高效结合**：采用仿真预训练+现实微调的混合训练策略，仅用少量真实数据即显著提升性能\n### 论文方法描述\n- **迭代调整策略**：在多指建立接触后，根据触觉图像与目标图像的差异，预测末端执行器的六自由度位姿增量（Δx），通过重复抓取-调整过程收敛至目标姿态\n- **触觉传感系统**：指尖集成11×9压阻式触觉阵列（像素间距1.1mm），实时输出触觉图像用于特征提取；在MuJoCo中建立物理基触觉模拟，弹性接触点模拟真实传感器弹性行为\n- **TacRefineNet网络架构**：\n 1. 多分支视觉编码器处理每根手指的当前/目标触觉图像\n 2. 融合多指触觉特征与关节位置信息\n 3. 三层MLP回归六自由度位姿增量\n- **跨组合训练方案**：随机配对当前与目标触觉图像构建N×N组合，增强策略对任意目标姿态的适应性\n- **数据增强**：对触觉图像实施缩放噪声，对关节位置添加高斯噪声提升鲁棒性\n### 论文使用数据集和训练资源\n- **仿真数据集（Ds_img）**：基于MuJoCo物理引擎生成，在限制维度范围内系统采样手部姿态（pitch/roll/y/z轴），记录对应触觉图像、关节位置及目标姿态\n- **真实数据集（Dr_img）**：复现实实验证中的可行姿态，增大采样步长降低数据密度，剔除实际碰撞姿态\n- **混合训练策略**：对比两种政策\n - 政策A：仅仿真数据训练\n - 政策B：仿真预训练 + 真实数据微调\n- **硬件资源**：\n - 仿真：MuJoCo物理引擎\n - 实物：11自由度灵巧手 + 11×9指尖触觉传感器阵列（0-255离散力值）\n### 论文使用的评估环境和评估指标\n- **实验场景**：\n - 多维度姿态调整：16种初始-目标姿态组合（pitch/roll/y/z轴对称设置）\n - 动态目标跟踪：连续扰动下的长期姿态维持任务\n - 未见物体泛化：相似几何但不同形态的物体测试\n- **评估指标**：\n 1. **精度指标（Metric 1）**：10步调整后的6DOF误差\n - 位置误差：δ_pos = \\|\\|p - p_g\\|\\|₂\n - 角度误差：δ_rot = 2arccos(\\|⟨Q, Q_g⟩\\|)\n 2. **效率指标（Metric 2）**：达到精度阈值的步数\n - 位置阈值：ε_pos = 0.005m\n - 角度阈值：ε_rot = 0.05rad\n 3. **成功率（Metric 3）**：R=5次试验的成功率\n - 成功判定：同时满足δ_pos ≤ ε_pos、δ_rot ≤ ε_rot，且调整步数≤S_max\n- **关键结论**：政策B在所有指标上显著优于政策A，最佳组合实现1.1mm位置精度/0.016rad角度精度，100%成功率。16种组合均达毫米级位置精度，最优姿态误差0.009rad。动态场景验证稳定性，未见物体在滚转维度表现出较好泛化性。\n\n> 注：论文方法在当前对象范围内有效，未来需融合视觉信息突破单对象限制。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>小米机器人（Xiaomi Robotics）</p>\n<h3>论文概述</h3>\n<p>针对机器人灵巧手抓取执行阶段的“最后一公里”精度问题，论文提出TacRefineNet——首个仅依靠多指指尖触觉反馈实现已知物体任意姿态间精确调整的框架。该方法通过迭代调整末端执行器姿态，利用高分辨率压阻式触觉传感器捕获指尖接触力，实现毫米级抓取精度。为跨越仿真到现实的鸿沟，研究团队构建了MuJoCo仿真环境与真实机器人结合的数据集，并设计了多分支融合策略网络。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首个纯触觉驱动框架</strong>：提出TacRefineNet，实现六自由度手部姿态的毫米级精确调整，完全依赖指尖触觉反馈</li><li><strong>多分支融合策略网络</strong>：通过融合多指触觉信号与本体感受，设计支持任意姿态间转换的策略网络，无需逐姿态重训练</li><li><strong>仿真-现实高效结合</strong>：采用仿真预训练+现实微调的混合训练策略，仅用少量真实数据即显著提升性能</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>迭代调整策略</strong>：在多指建立接触后，根据触觉图像与目标图像的差异，预测末端执行器的六自由度位姿增量（Δx），通过重复抓取-调整过程收敛至目标姿态</li><li><strong>触觉传感系统</strong>：指尖集成11×9压阻式触觉阵列（像素间距1.1mm），实时输出触觉图像用于特征提取；在MuJoCo中建立物理基触觉模拟，弹性接触点模拟真实传感器弹性行为</li><li><strong>TacRefineNet网络架构</strong>：</li></ul>\n<p> 1. 多分支视觉编码器处理每根手指的当前/目标触觉图像</p>\n<p> 2. 融合多指触觉特征与关节位置信息</p>\n<p> 3. 三层MLP回归六自由度位姿增量</p>\n<ul><li><strong>跨组合训练方案</strong>：随机配对当前与目标触觉图像构建N×N组合，增强策略对任意目标姿态的适应性</li><li><strong>数据增强</strong>：对触觉图像实施缩放噪声，对关节位置添加高斯噪声提升鲁棒性</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>仿真数据集（Ds_img）</strong>：基于MuJoCo物理引擎生成，在限制维度范围内系统采样手部姿态（pitch/roll/y/z轴），记录对应触觉图像、关节位置及目标姿态</li><li><strong>真实数据集（Dr_img）</strong>：复现实实验证中的可行姿态，增大采样步长降低数据密度，剔除实际碰撞姿态</li><li><strong>混合训练策略</strong>：对比两种政策</li></ul>\n<p> - 政策A：仅仿真数据训练</p>\n<p> - 政策B：仿真预训练 + 真实数据微调</p>\n<ul><li><strong>硬件资源</strong>：</li></ul>\n<p> - 仿真：MuJoCo物理引擎</p>\n<p> - 实物：11自由度灵巧手 + 11×9指尖触觉传感器阵列（0-255离散力值）</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>实验场景</strong>：</li></ul>\n<p> - 多维度姿态调整：16种初始-目标姿态组合（pitch/roll/y/z轴对称设置）</p>\n<p> - 动态目标跟踪：连续扰动下的长期姿态维持任务</p>\n<p> - 未见物体泛化：相似几何但不同形态的物体测试</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> 1. <strong>精度指标（Metric 1）</strong>：10步调整后的6DOF误差</p>\n<p> - 位置误差：δ_pos = \\|\\|p - p_g\\|\\|₂</p>\n<p> - 角度误差：δ_rot = 2arccos(\\|⟨Q, Q_g⟩\\|)</p>\n<p> 2. <strong>效率指标（Metric 2）</strong>：达到精度阈值的步数</p>\n<p> - 位置阈值：ε_pos = 0.005m</p>\n<p> - 角度阈值：ε_rot = 0.05rad</p>\n<p> 3. <strong>成功率（Metric 3）</strong>：R=5次试验的成功率</p>\n<p> - 成功判定：同时满足δ_pos ≤ ε_pos、δ_rot ≤ ε_rot，且调整步数≤S_max</p>\n<ul><li><strong>关键结论</strong>：政策B在所有指标上显著优于政策A，最佳组合实现1.1mm位置精度/0.016rad角度精度，100%成功率。16种组合均达毫米级位置精度，最优姿态误差0.009rad。动态场景验证稳定性，未见物体在滚转维度表现出较好泛化性。</li></ul>\n\n<p>> 注：论文方法在当前对象范围内有效，未来需融合视觉信息突破单对象限制。</p>"
  },
  {
    "date": "2025-09-30",
    "title": "VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning",
    "link": "http://arxiv.org/abs/2509.25718",
    "summary_markdown": "# VLA模型通过动作分块PPO和自监督行为克隆的后训练\n## 论文研究单位\n- 中国科学院自动化研究所多模态人工智能系统国家重点实验室\n- 中国科学院大学人工智能学院\n## 论文概述\n本文提出了一种基于动作分块近端策略优化（PPO）和自监督行为克隆的VLA（视觉-语言-动作）模型后训练方法。该方法针对强化学习在VLA模型后训练中面临的稀疏奖励和不稳定训练挑战，通过将连续动作聚合为动作分块来提高策略的时间一致性和反馈密度，同时结合动态更新的演示缓冲区进行自监督行为克隆辅助训练。\n## 论文核心贡献点\n1. 开发了用于VLA模型后训练的动作分块PPO算法，增加了有效的信息反馈密度\n2. 构建了基于动态演示缓冲区的自监督行为克隆辅助损失，在训练过程中持续收集高质量任务轨迹\n3. 实验表明，该方法仅用10个演示就超过了使用100个演示的监督微调性能，成功率达0.93，步数减少到42.17\n## 论文方法描述\n### 动作分块PPO\n- 将连续动作聚合为长度为h≥1的动作分块，提高奖励反馈频率\n- 在actor-critic架构上实现PPO框架，使用截断代理目标限制策略差异\n- 通过广义优势估计（GAE）计算优势函数\n### 自监督行为克隆\n- 初始化时使用专家轨迹构建动态演示缓冲区\n- 训练过程中将高质量成功轨迹加入缓冲区\n- 使用轨迹长度作为质量代理指标，较短轨迹表示更高质量\n- 构建辅助监督损失L_BC进行策略优化\n### 在线后训练\n- 总损失函数为加权组合：L_online = β_t * L_PPO + L_BC\n- β_t采用渐进调度：β_t = tanh(t/T_warmup)，实现从监督学习到强化学习的平滑过渡\n- 早期训练完全由行为克隆驱动，随训练进展逐步强调PPO目标\n## 论文使用数据集和训练资源\n- **数据集**：MetaWorld环境中的MT10基准，包含10个单任务环境\n- **演示数据**：每个任务使用规则策略收集演示轨迹，每条轨迹限制200步\n- **模型基础**：Octo-small作为VLA模型骨干网络\n- **训练配置**：\n - 优化器：AdamW，学习率10^-5\n - GAE λ=0.99，折扣因子γ=0.99\n - PPO截断ε=0.2，值损失权重=0.5，熵损失权重=0.0\n - 动作分块长度h=4，预热步数T_warmup=40k\n - 批次大小16，总训练步数500k\n## 论文使用的评估环境和评估指标\n- **评估环境**：MetaWorld MT10基准测试\n- **评估配置**：每任务评估128个回合，使用统一随机种子\n- **评估指标**：\n - 平均成功率（Acc.）：128个回合的成功率\n - 轨迹长度分布的第10百分位数（Len.）\n - 最短10%轨迹的平均长度（Avg(l).）\n- **对比基线**：\n - SFT：10个演示的监督微调\n - SFT_100：100个演示的监督微调\n - 标准PPO：无额外演示的标准PPO\n- **评估结果**：提出的方法在平均性能和大多数单任务指标上均排名第一，证明了RL后训练的可行性和有效性",
    "summary_html": "<h1>VLA模型通过动作分块PPO和自监督行为克隆的后训练</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>中国科学院自动化研究所多模态人工智能系统国家重点实验室</li><li>中国科学院大学人工智能学院</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文提出了一种基于动作分块近端策略优化（PPO）和自监督行为克隆的VLA（视觉-语言-动作）模型后训练方法。该方法针对强化学习在VLA模型后训练中面临的稀疏奖励和不稳定训练挑战，通过将连续动作聚合为动作分块来提高策略的时间一致性和反馈密度，同时结合动态更新的演示缓冲区进行自监督行为克隆辅助训练。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>开发了用于VLA模型后训练的动作分块PPO算法，增加了有效的信息反馈密度</li><li>构建了基于动态演示缓冲区的自监督行为克隆辅助损失，在训练过程中持续收集高质量任务轨迹</li><li>实验表明，该方法仅用10个演示就超过了使用100个演示的监督微调性能，成功率达0.93，步数减少到42.17</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<h3>动作分块PPO</h3>\n<ul><li>将连续动作聚合为长度为h≥1的动作分块，提高奖励反馈频率</li><li>在actor-critic架构上实现PPO框架，使用截断代理目标限制策略差异</li><li>通过广义优势估计（GAE）计算优势函数</li></ul>\n<h3>自监督行为克隆</h3>\n<ul><li>初始化时使用专家轨迹构建动态演示缓冲区</li><li>训练过程中将高质量成功轨迹加入缓冲区</li><li>使用轨迹长度作为质量代理指标，较短轨迹表示更高质量</li><li>构建辅助监督损失L_BC进行策略优化</li></ul>\n<h3>在线后训练</h3>\n<ul><li>总损失函数为加权组合：L_online = β_t * L_PPO + L_BC</li><li>β_t采用渐进调度：β_t = tanh(t/T_warmup)，实现从监督学习到强化学习的平滑过渡</li><li>早期训练完全由行为克隆驱动，随训练进展逐步强调PPO目标</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：MetaWorld环境中的MT10基准，包含10个单任务环境</li><li><strong>演示数据</strong>：每个任务使用规则策略收集演示轨迹，每条轨迹限制200步</li><li><strong>模型基础</strong>：Octo-small作为VLA模型骨干网络</li><li><strong>训练配置</strong>：</li></ul>\n<p> - 优化器：AdamW，学习率10^-5</p>\n<p> - GAE λ=0.99，折扣因子γ=0.99</p>\n<p> - PPO截断ε=0.2，值损失权重=0.5，熵损失权重=0.0</p>\n<p> - 动作分块长度h=4，预热步数T_warmup=40k</p>\n<p> - 批次大小16，总训练步数500k</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：MetaWorld MT10基准测试</li><li><strong>评估配置</strong>：每任务评估128个回合，使用统一随机种子</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 平均成功率（Acc.）：128个回合的成功率</p>\n<p> - 轨迹长度分布的第10百分位数（Len.）</p>\n<p> - 最短10%轨迹的平均长度（Avg(l).）</p>\n<ul><li><strong>对比基线</strong>：</li></ul>\n<p> - SFT：10个演示的监督微调</p>\n<p> - SFT_100：100个演示的监督微调</p>\n<p> - 标准PPO：无额外演示的标准PPO</p>\n<ul><li><strong>评估结果</strong>：提出的方法在平均性能和大多数单任务指标上均排名第一，证明了RL后训练的可行性和有效性</li></ul>"
  },
  {
    "date": "2025-09-30",
    "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought",
    "link": "http://arxiv.org/abs/2509.25681",
    "summary_markdown": "### 论文研究单位\n美的集团、北京大学、上海交通大学\n### 论文概述\n论文提出 dVLA（diffusion Vision-Language-Action），首个以离散扩散语言模型（DLM）为核心的视觉-语言-动作一体化框架。该模型在统一的扩散目标下，联合优化视觉推理、文本推理与动作生成，并通过多模态思维链（CoT）将高级指令分解为可执行的子目标和动作。在模拟和真实环境中验证有效性：LIBERO 基准平均成功率达 96.4%，真实 Franka 机器人完成多项任务，包括具有挑战性的多步分拣。\n### 论文核心贡献点\n- 首个基于离散扩散语言模型的 VLA 框架，统一视觉、语言与动作的概率建模与生成。\n- 提出多模态 CoT 训练范式，模型需同时生成子目标图像（视觉 CoT）、文本推理与离散动作，并在训练中对三者均执行随机掩码与重建，强化跨模态一致性。\n- 在 LIBERO 基准取得 96.4% 平均成功率，优于离散与连续动作策略；在真实机器人任务上同样领先。\n- 引入推理加速策略：块级因果前缀注意力（训练时）与 KV 缓存（推理时），获得约 2× 推理加速，性能损失微小。\n- 模型可预测不安全动作对应的失败视觉 CoT，体现对物理规律和执行后果的内在理解。\n### 论文方法描述\n- 统一离散化与扩散训练：将视觉（MAGVIT-v2）、文本（LLaDA tokenizer）与动作（FAST：DCT + BPE）统一为离散 token，在相同扩散目标下随机掩码并重建，仅对掩码 token 计算损失。\n- 架构与初始化：基于 MMaDA 扩展，使用不同分词器并将词汇表从 126,464 扩展至 136,704；所有模态共享一个离散扩散建模目标。\n- 多模态 CoT 数据与序列结构：输入为 [多视角图像、语言指令、机器人状态]，输出为 [视觉子目标图像、文本推理、离散动作块]，通过起止标记组织序列。\n- 推理生成：并行输出视觉 CoT（未来状态图像）与文本 CoT（子任务步骤），再据此生成可执行的离散动作。\n- 加速策略：训练时采用分块因果前缀注意力（块内双向，块间单向），推理时引入 KV 缓存（dLLM-Cache 思想），减少重复计算，实现约 2× 加速。\n### 论文使用数据集和训练资源\n- 模拟：LIBERO 四个套件（Spatial、Object、Goal、Long），共 40 个任务，每任务 50 条演示；分辨率提升至 256×256。\n- 真实：Franka 7-DoF 机械臂，1100 条轨迹，涵盖四项任务：\n - Bin Picking：600 条\n - Open Box：100 条\n - Hang Cups：200 条\n - Pick & place Object：200 条\n- 训练细节：\n - 输入图像统一 resize 至 256×256。\n - 子目标图像预测时域在 [0.9C, 1.1C]，其中 C=5（LIBERO）、C=50（真实任务），仅预测俯视相机图像，使用 classifier-free guidance（scale=3.5）。\n - 文本推理使用 SEED-1.5VL 的视频分割注释，长任务每 3 秒一段，简单任务可省略以加速推理。\n - 训练流程与 MMaDA 一致，未明确给出硬件资源。\n### 论文使用的评估环境和评估指标\n- 模拟评估：LIBERO 基准，每任务 50 次试验（合计 500 次），以任务成功率（SR）为主要指标；与多种连续与离散动作基线对比。\n- 真实评估：Franka 机器人（2 个 ZED 外置相机 + 1 个 Realsense 435i 手腕相机），每任务 10 次试验（合计 40 次），报告成功率。\n- 推理效率：比较全注意力与前缀注意力 + KV 缓存两方案下的动作频率（Hz）与成功率，验证约 2× 推理加速与性能保持。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>美的集团、北京大学、上海交通大学</p>\n<h3>论文概述</h3>\n<p>论文提出 dVLA（diffusion Vision-Language-Action），首个以离散扩散语言模型（DLM）为核心的视觉-语言-动作一体化框架。该模型在统一的扩散目标下，联合优化视觉推理、文本推理与动作生成，并通过多模态思维链（CoT）将高级指令分解为可执行的子目标和动作。在模拟和真实环境中验证有效性：LIBERO 基准平均成功率达 96.4%，真实 Franka 机器人完成多项任务，包括具有挑战性的多步分拣。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首个基于离散扩散语言模型的 VLA 框架，统一视觉、语言与动作的概率建模与生成。</li><li>提出多模态 CoT 训练范式，模型需同时生成子目标图像（视觉 CoT）、文本推理与离散动作，并在训练中对三者均执行随机掩码与重建，强化跨模态一致性。</li><li>在 LIBERO 基准取得 96.4% 平均成功率，优于离散与连续动作策略；在真实机器人任务上同样领先。</li><li>引入推理加速策略：块级因果前缀注意力（训练时）与 KV 缓存（推理时），获得约 2× 推理加速，性能损失微小。</li><li>模型可预测不安全动作对应的失败视觉 CoT，体现对物理规律和执行后果的内在理解。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>统一离散化与扩散训练：将视觉（MAGVIT-v2）、文本（LLaDA tokenizer）与动作（FAST：DCT + BPE）统一为离散 token，在相同扩散目标下随机掩码并重建，仅对掩码 token 计算损失。</li><li>架构与初始化：基于 MMaDA 扩展，使用不同分词器并将词汇表从 126,464 扩展至 136,704；所有模态共享一个离散扩散建模目标。</li><li>多模态 CoT 数据与序列结构：输入为 [多视角图像、语言指令、机器人状态]，输出为 [视觉子目标图像、文本推理、离散动作块]，通过起止标记组织序列。</li><li>推理生成：并行输出视觉 CoT（未来状态图像）与文本 CoT（子任务步骤），再据此生成可执行的离散动作。</li><li>加速策略：训练时采用分块因果前缀注意力（块内双向，块间单向），推理时引入 KV 缓存（dLLM-Cache 思想），减少重复计算，实现约 2× 加速。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>模拟：LIBERO 四个套件（Spatial、Object、Goal、Long），共 40 个任务，每任务 50 条演示；分辨率提升至 256×256。</li><li>真实：Franka 7-DoF 机械臂，1100 条轨迹，涵盖四项任务：</li></ul>\n<p> - Bin Picking：600 条</p>\n<p> - Open Box：100 条</p>\n<p> - Hang Cups：200 条</p>\n<p> - Pick & place Object：200 条</p>\n<ul><li>训练细节：</li></ul>\n<p> - 输入图像统一 resize 至 256×256。</p>\n<p> - 子目标图像预测时域在 [0.9C, 1.1C]，其中 C=5（LIBERO）、C=50（真实任务），仅预测俯视相机图像，使用 classifier-free guidance（scale=3.5）。</p>\n<p> - 文本推理使用 SEED-1.5VL 的视频分割注释，长任务每 3 秒一段，简单任务可省略以加速推理。</p>\n<p> - 训练流程与 MMaDA 一致，未明确给出硬件资源。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>模拟评估：LIBERO 基准，每任务 50 次试验（合计 500 次），以任务成功率（SR）为主要指标；与多种连续与离散动作基线对比。</li><li>真实评估：Franka 机器人（2 个 ZED 外置相机 + 1 个 Realsense 435i 手腕相机），每任务 10 次试验（合计 40 次），报告成功率。</li><li>推理效率：比较全注意力与前缀注意力 + KV 缓存两方案下的动作频率（Hz）与成功率，验证约 2× 推理加速与性能保持。</li></ul>"
  },
  {
    "date": "2025-09-29",
    "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training",
    "link": "http://arxiv.org/abs/2509.24948",
    "summary_markdown": "## 论文研究单位\nSun Yat-sen University, China; Amap, Alibaba Group\n## 论文概述\nVision-Language-Action (VLA) 模型在少样本与高风险场景下受限：模仿学习对大量演示数据依赖强，强化学习（RL）后训练需真实交互但环境状态不可重置、反馈稀疏且难以判定任务完成。作者提出 World-Env，将世界模型作为低成本、可安全探索的虚拟环境用于 VLA 的 RL 后训练：包括一个视频世界模型用于生成时序一致的未来视觉观测，和一个 VLM 引导的即时反射器用于连续奖励与实时终止信号。该方法在 5 条专家演示下即可显著提升复杂操控任务的成功率，兼顾数据效率与安全性。\n## 论文核心贡献点\n- 提出 World-Env 框架：以世界模型替代真实交互，支持 VLA 的低成本、风险可控的 RL 后训练。\n- 集成视频世界模拟器和 VLM 引导的即时反射器，提供语义对齐的连续奖励与语言对齐的任务完成判定。\n- 引入动态终止机制（基于 R(o1:t, g) > η），避免成功后冗余动作，提升执行效率与成功率。\n## 论文方法描述\n- 视频世界模拟器（EVAC 思路）：以动作 a_t 和下时刻本体感觉状态 s_{t+1}（6D末端位姿与 1D夹爪）作为输入，投影构建动作图（前景标记+黑背景），以像素级条件通过扩散图像生成预测下一帧视觉观测 o_{t+1}。训练数据混合人类演示与自探索轨迹；自探索通过在模拟器内执行 SFT 策略并用 Laplace 分布引入扰动（scale head 预测 β_t），以覆盖失败与次优行为分布。\n- VLM 引导的即时反射器：基于 LLaVA，使用冻结的视觉编码器与 LLM 以视频-文本为条件，奖励头输出 R(o1:t, g) ∈ [0,1]，采用阈值 η=0.5 触发终止；训练采用二进制交叉熵（BCE）监督，数据源自 LIBERO 成功判定与模拟器内 oracle 标记。\n- VLA 策略与 RL：基于 OpenVLA-OFT 的 VLA 策略添加 scale head 使动作分布为 Laplace(μ_t, β_t)，用于不确定度驱动的探索。RL 优化采用 LOOP（RLOO 优势 + PPO 截断目标）：对同一初始状态生成 N 条轨迹（N=8），用 RLOO 计算轨迹优势 A_n，基于 PPO 截断目标更新策略与 scale head。\n- 终止与奖励使用：在终止步 t_end 给出单步轨迹奖励 R_n = R(o1:t_end, g)，优势广播到轨迹各步；终止时提前结束 rollout。\n- 训练实现：LoRA 微调视觉-语言骨干（rank=32），LoRA LR=1e-4，动作/标度头全参 LR=1e-5，批量 4，8×H20 GPU（96GB）。\n## 论文使用数据集和训练资源\n- 数据集：LIBERO（含 LIBERO-Goal、LIBERO-Object、LIBERO-Spatial、LIBERO-Long 四套），每任务仅用 5 条专家演示进行 SFT 预训练；世界模型额外加入自探索过渡数据（含成功与失败）。\n- 资源：8×NVIDIA H20 GPU（96GB），LoRA（rank=32）进行参数高效微调。\n## 论文使用的评估环境和评估指标\n- 评估环境：LIBERO 仿真平台，面向机器人视觉-语言操控。\n- 指标：成功率（对全测试集）。在“真实反馈约束”设定下比较终止策略（无法获得真实终止信号），验证动态终止与连续奖励的稳定性。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>Sun Yat-sen University, China; Amap, Alibaba Group</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>Vision-Language-Action (VLA) 模型在少样本与高风险场景下受限：模仿学习对大量演示数据依赖强，强化学习（RL）后训练需真实交互但环境状态不可重置、反馈稀疏且难以判定任务完成。作者提出 World-Env，将世界模型作为低成本、可安全探索的虚拟环境用于 VLA 的 RL 后训练：包括一个视频世界模型用于生成时序一致的未来视觉观测，和一个 VLM 引导的即时反射器用于连续奖励与实时终止信号。该方法在 5 条专家演示下即可显著提升复杂操控任务的成功率，兼顾数据效率与安全性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出 World-Env 框架：以世界模型替代真实交互，支持 VLA 的低成本、风险可控的 RL 后训练。</li><li>集成视频世界模拟器和 VLM 引导的即时反射器，提供语义对齐的连续奖励与语言对齐的任务完成判定。</li><li>引入动态终止机制（基于 R(o1:t, g) > η），避免成功后冗余动作，提升执行效率与成功率。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>视频世界模拟器（EVAC 思路）：以动作 a_t 和下时刻本体感觉状态 s_{t+1}（6D末端位姿与 1D夹爪）作为输入，投影构建动作图（前景标记+黑背景），以像素级条件通过扩散图像生成预测下一帧视觉观测 o_{t+1}。训练数据混合人类演示与自探索轨迹；自探索通过在模拟器内执行 SFT 策略并用 Laplace 分布引入扰动（scale head 预测 β_t），以覆盖失败与次优行为分布。</li><li>VLM 引导的即时反射器：基于 LLaVA，使用冻结的视觉编码器与 LLM 以视频-文本为条件，奖励头输出 R(o1:t, g) ∈ [0,1]，采用阈值 η=0.5 触发终止；训练采用二进制交叉熵（BCE）监督，数据源自 LIBERO 成功判定与模拟器内 oracle 标记。</li><li>VLA 策略与 RL：基于 OpenVLA-OFT 的 VLA 策略添加 scale head 使动作分布为 Laplace(μ_t, β_t)，用于不确定度驱动的探索。RL 优化采用 LOOP（RLOO 优势 + PPO 截断目标）：对同一初始状态生成 N 条轨迹（N=8），用 RLOO 计算轨迹优势 A_n，基于 PPO 截断目标更新策略与 scale head。</li><li>终止与奖励使用：在终止步 t_end 给出单步轨迹奖励 R_n = R(o1:t_end, g)，优势广播到轨迹各步；终止时提前结束 rollout。</li><li>训练实现：LoRA 微调视觉-语言骨干（rank=32），LoRA LR=1e-4，动作/标度头全参 LR=1e-5，批量 4，8×H20 GPU（96GB）。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据集：LIBERO（含 LIBERO-Goal、LIBERO-Object、LIBERO-Spatial、LIBERO-Long 四套），每任务仅用 5 条专家演示进行 SFT 预训练；世界模型额外加入自探索过渡数据（含成功与失败）。</li><li>资源：8×NVIDIA H20 GPU（96GB），LoRA（rank=32）进行参数高效微调。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：LIBERO 仿真平台，面向机器人视觉-语言操控。</li><li>指标：成功率（对全测试集）。在“真实反馈约束”设定下比较终止策略（无法获得真实终止信号），验证动态终止与连续奖励的稳定性。</li></ul>"
  },
  {
    "date": "2025-09-29",
    "title": "IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks",
    "link": "http://arxiv.org/abs/2509.24768",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-29",
    "title": "Emergent World Representations in OpenVLA",
    "link": "http://arxiv.org/abs/2509.24559",
    "summary_markdown": "### 论文研究单位\n- LSE.AI, London School of Economics\n- ETH Zurich\n- Princeton University - Department of Computer Science\n- Mila - Quebec AI Institute\n### 论文概述\nVision-Language-Action models (VLAs) 如OpenVLA，训练于策略基础强化学习（RL），但是否隐式学习世界模型（状态转移函数）未知。论文通过embedding arithmetic在状态表示上实验，探测OpenVLA是否编码潜在世界模型。使用线性/非线性探针预测状态转移向量，发现激活探针优于embeddings基线，表明世界模型存在。调查训练进展，发现世界模型随计算扩展而出现，并提出SAEs的可解释规划管道。\n### 论文核心贡献点\n- 利用embedding arithmetic证明OpenVLA编码潜在世界模型。\n- 展示训练计算扩展增强世界模型发展，并定位其于中间层。\n- 提出SAEs应用于可解释规划：预测状态转移向量后分解为可解释特征。\n- 线性探针优于MLP探针，支持线性表示假设（LHR）。\n### 论文方法描述\n- **理论框架**：基于Koopman算子近似世界模型，定义K步状态转移算子。\n- **状态转移向量**：学习函数 \\( f: \\mathbf{a}_t \\mapsto \\Delta\\mathbf{e}_{t\\rightarrow t+K} \\)，其中 \\(\\Delta\\mathbf{e} = \\mathbf{e}_{t+K} - \\mathbf{e}_t\\)。\n- **探针**：\n - 线性探针（Lasso回归）预测 \\(\\Delta\\mathbf{e}\\) 从激活 \\(\\mathbf{a}_t\\)。\n - MLP探针测试非线性表示。\n- **基线对比**：训练探针于embeddings作为基线，隔离因果效应。\n- **评估**：R²分数和置换检验（p < 0.01）量化预测性能。\n### 论文使用数据集和训练资源\n- **数据集**：LIBERO数据集（goal, spatial, long, object子集），共400 episodes, 66,931步。\n- **模型**：OpenVLA (7B参数)，预训练于Open X-Embodiment数据集。\n- **训练资源**：探针训练使用网格搜索调优超参数；计算资源未详述，代码可用。\n### 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO各子集测试集。\n- **评估指标**：\n - 回归R²分数评估预测能力。\n - 置换检验（100次）检验统计显著性（p值）。\n - Allan方差分析长时状态转移信噪比。\n - 时间一致性（cosine similarity）测量嵌入稳定性。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>LSE.AI, London School of Economics</li><li>ETH Zurich</li><li>Princeton University - Department of Computer Science</li><li>Mila - Quebec AI Institute</li></ul>\n<h3>论文概述</h3>\n<p>Vision-Language-Action models (VLAs) 如OpenVLA，训练于策略基础强化学习（RL），但是否隐式学习世界模型（状态转移函数）未知。论文通过embedding arithmetic在状态表示上实验，探测OpenVLA是否编码潜在世界模型。使用线性/非线性探针预测状态转移向量，发现激活探针优于embeddings基线，表明世界模型存在。调查训练进展，发现世界模型随计算扩展而出现，并提出SAEs的可解释规划管道。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>利用embedding arithmetic证明OpenVLA编码潜在世界模型。</li><li>展示训练计算扩展增强世界模型发展，并定位其于中间层。</li><li>提出SAEs应用于可解释规划：预测状态转移向量后分解为可解释特征。</li><li>线性探针优于MLP探针，支持线性表示假设（LHR）。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>理论框架</strong>：基于Koopman算子近似世界模型，定义K步状态转移算子。</li><li><strong>状态转移向量</strong>：学习函数 \\( f: \\mathbf{a}_t \\mapsto \\Delta\\mathbf{e}_{t\\rightarrow t+K} \\)，其中 \\(\\Delta\\mathbf{e} = \\mathbf{e}_{t+K} - \\mathbf{e}_t\\)。</li><li><strong>探针</strong>：</li></ul>\n<p> - 线性探针（Lasso回归）预测 \\(\\Delta\\mathbf{e}\\) 从激活 \\(\\mathbf{a}_t\\)。</p>\n<p> - MLP探针测试非线性表示。</p>\n<ul><li><strong>基线对比</strong>：训练探针于embeddings作为基线，隔离因果效应。</li><li><strong>评估</strong>：R²分数和置换检验（p < 0.01）量化预测性能。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：LIBERO数据集（goal, spatial, long, object子集），共400 episodes, 66,931步。</li><li><strong>模型</strong>：OpenVLA (7B参数)，预训练于Open X-Embodiment数据集。</li><li><strong>训练资源</strong>：探针训练使用网格搜索调优超参数；计算资源未详述，代码可用。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：LIBERO各子集测试集。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 回归R²分数评估预测能力。</p>\n<p> - 置换检验（100次）检验统计显著性（p值）。</p>\n<p> - Allan方差分析长时状态转移信噪比。</p>\n<p> - 时间一致性（cosine similarity）测量嵌入稳定性。</p>"
  },
  {
    "date": "2025-09-29",
    "title": "PhysiAgent: An Embodied Agent Framework in Physical World",
    "link": "http://arxiv.org/abs/2509.24524",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-28",
    "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models",
    "link": "http://arxiv.org/abs/2509.23655",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-27",
    "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
    "link": "http://arxiv.org/abs/2509.23224",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-27",
    "title": "Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges",
    "link": "http://arxiv.org/abs/2509.23121",
    "summary_markdown": "## 论文研究单位\n\n辽宁辽河实验室（项目编号：LLL24ZZ-02-01 和 LLL24ZZ-02-02）\n## 论文概述\n\n该论文评估了视觉语言动作（VLA）模型在工业环境中的适用性。研究从工业部署的角度比较了现有SOTA VLA模型在工业场景中的性能，并从数据收集和模型架构两个角度分析了VLA模型在现实工业部署中的局限性。研究发现，当前VLA模型在复杂工业环境、多样化物体类别和高精度放置任务方面仍有很大改进空间。\n## 论文核心贡献点\n\n- 评估了最先进的VLA模型在工业场景中的拾取和放置任务性能\n- 从数据集和模型架构两个角度分析VLA模型对非结构化环境的适应性\n- 讨论了提高VLA模型鲁棒性和任务泛化能力的潜在方向\n- 提供了VLA模型在工业应用中的实用见解\n## 论文方法描述\n\n提出了VLA的通用技术框架，包含以下关键技术模块：\n- 归一化和反归一化：消除不同任务或机器人平台间的维度不一致性\n- 数据增强：包括图像增强（随机裁剪、颜色抖动、图像损坏等）和指令增强（提示变化、释义等）\n- 投影器：连接视觉编码器和LLM的桥梁，进行维度对齐和语义映射\n- 预训练VLM：包括视觉编码器（如DINOv2、SigLIP）和语言编码器（如LLaMA、Gemma、Qwen2-VL）\n- 政策头：三种主要架构类型包括自回归、扩散和混合模型\n## 论文使用数据集和训练资源\n\n- 使用来自真实工业场景采集的回合数据进行模型微调，每个任务使用100个回合\n- 在NVIDIA H20 96GB GPU上训练10小时\n- 使用Ubuntu 20.04和ROS 1 noetic系统\n- 在Mobile ALOHA双机械臂机器人上进行所有测试\n## 论文使用的评估环境和评估指标\n\n- 评估环境：三个工业场景，涵盖视觉遮挡、相机抖动、目标姿态随机化和目标多样性等扰动\n- 主要指标：成功率（%），计算方法为成功试验次数除以总试验次数\n- 精度指标：位置误差（2.2 cm）和方向误差（12.4°）\n- 实验设置：每个试验包含10次测试，涵盖目标姿态随机化和多样化物体类别\n- 评估结果：Pi0模型经过微调后，在简单抓取任务上达到约60%的成功率，但在高精度放置任务中精度仍有不足",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n\n<p>辽宁辽河实验室（项目编号：LLL24ZZ-02-01 和 LLL24ZZ-02-02）</p>\n<h2 class=\"section-title\">论文概述</h2>\n\n<p>该论文评估了视觉语言动作（VLA）模型在工业环境中的适用性。研究从工业部署的角度比较了现有SOTA VLA模型在工业场景中的性能，并从数据收集和模型架构两个角度分析了VLA模型在现实工业部署中的局限性。研究发现，当前VLA模型在复杂工业环境、多样化物体类别和高精度放置任务方面仍有很大改进空间。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n\n<ul><li>评估了最先进的VLA模型在工业场景中的拾取和放置任务性能</li><li>从数据集和模型架构两个角度分析VLA模型对非结构化环境的适应性</li><li>讨论了提高VLA模型鲁棒性和任务泛化能力的潜在方向</li><li>提供了VLA模型在工业应用中的实用见解</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n\n<p>提出了VLA的通用技术框架，包含以下关键技术模块：</p>\n<ul><li>归一化和反归一化：消除不同任务或机器人平台间的维度不一致性</li><li>数据增强：包括图像增强（随机裁剪、颜色抖动、图像损坏等）和指令增强（提示变化、释义等）</li><li>投影器：连接视觉编码器和LLM的桥梁，进行维度对齐和语义映射</li><li>预训练VLM：包括视觉编码器（如DINOv2、SigLIP）和语言编码器（如LLaMA、Gemma、Qwen2-VL）</li><li>政策头：三种主要架构类型包括自回归、扩散和混合模型</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n\n<ul><li>使用来自真实工业场景采集的回合数据进行模型微调，每个任务使用100个回合</li><li>在NVIDIA H20 96GB GPU上训练10小时</li><li>使用Ubuntu 20.04和ROS 1 noetic系统</li><li>在Mobile ALOHA双机械臂机器人上进行所有测试</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n\n<ul><li>评估环境：三个工业场景，涵盖视觉遮挡、相机抖动、目标姿态随机化和目标多样性等扰动</li><li>主要指标：成功率（%），计算方法为成功试验次数除以总试验次数</li><li>精度指标：位置误差（2.2 cm）和方向误差（12.4°）</li><li>实验设置：每个试验包含10次测试，涵盖目标姿态随机化和多样化物体类别</li><li>评估结果：Pi0模型经过微调后，在简单抓取任务上达到约60%的成功率，但在高精度放置任务中精度仍有不足</li></ul>"
  },
  {
    "date": "2025-09-26",
    "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search",
    "link": "http://arxiv.org/abs/2509.22643",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-26",
    "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation",
    "link": "http://arxiv.org/abs/2509.22441",
    "summary_markdown": "### 论文研究单位\n西湖大学工程学院（主单位），浙江大学信息与电子工程学院、环境与资源学院，澳大利亚国立大学。\n### 论文概述\n提出首个针对自主水下航行器（AUV）的视觉-语言-动作（VLA）框架 UnderwaterVLA，旨在解决海洋环境的极端挑战（水动力学干扰、感知退化、通信受限）。该框架首次将VLA模型应用于水下机器人，通过双脑架构解耦高层推理与低层控制，实现零数据训练和水动力实时补偿，显著提升导航精度与任务成功率。\n### 论文核心贡献点\n1. **首个水下VLA架构**：系统性解决VLA在水下应用的层级、数据需求和动力学补偿三大核心挑战。\n2. **零数据训练策略**：通过预训练多模态模型的迁移学习与物理先验融合，规避昂贵的水下演示数据。\n3. **双脑解耦控制**：云脑负责长时域任务规划，小脑实现实时感知-控制闭环，在通信/算力受限下保障鲁棒性。\n4. **水动力实时补偿**：在MPC中嵌入流体动力学模型，通过在线参数估计自适应复杂水流环境。\n### 论文方法描述\n1. **双脑架构**：\n - **云脑**：Surfacing时运行QVQmax模型，将高层指令分解为序列化子任务（如“右转躲避障碍→直行→低速接近”）。\n - **小脑**：本地Qwen-VL模型循环执行JSON格式控制指令，动态调整决策（如检测安全距离后自主终止任务）。\n - 结构示例：`云脑计划 → 子任务序列 → 本地执行（感知-动作循环）`。\n\n2. **提示工程与可解释性**：\n - 强制云脑/小脑输出思维链（CoT）推理过程，记录决策逻辑。\n - 小脑输出标准化JSON结构：\n ```json\n {\n \"reasoning\": \"检测到左侧障碍，右转\",\n \"decision\": \"right\",\n \"velocity\": \"medium\",\n \"sub_task_done\": false\n }\n ```\n\n3. **零数据MPC水动力控制**：\n - **运动剖面**：平移/旋转动作严格分为1秒周期的加速-恒速-减速阶段（速度：0.2/0.5/0.8 m/s，角速度：0.5/1.0/1.5 rad/s）。\n - **MPC优化**：最小化代价函数同时追踪参考信号、控制力和流体阻力：\n ```math\n J = \\int_0^1 \\left[ \\beta\\\\|v(t)-v_{\\text{ref}}\\\\|^2 + \\gamma\\\\|\\tau\\\\|^2 + \\delta\\\\|F_{\\text{drag}}\\\\|^2 \\right] dt\n ```\n - **流体参数在线估计**：基于IMU数据实时估算拖拽系数：\n ```math\n \\hat{D}_v = \\frac{\\tau_v - M\\dot{v}}{v\\|v\\|}\n ```\n### 论文使用数据集和训练资源\n- **训练数据需求**：**零**（完全依赖预训练模型 + 物理先验，无需水下演示数据）\n- **基线数据对比**：QUAR-VLA等基线需262K演示数据（来源自其他机器人任务）。\n- **模型资源**：\n - 云脑：QVQmax（大语言模型）\n - 小脑：Qwen 2.5-VL-7B（多模态视觉-语言模型）\n### 论文使用的评估环境和评估指标\n1. **实验环境**：\n - **实验室测试**：控制变量水池，3垂直柱状障碍，用于导航与避障任务。\n - **真实环境模拟**：降低光照强度至海平面5%，注入硅藻土提升浊度至18 NTU（模拟近岸浑浊水域）。\n\n2. **评估指标**：\n - **量化性能**：\n - 任务成功率：对比QUAR-VLA基线，感知/导航/隧道穿越/避障任务提升19%-27%。\n - 零数据效率：基线需262K数据，UnderwaterVLA无需任何水下演示数据。\n - **鲁棒性评估**：\n - 双脑架构（DBM）vs 单脑模型（SBM）：在浑浊环境中，DBM能自主终止任务避免过冲，SBM因目标丢失持续前进导致任务失败。\n - 可视化证据：轨迹对比图显示DBM在低能见度下精确接近目标，SBM偏离路径。\n\n**结论**：通过解耦控制层级、融合物理先验和可解释推理链，UnderwaterVLA在零数据条件下实现水下机器人任务性能突破，为复杂海洋环境中的自主作业提供了可行路径。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>西湖大学工程学院（主单位），浙江大学信息与电子工程学院、环境与资源学院，澳大利亚国立大学。</p>\n<h3>论文概述</h3>\n<p>提出首个针对自主水下航行器（AUV）的视觉-语言-动作（VLA）框架 UnderwaterVLA，旨在解决海洋环境的极端挑战（水动力学干扰、感知退化、通信受限）。该框架首次将VLA模型应用于水下机器人，通过双脑架构解耦高层推理与低层控制，实现零数据训练和水动力实时补偿，显著提升导航精度与任务成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首个水下VLA架构</strong>：系统性解决VLA在水下应用的层级、数据需求和动力学补偿三大核心挑战。</li><li><strong>零数据训练策略</strong>：通过预训练多模态模型的迁移学习与物理先验融合，规避昂贵的水下演示数据。</li><li><strong>双脑解耦控制</strong>：云脑负责长时域任务规划，小脑实现实时感知-控制闭环，在通信/算力受限下保障鲁棒性。</li><li><strong>水动力实时补偿</strong>：在MPC中嵌入流体动力学模型，通过在线参数估计自适应复杂水流环境。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>双脑架构</strong>：</li></ol>\n<p> - <strong>云脑</strong>：Surfacing时运行QVQmax模型，将高层指令分解为序列化子任务（如“右转躲避障碍→直行→低速接近”）。</p>\n<p> - <strong>小脑</strong>：本地Qwen-VL模型循环执行JSON格式控制指令，动态调整决策（如检测安全距离后自主终止任务）。</p>\n<p> - 结构示例：<code>云脑计划 → 子任务序列 → 本地执行（感知-动作循环）</code>。</p>\n\n<ol><li><strong>提示工程与可解释性</strong>：</li></ol>\n<p> - 强制云脑/小脑输出思维链（CoT）推理过程，记录决策逻辑。</p>\n<p> - 小脑输出标准化JSON结构：</p>\n<p> ```json</p>\n<p> {</p>\n<p> \"reasoning\": \"检测到左侧障碍，右转\",</p>\n<p> \"decision\": \"right\",</p>\n<p> \"velocity\": \"medium\",</p>\n<p> \"sub_task_done\": false</p>\n<p> }</p>\n<p> ```</p>\n\n<ol><li><strong>零数据MPC水动力控制</strong>：</li></ol>\n<p> - <strong>运动剖面</strong>：平移/旋转动作严格分为1秒周期的加速-恒速-减速阶段（速度：0.2/0.5/0.8 m/s，角速度：0.5/1.0/1.5 rad/s）。</p>\n<p> - <strong>MPC优化</strong>：最小化代价函数同时追踪参考信号、控制力和流体阻力：</p>\n<p> ```math</p>\n<p> J = \\int_0^1 \\left[ \\beta\\\\|v(t)-v_{\\text{ref}}\\\\|^2 + \\gamma\\\\|\\tau\\\\|^2 + \\delta\\\\|F_{\\text{drag}}\\\\|^2 \\right] dt</p>\n<p> ```</p>\n<p> - <strong>流体参数在线估计</strong>：基于IMU数据实时估算拖拽系数：</p>\n<p> ```math</p>\n<p> \\hat{D}_v = \\frac{\\tau_v - M\\dot{v}}{v\\|v\\|}</p>\n<p> ```</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>训练数据需求</strong>：<strong>零</strong>（完全依赖预训练模型 + 物理先验，无需水下演示数据）</li><li><strong>基线数据对比</strong>：QUAR-VLA等基线需262K演示数据（来源自其他机器人任务）。</li><li><strong>模型资源</strong>：</li></ul>\n<p> - 云脑：QVQmax（大语言模型）</p>\n<p> - 小脑：Qwen 2.5-VL-7B（多模态视觉-语言模型）</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>实验环境</strong>：</li></ol>\n<p> - <strong>实验室测试</strong>：控制变量水池，3垂直柱状障碍，用于导航与避障任务。</p>\n<p> - <strong>真实环境模拟</strong>：降低光照强度至海平面5%，注入硅藻土提升浊度至18 NTU（模拟近岸浑浊水域）。</p>\n\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>量化性能</strong>：</p>\n<p> - 任务成功率：对比QUAR-VLA基线，感知/导航/隧道穿越/避障任务提升19%-27%。</p>\n<p> - 零数据效率：基线需262K数据，UnderwaterVLA无需任何水下演示数据。</p>\n<p> - <strong>鲁棒性评估</strong>：</p>\n<p> - 双脑架构（DBM）vs 单脑模型（SBM）：在浑浊环境中，DBM能自主终止任务避免过冲，SBM因目标丢失持续前进导致任务失败。</p>\n<p> - 可视化证据：轨迹对比图显示DBM在低能见度下精确接近目标，SBM偏离路径。</p>\n\n<p><strong>结论</strong>：通过解耦控制层级、融合物理先验和可解释推理链，UnderwaterVLA在零数据条件下实现水下机器人任务性能突破，为复杂海洋环境中的自主作业提供了可行路径。</p>"
  },
  {
    "date": "2025-09-26",
    "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer",
    "link": "http://arxiv.org/abs/2509.22407",
    "summary_markdown": "### 论文研究单位\n- 所属机构：GigaAI、Peking University、Tsinghua University、CASIA\n### 论文概述\n- 目标：解决VLA（视觉–语言–动作）模型因真实机器人演示数据昂贵、难以扩展而导致的泛化能力不足问题\n- 核心方案：提出EMMA框架，结合生成式数据引擎与自适应训练策略，通过文本控制的视频生成扩充数据，提升策略在新外观与新环境的零样本泛化\n### 论文核心贡献点\n- 提出EMMA框架，集成DreamTransfer与AdaMix，显著提升真实世界机器人操作的泛化\n- DreamTransfer：DiT（扩散Transformer）双分支架构，融合深度与文本，实现多视角一致、几何可控的机器人操作视频生成\n- AdaMix：基于轨迹性能的自适应重采样，动态提高“困难样本”的训练权重，增强策略稳健性与泛化\n- 与SOTA相比，DreamTransfer在多视角一致性提升42%、深度一致性提升24%；真实世界零样本视觉任务相较仅用真实数据提升超过200%，AdaMix额外提升13%\n### 论文方法描述\n- DreamTransfer总体结构：主分支用于去噪潜空间视频令牌，ControlNet分支注入深度结构约束；多视角视频深度沿宽度拼接作为统一潜表示；T5文本编码器提供语义嵌入，并通过交叉注意力融合至主分支\n- 文本控制与几何一致性：用户可通过自然语言编辑前景、背景与光照，同时保持3D结构与几何合理性；多视角与深度约束共同维持跨视角与时间的一致性\n- AdaMix训练策略：初期使用高质量视频（低质量样本权重为零），稳定训练；在收敛后根据三类指标动态调整采样权重，聚焦“困难样本”\n - 指标：动作预测误差MSE（负号）、轨迹平滑度（角加速度二阶差分负号）、关节限位（越界为0，否则为1）\n - 组合得分：归一化后取平均\n - 更新权重：p(i) ∝ γ + λ·(1 − s_i)，保证最小支持并强调困难样本\n- 数据混合与筛选：真实与生成数据按比例混合训练；生成视频以多视角一致性、深度一致性与文本–视频相似度进行过滤与评分\n- 训练细节：两阶段微调（低分辨率稳定多视角一致性 → 高分辨率细节恢复）；优化器AdamW；分任务（折叠布料、清洁桌面、投掷瓶子）不同步数配置\n### 论文使用数据集和训练资源\n- 真实演示：折叠布料（50条）、清洁桌面（20条）、投掷瓶子（20条）\n- 生成数据：Agibot World与NVIDIA Isaac Sim采集；基于Agibot World构建5万条多视角视频（含对齐RGB、时间一致深度与模板化文本描述）\n- 生成流程：多视角深度一致性由Video Depth Anything估计；前景/背景/光照描述由Qwen2.5-VL-7B生成\n- 仿真演示：NVIDIA Isaac Sim采集，DreamTransfer执行视间一致的风格与外观转移\n- 训练硬件与平台：AgileX CobotMagic，双PiPER臂，三台Intel RealSense D435i摄像头；Isaac Sim用于仿真\n- 两阶段微调配置：阶段一576×128分辨率、批量32、3500步；阶段二1920×480分辨率、批量4、4500步\n### 论文使用的评估环境和评估指标\n- 任务：折叠布料（真实到真实）、清洁桌面/投掷瓶子（仿真到真实）\n- 视频生成质量评估\n - 多视角一致性：匹配像素数（Pix.Mat.）\n - 深度一致性：RMSE、绝对相对误差（Abs.Rel.）、平方相对误差（Sq.Rel.）\n - 文本–视频对齐：CLIP相似度（CLIPSim.）\n- 真实世界机器人评估\n - 行为得分：最大5分，按失败场景扣分（任务不同规则不同）\n - 成功率：每任务在5次试验与4种外观变化下评估，共20次运行\n - 执行质量：执行时间、轨迹平滑度（角加速度）、关节越界帧数\n- 数据混合实验：固定总量与步数，考察真实/生成比例对成功率的影响（50%为峰值）\n- 消融实验：固定混合比例（FixMix）对比AdaMix，显示AdaMix在行为得分、成功率与执行质量上的综合提升",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>所属机构：GigaAI、Peking University、Tsinghua University、CASIA</li></ul>\n<h3>论文概述</h3>\n<ul><li>目标：解决VLA（视觉–语言–动作）模型因真实机器人演示数据昂贵、难以扩展而导致的泛化能力不足问题</li><li>核心方案：提出EMMA框架，结合生成式数据引擎与自适应训练策略，通过文本控制的视频生成扩充数据，提升策略在新外观与新环境的零样本泛化</li></ul>\n<h3>论文核心贡献点</h3>\n<ul><li>提出EMMA框架，集成DreamTransfer与AdaMix，显著提升真实世界机器人操作的泛化</li><li>DreamTransfer：DiT（扩散Transformer）双分支架构，融合深度与文本，实现多视角一致、几何可控的机器人操作视频生成</li><li>AdaMix：基于轨迹性能的自适应重采样，动态提高“困难样本”的训练权重，增强策略稳健性与泛化</li><li>与SOTA相比，DreamTransfer在多视角一致性提升42%、深度一致性提升24%；真实世界零样本视觉任务相较仅用真实数据提升超过200%，AdaMix额外提升13%</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>DreamTransfer总体结构：主分支用于去噪潜空间视频令牌，ControlNet分支注入深度结构约束；多视角视频深度沿宽度拼接作为统一潜表示；T5文本编码器提供语义嵌入，并通过交叉注意力融合至主分支</li><li>文本控制与几何一致性：用户可通过自然语言编辑前景、背景与光照，同时保持3D结构与几何合理性；多视角与深度约束共同维持跨视角与时间的一致性</li><li>AdaMix训练策略：初期使用高质量视频（低质量样本权重为零），稳定训练；在收敛后根据三类指标动态调整采样权重，聚焦“困难样本”</li></ul>\n<p> - 指标：动作预测误差MSE（负号）、轨迹平滑度（角加速度二阶差分负号）、关节限位（越界为0，否则为1）</p>\n<p> - 组合得分：归一化后取平均</p>\n<p> - 更新权重：p(i) ∝ γ + λ·(1 − s_i)，保证最小支持并强调困难样本</p>\n<ul><li>数据混合与筛选：真实与生成数据按比例混合训练；生成视频以多视角一致性、深度一致性与文本–视频相似度进行过滤与评分</li><li>训练细节：两阶段微调（低分辨率稳定多视角一致性 → 高分辨率细节恢复）；优化器AdamW；分任务（折叠布料、清洁桌面、投掷瓶子）不同步数配置</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>真实演示：折叠布料（50条）、清洁桌面（20条）、投掷瓶子（20条）</li><li>生成数据：Agibot World与NVIDIA Isaac Sim采集；基于Agibot World构建5万条多视角视频（含对齐RGB、时间一致深度与模板化文本描述）</li><li>生成流程：多视角深度一致性由Video Depth Anything估计；前景/背景/光照描述由Qwen2.5-VL-7B生成</li><li>仿真演示：NVIDIA Isaac Sim采集，DreamTransfer执行视间一致的风格与外观转移</li><li>训练硬件与平台：AgileX CobotMagic，双PiPER臂，三台Intel RealSense D435i摄像头；Isaac Sim用于仿真</li><li>两阶段微调配置：阶段一576×128分辨率、批量32、3500步；阶段二1920×480分辨率、批量4、4500步</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>任务：折叠布料（真实到真实）、清洁桌面/投掷瓶子（仿真到真实）</li><li>视频生成质量评估</li></ul>\n<p> - 多视角一致性：匹配像素数（Pix.Mat.）</p>\n<p> - 深度一致性：RMSE、绝对相对误差（Abs.Rel.）、平方相对误差（Sq.Rel.）</p>\n<p> - 文本–视频对齐：CLIP相似度（CLIPSim.）</p>\n<ul><li>真实世界机器人评估</li></ul>\n<p> - 行为得分：最大5分，按失败场景扣分（任务不同规则不同）</p>\n<p> - 成功率：每任务在5次试验与4种外观变化下评估，共20次运行</p>\n<p> - 执行质量：执行时间、轨迹平滑度（角加速度）、关节越界帧数</p>\n<ul><li>数据混合实验：固定总量与步数，考察真实/生成比例对成功率的影响（50%为峰值）</li><li>消融实验：固定混合比例（FixMix）对比AdaMix，显示AdaMix在行为得分、成功率与执行质量上的综合提升</li></ul>"
  },
  {
    "date": "2025-09-26",
    "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training",
    "link": "http://arxiv.org/abs/2509.22199",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-26",
    "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting",
    "link": "http://arxiv.org/abs/2509.22195",
    "summary_markdown": "## 论文总结\n### 论文研究单位\n普林斯顿大学（Department of Mechanical and Aerospace Engineering, Department of Computer Science）\n### 论文概述\n本文针对将视觉语言模型（VLM）微调为视觉语言行动模型（VLA）时存在的核心挑战——灾难性遗忘进行了深入研究。传统方法在获得机器人操作能力的同时会严重削弱VLM的基础推理和多模态理解能力。本文提出了一种数据驱动的解决方案——**VLM2VLA**，其核心思想是将机器人动作转化为自然语言描述（Actions as Language），从而最小化微调数据与预训练数据之间的分布差异。该方法使得仅通过低秩适配（LoRA）即可实现有效微调，无需架构改造或昂贵的共同训练，并显著保持了原VLM的核心能力。\n### 论文核心贡献点\n1. **动作语言化表示**：提出将低级机器人动作转换为自然语言描述，直接利用VLM预训练的知识空间，避免分布偏移。\n2. **数据标注与训练管道**：提供了一套可扩展的机器人演示数据重标注流程，将轨迹分解为子任务、运动规划和动作块三层结构。\n3. **LoRA微调验证**：证明在动作语言化的前提下，LoRA可以有效进行VLA训练并维持VLM知识。\n4. **性能验证**：通过大量VQA实验和800+真实机器人实验，证明了方法在保持推理能力和零样本泛化方面的显著优势。\n### 论文方法描述\n1. **三层行动表示 (Actions as Language)**：\n - **高层子任务预测**：根据视觉观察和指令预测下一步子任务。\n - **中层运动规划**：基于当前子任务和观察，生成动作的空间方向描述（例如“向左并稍向下”）。\n - **低层动作生成**：生成最终执行的文本化动作指令序列（例如“向前移动4.2厘米”），包含各个自由度的具体指令。\n2. **数据重构**：\n - 使用Gemini模型自动标注机器人轨迹（BridgeData v2），将原始状态-动作序列转换为带有语言描述的序列：`\\{(图像, 子任务, 运动计划, 文本化动作块)\\}`。\n3. **训练与推理**：\n - **训练**：在重构后的语言化数据上，使用LoRA对Gemma-3-12B-IT等VLM进行微调。\n - **推理**：采用闭环方式，模型首先生成完整的子任务序列；在执行每个动作块后，调用一个验证器（使用Gemini 2.5 Pro）判断任务完成度，决定继续下一子任务或重试当前任务。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - **机器人数据**：BridgeData v2 数据集的一个子集（包含主要任务指令）。\n - **多模态数据**：用于评估VQA能力（未明确说明具体数据集，可能基于其预训练知识）。\n2. **训练资源**：\n - **基础模型**：Gemma-3-12B-IT (主要), Gemma-3-4B-IT (用于对比)。\n - **标注工具**：Gemini 2.5 Pro 和 Gemini 2.5 Flash（用于成本控制）用于自动生成语言化轨迹标签。\n - **微调方法**：LoRA（应用于所有线性模块）。\n - **标注成本**：约$900。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - **机器人平台**：WidowX 250S 6自由度机械臂（用于真实世界操作）。\n - **物理场景**：标准玩具厨房环境。\n2. **评估指标**：\n - **多模态理解能力**：\n - **指标**：多个VQA基准测试的准确率（如MMMU, MMStar, MME, OCRBench, MMB-en, MMB-cn, TextVQA, DocVQA, InfoVQA, AI2D, ChartQA, RealWorldQA）。\n - **机器人操作能力**：\n - **指标**：任务成功率（每任务30次试验，多语言任务为90次）。\n - **任务类型**：\n - **域内任务 (ID)**：拾取、拾取放置。\n - **组合任务**：多步骤操作。\n - **域外任务 (OOD)**：多语言指令翻译（西班牙语、普通话、印地语）；识别流行文化概念（识别“Ash Ketchum”图像上方的物体）。",
    "summary_html": "<h2 class=\"section-title\">论文总结</h2>\n<h3>论文研究单位</h3>\n<p>普林斯顿大学（Department of Mechanical and Aerospace Engineering, Department of Computer Science）</p>\n<h3>论文概述</h3>\n<p>本文针对将视觉语言模型（VLM）微调为视觉语言行动模型（VLA）时存在的核心挑战——灾难性遗忘进行了深入研究。传统方法在获得机器人操作能力的同时会严重削弱VLM的基础推理和多模态理解能力。本文提出了一种数据驱动的解决方案——<strong>VLM2VLA</strong>，其核心思想是将机器人动作转化为自然语言描述（Actions as Language），从而最小化微调数据与预训练数据之间的分布差异。该方法使得仅通过低秩适配（LoRA）即可实现有效微调，无需架构改造或昂贵的共同训练，并显著保持了原VLM的核心能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>动作语言化表示</strong>：提出将低级机器人动作转换为自然语言描述，直接利用VLM预训练的知识空间，避免分布偏移。</li><li><strong>数据标注与训练管道</strong>：提供了一套可扩展的机器人演示数据重标注流程，将轨迹分解为子任务、运动规划和动作块三层结构。</li><li><strong>LoRA微调验证</strong>：证明在动作语言化的前提下，LoRA可以有效进行VLA训练并维持VLM知识。</li><li><strong>性能验证</strong>：通过大量VQA实验和800+真实机器人实验，证明了方法在保持推理能力和零样本泛化方面的显著优势。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>三层行动表示 (Actions as Language)</strong>：</li></ol>\n<p> - <strong>高层子任务预测</strong>：根据视觉观察和指令预测下一步子任务。</p>\n<p> - <strong>中层运动规划</strong>：基于当前子任务和观察，生成动作的空间方向描述（例如“向左并稍向下”）。</p>\n<p> - <strong>低层动作生成</strong>：生成最终执行的文本化动作指令序列（例如“向前移动4.2厘米”），包含各个自由度的具体指令。</p>\n<ol><li><strong>数据重构</strong>：</li></ol>\n<p> - 使用Gemini模型自动标注机器人轨迹（BridgeData v2），将原始状态-动作序列转换为带有语言描述的序列：<code>\\{(图像, 子任务, 运动计划, 文本化动作块)\\}</code>。</p>\n<ol><li><strong>训练与推理</strong>：</li></ol>\n<p> - <strong>训练</strong>：在重构后的语言化数据上，使用LoRA对Gemma-3-12B-IT等VLM进行微调。</p>\n<p> - <strong>推理</strong>：采用闭环方式，模型首先生成完整的子任务序列；在执行每个动作块后，调用一个验证器（使用Gemini 2.5 Pro）判断任务完成度，决定继续下一子任务或重试当前任务。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - <strong>机器人数据</strong>：BridgeData v2 数据集的一个子集（包含主要任务指令）。</p>\n<p> - <strong>多模态数据</strong>：用于评估VQA能力（未明确说明具体数据集，可能基于其预训练知识）。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> - <strong>基础模型</strong>：Gemma-3-12B-IT (主要), Gemma-3-4B-IT (用于对比)。</p>\n<p> - <strong>标注工具</strong>：Gemini 2.5 Pro 和 Gemini 2.5 Flash（用于成本控制）用于自动生成语言化轨迹标签。</p>\n<p> - <strong>微调方法</strong>：LoRA（应用于所有线性模块）。</p>\n<p> - <strong>标注成本</strong>：约$900。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - <strong>机器人平台</strong>：WidowX 250S 6自由度机械臂（用于真实世界操作）。</p>\n<p> - <strong>物理场景</strong>：标准玩具厨房环境。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>多模态理解能力</strong>：</p>\n<p> - <strong>指标</strong>：多个VQA基准测试的准确率（如MMMU, MMStar, MME, OCRBench, MMB-en, MMB-cn, TextVQA, DocVQA, InfoVQA, AI2D, ChartQA, RealWorldQA）。</p>\n<p> - <strong>机器人操作能力</strong>：</p>\n<p> - <strong>指标</strong>：任务成功率（每任务30次试验，多语言任务为90次）。</p>\n<p> - <strong>任务类型</strong>：</p>\n<p> - <strong>域内任务 (ID)</strong>：拾取、拾取放置。</p>\n<p> - <strong>组合任务</strong>：多步骤操作。</p>\n<p> - <strong>域外任务 (OOD)</strong>：多语言指令翻译（西班牙语、普通话、印地语）；识别流行文化概念（识别“Ash Ketchum”图像上方的物体）。</p>"
  },
  {
    "date": "2025-09-26",
    "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation",
    "link": "http://arxiv.org/abs/2509.22093",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-26",
    "title": "Developing Vision-Language-Action Model from Egocentric Videos",
    "link": "http://arxiv.org/abs/2509.21986",
    "summary_markdown": "## 论文研究单位\n京都大学、国立情报学研究所（NII）、东京科学研究所、NII LLMC（东京）、索尼互动娱乐（东京）\n作者：Tomoya Yoshida（京都大学）、Shuhei Kurita（NII、东京科学研究所）、Taichi Nishimura（索尼互动娱乐）、Shinsuke Mori（京都大学）\n## 论文概述\n论文探讨了利用自我中心视频（egocentric videos）训练视觉-语言-动作模型（Vision-Language-Action models, VLAs）的可行性。传统方法依赖昂贵的人类远程操作数据，导致数据稀缺。论文提出通过EgoScaler框架从自我中心视频中自动提取6DoF对象操作轨迹，构建大规模预训练数据集，并在π0架构上验证其有效性。实验在仿真和真实环境中进行，结果显示该方法可显著提升任务成功率。\n## 论文核心贡献点\n1. 成功从自我中心视频训练π0模型，无需辅助标签（如手部姿态）。\n2. 构建的预训练数据集在性能上与真实机器人数据集竞争。\n3. 结合自我中心数据和真实机器人数据可进一步提升性能。\n## 论文方法描述\n- **EgoScaler框架**：用于从自我中心视频中提取6DoF对象操作轨迹。\n - 步骤：识别操作起止时间及对象；开词汇分割和3D点跟踪提取位置序列；点云配准投影到相机坐标系；奇异值分解计算旋转序列。\n- **数据处理**：应用于Ego4D、Ego-Exo4D、HD-EPIC和Nymeria四个数据集。初始提取124,559 episodes，经规则过滤（移动距离阈值、背景轨迹相似性阈值）和平滑处理后，得到45,157 episodes。\n- **训练策略**：使用π0架构预训练和后训练。动作表示为6DoF位移（平移+旋转），预训练优化MSE损失。后训练时合并真实机器人数据集并归一化动作维度。\n## 论文使用数据集和训练资源\n- **数据集**：\n - 自我中心视频数据集：Ego4D、Ego-Exo4D、HD-EPIC、Nymeria（用于预训练）。\n - 真实机器人数据集：BC-Z、BridgeData V2、Fractal（用于对比）。\n- **训练资源**：\n - 硬件：8×H200 GPUs。\n - 优化器：AdamW（学习率5×10^-5）。\n - 预训练步数：20,000步；后训练步数：40,000步。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真环境：SIMPLER（基于BridgeData V2的pick-and-place任务）。\n - 真实环境：ALOHA（语言引导的pick-and-place任务）。\n- **评估指标**：\n - 成功率：成功次数/总次数。仿真环境200次rollout；真实环境10次rollout。\n - 真实环境评分：抓取正确对象得0.5分，正确放置得0.5分。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>京都大学、国立情报学研究所（NII）、东京科学研究所、NII LLMC（东京）、索尼互动娱乐（东京）</p>\n<p>作者：Tomoya Yoshida（京都大学）、Shuhei Kurita（NII、东京科学研究所）、Taichi Nishimura（索尼互动娱乐）、Shinsuke Mori（京都大学）</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文探讨了利用自我中心视频（egocentric videos）训练视觉-语言-动作模型（Vision-Language-Action models, VLAs）的可行性。传统方法依赖昂贵的人类远程操作数据，导致数据稀缺。论文提出通过EgoScaler框架从自我中心视频中自动提取6DoF对象操作轨迹，构建大规模预训练数据集，并在π0架构上验证其有效性。实验在仿真和真实环境中进行，结果显示该方法可显著提升任务成功率。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>成功从自我中心视频训练π0模型，无需辅助标签（如手部姿态）。</li><li>构建的预训练数据集在性能上与真实机器人数据集竞争。</li><li>结合自我中心数据和真实机器人数据可进一步提升性能。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li><strong>EgoScaler框架</strong>：用于从自我中心视频中提取6DoF对象操作轨迹。</li></ul>\n<p> - 步骤：识别操作起止时间及对象；开词汇分割和3D点跟踪提取位置序列；点云配准投影到相机坐标系；奇异值分解计算旋转序列。</p>\n<ul><li><strong>数据处理</strong>：应用于Ego4D、Ego-Exo4D、HD-EPIC和Nymeria四个数据集。初始提取124,559 episodes，经规则过滤（移动距离阈值、背景轨迹相似性阈值）和平滑处理后，得到45,157 episodes。</li><li><strong>训练策略</strong>：使用π0架构预训练和后训练。动作表示为6DoF位移（平移+旋转），预训练优化MSE损失。后训练时合并真实机器人数据集并归一化动作维度。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 自我中心视频数据集：Ego4D、Ego-Exo4D、HD-EPIC、Nymeria（用于预训练）。</p>\n<p> - 真实机器人数据集：BC-Z、BridgeData V2、Fractal（用于对比）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 硬件：8×H200 GPUs。</p>\n<p> - 优化器：AdamW（学习率5×10^-5）。</p>\n<p> - 预训练步数：20,000步；后训练步数：40,000步。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真环境：SIMPLER（基于BridgeData V2的pick-and-place任务）。</p>\n<p> - 真实环境：ALOHA（语言引导的pick-and-place任务）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率：成功次数/总次数。仿真环境200次rollout；真实环境10次rollout。</p>\n<p> - 真实环境评分：抓取正确对象得0.5分，正确放置得0.5分。</p>"
  },
  {
    "date": "2025-09-25",
    "title": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2509.21243",
    "summary_markdown": "## 论文研究单位\n- 加川大学（School of Computing, Gachon University）\n## 论文概述\n- 针对VLA模型效率与性能的权衡难题，RetoVLA提出“复用Register Tokens”这一被ViT用于去伪影的中间信息，作为“空间上下文”直接注入Action Expert，在维持轻量结构的同时增强空间推理能力。\n- 基于LIBERO基准、自建仿真环境以及自建7-DOF机械臂进行评估，实机与仿真均显示在长时序与三维空间理解任务上显著提升。\n## 论文核心贡献点\n- 重新定义Register Tokens：从“去伪影的净化器”转为“空间上下文提供者”，设计可学习门控将其KV注入Action Expert的最后一层交叉注意力，实现语义与全局空间双流融合。\n- 证明复用信息可弥补轻量模型（如SmolVLA）深度压缩导致的全局理解能力损失，是替代“信息缩减”的新范式。\n- 三类实验验证：标准化仿真（LIBERO）、自建物理仿真（Unity+MuJoCo）、真实7-DOF机械臂上取得大幅提升（尤其长时序与3D空间任务）。\n## 论文方法描述\n- 架构\n - 采用浅层VLM主干（SmolVLM2-500M的前16层）以保证效率；维持单流语义特征与双流信息融合路径。\n- 空间上下文注入（核心）\n - Register Token生成：用可学习初始向量与图像patch做注意力，生成场景依赖的Register Tokens。\n - 注入与融合：将Register Tokens投影为KV并与VLM的KV在Action Expert最后一层交叉注意力处拼接。\n - 门控：引入可学习标量经sigmoid调制注入强度，避免对极精细局部控制造成干扰。\n- 训练目标\n - 条件流匹配（CFM）：学习将噪声动作序列朝真实动作序列演化的向量场，MSE损失优化。\n## 论文使用数据集和训练资源\n- 数据与任务\n - LIBERO四类基准：Spatial、Object、Goal、10(Long)。\n - 真实环境：自建7-DOF机械臂，7项操控任务，收集1,804条episode进行微调。\n - 自建仿真：Unity + MuJoCo插件，对应部分真实任务。\n- 训练资源（报告为主干与步骤细节）\n - 主干：SmolVLM2-500M（使用前16层）。\n - 训练步数：100k；批量：64。\n - 注入寄存器：2个Register Tokens（消融最优）。\n## 论文使用的评估环境和评估指标\n- 评估环境\n - 仿真：LIBERO基准与自定义Unity+MuJoCo仿真。\n - 真实：7-DOF机械臂，多视角（顶视/侧视/腕部）。\n- 评估指标\n - 主要指标：成功率（Success Rate, SR）、平均成功率（MSR）。\n - 性能对比：SmolVLA vs RetoVLA。\n- 关键结果摘要\n - 真实机器人MSR：50.28% → 67.42%，绝对提升17.14%p。\n - 仿真MSR：62.8% → 74.8%，提升12.0%p。\n - 长时序与复杂空间任务提升显著（如Build Domino Line、Close Drawer、Jenga等）；对极精细局部操作有轻微权衡。\n - LIBERO各类总体提升有限，但针对工作记忆与三维空间理解的任务明显受益。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>加川大学（School of Computing, Gachon University）</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>针对VLA模型效率与性能的权衡难题，RetoVLA提出“复用Register Tokens”这一被ViT用于去伪影的中间信息，作为“空间上下文”直接注入Action Expert，在维持轻量结构的同时增强空间推理能力。</li><li>基于LIBERO基准、自建仿真环境以及自建7-DOF机械臂进行评估，实机与仿真均显示在长时序与三维空间理解任务上显著提升。</li></ul>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>重新定义Register Tokens：从“去伪影的净化器”转为“空间上下文提供者”，设计可学习门控将其KV注入Action Expert的最后一层交叉注意力，实现语义与全局空间双流融合。</li><li>证明复用信息可弥补轻量模型（如SmolVLA）深度压缩导致的全局理解能力损失，是替代“信息缩减”的新范式。</li><li>三类实验验证：标准化仿真（LIBERO）、自建物理仿真（Unity+MuJoCo）、真实7-DOF机械臂上取得大幅提升（尤其长时序与3D空间任务）。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>架构</li></ul>\n<p> - 采用浅层VLM主干（SmolVLM2-500M的前16层）以保证效率；维持单流语义特征与双流信息融合路径。</p>\n<ul><li>空间上下文注入（核心）</li></ul>\n<p> - Register Token生成：用可学习初始向量与图像patch做注意力，生成场景依赖的Register Tokens。</p>\n<p> - 注入与融合：将Register Tokens投影为KV并与VLM的KV在Action Expert最后一层交叉注意力处拼接。</p>\n<p> - 门控：引入可学习标量经sigmoid调制注入强度，避免对极精细局部控制造成干扰。</p>\n<ul><li>训练目标</li></ul>\n<p> - 条件流匹配（CFM）：学习将噪声动作序列朝真实动作序列演化的向量场，MSE损失优化。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据与任务</li></ul>\n<p> - LIBERO四类基准：Spatial、Object、Goal、10(Long)。</p>\n<p> - 真实环境：自建7-DOF机械臂，7项操控任务，收集1,804条episode进行微调。</p>\n<p> - 自建仿真：Unity + MuJoCo插件，对应部分真实任务。</p>\n<ul><li>训练资源（报告为主干与步骤细节）</li></ul>\n<p> - 主干：SmolVLM2-500M（使用前16层）。</p>\n<p> - 训练步数：100k；批量：64。</p>\n<p> - 注入寄存器：2个Register Tokens（消融最优）。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境</li></ul>\n<p> - 仿真：LIBERO基准与自定义Unity+MuJoCo仿真。</p>\n<p> - 真实：7-DOF机械臂，多视角（顶视/侧视/腕部）。</p>\n<ul><li>评估指标</li></ul>\n<p> - 主要指标：成功率（Success Rate, SR）、平均成功率（MSR）。</p>\n<p> - 性能对比：SmolVLA vs RetoVLA。</p>\n<ul><li>关键结果摘要</li></ul>\n<p> - 真实机器人MSR：50.28% → 67.42%，绝对提升17.14%p。</p>\n<p> - 仿真MSR：62.8% → 74.8%，提升12.0%p。</p>\n<p> - 长时序与复杂空间任务提升显著（如Build Domino Line、Close Drawer、Jenga等）；对极精细局部操作有轻微权衡。</p>\n<p> - LIBERO各类总体提升有限，但针对工作记忆与三维空间理解的任务明显受益。</p>"
  },
  {
    "date": "2025-09-25",
    "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning",
    "link": "http://arxiv.org/abs/2509.21126",
    "summary_markdown": "## 论文研究单位\n武汉大学，中国。\n## 论文概述\n本文提出VARL框架，利用视觉语言模型(VLM)作为动作顾问，在在线强化学习(RL)中提供动作建议，旨在改进样本效率，特别是在稀疏奖励任务中。方法通过集成VLM建议的动作，而非修改奖励函数，丰富样本多样性并确保策略最优性和收敛性。论文在多种环境和代理设置中验证了框架的有效性。\n## 论文核心贡献点\n- 提出VLM作为动作顾问的框架，而非奖励设计师，保证策略最优性。\n- 设计门控机制（离散和连续动作空间）防止策略过度拟合VLM动作。\n- 通过策略塑形在早期训练阶段提供指导，提高样本效率。\n- 低计算开销：相比奖励塑形方法，显著减少VLM查询次数（如VARL仅3次查询对比奖励塑形方法的5000次）。\n- 适用于状态和视觉基础任务、离散和连续动作空间。\n## 论文方法描述\nVARL框架包含两个组件：\n- VLM动作生成器：定期采样最新状态-动作对，查询VLM生成启发式动作并存储至引导缓冲区。\n- 策略塑形模块：将VLM动作集成至策略训练，通过行为克隆损失和门控函数塑形策略，在指定步数后移除启发式动作。\n具体算法基于软演员-评论家(SAC)，使用GPT-5作为VLM，损失函数包括基线策略损失、行为克隆损失和门控函数。\n## 论文使用数据集和训练资源\n- 环境：Meta-World（操控）、AI2-THOR（导航）、真实世界（Realman Robotics RM-65B臂）。\n- 任务：10个任务，包括状态基础（Drawer Open、Sweep Into、Soccer）、视觉基础（Pick Up Plate、Open Phone、Toggle Off Lamp、Drawer Open、Push Cube）、真实世界视觉基础（Target Reach、Push Cube）。\n- 训练资源：最大步数500,000，参数λ=10、移除步数N_s=30,000，使用SAC求解器和GPT-5 VLM。\n## 论文使用的评估环境和评估指标\n- 评估环境：10个任务，分为三类（状态基础操控、视觉基础操控和导航、真实世界视觉基础操控）。\n- 评估指标：成功率、学习曲线（平均回报或成功率），与基线方法（SAC、SAC+专家数据、RL-VLM-F、ERL-VLM）比较，包括样本效率分析和消融研究。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>武汉大学，中国。</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文提出VARL框架，利用视觉语言模型(VLM)作为动作顾问，在在线强化学习(RL)中提供动作建议，旨在改进样本效率，特别是在稀疏奖励任务中。方法通过集成VLM建议的动作，而非修改奖励函数，丰富样本多样性并确保策略最优性和收敛性。论文在多种环境和代理设置中验证了框架的有效性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出VLM作为动作顾问的框架，而非奖励设计师，保证策略最优性。</li><li>设计门控机制（离散和连续动作空间）防止策略过度拟合VLM动作。</li><li>通过策略塑形在早期训练阶段提供指导，提高样本效率。</li><li>低计算开销：相比奖励塑形方法，显著减少VLM查询次数（如VARL仅3次查询对比奖励塑形方法的5000次）。</li><li>适用于状态和视觉基础任务、离散和连续动作空间。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>VARL框架包含两个组件：</p>\n<ul><li>VLM动作生成器：定期采样最新状态-动作对，查询VLM生成启发式动作并存储至引导缓冲区。</li><li>策略塑形模块：将VLM动作集成至策略训练，通过行为克隆损失和门控函数塑形策略，在指定步数后移除启发式动作。</li></ul>\n<p>具体算法基于软演员-评论家(SAC)，使用GPT-5作为VLM，损失函数包括基线策略损失、行为克隆损失和门控函数。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>环境：Meta-World（操控）、AI2-THOR（导航）、真实世界（Realman Robotics RM-65B臂）。</li><li>任务：10个任务，包括状态基础（Drawer Open、Sweep Into、Soccer）、视觉基础（Pick Up Plate、Open Phone、Toggle Off Lamp、Drawer Open、Push Cube）、真实世界视觉基础（Target Reach、Push Cube）。</li><li>训练资源：最大步数500,000，参数λ=10、移除步数N_s=30,000，使用SAC求解器和GPT-5 VLM。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：10个任务，分为三类（状态基础操控、视觉基础操控和导航、真实世界视觉基础操控）。</li><li>评估指标：成功率、学习曲线（平均回报或成功率），与基线方法（SAC、SAC+专家数据、RL-VLM-F、ERL-VLM）比较，包括样本效率分析和消融研究。</li></ul>"
  },
  {
    "date": "2025-09-25",
    "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
    "link": "http://arxiv.org/abs/2509.21006",
    "summary_markdown": "### 论文研究单位\nSkolkovo Institute of Science and Technology（莫斯科斯科尔科沃理工学院）智能空间机器人实验室。\n### 论文概述\nAnywhereVLA是一个模块化框架，用于在未见过的大型室内环境中实现自然语言驱动的抓取-放置任务。系统将语言指令转换为结构化任务图，结合LiDAR-SLAM、度量语义映射和主动探索策略，并在接近模块引导下由细调的SmolVLA模型执行操作。在消费级硬件（Jetson Orin NX和Intel NUC）上实时运行（≥10Hz），在实验室环境（静态场景和人类活动）中实现46%整体成功率，结合经典导航与VLA操作确保鲁棒性与灵活性。\n### 论文核心贡献点\n- **统一模块化框架**：单一语言指令同时驱动环境探索和VLA操作。\n- **实时性能**：嵌入式设备上实现≥10Hz的全模块运行。\n- **环境适应能力**：支持未见过的动态场景，探索并定位目标对象。\n- **方法融合**：结合经典SLAM导航与轻量级VLA（SmolVLA 450M）模型，提升操作可靠性与泛化能力。\n### 论文方法描述\n- **架构模块**：\n - **3D语义映射（SM）**：LiDAR-惯性-视觉SLAM构建语义点云，通过LiDAR密集化（插值环间点）、对象聚合（DBSCAN聚类）和置信度估计生成目标地图。\n - **主动环境探索（AEE）**：前沿驱动的探索策略，条件化目标类，优化视野覆盖并抑制虚假目标。\n - **接近模块（Approach）**：基于语义地图计算安全接近姿势，验证无碰撞导航。\n - **VLA操作**：细调SmolVLA模型生成操作动作，集成多视角相机输入。\n- **工作流**：解析语言指令→触发SM和AEE→SM构建地图→AEE定位目标→Approach导航→VLA执行操作。\n- **硬件部署**：Jetson Orin NX处理感知和VLA，Intel NUC处理SLAM和导航。\n### 论文使用数据集和训练资源\n- **数据集**：SO-101操作器抓取-放置轨迹50条（遥操作采集）。\n- **训练资源**：RTX 4090 GPU（16GB VRAM）细调SmolVLA（批量16，学习率0.0001，余弦退火，AdamW优化，梯度裁剪10.0）。\n- **部署硬件**：Jetson Orin NX（16GB）和Intel NUC（Core i7, 32GB）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：未见过的大型室内多房间实验室（动态场景，正常人类活动）。\n- **任务**：50次抓取-放置试验（目标对象随机分布在半径内）。\n- **指令模板**：自然语言指令，如“Pick up the <object> and place it in the <area>. And bring the <object> to <location>.”\n- **评估指标**：\n - 成功率（SR）：整体SR 46%，VLA操作模块SR 85%（细调后）。\n - 模块性能：SLAM SR 100%，AEE SR 75%，导航SR 90%，对象检测SR 85%，VLA操作SR 80%。\n - 实时性能：总任务时间（探索半径5m时平均133秒，10m时<10分钟）。\n - 计算效率：模块频率≥10Hz（如表I所示）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Skolkovo Institute of Science and Technology（莫斯科斯科尔科沃理工学院）智能空间机器人实验室。</p>\n<h3>论文概述</h3>\n<p>AnywhereVLA是一个模块化框架，用于在未见过的大型室内环境中实现自然语言驱动的抓取-放置任务。系统将语言指令转换为结构化任务图，结合LiDAR-SLAM、度量语义映射和主动探索策略，并在接近模块引导下由细调的SmolVLA模型执行操作。在消费级硬件（Jetson Orin NX和Intel NUC）上实时运行（≥10Hz），在实验室环境（静态场景和人类活动）中实现46%整体成功率，结合经典导航与VLA操作确保鲁棒性与灵活性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li><strong>统一模块化框架</strong>：单一语言指令同时驱动环境探索和VLA操作。</li><li><strong>实时性能</strong>：嵌入式设备上实现≥10Hz的全模块运行。</li><li><strong>环境适应能力</strong>：支持未见过的动态场景，探索并定位目标对象。</li><li><strong>方法融合</strong>：结合经典SLAM导航与轻量级VLA（SmolVLA 450M）模型，提升操作可靠性与泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>架构模块</strong>：</li></ul>\n<p> - <strong>3D语义映射（SM）</strong>：LiDAR-惯性-视觉SLAM构建语义点云，通过LiDAR密集化（插值环间点）、对象聚合（DBSCAN聚类）和置信度估计生成目标地图。</p>\n<p> - <strong>主动环境探索（AEE）</strong>：前沿驱动的探索策略，条件化目标类，优化视野覆盖并抑制虚假目标。</p>\n<p> - <strong>接近模块（Approach）</strong>：基于语义地图计算安全接近姿势，验证无碰撞导航。</p>\n<p> - <strong>VLA操作</strong>：细调SmolVLA模型生成操作动作，集成多视角相机输入。</p>\n<ul><li><strong>工作流</strong>：解析语言指令→触发SM和AEE→SM构建地图→AEE定位目标→Approach导航→VLA执行操作。</li><li><strong>硬件部署</strong>：Jetson Orin NX处理感知和VLA，Intel NUC处理SLAM和导航。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：SO-101操作器抓取-放置轨迹50条（遥操作采集）。</li><li><strong>训练资源</strong>：RTX 4090 GPU（16GB VRAM）细调SmolVLA（批量16，学习率0.0001，余弦退火，AdamW优化，梯度裁剪10.0）。</li><li><strong>部署硬件</strong>：Jetson Orin NX（16GB）和Intel NUC（Core i7, 32GB）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：未见过的大型室内多房间实验室（动态场景，正常人类活动）。</li><li><strong>任务</strong>：50次抓取-放置试验（目标对象随机分布在半径内）。</li><li><strong>指令模板</strong>：自然语言指令，如“Pick up the <object> and place it in the <area>. And bring the <object> to <location>.”</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（SR）：整体SR 46%，VLA操作模块SR 85%（细调后）。</p>\n<p> - 模块性能：SLAM SR 100%，AEE SR 75%，导航SR 90%，对象检测SR 85%，VLA操作SR 80%。</p>\n<p> - 实时性能：总任务时间（探索半径5m时平均133秒，10m时<10分钟）。</p>\n<p> - 计算效率：模块频率≥10Hz（如表I所示）。</p>"
  },
  {
    "date": "2025-09-25",
    "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2509.20841",
    "summary_markdown": "### 论文研究单位\n德坤卢、高伟和贾奎（Dekun Lu, Wei Gao, and Kui Jia）- 作者信息未明确标注所属机构\n### 论文概述\n论文提出了一种名为ImaginationPolicy的端到端机器人操作策略，旨在解决传统模块化管道信息丢失和特征对齐问题。该方法通过**可操作性导向的关键点**实现对多样化操作任务的统一表示，显著提升机器人在抓取、倒水等任务中的泛化能力和精度。研究采用Chain of Moving Oriented Keypoints (CoMOK)作为动作表示，将操作行为转化为关键点序列，使网络能在不同形状/尺寸物体间实现**亚厘米级精度**。\n### 论文核心贡献点\n1. **创新动作表示**：提出CoMOK（移动方向关键点链）作为统一操作表示，突破末端执行器姿态表示限制\n2. **任务泛化能力**：基于可操作性关键点自然适配不同物体形状/尺寸，包括可变形物体\n3. **多任务统一框架**：单神经网络同时处理抓取、倒水、稳定放置等任务\n4. **端到端训练**：融合Groma视觉语言模型进行任务规划，Score-Matching网络生成动作分布\n5. **复杂场景适配**：支持多阶段操作、多模态行为及轨迹动作生成\n### 论文方法描述\n**CoMOK核心公式**：\n```\n网络输出 (o_manipulated, T_affordance, T_action)\n```\n- **o_manipulated**: 机器人控制对象（工具/物体/局部区域）\n- **T_affordance**: 操作方向关键点（如杯子把手、缆绳抓取点）\n- **T_action**: 目标对齐姿态（若T_affordance与T_action对齐则任务完成）\n\n**扩展机制**：\n- **多阶段操作**：全局任务描述自动分解子任务（如倒水任务拆解为\"抓杯子→倒水→放置\"）\n- **多模态候选**：扩散模型生成动作分布（如多个抓取位姿可选）\n- **轨迹序列**：输出SE(3)位姿序列（如切割水果的上下两帧）\n\n**神经网络架构**：\n1. **任务规划层**：基于Groma VLM，从RGBD图像和全局指令生成子任务列表\n2. **动作预测层**：点云输入+阶段任务特征，通过Score-Matching生成关键点序列\n3. **轨迹生成层**：\n - 仿真：Motion Policy Networks学习生成关节空间轨迹\n - 实机：传统任务与运动规划算法筛选可行轨迹\n### 论文使用数据集和训练资源\n- **抓取检测**：复用现有点云数据集（SE(3)-DiffusionFields来源）\n- **仿真实验**：三类物体稳定放置任务\n - 20个瓶子、10个盒子、5个三脚架\n - 随机放置5个障碍物\n- **实机实验**：6种缆绳+3种挂钩的多样化操作\n- **硬件配置**：Rokae SR5六自由度机械臂 + 末端RGBD传感器\n### 论文使用的评估环境和评估指标\n**评估任务**：\n1. **抓取检测**：双帧输出（预抓取+抓取）\n2. **稳定放置**（仿真）：\n - 成功率：距离桌面≤1cm，Z轴偏差≤15°\n - 碰撞检测：放置后与障碍物碰撞\n3. **缆绳插入**（实机）：缆绳是否插入夹具\n4. **挂钩挂杯**（实机）：20种杯子×3种挂钩\n\n**评估指标**：\n- **精度**：位置误差（厘米级）、姿态误差（角度）\n- **泛化性**：跨物体形状/尺寸成功率\n- **可靠性**：任务级成功率（不含中间路径错误）\n- **动态适应**：可变形物体（缆绳）操作成功率\n\n**结论**：方法实现跨任务统一操作，成功率从稳定放置的仿真数据到实机缆绳插入均有验证，平均执行精度达厘米级且对物体变化鲁棒。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>德坤卢、高伟和贾奎（Dekun Lu, Wei Gao, and Kui Jia）- 作者信息未明确标注所属机构</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为ImaginationPolicy的端到端机器人操作策略，旨在解决传统模块化管道信息丢失和特征对齐问题。该方法通过<strong>可操作性导向的关键点</strong>实现对多样化操作任务的统一表示，显著提升机器人在抓取、倒水等任务中的泛化能力和精度。研究采用Chain of Moving Oriented Keypoints (CoMOK)作为动作表示，将操作行为转化为关键点序列，使网络能在不同形状/尺寸物体间实现<strong>亚厘米级精度</strong>。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>创新动作表示</strong>：提出CoMOK（移动方向关键点链）作为统一操作表示，突破末端执行器姿态表示限制</li><li><strong>任务泛化能力</strong>：基于可操作性关键点自然适配不同物体形状/尺寸，包括可变形物体</li><li><strong>多任务统一框架</strong>：单神经网络同时处理抓取、倒水、稳定放置等任务</li><li><strong>端到端训练</strong>：融合Groma视觉语言模型进行任务规划，Score-Matching网络生成动作分布</li><li><strong>复杂场景适配</strong>：支持多阶段操作、多模态行为及轨迹动作生成</li></ol>\n<h3>论文方法描述</h3>\n<p><strong>CoMOK核心公式</strong>：</p>\n<p>```</p>\n<p>网络输出 (o_manipulated, T_affordance, T_action)</p>\n<p>```</p>\n<ul><li><strong>o_manipulated</strong>: 机器人控制对象（工具/物体/局部区域）</li><li><strong>T_affordance</strong>: 操作方向关键点（如杯子把手、缆绳抓取点）</li><li><strong>T_action</strong>: 目标对齐姿态（若T_affordance与T_action对齐则任务完成）</li></ul>\n\n<p><strong>扩展机制</strong>：</p>\n<ul><li><strong>多阶段操作</strong>：全局任务描述自动分解子任务（如倒水任务拆解为\"抓杯子→倒水→放置\"）</li><li><strong>多模态候选</strong>：扩散模型生成动作分布（如多个抓取位姿可选）</li><li><strong>轨迹序列</strong>：输出SE(3)位姿序列（如切割水果的上下两帧）</li></ul>\n\n<p><strong>神经网络架构</strong>：</p>\n<ol><li><strong>任务规划层</strong>：基于Groma VLM，从RGBD图像和全局指令生成子任务列表</li><li><strong>动作预测层</strong>：点云输入+阶段任务特征，通过Score-Matching生成关键点序列</li><li><strong>轨迹生成层</strong>：</li></ol>\n<p> - 仿真：Motion Policy Networks学习生成关节空间轨迹</p>\n<p> - 实机：传统任务与运动规划算法筛选可行轨迹</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>抓取检测</strong>：复用现有点云数据集（SE(3)-DiffusionFields来源）</li><li><strong>仿真实验</strong>：三类物体稳定放置任务</li></ul>\n<p> - 20个瓶子、10个盒子、5个三脚架</p>\n<p> - 随机放置5个障碍物</p>\n<ul><li><strong>实机实验</strong>：6种缆绳+3种挂钩的多样化操作</li><li><strong>硬件配置</strong>：Rokae SR5六自由度机械臂 + 末端RGBD传感器</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估任务</strong>：</p>\n<ol><li><strong>抓取检测</strong>：双帧输出（预抓取+抓取）</li><li><strong>稳定放置</strong>（仿真）：</li></ol>\n<p> - 成功率：距离桌面≤1cm，Z轴偏差≤15°</p>\n<p> - 碰撞检测：放置后与障碍物碰撞</p>\n<ol><li><strong>缆绳插入</strong>（实机）：缆绳是否插入夹具</li><li><strong>挂钩挂杯</strong>（实机）：20种杯子×3种挂钩</li></ol>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li><strong>精度</strong>：位置误差（厘米级）、姿态误差（角度）</li><li><strong>泛化性</strong>：跨物体形状/尺寸成功率</li><li><strong>可靠性</strong>：任务级成功率（不含中间路径错误）</li><li><strong>动态适应</strong>：可变形物体（缆绳）操作成功率</li></ul>\n\n<p><strong>结论</strong>：方法实现跨任务统一操作，成功率从稳定放置的仿真数据到实机缆绳插入均有验证，平均执行精度达厘米级且对物体变化鲁棒。</p>"
  },
  {
    "date": "2025-09-24",
    "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
    "link": "http://arxiv.org/abs/2509.20109",
    "summary_markdown": "## 论文研究单位\n- **LiAuto**（理想汽车）\n- **清华大学**（Tsinghua University）\n## 论文概述\n本文针对端到端自动驾驶系统中模仿学习无法内在编码物理规则（如碰撞避免、遵守可行驶区域）的核心挑战，提出了一种名为 **ReflectDrive** 的创新框架。该框架将离散扩散模型应用于轨迹规划，并引入**安全反射机制**（Safety-Guided Regeneration），在推理阶段通过迭代局部搜索和安全锚点重绘，无需梯度计算即可实现轨迹的安全自校正，从而提升规划的可控性和可靠性。\n## 论文核心贡献点\n1. **首次将离散扩散应用于端到端自动驾驶规划**：将连续轨迹空间离散化为动作码本，利用预训练的扩散语言模型（DLM）进行微调，实现并行解码和双向特征融合。\n2. **提出安全反射机制**：在推理阶段结合目标条件生成（Goal-Conditioned Generation）和安全导向再生，通过局部搜索识别不安全点并替换为安全锚点，再利用扩散模型的修复能力（inpainting）重新生成轨迹。\n3. **验证安全约束的有效性**：在真实场景基准测试中，该框架在确保硬安全约束的同时，显著提升了轨迹质量，且性能接近人类驾驶水平。\n## 论文方法描述\n1. **轨迹离散化（Trajectory Discretization）**：\n - 将二维驾驶空间中的连续坐标 `(x, y)` 分别映射到预定义的1D码本 \\(\\mathcal{A}\\) 中，生成轨迹的离散化序列 \\(\\mathbf{y}\\)。\n - 使用均匀网格量化坐标范围 \\([-M, M]\\)，分辨率 \\(\\Delta_g\\) 可控，确保离散化后的轨迹可通过逆量化恢复为连续坐标。\n\n2. **离散扩散模型（Discrete Diffusion Model）**：\n - 基于离散扩散框架（掩码-去噪），采用预训练的VLA模型（如LLaDA-V）作为主干网络。\n - 通过监督微调训练模型，使其具备根据场景上下文生成离散轨迹序列的能力。\n\n3. **安全反射推理（Reflective Inference）**：\n - **目标条件生成**：对终点位置进行概率采样，应用非极大值抑制（NMS）获取空间多样化的目标点，生成多模态轨迹候选。\n - **安全导向再生**：\n - **全局评分器**：评估完整轨迹的安全性和质量。\n - **安全评分器**：定位不安全路标点。\n - **局部搜索**：在路标点附近（如曼哈顿距离 \\(\\delta \\leq 10\\)）搜索可行替代点作为“安全锚点”。\n - **轨迹修复**：固定安全锚点，通过扩散模型重绘周围轨迹段。\n## 论文使用数据集和训练资源\n- **数据集**：基于大规模真实世界自动驾驶基准 **NAVSIM** 的数据。\n- **训练资源**：\n - 输入：前、左前、右前摄像头图像及语言指令（如“左转”）。\n - 模型初始化：预训练的扩散语言模型（如LLaDA-V）。\n - 训练方法：监督微调（SFT），批量大小为16，学习率 \\(1 \\times 10^{-5}\\)，训练3轮。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - **NAVSIM 基准**：使用官方闭环仿真器进行评估。\n- **评估指标**：\n - **PDMS 分数**（主要指标）：聚合5项指标（分数越高越好）：\n - **NC**（No-Collision）：无碰撞率（防止事故）。\n - **DAC**（Drivable Area Compliance）：可行驶区域合规性（遵守道路边界）。\n - **TTC**（Time-to-Collision）：碰撞时间安全性（避让反应时间）。\n - **Comfort**：舒适性（加速度/急动度受控）。\n - **EP**（Ego Progress）：ego进展（路径完成度）。\n\n**结果摘要**（以相机输入为例）：\n\\|方法 \\|DAC ↑ \\|TTC ↑ \\|NC ↑ \\|EP ↑ \\|PDMS ↑ \\|\n\\|\n---\n--\n---\n--\n---\n--\n---\n-\\|\n---\n----\\|\n---\n----\\|\n---\n---\\|\n---\n---\\|\n---\n--\n---\n\\|\n\\|ReflectDrive \\|**99.3%** \\|93.5% \\|97.7% \\|**86.9%** \\|**91.1** \\|\n\\|ReflectDrive（无反射） \\|95.4% \\|92.2% \\|96.9% \\|79.0% \\|84.8 \\|\n\\|**人类表现** \\|100.0% \\|100.0% \\|100.0% \\|87.5% \\|94.8 \\|\n\n> **结论**：ReflectDrive 的安全反射机制显著提升轨迹安全性（如 DAC 提升 +3.9）同时改善路径完成度（EP 提升 +7.9），接近人类驾驶水平。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li><strong>LiAuto</strong>（理想汽车）</li><li><strong>清华大学</strong>（Tsinghua University）</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文针对端到端自动驾驶系统中模仿学习无法内在编码物理规则（如碰撞避免、遵守可行驶区域）的核心挑战，提出了一种名为 <strong>ReflectDrive</strong> 的创新框架。该框架将离散扩散模型应用于轨迹规划，并引入<strong>安全反射机制</strong>（Safety-Guided Regeneration），在推理阶段通过迭代局部搜索和安全锚点重绘，无需梯度计算即可实现轨迹的安全自校正，从而提升规划的可控性和可靠性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>首次将离散扩散应用于端到端自动驾驶规划</strong>：将连续轨迹空间离散化为动作码本，利用预训练的扩散语言模型（DLM）进行微调，实现并行解码和双向特征融合。</li><li><strong>提出安全反射机制</strong>：在推理阶段结合目标条件生成（Goal-Conditioned Generation）和安全导向再生，通过局部搜索识别不安全点并替换为安全锚点，再利用扩散模型的修复能力（inpainting）重新生成轨迹。</li><li><strong>验证安全约束的有效性</strong>：在真实场景基准测试中，该框架在确保硬安全约束的同时，显著提升了轨迹质量，且性能接近人类驾驶水平。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ol><li><strong>轨迹离散化（Trajectory Discretization）</strong>：</li></ol>\n<p> - 将二维驾驶空间中的连续坐标 <code>(x, y)</code> 分别映射到预定义的1D码本 \\(\\mathcal{A}\\) 中，生成轨迹的离散化序列 \\(\\mathbf{y}\\)。</p>\n<p> - 使用均匀网格量化坐标范围 \\([-M, M]\\)，分辨率 \\(\\Delta_g\\) 可控，确保离散化后的轨迹可通过逆量化恢复为连续坐标。</p>\n\n<ol><li><strong>离散扩散模型（Discrete Diffusion Model）</strong>：</li></ol>\n<p> - 基于离散扩散框架（掩码-去噪），采用预训练的VLA模型（如LLaDA-V）作为主干网络。</p>\n<p> - 通过监督微调训练模型，使其具备根据场景上下文生成离散轨迹序列的能力。</p>\n\n<ol><li><strong>安全反射推理（Reflective Inference）</strong>：</li></ol>\n<p> - <strong>目标条件生成</strong>：对终点位置进行概率采样，应用非极大值抑制（NMS）获取空间多样化的目标点，生成多模态轨迹候选。</p>\n<p> - <strong>安全导向再生</strong>：</p>\n<p> - <strong>全局评分器</strong>：评估完整轨迹的安全性和质量。</p>\n<p> - <strong>安全评分器</strong>：定位不安全路标点。</p>\n<p> - <strong>局部搜索</strong>：在路标点附近（如曼哈顿距离 \\(\\delta \\leq 10\\)）搜索可行替代点作为“安全锚点”。</p>\n<p> - <strong>轨迹修复</strong>：固定安全锚点，通过扩散模型重绘周围轨迹段。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：基于大规模真实世界自动驾驶基准 <strong>NAVSIM</strong> 的数据。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 输入：前、左前、右前摄像头图像及语言指令（如“左转”）。</p>\n<p> - 模型初始化：预训练的扩散语言模型（如LLaDA-V）。</p>\n<p> - 训练方法：监督微调（SFT），批量大小为16，学习率 \\(1 \\times 10^{-5}\\)，训练3轮。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>NAVSIM 基准</strong>：使用官方闭环仿真器进行评估。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>PDMS 分数</strong>（主要指标）：聚合5项指标（分数越高越好）：</p>\n<p> - <strong>NC</strong>（No-Collision）：无碰撞率（防止事故）。</p>\n<p> - <strong>DAC</strong>（Drivable Area Compliance）：可行驶区域合规性（遵守道路边界）。</p>\n<p> - <strong>TTC</strong>（Time-to-Collision）：碰撞时间安全性（避让反应时间）。</p>\n<p> - <strong>Comfort</strong>：舒适性（加速度/急动度受控）。</p>\n<p> - <strong>EP</strong>（Ego Progress）：ego进展（路径完成度）。</p>\n\n<p><strong>结果摘要</strong>（以相机输入为例）：</p>\n<p>\\|方法 \\|DAC ↑ \\|TTC ↑ \\|NC ↑ \\|EP ↑ \\|PDMS ↑ \\|</p>\n<p>\\|</p>\n<hr/>\n<p>--</p>\n<hr/>\n<p>--</p>\n<hr/>\n<p>--</p>\n<hr/>\n<p>-\\|</p>\n<hr/>\n<p>----\\|</p>\n<hr/>\n<p>----\\|</p>\n<hr/>\n<p>---\\|</p>\n<hr/>\n<p>---\\|</p>\n<hr/>\n<p>--</p>\n<hr/>\n<p>\\|</p>\n<p>\\|ReflectDrive \\|<strong>99.3%</strong> \\|93.5% \\|97.7% \\|<strong>86.9%</strong> \\|<strong>91.1</strong> \\|</p>\n<p>\\|ReflectDrive（无反射） \\|95.4% \\|92.2% \\|96.9% \\|79.0% \\|84.8 \\|</p>\n<p>\\|<strong>人类表现</strong> \\|100.0% \\|100.0% \\|100.0% \\|87.5% \\|94.8 \\|</p>\n\n<p>> <strong>结论</strong>：ReflectDrive 的安全反射机制显著提升轨迹安全性（如 DAC 提升 +3.9）同时改善路径完成度（EP 提升 +7.9），接近人类驾驶水平。</p>"
  },
  {
    "date": "2025-09-24",
    "title": "FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2509.19870",
    "summary_markdown": "论文研究单位\n- 复旦大学\n- 上海人工智能实验室\n- Sea AI Lab\n\n论文概述\n- 针对视觉-语言-动作（VLA）模型提出并形式化“行动冻结”（action-freezing）攻击：对抗图像使机器人忽略后续指令，进入持续无响应状态，可能在关键时刻导致不作为，引发实际安全风险。\n- 提出FreezeVLA框架，通过最小-最大双层优化生成跨提示可迁移的对抗图像，在三个SOTA VLA模型与四个机器人基准上实现高攻击成功率，显著优于现有方法。\n\n论文核心贡献点\n- 识别并形式化VLA模型的“行动冻结”漏洞，揭示与错误动作同等严重的“不作为”威胁。\n- 提出FreezeVLA：使用内部最大化（对抗提示优化）与外部最小化（对抗图像优化）的双层优化方法，扩展提示嵌入空间覆盖，显著提升跨提示迁移能力。\n- 跨模型验证：SpatialVLA、OpenVLA、π0 上分别达到平均攻击成功率73.3%、95.4%、59.8%，全面超越随机噪声、PGD、多提示等基线。\n\n论文方法描述\n- 威胁模型与目标：白盒访问VLA模型、黑盒用户指令，通过图像扰动使模型在任意指令下稳定输出冻结令牌（如<freeze>、<eos>或“do-nothing”token），导致行动终止。\n- 内部最大化（对抗提示优化）：从初始参考提示集出发，通过梯度分析定位高影响词并进行贪婪同义词替换，迭代生成更难被冻结的“硬提示”集，扩展提示语义空间。\n- 外部最小化（对抗图像优化）：基于“硬提示”集合，对图像进行梯度聚合与更新，使VLA模型在多种提示下均以高概率输出冻结令牌，实现持久冻结。\n- 形式化目标：min−max 双层优化，外层在 L∞ 扰动预算 ε 内最小化冻结损失，内层最大化难提示下的损失，使对抗图像在跨提示下保持有效性。\n- 提示多样性增强：对比“随机采样”与“GPT生成”提示，显示GPT生成的语义多样性进一步提升攻击迁移性。\n\n论文使用数据集和训练资源\n- 数据集与基准：LIBERO-10、LIBERO-Goal、LIBERO-Object、LIBERO-Spatial四个机器人操控基准。\n- 目标模型：SpatialVLA、OpenVLA、π0 三种SOTA VLA模型（具备动作分块/离散化/连续动作头等不同架构与冻结令牌策略）。\n- 训练与实验资源：HPC集群，32×NVIDIA A800-SXM4-80GB GPUs。\n\n论文使用的评估环境和评估指标\n- 评估指标：攻击成功率（ASR），定义为使VLA模型进入一致冻结状态的对抗图像比例。\n- 评估设置：\n - 扰动预算：L∞ 约束下的 ε=4/255（主实验），对比 ε∈{1/255,2/255,4/255,8/255,16/255} 的敏感性。\n - 优化步数与提示数：图像迭代K=100、步长α=1/255；参考提示规模\\|P\\|=20；内部提示迭代M=10。\n - 基线对比：随机噪声、Single-Prompt PGD、Multi-Prompt（随机/随机+GPT）、FreezeVLA（随机）与FreezeVLA+GPT。\n - 冻结策略差异：SpatialVLA/π0 使用 <eos>；OpenVLA 使用“do-nothing”token。\n- 主要结果：FreezeVLA+GPT 在 ε=4/255 下实现平均ASR 73.3%（SpatialVLA）、95.4%（OpenVLA）、59.8%（π0），并具有强跨提示可迁移性与随预算提升的稳定增益。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>复旦大学</li><li>上海人工智能实验室</li><li>Sea AI Lab</li></ul>\n\n<p>论文概述</p>\n<ul><li>针对视觉-语言-动作（VLA）模型提出并形式化“行动冻结”（action-freezing）攻击：对抗图像使机器人忽略后续指令，进入持续无响应状态，可能在关键时刻导致不作为，引发实际安全风险。</li><li>提出FreezeVLA框架，通过最小-最大双层优化生成跨提示可迁移的对抗图像，在三个SOTA VLA模型与四个机器人基准上实现高攻击成功率，显著优于现有方法。</li></ul>\n\n<p>论文核心贡献点</p>\n<ul><li>识别并形式化VLA模型的“行动冻结”漏洞，揭示与错误动作同等严重的“不作为”威胁。</li><li>提出FreezeVLA：使用内部最大化（对抗提示优化）与外部最小化（对抗图像优化）的双层优化方法，扩展提示嵌入空间覆盖，显著提升跨提示迁移能力。</li><li>跨模型验证：SpatialVLA、OpenVLA、π0 上分别达到平均攻击成功率73.3%、95.4%、59.8%，全面超越随机噪声、PGD、多提示等基线。</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>威胁模型与目标：白盒访问VLA模型、黑盒用户指令，通过图像扰动使模型在任意指令下稳定输出冻结令牌（如<freeze>、<eos>或“do-nothing”token），导致行动终止。</li><li>内部最大化（对抗提示优化）：从初始参考提示集出发，通过梯度分析定位高影响词并进行贪婪同义词替换，迭代生成更难被冻结的“硬提示”集，扩展提示语义空间。</li><li>外部最小化（对抗图像优化）：基于“硬提示”集合，对图像进行梯度聚合与更新，使VLA模型在多种提示下均以高概率输出冻结令牌，实现持久冻结。</li><li>形式化目标：min−max 双层优化，外层在 L∞ 扰动预算 ε 内最小化冻结损失，内层最大化难提示下的损失，使对抗图像在跨提示下保持有效性。</li><li>提示多样性增强：对比“随机采样”与“GPT生成”提示，显示GPT生成的语义多样性进一步提升攻击迁移性。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集与基准：LIBERO-10、LIBERO-Goal、LIBERO-Object、LIBERO-Spatial四个机器人操控基准。</li><li>目标模型：SpatialVLA、OpenVLA、π0 三种SOTA VLA模型（具备动作分块/离散化/连续动作头等不同架构与冻结令牌策略）。</li><li>训练与实验资源：HPC集群，32×NVIDIA A800-SXM4-80GB GPUs。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估指标：攻击成功率（ASR），定义为使VLA模型进入一致冻结状态的对抗图像比例。</li><li>评估设置：</li></ul>\n<p> - 扰动预算：L∞ 约束下的 ε=4/255（主实验），对比 ε∈{1/255,2/255,4/255,8/255,16/255} 的敏感性。</p>\n<p> - 优化步数与提示数：图像迭代K=100、步长α=1/255；参考提示规模\\|P\\|=20；内部提示迭代M=10。</p>\n<p> - 基线对比：随机噪声、Single-Prompt PGD、Multi-Prompt（随机/随机+GPT）、FreezeVLA（随机）与FreezeVLA+GPT。</p>\n<p> - 冻结策略差异：SpatialVLA/π0 使用 <eos>；OpenVLA 使用“do-nothing”token。</p>\n<ul><li>主要结果：FreezeVLA+GPT 在 ε=4/255 下实现平均ASR 73.3%（SpatialVLA）、95.4%（OpenVLA）、59.8%（π0），并具有强跨提示可迁移性与随预算提升的稳定增益。</li></ul>"
  },
  {
    "date": "2025-09-24",
    "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training",
    "link": "http://arxiv.org/abs/2509.19752",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-23",
    "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action",
    "link": "http://arxiv.org/abs/2509.19571",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-23",
    "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation",
    "link": "http://arxiv.org/abs/2509.19480",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-23",
    "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
    "link": "http://arxiv.org/abs/2509.19012",
    "summary_markdown": "# Pure Vision-Language-Action (VLA) Models: A Comprehensive Survey\n## 论文研究单位\n\n- 兰州大学信息科学与工程学院（中国）\n- 新加坡国立大学NExT++研究中心（新加坡）\n- 中国科学院计算技术研究所（中国）\n## 论文概述\n\n本论文是关于纯 Vision-Language-Action (VLA) 模型的综合调研，系统地回顾了该领域的研究进展。论文从传统基于策略的控制方法出发，聚焦于 VLA 如何将 Vision-Language Models (VLMs) 从被动序列生成器转变为主动的机器人和决策制定代理。调研覆盖了超过300项近期研究，提供了清晰的方法分类体系，分析了 VLA 在不同应用场景中的表现，并提出了未来研究的关键挑战和发展方向。\n## 论文核心贡献点\n\n- **系统化分类体系**：提出了基于动作生成策略的 VLA 方法分类，将现有方法分为自回归、扩散、强化学习和混合/专用方法四大类别\n- **方法创新总结**：深入分析了每个类别的方法动机、核心策略和实现机制，强调了定义性特征和技术创新\n- **资源全面概述**：提供了 VLA 模型训练和评估所需的关联资源（数据集、基准测试、仿真平台）的详细概览\n- **挑战与机遇分析**：识别了现有技术的关键局限性，并提出了潜在的研究探索方向\n## 论文方法描述\n\n论文将 VLA 方法分为以下四个主要类别：\n### 1. 自回归模型\n- 将动作序列视为时间依赖过程，步进式生成动作\n- 代表性方法包括 Gato、RT-1/RT-2、PaLM-E 等\n- 进一步细分为：通用 VLA 方法、LLM 驱动的语义规划、轨迹生成与视觉对齐、结构优化与高效推理机制\n### 2. 扩散模型\n- 将动作生成建模为条件扩散过程\n- 代表性方法包括 SE(3)-DiffusionFields、Diffusion Policy、3D Diffuser Actor 等\n- 涵盖：扩散式通用 VLA 方法、多模态架构融合、应用优化与部署\n### 3. 强化学习模型\n- 基于强化学习的 VLA 微调策略\n- 通过奖励机制优化策略性能\n### 4. 混合与专用方法\n- 结合多种范式的混合架构\n- 针对特定领域和应用的专门设计\n## 论文使用数据集和训练资源\n### 真实世界数据集\n- **Open X-Embodiment (OXE)**：整合了22个机器人数据集，包含527种技能和160,266个任务\n- **BridgeData**：涵盖10个环境中的71个任务\n- 自主驾驶领域的相关数据集\n### 仿真数据集\n- **Open X-Embodiment (OXE)** 仿真版本\n- **BridgeData** 仿真版本\n- 自主驾驶仿真数据集\n### 仿真平台\n- **THOR、Habitat**：室内环境仿真\n- **MuJoCo、Isaac Gym**：物理仿真\n- **CARLA**：自动驾驶仿真\n- 提供可扩展的虚拟环境，支持多模态标注生成\n## 论文使用的评估环境和评估指标\n\n论文主要围绕以下应用领域进行评估：\n### 机器人操作\n- 机械臂控制\n- 四足机器人\n- 人形机器人\n- 移动机器人\n### 自主驾驶\n- 2D/3D 感知融合\n- 轨迹预测\n- 闭环控制\n### 评估指标\n- 任务成功率\n- 轨迹精度\n- 跨平台泛化能力\n- 零样本/少样本性能\n- 实时推理效率\n- 多模态对齐质量\n\n论文强调需要建立更全面的评估框架，特别关注长时序任务稳定性、语义对齐鲁棒性和实际部署效率等关键指标。",
    "summary_html": "<h1>Pure Vision-Language-Action (VLA) Models: A Comprehensive Survey</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n\n<ul><li>兰州大学信息科学与工程学院（中国）</li><li>新加坡国立大学NExT++研究中心（新加坡）</li><li>中国科学院计算技术研究所（中国）</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n\n<p>本论文是关于纯 Vision-Language-Action (VLA) 模型的综合调研，系统地回顾了该领域的研究进展。论文从传统基于策略的控制方法出发，聚焦于 VLA 如何将 Vision-Language Models (VLMs) 从被动序列生成器转变为主动的机器人和决策制定代理。调研覆盖了超过300项近期研究，提供了清晰的方法分类体系，分析了 VLA 在不同应用场景中的表现，并提出了未来研究的关键挑战和发展方向。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n\n<ul><li><strong>系统化分类体系</strong>：提出了基于动作生成策略的 VLA 方法分类，将现有方法分为自回归、扩散、强化学习和混合/专用方法四大类别</li><li><strong>方法创新总结</strong>：深入分析了每个类别的方法动机、核心策略和实现机制，强调了定义性特征和技术创新</li><li><strong>资源全面概述</strong>：提供了 VLA 模型训练和评估所需的关联资源（数据集、基准测试、仿真平台）的详细概览</li><li><strong>挑战与机遇分析</strong>：识别了现有技术的关键局限性，并提出了潜在的研究探索方向</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n\n<p>论文将 VLA 方法分为以下四个主要类别：</p>\n<h3>1. 自回归模型</h3>\n<ul><li>将动作序列视为时间依赖过程，步进式生成动作</li><li>代表性方法包括 Gato、RT-1/RT-2、PaLM-E 等</li><li>进一步细分为：通用 VLA 方法、LLM 驱动的语义规划、轨迹生成与视觉对齐、结构优化与高效推理机制</li></ul>\n<h3>2. 扩散模型</h3>\n<ul><li>将动作生成建模为条件扩散过程</li><li>代表性方法包括 SE(3)-DiffusionFields、Diffusion Policy、3D Diffuser Actor 等</li><li>涵盖：扩散式通用 VLA 方法、多模态架构融合、应用优化与部署</li></ul>\n<h3>3. 强化学习模型</h3>\n<ul><li>基于强化学习的 VLA 微调策略</li><li>通过奖励机制优化策略性能</li></ul>\n<h3>4. 混合与专用方法</h3>\n<ul><li>结合多种范式的混合架构</li><li>针对特定领域和应用的专门设计</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<h3>真实世界数据集</h3>\n<ul><li><strong>Open X-Embodiment (OXE)</strong>：整合了22个机器人数据集，包含527种技能和160,266个任务</li><li><strong>BridgeData</strong>：涵盖10个环境中的71个任务</li><li>自主驾驶领域的相关数据集</li></ul>\n<h3>仿真数据集</h3>\n<ul><li><strong>Open X-Embodiment (OXE)</strong> 仿真版本</li><li><strong>BridgeData</strong> 仿真版本</li><li>自主驾驶仿真数据集</li></ul>\n<h3>仿真平台</h3>\n<ul><li><strong>THOR、Habitat</strong>：室内环境仿真</li><li><strong>MuJoCo、Isaac Gym</strong>：物理仿真</li><li><strong>CARLA</strong>：自动驾驶仿真</li><li>提供可扩展的虚拟环境，支持多模态标注生成</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n\n<p>论文主要围绕以下应用领域进行评估：</p>\n<h3>机器人操作</h3>\n<ul><li>机械臂控制</li><li>四足机器人</li><li>人形机器人</li><li>移动机器人</li></ul>\n<h3>自主驾驶</h3>\n<ul><li>2D/3D 感知融合</li><li>轨迹预测</li><li>闭环控制</li></ul>\n<h3>评估指标</h3>\n<ul><li>任务成功率</li><li>轨迹精度</li><li>跨平台泛化能力</li><li>零样本/少样本性能</li><li>实时推理效率</li><li>多模态对齐质量</li></ul>\n\n<p>论文强调需要建立更全面的评估框架，特别关注长时序任务稳定性、语义对齐鲁棒性和实际部署效率等关键指标。</p>"
  },
  {
    "date": "2025-09-23",
    "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
    "link": "http://arxiv.org/abs/2509.18953",
    "summary_markdown": "### 论文研究单位\n上海交通大学（人工智能学院重点实验室）、中国军事科学院国防创新研究院、智能博弈决策实验室\n### 论文概述\n当前视觉-语言-动作（VLA）模型在真实部署中易受物理变化影响（如光照、物体位置干扰），但缺乏系统性评估工具。本文提出Eva-VLA框架，将离散物理变化转化为连续参数优化问题，通过黑箱优化算法探索最恶劣场景，暴露了当前VLA模型在多种物理干扰下的严重脆弱性。\n### 论文核心贡献点\n1. **首次系统性分解物理变化**：将真实世界变化分为物体3D变换（旋转）、光照变化、对抗补丁三类，突破传统梯度攻击限制\n2. **统一参数化评估框架**：将各类变化转化为连续参数分布，通过仿真环境可复现地评估鲁棒性\n3. **揭示VLA系统脆弱性**：在OpenVLA等先进模型中验证，物理干扰使失败率从23.5%激增至82.6%，长时序任务达97.8%\n### 论文方法描述\n#### 三类物理变化参数化\n- **物体3D变换**：用Tait-Bryan角α,β,γ∈[-90°,90°]约束旋转参数\n- **光照变化**：通过高斯衰减函数建模点光源L(z)=I·exp(-\\|\\|z-(x,y)\\|\\|²/2σ²)，参数λ={x,y,σ,I}控制位置/强度\n- **对抗补丁**：用自然图像在桌面纹理上优化位置φ={x,y}∈[(W/3,2W/3)×(H/3,2H/3)]\n#### 黑箱优化算法\n使用CMA-ES（协方差矩阵自适应进化策略）优化参数分布：\n1. 将变化参数建模为多元高斯分布N(μ,Σ²C)\n2. 迭代采样配置，计算对抗损失函数L_adv=-∑cos(A_clean, A_adv)\n3. 基于损失值更新分布参数μ, C, Σ\n4. 引入学习率自适应和早停机制加速收敛\n### 论文使用数据集和训练资源\n- **仿真数据集**：LIBERO基准套件（包含空间/物体/目标/长时序四类任务，每类10任务×50轮试验）\n- **真实数据集**：BridgeData v2（用于部分真实场景验证）\n- **硬件资源**：NVIDIA A800 GPU (80GB内存) + AgileX Piper机械臂（7自由度） + RealSense D435i相机\n### 论文使用的评估环境和指标\n- **仿真评估**：基于LIBERO套件，按任务最大步长设超时标准，统计失败率FR=1-SR（成功率）\n- **真实评估**：在三任务（抓取/定位/放置）上执行50次试验，攻击成功率44.6%\n- **鲁棒性指标**：\n - 失败率变化率：清洁环境(23.5%) vs 物理干扰(82.6%)\n - 不同干扰类型影响：物体变换>对抗补丁>光照变化\n - 时序影响：长时序任务在干扰下失败率超97%",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学（人工智能学院重点实验室）、中国军事科学院国防创新研究院、智能博弈决策实验室</p>\n<h3>论文概述</h3>\n<p>当前视觉-语言-动作（VLA）模型在真实部署中易受物理变化影响（如光照、物体位置干扰），但缺乏系统性评估工具。本文提出Eva-VLA框架，将离散物理变化转化为连续参数优化问题，通过黑箱优化算法探索最恶劣场景，暴露了当前VLA模型在多种物理干扰下的严重脆弱性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首次系统性分解物理变化</strong>：将真实世界变化分为物体3D变换（旋转）、光照变化、对抗补丁三类，突破传统梯度攻击限制</li><li><strong>统一参数化评估框架</strong>：将各类变化转化为连续参数分布，通过仿真环境可复现地评估鲁棒性</li><li><strong>揭示VLA系统脆弱性</strong>：在OpenVLA等先进模型中验证，物理干扰使失败率从23.5%激增至82.6%，长时序任务达97.8%</li></ol>\n<h3>论文方法描述</h3>\n<h4>三类物理变化参数化</h4>\n<ul><li><strong>物体3D变换</strong>：用Tait-Bryan角α,β,γ∈[-90°,90°]约束旋转参数</li><li><strong>光照变化</strong>：通过高斯衰减函数建模点光源L(z)=I·exp(-\\|\\|z-(x,y)\\|\\|²/2σ²)，参数λ={x,y,σ,I}控制位置/强度</li><li><strong>对抗补丁</strong>：用自然图像在桌面纹理上优化位置φ={x,y}∈[(W/3,2W/3)×(H/3,2H/3)]</li></ul>\n<h4>黑箱优化算法</h4>\n<p>使用CMA-ES（协方差矩阵自适应进化策略）优化参数分布：</p>\n<ol><li>将变化参数建模为多元高斯分布N(μ,Σ²C)</li><li>迭代采样配置，计算对抗损失函数L_adv=-∑cos(A_clean, A_adv)</li><li>基于损失值更新分布参数μ, C, Σ</li><li>引入学习率自适应和早停机制加速收敛</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>仿真数据集</strong>：LIBERO基准套件（包含空间/物体/目标/长时序四类任务，每类10任务×50轮试验）</li><li><strong>真实数据集</strong>：BridgeData v2（用于部分真实场景验证）</li><li><strong>硬件资源</strong>：NVIDIA A800 GPU (80GB内存) + AgileX Piper机械臂（7自由度） + RealSense D435i相机</li></ul>\n<h3>论文使用的评估环境和指标</h3>\n<ul><li><strong>仿真评估</strong>：基于LIBERO套件，按任务最大步长设超时标准，统计失败率FR=1-SR（成功率）</li><li><strong>真实评估</strong>：在三任务（抓取/定位/放置）上执行50次试验，攻击成功率44.6%</li><li><strong>鲁棒性指标</strong>：</li></ul>\n<p> - 失败率变化率：清洁环境(23.5%) vs 物理干扰(82.6%)</p>\n<p> - 不同干扰类型影响：物体变换>对抗补丁>光照变化</p>\n<p> - 时序影响：长时序任务在干扰下失败率超97%</p>"
  },
  {
    "date": "2025-09-23",
    "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
    "link": "http://arxiv.org/abs/2509.18865",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-22",
    "title": "Latent Action Pretraining Through World Modeling",
    "link": "http://arxiv.org/abs/2509.18428",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n\nMohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE\nAlexandria University, Alexandria, Egypt\n## 论文概述\n\n本文提出了LAWM框架，通过世界建模从无标签视频数据中学习潜在动作表示，用于预训练模仿学习模型。该框架能够从机器人录制视频或人类操作日常物体的视频中进行自监督学习，无需依赖人工标注的动作数据。框架设计为模型无关，可以跨任务、环境和具身形态进行迁移，在LIBERO基准测试和真实世界设置中的表现优于使用真实机器人动作训练的模型和类似的预训练方法，同时更高效且适用于实际场景。\n## 论文核心贡献点\n\n1. 提出LAWM框架，这是一个模型无关的框架，可以从机器人和人类视频中学习动作块表示，无需动作标签\n2. 实验表明该框架可以从人类演示和机器人操作视频中学习优于监督预训练的动作先验，且无需使用真实动作标签\n3. 使用小型模型（BAKU和Dreamerv3）的框架在LIBERO基准测试上优于使用大型模型的类似方法（villa-X）\n## 论文方法描述\n\nLAWM包含两个阶段：\n\n潜在动作预训练阶段：\n- 输入包括图像帧和自然语言指令\n- 模仿学习模型处理输入产生n个潜在动作表示\n- 这些潜在动作与当前帧和后续n-1帧配对，输入世界模型\n- 世界模型基于RSSM架构，包含编码器、动态模型和解码器\n- 通过预测未来图像帧进行端到端训练，损失函数包括MSE重建损失和KL散度正则化项\n- 学习信号来自预测视频序列中的下一帧图像\n\n动作微调阶段：\n- 预训练的模仿学习模型适配到下游机器人任务\n- 不再使用世界模型\n- 使用标注演示数据将观察（图像、语言指令和机器人状态）直接映射到真实动作\n- 对BAKU采用负对数似然损失，对Diffusion Policy采用去噪扩散损失\n## 论文使用数据集和训练资源\n\n数据集：\n- BridgeData v2：60,096条轨迹，24个环境，包含拾取放置、推动、折叠等任务\n- Something-Something v2：220,847个人类操作日常物体的视频片段（使用其中10%用于预训练）\n- LIBERO基准测试：包含LIBERO-90（90个任务）和四个任务套件（Spatial、Object、Goal、Long），每个套件10个任务，每个任务50个演示样本\n- 真实世界自定义数据集：5个任务（3个拾取放置、1个堆叠、1个移动），每个任务50个演示，使用VR控制器收集\n\n训练资源：\n- 单个A100 GPU\n- DreamerV3世界模型使用50M参数配置\n- 潜在动作空间维度为7（与真实动作相同）\n- BAKU实验的动作块大小为10，Diffusion Policy实验为16\n- 在BridgeData v2或Something-Something v2上预训练约30小时\n- 在LIBERO-90上微调约24小时\n- 在LIBERO任务套件上微调约2小时\n- 真实世界数据集微调约20分钟\n## 论文使用的评估环境和评估指标\n\n评估环境：\n- LIBERO-90基准测试：90个多样化任务\n- LIBERO任务套件：\n - LIBERO-Spatial：新布局下的相同任务和对象类型\n - LIBERO-Object：新对象类型下的相同任务和布局\n - LIBERO-Goal：新任务下的相同对象类型和布局\n - LIBERO-Long：长时域任务，包含多样化的对象、布局和背景\n- 真实世界设置：6自由度Realman机器人臂，配备1自由度夹爪，双视角相机观测\n\n评估指标：\n- 成功率（Success Rate, SR）：成功试验次数占总尝试次数的百分比\n- 每个任务进行10次评估试验\n- 使用典型相关分析（CCA）的第一规范分量的Pearson相关系数来量化潜在动作与真实动作之间的对齐程度",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n\n<p>Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE</p>\n<p>Alexandria University, Alexandria, Egypt</p>\n<h2 class=\"section-title\">论文概述</h2>\n\n<p>本文提出了LAWM框架，通过世界建模从无标签视频数据中学习潜在动作表示，用于预训练模仿学习模型。该框架能够从机器人录制视频或人类操作日常物体的视频中进行自监督学习，无需依赖人工标注的动作数据。框架设计为模型无关，可以跨任务、环境和具身形态进行迁移，在LIBERO基准测试和真实世界设置中的表现优于使用真实机器人动作训练的模型和类似的预训练方法，同时更高效且适用于实际场景。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n\n<ol><li>提出LAWM框架，这是一个模型无关的框架，可以从机器人和人类视频中学习动作块表示，无需动作标签</li><li>实验表明该框架可以从人类演示和机器人操作视频中学习优于监督预训练的动作先验，且无需使用真实动作标签</li><li>使用小型模型（BAKU和Dreamerv3）的框架在LIBERO基准测试上优于使用大型模型的类似方法（villa-X）</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n\n<p>LAWM包含两个阶段：</p>\n\n<p>潜在动作预训练阶段：</p>\n<ul><li>输入包括图像帧和自然语言指令</li><li>模仿学习模型处理输入产生n个潜在动作表示</li><li>这些潜在动作与当前帧和后续n-1帧配对，输入世界模型</li><li>世界模型基于RSSM架构，包含编码器、动态模型和解码器</li><li>通过预测未来图像帧进行端到端训练，损失函数包括MSE重建损失和KL散度正则化项</li><li>学习信号来自预测视频序列中的下一帧图像</li></ul>\n\n<p>动作微调阶段：</p>\n<ul><li>预训练的模仿学习模型适配到下游机器人任务</li><li>不再使用世界模型</li><li>使用标注演示数据将观察（图像、语言指令和机器人状态）直接映射到真实动作</li><li>对BAKU采用负对数似然损失，对Diffusion Policy采用去噪扩散损失</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n\n<p>数据集：</p>\n<ul><li>BridgeData v2：60,096条轨迹，24个环境，包含拾取放置、推动、折叠等任务</li><li>Something-Something v2：220,847个人类操作日常物体的视频片段（使用其中10%用于预训练）</li><li>LIBERO基准测试：包含LIBERO-90（90个任务）和四个任务套件（Spatial、Object、Goal、Long），每个套件10个任务，每个任务50个演示样本</li><li>真实世界自定义数据集：5个任务（3个拾取放置、1个堆叠、1个移动），每个任务50个演示，使用VR控制器收集</li></ul>\n\n<p>训练资源：</p>\n<ul><li>单个A100 GPU</li><li>DreamerV3世界模型使用50M参数配置</li><li>潜在动作空间维度为7（与真实动作相同）</li><li>BAKU实验的动作块大小为10，Diffusion Policy实验为16</li><li>在BridgeData v2或Something-Something v2上预训练约30小时</li><li>在LIBERO-90上微调约24小时</li><li>在LIBERO任务套件上微调约2小时</li><li>真实世界数据集微调约20分钟</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n\n<p>评估环境：</p>\n<ul><li>LIBERO-90基准测试：90个多样化任务</li><li>LIBERO任务套件：</li></ul>\n<p> - LIBERO-Spatial：新布局下的相同任务和对象类型</p>\n<p> - LIBERO-Object：新对象类型下的相同任务和布局</p>\n<p> - LIBERO-Goal：新任务下的相同对象类型和布局</p>\n<p> - LIBERO-Long：长时域任务，包含多样化的对象、布局和背景</p>\n<ul><li>真实世界设置：6自由度Realman机器人臂，配备1自由度夹爪，双视角相机观测</li></ul>\n\n<p>评估指标：</p>\n<ul><li>成功率（Success Rate, SR）：成功试验次数占总尝试次数的百分比</li><li>每个任务进行10次评估试验</li><li>使用典型相关分析（CCA）的第一规范分量的Pearson相关系数来量化潜在动作与真实动作之间的对齐程度</li></ul>"
  },
  {
    "date": "2025-09-22",
    "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
    "link": "http://arxiv.org/abs/2509.18282",
    "summary_markdown": "## 论文研究单位\nUniversity of Washington, NVIDIA, University of Southern California, Allen Institute for AI。\n## 论文概述\n当前机器人操控策略往往同时学习“关注哪里（where）”“做什么（what）”“如何执行（how）”，导致泛化能力受限。该论文提出将“where/what”的高层语义推理交由视觉语言模型（VLM）承担，使低层策略专注“how”。为此提出了PEEK（Policy-agnostic Extraction of Essential Keypoints），一个策略无关的最小中间表示框架：由VLM预测2D末端执行器路径（what）与任务相关掩蔽点（where），并将标注直接叠加到观测图像上，再由任意RGB/RGB-D策略进行训练与推理。在535次真实世界评估中，PEEK显著提升零样本泛化，包括模拟训练的3D策略在真实场景中成功率提升41.4倍，以及对大型VLA与小型操控策略分别带来2–3.5倍增益。\n## 论文核心贡献点\n- 统一的点式中间表示：将“路径+掩蔽点”作为策略输入，实现策略无关、可迁移的表示。\n- 可扩展标注流水线：自动从机器人视频中提取任务相关点与末端执行器路径，支持不同视角与小目标。\n- 多策略与多具身验证：在2D/3D、不同规模策略与两种真实机械臂系统上统一验证。\n- 零样本泛化实证：面对视觉干扰与语义新任务大幅提升成功率与鲁棒性。\n## 论文方法描述\n- 目标表示：联合预测两个点集——末端执行器的2D轨迹点 p_t 与任务相关掩蔽点 m_t，两者作为自然语言响应由VLM输出。\n- VLM微调：以VILA-1.5-3B为基座，整合机器人与通用点预测/VQA数据；联合优化路径与掩蔽点预测，训练约20小时（A100×8）。\n- 数据标注流水线：先用点跟踪（CoTracker3）识别场景中显著移动的点集作为“任务相关点”，再以检测器+掩蔽机制构建末端执行器路径；通过“停止点数”K-Means将长轨迹切分为更短的“子轨迹”，以提高表示的最小性与可预测性。\n- 策略接口：推理时每隔H步查询VLM，将路径与掩蔽点以可视化方式叠加到观测图像上，供任意RGB或RGB-D策略训练与执行。掩蔽通过对预测点为中心的8%边长区域开窗；路径以随时间颜色渐变的线段表示。\n## 论文使用数据集和训练资源\n- 数据来源：Open X-Embodiment（OXE）20+子数据集、DROID、LIBERO-90、BRIDGE-v2、RoboPoint；总计2M+问答/点预测对、148k轨迹、9种具身。\n- 训练资源：VLM微调在8×NVIDIA A100上约20小时；推理在RTX 3090上单次查询约4–6秒。\n- 工具/模型：CoTracker3点跟踪、Detectron2末端执行器检测、FoundationStereo深度估计。\n## 论文使用的评估环境和评估指标\n- 评估环境：\n - Franka（Sim-to-Real）：仿真采集2.5k条“堆叠彩色方块”轨迹；真实世界Zed 2立体相机+FoundationStereoDepth；评估Basic/Clutter/Semantic三类任务。\n - WidowX（BRIDGE）：单目RGB环境，改换桌面与背景；在Basic/Clutter/Semantic三类任务上评估。\n- 评估指标：任务完成率与成功率（平均与分项），包含抓取/到达的Partial Credit；零样本跨场景与跨语义泛化能力；消融实验使用路径/掩蔽的组合成功率；VLM质量使用DTW、起点/终点L2、IoU。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>University of Washington, NVIDIA, University of Southern California, Allen Institute for AI。</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>当前机器人操控策略往往同时学习“关注哪里（where）”“做什么（what）”“如何执行（how）”，导致泛化能力受限。该论文提出将“where/what”的高层语义推理交由视觉语言模型（VLM）承担，使低层策略专注“how”。为此提出了PEEK（Policy-agnostic Extraction of Essential Keypoints），一个策略无关的最小中间表示框架：由VLM预测2D末端执行器路径（what）与任务相关掩蔽点（where），并将标注直接叠加到观测图像上，再由任意RGB/RGB-D策略进行训练与推理。在535次真实世界评估中，PEEK显著提升零样本泛化，包括模拟训练的3D策略在真实场景中成功率提升41.4倍，以及对大型VLA与小型操控策略分别带来2–3.5倍增益。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>统一的点式中间表示：将“路径+掩蔽点”作为策略输入，实现策略无关、可迁移的表示。</li><li>可扩展标注流水线：自动从机器人视频中提取任务相关点与末端执行器路径，支持不同视角与小目标。</li><li>多策略与多具身验证：在2D/3D、不同规模策略与两种真实机械臂系统上统一验证。</li><li>零样本泛化实证：面对视觉干扰与语义新任务大幅提升成功率与鲁棒性。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>目标表示：联合预测两个点集——末端执行器的2D轨迹点 p_t 与任务相关掩蔽点 m_t，两者作为自然语言响应由VLM输出。</li><li>VLM微调：以VILA-1.5-3B为基座，整合机器人与通用点预测/VQA数据；联合优化路径与掩蔽点预测，训练约20小时（A100×8）。</li><li>数据标注流水线：先用点跟踪（CoTracker3）识别场景中显著移动的点集作为“任务相关点”，再以检测器+掩蔽机制构建末端执行器路径；通过“停止点数”K-Means将长轨迹切分为更短的“子轨迹”，以提高表示的最小性与可预测性。</li><li>策略接口：推理时每隔H步查询VLM，将路径与掩蔽点以可视化方式叠加到观测图像上，供任意RGB或RGB-D策略训练与执行。掩蔽通过对预测点为中心的8%边长区域开窗；路径以随时间颜色渐变的线段表示。</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>数据来源：Open X-Embodiment（OXE）20+子数据集、DROID、LIBERO-90、BRIDGE-v2、RoboPoint；总计2M+问答/点预测对、148k轨迹、9种具身。</li><li>训练资源：VLM微调在8×NVIDIA A100上约20小时；推理在RTX 3090上单次查询约4–6秒。</li><li>工具/模型：CoTracker3点跟踪、Detectron2末端执行器检测、FoundationStereo深度估计。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：</li></ul>\n<p> - Franka（Sim-to-Real）：仿真采集2.5k条“堆叠彩色方块”轨迹；真实世界Zed 2立体相机+FoundationStereoDepth；评估Basic/Clutter/Semantic三类任务。</p>\n<p> - WidowX（BRIDGE）：单目RGB环境，改换桌面与背景；在Basic/Clutter/Semantic三类任务上评估。</p>\n<ul><li>评估指标：任务完成率与成功率（平均与分项），包含抓取/到达的Partial Credit；零样本跨场景与跨语义泛化能力；消融实验使用路径/掩蔽的组合成功率；VLM质量使用DTW、起点/终点L2、IoU。</li></ul>"
  },
  {
    "date": "2025-09-18",
    "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation",
    "link": "http://arxiv.org/abs/2509.18183",
    "summary_markdown": "### 论文研究单位\nLi Auto Inc.，中国北京\n### 论文概述\nVLA模型在视觉视角变化时（如不同位置、背景或高度）泛化性能下降，影响任务执行。论文提出轻量级视角适应融合框架VLA-LPAF，通过潜在空间融合多视角2D图像，仅使用单视图数据进行微调，弥补视角差异。与RoboFlamingo结合构建RoboFlamingo-LPAF，在CALVIN、LIBERO和自定义CabinEnv数据集上提升成功率8%-30%，并在真实任务中验证视角适应能力。\n### 论文核心贡献点\n- 首次实现基于2D图像的轻量级潜在视角特征融合框架VLA-LPAF，降低视角一致性约束。\n- 实例化VLA-LPAF为RoboFlamingo-LPAF，通过多数据集和真实任务验证有效性。\n- 三阶段训练策略（单视图仅动作、多视图仅融合、多视图联合训练）优化泛化能力。\n### 论文方法描述\n- **问题定义**：VLA模型映射多模态输入（图像、文本）到动作，但视角差异导致特征偏移。\n- **架构**：添加MLP基融合模块，利用单视图参考数据集（D_R）和多视图辅助数据集（D_M)。\n- **对齐融合**：融合模块在ViT编码的潜在空间中，对齐参考视图（R）和辅助视图（M）的特征。\n- **训练策略**：\n 1. 单视图阶段：冻结ViT，微调LLM参数（θ），使用动作损失（公式1）。\n 2. 多视图阶段：仅训练融合模块参数（θ'），使用对齐损失（公式4）。\n 3. 联合阶段：同时微调θ和θ'，结合动作损失（公式3）和对齐损失（公式5）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 模拟：CALVIN、LIBERO、CabinEnv（自定义舱内环境，包含按钮按压和杠杆翻转任务）。\n - 数据集构建：参考视图（0°）+ 辅助视图（±45°范围，v=4个视角）。\n- **训练资源**：8个NVIDIA A800 80GB GPU，图像统一尺寸224×224。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：使用多视角测试数据（如CALVIN中参考视图±90°范围，间隔10°）。\n - 真实环境：Realman RML机器人臂（单臂），Intel RealSense D415（全局参考和腕部摄像头），Azure Kinect（辅助全局摄像头），执行按钮按压和杠杆翻转任务。\n- **评估指标**：任务成功率（success rate），基线比较显示CALVIN平均提升8%，LIBERO提升15%，CabinEnv提升30%，并在真实任务中验证RoboFlamingo-LPAF完成未包含视角（如30°）的任务。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Li Auto Inc.，中国北京</p>\n<h3>论文概述</h3>\n<p>VLA模型在视觉视角变化时（如不同位置、背景或高度）泛化性能下降，影响任务执行。论文提出轻量级视角适应融合框架VLA-LPAF，通过潜在空间融合多视角2D图像，仅使用单视图数据进行微调，弥补视角差异。与RoboFlamingo结合构建RoboFlamingo-LPAF，在CALVIN、LIBERO和自定义CabinEnv数据集上提升成功率8%-30%，并在真实任务中验证视角适应能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首次实现基于2D图像的轻量级潜在视角特征融合框架VLA-LPAF，降低视角一致性约束。</li><li>实例化VLA-LPAF为RoboFlamingo-LPAF，通过多数据集和真实任务验证有效性。</li><li>三阶段训练策略（单视图仅动作、多视图仅融合、多视图联合训练）优化泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>问题定义</strong>：VLA模型映射多模态输入（图像、文本）到动作，但视角差异导致特征偏移。</li><li><strong>架构</strong>：添加MLP基融合模块，利用单视图参考数据集（D_R）和多视图辅助数据集（D_M)。</li><li><strong>对齐融合</strong>：融合模块在ViT编码的潜在空间中，对齐参考视图（R）和辅助视图（M）的特征。</li><li><strong>训练策略</strong>：</li></ul>\n<p> 1. 单视图阶段：冻结ViT，微调LLM参数（θ），使用动作损失（公式1）。</p>\n<p> 2. 多视图阶段：仅训练融合模块参数（θ'），使用对齐损失（公式4）。</p>\n<p> 3. 联合阶段：同时微调θ和θ'，结合动作损失（公式3）和对齐损失（公式5）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟：CALVIN、LIBERO、CabinEnv（自定义舱内环境，包含按钮按压和杠杆翻转任务）。</p>\n<p> - 数据集构建：参考视图（0°）+ 辅助视图（±45°范围，v=4个视角）。</p>\n<ul><li><strong>训练资源</strong>：8个NVIDIA A800 80GB GPU，图像统一尺寸224×224。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：使用多视角测试数据（如CALVIN中参考视图±90°范围，间隔10°）。</p>\n<p> - 真实环境：Realman RML机器人臂（单臂），Intel RealSense D415（全局参考和腕部摄像头），Azure Kinect（辅助全局摄像头），执行按钮按压和杠杆翻转任务。</p>\n<ul><li><strong>评估指标</strong>：任务成功率（success rate），基线比较显示CALVIN平均提升8%，LIBERO提升15%，CabinEnv提升30%，并在真实任务中验证RoboFlamingo-LPAF完成未包含视角（如30°）的任务。</li></ul>"
  },
  {
    "date": "2025-09-22",
    "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States",
    "link": "http://arxiv.org/abs/2509.18043",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-20",
    "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering",
    "link": "http://arxiv.org/abs/2509.16680",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n- Dartmouth College（达特茅斯学院）\n- Shandong University（山东大学）\n- Harvard University（哈佛大学）\n## 论文概述\n论文提出了ProtoVQA，这是一个用于可解释细粒度视觉问答的统一原型框架。该框架通过学习问题感知的原型作为推理锚点，将答案与判别性图像区域连接起来，并应用空间约束匹配来确保选择的证据具有连贯性和语义相关性。通过共享原型主干网络，该框架支持视觉问答和定位任务，并提出了视觉-语言对齐评分（VLAS）来评估解释质量。\n## 论文核心贡献点\n1. 引入适应性原型框架，能够无缝处理不同的视觉-语言下游任务，包括视觉问答和定位\n2. 采用空间约束的贪婪匹配策略建模动态视觉问题关系和几何变化\n3. 通过明确的视觉证据和系统化的视觉-语言对齐验证实现全面的可解释性\n## 论文方法描述\n**框架组成：**\n- **特征提取模块**：使用DeiT作为视觉特征提取器，DeBERTa作为文本编码器，将图像和文本特征投影到共享的视觉-语言空间\n- **可解释原型部分选择模块**：引入子补丁原型（m×k结构）和贪婪匹配算法，通过空间约束选择与原型最匹配的图像区域\n- **答案处理**：支持两种类型——Type 1（视觉定位）和Type 2（描述性问答），分别处理坐标输入和文本答案\n- **视觉-语言对齐评估**：提出VLAS指标，测量模型关注区域与真实证据的对齐程度\n\n**核心技术：**\n- 子补丁原型：每个原型由k个子补丁组成，形成语义锚点\n- 空间约束贪婪匹配：通过迭代选择相似度最高的补丁-子补丁对，并使用邻接掩码确保空间连续性\n- 权重共享机制：在问题编码和答案处理之间共享特征投影器参数\n## 论文使用数据集和训练资源\n**数据集：**\n- Visual7W：包含327,939个问题-答案对，覆盖47,300张COCO图像，每个问题配有4个人工选择选项和561,459个对象级定位标注\n\n**训练资源：**\n- 硬件：NVIDIA A800 GPU（80GB）\n- 训练配置：200个epoch，Adam优化器，学习率1×10^-4，批大小64\n- 图像处理：224×224像素，16×16补丁\n- 原型参数：m=10个原型，k=3个子补丁，空间约束半径r=3\n## 论文使用的评估环境和评估指标\n**评估环境：**\n- 主要在Visual7W测试集上进行评估\n- 对比基线包括SUPER、QOI_Attention、SDF of VLT、STL、CFR、BriVL、CTI、Bi-CMA等代表性VQA模型\n\n**评估指标：**\n- **准确性指标**：分类准确率（Accuracy）\n- **解释质量指标**：视觉-语言对齐评分（VLAS），通过IoU阈值（θ=0.5）计算模型关注区域与真实标注的对齐程度\n- **可视化分析**：定性展示模型选择的图像区域与真实标注的对应关系\n\n**实验结果：**\n- ProtoVQA在Visual7W上达到70.23%准确率，与强基线模型相当\n- 在VLAS指标上显著优于基线方法，VLAS@1达到0.4103（比Bi-CMA提升66.4%），VLAS@3达到0.2466（比Bi-CMA提升119.6%）\n- 定性分析显示模型能够准确关注与问题相关的语义区域",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>Dartmouth College（达特茅斯学院）</li><li>Shandong University（山东大学）</li><li>Harvard University（哈佛大学）</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文提出了ProtoVQA，这是一个用于可解释细粒度视觉问答的统一原型框架。该框架通过学习问题感知的原型作为推理锚点，将答案与判别性图像区域连接起来，并应用空间约束匹配来确保选择的证据具有连贯性和语义相关性。通过共享原型主干网络，该框架支持视觉问答和定位任务，并提出了视觉-语言对齐评分（VLAS）来评估解释质量。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>引入适应性原型框架，能够无缝处理不同的视觉-语言下游任务，包括视觉问答和定位</li><li>采用空间约束的贪婪匹配策略建模动态视觉问题关系和几何变化</li><li>通过明确的视觉证据和系统化的视觉-语言对齐验证实现全面的可解释性</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p><strong>框架组成：</strong></p>\n<ul><li><strong>特征提取模块</strong>：使用DeiT作为视觉特征提取器，DeBERTa作为文本编码器，将图像和文本特征投影到共享的视觉-语言空间</li><li><strong>可解释原型部分选择模块</strong>：引入子补丁原型（m×k结构）和贪婪匹配算法，通过空间约束选择与原型最匹配的图像区域</li><li><strong>答案处理</strong>：支持两种类型——Type 1（视觉定位）和Type 2（描述性问答），分别处理坐标输入和文本答案</li><li><strong>视觉-语言对齐评估</strong>：提出VLAS指标，测量模型关注区域与真实证据的对齐程度</li></ul>\n\n<p><strong>核心技术：</strong></p>\n<ul><li>子补丁原型：每个原型由k个子补丁组成，形成语义锚点</li><li>空间约束贪婪匹配：通过迭代选择相似度最高的补丁-子补丁对，并使用邻接掩码确保空间连续性</li><li>权重共享机制：在问题编码和答案处理之间共享特征投影器参数</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<p><strong>数据集：</strong></p>\n<ul><li>Visual7W：包含327,939个问题-答案对，覆盖47,300张COCO图像，每个问题配有4个人工选择选项和561,459个对象级定位标注</li></ul>\n\n<p><strong>训练资源：</strong></p>\n<ul><li>硬件：NVIDIA A800 GPU（80GB）</li><li>训练配置：200个epoch，Adam优化器，学习率1×10^-4，批大小64</li><li>图像处理：224×224像素，16×16补丁</li><li>原型参数：m=10个原型，k=3个子补丁，空间约束半径r=3</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p><strong>评估环境：</strong></p>\n<ul><li>主要在Visual7W测试集上进行评估</li><li>对比基线包括SUPER、QOI_Attention、SDF of VLT、STL、CFR、BriVL、CTI、Bi-CMA等代表性VQA模型</li></ul>\n\n<p><strong>评估指标：</strong></p>\n<ul><li><strong>准确性指标</strong>：分类准确率（Accuracy）</li><li><strong>解释质量指标</strong>：视觉-语言对齐评分（VLAS），通过IoU阈值（θ=0.5）计算模型关注区域与真实标注的对齐程度</li><li><strong>可视化分析</strong>：定性展示模型选择的图像区域与真实标注的对应关系</li></ul>\n\n<p><strong>实验结果：</strong></p>\n<ul><li>ProtoVQA在Visual7W上达到70.23%准确率，与强基线模型相当</li><li>在VLAS指标上显著优于基线方法，VLAS@1达到0.4103（比Bi-CMA提升66.4%），VLAS@3达到0.2466（比Bi-CMA提升119.6%）</li><li>定性分析显示模型能够准确关注与问题相关的语义区域</li></ul>"
  },
  {
    "date": "2025-09-19",
    "title": "Randomized Smoothing Meets Vision-Language Models",
    "link": "http://arxiv.org/abs/2509.16088",
    "summary_markdown": "## 论文研究单位\n- National Technical University of Athens, Athens, Greece\n- Université Grenoble Alpes, Grenoble, France\n- CSX‑AI, Grenoble, France\n- Carl von Ossietzky University of Oldenburg, Oldenburg, Germany\n- Chalmers University of Technology, Gothenburg, Sweden\n## 论文概述\n随机化平滑（Randomized Smoothing, RS）是目前在大规模分类模型上唯一可行的鲁棒性认证技术，但其原本只能处理离散标签，难以直接用于生成式的视觉‑语言模型（Vision‑Language Models, VLMs）。本文提出一种将生成式模型的输出映射为分类问题的通用方法，从而把 RS 迁移到 VLMs 场景。核心思路是引入一个“oracle”分类层，对模型生成的文本进行二分类（如有害/无害）或离散动作映射，或将语义等价的答案归为同一等价类。通过对图像添加高斯噪声、保持提示文本不变、进行多次采样并使用 oracle 进行投票，得到多数类的置信下界，进而推导出可验证的鲁棒半径。论文还从理论上分析了样本数对半径和认证精度的影响，给出改进的样本复杂度标度律，使得在实际应用中只需要 10²‑10³ 级别的样本即可得到与经典方法（10⁴‑10⁵ 样本）相匹配的证书。实验在最新的 VLM 上对最近的越狱式攻击进行了验证，展示了该方法的可扩展性和实用性。\n## 论文核心贡献点\n- 将随机化平滑从分类扩展到生成式 VLM，提出基于 oracle 的抽象层，使得生成结果可以被视作离散类。\n- 给出在 oracle 错误率 ε<0.5 下的概率下界修正公式\n \\[\n \\bar p_y = \\frac{\\bar q_y - \\epsilon}{1-2\\epsilon}\n \\]\n 并证明在 \\(\\bar q_y>0.5\\) 时该下界仍然是有效的。\n- 推导出样本数 \\(n\\) 与可验证半径的关系（利用中心极限定理和 Shore 对 \\(\\Phi^{-1}\\) 的近似），得到标度律\n \\[\n r_\\sigma(\\alpha,n) \\approx 1 - 1.64\\frac{z_\\alpha}{\\sqrt n}\n \\]\n 表明样本量只需降低 2‑3 个数量级即可保持接近最优的半径。\n- 在理论上放宽了前期工作对均匀分布的要求，仅需多数类概率分布主要集中在 \\([\\beta,1)\\) 且 \\(\\beta\\ge 0.7\\)，从而提升了方法的适用性。\n- 给出适用于 VLM 的完整认证算法（Algorithm 2），包括语义等价聚类的投票机制。\n- 通过在最新 VLMs 上进行实验，验证了对 jailbreak‑style 攻击的防御能力，并报告了认证半径、认证精度以及样本效率的定量结果。\n## 论文方法描述\n1. **模型与噪声注入**\n - 输入为图像 \\(\\mathbf x\\) 与文本提示 \\(\\mathbf t\\)。\n - 对图像加高斯噪声 \\(\\mathbf z\\sim\\mathcal N(\\mathbf 0,\\sigma^2\\mathbf I)\\) 生成 \\(\\mathbf x'\\)。\n - 文本保持不变，调用 VLM \\(f_\\theta(\\mathbf x',\\mathbf t)\\) 获得生成的回答 \\(\\mathbf y\\)。\n\n2. **Oracle 分类层**\n - **内容安全分类**：oracle 将 \\(\\mathbf y\\) 判为 “有害” 或 “无害”。\n - **离散动作映射**：若 VLM 充当 VLA，oracle 将答案映射为有限的动作集合（如 `base‑forward`、`gripper‑open`）。\n - **语义等价聚类**：oracle 判断新回答是否与已出现的回答语义相同，若相同则累计计数，否则创建新类。\n\n3. **投票与计数**（Algorithm 2）\n - 用字典 `ans` 存储每个等价类（或每个离散动作）的出现次数。\n - 对每个噪声样本执行 oracle 判断并更新相应计数。\n - 最终返回计数最高的回答 \\(y\\) 与其计数 \\(c\\)。\n\n4. **概率下界与证书半径**\n - 将计数 \\(c\\) 与样本数 \\(n\\) 带入 Clopper‑Pearson 方法得到 \\(\\bar q_y\\)（在置信度 \\(1-\\alpha\\) 下的下界）。\n - 考虑 oracle 误差 \\(\\epsilon\\)（假设 \\(\\epsilon<0.5\\)），修正得到真实概率下界\n \\[\n \\bar p_y = \\frac{\\bar q_y - \\epsilon}{1-2\\epsilon}\n \\]\n - 若 \\(\\bar p_y>0.5\\)，则可验证半径为\n \\[\n R = \\sigma \\Phi^{-1}(\\bar p_y)\n \\]\n - 对于二分类情形，若对 \\(\\epsilon\\) 没有任何已知信息，只要 \\(\\bar q_y>0.5\\) 仍可直接使用 \\(R = \\sigma\\Phi^{-1}(\\bar q_y)\\) 作为下界（Theorem 4.2）。\n\n5. **样本效率分析**\n - 利用 CLT 对 \\(\\bar p_y\\) 进行近似，得到期望半径\n \\[\n R_\\sigma^{\\alpha,n}(p_A) \\approx \\sigma \\Phi^{-1}\\!\\bigl(p_A - t_{\\alpha,n}\\bigr),\\quad\n t_{\\alpha,n}=z_\\alpha\\sqrt{p_A(1-p_A)/n}\n \\]\n - 通过 Shore 对 \\(\\Phi^{-1}\\) 的幂级数近似，进一步得到平均半径的下降比例\n \\[\n r_\\sigma(\\alpha,n) = \\frac{\\bar R_\\sigma(\\alpha,n)}{\\bar R_\\sigma(0,\\infty)} \\approx 1 - 1.64\\frac{z_\\alpha}{\\sqrt n}\n \\]\n - 该公式说明把样本数从 \\(10^5\\) 降到 \\(10^3\\) 仍能保持约 90% 以上的理想半径。\n\n6. **实验实现**\n - 采样若干噪声图像（如 \\(n=500\\)‑\\(2000\\)），对每个图像调用 VLM 并由强 LLM（如 GPT‑4、Llama‑70B）充当 oracle。\n - 计算多数类的计数、Clopper‑Pearson 下界与对应半径。\n - 与原始分类 RS 基线以及仅使用 Clopper‑Pearson 不进行 oracle 修正的方案进行对比。\n## 论文使用数据集和训练资源\n- **模型**：直接使用公开的预训练 VLM（如 LLaVA、InstructBLIP、GPT‑4V 等），不进行额外微调。\n- **Oracle**：使用更强的语言模型（例如 GPT‑4 或 Llama‑70B）作为分类器或语义等价判断器。\n- **实验数据**：在论文正文中未提供详细数据集描述，附录 B 中列有用于内容安全、VLA 动作与语义聚类的图像‑提示对（来源于公开的 VLM 评测集），并对每个样本进行多次噪声采样。\n- **计算资源**：实验基于多卡 GPU 集群执行 VLM 推理，oracle 的调用使用相同的 GPU 或 CPU 后端。由于模型已预训练，计算开销主要是多次前向推理（每张图像 \\(n\\) 次），具体硬件规格未在正文中给出。\n## 论文使用的评估环境和评估指标\n- **评估环境**：在标准机器学习服务器上运行（Python + PyTorch），所有 VLM 与 oracle 均通过相同的推理框架进行调用，实验代码已开源。\n- **鲁棒性指标**\n - **可验证半径 \\(R\\)**：在给定置信度 \\(\\alpha\\) 下，对每个输入返回的最大半径。\n - **认证精度**：在特定半径阈值（如 \\(R=0.5\\)）下，能够给出非“ABSTAIN”结果的样本比例。\n - **半径下降比例**：通过标度律或实际测量比较不同样本数 \\(n\\) 下的平均半径 \\(\\bar R_\\sigma(\\alpha,n)\\)。\n - **Oracle 误差敏感性**：评估在不同假设的 \\(\\epsilon\\)（oracle 错误率）下半径下界的变化。\n- **对抗评估**\n - 对最新的 jailbreak‑style 攻击（Qi et al., 2024）进行防御测试，统计攻击成功率的降低程度。\n - 与未加噪声的原始 VLM 对比，展示 RS 增强后的安全性提升。\n- **样本效率**\n - 记录在不同样本规模（如 \\(n=100, 500, 1000, 5000\\)）下的认证半径、认证精度以及所需的计算时间。\n - 通过经验曲线验证理论标度律 \\(1-1.64z_\\alpha/\\sqrt n\\) 的准确性。\n\n以上内容概括了论文的研究单位、整体概述、主要贡献、方法细节、实验使用的数据与资源以及评估环境和指标。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>National Technical University of Athens, Athens, Greece</li><li>Université Grenoble Alpes, Grenoble, France</li><li>CSX‑AI, Grenoble, France</li><li>Carl von Ossietzky University of Oldenburg, Oldenburg, Germany</li><li>Chalmers University of Technology, Gothenburg, Sweden</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>随机化平滑（Randomized Smoothing, RS）是目前在大规模分类模型上唯一可行的鲁棒性认证技术，但其原本只能处理离散标签，难以直接用于生成式的视觉‑语言模型（Vision‑Language Models, VLMs）。本文提出一种将生成式模型的输出映射为分类问题的通用方法，从而把 RS 迁移到 VLMs 场景。核心思路是引入一个“oracle”分类层，对模型生成的文本进行二分类（如有害/无害）或离散动作映射，或将语义等价的答案归为同一等价类。通过对图像添加高斯噪声、保持提示文本不变、进行多次采样并使用 oracle 进行投票，得到多数类的置信下界，进而推导出可验证的鲁棒半径。论文还从理论上分析了样本数对半径和认证精度的影响，给出改进的样本复杂度标度律，使得在实际应用中只需要 10²‑10³ 级别的样本即可得到与经典方法（10⁴‑10⁵ 样本）相匹配的证书。实验在最新的 VLM 上对最近的越狱式攻击进行了验证，展示了该方法的可扩展性和实用性。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>将随机化平滑从分类扩展到生成式 VLM，提出基于 oracle 的抽象层，使得生成结果可以被视作离散类。</li><li>给出在 oracle 错误率 ε<0.5 下的概率下界修正公式</li></ul>\n<p> \\[</p>\n<p> \\bar p_y = \\frac{\\bar q_y - \\epsilon}{1-2\\epsilon}</p>\n<p> \\]</p>\n<p> 并证明在 \\(\\bar q_y>0.5\\) 时该下界仍然是有效的。</p>\n<ul><li>推导出样本数 \\(n\\) 与可验证半径的关系（利用中心极限定理和 Shore 对 \\(\\Phi^{-1}\\) 的近似），得到标度律</li></ul>\n<p> \\[</p>\n<p> r_\\sigma(\\alpha,n) \\approx 1 - 1.64\\frac{z_\\alpha}{\\sqrt n}</p>\n<p> \\]</p>\n<p> 表明样本量只需降低 2‑3 个数量级即可保持接近最优的半径。</p>\n<ul><li>在理论上放宽了前期工作对均匀分布的要求，仅需多数类概率分布主要集中在 \\([\\beta,1)\\) 且 \\(\\beta\\ge 0.7\\)，从而提升了方法的适用性。</li><li>给出适用于 VLM 的完整认证算法（Algorithm 2），包括语义等价聚类的投票机制。</li><li>通过在最新 VLMs 上进行实验，验证了对 jailbreak‑style 攻击的防御能力，并报告了认证半径、认证精度以及样本效率的定量结果。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ol><li><strong>模型与噪声注入</strong></li></ol>\n<p> - 输入为图像 \\(\\mathbf x\\) 与文本提示 \\(\\mathbf t\\)。</p>\n<p> - 对图像加高斯噪声 \\(\\mathbf z\\sim\\mathcal N(\\mathbf 0,\\sigma^2\\mathbf I)\\) 生成 \\(\\mathbf x'\\)。</p>\n<p> - 文本保持不变，调用 VLM \\(f_\\theta(\\mathbf x',\\mathbf t)\\) 获得生成的回答 \\(\\mathbf y\\)。</p>\n\n<ol><li><strong>Oracle 分类层</strong></li></ol>\n<p> - <strong>内容安全分类</strong>：oracle 将 \\(\\mathbf y\\) 判为 “有害” 或 “无害”。</p>\n<p> - <strong>离散动作映射</strong>：若 VLM 充当 VLA，oracle 将答案映射为有限的动作集合（如 <code>base‑forward</code>、<code>gripper‑open</code>）。</p>\n<p> - <strong>语义等价聚类</strong>：oracle 判断新回答是否与已出现的回答语义相同，若相同则累计计数，否则创建新类。</p>\n\n<ol><li><strong>投票与计数</strong>（Algorithm 2）</li></ol>\n<p> - 用字典 <code>ans</code> 存储每个等价类（或每个离散动作）的出现次数。</p>\n<p> - 对每个噪声样本执行 oracle 判断并更新相应计数。</p>\n<p> - 最终返回计数最高的回答 \\(y\\) 与其计数 \\(c\\)。</p>\n\n<ol><li><strong>概率下界与证书半径</strong></li></ol>\n<p> - 将计数 \\(c\\) 与样本数 \\(n\\) 带入 Clopper‑Pearson 方法得到 \\(\\bar q_y\\)（在置信度 \\(1-\\alpha\\) 下的下界）。</p>\n<p> - 考虑 oracle 误差 \\(\\epsilon\\)（假设 \\(\\epsilon<0.5\\)），修正得到真实概率下界</p>\n<p> \\[</p>\n<p> \\bar p_y = \\frac{\\bar q_y - \\epsilon}{1-2\\epsilon}</p>\n<p> \\]</p>\n<p> - 若 \\(\\bar p_y>0.5\\)，则可验证半径为</p>\n<p> \\[</p>\n<p> R = \\sigma \\Phi^{-1}(\\bar p_y)</p>\n<p> \\]</p>\n<p> - 对于二分类情形，若对 \\(\\epsilon\\) 没有任何已知信息，只要 \\(\\bar q_y>0.5\\) 仍可直接使用 \\(R = \\sigma\\Phi^{-1}(\\bar q_y)\\) 作为下界（Theorem 4.2）。</p>\n\n<ol><li><strong>样本效率分析</strong></li></ol>\n<p> - 利用 CLT 对 \\(\\bar p_y\\) 进行近似，得到期望半径</p>\n<p> \\[</p>\n<p> R_\\sigma^{\\alpha,n}(p_A) \\approx \\sigma \\Phi^{-1}\\!\\bigl(p_A - t_{\\alpha,n}\\bigr),\\quad</p>\n<p> t_{\\alpha,n}=z_\\alpha\\sqrt{p_A(1-p_A)/n}</p>\n<p> \\]</p>\n<p> - 通过 Shore 对 \\(\\Phi^{-1}\\) 的幂级数近似，进一步得到平均半径的下降比例</p>\n<p> \\[</p>\n<p> r_\\sigma(\\alpha,n) = \\frac{\\bar R_\\sigma(\\alpha,n)}{\\bar R_\\sigma(0,\\infty)} \\approx 1 - 1.64\\frac{z_\\alpha}{\\sqrt n}</p>\n<p> \\]</p>\n<p> - 该公式说明把样本数从 \\(10^5\\) 降到 \\(10^3\\) 仍能保持约 90% 以上的理想半径。</p>\n\n<ol><li><strong>实验实现</strong></li></ol>\n<p> - 采样若干噪声图像（如 \\(n=500\\)‑\\(2000\\)），对每个图像调用 VLM 并由强 LLM（如 GPT‑4、Llama‑70B）充当 oracle。</p>\n<p> - 计算多数类的计数、Clopper‑Pearson 下界与对应半径。</p>\n<p> - 与原始分类 RS 基线以及仅使用 Clopper‑Pearson 不进行 oracle 修正的方案进行对比。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>模型</strong>：直接使用公开的预训练 VLM（如 LLaVA、InstructBLIP、GPT‑4V 等），不进行额外微调。</li><li><strong>Oracle</strong>：使用更强的语言模型（例如 GPT‑4 或 Llama‑70B）作为分类器或语义等价判断器。</li><li><strong>实验数据</strong>：在论文正文中未提供详细数据集描述，附录 B 中列有用于内容安全、VLA 动作与语义聚类的图像‑提示对（来源于公开的 VLM 评测集），并对每个样本进行多次噪声采样。</li><li><strong>计算资源</strong>：实验基于多卡 GPU 集群执行 VLM 推理，oracle 的调用使用相同的 GPU 或 CPU 后端。由于模型已预训练，计算开销主要是多次前向推理（每张图像 \\(n\\) 次），具体硬件规格未在正文中给出。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：在标准机器学习服务器上运行（Python + PyTorch），所有 VLM 与 oracle 均通过相同的推理框架进行调用，实验代码已开源。</li><li><strong>鲁棒性指标</strong></li></ul>\n<p> - <strong>可验证半径 \\(R\\)</strong>：在给定置信度 \\(\\alpha\\) 下，对每个输入返回的最大半径。</p>\n<p> - <strong>认证精度</strong>：在特定半径阈值（如 \\(R=0.5\\)）下，能够给出非“ABSTAIN”结果的样本比例。</p>\n<p> - <strong>半径下降比例</strong>：通过标度律或实际测量比较不同样本数 \\(n\\) 下的平均半径 \\(\\bar R_\\sigma(\\alpha,n)\\)。</p>\n<p> - <strong>Oracle 误差敏感性</strong>：评估在不同假设的 \\(\\epsilon\\)（oracle 错误率）下半径下界的变化。</p>\n<ul><li><strong>对抗评估</strong></li></ul>\n<p> - 对最新的 jailbreak‑style 攻击（Qi et al., 2024）进行防御测试，统计攻击成功率的降低程度。</p>\n<p> - 与未加噪声的原始 VLM 对比，展示 RS 增强后的安全性提升。</p>\n<ul><li><strong>样本效率</strong></li></ul>\n<p> - 记录在不同样本规模（如 \\(n=100, 500, 1000, 5000\\)）下的认证半径、认证精度以及所需的计算时间。</p>\n<p> - 通过经验曲线验证理论标度律 \\(1-1.64z_\\alpha/\\sqrt n\\) 的准确性。</p>\n\n<p>以上内容概括了论文的研究单位、整体概述、主要贡献、方法细节、实验使用的数据与资源以及评估环境和指标。</p>"
  },
  {
    "date": "2025-09-19",
    "title": "CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine",
    "link": "http://arxiv.org/abs/2509.15968",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-19",
    "title": "A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning",
    "link": "http://arxiv.org/abs/2509.15937",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-18",
    "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
    "link": "http://arxiv.org/abs/2509.15212",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-18",
    "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue",
    "link": "http://arxiv.org/abs/2509.15061",
    "summary_markdown": "## 论文研究单位\n复旦大学计算机科学与人工智能学院\n上海创新研究院\n加州大学伯克利分校机械系统控制实验室（UC Berkeley Mechanical Systems Control Lab）\n## 论文概述\n提出 Ask-to-Clarify 框架以解决真实环境中指令歧义性。框架首先通过多轮对话主动向用户提问澄清歧义，然后端到端生成低层动作执行具体任务。框架由协作组件（VLM）与动作组件（扩散专家）构成，并通过连接模块与两阶段“知识隔离”训练策略实现协作与动作生成能力的融合。在推理时通过信号检测器实现“提问/动作”模式的无缝切换。\n## 论文核心贡献点\n- 提出任务与框架：要求具身智能体先通过多轮问答澄清歧义，再执行指令；采用 VLM 协作与扩散动作专家的组合，并设计连接模块以在两者间建立可靠条件。\n- 两阶段知识隔离训练策略：先学习歧义澄清与交互，再学习端到端低层动作；通过冻结 VLM 防止灾难性遗忘，同时用连接模块补偿 VLM 与扩散模型的联动。\n- 真实世界实验与验证：在 8 项真实任务上对比多种 SOTA VLA，表明该框架显著提升在歧义指令与复杂场景下的成功率与鲁棒性。\n## 论文方法描述\n- 任务定义：对给定视觉观察与歧义指令，Agent 依序生成澄清问题并接收回答；若干轮后推断正确指令，然后生成低层动作序列完成任务。\n- 框架结构\n - 协作组件（VLM，Qwen2-VL-2B）：负责提问、判断歧义与推导正确指令。\n - 连接模块（FiLM）：以语言指令为条件对视觉观察进行特征调制，为动作专家提供更具区分性的条件。\n - 动作组件（扩散专家，ScaleDP-Huge）：端到端生成低层动作（动作块长度 50）。\n - 信号检测器：解析 VLM 输出末端的信号token（<AMBG>、<NOT_AMBG>、<ACT>、<REJ>），以无训练方式路由“提问/动作”状态。\n- 两阶段训练\n - Stage 1：使用歧义交互对话数据训练协作组件；冻结视觉编码器，微调 LLM，新增信号token用于区分歧义、行动/拒绝等状态。\n - Stage 2：冻结协作组件与视觉编码器，训练连接模块与扩散动作专家；采用专家演示的具身数据进行端到端动作生成。\n- 推理流程\n - 若指令被判为歧义则输出 <AMBG> 并生成问题；根据用户回答更新对话历史。\n - 多轮后由 VLM 推断正确指令并标记 <NOT_AMBG>；随后由检测器触发 <ACT>（目标可见则执行）或 <REJ>（目标不可见则拒绝）。\n## 论文使用数据集和训练资源\n- 机器人与设备：xArm 7（7 DoF + 1 DoF 夹爪）、RealSense D435 相机（手腕与第三视角），xArm Python SDK；Stage 2 采用 Meta Quest 3 进行遥操作采集演示。\n- 数据\n - Stage 1（对话数据）：收集多对象图像并由 Qwen3-235B-A22B 生成歧义指令、问答对与正确指令，构成交互对话集。\n - Stage 2（具身演示）：8 项任务，每项约 10 条演示。\n- 实现细节\n - 协作组件：Qwen2-VL-2B-Instruct\n - 动作组件：ScaleDP-Huge（扩散专家）\n - 训练超参数（示例）\n - Stage 1：学习率 1e-5，批量 128，50 轮，训练参数约 1.5B\n - Stage 2：学习率 2e-5，批量 64，40 轮，训练参数约 978M\n - 动作块长度：50 步\n## 论文使用的评估环境和评估指标\n- 环境与任务：xArm 7 真实场景下 8 项任务，含三类通用任务：\n - Put the Object on the plate（Apple/Peach/Orange）\n - Pour the water from the Color cup onto the plate（Red/Green/White）\n - Stack the Color1 block on top of the Color2 block（(Blue, Yellow)、(Yellow, Blue)）\n- 指标：成功率（每任务 20 次试验）；与基线对比（π0、π0-FAST、OpenVLA-OFT）；消融（知识隔离策略、连接模块、必要性）。\n- 附加评估\n - 协作能力：在“目标存在/不存在”场景下判断并执行的正确率。\n - 鲁棒性：低光照（灯光减半）、视觉干扰（相似干扰物）条件下的成功率。\n\n结果表明：Ask-to-Clarify 在全部 8 项任务显著优于基线；在低光照与干扰条件下仍保持较高成功率；两阶段训练与连接模块二者缺一不可，共同保证歧义澄清与端到端动作生成的综合能力。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>复旦大学计算机科学与人工智能学院</p>\n<p>上海创新研究院</p>\n<p>加州大学伯克利分校机械系统控制实验室（UC Berkeley Mechanical Systems Control Lab）</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>提出 Ask-to-Clarify 框架以解决真实环境中指令歧义性。框架首先通过多轮对话主动向用户提问澄清歧义，然后端到端生成低层动作执行具体任务。框架由协作组件（VLM）与动作组件（扩散专家）构成，并通过连接模块与两阶段“知识隔离”训练策略实现协作与动作生成能力的融合。在推理时通过信号检测器实现“提问/动作”模式的无缝切换。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出任务与框架：要求具身智能体先通过多轮问答澄清歧义，再执行指令；采用 VLM 协作与扩散动作专家的组合，并设计连接模块以在两者间建立可靠条件。</li><li>两阶段知识隔离训练策略：先学习歧义澄清与交互，再学习端到端低层动作；通过冻结 VLM 防止灾难性遗忘，同时用连接模块补偿 VLM 与扩散模型的联动。</li><li>真实世界实验与验证：在 8 项真实任务上对比多种 SOTA VLA，表明该框架显著提升在歧义指令与复杂场景下的成功率与鲁棒性。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li>任务定义：对给定视觉观察与歧义指令，Agent 依序生成澄清问题并接收回答；若干轮后推断正确指令，然后生成低层动作序列完成任务。</li><li>框架结构</li></ul>\n<p> - 协作组件（VLM，Qwen2-VL-2B）：负责提问、判断歧义与推导正确指令。</p>\n<p> - 连接模块（FiLM）：以语言指令为条件对视觉观察进行特征调制，为动作专家提供更具区分性的条件。</p>\n<p> - 动作组件（扩散专家，ScaleDP-Huge）：端到端生成低层动作（动作块长度 50）。</p>\n<p> - 信号检测器：解析 VLM 输出末端的信号token（<AMBG>、<NOT_AMBG>、<ACT>、<REJ>），以无训练方式路由“提问/动作”状态。</p>\n<ul><li>两阶段训练</li></ul>\n<p> - Stage 1：使用歧义交互对话数据训练协作组件；冻结视觉编码器，微调 LLM，新增信号token用于区分歧义、行动/拒绝等状态。</p>\n<p> - Stage 2：冻结协作组件与视觉编码器，训练连接模块与扩散动作专家；采用专家演示的具身数据进行端到端动作生成。</p>\n<ul><li>推理流程</li></ul>\n<p> - 若指令被判为歧义则输出 <AMBG> 并生成问题；根据用户回答更新对话历史。</p>\n<p> - 多轮后由 VLM 推断正确指令并标记 <NOT_AMBG>；随后由检测器触发 <ACT>（目标可见则执行）或 <REJ>（目标不可见则拒绝）。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li>机器人与设备：xArm 7（7 DoF + 1 DoF 夹爪）、RealSense D435 相机（手腕与第三视角），xArm Python SDK；Stage 2 采用 Meta Quest 3 进行遥操作采集演示。</li><li>数据</li></ul>\n<p> - Stage 1（对话数据）：收集多对象图像并由 Qwen3-235B-A22B 生成歧义指令、问答对与正确指令，构成交互对话集。</p>\n<p> - Stage 2（具身演示）：8 项任务，每项约 10 条演示。</p>\n<ul><li>实现细节</li></ul>\n<p> - 协作组件：Qwen2-VL-2B-Instruct</p>\n<p> - 动作组件：ScaleDP-Huge（扩散专家）</p>\n<p> - 训练超参数（示例）</p>\n<p> - Stage 1：学习率 1e-5，批量 128，50 轮，训练参数约 1.5B</p>\n<p> - Stage 2：学习率 2e-5，批量 64，40 轮，训练参数约 978M</p>\n<p> - 动作块长度：50 步</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li>环境与任务：xArm 7 真实场景下 8 项任务，含三类通用任务：</li></ul>\n<p> - Put the Object on the plate（Apple/Peach/Orange）</p>\n<p> - Pour the water from the Color cup onto the plate（Red/Green/White）</p>\n<p> - Stack the Color1 block on top of the Color2 block（(Blue, Yellow)、(Yellow, Blue)）</p>\n<ul><li>指标：成功率（每任务 20 次试验）；与基线对比（π0、π0-FAST、OpenVLA-OFT）；消融（知识隔离策略、连接模块、必要性）。</li><li>附加评估</li></ul>\n<p> - 协作能力：在“目标存在/不存在”场景下判断并执行的正确率。</p>\n<p> - 鲁棒性：低光照（灯光减半）、视觉干扰（相似干扰物）条件下的成功率。</p>\n\n<p>结果表明：Ask-to-Clarify 在全部 8 项任务显著优于基线；在低光照与干扰条件下仍保持较高成功率；两阶段训练与连接模块二者缺一不可，共同保证歧义澄清与端到端动作生成的综合能力。</p>"
  },
  {
    "date": "2025-09-18",
    "title": "Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale",
    "link": "http://arxiv.org/abs/2509.14932",
    "summary_markdown": "# Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale\n## 论文研究单位\n- Department of Computer Science & Artificial Intelligence, University of Technology Nuremberg, Germany\n- Learning, Adaptive Systems and Robotics (LASR) Lab, Faculty of Computer Science, TU Dresden, Germany\n- Siemens Foundational Technologies, Siemens AG, Germany\n- Chair for Robotics, Artificial Intelligence and Real-Time Systems, TUM School of Computation, Information and Technology, Technical University of Munich, Germany\n## 论文概述\n论文提出了Robot Control Stack (RCS)，一个为大规模机器人学习设计的轻量级生态系统。随着Vision-Language-Action models (VLAs)的发展，传统机器人软件框架成为瓶颈，RCS旨在弥合这一差距。RCS采用模块化分层架构，为模拟和物理机器人提供统一接口，促进sim-to-real转换，在保持最小依赖和轻量级设计的同时，提供完整功能集，支持真实世界实验和大规模模拟训练。\n## 论文核心贡献点\n1. 提出基于环境包装器的RCS架构，支持在不同抽象级别轻松添加新功能，同时支持Python和C++\n2. 在常见用例上评估RCS，包括跨实体支持、模拟和真实环境中的训练数据收集、VLA和RL智能体的训练和评估\n3. 在可重现的取物任务上对Octo、OpenVLA和π₀进行广泛实验，涵盖多种不同机器人\n4. 展示将合成数据与真实数据混合可以显著提升π₀在真实世界中的性能\n## 论文方法描述\nRCS基于环境包装器概念设计，通过包装器元组W=⟨f:S→S′,g:A′→A,P′,R′⟩将状态和动作从马尔可夫决策过程(MDP)进行转换。架构包含：\n- C++底层接口定义抽象机器人控制函数，支持Python绑定\n- 场景包装器序列，可变异或观察环境动作和观察空间\n- 硬件抽象：标准化传感器和执行器接口，支持同步/异步操作\n- 仿真集成：基于MuJoCo，提供面向对象的场景视图和回调机制\n- 机器人工具包：集成Pinocchio进行运动学计算，OMPL用于运动规划\n- 数字孪生：实时运行仿真作为安全检查器\n- Agents应用层：通过RPC通信解决VLA策略依赖冲突\n## 论文使用数据集和训练资源\n- **真实数据集**：FR3 (143个演示)、xArm7 (100个演示)、UR5e (167个演示)、SO101 (120个演示)，均为30Hz频率收集\n- **仿真数据集**：3000个脚本化演示，成功率73%，生成2193个成功演示\n- **任务设置**：Pick-Cuboid任务，要求抓取绿色3D打印立方体\n- **训练资源**：消费者级GPU笔记本电脑用于脚本化仿真数据生成，Nvidia RTX 4080 + 12核CPU用于RL训练\n## 论文使用的评估环境和评估指标\n**评估环境**：\n- 四个真实机器人设置：FR3、xArm7、UR5e、SO101\n- 匹配MuJoCo仿真环境，复制FR3设置\n- 支持多种传感器：RealSense摄像头、DIGIT触觉传感器、Tacto触觉传感器\n\n**评估指标**：\n- **成功率**：50次真实世界 rollout 的抓取成功百分比\n- **跨VLA模型比较**：在30Hz和5Hz操作下比较Octo、OpenVLA、π₀\n- **Sim-to-Real评估**：使用SIMPLER方法评估真实到仿真的域转移\n- **数据混合实验**：混合真实和仿真数据对性能的影响\n- **RL评估**：训练3小时内(8.5M环境步骤)达到100%成功率，吞吐量>2000步/秒(24个并行环境)",
    "summary_html": "<h1>Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>Department of Computer Science & Artificial Intelligence, University of Technology Nuremberg, Germany</li><li>Learning, Adaptive Systems and Robotics (LASR) Lab, Faculty of Computer Science, TU Dresden, Germany</li><li>Siemens Foundational Technologies, Siemens AG, Germany</li><li>Chair for Robotics, Artificial Intelligence and Real-Time Systems, TUM School of Computation, Information and Technology, Technical University of Munich, Germany</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文提出了Robot Control Stack (RCS)，一个为大规模机器人学习设计的轻量级生态系统。随着Vision-Language-Action models (VLAs)的发展，传统机器人软件框架成为瓶颈，RCS旨在弥合这一差距。RCS采用模块化分层架构，为模拟和物理机器人提供统一接口，促进sim-to-real转换，在保持最小依赖和轻量级设计的同时，提供完整功能集，支持真实世界实验和大规模模拟训练。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出基于环境包装器的RCS架构，支持在不同抽象级别轻松添加新功能，同时支持Python和C++</li><li>在常见用例上评估RCS，包括跨实体支持、模拟和真实环境中的训练数据收集、VLA和RL智能体的训练和评估</li><li>在可重现的取物任务上对Octo、OpenVLA和π₀进行广泛实验，涵盖多种不同机器人</li><li>展示将合成数据与真实数据混合可以显著提升π₀在真实世界中的性能</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>RCS基于环境包装器概念设计，通过包装器元组W=⟨f:S→S′,g:A′→A,P′,R′⟩将状态和动作从马尔可夫决策过程(MDP)进行转换。架构包含：</p>\n<ul><li>C++底层接口定义抽象机器人控制函数，支持Python绑定</li><li>场景包装器序列，可变异或观察环境动作和观察空间</li><li>硬件抽象：标准化传感器和执行器接口，支持同步/异步操作</li><li>仿真集成：基于MuJoCo，提供面向对象的场景视图和回调机制</li><li>机器人工具包：集成Pinocchio进行运动学计算，OMPL用于运动规划</li><li>数字孪生：实时运行仿真作为安全检查器</li><li>Agents应用层：通过RPC通信解决VLA策略依赖冲突</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>真实数据集</strong>：FR3 (143个演示)、xArm7 (100个演示)、UR5e (167个演示)、SO101 (120个演示)，均为30Hz频率收集</li><li><strong>仿真数据集</strong>：3000个脚本化演示，成功率73%，生成2193个成功演示</li><li><strong>任务设置</strong>：Pick-Cuboid任务，要求抓取绿色3D打印立方体</li><li><strong>训练资源</strong>：消费者级GPU笔记本电脑用于脚本化仿真数据生成，Nvidia RTX 4080 + 12核CPU用于RL训练</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p><strong>评估环境</strong>：</p>\n<ul><li>四个真实机器人设置：FR3、xArm7、UR5e、SO101</li><li>匹配MuJoCo仿真环境，复制FR3设置</li><li>支持多种传感器：RealSense摄像头、DIGIT触觉传感器、Tacto触觉传感器</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li><strong>成功率</strong>：50次真实世界 rollout 的抓取成功百分比</li><li><strong>跨VLA模型比较</strong>：在30Hz和5Hz操作下比较Octo、OpenVLA、π₀</li><li><strong>Sim-to-Real评估</strong>：使用SIMPLER方法评估真实到仿真的域转移</li><li><strong>数据混合实验</strong>：混合真实和仿真数据对性能的影响</li><li><strong>RL评估</strong>：训练3小时内(8.5M环境步骤)达到100%成功率，吞吐量>2000步/秒(24个并行环境)</li></ul>"
  },
  {
    "date": "2025-09-18",
    "title": "CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human",
    "link": "http://arxiv.org/abs/2509.14889",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-18",
    "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI",
    "link": "http://arxiv.org/abs/2509.14687",
    "summary_markdown": "# 论文研究单位\n\n中兴通讯股份有限公司，中国；香港中文大学（深圳），中国\n# 论文概述\n\nRealMirror是一个综合性、开源的具身AI视觉-语言-动作（VLA）平台，专门针对人形机器人VLA研究设计。该平台旨在解决当前具身AI研究中的关键瓶颈：数据采集成本高、缺乏标准化基准测试、以及仿真与现实世界之间的显著差距。RealMirror提供了一个端到端的解决方案，涵盖数据采集、模型训练、模型推理和性能评估的完整流程。\n# 论文核心贡献点\n\n1. 构建了一个高效、低成本的数据采集、模型训练和模型推理系统，实现了端到端VLA研究，无需真实机器人参与\n\n2. 提出专门针对人形机器人的VLA基准测试，通过多场景和多种VLA模型的广泛实验促进模型演进和公平比较\n\n3. 通过集成生成模型和3D高斯溅射技术，展示了零样本Sim2Real的可行性，使仅在仿真数据上训练的模型能够在真实机器人上无缝执行任务，无需任何微调\n# 论文方法描述\n## 物理仿真场景构建\n基于NVIDIA Isaac Sim平台构建多样化的室内仿真环境，整合CAD模型和来自各种资产库的资产，分配适当的物理属性（质量、摩擦、碰撞参数等），确保仿真中的合理性和人形机器人实体的兼容性。\n## 数据采集系统\n开发了基于远程操作的数据采集系统，包含两个主要组件：\n- 运动控制管线：实现多级滤波机制，包括IK关节控制跳跃滤波、末端执行器位姿通信和漂移补偿、IK求解器阈值滤波、跨帧末端执行器位姿阈值滤波\n- 轻量级WebXR通信系统：实现90 Hz传输频率，与通用通信框架相比减少了114毫秒的端到端延迟\n## 统一训练和推理框架\n支持多种代表性VLA模型（ACT、Diffusion Policy、SmolVLA），集成时间集成机制以增强动作预测稳健性，基于LeRobot库扩展以支持人形机器人实体，与Isaac Sim深度集成实现交互式评估。\n## Sim2Real转换框架\n采用多管齐下的策略：\n- 静态环境渲染：使用3D高斯溅射从多个视角捕获目标真实世界工作空间，重建整个静态场景\n- 高保真关节机器人模型：使用3D高斯溅射重建物理人形机器人并分割为单独连杆，通过S、R、T变换与Isaac Sim中的USD模型对齐\n- 交互对象差异化处理：高精度对象采用数字孪生方法，低精度对象使用少样本3D生成模型\n- 坐标系对齐和相机校准：使用ICP算法对齐CAD资产与3DGS重建环境，通过SfM解决相机位姿\n# 论文使用数据集和训练资源\n## 数据集构成\n构建了高质量的人形机器人VLA数据集，包含五个任务场景，每个场景240条轨迹，总计超过1200条仿真轨迹：\n- Kitchen Cleanup（厨房清理）：Pick and Place、双臂协作\n- Air Fryer Manipulation（空气炸锅操作）：Pick and Place、推拉、双臂协作\n- Assembly Line Sorting（装配线分拣）：Pick and Place、双臂协作、动态抓取\n- Cup-to-Cup Transfer（杯间转移）：双臂协作、精密控制\n- Can Stacking（易拉罐叠放）：Pick and Place、精密控制\n## 训练资源\n- 硬件：PICO Neo3 Pro头戴设备和Ada5880工作站\n- 训练参数：每个模型训练100,000步，批次大小为16\n- 统一动作空间：26维（每条机械臂13维：7维机械臂+6维手部）\n# 论文使用的评估环境和评估指标\n## 评估环境\n- 仿真环境：基于Isaac Sim的交互式评估环境\n- 真实环境：ZHIYUAN A2机器人进行Sim2Real实验验证\n## 评估指标\n- 主要指标：任务成功率（Task Success Rate）\n- 评估规模：Kitchen Cleanup (400次试验)、Air Fryer Manipulation (400次试验)、Can Stacking (400次试验)、Cup-to-Cup Transfer (200次试验)、Assembly Line Sorting (100次试验)\n- 技能评估：从任务中抽象出五个核心机器人技能进行细粒度分析\n- Sim2Real评估：基本任务（拾取和放置）达到92.86%准确率，复杂任务（球体转移）达到71.43%准确率，无需微调即可实现真实世界部署\n## 实验结果\n三个代表性VLA模型在基准测试中的表现：\n- ACT：平均成功率73.55%，在Kitchen Cleanup和Assembly Line Sorting表现突出\n- Diffusion Policy：平均成功率75.15%，在Air Fryer Manipulation表现最佳\n- SmolVLA：平均成功率79.75%，整体表现最为稳健，特别是在精密控制任务中",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>中兴通讯股份有限公司，中国；香港中文大学（深圳），中国</p>\n<h1>论文概述</h1>\n\n<p>RealMirror是一个综合性、开源的具身AI视觉-语言-动作（VLA）平台，专门针对人形机器人VLA研究设计。该平台旨在解决当前具身AI研究中的关键瓶颈：数据采集成本高、缺乏标准化基准测试、以及仿真与现实世界之间的显著差距。RealMirror提供了一个端到端的解决方案，涵盖数据采集、模型训练、模型推理和性能评估的完整流程。</p>\n<h1>论文核心贡献点</h1>\n\n<ol><li>构建了一个高效、低成本的数据采集、模型训练和模型推理系统，实现了端到端VLA研究，无需真实机器人参与</li></ol>\n\n<ol><li>提出专门针对人形机器人的VLA基准测试，通过多场景和多种VLA模型的广泛实验促进模型演进和公平比较</li></ol>\n\n<ol><li>通过集成生成模型和3D高斯溅射技术，展示了零样本Sim2Real的可行性，使仅在仿真数据上训练的模型能够在真实机器人上无缝执行任务，无需任何微调</li></ol>\n<h1>论文方法描述</h1>\n<h2 class=\"section-title\">物理仿真场景构建</h2>\n<p>基于NVIDIA Isaac Sim平台构建多样化的室内仿真环境，整合CAD模型和来自各种资产库的资产，分配适当的物理属性（质量、摩擦、碰撞参数等），确保仿真中的合理性和人形机器人实体的兼容性。</p>\n<h2 class=\"section-title\">数据采集系统</h2>\n<p>开发了基于远程操作的数据采集系统，包含两个主要组件：</p>\n<ul><li>运动控制管线：实现多级滤波机制，包括IK关节控制跳跃滤波、末端执行器位姿通信和漂移补偿、IK求解器阈值滤波、跨帧末端执行器位姿阈值滤波</li><li>轻量级WebXR通信系统：实现90 Hz传输频率，与通用通信框架相比减少了114毫秒的端到端延迟</li></ul>\n<h2 class=\"section-title\">统一训练和推理框架</h2>\n<p>支持多种代表性VLA模型（ACT、Diffusion Policy、SmolVLA），集成时间集成机制以增强动作预测稳健性，基于LeRobot库扩展以支持人形机器人实体，与Isaac Sim深度集成实现交互式评估。</p>\n<h2 class=\"section-title\">Sim2Real转换框架</h2>\n<p>采用多管齐下的策略：</p>\n<ul><li>静态环境渲染：使用3D高斯溅射从多个视角捕获目标真实世界工作空间，重建整个静态场景</li><li>高保真关节机器人模型：使用3D高斯溅射重建物理人形机器人并分割为单独连杆，通过S、R、T变换与Isaac Sim中的USD模型对齐</li><li>交互对象差异化处理：高精度对象采用数字孪生方法，低精度对象使用少样本3D生成模型</li><li>坐标系对齐和相机校准：使用ICP算法对齐CAD资产与3DGS重建环境，通过SfM解决相机位姿</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<h2 class=\"section-title\">数据集构成</h2>\n<p>构建了高质量的人形机器人VLA数据集，包含五个任务场景，每个场景240条轨迹，总计超过1200条仿真轨迹：</p>\n<ul><li>Kitchen Cleanup（厨房清理）：Pick and Place、双臂协作</li><li>Air Fryer Manipulation（空气炸锅操作）：Pick and Place、推拉、双臂协作</li><li>Assembly Line Sorting（装配线分拣）：Pick and Place、双臂协作、动态抓取</li><li>Cup-to-Cup Transfer（杯间转移）：双臂协作、精密控制</li><li>Can Stacking（易拉罐叠放）：Pick and Place、精密控制</li></ul>\n<h2 class=\"section-title\">训练资源</h2>\n<ul><li>硬件：PICO Neo3 Pro头戴设备和Ada5880工作站</li><li>训练参数：每个模型训练100,000步，批次大小为16</li><li>统一动作空间：26维（每条机械臂13维：7维机械臂+6维手部）</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<h2 class=\"section-title\">评估环境</h2>\n<ul><li>仿真环境：基于Isaac Sim的交互式评估环境</li><li>真实环境：ZHIYUAN A2机器人进行Sim2Real实验验证</li></ul>\n<h2 class=\"section-title\">评估指标</h2>\n<ul><li>主要指标：任务成功率（Task Success Rate）</li><li>评估规模：Kitchen Cleanup (400次试验)、Air Fryer Manipulation (400次试验)、Can Stacking (400次试验)、Cup-to-Cup Transfer (200次试验)、Assembly Line Sorting (100次试验)</li><li>技能评估：从任务中抽象出五个核心机器人技能进行细粒度分析</li><li>Sim2Real评估：基本任务（拾取和放置）达到92.86%准确率，复杂任务（球体转移）达到71.43%准确率，无需微调即可实现真实世界部署</li></ul>\n<h2 class=\"section-title\">实验结果</h2>\n<p>三个代表性VLA模型在基准测试中的表现：</p>\n<ul><li>ACT：平均成功率73.55%，在Kitchen Cleanup和Assembly Line Sorting表现突出</li><li>Diffusion Policy：平均成功率75.15%，在Air Fryer Manipulation表现最佳</li><li>SmolVLA：平均成功率79.75%，整体表现最为稳健，特别是在精密控制任务中</li></ul>"
  },
  {
    "date": "2025-09-17",
    "title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping",
    "link": "http://arxiv.org/abs/2509.14143",
    "summary_markdown": "# 论文研究单位\n德雷塞尔大学电气与计算机工程系；弗吉尼亚理工大学海产农业研究与推广中心与生物系统工程系\n# 论文概述\n提出CLAW（CLIP-Language-Action for Weight）框架，用于“重量感知”的机器人抓取。核心思想是将条件评估从动作生成中解耦：由轻量的微调CLIP作为提示生成器，实时监测电子秤读数并产生离散语言提示（continue/stop）；π₀ 作为流匹配VLA策略接收多视角视觉与语言提示，生成连续控制信号。验证表明CLAW在单物体与混合物体场景中均可可靠执行重量约束抓取，并优于原始与仅微调的π₀。\n# 论文核心贡献点\n- 引入CLAW：在标准VLA之上增加任务专用VLM用于显式条件监控，实现重量感知操控。\n- 设计CLIP微调：将电子秤数字显示转换为可被VLA理解的离散提示。\n- 使用提示监督微调π₀：使其能融合CLIP提示与多视角观测，产生精确动作。\n- 跨单物体与混合任务（双臂）场景评估，显示鲁棒性与一致尊重重量阈值；对比基线（原始π₀与仅微调π₀）显著提升。\n# 论文方法描述\n- 整体架构\n - 输入：人类指令（指定对象与目标重量）、电子秤图像、场景多视角图像。\n - 模块1（CLIP）：对电子秤图像与指令生成二元提示 m_t ∈ {continue, stop}，参数化为 p_φ(m_t \\|o_t^scale, l)。\n - 模块2（π₀）：对场景图像与提示生成连续动作 a_t，参数化为 p_θ(a_t \\|o_t^scene, m_t)，采用流匹配生成30Hz控制（50步动作块）。\n - 频率：CLIP以20Hz更新，π₀以30Hz控制；当某步CLIP未更新时复用最近提示。\n- 训练流程\n - CLIP微调：采集2000张秤显示裁剪图；每张与N个“load k g target”指令配对，标签由真实读数 w* 与阈值k比较得到（y=continue当k<w*，否则y=stop），共2000N样本，最小化分类损失。\n - π₀微调：每任务收集50条演示（抓取与撤碗两阶段）；在演示中人工标注clip_prompt（抓取阶段为“continue…”，撤碗阶段为“stop…”），以流匹配损失最小化 \\|\\|v_θ(x(t),t,o_t^scene,m_t)−(a_t−x(t))\\|\\|^2，在H200上训练60,000步。\n- 推理运行\n - CLIP实时监测秤并输出continue/stop提示；π₀据提示与场景多视角生成动作块；重量达标后提示切换为“stop”，π₀执行撤碗。\n# 论文使用数据集和训练资源\n- 数据集\n - CLIP微调：2000张电子秤显示裁剪图，每图与N个阈值指令配对，生成2000N条continue/stop样本。\n - π₀微调：每任务50条演示，覆盖抓取与撤碗阶段；演示中帧级标注“clip_prompt”。\n- 训练资源\n - π₀微调：H200 GPU；60,000步；控制30Hz；CLIP提示更新20Hz。\n - CLIP微调：硬件未详述。\n# 论文使用的评估环境和评估指标\n- 评估环境\n - 单物体：桌面设含装目标物的篮子、电子秤与空碗；任务为“抓取至目标重量并撤碗”，覆盖糖果（20/30/40g）与大蒜（20/30/40g）。\n - 混合物体：左右各置一盒（糖果/大蒜），左右臂分工（抓取与撤碗不同对象），评估跨对象与双臂协调。\n - 鲁棒性测试：在抓取中途投加过量物体，使秤瞬时超过阈值，检验系统是否能即时切换为撤碗并中断未完成动作。\n- 评估指标\n - 成功率（20次试验）：分别统计“动作完成”（抓取与撤碗）与“停止点准确”（达到指定重量阈值即停止），对比原始π₀、仅微调π₀与CLAW。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>德雷塞尔大学电气与计算机工程系；弗吉尼亚理工大学海产农业研究与推广中心与生物系统工程系</p>\n<h1>论文概述</h1>\n<p>提出CLAW（CLIP-Language-Action for Weight）框架，用于“重量感知”的机器人抓取。核心思想是将条件评估从动作生成中解耦：由轻量的微调CLIP作为提示生成器，实时监测电子秤读数并产生离散语言提示（continue/stop）；π₀ 作为流匹配VLA策略接收多视角视觉与语言提示，生成连续控制信号。验证表明CLAW在单物体与混合物体场景中均可可靠执行重量约束抓取，并优于原始与仅微调的π₀。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>引入CLAW：在标准VLA之上增加任务专用VLM用于显式条件监控，实现重量感知操控。</li><li>设计CLIP微调：将电子秤数字显示转换为可被VLA理解的离散提示。</li><li>使用提示监督微调π₀：使其能融合CLIP提示与多视角观测，产生精确动作。</li><li>跨单物体与混合任务（双臂）场景评估，显示鲁棒性与一致尊重重量阈值；对比基线（原始π₀与仅微调π₀）显著提升。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>整体架构</li></ul>\n<p> - 输入：人类指令（指定对象与目标重量）、电子秤图像、场景多视角图像。</p>\n<p> - 模块1（CLIP）：对电子秤图像与指令生成二元提示 m_t ∈ {continue, stop}，参数化为 p_φ(m_t \\|o_t^scale, l)。</p>\n<p> - 模块2（π₀）：对场景图像与提示生成连续动作 a_t，参数化为 p_θ(a_t \\|o_t^scene, m_t)，采用流匹配生成30Hz控制（50步动作块）。</p>\n<p> - 频率：CLIP以20Hz更新，π₀以30Hz控制；当某步CLIP未更新时复用最近提示。</p>\n<ul><li>训练流程</li></ul>\n<p> - CLIP微调：采集2000张秤显示裁剪图；每张与N个“load k g target”指令配对，标签由真实读数 w* 与阈值k比较得到（y=continue当k<w*，否则y=stop），共2000N样本，最小化分类损失。</p>\n<p> - π₀微调：每任务收集50条演示（抓取与撤碗两阶段）；在演示中人工标注clip_prompt（抓取阶段为“continue…”，撤碗阶段为“stop…”），以流匹配损失最小化 \\|\\|v_θ(x(t),t,o_t^scene,m_t)−(a_t−x(t))\\|\\|^2，在H200上训练60,000步。</p>\n<ul><li>推理运行</li></ul>\n<p> - CLIP实时监测秤并输出continue/stop提示；π₀据提示与场景多视角生成动作块；重量达标后提示切换为“stop”，π₀执行撤碗。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集</li></ul>\n<p> - CLIP微调：2000张电子秤显示裁剪图，每图与N个阈值指令配对，生成2000N条continue/stop样本。</p>\n<p> - π₀微调：每任务50条演示，覆盖抓取与撤碗阶段；演示中帧级标注“clip_prompt”。</p>\n<ul><li>训练资源</li></ul>\n<p> - π₀微调：H200 GPU；60,000步；控制30Hz；CLIP提示更新20Hz。</p>\n<p> - CLIP微调：硬件未详述。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境</li></ul>\n<p> - 单物体：桌面设含装目标物的篮子、电子秤与空碗；任务为“抓取至目标重量并撤碗”，覆盖糖果（20/30/40g）与大蒜（20/30/40g）。</p>\n<p> - 混合物体：左右各置一盒（糖果/大蒜），左右臂分工（抓取与撤碗不同对象），评估跨对象与双臂协调。</p>\n<p> - 鲁棒性测试：在抓取中途投加过量物体，使秤瞬时超过阈值，检验系统是否能即时切换为撤碗并中断未完成动作。</p>\n<ul><li>评估指标</li></ul>\n<p> - 成功率（20次试验）：分别统计“动作完成”（抓取与撤碗）与“停止点准确”（达到指定重量阈值即停止），对比原始π₀、仅微调π₀与CLAW。</p>"
  },
  {
    "date": "2025-09-17",
    "title": "SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2509.14138",
    "summary_markdown": "### 论文研究单位\n- **Virginia Seafood Agricultural Research and Extension Center, and Department of Biological Systems Engineering, Virginia Tech, USA**\n- **Department of Electrical and Computer Engineering, Drexel University, USA**\n### 论文概述\n长时序机器人操作任务要求执行多个相互依赖的子任务，错误检测机制可能引发级联失效。现有视觉-语言-动作（VLA）模型（如π₀）在连续低级控制方面表现出色，但缺乏识别子任务完成的内部信号，在顺序设置中表现脆弱。本研究提出SeqVLA，它是π₀的完成感知扩展，通过添加轻量级检测头感知当前子任务是否完成。该双头设计使模型不仅生成操作动作，还能自主触发子任务转换。研究调研了四种微调策略（联合或顺序微调，冻结或不冻结预训练主干网）。实验在沙拉包装（七个连续子任务）和糖果包装（四个子任务）上进行。结果显示SeqVLA显著提升整体成功率，其中联合微调并冻结主干网策略表现最佳，消除了顺序相关失效。\n### 论文核心贡献点\n- 集成学习任务完成检测头到π₀模型，实现从多模态上下文中推断子任务完成。\n- 识别最有效的微调策略：联合微调并冻结主干网，确保可靠顺序执行。\n- 在两个实际长时序场景中评估，框架在任务级性能上显著优于强基线。\n### 论文方法描述\n- **问题建模**：长时序任务表示为顺序子任务序列$\\mathcal{T}=\\{\\mathcal{T}_{1}, \\mathcal{T}_{2}, ..., \\mathcal{T}_{n}\\}$，子任务$\\mathcal{T}_{i}$的完成是启动$\\mathcal{T}_{i+1}$的先决条件。\n- **架构**：SeqVLA扩展π₀架构（基于SigLIP视觉编码器、Gemma-2B语言主干网和Gemma-300M动作专家），添加共享动作专家特征的轻量级完成检测头（线性分类器），输出子任务完成概率$p$：\n - $p = \\sigma(\\textbf{W} \\cdot \\textbf{F} + b)$，其中$\\textbf{W} \\in \\mathbb{R}^{1024}$, $b \\in \\mathbb{R}$为参数，$\\sigma$为sigmoid函数。\n - 总损失函数：$L_{\\text{total}} = L_{\\text{action}} + \\lambda \\cdot L_{\\text{completion}}$，$L_{\\text{completion}}$为二分类交叉熵损失，权重$\\lambda=0.1$。\n- **微调策略**：\n - **联合微调**：动作和分类头同时优化。\n - **顺序微调**：先训练动作头和主干网，后训练分类头。\n - **冻结策略**：冻结预训练VLM主干网以保留原始知识，或全微调适应域。\n - 组合为四种配置：SeqVLA-J（联合微调，不冻结）、SeqVLA-JF（联合微调，冻结）、SeqVLA-S（顺序微调，不冻结）、SeqVLA-SF（顺序微调，冻结）。\n- **子任务执行**：每个推理步骤输出动作块和执行概率$p$。当$p < \\tau = 0.2$时触发转换：停止当前动作、回程家姿态、切换到下一子任务提示。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **沙拉包装**：七个连续子任务（菠菜、卷心菜、肉丸、鸡、西红柿、酱料杯、容器关闭），收集350集子任务演示数据。\n - **糖果包装**：四个子任务（软糖、两次Kinder巧克力、两次士力架、棒棒糖），收集200集子任务演示数据。\n - 附加长时序演示数据：完整沙拉和糖果任务轨迹用于π₀基线微调。\n- **数据收集**：使用Aloha双机械臂（14自由度）进行示教，三摄像头（顶视、左抓手、右抓手）记录RGB图像和机器人状态。\n- **训练资源**：\n - 基于π₀预训练模型（物理智能版），硬件为Aloha机器人。\n - 训练环境涉及实时数据采集和微调流程，具体计算资源未详述。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 机器人：Aloha双机械臂（14自由度），三摄像头视角。\n - 任务：沙拉包装（七子任务）和糖果包装（四子任务）的长时序执行。\n - 基线：π₀模型在完整长时序演示上微调，直接执行无子任务监控。\n- **评估指标**：\n - **成功率**：整体任务和子任务级成功率（如图表显示）。\n - **分类置信度**：使用熵值衡量完成检测的不确定性（SeqVLA-J熵0.76 vs SeqVLA-S熵1.35）。\n - **统计可靠性**：Kolmogorov–Smirnov（KS）统计量评估执行和完成阶段分布差异（KS值0.75–0.85，p < 0.001）。\n - **行为比较**：通过执行记录图（图8、9）定性分析π₀顺序失效vs SeqVLA可靠性。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li><strong>Virginia Seafood Agricultural Research and Extension Center, and Department of Biological Systems Engineering, Virginia Tech, USA</strong></li><li><strong>Department of Electrical and Computer Engineering, Drexel University, USA</strong></li></ul>\n<h3>论文概述</h3>\n<p>长时序机器人操作任务要求执行多个相互依赖的子任务，错误检测机制可能引发级联失效。现有视觉-语言-动作（VLA）模型（如π₀）在连续低级控制方面表现出色，但缺乏识别子任务完成的内部信号，在顺序设置中表现脆弱。本研究提出SeqVLA，它是π₀的完成感知扩展，通过添加轻量级检测头感知当前子任务是否完成。该双头设计使模型不仅生成操作动作，还能自主触发子任务转换。研究调研了四种微调策略（联合或顺序微调，冻结或不冻结预训练主干网）。实验在沙拉包装（七个连续子任务）和糖果包装（四个子任务）上进行。结果显示SeqVLA显著提升整体成功率，其中联合微调并冻结主干网策略表现最佳，消除了顺序相关失效。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>集成学习任务完成检测头到π₀模型，实现从多模态上下文中推断子任务完成。</li><li>识别最有效的微调策略：联合微调并冻结主干网，确保可靠顺序执行。</li><li>在两个实际长时序场景中评估，框架在任务级性能上显著优于强基线。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>问题建模</strong>：长时序任务表示为顺序子任务序列$\\mathcal{T}=\\{\\mathcal{T}_{1}, \\mathcal{T}_{2}, ..., \\mathcal{T}_{n}\\}$，子任务$\\mathcal{T}_{i}$的完成是启动$\\mathcal{T}_{i+1}$的先决条件。</li><li><strong>架构</strong>：SeqVLA扩展π₀架构（基于SigLIP视觉编码器、Gemma-2B语言主干网和Gemma-300M动作专家），添加共享动作专家特征的轻量级完成检测头（线性分类器），输出子任务完成概率$p$：</li></ul>\n<p> - $p = \\sigma(\\textbf{W} \\cdot \\textbf{F} + b)$，其中$\\textbf{W} \\in \\mathbb{R}^{1024}$, $b \\in \\mathbb{R}$为参数，$\\sigma$为sigmoid函数。</p>\n<p> - 总损失函数：$L_{\\text{total}} = L_{\\text{action}} + \\lambda \\cdot L_{\\text{completion}}$，$L_{\\text{completion}}$为二分类交叉熵损失，权重$\\lambda=0.1$。</p>\n<ul><li><strong>微调策略</strong>：</li></ul>\n<p> - <strong>联合微调</strong>：动作和分类头同时优化。</p>\n<p> - <strong>顺序微调</strong>：先训练动作头和主干网，后训练分类头。</p>\n<p> - <strong>冻结策略</strong>：冻结预训练VLM主干网以保留原始知识，或全微调适应域。</p>\n<p> - 组合为四种配置：SeqVLA-J（联合微调，不冻结）、SeqVLA-JF（联合微调，冻结）、SeqVLA-S（顺序微调，不冻结）、SeqVLA-SF（顺序微调，冻结）。</p>\n<ul><li><strong>子任务执行</strong>：每个推理步骤输出动作块和执行概率$p$。当$p < \\tau = 0.2$时触发转换：停止当前动作、回程家姿态、切换到下一子任务提示。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>沙拉包装</strong>：七个连续子任务（菠菜、卷心菜、肉丸、鸡、西红柿、酱料杯、容器关闭），收集350集子任务演示数据。</p>\n<p> - <strong>糖果包装</strong>：四个子任务（软糖、两次Kinder巧克力、两次士力架、棒棒糖），收集200集子任务演示数据。</p>\n<p> - 附加长时序演示数据：完整沙拉和糖果任务轨迹用于π₀基线微调。</p>\n<ul><li><strong>数据收集</strong>：使用Aloha双机械臂（14自由度）进行示教，三摄像头（顶视、左抓手、右抓手）记录RGB图像和机器人状态。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 基于π₀预训练模型（物理智能版），硬件为Aloha机器人。</p>\n<p> - 训练环境涉及实时数据采集和微调流程，具体计算资源未详述。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 机器人：Aloha双机械臂（14自由度），三摄像头视角。</p>\n<p> - 任务：沙拉包装（七子任务）和糖果包装（四子任务）的长时序执行。</p>\n<p> - 基线：π₀模型在完整长时序演示上微调，直接执行无子任务监控。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：整体任务和子任务级成功率（如图表显示）。</p>\n<p> - <strong>分类置信度</strong>：使用熵值衡量完成检测的不确定性（SeqVLA-J熵0.76 vs SeqVLA-S熵1.35）。</p>\n<p> - <strong>统计可靠性</strong>：Kolmogorov–Smirnov（KS）统计量评估执行和完成阶段分布差异（KS值0.75–0.85，p < 0.001）。</p>\n<p> - <strong>行为比较</strong>：通过执行记录图（图8、9）定性分析π₀顺序失效vs SeqVLA可靠性。</p>"
  },
  {
    "date": "2025-09-17",
    "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2509.14117",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-17",
    "title": "Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach",
    "link": "http://arxiv.org/abs/2509.13774",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n\n北京小米机器人技术有限公司（Beijing Xiaomi Robot Technology Co., Ltd.）和香港城市大学（City University of Hong Kong）\n## 论文概述\n\n本文提出了一种基于人机交互的双执行器（dual-actor）VLA模型微调框架。该框架集成了一个主执行器用于鲁棒的多任务性能，以及一个精调执行器在潜在空间中进行自适应调整。除了标准的物理干预外，论文引入了一种轻量级的talk-and-tweak方案，将人类修正转换为语义化的语言命令，从而生成新的策略学习数据集。在真实世界多任务实验中，该方法在101分钟的在线微调内在三个任务上实现了100%的成功率。对于长视距任务，在超过12个连续操作中保持了50%的成功率。该框架还能有效扩展到多机器人训练，使用双机器人时效率提升可达2倍。\n## 论文核心贡献点\n\n1. 提出了一种新颖的双执行器VLA微调框架，集成了用于鲁棒多任务策略生成的主执行器和在潜在噪声空间中操作以实现细粒度可控动作调整的精调执行器\n\n2. 开发了一种轻量级人机交互方案，将实时物理修正（tweak）转换为语义化的语言命令（talk），形成talk-and-tweak数据集。精调执行器利用这些指令进行可解释的调整，而主执行器通过直接干预改进其基线策略\n\n3. 在真实机器人上验证了所提方法，展示了快速的多任务微调能力（101分钟内达到100%成功率）。对于长视距序列，在12个连续操作中保持50%成功率。此外，框架可无缝扩展到多机器人训练，使用双机器人训练时效率提升达2倍\n## 论文方法描述\n\n方法分为三个主要部分：\n\n1. 双执行器强化学习系统：包含预训练的VLA模型进行任务和状态编码，主执行器采用一致性策略（consistency policy）生成动作，精调执行器在潜在噪声空间中进行调整。训练分为离线预热阶段和在线交互阶段，主执行器通过混合目标（行为克隆+Q函数最大化）优化，精调执行器通过行为克隆、Q函数最大化和正则化损失联合训练\n\n2. Talk-and-Tweak人类干预设计：将人类物理修正（通过SpaceMouse）自动转换为自然语言精调命令。通过计算时间窗口内的累积位移，当超过阈值时生成相应的方向命令（如\"向右移动\"），形成包含（状态、动作、精调命令）的三元组数据集\n\n3. 高效多任务学习：采用共享的多任务执行器和任务特定的critic架构。为每个任务维护三个独立缓冲区（专家演示、策略rollouts、人类干预）。在在线阶段，主执行器从所有任务和缓冲区类型中均匀采样更新，精调执行器使用聚合的干预数据集训练，每个任务特定的critic仅使用其对应任务缓冲区的数据更新。引入任务权重机制平衡多任务学习进度\n## 论文使用数据集和训练资源\n\n数据集：使用Octo作为骨干VLA模型。预热阶段每个任务使用20条轨迹初始化策略（约3000个状态-动作对）。在线微调阶段在三个任务中收集约15000个交互对（每个任务约100条轨迹）。人类干预数据约占15%，用于训练精调策略\n\n硬件和训练资源：使用自主开发的7自由度机械臂。观察包括两个RGB图像（手腕相机128×128分辨率，头部相机256×256分辨率）和机械臂本体感知状态。动作空间为7维末端执行器增量位姿。数据采集和策略执行频率为10Hz。执行器进程运行在机器人上的NVIDIA Jetson Orin，学习器在配备NVIDIA RTX 3090 GPU的工作站上执行\n## 论文使用的评估环境和评估指标\n\n评估环境：真实物理环境中的螺栓操作任务，包括三个子任务：(1)将螺栓竖直放置，(2)拾取螺栓，(3)组装螺栓。机器人和物体位置在x-y轴上均匀随机化±5厘米以增强泛化性\n\n评估指标：\n1. 成功率：每个子任务如果在50个时间步内完成则视为成功，超过此限制计为失败。每个子任务独立评估成功率\n2. 片段长度：完成任务所需的时间步数\n3. 长视距任务评估：完全成功需要机器人顺序完成所有三个子任务\n4. 所有结果均为25次试验的平均值\n5. 多机器人扩展性：评估在不同机器人和训练配置下的训练效率和任务性能",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n\n<p>北京小米机器人技术有限公司（Beijing Xiaomi Robot Technology Co., Ltd.）和香港城市大学（City University of Hong Kong）</p>\n<h2 class=\"section-title\">论文概述</h2>\n\n<p>本文提出了一种基于人机交互的双执行器（dual-actor）VLA模型微调框架。该框架集成了一个主执行器用于鲁棒的多任务性能，以及一个精调执行器在潜在空间中进行自适应调整。除了标准的物理干预外，论文引入了一种轻量级的talk-and-tweak方案，将人类修正转换为语义化的语言命令，从而生成新的策略学习数据集。在真实世界多任务实验中，该方法在101分钟的在线微调内在三个任务上实现了100%的成功率。对于长视距任务，在超过12个连续操作中保持了50%的成功率。该框架还能有效扩展到多机器人训练，使用双机器人时效率提升可达2倍。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n\n<ol><li>提出了一种新颖的双执行器VLA微调框架，集成了用于鲁棒多任务策略生成的主执行器和在潜在噪声空间中操作以实现细粒度可控动作调整的精调执行器</li></ol>\n\n<ol><li>开发了一种轻量级人机交互方案，将实时物理修正（tweak）转换为语义化的语言命令（talk），形成talk-and-tweak数据集。精调执行器利用这些指令进行可解释的调整，而主执行器通过直接干预改进其基线策略</li></ol>\n\n<ol><li>在真实机器人上验证了所提方法，展示了快速的多任务微调能力（101分钟内达到100%成功率）。对于长视距序列，在12个连续操作中保持50%成功率。此外，框架可无缝扩展到多机器人训练，使用双机器人训练时效率提升达2倍</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n\n<p>方法分为三个主要部分：</p>\n\n<ol><li>双执行器强化学习系统：包含预训练的VLA模型进行任务和状态编码，主执行器采用一致性策略（consistency policy）生成动作，精调执行器在潜在噪声空间中进行调整。训练分为离线预热阶段和在线交互阶段，主执行器通过混合目标（行为克隆+Q函数最大化）优化，精调执行器通过行为克隆、Q函数最大化和正则化损失联合训练</li></ol>\n\n<ol><li>Talk-and-Tweak人类干预设计：将人类物理修正（通过SpaceMouse）自动转换为自然语言精调命令。通过计算时间窗口内的累积位移，当超过阈值时生成相应的方向命令（如\"向右移动\"），形成包含（状态、动作、精调命令）的三元组数据集</li></ol>\n\n<ol><li>高效多任务学习：采用共享的多任务执行器和任务特定的critic架构。为每个任务维护三个独立缓冲区（专家演示、策略rollouts、人类干预）。在在线阶段，主执行器从所有任务和缓冲区类型中均匀采样更新，精调执行器使用聚合的干预数据集训练，每个任务特定的critic仅使用其对应任务缓冲区的数据更新。引入任务权重机制平衡多任务学习进度</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n\n<p>数据集：使用Octo作为骨干VLA模型。预热阶段每个任务使用20条轨迹初始化策略（约3000个状态-动作对）。在线微调阶段在三个任务中收集约15000个交互对（每个任务约100条轨迹）。人类干预数据约占15%，用于训练精调策略</p>\n\n<p>硬件和训练资源：使用自主开发的7自由度机械臂。观察包括两个RGB图像（手腕相机128×128分辨率，头部相机256×256分辨率）和机械臂本体感知状态。动作空间为7维末端执行器增量位姿。数据采集和策略执行频率为10Hz。执行器进程运行在机器人上的NVIDIA Jetson Orin，学习器在配备NVIDIA RTX 3090 GPU的工作站上执行</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n\n<p>评估环境：真实物理环境中的螺栓操作任务，包括三个子任务：(1)将螺栓竖直放置，(2)拾取螺栓，(3)组装螺栓。机器人和物体位置在x-y轴上均匀随机化±5厘米以增强泛化性</p>\n\n<p>评估指标：</p>\n<ol><li>成功率：每个子任务如果在50个时间步内完成则视为成功，超过此限制计为失败。每个子任务独立评估成功率</li><li>片段长度：完成任务所需的时间步数</li><li>长视距任务评估：完全成功需要机器人顺序完成所有三个子任务</li><li>所有结果均为25次试验的平均值</li><li>多机器人扩展性：评估在不同机器人和训练配置下的训练效率和任务性能</li></ol>"
  },
  {
    "date": "2025-09-17",
    "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving",
    "link": "http://arxiv.org/abs/2509.13769",
    "summary_markdown": "# 论文研究单位\n清华大学（Tsinghua University）、小米EV（Xiaomi EV）、澳门大学（University of Macau）、南洋理工大学（Nanyang Technological University）、北京大学（Peking University）\n# 论文概述\nAdaThinkDrive提出一种“快速回答/慢速思考”的双模推理机制，解决在端到端自动驾驶中思维链（CoT）在简单场景下过度推理、复杂场景下推理收益不一致的问题。论文在Navsim基准上进行实验与评估。\n# 论文核心贡献点\n- 通过在Navsim上系统性对比发现：简单场景下使用CoT收益有限甚至有害，复杂场景下CoT收益显著。由此提出“按场景复杂度自适应推理”的必要性。\n- 设计AdaThinkDrive框架：三阶段训练范式——预训练获得驾驶常识与知识、两阶段SFT学习双模输出能力、基于GRPO的强化学习通过奖励信号学习“何时思考与何时直接作答”。\n- 提出“Adaptive Think Reward”策略：通过多 rollout 采样，比较同一场景下Thinking与Non-thinking轨迹质量（PDMS），动态奖惩并更新场景复杂度标签，避免对固定人工标签的依赖。\n- 在Navsim达成SOTA：PDMS 90.3（仅视觉输入），比最佳纯视觉基线提升1.7；同时在复杂场景中优先使用思考模式（96%），在简单场景中优先直接输出（84%）；相对Always-Think推理时间降低14%。\n# 论文方法描述\n- 问题建模：以联合分布P(m, o \\|q)建模对查询q选择模式m（Thinking/Non-thinking）与输出轨迹o的决策；通过最大化期望任务效用U(q, o)选择最优模式。\n- 数据准备：\n - 预训练数据：DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive等开源驾驶QA数据，用于获得基础驾驶常识。\n - 混合SFT数据：为同一查询生成Thinking（含完整推理轨迹）与Non-thinking（省略推理仅含轨迹）两种输出；场景注释通过Qwen2.5-VL-72B自动生成，包含交通灯状态、天气与道路边界等；动态交互对象分为CIPO-1（本车道障碍物）、CIPO-2（可能并线）、Motion Interaction（未来轨迹与自车相交）。\n - 场景分类：将Navsim训练与验证集按“是否靠近道路边界/是否有关键物体”划分为Level 1（简单，无条件）、Level 2（有单一条件）、Level 3（复杂，同时满足两个条件），并映射为D+（Level 2&3）与D-（Level 1）作为强化学习初始辅助标签。\n- 两阶段SFT：\n - 阶段一：在大规模驾驶QA数据上微调，提升场景理解与驾驶语义认知能力。\n - 阶段二：在Navsim规划数据（含Thinking/Non-thinking双模输出）上微调，使模型具备统一接口生成两种输出风格且不偏向其一。\n- 自适应思考强化学习（基于GRPO）：\n - 奖励设计：\n - PDMS Reward：基于轨迹质量评价PDMS。\n - Format Reward：强制输出遵循<think>、<answer>标签与轨迹格式规范。\n - Endpoint Reward：对轨迹终点误差进行分段奖励（<2m=1.0；<4m=0.8；<6m=0.6；<10m=0.4；<15m=0.2；否则=0）。\n - Adaptive Think Reward：依据多rollout比较Thinking与Non-thinking的平均PDMS与样本数，在D=0/D=1的当前标签下动态奖惩；当满足阈值T与条件时允许修正场景复杂度标签为“challenging/simple”。\n - 优化目标：采用GRPO优化，引入截断重要性权重与KL正则防止策略偏移。\n# 论文使用数据集和训练资源\n- 数据集：Navsim（主评估）、DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive（用于预训练与知识注入）\n- 模型：InternVL3-8B作为基础视觉-语言模型\n- 训练资源：\n - 阶段一：学习率1e-5，批量1，训练2个epoch\n - 阶段二：学习率4e-5，批量2，训练2个epoch\n - 阶段三（RL）：学习率2e-6，批量4，使用64×NVIDIA H20 GPU进行2个epoch\n# 论文使用的评估环境和评估指标\n- 评估环境：Navsim非反应式仿真，OpenScene平台支持\n- 评估指标：\n - PDMS（Predictive Driver Model Score）：综合评分，融合NC、DAC、TTC、CF、EP\n - 推理时间：预测4秒轨迹的平均推理时间（Navsim测试集）\n - 场景分层：按Level 1/2/3的Think vs Non-think选择比例与性能表现\n- 定量结果（主要指标）：\n - AdaThinkDrive（仅视觉）：PDMS 90.3，NC 98.4、DAC 97.8、TTC 95.2、CF 100、EP 84.4\n - Best-of-N：PDMS 93.0\n - 相对Non-Think RL：PDMS +2.0；推理时间 Non-Think RL 0.68s、Think RL 0.86s、AdaThinkDrive 0.74s\n - 相对Always-Think：推理时间减少14%",
    "summary_html": "<h1>论文研究单位</h1>\n<p>清华大学（Tsinghua University）、小米EV（Xiaomi EV）、澳门大学（University of Macau）、南洋理工大学（Nanyang Technological University）、北京大学（Peking University）</p>\n<h1>论文概述</h1>\n<p>AdaThinkDrive提出一种“快速回答/慢速思考”的双模推理机制，解决在端到端自动驾驶中思维链（CoT）在简单场景下过度推理、复杂场景下推理收益不一致的问题。论文在Navsim基准上进行实验与评估。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>通过在Navsim上系统性对比发现：简单场景下使用CoT收益有限甚至有害，复杂场景下CoT收益显著。由此提出“按场景复杂度自适应推理”的必要性。</li><li>设计AdaThinkDrive框架：三阶段训练范式——预训练获得驾驶常识与知识、两阶段SFT学习双模输出能力、基于GRPO的强化学习通过奖励信号学习“何时思考与何时直接作答”。</li><li>提出“Adaptive Think Reward”策略：通过多 rollout 采样，比较同一场景下Thinking与Non-thinking轨迹质量（PDMS），动态奖惩并更新场景复杂度标签，避免对固定人工标签的依赖。</li><li>在Navsim达成SOTA：PDMS 90.3（仅视觉输入），比最佳纯视觉基线提升1.7；同时在复杂场景中优先使用思考模式（96%），在简单场景中优先直接输出（84%）；相对Always-Think推理时间降低14%。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>问题建模：以联合分布P(m, o \\|q)建模对查询q选择模式m（Thinking/Non-thinking）与输出轨迹o的决策；通过最大化期望任务效用U(q, o)选择最优模式。</li><li>数据准备：</li></ul>\n<p> - 预训练数据：DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive等开源驾驶QA数据，用于获得基础驾驶常识。</p>\n<p> - 混合SFT数据：为同一查询生成Thinking（含完整推理轨迹）与Non-thinking（省略推理仅含轨迹）两种输出；场景注释通过Qwen2.5-VL-72B自动生成，包含交通灯状态、天气与道路边界等；动态交互对象分为CIPO-1（本车道障碍物）、CIPO-2（可能并线）、Motion Interaction（未来轨迹与自车相交）。</p>\n<p> - 场景分类：将Navsim训练与验证集按“是否靠近道路边界/是否有关键物体”划分为Level 1（简单，无条件）、Level 2（有单一条件）、Level 3（复杂，同时满足两个条件），并映射为D+（Level 2&3）与D-（Level 1）作为强化学习初始辅助标签。</p>\n<ul><li>两阶段SFT：</li></ul>\n<p> - 阶段一：在大规模驾驶QA数据上微调，提升场景理解与驾驶语义认知能力。</p>\n<p> - 阶段二：在Navsim规划数据（含Thinking/Non-thinking双模输出）上微调，使模型具备统一接口生成两种输出风格且不偏向其一。</p>\n<ul><li>自适应思考强化学习（基于GRPO）：</li></ul>\n<p> - 奖励设计：</p>\n<p> - PDMS Reward：基于轨迹质量评价PDMS。</p>\n<p> - Format Reward：强制输出遵循<think>、<answer>标签与轨迹格式规范。</p>\n<p> - Endpoint Reward：对轨迹终点误差进行分段奖励（<2m=1.0；<4m=0.8；<6m=0.6；<10m=0.4；<15m=0.2；否则=0）。</p>\n<p> - Adaptive Think Reward：依据多rollout比较Thinking与Non-thinking的平均PDMS与样本数，在D=0/D=1的当前标签下动态奖惩；当满足阈值T与条件时允许修正场景复杂度标签为“challenging/simple”。</p>\n<p> - 优化目标：采用GRPO优化，引入截断重要性权重与KL正则防止策略偏移。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：Navsim（主评估）、DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive（用于预训练与知识注入）</li><li>模型：InternVL3-8B作为基础视觉-语言模型</li><li>训练资源：</li></ul>\n<p> - 阶段一：学习率1e-5，批量1，训练2个epoch</p>\n<p> - 阶段二：学习率4e-5，批量2，训练2个epoch</p>\n<p> - 阶段三（RL）：学习率2e-6，批量4，使用64×NVIDIA H20 GPU进行2个epoch</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：Navsim非反应式仿真，OpenScene平台支持</li><li>评估指标：</li></ul>\n<p> - PDMS（Predictive Driver Model Score）：综合评分，融合NC、DAC、TTC、CF、EP</p>\n<p> - 推理时间：预测4秒轨迹的平均推理时间（Navsim测试集）</p>\n<p> - 场景分层：按Level 1/2/3的Think vs Non-think选择比例与性能表现</p>\n<ul><li>定量结果（主要指标）：</li></ul>\n<p> - AdaThinkDrive（仅视觉）：PDMS 90.3，NC 98.4、DAC 97.8、TTC 95.2、CF 100、EP 84.4</p>\n<p> - Best-of-N：PDMS 93.0</p>\n<p> - 相对Non-Think RL：PDMS +2.0；推理时间 Non-Think RL 0.68s、Think RL 0.86s、AdaThinkDrive 0.74s</p>\n<p> - 相对Always-Think：推理时间减少14%</p>"
  },
  {
    "date": "2025-09-13",
    "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft",
    "link": "http://arxiv.org/abs/2509.13347",
    "summary_markdown": "### 论文研究单位\n北京大学和新加坡国立大学的研究人员。\n### 论文概述\n该论文探讨了在开发具有端到端训练能力的智能体时，动作表示选择的关键问题。研究首先在Minecraft环境中对多种动作抽象方法进行了大规模、系统性的比较，发现没有单一的动作空间是普遍最优的，其有效性高度依赖于具体任务。为解决这一难题，论文提出了“动作链”（Chain of Action, CoA）框架，该框架将高层规划与低层控制统一在单个视觉-语言-动作（VLA）模型中。此外，研究还展示了一个“All-in-One”智能体，通过在混合动作空间数据集上使用CoA范式进行训练，学习到更鲁棒和泛化的策略，并在整体任务成功率上达到了新的最先进水平。为了促进可复现研究，论文发布了OpenHA套件，包括全面的基准测试、精选数据集、源代码和所有预训练模型检查点。\n### 论文核心贡献点\n1. 通过大规模系统性分析，证明了最优的抽象动作空间具有任务依赖性，不同的动作表示在特定任务领域各有优势。\n2. 提出了“动作链”（CoA）框架，通过使用抽象动作作为中间计划，协同了高层推理和低层控制，在性能上优于传统的双系统架构。\n3. 证明了在多样化动作空间的混合数据集上训练单个智能体，可以增强其在不同任务间的泛化能力，提升整体决策水平。\n4. 发布了一套全面的资源，包括各种分层智能体、分层VLA模型和OpenHA模型的训练检查点、代码和数据集，以支持动作表示和泛化方面的进一步研究。\n### 论文方法描述\n论文的核心方法是“动作链”（CoA）框架。该方法将动作生成构建为一个两步的自回归过程：首先，模型生成一个高层抽象动作，作为中间的“思考”或“推理”步骤；然后，以这个“思考”为条件，模型生成最终的底层可执行动作。例如，为了执行“砍树”指令，CoA智能体首先预测一个接地的动作，如`Approach(object=<tree>, coordinate=[136, 287])`，接着是到达该位置并执行动作所需的底层动作序列。这种方法有效地协同了高层推理与低层控制。该框架支持两种推理模式：解耦推理模式（快速）和统一自回归模式（慢速）。此外，论文提出了“All-in-One”训练策略，通过在包含多种动作表示的复合数据集上训练单个智能体，使其能够掌握多样化的动作，从而超越在单一类型动作上训练的专家模型。\n### 论文使用数据集和训练资源\n训练数据源自OpenAI Video Pre-Training (VPT) 数据集，该数据集包含Minecraft中的大量专家轨迹。由于原始VPT数据集仅提供视觉观察和底层环境动作对，研究开发了一个“程序化标注管道”，通过一套基于规则的启发式方法，将原始底层动作序列转换为对应的抽象动作表示（如运动、接地和语言技能动作），生成了三种类型的轨迹数据集：高层动作数据、底层动作数据和动作链数据。所有模型均基于Qwen2-VL-7B基础模型进行初始化，该模型已在Minecraft特定的VQA和字幕数据语料库上进行了后训练。训练使用了TRL和VeOmni库，并使用vLLM库支持高效的智能体推理。\n### 论文使用的评估环境和评估指标\n评估环境为Minecraft（版本1.16.5）。智能体的观察空间仅包含第一人称RGB视觉帧，分辨率为360x640x3。动作空间包括离散化的、类人鼠标和键盘控制。研究构建了一个包含近1000个不同任务的大规模基准，并将其分为三类：体现任务（需要导航和物理交互）、GUI任务（涉及图形用户界面交互）和战斗任务（需要与敌对生物交战）。为确保泛化测试，所有评估环境都与训练数据分布外，通过从Minecraft的程序化生成空间中采样新的初始世界种子和出生位置来实现。\n主要评估指标为：1. 成功率：智能体成功完成给定任务的运行百分比。2. 完成步数：完成任务所需的环境步数。最终分析报告了每个基准类别中所有任务的平均成功率和平均步数。每个智能体在每个任务上至少进行三次独立运行（使用不同的世界种子），并从三个类别中各选取10个不同难度的任务进行了超过10次运行的密集评估。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学和新加坡国立大学的研究人员。</p>\n<h3>论文概述</h3>\n<p>该论文探讨了在开发具有端到端训练能力的智能体时，动作表示选择的关键问题。研究首先在Minecraft环境中对多种动作抽象方法进行了大规模、系统性的比较，发现没有单一的动作空间是普遍最优的，其有效性高度依赖于具体任务。为解决这一难题，论文提出了“动作链”（Chain of Action, CoA）框架，该框架将高层规划与低层控制统一在单个视觉-语言-动作（VLA）模型中。此外，研究还展示了一个“All-in-One”智能体，通过在混合动作空间数据集上使用CoA范式进行训练，学习到更鲁棒和泛化的策略，并在整体任务成功率上达到了新的最先进水平。为了促进可复现研究，论文发布了OpenHA套件，包括全面的基准测试、精选数据集、源代码和所有预训练模型检查点。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>通过大规模系统性分析，证明了最优的抽象动作空间具有任务依赖性，不同的动作表示在特定任务领域各有优势。</li><li>提出了“动作链”（CoA）框架，通过使用抽象动作作为中间计划，协同了高层推理和低层控制，在性能上优于传统的双系统架构。</li><li>证明了在多样化动作空间的混合数据集上训练单个智能体，可以增强其在不同任务间的泛化能力，提升整体决策水平。</li><li>发布了一套全面的资源，包括各种分层智能体、分层VLA模型和OpenHA模型的训练检查点、代码和数据集，以支持动作表示和泛化方面的进一步研究。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文的核心方法是“动作链”（CoA）框架。该方法将动作生成构建为一个两步的自回归过程：首先，模型生成一个高层抽象动作，作为中间的“思考”或“推理”步骤；然后，以这个“思考”为条件，模型生成最终的底层可执行动作。例如，为了执行“砍树”指令，CoA智能体首先预测一个接地的动作，如<code>Approach(object=<tree>, coordinate=[136, 287])</code>，接着是到达该位置并执行动作所需的底层动作序列。这种方法有效地协同了高层推理与低层控制。该框架支持两种推理模式：解耦推理模式（快速）和统一自回归模式（慢速）。此外，论文提出了“All-in-One”训练策略，通过在包含多种动作表示的复合数据集上训练单个智能体，使其能够掌握多样化的动作，从而超越在单一类型动作上训练的专家模型。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>训练数据源自OpenAI Video Pre-Training (VPT) 数据集，该数据集包含Minecraft中的大量专家轨迹。由于原始VPT数据集仅提供视觉观察和底层环境动作对，研究开发了一个“程序化标注管道”，通过一套基于规则的启发式方法，将原始底层动作序列转换为对应的抽象动作表示（如运动、接地和语言技能动作），生成了三种类型的轨迹数据集：高层动作数据、底层动作数据和动作链数据。所有模型均基于Qwen2-VL-7B基础模型进行初始化，该模型已在Minecraft特定的VQA和字幕数据语料库上进行了后训练。训练使用了TRL和VeOmni库，并使用vLLM库支持高效的智能体推理。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境为Minecraft（版本1.16.5）。智能体的观察空间仅包含第一人称RGB视觉帧，分辨率为360x640x3。动作空间包括离散化的、类人鼠标和键盘控制。研究构建了一个包含近1000个不同任务的大规模基准，并将其分为三类：体现任务（需要导航和物理交互）、GUI任务（涉及图形用户界面交互）和战斗任务（需要与敌对生物交战）。为确保泛化测试，所有评估环境都与训练数据分布外，通过从Minecraft的程序化生成空间中采样新的初始世界种子和出生位置来实现。</p>\n<p>主要评估指标为：1. 成功率：智能体成功完成给定任务的运行百分比。2. 完成步数：完成任务所需的环境步数。最终分析报告了每个基准类别中所有任务的平均成功率和平均步数。每个智能体在每个任务上至少进行三次独立运行（使用不同的世界种子），并从三个类别中各选取10个不同难度的任务进行了超过10次运行的密集评估。</p>"
  },
  {
    "date": "2025-09-16",
    "title": "The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning",
    "link": "http://arxiv.org/abs/2509.12594",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-15",
    "title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning",
    "link": "http://arxiv.org/abs/2509.11839",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n浙江大学、西湖大学、上海交通大学、上海创新研究院\n## 论文概述\n当前视觉-语言-动作（VLA）模型在跨实体泛化方面显示出潜力，但在高质量演示稀缺时，特别是对于双足人形机器人，难以快速对齐新的机器人动作空间。本文提出了TrajBooster，一个跨实体框架，利用丰富的轮式人形机器人数据来提升双足VLA性能。\n## 论文核心贡献点\n1. 首次利用重定向动作数据微调VLA模型并实现双足人形机器人全身操作的真实世界应用\n2. 提出了TrajBooster真实到仿真到真实的跨实体框架，使用末端执行器轨迹作为形态不可知的信号\n3. 在Unitree G1上仅需10分钟遥操作数据收集，就能实现超越桌面的家庭任务，包括蹲下、跨高度操作和协调的全身运动\n## 论文方法描述\n### 1. 真实轨迹提取\n从Agibot-World beta数据集中提取6D双臂末端执行器轨迹，并将其从Agibot数据映射到Unitree官方G1操作数据集，解决工作空间差异问题。\n### 2. 仿真中的重定向\n设计复合层次化模型用于全身操作重定向，包括：\n- **手臂策略**：使用闭环逆运动学（CLIK）计算目标关节角度\n- **工作者策略**：基于目标的条件强化学习策略，输出12-DoF下肢目标关节位置\n- **管理者策略**：从手腕姿态生成下肢命令（基座速度命令和躯干高度）\n\n采用启发式增强的调和在线DAgger算法进行训练。\n### 3. 双步后训练\n- **后预训练**：使用重定向的动作-视觉-语言三元组数据对预训练GR00T N1.5模型进行后预训练\n- **后训练**：使用收集的遥操作数据进行微调\n## 论文使用数据集和训练资源\n- **源数据**：Agibot-World beta数据集，包含超过一百万条真实机器人轨迹\n- **目标数据**：28个真实世界全身操作数据集片段，涵盖四种不同高度配置\n- **训练资源**：\n - 重定向模型训练：512个并行环境，200次训练迭代，单个RTX 4090 GPU\n - 后预训练：双A100 80GB GPU，批次大小=128，60K步数\n - 后训练：单个A100 GPU，批次大小=16，3K步数\n## 论文使用的评估环境和评估指标\n### 评估环境\n- **仿真评估**：在Isaac Gym中评估重定向模型性能\n- **真实世界评估**：在Unitree G1人形机器人上进行四个遥操作任务评估\n### 评估指标\n- **轨迹跟踪精度**：位置误差E_p（厘米）和旋转误差E_r（度）\n- **任务成功率**：在十个试验中完成任务的百分比\n- **轨迹泛化能力**：使用FastDTW算法计算生成轨迹与真实遥操作数据的相似性\n- **零样本技能泛化**：评估未见任务的执行能力",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>浙江大学、西湖大学、上海交通大学、上海创新研究院</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>当前视觉-语言-动作（VLA）模型在跨实体泛化方面显示出潜力，但在高质量演示稀缺时，特别是对于双足人形机器人，难以快速对齐新的机器人动作空间。本文提出了TrajBooster，一个跨实体框架，利用丰富的轮式人形机器人数据来提升双足VLA性能。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>首次利用重定向动作数据微调VLA模型并实现双足人形机器人全身操作的真实世界应用</li><li>提出了TrajBooster真实到仿真到真实的跨实体框架，使用末端执行器轨迹作为形态不可知的信号</li><li>在Unitree G1上仅需10分钟遥操作数据收集，就能实现超越桌面的家庭任务，包括蹲下、跨高度操作和协调的全身运动</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<h3>1. 真实轨迹提取</h3>\n<p>从Agibot-World beta数据集中提取6D双臂末端执行器轨迹，并将其从Agibot数据映射到Unitree官方G1操作数据集，解决工作空间差异问题。</p>\n<h3>2. 仿真中的重定向</h3>\n<p>设计复合层次化模型用于全身操作重定向，包括：</p>\n<ul><li><strong>手臂策略</strong>：使用闭环逆运动学（CLIK）计算目标关节角度</li><li><strong>工作者策略</strong>：基于目标的条件强化学习策略，输出12-DoF下肢目标关节位置</li><li><strong>管理者策略</strong>：从手腕姿态生成下肢命令（基座速度命令和躯干高度）</li></ul>\n\n<p>采用启发式增强的调和在线DAgger算法进行训练。</p>\n<h3>3. 双步后训练</h3>\n<ul><li><strong>后预训练</strong>：使用重定向的动作-视觉-语言三元组数据对预训练GR00T N1.5模型进行后预训练</li><li><strong>后训练</strong>：使用收集的遥操作数据进行微调</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>源数据</strong>：Agibot-World beta数据集，包含超过一百万条真实机器人轨迹</li><li><strong>目标数据</strong>：28个真实世界全身操作数据集片段，涵盖四种不同高度配置</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 重定向模型训练：512个并行环境，200次训练迭代，单个RTX 4090 GPU</p>\n<p> - 后预训练：双A100 80GB GPU，批次大小=128，60K步数</p>\n<p> - 后训练：单个A100 GPU，批次大小=16，3K步数</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<h3>评估环境</h3>\n<ul><li><strong>仿真评估</strong>：在Isaac Gym中评估重定向模型性能</li><li><strong>真实世界评估</strong>：在Unitree G1人形机器人上进行四个遥操作任务评估</li></ul>\n<h3>评估指标</h3>\n<ul><li><strong>轨迹跟踪精度</strong>：位置误差E_p（厘米）和旋转误差E_r（度）</li><li><strong>任务成功率</strong>：在十个试验中完成任务的百分比</li><li><strong>轨迹泛化能力</strong>：使用FastDTW算法计算生成轨迹与真实遥操作数据的相似性</li><li><strong>零样本技能泛化</strong>：评估未见任务的执行能力</li></ul>"
  },
  {
    "date": "2025-09-15",
    "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
    "link": "http://arxiv.org/abs/2509.11480",
    "summary_markdown": "### 论文研究单位\n* 东北大学电气与计算机工程系\n* EmbodyX Inc.\n### 论文概述\n该论文系统地评估了五种视觉-语言-动作模型在不同硬件平台（从边缘设备到数据中心GPU）以及不同功率预算下的性能缩放情况。通过在LIBERO基准上测试，作者测量了模型的准确率、延迟、吞吐量和峰值内存使用等指标，旨在揭示模型架构、硬件类别和功耗限制之间的相互作用，并为不同部署场景下的模型选择和优化提供实践指导。\n### 论文核心贡献点\n* 对VLA模型在边缘和数据中心GPU上的跨平台性能进行了首次系统性评估。\n* 识别了关键的缩放趋势：架构选择（如动作分块大小、模型骨干尺寸）显著影响吞吐量和内存占用；功率受限的边缘设备表现出非线性的性能退化；高吞吐量模型可以在不显著牺牲准确率的情况下实现。\n* 提出了一个挑战性观点：现代高端边缘设备（如Jetson AGX Orin）在特定配置下的性能可以超越旧款的数据中心GPU（如V100）。\n* 提供了可操作的见解，用于在不同部署约束下选择和优化VLA模型。\n### 论文方法描述\n论文评估了五个VLA模型，包括三个基线模型和两个新提出的架构。这些模型在语言骨干、视觉编码器和动作头设计上有所不同。\n* **评估模型**：OpenVLA (7B), SpatialVLA (4B), OpenVLA-OFT (7B), VOTE (7B, 包含1T, 2T, MLP4三种配置) 和 QwenVLA (2.6B)。\n* **新架构**：VOTE模型通过使用特殊动作令牌（ST）来减少输出令牌数量，从而降低延迟。QwenVLA模型采用更小的语言骨干，以探索模型尺寸与效率的权衡。\n* **实验方法**：在所有硬件平台上，对每个模型运行基准测试，测量生成一个动作块的平均延迟和每秒生成的动作数。在测量前进行预热，并记录100次连续推理的时间以确保稳定性。\n### 论文使用数据集和训练资源\n* **数据集**：LIBERO基准，该基准包含四个任务套件，分别测试模型在空间关系、物体类型、任务导向行为和长时任务上的泛化能力。\n* **训练资源**：新提出的VOTE和QwenVLA模型在LIBERO基准上进行微调。优化器为AdamW，VOTE和QwenVLA的学习率分别为1e-4和1e-3。使用了低秩自适应进行参数高效微调，LoRA的秩为32，alpha为16。VOTE和QwenVLA的全局批处理大小分别为40和64。\n### 论文使用的评估环境和评估指标\n* **评估环境**：\n * **边缘平台**：NVIDIA Jetson AGX Orin，在四种功率模式下运行（15W, 30W, 50W, MAX），以模拟不同的功耗约束。\n * **数据中心平台**：四款NVIDIA数据中心GPU，包括H100 (Hopper架构)、A100 (Ampere架构)、A6000 (Ampere架构) 和 V100 (Volta架构)。\n* **评估指标**：\n * **准确率**：在LIBERO基准的四个任务套件上的任务成功率（Success Rate, SR）。\n * **延迟**：生成一个动作块所需的平均时间（毫秒）。\n * **吞吐量**：每秒生成的动作数量。\n * **峰值内存使用**：模型推理时占用的最大显存（GB）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>东北大学电气与计算机工程系</li><li>EmbodyX Inc.</li></ul>\n<h3>论文概述</h3>\n<p>该论文系统地评估了五种视觉-语言-动作模型在不同硬件平台（从边缘设备到数据中心GPU）以及不同功率预算下的性能缩放情况。通过在LIBERO基准上测试，作者测量了模型的准确率、延迟、吞吐量和峰值内存使用等指标，旨在揭示模型架构、硬件类别和功耗限制之间的相互作用，并为不同部署场景下的模型选择和优化提供实践指导。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>对VLA模型在边缘和数据中心GPU上的跨平台性能进行了首次系统性评估。</li><li>识别了关键的缩放趋势：架构选择（如动作分块大小、模型骨干尺寸）显著影响吞吐量和内存占用；功率受限的边缘设备表现出非线性的性能退化；高吞吐量模型可以在不显著牺牲准确率的情况下实现。</li><li>提出了一个挑战性观点：现代高端边缘设备（如Jetson AGX Orin）在特定配置下的性能可以超越旧款的数据中心GPU（如V100）。</li><li>提供了可操作的见解，用于在不同部署约束下选择和优化VLA模型。</li></ul>\n<h3>论文方法描述</h3>\n<p>论文评估了五个VLA模型，包括三个基线模型和两个新提出的架构。这些模型在语言骨干、视觉编码器和动作头设计上有所不同。</p>\n<ul><li><strong>评估模型</strong>：OpenVLA (7B), SpatialVLA (4B), OpenVLA-OFT (7B), VOTE (7B, 包含1T, 2T, MLP4三种配置) 和 QwenVLA (2.6B)。</li><li><strong>新架构</strong>：VOTE模型通过使用特殊动作令牌（ST）来减少输出令牌数量，从而降低延迟。QwenVLA模型采用更小的语言骨干，以探索模型尺寸与效率的权衡。</li><li><strong>实验方法</strong>：在所有硬件平台上，对每个模型运行基准测试，测量生成一个动作块的平均延迟和每秒生成的动作数。在测量前进行预热，并记录100次连续推理的时间以确保稳定性。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：LIBERO基准，该基准包含四个任务套件，分别测试模型在空间关系、物体类型、任务导向行为和长时任务上的泛化能力。</li><li><strong>训练资源</strong>：新提出的VOTE和QwenVLA模型在LIBERO基准上进行微调。优化器为AdamW，VOTE和QwenVLA的学习率分别为1e-4和1e-3。使用了低秩自适应进行参数高效微调，LoRA的秩为32，alpha为16。VOTE和QwenVLA的全局批处理大小分别为40和64。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> * <strong>边缘平台</strong>：NVIDIA Jetson AGX Orin，在四种功率模式下运行（15W, 30W, 50W, MAX），以模拟不同的功耗约束。</p>\n<p> * <strong>数据中心平台</strong>：四款NVIDIA数据中心GPU，包括H100 (Hopper架构)、A100 (Ampere架构)、A6000 (Ampere架构) 和 V100 (Volta架构)。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> * <strong>准确率</strong>：在LIBERO基准的四个任务套件上的任务成功率（Success Rate, SR）。</p>\n<p> * <strong>延迟</strong>：生成一个动作块所需的平均时间（毫秒）。</p>\n<p> * <strong>吞吐量</strong>：每秒生成的动作数量。</p>\n<p> * <strong>峰值内存使用</strong>：模型推理时占用的最大显存（GB）。</p>"
  },
  {
    "date": "2025-09-14",
    "title": "Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations",
    "link": "http://arxiv.org/abs/2509.11417",
    "summary_markdown": "# 论文研究单位\n- UC San Diego（加州大学圣地亚哥分校）\n- Hillbot\n# 论文概述\nVLA模型往往通过直接微调预训练的VLM获取机器人控制能力，但会导致预训练表征严重退化，在视觉背景变化与指令同义改写下性能显著下降。本文提出保留预训练表征的VLA训练框架，在仿真与真实机器人上均显著提升稳健性与泛化。\n# 论文核心贡献点\n- 部分冻结的双视觉编码器：冻结一个编码器作为“锚点”保留预训练表征，另一个可训练以适配机器人任务，拼接后输入后续模块。\n- 字符串动作分词器：将连续动作转换为字符序列，使用与语言一致的词表和自回归目标，使动作预测与VLM预训练目标对齐，支持细粒度逐步生成。\n- 特征正则化与联合训练：利用统一的字符串输出空间，在机器人数据与强调空间推理/可供性的视觉-语言数据上进行等量（50%/批次）联合训练，防止灾难性遗忘并增强泛化。\n- 架构与配方通用性：可无缝适配OpenVLA与π0等多种VLA体系。\n# 论文方法描述\n- 部分冻结双编码器架构：使用两个视觉编码器（一个冻结、一个训练），对同一观测生成两组特征并拼接 z_t = [φ_frozen(o_t) \\|\\|φ_train(o_t)]，再与语言指令 c 一起送入动作分词器 ψ 生成动作。\n- 字符串动作分词器：将动作各维度（如Δx=0.0312）序列化为字符序列（如“0 . 0 3 1 2”），利用语言词表与自回归生成目标进行预测，使动作空间与语言空间对齐，支持在语言模型内统一优化机器人与视觉-语言任务。\n- 联合训练策略：每批训练从机器人数据与视觉-语言数据各取50%，数据涵盖空间推理、视觉问答、目标定位等，确保与任务相关且一致的梯度来源。\n# 论文使用数据集和训练资源\n- 数据集\n - 机器人：Open X-Embodiment (OXE)\n - 视觉-语言：LLaVA Visual Instruct CC3M、VQASynth-Spatial、LLaVA OneVision、RoboPoint\n- 仿真评估：SimplerEnv（Visual Matching与Visual Variant Aggregation），包含背景、光照、桌面纹理、干扰物与视角扰动\n- 训练与评估硬件：双GTX 1080 Ti；真实机器人使用Logitech C920 Webcam作为视觉输入\n# 论文使用的评估环境和评估指标\n- 仿真：SimplerEnv的Visual Matching与Visual Variant Aggregation\n- 视觉稳健性：背景遮挡与视觉多样性扰动\n- 语言稳健性：同义指令改写（GPT-4生成），如“grasp the can”→“get the can”\n- 泛化与推理：在Text-VQA、POPE、GQA、VizWiz、VSR等VLM基准上评估保留的推理能力\n- 指标\n - 任务成功率（%）\n - VQA准确率（%）\n - CIFAR-10线性探测准确率（%）与t-SNE可视化\n - 真实机器人每任务25次试验的成功次数（表V）",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>UC San Diego（加州大学圣地亚哥分校）</li><li>Hillbot</li></ul>\n<h1>论文概述</h1>\n<p>VLA模型往往通过直接微调预训练的VLM获取机器人控制能力，但会导致预训练表征严重退化，在视觉背景变化与指令同义改写下性能显著下降。本文提出保留预训练表征的VLA训练框架，在仿真与真实机器人上均显著提升稳健性与泛化。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>部分冻结的双视觉编码器：冻结一个编码器作为“锚点”保留预训练表征，另一个可训练以适配机器人任务，拼接后输入后续模块。</li><li>字符串动作分词器：将连续动作转换为字符序列，使用与语言一致的词表和自回归目标，使动作预测与VLM预训练目标对齐，支持细粒度逐步生成。</li><li>特征正则化与联合训练：利用统一的字符串输出空间，在机器人数据与强调空间推理/可供性的视觉-语言数据上进行等量（50%/批次）联合训练，防止灾难性遗忘并增强泛化。</li><li>架构与配方通用性：可无缝适配OpenVLA与π0等多种VLA体系。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>部分冻结双编码器架构：使用两个视觉编码器（一个冻结、一个训练），对同一观测生成两组特征并拼接 z_t = [φ_frozen(o_t) \\|\\|φ_train(o_t)]，再与语言指令 c 一起送入动作分词器 ψ 生成动作。</li><li>字符串动作分词器：将动作各维度（如Δx=0.0312）序列化为字符序列（如“0 . 0 3 1 2”），利用语言词表与自回归生成目标进行预测，使动作空间与语言空间对齐，支持在语言模型内统一优化机器人与视觉-语言任务。</li><li>联合训练策略：每批训练从机器人数据与视觉-语言数据各取50%，数据涵盖空间推理、视觉问答、目标定位等，确保与任务相关且一致的梯度来源。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集</li></ul>\n<p> - 机器人：Open X-Embodiment (OXE)</p>\n<p> - 视觉-语言：LLaVA Visual Instruct CC3M、VQASynth-Spatial、LLaVA OneVision、RoboPoint</p>\n<ul><li>仿真评估：SimplerEnv（Visual Matching与Visual Variant Aggregation），包含背景、光照、桌面纹理、干扰物与视角扰动</li><li>训练与评估硬件：双GTX 1080 Ti；真实机器人使用Logitech C920 Webcam作为视觉输入</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>仿真：SimplerEnv的Visual Matching与Visual Variant Aggregation</li><li>视觉稳健性：背景遮挡与视觉多样性扰动</li><li>语言稳健性：同义指令改写（GPT-4生成），如“grasp the can”→“get the can”</li><li>泛化与推理：在Text-VQA、POPE、GQA、VizWiz、VSR等VLM基准上评估保留的推理能力</li><li>指标</li></ul>\n<p> - 任务成功率（%）</p>\n<p> - VQA准确率（%）</p>\n<p> - CIFAR-10线性探测准确率（%）与t-SNE可视化</p>\n<p> - 真实机器人每任务25次试验的成功次数（表V）</p>"
  },
  {
    "date": "2025-09-11",
    "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "link": "http://arxiv.org/abs/2509.09674",
    "summary_markdown": "### 论文研究单位\n上海交通大学, 北京大学, 香港大学, 上海AI实验室\n### 论文概述\n论文针对视觉-语言-动作（VLA）模型在监督微调（SFT）阶段面临的两个核心挑战：大规模人类操作轨迹数据的稀缺与高昂成本，以及在分布偏移任务上的泛化能力有限。受大型推理模型通过强化学习（RL）取得成功的启发，论文提出了SimpleVLA-RL，一个专为VLA模型设计的高效在线强化学习框架。该方法基于veRL框架，实现了交互式轨迹采样、可扩展的并行化和优化的损失计算。通过采用简单的二元结果奖励和一系列探索增强策略，该方法在LIBERO和RoboTwin等基准上取得了最先进的性能，显著提升了数据效率和泛化能力，并能有效迁移至真实世界任务。此外，研究还发现了一种名为“pushcut”的新颖行为模式，即策略在RL训练中学会了训练数据之外的新策略。\n### 论文核心贡献点\n- 提出了一个高效、端到端的VLA在线RL框架，该框架基于veRL，为VLA实现了稳定的、样本高效的训练，并优化了渲染并行化和分布式训练。\n- 通过引入探索增强策略，实现了10-15%的一致性性能提升。SimpleVLA-RL在LIBERO和RoboTwin 1.0 & 2.0上均超越了多个SOTA基线模型。\n- 展现了卓越的数据效率和泛化能力。仅使用每个任务一个演示，RL就将LIBERO-Long任务的成功率从17.1%提升至91.7%，并在空间、物体和任务泛化上显著优于SFT。\n- 具备真实世界部署能力。在模拟器上训练的策略可以有效迁移到真实世界，在无需任何真实机器人数据的情况下实现了强大的模拟到现实的性能增益。\n- 发现了“pushcut”现象，即策略在RL训练期间发现了先前在监督训练数据中未见过的全新行为模式。\n### 论文方法描述\nSimpleVLA-RL是一个基于veRL框架的在线RL训练方法，专为VLA模型设计。其核心方法包括：\n1. **交互式VLA轨迹生成**：与LLM的纯文本生成不同，VLA需要与模拟环境进行多轮交互，根据实时感官反馈动态更新状态和动作。模型通过在动作token上进行随机采样来生成多样化的轨迹。\n2. **结果奖励建模**：采用简单且可扩展的二元奖励函数。一条轨迹如果任务成功则获得奖励1，否则为0。该奖励会均匀分配给轨迹中的所有动作token。\n3. **探索增强策略**：为提升RL的探索效率，引入了三种策略：\n - **动态采样**：在rollout时，只保留包含混合成功和失败轨迹的批次，确保优势估计非零，避免梯度消失。\n - **调整裁剪范围**：将GRPO目标中的重要性采样比率裁剪范围从[0.8, 1.2]调整为[0.8, 1.28]，以鼓励对低概率动作的探索。\n - **提高采样温度**：将rollout阶段的采样温度从1.0提高到1.6，以生成更多样的探索轨迹。\n4. **训练目标**：采用改进的Group Relative Policy Optimization (GRPO)算法。关键修改是移除了KL散度正则化项，消除了对参考模型的需求，降低了内存消耗，并鼓励策略探索新行为。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **LIBERO**：一个包含五个任务套件的语言引导操作终身学习基准，用于评估模型的长期规划能力。\n - **RoboTwin 1.0**：一个包含17个双臂任务的模拟基准。\n - **RoboTwin 2.0**：RoboTwin 1.0的扩展版，包含50个任务、多种机器人形态、731个物体实例和全面的域随机化，任务按步长分为短、中、长、超长四种水平。\n- **训练资源**：使用8块NVIDIA A800 80GB GPU进行全参数训练。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：LIBERO、RoboTwin 1.0和RoboTwin 2.0的模拟器环境。\n - **真实世界环境**：使用Agilex Piper机械臂进行真实世界实验，以验证模拟到现实的迁移能力。\n- **评估指标**：\n - **成功率**：主要评估指标，指任务成功完成的百分比。对于LIBERO，是每个任务在50个保留测试场景上的平均成功率；对于RoboTwin，是每个任务在100个保留测试场景上的平均成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学, 北京大学, 香港大学, 上海AI实验室</p>\n<h3>论文概述</h3>\n<p>论文针对视觉-语言-动作（VLA）模型在监督微调（SFT）阶段面临的两个核心挑战：大规模人类操作轨迹数据的稀缺与高昂成本，以及在分布偏移任务上的泛化能力有限。受大型推理模型通过强化学习（RL）取得成功的启发，论文提出了SimpleVLA-RL，一个专为VLA模型设计的高效在线强化学习框架。该方法基于veRL框架，实现了交互式轨迹采样、可扩展的并行化和优化的损失计算。通过采用简单的二元结果奖励和一系列探索增强策略，该方法在LIBERO和RoboTwin等基准上取得了最先进的性能，显著提升了数据效率和泛化能力，并能有效迁移至真实世界任务。此外，研究还发现了一种名为“pushcut”的新颖行为模式，即策略在RL训练中学会了训练数据之外的新策略。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个高效、端到端的VLA在线RL框架，该框架基于veRL，为VLA实现了稳定的、样本高效的训练，并优化了渲染并行化和分布式训练。</li><li>通过引入探索增强策略，实现了10-15%的一致性性能提升。SimpleVLA-RL在LIBERO和RoboTwin 1.0 & 2.0上均超越了多个SOTA基线模型。</li><li>展现了卓越的数据效率和泛化能力。仅使用每个任务一个演示，RL就将LIBERO-Long任务的成功率从17.1%提升至91.7%，并在空间、物体和任务泛化上显著优于SFT。</li><li>具备真实世界部署能力。在模拟器上训练的策略可以有效迁移到真实世界，在无需任何真实机器人数据的情况下实现了强大的模拟到现实的性能增益。</li><li>发现了“pushcut”现象，即策略在RL训练期间发现了先前在监督训练数据中未见过的全新行为模式。</li></ul>\n<h3>论文方法描述</h3>\n<p>SimpleVLA-RL是一个基于veRL框架的在线RL训练方法，专为VLA模型设计。其核心方法包括：</p>\n<ol><li><strong>交互式VLA轨迹生成</strong>：与LLM的纯文本生成不同，VLA需要与模拟环境进行多轮交互，根据实时感官反馈动态更新状态和动作。模型通过在动作token上进行随机采样来生成多样化的轨迹。</li><li><strong>结果奖励建模</strong>：采用简单且可扩展的二元奖励函数。一条轨迹如果任务成功则获得奖励1，否则为0。该奖励会均匀分配给轨迹中的所有动作token。</li><li><strong>探索增强策略</strong>：为提升RL的探索效率，引入了三种策略：</li></ol>\n<p> - <strong>动态采样</strong>：在rollout时，只保留包含混合成功和失败轨迹的批次，确保优势估计非零，避免梯度消失。</p>\n<p> - <strong>调整裁剪范围</strong>：将GRPO目标中的重要性采样比率裁剪范围从[0.8, 1.2]调整为[0.8, 1.28]，以鼓励对低概率动作的探索。</p>\n<p> - <strong>提高采样温度</strong>：将rollout阶段的采样温度从1.0提高到1.6，以生成更多样的探索轨迹。</p>\n<ol><li><strong>训练目标</strong>：采用改进的Group Relative Policy Optimization (GRPO)算法。关键修改是移除了KL散度正则化项，消除了对参考模型的需求，降低了内存消耗，并鼓励策略探索新行为。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>LIBERO</strong>：一个包含五个任务套件的语言引导操作终身学习基准，用于评估模型的长期规划能力。</p>\n<p> - <strong>RoboTwin 1.0</strong>：一个包含17个双臂任务的模拟基准。</p>\n<p> - <strong>RoboTwin 2.0</strong>：RoboTwin 1.0的扩展版，包含50个任务、多种机器人形态、731个物体实例和全面的域随机化，任务按步长分为短、中、长、超长四种水平。</p>\n<ul><li><strong>训练资源</strong>：使用8块NVIDIA A800 80GB GPU进行全参数训练。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：LIBERO、RoboTwin 1.0和RoboTwin 2.0的模拟器环境。</p>\n<p> - <strong>真实世界环境</strong>：使用Agilex Piper机械臂进行真实世界实验，以验证模拟到现实的迁移能力。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：主要评估指标，指任务成功完成的百分比。对于LIBERO，是每个任务在50个保留测试场景上的平均成功率；对于RoboTwin，是每个任务在100个保留测试场景上的平均成功率。</p>"
  },
  {
    "date": "2025-09-11",
    "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2509.09372",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-11",
    "title": "SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2509.09090",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-10",
    "title": "RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation",
    "link": "http://arxiv.org/abs/2509.08820",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-09",
    "title": "TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2509.07962",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-09",
    "title": "Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation",
    "link": "http://arxiv.org/abs/2509.07957",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-08",
    "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
    "link": "http://arxiv.org/abs/2509.06951",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-08",
    "title": "LLaDA-VLA: Vision Language Diffusion Action Models",
    "link": "http://arxiv.org/abs/2509.06932",
    "summary_markdown": "### 论文研究单位\n\n中国科学技术大学、南京大学、Dexmal\n### 论文概述\n\n该论文提出了LLaDA-VLA，这是首个基于预训练扩散视觉语言模型构建的视觉-语言-扩散-动作模型，用于机器人操作任务。为了解决预训练d-VLMs与机器人动作生成之间的领域差距以及掩码扩散范式在生成结构化动作序列方面的不足，论文引入了两个核心设计：一是局部化特殊令牌分类策略，通过将分类空间限制在特殊动作令牌上，降低了模型适应的难度；二是分层动作结构化解码策略，该策略在解码过程中显式地考虑动作内部和动作之间的依赖关系，生成了更连贯、精确的动作序列。大量的实验表明，LLaDA-VLA在模拟环境和真实机器人上的性能均显著优于现有的最先进视觉-语言-动作模型。\n### 论文核心贡献点\n\n1. 提出了首个基于预训练d-VLMs构建的视觉-语言-扩散-动作模型LLaDA-VLA，为机器人策略学习建立了一个新范式。\n2. 设计了两种关键技术使掩码扩散模型适用于动作生成：用于简化领域适应的局部化特殊令牌分类策略，以及用于无缝集成到动作生成中的分层动作结构化解码策略。\n3. 在SimplerEnv和CALVIN模拟基准以及WidowX真实机器人上的广泛实验，验证了LLaDA-VLA的卓越性能，凸显了d-VLMs在机器人操作中的潜力。\n### 论文方法描述\n\n1. **模型架构**:\n * 模型基于LLaDA-V预训练d-VLM，采用SigLIP-2作为视觉编码器，MLP作为投影器。\n * 输入为语言指令和RGB图像。图像通过视觉编码器提取特征，经投影后与文本令牌拼接，一同输入到大语言扩散模型中。\n * **动作分词化**: 将连续的动作值离散化为32个区间，并引入对应数量的特殊动作令牌。一个包含7维（3个位置、3个旋转、1个夹爪状态）的动作由7个特殊令牌表示。模型一次性预测一个包含K个时间步的动作块（K*D个令牌）。\n2. **局部化特殊令牌分类**:\n * 在训练和推理时，模型不进行全词汇表分类，而是仅在新增的特殊动作令牌集合上进行分类。\n * 损失函数也相应修改，只计算在掩码位置上的动作令牌的交叉熵损失，忽略其他令牌。此举缩小了学习目标，使模型更容易适应机器人领域。\n3. **分层动作结构化解码**:\n * 在标准的掩码扩散“预测-重掩码”流程基础上，引入了针对动作块的分层解码。\n * **动作级解码**: 每个迭代步骤，计算每个动作（包含D个令牌）的置信度分数（将内部令牌的置信度求和）。保留置信度最高的动作，将其他所有动作重新掩码。\n * **令牌级解码**: 在保留的动作内部，根据令牌级置信度进行排序，保留高置信度令牌，重掩码其余令牌。\n * 通过迭代此过程，模型能以动作为单位逐步完善输出，从而生成结构合理、前后一致的动作序列。\n### 论文使用数据集和训练资源\n\n* **数据集**:\n * **SimplerEnv**: 使用WidowX机器人在Visual Matching设置下进行评估，包含4项操作任务。\n * **CALVIN**: 一个长视野、语言条件操作的基准。采用ABC-D协议，在A、B、C环境上训练，在D环境上评估泛化性能。\n * **真实世界WidowX机器人**: 使用WidowX 250S机器人，评估了4个域内任务和4个分布外（OOD）泛化任务。\n* **训练资源和细节**:\n * 基于开源的LLaDA-V预训练权重进行微调。\n * 训练3个周期，学习率为2e-5，批次大小为128。\n * 动作块大小K设置为5，预测的是增量动作。\n * 推理时使用10次迭代扩散步骤，并采用dllm-cache方法加速解码。\n### 论文使用的评估环境和评估指标\n\n* **评估环境**:\n * **模拟环境**: SimplerEnv和CALVIN。\n * **真实机器人**: WidowX 250S机械臂。\n* **评估指标**:\n * **SimplerEnv**: 任务成功率（%）。\n * **CALVIN**: （1）连续完成1到5个任务的成功率（%）；（2）连续完成5个任务的平均长度。\n * **真实机器人**: 域内任务和分布外（OOD）任务的任务成功率（%）。",
    "summary_html": "<h3>论文研究单位</h3>\n\n<p>中国科学技术大学、南京大学、Dexmal</p>\n<h3>论文概述</h3>\n\n<p>该论文提出了LLaDA-VLA，这是首个基于预训练扩散视觉语言模型构建的视觉-语言-扩散-动作模型，用于机器人操作任务。为了解决预训练d-VLMs与机器人动作生成之间的领域差距以及掩码扩散范式在生成结构化动作序列方面的不足，论文引入了两个核心设计：一是局部化特殊令牌分类策略，通过将分类空间限制在特殊动作令牌上，降低了模型适应的难度；二是分层动作结构化解码策略，该策略在解码过程中显式地考虑动作内部和动作之间的依赖关系，生成了更连贯、精确的动作序列。大量的实验表明，LLaDA-VLA在模拟环境和真实机器人上的性能均显著优于现有的最先进视觉-语言-动作模型。</p>\n<h3>论文核心贡献点</h3>\n\n<ol><li>提出了首个基于预训练d-VLMs构建的视觉-语言-扩散-动作模型LLaDA-VLA，为机器人策略学习建立了一个新范式。</li><li>设计了两种关键技术使掩码扩散模型适用于动作生成：用于简化领域适应的局部化特殊令牌分类策略，以及用于无缝集成到动作生成中的分层动作结构化解码策略。</li><li>在SimplerEnv和CALVIN模拟基准以及WidowX真实机器人上的广泛实验，验证了LLaDA-VLA的卓越性能，凸显了d-VLMs在机器人操作中的潜力。</li></ol>\n<h3>论文方法描述</h3>\n\n<ol><li><strong>模型架构</strong>:</li></ol>\n<p> * 模型基于LLaDA-V预训练d-VLM，采用SigLIP-2作为视觉编码器，MLP作为投影器。</p>\n<p> * 输入为语言指令和RGB图像。图像通过视觉编码器提取特征，经投影后与文本令牌拼接，一同输入到大语言扩散模型中。</p>\n<p> * <strong>动作分词化</strong>: 将连续的动作值离散化为32个区间，并引入对应数量的特殊动作令牌。一个包含7维（3个位置、3个旋转、1个夹爪状态）的动作由7个特殊令牌表示。模型一次性预测一个包含K个时间步的动作块（K*D个令牌）。</p>\n<ol><li><strong>局部化特殊令牌分类</strong>:</li></ol>\n<p> * 在训练和推理时，模型不进行全词汇表分类，而是仅在新增的特殊动作令牌集合上进行分类。</p>\n<p> * 损失函数也相应修改，只计算在掩码位置上的动作令牌的交叉熵损失，忽略其他令牌。此举缩小了学习目标，使模型更容易适应机器人领域。</p>\n<ol><li><strong>分层动作结构化解码</strong>:</li></ol>\n<p> * 在标准的掩码扩散“预测-重掩码”流程基础上，引入了针对动作块的分层解码。</p>\n<p> * <strong>动作级解码</strong>: 每个迭代步骤，计算每个动作（包含D个令牌）的置信度分数（将内部令牌的置信度求和）。保留置信度最高的动作，将其他所有动作重新掩码。</p>\n<p> * <strong>令牌级解码</strong>: 在保留的动作内部，根据令牌级置信度进行排序，保留高置信度令牌，重掩码其余令牌。</p>\n<p> * 通过迭代此过程，模型能以动作为单位逐步完善输出，从而生成结构合理、前后一致的动作序列。</p>\n<h3>论文使用数据集和训练资源</h3>\n\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> * <strong>SimplerEnv</strong>: 使用WidowX机器人在Visual Matching设置下进行评估，包含4项操作任务。</p>\n<p> * <strong>CALVIN</strong>: 一个长视野、语言条件操作的基准。采用ABC-D协议，在A、B、C环境上训练，在D环境上评估泛化性能。</p>\n<p> * <strong>真实世界WidowX机器人</strong>: 使用WidowX 250S机器人，评估了4个域内任务和4个分布外（OOD）泛化任务。</p>\n<ul><li><strong>训练资源和细节</strong>:</li></ul>\n<p> * 基于开源的LLaDA-V预训练权重进行微调。</p>\n<p> * 训练3个周期，学习率为2e-5，批次大小为128。</p>\n<p> * 动作块大小K设置为5，预测的是增量动作。</p>\n<p> * 推理时使用10次迭代扩散步骤，并采用dllm-cache方法加速解码。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> * <strong>模拟环境</strong>: SimplerEnv和CALVIN。</p>\n<p> * <strong>真实机器人</strong>: WidowX 250S机械臂。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> * <strong>SimplerEnv</strong>: 任务成功率（%）。</p>\n<p> * <strong>CALVIN</strong>: （1）连续完成1到5个任务的成功率（%）；（2）连续完成5个任务的平均长度。</p>\n<p> * <strong>真实机器人</strong>: 域内任务和分布外（OOD）任务的任务成功率（%）。</p>"
  },
  {
    "date": "2025-09-06",
    "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning",
    "link": "http://arxiv.org/abs/2509.05614",
    "summary_markdown": "### 论文研究单位\n未在提供的文本中明确提及。\n### 论文概述\n本文提出了一种名为SpecPrune-VLA的免训练剪枝方法，用于加速视觉-语言-动作（VLA）模型的推理。该方法通过结合当前动作生成的局部信息和先前动作的全局信息来选择token，实现两级剪枝和启发式控制。实验表明，在LIBERO模拟基准测试中，相比高性能模型OpenVLA-OFT，该方法在NVIDIA A800和RTX 3090 GPU上分别实现了1.46倍和1.57倍的平均加速，且任务成功率损失可忽略不计。\n### 论文核心贡献点\n1. 提出了一种新见解：连续动作之间的信息高度相似，应结合当前动作的局部信息和先前动作的全局信息进行token选择。\n2. 设计了SpecPrune-VLA框架，包含三个技术：动作级静态token剪枝、层级动态token剪枝和轻量级动作感知控制器。\n3. 实现了在不同硬件平台上的高效推理加速，且保持任务成功率。\n### 论文方法描述\n1. **动作级静态剪枝**：利用先前动作的全局注意力信息选择top-K全局重要token，通过基于速度的帧比较和自推测token选择补充动态token和任务相关token，融合局部和全局选择后剪枝50-70%的视觉token。\n2. **层级动态剪枝**：通过动态更新token的重要性分数并在不同层重新评估token重要性，引入基于排名的权重和层级置信度分数计算最终分数。\n3. **轻量级动作感知控制器**：根据末端执行器速度将动作分为粗粒度（如大位移）和细粒度（如抓取），动态调整剪枝策略，以平衡速度和精度。\n### 论文使用数据集和训练资源\n- **数据集**：LIBERO模拟基准，包含四个任务套件（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个套件10个任务。\n- **训练资源**：未提及训练细节，方法为免训练，基于OpenVLA-OFT实现，实验在NVIDIA A800-80GB和RTX 3090 GPU进行。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Linux工作站，使用NVIDIA A800-80GB和RTX 3090 GPU，模拟环境为Franka Emika Panda机械臂。\n- **评估指标**：\n - **任务成功率（Success Rate, SR）**：衡量任务完成的百分比。\n - **延迟（Latency）**：从接收输入到生成动作的时间，单位毫秒。\n - **加速比（Speedup）**：相对于基线模型的延迟减少倍数。\n - **FLOPs**：计算量，用于量化剪枝效果。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>未在提供的文本中明确提及。</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为SpecPrune-VLA的免训练剪枝方法，用于加速视觉-语言-动作（VLA）模型的推理。该方法通过结合当前动作生成的局部信息和先前动作的全局信息来选择token，实现两级剪枝和启发式控制。实验表明，在LIBERO模拟基准测试中，相比高性能模型OpenVLA-OFT，该方法在NVIDIA A800和RTX 3090 GPU上分别实现了1.46倍和1.57倍的平均加速，且任务成功率损失可忽略不计。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新见解：连续动作之间的信息高度相似，应结合当前动作的局部信息和先前动作的全局信息进行token选择。</li><li>设计了SpecPrune-VLA框架，包含三个技术：动作级静态token剪枝、层级动态token剪枝和轻量级动作感知控制器。</li><li>实现了在不同硬件平台上的高效推理加速，且保持任务成功率。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>动作级静态剪枝</strong>：利用先前动作的全局注意力信息选择top-K全局重要token，通过基于速度的帧比较和自推测token选择补充动态token和任务相关token，融合局部和全局选择后剪枝50-70%的视觉token。</li><li><strong>层级动态剪枝</strong>：通过动态更新token的重要性分数并在不同层重新评估token重要性，引入基于排名的权重和层级置信度分数计算最终分数。</li><li><strong>轻量级动作感知控制器</strong>：根据末端执行器速度将动作分为粗粒度（如大位移）和细粒度（如抓取），动态调整剪枝策略，以平衡速度和精度。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：LIBERO模拟基准，包含四个任务套件（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个套件10个任务。</li><li><strong>训练资源</strong>：未提及训练细节，方法为免训练，基于OpenVLA-OFT实现，实验在NVIDIA A800-80GB和RTX 3090 GPU进行。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Linux工作站，使用NVIDIA A800-80GB和RTX 3090 GPU，模拟环境为Franka Emika Panda机械臂。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率（Success Rate, SR）</strong>：衡量任务完成的百分比。</p>\n<p> - <strong>延迟（Latency）</strong>：从接收输入到生成动作的时间，单位毫秒。</p>\n<p> - <strong>加速比（Speedup）</strong>：相对于基线模型的延迟减少倍数。</p>\n<p> - <strong>FLOPs</strong>：计算量，用于量化剪枝效果。</p>"
  },
  {
    "date": "2025-09-05",
    "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies",
    "link": "http://arxiv.org/abs/2509.04996",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-04",
    "title": "Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models",
    "link": "http://arxiv.org/abs/2509.04063",
    "summary_markdown": "### 论文研究单位\n论文作者包括 Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang。单位信息部分缺失，但通讯作者 Donglin Wang 来自西湖大学。\n### 论文概述\n针对基于流匹配的视觉-语言-行动（VLA）模型在复杂任务中行动准确性不足的问题，论文提出了一种离线强化学习后训练方法 ARFM（自适应强化流匹配）。该方法通过在 VLA 流模型损失中引入自适应缩放因子，理论构建了一个权衡偏差-方差的优化目标，以平衡强化学习信号和梯度方差影响，从而实现更稳定和高效的微调过程。实验表明 ARFM 在泛化、鲁棒性、少样本学习和持续学习方面表现优异。\n### 论文核心贡献点\n1. 提出 ARFM 方法，一种用于 VLA 流模型的新型离线强化学习后训练方法，能够自适应调整数据质量分布。\n2. 理论建立缩放因子 α 的优化目标，并诱导二分迭代算法进行实时更新，实现高效 VLA 流模型微调。\n3. 通过仿真和真实机器人操作任务实验，验证 ARFM 在泛化能力、对动态扰动的鲁棒性、少样本学习和持续学习场景中的卓越性能。\n### 论文方法描述\n方法基于能量加权流匹配（EWFM）框架：\n- **能量加权 VLA 流模型**：构建能量引导分布 π(A_t\\|o_t) ∝ p(A_t\\|o_t) exp(α R*(A_t, o_t))，其中 R* 为标准化强化学习优势，α 为缩放因子。使用条件能量加权流匹配（CEFM）损失优化向量场，学习策略分布。\n- **自适应缩放因子 α 调整**：通过最小化目标函数 J(α) = Var(ĝ(α)) - λ S(α) 实现信号与方差的权衡，其中 ĝ(α) 是损失梯度，S(α) 是强化学习优势评分函数。基于高斯假设，推导出 J(α) 的具体形式，并通过二分迭代算法（Algorithm 1）实时求解最优 α*。最终算法（Algorithm 2）集成到 VLA 流模型微调中。\n### 论文使用数据集和训练资源\n- **数据集**：使用 LIBERO 仿真基准（包括 Object、Long、Spatial 和 Goal 四个任务套件）和真实世界 UR5 机器人平台（三个抓取和放置任务）。\n- **训练资源**：论文未明确指定硬件资源，但假设采用标准计算环境（如 GPU）进行仿真和实验。模型基于流匹配实现，训练过程涉及数据采样和损失优化。\n### 论文使用的评估环境和评估指标\n- **评估环境**： LIBERO 仿真环境和 UR5 真实机器人实验平台。\n- **评估指标**：主要使用成功率（Success Rate, SR）衡量任务完成度；在动作扰动实验中，添加高斯噪声（0.1 至 0.3 级别）评估鲁棒性，通过扰动后成功率评估性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>论文作者包括 Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang。单位信息部分缺失，但通讯作者 Donglin Wang 来自西湖大学。</p>\n<h3>论文概述</h3>\n<p>针对基于流匹配的视觉-语言-行动（VLA）模型在复杂任务中行动准确性不足的问题，论文提出了一种离线强化学习后训练方法 ARFM（自适应强化流匹配）。该方法通过在 VLA 流模型损失中引入自适应缩放因子，理论构建了一个权衡偏差-方差的优化目标，以平衡强化学习信号和梯度方差影响，从而实现更稳定和高效的微调过程。实验表明 ARFM 在泛化、鲁棒性、少样本学习和持续学习方面表现优异。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出 ARFM 方法，一种用于 VLA 流模型的新型离线强化学习后训练方法，能够自适应调整数据质量分布。</li><li>理论建立缩放因子 α 的优化目标，并诱导二分迭代算法进行实时更新，实现高效 VLA 流模型微调。</li><li>通过仿真和真实机器人操作任务实验，验证 ARFM 在泛化能力、对动态扰动的鲁棒性、少样本学习和持续学习场景中的卓越性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>方法基于能量加权流匹配（EWFM）框架：</p>\n<ul><li><strong>能量加权 VLA 流模型</strong>：构建能量引导分布 π(A_t\\|o_t) ∝ p(A_t\\|o_t) exp(α R*(A_t, o_t))，其中 R* 为标准化强化学习优势，α 为缩放因子。使用条件能量加权流匹配（CEFM）损失优化向量场，学习策略分布。</li><li><strong>自适应缩放因子 α 调整</strong>：通过最小化目标函数 J(α) = Var(ĝ(α)) - λ S(α) 实现信号与方差的权衡，其中 ĝ(α) 是损失梯度，S(α) 是强化学习优势评分函数。基于高斯假设，推导出 J(α) 的具体形式，并通过二分迭代算法（Algorithm 1）实时求解最优 α*。最终算法（Algorithm 2）集成到 VLA 流模型微调中。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：使用 LIBERO 仿真基准（包括 Object、Long、Spatial 和 Goal 四个任务套件）和真实世界 UR5 机器人平台（三个抓取和放置任务）。</li><li><strong>训练资源</strong>：论文未明确指定硬件资源，但假设采用标准计算环境（如 GPU）进行仿真和实验。模型基于流匹配实现，训练过程涉及数据采样和损失优化。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>： LIBERO 仿真环境和 UR5 真实机器人实验平台。</li><li><strong>评估指标</strong>：主要使用成功率（Success Rate, SR）衡量任务完成度；在动作扰动实验中，添加高斯噪声（0.1 至 0.3 级别）评估鲁棒性，通过扰动后成功率评估性能。</li></ul>"
  },
  {
    "date": "2025-09-04",
    "title": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction",
    "link": "http://arxiv.org/abs/2509.04018",
    "summary_markdown": "### 论文研究单位\n南开大学机器人与自动化信息系统研究所、小米汽车、东北大学机器人科学与工程学院、澳门大学\n### 论文概述\n论文提出了FPC-VLA框架，这是一个结合视觉-语言-动作（VLA）模型与监督器的双模型系统，旨在预测和修正机器人操作中的潜在失败。传统VLA模型缺乏失败恢复机制，而该框架通过VLM监督器评估动作可行性并生成修正策略，同时引入相似性引导的动作融合模块提升动作平滑性，在仿真和真实环境中均表现出优越性能。\n### 论文核心贡献点\n1. 提出基于VLM的监督器，通过结构化提示和图像输入评估夹爪动作，预测潜在失败并生成修正指令。\n2. 设计相似性引导的动作融合模块，聚合历史动作预测以生成更平滑、可靠的机器人运动。\n3. 在SIMPLER和LIBERO基准测试中，使用WidowX、Google Robot和Franka机器人进行评估，FPC-VLA在零样本和微调设置下均超越最先进方法。\n4. 在真实机器人上成功部署多样化长视野任务，验证了框架的泛化能力和实用价值。\n### 论文方法描述\n1. **架构概述**：输入为RGB图像和自然语言指令，输出为精细化动作。包含三个模块：数据集生成、VLM监督器和动作融合。\n2. **数据集生成**：从RLDS格式数据中自动生成故障预测修正数据集，通过检测夹爪状态变化识别抓取事件，将姿态差转换为结构化语言描述。\n3. **VLM监督器**：当夹爪状态变化超过阈值时触发，输入当前图像和提示，输出“是”或“否”及修正方向/幅度（如“向上移动. 大. 顺时针旋转. 小”）。\n4. **动作融合模块**：收集历史动作预测，计算与最新预测的余弦相似性，结合时间衰减权重生成融合动作，提高鲁棒性。\n### 论文使用数据集和训练资源\n1. **数据集**：基于BridgeV2（408,771条目）、RT-1（728,760条目）和RT-X（15,604条目）生成故障预测修正数据集。\n2. **训练资源**：\n - VLA模型：8块NVIDIA H100 GPU，批量大小256，学习率2×10⁻⁵，预测15步动作。\n - 监督器模型：Qwen2.5-vl 7B，bfloat16精度，批量大小64，学习率10⁻⁴，LoRA秩8。\n - 动作融合参数：α=0.1, λ=β=0.01，动作窗口大小N根据任务调整（2-11）。\n### 论文使用的评估环境和评估指标\n1. **仿真环境**：\n - SIMPLER：测试WidowX（抓取、堆叠任务）和Google Robot（抓取、抽屉操作任务）。\n - LIBERO：测试Franka机器人，包括目标导向、空间配置、对象多样性和长视野任务。\n2. **真实环境**：小米机器人和ALOHA平台，执行抓取、堆叠等长视野任务。\n3. **评估指标**：\n - 抓取成功率（Grasp Success Rate）。\n - 任务成功率（Task Success Rate）。\n - 平均抓取成功率（Avg. Grasp）和平均任务成功率（Avg. Success）。\n - 执行时间对比（关键帧1.766s vs 非关键帧0.176s）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南开大学机器人与自动化信息系统研究所、小米汽车、东北大学机器人科学与工程学院、澳门大学</p>\n<h3>论文概述</h3>\n<p>论文提出了FPC-VLA框架，这是一个结合视觉-语言-动作（VLA）模型与监督器的双模型系统，旨在预测和修正机器人操作中的潜在失败。传统VLA模型缺乏失败恢复机制，而该框架通过VLM监督器评估动作可行性并生成修正策略，同时引入相似性引导的动作融合模块提升动作平滑性，在仿真和真实环境中均表现出优越性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出基于VLM的监督器，通过结构化提示和图像输入评估夹爪动作，预测潜在失败并生成修正指令。</li><li>设计相似性引导的动作融合模块，聚合历史动作预测以生成更平滑、可靠的机器人运动。</li><li>在SIMPLER和LIBERO基准测试中，使用WidowX、Google Robot和Franka机器人进行评估，FPC-VLA在零样本和微调设置下均超越最先进方法。</li><li>在真实机器人上成功部署多样化长视野任务，验证了框架的泛化能力和实用价值。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>架构概述</strong>：输入为RGB图像和自然语言指令，输出为精细化动作。包含三个模块：数据集生成、VLM监督器和动作融合。</li><li><strong>数据集生成</strong>：从RLDS格式数据中自动生成故障预测修正数据集，通过检测夹爪状态变化识别抓取事件，将姿态差转换为结构化语言描述。</li><li><strong>VLM监督器</strong>：当夹爪状态变化超过阈值时触发，输入当前图像和提示，输出“是”或“否”及修正方向/幅度（如“向上移动. 大. 顺时针旋转. 小”）。</li><li><strong>动作融合模块</strong>：收集历史动作预测，计算与最新预测的余弦相似性，结合时间衰减权重生成融合动作，提高鲁棒性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：基于BridgeV2（408,771条目）、RT-1（728,760条目）和RT-X（15,604条目）生成故障预测修正数据集。</li><li><strong>训练资源</strong>：</li></ol>\n<p> - VLA模型：8块NVIDIA H100 GPU，批量大小256，学习率2×10⁻⁵，预测15步动作。</p>\n<p> - 监督器模型：Qwen2.5-vl 7B，bfloat16精度，批量大小64，学习率10⁻⁴，LoRA秩8。</p>\n<p> - 动作融合参数：α=0.1, λ=β=0.01，动作窗口大小N根据任务调整（2-11）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>仿真环境</strong>：</li></ol>\n<p> - SIMPLER：测试WidowX（抓取、堆叠任务）和Google Robot（抓取、抽屉操作任务）。</p>\n<p> - LIBERO：测试Franka机器人，包括目标导向、空间配置、对象多样性和长视野任务。</p>\n<ol><li><strong>真实环境</strong>：小米机器人和ALOHA平台，执行抓取、堆叠等长视野任务。</li><li><strong>评估指标</strong>：</li></ol>\n<p> - 抓取成功率（Grasp Success Rate）。</p>\n<p> - 任务成功率（Task Success Rate）。</p>\n<p> - 平均抓取成功率（Avg. Grasp）和平均任务成功率（Avg. Success）。</p>\n<p> - 执行时间对比（关键帧1.766s vs 非关键帧0.176s）。</p>"
  },
  {
    "date": "2025-09-03",
    "title": "ANNIE: Be Careful of Your Robots",
    "link": "http://arxiv.org/abs/2509.03383",
    "summary_markdown": "## 论文研究单位\n\n中国科学院计算技术研究所、中国科学院自动化研究所、佐治亚理工学院、德克萨斯大学达拉斯分校\n## 论文概述\n\n论文研究了具身AI（EAI）系统中的对抗性安全攻击问题。针对EAI系统将视觉-语言-动作（VLA）模型整合到机器人中带来的新安全风险，提出了第一个系统性的对抗安全攻击研究框架。论文基于ISO/TS 15066标准，建立了从传统机器学习安全向物理AI安全转变的范式转移，并通过仿真和真实机器人实验验证了攻击效果。\n## 论文核心贡献点\n\n- **安全定义与分类**：提出了基于ISO/TS 15066标准的EAI系统安全定义，将安全违规分为critical、dangerous、risky三个等级\n- **基准测试数据集**：构建了Annie-Bench基准，包含9个安全关键场景和2400个视频动作序列，专注于评估EAI安全性能\n- **攻击框架**：开发了Annie-Attack任务感知对抗框架，通过攻击领导者模型将长期目标分解为帧级扰动\n- **实证验证**：在ACT和Baku两个代表性VLA模型上验证了攻击效果，各安全类别攻击成功率均超过50%\n## 论文方法描述\n\n- **安全标准定义**：基于人机协作ISO标准，定义了距离约束、速度约束和碰撞约束三个安全标准\n- **攻击框架设计**：Annie-Attack包含攻击领导者模型，将高层攻击目标转换为帧级动作扰动，使用PGD方法生成对抗样本\n- **行动空间设置**：使用末端执行器的4维动作表示（3维笛卡尔空间移动+1维夹爪状态）\n- **稀疏攻击策略**：提出自适应稀疏攻击，根据动作序列阶段动态调整攻击频率，在早期阶段使用高频率攻击，在后期阶段降低频率\n## 论文使用数据集和训练资源\n\n- **仿真环境**：基于ManiSkill仿真平台构建数据集\n- **硬件配置**：使用Franka Emika Panda 7自由度机械臂，配备2自由度夹爪和双摄像头系统\n- **数据集规模**：生成2400个视频动作序列，包含9个不同安全级别场景\n- **训练数据**：每个场景约240个演示序列用于训练专门的攻击领导者模型\n- **评估数据**：每个场景20个测试序列用于评估\n## 论文使用的评估环境和评估指标\n\n- **评估环境**：仿真环境中的9个测试场景，覆盖所有三个安全违规等级\n- **评估指标**：\n - 攻击成功率（ASR）：衡量安全约束规则被违反的百分比\n - 动作一致性（AC）：评估攻击是否导致动作序列中的突然变化\n - 动作偏差（AD）：测量对抗动作与原始动作分布的偏离程度\n - 任务成功率变化（TSRC）：攻击前后任务成功率的变化百分比\n- **基线模型**：在ACT和Baku两个VLA模型上进行评估\n- **真实实验**：在真实Franka Panda机器人上验证了攻击的物理影响",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n\n<p>中国科学院计算技术研究所、中国科学院自动化研究所、佐治亚理工学院、德克萨斯大学达拉斯分校</p>\n<h2 class=\"section-title\">论文概述</h2>\n\n<p>论文研究了具身AI（EAI）系统中的对抗性安全攻击问题。针对EAI系统将视觉-语言-动作（VLA）模型整合到机器人中带来的新安全风险，提出了第一个系统性的对抗安全攻击研究框架。论文基于ISO/TS 15066标准，建立了从传统机器学习安全向物理AI安全转变的范式转移，并通过仿真和真实机器人实验验证了攻击效果。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n\n<ul><li><strong>安全定义与分类</strong>：提出了基于ISO/TS 15066标准的EAI系统安全定义，将安全违规分为critical、dangerous、risky三个等级</li><li><strong>基准测试数据集</strong>：构建了Annie-Bench基准，包含9个安全关键场景和2400个视频动作序列，专注于评估EAI安全性能</li><li><strong>攻击框架</strong>：开发了Annie-Attack任务感知对抗框架，通过攻击领导者模型将长期目标分解为帧级扰动</li><li><strong>实证验证</strong>：在ACT和Baku两个代表性VLA模型上验证了攻击效果，各安全类别攻击成功率均超过50%</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n\n<ul><li><strong>安全标准定义</strong>：基于人机协作ISO标准，定义了距离约束、速度约束和碰撞约束三个安全标准</li><li><strong>攻击框架设计</strong>：Annie-Attack包含攻击领导者模型，将高层攻击目标转换为帧级动作扰动，使用PGD方法生成对抗样本</li><li><strong>行动空间设置</strong>：使用末端执行器的4维动作表示（3维笛卡尔空间移动+1维夹爪状态）</li><li><strong>稀疏攻击策略</strong>：提出自适应稀疏攻击，根据动作序列阶段动态调整攻击频率，在早期阶段使用高频率攻击，在后期阶段降低频率</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n\n<ul><li><strong>仿真环境</strong>：基于ManiSkill仿真平台构建数据集</li><li><strong>硬件配置</strong>：使用Franka Emika Panda 7自由度机械臂，配备2自由度夹爪和双摄像头系统</li><li><strong>数据集规模</strong>：生成2400个视频动作序列，包含9个不同安全级别场景</li><li><strong>训练数据</strong>：每个场景约240个演示序列用于训练专门的攻击领导者模型</li><li><strong>评估数据</strong>：每个场景20个测试序列用于评估</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n\n<ul><li><strong>评估环境</strong>：仿真环境中的9个测试场景，覆盖所有三个安全违规等级</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 攻击成功率（ASR）：衡量安全约束规则被违反的百分比</p>\n<p> - 动作一致性（AC）：评估攻击是否导致动作序列中的突然变化</p>\n<p> - 动作偏差（AD）：测量对抗动作与原始动作分布的偏离程度</p>\n<p> - 任务成功率变化（TSRC）：攻击前后任务成功率的变化百分比</p>\n<ul><li><strong>基线模型</strong>：在ACT和Baku两个VLA模型上进行评估</li><li><strong>真实实验</strong>：在真实Franka Panda机器人上验证了攻击的物理影响</li></ul>"
  },
  {
    "date": "2025-09-02",
    "title": "Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance",
    "link": "http://arxiv.org/abs/2509.02055",
    "summary_markdown": "### 论文研究单位\n- Institute of Artificial Intelligence, China Telecom\n- Tsinghua University\n- The Chinese University of Hong Kong, Shenzhen\n- Northwestern Polytechnical University\n### 论文概述\nVision-Language-Action（VLA）模型在预训练后能处理多任务机器人操作，但跨 embodiment（如单臂到双臂）和跨任务的适应常因动作分布不匹配而效率低下。本文提出Align-Then-stEer（ATE）框架，通过两阶段策略实现高效适应：先构建统一动作潜在空间桥接预训练和适应数据分布，再以分类器引导机制在潜在空间中引导VLA生成过程向目标域偏移。实验在仿真（RoboTwin 1.0、ManiSkill3）和真实双臂机器人上验证，ATE显著提升成功率，仿真平均增益9.8%，真实跨 embodiment场景增益达32%。\n### 论文核心贡献点\n- 提出动作分布对齐策略，利用反向KL散度的模式寻求特性构建统一潜在空间，将适应动作嵌入预训练潜在分布模式中。\n- 设计基于统一潜在空间的分类器引导机制，无缝集成至扩散/流匹配VLA训练目标，实现精确快速适应。\n- ATE为即插即用设计，模型无关且计算开销小，仅需训练两个轻量级VAEs。\n- 跨 embodiment、任务和架构（扩散/流匹配）验证有效性，突出适应效率提升。\n### 论文方法描述\nATE分两阶段：\n1. **统一动作潜在空间对齐（Stage 1）**：\n - 预训练阶段：在预训练动作数据（如DROID、Open X-Embodiment）上训练InfoVAE（V_pretrain），学习潜在分布。\n - 适应阶段：在适应数据（目标 embodiment）上训练另一个InfoVAE（V_adaptation），通过最小化反向KL散度 D_KL(q_ψ(z\\|ã) \\|\\|q_φ(z))，将适应潜在分布嵌入预训练潜在空间的特定模式，生成统一潜在空间Z。\n2. **分类器引导适应（Stage 2）**：\n - 定义引导函数g = -∇\\|\\|E_ψ(â_t:t+h^k) - E_ψ(a_t:t+h^0)\\|\\|²，度量中间动作与目标动作在潜在空间中的距离。\n - 对扩散模型，修改噪声预测为 ε̂ = ε_θ - √(1-ā_k)g，并嵌入训练目标 L(θ) = E[\\|\\|ε - ε_θ(...) + √(1-ā_k)·λ·g\\|\\|²]。\n - 对流匹配模型，修改速度场为 v̂_θ = v_θ + ((1-τ)/τ)·λ·g，并嵌入训练目标。\n 引导机制确保VLA输出保持于统一潜在空间内，在适配中保留预训练知识。\n### 论文使用数据集和训练资源\n- **数据集来源**：\n - 预训练：大规模机器人数据（如DROID、Open X-Embodiment子集、Kuka、ALOHA）。\n - 适应：仿真（RoboTwin 1.0含17任务，ManiSkill3含2任务）、真实（双RealMan 7-DoF机器人长期任务）。\n- **训练资源**：\n - InfoVAE训练分两步：Step 1用通用数据（3000 episodes）训练潜在结构（约12小时）；Step 2用域特定数据微调（RoboTwin/ManiSkill每任务50-100轨迹，真实每任务50轨迹，约0.5小时）。\n - 潜在维度512，训练含互信息项以增强表示。\n### 论文使用的评估环境和评估指标\n- **仿真环境**：\n - RoboTwin 1.0基准：17项单/双臂任务（工具调整、双瓶拾取）。\n - ManiSkill3基准：2项接触丰富单臂操作（推立方体、拾取立方体）。\n- **真实环境**：\n - 双RealMan 7-DoF双臂机器人：4项长期任务（插入、协调操作）和工具使用任务。\n- **评估指标**：\n - 成功率（Success Rate）：以任务完成百分比衡量。\n - 样本效率：比较达到性能阈值所需训练步数（如RDT基线90k步 vs ATE 70k步）。\n - 泛化能力：在光照、物体位置、视觉干扰下测试鲁棒性。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>Institute of Artificial Intelligence, China Telecom</li><li>Tsinghua University</li><li>The Chinese University of Hong Kong, Shenzhen</li><li>Northwestern Polytechnical University</li></ul>\n<h3>论文概述</h3>\n<p>Vision-Language-Action（VLA）模型在预训练后能处理多任务机器人操作，但跨 embodiment（如单臂到双臂）和跨任务的适应常因动作分布不匹配而效率低下。本文提出Align-Then-stEer（ATE）框架，通过两阶段策略实现高效适应：先构建统一动作潜在空间桥接预训练和适应数据分布，再以分类器引导机制在潜在空间中引导VLA生成过程向目标域偏移。实验在仿真（RoboTwin 1.0、ManiSkill3）和真实双臂机器人上验证，ATE显著提升成功率，仿真平均增益9.8%，真实跨 embodiment场景增益达32%。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出动作分布对齐策略，利用反向KL散度的模式寻求特性构建统一潜在空间，将适应动作嵌入预训练潜在分布模式中。</li><li>设计基于统一潜在空间的分类器引导机制，无缝集成至扩散/流匹配VLA训练目标，实现精确快速适应。</li><li>ATE为即插即用设计，模型无关且计算开销小，仅需训练两个轻量级VAEs。</li><li>跨 embodiment、任务和架构（扩散/流匹配）验证有效性，突出适应效率提升。</li></ul>\n<h3>论文方法描述</h3>\n<p>ATE分两阶段：</p>\n<ol><li><strong>统一动作潜在空间对齐（Stage 1）</strong>：</li></ol>\n<p> - 预训练阶段：在预训练动作数据（如DROID、Open X-Embodiment）上训练InfoVAE（V_pretrain），学习潜在分布。</p>\n<p> - 适应阶段：在适应数据（目标 embodiment）上训练另一个InfoVAE（V_adaptation），通过最小化反向KL散度 D_KL(q_ψ(z\\|ã) \\|\\|q_φ(z))，将适应潜在分布嵌入预训练潜在空间的特定模式，生成统一潜在空间Z。</p>\n<ol><li><strong>分类器引导适应（Stage 2）</strong>：</li></ol>\n<p> - 定义引导函数g = -∇\\|\\|E_ψ(â_t:t+h^k) - E_ψ(a_t:t+h^0)\\|\\|²，度量中间动作与目标动作在潜在空间中的距离。</p>\n<p> - 对扩散模型，修改噪声预测为 ε̂ = ε_θ - √(1-ā_k)g，并嵌入训练目标 L(θ) = E[\\|\\|ε - ε_θ(...) + √(1-ā_k)·λ·g\\|\\|²]。</p>\n<p> - 对流匹配模型，修改速度场为 v̂_θ = v_θ + ((1-τ)/τ)·λ·g，并嵌入训练目标。</p>\n<p> 引导机制确保VLA输出保持于统一潜在空间内，在适配中保留预训练知识。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集来源</strong>：</li></ul>\n<p> - 预训练：大规模机器人数据（如DROID、Open X-Embodiment子集、Kuka、ALOHA）。</p>\n<p> - 适应：仿真（RoboTwin 1.0含17任务，ManiSkill3含2任务）、真实（双RealMan 7-DoF机器人长期任务）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - InfoVAE训练分两步：Step 1用通用数据（3000 episodes）训练潜在结构（约12小时）；Step 2用域特定数据微调（RoboTwin/ManiSkill每任务50-100轨迹，真实每任务50轨迹，约0.5小时）。</p>\n<p> - 潜在维度512，训练含互信息项以增强表示。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>仿真环境</strong>：</li></ul>\n<p> - RoboTwin 1.0基准：17项单/双臂任务（工具调整、双瓶拾取）。</p>\n<p> - ManiSkill3基准：2项接触丰富单臂操作（推立方体、拾取立方体）。</p>\n<ul><li><strong>真实环境</strong>：</li></ul>\n<p> - 双RealMan 7-DoF双臂机器人：4项长期任务（插入、协调操作）和工具使用任务。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（Success Rate）：以任务完成百分比衡量。</p>\n<p> - 样本效率：比较达到性能阈值所需训练步数（如RDT基线90k步 vs ATE 70k步）。</p>\n<p> - 泛化能力：在光照、物体位置、视觉干扰下测试鲁棒性。</p>"
  },
  {
    "date": "2025-09-02",
    "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving",
    "link": "http://arxiv.org/abs/2509.01944",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-31",
    "title": "OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving",
    "link": "http://arxiv.org/abs/2509.00789",
    "summary_markdown": "### 论文研究单位\n未在提供的HTML原文中明确提及。\n### 论文概述\nOmniReason是一个用于自动驾驶的时空引导视觉-语言-动作框架，旨在解决现有方法在动态驾驶场景中忽视时间维度的问题。该框架通过联合建模动态3D环境和决策过程，建立了鲁棒的时空推理能力。论文提出了OmniReason-Data数据集和OmniReason-Agent模型架构，数据集包含密集时空标注和自然语言解释，模型通过时空知识蒸馏实现可解释的决策。\n### 论文核心贡献点\n1. 提出OmniReason-nuScenes和OmniReason-Bench2Drive两个大规模视觉-语言-动作（VLA）数据集，包含密集时空标注和自然语言解释，通过减少幻觉的自动标注流水线生成。\n2. 设计OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器，通过时空知识蒸馏捕获因果推理模式，生成人类可解释的决策依据。\n3. 在开环规划和视觉问答（VQA）任务上实现最先进性能，显著提升安全性、舒适性和解释质量。\n### 论文方法描述\n1. **OmniReason-Data构建**：\n - 基于nuScenes和Bench2Drive，通过规则和原则模板整合人类先验知识，引导Qwen2.5VL 72B模型生成场景感知描述和因果推理链。\n - 流水线包括场景空间标注、人类先验知识引导和MLLM时序推理，确保物理合理性和时间连贯性。\n2. **OmniReason-Agent架构**：\n - 视觉主干网络通过分层编码将多视图输入转换为时空令牌，稀疏时间记忆模块使用记忆压缩查询聚合长期上下文。\n - VLM推理核心处理历史驾驶状态和语言指令，通过轻量适配器增强冻结VLM。\n - 运动感知归一化模块动态适应对象状态，混合注意力机制传播对象查询，跨模态聚合融合图像特征。\n3. **训练目标**：\n - 联合优化3D检测和场景理解，检测损失包括分类（Focal Loss）和回归（L1），车道分析损失类似。\n - LLM使用自回归交叉熵损失，总损失为感知和语言损失的加权和。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - OmniReason-nuScenes和OmniReason-Bench2Drive，基于nuScenes和Bench2Drive构建，包含多视图视频、对象标注和轨迹数据。\n - 涵盖环境描述、动态/静态对象、因果推理和动作标注，支持VQA和开环规划任务。\n2. **训练资源**：\n - 128块NVIDIA H20 GPU（96GB内存）。\n - 视觉编码器使用EVA-02-L（CLIP知识蒸馏预训练），基础模型为LLaVA v1.5。\n - 微调阶段使用AdamW优化器，批大小16，学习率分层：投影模块4e-4，视觉编码器和LLM为2e-5。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 开环规划任务在nuScenes基准测试，VQA任务在自定义OmniReason数据集评估。\n - 实施细节中提及使用标准训练配置和余弦退火调度。\n2. **评估指标**：\n - **开环规划**：L2位移误差（1/2/3秒）、平均碰撞率（CR）、违规率（IR）。\n - **VQA**：CIDEr、BLEU-1/4、METEOR、ROUGE-L、Precision、Recall，衡量语言理解和多模态对齐。\n - 消融实验中分析语言组件和记忆模块对BL-1、L2、CR、IR的影响。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>未在提供的HTML原文中明确提及。</p>\n<h3>论文概述</h3>\n<p>OmniReason是一个用于自动驾驶的时空引导视觉-语言-动作框架，旨在解决现有方法在动态驾驶场景中忽视时间维度的问题。该框架通过联合建模动态3D环境和决策过程，建立了鲁棒的时空推理能力。论文提出了OmniReason-Data数据集和OmniReason-Agent模型架构，数据集包含密集时空标注和自然语言解释，模型通过时空知识蒸馏实现可解释的决策。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出OmniReason-nuScenes和OmniReason-Bench2Drive两个大规模视觉-语言-动作（VLA）数据集，包含密集时空标注和自然语言解释，通过减少幻觉的自动标注流水线生成。</li><li>设计OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器，通过时空知识蒸馏捕获因果推理模式，生成人类可解释的决策依据。</li><li>在开环规划和视觉问答（VQA）任务上实现最先进性能，显著提升安全性、舒适性和解释质量。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>OmniReason-Data构建</strong>：</li></ol>\n<p> - 基于nuScenes和Bench2Drive，通过规则和原则模板整合人类先验知识，引导Qwen2.5VL 72B模型生成场景感知描述和因果推理链。</p>\n<p> - 流水线包括场景空间标注、人类先验知识引导和MLLM时序推理，确保物理合理性和时间连贯性。</p>\n<ol><li><strong>OmniReason-Agent架构</strong>：</li></ol>\n<p> - 视觉主干网络通过分层编码将多视图输入转换为时空令牌，稀疏时间记忆模块使用记忆压缩查询聚合长期上下文。</p>\n<p> - VLM推理核心处理历史驾驶状态和语言指令，通过轻量适配器增强冻结VLM。</p>\n<p> - 运动感知归一化模块动态适应对象状态，混合注意力机制传播对象查询，跨模态聚合融合图像特征。</p>\n<ol><li><strong>训练目标</strong>：</li></ol>\n<p> - 联合优化3D检测和场景理解，检测损失包括分类（Focal Loss）和回归（L1），车道分析损失类似。</p>\n<p> - LLM使用自回归交叉熵损失，总损失为感知和语言损失的加权和。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - OmniReason-nuScenes和OmniReason-Bench2Drive，基于nuScenes和Bench2Drive构建，包含多视图视频、对象标注和轨迹数据。</p>\n<p> - 涵盖环境描述、动态/静态对象、因果推理和动作标注，支持VQA和开环规划任务。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> - 128块NVIDIA H20 GPU（96GB内存）。</p>\n<p> - 视觉编码器使用EVA-02-L（CLIP知识蒸馏预训练），基础模型为LLaVA v1.5。</p>\n<p> - 微调阶段使用AdamW优化器，批大小16，学习率分层：投影模块4e-4，视觉编码器和LLM为2e-5。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - 开环规划任务在nuScenes基准测试，VQA任务在自定义OmniReason数据集评估。</p>\n<p> - 实施细节中提及使用标准训练配置和余弦退火调度。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>开环规划</strong>：L2位移误差（1/2/3秒）、平均碰撞率（CR）、违规率（IR）。</p>\n<p> - <strong>VQA</strong>：CIDEr、BLEU-1/4、METEOR、ROUGE-L、Precision、Recall，衡量语言理解和多模态对齐。</p>\n<p> - 消融实验中分析语言组件和记忆模块对BL-1、L2、CR、IR的影响。</p>"
  },
  {
    "date": "2025-08-30",
    "title": "Galaxea Open-World Dataset and G0 Dual-System VLA Model",
    "link": "http://arxiv.org/abs/2509.00576",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-30",
    "title": "Mechanistic interpretability for steering vision-language-action models",
    "link": "http://arxiv.org/abs/2509.00328",
    "summary_markdown": "### 论文研究单位\n加州大学伯克利分校电气工程与计算机科学系（University of California, Berkeley, Department of Electrical Engineering and Computer Sciences）\n### 论文概述\nVLA模型结合视觉和语言信息实现机器人动作控制，但缺乏类似传统机器人管道的可解释性机制。本研究将机械可解释性技术应用于VLA模型，通过分析模型内部激活单元的语义含义，实现无需微调的实时行为引导。核心创新在于提取FFN层的语义价值向量（value vectors），识别与动作选择因果相关的控制方向（如速度、方向），并将其作为实时控制接口激活。\n### 论文核心贡献点\n1. **语义保留发现**：VLA模型在训练中保留大量预训练语义概念（<25% FFN神经元用于动作预测，其余维持语义结构）\n2. **因果关联验证**：验证内部概念与动作的因果关系（如\"慢\"概念直接导致末端执行器缓慢移动）\n3. **实时控制接口**：首次实现基于内部表征的零样本行为控制方法，突破微调/环境交互依赖\n### 论文方法描述\n1. **价值向量提取**\n - 分析FFN输出层权重矩阵`Wθ`，提取独立于输入的固定值向量`wθ(i)`（式1-2）\n - 将值向量投影至标记空间，通过标记概率分布赋予语义含义\n\n2. **概念激活引导**\n - 通过kNN聚类或人工选择识别语义对齐的神经元簇`𝒮`（如\"up\"、\"slow\"）\n - 在推理时覆盖该簇的激活值为固定标量`α`（式3-4）\n - 剩余神经元保持原始激活，生成控制残差`Δx`影响最终动作标记分布\n\n3. **跨框架实现**\n - PyTorch框架：在OpenVLA的FFN下投影层应用前向钩子\n - JAX框架：修改π₀的FFN计算图插入引导算子`I𝒮^α`\n### 论文使用数据集和训练资源\n- **基础预训练数据**：Open X-Embodiment跨平台机器人数据集（OpenVLA/π₀预训练）\n- **模拟评估**：LIBERO-Long长程操作基准（10项任务，OpenVLA 7B模型）\n- **实物实验**：DROID平台数据用于π₀-FAST微调（LoRA方法，5000步）\n- **硬件资源**：\n - 模拟：NVIDIA H100 GPU\n - 实物：NVIDIA A4500 GPU + UR5机械臂（配Robotiq 2F-140夹爪）\n### 论文使用的评估环境和评估指标\n#### 模拟实验（OpenVLA/LIBERO）\n- **环境**：10个长程操作任务（抓取、放置、器具操作等）\n- **干预参数**：对比\"快/慢\"与\"上\"概念簇，测试不同层深度注入效果\n- **指标**：\n - 末端执行器位移变化率（平均提升27.73%，最大148.54%）\n - 统计显著性（配对t检验，p<0.001）\n - 效应大小（Cohen's d范围：-0.091至-1.419）\n#### 实物实验（π₀-FAST/UR5）\n- **任务场景**：\n - 低/高运输：玩具企鹅提升高度控制（75演示轨迹）\n - 慢/快运输：玩具海豹速度控制（120演示轨迹）\n- **基线对比**：无干预/提示词修改/随机向量干预\n- **指标**：\n - 低高组：末端最大高度分布（箱线图统计）\n - 慢快组：平均位移/累积位移时间序列\n- **关键发现**：\n - 低/慢干预显著降低轨迹幅度（p<0.05）\n - 高/快干预效果接近基线（模型已内建高速行为）\n - 语义引导优于随机干预与提示词修改\n\n> **注**：语义方向在不同任务/模型间存在迁移性差异，概念簇（如\"小心\"vs\"卡顿\"）可能引发混淆行为。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>加州大学伯克利分校电气工程与计算机科学系（University of California, Berkeley, Department of Electrical Engineering and Computer Sciences）</p>\n<h3>论文概述</h3>\n<p>VLA模型结合视觉和语言信息实现机器人动作控制，但缺乏类似传统机器人管道的可解释性机制。本研究将机械可解释性技术应用于VLA模型，通过分析模型内部激活单元的语义含义，实现无需微调的实时行为引导。核心创新在于提取FFN层的语义价值向量（value vectors），识别与动作选择因果相关的控制方向（如速度、方向），并将其作为实时控制接口激活。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>语义保留发现</strong>：VLA模型在训练中保留大量预训练语义概念（<25% FFN神经元用于动作预测，其余维持语义结构）</li><li><strong>因果关联验证</strong>：验证内部概念与动作的因果关系（如\"慢\"概念直接导致末端执行器缓慢移动）</li><li><strong>实时控制接口</strong>：首次实现基于内部表征的零样本行为控制方法，突破微调/环境交互依赖</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>价值向量提取</strong></li></ol>\n<p> - 分析FFN输出层权重矩阵<code>Wθ</code>，提取独立于输入的固定值向量<code>wθ(i)</code>（式1-2）</p>\n<p> - 将值向量投影至标记空间，通过标记概率分布赋予语义含义</p>\n\n<ol><li><strong>概念激活引导</strong></li></ol>\n<p> - 通过kNN聚类或人工选择识别语义对齐的神经元簇<code>𝒮</code>（如\"up\"、\"slow\"）</p>\n<p> - 在推理时覆盖该簇的激活值为固定标量<code>α</code>（式3-4）</p>\n<p> - 剩余神经元保持原始激活，生成控制残差<code>Δx</code>影响最终动作标记分布</p>\n\n<ol><li><strong>跨框架实现</strong></li></ol>\n<p> - PyTorch框架：在OpenVLA的FFN下投影层应用前向钩子</p>\n<p> - JAX框架：修改π₀的FFN计算图插入引导算子<code>I𝒮^α</code></p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>基础预训练数据</strong>：Open X-Embodiment跨平台机器人数据集（OpenVLA/π₀预训练）</li><li><strong>模拟评估</strong>：LIBERO-Long长程操作基准（10项任务，OpenVLA 7B模型）</li><li><strong>实物实验</strong>：DROID平台数据用于π₀-FAST微调（LoRA方法，5000步）</li><li><strong>硬件资源</strong>：</li></ul>\n<p> - 模拟：NVIDIA H100 GPU</p>\n<p> - 实物：NVIDIA A4500 GPU + UR5机械臂（配Robotiq 2F-140夹爪）</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<h4>模拟实验（OpenVLA/LIBERO）</h4>\n<ul><li><strong>环境</strong>：10个长程操作任务（抓取、放置、器具操作等）</li><li><strong>干预参数</strong>：对比\"快/慢\"与\"上\"概念簇，测试不同层深度注入效果</li><li><strong>指标</strong>：</li></ul>\n<p> - 末端执行器位移变化率（平均提升27.73%，最大148.54%）</p>\n<p> - 统计显著性（配对t检验，p<0.001）</p>\n<p> - 效应大小（Cohen's d范围：-0.091至-1.419）</p>\n<h4>实物实验（π₀-FAST/UR5）</h4>\n<ul><li><strong>任务场景</strong>：</li></ul>\n<p> - 低/高运输：玩具企鹅提升高度控制（75演示轨迹）</p>\n<p> - 慢/快运输：玩具海豹速度控制（120演示轨迹）</p>\n<ul><li><strong>基线对比</strong>：无干预/提示词修改/随机向量干预</li><li><strong>指标</strong>：</li></ul>\n<p> - 低高组：末端最大高度分布（箱线图统计）</p>\n<p> - 慢快组：平均位移/累积位移时间序列</p>\n<ul><li><strong>关键发现</strong>：</li></ul>\n<p> - 低/慢干预显著降低轨迹幅度（p<0.05）</p>\n<p> - 高/快干预效果接近基线（模型已内建高速行为）</p>\n<p> - 语义引导优于随机干预与提示词修改</p>\n\n<p>> <strong>注</strong>：语义方向在不同任务/模型间存在迁移性差异，概念簇（如\"小心\"vs\"卡顿\"）可能引发混淆行为。</p>"
  },
  {
    "date": "2025-08-28",
    "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control",
    "link": "http://arxiv.org/abs/2508.21112",
    "summary_markdown": "# EO-1: 通用机器人控制中的交错视觉-文本-动作预训练\n## 论文研究单位\nShanghai AI Laboratory, Fudan University, AgiBot, Northwestern Polytechnical University\n## 论文概述\nEO-1是上海人工智能实验室等机构提出的统一具身基础模型，通过交错视觉-文本-动作预训练实现通用机器人控制。该研究针对当前视觉-语言-动作(VLA)模型在开放世界泛化和交错推理能力方面的不足，提出了统一的模型架构和大规模多模态数据集，在具身推理和机器人控制任务中展现出显著优势。\n## 论文核心贡献点\n1. **统一架构**：EO-1采用单一统一解码器转换器，整合离散自回归解码与连续流匹配去噪，无需额外动作特定参数即可实现跨模态知识传递\n2. **交错式具身数据集**：构建包含150万样本的EO-Data1.5M数据集，专门针对交错视觉-文本-动作理解和学习\n3. **真实世界泛化能力**：在ERQA、LIBERO、SimplerEnv等多个开源基准测试中超越现有模型，展现出强大的开放世界理解和控制能力\n## 论文方法描述\nEO-1基于预训练VLM构建统一解码器转换器架构，处理交错多模态输入序列：\n- **输入处理**：文本标记器、视觉编码器、机器人状态投影器和动作去噪投影器将不同模态统一映射到R^d嵌入空间\n- **共享骨干**：初始化自Qwen2.5-VL的转换器骨干，通过因果注意力处理整个交错序列\n- **输出机制**：语言头用于文本解码，流头用于连续动作去噪生成\n- **交错修正采样**：针对混合模态生成中的因果关系破坏问题，提出采样策略确保动作生成段的正确训练\n- **训练目标**：结合自回归的下一个token预测和流匹配的向量场预测损失\n## 论文使用数据集和训练资源\n**数据规模**：\n- 网络多模态数据：570万样本，71亿tokens\n- 机器人控制数据：120万 эпизод，1273亿tokens\n- 交错具身数据：EO-Data1.5M，10亿tokens\n\n**训练资源**：\n- 五个epoch训练，使用Flash-Attention变长打包\n- 批量大小为1，平均序列长度16384\n- 主干学习率5×10^-5，视觉编码器1×10^-5\n- DeepSpeed ZeRO-1优化器\n- 推理时仅需6GB GPU内存\n## 论文使用的评估环境和评估指标\n**评估环境**：\n- **具身推理基准**：RoboVQA、ERQA、EO-Bench\n- **机器人控制基准**：LIBERO、SimplerEnv\n- **真实世界评估**：Franka Panda、WidowX 250S、Agibot G-1等多种机器人平台\n\n**评估指标**：\n- **具身推理**：BLEU-4分数(RoboVQA)，准确率(ERQA)，多选VQA准确率(EO-Bench)\n- **机器人控制**：成功率(SR)，在Google Robot基准中采用匹配和聚合两种评估方式\n- **多维度评估**：空间理解、物理常识、任务推理、状态估计四个维度共648个QA对\n\nEO-1在所有基准测试中均展现出优异性能，平均成功率达98.2%，显著超越OpenVLA、π₀等现有开源模型。",
    "summary_html": "<h1>EO-1: 通用机器人控制中的交错视觉-文本-动作预训练</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>Shanghai AI Laboratory, Fudan University, AgiBot, Northwestern Polytechnical University</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>EO-1是上海人工智能实验室等机构提出的统一具身基础模型，通过交错视觉-文本-动作预训练实现通用机器人控制。该研究针对当前视觉-语言-动作(VLA)模型在开放世界泛化和交错推理能力方面的不足，提出了统一的模型架构和大规模多模态数据集，在具身推理和机器人控制任务中展现出显著优势。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li><strong>统一架构</strong>：EO-1采用单一统一解码器转换器，整合离散自回归解码与连续流匹配去噪，无需额外动作特定参数即可实现跨模态知识传递</li><li><strong>交错式具身数据集</strong>：构建包含150万样本的EO-Data1.5M数据集，专门针对交错视觉-文本-动作理解和学习</li><li><strong>真实世界泛化能力</strong>：在ERQA、LIBERO、SimplerEnv等多个开源基准测试中超越现有模型，展现出强大的开放世界理解和控制能力</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>EO-1基于预训练VLM构建统一解码器转换器架构，处理交错多模态输入序列：</p>\n<ul><li><strong>输入处理</strong>：文本标记器、视觉编码器、机器人状态投影器和动作去噪投影器将不同模态统一映射到R^d嵌入空间</li><li><strong>共享骨干</strong>：初始化自Qwen2.5-VL的转换器骨干，通过因果注意力处理整个交错序列</li><li><strong>输出机制</strong>：语言头用于文本解码，流头用于连续动作去噪生成</li><li><strong>交错修正采样</strong>：针对混合模态生成中的因果关系破坏问题，提出采样策略确保动作生成段的正确训练</li><li><strong>训练目标</strong>：结合自回归的下一个token预测和流匹配的向量场预测损失</li></ul>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<p><strong>数据规模</strong>：</p>\n<ul><li>网络多模态数据：570万样本，71亿tokens</li><li>机器人控制数据：120万 эпизод，1273亿tokens</li><li>交错具身数据：EO-Data1.5M，10亿tokens</li></ul>\n\n<p><strong>训练资源</strong>：</p>\n<ul><li>五个epoch训练，使用Flash-Attention变长打包</li><li>批量大小为1，平均序列长度16384</li><li>主干学习率5×10^-5，视觉编码器1×10^-5</li><li>DeepSpeed ZeRO-1优化器</li><li>推理时仅需6GB GPU内存</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p><strong>评估环境</strong>：</p>\n<ul><li><strong>具身推理基准</strong>：RoboVQA、ERQA、EO-Bench</li><li><strong>机器人控制基准</strong>：LIBERO、SimplerEnv</li><li><strong>真实世界评估</strong>：Franka Panda、WidowX 250S、Agibot G-1等多种机器人平台</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li><strong>具身推理</strong>：BLEU-4分数(RoboVQA)，准确率(ERQA)，多选VQA准确率(EO-Bench)</li><li><strong>机器人控制</strong>：成功率(SR)，在Google Robot基准中采用匹配和聚合两种评估方式</li><li><strong>多维度评估</strong>：空间理解、物理常识、任务推理、状态估计四个维度共648个QA对</li></ul>\n\n<p>EO-1在所有基准测试中均展现出优异性能，平均成功率达98.2%，显著超越OpenVLA、π₀等现有开源模型。</p>"
  },
  {
    "date": "2025-08-28",
    "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
    "link": "http://arxiv.org/abs/2508.21046",
    "summary_markdown": "## 论文研究单位\nSchool of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\n## 论文概述\nCogVLA是一个受人类多模态协调启发的视觉-语言-动作模型，通过指令驱动的路由和稀疏化技术，在保持高性能的同时显著降低计算成本。该模型采用三阶段渐进架构，模拟人类的视觉注意力系统（VAS）、补充运动区（SMA）和前运动皮质（PMC），分别用于感知聚焦、语义意图过滤和动作规划。在LIBERO仿真基准和真实世界机器人任务上，CogVLA实现了97.4%和70.0%的任务成功率，相比OpenVLA训练成本降低2.5倍、推理延迟减少2.8倍，同时在多个维度上优于现有高效VLA方法。\n## 论文核心贡献点\n- 提出认知对齐的CogVLA框架，通过EFA-Routing、LFP-Routing和CAtten模拟人类多模态协调机制，实现从感知到控制的端到端优化。\n- 开发EFA-Routing和LFP-Routing，实现基于指令的视觉稀疏化，有效减少视觉令牌数量（Stage 1压缩至25%，Stage 2修剪50%）。\n- 设计V-L-A Coupled Attention（CAtten），融合因果视觉-语言注意和双向动作并行解码，确保语义一致性和时间连贯性。\n- 在LIBERO和ALOHA基准上验证了优越性能和效率，为可扩展机器人控制提供了解决方案。\n## 论文方法描述\nCogVLA采用三阶段渐进设计：\n1. **Encoder-FiLM based Aggregation Routing (EFA-Routing)**：在视觉编码阶段，通过指令调制的FiLM模块动态聚合视觉令牌到聚合令牌（压缩至原始输入25%），并使用门控机制融合多个编码器分支（如SigLIP和DINOv2）。\n2. **LLM-FiLM based Pruning Routing (LFP-Routing)**：在语言模型阶段，通过任务引导的剪枝路由器基于指令相关性筛选视觉令牌（减少50%令牌），保留语义关键信息以降低计算开销。\n3. **V-L-A Coupled Attention (CAtten)**：采用混合注意机制，结合因果视觉-语言注意和双向动作注意，支持动作块的并行解码，确保跨模态逻辑一致性和时间连贯性。\n整个流程支持指令驱动的端到端优化，通过稀疏化视觉输入和并行解码提高效率。\n## 论文使用数据集和训练资源\n- **数据集**：\n - 仿真环境：LIBERO基准，包含四个任务套件（空间、物体、目标、长时序），每个套件10个任务50个演示。\n - 真实世界：Cobot Agilex ALOHA平台的三个长时序任务（物体放置、抽屉操作、T恤折叠），分别45、45、30个演示。\n- **训练资源**：\n - 使用4×A800 GPUs（80GB显存）进行训练和评估，得益于指令驱动的稀疏化策略。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真环境：LIBERO基准测试，模拟各种指令遵循任务。\n - 真实世界：Cobot Agilex ALOHA平台，进行真实机器人操作评估。\n- **评估指标**：\n - 任务成功率（Success Rate）：以百分比衡量任务完成能力。\n - 效率指标：推理时间（秒）、吞吐量（Hz）、FLOPs（计算量）、训练成本（小时/10k步）。\n - 性能指标：在LIBERO套件上分空间、物体、目标、长时序子任务报告成功率；真实世界任务中统计子任务成功率（如抽屉操作的三步骤）。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>CogVLA是一个受人类多模态协调启发的视觉-语言-动作模型，通过指令驱动的路由和稀疏化技术，在保持高性能的同时显著降低计算成本。该模型采用三阶段渐进架构，模拟人类的视觉注意力系统（VAS）、补充运动区（SMA）和前运动皮质（PMC），分别用于感知聚焦、语义意图过滤和动作规划。在LIBERO仿真基准和真实世界机器人任务上，CogVLA实现了97.4%和70.0%的任务成功率，相比OpenVLA训练成本降低2.5倍、推理延迟减少2.8倍，同时在多个维度上优于现有高效VLA方法。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出认知对齐的CogVLA框架，通过EFA-Routing、LFP-Routing和CAtten模拟人类多模态协调机制，实现从感知到控制的端到端优化。</li><li>开发EFA-Routing和LFP-Routing，实现基于指令的视觉稀疏化，有效减少视觉令牌数量（Stage 1压缩至25%，Stage 2修剪50%）。</li><li>设计V-L-A Coupled Attention（CAtten），融合因果视觉-语言注意和双向动作并行解码，确保语义一致性和时间连贯性。</li><li>在LIBERO和ALOHA基准上验证了优越性能和效率，为可扩展机器人控制提供了解决方案。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>CogVLA采用三阶段渐进设计：</p>\n<ol><li><strong>Encoder-FiLM based Aggregation Routing (EFA-Routing)</strong>：在视觉编码阶段，通过指令调制的FiLM模块动态聚合视觉令牌到聚合令牌（压缩至原始输入25%），并使用门控机制融合多个编码器分支（如SigLIP和DINOv2）。</li><li><strong>LLM-FiLM based Pruning Routing (LFP-Routing)</strong>：在语言模型阶段，通过任务引导的剪枝路由器基于指令相关性筛选视觉令牌（减少50%令牌），保留语义关键信息以降低计算开销。</li><li><strong>V-L-A Coupled Attention (CAtten)</strong>：采用混合注意机制，结合因果视觉-语言注意和双向动作注意，支持动作块的并行解码，确保跨模态逻辑一致性和时间连贯性。</li></ol>\n<p>整个流程支持指令驱动的端到端优化，通过稀疏化视觉输入和并行解码提高效率。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 仿真环境：LIBERO基准，包含四个任务套件（空间、物体、目标、长时序），每个套件10个任务50个演示。</p>\n<p> - 真实世界：Cobot Agilex ALOHA平台的三个长时序任务（物体放置、抽屉操作、T恤折叠），分别45、45、30个演示。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 使用4×A800 GPUs（80GB显存）进行训练和评估，得益于指令驱动的稀疏化策略。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真环境：LIBERO基准测试，模拟各种指令遵循任务。</p>\n<p> - 真实世界：Cobot Agilex ALOHA平台，进行真实机器人操作评估。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（Success Rate）：以百分比衡量任务完成能力。</p>\n<p> - 效率指标：推理时间（秒）、吞吐量（Hz）、FLOPs（计算量）、训练成本（小时/10k步）。</p>\n<p> - 性能指标：在LIBERO套件上分空间、物体、目标、长时序子任务报告成功率；真实世界任务中统计子任务成功率（如抽屉操作的三步骤）。</p>"
  },
  {
    "date": "2025-08-27",
    "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
    "link": "http://arxiv.org/abs/2508.20072",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-27",
    "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation",
    "link": "http://arxiv.org/abs/2508.19958",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-27",
    "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
    "link": "http://arxiv.org/abs/2508.19852",
    "summary_markdown": "### 论文研究单位\n未明确提供\n### 论文概述\n本文提出Ego-PM（Ego-centric Predictive Model），一种基于手部轨迹条件的自我中心预测模型，旨在同时预测未来动作（手部轨迹）和视觉结果。该模型采用两阶段统一框架：第一阶段通过连续状态建模显式预测手部轨迹，第二阶段引入因果交叉注意力融合多模态线索，引导潜扩散模型逐帧生成视频。Ego-PM是首个同时处理人类自我中心活动和机器人操作任务的模型，无需额外标注即可自主预测动作及其视觉后果。\n### 论文核心贡献点\n1. **首个统一预测模型**：首次实现同时预测未来动作和视觉帧，解决以往方法分离处理动作与视觉预测的局限。\n2. **连续状态建模（CoSMo）与注意力机制**：提出CoSMo策略利用历史状态预测手部轨迹，并设计因果交叉注意力增强动作条件融合，显著提升预测连贯性。\n3. **跨场景通用性**：首个统一模型适用于人类自我中心视角（Ego4D）和机器人操作（BridgeData、RLBench），在动作预测与视频生成任务上均超越基线。\n### 论文方法描述\n模型分为两个训练阶段：\n1. **阶段一：显式动作建模**\n - **视觉编码器**：使用CLIP提取帧特征并投影至文本嵌入空间。\n - **动作编码器/解码器**：设计轻量MLP处理动作嵌入，通过特殊令牌<ACT>标识动作。\n - **自回归模型**：基于LLaVA处理多模态序列（视觉、文本、动作）。\n - **连续状态建模（CoSMo）**：采用相邻两个状态（t和t-1）作为输入，增强时序依赖性。\n2. **阶段二：动作增强帧预测**\n - **多模态条件融合**：通过因果交叉注意力将视觉、文本与动作嵌入对齐，约束未来帧生成。\n - **帧预测**：基于潜扩散模型（LDM），以融合条件为指导，从最后一帧开始迭代生成未来帧。\n训练目标包括阶段一的语言与动作损失（L1+GIoU），以及阶段二的扩散损失。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 人类活动：Ego4D（PRE-15和PNR时刻，文本叙述与手部轨迹）。\n - 机器人操作：BridgeData V2（人类演示视频与指令）和RLBench（9项多任务评估）。\n- **训练资源**：\n - 输入分辨率：256×256。\n - 初始化权重：LLaVA（Ego4D）或OpenVLA（BridgeData），LDM使用Stable Diffusion。\n - 训练配置：微调3个epoch，损失权重λ1=0.1、λ2=0.01。\n - 计算资源：未明确说明（原文未提及硬件细节）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：未明确提供具体环境（如硬件或软件框架），依赖学术评估设置。\n- **评估指标**：\n - **帧预测**：\n - 对齐分数：EgoVLP、EgoVLP+（视频-文本）。\n - 图像质量：CLIP相似度（越高越好）、FID（越低越好）、PSNR（越高越好）、SSIM（越高越好）、LPIPS（越低越好）。\n - **动作预测**：\n - Ego4D：手部掩码IoU（预测与真实区域重叠，越高越好）。\n - BridgeData/RLBench：任务成功率（百分比，越高越好）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>未明确提供</p>\n<h3>论文概述</h3>\n<p>本文提出Ego-PM（Ego-centric Predictive Model），一种基于手部轨迹条件的自我中心预测模型，旨在同时预测未来动作（手部轨迹）和视觉结果。该模型采用两阶段统一框架：第一阶段通过连续状态建模显式预测手部轨迹，第二阶段引入因果交叉注意力融合多模态线索，引导潜扩散模型逐帧生成视频。Ego-PM是首个同时处理人类自我中心活动和机器人操作任务的模型，无需额外标注即可自主预测动作及其视觉后果。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首个统一预测模型</strong>：首次实现同时预测未来动作和视觉帧，解决以往方法分离处理动作与视觉预测的局限。</li><li><strong>连续状态建模（CoSMo）与注意力机制</strong>：提出CoSMo策略利用历史状态预测手部轨迹，并设计因果交叉注意力增强动作条件融合，显著提升预测连贯性。</li><li><strong>跨场景通用性</strong>：首个统一模型适用于人类自我中心视角（Ego4D）和机器人操作（BridgeData、RLBench），在动作预测与视频生成任务上均超越基线。</li></ol>\n<h3>论文方法描述</h3>\n<p>模型分为两个训练阶段：</p>\n<ol><li><strong>阶段一：显式动作建模</strong></li></ol>\n<p> - <strong>视觉编码器</strong>：使用CLIP提取帧特征并投影至文本嵌入空间。</p>\n<p> - <strong>动作编码器/解码器</strong>：设计轻量MLP处理动作嵌入，通过特殊令牌<ACT>标识动作。</p>\n<p> - <strong>自回归模型</strong>：基于LLaVA处理多模态序列（视觉、文本、动作）。</p>\n<p> - <strong>连续状态建模（CoSMo）</strong>：采用相邻两个状态（t和t-1）作为输入，增强时序依赖性。</p>\n<ol><li><strong>阶段二：动作增强帧预测</strong></li></ol>\n<p> - <strong>多模态条件融合</strong>：通过因果交叉注意力将视觉、文本与动作嵌入对齐，约束未来帧生成。</p>\n<p> - <strong>帧预测</strong>：基于潜扩散模型（LDM），以融合条件为指导，从最后一帧开始迭代生成未来帧。</p>\n<p>训练目标包括阶段一的语言与动作损失（L1+GIoU），以及阶段二的扩散损失。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 人类活动：Ego4D（PRE-15和PNR时刻，文本叙述与手部轨迹）。</p>\n<p> - 机器人操作：BridgeData V2（人类演示视频与指令）和RLBench（9项多任务评估）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 输入分辨率：256×256。</p>\n<p> - 初始化权重：LLaVA（Ego4D）或OpenVLA（BridgeData），LDM使用Stable Diffusion。</p>\n<p> - 训练配置：微调3个epoch，损失权重λ1=0.1、λ2=0.01。</p>\n<p> - 计算资源：未明确说明（原文未提及硬件细节）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：未明确提供具体环境（如硬件或软件框架），依赖学术评估设置。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>帧预测</strong>：</p>\n<p> - 对齐分数：EgoVLP、EgoVLP+（视频-文本）。</p>\n<p> - 图像质量：CLIP相似度（越高越好）、FID（越低越好）、PSNR（越高越好）、SSIM（越高越好）、LPIPS（越低越好）。</p>\n<p> - <strong>动作预测</strong>：</p>\n<p> - Ego4D：手部掩码IoU（预测与真实区域重叠，越高越好）。</p>\n<p> - BridgeData/RLBench：任务成功率（百分比，越高越好）。</p>"
  },
  {
    "date": "2025-08-15",
    "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2508.19257",
    "summary_markdown": "### 论文研究单位\n北京大学 (Peking University)\n### 论文概述\n当前视觉-语言-动作（VLA）模型在处理连续帧时缺乏时序信息整合，逐帧独立处理导致视觉噪声敏感性和时序关联丢失。论文提出TTF-VLA（时序令牌融合），通过融合历史与当前视觉特征增强VLA推理质量，在不训练的前提下改善机器人操作任务的鲁棒性和效率。\n### 论文核心贡献点\n1. **双维度检测机制**：提出结合灰度像素差异分析与注意力语义相关性的检测方法，用于识别视觉变化区域和任务关键区域\n2. **硬融合策略与关键帧**：采用二值令牌选择决策，并通过周期性关键帧重置防止误差累积\n3. **跨模型适用性**：验证在OpenVLA和VLA-Cache架构上的通用性，无需任务特定调参\n4. **Query矩阵复用发现**：揭示选择性Query矩阵复用可提升性能，提出KQV矩阵直接复用的潜在加速方向\n### 论文方法描述\n1. **时序令牌融合框架**\n - 输入：当前帧I_t、历史帧I_{t-1}、历史令牌T_{t-1}、任务指令L_t\n - 输出：融合令牌T̃_t\n - 公式：T̃_t = F(T_t, T_{t-1}, I_t, I_{t-1}, L_t)\n\n2. **硬融合决策**\n - 对每个令牌i选择当前或历史令牌：\n t̃_t^(i) = t_t^(i) 若 m_i^fusion=1，否则 t_{t-1}^(i)\n\n3. **关键帧机制**\n - 定期重置所有令牌：IsKeyframe(t) = (t mod K=0) ∨ (T_{t-1}=∅)\n - 参数K=3平衡时序稳定性和响应速度\n\n4. **双维度检测融合**\n - 像素维度：计算灰度图绝对差异d_i^pixel = (1/196) ∑\\|G_t(u,v)-G_{t-1}(u,v)\\|\n - 注意力维度：提取文本-视觉和动作-视觉注意力S_text^((l))和S_action^((l))\n - 最终融合：m_i^fusion = m_i^pixel ∨ m_i^attention\n### 论文使用数据集和训练资源\n1. **仿真数据集**\n - LIBERO：4套任务（Object/Spatial/Goal/Long），每套10任务×20 episodes\n - SimplerEnv：3任务（Move Near/Pick Coke/Drawer），总756 episodes\n\n2. **真实机器人**\n - 设备：Franka Research 3机械臂\n - 数据：3任务×80 demos，5Hz操作频率\n\n3. **训练资源**\n - 微调配置：OpenVLA-7B，20,000步，batch size=8，8×A100 GPUs\n - 推理部署：单A100 GPU，5Hz控制频率\n### 论文使用的评估环境和评估指标\n1. **评估环境**\n - 仿真：LIBERO基准套件 + SimplerEnv跨环境平台\n - 真实：Franka机械臂实验室场景\n\n2. **评估指标**\n - **任务成功率**：成功执行次数/总次数\n - **时序融合率**：复用历史令牌比例\n - **计算效率**：令牌复用带来的加速效果\n\n3. **主要结果**\n - LIBERO平均提升：OpenVLA +4.0pp (72.4% vs 68.4%)，VLA-Cache +2.7pp\n - SimplerEnv跨环境：+4.8%相对提升\n - 真实机器人：+8.7%相对提升，Pick-and-Place任务改善显著\n - 消融研究：双维度融合优于单一维度，关键帧K=3性能最优\n\n论文证明TTF-VLA能有效提升VLA模型在多环境下的操作能力，特别是在长序列任务和噪声场景中表现突出，为时序信息利用和计算加速提供了新方向。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学 (Peking University)</p>\n<h3>论文概述</h3>\n<p>当前视觉-语言-动作（VLA）模型在处理连续帧时缺乏时序信息整合，逐帧独立处理导致视觉噪声敏感性和时序关联丢失。论文提出TTF-VLA（时序令牌融合），通过融合历史与当前视觉特征增强VLA推理质量，在不训练的前提下改善机器人操作任务的鲁棒性和效率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>双维度检测机制</strong>：提出结合灰度像素差异分析与注意力语义相关性的检测方法，用于识别视觉变化区域和任务关键区域</li><li><strong>硬融合策略与关键帧</strong>：采用二值令牌选择决策，并通过周期性关键帧重置防止误差累积</li><li><strong>跨模型适用性</strong>：验证在OpenVLA和VLA-Cache架构上的通用性，无需任务特定调参</li><li><strong>Query矩阵复用发现</strong>：揭示选择性Query矩阵复用可提升性能，提出KQV矩阵直接复用的潜在加速方向</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>时序令牌融合框架</strong></li></ol>\n<p> - 输入：当前帧I_t、历史帧I_{t-1}、历史令牌T_{t-1}、任务指令L_t</p>\n<p> - 输出：融合令牌T̃_t</p>\n<p> - 公式：T̃_t = F(T_t, T_{t-1}, I_t, I_{t-1}, L_t)</p>\n\n<ol><li><strong>硬融合决策</strong></li></ol>\n<p> - 对每个令牌i选择当前或历史令牌：</p>\n<p> t̃_t^(i) = t_t^(i) 若 m_i^fusion=1，否则 t_{t-1}^(i)</p>\n\n<ol><li><strong>关键帧机制</strong></li></ol>\n<p> - 定期重置所有令牌：IsKeyframe(t) = (t mod K=0) ∨ (T_{t-1}=∅)</p>\n<p> - 参数K=3平衡时序稳定性和响应速度</p>\n\n<ol><li><strong>双维度检测融合</strong></li></ol>\n<p> - 像素维度：计算灰度图绝对差异d_i^pixel = (1/196) ∑\\|G_t(u,v)-G_{t-1}(u,v)\\|</p>\n<p> - 注意力维度：提取文本-视觉和动作-视觉注意力S_text^((l))和S_action^((l))</p>\n<p> - 最终融合：m_i^fusion = m_i^pixel ∨ m_i^attention</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>仿真数据集</strong></li></ol>\n<p> - LIBERO：4套任务（Object/Spatial/Goal/Long），每套10任务×20 episodes</p>\n<p> - SimplerEnv：3任务（Move Near/Pick Coke/Drawer），总756 episodes</p>\n\n<ol><li><strong>真实机器人</strong></li></ol>\n<p> - 设备：Franka Research 3机械臂</p>\n<p> - 数据：3任务×80 demos，5Hz操作频率</p>\n\n<ol><li><strong>训练资源</strong></li></ol>\n<p> - 微调配置：OpenVLA-7B，20,000步，batch size=8，8×A100 GPUs</p>\n<p> - 推理部署：单A100 GPU，5Hz控制频率</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong></li></ol>\n<p> - 仿真：LIBERO基准套件 + SimplerEnv跨环境平台</p>\n<p> - 真实：Franka机械臂实验室场景</p>\n\n<ol><li><strong>评估指标</strong></li></ol>\n<p> - <strong>任务成功率</strong>：成功执行次数/总次数</p>\n<p> - <strong>时序融合率</strong>：复用历史令牌比例</p>\n<p> - <strong>计算效率</strong>：令牌复用带来的加速效果</p>\n\n<ol><li><strong>主要结果</strong></li></ol>\n<p> - LIBERO平均提升：OpenVLA +4.0pp (72.4% vs 68.4%)，VLA-Cache +2.7pp</p>\n<p> - SimplerEnv跨环境：+4.8%相对提升</p>\n<p> - 真实机器人：+8.7%相对提升，Pick-and-Place任务改善显著</p>\n<p> - 消融研究：双维度融合优于单一维度，关键帧K=3性能最优</p>\n\n<p>论文证明TTF-VLA能有效提升VLA模型在多环境下的操作能力，特别是在长序列任务和噪声场景中表现突出，为时序信息利用和计算加速提供了新方向。</p>"
  },
  {
    "date": "2025-08-26",
    "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2508.19236",
    "summary_markdown": "### 论文研究单位\n清华大学自动化系BNRist、Dexmal、MEGVII Technology、天津大学、哈尔滨工业大学、StepFun\n### 论文概述\nMemoryVLA是一个受认知科学启发的视觉-语言-动作模型框架，通过引入感知-认知记忆机制解决机器人操作中的长期时间依赖问题。该方法结合了工作记忆（短期神经网络活动）和长时记忆（海马体系统），用于处理非马尔可夫性的操作任务，如\"推按钮\"等视觉变化不明显的场景。\n### 论文核心贡献点\n- 提出认知-记忆-行动框架，利用VLM常识先验和记忆机制建模长期时间依赖\n- 设计感知-认知记忆库（PCMB），支持高低层特征的记忆检索、门控融合和记忆整合\n- 实现记忆条件化扩散动作专家，生成时间感知的动作序列\n- 在150+仿真和真实任务上达到SOTA性能，长期任务提升显著\n### 论文方法描述\n1. **视觉-语言认知模块**：\n - 使用7B Prismatic VLM处理RGB图像和语言指令\n - DINOv2和SigLIP视觉编码器提取感知token（256维）\n - LLaMA-7B生成认知token（1维），形成工作记忆\n\n2. **感知-认知记忆模块**：\n - **记忆检索**：通过带时间位置编码的交叉注意力从PCMB获取历史上下文\n - **门控融合**：用学习门控自适应融合当前token和检索内容\n - **记忆整合**：当容量满时合并时间相邻且语义相似的条目\n\n3. **记忆条件化动作专家**：\n - 基于扩散的Transformer（DiT）生成16步7-DoF动作序列\n - 认知token提供高层语义指导，感知token补充细节\n - DDIM采样10步，使用分类器自由引导（CFG）\n### 论文使用数据集和训练资源\n- **数据集**：Bridge v2、RT-1、LIBERO（5套件）、真实世界数据\n- **训练资源**：8块NVIDIA A100 GPU，PyTorch FSDP，全局批大小256，学习率2e-5\n- **模型参数**：VLM 7B，扩散专家约300M\n### 论文使用的评估环境和评估指标\n- **仿真环境**：\n - SimplerEnv-Bridge（WidowX机器人）\n - SimplerEnv-Fractal（Google机器人，含VM/VA设置）\n - LIBERO（Franka机器人，5套件：Spatial/Object/Goal/Long/LIBERO-90）\n- **真实环境**：\n - Franka和WidowX机器人\n - Intel RealSense D435 RGB摄像头（640×480→224×224）\n - ROS集成系统\n- **评估指标**：\n - 成功率（%），每个任务15-50次试验\n - 长期任务采用子目标逐步评分",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学自动化系BNRist、Dexmal、MEGVII Technology、天津大学、哈尔滨工业大学、StepFun</p>\n<h3>论文概述</h3>\n<p>MemoryVLA是一个受认知科学启发的视觉-语言-动作模型框架，通过引入感知-认知记忆机制解决机器人操作中的长期时间依赖问题。该方法结合了工作记忆（短期神经网络活动）和长时记忆（海马体系统），用于处理非马尔可夫性的操作任务，如\"推按钮\"等视觉变化不明显的场景。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出认知-记忆-行动框架，利用VLM常识先验和记忆机制建模长期时间依赖</li><li>设计感知-认知记忆库（PCMB），支持高低层特征的记忆检索、门控融合和记忆整合</li><li>实现记忆条件化扩散动作专家，生成时间感知的动作序列</li><li>在150+仿真和真实任务上达到SOTA性能，长期任务提升显著</li></ul>\n<h3>论文方法描述</h3>\n<ol><li><strong>视觉-语言认知模块</strong>：</li></ol>\n<p> - 使用7B Prismatic VLM处理RGB图像和语言指令</p>\n<p> - DINOv2和SigLIP视觉编码器提取感知token（256维）</p>\n<p> - LLaMA-7B生成认知token（1维），形成工作记忆</p>\n\n<ol><li><strong>感知-认知记忆模块</strong>：</li></ol>\n<p> - <strong>记忆检索</strong>：通过带时间位置编码的交叉注意力从PCMB获取历史上下文</p>\n<p> - <strong>门控融合</strong>：用学习门控自适应融合当前token和检索内容</p>\n<p> - <strong>记忆整合</strong>：当容量满时合并时间相邻且语义相似的条目</p>\n\n<ol><li><strong>记忆条件化动作专家</strong>：</li></ol>\n<p> - 基于扩散的Transformer（DiT）生成16步7-DoF动作序列</p>\n<p> - 认知token提供高层语义指导，感知token补充细节</p>\n<p> - DDIM采样10步，使用分类器自由引导（CFG）</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Bridge v2、RT-1、LIBERO（5套件）、真实世界数据</li><li><strong>训练资源</strong>：8块NVIDIA A100 GPU，PyTorch FSDP，全局批大小256，学习率2e-5</li><li><strong>模型参数</strong>：VLM 7B，扩散专家约300M</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>仿真环境</strong>：</li></ul>\n<p> - SimplerEnv-Bridge（WidowX机器人）</p>\n<p> - SimplerEnv-Fractal（Google机器人，含VM/VA设置）</p>\n<p> - LIBERO（Franka机器人，5套件：Spatial/Object/Goal/Long/LIBERO-90）</p>\n<ul><li><strong>真实环境</strong>：</li></ul>\n<p> - Franka和WidowX机器人</p>\n<p> - Intel RealSense D435 RGB摄像头（640×480→224×224）</p>\n<p> - ROS集成系统</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（%），每个任务15-50次试验</p>\n<p> - 长期任务采用子目标逐步评分</p>"
  },
  {
    "date": "2025-08-25",
    "title": "FlowVLA: Thinking in Motion with a Visual Chain of Thought",
    "link": "http://arxiv.org/abs/2508.18269",
    "summary_markdown": "# 论文研究单位\n未明确标注单位，仅在作者列表中出现两个上标标记；推断主要来自多机构合作，具体单位信息未在页面中明确给出。\n# 论文概述\n当前视觉-语言-动作（Vision-Language-Action, VLA）模型常以“下一帧预测”作为世界模型的预训练范式，直接从当前帧预测未来帧。这种跳过显式物理推理的方式容易导致“像素复制陷阱”，使预测在物理上不可信，并在长时序中不稳定。为解决该问题，论文提出“视觉思维链”（Visual Chain of Thought, Visual CoT），将预测分解为显式的“先推理运动，再生成外观”的结构化过程，具体采用v_t→f_t→v_{t+1}的因果链条，并以光流f_t作为中间运动表示。论文据此构建FlowVLA：一个两阶段的统一自回归Transformer，第一阶段用Visual CoT进行世界模型预训练，第二阶段进行策略微调以生成动作块。FlowVLA在LIBERO、SimplerEnv等仿真基准以及AgileX Cobot真实机器人平台上实现了更物理合理、更高效的策略学习，并提升样本效率和收敛速度。\n# 论文核心贡献点\n- 指出下一帧预测范式的根本缺陷（像素复制陷阱、缺乏物理因果性），提出“视觉思维链”（Visual CoT）作为世界模型学习新范式。\n- 通过v_t→f_t→v_{t+1}显式将运动推理与外观生成解耦，并在预训练中引入强归纳偏置以促进动态理解。\n- 设计FlowVLA：单一统一自回归Transformer，使用共享的矢量量化（VQ）分词器将RGB帧与光流映射为同一词表，实现端到端的跨模态推理。\n- 两阶段训练：阶段一以Visual CoT进行大规模视频的世界模型预训练；阶段二微调为策略模型以生成动作块，显著缩小预训练-下游任务的领域鸿沟。\n- 在仿真和真实平台取得最优或大幅领先的性能，并验证样本效率、收敛速度与长时序规划能力提升。\n# 论文方法描述\n- 视觉思维链（Visual CoT）\n - 将建模目标从P(v_{t+1}\\|v_t, L)转化为联合建模P(v_{t+1}, f_t\\|v_t, L)，并因式分解为两步：\n 1) 运动推理：P(f_t\\|v_t, L)，要求显式预测光流f_t；\n 2) 外观生成：P(v_{t+1}\\|f_t, v_t, L)，在给定光流条件下生成下一帧。\n - 将学习从像素回归重构为结构化物理推理，为策略学习提供与“如何动”对齐的中间表示。\n- 两阶段训练框架\n - 阶段一：世界模型预训练（Visual CoT）\n - 输入序列构造为交错的出现（帧）与运动（光流）token：S_wm = {L_instr, v_0, f_0, v_1, f_1, …, v_T, f_T}。\n - 统一分词：用同一VQ-GAN分词器将RGB帧与光流映射为离散token；光流由RAFT计算，并通过VideoJAM方式将二维位移编码为3通道RGB图，再进行非线性归一化以保留细微运动。\n - 训练目标：L_WM为对flow token与next-frame token的交叉熵损失之和（通常权重λ=1），以标准的“下一token预测”优化，交错预测f_t与v_{t+1}。\n - 阶段二：策略微调（动作预测）\n - 权重初始化自预训练世界模型。\n - 输入序列为S_policy = {L_instr, v_0, a_0, v_1, a_1, …}；动作按FAST方法离散化为token。\n - 训练目标：L_policy仅在动作token上计算交叉熵，使模型将已学到的视觉-动力知识聚焦于行动生成。\n- 统一与简洁性\n - 无需引入运动特定的网络分支：光流与帧共享同一分词器，单一自回归Transformer即可学习跨模态交错序列，兼具参数与结构效率。\n# 论文使用数据集和训练资源\n- 数据与基准\n - 仿真基准：LIBERO（空间/目标/长期组合四套Suite）、SimplerEnv-WidowX（评估域移鲁棒性：光照、纹理、视角变化）。\n - 真实机器人：AgileX Cobot双机械臂平台，包含腕部与前向摄像头；设计四项单臂与双臂任务，采集每任务50–200条人类遥操作演示用于微调。\n- 预训练与微调设置（典型）\n - 基于Emu3（约8.5B参数）架构；光流由RAFT预计算；分词采用VQ-GAN。\n - LIBERO：世界模型预训练约5k步（batch=16）；策略微调约5k步（batch=96）。\n - SimplerEnv：预训练约12k步（batch=32）；微调约20k步（batch=128）。\n- 实现细节\n - 光流转换：2通道(u, v)映射为3通道RGB，方向→色相，速度→饱和度与明度；非线性归一化保留细微运动并避免饱和。\n - 损失平衡：L_WM中λ=1，交错预测flow与帧；动作token化遵循FAST。\n# 论文使用的评估环境和评估指标\n- 评估环境\n - LIBERO四套Suite（空间泛化、目标泛化、目标改变、长期组合）以及SimplerEnv-WidowX域移场景。\n - 真实AgileX Cobot平台的双臂操作任务。\n- 评估指标\n - 任务成功率（%）：在LIBERO与SimplerEnv的仿真结果以及真实机器人任务上进行报告，通常每任务多次试验（真实实验每任务25次）以保证统计可靠性。\n - 收敛与样本效率：通过成功率和训练步数曲线，对比FlowVLA与基线（如UniVLA）在全数据与低数据（50%）条件下的收敛速度与最终性能。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>未明确标注单位，仅在作者列表中出现两个上标标记；推断主要来自多机构合作，具体单位信息未在页面中明确给出。</p>\n<h1>论文概述</h1>\n<p>当前视觉-语言-动作（Vision-Language-Action, VLA）模型常以“下一帧预测”作为世界模型的预训练范式，直接从当前帧预测未来帧。这种跳过显式物理推理的方式容易导致“像素复制陷阱”，使预测在物理上不可信，并在长时序中不稳定。为解决该问题，论文提出“视觉思维链”（Visual Chain of Thought, Visual CoT），将预测分解为显式的“先推理运动，再生成外观”的结构化过程，具体采用v_t→f_t→v_{t+1}的因果链条，并以光流f_t作为中间运动表示。论文据此构建FlowVLA：一个两阶段的统一自回归Transformer，第一阶段用Visual CoT进行世界模型预训练，第二阶段进行策略微调以生成动作块。FlowVLA在LIBERO、SimplerEnv等仿真基准以及AgileX Cobot真实机器人平台上实现了更物理合理、更高效的策略学习，并提升样本效率和收敛速度。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>指出下一帧预测范式的根本缺陷（像素复制陷阱、缺乏物理因果性），提出“视觉思维链”（Visual CoT）作为世界模型学习新范式。</li><li>通过v_t→f_t→v_{t+1}显式将运动推理与外观生成解耦，并在预训练中引入强归纳偏置以促进动态理解。</li><li>设计FlowVLA：单一统一自回归Transformer，使用共享的矢量量化（VQ）分词器将RGB帧与光流映射为同一词表，实现端到端的跨模态推理。</li><li>两阶段训练：阶段一以Visual CoT进行大规模视频的世界模型预训练；阶段二微调为策略模型以生成动作块，显著缩小预训练-下游任务的领域鸿沟。</li><li>在仿真和真实平台取得最优或大幅领先的性能，并验证样本效率、收敛速度与长时序规划能力提升。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>视觉思维链（Visual CoT）</li></ul>\n<p> - 将建模目标从P(v_{t+1}\\|v_t, L)转化为联合建模P(v_{t+1}, f_t\\|v_t, L)，并因式分解为两步：</p>\n<p> 1) 运动推理：P(f_t\\|v_t, L)，要求显式预测光流f_t；</p>\n<p> 2) 外观生成：P(v_{t+1}\\|f_t, v_t, L)，在给定光流条件下生成下一帧。</p>\n<p> - 将学习从像素回归重构为结构化物理推理，为策略学习提供与“如何动”对齐的中间表示。</p>\n<ul><li>两阶段训练框架</li></ul>\n<p> - 阶段一：世界模型预训练（Visual CoT）</p>\n<p> - 输入序列构造为交错的出现（帧）与运动（光流）token：S_wm = {L_instr, v_0, f_0, v_1, f_1, …, v_T, f_T}。</p>\n<p> - 统一分词：用同一VQ-GAN分词器将RGB帧与光流映射为离散token；光流由RAFT计算，并通过VideoJAM方式将二维位移编码为3通道RGB图，再进行非线性归一化以保留细微运动。</p>\n<p> - 训练目标：L_WM为对flow token与next-frame token的交叉熵损失之和（通常权重λ=1），以标准的“下一token预测”优化，交错预测f_t与v_{t+1}。</p>\n<p> - 阶段二：策略微调（动作预测）</p>\n<p> - 权重初始化自预训练世界模型。</p>\n<p> - 输入序列为S_policy = {L_instr, v_0, a_0, v_1, a_1, …}；动作按FAST方法离散化为token。</p>\n<p> - 训练目标：L_policy仅在动作token上计算交叉熵，使模型将已学到的视觉-动力知识聚焦于行动生成。</p>\n<ul><li>统一与简洁性</li></ul>\n<p> - 无需引入运动特定的网络分支：光流与帧共享同一分词器，单一自回归Transformer即可学习跨模态交错序列，兼具参数与结构效率。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据与基准</li></ul>\n<p> - 仿真基准：LIBERO（空间/目标/长期组合四套Suite）、SimplerEnv-WidowX（评估域移鲁棒性：光照、纹理、视角变化）。</p>\n<p> - 真实机器人：AgileX Cobot双机械臂平台，包含腕部与前向摄像头；设计四项单臂与双臂任务，采集每任务50–200条人类遥操作演示用于微调。</p>\n<ul><li>预训练与微调设置（典型）</li></ul>\n<p> - 基于Emu3（约8.5B参数）架构；光流由RAFT预计算；分词采用VQ-GAN。</p>\n<p> - LIBERO：世界模型预训练约5k步（batch=16）；策略微调约5k步（batch=96）。</p>\n<p> - SimplerEnv：预训练约12k步（batch=32）；微调约20k步（batch=128）。</p>\n<ul><li>实现细节</li></ul>\n<p> - 光流转换：2通道(u, v)映射为3通道RGB，方向→色相，速度→饱和度与明度；非线性归一化保留细微运动并避免饱和。</p>\n<p> - 损失平衡：L_WM中λ=1，交错预测flow与帧；动作token化遵循FAST。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境</li></ul>\n<p> - LIBERO四套Suite（空间泛化、目标泛化、目标改变、长期组合）以及SimplerEnv-WidowX域移场景。</p>\n<p> - 真实AgileX Cobot平台的双臂操作任务。</p>\n<ul><li>评估指标</li></ul>\n<p> - 任务成功率（%）：在LIBERO与SimplerEnv的仿真结果以及真实机器人任务上进行报告，通常每任务多次试验（真实实验每任务25次）以保证统计可靠性。</p>\n<p> - 收敛与样本效率：通过成功率和训练步数曲线，对比FlowVLA与基线（如UniVLA）在全数据与低数据（50%）条件下的收敛速度与最终性能。</p>"
  },
  {
    "date": "2025-08-23",
    "title": "NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows",
    "link": "http://arxiv.org/abs/2508.16845",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-22",
    "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
    "link": "http://arxiv.org/abs/2508.16292",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-21",
    "title": "Survey of Vision-Language-Action Models for Embodied Manipulation",
    "link": "http://arxiv.org/abs/2508.15201",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-19",
    "title": "CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2508.13446",
    "summary_markdown": "## 论文研究单位\nUniversity of California Berkeley, Princeton University\n## 论文概述\n本文提出了一种名为CAST（Counterfactual Augmentation with Synthetic Trajectories）的新方法，用于改进视觉-语言-动作（VLA）模型遵循指令的能力。当前VLA模型在遵循细粒度命令方面存在困难，原因在于现有机器人数据集缺乏语义多样性和语言基础。CAST利用视觉-语言模型（VLM）为现有机器人数据集生成反事实标签，通过增加语言基础的多样性和粒度来提升VLA的性能。该方法无需额外收集数据，通过合成反事实语言和动作，显著提高了策略遵循指令的能力。在三个真实环境中的视觉语言导航实验表明，使用CAST的策略在导航任务上的成功率比基线提高了27%，达到了与最先进方法相当的水平。\n## 论文核心贡献点\n- 提出了CAST方法：一种新颖的数据增强方案，通过利用VLM为现有数据集生成反事实的语言和动作标签，以解决语言条件策略中的后验崩溃问题。\n- 创建了CAST数据集：通过上述方法生成的增强数据集，包含原始轨迹和分支出去的反事实轨迹。\n- 训练了CounterfactualVLA模型：一个基于PaliGemma的3B参数VLA模型，在CAST数据集上进行微调。\n- 实验验证：在27个语言指令和3个真实环境中的实验表明，该方法在指令遵循任务上比仅使用后验标签的基线提升了27%的成功率，并优于多个SOTA基线。\n- 开源贡献：公开了CAST增强代码、CounterfactualVLA策略训练代码、CAST数据集以及训练好的模型检查点。\n## 论文方法描述\nCAST方法旨在为同一观察生成多个不同的语言-动作对，迫使策略关注语言指令以选择正确动作，从而避免后验崩溃。该方法包含以下步骤：\n1. **原子标签生成**：将原始轨迹中的动作离散化为简单的原子命令，如“turn left”、“turn right”、“go forward”、“stop”等，形成一个带原子标签的数据集。\n2. **原子策略训练**：使用上述数据集训练一个原子策略，该策略专门用于执行这些短视距的原子命令。该策略基于EfficientNet-b2和扩散模型。\n3. **反事实指令生成**：在原始轨迹的决策点，使用VLM根据当前观察和原始指令生成多种可行的反事实指令及其对应的原子命令。\n4. **反事实轨迹合成**：利用训练好的原子策略，为每个反事实的原子命令采样生成相应的动作序列，从而构建出从原始轨迹分支出但未实际执行的反事实轨迹。\n5. **VLA模型训练**：将原始数据和合成的反事实数据（观察、指令、动作）结合，用于训练一个高容量的VLA模型（CounterfactualVLA），该模型基于PaliGemma，能够将复杂的语言指令映射到机器人动作。\n## 论文使用数据集和训练资源\n- **数据集**：使用GNM数据集，该数据集包含多种机器人形态（轮式机器人、四足机器人等）在室内和室外环境中的导航轨迹。通过CAST方法对该数据集进行增强，生成用于训练的CAST数据集。\n- **训练资源**：CounterfactualVLA模型在v4-8 TPU虚拟机上训练了100,000步，耗时约50小时。批次大小为192，学习率为1e-4。若使用v4-32 TPO pod，可将训练时间缩短至约20小时。\n## 论文使用的评估环境和评估指标\n- **评估环境**：在3个真实的物理环境中进行评估，包括拥挤的办公室走廊、厨房/公共区域和一个室外公园。\n- **任务**：共27个语言指令，分为三类：\n - Object Navigation：导航到特定物体（如“移动到橙色椅子”）。\n - Referential Navigation：相对于物体或结构进行导航，需要空间理解（如“移动到椅子的右边”）。\n - Continuous Navigation：执行相对于环境的连续行为（如“沿着白墙移动”）。\n- **评估指标**：主要指标是**成功率**。根据任务类型定义不同的成功标准：物体导航中，成功定义为到达目标物体50厘米范围内；关系导航中，成功定义为正确执行相对位置移动且无碰撞；连续导航中，成功定义为在参考结构1米内移动并持续行为2米以上。每个指令在每种环境中进行5次试验，结果以成功次数/总试验数（如3/5）和平均成功率报告，并计算标准误差。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>University of California Berkeley, Princeton University</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文提出了一种名为CAST（Counterfactual Augmentation with Synthetic Trajectories）的新方法，用于改进视觉-语言-动作（VLA）模型遵循指令的能力。当前VLA模型在遵循细粒度命令方面存在困难，原因在于现有机器人数据集缺乏语义多样性和语言基础。CAST利用视觉-语言模型（VLM）为现有机器人数据集生成反事实标签，通过增加语言基础的多样性和粒度来提升VLA的性能。该方法无需额外收集数据，通过合成反事实语言和动作，显著提高了策略遵循指令的能力。在三个真实环境中的视觉语言导航实验表明，使用CAST的策略在导航任务上的成功率比基线提高了27%，达到了与最先进方法相当的水平。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了CAST方法：一种新颖的数据增强方案，通过利用VLM为现有数据集生成反事实的语言和动作标签，以解决语言条件策略中的后验崩溃问题。</li><li>创建了CAST数据集：通过上述方法生成的增强数据集，包含原始轨迹和分支出去的反事实轨迹。</li><li>训练了CounterfactualVLA模型：一个基于PaliGemma的3B参数VLA模型，在CAST数据集上进行微调。</li><li>实验验证：在27个语言指令和3个真实环境中的实验表明，该方法在指令遵循任务上比仅使用后验标签的基线提升了27%的成功率，并优于多个SOTA基线。</li><li>开源贡献：公开了CAST增强代码、CounterfactualVLA策略训练代码、CAST数据集以及训练好的模型检查点。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>CAST方法旨在为同一观察生成多个不同的语言-动作对，迫使策略关注语言指令以选择正确动作，从而避免后验崩溃。该方法包含以下步骤：</p>\n<ol><li><strong>原子标签生成</strong>：将原始轨迹中的动作离散化为简单的原子命令，如“turn left”、“turn right”、“go forward”、“stop”等，形成一个带原子标签的数据集。</li><li><strong>原子策略训练</strong>：使用上述数据集训练一个原子策略，该策略专门用于执行这些短视距的原子命令。该策略基于EfficientNet-b2和扩散模型。</li><li><strong>反事实指令生成</strong>：在原始轨迹的决策点，使用VLM根据当前观察和原始指令生成多种可行的反事实指令及其对应的原子命令。</li><li><strong>反事实轨迹合成</strong>：利用训练好的原子策略，为每个反事实的原子命令采样生成相应的动作序列，从而构建出从原始轨迹分支出但未实际执行的反事实轨迹。</li><li><strong>VLA模型训练</strong>：将原始数据和合成的反事实数据（观察、指令、动作）结合，用于训练一个高容量的VLA模型（CounterfactualVLA），该模型基于PaliGemma，能够将复杂的语言指令映射到机器人动作。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：使用GNM数据集，该数据集包含多种机器人形态（轮式机器人、四足机器人等）在室内和室外环境中的导航轨迹。通过CAST方法对该数据集进行增强，生成用于训练的CAST数据集。</li><li><strong>训练资源</strong>：CounterfactualVLA模型在v4-8 TPU虚拟机上训练了100,000步，耗时约50小时。批次大小为192，学习率为1e-4。若使用v4-32 TPO pod，可将训练时间缩短至约20小时。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：在3个真实的物理环境中进行评估，包括拥挤的办公室走廊、厨房/公共区域和一个室外公园。</li><li><strong>任务</strong>：共27个语言指令，分为三类：</li></ul>\n<p> - Object Navigation：导航到特定物体（如“移动到橙色椅子”）。</p>\n<p> - Referential Navigation：相对于物体或结构进行导航，需要空间理解（如“移动到椅子的右边”）。</p>\n<p> - Continuous Navigation：执行相对于环境的连续行为（如“沿着白墙移动”）。</p>\n<ul><li><strong>评估指标</strong>：主要指标是<strong>成功率</strong>。根据任务类型定义不同的成功标准：物体导航中，成功定义为到达目标物体50厘米范围内；关系导航中，成功定义为正确执行相对位置移动且无碰撞；连续导航中，成功定义为在参考结构1米内移动并持续行为2米以上。每个指令在每种环境中进行5次试验，结果以成功次数/总试验数（如3/5）和平均成功率报告，并计算标准误差。</li></ul>"
  },
  {
    "date": "2025-08-18",
    "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy",
    "link": "http://arxiv.org/abs/2508.13103",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n\n浙江大学计算机科学与技术学院、上海人工智能实验室、商汤科技研究院、南京大学、清华大学\n## 论文概述\n\n论文提出了Observation-Centric VLA (OC-VLA)框架，旨在解决视觉-语言-动作（VLA）模型在真实环境中的泛化问题。现有VLA模型存在观察空间与动作空间不一致的问题：训练数据来自不同相机视角，但模型通常在机器人基坐标系中预测末端执行器姿态，导致空间不一致。OC-VLA通过利用相机外参标定矩阵，将动作预测从机器人基坐标系转换到相机坐标系，统一了不同视角下的预测目标，从而提高模型对相机视角变化的鲁棒性和泛化能力。\n## 论文核心贡献点\n\n提出观察中心的VLA框架，将动作预测直接锚定在相机观察空间，通过相机外参矩阵实现坐标系转换\n\n轻量级、即插即用的策略，与现有VLA架构完全兼容，无需架构修改\n\n从优化角度分析了相机坐标系预测相比机器人坐标系预测的优势，相机坐标与图像坐标的转换仅需内参矩阵，而机器人坐标转换需要外参矩阵\n\n在仿真和真实机器人实验中验证了方法的有效性，显著提升任务成功率、加速收敛并增强跨视角泛化能力\n## 论文方法描述\n\n模型架构采用轻量级VLA模型（约334M参数），使用CLIP文本编码器处理语言指令，DINOv2处理RGB图像，通过Q-Former（4层）进行特征融合和压缩，最后使用LLaMA2风格的Transformer（12层）预测动作\n\n支持连续动作空间（使用Diffusion Transformer，DDPM 100步训练，DDIM 10步推理）和离散动作空间（非自回归预测）两种模式\n\n坐标系转换：利用相机外参矩阵T，将机器人基坐标系中的末端执行器姿态转换到相机坐标系，公式为A_cam = T × A_world × T^(-1)\n\n训练时使用相机坐标系下的动作作为监督目标，推理时将预测的相机坐标系动作转换回机器人坐标系用于机器人执行\n## 论文使用数据集和训练资源\n\n预训练数据集：Droid数据集，包含来自1417个不同第三人称相机视角的机器人操作轨迹\n\n仿真评估数据：ManiSkill2数据集，选择5个任务（PickCube, StackCube, PickSingleYCB, PickClutterYCB, PickSingleEGAD），生成约40,000条轨迹，每条轨迹从300,000个随机相机视角池中采样20个相机进行渲染\n\n真实机器人数据：使用Franka Emika Panda机械臂收集两组数据，Camera 1收集15个任务（固定相机），Camera 2收集8个任务（相机位置有轻微扰动），每个任务10条演示轨迹（10-shot设置）\n\n训练资源：8张NVIDIA A100 GPU，总batch size 2048（每GPU 256样本），使用AdamW优化器训练30,000步，学习率为Transformer和Q-Former 1e-4，DINOv2为1e-5\n## 论文使用的评估环境和评估指标\n\n仿真评估环境：ManiSkill2 benchmark，5个任务类型，每个任务从验证集随机采样100条轨迹进行评估（共500条轨迹）\n\n真实机器人平台：Franka Emika Panda 7自由度机械臂，配备Robotiq 2F-85夹爪和多个RealSense D435i RGB-D相机\n\n评估指标：任务成功率（Success Rate）\n\n评估设置包括三种场景：固定相机视角（与训练视角一致）、轻微相机扰动（训练时引入相机位置变化）、新相机视角（零样本评估，使用训练时未见过的相机）\n\n基线方法：OpenVLA-OFT、π0、以及使用机器人基坐标系预测的相同架构模型\n\n真实机器人任务涵盖pick & place、pouring、stacking、pick & rotation、pull & push等多种类型，共15个任务，每个任务进行10次试验",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n\n<p>浙江大学计算机科学与技术学院、上海人工智能实验室、商汤科技研究院、南京大学、清华大学</p>\n<h2 class=\"section-title\">论文概述</h2>\n\n<p>论文提出了Observation-Centric VLA (OC-VLA)框架，旨在解决视觉-语言-动作（VLA）模型在真实环境中的泛化问题。现有VLA模型存在观察空间与动作空间不一致的问题：训练数据来自不同相机视角，但模型通常在机器人基坐标系中预测末端执行器姿态，导致空间不一致。OC-VLA通过利用相机外参标定矩阵，将动作预测从机器人基坐标系转换到相机坐标系，统一了不同视角下的预测目标，从而提高模型对相机视角变化的鲁棒性和泛化能力。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n\n<p>提出观察中心的VLA框架，将动作预测直接锚定在相机观察空间，通过相机外参矩阵实现坐标系转换</p>\n\n<p>轻量级、即插即用的策略，与现有VLA架构完全兼容，无需架构修改</p>\n\n<p>从优化角度分析了相机坐标系预测相比机器人坐标系预测的优势，相机坐标与图像坐标的转换仅需内参矩阵，而机器人坐标转换需要外参矩阵</p>\n\n<p>在仿真和真实机器人实验中验证了方法的有效性，显著提升任务成功率、加速收敛并增强跨视角泛化能力</p>\n<h2 class=\"section-title\">论文方法描述</h2>\n\n<p>模型架构采用轻量级VLA模型（约334M参数），使用CLIP文本编码器处理语言指令，DINOv2处理RGB图像，通过Q-Former（4层）进行特征融合和压缩，最后使用LLaMA2风格的Transformer（12层）预测动作</p>\n\n<p>支持连续动作空间（使用Diffusion Transformer，DDPM 100步训练，DDIM 10步推理）和离散动作空间（非自回归预测）两种模式</p>\n\n<p>坐标系转换：利用相机外参矩阵T，将机器人基坐标系中的末端执行器姿态转换到相机坐标系，公式为A_cam = T × A_world × T^(-1)</p>\n\n<p>训练时使用相机坐标系下的动作作为监督目标，推理时将预测的相机坐标系动作转换回机器人坐标系用于机器人执行</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n\n<p>预训练数据集：Droid数据集，包含来自1417个不同第三人称相机视角的机器人操作轨迹</p>\n\n<p>仿真评估数据：ManiSkill2数据集，选择5个任务（PickCube, StackCube, PickSingleYCB, PickClutterYCB, PickSingleEGAD），生成约40,000条轨迹，每条轨迹从300,000个随机相机视角池中采样20个相机进行渲染</p>\n\n<p>真实机器人数据：使用Franka Emika Panda机械臂收集两组数据，Camera 1收集15个任务（固定相机），Camera 2收集8个任务（相机位置有轻微扰动），每个任务10条演示轨迹（10-shot设置）</p>\n\n<p>训练资源：8张NVIDIA A100 GPU，总batch size 2048（每GPU 256样本），使用AdamW优化器训练30,000步，学习率为Transformer和Q-Former 1e-4，DINOv2为1e-5</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n\n<p>仿真评估环境：ManiSkill2 benchmark，5个任务类型，每个任务从验证集随机采样100条轨迹进行评估（共500条轨迹）</p>\n\n<p>真实机器人平台：Franka Emika Panda 7自由度机械臂，配备Robotiq 2F-85夹爪和多个RealSense D435i RGB-D相机</p>\n\n<p>评估指标：任务成功率（Success Rate）</p>\n\n<p>评估设置包括三种场景：固定相机视角（与训练视角一致）、轻微相机扰动（训练时引入相机位置变化）、新相机视角（零样本评估，使用训练时未见过的相机）</p>\n\n<p>基线方法：OpenVLA-OFT、π0、以及使用机器人基坐标系预测的相同架构模型</p>\n\n<p>真实机器人任务涵盖pick & place、pouring、stacking、pick & rotation、pull & push等多种类型，共15个任务，每个任务进行10次试验</p>"
  },
  {
    "date": "2025-08-18",
    "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey",
    "link": "http://arxiv.org/abs/2508.13073",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-17",
    "title": "Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search",
    "link": "http://arxiv.org/abs/2508.12211",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-16",
    "title": "Toward General Physical Intelligence for Resilient Agile Manufacturing Automation",
    "link": "http://arxiv.org/abs/2508.11960",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-14",
    "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model",
    "link": "http://arxiv.org/abs/2508.10416",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-14",
    "title": "Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning",
    "link": "http://arxiv.org/abs/2508.10399",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-14",
    "title": "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver",
    "link": "http://arxiv.org/abs/2508.10333",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-12",
    "title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2508.09071",
    "summary_markdown": "### 论文研究单位\n- 天津大学\n- Dexmal\n- 清华大学\n### 论文概述\n该论文提出了GeoVLA，一种新的视觉-语言-动作（VLA）模型框架，通过有效整合3D信息来推进机器人操作。当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界中的丰富几何信息，这限制了它们的空间感知能力和适应性。GeoVLA使用视觉-语言模型（VLM）处理图像和语言指令，提取融合的视觉-语言嵌入；同时，将深度图转换为点云，并采用定制的点嵌入网络（PEN）独立生成3D几何嵌入。这些嵌入随后被拼接，并由提出的空间感知动作专家（3DAE）处理，以生成精确的动作序列。通过在模拟和真实世界环境中的大量实验，GeoVLA展示了优越的性能和鲁棒性。\n### 论文核心贡献点\n1. 提出GeoVLA框架，首次在VLA模型中并行处理视觉和点云模态，增强空间理解和几何感知能力。\n2. 引入点嵌入网络（PEN）和3D增强动作专家（3DAE）两个新模块：PEN提取判别性几何特征，3DAE通过模态特定专家有效整合视觉和几何线索。\n3. 在LIBERO和ManiSkill2模拟基准上达到SOTA性能，并在真实世界任务中表现出对高度适应性、尺度感知和视角不变性的显著鲁棒性。\n### 论文方法描述\nGeoVLA采用三阶段端到端架构：\n1. **视觉-语言处理**：使用预训练VLM（如Prismatic-7B）处理RGB图像和语言指令，生成通用理解特征F_VL。\n2. **3D点云编码**：将深度图转换为以末端执行器为中心的点云，通过PEN进行双路径处理：\n - 几何特征路径：使用大卷积核和局部池化提取patch级特征。\n - 位置编码路径：通过旋转位置编码（RoPE）保留3D空间信息。\n 最终选择末端执行器对应的锚点token作为输出特征F_P。\n3. **动作生成**：3DAE基于扩散变换器（DiT）架构，采用静态路由的混合专家（MoE）设计：\n - 训练时随机丢弃一种模态（纯视觉语言、纯语言+几何、全模态），确保专家平衡。\n - 推理时基于DDIM采样，由多模态条件逐步去噪生成动作块（T=16）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - LIBERO：包含5个任务套件（LIBERO-Spatial/Object/Goal/Long/90），评估空间、物体多样性和长时序操作。\n - ManiSkill2：使用5个抓取任务（如PickCube、PickSingleYCB），测试基本操作和泛化能力。\n - 真实世界数据：8项任务（如放置胡萝卜、叠积木、套杯等），包含高度、尺度和视角变体。\n- **训练资源**：\n - 8块NVIDIA A100 GPU（FSDP策略）。\n - 总批次大小256，学习率2e-5（AdamW优化器）。\n - 混合精度训练，LIBERO训练约6轮（20小时），ManiSkill2约2轮。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：LIBERO和ManiSkill2标准基准。\n - 真实环境：WidowX-250s机械臂搭配RealSense-435i深度相机（0.8米距离）。\n- **评估指标**：\n - 成功率（SR）：LIBERO各任务50次尝试、ManiSkill2各20次尝试的平均成功率。\n - 泛化测试：真实世界任务中调整目标高度（篮筐层数）、物体尺度（套娃大小）、相机视角（0°-45°）和背景（海绵垫移除）下的成功率变化。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>天津大学</li><li>Dexmal</li><li>清华大学</li></ul>\n<h3>论文概述</h3>\n<p>该论文提出了GeoVLA，一种新的视觉-语言-动作（VLA）模型框架，通过有效整合3D信息来推进机器人操作。当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界中的丰富几何信息，这限制了它们的空间感知能力和适应性。GeoVLA使用视觉-语言模型（VLM）处理图像和语言指令，提取融合的视觉-语言嵌入；同时，将深度图转换为点云，并采用定制的点嵌入网络（PEN）独立生成3D几何嵌入。这些嵌入随后被拼接，并由提出的空间感知动作专家（3DAE）处理，以生成精确的动作序列。通过在模拟和真实世界环境中的大量实验，GeoVLA展示了优越的性能和鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出GeoVLA框架，首次在VLA模型中并行处理视觉和点云模态，增强空间理解和几何感知能力。</li><li>引入点嵌入网络（PEN）和3D增强动作专家（3DAE）两个新模块：PEN提取判别性几何特征，3DAE通过模态特定专家有效整合视觉和几何线索。</li><li>在LIBERO和ManiSkill2模拟基准上达到SOTA性能，并在真实世界任务中表现出对高度适应性、尺度感知和视角不变性的显著鲁棒性。</li></ol>\n<h3>论文方法描述</h3>\n<p>GeoVLA采用三阶段端到端架构：</p>\n<ol><li><strong>视觉-语言处理</strong>：使用预训练VLM（如Prismatic-7B）处理RGB图像和语言指令，生成通用理解特征F_VL。</li><li><strong>3D点云编码</strong>：将深度图转换为以末端执行器为中心的点云，通过PEN进行双路径处理：</li></ol>\n<p> - 几何特征路径：使用大卷积核和局部池化提取patch级特征。</p>\n<p> - 位置编码路径：通过旋转位置编码（RoPE）保留3D空间信息。</p>\n<p> 最终选择末端执行器对应的锚点token作为输出特征F_P。</p>\n<ol><li><strong>动作生成</strong>：3DAE基于扩散变换器（DiT）架构，采用静态路由的混合专家（MoE）设计：</li></ol>\n<p> - 训练时随机丢弃一种模态（纯视觉语言、纯语言+几何、全模态），确保专家平衡。</p>\n<p> - 推理时基于DDIM采样，由多模态条件逐步去噪生成动作块（T=16）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - LIBERO：包含5个任务套件（LIBERO-Spatial/Object/Goal/Long/90），评估空间、物体多样性和长时序操作。</p>\n<p> - ManiSkill2：使用5个抓取任务（如PickCube、PickSingleYCB），测试基本操作和泛化能力。</p>\n<p> - 真实世界数据：8项任务（如放置胡萝卜、叠积木、套杯等），包含高度、尺度和视角变体。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 8块NVIDIA A100 GPU（FSDP策略）。</p>\n<p> - 总批次大小256，学习率2e-5（AdamW优化器）。</p>\n<p> - 混合精度训练，LIBERO训练约6轮（20小时），ManiSkill2约2轮。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：LIBERO和ManiSkill2标准基准。</p>\n<p> - 真实环境：WidowX-250s机械臂搭配RealSense-435i深度相机（0.8米距离）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（SR）：LIBERO各任务50次尝试、ManiSkill2各20次尝试的平均成功率。</p>\n<p> - 泛化测试：真实世界任务中调整目标高度（篮筐层数）、物体尺度（套娃大小）、相机视角（0°-45°）和背景（海绵垫移除）下的成功率变化。</p>"
  },
  {
    "date": "2025-08-12",
    "title": "Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding",
    "link": "http://arxiv.org/abs/2508.09032",
    "summary_markdown": "### 论文研究单位\n莫斯科物理理工学院 (MIPT), 俄罗斯多尔戈普鲁德内, 141701\n人工智能研究所 (AIRI), 俄罗斯莫斯科, 121170\n### 论文概述\n该论文提出了一种名为“Spatial Traces”的新方法，旨在增强视觉-语言-动作（VLA）模型。该方法通过将历史关键点的视觉轨迹投影到深度图上，将空间信息和时间信息集成到一个统一的视觉提示中。这种设计使得模型能够同时捕捉空间和时间上下文，从而提高在复杂操作任务中的性能。在SimplerEnv环境中的实验表明，该方法仅需最少的训练数据即可实现性能提升，其平均任务完成成功率比SpatialVLA高出4%，比TraceVLA高出19%。\n### 论文核心贡献点\n1. 提出了一种名为“Spatial Traces”的方法，该方法能够让VLA模型通过单张图像同时利用环境中的空间和时间信息。\n2. 在SimplerEnv中的实验表明，仅用52条训练轨迹进行微调，该方法相较于SpatialVLA提升了4%的任务成功率，相较于TraceVLA提升了19%，证明了在数据收集困难的真实世界应用中，融合空间和时间信息的价值。\n### 论文方法描述\n该方法的核心是构建一个融合了空间和时间信息的视觉表示。具体来说，它包含两个主要组成部分：深度信息和视觉轨迹。首先，使用一个深度估计模型（如ZoeDepth）从当前观察中预测深度图。接着，使用一个轨迹预测模型（如Co-Tracker）从过去一系列观察中追踪并生成关键点的视觉轨迹。然后，将这些二维轨迹投影到当前预测的深度图上，形成一个带有轨迹标记的深度图。最后，原始的RGB观察图像和这个带有轨迹的深度图分别通过各自的图像处理器（如Siglip和Ego3D Positional Encoder）转换成嵌入向量，并将这两个嵌入向量相加，生成最终的视觉嵌入。这个融合的视觉嵌入与文本指令的嵌入一起被输入到VLA模型（如PaliGemma2）中，以预测下一步的动作。\n### 论文使用数据集和训练资源\n数据集：Bridge数据集的一个子集，包含52条来自真实机器人操作的轨迹，总计1969个步骤。\n训练资源：模型在单个NVIDIA TITAN RTX GPU上进行训练。训练采用了LoRA适配器，应用于模型的所有线性层。训练参数设置如下：学习率为5e-5，批次大小为1，训练周期为2，LoRA秩为32，优化器为AdamW。完成一个模型的训练大约需要2小时。\n### 论文使用的评估环境和评估指标\n评估环境：SimplerEnv，一个基于Robosuite构建的虚拟环境，专门用于机器人操作任务。评估选取了四个代表性任务：“Put Spoon”（放勺子）、“Put Carrot”（放胡萝卜）、“Stack Blocks”（堆叠积木）和“Put Eggplant”（放茄子）。\n评估指标：主要使用两个指标：\n1. 目标条件成功率（GCS）：衡量智能体是否成功达到基本的任务完成条件（例如，将目标物体从桌面上拿起）。\n2. 成功率（SR）：衡量整个任务是否完全成功完成，达到最终的目标状态。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>莫斯科物理理工学院 (MIPT), 俄罗斯多尔戈普鲁德内, 141701</p>\n<p>人工智能研究所 (AIRI), 俄罗斯莫斯科, 121170</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种名为“Spatial Traces”的新方法，旨在增强视觉-语言-动作（VLA）模型。该方法通过将历史关键点的视觉轨迹投影到深度图上，将空间信息和时间信息集成到一个统一的视觉提示中。这种设计使得模型能够同时捕捉空间和时间上下文，从而提高在复杂操作任务中的性能。在SimplerEnv环境中的实验表明，该方法仅需最少的训练数据即可实现性能提升，其平均任务完成成功率比SpatialVLA高出4%，比TraceVLA高出19%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种名为“Spatial Traces”的方法，该方法能够让VLA模型通过单张图像同时利用环境中的空间和时间信息。</li><li>在SimplerEnv中的实验表明，仅用52条训练轨迹进行微调，该方法相较于SpatialVLA提升了4%的任务成功率，相较于TraceVLA提升了19%，证明了在数据收集困难的真实世界应用中，融合空间和时间信息的价值。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法的核心是构建一个融合了空间和时间信息的视觉表示。具体来说，它包含两个主要组成部分：深度信息和视觉轨迹。首先，使用一个深度估计模型（如ZoeDepth）从当前观察中预测深度图。接着，使用一个轨迹预测模型（如Co-Tracker）从过去一系列观察中追踪并生成关键点的视觉轨迹。然后，将这些二维轨迹投影到当前预测的深度图上，形成一个带有轨迹标记的深度图。最后，原始的RGB观察图像和这个带有轨迹的深度图分别通过各自的图像处理器（如Siglip和Ego3D Positional Encoder）转换成嵌入向量，并将这两个嵌入向量相加，生成最终的视觉嵌入。这个融合的视觉嵌入与文本指令的嵌入一起被输入到VLA模型（如PaliGemma2）中，以预测下一步的动作。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：Bridge数据集的一个子集，包含52条来自真实机器人操作的轨迹，总计1969个步骤。</p>\n<p>训练资源：模型在单个NVIDIA TITAN RTX GPU上进行训练。训练采用了LoRA适配器，应用于模型的所有线性层。训练参数设置如下：学习率为5e-5，批次大小为1，训练周期为2，LoRA秩为32，优化器为AdamW。完成一个模型的训练大约需要2小时。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：SimplerEnv，一个基于Robosuite构建的虚拟环境，专门用于机器人操作任务。评估选取了四个代表性任务：“Put Spoon”（放勺子）、“Put Carrot”（放胡萝卜）、“Stack Blocks”（堆叠积木）和“Put Eggplant”（放茄子）。</p>\n<p>评估指标：主要使用两个指标：</p>\n<ol><li>目标条件成功率（GCS）：衡量智能体是否成功达到基本的任务完成条件（例如，将目标物体从桌面上拿起）。</li><li>成功率（SR）：衡量整个任务是否完全成功完成，达到最终的目标状态。</li></ol>"
  },
  {
    "date": "2025-08-12",
    "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing",
    "link": "http://arxiv.org/abs/2508.08706",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-11",
    "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions",
    "link": "http://arxiv.org/abs/2508.07650",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-07",
    "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
    "link": "http://arxiv.org/abs/2508.06571",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-06",
    "title": "Static and Plugged: Make Embodied Evaluation Simple",
    "link": "http://arxiv.org/abs/2508.06553",
    "summary_markdown": "### 论文研究单位\n上海人工智能实验室 (Shanghai AI Laboratory)\n### 论文概述\n论文指出了当前具身智能评估基准的两个主要问题：一是严重依赖高成本、高配置的模拟环境，二是缺乏统一的评估框架来同时评估高级认知和低级执行能力。为解决这些问题，论文提出了StaticEmbodiedBench，一个静态、统一、即插即用的基准。它通过仅使用静态关键帧和文本元数据来评估模型，无需复杂的模拟器，从而显著降低了评估的门槛。该基准包含42个场景和8个核心维度，能够分别评估视觉语言模型（VLM）的认知与决策能力以及视觉-语言-动作模型（VLA）的执行能力。\n### 论文核心贡献点\n- 提出了一种新颖的基于静态关键帧的评估方法和大脑-小脑协同评估框架，用于解耦和评估VLM与VLA在具身场景下的能力。\n- 构建了StaticEmbodiedBench，一个静态、统一、即插即用的基准，包含42个场景和8个评估维度，并发布了包含200个样本的子集。\n- 对19个VLM和11个VLA进行了全面评估，建立了首个具身智能的统一静态排行榜，并进行了深入分析。\n- 通过在真实机器人上实验，量化了静态评估与动态执行之间的性能差距（Static-to-Dynamic Gap），验证了静态评估方法的有效性。\n### 论文方法描述\n- **静态关键帧评估思路：** 核心洞察是，大多数具身任务的成功取决于轨迹中少数几个关键点（如识别目标物体、执行抓取等）。通过仅保留这些关键帧进行评估，可以极大减少计算和工程开销，同时保留对核心能力的评估。\n- **大脑-小脑协同评估设计：**\n - **StaticEmbodiedBench-VLM（大脑）：** 评估VLM作为认知核心的能力，涵盖三个维度：宏观规划（将高级指令分解为子任务）、微观感知（识别细粒度视觉线索）和分阶段推理（判断当前步骤并确定下一步动作）。评估在第一人称和第三人称两种视角下进行。\n - **StaticEmbodiedBench-VLA（小脑）：** 评估VLA的执行能力。VLA模型需预测一个7自由度（7-DoF）的动作向量（包含位置、姿态和夹爪状态）。通过计算预测动作与专家轨迹之间的L2距离来评分，并进一步将误差分解为位置误差、姿态误差和末端执行器误差，以提供更细粒度的分析。\n### 论文使用数据集和训练资源\n- **StaticEmbodiedBench-VLM：** 从Droid、VLABench等5个高质量数据集中收集了超过30万张高分辨率图像，经过GPT自动过滤、GPT-4o生成问答对和人工校验后，最终构建了一个包含1000个高质量任务样本的数据集。\n- **StaticEmbodiedBench-VLA：** 在真实实验室环境中，使用UR5机械臂构建了100个桌面操作任务数据集。每个任务组合了6种背景、10种物体和5种交互动词，记录了50个均匀采样的动作步骤及其对应的多视角图像和专家轨迹。\n- **训练资源：** 评估过程本身不涉及模型训练。VLM评估使用了两块NVIDIA A800 GPU，VLA评估使用了一块A800 GPU。\n### 论文使用的评估环境和评估指标\n- **评估环境：** 评估在静态数据上进行，不需要任何模拟器或真实机器人。为了验证静态评估的有效性，部分实验（S2D Gap验证）在真实的UR5机器人上部署执行。\n- **评估指标：**\n - **VLM指标：** 采用基于问答的准确率作为评估指标，分别计算宏观规划、微观感知、分阶段推理三个维度的得分，以及第一人称和第三人称视角下的得分，最后汇总为总分。\n - **VLA指标：** 采用预测的7-DoF动作向量与专家轨迹之间的L2距离作为核心指标。评分标准为：距离小于等于1毫米得100分，大于等于1米得0分，中间距离通过对数映射到0-100分。同时报告位置、姿态、末端执行器三个分解维度的得分。\n - **静态到动态差距（S2D）：** 使用皮尔逊线性相关系数（PLCC）来衡量静态评估分数与真实世界执行成功率之间的相关性，以此验证静态评估方法的可靠性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海人工智能实验室 (Shanghai AI Laboratory)</p>\n<h3>论文概述</h3>\n<p>论文指出了当前具身智能评估基准的两个主要问题：一是严重依赖高成本、高配置的模拟环境，二是缺乏统一的评估框架来同时评估高级认知和低级执行能力。为解决这些问题，论文提出了StaticEmbodiedBench，一个静态、统一、即插即用的基准。它通过仅使用静态关键帧和文本元数据来评估模型，无需复杂的模拟器，从而显著降低了评估的门槛。该基准包含42个场景和8个核心维度，能够分别评估视觉语言模型（VLM）的认知与决策能力以及视觉-语言-动作模型（VLA）的执行能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一种新颖的基于静态关键帧的评估方法和大脑-小脑协同评估框架，用于解耦和评估VLM与VLA在具身场景下的能力。</li><li>构建了StaticEmbodiedBench，一个静态、统一、即插即用的基准，包含42个场景和8个评估维度，并发布了包含200个样本的子集。</li><li>对19个VLM和11个VLA进行了全面评估，建立了首个具身智能的统一静态排行榜，并进行了深入分析。</li><li>通过在真实机器人上实验，量化了静态评估与动态执行之间的性能差距（Static-to-Dynamic Gap），验证了静态评估方法的有效性。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>静态关键帧评估思路：</strong> 核心洞察是，大多数具身任务的成功取决于轨迹中少数几个关键点（如识别目标物体、执行抓取等）。通过仅保留这些关键帧进行评估，可以极大减少计算和工程开销，同时保留对核心能力的评估。</li><li><strong>大脑-小脑协同评估设计：</strong></li></ul>\n<p> - <strong>StaticEmbodiedBench-VLM（大脑）：</strong> 评估VLM作为认知核心的能力，涵盖三个维度：宏观规划（将高级指令分解为子任务）、微观感知（识别细粒度视觉线索）和分阶段推理（判断当前步骤并确定下一步动作）。评估在第一人称和第三人称两种视角下进行。</p>\n<p> - <strong>StaticEmbodiedBench-VLA（小脑）：</strong> 评估VLA的执行能力。VLA模型需预测一个7自由度（7-DoF）的动作向量（包含位置、姿态和夹爪状态）。通过计算预测动作与专家轨迹之间的L2距离来评分，并进一步将误差分解为位置误差、姿态误差和末端执行器误差，以提供更细粒度的分析。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>StaticEmbodiedBench-VLM：</strong> 从Droid、VLABench等5个高质量数据集中收集了超过30万张高分辨率图像，经过GPT自动过滤、GPT-4o生成问答对和人工校验后，最终构建了一个包含1000个高质量任务样本的数据集。</li><li><strong>StaticEmbodiedBench-VLA：</strong> 在真实实验室环境中，使用UR5机械臂构建了100个桌面操作任务数据集。每个任务组合了6种背景、10种物体和5种交互动词，记录了50个均匀采样的动作步骤及其对应的多视角图像和专家轨迹。</li><li><strong>训练资源：</strong> 评估过程本身不涉及模型训练。VLM评估使用了两块NVIDIA A800 GPU，VLA评估使用了一块A800 GPU。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境：</strong> 评估在静态数据上进行，不需要任何模拟器或真实机器人。为了验证静态评估的有效性，部分实验（S2D Gap验证）在真实的UR5机器人上部署执行。</li><li><strong>评估指标：</strong></li></ul>\n<p> - <strong>VLM指标：</strong> 采用基于问答的准确率作为评估指标，分别计算宏观规划、微观感知、分阶段推理三个维度的得分，以及第一人称和第三人称视角下的得分，最后汇总为总分。</p>\n<p> - <strong>VLA指标：</strong> 采用预测的7-DoF动作向量与专家轨迹之间的L2距离作为核心指标。评分标准为：距离小于等于1毫米得100分，大于等于1米得0分，中间距离通过对数映射到0-100分。同时报告位置、姿态、末端执行器三个分解维度的得分。</p>\n<p> - <strong>静态到动态差距（S2D）：</strong> 使用皮尔逊线性相关系数（PLCC）来衡量静态评估分数与真实世界执行成功率之间的相关性，以此验证静态评估方法的可靠性。</p>"
  },
  {
    "date": "2025-08-06",
    "title": "A tutorial note on collecting simulated data for vision-language-action models",
    "link": "http://arxiv.org/abs/2508.06547",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-07",
    "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control",
    "link": "http://arxiv.org/abs/2508.05342",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-07",
    "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction",
    "link": "http://arxiv.org/abs/2508.05294",
    "summary_markdown": "### 论文研究单位\n- University of Turku\n- Zurich University of Applied Sciences\n- Centre for Artificial Ingelligence, Zurich University of Applied Sciences\n- Agentic Systems Lab, Department of Management, Technology and Economics, ETH Zürich\n- Faculty of Mathematics and Information Science, Warsaw University of Technology\n- Robotec.ai\n- Binabik.ai\n### 论文概述\n本文是一篇综述论文，回顾和分类了基于大语言模型（LLM）、视觉语言模型（VLM）和视觉-语言-动作模型（VLA）的机器人自主性与交互方法。文章聚焦于将基础模型作为高级代理的模块化系统，这些代理能够理解用户意图、生成计划、调用机器人API或与中间件（如ROS）交互，而不是取代底层的机器人软件栈。论文旨在填补现有综述的空白，特别关注AI代理与现有控制软件、库或中间件交互的新兴设计模式，并包括社区驱动项目、ROS包和工业框架。\n### 论文核心贡献点\n- 提出了一个双维度的分类法：一个维度是模型与机器人系统的集成方法（协议集成、接口集成、编排导向集成、嵌入式/直接集成）；另一个维度是代理在系统中扮演的角色（规划代理、编排代理、任务特定代理、以模型为中心的代理、通用代理、通用系统代理）。\n- 对现有学术工作和社区项目进行了全面的回顾和比较分析，涵盖了从早期的“代码即策略”到最新的通用VLA模型和代理框架。\n- 提供了一个实践设计工具包部分，指导开发者和研究人员的实现与原型设计。\n- 在讨论部分深入探讨了关键开放性问题，并概述了代理式具身AI的未来研究方向，如多智能体系统、伦理考虑、边缘-云计算连续体、记忆机制和世界模型等。\n### 论文方法描述\n论文的核心方法是提出并使用其分类法来分析和组织现有工作。该方法包括四个主要集成途径：\n1. **协议集成**：将LLM用作不同协议间的“翻译器”，例如将自然语言指令转换为单个ROS命令行调用（如 `ros2ai`）或通过模型上下文协议（MCP）服务器进行工具调用。\n2. **接口集成**：在协议集成的基础上增强交互性，代理通过多轮工具调用与环境和用户进行交互。代理可以分解高级指令，并根据工具调用的结果进行适应性调整，例如ROSA、RAI、BUMBLE等框架。\n3. **编排导向集成**：LLM/VLM作为资源管理器、规划器或协调器，负责管理多个子系统、工具或下属代理。其重点在于资源分配和任务调度，而非直接的用户交互。\n4. **嵌入式或直接集成**：模型直接映射感知输入到机器人动作（端到端），或作为特定子系统（如感知模块）。这通常指VLA或大型行为模型（LBM），例如RT-2、π0。\n### 论文使用数据集和训练资源\n作为一篇综述论文，本文本身没有提出需要特定数据集或训练资源的新模型。其分析和分类基于已发表的学术论文和公开的社区项目。论文回顾的这些基础模型（如RT-2、π0等）是在各种公开的机器人轨迹数据和互联网规模的视觉-语言数据集上进行训练的，但本文未详细列出这些具体的数据集和计算资源。\n### 论文使用的评估环境和评估指标\n本文不进行新的实验评估，而是回顾和比较了其所引用工作中的评估方法。评估环境多样化，包括物理机器人平台（如JPL的NeBula-Spot四足机器人、Husarion ROSBot XL）和仿真环境（如NVIDIA Isaac Sim）。评估指标因任务而异，通常包括操作成功率、导航效率、任务完成时间以及人机交互的自然性和安全性等。论文指出，目前该领域缺乏统一的评估基准和标准化的指标体系，这是一个重要的开放性问题。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>University of Turku</li><li>Zurich University of Applied Sciences</li><li>Centre for Artificial Ingelligence, Zurich University of Applied Sciences</li><li>Agentic Systems Lab, Department of Management, Technology and Economics, ETH Zürich</li><li>Faculty of Mathematics and Information Science, Warsaw University of Technology</li><li>Robotec.ai</li><li>Binabik.ai</li></ul>\n<h3>论文概述</h3>\n<p>本文是一篇综述论文，回顾和分类了基于大语言模型（LLM）、视觉语言模型（VLM）和视觉-语言-动作模型（VLA）的机器人自主性与交互方法。文章聚焦于将基础模型作为高级代理的模块化系统，这些代理能够理解用户意图、生成计划、调用机器人API或与中间件（如ROS）交互，而不是取代底层的机器人软件栈。论文旨在填补现有综述的空白，特别关注AI代理与现有控制软件、库或中间件交互的新兴设计模式，并包括社区驱动项目、ROS包和工业框架。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个双维度的分类法：一个维度是模型与机器人系统的集成方法（协议集成、接口集成、编排导向集成、嵌入式/直接集成）；另一个维度是代理在系统中扮演的角色（规划代理、编排代理、任务特定代理、以模型为中心的代理、通用代理、通用系统代理）。</li><li>对现有学术工作和社区项目进行了全面的回顾和比较分析，涵盖了从早期的“代码即策略”到最新的通用VLA模型和代理框架。</li><li>提供了一个实践设计工具包部分，指导开发者和研究人员的实现与原型设计。</li><li>在讨论部分深入探讨了关键开放性问题，并概述了代理式具身AI的未来研究方向，如多智能体系统、伦理考虑、边缘-云计算连续体、记忆机制和世界模型等。</li></ul>\n<h3>论文方法描述</h3>\n<p>论文的核心方法是提出并使用其分类法来分析和组织现有工作。该方法包括四个主要集成途径：</p>\n<ol><li><strong>协议集成</strong>：将LLM用作不同协议间的“翻译器”，例如将自然语言指令转换为单个ROS命令行调用（如 <code>ros2ai</code>）或通过模型上下文协议（MCP）服务器进行工具调用。</li><li><strong>接口集成</strong>：在协议集成的基础上增强交互性，代理通过多轮工具调用与环境和用户进行交互。代理可以分解高级指令，并根据工具调用的结果进行适应性调整，例如ROSA、RAI、BUMBLE等框架。</li><li><strong>编排导向集成</strong>：LLM/VLM作为资源管理器、规划器或协调器，负责管理多个子系统、工具或下属代理。其重点在于资源分配和任务调度，而非直接的用户交互。</li><li><strong>嵌入式或直接集成</strong>：模型直接映射感知输入到机器人动作（端到端），或作为特定子系统（如感知模块）。这通常指VLA或大型行为模型（LBM），例如RT-2、π0。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>作为一篇综述论文，本文本身没有提出需要特定数据集或训练资源的新模型。其分析和分类基于已发表的学术论文和公开的社区项目。论文回顾的这些基础模型（如RT-2、π0等）是在各种公开的机器人轨迹数据和互联网规模的视觉-语言数据集上进行训练的，但本文未详细列出这些具体的数据集和计算资源。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>本文不进行新的实验评估，而是回顾和比较了其所引用工作中的评估方法。评估环境多样化，包括物理机器人平台（如JPL的NeBula-Spot四足机器人、Husarion ROSBot XL）和仿真环境（如NVIDIA Isaac Sim）。评估指标因任务而异，通常包括操作成功率、导航效率、任务完成时间以及人机交互的自然性和安全性等。论文指出，目前该领域缺乏统一的评估基准和标准化的指标体系，这是一个重要的开放性问题。</p>"
  },
  {
    "date": "2025-08-07",
    "title": "Learning to See and Act: Task-Aware View Planning for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2508.05186",
    "summary_markdown": "### 论文研究单位\n中山大学、鹏城实验室、南洋理工大学、中国科学院深圳先进技术研究院、X-Era AI Lab\n### 论文概述\n论文提出Task-Aware View Planning (TAVP)框架，通过整合主动视角规划与任务感知表征学习，解决多任务机器人操作中固定视角导致的3D感知受限和任务干扰问题。该方法利用高效的探索策略获取信息丰富的视角，并结合任务感知的视觉编码器提升多任务泛化能力，在RLBench任务上实现了优于固定视角基线模型的性能。\n### 论文核心贡献点\n1. Multi-Viewpoint Exploration Policy (MVEP)：通过动态多视角重渲染解决遮挡和视角不足问题，增强3D感知能力。\n2. Task-aware Mixture-of-Experts (TaskMoE)：根据任务相关的指令和场景视觉信息动态选择感知与动作生成专家，提升多任务处理能力和鲁棒性。\n3. 在18个RLBench仿真任务上实现显著优于现有基线的多任务操作性能，在准确性和鲁棒性方面表现突出。\n### 论文方法描述\nTAVP框架包含三个阶段：\n1. **固定视角训练**：使用三个默认视角训练TaskMoE集成的RVT-2变体，损失函数包括热力图交叉熵损失、旋转损失、夹爪状态损失和碰撞损失。\n2. **视角策略优化**：通过伪环境交互机制和PPO算法训练MVEP，奖励函数包括任务损失奖励、置信度奖励和视角多样性奖励。\n3. **联合微调**：使用相同损失函数微调整个TAVP模型（除MVEP外）。\nTaskMoE通过跨模态模块融合指令和视觉信息，使用解耦门控策略实现参数共享，MVEP基于点云预测K个相机姿态并使用look-at模型参数化。\n### 论文使用数据集和训练资源\n- **数据集**：RLBench仿真环境18个操作任务，每个任务包含多个变体。\n- **训练资源**：4×NVIDIA RTX A800 GPU（80GB内存）。\n- **关键参数**：视角数K=3，TaskMoE门数N_G=8，专家数N_E=16，每个任务选择Top-2专家，相机径向约束0.75-1.3米。\n### 论文使用的评估环境和评估指标\n- **评估环境**：RLBench仿真环境（CoppeliaSim）和真实世界设置（6-DoF Dobot Nova 2机械臂+3个Intel RealSense深度相机）。\n- **评估指标**：任务平均成功率（%）及其标准差，按任务类型细分为单独成功率。\n- **基线对比**：与固定视角模型（RVT2, ARP, ARP+）和3D基线模型（PerAct, HiveFormer等）在18个任务上对比成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中山大学、鹏城实验室、南洋理工大学、中国科学院深圳先进技术研究院、X-Era AI Lab</p>\n<h3>论文概述</h3>\n<p>论文提出Task-Aware View Planning (TAVP)框架，通过整合主动视角规划与任务感知表征学习，解决多任务机器人操作中固定视角导致的3D感知受限和任务干扰问题。该方法利用高效的探索策略获取信息丰富的视角，并结合任务感知的视觉编码器提升多任务泛化能力，在RLBench任务上实现了优于固定视角基线模型的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>Multi-Viewpoint Exploration Policy (MVEP)：通过动态多视角重渲染解决遮挡和视角不足问题，增强3D感知能力。</li><li>Task-aware Mixture-of-Experts (TaskMoE)：根据任务相关的指令和场景视觉信息动态选择感知与动作生成专家，提升多任务处理能力和鲁棒性。</li><li>在18个RLBench仿真任务上实现显著优于现有基线的多任务操作性能，在准确性和鲁棒性方面表现突出。</li></ol>\n<h3>论文方法描述</h3>\n<p>TAVP框架包含三个阶段：</p>\n<ol><li><strong>固定视角训练</strong>：使用三个默认视角训练TaskMoE集成的RVT-2变体，损失函数包括热力图交叉熵损失、旋转损失、夹爪状态损失和碰撞损失。</li><li><strong>视角策略优化</strong>：通过伪环境交互机制和PPO算法训练MVEP，奖励函数包括任务损失奖励、置信度奖励和视角多样性奖励。</li><li><strong>联合微调</strong>：使用相同损失函数微调整个TAVP模型（除MVEP外）。</li></ol>\n<p>TaskMoE通过跨模态模块融合指令和视觉信息，使用解耦门控策略实现参数共享，MVEP基于点云预测K个相机姿态并使用look-at模型参数化。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：RLBench仿真环境18个操作任务，每个任务包含多个变体。</li><li><strong>训练资源</strong>：4×NVIDIA RTX A800 GPU（80GB内存）。</li><li><strong>关键参数</strong>：视角数K=3，TaskMoE门数N_G=8，专家数N_E=16，每个任务选择Top-2专家，相机径向约束0.75-1.3米。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：RLBench仿真环境（CoppeliaSim）和真实世界设置（6-DoF Dobot Nova 2机械臂+3个Intel RealSense深度相机）。</li><li><strong>评估指标</strong>：任务平均成功率（%）及其标准差，按任务类型细分为单独成功率。</li><li><strong>基线对比</strong>：与固定视角模型（RVT2, ARP, ARP+）和3D基线模型（PerAct, HiveFormer等）在18个任务上对比成功率。</li></ul>"
  },
  {
    "date": "2025-08-04",
    "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
    "link": "http://arxiv.org/abs/2508.02549",
    "summary_markdown": "### 论文研究单位\nShuo Wang<sup>1,3</sup>, Yongcai Wang<sup>1</sup>, Zhaoxin Fan<sup>2</sup>, Yucheng Wang<sup>3</sup>, Maiyue Chen<sup>3</sup>, Kaihui Wang<sup>3</sup>, Zhizhong Su<sup>3</sup>, Wanting Li<sup>1</sup>, Xudong Cai<sup>1</sup>, Yeying Jin<sup>4</sup>, Deying Li<sup>1</sup>。注：原文HTML未提供具体机构名称。\n### 论文概述\n本文针对单目视觉语言导航任务中，由于视角受限导致性能显著低于使用全景RGB-D输入方法的问题，提出了一个名为MonoDream的轻量级视觉语言行动框架。该框架使单目智能体能够学习一个统一的导航表示，通过潜在全景梦境任务进行监督，从而仅凭单目输入就能推断出隐式的全局、几何和时序上下文。实验表明，MonoDream在多个VLN基准测试中持续提升单目导航性能，并显著缩小了与基于全景智能体之间的性能差距。\n### 论文核心贡献点\n1. 提出了MonoDream，一种新颖的单目VLN框架，通过增强智能体的内部全局感知能力，使其能够从单目图像中推断隐式的全局、几何和时序上下文。\n2. 引入了两个核心组件：统一导航表示，用于共同编码导航动作和潜在的全局场景信息；以及潜在全景梦境任务，通过监督当前和未来全景RGB-D的潜在特征来学习UNR。\n3. 在R2R-CE和RxR-CE等单目VLN-CE基准上取得了最先进的性能，并且在没有使用外部训练数据的情况下展现了强大的泛化能力。\n### 论文方法描述\nMonoDream构建于一个视觉语言模型之上，核心是统一导航表示。该表示是一个共享的潜在空间，共同对齐了导航相关的信息，包括动作意图、全局场景布局、深度感知和未来动态。为了训练UNR，方法设计了潜在全景梦境任务作为辅助监督信号，仅在训练中使用。LPD任务引导模型从单目输入中预测当前和未来步骤的全景RGB和深度观察的潜在特征。整个框架通过多任务协同训练进行优化，联合训练动作预测、指令推理和LPD任务，使模型学习将动作决策与想象中的全局和未来上下文对齐。\n### 论文使用数据集和训练资源\n数据集：使用了模拟环境中的数据，包括R2R-CE和RxR-CE的训练集，并通过DAgger策略额外收集了50万个样本，总计约142万个步级样本。\n训练资源：基础模型为NVILA-lite-2B，使用8块NVIDIA H20 GPU进行训练，训练了5个周期，学习率为1e-5，批大小为80。\n### 论文使用的评估环境和评估指标\n评估环境：在VLN-CE基准测试中进行评估，具体包括R2R-CE和RxR-CE数据集的Val-Unseen拆分。\n评估指标：遵循标准VLN评估协议，主要指标包括导航误差、成功率、成功率加权路径长度和导航成功率，其中SR和SPL被认为是主要评估指标。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Shuo Wang<sup>1,3</sup>, Yongcai Wang<sup>1</sup>, Zhaoxin Fan<sup>2</sup>, Yucheng Wang<sup>3</sup>, Maiyue Chen<sup>3</sup>, Kaihui Wang<sup>3</sup>, Zhizhong Su<sup>3</sup>, Wanting Li<sup>1</sup>, Xudong Cai<sup>1</sup>, Yeying Jin<sup>4</sup>, Deying Li<sup>1</sup>。注：原文HTML未提供具体机构名称。</p>\n<h3>论文概述</h3>\n<p>本文针对单目视觉语言导航任务中，由于视角受限导致性能显著低于使用全景RGB-D输入方法的问题，提出了一个名为MonoDream的轻量级视觉语言行动框架。该框架使单目智能体能够学习一个统一的导航表示，通过潜在全景梦境任务进行监督，从而仅凭单目输入就能推断出隐式的全局、几何和时序上下文。实验表明，MonoDream在多个VLN基准测试中持续提升单目导航性能，并显著缩小了与基于全景智能体之间的性能差距。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了MonoDream，一种新颖的单目VLN框架，通过增强智能体的内部全局感知能力，使其能够从单目图像中推断隐式的全局、几何和时序上下文。</li><li>引入了两个核心组件：统一导航表示，用于共同编码导航动作和潜在的全局场景信息；以及潜在全景梦境任务，通过监督当前和未来全景RGB-D的潜在特征来学习UNR。</li><li>在R2R-CE和RxR-CE等单目VLN-CE基准上取得了最先进的性能，并且在没有使用外部训练数据的情况下展现了强大的泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<p>MonoDream构建于一个视觉语言模型之上，核心是统一导航表示。该表示是一个共享的潜在空间，共同对齐了导航相关的信息，包括动作意图、全局场景布局、深度感知和未来动态。为了训练UNR，方法设计了潜在全景梦境任务作为辅助监督信号，仅在训练中使用。LPD任务引导模型从单目输入中预测当前和未来步骤的全景RGB和深度观察的潜在特征。整个框架通过多任务协同训练进行优化，联合训练动作预测、指令推理和LPD任务，使模型学习将动作决策与想象中的全局和未来上下文对齐。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：使用了模拟环境中的数据，包括R2R-CE和RxR-CE的训练集，并通过DAgger策略额外收集了50万个样本，总计约142万个步级样本。</p>\n<p>训练资源：基础模型为NVILA-lite-2B，使用8块NVIDIA H20 GPU进行训练，训练了5个周期，学习率为1e-5，批大小为80。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：在VLN-CE基准测试中进行评估，具体包括R2R-CE和RxR-CE数据集的Val-Unseen拆分。</p>\n<p>评估指标：遵循标准VLN评估协议，主要指标包括导航误差、成功率、成功率加权路径长度和导航成功率，其中SR和SPL被认为是主要评估指标。</p>"
  },
  {
    "date": "2025-08-04",
    "title": "CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning",
    "link": "http://arxiv.org/abs/2508.02219",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-04",
    "title": "FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2508.02190",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-04",
    "title": "RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2508.02062",
    "summary_markdown": "### 论文研究单位\n宾夕法尼亚大学（University of Pennsylvania）和不列颠哥伦比亚大学（University of British Columbia）\n### 论文概述\n本文提出了一种名为RICL（Retraining for In-Context Learning）的方法，旨在向预训练的视觉-语言-动作（VLA）模型中注入上下文学习能力。传统VLA模型缺乏上下文学习（ICL）能力，而RICL通过微调少量机器人演示数据，使模型能够利用检索增强生成（RAG）和ICL适应新任务。用户只需提供10-20个目标任务演示，RICL即可检索相关上下文信息并提升任务性能，无需参数更新。实验应用于π₀-FAST VLA，在涉及未见对象、新颖动作和新场景的操作任务中验证了有效性。\n### 论文核心贡献点\n- 提出RICL方法，首次实现向预训练VLA中注入ICL能力，无需从头训练。\n- 允许用户通过少量演示（10-20个）教导VLA执行新任务，支持零参数更新适应。\n- 结合RAG和ICL，提升模型在未见对象、新颖动作和新场景中的泛化能力。\n- 实验证明RICL-π₀在8个任务上平均完整任务成功率达31.25%，远高于基线的2.5%。\n- 支持进一步微调：在目标任务演示上微调RICL-VLA，可将平均成功率提升至61.67%。\n- 开源代码和模型权重（RICL-π₀），提供首个机器人操作任务的简单ICL接口。\n### 论文方法描述\n- **RICL训练流程**：后训练预训练VLA，输入序列包含查询图像/状态和检索自演示的多个图像、状态、动作。使用DINO-v2图像编码器和ℓ₂距离检索最相关的4组邻居数据，按距离排序（最近邻居置于上下文左侧）。\n- **动作插值层**：预测动作通过加权插值结合检索动作a'和LLM输出，公式为：\n ```\n πᵗʰᵉᵗᵃ_RICL-VLA = e^{-λd} * one-hot(a') + (1 - e^{-λd}) * σ(π_θ(retrieved, query))\n ```\n 其中d为查询图像与最近邻居的ℓ₂距离，λ=10，σ为Softmax函数。\n- **训练细节**：仅微调LLM部分，冻结图像编码器；最小化查询动作块的交叉熵损失；使用CosineDecaySchedule学习率调度。\n- **部署与微调**：部署时检索任务演示数据并执行ICL；微调阶段在相同演示上进行检索增强训练，进一步提升性能。\n### 论文使用数据集和训练资源\n- **训练数据**：使用Franka DROID平台收集400个演示（20个拾取放置任务，每任务20个演示），任务如\"移动物体至左侧/右侧\"或\"拾取物体放入碗中\"。\n- **基础模型**：π₀-FAST-DROID（基于π₀-FAST在DROID数据集微调）。\n- **检索数据**：评估时每个目标任务收集20个演示（如pokeball、idliplate等），用于RAG和ICL。\n- **训练资源**：两个NVIDIA A100 GPU，批量大小16，训练3个epoch，峰值学习率2.5e-5，动作块长度15。\n- **硬件平台**：Franka机械臂配移动底座，搭载顶部、右侧和手腕摄像头（图3）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Franka DROID平台，测试场景包括桌面（tabletop）和厨房水槽（sink），涉及新摄像头位置、光照和干扰物。\n- **评估任务**：8个任务涵盖未见对象（如pokeball、squeegee）、新颖动作（如推动lever、打开door）和新场景（如sink-idliplate）。任务细节如：\n - pokeball：拾取pokeball放入托盘。\n - squeegee：拖动刮刀清洁台面。\n - door：打开底部柜门。\n- **评估指标**：每个任务10次测试轨迹，随机化初始位置/方向。报告：\n - 完整任务成功率。\n - 中间检查点成功率（如抓取成功、移动成功）。\n - 结果以堆叠条形图展示（图4），包括不同方法对比。\n- **对比方法**：原始π₀-FAST-DROID、Retrieve and Play基线、Diffusion Policy基线，以及RICL-π₀的微调版本。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>宾夕法尼亚大学（University of Pennsylvania）和不列颠哥伦比亚大学（University of British Columbia）</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为RICL（Retraining for In-Context Learning）的方法，旨在向预训练的视觉-语言-动作（VLA）模型中注入上下文学习能力。传统VLA模型缺乏上下文学习（ICL）能力，而RICL通过微调少量机器人演示数据，使模型能够利用检索增强生成（RAG）和ICL适应新任务。用户只需提供10-20个目标任务演示，RICL即可检索相关上下文信息并提升任务性能，无需参数更新。实验应用于π₀-FAST VLA，在涉及未见对象、新颖动作和新场景的操作任务中验证了有效性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出RICL方法，首次实现向预训练VLA中注入ICL能力，无需从头训练。</li><li>允许用户通过少量演示（10-20个）教导VLA执行新任务，支持零参数更新适应。</li><li>结合RAG和ICL，提升模型在未见对象、新颖动作和新场景中的泛化能力。</li><li>实验证明RICL-π₀在8个任务上平均完整任务成功率达31.25%，远高于基线的2.5%。</li><li>支持进一步微调：在目标任务演示上微调RICL-VLA，可将平均成功率提升至61.67%。</li><li>开源代码和模型权重（RICL-π₀），提供首个机器人操作任务的简单ICL接口。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>RICL训练流程</strong>：后训练预训练VLA，输入序列包含查询图像/状态和检索自演示的多个图像、状态、动作。使用DINO-v2图像编码器和ℓ₂距离检索最相关的4组邻居数据，按距离排序（最近邻居置于上下文左侧）。</li><li><strong>动作插值层</strong>：预测动作通过加权插值结合检索动作a'和LLM输出，公式为：</li></ul>\n<p> ```</p>\n<p> πᵗʰᵉᵗᵃ_RICL-VLA = e^{-λd} * one-hot(a') + (1 - e^{-λd}) * σ(π_θ(retrieved, query))</p>\n<p> ```</p>\n<p> 其中d为查询图像与最近邻居的ℓ₂距离，λ=10，σ为Softmax函数。</p>\n<ul><li><strong>训练细节</strong>：仅微调LLM部分，冻结图像编码器；最小化查询动作块的交叉熵损失；使用CosineDecaySchedule学习率调度。</li><li><strong>部署与微调</strong>：部署时检索任务演示数据并执行ICL；微调阶段在相同演示上进行检索增强训练，进一步提升性能。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>训练数据</strong>：使用Franka DROID平台收集400个演示（20个拾取放置任务，每任务20个演示），任务如\"移动物体至左侧/右侧\"或\"拾取物体放入碗中\"。</li><li><strong>基础模型</strong>：π₀-FAST-DROID（基于π₀-FAST在DROID数据集微调）。</li><li><strong>检索数据</strong>：评估时每个目标任务收集20个演示（如pokeball、idliplate等），用于RAG和ICL。</li><li><strong>训练资源</strong>：两个NVIDIA A100 GPU，批量大小16，训练3个epoch，峰值学习率2.5e-5，动作块长度15。</li><li><strong>硬件平台</strong>：Franka机械臂配移动底座，搭载顶部、右侧和手腕摄像头（图3）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Franka DROID平台，测试场景包括桌面（tabletop）和厨房水槽（sink），涉及新摄像头位置、光照和干扰物。</li><li><strong>评估任务</strong>：8个任务涵盖未见对象（如pokeball、squeegee）、新颖动作（如推动lever、打开door）和新场景（如sink-idliplate）。任务细节如：</li></ul>\n<p> - pokeball：拾取pokeball放入托盘。</p>\n<p> - squeegee：拖动刮刀清洁台面。</p>\n<p> - door：打开底部柜门。</p>\n<ul><li><strong>评估指标</strong>：每个任务10次测试轨迹，随机化初始位置/方向。报告：</li></ul>\n<p> - 完整任务成功率。</p>\n<p> - 中间检查点成功率（如抓取成功、移动成功）。</p>\n<p> - 结果以堆叠条形图展示（图4），包括不同方法对比。</p>\n<ul><li><strong>对比方法</strong>：原始π₀-FAST-DROID、Retrieve and Play基线、Diffusion Policy基线，以及RICL-π₀的微调版本。</li></ul>"
  },
  {
    "date": "2025-07-31",
    "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation",
    "link": "http://arxiv.org/abs/2508.00097",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-31",
    "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2507.23682",
    "summary_markdown": "### 论文研究单位\n清华大学\n微软研究院\n### 论文概述\n本文提出了villa-X框架，一种视觉-语言-潜在动作模型，旨在提升机器人操作策略的泛化能力。该方法通过改进潜在动作的学习和融入视觉-语言-动作模型的预训练，实现了在仿真环境和真实机器人任务中的优越性能。\n### 论文核心贡献点\n1. 改进的潜在动作学习：引入本体感知前向动力学模型(proprioceptive FDM)，将潜在动作与物理动力学对齐，提升动作质量。\n2. 联合扩散建模：提出ACT模块，包含潜在动作专家(ACT-latent)和机器人动作专家(ACT-robot)，通过条件生成有效利用潜在动作。\n3. 零样本泛化能力：通过大规模预训练，模型能够泛化到未见过的机器人形态和开放词汇符号理解。\n### 论文方法描述\n1. 潜在动作模型(LAM)：\n - 逆向动力学模型(IDM)：从帧对预测潜在动作。\n - 视觉前向动力学模型(FDM)：重建未来观测。\n - 本体感知前向动力学模型(proprioceptive FDM)：预测未来机器人状态和动作，引入形态上下文来区分异构数据。\n - 联合优化图像重建损失、本体感知预测损失和向量量化承诺。\n\n2. 演员模块(ACT)：\n - 架构：包含VLM编码器、潜在动作专家(ACT-latent)和机器人动作专家(ACT-robot)。\n - 注意力掩码策略：训练中随机掩码机器人动作到潜在动作的注意力，增强鲁棒性。\n - 联合扩散建模：使用条件流匹配框架联合建模潜在动作和机器人动作序列。\n### 论文使用数据集和训练资源\n- 数据集：Bridge V2、Fractal、Something-Something V2、LIBERO、Xhand Dataset等混合数据集。\n- 训练资源：大规模混合数据预训练，特定机器人形态微调。具体硬件未明确提及，但涉及视觉-语言模型预训练和扩散模型训练。\n### 论文使用的评估环境和评估指标\n- 仿真环境：SIMPLER benchmark（包含Google机器人和WidowX机器人任务）。\n- 真实机器人：Realman RM 75机械臂（夹爪）和XArm + XHand灵巧手平台。\n- 评估指标：任务成功率（%），在视觉匹配设置下评估，包括平均成功率、零样本泛化测试等。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学</p>\n<p>微软研究院</p>\n<h3>论文概述</h3>\n<p>本文提出了villa-X框架，一种视觉-语言-潜在动作模型，旨在提升机器人操作策略的泛化能力。该方法通过改进潜在动作的学习和融入视觉-语言-动作模型的预训练，实现了在仿真环境和真实机器人任务中的优越性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>改进的潜在动作学习：引入本体感知前向动力学模型(proprioceptive FDM)，将潜在动作与物理动力学对齐，提升动作质量。</li><li>联合扩散建模：提出ACT模块，包含潜在动作专家(ACT-latent)和机器人动作专家(ACT-robot)，通过条件生成有效利用潜在动作。</li><li>零样本泛化能力：通过大规模预训练，模型能够泛化到未见过的机器人形态和开放词汇符号理解。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>潜在动作模型(LAM)：</li></ol>\n<p> - 逆向动力学模型(IDM)：从帧对预测潜在动作。</p>\n<p> - 视觉前向动力学模型(FDM)：重建未来观测。</p>\n<p> - 本体感知前向动力学模型(proprioceptive FDM)：预测未来机器人状态和动作，引入形态上下文来区分异构数据。</p>\n<p> - 联合优化图像重建损失、本体感知预测损失和向量量化承诺。</p>\n\n<ol><li>演员模块(ACT)：</li></ol>\n<p> - 架构：包含VLM编码器、潜在动作专家(ACT-latent)和机器人动作专家(ACT-robot)。</p>\n<p> - 注意力掩码策略：训练中随机掩码机器人动作到潜在动作的注意力，增强鲁棒性。</p>\n<p> - 联合扩散建模：使用条件流匹配框架联合建模潜在动作和机器人动作序列。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：Bridge V2、Fractal、Something-Something V2、LIBERO、Xhand Dataset等混合数据集。</li><li>训练资源：大规模混合数据预训练，特定机器人形态微调。具体硬件未明确提及，但涉及视觉-语言模型预训练和扩散模型训练。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>仿真环境：SIMPLER benchmark（包含Google机器人和WidowX机器人任务）。</li><li>真实机器人：Realman RM 75机械臂（夹爪）和XArm + XHand灵巧手平台。</li><li>评估指标：任务成功率（%），在视觉匹配设置下评估，包括平均成功率、零样本泛化测试等。</li></ul>"
  },
  {
    "date": "2025-07-31",
    "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving",
    "link": "http://arxiv.org/abs/2507.23540",
    "summary_markdown": "## 论文研究单位\nTechnical University of Munich (TUM)\n## 论文概述\n论文提出一个统一的感知-语言-行动框架，以解决自动驾驶系统在复杂、开放世界环境中实现类似人类的适应性、鲁棒性和可解释性所面临的挑战。该框架集成了多传感器融合（相机、激光雷达、雷达）与由大型语言模型（LLM）增强的视觉-语言-行动（VLA）架构，将低级感官处理与高级上下文推理相结合，实现了上下文感知、可解释和有安全边界的自动驾驶。通过对一个包含施工区的城市交叉路口场景的评估，证明了该框架在轨迹跟踪、速度预测和自适应规划方面的优越性能。\n## 论文核心贡献点\n- 提出了一个统一的认知框架，将多模态感知与基于LLM的推理和运动规划紧密耦合。\n- 开发了一个鲁棒的多传感器语义融合模块，将激光雷达、雷达和相机数据融合成结构化的场景描述。\n- 通过融入LLM驱动的推理，提高了对施工区或不可预测行人行为等未知场景的泛化能力。\n- 通过在nuScenes数据集上对具有施工区的城市交叉路口案例研究，进行了实证验证，展示了框架的有效性和实时适应性。\n## 论文方法描述\n该框架包含三个主要层次：\n1. **感知层**：处理来自相机、激光雷达和雷达的原始数据。相机图像由GPT-4.1进行解释，激光雷达点云通过CNN进行3D目标检测，雷达数据通过欧几里得聚类进行目标划分。融合机制将激光雷达和雷达的输出集成为包含精确位置和速度信息的结构化文本文件。\n2. **语言层**：处理结构化文本文件和相机图像，利用增强的VLA推理核心（GPT-4.1）进行全面的场景风险分析和理解，生成精确的驾驶指令和轨迹可视化。\n3. **行动层**：接收来自语言层的驾驶指令和轨迹可视化，进行详细的轨迹规划，并在高保真数字孪生仿真中进行验证，最终输出对车辆运动的直接控制。\n## 论文使用数据集和训练资源\n- **数据集**：nuScenes数据集。\n- **关键模型/组件**：\n - GPT-4.1作为VLA推理核心。\n - PointPillars架构用于激光雷达点云处理。\n - 基于CNN的模型用于相机特征提取和目标检测。\n## 论文使用的评估环境和评估指标\n- **评估环境**：在一个具有施工区的城市交叉路口的跟车任务中进行案例研究。使用高保真数字孪生仿真进行轨迹的安全性和效率验证。\n- **评估指标**：\n - **速度和转向角预测**：平均绝对误差（MAE）和决定系数（R² Score）。\n - **轨迹准确性**：平均位移误差（ADE）和最终位移误差（FDE）。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>Technical University of Munich (TUM)</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文提出一个统一的感知-语言-行动框架，以解决自动驾驶系统在复杂、开放世界环境中实现类似人类的适应性、鲁棒性和可解释性所面临的挑战。该框架集成了多传感器融合（相机、激光雷达、雷达）与由大型语言模型（LLM）增强的视觉-语言-行动（VLA）架构，将低级感官处理与高级上下文推理相结合，实现了上下文感知、可解释和有安全边界的自动驾驶。通过对一个包含施工区的城市交叉路口场景的评估，证明了该框架在轨迹跟踪、速度预测和自适应规划方面的优越性能。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了一个统一的认知框架，将多模态感知与基于LLM的推理和运动规划紧密耦合。</li><li>开发了一个鲁棒的多传感器语义融合模块，将激光雷达、雷达和相机数据融合成结构化的场景描述。</li><li>通过融入LLM驱动的推理，提高了对施工区或不可预测行人行为等未知场景的泛化能力。</li><li>通过在nuScenes数据集上对具有施工区的城市交叉路口案例研究，进行了实证验证，展示了框架的有效性和实时适应性。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>该框架包含三个主要层次：</p>\n<ol><li><strong>感知层</strong>：处理来自相机、激光雷达和雷达的原始数据。相机图像由GPT-4.1进行解释，激光雷达点云通过CNN进行3D目标检测，雷达数据通过欧几里得聚类进行目标划分。融合机制将激光雷达和雷达的输出集成为包含精确位置和速度信息的结构化文本文件。</li><li><strong>语言层</strong>：处理结构化文本文件和相机图像，利用增强的VLA推理核心（GPT-4.1）进行全面的场景风险分析和理解，生成精确的驾驶指令和轨迹可视化。</li><li><strong>行动层</strong>：接收来自语言层的驾驶指令和轨迹可视化，进行详细的轨迹规划，并在高保真数字孪生仿真中进行验证，最终输出对车辆运动的直接控制。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：nuScenes数据集。</li><li><strong>关键模型/组件</strong>：</li></ul>\n<p> - GPT-4.1作为VLA推理核心。</p>\n<p> - PointPillars架构用于激光雷达点云处理。</p>\n<p> - 基于CNN的模型用于相机特征提取和目标检测。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：在一个具有施工区的城市交叉路口的跟车任务中进行案例研究。使用高保真数字孪生仿真进行轨迹的安全性和效率验证。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>速度和转向角预测</strong>：平均绝对误差（MAE）和决定系数（R² Score）。</p>\n<p> - <strong>轨迹准确性</strong>：平均位移误差（ADE）和最终位移误差（FDE）。</p>"
  },
  {
    "date": "2025-07-31",
    "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning",
    "link": "http://arxiv.org/abs/2507.23318",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n中国的研究团队（由国家自然科学基金62476011资助）\n## 论文概述\nFastDriveVLA是一种针对端到端自动驾驶中视觉-语言-动作（VLA）模型的重建式视觉token剪枝框架。现有VLA模型由于大量视觉token导致计算成本高，而传统的基于注意力或相似度的剪枝方法在自动驾驶场景中表现不佳。受人类驾驶员主要关注前景区域的启发，本文提出了一种通过MAE风格像素重建来优先处理前景信息的方法，从而有效减少视觉token数量。\n## 论文核心贡献点\n1. 提出FastDriveVLA框架，一种新颖的基于重建的token剪枝方法，区别于现有的基于注意力和相似度的剪枝方法\n2. 设计ReconPruner，一个即插即用的剪枝器，通过MAE风格的像素重建进行训练\n3. 引入对抗性前景-背景重建策略，增强识别有价值token的能力\n4. 构建nuScenes-FG数据集，包含241K个带有前景分割标注的图像-掩码对\n5. 在nuScenes开环规划基准上，在不同剪枝比例下均达到SOTA性能\n## 论文方法描述\n1. **ReconPruner架构**：包含一个PrunerLayer（Qwen2.5-VL-3B的单个解码器层）和一个Scorer（单层前馈网络），总参数仅0.07B\n2. **训练策略**：\n - 使用可学习查询token捕获视觉token的前景显著性\n - 通过MAE风格像素重建训练，鼓励模型关注前景区域\n - 采用对抗性前景-背景重建策略，避免模型对所有token分配高显著性分数\n3. **推理过程**：根据目标剪枝比例，选择显著性分数最高的前K个token，保留其空间语义信息后输入大语言模型\n## 论文使用数据集和训练资源\n1. **数据集**：\n - nuScenes-FG：基于nuScenes构建的大规模数据集，包含241K个图像-掩码对，覆盖6个摄像头视角\n - 前景定义包括人、道路、车辆、交通标志（包括交通灯）和交通障碍物\n - 使用Grounded-SAM生成精细的前景分割标注\n2. **训练资源**：\n - 学习率：2e-5，使用余弦调度器\n - 训练周期：10个epoch\n - 硬件：两台H800 GPU\n - 训练时间：3小时\n## 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 数据集：nuScenes，包含1000个驾驶场景，每个约20秒\n - 测试样本：6019个\n - 基础模型：Impromptu-VLA（当前SOTA的端到端VLA模型）\n2. **评估指标**：\n - L2误差：轨迹预测的L2距离误差（1s、2s、3s和平均值）\n - 碰撞率：预测轨迹与物体的碰撞百分比\n - 交叉率：预测轨迹与道路边界的交叉百分比\n3. **性能表现**：\n - 在25%剪枝率下，甚至在L2和交叉指标上超过原始未剪枝模型（分别提升0.1%和1.0%）\n - 在50%剪枝率下达到最平衡的性能（保持99.1%的原始模型性能）\n - 实现7.5倍FLOPs减少，预填充和解码时间分别减少3.7倍和1.3倍",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>中国的研究团队（由国家自然科学基金62476011资助）</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>FastDriveVLA是一种针对端到端自动驾驶中视觉-语言-动作（VLA）模型的重建式视觉token剪枝框架。现有VLA模型由于大量视觉token导致计算成本高，而传统的基于注意力或相似度的剪枝方法在自动驾驶场景中表现不佳。受人类驾驶员主要关注前景区域的启发，本文提出了一种通过MAE风格像素重建来优先处理前景信息的方法，从而有效减少视觉token数量。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出FastDriveVLA框架，一种新颖的基于重建的token剪枝方法，区别于现有的基于注意力和相似度的剪枝方法</li><li>设计ReconPruner，一个即插即用的剪枝器，通过MAE风格的像素重建进行训练</li><li>引入对抗性前景-背景重建策略，增强识别有价值token的能力</li><li>构建nuScenes-FG数据集，包含241K个带有前景分割标注的图像-掩码对</li><li>在nuScenes开环规划基准上，在不同剪枝比例下均达到SOTA性能</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ol><li><strong>ReconPruner架构</strong>：包含一个PrunerLayer（Qwen2.5-VL-3B的单个解码器层）和一个Scorer（单层前馈网络），总参数仅0.07B</li><li><strong>训练策略</strong>：</li></ol>\n<p> - 使用可学习查询token捕获视觉token的前景显著性</p>\n<p> - 通过MAE风格像素重建训练，鼓励模型关注前景区域</p>\n<p> - 采用对抗性前景-背景重建策略，避免模型对所有token分配高显著性分数</p>\n<ol><li><strong>推理过程</strong>：根据目标剪枝比例，选择显著性分数最高的前K个token，保留其空间语义信息后输入大语言模型</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - nuScenes-FG：基于nuScenes构建的大规模数据集，包含241K个图像-掩码对，覆盖6个摄像头视角</p>\n<p> - 前景定义包括人、道路、车辆、交通标志（包括交通灯）和交通障碍物</p>\n<p> - 使用Grounded-SAM生成精细的前景分割标注</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> - 学习率：2e-5，使用余弦调度器</p>\n<p> - 训练周期：10个epoch</p>\n<p> - 硬件：两台H800 GPU</p>\n<p> - 训练时间：3小时</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - 数据集：nuScenes，包含1000个驾驶场景，每个约20秒</p>\n<p> - 测试样本：6019个</p>\n<p> - 基础模型：Impromptu-VLA（当前SOTA的端到端VLA模型）</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - L2误差：轨迹预测的L2距离误差（1s、2s、3s和平均值）</p>\n<p> - 碰撞率：预测轨迹与物体的碰撞百分比</p>\n<p> - 交叉率：预测轨迹与道路边界的交叉百分比</p>\n<ol><li><strong>性能表现</strong>：</li></ol>\n<p> - 在25%剪枝率下，甚至在L2和交叉指标上超过原始未剪枝模型（分别提升0.1%和1.0%）</p>\n<p> - 在50%剪枝率下达到最平衡的性能（保持99.1%的原始模型性能）</p>\n<p> - 实现7.5倍FLOPs减少，预填充和解码时间分别减少3.7倍和1.3倍</p>"
  },
  {
    "date": "2025-07-30",
    "title": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance",
    "link": "http://arxiv.org/abs/2507.22424",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-23",
    "title": "InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation",
    "link": "http://arxiv.org/abs/2507.17520",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-23",
    "title": "ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents",
    "link": "http://arxiv.org/abs/2507.17462",
    "summary_markdown": "论文研究单位\n- 上海交通大学自动化与智能感知学院及系统控制与信息处理教育部重点实验室\n- 剑桥大学工程系\n\n论文概述\n论文提出ERMV（Editing 4D Robotic Multi-view images）框架，用于编辑4D机器人多视角序列图像，以缓解具身智能体（如VLA模型）训练中的数据稀缺问题。现有方法面临三大挑战：动态多视角下的时空一致性维护、计算成本限制导致的工作窗口狭小、以及关键物体（如机械臂）的语义完整性维护。ERMV通过视觉引导、状态注入、稀疏时空建模和反馈干预机制解决这些问题，显著提升下游策略的鲁棒性和泛化性。\n\n论文核心贡献点\n- 提出首个面向4D机器人多视角序列编辑的框架ERMV，有效增强VLA模型训练数据\n- 设计极线运动感知注意力（EMA-Attn）处理运动模糊，结合稀疏时空模块（SST）实现大窗口低计算编辑\n- 引入多模态大语言模型（MLLM）的反馈干预机制，最小化人工参与下保障核心物体一致性\n- 在仿真、真实数据集和物理机器人平台验证效果，并证明其缩小sim-to-real差距的能力\n\n论文方法描述\n- 基于潜在扩散模型（LDM），学习条件分布 p(X'\\|X,C_guide,C_state,C_history)\n- 视觉引导：使用单帧编辑图像通过CLIP编码作为精确语义目标，避免文本歧义\n- 状态注入：融合相机位姿P_t^(v)、机器人动作q_t及其运动差分ΔP_t^(v)、Δq_t，经MLP编码为时空条件\n- 稀疏时空模块：随机采样K张图像（K<<L×N），解耦时空维度，将4D编辑重构为单帧多视角问题\n- 极线运动感知注意力：在应用几何约束前预测运动引起的像素偏移，保持运动模糊下的跨视图一致性\n- 反馈干预：MLLM检测编辑不一致性，仅在必要时请求专家提供分割掩码修正\n\n论文使用数据集和训练资源\n- 仿真数据集：RoboTwin多视角操作基准\n- 真实数据集：RDT真实机器人数据集及自建双臂机器人平台数据\n- 训练资源：单个消费级GPU（通过稀疏采样降低内存需求）\n\n论文使用的评估环境和评估指标\n- 仿真环境：RoboTwin基准测试\n- 真实环境：RDT数据集评估及物理双臂机器人部署\n- 评估指标：\n - VLA模型任务成功率\n - 未知环境泛化性能\n - 编辑结果的时空一致性指标\n - sim-to-real差距缩小程度",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>上海交通大学自动化与智能感知学院及系统控制与信息处理教育部重点实验室</li><li>剑桥大学工程系</li></ul>\n\n<p>论文概述</p>\n<p>论文提出ERMV（Editing 4D Robotic Multi-view images）框架，用于编辑4D机器人多视角序列图像，以缓解具身智能体（如VLA模型）训练中的数据稀缺问题。现有方法面临三大挑战：动态多视角下的时空一致性维护、计算成本限制导致的工作窗口狭小、以及关键物体（如机械臂）的语义完整性维护。ERMV通过视觉引导、状态注入、稀疏时空建模和反馈干预机制解决这些问题，显著提升下游策略的鲁棒性和泛化性。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出首个面向4D机器人多视角序列编辑的框架ERMV，有效增强VLA模型训练数据</li><li>设计极线运动感知注意力（EMA-Attn）处理运动模糊，结合稀疏时空模块（SST）实现大窗口低计算编辑</li><li>引入多模态大语言模型（MLLM）的反馈干预机制，最小化人工参与下保障核心物体一致性</li><li>在仿真、真实数据集和物理机器人平台验证效果，并证明其缩小sim-to-real差距的能力</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>基于潜在扩散模型（LDM），学习条件分布 p(X'\\|X,C_guide,C_state,C_history)</li><li>视觉引导：使用单帧编辑图像通过CLIP编码作为精确语义目标，避免文本歧义</li><li>状态注入：融合相机位姿P_t^(v)、机器人动作q_t及其运动差分ΔP_t^(v)、Δq_t，经MLP编码为时空条件</li><li>稀疏时空模块：随机采样K张图像（K<<L×N），解耦时空维度，将4D编辑重构为单帧多视角问题</li><li>极线运动感知注意力：在应用几何约束前预测运动引起的像素偏移，保持运动模糊下的跨视图一致性</li><li>反馈干预：MLLM检测编辑不一致性，仅在必要时请求专家提供分割掩码修正</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>仿真数据集：RoboTwin多视角操作基准</li><li>真实数据集：RDT真实机器人数据集及自建双臂机器人平台数据</li><li>训练资源：单个消费级GPU（通过稀疏采样降低内存需求）</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>仿真环境：RoboTwin基准测试</li><li>真实环境：RDT数据集评估及物理双臂机器人部署</li><li>评估指标：</li></ul>\n<p> - VLA模型任务成功率</p>\n<p> - 未知环境泛化性能</p>\n<p> - 编辑结果的时空一致性指标</p>\n<p> - sim-to-real差距缩小程度</p>"
  },
  {
    "date": "2025-07-23",
    "title": "Confidence Calibration in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2507.17383",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-23",
    "title": "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback",
    "link": "http://arxiv.org/abs/2507.17294",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-22",
    "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning",
    "link": "http://arxiv.org/abs/2507.16815",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-21",
    "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos",
    "link": "http://arxiv.org/abs/2507.15597",
    "summary_markdown": "```markdown\n### 论文研究单位\n北京大学、中国人民大学、BeingBeyond\n### 论文概述\n本文提出了Being-H0，一个基于大规模人类视频训练的灵巧视觉-语言-动作模型（VLA）。现有VLA在复杂操作任务中表现不佳，主要依赖于存在仿真到现实差距的合成数据或缺乏规模和多样性的远程操作演示。为解决这一数据瓶颈，论文提出利用人类手部作为“基础操作器”，利用网络数据中丰富的灵巧性和可扩展性。方法核心是“物理指令微调”，一种新的训练范式，结合了大规模VLA预训练、物理空间对齐和用于机器人任务的后训练自适应。此外，论文提出了一种部分级别的运动标记化方法，实现毫米级重建精度，以建模用于动作学习的精确手部轨迹。为了支持所提出的范式，论文进一步开发了一个全面的数据整理管道，将异构来源（包括动作捕捉、VR和仅RGB视频）集成到包含数百万个基于运动指令实例的大规模数据集中。\n### 论文核心贡献点\n1. **物理指令微调**：一种新的范式，将人类手部确立为机器人手部转移的基础操作器，弥合了人类视频与具身动作之间的差距。\n2. **部分级别运动标记化**：一种量化方法，在连续手部运动中保持毫米级精度，同时与自回归语言模型的离散架构兼容。\n3. **UniHand**：一个包含超过1.5亿个指令跟随样本的大规模数据集，跨越多样化操作场景，通过可扩展的数据管道统一动作捕捉、VR和仅RGB视频收集。\n4. **Being-H0**：通过整合上述创新，提出了第一个基于大规模人类视频中显式运动建模训练的灵巧VLA。该模型在视觉、语言和细粒度手部运动之间实现了强大的跨模态推理，并为下游机器人操作任务提供了定制的自适应策略。\n### 论文方法描述\n论文方法称为“物理指令微调”，包含三个关键组成部分：\n1. **预训练**：在人类视频上对基础VLA进行预训练，将人类手部视为理想操作器，机器人等价物为其灵巧性的简化版本。模型训练目标是根据视觉和语言预测手部运动。\n2. **物理空间对齐**：通过弱透视投影对齐和视图不变运动分布平衡来统一来自不同相机系统和记录条件的异构数据源，同时嵌入3D空间推理能力。\n3. **后训练自适应**：在预训练和物理空间对齐后，将基础VLA适应下游操作任务。本文采用简单的基于MLP的投影策略，并计划在未来探索更复杂的方法。\n\n模型采用统一的架构，通过共享注意力机制在视觉、语言和运动之间实现无缝跨模态推理。对于精确的运动标记化，引入了基于分组残差量化（GRQ）的有效部分级别运动标记化，实现毫米级重建精度。\n### 论文使用数据集和训练资源\n论文构建了名为UniHand的大规模数据集，包含超过1.5亿个样本，整合了动作捕捉、VR记录和仅RGB视频，跨越1000多个小时和150多个任务。数据整理管道包括手部姿势标准化、任务描述标注和指令数据生成。训练资源方面，论文使用InternVL3架构作为基础，包括预训练的InternViT-300M视觉编码器和2层MLP投影器。运动标记化使用1D-CNN编码器将连续MANO参数序列离散化为标记。\n### 论文使用的评估环境和评估指标\n评估在多个环境中进行：\n1. **手部运动生成评估**：包括手部运动生成和翻译任务，评估模型根据指令生成精确手部运动的能力。\n2. **长时程运动生成评估**：评估模型在长时间序列上生成连贯运动的能力。\n3. **灵巧操作评估**：在真实机器人操作任务上评估模型通过后训练自适应学习到的策略。\n评估指标包括：\n- 运动重建精度（毫米级误差）\n- 指令跟随准确性\n- 任务完成成功率\n- 跨模态推理能力\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>北京大学、中国人民大学、BeingBeyond</p>\n<h3>论文概述</h3>\n<p>本文提出了Being-H0，一个基于大规模人类视频训练的灵巧视觉-语言-动作模型（VLA）。现有VLA在复杂操作任务中表现不佳，主要依赖于存在仿真到现实差距的合成数据或缺乏规模和多样性的远程操作演示。为解决这一数据瓶颈，论文提出利用人类手部作为“基础操作器”，利用网络数据中丰富的灵巧性和可扩展性。方法核心是“物理指令微调”，一种新的训练范式，结合了大规模VLA预训练、物理空间对齐和用于机器人任务的后训练自适应。此外，论文提出了一种部分级别的运动标记化方法，实现毫米级重建精度，以建模用于动作学习的精确手部轨迹。为了支持所提出的范式，论文进一步开发了一个全面的数据整理管道，将异构来源（包括动作捕捉、VR和仅RGB视频）集成到包含数百万个基于运动指令实例的大规模数据集中。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>物理指令微调</strong>：一种新的范式，将人类手部确立为机器人手部转移的基础操作器，弥合了人类视频与具身动作之间的差距。</li><li><strong>部分级别运动标记化</strong>：一种量化方法，在连续手部运动中保持毫米级精度，同时与自回归语言模型的离散架构兼容。</li><li><strong>UniHand</strong>：一个包含超过1.5亿个指令跟随样本的大规模数据集，跨越多样化操作场景，通过可扩展的数据管道统一动作捕捉、VR和仅RGB视频收集。</li><li><strong>Being-H0</strong>：通过整合上述创新，提出了第一个基于大规模人类视频中显式运动建模训练的灵巧VLA。该模型在视觉、语言和细粒度手部运动之间实现了强大的跨模态推理，并为下游机器人操作任务提供了定制的自适应策略。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文方法称为“物理指令微调”，包含三个关键组成部分：</p>\n<ol><li><strong>预训练</strong>：在人类视频上对基础VLA进行预训练，将人类手部视为理想操作器，机器人等价物为其灵巧性的简化版本。模型训练目标是根据视觉和语言预测手部运动。</li><li><strong>物理空间对齐</strong>：通过弱透视投影对齐和视图不变运动分布平衡来统一来自不同相机系统和记录条件的异构数据源，同时嵌入3D空间推理能力。</li><li><strong>后训练自适应</strong>：在预训练和物理空间对齐后，将基础VLA适应下游操作任务。本文采用简单的基于MLP的投影策略，并计划在未来探索更复杂的方法。</li></ol>\n\n<p>模型采用统一的架构，通过共享注意力机制在视觉、语言和运动之间实现无缝跨模态推理。对于精确的运动标记化，引入了基于分组残差量化（GRQ）的有效部分级别运动标记化，实现毫米级重建精度。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>论文构建了名为UniHand的大规模数据集，包含超过1.5亿个样本，整合了动作捕捉、VR记录和仅RGB视频，跨越1000多个小时和150多个任务。数据整理管道包括手部姿势标准化、任务描述标注和指令数据生成。训练资源方面，论文使用InternVL3架构作为基础，包括预训练的InternViT-300M视觉编码器和2层MLP投影器。运动标记化使用1D-CNN编码器将连续MANO参数序列离散化为标记。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估在多个环境中进行：</p>\n<ol><li><strong>手部运动生成评估</strong>：包括手部运动生成和翻译任务，评估模型根据指令生成精确手部运动的能力。</li><li><strong>长时程运动生成评估</strong>：评估模型在长时间序列上生成连贯运动的能力。</li><li><strong>灵巧操作评估</strong>：在真实机器人操作任务上评估模型通过后训练自适应学习到的策略。</li></ol>\n<p>评估指标包括：</p>\n<ul><li>运动重建精度（毫米级误差）</li><li>指令跟随准确性</li><li>任务完成成功率</li><li>跨模态推理能力</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-07-21",
    "title": "GR-3 Technical Report",
    "link": "http://arxiv.org/abs/2507.15493",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-18",
    "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
    "link": "http://arxiv.org/abs/2507.14067",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-18",
    "title": "EdgeVLA: Efficient Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2507.14049",
    "summary_markdown": "### 论文研究单位\nK-Scale Labs, McGill University\n### 论文概述\n该论文介绍了EdgeVLA (EVLA)，一种新颖的视觉-语言-动作（VLA）模型，旨在显著提高VLA模型的推理速度和效率，使其能在资源受限的边缘设备上实现实时性能。传统的大规模VLA模型如OpenVLA虽然性能强大，但在移动操控系统等计算资源有限的平台上部署困难。EVLA通过两个核心创新解决了这个问题：1) 取消了末端执行器位置预测的自回归要求，从而将推理速度提升了7倍；2) 利用小型语言模型（SLMs）的效率，以显著降低的计算需求实现了与更大模型可比的训练性能。论文的早期结果表明，EVLA在保持与OpenVLA相当训练特性的同时，在推理速度和内存效率方面获得了巨大提升。\n### 论文核心贡献点\n1. 通过消除末端执行器位置预测的自回归特性，实现了7倍的推理速度提升。\n2. 利用小型语言模型（SLMs）的效率，在模型大小显著减小（1B参数）的情况下，保持了与更大规模模型（如7.5B参数的OpenVLA）可比的训练性能。\n### 论文方法描述\nEdgeVLA的训练方法分为两个阶段：\n1. **阶段1：VLM预训练**: 模型基于一个视觉-语言模型（VLM）进行预训练，使用了来自多样化字幕数据集的120万个图文对，遵循了PrismaticVLM家族的模型配方。语言模型部分采用0.5B参数的Qwen2。视觉编码器采用两部分结构，结合了预训练的SigLIP和DinoV2模型。模型学习一个投影层，将视觉特征映射到语言模型的词元空间。\n2. **阶段2：用于末端执行器预测的联合控制**: 在此阶段，使用来自OpenX数据集的大约100万个机械臂操作示例对模型进行微调。该阶段的关键创新是摒弃了传统的自回归方式来预测末端执行器位置，转而采用“联合控制”的方式，即一次性预测整个末端执行器的位置。这是通过移除语言模型中的因果掩码并训练模型一次性输出所有位置坐标来实现的。\n### 论文使用数据集和训练资源\n- **数据集**:\n - 预训练数据：包含120万个图文对的多样化字幕数据集。\n - 机器人操作训练数据：OpenX数据集（约100万个样本）。\n - 评估数据集：BridgeData V2 和 OpenX。\n- **训练资源**:\n - BridgeData V2训练：使用单个节点的8块A100-80GB GPU。\n - OpenX训练：使用80块A100-40GB GPU，训练时间约为5天。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - 推理性能评估在A100-40GB GPU上进行。\n - 论文的目标是使模型能够在Jetson Nano等边缘设备上部署。\n- **评估指标**:\n - **效率指标**: 推理时间（毫秒）、内存使用量（GB）。\n - **训练性能指标**: 训练损失、动作词元准确率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>K-Scale Labs, McGill University</p>\n<h3>论文概述</h3>\n<p>该论文介绍了EdgeVLA (EVLA)，一种新颖的视觉-语言-动作（VLA）模型，旨在显著提高VLA模型的推理速度和效率，使其能在资源受限的边缘设备上实现实时性能。传统的大规模VLA模型如OpenVLA虽然性能强大，但在移动操控系统等计算资源有限的平台上部署困难。EVLA通过两个核心创新解决了这个问题：1) 取消了末端执行器位置预测的自回归要求，从而将推理速度提升了7倍；2) 利用小型语言模型（SLMs）的效率，以显著降低的计算需求实现了与更大模型可比的训练性能。论文的早期结果表明，EVLA在保持与OpenVLA相当训练特性的同时，在推理速度和内存效率方面获得了巨大提升。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>通过消除末端执行器位置预测的自回归特性，实现了7倍的推理速度提升。</li><li>利用小型语言模型（SLMs）的效率，在模型大小显著减小（1B参数）的情况下，保持了与更大规模模型（如7.5B参数的OpenVLA）可比的训练性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>EdgeVLA的训练方法分为两个阶段：</p>\n<ol><li><strong>阶段1：VLM预训练</strong>: 模型基于一个视觉-语言模型（VLM）进行预训练，使用了来自多样化字幕数据集的120万个图文对，遵循了PrismaticVLM家族的模型配方。语言模型部分采用0.5B参数的Qwen2。视觉编码器采用两部分结构，结合了预训练的SigLIP和DinoV2模型。模型学习一个投影层，将视觉特征映射到语言模型的词元空间。</li><li><strong>阶段2：用于末端执行器预测的联合控制</strong>: 在此阶段，使用来自OpenX数据集的大约100万个机械臂操作示例对模型进行微调。该阶段的关键创新是摒弃了传统的自回归方式来预测末端执行器位置，转而采用“联合控制”的方式，即一次性预测整个末端执行器的位置。这是通过移除语言模型中的因果掩码并训练模型一次性输出所有位置坐标来实现的。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> - 预训练数据：包含120万个图文对的多样化字幕数据集。</p>\n<p> - 机器人操作训练数据：OpenX数据集（约100万个样本）。</p>\n<p> - 评估数据集：BridgeData V2 和 OpenX。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - BridgeData V2训练：使用单个节点的8块A100-80GB GPU。</p>\n<p> - OpenX训练：使用80块A100-40GB GPU，训练时间约为5天。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - 推理性能评估在A100-40GB GPU上进行。</p>\n<p> - 论文的目标是使模型能够在Jetson Nano等边缘设备上部署。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>效率指标</strong>: 推理时间（毫秒）、内存使用量（GB）。</p>\n<p> - <strong>训练性能指标</strong>: 训练损失、动作词元准确率。</p>"
  },
  {
    "date": "2025-07-17",
    "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
    "link": "http://arxiv.org/abs/2507.12768",
    "summary_markdown": "### 论文研究单位\n清华大学计算机科学与技术系、人工智能研究院、清华-博世联合机器学习中心\n### 论文概述\n本文提出一种任务无关动作（task-agnostic action）范式，将动作执行与任务条件解耦，以提升可扩展性和效率。为此，设计了ATARA数据收集框架和AnyPos模型：ATARA通过脚本策略自动生成任务无关数据，收集速度比人工操作快30倍；AnyPos作为逆动力学模型，通过手臂解耦估计和方向感知解码器从任务无关数据学习动作预测。实验表明，该方法在动作预测准确率上提升51%，真实世界任务成功率提升30-40%。\n### 论文核心贡献点\n- 任务无关动作范式：解耦动作与特定任务，降低数据采集成本。\n- ATARA框架：自动化、可扩展的任务无关数据收集，无人工干预。\n- AnyPos模型：引入手臂解耦估计和方向感知解码器（DAD），提升学习效率和精度。\n- 两阶段框架：结合视频生成模型（语义先验）和逆动力学模型（物理控制），实现跨任务泛化。\n### 论文方法描述\n1. **任务无关动作数据**：\n - 定义为不依赖任务指令或奖励的轨迹，直接建模 \\(p(\\text{action} \\mid \\text{image})\\)。\n - 理论推导：任务特定动作 \\(p(a \\mid x, l)\\) 可分解为任务特定图像生成 \\(p(x \\mid x_0, l)\\) 与任务无关动作 \\(p(a \\mid x)\\) 的乘积（公式4）。\n2. **ATARA数据收集**：\n - 使用脚本策略 uniformly 探索双臂机器人的立方工作空间，生成610k图像-动作对。\n3. **AnyPos模型**：\n - **手臂解耦估计**：通过空间分割（flood-fill 和对称线）隔离单臂区域，减少假设空间，提升精度约20%。\n - **方向感知解码器（DAD）**：对齐视觉特征（DINOv2）与物理运动方向（关节角度、连杆方向），增强鲁棒性，再提升20%性能。\n### 论文使用数据集和训练资源\n- **训练数据**：ATARA生成的610k图像-动作对（双臂14维动作空间）。\n- **测试数据**：真实世界收集的2.5k图像-动作对（包含未见技能和物体）。\n- **训练资源**：未明确细节，但涉及大规模数据集和预训练视觉骨干（如DINOv2），需高性能计算资源。\n### 论文使用的评估环境和评估指标\n1. **动作预测评估**：\n - 环境：真实世界测试集（未见任务）。\n - 指标：动作预测准确率（AnyPos: 57.13%，比基线高51%）。\n2. **真实世界重放评估**：\n - 环境：物理机器人执行提升、抓取、点击等任务。\n - 指标：任务成功率（AnyPos-ATARA: 92.59%，比人工数据高33%）。\n3. **视频生成模型部署**：\n - 环境：结合扩散视频模型进行动作验证。\n - 指标：下游任务成功率（30-40%提升）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学计算机科学与技术系、人工智能研究院、清华-博世联合机器学习中心</p>\n<h3>论文概述</h3>\n<p>本文提出一种任务无关动作（task-agnostic action）范式，将动作执行与任务条件解耦，以提升可扩展性和效率。为此，设计了ATARA数据收集框架和AnyPos模型：ATARA通过脚本策略自动生成任务无关数据，收集速度比人工操作快30倍；AnyPos作为逆动力学模型，通过手臂解耦估计和方向感知解码器从任务无关数据学习动作预测。实验表明，该方法在动作预测准确率上提升51%，真实世界任务成功率提升30-40%。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>任务无关动作范式：解耦动作与特定任务，降低数据采集成本。</li><li>ATARA框架：自动化、可扩展的任务无关数据收集，无人工干预。</li><li>AnyPos模型：引入手臂解耦估计和方向感知解码器（DAD），提升学习效率和精度。</li><li>两阶段框架：结合视频生成模型（语义先验）和逆动力学模型（物理控制），实现跨任务泛化。</li></ul>\n<h3>论文方法描述</h3>\n<ol><li><strong>任务无关动作数据</strong>：</li></ol>\n<p> - 定义为不依赖任务指令或奖励的轨迹，直接建模 \\(p(\\text{action} \\mid \\text{image})\\)。</p>\n<p> - 理论推导：任务特定动作 \\(p(a \\mid x, l)\\) 可分解为任务特定图像生成 \\(p(x \\mid x_0, l)\\) 与任务无关动作 \\(p(a \\mid x)\\) 的乘积（公式4）。</p>\n<ol><li><strong>ATARA数据收集</strong>：</li></ol>\n<p> - 使用脚本策略 uniformly 探索双臂机器人的立方工作空间，生成610k图像-动作对。</p>\n<ol><li><strong>AnyPos模型</strong>：</li></ol>\n<p> - <strong>手臂解耦估计</strong>：通过空间分割（flood-fill 和对称线）隔离单臂区域，减少假设空间，提升精度约20%。</p>\n<p> - <strong>方向感知解码器（DAD）</strong>：对齐视觉特征（DINOv2）与物理运动方向（关节角度、连杆方向），增强鲁棒性，再提升20%性能。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>训练数据</strong>：ATARA生成的610k图像-动作对（双臂14维动作空间）。</li><li><strong>测试数据</strong>：真实世界收集的2.5k图像-动作对（包含未见技能和物体）。</li><li><strong>训练资源</strong>：未明确细节，但涉及大规模数据集和预训练视觉骨干（如DINOv2），需高性能计算资源。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>动作预测评估</strong>：</li></ol>\n<p> - 环境：真实世界测试集（未见任务）。</p>\n<p> - 指标：动作预测准确率（AnyPos: 57.13%，比基线高51%）。</p>\n<ol><li><strong>真实世界重放评估</strong>：</li></ol>\n<p> - 环境：物理机器人执行提升、抓取、点击等任务。</p>\n<p> - 指标：任务成功率（AnyPos-ATARA: 92.59%，比人工数据高33%）。</p>\n<ol><li><strong>视频生成模型部署</strong>：</li></ol>\n<p> - 环境：结合扩散视频模型进行动作验证。</p>\n<p> - 指标：下游任务成功率（30-40%提升）。</p>"
  },
  {
    "date": "2025-07-16",
    "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos",
    "link": "http://arxiv.org/abs/2507.12440",
    "summary_markdown": "### 论文研究单位\nUC San Diego, UIUC, MIT, NVIDIA\n### 论文概述\n论文提出了EgoVLA，一种从第一人称人类视频中学习视觉-语言-动作（VLA）模型的方法。该方法旨在通过大规模人类视频数据预训练模型，然后使用少量机器人演示进行微调，以解决机器人数据收集的规模和多样性瓶颈。论文还引入了一个名为Ego Humanoid Manipulation Benchmark的仿真基准测试，用于评估人形机器人操作策略。\n### 论文核心贡献点\n1. 提出了一种利用第一人称人类视频预训练VLA模型（EgoVLA）的新范式，减少了大规模机器人数据的依赖。\n2. 设计了统一的人类与机器人动作空间，通过逆运动学和动作重定向将人类动作转换为机器人动作，实现了跨具身迁移。\n3. 构建了大规模的第一人称人类操作数据集，整合了HOI4D、HOT3D、HoloAssist和TACO等多个数据源。\n4. 提出了Ego Humanoid Manipulation Benchmark，包含12个双手机器人操作任务，用于在仿真中评估策略的泛化能力。\n### 论文方法描述\n1. **数据集构建**：整合了多个第一人称人类视频数据集，包含RGB观测、手腕姿态、手部姿态和相机姿态，数据总量约50万图像-动作对。\n2. **模型架构**：基于NVILA-2B视觉语言模型构建EgoVLA，输入包括历史视觉观测、语言指令、动作查询令牌和人类本体感受状态，输出未来人类或机器人动作序列。\n3. **动作表示**：动作空间包括手腕姿态（相机坐标系下的3D平移和旋转）和手部关节角度（使用MANO手部模型的前15个PCA分量）。\n4. **训练过程**：先在人类数据集上预训练20个周期，然后在机器人演示数据上微调115个周期。\n5. **重定向方法**：通过优化MANO参数使预测的指尖位置与机器人手部指尖位置对齐，再通过轻量级MLP将预测的指尖位置映射为机器人关节命令。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - 人类数据：HOI4D（4000个视频）、HOT3D（833分钟视频）、HoloAssist（166小时视频，采样1/10）、TACO（2317个运动序列）。\n - 机器人数据：在Ego Humanoid Manipulation Benchmark中通过遥操作收集的100个任务演示。\n2. **训练资源**：未明确提及，但模型基于NVILA-2B（2B参数），使用AdamW优化器，预训练20个周期，微调115个周期。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 仿真环境：NVIDIA Isaac Lab中的Ego Humanoid Manipulation Benchmark，使用Unitree H1人形机器人和Inspire灵巧手。\n - 视觉配置：包括5种房间纹理和5种桌子纹理，共25种视觉背景组合，分为已见和未见两种设置。\n2. **评估指标**：\n - 成功率（SR）：任务整体成功的比例。\n - 进度率（PSR）：长时程任务中完成的子任务平均比例。\n - 人类动作预测误差：手腕平移误差（约8厘米）和2D图像平面归一化误差（约0.13）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>UC San Diego, UIUC, MIT, NVIDIA</p>\n<h3>论文概述</h3>\n<p>论文提出了EgoVLA，一种从第一人称人类视频中学习视觉-语言-动作（VLA）模型的方法。该方法旨在通过大规模人类视频数据预训练模型，然后使用少量机器人演示进行微调，以解决机器人数据收集的规模和多样性瓶颈。论文还引入了一个名为Ego Humanoid Manipulation Benchmark的仿真基准测试，用于评估人形机器人操作策略。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种利用第一人称人类视频预训练VLA模型（EgoVLA）的新范式，减少了大规模机器人数据的依赖。</li><li>设计了统一的人类与机器人动作空间，通过逆运动学和动作重定向将人类动作转换为机器人动作，实现了跨具身迁移。</li><li>构建了大规模的第一人称人类操作数据集，整合了HOI4D、HOT3D、HoloAssist和TACO等多个数据源。</li><li>提出了Ego Humanoid Manipulation Benchmark，包含12个双手机器人操作任务，用于在仿真中评估策略的泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据集构建</strong>：整合了多个第一人称人类视频数据集，包含RGB观测、手腕姿态、手部姿态和相机姿态，数据总量约50万图像-动作对。</li><li><strong>模型架构</strong>：基于NVILA-2B视觉语言模型构建EgoVLA，输入包括历史视觉观测、语言指令、动作查询令牌和人类本体感受状态，输出未来人类或机器人动作序列。</li><li><strong>动作表示</strong>：动作空间包括手腕姿态（相机坐标系下的3D平移和旋转）和手部关节角度（使用MANO手部模型的前15个PCA分量）。</li><li><strong>训练过程</strong>：先在人类数据集上预训练20个周期，然后在机器人演示数据上微调115个周期。</li><li><strong>重定向方法</strong>：通过优化MANO参数使预测的指尖位置与机器人手部指尖位置对齐，再通过轻量级MLP将预测的指尖位置映射为机器人关节命令。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - 人类数据：HOI4D（4000个视频）、HOT3D（833分钟视频）、HoloAssist（166小时视频，采样1/10）、TACO（2317个运动序列）。</p>\n<p> - 机器人数据：在Ego Humanoid Manipulation Benchmark中通过遥操作收集的100个任务演示。</p>\n<ol><li><strong>训练资源</strong>：未明确提及，但模型基于NVILA-2B（2B参数），使用AdamW优化器，预训练20个周期，微调115个周期。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - 仿真环境：NVIDIA Isaac Lab中的Ego Humanoid Manipulation Benchmark，使用Unitree H1人形机器人和Inspire灵巧手。</p>\n<p> - 视觉配置：包括5种房间纹理和5种桌子纹理，共25种视觉背景组合，分为已见和未见两种设置。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - 成功率（SR）：任务整体成功的比例。</p>\n<p> - 进度率（PSR）：长时程任务中完成的子任务平均比例。</p>\n<p> - 人类动作预测误差：手腕平移误差（约8厘米）和2D图像平面归一化误差（约0.13）。</p>"
  },
  {
    "date": "2025-07-14",
    "title": "Vision Language Action Models in Robotic Manipulation: A Systematic Review",
    "link": "http://arxiv.org/abs/2507.10672",
    "summary_markdown": "### 论文研究单位\n- Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, United Arab Emirates.\n- Institute of Industrial and Control Engineering (IOC), Universitat Politecnica de Catalunya, Spain.\n### 论文概述\n本文是一篇关于机器人操作领域中视觉语言动作模型的系统性综述。它旨在全面整合视觉感知、自然语言理解和具身控制于一个统一的学习框架中。该综述分析了102个VLA模型、26个基础数据集和12个仿真平台，并提出了一个新颖的VLA数据集基准测试框架，该框架基于任务复杂性和模态丰富度。文章还探讨了VLA模型的应用、评估方法、当前面临的挑战以及未来的研究方向，为推进具身智能和通用机器人控制提供了技术参考和概念路线图。\n### 论文核心贡献点\n- 提出了一个结构化的VLA模型架构分类法，根据其整合感知、语言理解和控制的不同方法进行归类。\n- 引入了一个新颖的定量VLA数据集基准框架，使用任务复杂度和模态丰富度等指标，通过二维图表可视化当前数据格局，识别出研究空白。\n- 对关键的仿真平台进行了深入回顾，评估了它们在生成大规模数据、促进从仿真到现实的迁移以及支持任务多样性方面的作用。\n- 识别了VLA模型开发中存在的持续性挑战，并为未来研究提供了清晰的路线图，强调了模块化架构设计、可扩展数据生成策略和统一的语言基础API等方向。\n### 论文方法描述\n本文采用系统性文献综述的方法。首先，在IEEE Xplore、Elsevier、Springer Nature、MDPI、Wiley和arXiv等多个学术数据库中，针对VLA模型、数据集和仿真工具使用特定关键词组合进行全面检索。其次，通过对话式查询大型语言模型（如GPT）来补充文献库。然后，根据明确的纳入标准（如提出或评估了新颖的VLA模型、数据集或仿真器）对检索到的文献进行筛选和验证。最终，对入选的文献进行深入分析、分类和总结，以构建一个全面的VLA领域知识图谱。\n### 论文使用数据集和训练资源\n- 论文分析了多个关键数据集，包括：\n - Open X-Embodiment: 统一了来自22种机器人形态和超过500个任务的数据。\n - DROID: 结合了人类标注语言与机器人视频演示的大规模数据集。\n - 其他如RT-1-Kitchen, VIMA, RLBench等。\n- 论文回顾了多个仿真工具，包括：\n - Habitat, Isaac Gym, RoboSuite, iGibson, AI2-THOR。\n- 作者创建了一个公共代码库，用于总结VLA模型、数据集和仿真器，链接为: https://github.com/Muhayyuddin/VLAs。\n### 论文使用的评估环境和评估指标\n- 本文作为综述论文，未在单一物理或仿真环境中进行具体实验。其核心是提出和分析了现有的评估协议。\n- **数据集评估指标**: 提出了一个基于任务复杂性和模态丰富度的二维定量评估框架，用于系统地比较和基准化VLA训练数据集，以揭示当前数据集中任务多样性和多模态对齐的不足。\n- **模型评估分析**: 讨论了评估VLA模型的标准和方法，包括在真实世界的成功率、零样本泛化能力、以及在不同应用领域（如家居、工业、导航）的性能表现，并指出了当前评估协议中的局限性。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, United Arab Emirates.</li><li>Institute of Industrial and Control Engineering (IOC), Universitat Politecnica de Catalunya, Spain.</li></ul>\n<h3>论文概述</h3>\n<p>本文是一篇关于机器人操作领域中视觉语言动作模型的系统性综述。它旨在全面整合视觉感知、自然语言理解和具身控制于一个统一的学习框架中。该综述分析了102个VLA模型、26个基础数据集和12个仿真平台，并提出了一个新颖的VLA数据集基准测试框架，该框架基于任务复杂性和模态丰富度。文章还探讨了VLA模型的应用、评估方法、当前面临的挑战以及未来的研究方向，为推进具身智能和通用机器人控制提供了技术参考和概念路线图。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个结构化的VLA模型架构分类法，根据其整合感知、语言理解和控制的不同方法进行归类。</li><li>引入了一个新颖的定量VLA数据集基准框架，使用任务复杂度和模态丰富度等指标，通过二维图表可视化当前数据格局，识别出研究空白。</li><li>对关键的仿真平台进行了深入回顾，评估了它们在生成大规模数据、促进从仿真到现实的迁移以及支持任务多样性方面的作用。</li><li>识别了VLA模型开发中存在的持续性挑战，并为未来研究提供了清晰的路线图，强调了模块化架构设计、可扩展数据生成策略和统一的语言基础API等方向。</li></ul>\n<h3>论文方法描述</h3>\n<p>本文采用系统性文献综述的方法。首先，在IEEE Xplore、Elsevier、Springer Nature、MDPI、Wiley和arXiv等多个学术数据库中，针对VLA模型、数据集和仿真工具使用特定关键词组合进行全面检索。其次，通过对话式查询大型语言模型（如GPT）来补充文献库。然后，根据明确的纳入标准（如提出或评估了新颖的VLA模型、数据集或仿真器）对检索到的文献进行筛选和验证。最终，对入选的文献进行深入分析、分类和总结，以构建一个全面的VLA领域知识图谱。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>论文分析了多个关键数据集，包括：</li></ul>\n<p> - Open X-Embodiment: 统一了来自22种机器人形态和超过500个任务的数据。</p>\n<p> - DROID: 结合了人类标注语言与机器人视频演示的大规模数据集。</p>\n<p> - 其他如RT-1-Kitchen, VIMA, RLBench等。</p>\n<ul><li>论文回顾了多个仿真工具，包括：</li></ul>\n<p> - Habitat, Isaac Gym, RoboSuite, iGibson, AI2-THOR。</p>\n<ul><li>作者创建了一个公共代码库，用于总结VLA模型、数据集和仿真器，链接为: https://github.com/Muhayyuddin/VLAs。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>本文作为综述论文，未在单一物理或仿真环境中进行具体实验。其核心是提出和分析了现有的评估协议。</li><li><strong>数据集评估指标</strong>: 提出了一个基于任务复杂性和模态丰富度的二维定量评估框架，用于系统地比较和基准化VLA训练数据集，以揭示当前数据集中任务多样性和多模态对齐的不足。</li><li><strong>模型评估分析</strong>: 讨论了评估VLA模型的标准和方法，包括在真实世界的成功率、零样本泛化能力、以及在不同应用领域（如家居、工业、导航）的性能表现，并指出了当前评估协议中的局限性。</li></ul>"
  },
  {
    "date": "2025-07-12",
    "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization",
    "link": "http://arxiv.org/abs/2507.09160",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-07",
    "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting",
    "link": "http://arxiv.org/abs/2507.05116",
    "summary_markdown": "### 论文研究单位\n东北大学和EmbodyX公司。\n### 论文概述\n论文提出了VOTE（Vision-Language-Action Optimization with Trajectory Ensemble Voting）框架，旨在优化视觉-语言-动作（VLA）模型的训练和推理过程。该框架通过引入特殊令牌和集成投票策略，解决了现有VLA模型计算开销大和动作利用不足的问题，实现了更快的推理速度和更高的任务成功率，同时降低了训练成本。\n### 论文核心贡献点\n1. 提出轻量级VLA训练框架，通过单个特殊令牌替代多个动作令牌，显著减少生成令牌数量，降低训练和推理成本。\n2. 设计集成投票策略，结合当前和历史动作预测，通过投票机制选择最终动作，提高动作利用率和任务成功率。\n3. 实验表明，该方法在多个基准上达到最优性能，推理速度比OpenVLA快39倍，边缘设备吞吐量达46 Hz，具有实际部署潜力。\n### 论文方法描述\n训练框架：在指令序列末尾插入特殊令牌`<ACT>`，VLA模型仅生成该令牌而非多个动作令牌。其隐藏状态通过瓶颈式MLP动作头直接预测连续动作块，避免顺序解码。训练损失结合令牌级交叉熵和动作级L1损失，权重分别为0.01和0.99。\n集成投票：推理时，收集当前及过去K步的动作预测。计算各历史动作与当前动作的余弦相似度，按阈值0.5分为相似集`M`和不相似集`N`。选择票数更多的集合，取平均作为最终动作。若当前预测获票少则被忽略，优先历史一致预测。\n### 论文使用数据集和训练资源\n数据集：SimplerEnv（使用BridgeData V2和Fractal数据微调）和LIBERO基准。\n训练资源：SimplerEnv训练在4块H100 GPU上，全局批量大小80，微调60K步；LIBERO训练在2块H100 GPU上，批量大小40。优化器为AdamW，学习率1e-4，LoRA秩32，α=16。\n### 论文使用的评估环境和评估指标\n评估环境：NVIDIA RTX A6000和H100 GPU进行模拟评估；边缘设备NVIDIA Jetson Orin测试部署性能。\n评估指标：任务成功率（SR）、推理延迟、吞吐量及GPU内存占用。SimplerEnv中每个任务重复24次试验，LIBERO报告各任务套件平均SR。</think>\n### 论文研究单位\n东北大学和EmbodyX公司。\n### 论文概述\n论文提出了VOTE（Vision-Language-Action Optimization with Trajectory Ensemble Voting）框架，旨在优化视觉-语言-动作（VLA）模型的训练和推理过程。该框架通过引入特殊令牌和集成投票策略，解决了现有VLA模型计算开销大和动作利用不足的问题，实现了更快的推理速度和更高的任务成功率，同时降低了训练成本。\n### 论文核心贡献点\n1. 提出轻量级VLA训练框架，通过单个特殊令牌替代多个动作令牌，显著减少生成令牌数量，降低训练和推理成本。\n2. 设计集成投票策略，结合当前和历史动作预测，通过投票机制选择最终动作，提高动作利用率和任务成功率。\n3. 实验表明，该方法在多个基准上达到最优性能，推理速度比OpenVLA快39倍，边缘设备吞吐量达46 Hz，具有实际部署潜力。\n### 论文方法描述\n训练框架：在指令序列末尾插入特殊令牌`<ACT>`，VLA模型仅生成该令牌而非多个动作令牌。其隐藏状态通过瓶颈式MLP动作头直接预测连续动作块，避免顺序解码。训练损失结合令牌级交叉熵和动作级L1损失，权重分别为0.01和0.99。\n集成投票：推理时，收集当前及过去K步的动作预测。计算各历史动作与当前动作的余弦相似度，按阈值0.5分为相似集`M`和不相似集`N`。选择票数更多的集合，取平均作为最终动作。若当前预测获票少则被忽略，优先历史一致预测。\n### 论文使用数据集和训练资源\n数据集：SimplerEnv（使用BridgeData V2和Fractal数据微调）和LIBERO基准。\n训练资源：SimplerEnv训练在4块H100 GPU上，全局批量大小80，微调60K步；LIBERO训练在2块H100 GPU上，批量大小40。优化器为AdamW，学习率1e-4，LoRA秩32，α=16。\n### 论文使用的评估环境和评估指标\n评估环境：NVIDIA RTX A6000和H100 GPU进行模拟评估；边缘设备NVIDIA Jetson Orin测试部署性能。\n评估指标：任务成功率（SR）、推理延迟、吞吐量及GPU内存占用。SimplerEnv中每个任务重复24次试验，LIBERO报告各任务套件平均SR。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>东北大学和EmbodyX公司。</p>\n<h3>论文概述</h3>\n<p>论文提出了VOTE（Vision-Language-Action Optimization with Trajectory Ensemble Voting）框架，旨在优化视觉-语言-动作（VLA）模型的训练和推理过程。该框架通过引入特殊令牌和集成投票策略，解决了现有VLA模型计算开销大和动作利用不足的问题，实现了更快的推理速度和更高的任务成功率，同时降低了训练成本。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出轻量级VLA训练框架，通过单个特殊令牌替代多个动作令牌，显著减少生成令牌数量，降低训练和推理成本。</li><li>设计集成投票策略，结合当前和历史动作预测，通过投票机制选择最终动作，提高动作利用率和任务成功率。</li><li>实验表明，该方法在多个基准上达到最优性能，推理速度比OpenVLA快39倍，边缘设备吞吐量达46 Hz，具有实际部署潜力。</li></ol>\n<h3>论文方法描述</h3>\n<p>训练框架：在指令序列末尾插入特殊令牌<code><ACT></code>，VLA模型仅生成该令牌而非多个动作令牌。其隐藏状态通过瓶颈式MLP动作头直接预测连续动作块，避免顺序解码。训练损失结合令牌级交叉熵和动作级L1损失，权重分别为0.01和0.99。</p>\n<p>集成投票：推理时，收集当前及过去K步的动作预测。计算各历史动作与当前动作的余弦相似度，按阈值0.5分为相似集<code>M</code>和不相似集<code>N</code>。选择票数更多的集合，取平均作为最终动作。若当前预测获票少则被忽略，优先历史一致预测。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：SimplerEnv（使用BridgeData V2和Fractal数据微调）和LIBERO基准。</p>\n<p>训练资源：SimplerEnv训练在4块H100 GPU上，全局批量大小80，微调60K步；LIBERO训练在2块H100 GPU上，批量大小40。优化器为AdamW，学习率1e-4，LoRA秩32，α=16。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：NVIDIA RTX A6000和H100 GPU进行模拟评估；边缘设备NVIDIA Jetson Orin测试部署性能。</p>\n<p>评估指标：任务成功率（SR）、推理延迟、吞吐量及GPU内存占用。SimplerEnv中每个任务重复24次试验，LIBERO报告各任务套件平均SR。</think></p>\n<h3>论文研究单位</h3>\n<p>东北大学和EmbodyX公司。</p>\n<h3>论文概述</h3>\n<p>论文提出了VOTE（Vision-Language-Action Optimization with Trajectory Ensemble Voting）框架，旨在优化视觉-语言-动作（VLA）模型的训练和推理过程。该框架通过引入特殊令牌和集成投票策略，解决了现有VLA模型计算开销大和动作利用不足的问题，实现了更快的推理速度和更高的任务成功率，同时降低了训练成本。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出轻量级VLA训练框架，通过单个特殊令牌替代多个动作令牌，显著减少生成令牌数量，降低训练和推理成本。</li><li>设计集成投票策略，结合当前和历史动作预测，通过投票机制选择最终动作，提高动作利用率和任务成功率。</li><li>实验表明，该方法在多个基准上达到最优性能，推理速度比OpenVLA快39倍，边缘设备吞吐量达46 Hz，具有实际部署潜力。</li></ol>\n<h3>论文方法描述</h3>\n<p>训练框架：在指令序列末尾插入特殊令牌<code><ACT></code>，VLA模型仅生成该令牌而非多个动作令牌。其隐藏状态通过瓶颈式MLP动作头直接预测连续动作块，避免顺序解码。训练损失结合令牌级交叉熵和动作级L1损失，权重分别为0.01和0.99。</p>\n<p>集成投票：推理时，收集当前及过去K步的动作预测。计算各历史动作与当前动作的余弦相似度，按阈值0.5分为相似集<code>M</code>和不相似集<code>N</code>。选择票数更多的集合，取平均作为最终动作。若当前预测获票少则被忽略，优先历史一致预测。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：SimplerEnv（使用BridgeData V2和Fractal数据微调）和LIBERO基准。</p>\n<p>训练资源：SimplerEnv训练在4块H100 GPU上，全局批量大小80，微调60K步；LIBERO训练在2块H100 GPU上，批量大小40。优化器为AdamW，学习率1e-4，LoRA秩32，α=16。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：NVIDIA RTX A6000和H100 GPU进行模拟评估；边缘设备NVIDIA Jetson Orin测试部署性能。</p>\n<p>评估指标：任务成功率（SR）、推理延迟、吞吐量及GPU内存占用。SimplerEnv中每个任务重复24次试验，LIBERO报告各任务套件平均SR。</p>"
  },
  {
    "date": "2025-07-06",
    "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge",
    "link": "http://arxiv.org/abs/2507.04447",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-03",
    "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale",
    "link": "http://arxiv.org/abs/2507.02747",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-02",
    "title": "cVLA: Towards Efficient Camera-Space VLAs",
    "link": "http://arxiv.org/abs/2507.02190",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-02",
    "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective",
    "link": "http://arxiv.org/abs/2507.01925",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-02",
    "title": "TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control",
    "link": "http://arxiv.org/abs/2507.01424",
    "summary_markdown": "### 1. 论文研究单位\nFudan University and Shanghai Innovation Institute\n### 2. 论文概述\nTriVLA 是一个受认知神经科学中的情景记忆理论启发的视觉-语言-动作（VLA）统一框架，通过三系统架构实现了情景世界模型。该模型使机器人能够积累、回忆和预测顺序的多模态体验，从而在动态环境中实现鲁棒的长期任务执行。当前基于VLM的VLA系统主要依赖静态表示和有限的时间上下文，而TriVLA通过集成预训练的视觉语言模型（VLM）和视频扩散模型（VDM），提供了高层次的推理和动态预测能力，实验证明其在模拟和真实世界中均优于现有基线。\n### 3. 论文核心贡献点\n1. **受认知神经科学启发的情景世界模型**：提出了用于具身智能体的情景世界模型，使机器人能够积累、回忆和预测顺序的多模态体验。\n2. **统一的三系统组合架构**：提出了TriVLA，一个实现情景世界模型的三系统架构，包括情景多模态感知（System 2）、情景动态感知（System 3）和策略学习模块（System 1）。\n3. **最先进的性能**：在Calvin ABC→D、LIBERO和MetaWorld基准测试中，TriVLA相比之前的最先进方法分别提升了0.21、0.11和0.13，并在真实世界的长视野任务中表现出色。\n### 4. 论文方法描述\nTriVLA采用三系统架构：\n- **System 2（情景多模态感知）**：使用预训练的Eagle-2 VLM处理视觉和语言输入，提取视觉语言嵌入。\n- **System 3（情景动态感知）**：使用在人类和机器人操控数据集上微调的1.5B参数Stable Video Diffusion (SVD)模型，编码过去状态序列并预测未来场景轨迹。为避免计算密集的完整视频去噪，采用单次前向传播提取特征，并自动聚合多层上采样特征。\n- **System 1（策略学习）**：集成Systems 2和3的输出，使用扩散策略和扩散变换器（DiT）生成动作。通过流匹配和跨模态注意力机制，结合视觉语言标记、动态预测标记、机器人状态和动作历史，预测动作序列。训练时采用扩散损失函数。\n### 5. 论文使用数据集和训练资源\n- **数据集**：\n - 模拟基准：Calvin（ABC→D设置）、LIBERO（Spatial、Object、Goal、Long四个子集）、MetaWorld（50个Sawyer机器人任务）。\n - 真实世界：使用KINOVA GEN2机器人和RealSense D455相机，在室内环境中操作各种物体，并设计了长视野高动态任务。\n- **训练资源**：\n - System 2：使用预训练的Eagle-2 VLM。\n - System 3：微调SVD模型，使用193,690个人类轨迹和179,074个机器人轨迹，补充Calvin ABC、MetaWorld和真实任务视频。在8块NVIDIA H100 GPU上训练2-3天。\n - System 1：在任务数据集上训练，使用4块H100 GPU，耗时5-9小时。\n### 6. 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：Calvin、LIBERO、MetaWorld基准测试环境。\n - 真实环境：KINOVA GEN2机器人配备RealSense D455深度相机，采用眼在手配置，在室内场景进行操作。\n- **评估指标**：\n - Calvin：任务序列的平均成功长度（Avg. Len）。\n - LIBERO和MetaWorld：平均任务成功率（%）。\n - 真实世界：任务执行的定性分析，包括长视野任务的成功率。\n - 额外指标：推理延迟（ms）和模型参数量（B）。</think>\n### 论文研究单位\nFudan University and Shanghai Innovation Institute\n### 论文概述\nTriVLA 是一个受认知神经科学中的情景记忆理论启发的视觉-语言-动作（VLA）统一框架，通过三系统架构实现了情景世界模型。该模型使机器人能够积累、回忆和预测顺序的多模态体验，从而在动态环境中实现鲁棒的长期任务执行。当前基于VLM的VLA系统主要依赖静态表示和有限的时间上下文，而TriVLA通过集成预训练的视觉语言模型（VLM）和视频扩散模型（VDM），提供了高层次的推理和动态预测能力，实验证明其在模拟和真实世界中均优于现有基线。\n### 论文核心贡献点\n1. 受认知神经科学启发的情景世界模型：提出了用于具身智能体的情景世界模型，使机器人能够积累、回忆和预测顺序的多模态体验。\n2. 统一的三系统组合架构：提出了TriVLA，一个实现情景世界模型的三系统架构，包括情景多模态感知（System 2）、情景动态感知（System 3）和策略学习模块（System 1）。\n3. 最先进的性能：在Calvin ABC→D、LIBERO和MetaWorld基准测试中，TriVLA相比之前的最先进方法分别提升了0.21、0.11和0.13，并在真实世界的长视野任务中表现出色。\n### 论文方法描述\nTriVLA采用三系统架构：\n- System 2（情景多模态感知）：使用预训练的Eagle-2 VLM处理视觉和语言输入，提取视觉语言嵌入。\n- System 3（情景动态感知）：使用在人类和机器人操控数据集上微调的1.5B参数Stable Video Diffusion (SVD)模型，编码过去状态序列并预测未来场景轨迹。为避免计算密集的完整视频去噪，采用单次前向传播提取特征，并自动聚合多层上采样特征。\n- System 1（策略学习）：集成Systems 2和3的输出，使用扩散策略和扩散变换器（DiT）生成动作。通过流匹配和跨模态注意力机制，结合视觉语言标记、动态预测标记、机器人状态和动作历史，预测动作序列。训练时采用扩散损失函数。\n### 论文使用数据集和训练资源\n- 数据集：\n - 模拟基准：Calvin（ABC→D设置）、LIBERO（Spatial、Object、Goal、Long四个子集）、MetaWorld（50个Sawyer机器人任务）。\n - 真实世界：使用KINOVA GEN2机器人和RealSense D455相机，在室内环境中操作各种物体，并设计了长视野高动态任务。\n- 训练资源：\n - System 2：使用预训练的Eagle-2 VLM。\n - System 3：微调SVD模型，使用193,690个人类轨迹和179,074个机器人轨迹，补充Calvin ABC、MetaWorld和真实任务视频。在8块NVIDIA H100 GPU上训练2-3天。\n - System 1：在任务数据集上训练，使用4块H100 GPU，耗时5-9小时。\n### 论文使用的评估环境和评估指标\n- 评估环境：\n - 模拟环境：Calvin、LIBERO、MetaWorld基准测试环境。\n - 真实环境：KINOVA GEN2机器人配备RealSense D455深度相机，采用眼在手配置，在室内场景进行操作。\n- 评估指标：\n - Calvin：任务序列的平均成功长度（Avg. Len）。\n - LIBERO和MetaWorld：平均任务成功率（%）。\n - 真实世界：任务执行的定性分析，包括长视野任务的成功率。\n - 额外指标：推理延迟（ms）和模型参数量（B）。",
    "summary_html": "<h3>1. 论文研究单位</h3>\n<p>Fudan University and Shanghai Innovation Institute</p>\n<h3>2. 论文概述</h3>\n<p>TriVLA 是一个受认知神经科学中的情景记忆理论启发的视觉-语言-动作（VLA）统一框架，通过三系统架构实现了情景世界模型。该模型使机器人能够积累、回忆和预测顺序的多模态体验，从而在动态环境中实现鲁棒的长期任务执行。当前基于VLM的VLA系统主要依赖静态表示和有限的时间上下文，而TriVLA通过集成预训练的视觉语言模型（VLM）和视频扩散模型（VDM），提供了高层次的推理和动态预测能力，实验证明其在模拟和真实世界中均优于现有基线。</p>\n<h3>3. 论文核心贡献点</h3>\n<ol><li><strong>受认知神经科学启发的情景世界模型</strong>：提出了用于具身智能体的情景世界模型，使机器人能够积累、回忆和预测顺序的多模态体验。</li><li><strong>统一的三系统组合架构</strong>：提出了TriVLA，一个实现情景世界模型的三系统架构，包括情景多模态感知（System 2）、情景动态感知（System 3）和策略学习模块（System 1）。</li><li><strong>最先进的性能</strong>：在Calvin ABC→D、LIBERO和MetaWorld基准测试中，TriVLA相比之前的最先进方法分别提升了0.21、0.11和0.13，并在真实世界的长视野任务中表现出色。</li></ol>\n<h3>4. 论文方法描述</h3>\n<p>TriVLA采用三系统架构：</p>\n<ul><li><strong>System 2（情景多模态感知）</strong>：使用预训练的Eagle-2 VLM处理视觉和语言输入，提取视觉语言嵌入。</li><li><strong>System 3（情景动态感知）</strong>：使用在人类和机器人操控数据集上微调的1.5B参数Stable Video Diffusion (SVD)模型，编码过去状态序列并预测未来场景轨迹。为避免计算密集的完整视频去噪，采用单次前向传播提取特征，并自动聚合多层上采样特征。</li><li><strong>System 1（策略学习）</strong>：集成Systems 2和3的输出，使用扩散策略和扩散变换器（DiT）生成动作。通过流匹配和跨模态注意力机制，结合视觉语言标记、动态预测标记、机器人状态和动作历史，预测动作序列。训练时采用扩散损失函数。</li></ul>\n<h3>5. 论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟基准：Calvin（ABC→D设置）、LIBERO（Spatial、Object、Goal、Long四个子集）、MetaWorld（50个Sawyer机器人任务）。</p>\n<p> - 真实世界：使用KINOVA GEN2机器人和RealSense D455相机，在室内环境中操作各种物体，并设计了长视野高动态任务。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - System 2：使用预训练的Eagle-2 VLM。</p>\n<p> - System 3：微调SVD模型，使用193,690个人类轨迹和179,074个机器人轨迹，补充Calvin ABC、MetaWorld和真实任务视频。在8块NVIDIA H100 GPU上训练2-3天。</p>\n<p> - System 1：在任务数据集上训练，使用4块H100 GPU，耗时5-9小时。</p>\n<h3>6. 论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：Calvin、LIBERO、MetaWorld基准测试环境。</p>\n<p> - 真实环境：KINOVA GEN2机器人配备RealSense D455深度相机，采用眼在手配置，在室内场景进行操作。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - Calvin：任务序列的平均成功长度（Avg. Len）。</p>\n<p> - LIBERO和MetaWorld：平均任务成功率（%）。</p>\n<p> - 真实世界：任务执行的定性分析，包括长视野任务的成功率。</p>\n<p> - 额外指标：推理延迟（ms）和模型参数量（B）。</think></p>\n<h3>论文研究单位</h3>\n<p>Fudan University and Shanghai Innovation Institute</p>\n<h3>论文概述</h3>\n<p>TriVLA 是一个受认知神经科学中的情景记忆理论启发的视觉-语言-动作（VLA）统一框架，通过三系统架构实现了情景世界模型。该模型使机器人能够积累、回忆和预测顺序的多模态体验，从而在动态环境中实现鲁棒的长期任务执行。当前基于VLM的VLA系统主要依赖静态表示和有限的时间上下文，而TriVLA通过集成预训练的视觉语言模型（VLM）和视频扩散模型（VDM），提供了高层次的推理和动态预测能力，实验证明其在模拟和真实世界中均优于现有基线。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>受认知神经科学启发的情景世界模型：提出了用于具身智能体的情景世界模型，使机器人能够积累、回忆和预测顺序的多模态体验。</li><li>统一的三系统组合架构：提出了TriVLA，一个实现情景世界模型的三系统架构，包括情景多模态感知（System 2）、情景动态感知（System 3）和策略学习模块（System 1）。</li><li>最先进的性能：在Calvin ABC→D、LIBERO和MetaWorld基准测试中，TriVLA相比之前的最先进方法分别提升了0.21、0.11和0.13，并在真实世界的长视野任务中表现出色。</li></ol>\n<h3>论文方法描述</h3>\n<p>TriVLA采用三系统架构：</p>\n<ul><li>System 2（情景多模态感知）：使用预训练的Eagle-2 VLM处理视觉和语言输入，提取视觉语言嵌入。</li><li>System 3（情景动态感知）：使用在人类和机器人操控数据集上微调的1.5B参数Stable Video Diffusion (SVD)模型，编码过去状态序列并预测未来场景轨迹。为避免计算密集的完整视频去噪，采用单次前向传播提取特征，并自动聚合多层上采样特征。</li><li>System 1（策略学习）：集成Systems 2和3的输出，使用扩散策略和扩散变换器（DiT）生成动作。通过流匹配和跨模态注意力机制，结合视觉语言标记、动态预测标记、机器人状态和动作历史，预测动作序列。训练时采用扩散损失函数。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：</li></ul>\n<p> - 模拟基准：Calvin（ABC→D设置）、LIBERO（Spatial、Object、Goal、Long四个子集）、MetaWorld（50个Sawyer机器人任务）。</p>\n<p> - 真实世界：使用KINOVA GEN2机器人和RealSense D455相机，在室内环境中操作各种物体，并设计了长视野高动态任务。</p>\n<ul><li>训练资源：</li></ul>\n<p> - System 2：使用预训练的Eagle-2 VLM。</p>\n<p> - System 3：微调SVD模型，使用193,690个人类轨迹和179,074个机器人轨迹，补充Calvin ABC、MetaWorld和真实任务视频。在8块NVIDIA H100 GPU上训练2-3天。</p>\n<p> - System 1：在任务数据集上训练，使用4块H100 GPU，耗时5-9小时。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：</li></ul>\n<p> - 模拟环境：Calvin、LIBERO、MetaWorld基准测试环境。</p>\n<p> - 真实环境：KINOVA GEN2机器人配备RealSense D455深度相机，采用眼在手配置，在室内场景进行操作。</p>\n<ul><li>评估指标：</li></ul>\n<p> - Calvin：任务序列的平均成功长度（Avg. Len）。</p>\n<p> - LIBERO和MetaWorld：平均任务成功率（%）。</p>\n<p> - 真实世界：任务执行的定性分析，包括长视野任务的成功率。</p>\n<p> - 额外指标：推理延迟（ms）和模型参数量（B）。</p>"
  },
  {
    "date": "2025-07-01",
    "title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding",
    "link": "http://arxiv.org/abs/2507.00416",
    "summary_markdown": "### 论文研究单位\n上海交通大学人工智能学院、EvoMind Tech、上海市人工智能研究院、剑桥大学。\n### 论文概述\n该论文提出了Evo-0，一种具有隐式空间理解的视觉-语言-动作（VLA）模型。现有的VLA模型通常依赖于在2D图像-文本对上预训练的视觉-语言模型（VLM），因此缺乏精确的3D空间理解能力。为解决此问题，一些方法引入了显式的3D输入（如点云或深度图），但这需要额外的深度传感器或预训练的深度估计模型。Evo-0通过一个即插即用的模块，利用一个现成的视觉几何基础模型（VGGT）隐式地将3D几何特征融入VLA模型，仅从RGB图像中就能为模型提供具有深度感知的视觉表示，从而增强其对场景几何结构和物体空间关系的理解。论文在模拟和真实世界的多种空间挑战性任务上进行了评估，结果表明Evo-0显著优于最先进的VLA模型。\n### 论文核心贡献点\n1. 提出了一个即插即用的模块，通过隐式注入3D几何先验来增强VLA模型的空间理解能力，而无需使用深度传感器或显式的深度估计。\n2. 在模拟和真实世界的一系列具有空间挑战性的任务上评估了该方法，并证明了其相对于强基线模型的持续改进。\n3. 设计了一个在多种扰动条件下的鲁棒性评估设置，以验证该方法在真实世界扰动中的有效性。\n### 论文方法描述\nEvo-0基于开源VLA模型π₀构建。该方法的架构包含两个编码器：一个2D图像编码器（来自ViT）和一个VGGT空间编码器。输入是多视角RGB图像。VGGT编码器输出包含3D几何信息的特征，这些特征被提取为3D tokens。然后，一个轻量级的融合模块通过单层交叉注意力机制将2D视觉 tokens（作为查询）与VGGT的3D tokens（作为键和值）进行融合。融合后的特征被输入到PaliGemma视觉语言模型中，该模型结合几何增强的视觉输入和语言 tokens 来预测机器人动作。为了保持计算效率，核心VLM参数被冻结，仅在融合模块、LoRA层和流匹配动作专家模块上进行微调。\n### 论文使用数据集和训练资源\n- **模拟数据集**: RLBench基准测试中的五个任务：PlayJenga, PutKnifeOnChoppingBoard, TakeUmbrellaOutOfUmbrellaStand, PlaceHangerOnRack, MoveHanger。每个任务生成100条演示轨迹进行多任务训练。\n- **真实世界数据集**: 针对五个真实世界任务，通过遥操作收集了100条专家演示数据。这些任务包括：在目标上居中圆柱体、轴孔插入、抓取中间的瓶子、罐子抓取放置、透明物体抓取放置。\n- **训练资源**: 使用单块NVIDIA A800 GPU（80GB）进行训练，采用bfloat16混合精度，批次大小为32。优化器为AdamW，权重衰减为10^{-10}，采用余弦学习率调度，初始学习率为2.5 × 10^{-5}，在1000步内预热后衰减至2.5 × 10^{-6}。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - **模拟**: 在CoppeliaSim模拟器中的RLBench环境，使用配备前置、腕部和俯视摄像头的Franka Panda机器人。\n - **真实世界**: 使用真实机器人臂执行五个空间操作任务。\n - **鲁棒性测试**: 在真实世界环境中，对一个简化任务逐步引入五种扰动：未见过的干扰物、背景颜色变化、目标位置偏移、目标高度变化和相机视角变化。\n- **评估指标**:\n - **主要指标**: 任务成功率。在模拟任务中，成功标准依据官方RLBench规范。\n - **真实世界任务**: 除“在目标上居中圆柱体”任务外，均采用二元成功指标。该任务采用0-5分的评分系统，根据圆柱体中心落在目标靶的哪个环来评分，得分越高代表精度越高。\n - **鲁棒性测试**: 同样使用任务成功率，并在特定条件下（如存在干扰物时）评估子任务的成功率（如选择正确物体的成功率）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学人工智能学院、EvoMind Tech、上海市人工智能研究院、剑桥大学。</p>\n<h3>论文概述</h3>\n<p>该论文提出了Evo-0，一种具有隐式空间理解的视觉-语言-动作（VLA）模型。现有的VLA模型通常依赖于在2D图像-文本对上预训练的视觉-语言模型（VLM），因此缺乏精确的3D空间理解能力。为解决此问题，一些方法引入了显式的3D输入（如点云或深度图），但这需要额外的深度传感器或预训练的深度估计模型。Evo-0通过一个即插即用的模块，利用一个现成的视觉几何基础模型（VGGT）隐式地将3D几何特征融入VLA模型，仅从RGB图像中就能为模型提供具有深度感知的视觉表示，从而增强其对场景几何结构和物体空间关系的理解。论文在模拟和真实世界的多种空间挑战性任务上进行了评估，结果表明Evo-0显著优于最先进的VLA模型。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个即插即用的模块，通过隐式注入3D几何先验来增强VLA模型的空间理解能力，而无需使用深度传感器或显式的深度估计。</li><li>在模拟和真实世界的一系列具有空间挑战性的任务上评估了该方法，并证明了其相对于强基线模型的持续改进。</li><li>设计了一个在多种扰动条件下的鲁棒性评估设置，以验证该方法在真实世界扰动中的有效性。</li></ol>\n<h3>论文方法描述</h3>\n<p>Evo-0基于开源VLA模型π₀构建。该方法的架构包含两个编码器：一个2D图像编码器（来自ViT）和一个VGGT空间编码器。输入是多视角RGB图像。VGGT编码器输出包含3D几何信息的特征，这些特征被提取为3D tokens。然后，一个轻量级的融合模块通过单层交叉注意力机制将2D视觉 tokens（作为查询）与VGGT的3D tokens（作为键和值）进行融合。融合后的特征被输入到PaliGemma视觉语言模型中，该模型结合几何增强的视觉输入和语言 tokens 来预测机器人动作。为了保持计算效率，核心VLM参数被冻结，仅在融合模块、LoRA层和流匹配动作专家模块上进行微调。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>模拟数据集</strong>: RLBench基准测试中的五个任务：PlayJenga, PutKnifeOnChoppingBoard, TakeUmbrellaOutOfUmbrellaStand, PlaceHangerOnRack, MoveHanger。每个任务生成100条演示轨迹进行多任务训练。</li><li><strong>真实世界数据集</strong>: 针对五个真实世界任务，通过遥操作收集了100条专家演示数据。这些任务包括：在目标上居中圆柱体、轴孔插入、抓取中间的瓶子、罐子抓取放置、透明物体抓取放置。</li><li><strong>训练资源</strong>: 使用单块NVIDIA A800 GPU（80GB）进行训练，采用bfloat16混合精度，批次大小为32。优化器为AdamW，权重衰减为10^{-10}，采用余弦学习率调度，初始学习率为2.5 × 10^{-5}，在1000步内预热后衰减至2.5 × 10^{-6}。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - <strong>模拟</strong>: 在CoppeliaSim模拟器中的RLBench环境，使用配备前置、腕部和俯视摄像头的Franka Panda机器人。</p>\n<p> - <strong>真实世界</strong>: 使用真实机器人臂执行五个空间操作任务。</p>\n<p> - <strong>鲁棒性测试</strong>: 在真实世界环境中，对一个简化任务逐步引入五种扰动：未见过的干扰物、背景颜色变化、目标位置偏移、目标高度变化和相机视角变化。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>主要指标</strong>: 任务成功率。在模拟任务中，成功标准依据官方RLBench规范。</p>\n<p> - <strong>真实世界任务</strong>: 除“在目标上居中圆柱体”任务外，均采用二元成功指标。该任务采用0-5分的评分系统，根据圆柱体中心落在目标靶的哪个环来评分，得分越高代表精度越高。</p>\n<p> - <strong>鲁棒性测试</strong>: 同样使用任务成功率，并在特定条件下（如存在干扰物时）评估子任务的成功率（如选择正确物体的成功率）。</p>"
  },
  {
    "date": "2025-06-30",
    "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
    "link": "http://arxiv.org/abs/2506.24044",
    "summary_markdown": "### 论文研究单位\nMcGill University (加拿大), Tsinghua University (中国), Xiaomi Corporation, University of Wisconsin–Madison (美国), University of Minnesota–Twin Cities (美国), State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University (中国)\n### 论文概述\n这是第一篇关于自动驾驶视觉-语言-动作（VLA）模型的全面综述，系统梳理了VLA4AD领域的发展脉络。论文从自动驾驶技术演进切入，对比了经典模块化流水线、端到端学习、视觉语言模型（VLM）和最新的VLA范式，分析了超过20个代表性模型，整合了现有数据集和基准测试，并探讨了开放挑战与未来方向。\n### 论文核心贡献点\n- 首次系统性地梳理自动驾驶领域的VLA模型研究进展\n- 形式化了VLA4AD的通用架构构建模块\n- 追溯了从早期解释器到推理中心VLA模型的四个演进阶段\n- 对比分析了20多个代表性VLA模型的核心特性与技术差异\n- 整合了现有数据集和评估基准，提出联合评估驾驶安全性、准确性和解释质量的协议\n- 详述了鲁棒性、实时效率、形式验证等开放挑战\n- 提出了基础规模驾驶模型、神经符号安全内核等未来研究方向\n### 论文方法描述\nVLA4AD的核心架构包含三个主要部分：\n1. 多模态输入处理：视觉数据（单目/多目相机、BEV表示）、其他传感器数据（LiDAR、雷达、IMU、GPS）和语言输入（导航指令、环境查询、任务级规范）\n2. 核心架构模块：\n - 视觉编码器：使用DINOv2、ConvNeXt-V2或CLIP等自监督主干网络\n - 语言处理器：基于LLaMA2或GPT风格的Transformer，支持指令微调和检索增强\n - 动作解码器：包括自回归分词器、扩散头或流匹配/策略梯度专家\n3. 驾驶输出：低级动作（转向角、油门、制动）和轨迹规划（BEV坐标下的路径点）\n### 论文使用数据集和训练资源\n- BDD100K/BDD-X：100k美国驾驶视频，7k带有人类解释标注的片段\n- nuScenes：1k个20秒真实场景（波士顿/新加坡），含6摄像头+LiDAR+雷达\n- Bench2Drive：CARLA闭环基准，220条路线覆盖44种场景类型\n- Reason2Drive：600k视频-文本对，包含CoT风格的问答标注\n- DriveLM-Data：18k场景图结构QA，支持条件推理\n- Impromptu VLA：80k边缘案例片段（30秒），密集人群/救护车/恶劣天气\n- NuInteract：1k多视图场景，密集标注和3D多轮问答\n- DriveAction：2.6k真实场景和16.2k视觉语言QA对\n训练资源包括大规模多传感器日志（如nuScenes、Waymo车队数据）和互联网规模视觉-语言预训练模型\n### 论文使用的评估环境和评估指标\n评估环境：\n- 闭环驾驶测试（CARLA模拟器、Navsim-v2）\n- 开环预测评估\n- 多基准测试套件（Bench2Drive、Reason2Drive、DriveLM）\n\n评估指标：\n- 驾驶安全性指标（碰撞率、违规率）\n- 控制准确性指标（轨迹偏差、成功率）\n- 语言能力评估（BLEU、图一致性、逻辑一致性）\n- 鲁棒性测试（压力场景表现）\n- 人类偏好对齐评估（通过DriveAction等数据集）\n- 实时性指标（推理延迟、控制频率）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>McGill University (加拿大), Tsinghua University (中国), Xiaomi Corporation, University of Wisconsin–Madison (美国), University of Minnesota–Twin Cities (美国), State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University (中国)</p>\n<h3>论文概述</h3>\n<p>这是第一篇关于自动驾驶视觉-语言-动作（VLA）模型的全面综述，系统梳理了VLA4AD领域的发展脉络。论文从自动驾驶技术演进切入，对比了经典模块化流水线、端到端学习、视觉语言模型（VLM）和最新的VLA范式，分析了超过20个代表性模型，整合了现有数据集和基准测试，并探讨了开放挑战与未来方向。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首次系统性地梳理自动驾驶领域的VLA模型研究进展</li><li>形式化了VLA4AD的通用架构构建模块</li><li>追溯了从早期解释器到推理中心VLA模型的四个演进阶段</li><li>对比分析了20多个代表性VLA模型的核心特性与技术差异</li><li>整合了现有数据集和评估基准，提出联合评估驾驶安全性、准确性和解释质量的协议</li><li>详述了鲁棒性、实时效率、形式验证等开放挑战</li><li>提出了基础规模驾驶模型、神经符号安全内核等未来研究方向</li></ul>\n<h3>论文方法描述</h3>\n<p>VLA4AD的核心架构包含三个主要部分：</p>\n<ol><li>多模态输入处理：视觉数据（单目/多目相机、BEV表示）、其他传感器数据（LiDAR、雷达、IMU、GPS）和语言输入（导航指令、环境查询、任务级规范）</li><li>核心架构模块：</li></ol>\n<p> - 视觉编码器：使用DINOv2、ConvNeXt-V2或CLIP等自监督主干网络</p>\n<p> - 语言处理器：基于LLaMA2或GPT风格的Transformer，支持指令微调和检索增强</p>\n<p> - 动作解码器：包括自回归分词器、扩散头或流匹配/策略梯度专家</p>\n<ol><li>驾驶输出：低级动作（转向角、油门、制动）和轨迹规划（BEV坐标下的路径点）</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>BDD100K/BDD-X：100k美国驾驶视频，7k带有人类解释标注的片段</li><li>nuScenes：1k个20秒真实场景（波士顿/新加坡），含6摄像头+LiDAR+雷达</li><li>Bench2Drive：CARLA闭环基准，220条路线覆盖44种场景类型</li><li>Reason2Drive：600k视频-文本对，包含CoT风格的问答标注</li><li>DriveLM-Data：18k场景图结构QA，支持条件推理</li><li>Impromptu VLA：80k边缘案例片段（30秒），密集人群/救护车/恶劣天气</li><li>NuInteract：1k多视图场景，密集标注和3D多轮问答</li><li>DriveAction：2.6k真实场景和16.2k视觉语言QA对</li></ul>\n<p>训练资源包括大规模多传感器日志（如nuScenes、Waymo车队数据）和互联网规模视觉-语言预训练模型</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：</p>\n<ul><li>闭环驾驶测试（CARLA模拟器、Navsim-v2）</li><li>开环预测评估</li><li>多基准测试套件（Bench2Drive、Reason2Drive、DriveLM）</li></ul>\n\n<p>评估指标：</p>\n<ul><li>驾驶安全性指标（碰撞率、违规率）</li><li>控制准确性指标（轨迹偏差、成功率）</li><li>语言能力评估（BLEU、图一致性、逻辑一致性）</li><li>鲁棒性测试（压力场景表现）</li><li>人类偏好对齐评估（通过DriveAction等数据集）</li><li>实时性指标（推理延迟、控制频率）</li></ul>"
  },
  {
    "date": "2025-06-29",
    "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering",
    "link": "http://arxiv.org/abs/2506.23329",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-27",
    "title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration",
    "link": "http://arxiv.org/abs/2506.22242",
    "summary_markdown": "### 论文研究单位\n- 复旦大学数据科学学院\n- 华为诺亚方舟实验室\n### 论文概述\n- 论文提出4D-VLA模型，旨在解决现有机器人预训练方法中因输入信息不完整导致的“坐标系混乱”和“状态混乱”问题。\n- 通过引入RGB-D序列输入，融合深度和时空信息，对齐机器人与场景的坐标系，增强时空推理能力。\n- 提出“记忆库采样”策略，从历史帧中提取关键信息，提升效率和效果。\n- 引入多视图仿真基准MV-Bench评估空间感知和新视角泛化能力。\n### 论文核心贡献点\n- 提出4D-VLA模型，整合空间模块生成3D感知的视觉token，缓解坐标系和状态混乱。\n- 设计记忆库采样方法，高效利用历史信息。\n- 在LIBERO和真实世界任务中显著超越基线，如在LIBERO-LONG任务上成功率比OpenVLA高25.4%。\n- 构建MV-Bench多视角数据集，验证模型的空间理解和新视角泛化能力。\n### 论文方法描述\n- 输入处理：使用序列RGB-D图像，通过3D坐标嵌入生成空间感知视觉token。\n- 时空编码：将历史帧与当前帧融合，加入可学习时间位置编码，形成4D表示。\n- 记忆库采样：自适应选择信息量大的历史帧，避免均匀采样的冗余。\n- 模型架构：基于InternVL-4B，冻结视觉编码器，训练其他参数。\n- 损失函数：结合平移、旋转、夹爪和方向损失，其中方向损失强调动作方向准确性。\n### 论文使用数据集和训练资源\n- 数据集：DROID（真实世界，76k轨迹）、LIBERO（仿真，多任务集）、自建MV-Bench（多视角）。\n- 训练资源：8块NVIDIA A6000 GPU，训练约96小时，预训练 batch size 512，微调batch size 128。\n### 论文使用的评估环境和评估指标\n- 评估环境：\n - 仿真：LIBERO任务集（LIBERO-SPATIAL、OBJECT、GOAL、LONG）、MV-Bench多视角基准。\n - 真实：Franka机械臂执行4项任务（空间泛化、抗干扰、精确放置、指令跟随）和2项多视角任务。\n- 评估指标：\n - 任务成功率（仿真/真实任务）。\n - 平均成功率（多视角任务分视角和跨视角评估）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>复旦大学数据科学学院</li><li>华为诺亚方舟实验室</li></ul>\n<h3>论文概述</h3>\n<ul><li>论文提出4D-VLA模型，旨在解决现有机器人预训练方法中因输入信息不完整导致的“坐标系混乱”和“状态混乱”问题。</li><li>通过引入RGB-D序列输入，融合深度和时空信息，对齐机器人与场景的坐标系，增强时空推理能力。</li><li>提出“记忆库采样”策略，从历史帧中提取关键信息，提升效率和效果。</li><li>引入多视图仿真基准MV-Bench评估空间感知和新视角泛化能力。</li></ul>\n<h3>论文核心贡献点</h3>\n<ul><li>提出4D-VLA模型，整合空间模块生成3D感知的视觉token，缓解坐标系和状态混乱。</li><li>设计记忆库采样方法，高效利用历史信息。</li><li>在LIBERO和真实世界任务中显著超越基线，如在LIBERO-LONG任务上成功率比OpenVLA高25.4%。</li><li>构建MV-Bench多视角数据集，验证模型的空间理解和新视角泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>输入处理：使用序列RGB-D图像，通过3D坐标嵌入生成空间感知视觉token。</li><li>时空编码：将历史帧与当前帧融合，加入可学习时间位置编码，形成4D表示。</li><li>记忆库采样：自适应选择信息量大的历史帧，避免均匀采样的冗余。</li><li>模型架构：基于InternVL-4B，冻结视觉编码器，训练其他参数。</li><li>损失函数：结合平移、旋转、夹爪和方向损失，其中方向损失强调动作方向准确性。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：DROID（真实世界，76k轨迹）、LIBERO（仿真，多任务集）、自建MV-Bench（多视角）。</li><li>训练资源：8块NVIDIA A6000 GPU，训练约96小时，预训练 batch size 512，微调batch size 128。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：</li></ul>\n<p> - 仿真：LIBERO任务集（LIBERO-SPATIAL、OBJECT、GOAL、LONG）、MV-Bench多视角基准。</p>\n<p> - 真实：Franka机械臂执行4项任务（空间泛化、抗干扰、精确放置、指令跟随）和2项多视角任务。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 任务成功率（仿真/真实任务）。</p>\n<p> - 平均成功率（多视角任务分视角和跨视角评估）。</p>"
  },
  {
    "date": "2025-06-26",
    "title": "WorldVLA: Towards Autoregressive Action World Model",
    "link": "http://arxiv.org/abs/2506.21539",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-26",
    "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends",
    "link": "http://arxiv.org/abs/2506.20966",
    "summary_markdown": "### 论文研究单位\n中国科学院自动化研究所多模态人工智能系统重点实验室，中国科学院大学人工智能学院，美国伊利诺伊大学厄巴纳-香槟分校格兰杰工程学院。\n### 论文概述\n本文是一篇综述性论文，探讨了视觉-语言-行动模型的后训练与人类运动学习之间的相似性。论文指出，VLA模型在经过大规模预训练后，仍需通过后训练来适应特定的下游应用，以提高在特定环境、任务和机器人实体上的性能。文章借鉴人类运动学习的视角，围绕环境、实体和任务三个维度，系统性地回顾了VLA模型的后训练策略，并提出了一个与之对齐的分类法，最后讨论了该领域面临的挑战和未来趋势。\n### 论文核心贡献点\n1. 首次从人类运动学习的视角，对VLA模型的后训练方法进行了系统性综述。\n2. 提出了一个围绕环境、实体和任务的结构化分类法，该分类法与人类运动学习机制相呼应，用于组织和分析现有的VLA后训练方法。\n3. 识别并阐述了当前VLA模型后训练面临的关键挑战和未来的研究方向，为该领域的后续研究提供了概念框架和实践见解。\n### 论文方法描述\n论文提出一个与人类运动学习机制对齐的分类法来综述VLA模型的后训练方法，该分类法包含四个主要方面：\n1. 增强环境感知：旨在提升模型对操作环境的理解能力。方法包括可供性引导学习、为操作任务增强编码器和操作任务增强表征。\n2. 改进实体意识：旨在让模型更好地理解和适应特定机器人实体的动态特性。方法包括学习正向/逆向运动学、设计更好的动作输出头。\n3. 深化任务理解：旨在让模型更深刻地掌握操作任务的逻辑和层次。方法包括人机交互学习、分层任务操作。\n4. 多组件集成：旨在将环境、实体和任务的多个方面进行协同优化。方法包括使用强化学习、视觉交互预测和主动数据处理。\n### 论文使用数据集和训练资源\n论文主要提到了Open X-Embodiment数据集，该数据集聚合了58个现有的机器人操作数据集。此外，还提及了多种高保真度模拟器和任务特定的模拟环境被用于生成操作数据以降低数据采集成本。关于具体的训练资源（如计算硬件、训练时长等），原文中未提及。\n### 论文使用的评估环境和评估指标\n所提供的论文HTML原文中未包含此部分内容。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中国科学院自动化研究所多模态人工智能系统重点实验室，中国科学院大学人工智能学院，美国伊利诺伊大学厄巴纳-香槟分校格兰杰工程学院。</p>\n<h3>论文概述</h3>\n<p>本文是一篇综述性论文，探讨了视觉-语言-行动模型的后训练与人类运动学习之间的相似性。论文指出，VLA模型在经过大规模预训练后，仍需通过后训练来适应特定的下游应用，以提高在特定环境、任务和机器人实体上的性能。文章借鉴人类运动学习的视角，围绕环境、实体和任务三个维度，系统性地回顾了VLA模型的后训练策略，并提出了一个与之对齐的分类法，最后讨论了该领域面临的挑战和未来趋势。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>首次从人类运动学习的视角，对VLA模型的后训练方法进行了系统性综述。</li><li>提出了一个围绕环境、实体和任务的结构化分类法，该分类法与人类运动学习机制相呼应，用于组织和分析现有的VLA后训练方法。</li><li>识别并阐述了当前VLA模型后训练面临的关键挑战和未来的研究方向，为该领域的后续研究提供了概念框架和实践见解。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文提出一个与人类运动学习机制对齐的分类法来综述VLA模型的后训练方法，该分类法包含四个主要方面：</p>\n<ol><li>增强环境感知：旨在提升模型对操作环境的理解能力。方法包括可供性引导学习、为操作任务增强编码器和操作任务增强表征。</li><li>改进实体意识：旨在让模型更好地理解和适应特定机器人实体的动态特性。方法包括学习正向/逆向运动学、设计更好的动作输出头。</li><li>深化任务理解：旨在让模型更深刻地掌握操作任务的逻辑和层次。方法包括人机交互学习、分层任务操作。</li><li>多组件集成：旨在将环境、实体和任务的多个方面进行协同优化。方法包括使用强化学习、视觉交互预测和主动数据处理。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>论文主要提到了Open X-Embodiment数据集，该数据集聚合了58个现有的机器人操作数据集。此外，还提及了多种高保真度模拟器和任务特定的模拟环境被用于生成操作数据以降低数据采集成本。关于具体的训练资源（如计算硬件、训练时长等），原文中未提及。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>所提供的论文HTML原文中未包含此部分内容。</p>"
  },
  {
    "date": "2025-06-24",
    "title": "Unified Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2506.19850",
    "summary_markdown": "# 论文总结\n## 论文研究单位\nCASIA, BAAI, THU, HKISI\n## 论文概述\n该论文提出了UniVLA，一个统一的视觉-语言-动作（VLA）模型。与以往依赖外部视觉编码器且仅输出动作的VLA方法不同，UniVLA将视觉、语言和动作信号表示为离散令牌序列，并在统一的自回归框架中进行建模。这种统一的多模态范式支持灵活的多模态任务学习，特别是从大规模视频数据中学习。通过在后训练阶段结合世界模型，UniVLA能够从视频中捕捉因果动态，从而有效迁移到下游的策略学习，尤其是在长视野任务中。\n## 论文核心贡献点\n1. 提出了UniVLA，这是第一个将视觉、语言和动作编码为共享词汇表中的离散令牌，并通过自回归序列学习联合建模的统一VLA模型。\n2. 统一的序列建模框架支持广泛的多模态任务。通过研究不同的后训练策略，证明了世界模型可以有效地从视频数据中学习时间动态，显著提升下游策略学习的性能和效率，尤其是在长视野和分布外场景中。\n3. 模型在多个模拟基准测试中取得了最先进的性能，并支持大规模视频训练的开源VLA方法。进一步探索了其在空间推理和视频预测等不同模态的能力，并展示了其在自动驾驶场景中的有效迁移，突显了其在通用具身智能方面的潜力。\n## 论文方法描述\n1. **统一多模态模型**：视觉、语言和动作信号被转换为离散令牌，并使用共享词汇表进行自回归建模。图像通过VQ分词器进行离散化，动作通过FAST分词器使用离散余弦变换（DCT）进行编码。训练目标是标准的下一个令牌预测任务，使用交叉熵损失。\n2. **统一多模态序列建模**：采用基于马尔可夫链的自回归序列建模方法，其中观察和动作交错排列。这自然地包含了因果依赖关系，使模型能够对时间动态进行推理。\n - **世界模型（后训练）**：通过仅对视觉令牌进行监督来学习环境的动态，使模型能够生成给定指令和观察状态条件的视觉预测。\n - **策略学习（微调）**：在交错格式中，采用两阶段训练策略。模型首先使用视觉语言（VL）对齐的检查点进行初始化，后训练阶段利用大规模机器人中心视频数据集来研究不同后训练策略对下游策略学习的影响，微调阶段则专注于动作学习以细化任务特定行为。\n## 论文使用数据集和训练资源\n1. **数据集**：\n - CALVIN：用于评估长视野、语言条件机器人操作的模拟基准。\n - LIBERO：终身机器人操作的综合套件，包含四个任务套件。\n - SimplerEnv：用于评估在真实世界视频数据上训练的模型的迁移性和泛化能力的模拟基准。\n2. **训练资源**：\n - 模型采用8.5B参数的纯自回归Transformer架构。\n - 后训练阶段使用622K视频，训练30K步，批量大小为64。\n - 微调阶段：CALVIN基准使用A100 GPU，批量大小192，训练8k步；LIBERO基准使用A100 GPU，批量大小192，训练8k步；SimplerEnv基准使用A100 GPU，批量大小128，训练20k步。\n## 论文使用的评估环境和评估指标\n1. **评估环境**：\n - CALVIN模拟环境：评估长视野任务，包括ABC→D和ABCD→D设置。\n - LIBERO模拟环境：评估空间推理、对象级泛化、目标条件行为和长视野组合任务。\n - SimplerEnv模拟环境：评估在WidowX和Google Robot平台上的迁移性和泛化能力。\n2. **评估指标**：\n - CALVIN：平均连续完成的子任务数（Avg. Len ↑）。\n - LIBERO：每个任务套件的平均成功率（%），在500个剧集上进行评估。\n - SimplerEnv：各种操作任务的平均成功率（%），包括抓取成功率（Grasp）和整体成功率。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>CASIA, BAAI, THU, HKISI</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>该论文提出了UniVLA，一个统一的视觉-语言-动作（VLA）模型。与以往依赖外部视觉编码器且仅输出动作的VLA方法不同，UniVLA将视觉、语言和动作信号表示为离散令牌序列，并在统一的自回归框架中进行建模。这种统一的多模态范式支持灵活的多模态任务学习，特别是从大规模视频数据中学习。通过在后训练阶段结合世界模型，UniVLA能够从视频中捕捉因果动态，从而有效迁移到下游的策略学习，尤其是在长视野任务中。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出了UniVLA，这是第一个将视觉、语言和动作编码为共享词汇表中的离散令牌，并通过自回归序列学习联合建模的统一VLA模型。</li><li>统一的序列建模框架支持广泛的多模态任务。通过研究不同的后训练策略，证明了世界模型可以有效地从视频数据中学习时间动态，显著提升下游策略学习的性能和效率，尤其是在长视野和分布外场景中。</li><li>模型在多个模拟基准测试中取得了最先进的性能，并支持大规模视频训练的开源VLA方法。进一步探索了其在空间推理和视频预测等不同模态的能力，并展示了其在自动驾驶场景中的有效迁移，突显了其在通用具身智能方面的潜力。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ol><li><strong>统一多模态模型</strong>：视觉、语言和动作信号被转换为离散令牌，并使用共享词汇表进行自回归建模。图像通过VQ分词器进行离散化，动作通过FAST分词器使用离散余弦变换（DCT）进行编码。训练目标是标准的下一个令牌预测任务，使用交叉熵损失。</li><li><strong>统一多模态序列建模</strong>：采用基于马尔可夫链的自回归序列建模方法，其中观察和动作交错排列。这自然地包含了因果依赖关系，使模型能够对时间动态进行推理。</li></ol>\n<p> - <strong>世界模型（后训练）</strong>：通过仅对视觉令牌进行监督来学习环境的动态，使模型能够生成给定指令和观察状态条件的视觉预测。</p>\n<p> - <strong>策略学习（微调）</strong>：在交错格式中，采用两阶段训练策略。模型首先使用视觉语言（VL）对齐的检查点进行初始化，后训练阶段利用大规模机器人中心视频数据集来研究不同后训练策略对下游策略学习的影响，微调阶段则专注于动作学习以细化任务特定行为。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - CALVIN：用于评估长视野、语言条件机器人操作的模拟基准。</p>\n<p> - LIBERO：终身机器人操作的综合套件，包含四个任务套件。</p>\n<p> - SimplerEnv：用于评估在真实世界视频数据上训练的模型的迁移性和泛化能力的模拟基准。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> - 模型采用8.5B参数的纯自回归Transformer架构。</p>\n<p> - 后训练阶段使用622K视频，训练30K步，批量大小为64。</p>\n<p> - 微调阶段：CALVIN基准使用A100 GPU，批量大小192，训练8k步；LIBERO基准使用A100 GPU，批量大小192，训练8k步；SimplerEnv基准使用A100 GPU，批量大小128，训练20k步。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - CALVIN模拟环境：评估长视野任务，包括ABC→D和ABCD→D设置。</p>\n<p> - LIBERO模拟环境：评估空间推理、对象级泛化、目标条件行为和长视野组合任务。</p>\n<p> - SimplerEnv模拟环境：评估在WidowX和Google Robot平台上的迁移性和泛化能力。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - CALVIN：平均连续完成的子任务数（Avg. Len ↑）。</p>\n<p> - LIBERO：每个任务套件的平均成功率（%），在500个剧集上进行评估。</p>\n<p> - SimplerEnv：各种操作任务的平均成功率（%），包括抓取成功率（Grasp）和整体成功率。</p>"
  },
  {
    "date": "2025-06-24",
    "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation",
    "link": "http://arxiv.org/abs/2506.19816",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-23",
    "title": "MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis",
    "link": "http://arxiv.org/abs/2506.18897",
    "summary_markdown": "```markdown\n### 论文研究单位\n论文作者来自多个机构，包括编号1、2、3所属的学术研究机构和企业研究部门（具体机构名称未在HTML中明确列出）。\n### 论文概述\n论文提出MinD（Manipulate in Dream）框架，一个双系统世界模型，用于解决视频生成模型在机器人实时控制中的应用挑战。现有方法将VGMs仅用作特征提取器，未能充分利用其预测未来状态的能力。MinD通过结合低频视频生成和高频动作策略，实现实时规划和隐式风险分析，在RL-Bench上达到63%成功率，真实Franka机器人上达到60%成功率，以11.3 FPS运行。\n### 论文核心贡献点\n1. 提出MinD双系统扩散世界模型，统一低频视频想象过程与高频动作策略。\n2. 引入DiffMatcher模块，采用扩散强制（diffusion-forcing）协同训练策略，对齐异步视频和动作扩散过程。\n3. 实现最先进性能（RL-Bench 63%、真实任务60%），并能提前识别74%的潜在任务失败，为安全监控提供新范式。\n### 论文方法描述\n- 分层扩散框架：包括低频视频生成器（LoDiff-Visual）生成未来场景，高频动作策略（HiDiff-Policy）输出实时动作，通过DiffMatcher模块连接两者。\n- 异步协同训练策略：联合优化三个损失函数（视频损失、动作损失、对齐损失），其中对齐损失强制DiffMatcher学习噪声不变表示。\n- 单步预测推理：LoDiff-Visual仅执行单步去噪生成潜在特征，经DiffMatcher映射后 conditioning HiDiff-Policy，实现高效动作生成。\n- 隐式风险评估：通过分析生成视频的潜在特征分布和视觉失败案例，预测任务执行风险。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：RT-1、Robomind、OXE的混合数据集。\n - 微调：RL-Bench（1000条轨迹，7任务）和真实世界Franka机器人任务（每任务100条演示）。\n- **训练资源**：\n - 硬件：4×A100 GPU。\n - 软件：PyTorch 2.5.1，CUDA 12.1。\n - 参数：AdamW优化器，学习率2e-5，批次大小16，预训练50k步，微调10k步。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟：RL-Bench（CoppeliaSim环境，Franka Panda机器人）。\n - 真实世界：Franka Research 3机器人，配备前视和腕视相机。\n- **评估指标**：\n - 任务成功率（Success Rate）：主要指标，衡量任务完成百分比。\n - Fréchet Video Distance（FVD）：评估生成视频质量。\n - 推理速度（FPS）：衡量实时控制能力。\n - 风险识别率：通过生成视频预测失败的能力。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>论文作者来自多个机构，包括编号1、2、3所属的学术研究机构和企业研究部门（具体机构名称未在HTML中明确列出）。</p>\n<h3>论文概述</h3>\n<p>论文提出MinD（Manipulate in Dream）框架，一个双系统世界模型，用于解决视频生成模型在机器人实时控制中的应用挑战。现有方法将VGMs仅用作特征提取器，未能充分利用其预测未来状态的能力。MinD通过结合低频视频生成和高频动作策略，实现实时规划和隐式风险分析，在RL-Bench上达到63%成功率，真实Franka机器人上达到60%成功率，以11.3 FPS运行。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出MinD双系统扩散世界模型，统一低频视频想象过程与高频动作策略。</li><li>引入DiffMatcher模块，采用扩散强制（diffusion-forcing）协同训练策略，对齐异步视频和动作扩散过程。</li><li>实现最先进性能（RL-Bench 63%、真实任务60%），并能提前识别74%的潜在任务失败，为安全监控提供新范式。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li>分层扩散框架：包括低频视频生成器（LoDiff-Visual）生成未来场景，高频动作策略（HiDiff-Policy）输出实时动作，通过DiffMatcher模块连接两者。</li><li>异步协同训练策略：联合优化三个损失函数（视频损失、动作损失、对齐损失），其中对齐损失强制DiffMatcher学习噪声不变表示。</li><li>单步预测推理：LoDiff-Visual仅执行单步去噪生成潜在特征，经DiffMatcher映射后 conditioning HiDiff-Policy，实现高效动作生成。</li><li>隐式风险评估：通过分析生成视频的潜在特征分布和视觉失败案例，预测任务执行风险。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：RT-1、Robomind、OXE的混合数据集。</p>\n<p> - 微调：RL-Bench（1000条轨迹，7任务）和真实世界Franka机器人任务（每任务100条演示）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 硬件：4×A100 GPU。</p>\n<p> - 软件：PyTorch 2.5.1，CUDA 12.1。</p>\n<p> - 参数：AdamW优化器，学习率2e-5，批次大小16，预训练50k步，微调10k步。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟：RL-Bench（CoppeliaSim环境，Franka Panda机器人）。</p>\n<p> - 真实世界：Franka Research 3机器人，配备前视和腕视相机。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（Success Rate）：主要指标，衡量任务完成百分比。</p>\n<p> - Fréchet Video Distance（FVD）：评估生成视频质量。</p>\n<p> - 推理速度（FPS）：衡量实时控制能力。</p>\n<p> - 风险识别率：通过生成视频预测失败的能力。</p>\n<p>```</p>"
  },
  {
    "date": "2025-06-22",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation",
    "link": "http://arxiv.org/abs/2506.18088",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-21",
    "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.17811",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-21",
    "title": "RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.17639",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-21",
    "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.17561",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-19",
    "title": "CapsDT: Diffusion-Transformer for Capsule Robot Manipulation",
    "link": "http://arxiv.org/abs/2506.16263",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-19",
    "title": "ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.16211",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-17",
    "title": "FormGym: Doing Paperwork with Agents",
    "link": "http://arxiv.org/abs/2506.14079",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-16",
    "title": "GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics",
    "link": "http://arxiv.org/abs/2506.14009",
    "summary_markdown": "### 论文研究单位\n斯坦福大学机械工程系和航空航天系\n### 论文概述\n本文提出了GRaD-Nav++，一个轻量级的视觉-语言-行动(VLA)框架，能够在无人机机载计算硬件上完全运行，实时遵循自然语言指令。该策略通过可微强化学习(DiffRL)在照片般逼真的3D高斯分布(3DGS)模拟器中训练，能够从视觉和语言输入中高效学习低级控制。\n### 论文核心贡献点\n1. 提出了一种新颖的轻量级无人机飞行VLA框架，完全在无人机机载计算硬件上运行\n2. VLA策略使无人机能够基于高级自然语言指令完成复杂任务，展示了对未见任务的泛化能力和对不同环境条件的适应性\n3. 开发了新颖的多专家混合(MoE)动作模块，使用3DGS和DiffRL训练，在样本效率和任务成功率方面实现了最先进的性能\n### 论文方法描述\n- 使用预训练的CLIP模型进行高级场景理解和视觉指令匹配，冻结CLIP模型参数，微调一个线性层来融合视觉和文本嵌入\n- 策略网络采用MoE架构，包含两个专家子网络，每个时间步路由器激活top-k=2个专家\n- 每个专家是多层感知机，处理VLM特征向量和观测值\n- 使用3DGS进行场景表示，支持高保真视觉渲染\n- 使用可微动力学模型进行无人机仿真，包括角加速度、方向更新和线性加速度计算\n- 引入上下文估计网络来缩小模拟到现实的差距，提高策略鲁棒性\n### 论文使用数据集和训练资源\n- 在12个两阶段任务上训练，包括8个训练任务和4个零样本评估任务\n- 每个任务涉及选择正确的方向(通过、左、右、上)通过门，并识别飞向目标对象\n- 使用4个关键航点定义参考轨迹，通过A*规划连接\n- 训练在3DGS构建的仿真环境中进行\n- 使用DiffRL进行策略训练，结合3DGS渲染和可微动力学\n### 论文使用的评估环境和评估指标\n评估环境：\n- 多任务泛化实验：在训练过的任务和未见任务上评估\n- 多环境适应实验：在两个不同的3DGS环境中评估\n- 任务切换实验：评估环境变化时的适应能力\n- 在仿真环境和真实世界硬件部署中都进行测试\n\n评估指标：\n- 任务成功率作为主要评估指标\n- 在仿真环境中：训练任务成功率83%，未见任务成功率75%\n- 在真实硬件中：训练任务成功率67%，未见任务成功率50%\n- 多环境平均成功率：仿真81%，真实世界67%",
    "summary_html": "<h3>论文研究单位</h3>\n<p>斯坦福大学机械工程系和航空航天系</p>\n<h3>论文概述</h3>\n<p>本文提出了GRaD-Nav++，一个轻量级的视觉-语言-行动(VLA)框架，能够在无人机机载计算硬件上完全运行，实时遵循自然语言指令。该策略通过可微强化学习(DiffRL)在照片般逼真的3D高斯分布(3DGS)模拟器中训练，能够从视觉和语言输入中高效学习低级控制。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新颖的轻量级无人机飞行VLA框架，完全在无人机机载计算硬件上运行</li><li>VLA策略使无人机能够基于高级自然语言指令完成复杂任务，展示了对未见任务的泛化能力和对不同环境条件的适应性</li><li>开发了新颖的多专家混合(MoE)动作模块，使用3DGS和DiffRL训练，在样本效率和任务成功率方面实现了最先进的性能</li></ol>\n<h3>论文方法描述</h3>\n<ul><li>使用预训练的CLIP模型进行高级场景理解和视觉指令匹配，冻结CLIP模型参数，微调一个线性层来融合视觉和文本嵌入</li><li>策略网络采用MoE架构，包含两个专家子网络，每个时间步路由器激活top-k=2个专家</li><li>每个专家是多层感知机，处理VLM特征向量和观测值</li><li>使用3DGS进行场景表示，支持高保真视觉渲染</li><li>使用可微动力学模型进行无人机仿真，包括角加速度、方向更新和线性加速度计算</li><li>引入上下文估计网络来缩小模拟到现实的差距，提高策略鲁棒性</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>在12个两阶段任务上训练，包括8个训练任务和4个零样本评估任务</li><li>每个任务涉及选择正确的方向(通过、左、右、上)通过门，并识别飞向目标对象</li><li>使用4个关键航点定义参考轨迹，通过A*规划连接</li><li>训练在3DGS构建的仿真环境中进行</li><li>使用DiffRL进行策略训练，结合3DGS渲染和可微动力学</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：</p>\n<ul><li>多任务泛化实验：在训练过的任务和未见任务上评估</li><li>多环境适应实验：在两个不同的3DGS环境中评估</li><li>任务切换实验：评估环境变化时的适应能力</li><li>在仿真环境和真实世界硬件部署中都进行测试</li></ul>\n\n<p>评估指标：</p>\n<ul><li>任务成功率作为主要评估指标</li><li>在仿真环境中：训练任务成功率83%，未见任务成功率75%</li><li>在真实硬件中：训练任务成功率67%，未见任务成功率50%</li><li>多环境平均成功率：仿真81%，真实世界67%</li></ul>"
  },
  {
    "date": "2025-06-16",
    "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
    "link": "http://arxiv.org/abs/2506.13757",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-16",
    "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction",
    "link": "http://arxiv.org/abs/2506.13751",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-16",
    "title": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding",
    "link": "http://arxiv.org/abs/2506.13725",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-16",
    "title": "ROSA: Harnessing Robot States for Vision-Language and Action Alignment",
    "link": "http://arxiv.org/abs/2506.13679",
    "summary_markdown": "### 论文研究单位\n中国科学技术大学, 南京大学, Dexmal\n### 论文概述\n该论文提出了ROSA，一种新的训练范式，旨在解决视觉-语言-动作模型在将高级视觉-语言理解与低级机器人物理动作对齐时存在的时空鸿沟问题。现有方法直接微调视觉-语言模型，但面临数据效率低下和对人工标注依赖性强的问题。ROSA通过引入一种自动收集的机器人状态估计数据作为辅助监督，增强模型对3D空间的理解和自我感知能力，从而提升模型在数据有限情况下的性能和泛化能力。实验在RLBench模拟环境和真实的WidowX机器人平台上验证了该方法的有效性。\n### 论文核心贡献点\n1. 提出了一种名为ROSA的新颖训练范式，它利用机器人状态估计数据来实现视觉-语言和动作空间之间更好的对齐。\n2. 提出了一种简单而有效的解决方案来创建机器人状态估计数据，在不需要额外人类收集工作的情况下，显著增强了VLA的数据效率。\n3. 在RLBench模拟和一个真实世界的WidowX平台上进行了广泛的实验，证明ROSA有效增强了当前的VLA模型，并实现了优于先前方法的性能。\n### 论文方法描述\nROSA方法的核心是将VLA模型的训练分解为两个互补部分：预测未来动作和估计当前机器人状态。\n1. **训练数据**：\n * **专家动作数据**：由人类操作员收集的轨迹数据，包含视觉观测、语言指令和对应的7自由度机器人动作（3D位置、3D欧拉角、1个夹爪开合状态）。\n * **机器人状态估计数据**：通过自动化脚本控制机器人在预定义环境中随机移动并记录观测与状态。这些数据同样包含7自由度的状态信息，并配以统一的语言指令，如“What is the current state of the robot?”，以实现与专家数据的格式统一和联合训练。\n2. **模型架构**：\n * 基于LLaVA架构，包含一个视觉编码器、一个投影器和一个大语言模型。\n * 视觉编码器使用CLIP ViT-L/14，投影器为两层MLP，大语言模型骨干为Qwen-2.5-7B。\n * 为了让LLM预测连续的动作和状态，采用线性量化的方法将连续值离散化为token，推理时再通过逆映射恢复为连续值。\n3. **训练目标**：\n * 对专家动作数据和机器人状态数据使用统一的训练目标，即下一个token预测的交叉熵损失。\n * 模型以1:4的比例混合两种数据进行训练，并完全微调所有层。\n### 论文使用数据集和训练资源\n1. **数据集**：\n * **RLBench**：用于模拟实验，包含12个任务，每个任务包含多个训练变体。\n * **真实机器人数据**：在WidowX 250S机器人上采集，包括4个已见任务和4个用于泛化评估的未见任务。\n2. **训练资源**：\n * 使用8块NVIDIA A100 GPU进行训练。\n * 基于Qwen-2.5-7B、CLIP ViT-L/14模型构建。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n * **RLBench模拟环境**：使用Franka Panda机器人，配备一个固定的前置RGB摄像头（336x336分辨率）。\n * **真实世界环境**：使用WidowX 250S机器人和一个Intel RealSense D435摄像头提供第三人称视角。\n2. **评估指标**：\n * 主要评估指标是**成功率**，即任务成功的episode或试验次数占总数的百分比。\n * 在RLBench上，每个任务评估25个episode，并重复三次，报告平均成功率。\n * 在真实机器人上，每个任务评估10次试验。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中国科学技术大学, 南京大学, Dexmal</p>\n<h3>论文概述</h3>\n<p>该论文提出了ROSA，一种新的训练范式，旨在解决视觉-语言-动作模型在将高级视觉-语言理解与低级机器人物理动作对齐时存在的时空鸿沟问题。现有方法直接微调视觉-语言模型，但面临数据效率低下和对人工标注依赖性强的问题。ROSA通过引入一种自动收集的机器人状态估计数据作为辅助监督，增强模型对3D空间的理解和自我感知能力，从而提升模型在数据有限情况下的性能和泛化能力。实验在RLBench模拟环境和真实的WidowX机器人平台上验证了该方法的有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种名为ROSA的新颖训练范式，它利用机器人状态估计数据来实现视觉-语言和动作空间之间更好的对齐。</li><li>提出了一种简单而有效的解决方案来创建机器人状态估计数据，在不需要额外人类收集工作的情况下，显著增强了VLA的数据效率。</li><li>在RLBench模拟和一个真实世界的WidowX平台上进行了广泛的实验，证明ROSA有效增强了当前的VLA模型，并实现了优于先前方法的性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>ROSA方法的核心是将VLA模型的训练分解为两个互补部分：预测未来动作和估计当前机器人状态。</p>\n<ol><li><strong>训练数据</strong>：</li></ol>\n<p> * <strong>专家动作数据</strong>：由人类操作员收集的轨迹数据，包含视觉观测、语言指令和对应的7自由度机器人动作（3D位置、3D欧拉角、1个夹爪开合状态）。</p>\n<p> * <strong>机器人状态估计数据</strong>：通过自动化脚本控制机器人在预定义环境中随机移动并记录观测与状态。这些数据同样包含7自由度的状态信息，并配以统一的语言指令，如“What is the current state of the robot?”，以实现与专家数据的格式统一和联合训练。</p>\n<ol><li><strong>模型架构</strong>：</li></ol>\n<p> * 基于LLaVA架构，包含一个视觉编码器、一个投影器和一个大语言模型。</p>\n<p> * 视觉编码器使用CLIP ViT-L/14，投影器为两层MLP，大语言模型骨干为Qwen-2.5-7B。</p>\n<p> * 为了让LLM预测连续的动作和状态，采用线性量化的方法将连续值离散化为token，推理时再通过逆映射恢复为连续值。</p>\n<ol><li><strong>训练目标</strong>：</li></ol>\n<p> * 对专家动作数据和机器人状态数据使用统一的训练目标，即下一个token预测的交叉熵损失。</p>\n<p> * 模型以1:4的比例混合两种数据进行训练，并完全微调所有层。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> * <strong>RLBench</strong>：用于模拟实验，包含12个任务，每个任务包含多个训练变体。</p>\n<p> * <strong>真实机器人数据</strong>：在WidowX 250S机器人上采集，包括4个已见任务和4个用于泛化评估的未见任务。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> * 使用8块NVIDIA A100 GPU进行训练。</p>\n<p> * 基于Qwen-2.5-7B、CLIP ViT-L/14模型构建。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> * <strong>RLBench模拟环境</strong>：使用Franka Panda机器人，配备一个固定的前置RGB摄像头（336x336分辨率）。</p>\n<p> * <strong>真实世界环境</strong>：使用WidowX 250S机器人和一个Intel RealSense D435摄像头提供第三人称视角。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> * 主要评估指标是<strong>成功率</strong>，即任务成功的episode或试验次数占总数的百分比。</p>\n<p> * 在RLBench上，每个任务评估25个episode，并重复三次，报告平均成功率。</p>\n<p> * 在真实机器人上，每个任务评估10次试验。</p>"
  },
  {
    "date": "2025-06-15",
    "title": "SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration",
    "link": "http://arxiv.org/abs/2506.12723",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-11",
    "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.10100",
    "summary_markdown": "### 论文研究单位\n上海交通大学人工智能学院，哈尔滨工业大学，西安交通大学，电子科技大学\n### 论文概述\n论文提出EfficientVLA，一个无需训练的结构化推理加速框架，用于解决视觉-语言-动作（VLA）模型在推理阶段的计算和内存瓶颈。该方法通过系统性地消除多层冗余：基于层间相似性分析修剪语言模型中的冗余层；采用任务感知策略选择紧凑且多样化的视觉token；在扩散式动作头中缓存中间特征以减少时间冗余。实验表明，该方法在CogACT模型上实现1.93倍推理加速，FLOPs降至28.9%，成功率仅下降0.6%。\n### 论文核心贡献点\n1. 系统性分析扩散式VLA架构的计算瓶颈与多层冗余机制。\n2. 提出无需训练的EfficientVLA框架，集成三种冗余消除策略：语言层修剪、任务感知视觉token选择、动作头特征缓存。\n3. 设计时间相关性缓存机制，在扩散去噪过程中重用中间注意力与MLP计算。\n4. 在SIMPLER基准上验证有效性：实现1.93×加速，FLOPs降至28.9%，精度损失0.6%。\n### 论文方法描述\n1. **语言模型修剪**：通过计算层输入/输出隐藏状态的余弦相似度，定义重要性分数（公式1），修剪低分数的非连续层。\n2. **视觉token修剪**：量化任务相关性（基于视觉-语言交叉注意力分数），选择关键token后通过多样性增强（任务驱动+多样性驱动）平衡相关性与信息覆盖。\n3. **特征缓存**：利用DiT块中特征的时间连贯性，在扩散动作头中静态缓存N步中间特征，减少相邻步骤的重复计算。\n### 论文使用数据集和训练资源\n- **基础模型**：标准VLA模型CogACT\n- **评估数据集**：SIMPLER环境（仿真环境）的任务数据集\n- **训练资源**：方法无需额外训练，直接应用于预训练模型\n### 论文使用的评估环境和评估指标\n- **评估环境**：SIMPLER仿真环境\n- **评估指标**：推理速度（加速倍数）、FLOPs（计算量）、成功率（任务完成率）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学人工智能学院，哈尔滨工业大学，西安交通大学，电子科技大学</p>\n<h3>论文概述</h3>\n<p>论文提出EfficientVLA，一个无需训练的结构化推理加速框架，用于解决视觉-语言-动作（VLA）模型在推理阶段的计算和内存瓶颈。该方法通过系统性地消除多层冗余：基于层间相似性分析修剪语言模型中的冗余层；采用任务感知策略选择紧凑且多样化的视觉token；在扩散式动作头中缓存中间特征以减少时间冗余。实验表明，该方法在CogACT模型上实现1.93倍推理加速，FLOPs降至28.9%，成功率仅下降0.6%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>系统性分析扩散式VLA架构的计算瓶颈与多层冗余机制。</li><li>提出无需训练的EfficientVLA框架，集成三种冗余消除策略：语言层修剪、任务感知视觉token选择、动作头特征缓存。</li><li>设计时间相关性缓存机制，在扩散去噪过程中重用中间注意力与MLP计算。</li><li>在SIMPLER基准上验证有效性：实现1.93×加速，FLOPs降至28.9%，精度损失0.6%。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>语言模型修剪</strong>：通过计算层输入/输出隐藏状态的余弦相似度，定义重要性分数（公式1），修剪低分数的非连续层。</li><li><strong>视觉token修剪</strong>：量化任务相关性（基于视觉-语言交叉注意力分数），选择关键token后通过多样性增强（任务驱动+多样性驱动）平衡相关性与信息覆盖。</li><li><strong>特征缓存</strong>：利用DiT块中特征的时间连贯性，在扩散动作头中静态缓存N步中间特征，减少相邻步骤的重复计算。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>基础模型</strong>：标准VLA模型CogACT</li><li><strong>评估数据集</strong>：SIMPLER环境（仿真环境）的任务数据集</li><li><strong>训练资源</strong>：方法无需额外训练，直接应用于预训练模型</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：SIMPLER仿真环境</li><li><strong>评估指标</strong>：推理速度（加速倍数）、FLOPs（计算量）、成功率（任务完成率）</li></ul>"
  },
  {
    "date": "2025-06-11",
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.09937",
    "summary_markdown": "## 论文研究单位\n多伦多大学、多伦多大学机器人研究所、Vector研究所、丰田研究所。\n## 论文概述\n论文针对视觉-语言-动作（VLA）模型在部署于新任务时成功率有限的问题，提出了一个名为SAFE的多任务失败检测框架。SAFE旨在作为VLA等通用机器人策略的即插即用型失败检测器，通过分析VLA的内部特征来预测任务失败的可能性，从而允许机器人及时停止、回溯或寻求帮助。\n## 论文核心贡献点\n1. 通过分析VLA的潜在空间，发现不同任务中成功与失败轨迹的内部特征在特征空间中存在明显且一致的分离现象。\n2. 提出了SAFE框架，一个可扩展的、基于VLA内部特征的多任务失败检测方法。SAFE通过训练学习任务通用的失败表示，并使用保形预测（CP）校准阈值。\n3. 在多个VLA模型（OpenVLA, π₀, π₀-FAST）上进行了广泛的模拟和真实世界实验，并与多种基线方法比较，验证了SAFE的先进性能。\n## 论文方法描述\n1. **视觉分析**：使用t-SNE可视化VLA（如π₀-FAST）在LIBERO-10数据集上的特征，发现失败轨迹的特征会聚集在一个“失败区”，且该现象在不同任务间具有普遍性。\n2. **特征探测失败检测**：SAFE从VLA最后一层提取特征，并使用简单的MLP或LSTM网络将特征回归为一个标量失败分数。MLP模型对每个时间步的特征独立处理并累加分数，使用L1损失；LSTM模型则序列处理特征，使用二元交叉熵损失。\n3. **保形预测阈值选择**：采用功能型保形预测（CP），利用校准集上成功轨迹的分数分布，构建一个随时间变化的预测带。在测试时，若预测分数超出该带，则判定为失败，从而在保持低误报率的同时平衡检测的准确性与及时性。\n## 论文使用数据集和训练资源\n1. **数据集**：\n - 模拟：LIBERO-10（10个长时序任务）、SimplerEnv（基于RT系列和BridgeData V2的高保真环境）。\n - 真实世界：在Franka Emika Panda机器人上部署π₀-FAST-DROID模型，设计了13个任务，收集了30个成功和30个失败轨迹；在WidowX机器人上部署OpenVLA，收集了8个任务的532个轨迹。\n2. **训练资源**：论文未明确提及具体的硬件资源，但SAFE本身是一个轻量级模型（如LSTM含230万参数），引入的计算开销极小（<1毫秒）。\n## 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 在多个模拟环境（LIBERO-10, SimplerEnv）和真实世界机器人（Franka, WidowX）上对三种VLA模型（OpenVLA, π₀, π₀-FAST）进行评估。\n - 任务被划分为训练集（Seen任务）、验证集和测试集（Unseen任务）以评估跨任务的零样本泛化能力。\n2. **评估指标**：\n - ROC曲线下面积（ROC-AUC）：衡量所有可能阈值下区分成功与失败的平均性能。\n - 平衡准确率、真阳性率、假阳性率：用于使用保形预测阈值时的二分类性能评估。\n - 平均检测时间（T-det）：检测到失败的相对时间步，衡量检测的及时性。\n - 此外，通过可视化失败模式与人类判断的匹配程度进行定性评估。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<p>多伦多大学、多伦多大学机器人研究所、Vector研究所、丰田研究所。</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>论文针对视觉-语言-动作（VLA）模型在部署于新任务时成功率有限的问题，提出了一个名为SAFE的多任务失败检测框架。SAFE旨在作为VLA等通用机器人策略的即插即用型失败检测器，通过分析VLA的内部特征来预测任务失败的可能性，从而允许机器人及时停止、回溯或寻求帮助。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>通过分析VLA的潜在空间，发现不同任务中成功与失败轨迹的内部特征在特征空间中存在明显且一致的分离现象。</li><li>提出了SAFE框架，一个可扩展的、基于VLA内部特征的多任务失败检测方法。SAFE通过训练学习任务通用的失败表示，并使用保形预测（CP）校准阈值。</li><li>在多个VLA模型（OpenVLA, π₀, π₀-FAST）上进行了广泛的模拟和真实世界实验，并与多种基线方法比较，验证了SAFE的先进性能。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ol><li><strong>视觉分析</strong>：使用t-SNE可视化VLA（如π₀-FAST）在LIBERO-10数据集上的特征，发现失败轨迹的特征会聚集在一个“失败区”，且该现象在不同任务间具有普遍性。</li><li><strong>特征探测失败检测</strong>：SAFE从VLA最后一层提取特征，并使用简单的MLP或LSTM网络将特征回归为一个标量失败分数。MLP模型对每个时间步的特征独立处理并累加分数，使用L1损失；LSTM模型则序列处理特征，使用二元交叉熵损失。</li><li><strong>保形预测阈值选择</strong>：采用功能型保形预测（CP），利用校准集上成功轨迹的分数分布，构建一个随时间变化的预测带。在测试时，若预测分数超出该带，则判定为失败，从而在保持低误报率的同时平衡检测的准确性与及时性。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - 模拟：LIBERO-10（10个长时序任务）、SimplerEnv（基于RT系列和BridgeData V2的高保真环境）。</p>\n<p> - 真实世界：在Franka Emika Panda机器人上部署π₀-FAST-DROID模型，设计了13个任务，收集了30个成功和30个失败轨迹；在WidowX机器人上部署OpenVLA，收集了8个任务的532个轨迹。</p>\n<ol><li><strong>训练资源</strong>：论文未明确提及具体的硬件资源，但SAFE本身是一个轻量级模型（如LSTM含230万参数），引入的计算开销极小（<1毫秒）。</li></ol>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - 在多个模拟环境（LIBERO-10, SimplerEnv）和真实世界机器人（Franka, WidowX）上对三种VLA模型（OpenVLA, π₀, π₀-FAST）进行评估。</p>\n<p> - 任务被划分为训练集（Seen任务）、验证集和测试集（Unseen任务）以评估跨任务的零样本泛化能力。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - ROC曲线下面积（ROC-AUC）：衡量所有可能阈值下区分成功与失败的平均性能。</p>\n<p> - 平衡准确率、真阳性率、假阳性率：用于使用保形预测阈值时的二分类性能评估。</p>\n<p> - 平均检测时间（T-det）：检测到失败的相对时间步，衡量检测的及时性。</p>\n<p> - 此外，通过可视化失败模式与人类判断的匹配程度进行定性评估。</p>"
  },
  {
    "date": "2025-06-11",
    "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.09930",
    "summary_markdown": "### 论文研究单位\nNew York University\n### 论文概述\n本文提出了一个名为INT-ACT的视觉-语言-动作（VLA）模型泛化能力探测套件，包含50个模拟任务，覆盖10个子类别，旨在系统性地评估当前最先进VLA模型的泛化边界。研究发现，尽管VLA模型在语义理解（意图）方面表现出色，但在执行动作时存在显著差距，特别是在分布外（OOD）观察下，策略往往表现出连贯的意图，却在动作执行上失败。此外，在动作数据上的微调可能会削弱原始VLM的通用推理能力。论文发布了任务套件和评估代码，作为未来VLA模型的标准化基准。\n### 论文核心贡献点\n1. 提出并开源了INT-ACT，一个全面的VLA泛化探测套件，包含跨越3大类和10个子类别的50个任务，显著扩展了现有VLA基准的范围。\n2. 通过广泛的基准测试，揭示了当前最先进VLA模型的两个关键失败模式：\n - 持续且显著的意图-行动差距（Intention-Action Gap），即在分布偏移下强大的语义理解未能转化为可靠的执行。\n - 脆弱的多模态泛化能力，尤其是在语言变化和复合视觉-语言分布偏移下表现不佳。\n### 论文方法描述\n1. **测试平台选择**：基于SimplerEnv基准（构建于ManiSkill2模拟器之上），因其设计能紧密匹配模型在真实世界中的性能。\n2. **设计原则**：探测任务分为三个主要类别：\n - **对象多样性（Object Diversity）**：引入分布外对象，包括家居物品和工业工具，测试模型对新对象、外观和功能性的泛化能力。\n - **语言复杂性（Language Complexity）**：从模板化命令（如“把A放在B上”）扩展到组合性、知识和推理密集的指令，包括动作动词改写、语义否定和参考性外观描述。\n - **视觉-语言思维（Vision-Language Thinking）**：通过添加视觉干扰物和需要常识推理的干扰物（如橙子与橙汁盒），测试模型在复杂环境中的鲁棒性。\n3. **评估指标**：\n - **抓取成功率**：机器人夹爪是否成功抓取正确的源对象。\n - **意图正确率**：夹爪是否在任何一帧内移动到正确源对象的小半径内，捕获策略意图。\n - **任务成功率**：任务是否成功完成。\n### 论文使用数据集和训练资源\n1. **数据集**：主要基于BridgeV2数据集进行微调和评估，该数据集与SimplerEnv一致。\n2. **模型选择与训练协议**：评估了四种代表性模型（π₀、SpatialVLA、Magma、Octo）及其变体，所有微调实验均在BridgeV2上进行，遵循各自论文的训练和微调协议以确保公平比较。\n3. **训练资源**：使用了NYU IT高性能计算资源、服务和专业知识，并得到NSF资助（拨款号2238968, 2322242, 2026479）。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：完全在模拟环境中进行，基于ManiSkill2模拟器构建的SimplerEnv基准，确保部署的低门槛和可重复性。\n2. **评估指标**：\n - **抓取成功率（Grasp Success Rate）**：衡量夹爪成功抓取正确源对象的能力。\n - **意图正确率（Intention Correct Rate）**：新增指标，评估夹爪是否移动到正确源对象附近，即使后续抓取失败也记录意图。\n - **任务成功率（Task Success Rate）**：衡量任务整体完成情况。\n - **错误对象尝试率（Wrong Object Attempt Rate）**：在干扰物场景中监控是否移动了非源对象。\n3. **评估协议**：每个任务评估24个情节，对应所有预定义的场景和对象配置，每个配置重复3个随机种子，所有指标平均跨情节和种子计算。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>New York University</p>\n<h3>论文概述</h3>\n<p>本文提出了一个名为INT-ACT的视觉-语言-动作（VLA）模型泛化能力探测套件，包含50个模拟任务，覆盖10个子类别，旨在系统性地评估当前最先进VLA模型的泛化边界。研究发现，尽管VLA模型在语义理解（意图）方面表现出色，但在执行动作时存在显著差距，特别是在分布外（OOD）观察下，策略往往表现出连贯的意图，却在动作执行上失败。此外，在动作数据上的微调可能会削弱原始VLM的通用推理能力。论文发布了任务套件和评估代码，作为未来VLA模型的标准化基准。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出并开源了INT-ACT，一个全面的VLA泛化探测套件，包含跨越3大类和10个子类别的50个任务，显著扩展了现有VLA基准的范围。</li><li>通过广泛的基准测试，揭示了当前最先进VLA模型的两个关键失败模式：</li></ol>\n<p> - 持续且显著的意图-行动差距（Intention-Action Gap），即在分布偏移下强大的语义理解未能转化为可靠的执行。</p>\n<p> - 脆弱的多模态泛化能力，尤其是在语言变化和复合视觉-语言分布偏移下表现不佳。</p>\n<h3>论文方法描述</h3>\n<ol><li><strong>测试平台选择</strong>：基于SimplerEnv基准（构建于ManiSkill2模拟器之上），因其设计能紧密匹配模型在真实世界中的性能。</li><li><strong>设计原则</strong>：探测任务分为三个主要类别：</li></ol>\n<p> - <strong>对象多样性（Object Diversity）</strong>：引入分布外对象，包括家居物品和工业工具，测试模型对新对象、外观和功能性的泛化能力。</p>\n<p> - <strong>语言复杂性（Language Complexity）</strong>：从模板化命令（如“把A放在B上”）扩展到组合性、知识和推理密集的指令，包括动作动词改写、语义否定和参考性外观描述。</p>\n<p> - <strong>视觉-语言思维（Vision-Language Thinking）</strong>：通过添加视觉干扰物和需要常识推理的干扰物（如橙子与橙汁盒），测试模型在复杂环境中的鲁棒性。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>抓取成功率</strong>：机器人夹爪是否成功抓取正确的源对象。</p>\n<p> - <strong>意图正确率</strong>：夹爪是否在任何一帧内移动到正确源对象的小半径内，捕获策略意图。</p>\n<p> - <strong>任务成功率</strong>：任务是否成功完成。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：主要基于BridgeV2数据集进行微调和评估，该数据集与SimplerEnv一致。</li><li><strong>模型选择与训练协议</strong>：评估了四种代表性模型（π₀、SpatialVLA、Magma、Octo）及其变体，所有微调实验均在BridgeV2上进行，遵循各自论文的训练和微调协议以确保公平比较。</li><li><strong>训练资源</strong>：使用了NYU IT高性能计算资源、服务和专业知识，并得到NSF资助（拨款号2238968, 2322242, 2026479）。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：完全在模拟环境中进行，基于ManiSkill2模拟器构建的SimplerEnv基准，确保部署的低门槛和可重复性。</li><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>抓取成功率（Grasp Success Rate）</strong>：衡量夹爪成功抓取正确源对象的能力。</p>\n<p> - <strong>意图正确率（Intention Correct Rate）</strong>：新增指标，评估夹爪是否移动到正确源对象附近，即使后续抓取失败也记录意图。</p>\n<p> - <strong>任务成功率（Task Success Rate）</strong>：衡量任务整体完成情况。</p>\n<p> - <strong>错误对象尝试率（Wrong Object Attempt Rate）</strong>：在干扰物场景中监控是否移动了非源对象。</p>\n<ol><li><strong>评估协议</strong>：每个任务评估24个情节，对应所有预定义的场景和对象配置，每个配置重复3个随机种子，所有指标平均跨情节和种子计算。</li></ol>"
  },
  {
    "date": "2025-06-11",
    "title": "OctoNav: Towards Generalist Embodied Navigation",
    "link": "http://arxiv.org/abs/2506.09839",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-10",
    "title": "An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models",
    "link": "http://arxiv.org/abs/2506.09172",
    "summary_markdown": "### 论文研究单位\nPranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Jaewoo Song, Harshvardhan Sikka\n### 论文概述\n论文介绍了 MultiNet，一个全新的、完全开源的基准测试和配套的软件生态系统，旨在严格评估和适应视觉、语言和动作领域的模型。该工作建立了标准化的评估协议来评估视觉语言模型（VLMs）和视觉-语言-动作模型（VLAs），并提供开源软件以下载相关数据、模型和进行评估。此外，论文提供了一个包含超过1.3万亿个令牌的复合数据集，涵盖了图像字幕、视觉问答、常识推理、机器人控制、数字游戏、模拟运动/操作等多种任务。MultiNet 的基准、框架、工具包和评估框架已被用于下游研究，探讨VLA泛化的局限性。\n### 论文核心贡献点\n1. **大规模通用数据集**：发布了一个庞大的开源数据集，整合了视觉、语言和动作等多个领域的不同数据源，适用于训练和评估通用模型。\n2. **开源数据整理SDK**：提供了一个开源的软件工具包，方便访问整合后的数据集，并将来自不同来源的控制数据（强化学习和机器人数据）标准化为通用、易于访问的格式。\n3. **系统性评估框架**：引入了一个标准化的、设计合理的评估方法，包括测试集拆分和精心设计的指标，用于评估最先进的VLMs和VLAs在熟悉和全新领域（包括真实世界机器人任务和过程生成游戏环境）中的泛化能力。\n4. **最先进模型的开源适配**：开源了对最先进的VLMs和VLAs的适配，使其能够有效地处理MultiNet中的数据格式和多样化领域，甚至是那些在其原始训练中未见过的领域。\n5. **深入的实验与分析**：利用MultiNet的基准、框架和评估工具，对领先的VLMs、VLAs和新兴通用模型的性能进行了评估和分析。\n### 论文方法描述\n1. **开源数据集SDK**：提供了一个代码库，可以无缝下载集合中的任何或所有数据集，并包含一个用于标准化机器人和强化学习数据的工具包。该工具包将不同格式的控制数据转换为统一的TensorFlow数据集格式。\n2. **评估框架**：为集合中的数据集引入了系统性的测试集拆分，以防止数据污染。同时，设计了一套评估指标套件，用于公平、可量化地捕捉模型的性能。\n3. **通用提示框架**：提出了一个名为GenESIS的开源模块化框架，用于简化将不同VLMs集成到各种任务和数据集中的过程。该框架基于可互换性、抽象性、封装性和提示工程等核心原则，包含系统级指令、任务和环境上下文、多模态输入集成、动作空间定义和输出指令等关键元素。\n4. **模型适配**：对多个最先进的VLA和VLM模型进行了架构和后处理适配，使其能够处理离线机器人数据集和过程生成的离散动作环境等OOD领域。具体适配的模型包括JAT、OpenVLA、Pi0 Base和Pi0 Fast，通过处理模型输入输出、架构更改和推理管道来适应特定领域的结构和统计特性。\n### 论文使用数据集和训练资源\n1. **数据集**：论文整合了一个大规模、多样化的数据集集合，包含超过1.3万亿个令牌。数据集分为三类：\n * **视觉语言**：占比29%，包括OBELICS、DataComp-1B、COYO-700M、MS-COCO Captions、Conceptual Captions、VQA-V2等。\n * **语言**：占比13%，包括Fineweb-edu、HellaSwag、ARC、CommonsenseQA、MMLU等。\n * **控制**：占比58%，主要包括OpenX-Embodiment，以及DM Lab、ALE Atari、BabyAI、MuJoCo、Meta-World、Procgen等。\n2. **训练资源**：论文本身是关于基准测试和工具包，因此未详细说明训练特定模型所需的计算资源（如GPU类型和数量）。但是，研究中评估了如GPT-4o、JAT、OpenVLA、Pi0等现有模型，这些模型的原始训练资源在其各自的论文中有描述。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n * **分布式机器人环境**：使用来自OpenX-Embodiment数据集的离线轨迹进行评估，测试模型在真实机器人操作和运动任务上的泛化能力。\n * **分布式过程生成环境**：使用如Procgen等程序生成的游戏环境进行评估，测试模型在数字世界中面对全新、动态场景时的泛化能力。\n * 评估采用了系统性的测试集拆分，以确保基准测试的可靠性并防止数据污染。\n2. **评估指标**：\n * **强化学习与机器人**：均方误差（MSE）、布里尔平均绝对误差（Brier MAE）、精确率、召回率、F1分数、无效输出百分比。\n * **图像描述与检索**：CIDEr（用于图像描述和基于图像的文本检索）、Recall@K（用于图像理解和基于文本的图像检索）。\n * **视觉问答与文本理解**：VQA准确率、准确率（用于常识推理和文本理解）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Jaewoo Song, Harshvardhan Sikka</p>\n<h3>论文概述</h3>\n<p>论文介绍了 MultiNet，一个全新的、完全开源的基准测试和配套的软件生态系统，旨在严格评估和适应视觉、语言和动作领域的模型。该工作建立了标准化的评估协议来评估视觉语言模型（VLMs）和视觉-语言-动作模型（VLAs），并提供开源软件以下载相关数据、模型和进行评估。此外，论文提供了一个包含超过1.3万亿个令牌的复合数据集，涵盖了图像字幕、视觉问答、常识推理、机器人控制、数字游戏、模拟运动/操作等多种任务。MultiNet 的基准、框架、工具包和评估框架已被用于下游研究，探讨VLA泛化的局限性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>大规模通用数据集</strong>：发布了一个庞大的开源数据集，整合了视觉、语言和动作等多个领域的不同数据源，适用于训练和评估通用模型。</li><li><strong>开源数据整理SDK</strong>：提供了一个开源的软件工具包，方便访问整合后的数据集，并将来自不同来源的控制数据（强化学习和机器人数据）标准化为通用、易于访问的格式。</li><li><strong>系统性评估框架</strong>：引入了一个标准化的、设计合理的评估方法，包括测试集拆分和精心设计的指标，用于评估最先进的VLMs和VLAs在熟悉和全新领域（包括真实世界机器人任务和过程生成游戏环境）中的泛化能力。</li><li><strong>最先进模型的开源适配</strong>：开源了对最先进的VLMs和VLAs的适配，使其能够有效地处理MultiNet中的数据格式和多样化领域，甚至是那些在其原始训练中未见过的领域。</li><li><strong>深入的实验与分析</strong>：利用MultiNet的基准、框架和评估工具，对领先的VLMs、VLAs和新兴通用模型的性能进行了评估和分析。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>开源数据集SDK</strong>：提供了一个代码库，可以无缝下载集合中的任何或所有数据集，并包含一个用于标准化机器人和强化学习数据的工具包。该工具包将不同格式的控制数据转换为统一的TensorFlow数据集格式。</li><li><strong>评估框架</strong>：为集合中的数据集引入了系统性的测试集拆分，以防止数据污染。同时，设计了一套评估指标套件，用于公平、可量化地捕捉模型的性能。</li><li><strong>通用提示框架</strong>：提出了一个名为GenESIS的开源模块化框架，用于简化将不同VLMs集成到各种任务和数据集中的过程。该框架基于可互换性、抽象性、封装性和提示工程等核心原则，包含系统级指令、任务和环境上下文、多模态输入集成、动作空间定义和输出指令等关键元素。</li><li><strong>模型适配</strong>：对多个最先进的VLA和VLM模型进行了架构和后处理适配，使其能够处理离线机器人数据集和过程生成的离散动作环境等OOD领域。具体适配的模型包括JAT、OpenVLA、Pi0 Base和Pi0 Fast，通过处理模型输入输出、架构更改和推理管道来适应特定领域的结构和统计特性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：论文整合了一个大规模、多样化的数据集集合，包含超过1.3万亿个令牌。数据集分为三类：</li></ol>\n<p> * <strong>视觉语言</strong>：占比29%，包括OBELICS、DataComp-1B、COYO-700M、MS-COCO Captions、Conceptual Captions、VQA-V2等。</p>\n<p> * <strong>语言</strong>：占比13%，包括Fineweb-edu、HellaSwag、ARC、CommonsenseQA、MMLU等。</p>\n<p> * <strong>控制</strong>：占比58%，主要包括OpenX-Embodiment，以及DM Lab、ALE Atari、BabyAI、MuJoCo、Meta-World、Procgen等。</p>\n<ol><li><strong>训练资源</strong>：论文本身是关于基准测试和工具包，因此未详细说明训练特定模型所需的计算资源（如GPU类型和数量）。但是，研究中评估了如GPT-4o、JAT、OpenVLA、Pi0等现有模型，这些模型的原始训练资源在其各自的论文中有描述。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> * <strong>分布式机器人环境</strong>：使用来自OpenX-Embodiment数据集的离线轨迹进行评估，测试模型在真实机器人操作和运动任务上的泛化能力。</p>\n<p> * <strong>分布式过程生成环境</strong>：使用如Procgen等程序生成的游戏环境进行评估，测试模型在数字世界中面对全新、动态场景时的泛化能力。</p>\n<p> * 评估采用了系统性的测试集拆分，以确保基准测试的可靠性并防止数据污染。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> * <strong>强化学习与机器人</strong>：均方误差（MSE）、布里尔平均绝对误差（Brier MAE）、精确率、召回率、F1分数、无效输出百分比。</p>\n<p> * <strong>图像描述与检索</strong>：CIDEr（用于图像描述和基于图像的文本检索）、Recall@K（用于图像理解和基于文本的图像检索）。</p>\n<p> * <strong>视觉问答与文本理解</strong>：VQA准确率、准确率（用于常识推理和文本理解）。</p>"
  },
  {
    "date": "2025-06-10",
    "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency",
    "link": "http://arxiv.org/abs/2506.08822",
    "summary_markdown": "### 论文研究单位\n\n北京类人机器人创新中心 (Beijing Innovation Center of Humanoid Robotics)\n中国科学院自动化研究所模式识别国家重点实验室 (NLPR, MAIS, Institute of Automation of Chinese Academy of Sciences)\n### 论文概述\n\n论文提出了FreqPolicy，一种新颖的基于流的视觉运动策略，旨在通过频率一致性约束实现高效的单步动作生成。该方法首次在基于流的视觉运动策略中引入时间知识，以解决机器人操作中动作轨迹的时间依赖性问题。通过在频域中对齐不同时间步的动作特征，并结合自适应频率分量损失，FreqPolicy在保持生成动作质量的同时显著提高了推理速度。\n### 论文核心贡献点\n\n1. 首个利用时间知识进行机器人操作的单步视觉运动策略。\n2. 借鉴时间序列和语音处理领域，提出频率一致性约束目标以增强任意两个动作速度的规律化，并提出自适应频率分量损失以有效捕捉动作序列的结构性时间变化。\n3. 在3个模拟基准的53个任务上进行广泛实验，证明其优于现有单步动作生成器，例如在MetaWorld上达到84.2%的成功率。\n4. 将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在不损失任务性能的情况下显著提升推理速度（例如，5倍加速）。\n### 论文方法描述\n\nFreqPolicy基于流匹配框架，通过学习一个时间依赖的向量场将先验分布（如高斯噪声）转换到目标动作分布。核心创新在于引入频率一致性约束，强制在频域中对齐沿流路径不同时间步的动作速度，从而促进单步动作生成向目标分布收敛。此外，设计了一种自适应频率分量损失，动态强调具有更大差异的频率分量，以捕捉机器人操作任务中固有的结构性时间变化。模型支持2D图像和3D点云输入，输出动作块的速度向量。\n### 论文使用数据集和训练资源\n\n数据集: 在3个模拟基准（MetaWorld、Robomimic、Libero）的53个任务上进行评估。\n训练资源: 未明确指定硬件细节，但提及模型训练和推理在标准GPU环境下进行，其中现实世界实验达到93.5 Hz的推理频率。\n### 论文使用的评估环境和评估指标\n\n评估环境: 模拟环境（MetaWorld、Robomimic、Libero）和真实世界机器人场景。\n评估指标: 任务成功率（如MetaWorld上的平均成功率）、推理速度（Hz）、与基线方法（如FlowPolicy、CP、ManiCM）的性能比较，以及在VLA模型集成后的任务完成率和推理加速比。",
    "summary_html": "<h3>论文研究单位</h3>\n\n<p>北京类人机器人创新中心 (Beijing Innovation Center of Humanoid Robotics)</p>\n<p>中国科学院自动化研究所模式识别国家重点实验室 (NLPR, MAIS, Institute of Automation of Chinese Academy of Sciences)</p>\n<h3>论文概述</h3>\n\n<p>论文提出了FreqPolicy，一种新颖的基于流的视觉运动策略，旨在通过频率一致性约束实现高效的单步动作生成。该方法首次在基于流的视觉运动策略中引入时间知识，以解决机器人操作中动作轨迹的时间依赖性问题。通过在频域中对齐不同时间步的动作特征，并结合自适应频率分量损失，FreqPolicy在保持生成动作质量的同时显著提高了推理速度。</p>\n<h3>论文核心贡献点</h3>\n\n<ol><li>首个利用时间知识进行机器人操作的单步视觉运动策略。</li><li>借鉴时间序列和语音处理领域，提出频率一致性约束目标以增强任意两个动作速度的规律化，并提出自适应频率分量损失以有效捕捉动作序列的结构性时间变化。</li><li>在3个模拟基准的53个任务上进行广泛实验，证明其优于现有单步动作生成器，例如在MetaWorld上达到84.2%的成功率。</li><li>将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在不损失任务性能的情况下显著提升推理速度（例如，5倍加速）。</li></ol>\n<h3>论文方法描述</h3>\n\n<p>FreqPolicy基于流匹配框架，通过学习一个时间依赖的向量场将先验分布（如高斯噪声）转换到目标动作分布。核心创新在于引入频率一致性约束，强制在频域中对齐沿流路径不同时间步的动作速度，从而促进单步动作生成向目标分布收敛。此外，设计了一种自适应频率分量损失，动态强调具有更大差异的频率分量，以捕捉机器人操作任务中固有的结构性时间变化。模型支持2D图像和3D点云输入，输出动作块的速度向量。</p>\n<h3>论文使用数据集和训练资源</h3>\n\n<p>数据集: 在3个模拟基准（MetaWorld、Robomimic、Libero）的53个任务上进行评估。</p>\n<p>训练资源: 未明确指定硬件细节，但提及模型训练和推理在标准GPU环境下进行，其中现实世界实验达到93.5 Hz的推理频率。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n\n<p>评估环境: 模拟环境（MetaWorld、Robomimic、Libero）和真实世界机器人场景。</p>\n<p>评估指标: 任务成功率（如MetaWorld上的平均成功率）、推理速度（Hz）、与基线方法（如FlowPolicy、CP、ManiCM）的性能比较，以及在VLA模型集成后的任务完成率和推理加速比。</p>"
  },
  {
    "date": "2025-06-10",
    "title": "Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing",
    "link": "http://arxiv.org/abs/2506.08462",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-10",
    "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization",
    "link": "http://arxiv.org/abs/2506.08440",
    "summary_markdown": "### 论文研究单位\n吉林大学人工智能学院\n### 论文概述\n论文针对视觉-语言-动作模型主要依赖成功演示数据训练，在分布外场景下泛化能力有限的问题，提出了一种基于在线强化学习的VLA模型微调框架TGRPO。该方法利用大语言模型自动构建多阶段密集奖励函数，并结合一种基于分组的策略优化算法，通过相对比较降低训练方差，从而提升模型在复杂长时程任务中的性能。\n### 论文核心贡献点\n1. 提出了一个面向VLA模型的在线强化学习训练框架，使其能够通过与环境的交互从失败中学习，克服了仅依赖成功演示的根本限制。\n2. 设计了TGRPO算法，该算法是一种新颖的基于分组的策略优化方法，融合了轨迹级和步级优势估计，以更好地捕获长时程机器人任务的结构，改善了信誉分配并增强了策略稳定性。\n3. 强调了密集奖励设计与基于分组优化的协同作用，利用LLM解析任务并生成阶段性密集奖励，再结合分组策略，显著提升了强化学习在复杂机器人任务中训练VLA模型的效果。\n### 论文方法描述\n该方法包含两部分。首先，利用大语言模型（如Claude 3.7 Sonnet）将自然语言指令分解为子任务，并结合环境中关键物体位置与预采集的成功演示中的机器人末端姿态，设计一个多阶段的密集奖励函数，为每个步骤提供细粒度的反馈。其次，提出轨迹分组相对策略优化算法（TGRPO），算法并行采样多条轨迹，然后将这些轨迹在步级和轨迹级两个维度进行分组，分别计算步级相对优势和轨迹级相对优势。通过加权融合这两种优势，得到最终的相对优势信号，并用于一个类似于PPO的裁剪目标函数中，以无价值网络的方式更新VLA模型策略。\n### 论文使用数据集和训练资源\n实验使用LIBERO机器人模拟器基准进行评估，该基准包含Spatial、Object、Goal、Long四个任务套件。基础模型采用OpenVLA，并使用LoRA进行微调。所有实验均在单块NVIDIA A100 GPU上进行。\n### 论文使用的评估环境和评估指标\n评估环境为LIBERO模拟器。评估指标为任务成功率，具体为在每个任务上运行50个测试回合，报告每个任务和每个任务套件的平均成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>吉林大学人工智能学院</p>\n<h3>论文概述</h3>\n<p>论文针对视觉-语言-动作模型主要依赖成功演示数据训练，在分布外场景下泛化能力有限的问题，提出了一种基于在线强化学习的VLA模型微调框架TGRPO。该方法利用大语言模型自动构建多阶段密集奖励函数，并结合一种基于分组的策略优化算法，通过相对比较降低训练方差，从而提升模型在复杂长时程任务中的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个面向VLA模型的在线强化学习训练框架，使其能够通过与环境的交互从失败中学习，克服了仅依赖成功演示的根本限制。</li><li>设计了TGRPO算法，该算法是一种新颖的基于分组的策略优化方法，融合了轨迹级和步级优势估计，以更好地捕获长时程机器人任务的结构，改善了信誉分配并增强了策略稳定性。</li><li>强调了密集奖励设计与基于分组优化的协同作用，利用LLM解析任务并生成阶段性密集奖励，再结合分组策略，显著提升了强化学习在复杂机器人任务中训练VLA模型的效果。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法包含两部分。首先，利用大语言模型（如Claude 3.7 Sonnet）将自然语言指令分解为子任务，并结合环境中关键物体位置与预采集的成功演示中的机器人末端姿态，设计一个多阶段的密集奖励函数，为每个步骤提供细粒度的反馈。其次，提出轨迹分组相对策略优化算法（TGRPO），算法并行采样多条轨迹，然后将这些轨迹在步级和轨迹级两个维度进行分组，分别计算步级相对优势和轨迹级相对优势。通过加权融合这两种优势，得到最终的相对优势信号，并用于一个类似于PPO的裁剪目标函数中，以无价值网络的方式更新VLA模型策略。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>实验使用LIBERO机器人模拟器基准进行评估，该基准包含Spatial、Object、Goal、Long四个任务套件。基础模型采用OpenVLA，并使用LoRA进行微调。所有实验均在单块NVIDIA A100 GPU上进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境为LIBERO模拟器。评估指标为任务成功率，具体为在每个任务上运行50个测试回合，报告每个任务和每个任务套件的平均成功率。</p>"
  },
  {
    "date": "2025-06-09",
    "title": "HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation",
    "link": "http://arxiv.org/abs/2506.08296",
    "summary_markdown": "### 论文研究单位\n约翰霍普金斯大学、意大利技术研究院、多伦多大学、哈佛大学麻省总医院\n### 论文概述\n本文提出了HiBerNAC（Hierarchical Brain-emulated Robotic Neural Agent Collective），一个受神经科学启发的多智能体框架，用于解决复杂机器人操作任务。该框架结合了多模态视觉-语言-动作（VLA）模型规划与神经启发的反思及多智能体机制，旨在克服现有方法在长视野任务、动态环境适应和实时性能方面的局限性。通过在7自由度Franka机器人上的实验，HiBerNAC在长视野任务平均完成时间上减少23%，在多路径任务上实现了12-31%的非零成功率。\n### 论文核心贡献点\n1. 提出了HiBerNAC框架，首次将神经科学的分层决策机制与多智能体协作结合，用于机器人操作。\n2. 设计了多频率异步处理架构，包括0.01Hz的高层规划、0.1Hz的海马体记忆和100Hz的实时反应式VLA控制。\n3. 实现了模块化神经智能体结构，包括前额叶规划器（PFP）、感知智能体（PA）、语义智能体（SA）、操作智能体（MA）和海马体模块（HM）。\n4. 开发了基于DAG的异步管道（DBHTN），支持分层任务管理和动态资源分配。\n### 论文方法描述\nHiBerNAC采用三层架构：\n1. 多智能体神经结构：模拟人脑神经分工，PFP（0.01Hz）分解高级任务，SA处理语言指令，MA执行动作，IA提供误差校正，HM（0.1Hz）管理情景记忆。智能体间通过连接矩阵F_ij通信。\n2. 异步管道：实现DBHTN规划器，将任务分解为状态和转换，使用潜在向量l_t、动作历史h_a和语义记忆m_s。状态审查机制以0.1Hz运行。\n3. 反应式VLA（RVLA）：基于开源预训练模型，以100Hz处理低级控制，用于简单反应任务。\n\n系统使用数学公式建模智能体交互：o_t^i = A_i(x_t^i, s_t^i, m_t; Θ_i) + ∑_{j≠i}F_ij · o_{t-1}^j，其中A_i为智能体特定函数，F_ij为连接矩阵。\n### 论文使用数据集和训练资源\n数据集：Open X-Embodiment数据集\n训练资源：未明确指定硬件，但提及使用预训练VLA模型（如OpenVLA）和神经网络模块。实验在7自由度Franka机器人上进行，包括仿真和真实世界测试。\n### 论文使用的评估环境和评估指标\n评估环境：Isaac Gym仿真环境和真实世界的7自由度Franka机器人平台。\n评估指标：1. 任务成功率（多路径任务的非零成功率） 2. 平均任务完成时间（长视野任务） 3. 动态环境适应性（遮挡/动态删除场景） 4. 多模态任务成功率（语言-视觉-动作一致性）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>约翰霍普金斯大学、意大利技术研究院、多伦多大学、哈佛大学麻省总医院</p>\n<h3>论文概述</h3>\n<p>本文提出了HiBerNAC（Hierarchical Brain-emulated Robotic Neural Agent Collective），一个受神经科学启发的多智能体框架，用于解决复杂机器人操作任务。该框架结合了多模态视觉-语言-动作（VLA）模型规划与神经启发的反思及多智能体机制，旨在克服现有方法在长视野任务、动态环境适应和实时性能方面的局限性。通过在7自由度Franka机器人上的实验，HiBerNAC在长视野任务平均完成时间上减少23%，在多路径任务上实现了12-31%的非零成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了HiBerNAC框架，首次将神经科学的分层决策机制与多智能体协作结合，用于机器人操作。</li><li>设计了多频率异步处理架构，包括0.01Hz的高层规划、0.1Hz的海马体记忆和100Hz的实时反应式VLA控制。</li><li>实现了模块化神经智能体结构，包括前额叶规划器（PFP）、感知智能体（PA）、语义智能体（SA）、操作智能体（MA）和海马体模块（HM）。</li><li>开发了基于DAG的异步管道（DBHTN），支持分层任务管理和动态资源分配。</li></ol>\n<h3>论文方法描述</h3>\n<p>HiBerNAC采用三层架构：</p>\n<ol><li>多智能体神经结构：模拟人脑神经分工，PFP（0.01Hz）分解高级任务，SA处理语言指令，MA执行动作，IA提供误差校正，HM（0.1Hz）管理情景记忆。智能体间通过连接矩阵F_ij通信。</li><li>异步管道：实现DBHTN规划器，将任务分解为状态和转换，使用潜在向量l_t、动作历史h_a和语义记忆m_s。状态审查机制以0.1Hz运行。</li><li>反应式VLA（RVLA）：基于开源预训练模型，以100Hz处理低级控制，用于简单反应任务。</li></ol>\n\n<p>系统使用数学公式建模智能体交互：o_t^i = A_i(x_t^i, s_t^i, m_t; Θ_i) + ∑_{j≠i}F_ij · o_{t-1}^j，其中A_i为智能体特定函数，F_ij为连接矩阵。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：Open X-Embodiment数据集</p>\n<p>训练资源：未明确指定硬件，但提及使用预训练VLA模型（如OpenVLA）和神经网络模块。实验在7自由度Franka机器人上进行，包括仿真和真实世界测试。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：Isaac Gym仿真环境和真实世界的7自由度Franka机器人平台。</p>\n<p>评估指标：1. 任务成功率（多路径任务的非零成功率） 2. 平均任务完成时间（长视野任务） 3. 动态环境适应性（遮挡/动态删除场景） 4. 多模态任务成功率（语言-视觉-动作一致性）</p>"
  },
  {
    "date": "2025-06-09",
    "title": "Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework",
    "link": "http://arxiv.org/abs/2506.08185",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-09",
    "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models",
    "link": "http://arxiv.org/abs/2506.07961",
    "summary_markdown": "### 论文研究单位\nCASIA, ByteDance Seed, UCAS, FiveAges, NJU.\n### 论文概述\n论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。\n### 论文核心贡献点\n1. 提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。\n2. 提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。\n3. 在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。\n### 论文方法描述\nBridgeVLA采用两阶段训练策略：\n1. 2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。\n2. 3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：RoboPoint的120K对象检测分割。\n - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。\n- **训练资源**：\n - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。\n - 训练采用AdamW优化器，具体超参数见附录。\n - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。\n - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。\n- **评估指标**：\n - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。\n - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。\n - 样本效率：在仅3条轨迹/任务训练下的成功率。</think>\n### 论文研究单位\nCASIA, ByteDance Seed, UCAS, FiveAges, NJU\n### 论文概述\n论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。\n### 论文核心贡献点\n1. 提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。\n2. 提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。\n3. 在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。\n### 论文方法描述\nBridgeVLA采用两阶段训练策略：\n1. 2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。\n2. 3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：RoboPoint的120K对象检测分割。\n - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。\n- **训练资源**：\n - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。\n - 训练采用AdamW优化器，具体超参数见附录。\n - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。\n - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。\n- **评估指标**：\n - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。\n - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。\n - 样本效率：在仅3条轨迹/任务训练下的成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>CASIA, ByteDance Seed, UCAS, FiveAges, NJU.</p>\n<h3>论文概述</h3>\n<p>论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。</li><li>提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。</li><li>在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。</li></ol>\n<h3>论文方法描述</h3>\n<p>BridgeVLA采用两阶段训练策略：</p>\n<ol><li>2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。</li><li>3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：RoboPoint的120K对象检测分割。</p>\n<p> - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。</p>\n<p> - 训练采用AdamW优化器，具体超参数见附录。</p>\n<p> - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。</p>\n<p> - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。</p>\n<p> - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。</p>\n<p> - 样本效率：在仅3条轨迹/任务训练下的成功率。</think></p>\n<h3>论文研究单位</h3>\n<p>CASIA, ByteDance Seed, UCAS, FiveAges, NJU</p>\n<h3>论文概述</h3>\n<p>论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。</li><li>提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。</li><li>在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。</li></ol>\n<h3>论文方法描述</h3>\n<p>BridgeVLA采用两阶段训练策略：</p>\n<ol><li>2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。</li><li>3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：RoboPoint的120K对象检测分割。</p>\n<p> - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。</p>\n<p> - 训练采用AdamW优化器，具体超参数见附录。</p>\n<p> - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。</p>\n<p> - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。</p>\n<p> - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。</p>\n<p> - 样本效率：在仅3条轨迹/任务训练下的成功率。</p>"
  },
  {
    "date": "2025-06-09",
    "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
    "link": "http://arxiv.org/abs/2506.07639",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-09",
    "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
    "link": "http://arxiv.org/abs/2506.07530",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-09",
    "title": "Real-Time Execution of Action Chunking Flow Policies",
    "link": "http://arxiv.org/abs/2506.07339",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-08",
    "title": "Robotic Policy Learning via Human-assisted Action Preference Optimization",
    "link": "http://arxiv.org/abs/2506.07127",
    "summary_markdown": "```markdown\n### 论文研究单位\n中国人民大学高瓴人工智能学院、ByteDance Seed、下一代智能搜索与推荐教育部工程研究中心、大型模型与智能治理北京市重点实验室。\n### 论文概述\n该论文提出了一种名为“动作偏好优化（APO）”的新方法，用于通过人类辅助的偏好对齐来优化视觉-语言-动作（VLA）模型。该方法旨在解决VLA模型在部署后难以通过失败经验进行持续优化的局限性。APO包含一个用于可靠部署和交互轨迹收集的人-机器人协作框架，以及一个利用二元可取性信号和自适应重新加权算法的优化过程。该算法能有效抑制失败动作并增强纠错能力，使VLA模型能够从失败中学习，实现迭代改进，从而在动态环境中可靠部署。实验在模拟和真实场景中验证了该方法的有效性。\n### 论文核心贡献点\n1. 提出了一种新的VLA模型优化范式——动作偏好优化（APO），它通过动作级别的偏好对齐，超越了行为克隆和强化学习的限制。\n2. 设计了一个用于可靠部署和交互轨迹收集的人-机器人协作框架，通过实时人工干预保证任务成功并提供修正轨迹。\n3. 针对机器人交互的不可逆性和VLA模型中动作标记与连续动作之间的概率失配问题，提出了一种自适应重新加权算法。该算法利用从交互中派生的二元可取性信号，引导模型有效学习。\n4. APO使VLA模型具备了从失败中学习的关键能力，为机器人在真实世界中的持续迭代优化铺平了道路。\n5. 在多种模拟和真实世界的操作任务上进行了全面实验，证明了APO在泛化性和鲁棒性方面的优越性能。\n### 论文方法描述\n该方法分为两个主要部分：\n1. **人-机器人协作部署**：\n - 首先，通过行为克隆（BC）在专家演示数据集上微调预训练的VLA模型，得到一个基础策略。\n - 部署该策略与环境交互。当策略执行失败时，人工操作员进行实时干预，确保任务完成并收集修正轨迹。\n - 对交互轨迹进行标注：将干预前K步内的动作标记为“不可取”，而人类干预和策略成功执行的动作标记为“可取”。\n - 将连续动作离散化为动作标记。\n2. **动作偏好优化（APO）**：\n - 针对不可逆交互和动作标记概率失配两个挑战，构建了一个基于前景理论的效用函数，该函数从二元可取性信号（而非配对偏好）中学习。\n - 采用一个损失函数，通过KL散度项约束优化过程，使模型在参考模型的基础上学习偏好对齐，避免灾难性遗忘。\n - 提出一种自适应重新加权方法：计算每个样本的连续动作L1损失并进行批次归一化，得到样本权重。然后动态调整可取和不可取样本在损失函数中的权重，使模型更关注预测误差大的样本（即失败相关的动作），从而弥合了离散标记预测和连续动作回归之间的差距。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **模拟**：使用RoboMimic数据集，包含`Coffee_D0`、`StackThree_D0`、`ThreePieceAssembly_D0`和`Square_D0`等4个长时程操作任务。每个任务使用300个专家演示进行初始微调，并收集50条交互轨迹用于APO优化。\n - **真实世界**：设计了“将方块插入杆中”的精细操作任务。通过SpaceMouse设备收集了100条专家演示轨迹，并部署策略后收集了20条人工干预轨迹。此外，还在“挂杯子”和“放柠檬”等任务上进行了验证。\n- **训练资源**：\n - **基础模型**：主要对OpenVLA模型进行微调，并在π0-FAST模型上验证了泛化性。\n - **硬件**：初始微调使用8块NVIDIA A100 GPU（LoRA, rank=32, batch size=16），APO优化阶段使用4块NVIDIA A100 GPU（learning rate=5e-5, batch size=8）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：RoboMimic模拟环境。\n - **真实世界环境**：使用真实机械臂执行操作任务。为了测试泛化能力，设计了多种扰动场景，包括位置扰动（改变物体初始位置）、背景扰动（更换背景颜色）和纹理扰动（更换物体纹理）。\n- **评估指标**：\n - 主要评估指标是**任务平均成功率**。在模拟环境中，每个任务在不同随机种子下进行50次试验并报告平均成功率。在真实世界中，对每种场景进行20次试验并报告平均成功率。\n - 在终身学习实验中，每20次交互后更新模型并进行50次评估，绘制成功率随迭代次数的变化曲线。\n - 通过比较不同方法在原始任务和扰动场景下的性能，评估模型的泛化和抗干扰能力。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>中国人民大学高瓴人工智能学院、ByteDance Seed、下一代智能搜索与推荐教育部工程研究中心、大型模型与智能治理北京市重点实验室。</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种名为“动作偏好优化（APO）”的新方法，用于通过人类辅助的偏好对齐来优化视觉-语言-动作（VLA）模型。该方法旨在解决VLA模型在部署后难以通过失败经验进行持续优化的局限性。APO包含一个用于可靠部署和交互轨迹收集的人-机器人协作框架，以及一个利用二元可取性信号和自适应重新加权算法的优化过程。该算法能有效抑制失败动作并增强纠错能力，使VLA模型能够从失败中学习，实现迭代改进，从而在动态环境中可靠部署。实验在模拟和真实场景中验证了该方法的有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新的VLA模型优化范式——动作偏好优化（APO），它通过动作级别的偏好对齐，超越了行为克隆和强化学习的限制。</li><li>设计了一个用于可靠部署和交互轨迹收集的人-机器人协作框架，通过实时人工干预保证任务成功并提供修正轨迹。</li><li>针对机器人交互的不可逆性和VLA模型中动作标记与连续动作之间的概率失配问题，提出了一种自适应重新加权算法。该算法利用从交互中派生的二元可取性信号，引导模型有效学习。</li><li>APO使VLA模型具备了从失败中学习的关键能力，为机器人在真实世界中的持续迭代优化铺平了道路。</li><li>在多种模拟和真实世界的操作任务上进行了全面实验，证明了APO在泛化性和鲁棒性方面的优越性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法分为两个主要部分：</p>\n<ol><li><strong>人-机器人协作部署</strong>：</li></ol>\n<p> - 首先，通过行为克隆（BC）在专家演示数据集上微调预训练的VLA模型，得到一个基础策略。</p>\n<p> - 部署该策略与环境交互。当策略执行失败时，人工操作员进行实时干预，确保任务完成并收集修正轨迹。</p>\n<p> - 对交互轨迹进行标注：将干预前K步内的动作标记为“不可取”，而人类干预和策略成功执行的动作标记为“可取”。</p>\n<p> - 将连续动作离散化为动作标记。</p>\n<ol><li><strong>动作偏好优化（APO）</strong>：</li></ol>\n<p> - 针对不可逆交互和动作标记概率失配两个挑战，构建了一个基于前景理论的效用函数，该函数从二元可取性信号（而非配对偏好）中学习。</p>\n<p> - 采用一个损失函数，通过KL散度项约束优化过程，使模型在参考模型的基础上学习偏好对齐，避免灾难性遗忘。</p>\n<p> - 提出一种自适应重新加权方法：计算每个样本的连续动作L1损失并进行批次归一化，得到样本权重。然后动态调整可取和不可取样本在损失函数中的权重，使模型更关注预测误差大的样本（即失败相关的动作），从而弥合了离散标记预测和连续动作回归之间的差距。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>模拟</strong>：使用RoboMimic数据集，包含<code>Coffee_D0</code>、<code>StackThree_D0</code>、<code>ThreePieceAssembly_D0</code>和<code>Square_D0</code>等4个长时程操作任务。每个任务使用300个专家演示进行初始微调，并收集50条交互轨迹用于APO优化。</p>\n<p> - <strong>真实世界</strong>：设计了“将方块插入杆中”的精细操作任务。通过SpaceMouse设备收集了100条专家演示轨迹，并部署策略后收集了20条人工干预轨迹。此外，还在“挂杯子”和“放柠檬”等任务上进行了验证。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - <strong>基础模型</strong>：主要对OpenVLA模型进行微调，并在π0-FAST模型上验证了泛化性。</p>\n<p> - <strong>硬件</strong>：初始微调使用8块NVIDIA A100 GPU（LoRA, rank=32, batch size=16），APO优化阶段使用4块NVIDIA A100 GPU（learning rate=5e-5, batch size=8）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：RoboMimic模拟环境。</p>\n<p> - <strong>真实世界环境</strong>：使用真实机械臂执行操作任务。为了测试泛化能力，设计了多种扰动场景，包括位置扰动（改变物体初始位置）、背景扰动（更换背景颜色）和纹理扰动（更换物体纹理）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要评估指标是<strong>任务平均成功率</strong>。在模拟环境中，每个任务在不同随机种子下进行50次试验并报告平均成功率。在真实世界中，对每种场景进行20次试验并报告平均成功率。</p>\n<p> - 在终身学习实验中，每20次交互后更新模型并进行50次评估，绘制成功率随迭代次数的变化曲线。</p>\n<p> - 通过比较不同方法在原始任务和扰动场景下的性能，评估模型的泛化和抗干扰能力。</p>\n<p>```</p>"
  },
  {
    "date": "2025-06-07",
    "title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation",
    "link": "http://arxiv.org/abs/2506.06677",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-06",
    "title": "DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models",
    "link": "http://arxiv.org/abs/2506.05667",
    "summary_markdown": "### 论文研究单位\nLi Auto Inc.\n### 论文概述\n该论文介绍了 DriveAction，一个专为视觉-语言-动作（VLA）模型设计的、以动作为驱动的自动驾驶基准。该基准旨在解决现有基准在场景多样性、可靠的动作级标注以及与人类偏好对齐的评估协议方面的不足。DriveAction 包含 16,185 个 QA 对，源自 2,610 个驾驶场景，它利用自动驾驶车辆驾驶员主动收集的真实世界数据，提供与人类驾驶偏好对齐的高级别离散动作标签，并实施一个以动作为根的树状评估框架，以系统地评估从视觉、语言到动作的完整决策过程。\n### 论文核心贡献点\n1. 驾驶员贡献的广泛覆盖驾驶场景：数据来源于自动驾驶车辆驾驶员主动收集的真实世界数据，覆盖广泛且具有代表性的日常和挑战性驾驶场景，并通过人工筛选保证质量。\n2. 与人类驾驶偏好对齐的真实标签：动作标签直接从驾驶员的实时驾驶操作中收集，忠实地捕捉了决策时刻的人类意图。这些标签被离散化为高级别动作，与端到端大模型的输出粒度相匹配，并经过多轮人工验证。\n3. 以动作为根的树状结构评估：引入了一个以动作为根、树状结构的评估框架。该框架根据目标动作动态确定所需的视觉和语言任务，将 V-L-A 任务系统地整合到一个可扩展的框架中，支持综合评估和任务特定评估，并能分析视觉和语言信息对最终动作决策的影响。\n### 论文方法描述\nDriveAction 的方法包含三个核心部分：\n1. 场景构建：数据来自公司运营的自动驾驶车队，覆盖 148 个城市和多款车型，涵盖了匝道、主辅路切换、导航/效率变道、绕行弱势交通参与者、交叉口和路段等七大关键场景类别。\n2. 标注对齐：动作标签源于真实驾驶操作，而非事后人工标注。为匹配大模型决策频率，标签被离散化为如“变道”、“减速”等高级别动作。所有数据均经过人工审核，排除了错误、不合理或违法的行为。\n3. 评估框架：采用以动作为根的树状任务架构，顶层是动作节点，中间是语言任务（如导航遵循），底层是视觉任务（如车道线检测）。评估时提供连续视觉帧、导航指令和车速等关键场景信息。支持四种综合评估模式（V-L-A, V-A, L-A, A）和针对每个节点的任务特定评估。\n### 论文使用数据集和训练资源\n1. 数据集：使用的数据集是 DriveAction 基准，包含 16,185 个 QA 对，由 2,610 个驾驶场景生成。该数据集由驾驶员贡献的数据构建，已在 Hugging Face 上公开。\n2. 训练资源：为了在驾驶领域进行评估，论文使用专有驾驶数据训练了两个轻量级车载模型：一个非 MOE 架构模型（0.5B 参数）和一个 MOE 架构模型（8×0.4B 参数）。论文未提及训练这些模型所使用的具体硬件资源（如 GPU 类型或数量）。\n### 论文使用的评估环境和评估指标\n1. 评估环境：\n * 模型：评估了 12 个广泛使用的视觉语言模型（VLM），包括 GPT-4o、Claude 3.5 Sonnet、Qwen-Max-Latest 等非推理模型，以及 o1、o3、Claude 3.7 Sonnet Thinking 等推理模型。同时，也评估了上述两个专有的驾驶领域模型。\n * 框架：所有实验使用 VLMEvalKit 工具包实现。\n * 对比基准：与 BDD-X、Next-qa、TextVQA、RealWorldQA、Reason2Drive 等现有基准进行了对比。\n2. 评估指标：\n * 主要评估指标是准确率，用于衡量所有问题类型的模型性能。\n * 在跨基准比较中，使用统一的评估指标，即每个基准所有问题的平均得分（0-100分制）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Li Auto Inc.</p>\n<h3>论文概述</h3>\n<p>该论文介绍了 DriveAction，一个专为视觉-语言-动作（VLA）模型设计的、以动作为驱动的自动驾驶基准。该基准旨在解决现有基准在场景多样性、可靠的动作级标注以及与人类偏好对齐的评估协议方面的不足。DriveAction 包含 16,185 个 QA 对，源自 2,610 个驾驶场景，它利用自动驾驶车辆驾驶员主动收集的真实世界数据，提供与人类驾驶偏好对齐的高级别离散动作标签，并实施一个以动作为根的树状评估框架，以系统地评估从视觉、语言到动作的完整决策过程。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>驾驶员贡献的广泛覆盖驾驶场景：数据来源于自动驾驶车辆驾驶员主动收集的真实世界数据，覆盖广泛且具有代表性的日常和挑战性驾驶场景，并通过人工筛选保证质量。</li><li>与人类驾驶偏好对齐的真实标签：动作标签直接从驾驶员的实时驾驶操作中收集，忠实地捕捉了决策时刻的人类意图。这些标签被离散化为高级别动作，与端到端大模型的输出粒度相匹配，并经过多轮人工验证。</li><li>以动作为根的树状结构评估：引入了一个以动作为根、树状结构的评估框架。该框架根据目标动作动态确定所需的视觉和语言任务，将 V-L-A 任务系统地整合到一个可扩展的框架中，支持综合评估和任务特定评估，并能分析视觉和语言信息对最终动作决策的影响。</li></ol>\n<h3>论文方法描述</h3>\n<p>DriveAction 的方法包含三个核心部分：</p>\n<ol><li>场景构建：数据来自公司运营的自动驾驶车队，覆盖 148 个城市和多款车型，涵盖了匝道、主辅路切换、导航/效率变道、绕行弱势交通参与者、交叉口和路段等七大关键场景类别。</li><li>标注对齐：动作标签源于真实驾驶操作，而非事后人工标注。为匹配大模型决策频率，标签被离散化为如“变道”、“减速”等高级别动作。所有数据均经过人工审核，排除了错误、不合理或违法的行为。</li><li>评估框架：采用以动作为根的树状任务架构，顶层是动作节点，中间是语言任务（如导航遵循），底层是视觉任务（如车道线检测）。评估时提供连续视觉帧、导航指令和车速等关键场景信息。支持四种综合评估模式（V-L-A, V-A, L-A, A）和针对每个节点的任务特定评估。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li>数据集：使用的数据集是 DriveAction 基准，包含 16,185 个 QA 对，由 2,610 个驾驶场景生成。该数据集由驾驶员贡献的数据构建，已在 Hugging Face 上公开。</li><li>训练资源：为了在驾驶领域进行评估，论文使用专有驾驶数据训练了两个轻量级车载模型：一个非 MOE 架构模型（0.5B 参数）和一个 MOE 架构模型（8×0.4B 参数）。论文未提及训练这些模型所使用的具体硬件资源（如 GPU 类型或数量）。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li>评估环境：</li></ol>\n<p> * 模型：评估了 12 个广泛使用的视觉语言模型（VLM），包括 GPT-4o、Claude 3.5 Sonnet、Qwen-Max-Latest 等非推理模型，以及 o1、o3、Claude 3.7 Sonnet Thinking 等推理模型。同时，也评估了上述两个专有的驾驶领域模型。</p>\n<p> * 框架：所有实验使用 VLMEvalKit 工具包实现。</p>\n<p> * 对比基准：与 BDD-X、Next-qa、TextVQA、RealWorldQA、Reason2Drive 等现有基准进行了对比。</p>\n<ol><li>评估指标：</li></ol>\n<p> * 主要评估指标是准确率，用于衡量所有问题类型的模型性能。</p>\n<p> * 在跨基准比较中，使用统一的评估指标，即每个基准所有问题的平均得分（0-100分制）。</p>"
  },
  {
    "date": "2025-06-04",
    "title": "SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.03574",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-03",
    "title": "Adversarial Attacks on Robotic Vision Language Action Models",
    "link": "http://arxiv.org/abs/2506.03350",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-02",
    "title": "SAB3R: Semantic-Augmented Backbone in 3D Reconstruction",
    "link": "http://arxiv.org/abs/2506.02112",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-02",
    "title": "Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning",
    "link": "http://arxiv.org/abs/2506.01953",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-02",
    "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics",
    "link": "http://arxiv.org/abs/2506.01844",
    "summary_markdown": "### 论文研究单位\nHugging Face, Sorbonne University, valeo.ai, École Normale Supérieure Paris-Saclay\n### 论文概述\n本文提出了SmolVLA，一个轻量级、高效且由社区驱动的视觉-语言-动作（VLA）模型。SmolVLA旨在显著降低训练和推理成本，同时保持与大型VLA模型相当的性能。它被设计为可在单个GPU上训练，并部署在消费级GPU甚至CPU上。为了进一步提高响应速度，论文引入了一个异步推理栈，将感知和动作预测与动作执行解耦，允许通过分块动作生成实现更高的控制频率。尽管体积紧凑，SmolVLA在模拟和真实世界的机器人基准测试中，实现了与比其大10倍的VLA模型相媲美的性能。\n### 论文核心贡献点\n1. 轻量级架构：提出了SmolVLA，一个为在消费级GPU上训练和部署而优化的紧凑高效视觉-语言代理。\n2. 基于社区驱动数据集的预训练：SmolVLA在完全来自公开社区贡献数据集的少于3万次训练回合上进行端到端训练。\n3. 异步推理：引入了一个优化的异步推理栈，将动作执行与观察处理和动作预测解耦，以降低延迟并实现快速、资源高效的推理。\n### 论文方法描述\nSmolVLA由两个主要组件组成：一个用于感知的预训练视觉-语言模型（VLM）和一个用于行动的动作专家。VLM处理来自多个RGB摄像头的图像、描述任务的语言指令以及机器人的传感器运动状态。VLM输出的特征直接馈送给动作专家，后者输出最终的连续动作。\n模型架构的关键设计包括：\n1. VLM：使用SmolVLM-2作为骨干，通过视觉编码器和像素洗牌技术处理图像序列。\n2. 状态、动作和特征投影器：使用线性投影层将状态投影到VLM维度，将动作投影到动作专家维度，并使VLM特征与动作专家的维度对齐。\n3. 视觉令牌缩减：不使用图像分块，仅使用全局图像，并通过像素洗牌操作将每帧的视觉令牌限制为64个。\n4. 通过层跳过实现更快的推理：跳过VLM中的计算，仅使用到指定层N（通常为总层的一半）的特征，将LLM和动作专家的计算成本减半。\n5. 流匹配动作专家：使用Transformer架构的条件流匹配Transformer作为动作专家，训练目标为预测从VLM特征和带噪声动作中得到的向量场。\n6. 交错交叉注意力和因果自注意力层：在动作专家中交错使用交叉注意力和自注意力层，其中交叉注意力层交叉关注VLM的键和值，自注意力层允许动作令牌相互关注。\n### 论文使用数据集和训练资源\n数据集：使用来自Hugging Face的481个公开社区数据集进行预训练，总计约22.9k次训练回合和1060万帧数据。数据筛选依据是实体类型、回合数、整体数据质量和帧覆盖率。使用VLM（Qwen2.5-VL-3B-Instruct）自动生成简洁的任务描述，并手动将相机映射到标准化视角类型（顶视、腕视、侧视）。\n训练资源：可在单个消费级GPU上进行训练。\n### 论文使用的评估环境和评估指标\n评估环境：在模拟环境和真实世界机器人任务上进行评估。\n评估指标：成功率（Simulation Evaluation 和 Real-World Evaluation），以及在异步推理中控制频率和响应速度的提升。\n基线模型：与π0和ACT等模型进行比较。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Hugging Face, Sorbonne University, valeo.ai, École Normale Supérieure Paris-Saclay</p>\n<h3>论文概述</h3>\n<p>本文提出了SmolVLA，一个轻量级、高效且由社区驱动的视觉-语言-动作（VLA）模型。SmolVLA旨在显著降低训练和推理成本，同时保持与大型VLA模型相当的性能。它被设计为可在单个GPU上训练，并部署在消费级GPU甚至CPU上。为了进一步提高响应速度，论文引入了一个异步推理栈，将感知和动作预测与动作执行解耦，允许通过分块动作生成实现更高的控制频率。尽管体积紧凑，SmolVLA在模拟和真实世界的机器人基准测试中，实现了与比其大10倍的VLA模型相媲美的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>轻量级架构：提出了SmolVLA，一个为在消费级GPU上训练和部署而优化的紧凑高效视觉-语言代理。</li><li>基于社区驱动数据集的预训练：SmolVLA在完全来自公开社区贡献数据集的少于3万次训练回合上进行端到端训练。</li><li>异步推理：引入了一个优化的异步推理栈，将动作执行与观察处理和动作预测解耦，以降低延迟并实现快速、资源高效的推理。</li></ol>\n<h3>论文方法描述</h3>\n<p>SmolVLA由两个主要组件组成：一个用于感知的预训练视觉-语言模型（VLM）和一个用于行动的动作专家。VLM处理来自多个RGB摄像头的图像、描述任务的语言指令以及机器人的传感器运动状态。VLM输出的特征直接馈送给动作专家，后者输出最终的连续动作。</p>\n<p>模型架构的关键设计包括：</p>\n<ol><li>VLM：使用SmolVLM-2作为骨干，通过视觉编码器和像素洗牌技术处理图像序列。</li><li>状态、动作和特征投影器：使用线性投影层将状态投影到VLM维度，将动作投影到动作专家维度，并使VLM特征与动作专家的维度对齐。</li><li>视觉令牌缩减：不使用图像分块，仅使用全局图像，并通过像素洗牌操作将每帧的视觉令牌限制为64个。</li><li>通过层跳过实现更快的推理：跳过VLM中的计算，仅使用到指定层N（通常为总层的一半）的特征，将LLM和动作专家的计算成本减半。</li><li>流匹配动作专家：使用Transformer架构的条件流匹配Transformer作为动作专家，训练目标为预测从VLM特征和带噪声动作中得到的向量场。</li><li>交错交叉注意力和因果自注意力层：在动作专家中交错使用交叉注意力和自注意力层，其中交叉注意力层交叉关注VLM的键和值，自注意力层允许动作令牌相互关注。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：使用来自Hugging Face的481个公开社区数据集进行预训练，总计约22.9k次训练回合和1060万帧数据。数据筛选依据是实体类型、回合数、整体数据质量和帧覆盖率。使用VLM（Qwen2.5-VL-3B-Instruct）自动生成简洁的任务描述，并手动将相机映射到标准化视角类型（顶视、腕视、侧视）。</p>\n<p>训练资源：可在单个消费级GPU上进行训练。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：在模拟环境和真实世界机器人任务上进行评估。</p>\n<p>评估指标：成功率（Simulation Evaluation 和 Real-World Evaluation），以及在异步推理中控制频率和响应速度的提升。</p>\n<p>基线模型：与π0和ACT等模型进行比较。</p>"
  },
  {
    "date": "2025-06-01",
    "title": "OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation",
    "link": "http://arxiv.org/abs/2506.01196",
    "summary_markdown": "- **论文研究单位**\n - 南加州大学 (University of Southern California)\n - NVIDIA\n\n- **论文概述**\n - OG-VLA 是一种新颖的架构和学习框架，结合了视觉语言动作模型 (VLA) 的泛化能力和 3D 感知策略的鲁棒性。\n - 该方法解决了将自然语言指令和一个或多个 RGBD 观测映射到准静态机器人动作的挑战。\n - 通过利用语言和视觉基础模型中的先验知识，提升了 3D 感知关键帧策略的泛化能力。\n - 核心创新是将输入观测反投影为点云，并从正交视图渲染，确保输入视图不变性和输入输出空间一致性。\n - 实验表明，在 Arnold 和 Colosseum 基准测试中，对未见过环境的泛化能力有超过 40% 的相对提升，同时在已见设置中保持鲁棒性能。\n\n- **论文核心贡献点**\n - 提出了一种结合 VLA 泛化能力与 3D 感知策略鲁棒性的新型架构 OG-VLA。\n - 引入正交图像生成机制，通过将 RGBD 输入转换为点云并渲染正交视图，实现输入视图不变性。\n - 使用视觉骨干网络、LLM 和图像扩散模型生成编码末端执行器位置和方向的图像。\n - 在 Arnold 和 Colosseum 基准测试中实现了最先进的泛化性能，相对提升超过 40%。\n - 展示了真实机器人上的快速适应能力，仅需 3-5 个演示即可学习任务。\n\n- **论文方法描述**\n - 系统输入为语言指令和一组带姿态的 RGBD 观测，输出为末端执行器状态（位置和旋转）。\n - 点云渲染：将多视角输入观测反投影为点云，再从固定正交视图（如前、上、左、右）渲染场景。\n - 多模态处理：使用视觉编码器处理正交视图，生成 CLS 和块嵌入。\n - LLM 处理：将嵌入投影到 LLM 空间，与指令提示拼接后输入 LLM，生成动作 token。\n - 图像生成：使用 Stable Diffusion 1.5 将动作 token 解码为正交视图上的热图，编码位置和旋转。\n - 3D 姿态解码：通过优化问题从热图中解码 3D 位置和欧拉角，并处理夹爪状态。\n - 训练细节：冻结视觉编码器，使用 DeepSpeed 训练 LLM、投影层和图像生成器，采用 SE(3) 数据增强。\n\n- **论文使用数据集和训练资源**\n - 数据集：Arnold（8 个任务，约 500 个演示/任务，2 关键帧/演示）和 Colosseum（20 个任务，100 个演示/任务）。\n - 训练资源：8 × A100 GPU，批量大小 64；Arnold 训练 30k 次迭代（1.5 天）或 100k 次（5 天），Colosseum 训练 250k 次；真实世界微调使用 4 × L40 GPU。\n\n- **论文使用的评估环境和评估指标**\n - 评估环境：模拟环境（Arnold 和 Colosseum 基准）及真实世界（Franka Emika Panda 机械臂）。\n - 评估指标：成功率（%），覆盖任务完成统计；Arnold 测试分 Novel Pose、Object、Scene、State；Colosseum 测试集为 all perturbation；真实世界测试 novel object 和 scene 变体。\n - 额外指标：模型延迟（秒/步、步数/回合、总时间），与基线（如 π₀-FAST、PerAct）比较。</think>\n### 论文研究单位\n南加州大学 (University of Southern California)\nNVIDIA\n### 论文概述\nOG-VLA 是一种结合视觉语言动作模型 (VLA) 泛化能力与 3D 感知策略鲁棒性的新型架构和学习框架。该方法旨在解决将自然语言指令和一个或多个 RGBD 观测映射到准静态机器人动作的挑战。通过利用语言和视觉基础模型中的先验知识，OG-VLA 提升了 3D 感知关键帧策略的泛化能力，同时保持精确控制。核心创新是将输入观测反投影为点云并从正交视图渲染，确保输入视图不变性，并通过视觉骨干网络、LLM 和图像扩散模型生成编码末端执行器状态的动作图像。实验在 Arnold 和 Colosseum 基准测试中展示了最先进的泛化性能，相对提升超过 40%，并在真实机器人上实现了 3-5 个演示的快速适应。\n### 论文核心贡献点\n- 提出 OG-VLA 架构，融合 VLA 的泛化优势与 3D 感知策略的鲁棒性。\n- 引入正交图像生成机制：将 RGBD 输入转换为点云，渲染正交视图（如前、上、左、右），实现输入视图不变性和空间一致性。\n- 使用视觉骨干网络、LLM 和图像扩散模型生成动作图像，编码末端执行器的位置、方向和夹爪状态。\n- 在 Arnold 和 Colosseum 基准测试中，对未见过环境的泛化成功率达相对提升超过 40%，同时保持已见环境的鲁棒性。\n- 展示真实机器人适应能力：仅需 3-5 个演示即可学习新任务，并在未见过物体和场景中泛化。\n### 论文方法描述\n系统输入为自然语言指令和多视角 RGBD 观测，输出为末端执行器 6-DOF 姿态（位置和旋转）。流程包括：\n1. **点云渲染**：将所有输入观测反投影为点云，在固定参考系中聚合，并从一组正交相机（前、左、右、上）渲染正交 RGB 图像。\n2. **多模态处理**：使用视觉编码器（如 ImageBind）处理正交视图，生成 CLS token 和块嵌入，通过投影层映射到 LLM 空间。\n3. **LLM 推理**：将嵌入与指令提示拼接后输入 LLM（如 Vicuna-7B），输出动作 token 序列，结合文本响应。\n4. **图像生成**：使用图像扩散模型（如 Stable Diffusion 1.5）将动作 token 解码为热图图像，覆盖在正交视图上，通过颜色编码位置（红）、旋转（黄、蓝、绿）和夹爪状态（左上角颜色）。\n5. **3D 姿态解码**：从热图解码 3D 位置（优化多视图概率积）和旋转（计算相对于水平线的角度），结合夹爪开闭状态。\n6. **训练细节**：冻结视觉编码器，使用 DeepSpeed 训练 LLM、投影层和图像生成器；采用 SE(3) 数据增强（平移 ±0.1m，旋转 ±90°）；Arnold 训练 30k 或 100k 次迭代，Colosseum 训练 250k 次。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **Arnold**：8 个任务（如抓取、开门、倒水），每个任务约 500 个演示，每个演示 2 个关键帧，总计约 7100 关键帧。测试分 Novel Pose、Object、Scene、State。\n - **Colosseum**：20 个桌面任务（如关箱、洗碗机），每个任务 100 个演示，平均 6 个关键帧。测试集为 all perturbation，同时改变物体、光照、相机姿态和干扰物。\n - **真实世界**：4 个任务（抓取、放物、开抽屉、关抽屉），共 22 个演示（3-5 个/任务），使用单相机。\n- **训练资源**：\n - 模拟训练：8 × A100 GPU，批量大小 64；Arnold 训练 30k 次（1.5 天）或 100k 次（5 天），Colosseum 训练 250k 次。\n - 真实世界微调：从 Arnold 预训练检查点微调 10k 次；VLA 基线（如 π₀-FAST）使用 4 × L40 GPU，LoRA 微调 0.5 天，全微调 2 天。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：Arnold 和 Colosseum 基准测试，在 Isaac Gym 中运行，包含多相机观测和运动规划器。\n - **真实世界**：Franka Emika Panda 机械臂，桌面设置，单前置相机，使用运动规划器（如 Viola）执行关键帧。\n- **评估指标**：\n - **成功率 (%)**：任务完成比例，Arnold 每测试集 20 个回合，Colosseum 25 个，真实世界 10 个，报告均值和标准差。\n - **模型延迟**：单步推理时间（秒）、每回合推理步数、总回合时间（含环境执行）。\n - **泛化测试**：包括 Arnold 的 Novel Pose（未见过物体/机器人位姿）、Novel Object（未见过物体）、Novel Scene（未见过场景）、Novel State（未见过目标状态）；Colosseum 的 all perturbation；真实世界的 novel object 和 scene 变体。",
    "summary_html": "<ul><li><strong>论文研究单位</strong></li></ul>\n<p> - 南加州大学 (University of Southern California)</p>\n<p> - NVIDIA</p>\n\n<ul><li><strong>论文概述</strong></li></ul>\n<p> - OG-VLA 是一种新颖的架构和学习框架，结合了视觉语言动作模型 (VLA) 的泛化能力和 3D 感知策略的鲁棒性。</p>\n<p> - 该方法解决了将自然语言指令和一个或多个 RGBD 观测映射到准静态机器人动作的挑战。</p>\n<p> - 通过利用语言和视觉基础模型中的先验知识，提升了 3D 感知关键帧策略的泛化能力。</p>\n<p> - 核心创新是将输入观测反投影为点云，并从正交视图渲染，确保输入视图不变性和输入输出空间一致性。</p>\n<p> - 实验表明，在 Arnold 和 Colosseum 基准测试中，对未见过环境的泛化能力有超过 40% 的相对提升，同时在已见设置中保持鲁棒性能。</p>\n\n<ul><li><strong>论文核心贡献点</strong></li></ul>\n<p> - 提出了一种结合 VLA 泛化能力与 3D 感知策略鲁棒性的新型架构 OG-VLA。</p>\n<p> - 引入正交图像生成机制，通过将 RGBD 输入转换为点云并渲染正交视图，实现输入视图不变性。</p>\n<p> - 使用视觉骨干网络、LLM 和图像扩散模型生成编码末端执行器位置和方向的图像。</p>\n<p> - 在 Arnold 和 Colosseum 基准测试中实现了最先进的泛化性能，相对提升超过 40%。</p>\n<p> - 展示了真实机器人上的快速适应能力，仅需 3-5 个演示即可学习任务。</p>\n\n<ul><li><strong>论文方法描述</strong></li></ul>\n<p> - 系统输入为语言指令和一组带姿态的 RGBD 观测，输出为末端执行器状态（位置和旋转）。</p>\n<p> - 点云渲染：将多视角输入观测反投影为点云，再从固定正交视图（如前、上、左、右）渲染场景。</p>\n<p> - 多模态处理：使用视觉编码器处理正交视图，生成 CLS 和块嵌入。</p>\n<p> - LLM 处理：将嵌入投影到 LLM 空间，与指令提示拼接后输入 LLM，生成动作 token。</p>\n<p> - 图像生成：使用 Stable Diffusion 1.5 将动作 token 解码为正交视图上的热图，编码位置和旋转。</p>\n<p> - 3D 姿态解码：通过优化问题从热图中解码 3D 位置和欧拉角，并处理夹爪状态。</p>\n<p> - 训练细节：冻结视觉编码器，使用 DeepSpeed 训练 LLM、投影层和图像生成器，采用 SE(3) 数据增强。</p>\n\n<ul><li><strong>论文使用数据集和训练资源</strong></li></ul>\n<p> - 数据集：Arnold（8 个任务，约 500 个演示/任务，2 关键帧/演示）和 Colosseum（20 个任务，100 个演示/任务）。</p>\n<p> - 训练资源：8 × A100 GPU，批量大小 64；Arnold 训练 30k 次迭代（1.5 天）或 100k 次（5 天），Colosseum 训练 250k 次；真实世界微调使用 4 × L40 GPU。</p>\n\n<ul><li><strong>论文使用的评估环境和评估指标</strong></li></ul>\n<p> - 评估环境：模拟环境（Arnold 和 Colosseum 基准）及真实世界（Franka Emika Panda 机械臂）。</p>\n<p> - 评估指标：成功率（%），覆盖任务完成统计；Arnold 测试分 Novel Pose、Object、Scene、State；Colosseum 测试集为 all perturbation；真实世界测试 novel object 和 scene 变体。</p>\n<p> - 额外指标：模型延迟（秒/步、步数/回合、总时间），与基线（如 π₀-FAST、PerAct）比较。</think></p>\n<h3>论文研究单位</h3>\n<p>南加州大学 (University of Southern California)</p>\n<p>NVIDIA</p>\n<h3>论文概述</h3>\n<p>OG-VLA 是一种结合视觉语言动作模型 (VLA) 泛化能力与 3D 感知策略鲁棒性的新型架构和学习框架。该方法旨在解决将自然语言指令和一个或多个 RGBD 观测映射到准静态机器人动作的挑战。通过利用语言和视觉基础模型中的先验知识，OG-VLA 提升了 3D 感知关键帧策略的泛化能力，同时保持精确控制。核心创新是将输入观测反投影为点云并从正交视图渲染，确保输入视图不变性，并通过视觉骨干网络、LLM 和图像扩散模型生成编码末端执行器状态的动作图像。实验在 Arnold 和 Colosseum 基准测试中展示了最先进的泛化性能，相对提升超过 40%，并在真实机器人上实现了 3-5 个演示的快速适应。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出 OG-VLA 架构，融合 VLA 的泛化优势与 3D 感知策略的鲁棒性。</li><li>引入正交图像生成机制：将 RGBD 输入转换为点云，渲染正交视图（如前、上、左、右），实现输入视图不变性和空间一致性。</li><li>使用视觉骨干网络、LLM 和图像扩散模型生成动作图像，编码末端执行器的位置、方向和夹爪状态。</li><li>在 Arnold 和 Colosseum 基准测试中，对未见过环境的泛化成功率达相对提升超过 40%，同时保持已见环境的鲁棒性。</li><li>展示真实机器人适应能力：仅需 3-5 个演示即可学习新任务，并在未见过物体和场景中泛化。</li></ul>\n<h3>论文方法描述</h3>\n<p>系统输入为自然语言指令和多视角 RGBD 观测，输出为末端执行器 6-DOF 姿态（位置和旋转）。流程包括：</p>\n<ol><li><strong>点云渲染</strong>：将所有输入观测反投影为点云，在固定参考系中聚合，并从一组正交相机（前、左、右、上）渲染正交 RGB 图像。</li><li><strong>多模态处理</strong>：使用视觉编码器（如 ImageBind）处理正交视图，生成 CLS token 和块嵌入，通过投影层映射到 LLM 空间。</li><li><strong>LLM 推理</strong>：将嵌入与指令提示拼接后输入 LLM（如 Vicuna-7B），输出动作 token 序列，结合文本响应。</li><li><strong>图像生成</strong>：使用图像扩散模型（如 Stable Diffusion 1.5）将动作 token 解码为热图图像，覆盖在正交视图上，通过颜色编码位置（红）、旋转（黄、蓝、绿）和夹爪状态（左上角颜色）。</li><li><strong>3D 姿态解码</strong>：从热图解码 3D 位置（优化多视图概率积）和旋转（计算相对于水平线的角度），结合夹爪开闭状态。</li><li><strong>训练细节</strong>：冻结视觉编码器，使用 DeepSpeed 训练 LLM、投影层和图像生成器；采用 SE(3) 数据增强（平移 ±0.1m，旋转 ±90°）；Arnold 训练 30k 或 100k 次迭代，Colosseum 训练 250k 次。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>Arnold</strong>：8 个任务（如抓取、开门、倒水），每个任务约 500 个演示，每个演示 2 个关键帧，总计约 7100 关键帧。测试分 Novel Pose、Object、Scene、State。</p>\n<p> - <strong>Colosseum</strong>：20 个桌面任务（如关箱、洗碗机），每个任务 100 个演示，平均 6 个关键帧。测试集为 all perturbation，同时改变物体、光照、相机姿态和干扰物。</p>\n<p> - <strong>真实世界</strong>：4 个任务（抓取、放物、开抽屉、关抽屉），共 22 个演示（3-5 个/任务），使用单相机。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 模拟训练：8 × A100 GPU，批量大小 64；Arnold 训练 30k 次（1.5 天）或 100k 次（5 天），Colosseum 训练 250k 次。</p>\n<p> - 真实世界微调：从 Arnold 预训练检查点微调 10k 次；VLA 基线（如 π₀-FAST）使用 4 × L40 GPU，LoRA 微调 0.5 天，全微调 2 天。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：Arnold 和 Colosseum 基准测试，在 Isaac Gym 中运行，包含多相机观测和运动规划器。</p>\n<p> - <strong>真实世界</strong>：Franka Emika Panda 机械臂，桌面设置，单前置相机，使用运动规划器（如 Viola）执行关键帧。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率 (%)</strong>：任务完成比例，Arnold 每测试集 20 个回合，Colosseum 25 个，真实世界 10 个，报告均值和标准差。</p>\n<p> - <strong>模型延迟</strong>：单步推理时间（秒）、每回合推理步数、总回合时间（含环境执行）。</p>\n<p> - <strong>泛化测试</strong>：包括 Arnold 的 Novel Pose（未见过物体/机器人位姿）、Novel Object（未见过物体）、Novel Scene（未见过场景）、Novel State（未见过目标状态）；Colosseum 的 all perturbation；真实世界的 novel object 和 scene 变体。</p>"
  },
  {
    "date": "2025-06-01",
    "title": "GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering",
    "link": "http://arxiv.org/abs/2506.01174",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-31",
    "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks",
    "link": "http://arxiv.org/abs/2506.00411",
    "summary_markdown": "### 论文研究单位\n复旦大学、上海科技大学、上海交通大学\n### 论文概述\n论文提出LoHoVLA，一个统一的视觉-语言-动作模型，用于处理长视野具身任务。该模型通过将高级任务规划和低级运动控制集成到单一框架中，解决现有VLA模型在规划能力上的不足和分层架构中的协调问题。LoHoVLA利用预训练视觉语言模型作为主干，联合生成语言子任务和机器人动作标记，并通过分层闭环控制机制增强鲁棒性。\n### 论文核心贡献点\n1. 统一架构：将高级任务规划和低级运动控制整合为单一模型，避免分层方法的协调问题。\n2. 共享表示：使用同一VLM主干处理语言和动作生成，促进任务间的泛化。\n3. 分层闭环控制机制：根据失败次数阈值动态调整子任务重规划或动作重预测，减少误差传播。\n4. LoHoSet数据集：构建基于Ravens模拟器的数据集，包含20个长视野任务，每个任务1000个专家演示，涵盖视觉观察、语言目标、子任务和动作。\n### 论文方法描述\nLoHoVLA采用PaliGemma作为主干模型，包括SigLIP图像编码器、Gemma-2B语言模型和线性投影层。动作通过离散化为1024个分箱进行标记化，并在推理时去标记化恢复。控制策略使用Algorithm 1：在子任务执行失败次数超阈值K时触发重规划，否则仅更新动作。训练分两阶段：第一阶段优化文本损失提升规划能力，第二阶段加入原始任务优化动作损失。总损失函数为语言损失和动作损失之和。\n### 论文使用数据集和训练资源\n数据集：LoHoSet，基于Ravens机器人模拟器构建，包含块、碗和区域三种对象，11种颜色，涉及10个长视野任务（如堆叠、放置）和3个拾取-放置原始任务，额外增加10个长视野任务提升泛化。每个任务1000个演示，总数据涵盖视觉观察（RGB和深度图像）、语言目标、子任务描述和机器人动作。\n训练资源：未明确指定硬件，但模型在合成数据集上微调，PaliGemma主干固定图像编码器。\n### 论文使用的评估环境和评估指标\n评估环境：Ravens模拟器，包含UR5e机械臂和吸盘夹具，引入观察噪声和物体掉落概率模拟不确定性。\n评估指标：主要指标包括任务成功率，用于比较LoHoVLA与分层基线和标准VLA方法在已见和未完成任务上的性能。实验还通过闭环策略比较和消融研究验证设计有效性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>复旦大学、上海科技大学、上海交通大学</p>\n<h3>论文概述</h3>\n<p>论文提出LoHoVLA，一个统一的视觉-语言-动作模型，用于处理长视野具身任务。该模型通过将高级任务规划和低级运动控制集成到单一框架中，解决现有VLA模型在规划能力上的不足和分层架构中的协调问题。LoHoVLA利用预训练视觉语言模型作为主干，联合生成语言子任务和机器人动作标记，并通过分层闭环控制机制增强鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>统一架构：将高级任务规划和低级运动控制整合为单一模型，避免分层方法的协调问题。</li><li>共享表示：使用同一VLM主干处理语言和动作生成，促进任务间的泛化。</li><li>分层闭环控制机制：根据失败次数阈值动态调整子任务重规划或动作重预测，减少误差传播。</li><li>LoHoSet数据集：构建基于Ravens模拟器的数据集，包含20个长视野任务，每个任务1000个专家演示，涵盖视觉观察、语言目标、子任务和动作。</li></ol>\n<h3>论文方法描述</h3>\n<p>LoHoVLA采用PaliGemma作为主干模型，包括SigLIP图像编码器、Gemma-2B语言模型和线性投影层。动作通过离散化为1024个分箱进行标记化，并在推理时去标记化恢复。控制策略使用Algorithm 1：在子任务执行失败次数超阈值K时触发重规划，否则仅更新动作。训练分两阶段：第一阶段优化文本损失提升规划能力，第二阶段加入原始任务优化动作损失。总损失函数为语言损失和动作损失之和。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：LoHoSet，基于Ravens机器人模拟器构建，包含块、碗和区域三种对象，11种颜色，涉及10个长视野任务（如堆叠、放置）和3个拾取-放置原始任务，额外增加10个长视野任务提升泛化。每个任务1000个演示，总数据涵盖视觉观察（RGB和深度图像）、语言目标、子任务描述和机器人动作。</p>\n<p>训练资源：未明确指定硬件，但模型在合成数据集上微调，PaliGemma主干固定图像编码器。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：Ravens模拟器，包含UR5e机械臂和吸盘夹具，引入观察噪声和物体掉落概率模拟不确定性。</p>\n<p>评估指标：主要指标包括任务成功率，用于比较LoHoVLA与分层基线和标准VLA方法在已见和未完成任务上的性能。实验还通过闭环策略比较和消融研究验证设计有效性。</p>"
  },
  {
    "date": "2025-05-30",
    "title": "Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction",
    "link": "http://arxiv.org/abs/2505.24156",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-29",
    "title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2505.23757",
    "summary_markdown": "### 论文研究单位\n清华大学智能产业研究院, 博世研究中心, 清华大学交叉信息研究院\n### 论文概述\n论文提出了Impromptu VLA，一个开放权重和开放数据的驾驶视觉-语言-动作模型。其核心贡献是Impromptu VLA数据集，该数据集专注于非结构化道路的“corner case”场景，旨在解决当前视觉-语言-动作（VLA）模型在这些场景下表现不佳的问题。数据集包含超过80,000个精心筛选的视频片段，源自8个开源的大规模数据集，总计超过200万个源片段。该数据集基于一个包含四种具有挑战性的非结构化类别的新分类法构建，并提供了丰富的、面向规划的问答标注和动作轨迹。实验表明，使用该数据集训练的VLA模型在既定基准上实现了显著的性能提升，包括提高了NeuroNCAP闭环评分和降低碰撞率，并在nuScenes开放环轨迹预测中达到接近最先进的L2准确度。\n### 论文核心贡献点\n1. Impromptu VLA数据集：一个公开可用的大规模、丰富标注的资源，专门专注于多样化和具有挑战性的非结构化驾驶场景，旨在填补现有数据资源的空白。\n2. 一个用于非结构化道路条件的系统分类法：定义了四类非结构化场景（边界不清的道路、临时交通规则变更、非常规动态障碍物、具有挑战性的道路条件），以及一个可扩展的、以VLM为中心的数据整理流程，用于识别、分类和全面的、适合训练高级VLM的多任务问答标注。\n3. 广泛的实验证据：证明了使用Impromptu VLA数据集训练可以显著提升标准驾驶基准上的结果，并可作为评估和提高VLM在非结构化环境中能力的有效诊断工具。\n### 论文方法描述\n1. 数据收集与筛选：从8个公共数据集中聚合了超过200万个片段，并标准化到2Hz的统一时间频率。\n2. 非结构化场景分类法定义：通过使用VLM对数据进行无偏探索和描述，然后进行分类和迭代优化提示，最终自下而上地汇聚并归纳出四个高层类别。\n3. 数据处理与标注流程：包括关键片段选择和稳定性过滤、使用思维链提示的场景分类与结构化信息提取、多任务标注生成（如场景描述、交通信号检测、VRU识别、运动意图预测、元动作规划、规划解释和轨迹预测）以及全面的人工验证。\n4. 模型训练：比较了两种训练流程：直接在nuScenes上微调基础模型，以及先在Impromptu VLA上微调再在nuScenes上微调。\n### 论文使用数据集和训练资源\n* 数据集：源数据集包括Mapillary, ONCE, NAVSIM, nuScenes, Waymo, Argoverse-V2, KITTI, IDD。Impromptu VLA数据集最终包含约80,000个片段，标注数据大小约43.5GB，按80:20比例划分为训练集和验证集。\n* 训练资源：论文中未明确提及使用的具体计算资源。\n### 论文使用的评估环境和评估指标\n* 评估环境：闭环评估使用NeuroNCAP；开放环评估使用nuScenes数据集；诊断评估在Impromptu VLA自身的验证集上进行。\n* 评估指标：闭环指标包括NeuroNCAP评分和碰撞率；开放环指标为不同未来时间点的L2误差；诊断指标包括问答准确率和轨迹预测L2误差。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学智能产业研究院, 博世研究中心, 清华大学交叉信息研究院</p>\n<h3>论文概述</h3>\n<p>论文提出了Impromptu VLA，一个开放权重和开放数据的驾驶视觉-语言-动作模型。其核心贡献是Impromptu VLA数据集，该数据集专注于非结构化道路的“corner case”场景，旨在解决当前视觉-语言-动作（VLA）模型在这些场景下表现不佳的问题。数据集包含超过80,000个精心筛选的视频片段，源自8个开源的大规模数据集，总计超过200万个源片段。该数据集基于一个包含四种具有挑战性的非结构化类别的新分类法构建，并提供了丰富的、面向规划的问答标注和动作轨迹。实验表明，使用该数据集训练的VLA模型在既定基准上实现了显著的性能提升，包括提高了NeuroNCAP闭环评分和降低碰撞率，并在nuScenes开放环轨迹预测中达到接近最先进的L2准确度。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>Impromptu VLA数据集：一个公开可用的大规模、丰富标注的资源，专门专注于多样化和具有挑战性的非结构化驾驶场景，旨在填补现有数据资源的空白。</li><li>一个用于非结构化道路条件的系统分类法：定义了四类非结构化场景（边界不清的道路、临时交通规则变更、非常规动态障碍物、具有挑战性的道路条件），以及一个可扩展的、以VLM为中心的数据整理流程，用于识别、分类和全面的、适合训练高级VLM的多任务问答标注。</li><li>广泛的实验证据：证明了使用Impromptu VLA数据集训练可以显著提升标准驾驶基准上的结果，并可作为评估和提高VLM在非结构化环境中能力的有效诊断工具。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>数据收集与筛选：从8个公共数据集中聚合了超过200万个片段，并标准化到2Hz的统一时间频率。</li><li>非结构化场景分类法定义：通过使用VLM对数据进行无偏探索和描述，然后进行分类和迭代优化提示，最终自下而上地汇聚并归纳出四个高层类别。</li><li>数据处理与标注流程：包括关键片段选择和稳定性过滤、使用思维链提示的场景分类与结构化信息提取、多任务标注生成（如场景描述、交通信号检测、VRU识别、运动意图预测、元动作规划、规划解释和轨迹预测）以及全面的人工验证。</li><li>模型训练：比较了两种训练流程：直接在nuScenes上微调基础模型，以及先在Impromptu VLA上微调再在nuScenes上微调。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：源数据集包括Mapillary, ONCE, NAVSIM, nuScenes, Waymo, Argoverse-V2, KITTI, IDD。Impromptu VLA数据集最终包含约80,000个片段，标注数据大小约43.5GB，按80:20比例划分为训练集和验证集。</li><li>训练资源：论文中未明确提及使用的具体计算资源。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：闭环评估使用NeuroNCAP；开放环评估使用nuScenes数据集；诊断评估在Impromptu VLA自身的验证集上进行。</li><li>评估指标：闭环指标包括NeuroNCAP评分和碰撞率；开放环指标为不同未来时间点的L2误差；诊断指标包括问答准确率和轨迹预测L2误差。</li></ul>"
  },
  {
    "date": "2025-05-29",
    "title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better",
    "link": "http://arxiv.org/abs/2505.23705",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-29",
    "title": "TrackVLA: Embodied Visual Tracking in the Wild",
    "link": "http://arxiv.org/abs/2505.23189",
    "summary_markdown": "### 论文研究单位\nPeking University, Galbot, Beihang University, Beijing Normal University, BAAI\n### 论文概述\n论文提出TrackVLA，一个Vision-Language-Action (VLA)模型，用于解决具身视觉跟踪任务。该任务要求智能体在动态环境中基于自身视角持续跟踪特定目标。现有方法通常将目标识别和轨迹规划解耦为分离模块，导致错误累积。TrackVLA通过共享LLM主干，联合学习目标识别和轨迹规划的协同作用。\n### 论文核心贡献点\n1. 提出TrackVLA，首个统一VLA框架，同时处理目标识别和轨迹规划\n2. 构建了EVT-Bench (Embodied Visual Tracking Benchmark)基准，包含100个人形化身和804个场景\n3. 收集了170万样本的数据集（85.5万跟踪样本+85.5万识别样本）\n4. 在合成和真实环境中展示了SOTA性能，零样本迁移能力强\n### 论文方法描述\nTrackVLA架构使用Vicuna-7B作为LLM主干。视觉编码采用EVA-CLIP预训练模型，使用网格池化策略。双头设计包括语言建模头用于识别任务，自回归解码文本；基于锚点的扩散模型头用于轨迹规划，从预定义锚点去噪生成轨迹。训练策略联合优化跟踪损失和文本预测损失。推理优化只需2步去噪，实现5倍加速。\n### 论文使用数据集和训练资源\nEVT-Bench数据集包含100个人形化身和804个场景，总计25,986个episode，分为训练集(21,771)和测试集(4,215)，涵盖三个子任务：单目标跟踪(STT)、干扰跟踪(DT)、模糊跟踪(AT)。85.5万跟踪样本来自EVT-Bench训练集。85.5万识别样本包括36.2万人识别样本（基于SYNTH-PEDES）和49.3万开放世界VQA样本。训练资源方面，冻结视觉编码器参数，仅训练1个epoch。\n### 论文使用的评估环境和评估指标\n评估环境包括合成环境（EVT-Bench、Gym-UnrealCV公共基准）和真实环境（配备RGB摄像头的机器人平台）。评估指标包括跟踪性能（成功率，保持1-3米跟随距离且面向目标）、识别性能（文本预测准确率）、推理速度（FPS，报告10 FPS），以及轨迹回归误差(MSE)和分类损失(BCE)。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Peking University, Galbot, Beihang University, Beijing Normal University, BAAI</p>\n<h3>论文概述</h3>\n<p>论文提出TrackVLA，一个Vision-Language-Action (VLA)模型，用于解决具身视觉跟踪任务。该任务要求智能体在动态环境中基于自身视角持续跟踪特定目标。现有方法通常将目标识别和轨迹规划解耦为分离模块，导致错误累积。TrackVLA通过共享LLM主干，联合学习目标识别和轨迹规划的协同作用。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出TrackVLA，首个统一VLA框架，同时处理目标识别和轨迹规划</li><li>构建了EVT-Bench (Embodied Visual Tracking Benchmark)基准，包含100个人形化身和804个场景</li><li>收集了170万样本的数据集（85.5万跟踪样本+85.5万识别样本）</li><li>在合成和真实环境中展示了SOTA性能，零样本迁移能力强</li></ol>\n<h3>论文方法描述</h3>\n<p>TrackVLA架构使用Vicuna-7B作为LLM主干。视觉编码采用EVA-CLIP预训练模型，使用网格池化策略。双头设计包括语言建模头用于识别任务，自回归解码文本；基于锚点的扩散模型头用于轨迹规划，从预定义锚点去噪生成轨迹。训练策略联合优化跟踪损失和文本预测损失。推理优化只需2步去噪，实现5倍加速。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>EVT-Bench数据集包含100个人形化身和804个场景，总计25,986个episode，分为训练集(21,771)和测试集(4,215)，涵盖三个子任务：单目标跟踪(STT)、干扰跟踪(DT)、模糊跟踪(AT)。85.5万跟踪样本来自EVT-Bench训练集。85.5万识别样本包括36.2万人识别样本（基于SYNTH-PEDES）和49.3万开放世界VQA样本。训练资源方面，冻结视觉编码器参数，仅训练1个epoch。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境包括合成环境（EVT-Bench、Gym-UnrealCV公共基准）和真实环境（配备RGB摄像头的机器人平台）。评估指标包括跟踪性能（成功率，保持1-3米跟随距离且面向目标）、识别性能（文本预测准确率）、推理速度（FPS，报告10 FPS），以及轨迹回归误差(MSE)和分类损失(BCE)。</p>"
  },
  {
    "date": "2025-05-28",
    "title": "Zero-Shot 3D Visual Grounding from Vision-Language Models",
    "link": "http://arxiv.org/abs/2505.22429",
    "summary_markdown": "## 论文研究单位\n- 香港科技大学（广州）\n- 新加坡科技研究局信息通信研究院 (I2R, A*STAR)\n- 新加坡国立大学 (NUS)\n- 香港科技大学计算机科学与工程系 (CSE, HKUST)\n## 论文概述\n3D视觉定位（3DVG）旨在根据自然语言描述在3D场景中定位目标对象，对增强现实和机器人等下游应用至关重要。现有方法通常依赖于带标注的3D数据和预定义类别，限制了其在开放世界场景下的可扩展性。本文提出了SeeGround，一个零样本的3DVG框架，它利用2D视觉语言模型（VLMs）来绕过对3D特定训练的需求。为了弥合模态差距，论文引入了一种混合输入格式，该格式将查询对齐的渲染视图与空间丰富的文本描述相结合。该框架包含两个核心组件：一个基于查询动态选择最优视点的视角自适应模块，以及一个融合视觉和空间信号以提高定位精度的融合对齐模块。在ScanRefer和Nr3D数据集上的广泛评估证实，SeeGround显著优于现有的零样本基线，并与完全监督的替代方案相媲美，在挑战性条件下展现了强大的泛化能力。\n## 论文核心贡献点\n- 提出了SeeGround，一个用于零样本3D视觉定位的免训练方法，该方法通过渲染视图和空间文本将3D场景重新格式化为适合2D-VLMs的输入。\n- 设计了一种查询引导的视角选择策略，以捕获对象特定线索和空间上下文。\n- 提出了一种视觉提示机制，以对齐2D图像特征与3D空间描述，减少杂乱场景中的定位模糊性。\n- 该方法在ScanRefer和Nr3D上实现了最先进的零样本结果，证明了无需任何3D特定训练即可实现的强大泛化能力。\n## 论文方法描述\nSeeGround框架主要由三个部分组成：\n1. **多模态3D表示**：首先使用开放词汇3D检测器检测场景中的所有对象，并将其边界框和语义标签存储在一个对象查找表（OLT）中。然后，通过查询驱动的渲染函数生成一个与查询对齐的2D图像和相应的空间文本描述，构成混合表示。\n2. **视角自适应模块**：为了解决传统视角与查询视角不匹配的问题，该模块使用2D-VLM从查询中识别一个锚点对象和一组候选目标。虚拟相机被放置在场景中心，面向锚点对象，并略微后移和上移以获得更好的视野。如果无法明确提取锚点，则默认使用候选对象的质心作为伪锚点。\n3. **融合对齐模块**：该模块采用视觉提示技术。在渲染的图像上，将OLT中可见对象的投影2D边界框高亮显示，引导2D-VLM关注相关区域，解决模糊性问题。被提示的图像、空间文本和原始查询一起被输入到2D-VLM中，以输出目标对象的ID，最后从OLT中检索其对应的3D边界框。\n## 论文使用数据集和训练资源\n- **数据集**：\n - ScanRefer\n - Nr3D\n- **训练资源**：\n - 该方法是免训练和零样本的，不需要进行任何3D特定的训练。\n - 它使用预训练的2D视觉语言模型Qwen2-VL-72B作为核心代理。\n## 论文使用的评估环境和评估指标\n- **评估指标**：\n - 在不同交并比阈值下的定位准确率，具体为Acc@0.25IoU和Acc@0.5IoU。\n- **评估环境**：\n - 实验在ScanRefer和Nr3D的验证集上进行。文中未明确指定具体的硬件环境，但推断过程依赖于大型预训练模型Qwen2-VL-72B。",
    "summary_html": "<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>香港科技大学（广州）</li><li>新加坡科技研究局信息通信研究院 (I2R, A*STAR)</li><li>新加坡国立大学 (NUS)</li><li>香港科技大学计算机科学与工程系 (CSE, HKUST)</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<p>3D视觉定位（3DVG）旨在根据自然语言描述在3D场景中定位目标对象，对增强现实和机器人等下游应用至关重要。现有方法通常依赖于带标注的3D数据和预定义类别，限制了其在开放世界场景下的可扩展性。本文提出了SeeGround，一个零样本的3DVG框架，它利用2D视觉语言模型（VLMs）来绕过对3D特定训练的需求。为了弥合模态差距，论文引入了一种混合输入格式，该格式将查询对齐的渲染视图与空间丰富的文本描述相结合。该框架包含两个核心组件：一个基于查询动态选择最优视点的视角自适应模块，以及一个融合视觉和空间信号以提高定位精度的融合对齐模块。在ScanRefer和Nr3D数据集上的广泛评估证实，SeeGround显著优于现有的零样本基线，并与完全监督的替代方案相媲美，在挑战性条件下展现了强大的泛化能力。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了SeeGround，一个用于零样本3D视觉定位的免训练方法，该方法通过渲染视图和空间文本将3D场景重新格式化为适合2D-VLMs的输入。</li><li>设计了一种查询引导的视角选择策略，以捕获对象特定线索和空间上下文。</li><li>提出了一种视觉提示机制，以对齐2D图像特征与3D空间描述，减少杂乱场景中的定位模糊性。</li><li>该方法在ScanRefer和Nr3D上实现了最先进的零样本结果，证明了无需任何3D特定训练即可实现的强大泛化能力。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>SeeGround框架主要由三个部分组成：</p>\n<ol><li><strong>多模态3D表示</strong>：首先使用开放词汇3D检测器检测场景中的所有对象，并将其边界框和语义标签存储在一个对象查找表（OLT）中。然后，通过查询驱动的渲染函数生成一个与查询对齐的2D图像和相应的空间文本描述，构成混合表示。</li><li><strong>视角自适应模块</strong>：为了解决传统视角与查询视角不匹配的问题，该模块使用2D-VLM从查询中识别一个锚点对象和一组候选目标。虚拟相机被放置在场景中心，面向锚点对象，并略微后移和上移以获得更好的视野。如果无法明确提取锚点，则默认使用候选对象的质心作为伪锚点。</li><li><strong>融合对齐模块</strong>：该模块采用视觉提示技术。在渲染的图像上，将OLT中可见对象的投影2D边界框高亮显示，引导2D-VLM关注相关区域，解决模糊性问题。被提示的图像、空间文本和原始查询一起被输入到2D-VLM中，以输出目标对象的ID，最后从OLT中检索其对应的3D边界框。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - ScanRefer</p>\n<p> - Nr3D</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 该方法是免训练和零样本的，不需要进行任何3D特定的训练。</p>\n<p> - 它使用预训练的2D视觉语言模型Qwen2-VL-72B作为核心代理。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 在不同交并比阈值下的定位准确率，具体为Acc@0.25IoU和Acc@0.5IoU。</p>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 实验在ScanRefer和Nr3D的验证集上进行。文中未明确指定具体的硬件环境，但推断过程依赖于大型预训练模型Qwen2-VL-72B。</p>"
  },
  {
    "date": "2025-05-28",
    "title": "ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation",
    "link": "http://arxiv.org/abs/2505.22159",
    "summary_markdown": "### 论文研究单位\n复旦大学、上海交通大学、上海创新研究院、上海AI实验室、新加坡国立大学、上海大学、西安交通大学、Noematrix Intelligence\n### 论文概述\n论文提出ForceVLA，一种增强视觉-语言-动作（VLA）模型的端到端操作框架，通过将外部力感知作为核心模态，解决接触密集型操作任务中精细力控制的需求。该框架引入FVLMoE（力感知混合专家模块），动态融合预训练视觉-语言嵌入与实时6轴力反馈，并在五个接触密集任务中验证有效性，平均成功率提升23.2%，最高达80%。\n### 论文核心贡献点\n- 提出力感知多模态框架：首次将力作为一等公民模态集成到VLA系统，通过FVLMoE实现动态模态感知融合，增强物理交互能力。\n- 构建数据集与工具链：发布ForceVLA-Data数据集（244条轨迹，14万时间步）及完整数据采集系统（含遥操作工具和转换器），开源以支持社区研究。\n- 实验性能突破：在插拔、擦拭等任务中成功率显著超越基线（如π₀模型），泛化至新物体、遮挡和物理扰动场景，验证鲁棒性。\n### 论文方法描述\n- **整体架构**：基于π₀框架，扩展多模态输入（视觉、语言、本体感觉、力）。视觉和语言通过SigLIP编码为嵌入，力信号经线性投影后与VL嵌入融合。\n- **FVLMoE模块**：采用后融合策略，力令牌在VLM处理后输入MoE。包含4个专家网络（MLP）和路由器，通过Top-1路由动态选择专家，输出通过残差连接注入动作流解码器，指导力感知动作生成。\n- **动作生成**：采用条件流匹配模型，融合特征通过逐元素加法调制噪声动作轨迹，迭代去噪输出H步动作块。\n### 论文使用数据集和训练资源\n- **数据集**：ForceVLA-Data，涵盖5个任务（按瓶、插拔、USB插入、擦白板、削黄瓜）。通过Flexiv Rizon 7-DOF机械臂采集，双RGB-D相机（第三人称1280×720@30fps、手腕640×480@30fps）同步视觉、本体感觉和6轴力矩数据。\n- **训练资源**：8×NVIDIA RTX 4090 GPU（24GB VRAM），64核CPU，251GB RAM。使用Adam优化器（β₁=0.9, β₂=0.95），峰值学习率2.5×10⁻⁵，多任务训练30k步（约12小时），单任务训练10k步（约9小时），精度bfloat16。\n### 论文使用的评估环境和评估指标\n- **环境**：真实物理场景，五项接触密集任务：瓶泵按压、插头插入、USB插入、白板擦拭、黄瓜削皮。泛化测试包括物体更换（瓶/插头类型）、高度变化、视觉遮挡、插座不稳等扰动。\n- **指标**：主要任务成功率（按任务特定标准，如完全插入/擦拭）。削黄瓜任务附加指标：平均削皮长度（cm↑）、最少完成行程数（↓）。泛化场景报告各条件下成功率（%）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>复旦大学、上海交通大学、上海创新研究院、上海AI实验室、新加坡国立大学、上海大学、西安交通大学、Noematrix Intelligence</p>\n<h3>论文概述</h3>\n<p>论文提出ForceVLA，一种增强视觉-语言-动作（VLA）模型的端到端操作框架，通过将外部力感知作为核心模态，解决接触密集型操作任务中精细力控制的需求。该框架引入FVLMoE（力感知混合专家模块），动态融合预训练视觉-语言嵌入与实时6轴力反馈，并在五个接触密集任务中验证有效性，平均成功率提升23.2%，最高达80%。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出力感知多模态框架：首次将力作为一等公民模态集成到VLA系统，通过FVLMoE实现动态模态感知融合，增强物理交互能力。</li><li>构建数据集与工具链：发布ForceVLA-Data数据集（244条轨迹，14万时间步）及完整数据采集系统（含遥操作工具和转换器），开源以支持社区研究。</li><li>实验性能突破：在插拔、擦拭等任务中成功率显著超越基线（如π₀模型），泛化至新物体、遮挡和物理扰动场景，验证鲁棒性。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>整体架构</strong>：基于π₀框架，扩展多模态输入（视觉、语言、本体感觉、力）。视觉和语言通过SigLIP编码为嵌入，力信号经线性投影后与VL嵌入融合。</li><li><strong>FVLMoE模块</strong>：采用后融合策略，力令牌在VLM处理后输入MoE。包含4个专家网络（MLP）和路由器，通过Top-1路由动态选择专家，输出通过残差连接注入动作流解码器，指导力感知动作生成。</li><li><strong>动作生成</strong>：采用条件流匹配模型，融合特征通过逐元素加法调制噪声动作轨迹，迭代去噪输出H步动作块。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：ForceVLA-Data，涵盖5个任务（按瓶、插拔、USB插入、擦白板、削黄瓜）。通过Flexiv Rizon 7-DOF机械臂采集，双RGB-D相机（第三人称1280×720@30fps、手腕640×480@30fps）同步视觉、本体感觉和6轴力矩数据。</li><li><strong>训练资源</strong>：8×NVIDIA RTX 4090 GPU（24GB VRAM），64核CPU，251GB RAM。使用Adam优化器（β₁=0.9, β₂=0.95），峰值学习率2.5×10⁻⁵，多任务训练30k步（约12小时），单任务训练10k步（约9小时），精度bfloat16。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>环境</strong>：真实物理场景，五项接触密集任务：瓶泵按压、插头插入、USB插入、白板擦拭、黄瓜削皮。泛化测试包括物体更换（瓶/插头类型）、高度变化、视觉遮挡、插座不稳等扰动。</li><li><strong>指标</strong>：主要任务成功率（按任务特定标准，如完全插入/擦拭）。削黄瓜任务附加指标：平均削皮长度（cm↑）、最少完成行程数（↓）。泛化场景报告各条件下成功率（%）。</li></ul>"
  },
  {
    "date": "2025-05-28",
    "title": "ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge",
    "link": "http://arxiv.org/abs/2505.21906",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-27",
    "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2505.21567",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-27",
    "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
    "link": "http://arxiv.org/abs/2505.21432",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-27",
    "title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2505.21200",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-27",
    "title": "Hierarchical Instruction-aware Embodied Visual Tracking",
    "link": "http://arxiv.org/abs/2505.20710",
    "summary_markdown": "### 论文研究单位\nBeihang University, City University of Macau, Peking University, Mohamed bin Zayed University of Artificial Intelligence, Beijing Normal University\n### 论文概述\n本文提出了一个新的任务：用户中心具身视觉跟踪，并设计了一个名为分层指令感知具身视觉跟踪的代理来解决这个问题。该代理通过引入中间的“空间目标”作为桥梁，将高级用户指令与低级代理行为连接起来。该代理包含两个核心组件：一个基于LLM的语义-空间目标对齐器，用于将用户指令翻译成空间目标；以及一个基于RL的自适应目标对齐策略，使跟踪器能够根据空间目标定位目标。\n### 论文核心贡献点\n1. 引入了用户中心具身视觉跟踪（UC-EVT）任务，为以用户为中心的人机交互奠定了基础。\n2. 提出了一种新颖的分层指令感知EVT（HIEVT）模型，有效解决了现有最先进模型的局限性。\n3. 通过准备1000万条标注轨迹的大规模数据集，并在10个不同的虚拟环境中进行了广泛评估，为UC-EVT任务建立了基准。\n4. 在10个虚拟环境和不同移动速度上的大量实验表明HIEVT显著优于现有基线。此外，在三个不同环境中的真实世界部署验证了该方法的鲁棒性和有效性。\n### 论文方法描述\nHIEVT模型将用户指令与代理状态之间的距离分解为两部分：用户指令与中间目标之间的距离，以及中间目标与代理状态之间的距离。\n- 基于LLM的语义-空间目标对齐器（SSGA）：负责将用户指令翻译成空间目标，该目标由目标类别和边界框格式的空间位置表示。该对齐器包含三个核心组件：语义解析、空间目标生成和检索增强的目标校正。\n - 语义解析：从用户指令中提取关键信息，如目标类别。\n - 空间目标生成：使用基于链式思维（COT）的推理机制，增量地调整当前目标的空间表示，该表示由视觉基础模型从环境观测中提取。\n - 检索增强的目标校正：使用检索增强生成（RAG）机制，确保生成的边界框与存储在记忆库中的轨迹先验保持一致。\n- 基于RL的自适应目标对齐策略：一个通用的离线策略，它结合了空间目标和视觉基础模型处理的观测，输入到策略网络中。该策略网络包括目标状态对齐器和循环策略，以生成适当的动作信号，从而维持与目标的期望空间关系。\n### 论文使用数据集和训练资源\n- 数据集：收集了超过1000万条用于训练的标注轨迹，并在一个已见环境和九个未见过的具有挑战性的环境中进行评估。\n- 训练资源：未在提供的HTML文本中明确说明。\n### 论文使用的评估环境和评估指标\n- 评估环境：在10个多样化的虚拟环境（1个已见环境和9个未见过的环境）和不同的移动速度下进行评估。此外，还在三个不同的真实世界环境中进行了部署和验证。\n- 评估指标：未在提供的HTML文本中明确说明。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Beihang University, City University of Macau, Peking University, Mohamed bin Zayed University of Artificial Intelligence, Beijing Normal University</p>\n<h3>论文概述</h3>\n<p>本文提出了一个新的任务：用户中心具身视觉跟踪，并设计了一个名为分层指令感知具身视觉跟踪的代理来解决这个问题。该代理通过引入中间的“空间目标”作为桥梁，将高级用户指令与低级代理行为连接起来。该代理包含两个核心组件：一个基于LLM的语义-空间目标对齐器，用于将用户指令翻译成空间目标；以及一个基于RL的自适应目标对齐策略，使跟踪器能够根据空间目标定位目标。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>引入了用户中心具身视觉跟踪（UC-EVT）任务，为以用户为中心的人机交互奠定了基础。</li><li>提出了一种新颖的分层指令感知EVT（HIEVT）模型，有效解决了现有最先进模型的局限性。</li><li>通过准备1000万条标注轨迹的大规模数据集，并在10个不同的虚拟环境中进行了广泛评估，为UC-EVT任务建立了基准。</li><li>在10个虚拟环境和不同移动速度上的大量实验表明HIEVT显著优于现有基线。此外，在三个不同环境中的真实世界部署验证了该方法的鲁棒性和有效性。</li></ol>\n<h3>论文方法描述</h3>\n<p>HIEVT模型将用户指令与代理状态之间的距离分解为两部分：用户指令与中间目标之间的距离，以及中间目标与代理状态之间的距离。</p>\n<ul><li>基于LLM的语义-空间目标对齐器（SSGA）：负责将用户指令翻译成空间目标，该目标由目标类别和边界框格式的空间位置表示。该对齐器包含三个核心组件：语义解析、空间目标生成和检索增强的目标校正。</li></ul>\n<p> - 语义解析：从用户指令中提取关键信息，如目标类别。</p>\n<p> - 空间目标生成：使用基于链式思维（COT）的推理机制，增量地调整当前目标的空间表示，该表示由视觉基础模型从环境观测中提取。</p>\n<p> - 检索增强的目标校正：使用检索增强生成（RAG）机制，确保生成的边界框与存储在记忆库中的轨迹先验保持一致。</p>\n<ul><li>基于RL的自适应目标对齐策略：一个通用的离线策略，它结合了空间目标和视觉基础模型处理的观测，输入到策略网络中。该策略网络包括目标状态对齐器和循环策略，以生成适当的动作信号，从而维持与目标的期望空间关系。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：收集了超过1000万条用于训练的标注轨迹，并在一个已见环境和九个未见过的具有挑战性的环境中进行评估。</li><li>训练资源：未在提供的HTML文本中明确说明。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：在10个多样化的虚拟环境（1个已见环境和9个未见过的环境）和不同的移动速度下进行评估。此外，还在三个不同的真实世界环境中进行了部署和验证。</li><li>评估指标：未在提供的HTML文本中明确说明。</li></ul>"
  },
  {
    "date": "2025-05-26",
    "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
    "link": "http://arxiv.org/abs/2505.19789",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-26",
    "title": "RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback",
    "link": "http://arxiv.org/abs/2505.19767",
    "summary_markdown": "### 论文研究单位\n北京大学王选计算机技术研究所\n### 论文概述\n本文提出了一种名为RFTF的强化微调方法，用于具有时间反馈的具身智能体。该方法旨在解决现有具身智能体在行为克隆训练中依赖昂贵数据和计算资源的问题，以及强化微调中稀疏奖励无法为特定动作提供细粒度反馈的挑战。RFTF通过一个价值模型生成密集奖励，该模型使用时间信息训练，无需昂贵的机器人动作标签。实验结果表明，在CALVIN ABC-D基准测试中，RFTF实现了新的最先进性能，平均成功长度为4.296，并能快速适应新环境。\n### 论文核心贡献点\n1. 提出了一种无需人工机器人动作标签的密集奖励强化微调方法。\n2. 设计了一个利用时间信息训练的价值模型，用于生成密集奖励，并结合奖励塑形和GAE策略以促进RL微调过程。\n3. 在CALVIN基准上的实验验证了方法的有效性，实现了新的最先进性能，并展示了在新环境中的优越适应能力。\n### 论文方法描述\n方法包含两个阶段：\n1. 价值模型训练：使用专家演示轨迹中的时间信息，假设状态价值随时间单调递增，通过对比学习训练模型预测每个状态的价值。模型结构基于VLA，将动作token替换为价值token。\n2. RL微调流程：集成训练好的价值模型到PPO框架中，使用价值模型输出的状态值计算密集奖励，采用GAE计算优势函数，并引入样本平衡系数处理成功与失败样本的不平衡。奖励函数定义为R_t = γV(s_{t+1},l) - V(s_t,l)，优势函数结合任务成功/失败指示器。\n### 论文使用数据集和训练资源\n数据集：CALVIN基准，包含34个任务和四个不同的模拟环境。\n训练资源：价值模型训练使用批大小4×8和学习率1e-5；RL微调在4块NVIDIA A40 GPU上进行，Seer-Large模型约需10小时，GR-MG模型约需14小时，覆盖约1000个回合。\n### 论文使用的评估环境和评估指标\n评估环境：CALVIN基准的四个模拟环境。\n评估指标：\n- 平均成功长度：平均连续完成的任务数，越高越好。\n- 任务完成率：在1000个序列中完成1至5个任务的比例，从L1到L5分别表示完成n个任务的比例。\n- 价值模型准确率：评估价值模型预测状态值单调性的准确率，超过94%后选择第一轮次模型。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学王选计算机技术研究所</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为RFTF的强化微调方法，用于具有时间反馈的具身智能体。该方法旨在解决现有具身智能体在行为克隆训练中依赖昂贵数据和计算资源的问题，以及强化微调中稀疏奖励无法为特定动作提供细粒度反馈的挑战。RFTF通过一个价值模型生成密集奖励，该模型使用时间信息训练，无需昂贵的机器人动作标签。实验结果表明，在CALVIN ABC-D基准测试中，RFTF实现了新的最先进性能，平均成功长度为4.296，并能快速适应新环境。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种无需人工机器人动作标签的密集奖励强化微调方法。</li><li>设计了一个利用时间信息训练的价值模型，用于生成密集奖励，并结合奖励塑形和GAE策略以促进RL微调过程。</li><li>在CALVIN基准上的实验验证了方法的有效性，实现了新的最先进性能，并展示了在新环境中的优越适应能力。</li></ol>\n<h3>论文方法描述</h3>\n<p>方法包含两个阶段：</p>\n<ol><li>价值模型训练：使用专家演示轨迹中的时间信息，假设状态价值随时间单调递增，通过对比学习训练模型预测每个状态的价值。模型结构基于VLA，将动作token替换为价值token。</li><li>RL微调流程：集成训练好的价值模型到PPO框架中，使用价值模型输出的状态值计算密集奖励，采用GAE计算优势函数，并引入样本平衡系数处理成功与失败样本的不平衡。奖励函数定义为R_t = γV(s_{t+1},l) - V(s_t,l)，优势函数结合任务成功/失败指示器。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：CALVIN基准，包含34个任务和四个不同的模拟环境。</p>\n<p>训练资源：价值模型训练使用批大小4×8和学习率1e-5；RL微调在4块NVIDIA A40 GPU上进行，Seer-Large模型约需10小时，GR-MG模型约需14小时，覆盖约1000个回合。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：CALVIN基准的四个模拟环境。</p>\n<p>评估指标：</p>\n<ul><li>平均成功长度：平均连续完成的任务数，越高越好。</li><li>任务完成率：在1000个序列中完成1至5个任务的比例，从L1到L5分别表示完成n个任务的比例。</li><li>价值模型准确率：评估价值模型预测状态值单调性的准确率，超过94%后选择第一轮次模型。</li></ul>"
  },
  {
    "date": "2025-05-26",
    "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving",
    "link": "http://arxiv.org/abs/2505.19381",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-25",
    "title": "ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning",
    "link": "http://arxiv.org/abs/2505.19080",
    "summary_markdown": "```markdown\n### 论文研究单位\nVinRobotics (越南), Max Planck Research School for Intelligent Systems (德国), University of Texas at Arlington (美国), Automation & Control Institute, TU Wien (奥地利), Austrian Institute of Technology (AIT) (奥地利)\n### 论文概述\n本文提出 ReFineVLA，一个通过教师引导的微调框架，为视觉-语言-动作（VLA）模型注入显式多模态推理能力。传统VLA模型学习观察-动作的直接映射，缺乏推理步骤，限制了在复杂任务中的泛化和可解释性。ReFineVLA利用专家教师模型（如Gemini）为机器人轨迹生成结构化的推理标注，通过选择性微调上层参数，联合优化动作预测和推理生成，提升模型对任务逻辑的理解和决策透明度。\n### 论文核心贡献点\n1. 提出首个将显式多模态推理注入VLA模型的微调框架ReFineVLA，通过教师生成的自然语言推理链指导策略学习。\n2. 构建包含125,000个推理增强轨迹的数据集，覆盖观察分析、空间推理、任务规划等结构化推理步骤。\n3. 设计选择性微调策略，仅更新上层参数以保留预训练泛化能力，同时降低计算开销。\n4. 通过注意力图可视化验证，显示模型在推理注入后更关注语义相关物体而非局部动作线索。\n5. 在SimplerEnv基准上超越现有SOTA：WidowX任务平均成功率提升5.0%，变体聚合设置提升8.6%，视觉匹配任务提升1.7%。\n### 论文方法描述\n1. **推理标注生成**：使用教师模型（Gemini）为每个观察-动作对生成多模态推理标注，包含四部分结构：观察（物体识别）、情境分析（上下文理解）、空间推理（物体关系）、任务规划（动作步骤）。\n2. **选择性微调**：冻结VLA模型底层参数（如视觉编码器低层），仅微调上层Transformer块和策略头，避免破坏预训练特征。\n3. **多目标联合训练**：损失函数为动作预测损失（行为克隆）与推理生成损失（语言建模）的加权和，公式为：\n \\[\n \\mathcal{L}_{\\text{ReFineVLA}} = \\mathcal{L}_{\\text{action}} + \\lambda_{\\text{r}} \\mathcal{L}_{\\text{reasoning}}\n \\]\n 其中 \\(\\mathcal{L}_{\\text{action}} = -\\sum_t \\log \\mathbb{P}(a_{i,t} \\mid o_i, a_{i,<t}; \\theta)\\)，\\(\\mathcal{L}_{\\text{reasoning}}\\) 为推理文本的负对数似然。\n### 论文使用数据集和训练资源\n- **数据集**：自建125,000条机器人操作轨迹，每条包含观察（图像+指令）、专家动作及教师生成的推理标注。\n- **基础模型**：2B参数的预训练VLA模型（具体架构未说明）。\n- **训练资源**：未明确提及硬件配置，但强调选择性微调可减少计算负担。\n### 论文使用的评估环境和评估指标\n- **环境**：SimplerEnv模拟器，测试两种机器人平台：\n - WidowX Robot（7自由度机械臂）\n - Google Robot（移动操作平台）\n- **评估指标**：\n - **任务成功率**（Success Rate）：任务完成的成功比例，核心指标。\n - **注意力可视化**：通过分析动作token的注意力热图，验证模型对相关物体的关注程度。\n - **变体设置**：包括环境变化（如物体位置扰动）下的鲁棒性测试。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>VinRobotics (越南), Max Planck Research School for Intelligent Systems (德国), University of Texas at Arlington (美国), Automation & Control Institute, TU Wien (奥地利), Austrian Institute of Technology (AIT) (奥地利)</p>\n<h3>论文概述</h3>\n<p>本文提出 ReFineVLA，一个通过教师引导的微调框架，为视觉-语言-动作（VLA）模型注入显式多模态推理能力。传统VLA模型学习观察-动作的直接映射，缺乏推理步骤，限制了在复杂任务中的泛化和可解释性。ReFineVLA利用专家教师模型（如Gemini）为机器人轨迹生成结构化的推理标注，通过选择性微调上层参数，联合优化动作预测和推理生成，提升模型对任务逻辑的理解和决策透明度。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出首个将显式多模态推理注入VLA模型的微调框架ReFineVLA，通过教师生成的自然语言推理链指导策略学习。</li><li>构建包含125,000个推理增强轨迹的数据集，覆盖观察分析、空间推理、任务规划等结构化推理步骤。</li><li>设计选择性微调策略，仅更新上层参数以保留预训练泛化能力，同时降低计算开销。</li><li>通过注意力图可视化验证，显示模型在推理注入后更关注语义相关物体而非局部动作线索。</li><li>在SimplerEnv基准上超越现有SOTA：WidowX任务平均成功率提升5.0%，变体聚合设置提升8.6%，视觉匹配任务提升1.7%。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>推理标注生成</strong>：使用教师模型（Gemini）为每个观察-动作对生成多模态推理标注，包含四部分结构：观察（物体识别）、情境分析（上下文理解）、空间推理（物体关系）、任务规划（动作步骤）。</li><li><strong>选择性微调</strong>：冻结VLA模型底层参数（如视觉编码器低层），仅微调上层Transformer块和策略头，避免破坏预训练特征。</li><li><strong>多目标联合训练</strong>：损失函数为动作预测损失（行为克隆）与推理生成损失（语言建模）的加权和，公式为：</li></ol>\n<p> \\[</p>\n<p> \\mathcal{L}_{\\text{ReFineVLA}} = \\mathcal{L}_{\\text{action}} + \\lambda_{\\text{r}} \\mathcal{L}_{\\text{reasoning}}</p>\n<p> \\]</p>\n<p> 其中 \\(\\mathcal{L}_{\\text{action}} = -\\sum_t \\log \\mathbb{P}(a_{i,t} \\mid o_i, a_{i,<t}; \\theta)\\)，\\(\\mathcal{L}_{\\text{reasoning}}\\) 为推理文本的负对数似然。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：自建125,000条机器人操作轨迹，每条包含观察（图像+指令）、专家动作及教师生成的推理标注。</li><li><strong>基础模型</strong>：2B参数的预训练VLA模型（具体架构未说明）。</li><li><strong>训练资源</strong>：未明确提及硬件配置，但强调选择性微调可减少计算负担。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>环境</strong>：SimplerEnv模拟器，测试两种机器人平台：</li></ul>\n<p> - WidowX Robot（7自由度机械臂）</p>\n<p> - Google Robot（移动操作平台）</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率</strong>（Success Rate）：任务完成的成功比例，核心指标。</p>\n<p> - <strong>注意力可视化</strong>：通过分析动作token的注意力热图，验证模型对相关物体的关注程度。</p>\n<p> - <strong>变体设置</strong>：包括环境变化（如物体位置扰动）下的鲁棒性测试。</p>\n<p>```</p>"
  },
  {
    "date": "2025-05-24",
    "title": "Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance",
    "link": "http://arxiv.org/abs/2505.18793",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-24",
    "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
    "link": "http://arxiv.org/abs/2505.18719",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-22",
    "title": "ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems",
    "link": "http://arxiv.org/abs/2505.17295",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-22",
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2505.17016",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-22",
    "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2505.16278",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-21",
    "title": "UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning",
    "link": "http://arxiv.org/abs/2505.15725",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-21",
    "title": "From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems",
    "link": "http://arxiv.org/abs/2505.15685",
    "summary_markdown": "### 论文研究单位\n- IHPC, Agency for Science, Technology and Research, Singapore\n- Nanyang Technological University, Singapore\n### 论文概述\n论文研究了三种基础模型（Foundation Models）在具身机器人系统中的集成策略：端到端视觉-语言-动作模型（VLAs）、模块化视觉-语言模型（VLM）管道，以及多模态大语言模型（MLLM）代理作为编排者。通过两个案例研究——指令基础（instruction grounding）和机器人操作（robotic manipulation）——评估了这些策略在复杂指令跟随和通用动作生成中的权衡。实验揭示了模型规模、泛化能力和数据效率之间的平衡点，为语言驱动的物理代理提供了设计见解。\n### 论文核心贡献点\n1. **范式分析**：系统分析了三种FM集成范式在共享具身任务上的能力与权衡。\n2. **资源发布**：发布了一个数据集和代码，用于评估指令基础和对象操作，涵盖跨模态推理和不同布局下的技能适应。\n3. **实践见解**：提供了对最先进VLAs和MLLMs的及时洞察，调查了它们的能力和失败模式，为实践者选择FM堆栈提供了实用权衡指导。\n4. **系统演示**：发布了一个完整的端到端抓娃娃机器人系统作为真实世界FM集成演示。\n### 论文方法描述\n1. **端到端VLA模型**：\n - **自回归模型**：逐步生成动作，基于当前感知输入和历史输出预测低级控制令牌（如关节角度）。\n - **扩散模型**：通过去噪轨迹生成动作，建模未来动作的分布而非逐步生成。\n - **优势与局限**：利用大规模预训练实现任务泛化，但受限于数据稀缺性和对新任务/环境的脆弱性。\n2. **模块化VLM管道**：\n - 专业化VLM处理感知，输出符号化场景信息（如边界框、分割掩码），下游规划器生成动作。\n - 优势：可解释性和高效率（模型参数仅100M-600M）。\n - 局限：交互刚性，感知错误传播无缓解。\n3. **多模态LLM代理作为编排者**：\n - MLLM作为认知枢纽，通过函数调用调用视觉工具，推理后输出高级动作基元。\n - 优势：视觉常识推理和指令跟随能力强。\n - 局限：资源密集型，部署挑战大。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **杂乱桌面操作数据集**：163个演示片段，目标是在杂乱环境中拾取螺丝刀，使用UR5机械臂和RealSense相机收集。\n - **复杂指令基础数据集**：473条指令，包含隐式、属性显式和空间关系指令，用于跨模态消歧。\n- **训练资源**：\n - 部分微调：单块NVIDIA A6000 GPU（48GB VRAM），持续3天。\n - 全量微调：8×H100 GPU节点。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **真实环境**：WidowX机器人平台（真实世界任务）。\n - **仿真环境**：LIBERO基准测试（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。\n- **评估指标**：\n - **成功率（%）**：操作任务中的任务完成率，每个任务500次试验。\n - **宏平均准确率**：指令基础任务中物体定位和复杂指令理解的准确率。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>IHPC, Agency for Science, Technology and Research, Singapore</li><li>Nanyang Technological University, Singapore</li></ul>\n<h3>论文概述</h3>\n<p>论文研究了三种基础模型（Foundation Models）在具身机器人系统中的集成策略：端到端视觉-语言-动作模型（VLAs）、模块化视觉-语言模型（VLM）管道，以及多模态大语言模型（MLLM）代理作为编排者。通过两个案例研究——指令基础（instruction grounding）和机器人操作（robotic manipulation）——评估了这些策略在复杂指令跟随和通用动作生成中的权衡。实验揭示了模型规模、泛化能力和数据效率之间的平衡点，为语言驱动的物理代理提供了设计见解。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>范式分析</strong>：系统分析了三种FM集成范式在共享具身任务上的能力与权衡。</li><li><strong>资源发布</strong>：发布了一个数据集和代码，用于评估指令基础和对象操作，涵盖跨模态推理和不同布局下的技能适应。</li><li><strong>实践见解</strong>：提供了对最先进VLAs和MLLMs的及时洞察，调查了它们的能力和失败模式，为实践者选择FM堆栈提供了实用权衡指导。</li><li><strong>系统演示</strong>：发布了一个完整的端到端抓娃娃机器人系统作为真实世界FM集成演示。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>端到端VLA模型</strong>：</li></ol>\n<p> - <strong>自回归模型</strong>：逐步生成动作，基于当前感知输入和历史输出预测低级控制令牌（如关节角度）。</p>\n<p> - <strong>扩散模型</strong>：通过去噪轨迹生成动作，建模未来动作的分布而非逐步生成。</p>\n<p> - <strong>优势与局限</strong>：利用大规模预训练实现任务泛化，但受限于数据稀缺性和对新任务/环境的脆弱性。</p>\n<ol><li><strong>模块化VLM管道</strong>：</li></ol>\n<p> - 专业化VLM处理感知，输出符号化场景信息（如边界框、分割掩码），下游规划器生成动作。</p>\n<p> - 优势：可解释性和高效率（模型参数仅100M-600M）。</p>\n<p> - 局限：交互刚性，感知错误传播无缓解。</p>\n<ol><li><strong>多模态LLM代理作为编排者</strong>：</li></ol>\n<p> - MLLM作为认知枢纽，通过函数调用调用视觉工具，推理后输出高级动作基元。</p>\n<p> - 优势：视觉常识推理和指令跟随能力强。</p>\n<p> - 局限：资源密集型，部署挑战大。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>杂乱桌面操作数据集</strong>：163个演示片段，目标是在杂乱环境中拾取螺丝刀，使用UR5机械臂和RealSense相机收集。</p>\n<p> - <strong>复杂指令基础数据集</strong>：473条指令，包含隐式、属性显式和空间关系指令，用于跨模态消歧。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 部分微调：单块NVIDIA A6000 GPU（48GB VRAM），持续3天。</p>\n<p> - 全量微调：8×H100 GPU节点。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>真实环境</strong>：WidowX机器人平台（真实世界任务）。</p>\n<p> - <strong>仿真环境</strong>：LIBERO基准测试（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（%）</strong>：操作任务中的任务完成率，每个任务500次试验。</p>\n<p> - <strong>宏平均准确率</strong>：指令基础任务中物体定位和复杂指令理解的准确率。</p>"
  },
  {
    "date": "2025-05-21",
    "title": "Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization",
    "link": "http://arxiv.org/abs/2505.15660",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-21",
    "title": "FLARE: Robot Learning with Implicit World Modeling",
    "link": "http://arxiv.org/abs/2505.15659",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-21",
    "title": "Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control",
    "link": "http://arxiv.org/abs/2505.15304",
    "summary_markdown": "论文研究单位\nHanyang University, Hyundai Motor Company, Seoul, Republic of Korea\n\n论文概述\n本文提出显著性感知量化模仿学习（SQIL）方法，用于高效机器人控制。深度神经网络策略模型（如视觉-语言-动作模型）在复杂决策中表现出色，但模型规模扩大导致计算开销增加，难以在资源受限设备（如机器人操作、自动驾驶）上部署。SQIL结合量化感知训练与选择性损失加权策略，通过显著性分数识别任务关键状态并在训练损失中强调这些状态，从而在低比特精度下保持决策保真度。实验表明，SQIL在多个模拟基准、真实任务和跨域任务（自动驾驶、物理模拟）中恢复全精度性能：例如，在机器人操作LIBERO基准上，4-bit权重量化模型在边缘GPU实现2.5倍加速和2.5倍节能；在自动驾驶NoCrash基准上，4-bit权重和激活量化策略保持全精度性能，在低端GPU实现3.7倍加速和3.1倍节能，展示了其在资源受限设备上部署大规模策略的潜力。\n\n论文核心贡献点\n首次系统研究量化模仿学习，识别任务关键状态的重要性，发现量化策略失败主要源于物理交互中的粗粒度控制，解释朴素量化失败原因。\n提出基于策略的关键状态检测方法SIS，利用策略动作敏感性识别任务关键状态，超越传统视觉-语言关键帧检测器。\n设计SQIL框架，将4-bit量化感知训练与SIS加权损失结合，实现2-4倍加速和节能，同时成功率保持在全精度基线1%以内。\n在机器人操作、自动驾驶和MuJoCo控制等跨域任务中进行广泛验证，证实SQIL在模拟和真实环境中的通用性。\n\n论文方法描述\nSQIL包含两个核心组件：显著性状态重要性评分（SIS）和量化鲁棒动作蒸馏（QRD）。SIS通过测量视觉扰动下的动作差异量化状态重要性，公式为 $S_{\\pi}(s_{t},k) = \\frac{1}{2} \\\\|\\pi(s_{t}) - \\pi(\\phi(s_{t},k)) \\\\|^{2}$，平均分数 $SIS^{s_{t}}_{\\pi}$ 越高表示状态越关键，如抓取或释放物体时刻；QRD在量化感知训练中，对SIS识别的高重要性状态施加更高损失权重，使量化策略更贴近全精度策略在关键状态下的决策。训练时从全精度策略初始化量化策略，计算SIS后，循环N个epoch遍历专家数据，更新量化参数。\n\n论文使用数据集和训练资源\n数据集包括LIBERO机器人操作基准（1700个模拟episode用于微调OpenVLA）、NoCrash自动驾驶基准（用于CILRS模型）和D4RL物理模拟基准（用于MuJoCo控制），并涉及真实世界任务。训练资源使用预训练模型如OpenVLA（970k episode预训练后LoRA微调）、CILRS和D4RL策略，量化至4-bit权重或权重-激活；训练超参数（如学习率）与全精度训练一致，硬件未明确指定。\n\n论文使用的评估环境和评估指标\n评估环境涵盖模拟环境（LIBERO、NoCrash、D4RL）和真实机器人任务，硬件平台包括边缘GPU（机器人操作）和低端GPU（自动驾驶）。主要评估指标为成功率（如LIBERO平均成功率），次要指标包括动作差异（L2-norm）、推理速度加速比（如2.5x）和节能比（如2.5x），并测试环境变化下的泛化性能。",
    "summary_html": "<p>论文研究单位</p>\n<p>Hanyang University, Hyundai Motor Company, Seoul, Republic of Korea</p>\n\n<p>论文概述</p>\n<p>本文提出显著性感知量化模仿学习（SQIL）方法，用于高效机器人控制。深度神经网络策略模型（如视觉-语言-动作模型）在复杂决策中表现出色，但模型规模扩大导致计算开销增加，难以在资源受限设备（如机器人操作、自动驾驶）上部署。SQIL结合量化感知训练与选择性损失加权策略，通过显著性分数识别任务关键状态并在训练损失中强调这些状态，从而在低比特精度下保持决策保真度。实验表明，SQIL在多个模拟基准、真实任务和跨域任务（自动驾驶、物理模拟）中恢复全精度性能：例如，在机器人操作LIBERO基准上，4-bit权重量化模型在边缘GPU实现2.5倍加速和2.5倍节能；在自动驾驶NoCrash基准上，4-bit权重和激活量化策略保持全精度性能，在低端GPU实现3.7倍加速和3.1倍节能，展示了其在资源受限设备上部署大规模策略的潜力。</p>\n\n<p>论文核心贡献点</p>\n<p>首次系统研究量化模仿学习，识别任务关键状态的重要性，发现量化策略失败主要源于物理交互中的粗粒度控制，解释朴素量化失败原因。</p>\n<p>提出基于策略的关键状态检测方法SIS，利用策略动作敏感性识别任务关键状态，超越传统视觉-语言关键帧检测器。</p>\n<p>设计SQIL框架，将4-bit量化感知训练与SIS加权损失结合，实现2-4倍加速和节能，同时成功率保持在全精度基线1%以内。</p>\n<p>在机器人操作、自动驾驶和MuJoCo控制等跨域任务中进行广泛验证，证实SQIL在模拟和真实环境中的通用性。</p>\n\n<p>论文方法描述</p>\n<p>SQIL包含两个核心组件：显著性状态重要性评分（SIS）和量化鲁棒动作蒸馏（QRD）。SIS通过测量视觉扰动下的动作差异量化状态重要性，公式为 $S_{\\pi}(s_{t},k) = \\frac{1}{2} \\\\|\\pi(s_{t}) - \\pi(\\phi(s_{t},k)) \\\\|^{2}$，平均分数 $SIS^{s_{t}}_{\\pi}$ 越高表示状态越关键，如抓取或释放物体时刻；QRD在量化感知训练中，对SIS识别的高重要性状态施加更高损失权重，使量化策略更贴近全精度策略在关键状态下的决策。训练时从全精度策略初始化量化策略，计算SIS后，循环N个epoch遍历专家数据，更新量化参数。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集包括LIBERO机器人操作基准（1700个模拟episode用于微调OpenVLA）、NoCrash自动驾驶基准（用于CILRS模型）和D4RL物理模拟基准（用于MuJoCo控制），并涉及真实世界任务。训练资源使用预训练模型如OpenVLA（970k episode预训练后LoRA微调）、CILRS和D4RL策略，量化至4-bit权重或权重-激活；训练超参数（如学习率）与全精度训练一致，硬件未明确指定。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境涵盖模拟环境（LIBERO、NoCrash、D4RL）和真实机器人任务，硬件平台包括边缘GPU（机器人操作）和低端GPU（自动驾驶）。主要评估指标为成功率（如LIBERO平均成功率），次要指标包括动作差异（L2-norm）、推理速度加速比（如2.5x）和节能比（如2.5x），并测试环境变化下的泛化性能。</p>"
  },
  {
    "date": "2025-05-21",
    "title": "EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy",
    "link": "http://arxiv.org/abs/2505.15206",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-21",
    "title": "Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation",
    "link": "http://arxiv.org/abs/2505.15098",
    "summary_markdown": "### 论文研究单位\nJD Explore Academy, JD Company\n北京交通大学 (Beijing Jiaotong University)\n### 论文概述\n本文提出了一种名为Object-Focus Actor (OFA)的新型数据高效方法，用于机器人的泛化灵巧操作。该方法利用在灵巧操作任务中观察到的轨迹一致性，允许高效的策略训练。OFA采用分层流程：物体感知和姿态估计、预操作姿态到达和OFA策略执行。通过在真实世界中的七个任务进行综合实验，证明OFA在位置和背景泛化测试中显著优于基线方法。值得注意的是，OFA仅用10次演示就能实现稳健的性能，突显了其数据效率。\n### 论文核心贡献点\n1. 提出了Object-Focus Actor (OFA)框架，通过关注物体最终的一致性轨迹，实现数据高效的泛化灵巧操作。\n2. 设计了分层处理流程，包括物体感知与姿态估计、预操作姿态到达和物体中心策略学习，解决了物体位置泛化的挑战。\n3. 引入了Hand-Focus图像和相对本体感觉表示，使策略能专注于核心操作区域，减少对背景的敏感性。\n4. 在真实世界的七个任务上验证了方法的有效性，证明了在位置和背景泛化上的优越性能，并展示了仅需10次演示的数据效率。\n### 论文方法描述\nOFA方法包含三个核心模块：\n1. **物体感知与姿态估计**：使用GroundingDINO进行物体定位，SAM进行分割，FoundationPose进行6D姿态估计，基于物体的CAD模型和单目图像。\n2. **预操作姿态到达**：根据物体类别设置统一的预操作姿态偏移（旋转和平移），通过CuRobo规划器生成无碰撞的运动轨迹，引导机械手到达该姿态。\n3. **物体中心策略学习**：\n - 构建Hand-Focus图像：通过机器人正向运动学计算手部在相机坐标系中的投影区域，放大至包含手部和物体，并裁剪为统一尺寸。\n - 采用相对本体感觉：包括相对于预操作姿态的姿态（位置和轴角）和手指关节角度。\n - 使用相对动作块：策略预测未来k步的相对动作序列，包含相对姿态和手指角度。\n - 模型架构：基于条件变分自编码器（CVAE），类似ACT，输入为Hand-Focus图像、相对本体感觉，输出为相对动作块。\n - 训练损失：包括重建损失和KL散度正则化项。\n### 论文使用数据集和训练资源\n- **数据集**：针对7个灵巧操作任务（如抓杯子、拿马克杯、持条码扫描器等）收集人类演示数据，每个任务使用30次演示。演示过程中物体位置随机放置，机械手初始位置固定。\n- **硬件平台**：Realman双臂平台，配备6自由度机械臂、Inspire RH56BFX灵巧手和头部ZED2立体相机。\n- **训练资源**：未明确提及，但通过变分自编码器架构和批量处理进行策略学习，训练使用相对动作块和Hand-Focus图像。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在真实物理环境中进行实验，使用双臂机器人平台和固定相机。\n- **评估指标**：\n - **成功率**：任务完成的百分比，在不同条件下评估（如标准测试、位置泛化、背景变化）。\n - **泛化测试**：\n - 位置泛化：物体放置于训练数据未见的区域，评估10次测试。\n - 背景泛化：改变环境背景，评估10次测试。\n - **数据效率分析**：比较不同演示数量（10、20、30次）下的性能，每个条件评估10次。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>JD Explore Academy, JD Company</p>\n<p>北京交通大学 (Beijing Jiaotong University)</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为Object-Focus Actor (OFA)的新型数据高效方法，用于机器人的泛化灵巧操作。该方法利用在灵巧操作任务中观察到的轨迹一致性，允许高效的策略训练。OFA采用分层流程：物体感知和姿态估计、预操作姿态到达和OFA策略执行。通过在真实世界中的七个任务进行综合实验，证明OFA在位置和背景泛化测试中显著优于基线方法。值得注意的是，OFA仅用10次演示就能实现稳健的性能，突显了其数据效率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了Object-Focus Actor (OFA)框架，通过关注物体最终的一致性轨迹，实现数据高效的泛化灵巧操作。</li><li>设计了分层处理流程，包括物体感知与姿态估计、预操作姿态到达和物体中心策略学习，解决了物体位置泛化的挑战。</li><li>引入了Hand-Focus图像和相对本体感觉表示，使策略能专注于核心操作区域，减少对背景的敏感性。</li><li>在真实世界的七个任务上验证了方法的有效性，证明了在位置和背景泛化上的优越性能，并展示了仅需10次演示的数据效率。</li></ol>\n<h3>论文方法描述</h3>\n<p>OFA方法包含三个核心模块：</p>\n<ol><li><strong>物体感知与姿态估计</strong>：使用GroundingDINO进行物体定位，SAM进行分割，FoundationPose进行6D姿态估计，基于物体的CAD模型和单目图像。</li><li><strong>预操作姿态到达</strong>：根据物体类别设置统一的预操作姿态偏移（旋转和平移），通过CuRobo规划器生成无碰撞的运动轨迹，引导机械手到达该姿态。</li><li><strong>物体中心策略学习</strong>：</li></ol>\n<p> - 构建Hand-Focus图像：通过机器人正向运动学计算手部在相机坐标系中的投影区域，放大至包含手部和物体，并裁剪为统一尺寸。</p>\n<p> - 采用相对本体感觉：包括相对于预操作姿态的姿态（位置和轴角）和手指关节角度。</p>\n<p> - 使用相对动作块：策略预测未来k步的相对动作序列，包含相对姿态和手指角度。</p>\n<p> - 模型架构：基于条件变分自编码器（CVAE），类似ACT，输入为Hand-Focus图像、相对本体感觉，输出为相对动作块。</p>\n<p> - 训练损失：包括重建损失和KL散度正则化项。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：针对7个灵巧操作任务（如抓杯子、拿马克杯、持条码扫描器等）收集人类演示数据，每个任务使用30次演示。演示过程中物体位置随机放置，机械手初始位置固定。</li><li><strong>硬件平台</strong>：Realman双臂平台，配备6自由度机械臂、Inspire RH56BFX灵巧手和头部ZED2立体相机。</li><li><strong>训练资源</strong>：未明确提及，但通过变分自编码器架构和批量处理进行策略学习，训练使用相对动作块和Hand-Focus图像。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在真实物理环境中进行实验，使用双臂机器人平台和固定相机。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：任务完成的百分比，在不同条件下评估（如标准测试、位置泛化、背景变化）。</p>\n<p> - <strong>泛化测试</strong>：</p>\n<p> - 位置泛化：物体放置于训练数据未见的区域，评估10次测试。</p>\n<p> - 背景泛化：改变环境背景，评估10次测试。</p>\n<p> - <strong>数据效率分析</strong>：比较不同演示数量（10、20、30次）下的性能，每个条件评估10次。</p>"
  },
  {
    "date": "2025-05-20",
    "title": "AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory",
    "link": "http://arxiv.org/abs/2505.14030",
    "summary_markdown": "### 论文研究单位\n香港大学（HKU）、TeleAI、清华大学（THU）、上海交通大学（SJTU）、香港大学上海智能计算中心\n### 论文概述\nAutoBio 是一个针对数字生物实验室的机器人自动化模拟框架和基准测试。现有视觉-语言-动作（VLA）模型的基准测试主要聚焦于家庭任务，缺乏对专业科学领域的评估。生物实验室实验具有结构化协议、高精度要求和多模态交互的特点，但现有模拟器在处理流体、数字界面和实验室特有物理机制方面存在局限。AutoBio 通过仪器数字化、物理插件和渲染技术，构建了一个高保真度的生物实验模拟环境，并设计了一套涵盖基础生物学操作的基准任务，用于评估VLA模型在科学工作流中的表现。\n### 论文核心贡献点\n1. **生物实验室专用模拟器**：开发了仪器数字化流程、物理插件（螺纹/棘轮机制、准静态液体）和基于物理的渲染（PBR）支持透明材料与动态界面。\n2. **生物学基础任务基准**：提供16项任务，分三个难度级别，支持轨迹合成和VLA模型集成，用于标准化评估实验协议中的机器人操作。\n3. **科学导向环境的VLA评估**：系统测试了两个SOTA VLA模型（π₀和RDT），揭示了当前模型在精度操作、指令遵循和视觉推理方面的显著差距。\n### 论文方法描述\n**模拟器架构**：\n- **AutoBio Assets**：通过3D高斯泼溅（3DGS）结合CAD细化流程，将真实实验室仪器（如离心机、热循环仪）转换为可交互的数字资产。\n- **AutoBio Physics**：基于MuJoCo引擎，开发定制物理插件，包括螺纹机制（使用圆形螺旋线的SDF模拟）、棘轮机制（离散位置反馈）、偏心机制（振荡运动模拟）和准静态液体（平面界面简化）。\n- **AutoBio Rendering**：支持两种后端——基础渲染（快速OpenGL渲染）和高级渲染（Blender PBR实现透明材料与动态纹理渲染），后者确保容器和液体的视觉保真度。\n- **反应式界面**：动态纹理渲染实现仪器控制面板的实时交互反馈。\n\n**基准设计**：\n- **任务分级**：简单级（如关闭热循环仪盖）、中等难度（如旋开离心管盖）、困难级（如操作混匀仪面板），任务涵盖场景初始化、示范合成、状态检查和VLA接口。\n- **机器人配置**：支持单臂（Aloha）和双臂（UR5e搭配Robotiq夹爪或DexHand），提供多相机视角（全局和手腕相机）。\n### 论文使用数据集和训练资源\n- **数据集**：从9个任务中生成100条示范轨迹（50Hz），总计792k帧（约4.4小时连续数据），格式为LeRobot数据集，发布于HuggingFace。\n- **训练资源**：在NVIDIA H800 GPU上训练，批次大小32，30,000步，单次运行10-14小时，总计约1,500 GPU小时。\n### 论文使用的评估环境和评估指标\n- **评估环境**：AutoBio模拟器，包含16个任务，分三个难度级别（每级3个测试任务），环境支持随机化参数（如目标位置）。\n- **评估指标**：\n - **任务成功率**：二进制评分（成功=1，失败=0），适用于大部分任务。\n - **相对进度分数**：用于\"操作混匀仪面板\"任务，反映部分完成度。\n - **指标计算**：100次测试 episodes的平均成功率（%），报告均值及标准误（如99.7±0.3）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>香港大学（HKU）、TeleAI、清华大学（THU）、上海交通大学（SJTU）、香港大学上海智能计算中心</p>\n<h3>论文概述</h3>\n<p>AutoBio 是一个针对数字生物实验室的机器人自动化模拟框架和基准测试。现有视觉-语言-动作（VLA）模型的基准测试主要聚焦于家庭任务，缺乏对专业科学领域的评估。生物实验室实验具有结构化协议、高精度要求和多模态交互的特点，但现有模拟器在处理流体、数字界面和实验室特有物理机制方面存在局限。AutoBio 通过仪器数字化、物理插件和渲染技术，构建了一个高保真度的生物实验模拟环境，并设计了一套涵盖基础生物学操作的基准任务，用于评估VLA模型在科学工作流中的表现。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>生物实验室专用模拟器</strong>：开发了仪器数字化流程、物理插件（螺纹/棘轮机制、准静态液体）和基于物理的渲染（PBR）支持透明材料与动态界面。</li><li><strong>生物学基础任务基准</strong>：提供16项任务，分三个难度级别，支持轨迹合成和VLA模型集成，用于标准化评估实验协议中的机器人操作。</li><li><strong>科学导向环境的VLA评估</strong>：系统测试了两个SOTA VLA模型（π₀和RDT），揭示了当前模型在精度操作、指令遵循和视觉推理方面的显著差距。</li></ol>\n<h3>论文方法描述</h3>\n<p><strong>模拟器架构</strong>：</p>\n<ul><li><strong>AutoBio Assets</strong>：通过3D高斯泼溅（3DGS）结合CAD细化流程，将真实实验室仪器（如离心机、热循环仪）转换为可交互的数字资产。</li><li><strong>AutoBio Physics</strong>：基于MuJoCo引擎，开发定制物理插件，包括螺纹机制（使用圆形螺旋线的SDF模拟）、棘轮机制（离散位置反馈）、偏心机制（振荡运动模拟）和准静态液体（平面界面简化）。</li><li><strong>AutoBio Rendering</strong>：支持两种后端——基础渲染（快速OpenGL渲染）和高级渲染（Blender PBR实现透明材料与动态纹理渲染），后者确保容器和液体的视觉保真度。</li><li><strong>反应式界面</strong>：动态纹理渲染实现仪器控制面板的实时交互反馈。</li></ul>\n\n<p><strong>基准设计</strong>：</p>\n<ul><li><strong>任务分级</strong>：简单级（如关闭热循环仪盖）、中等难度（如旋开离心管盖）、困难级（如操作混匀仪面板），任务涵盖场景初始化、示范合成、状态检查和VLA接口。</li><li><strong>机器人配置</strong>：支持单臂（Aloha）和双臂（UR5e搭配Robotiq夹爪或DexHand），提供多相机视角（全局和手腕相机）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：从9个任务中生成100条示范轨迹（50Hz），总计792k帧（约4.4小时连续数据），格式为LeRobot数据集，发布于HuggingFace。</li><li><strong>训练资源</strong>：在NVIDIA H800 GPU上训练，批次大小32，30,000步，单次运行10-14小时，总计约1,500 GPU小时。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：AutoBio模拟器，包含16个任务，分三个难度级别（每级3个测试任务），环境支持随机化参数（如目标位置）。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率</strong>：二进制评分（成功=1，失败=0），适用于大部分任务。</p>\n<p> - <strong>相对进度分数</strong>：用于\"操作混匀仪面板\"任务，反映部分完成度。</p>\n<p> - <strong>指标计算</strong>：100次测试 episodes的平均成功率（%），报告均值及标准误（如99.7±0.3）。</p>"
  },
  {
    "date": "2025-05-20",
    "title": "InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning",
    "link": "http://arxiv.org/abs/2505.13888",
    "summary_markdown": "### 论文研究单位\n西南交通大学、电子科技大学、同济大学\n### 论文概述\n本文提出了一种名为InSpire（Intrinsic Spatial Reasoning）的方法，旨在提升视觉-语言-动作模型（VLAs）的空间推理能力，以减少虚假相关性对模型泛化性能的负面影响。现有VLAs倾向于在任务无关的视觉特征与动作之间建立虚假相关性，限制了其泛化能力。InSpire通过在语言指令前添加一个关于目标物体相对于机器人方向的视觉问答（VQA）问题，引导模型关注任务相关因素，并将模型的答案与真实动作对齐，从而增强空间推理。该方法无需额外数据或与其他大型模型交互，可作为插件式模块增强现有自回归VLAs。\n### 论文核心贡献点\n1. 提出InSpire，一种缓解虚假相关性对VLAs泛化性能负面影响的新方法。\n2. 无需使用额外数据或与其他大型模型交互，以即插即用的方式赋予VLAs空间推理能力。\n3. 在模拟和真实世界环境中进行了全面评估，验证了InSpire的有效性和灵活性。\n### 论文方法描述\n1. InSpire通过在语言指令前添加问题“In which direction is the [object] relative to the robot?”，引导VLA进行空间推理。\n2. 模型首先生成该问题的文本答案（如right/left/up/down/front/back/grasped），该答案与问题共同构成任务相关因素的文本表示。\n3. 随后将此文本表示作为额外输入，传递给同一VLA以生成最终动作。\n4. 训练时，使用基于规则的自动化标注方法，通过机器人末端执行器与目标物体的3D位置差计算真实空间关系标签，对空间推理答案和动作同时施加自回归损失。\n### 论文使用数据集和训练资源\n1. 数据集：LIBERO（包含LIBERO-90、Spatial、Object、Goal、Long子集）、CALVIN（长期语言条件操作任务）、以及真实世界数据（10个已见任务和5个未见任务）。\n2. 训练资源：基于miniVLA-VQ和π₀-FAST模型进行实验，学习率1e-5，训练步数50,000，使用3个随机种子。真实世界实验使用AGILEX PiPER 6DOF机械臂，每个任务10条演示轨迹。\n### 论文使用的评估环境和评估指标\n1. 评估环境：模拟环境（LIBERO和CALVIN）、真实世界环境（AGILEX PiPER 6DOF机械臂）。\n2. 评估指标：成功率（Success Rate），其中LIBERO和CALVIN分别对每个任务进行100次和500次试验；真实世界任务每个任务进行10次试验。CALVIN额外评估顺序任务完成的平均数量。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>西南交通大学、电子科技大学、同济大学</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为InSpire（Intrinsic Spatial Reasoning）的方法，旨在提升视觉-语言-动作模型（VLAs）的空间推理能力，以减少虚假相关性对模型泛化性能的负面影响。现有VLAs倾向于在任务无关的视觉特征与动作之间建立虚假相关性，限制了其泛化能力。InSpire通过在语言指令前添加一个关于目标物体相对于机器人方向的视觉问答（VQA）问题，引导模型关注任务相关因素，并将模型的答案与真实动作对齐，从而增强空间推理。该方法无需额外数据或与其他大型模型交互，可作为插件式模块增强现有自回归VLAs。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出InSpire，一种缓解虚假相关性对VLAs泛化性能负面影响的新方法。</li><li>无需使用额外数据或与其他大型模型交互，以即插即用的方式赋予VLAs空间推理能力。</li><li>在模拟和真实世界环境中进行了全面评估，验证了InSpire的有效性和灵活性。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>InSpire通过在语言指令前添加问题“In which direction is the [object] relative to the robot?”，引导VLA进行空间推理。</li><li>模型首先生成该问题的文本答案（如right/left/up/down/front/back/grasped），该答案与问题共同构成任务相关因素的文本表示。</li><li>随后将此文本表示作为额外输入，传递给同一VLA以生成最终动作。</li><li>训练时，使用基于规则的自动化标注方法，通过机器人末端执行器与目标物体的3D位置差计算真实空间关系标签，对空间推理答案和动作同时施加自回归损失。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li>数据集：LIBERO（包含LIBERO-90、Spatial、Object、Goal、Long子集）、CALVIN（长期语言条件操作任务）、以及真实世界数据（10个已见任务和5个未见任务）。</li><li>训练资源：基于miniVLA-VQ和π₀-FAST模型进行实验，学习率1e-5，训练步数50,000，使用3个随机种子。真实世界实验使用AGILEX PiPER 6DOF机械臂，每个任务10条演示轨迹。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li>评估环境：模拟环境（LIBERO和CALVIN）、真实世界环境（AGILEX PiPER 6DOF机械臂）。</li><li>评估指标：成功率（Success Rate），其中LIBERO和CALVIN分别对每个任务进行100次和500次试验；真实世界任务每个任务进行10次试验。CALVIN额外评估顺序任务完成的平均数量。</li></ol>"
  },
  {
    "date": "2025-05-19",
    "title": "SPKLIP: Aligning Spike Video Streams with Natural Language",
    "link": "http://arxiv.org/abs/2505.12656",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-18",
    "title": "RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction",
    "link": "http://arxiv.org/abs/2505.12224",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-16",
    "title": "Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions",
    "link": "http://arxiv.org/abs/2505.11214",
    "summary_markdown": "## 研究单位\n西湖大学与浙江大学\n## 论文概述\n本文提出了一种名为OE-VLA的视觉-语言-行动（VLA）模型，旨在解决传统VLA模型仅接受语言指令的限制，扩展其对开放式多模态指令的处理能力。通过统一架构，模型能处理包括图像、视频和文本在内的自由形式人类指令，从而增强人机交互的自然性。实验表明，OE-VLA在语言指令任务上与传统模型性能相当，并在多种开放式任务中表现优异，显著扩大了VLA模型的应用范围。\n## 论文核心贡献点\n1. 提出了OE-VLA，一种新型VLA模型，通过统一神经架构处理多样化开放式多模态人类指令。\n2. 提出了一种通用方法，利用现有数据集构建包含自由形式多模态指令的机器人数据集，并采用两阶段课程学习算法将基础模型微调为支持交错模态的VLA模型。\n3. 引入了基于CALVIN套件的两个新基准OE-CALVIN_base和OE-CALVIN_hard，包含多样化的开放式指令，用于评估模型性能。\n## 论文方法描述\n模型架构基于LLaVA-Next-Interleave，包含三个组件：视觉编码器（使用SigLIP-400M处理图像）、LLM主干（Qwen-1.5，最大上下文长度32k）和动作分词器（将连续动作离散化为256个bin，用语言token表示）。训练数据构建采用转换方法：从现有语言指令数据集中，随机抽取样本并转换为多模态形式，包括视觉对象规范（VOS）、光学指令跟随（OIF）、视觉目标达成（VGR）和视频演示学习（VDL）。训练流程采用两阶段课程学习：第一阶段使用多图像定位数据集增强空间感知；第二阶段在构建的多模态机器人数据上微调模型，样本格式统一为((<obs>, (text1, <img1>, text2, <img2>), <act>)。\n## 论文使用数据集和训练资源\n使用CALVIN数据集及其衍生基准OE-CALVIN_base和OE-CALVIN_hard进行训练和评估，后者包含约1000个评估序列，指令类型覆盖多模态场景。训练资源方面，模型提供了0.5B和7B参数版本，所有模型训练一个epoch以确保公平比较；硬件资源未明确提及，但7B模型需大规模计算支持。\n## 论文使用的评估环境和评估指标\n评估环境基于CALVIN测试套件，该套件包含34项任务的1000个评估序列，用于测试语言条件策略学习。新增的OE-CALVIN基准进一步提供两种难度（base和hard），测试多模态指令下的性能。评估指标为成功序列长度（success sequence length），包括5步长任务的平均成功率（LH-1至LH-5）和整体序列长度，衡量模型在连续子任务中的执行能力。",
    "summary_html": "<h2 class=\"section-title\">研究单位</h2>\n<p>西湖大学与浙江大学</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文提出了一种名为OE-VLA的视觉-语言-行动（VLA）模型，旨在解决传统VLA模型仅接受语言指令的限制，扩展其对开放式多模态指令的处理能力。通过统一架构，模型能处理包括图像、视频和文本在内的自由形式人类指令，从而增强人机交互的自然性。实验表明，OE-VLA在语言指令任务上与传统模型性能相当，并在多种开放式任务中表现优异，显著扩大了VLA模型的应用范围。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出了OE-VLA，一种新型VLA模型，通过统一神经架构处理多样化开放式多模态人类指令。</li><li>提出了一种通用方法，利用现有数据集构建包含自由形式多模态指令的机器人数据集，并采用两阶段课程学习算法将基础模型微调为支持交错模态的VLA模型。</li><li>引入了基于CALVIN套件的两个新基准OE-CALVIN_base和OE-CALVIN_hard，包含多样化的开放式指令，用于评估模型性能。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>模型架构基于LLaVA-Next-Interleave，包含三个组件：视觉编码器（使用SigLIP-400M处理图像）、LLM主干（Qwen-1.5，最大上下文长度32k）和动作分词器（将连续动作离散化为256个bin，用语言token表示）。训练数据构建采用转换方法：从现有语言指令数据集中，随机抽取样本并转换为多模态形式，包括视觉对象规范（VOS）、光学指令跟随（OIF）、视觉目标达成（VGR）和视频演示学习（VDL）。训练流程采用两阶段课程学习：第一阶段使用多图像定位数据集增强空间感知；第二阶段在构建的多模态机器人数据上微调模型，样本格式统一为((<obs>, (text1, <img1>, text2, <img2>), <act>)。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<p>使用CALVIN数据集及其衍生基准OE-CALVIN_base和OE-CALVIN_hard进行训练和评估，后者包含约1000个评估序列，指令类型覆盖多模态场景。训练资源方面，模型提供了0.5B和7B参数版本，所有模型训练一个epoch以确保公平比较；硬件资源未明确提及，但7B模型需大规模计算支持。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p>评估环境基于CALVIN测试套件，该套件包含34项任务的1000个评估序列，用于测试语言条件策略学习。新增的OE-CALVIN基准进一步提供两种难度（base和hard），测试多模态指令下的性能。评估指标为成功序列长度（success sequence length），包括5步长任务的平均成功率（LH-1至LH-5）和整体序列长度，衡量模型在连续子任务中的执行能力。</p>"
  },
  {
    "date": "2025-05-16",
    "title": "Conditioning Matters: Training Diffusion Policies is Faster Than You Think",
    "link": "http://arxiv.org/abs/2505.11123",
    "summary_markdown": "论文研究单位\n- 天津大学\n- 清华大学\n- 华为诺亚方舟实验室\n\n论文概述\n该论文指出了在训练视觉-语言-动作（VLA）模型时，条件扩散策略存在的一个核心挑战：当生成条件难以区分时，训练目标会退化为建模边际动作分布，这一现象被称为“损失坍塌”。为解决此问题，论文提出了一种名为Cocos（condition-conditioned source distribution）的通用解决方案。该方法通过修改条件流匹配中的源分布，使其依赖于条件输入，具体做法是利用一个自编码器从条件中提取语义，并将源分布的中心锚定在该语义上，同时保持固定的标准差。Cocos鼓励模型更强地整合条件信息，从而防止损失坍塌，显著提升了训练效率和最终性能。\n\n论文核心贡献点\n- 构建了带生成条件的流匹配数学框架，并证明了当条件难以区分时，策略网络会主动忽略这些条件。\n- 提出了Cocos方法，这是一种简单且有效的源分布修改技术，可以防止损失坍塌，并显著提高扩散策略的训练效率和性能。\n- 建立了一个全面的评估基准，涵盖了从模拟（LIBERO, MetaWorld）到真实世界（低成本SO-100和高性能xArm机器人）的多种任务设置，验证了Cocos作为一种通用、即插即用解决方案的有效性。\n\n论文方法描述\nCocos的核心思想是替换标准条件流匹配中与条件无关的源分布（通常是标准高斯分布）。具体实现上，该方法使用一个视觉-语言自编码器来压缩条件（如观察图像和语言指令）的表示，将其作为均值来构建一个条件化的高斯源分布 q(z\\|c)，其中c代表条件输入，而标准差则保持固定。通过这种方式，扩散过程的起点本身就包含了条件的语义信息，强制策略网络在训练的每一步都必须利用条件，从而从根本上解决了条件被忽略和损失坍塌的问题。\n\n论文使用数据集和训练资源\n- 数据集:\n - 模拟环境：LIBERO和MetaWorld基准中的70个任务。\n - 真实世界环境：在开源低成本SO-100机器人平台上的10个任务，以及在高性能xArm机器人平台上的10个任务。\n- 训练资源:\n - 计算资源：使用配备了8块NVIDIA A100 GPU的服务器进行实验。\n\n论文使用的评估环境和评估指标\n- 评估环境:\n - 模拟环境：LIBERO和MetaWorld。\n - 真实世界环境：配备摄像头的SO-100和xArm机器人手臂。\n- 评估指标:\n - 任务成功率：在不同任务上成功完成任务的比率。\n - 训练效率/收敛速度：达到特定性能水平（如π₀性能）所需的梯度步数或训练时间。论文中特别使用了达到目标性能所需的时间倍数来衡量加速比。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>天津大学</li><li>清华大学</li><li>华为诺亚方舟实验室</li></ul>\n\n<p>论文概述</p>\n<p>该论文指出了在训练视觉-语言-动作（VLA）模型时，条件扩散策略存在的一个核心挑战：当生成条件难以区分时，训练目标会退化为建模边际动作分布，这一现象被称为“损失坍塌”。为解决此问题，论文提出了一种名为Cocos（condition-conditioned source distribution）的通用解决方案。该方法通过修改条件流匹配中的源分布，使其依赖于条件输入，具体做法是利用一个自编码器从条件中提取语义，并将源分布的中心锚定在该语义上，同时保持固定的标准差。Cocos鼓励模型更强地整合条件信息，从而防止损失坍塌，显著提升了训练效率和最终性能。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>构建了带生成条件的流匹配数学框架，并证明了当条件难以区分时，策略网络会主动忽略这些条件。</li><li>提出了Cocos方法，这是一种简单且有效的源分布修改技术，可以防止损失坍塌，并显著提高扩散策略的训练效率和性能。</li><li>建立了一个全面的评估基准，涵盖了从模拟（LIBERO, MetaWorld）到真实世界（低成本SO-100和高性能xArm机器人）的多种任务设置，验证了Cocos作为一种通用、即插即用解决方案的有效性。</li></ul>\n\n<p>论文方法描述</p>\n<p>Cocos的核心思想是替换标准条件流匹配中与条件无关的源分布（通常是标准高斯分布）。具体实现上，该方法使用一个视觉-语言自编码器来压缩条件（如观察图像和语言指令）的表示，将其作为均值来构建一个条件化的高斯源分布 q(z\\|c)，其中c代表条件输入，而标准差则保持固定。通过这种方式，扩散过程的起点本身就包含了条件的语义信息，强制策略网络在训练的每一步都必须利用条件，从而从根本上解决了条件被忽略和损失坍塌的问题。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集:</li></ul>\n<p> - 模拟环境：LIBERO和MetaWorld基准中的70个任务。</p>\n<p> - 真实世界环境：在开源低成本SO-100机器人平台上的10个任务，以及在高性能xArm机器人平台上的10个任务。</p>\n<ul><li>训练资源:</li></ul>\n<p> - 计算资源：使用配备了8块NVIDIA A100 GPU的服务器进行实验。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境:</li></ul>\n<p> - 模拟环境：LIBERO和MetaWorld。</p>\n<p> - 真实世界环境：配备摄像头的SO-100和xArm机器人手臂。</p>\n<ul><li>评估指标:</li></ul>\n<p> - 任务成功率：在不同任务上成功完成任务的比率。</p>\n<p> - 训练效率/收敛速度：达到特定性能水平（如π₀性能）所需的梯度步数或训练时间。论文中特别使用了达到目标性能所需的时间倍数来衡量加速比。</p>"
  },
  {
    "date": "2025-05-14",
    "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware",
    "link": "http://arxiv.org/abs/2505.09601",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-14",
    "title": "VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation",
    "link": "http://arxiv.org/abs/2505.09577",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-13",
    "title": "From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2505.08548",
    "summary_markdown": "### 论文研究单位\n天津大学\n### 论文概述\n论文提出FSD（From Seeing to Doing）模型，通过空间关系推理生成中间视觉表示（如空间affordance boxes/points和visual traces），解决机器人操作中的泛化问题，克服现有VLA模型因数据稀缺性和异质性导致的零样本性能限制。\n### 论文核心贡献点\n1. 提出FSD模型，通过空间关系聚焦的视觉思维链（SrCoT）生成视觉辅助，实现推理与决策的桥接。\n2. 设计弱到强的分层数据构建管道，结合自我一致性机制对齐空间理解与生成。\n3. 构建VABench基准，用于评估复杂场景下的视觉辅助生成能力。\n4. 在8个空间推理基准和真实机器人任务中实现SOTA性能，零样本成功率显著提升。\n### 论文方法描述\n1. **视觉辅助定义**：使用三种中间表示：空间affordance boxes（目标区域）、spatial affordance points（精确点）和object-centric visual traces（操作轨迹序列），所有坐标归一化到[0, 1000]空间。\n2. **SrCoT机制**：分两阶段推理：描述阶段构建物体为中心的空间关系图，推理阶段基于该图逐步推导坐标，绑定对象与位置。\n3. **数据构建**：五级能力分层（区域定位→空间关系→空间推理→affordance生成→视觉轨迹生成），自动从BridgeDataV2等数据集生成300K样本。\n4. **自一致性对齐**：双向任务设计（图像→轨迹 vs. 轨迹→指令），增强坐标与视觉信号对齐。\n### 论文使用数据集和训练资源\n1. **数据集**：BridgeDataV2、RT-X、Droid等机器人数据集，结合通用VQA数据，自动标注空间关系和轨迹。\n2. **训练资源**：基于LLaVA-1.5架构，使用CLIP-ViT-L图像编码器和Vicuna-13B LLM，冻结视觉编码器，微调投影层和LLM，混合1.4M样本训练。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：SimplerEnv仿真环境（WidowX机器人）和真实xArm机器人平台。\n2. **评估指标**：\n - 空间推理基准：CVBench等5个数据集的18个子任务平均准确率。\n - 物体/区域参考：RoboRefIt和Where2Place的点在目标区域比例。\n - VABench：VABench-Point的准确率，VABench-VisualTrace的RMSE/MAE和GPT Score。\n - 机器人任务：零样本操作成功率（SimplerEnv和8个真实任务）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>天津大学</p>\n<h3>论文概述</h3>\n<p>论文提出FSD（From Seeing to Doing）模型，通过空间关系推理生成中间视觉表示（如空间affordance boxes/points和visual traces），解决机器人操作中的泛化问题，克服现有VLA模型因数据稀缺性和异质性导致的零样本性能限制。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出FSD模型，通过空间关系聚焦的视觉思维链（SrCoT）生成视觉辅助，实现推理与决策的桥接。</li><li>设计弱到强的分层数据构建管道，结合自我一致性机制对齐空间理解与生成。</li><li>构建VABench基准，用于评估复杂场景下的视觉辅助生成能力。</li><li>在8个空间推理基准和真实机器人任务中实现SOTA性能，零样本成功率显著提升。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>视觉辅助定义</strong>：使用三种中间表示：空间affordance boxes（目标区域）、spatial affordance points（精确点）和object-centric visual traces（操作轨迹序列），所有坐标归一化到[0, 1000]空间。</li><li><strong>SrCoT机制</strong>：分两阶段推理：描述阶段构建物体为中心的空间关系图，推理阶段基于该图逐步推导坐标，绑定对象与位置。</li><li><strong>数据构建</strong>：五级能力分层（区域定位→空间关系→空间推理→affordance生成→视觉轨迹生成），自动从BridgeDataV2等数据集生成300K样本。</li><li><strong>自一致性对齐</strong>：双向任务设计（图像→轨迹 vs. 轨迹→指令），增强坐标与视觉信号对齐。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：BridgeDataV2、RT-X、Droid等机器人数据集，结合通用VQA数据，自动标注空间关系和轨迹。</li><li><strong>训练资源</strong>：基于LLaVA-1.5架构，使用CLIP-ViT-L图像编码器和Vicuna-13B LLM，冻结视觉编码器，微调投影层和LLM，混合1.4M样本训练。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：SimplerEnv仿真环境（WidowX机器人）和真实xArm机器人平台。</li><li><strong>评估指标</strong>：</li></ol>\n<p> - 空间推理基准：CVBench等5个数据集的18个子任务平均准确率。</p>\n<p> - 物体/区域参考：RoboRefIt和Where2Place的点在目标区域比例。</p>\n<p> - VABench：VABench-Point的准确率，VABench-VisualTrace的RMSE/MAE和GPT Score。</p>\n<p> - 机器人任务：零样本操作成功率（SimplerEnv和8个真实任务）。</p>"
  },
  {
    "date": "2025-05-13",
    "title": "Training Strategies for Efficient Embodied Reasoning",
    "link": "http://arxiv.org/abs/2505.08243",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-12",
    "title": "ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning",
    "link": "http://arxiv.org/abs/2505.07395",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n论文作者为 Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang。未在提供的HTML原文中明确提及具体研究单位。\n## 论文概述\n本文提出了ReinboT（Reinforced robot GPT），一种新颖的端到端视觉-语言-动作（VLA）模型，旨在通过集成强化学习（RL）的最大化累积回报原理来增强机器人视觉语言操作能力。ReinboT通过预测密集回报来深入理解数据质量分布，使机器人能够生成更稳健、面向未来利益最大化的决策行为。在CALVIN混合质量数据集上的实验表明，ReinboT实现了最先进的性能，并在真实世界的任务中表现出卓越的少样本学习和分布外泛化能力。\n## 论文核心贡献点\n- 提出了ReinboT，一种新颖的端到端VLA模型，集成了RL回报最大化以增强机器人操作能力。\n- 引入了一种奖励密集化方法，使ReinboT能够深入理解数据质量以进行更稳健的学习。\n- 通过广泛的实验证明了ReinboT的最先进性能，显著优于模拟和真实世界任务中的基线模型。\n## 论文方法描述\nReinboT方法的核心是奖励密集化和端到端强化VLA模型设计。\n1. **奖励密集化**：将长时程操作轨迹自动分解为多个包含单一子目标的轨迹段，并设计一个包含四个组件的密集奖励来捕捉操作任务特性：\n - 子目标达成（r1）：通过MSE、SSIM和ORB算法计算当前状态与子目标状态的差异。\n - 任务进展（r2）：基于子目标序列在整体轨迹中的位置分配权重，越接近最终目标的序列权重越高。\n - 行为平滑性（r3）：惩罚过大的关节速度、加速度和动作变化，促进平滑自然的运动。\n - 任务完成（r4）：若轨迹成功完成指令则给予奖励。\n2. **端到端强化VLA模型**：基于GPT风格的Transformer，引入了三种预测标记嵌入（[RTG]、[ACTION]和[IMAGE]）来分别预测ReturnToGo、机器人动作和未来图像状态。模型使用CLIP编码语言指令，ViT和perceiver resampler处理图像状态，MLP处理本体感觉。训练时，利用期望分位数回归损失函数预测最大化回报，使模型在推理时能预测最大回报以指导执行更优的动作。\n## 论文使用数据集和训练资源\n- **数据集**：主要在CALVIN混合质量数据集上进行评估。\n- **训练资源**：未在提供的HTML原文中明确提及具体的计算资源（如GPU类型、数量、训练时长等）。\n## 论文使用的评估环境和评估指标\n- **评估环境**：在模拟环境（CALVIN数据集）和真实世界任务中进行评估。\n- **评估指标**：\n - 在CALVIN混合质量数据集上的任务成功率。\n - 少样本学习能力和分布外（OOD）泛化能力的评估。\n - 真实世界任务的成功率和性能表现。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>论文作者为 Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang。未在提供的HTML原文中明确提及具体研究单位。</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文提出了ReinboT（Reinforced robot GPT），一种新颖的端到端视觉-语言-动作（VLA）模型，旨在通过集成强化学习（RL）的最大化累积回报原理来增强机器人视觉语言操作能力。ReinboT通过预测密集回报来深入理解数据质量分布，使机器人能够生成更稳健、面向未来利益最大化的决策行为。在CALVIN混合质量数据集上的实验表明，ReinboT实现了最先进的性能，并在真实世界的任务中表现出卓越的少样本学习和分布外泛化能力。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了ReinboT，一种新颖的端到端VLA模型，集成了RL回报最大化以增强机器人操作能力。</li><li>引入了一种奖励密集化方法，使ReinboT能够深入理解数据质量以进行更稳健的学习。</li><li>通过广泛的实验证明了ReinboT的最先进性能，显著优于模拟和真实世界任务中的基线模型。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>ReinboT方法的核心是奖励密集化和端到端强化VLA模型设计。</p>\n<ol><li><strong>奖励密集化</strong>：将长时程操作轨迹自动分解为多个包含单一子目标的轨迹段，并设计一个包含四个组件的密集奖励来捕捉操作任务特性：</li></ol>\n<p> - 子目标达成（r1）：通过MSE、SSIM和ORB算法计算当前状态与子目标状态的差异。</p>\n<p> - 任务进展（r2）：基于子目标序列在整体轨迹中的位置分配权重，越接近最终目标的序列权重越高。</p>\n<p> - 行为平滑性（r3）：惩罚过大的关节速度、加速度和动作变化，促进平滑自然的运动。</p>\n<p> - 任务完成（r4）：若轨迹成功完成指令则给予奖励。</p>\n<ol><li><strong>端到端强化VLA模型</strong>：基于GPT风格的Transformer，引入了三种预测标记嵌入（[RTG]、[ACTION]和[IMAGE]）来分别预测ReturnToGo、机器人动作和未来图像状态。模型使用CLIP编码语言指令，ViT和perceiver resampler处理图像状态，MLP处理本体感觉。训练时，利用期望分位数回归损失函数预测最大化回报，使模型在推理时能预测最大回报以指导执行更优的动作。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：主要在CALVIN混合质量数据集上进行评估。</li><li><strong>训练资源</strong>：未在提供的HTML原文中明确提及具体的计算资源（如GPU类型、数量、训练时长等）。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：在模拟环境（CALVIN数据集）和真实世界任务中进行评估。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 在CALVIN混合质量数据集上的任务成功率。</p>\n<p> - 少样本学习能力和分布外（OOD）泛化能力的评估。</p>\n<p> - 真实世界任务的成功率和性能表现。</p>"
  },
  {
    "date": "2025-05-09",
    "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
    "link": "http://arxiv.org/abs/2505.06111",
    "summary_markdown": "### 论文研究单位\n论文作者来自多个机构，包括北京大学的1,2单位，以及北京大学的其他合作单位，例如第3单位。\n### 论文概述\n论文提出了UniVLA，一个用于学习跨具身视觉-语言-动作（VLA）策略的新框架。核心创新是通过一个潜在动作模型从视频中导出任务中心的动作表示，从而能够利用跨越广泛具身和视角的庞大数据。论文通过结合语言指令并在DINO特征空间中建立潜在动作模型，来减轻任务无关动态的影响。学习自互联网规模视频的通用策略可以通过高效的潜在动作解码部署到各种机器人。论文在多个操作和导航基准测试以及真实机器人部署中取得了最先进的结果。UniVLA在计算成本不到OpenVLA的1/20，下游数据不到1/10的情况下，取得了优于OpenVLA的性能。\n### 论文核心贡献点\n1. 提出了UniVLA，一个通过在统一的、具身不可知的动作空间中规划来实现通用策略的框架，能够通过从网络规模视频中学习来实现可扩展和高效的决策。\n2. 引入了一种从跨具身视频中提取任务相关潜在动作的新方法，将任务中心动态与无关的视觉变化解耦。定性和定量实验突出了其优点和相对于现有工作的优势。\n3. UniVLA在多个基准测试和真实机器人测试中取得了最先进的性能，在LIBERO基准上成功率比OpenVLA提高18.5%，导航任务中提高29.6%，真实世界部署中提高36.7%。\n### 论文方法描述\nUniVLA方法分为三个阶段：\n1. **任务中心潜在动作学习**：通过语言指令引导，从跨具身视频中无监督地提取任务相关的动作表示。使用VQ-VAE对连续帧对的逆动态进行离散化，得到量化的潜在动作。在DINOv2特征空间中操作以过滤视觉噪声，并解耦出任务中心动态。\n2. **通用策略预训练**：基于Prismatic-7B视觉语言模型，通过自回归方式预测离散化的潜在动作标记，训练一个具身不可知的通用策略。\n3. **部署后训练**：引入轻量级动作解码器（约10.8M参数），将潜在动作解码为可执行轨迹，实现对新机器人的高效适配。同时利用历史潜在动作输出作为提示输入，增强长时程任务表现。\n### 论文使用数据集和训练资源\n**数据集**：\n- 预训练：Open X-Embodiment（操作数据）、GNM（导航数据）和Ego4D（人类视频）。\n- 评估：LIBERO（操作）、CALVIN（操作）、SimplerEnv（操作）、Room2Room（导航）以及真实世界机器人任务。\n**训练资源**：\n- 预训练：960 A100小时，显著低于OpenVLA的21,500 A100小时。\n- 后训练：使用LoRA进行参数高效微调，新增约123M可训练参数。\n- 推理：在NVIDIA RTX 4090 GPU上实现10Hz实时闭环推理。\n### 论文使用的评估环境和评估指标\n**评估环境**：\n- 模拟环境：LIBERO、CALVIN、SimplerEnv和VLN-CE的Room2Room（R2R）。\n- 真实环境：配备7-DoF机械臂和Orbecc DABAI相机的Piper机器人。\n**评估指标**：\n- 操作任务：成功率（Success Rate）。\n- 导航任务：Oracle成功率（在VLN-CE中，代理到达目标3米内视为成功）。\n- 真实任务：成功率（Success Rate）和逐步得分（Step-wise Score，每个任务最高3分）。\n- 通用性评估：光照变化、视觉干扰物、新物体场景下的成功率。</think>\n### 论文研究单位\n论文作者来自多个机构，包括北京大学的1,2单位，以及北京大学的其他合作单位，例如第3单位。\n### 论文概述\n论文提出了UniVLA，一个用于学习跨具身视觉-语言-动作（VLA）策略的新框架。核心创新是通过一个潜在动作模型从视频中导出任务中心的动作表示，从而能够利用跨越广泛具身和视角的庞大数据。论文通过结合语言指令并在DINO特征空间中建立潜在动作模型，来减轻任务无关动态的影响。学习自互联网规模视频的通用策略可以通过高效的潜在动作解码部署到各种机器人。论文在多个操作和导航基准测试以及真实机器人部署中取得了最先进的结果。UniVLA在计算成本不到OpenVLA的1/20，下游数据不到1/10的情况下，取得了优于OpenVLA的性能。\n### 论文核心贡献点\n1. 提出了UniVLA，一个通过在统一的、具身不可知的动作空间中规划来实现通用策略的框架，能够通过从网络规模视频中学习来实现可扩展和高效的决策。\n2. 引入了一种从跨具身视频中提取任务相关潜在动作的新方法，将任务中心动态与无关的视觉变化解耦。定性和定量实验突出了其优点和相对于现有工作的优势。\n3. UniVLA在多个基准测试和真实机器人测试中取得了最先进的性能，在LIBERO基准上成功率比OpenVLA提高18.5%，导航任务中提高29.6%，真实世界部署中提高36.7%。\n### 论文方法描述\nUniVLA方法分为三个阶段：\n1. **任务中心潜在动作学习**：通过语言指令引导，从跨具身视频中无监督地提取任务相关的动作表示。使用VQ-VAE对连续帧对的逆动态进行离散化，得到量化的潜在动作。在DINOv2特征空间中操作以过滤视觉噪声，并解耦出任务中心动态。\n2. **通用策略预训练**：基于Prismatic-7B视觉语言模型，通过自回归方式预测离散化的潜在动作标记，训练一个具身不可知的通用策略。\n3. **部署后训练**：引入轻量级动作解码器（约10.8M参数），将潜在动作解码为可执行轨迹，实现对新机器人的高效适配。同时利用历史潜在动作输出作为提示输入，增强长时程任务表现。\n### 论文使用数据集和训练资源\n**数据集**：\n- 预训练：Open X-Embodiment（操作数据）、GNM（导航数据）和Ego4D（人类视频）。\n- 评估：LIBERO（操作）、CALVIN（操作）、SimplerEnv（操作）、Room2Room（导航）以及真实世界机器人任务。\n**训练资源**：\n- 预训练：960 A100小时，显著低于OpenVLA的21,500 A100小时。\n- 后训练：使用LoRA进行参数高效微调，新增约123M可训练参数。\n- 推理：在NVIDIA RTX 4090 GPU上实现10Hz实时闭环推理。\n### 论文使用的评估环境和评估指标\n**评估环境**：\n- 模拟环境：LIBERO、CALVIN、SimplerEnv和VLN-CE的Room2Room（R2R）。\n- 真实环境：配备7-DoF机械臂和Orbecc DABAI相机的Piper机器人。\n**评估指标**：\n- 操作任务：成功率（Success Rate）。\n- 导航任务：Oracle成功率（在VLN-CE中，代理到达目标3米内视为成功）。\n- 真实任务：成功率（Success Rate）和逐步得分（Step-wise Score，每个任务最高3分）。\n- 通用性评估：光照变化、视觉干扰物、新物体场景下的成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>论文作者来自多个机构，包括北京大学的1,2单位，以及北京大学的其他合作单位，例如第3单位。</p>\n<h3>论文概述</h3>\n<p>论文提出了UniVLA，一个用于学习跨具身视觉-语言-动作（VLA）策略的新框架。核心创新是通过一个潜在动作模型从视频中导出任务中心的动作表示，从而能够利用跨越广泛具身和视角的庞大数据。论文通过结合语言指令并在DINO特征空间中建立潜在动作模型，来减轻任务无关动态的影响。学习自互联网规模视频的通用策略可以通过高效的潜在动作解码部署到各种机器人。论文在多个操作和导航基准测试以及真实机器人部署中取得了最先进的结果。UniVLA在计算成本不到OpenVLA的1/20，下游数据不到1/10的情况下，取得了优于OpenVLA的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了UniVLA，一个通过在统一的、具身不可知的动作空间中规划来实现通用策略的框架，能够通过从网络规模视频中学习来实现可扩展和高效的决策。</li><li>引入了一种从跨具身视频中提取任务相关潜在动作的新方法，将任务中心动态与无关的视觉变化解耦。定性和定量实验突出了其优点和相对于现有工作的优势。</li><li>UniVLA在多个基准测试和真实机器人测试中取得了最先进的性能，在LIBERO基准上成功率比OpenVLA提高18.5%，导航任务中提高29.6%，真实世界部署中提高36.7%。</li></ol>\n<h3>论文方法描述</h3>\n<p>UniVLA方法分为三个阶段：</p>\n<ol><li><strong>任务中心潜在动作学习</strong>：通过语言指令引导，从跨具身视频中无监督地提取任务相关的动作表示。使用VQ-VAE对连续帧对的逆动态进行离散化，得到量化的潜在动作。在DINOv2特征空间中操作以过滤视觉噪声，并解耦出任务中心动态。</li><li><strong>通用策略预训练</strong>：基于Prismatic-7B视觉语言模型，通过自回归方式预测离散化的潜在动作标记，训练一个具身不可知的通用策略。</li><li><strong>部署后训练</strong>：引入轻量级动作解码器（约10.8M参数），将潜在动作解码为可执行轨迹，实现对新机器人的高效适配。同时利用历史潜在动作输出作为提示输入，增强长时程任务表现。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>：</p>\n<ul><li>预训练：Open X-Embodiment（操作数据）、GNM（导航数据）和Ego4D（人类视频）。</li><li>评估：LIBERO（操作）、CALVIN（操作）、SimplerEnv（操作）、Room2Room（导航）以及真实世界机器人任务。</li></ul>\n<p><strong>训练资源</strong>：</p>\n<ul><li>预训练：960 A100小时，显著低于OpenVLA的21,500 A100小时。</li><li>后训练：使用LoRA进行参数高效微调，新增约123M可训练参数。</li><li>推理：在NVIDIA RTX 4090 GPU上实现10Hz实时闭环推理。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>：</p>\n<ul><li>模拟环境：LIBERO、CALVIN、SimplerEnv和VLN-CE的Room2Room（R2R）。</li><li>真实环境：配备7-DoF机械臂和Orbecc DABAI相机的Piper机器人。</li></ul>\n<p><strong>评估指标</strong>：</p>\n<ul><li>操作任务：成功率（Success Rate）。</li><li>导航任务：Oracle成功率（在VLN-CE中，代理到达目标3米内视为成功）。</li><li>真实任务：成功率（Success Rate）和逐步得分（Step-wise Score，每个任务最高3分）。</li><li>通用性评估：光照变化、视觉干扰物、新物体场景下的成功率。</think></li></ul>\n<h3>论文研究单位</h3>\n<p>论文作者来自多个机构，包括北京大学的1,2单位，以及北京大学的其他合作单位，例如第3单位。</p>\n<h3>论文概述</h3>\n<p>论文提出了UniVLA，一个用于学习跨具身视觉-语言-动作（VLA）策略的新框架。核心创新是通过一个潜在动作模型从视频中导出任务中心的动作表示，从而能够利用跨越广泛具身和视角的庞大数据。论文通过结合语言指令并在DINO特征空间中建立潜在动作模型，来减轻任务无关动态的影响。学习自互联网规模视频的通用策略可以通过高效的潜在动作解码部署到各种机器人。论文在多个操作和导航基准测试以及真实机器人部署中取得了最先进的结果。UniVLA在计算成本不到OpenVLA的1/20，下游数据不到1/10的情况下，取得了优于OpenVLA的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了UniVLA，一个通过在统一的、具身不可知的动作空间中规划来实现通用策略的框架，能够通过从网络规模视频中学习来实现可扩展和高效的决策。</li><li>引入了一种从跨具身视频中提取任务相关潜在动作的新方法，将任务中心动态与无关的视觉变化解耦。定性和定量实验突出了其优点和相对于现有工作的优势。</li><li>UniVLA在多个基准测试和真实机器人测试中取得了最先进的性能，在LIBERO基准上成功率比OpenVLA提高18.5%，导航任务中提高29.6%，真实世界部署中提高36.7%。</li></ol>\n<h3>论文方法描述</h3>\n<p>UniVLA方法分为三个阶段：</p>\n<ol><li><strong>任务中心潜在动作学习</strong>：通过语言指令引导，从跨具身视频中无监督地提取任务相关的动作表示。使用VQ-VAE对连续帧对的逆动态进行离散化，得到量化的潜在动作。在DINOv2特征空间中操作以过滤视觉噪声，并解耦出任务中心动态。</li><li><strong>通用策略预训练</strong>：基于Prismatic-7B视觉语言模型，通过自回归方式预测离散化的潜在动作标记，训练一个具身不可知的通用策略。</li><li><strong>部署后训练</strong>：引入轻量级动作解码器（约10.8M参数），将潜在动作解码为可执行轨迹，实现对新机器人的高效适配。同时利用历史潜在动作输出作为提示输入，增强长时程任务表现。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>：</p>\n<ul><li>预训练：Open X-Embodiment（操作数据）、GNM（导航数据）和Ego4D（人类视频）。</li><li>评估：LIBERO（操作）、CALVIN（操作）、SimplerEnv（操作）、Room2Room（导航）以及真实世界机器人任务。</li></ul>\n<p><strong>训练资源</strong>：</p>\n<ul><li>预训练：960 A100小时，显著低于OpenVLA的21,500 A100小时。</li><li>后训练：使用LoRA进行参数高效微调，新增约123M可训练参数。</li><li>推理：在NVIDIA RTX 4090 GPU上实现10Hz实时闭环推理。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>：</p>\n<ul><li>模拟环境：LIBERO、CALVIN、SimplerEnv和VLN-CE的Room2Room（R2R）。</li><li>真实环境：配备7-DoF机械臂和Orbecc DABAI相机的Piper机器人。</li></ul>\n<p><strong>评估指标</strong>：</p>\n<ul><li>操作任务：成功率（Success Rate）。</li><li>导航任务：Oracle成功率（在VLN-CE中，代理到达目标3米内视为成功）。</li><li>真实任务：成功率（Success Rate）和逐步得分（Step-wise Score，每个任务最高3分）。</li><li>通用性评估：光照变化、视觉干扰物、新物体场景下的成功率。</li></ul>"
  },
  {
    "date": "2025-05-09",
    "title": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks",
    "link": "http://arxiv.org/abs/2505.05800",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-08",
    "title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments",
    "link": "http://arxiv.org/abs/2505.05540",
    "summary_markdown": "### 论文研究单位\n- Manifold Research\n- Metarch.ai\n- Georgia Tech\n- MIT\n### 论文概述\n该论文介绍了MultiNet v0.2，一个用于评估视觉-语言-动作（VLA）模型在程序生成环境中零样本泛化能力的综合基准。研究聚焦于分析GPT-4o、GPT-4.1、OpenVLA、Pi0 Base和Pi0 FAST等最先进模型在Procgen基准上的性能，揭示了这些模型在处理分布外（OOD）任务时的关键局限性。\n### 论文核心贡献点\n1. 提出了一个系统化的基准测试框架，用于评估最先进模型在多种程序生成游戏环境中的性能。\n2. 提供了初始VLA和VLM模型的详细性能分析。\n3. 分析了架构选择、训练数据和输出处理技术对模型泛化能力的影响。\n4. 深入探讨了动作空间表示和图像复杂度等因素对模型性能的影响。\n### 论文方法描述\n1. **数据预处理**：将Procgen的专家强化学习轨迹转换为TensorFlow Datasets格式，并创建随机采样的测试集。\n2. **模型适配**：\n - **GPT模型**：使用Genesis提示工程框架将轨迹数据转换为结构化文本表示。\n - **OpenVLA**：接收原始图像输入和简洁的任务描述，限制自回归步骤为一步。\n - **Pi0模型**：使用单视图图像输入，零数组补充其他视图和本体感受状态。\n3. **推理基础设施**：针对不同模型配置专用硬件，包括NVIDIA L4、A100 GPU和OpenAI Batch API。\n4. **评估指标**：采用多维指标体系，包括Brier MAE、精度/召回率/F1分数、无效预测比例等。\n### 论文使用数据集和训练资源\n- **数据集**：Procgen基准的16个子数据集，包含专家RL代理的离线轨迹，测试集占比10%。\n- **训练资源**：\n - OpenVLA：使用HuggingFace提供的7B预训练权重。\n - Pi0模型：使用Openpi代码库中的检查点。\n - GPT模型：通过OpenAI Batch API进行推理。\n - 硬件：NVIDIA L4（OpenVLA）、单张A100 40GB（Pi0 Base）、四张A100（Pi0 Fast）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Procgen程序生成的16个不同2D游戏环境，每个环境具有独特的任务、动作空间和奖励结构。\n- **评估指标**：\n - **校准度指标**：Brier MAE（最大值2）、归一化Brier MAE、分位数过滤Brier MAE。\n - **分类指标**：微观/宏观精度、召回率、F1分数、精确匹配率。\n - **鲁棒性指标**：无效预测比例、最大相对Brier MAE。\n - **类别分析**：类别级精度/召回率/F1分数，用于检测模型偏差。\n - **特殊指标**：宏观召回率作为最代表性指标，因不受多数类影响。</think>\n### 论文研究单位\n- Manifold Research\n- Metarch.ai\n- Georgia Tech\n- MIT\n### 论文概述\n该论文介绍了MultiNet v0.2，一个用于评估视觉-语言-动作（VLA）模型在程序生成环境中零样本泛化能力的综合基准。研究聚焦于分析GPT-4o、GPT-4.1、OpenVLA、Pi0 Base和Pi0 FAST等最先进模型在Procgen基准上的性能，揭示了这些模型在处理分布外（OOD）任务时的关键局限性。\n### 论文核心贡献点\n1. 提出了一个系统化的基准测试框架，用于评估最先进模型在多种程序生成游戏环境中的性能。\n2. 提供了初始VLA和VLM模型的详细性能分析。\n3. 分析了架构选择、训练数据和输出处理技术对模型泛化能力的影响。\n4. 深入探讨了动作空间表示和图像复杂度等因素对模型性能的影响。\n### 论文方法描述\n1. **数据预处理**：将Procgen的专家强化学习轨迹转换为TensorFlow Datasets格式，并创建随机采样的测试集。\n2. **模型适配**：\n - **GPT模型**：使用Genesis提示工程框架将轨迹数据转换为结构化文本表示。\n - **OpenVLA**：接收原始图像输入和简洁的任务描述，限制自回归步骤为一步。\n - **Pi0模型**：使用单视图图像输入，零数组补充其他视图和本体感受状态。\n3. **推理基础设施**：针对不同模型配置专用硬件，包括NVIDIA L4、A100 GPU和OpenAI Batch API。\n4. **评估指标**：采用多维指标体系，包括Brier MAE、精度/召回率/F1分数、无效预测比例等。\n### 论文使用数据集和训练资源\n- **数据集**：Procgen基准的16个子数据集，包含专家RL代理的离线轨迹，测试集占比10%。\n- **训练资源**：\n - OpenVLA：使用HuggingFace提供的7B预训练权重。\n - Pi0模型：使用Openpi代码库中的检查点。\n - GPT模型：通过OpenAI Batch API进行推理。\n - 硬件：NVIDIA L4（OpenVLA）、单张A100 40GB（Pi0 Base）、四张A100（Pi0 Fast）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Procgen程序生成的16个不同2D游戏环境，每个环境具有独特的任务、动作空间和奖励结构。\n- **评估指标**：\n - **校准度指标**：Brier MAE（最大值2）、归一化Brier MAE、分位数过滤Brier MAE。\n - **分类指标**：微观/宏观精度、召回率、F1分数、精确匹配率。\n - **鲁棒性指标**：无效预测比例、最大相对Brier MAE。\n - **类别分析**：类别级精度/召回率/F1分数，用于检测模型偏差。\n - **特殊指标**：宏观召回率作为最代表性指标，因不受多数类影响。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>Manifold Research</li><li>Metarch.ai</li><li>Georgia Tech</li><li>MIT</li></ul>\n<h3>论文概述</h3>\n<p>该论文介绍了MultiNet v0.2，一个用于评估视觉-语言-动作（VLA）模型在程序生成环境中零样本泛化能力的综合基准。研究聚焦于分析GPT-4o、GPT-4.1、OpenVLA、Pi0 Base和Pi0 FAST等最先进模型在Procgen基准上的性能，揭示了这些模型在处理分布外（OOD）任务时的关键局限性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个系统化的基准测试框架，用于评估最先进模型在多种程序生成游戏环境中的性能。</li><li>提供了初始VLA和VLM模型的详细性能分析。</li><li>分析了架构选择、训练数据和输出处理技术对模型泛化能力的影响。</li><li>深入探讨了动作空间表示和图像复杂度等因素对模型性能的影响。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据预处理</strong>：将Procgen的专家强化学习轨迹转换为TensorFlow Datasets格式，并创建随机采样的测试集。</li><li><strong>模型适配</strong>：</li></ol>\n<p> - <strong>GPT模型</strong>：使用Genesis提示工程框架将轨迹数据转换为结构化文本表示。</p>\n<p> - <strong>OpenVLA</strong>：接收原始图像输入和简洁的任务描述，限制自回归步骤为一步。</p>\n<p> - <strong>Pi0模型</strong>：使用单视图图像输入，零数组补充其他视图和本体感受状态。</p>\n<ol><li><strong>推理基础设施</strong>：针对不同模型配置专用硬件，包括NVIDIA L4、A100 GPU和OpenAI Batch API。</li><li><strong>评估指标</strong>：采用多维指标体系，包括Brier MAE、精度/召回率/F1分数、无效预测比例等。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Procgen基准的16个子数据集，包含专家RL代理的离线轨迹，测试集占比10%。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - OpenVLA：使用HuggingFace提供的7B预训练权重。</p>\n<p> - Pi0模型：使用Openpi代码库中的检查点。</p>\n<p> - GPT模型：通过OpenAI Batch API进行推理。</p>\n<p> - 硬件：NVIDIA L4（OpenVLA）、单张A100 40GB（Pi0 Base）、四张A100（Pi0 Fast）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Procgen程序生成的16个不同2D游戏环境，每个环境具有独特的任务、动作空间和奖励结构。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>校准度指标</strong>：Brier MAE（最大值2）、归一化Brier MAE、分位数过滤Brier MAE。</p>\n<p> - <strong>分类指标</strong>：微观/宏观精度、召回率、F1分数、精确匹配率。</p>\n<p> - <strong>鲁棒性指标</strong>：无效预测比例、最大相对Brier MAE。</p>\n<p> - <strong>类别分析</strong>：类别级精度/召回率/F1分数，用于检测模型偏差。</p>\n<p> - <strong>特殊指标</strong>：宏观召回率作为最代表性指标，因不受多数类影响。</think></p>\n<h3>论文研究单位</h3>\n<ul><li>Manifold Research</li><li>Metarch.ai</li><li>Georgia Tech</li><li>MIT</li></ul>\n<h3>论文概述</h3>\n<p>该论文介绍了MultiNet v0.2，一个用于评估视觉-语言-动作（VLA）模型在程序生成环境中零样本泛化能力的综合基准。研究聚焦于分析GPT-4o、GPT-4.1、OpenVLA、Pi0 Base和Pi0 FAST等最先进模型在Procgen基准上的性能，揭示了这些模型在处理分布外（OOD）任务时的关键局限性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个系统化的基准测试框架，用于评估最先进模型在多种程序生成游戏环境中的性能。</li><li>提供了初始VLA和VLM模型的详细性能分析。</li><li>分析了架构选择、训练数据和输出处理技术对模型泛化能力的影响。</li><li>深入探讨了动作空间表示和图像复杂度等因素对模型性能的影响。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据预处理</strong>：将Procgen的专家强化学习轨迹转换为TensorFlow Datasets格式，并创建随机采样的测试集。</li><li><strong>模型适配</strong>：</li></ol>\n<p> - <strong>GPT模型</strong>：使用Genesis提示工程框架将轨迹数据转换为结构化文本表示。</p>\n<p> - <strong>OpenVLA</strong>：接收原始图像输入和简洁的任务描述，限制自回归步骤为一步。</p>\n<p> - <strong>Pi0模型</strong>：使用单视图图像输入，零数组补充其他视图和本体感受状态。</p>\n<ol><li><strong>推理基础设施</strong>：针对不同模型配置专用硬件，包括NVIDIA L4、A100 GPU和OpenAI Batch API。</li><li><strong>评估指标</strong>：采用多维指标体系，包括Brier MAE、精度/召回率/F1分数、无效预测比例等。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Procgen基准的16个子数据集，包含专家RL代理的离线轨迹，测试集占比10%。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - OpenVLA：使用HuggingFace提供的7B预训练权重。</p>\n<p> - Pi0模型：使用Openpi代码库中的检查点。</p>\n<p> - GPT模型：通过OpenAI Batch API进行推理。</p>\n<p> - 硬件：NVIDIA L4（OpenVLA）、单张A100 40GB（Pi0 Base）、四张A100（Pi0 Fast）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Procgen程序生成的16个不同2D游戏环境，每个环境具有独特的任务、动作空间和奖励结构。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>校准度指标</strong>：Brier MAE（最大值2）、归一化Brier MAE、分位数过滤Brier MAE。</p>\n<p> - <strong>分类指标</strong>：微观/宏观精度、召回率、F1分数、精确匹配率。</p>\n<p> - <strong>鲁棒性指标</strong>：无效预测比例、最大相对Brier MAE。</p>\n<p> - <strong>类别分析</strong>：类别级精度/召回率/F1分数，用于检测模型偏差。</p>\n<p> - <strong>特殊指标</strong>：宏观召回率作为最代表性指标，因不受多数类影响。</p>"
  },
  {
    "date": "2025-05-07",
    "title": "Vision-Language-Action Models: Concepts, Progress, Applications and Challenges",
    "link": "http://arxiv.org/abs/2505.04769",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-06",
    "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2505.03912",
    "summary_markdown": "### 论文研究单位\n西湖大学、浙江大学、西安交通大学、香港科技大学（广州）\n### 论文概述\n本文对双系统视觉-语言-动作（VLA）架构进行了简短综述和实证分析，并提出一个名为OpenHelix的开源双系统VLA模型，旨在解决现有开源双系统VLA工作不足的问题，并为性能分析和优化提供基础。项目旨在持续更新实验结论和性能改进的开源模型。\n### 论文核心贡献点\n1. 对现有双系统VLA架构的结构设计进行了总结和比较。\n2. 对双系统架构的核心设计元素进行了系统的实证评估，包括MLLM选择、策略选择、潜在特征表示选择、MLLM训练策略、策略训练策略、双系统集成策略和双系统异步策略。\n3. 提供了一个低成本的开源模型OpenHelix，用于进一步探索，并计划持续更新。\n4. 定义了双系统VLA的正式标准，指出System 1必须包含实时感知输入（如RGB图像）。\n### 论文方法描述\nOpenHelix采用双系统架构：\n- 高层MLLM（System 2）：使用LLaVA1.0作为视觉语言模型，接收第三人称RGB图像、任务指令和可学习的<ACT>标记作为输入。通过提示调优（Prompt-tuning）训练MLLM，仅训练lm-head层，保持其他模型参数固定，以减少训练成本并保留多模态推理能力。输出<ACT>标记的最终层特征嵌入作为低层策略的潜在目标。\n- 低层策略（System 1）：使用预训练的3D Diffusion Actor（3DDA）作为策略，接收高层MLLM的潜在目标、3D场景标记和本体感觉信息，通过交叉注意力层迭代预测动作噪声以产生精确的动作轨迹和夹爪状态。使用线性层替换3D Diffusion Actor的文本编码器，对齐高层MLLM输出嵌入与低层策略输入维度。\n- 双系统集成：使用MLP投影仪连接高层和低层组件，采用两阶段训练策略：首先冻结高层MLLM，训练MLP投影仪和低层策略，然后解冻高层MLLM进行联合训练。引入辅助任务（预测动作的位置、旋转、开/关），强制模型捕获更多视觉信息，确保潜在嵌入包含视觉和文本信息。\n- 异步策略：采用同步训练和异步测试，在测试时高层MLLM推理频率低于低层策略。\n### 论文使用数据集和训练资源\n数据集：CALVIN环境及其变体CALVIN-E（用于语言指令泛化测试）和CALVIN-D（用于动态场景测试，物体以四种不同方式移动）。\n训练资源：未明确提及具体硬件，但使用预训练的LLaVA1.0（7B）和3D Diffusion Actor作为基础模型，采用提示调优和两阶段投影仪训练策略。\n### 论文使用的评估环境和评估指标\n评估环境：CALVIN模拟环境（标准ABC-D场景）、CALVIN-E（增强语言指令）和CALVIN-D（动态场景，物体移动）。\n评估指标：连续完成任务成功率（百分比），平均任务长度。评估时使用前100个任务进行快速消融实验，最终评估扩展至1000个任务。指标越高表示性能越好（符号↑）。</think>\n### 论文研究单位\n西湖大学、浙江大学、西安交通大学、香港科技大学（广州）\n### 论文概述\n本文对双系统视觉-语言-动作（VLA）架构进行了简短综述和实证分析，并提出一个名为OpenHelix的开源双系统VLA模型，旨在解决现有开源双系统VLA工作不足的问题，并为性能分析和优化提供基础。项目旨在持续更新实验结论和性能改进的开源模型。\n### 论文核心贡献点\n1. 对现有双系统VLA架构的结构设计进行了总结和比较。\n2. 对双系统架构的核心设计元素进行了系统的实证评估，包括MLLM选择、策略选择、潜在特征表示选择、MLLM训练策略、策略训练策略、双系统集成策略和双系统异步策略。\n3. 提供了一个低成本的开源模型OpenHelix，用于进一步探索，并计划持续更新。\n4. 定义了双系统VLA的正式标准，指出System 1必须包含实时感知输入（如RGB图像）。\n### 论文方法描述\nOpenHelix采用双系统架构：\n- 高层MLLM（System 2）：使用LLaVA1.0作为视觉语言模型，接收第三人称RGB图像、任务指令和可学习的<ACT>标记作为输入。通过提示调优（Prompt-tuning）训练MLLM，仅训练lm-head层，保持其他模型参数固定，以减少训练成本并保留多模态推理能力。输出<ACT>标记的最终层特征嵌入作为低层策略的潜在目标。\n- 低层策略（System 1）：使用预训练的3D Diffusion Actor（3DDA）作为策略，接收高层MLLM的潜在目标、3D场景标记和本体感觉信息，通过交叉注意力层迭代预测动作噪声以产生精确的动作轨迹和夹爪状态。使用线性层替换3D Diffusion Actor的文本编码器，对齐高层MLLM输出嵌入与低层策略输入维度。\n- 双系统集成：使用MLP投影仪连接高层和低层组件，采用两阶段训练策略：首先冻结高层MLLM，训练MLP投影仪和低层策略，然后解冻高层MLLM进行联合训练。引入辅助任务（预测动作的位置、旋转、开/关），强制模型捕获更多视觉信息，确保潜在嵌入包含视觉和文本信息。\n- 异步策略：采用同步训练和异步测试，在测试时高层MLLM推理频率低于低层策略。\n### 论文使用数据集和训练资源\n数据集：CALVIN环境及其变体CALVIN-E（用于语言指令泛化测试）和CALVIN-D（用于动态场景测试，物体以四种不同方式移动）。\n训练资源：未明确提及具体硬件，但使用预训练的LLaVA1.0（7B）和3D Diffusion Actor作为基础模型，采用提示调优和两阶段投影仪训练策略。\n### 论文使用的评估环境和评估指标\n评估环境：CALVIN模拟环境（标准ABC-D场景）、CALVIN-E（增强语言指令）和CALVIN-D（动态场景，物体移动）。\n评估指标：连续完成任务成功率（百分比），平均任务长度。评估时使用前100个任务进行快速消融实验，最终评估扩展至1000个任务。指标越高表示性能越好（符号↑）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>西湖大学、浙江大学、西安交通大学、香港科技大学（广州）</p>\n<h3>论文概述</h3>\n<p>本文对双系统视觉-语言-动作（VLA）架构进行了简短综述和实证分析，并提出一个名为OpenHelix的开源双系统VLA模型，旨在解决现有开源双系统VLA工作不足的问题，并为性能分析和优化提供基础。项目旨在持续更新实验结论和性能改进的开源模型。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>对现有双系统VLA架构的结构设计进行了总结和比较。</li><li>对双系统架构的核心设计元素进行了系统的实证评估，包括MLLM选择、策略选择、潜在特征表示选择、MLLM训练策略、策略训练策略、双系统集成策略和双系统异步策略。</li><li>提供了一个低成本的开源模型OpenHelix，用于进一步探索，并计划持续更新。</li><li>定义了双系统VLA的正式标准，指出System 1必须包含实时感知输入（如RGB图像）。</li></ol>\n<h3>论文方法描述</h3>\n<p>OpenHelix采用双系统架构：</p>\n<ul><li>高层MLLM（System 2）：使用LLaVA1.0作为视觉语言模型，接收第三人称RGB图像、任务指令和可学习的<ACT>标记作为输入。通过提示调优（Prompt-tuning）训练MLLM，仅训练lm-head层，保持其他模型参数固定，以减少训练成本并保留多模态推理能力。输出<ACT>标记的最终层特征嵌入作为低层策略的潜在目标。</li><li>低层策略（System 1）：使用预训练的3D Diffusion Actor（3DDA）作为策略，接收高层MLLM的潜在目标、3D场景标记和本体感觉信息，通过交叉注意力层迭代预测动作噪声以产生精确的动作轨迹和夹爪状态。使用线性层替换3D Diffusion Actor的文本编码器，对齐高层MLLM输出嵌入与低层策略输入维度。</li><li>双系统集成：使用MLP投影仪连接高层和低层组件，采用两阶段训练策略：首先冻结高层MLLM，训练MLP投影仪和低层策略，然后解冻高层MLLM进行联合训练。引入辅助任务（预测动作的位置、旋转、开/关），强制模型捕获更多视觉信息，确保潜在嵌入包含视觉和文本信息。</li><li>异步策略：采用同步训练和异步测试，在测试时高层MLLM推理频率低于低层策略。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：CALVIN环境及其变体CALVIN-E（用于语言指令泛化测试）和CALVIN-D（用于动态场景测试，物体以四种不同方式移动）。</p>\n<p>训练资源：未明确提及具体硬件，但使用预训练的LLaVA1.0（7B）和3D Diffusion Actor作为基础模型，采用提示调优和两阶段投影仪训练策略。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：CALVIN模拟环境（标准ABC-D场景）、CALVIN-E（增强语言指令）和CALVIN-D（动态场景，物体移动）。</p>\n<p>评估指标：连续完成任务成功率（百分比），平均任务长度。评估时使用前100个任务进行快速消融实验，最终评估扩展至1000个任务。指标越高表示性能越好（符号↑）。</think></p>\n<h3>论文研究单位</h3>\n<p>西湖大学、浙江大学、西安交通大学、香港科技大学（广州）</p>\n<h3>论文概述</h3>\n<p>本文对双系统视觉-语言-动作（VLA）架构进行了简短综述和实证分析，并提出一个名为OpenHelix的开源双系统VLA模型，旨在解决现有开源双系统VLA工作不足的问题，并为性能分析和优化提供基础。项目旨在持续更新实验结论和性能改进的开源模型。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>对现有双系统VLA架构的结构设计进行了总结和比较。</li><li>对双系统架构的核心设计元素进行了系统的实证评估，包括MLLM选择、策略选择、潜在特征表示选择、MLLM训练策略、策略训练策略、双系统集成策略和双系统异步策略。</li><li>提供了一个低成本的开源模型OpenHelix，用于进一步探索，并计划持续更新。</li><li>定义了双系统VLA的正式标准，指出System 1必须包含实时感知输入（如RGB图像）。</li></ol>\n<h3>论文方法描述</h3>\n<p>OpenHelix采用双系统架构：</p>\n<ul><li>高层MLLM（System 2）：使用LLaVA1.0作为视觉语言模型，接收第三人称RGB图像、任务指令和可学习的<ACT>标记作为输入。通过提示调优（Prompt-tuning）训练MLLM，仅训练lm-head层，保持其他模型参数固定，以减少训练成本并保留多模态推理能力。输出<ACT>标记的最终层特征嵌入作为低层策略的潜在目标。</li><li>低层策略（System 1）：使用预训练的3D Diffusion Actor（3DDA）作为策略，接收高层MLLM的潜在目标、3D场景标记和本体感觉信息，通过交叉注意力层迭代预测动作噪声以产生精确的动作轨迹和夹爪状态。使用线性层替换3D Diffusion Actor的文本编码器，对齐高层MLLM输出嵌入与低层策略输入维度。</li><li>双系统集成：使用MLP投影仪连接高层和低层组件，采用两阶段训练策略：首先冻结高层MLLM，训练MLP投影仪和低层策略，然后解冻高层MLLM进行联合训练。引入辅助任务（预测动作的位置、旋转、开/关），强制模型捕获更多视觉信息，确保潜在嵌入包含视觉和文本信息。</li><li>异步策略：采用同步训练和异步测试，在测试时高层MLLM推理频率低于低层策略。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：CALVIN环境及其变体CALVIN-E（用于语言指令泛化测试）和CALVIN-D（用于动态场景测试，物体以四种不同方式移动）。</p>\n<p>训练资源：未明确提及具体硬件，但使用预训练的LLaVA1.0（7B）和3D Diffusion Actor作为基础模型，采用提示调优和两阶段投影仪训练策略。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：CALVIN模拟环境（标准ABC-D场景）、CALVIN-E（增强语言指令）和CALVIN-D（动态场景，物体移动）。</p>\n<p>评估指标：连续完成任务成功率（百分比），平均任务长度。评估时使用前100个任务进行快速消融实验，最终评估扩展至1000个任务。指标越高表示性能越好（符号↑）。</p>"
  },
  {
    "date": "2025-05-06",
    "title": "RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration",
    "link": "http://arxiv.org/abs/2505.03673",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-06",
    "title": "Task Reconstruction and Extrapolation for $π_0$ using Text Latent",
    "link": "http://arxiv.org/abs/2505.03500",
    "summary_markdown": "### 论文研究单位\nIndependent\n### 论文概述\n该论文研究了视觉-语言-动作模型在任务外推方面的局限性，发现这些模型虽然在演示过的任务上表现良好，但在需要组合不同任务中的技能以解决新任务时常常失败。作者提出了一种名为“text latent”的方法，这是一种从模型内部隐藏状态中提取的任务特定向量。通过将“text latent”注入模型的残差流，可以重构相应的任务行为。进一步地，通过线性插值混合不同任务的“text latent”（Text Latent Interpolation, TLI），可以组合子技能以生成新颖的行为，从而实现任务外推。研究揭示了π₀模型内部编码了独立且可组合的技能表示，但其本身无法自主地组合这些表示。此外，论文还提出并验证了一个新的基准测试集libero-ood，并发现当前最先进的VLAs普遍存在“空间过拟合”问题，即将对象名称与其在演示场景中的空间位置相关联，而非真正理解对象。\n### 论文核心贡献点\n1. **引入“text latent”**：提出并验证了一种从模型文本标记的隐藏状态中提取的任务特定向量，它编码了完成任务所需的核心语义知识。\n2. **任务重构**：证明了通过将“text latent”注入模型的残差流，可以在没有原始任务提示的情况下重构任务行为，成功率超过80%。\n3. **提出Text Latent Interpolation (TLI)**：开发了一种通过在时间步上线性插值两个“text latent”来组合不同任务技能的方法，使得模型能够完成需要技能拼接的泛化任务。\n4. **提出libero-ood基准**：创建了一个包含20个外推任务的新基准测试集，用于评估VLAs的组合泛化能力，这些任务需要模型拼接已学习但未曾组合过的子轨迹。\n5. **显著提升外推性能**：通过应用TLI方法，将π₀模型在libero-ood基准上的成功率从9%提升至83%，证明了其内在的可组合潜力。\n6. **发现VLAs的共性缺陷**：通过在多个SOTA VLAs上测试，发现它们普遍存在“空间过拟合”问题，即模型将对象名称与训练数据中的特定空间位置绑定，缺乏真正的对象和目标理解能力。\n### 论文方法描述\n1. **Preliminary**：首先阐述了基于Transformer的VLAs的工作原理，即视觉、语言和本体感觉信息被编码为嵌入，并依次通过L层Transformer，最终由这些嵌入和隐藏状态生成动作。\n2. **Text Latent 提取**：对于一个给定的任务，通过运行模型在多个演示轨迹上，收集所有时间步和所有Transformer层中对应文本标记的隐藏状态。然后对这些状态进行元素平均，得到一个形状为 `(L-1) x \\|T\\|x d` 的张量，即该任务的“text latent”。\n3. **Text Latent Interpolation (TLI)**：为了解决需要组合两个基础任务技能的新任务，该方法在每个时间步 `i` 计算一个插值系数 `α = i / λ`（其中 `λ` 是预设的过渡步数）。然后，将两个基础任务的“text latent” (`𝒯¹`, `𝒯²`) 按该系数进行加权，并将结果注入到当前文本标记的隐藏状态中。这使得模型在任务开始时执行任务1的行为，随着时间推进，行为逐渐平滑过渡到任务2。\n### 论文使用数据集和训练资源\n1. **数据集**：\n * **标准基准**：LIBERO仿真环境中的三个任务套件，包括 `libero-goal`, `libero-object`, `libero-spatial`。\n * **新基准**：作者提出的 `libero-ood`，包含 `libero-goal-ood` 和 `libero-spatial-ood` 两个子集，共20个任务。这些任务的设计理念是，完成任务所需的抓取和放置动作分别在训练任务中出现过，但它们的组合是全新的。\n2. **训练/计算资源**：\n * “text latent” 的提取基于每个任务的20个演示轨迹。\n * 所有实验均在 **Nvidia RTX 4090** GPU上完成。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：所有实验均在 **LIBERO仿真环境**中进行。\n2. **评估指标**：\n * 主要指标为 **任务成功率**。对于每个任务，执行10次独立的评估（使用不同随机种子）。每个任务套体的最终成功率是总100次尝试中成功次数的比例。\n * 辅以 **定性分析**，通过可视化模型的行为轨迹来分析其决策过程，特别是“空间过拟合”现象。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Independent</p>\n<h3>论文概述</h3>\n<p>该论文研究了视觉-语言-动作模型在任务外推方面的局限性，发现这些模型虽然在演示过的任务上表现良好，但在需要组合不同任务中的技能以解决新任务时常常失败。作者提出了一种名为“text latent”的方法，这是一种从模型内部隐藏状态中提取的任务特定向量。通过将“text latent”注入模型的残差流，可以重构相应的任务行为。进一步地，通过线性插值混合不同任务的“text latent”（Text Latent Interpolation, TLI），可以组合子技能以生成新颖的行为，从而实现任务外推。研究揭示了π₀模型内部编码了独立且可组合的技能表示，但其本身无法自主地组合这些表示。此外，论文还提出并验证了一个新的基准测试集libero-ood，并发现当前最先进的VLAs普遍存在“空间过拟合”问题，即将对象名称与其在演示场景中的空间位置相关联，而非真正理解对象。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>引入“text latent”</strong>：提出并验证了一种从模型文本标记的隐藏状态中提取的任务特定向量，它编码了完成任务所需的核心语义知识。</li><li><strong>任务重构</strong>：证明了通过将“text latent”注入模型的残差流，可以在没有原始任务提示的情况下重构任务行为，成功率超过80%。</li><li><strong>提出Text Latent Interpolation (TLI)</strong>：开发了一种通过在时间步上线性插值两个“text latent”来组合不同任务技能的方法，使得模型能够完成需要技能拼接的泛化任务。</li><li><strong>提出libero-ood基准</strong>：创建了一个包含20个外推任务的新基准测试集，用于评估VLAs的组合泛化能力，这些任务需要模型拼接已学习但未曾组合过的子轨迹。</li><li><strong>显著提升外推性能</strong>：通过应用TLI方法，将π₀模型在libero-ood基准上的成功率从9%提升至83%，证明了其内在的可组合潜力。</li><li><strong>发现VLAs的共性缺陷</strong>：通过在多个SOTA VLAs上测试，发现它们普遍存在“空间过拟合”问题，即模型将对象名称与训练数据中的特定空间位置绑定，缺乏真正的对象和目标理解能力。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>Preliminary</strong>：首先阐述了基于Transformer的VLAs的工作原理，即视觉、语言和本体感觉信息被编码为嵌入，并依次通过L层Transformer，最终由这些嵌入和隐藏状态生成动作。</li><li><strong>Text Latent 提取</strong>：对于一个给定的任务，通过运行模型在多个演示轨迹上，收集所有时间步和所有Transformer层中对应文本标记的隐藏状态。然后对这些状态进行元素平均，得到一个形状为 <code>(L-1) x \\|T\\|x d</code> 的张量，即该任务的“text latent”。</li><li><strong>Text Latent Interpolation (TLI)</strong>：为了解决需要组合两个基础任务技能的新任务，该方法在每个时间步 <code>i</code> 计算一个插值系数 <code>α = i / λ</code>（其中 <code>λ</code> 是预设的过渡步数）。然后，将两个基础任务的“text latent” (<code>𝒯¹</code>, <code>𝒯²</code>) 按该系数进行加权，并将结果注入到当前文本标记的隐藏状态中。这使得模型在任务开始时执行任务1的行为，随着时间推进，行为逐渐平滑过渡到任务2。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> * <strong>标准基准</strong>：LIBERO仿真环境中的三个任务套件，包括 <code>libero-goal</code>, <code>libero-object</code>, <code>libero-spatial</code>。</p>\n<p> * <strong>新基准</strong>：作者提出的 <code>libero-ood</code>，包含 <code>libero-goal-ood</code> 和 <code>libero-spatial-ood</code> 两个子集，共20个任务。这些任务的设计理念是，完成任务所需的抓取和放置动作分别在训练任务中出现过，但它们的组合是全新的。</p>\n<ol><li><strong>训练/计算资源</strong>：</li></ol>\n<p> * “text latent” 的提取基于每个任务的20个演示轨迹。</p>\n<p> * 所有实验均在 <strong>Nvidia RTX 4090</strong> GPU上完成。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：所有实验均在 <strong>LIBERO仿真环境</strong>中进行。</li><li><strong>评估指标</strong>：</li></ol>\n<p> * 主要指标为 <strong>任务成功率</strong>。对于每个任务，执行10次独立的评估（使用不同随机种子）。每个任务套体的最终成功率是总100次尝试中成功次数的比例。</p>\n<p> * 辅以 <strong>定性分析</strong>，通过可视化模型的行为轨迹来分析其决策过程，特别是“空间过拟合”现象。</p>"
  },
  {
    "date": "2025-05-06",
    "title": "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data",
    "link": "http://arxiv.org/abs/2505.03233",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-06",
    "title": "Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets",
    "link": "http://arxiv.org/abs/2505.03174",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-04",
    "title": "Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions",
    "link": "http://arxiv.org/abs/2505.02152",
    "summary_markdown": "### 论文研究单位\n- 上海交通大学\n- 加州大学伯克利分校\n- 北卡罗来纳大学教堂山分校\n### 论文概述\n本文提出了一种名为Interleave-VLA的新型机器人学习范式，旨在通过交错的图像-文本指令增强机器人操作。现有的文本指令机器人策略在泛化到未见场景时存在局限，而交错图像-文本指令能提供更丰富、偏差更小的上下文，使机器人更好地处理基于上下文视觉定位的未见任务。论文介绍了首个能够理解交错指令并直接生成连续动作序列的Interleave-VLA范式，以及一个包含21万次交互的大规模真实世界交错数据集Open Interleaved X-Embodiment。实验表明，该方法在域外泛化上比文本基线高2倍，并支持零样本泛化到草图等多样化指令。\n### 论文核心贡献点\n1. **提出Interleave-VLA范式**：首个支持交错图像-文本指令输入的机器人学习范式，通过最小修改扩展现有VLA模型（如π₀和OpenVLA），实现强零样本泛化。\n2. **构建大规模数据集**：开发了自动化流程，将Open X-Embodiment的文本指令转换为交错指令，生成包含21万次交互和1300万帧的真实世界数据集。\n3. **揭示注意力幻觉问题**：分析表明文本VLA的泛化失败常源于注意力偏差、扩散和泄漏，而交错指令通过上下文视觉定位有效缓解此问题。\n### 论文方法描述\n- **范式设计**：Interleave-VLA包含三个核心组件：\n 1. **轻量适配模块**：在tokenizer中引入特殊分隔符（如<BOI>和<EOI>），使现有VLA无需架构变更即可处理交错输入。\n 2. **可扩展训练流程**：利用交错数据集训练，保留原始目标和超参数。\n 3. **多功能推理接口**：支持文本或交错指令，允许使用实时摄像头裁剪图、网络图片或手绘草图。\n- **数据集构建**：自动化流程包括三步：\n 1. **指令解析**：用Qwen2.5提取关键对象。\n 2. **开放词汇检测**：用OWLv2定位并裁剪目标对象（准确率82.6%）。\n 3. **数据验证**：用Qwen2.5-VL验证并结合Segment Anything优化，准确率提升至95.6%。\n### 论文使用数据集和训练资源\n- **数据集**：基于Open X-Embodiment构建的Open Interleaved X-Embodiment数据集，包含21万次交互、1300万帧、3500个独特物体和多种任务。数据增强包括随机混合网络图像。\n- **训练资源**：使用π₀和OpenVLA作为基础模型，在交错数据集上微调，无需修改超参数。具体GPU集群细节未明确说明，但实验涉及大规模训练。\n### 论文使用的评估环境和评估指标\n- **仿真环境**：SimplerEnv（WidowX配置），评估域内（4个任务）和域外任务（视觉泛化、语义泛化各3个任务）。\n- **真实机器人**：FANUC LRMate 200iD/7L机械臂，SMC夹爪，测试食品抓取和厨具放置任务。\n- **评估指标**：\n - **成功率**：任务完成比例。\n - **物体选择准确率**：正确选择目标物体的比例。\n - **泛化性能**：域外任务的相对提升（如2×增益）。\n - **零样本能力**：对草图、裁剪图等未见指令类型的准确率。</think>\n### 论文研究单位\n上海交通大学, UC Berkeley, UNC Chapel Hill\n### 论文概述\n本文提出了一种名为Interleave-VLA的新型机器人学习范式，旨在通过交错的图像-文本指令增强机器人操作能力。现有基于纯文本指令的机器人策略在泛化到未见场景时存在局限，而交错图像-文本指令能提供更丰富、偏差更小的上下文，使机器人更好地处理基于上下文视觉定位的未见任务。论文介绍了首个能够理解交错指令并直接生成连续动作序列的Interleave-VLA范式，以及一个包含21万次交互的大规模真实世界交错数据集Open Interleaved X-Embodiment。实验表明，该方法在域外泛化上比文本基线高2倍，并支持零样本泛化到草图等多样化指令。\n### 论文核心贡献点\n1. 提出Interleave-VLA范式：首个支持交错图像-文本指令输入的机器人学习范式，通过最小修改扩展现有VLA模型（如π₀和OpenVLA），实现强零样本泛化。\n2. 构建大规模数据集：开发了自动化流程，将Open X-Embodiment的文本指令转换为交错指令，生成包含21万次交互和1300万帧的真实世界数据集。\n3. 揭示注意力幻觉问题：分析表明文本VLA的泛化失败常源于注意力偏差、扩散和泄漏，而交错指令通过上下文视觉定位有效缓解此问题。\n### 论文方法描述\nInterleave-VLA包含三个核心组件：\n1. 轻量适配模块：在tokenizer中引入特殊分隔符（如<BOI>和<EOI>），使现有VLA无需架构变更即可处理交错输入。\n2. 可扩展训练流程：利用交错数据集训练，保留原始目标和超参数。\n3. 多功能推理接口：支持文本或交错指令，允许使用实时摄像头裁剪图、网络图片或手绘草图。\n数据集构建流程包括三步：指令解析（用Qwen2.5提取关键对象）、开放词汇检测（用OWLv2定位并裁剪目标对象，准确率82.6%）、数据验证（用Qwen2.5-VL验证并结合Segment Anything优化，准确率提升至95.6%）。\n### 论文使用数据集和训练资源\n数据集为Open Interleaved X-Embodiment，基于Open X-Embodiment构建，包含21万次交互、1300万帧、3500个独特物体和多种任务。数据增强包括随机混合网络图像。训练资源使用π₀和OpenVLA作为基础模型，在交错数据集上微调，无需修改超参数。\n### 论文使用的评估环境和评估指标\n仿真环境为SimplerEnv（WidowX配置），评估域内（4个任务）和域外任务（视觉泛化、语义泛化各3个任务）。真实机器人为FANUC LRMate 200iD/7L机械臂，SMC夹爪，测试食品抓取和厨具放置任务。评估指标包括成功率、物体选择准确率、泛化性能（域外任务相对提升）和零样本能力（对草图、裁剪图等未见指令类型的准确率）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>上海交通大学</li><li>加州大学伯克利分校</li><li>北卡罗来纳大学教堂山分校</li></ul>\n<h3>论文概述</h3>\n<p>本文提出了一种名为Interleave-VLA的新型机器人学习范式，旨在通过交错的图像-文本指令增强机器人操作。现有的文本指令机器人策略在泛化到未见场景时存在局限，而交错图像-文本指令能提供更丰富、偏差更小的上下文，使机器人更好地处理基于上下文视觉定位的未见任务。论文介绍了首个能够理解交错指令并直接生成连续动作序列的Interleave-VLA范式，以及一个包含21万次交互的大规模真实世界交错数据集Open Interleaved X-Embodiment。实验表明，该方法在域外泛化上比文本基线高2倍，并支持零样本泛化到草图等多样化指令。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>提出Interleave-VLA范式</strong>：首个支持交错图像-文本指令输入的机器人学习范式，通过最小修改扩展现有VLA模型（如π₀和OpenVLA），实现强零样本泛化。</li><li><strong>构建大规模数据集</strong>：开发了自动化流程，将Open X-Embodiment的文本指令转换为交错指令，生成包含21万次交互和1300万帧的真实世界数据集。</li><li><strong>揭示注意力幻觉问题</strong>：分析表明文本VLA的泛化失败常源于注意力偏差、扩散和泄漏，而交错指令通过上下文视觉定位有效缓解此问题。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>范式设计</strong>：Interleave-VLA包含三个核心组件：</li></ul>\n<p> 1. <strong>轻量适配模块</strong>：在tokenizer中引入特殊分隔符（如<BOI>和<EOI>），使现有VLA无需架构变更即可处理交错输入。</p>\n<p> 2. <strong>可扩展训练流程</strong>：利用交错数据集训练，保留原始目标和超参数。</p>\n<p> 3. <strong>多功能推理接口</strong>：支持文本或交错指令，允许使用实时摄像头裁剪图、网络图片或手绘草图。</p>\n<ul><li><strong>数据集构建</strong>：自动化流程包括三步：</li></ul>\n<p> 1. <strong>指令解析</strong>：用Qwen2.5提取关键对象。</p>\n<p> 2. <strong>开放词汇检测</strong>：用OWLv2定位并裁剪目标对象（准确率82.6%）。</p>\n<p> 3. <strong>数据验证</strong>：用Qwen2.5-VL验证并结合Segment Anything优化，准确率提升至95.6%。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：基于Open X-Embodiment构建的Open Interleaved X-Embodiment数据集，包含21万次交互、1300万帧、3500个独特物体和多种任务。数据增强包括随机混合网络图像。</li><li><strong>训练资源</strong>：使用π₀和OpenVLA作为基础模型，在交错数据集上微调，无需修改超参数。具体GPU集群细节未明确说明，但实验涉及大规模训练。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>仿真环境</strong>：SimplerEnv（WidowX配置），评估域内（4个任务）和域外任务（视觉泛化、语义泛化各3个任务）。</li><li><strong>真实机器人</strong>：FANUC LRMate 200iD/7L机械臂，SMC夹爪，测试食品抓取和厨具放置任务。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：任务完成比例。</p>\n<p> - <strong>物体选择准确率</strong>：正确选择目标物体的比例。</p>\n<p> - <strong>泛化性能</strong>：域外任务的相对提升（如2×增益）。</p>\n<p> - <strong>零样本能力</strong>：对草图、裁剪图等未见指令类型的准确率。</think></p>\n<h3>论文研究单位</h3>\n<p>上海交通大学, UC Berkeley, UNC Chapel Hill</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为Interleave-VLA的新型机器人学习范式，旨在通过交错的图像-文本指令增强机器人操作能力。现有基于纯文本指令的机器人策略在泛化到未见场景时存在局限，而交错图像-文本指令能提供更丰富、偏差更小的上下文，使机器人更好地处理基于上下文视觉定位的未见任务。论文介绍了首个能够理解交错指令并直接生成连续动作序列的Interleave-VLA范式，以及一个包含21万次交互的大规模真实世界交错数据集Open Interleaved X-Embodiment。实验表明，该方法在域外泛化上比文本基线高2倍，并支持零样本泛化到草图等多样化指令。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出Interleave-VLA范式：首个支持交错图像-文本指令输入的机器人学习范式，通过最小修改扩展现有VLA模型（如π₀和OpenVLA），实现强零样本泛化。</li><li>构建大规模数据集：开发了自动化流程，将Open X-Embodiment的文本指令转换为交错指令，生成包含21万次交互和1300万帧的真实世界数据集。</li><li>揭示注意力幻觉问题：分析表明文本VLA的泛化失败常源于注意力偏差、扩散和泄漏，而交错指令通过上下文视觉定位有效缓解此问题。</li></ol>\n<h3>论文方法描述</h3>\n<p>Interleave-VLA包含三个核心组件：</p>\n<ol><li>轻量适配模块：在tokenizer中引入特殊分隔符（如<BOI>和<EOI>），使现有VLA无需架构变更即可处理交错输入。</li><li>可扩展训练流程：利用交错数据集训练，保留原始目标和超参数。</li><li>多功能推理接口：支持文本或交错指令，允许使用实时摄像头裁剪图、网络图片或手绘草图。</li></ol>\n<p>数据集构建流程包括三步：指令解析（用Qwen2.5提取关键对象）、开放词汇检测（用OWLv2定位并裁剪目标对象，准确率82.6%）、数据验证（用Qwen2.5-VL验证并结合Segment Anything优化，准确率提升至95.6%）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集为Open Interleaved X-Embodiment，基于Open X-Embodiment构建，包含21万次交互、1300万帧、3500个独特物体和多种任务。数据增强包括随机混合网络图像。训练资源使用π₀和OpenVLA作为基础模型，在交错数据集上微调，无需修改超参数。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>仿真环境为SimplerEnv（WidowX配置），评估域内（4个任务）和域外任务（视觉泛化、语义泛化各3个任务）。真实机器人为FANUC LRMate 200iD/7L机械臂，SMC夹爪，测试食品抓取和厨具放置任务。评估指标包括成功率、物体选择准确率、泛化性能（域外任务相对提升）和零样本能力（对草图、裁剪图等未见指令类型的准确率）。</p>"
  },
  {
    "date": "2025-04-28",
    "title": "NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks",
    "link": "http://arxiv.org/abs/2504.19854",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n新加坡科技与设计大学，Lambda Labs\n## 论文概述\n本文提出了NORA（Neural Orchestrator for Robotic Autonomy），一个3B参数的小型开源通用视觉-语言-动作（VLA）模型，用于处理具身任务。NORA旨在减少现有大型VLA模型的高计算开销，同时保持强大的任务执行能力。模型基于Qwen-2.5-VL-3B多模态模型，并利用FAST+标记器进行高效的动作序列生成。实验表明，NORA在多个现实世界和模拟任务中优于现有的大型VLA模型，以显著降低的计算开销实现了更好的任务性能，使其成为实时机器人自主性更实用的解决方案。\n## 论文核心贡献点\n1. 提出了NORA，一个3B参数的VLA模型，基于Qwen-2.5-VL-3B主干，采用高效的动作解码策略压缩高度相关的动作标记，确保在各种机器人任务中的鲁棒性能。\n2. 进行了全面的实验，分析了不同动作预测策略的影响，包括单步与分块动作预测的详细比较，证明了其设计在提高动作生成效率方面的有效性。\n3. 开源了完整的NORA框架，包括模型检查点、训练策略和评估协议，以促进可重复性并推动可扩展的视觉-语言-动作模型在机器人学领域的进一步研究。\n## 论文方法描述\nNORA的架构基于预训练的视觉-语言模型（VLM）M，该模型自回归地预测从时间t到t+N的动作块，编码未来的动作。输入包括自然语言任务指令c和t时间的n帧视觉观测ot，形成整体输入Xt=[ot,c]。动作块at:t+N由FAST+机器人标记器在训练时编码的离散标记序列R=[rt,...,rt+N]表示。VLM M基于Xt条件预测动作块的标记序列R，然后使用FAST+_decode将标记序列解码为连续动作。模型选择了Qwen-2.5-VL作为主干，因其3B的小参数规模，并通过FAST+标记器引入了2048个额外标记来增强VLM标记器的词汇量。观测ot保持为单个视觉帧，动作块大小选择为1。NORA使用标准的下一标记预测损失语言建模目标进行训练。\n## 论文使用数据集和训练资源\n模型在Open X-Embodiment数据集上进行预训练，该数据集包含970k真实世界机器人演示，来自不同机器人执行的各种任务，包括BridgeV2和DROID等子集。训练在单个8x H100 GPU节点上进行约三周，总计约4000 H100 GPU小时。使用批量大小为256，采用AdamW优化器进行110万次梯度更新。应用了前50k步的线性预热，峰值学习率为5e-5，随后采用余弦衰减至零。为增强训练效率和减少内存占用，使用了FlashAttention并以bf16精度训练。此外，训练了NORA-Long变体，使用动作块大小为5，在相同预训练数据集上预训练了90万步。\n## 论文使用的评估环境和评估指标\n评估在两种环境中进行：真实世界的WidowX机器人平台和LIBERO模拟基准。真实世界评估包括9个多样任务，每个任务进行10次试验，评估指令理解、空间推理和多任务运动规划能力。模拟基准包括30个程序生成的解耦任务，涉及空间布局（LIBERO-Spatial）、物体（LIBERO-Object）、任务目标（LIBERO-Goal）和10个长视野纠缠任务（LIBERO-Long）。所有策略在相同设置下进行500次试验评估。主要评估指标为成功率，定义为成功完成任务的比例，计算公式为：% success rate := (100 E_{τ~D_eval} 1[task τ is successfully completed])%。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>新加坡科技与设计大学，Lambda Labs</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文提出了NORA（Neural Orchestrator for Robotic Autonomy），一个3B参数的小型开源通用视觉-语言-动作（VLA）模型，用于处理具身任务。NORA旨在减少现有大型VLA模型的高计算开销，同时保持强大的任务执行能力。模型基于Qwen-2.5-VL-3B多模态模型，并利用FAST+标记器进行高效的动作序列生成。实验表明，NORA在多个现实世界和模拟任务中优于现有的大型VLA模型，以显著降低的计算开销实现了更好的任务性能，使其成为实时机器人自主性更实用的解决方案。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出了NORA，一个3B参数的VLA模型，基于Qwen-2.5-VL-3B主干，采用高效的动作解码策略压缩高度相关的动作标记，确保在各种机器人任务中的鲁棒性能。</li><li>进行了全面的实验，分析了不同动作预测策略的影响，包括单步与分块动作预测的详细比较，证明了其设计在提高动作生成效率方面的有效性。</li><li>开源了完整的NORA框架，包括模型检查点、训练策略和评估协议，以促进可重复性并推动可扩展的视觉-语言-动作模型在机器人学领域的进一步研究。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<p>NORA的架构基于预训练的视觉-语言模型（VLM）M，该模型自回归地预测从时间t到t+N的动作块，编码未来的动作。输入包括自然语言任务指令c和t时间的n帧视觉观测ot，形成整体输入Xt=[ot,c]。动作块at:t+N由FAST+机器人标记器在训练时编码的离散标记序列R=[rt,...,rt+N]表示。VLM M基于Xt条件预测动作块的标记序列R，然后使用FAST+_decode将标记序列解码为连续动作。模型选择了Qwen-2.5-VL作为主干，因其3B的小参数规模，并通过FAST+标记器引入了2048个额外标记来增强VLM标记器的词汇量。观测ot保持为单个视觉帧，动作块大小选择为1。NORA使用标准的下一标记预测损失语言建模目标进行训练。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<p>模型在Open X-Embodiment数据集上进行预训练，该数据集包含970k真实世界机器人演示，来自不同机器人执行的各种任务，包括BridgeV2和DROID等子集。训练在单个8x H100 GPU节点上进行约三周，总计约4000 H100 GPU小时。使用批量大小为256，采用AdamW优化器进行110万次梯度更新。应用了前50k步的线性预热，峰值学习率为5e-5，随后采用余弦衰减至零。为增强训练效率和减少内存占用，使用了FlashAttention并以bf16精度训练。此外，训练了NORA-Long变体，使用动作块大小为5，在相同预训练数据集上预训练了90万步。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<p>评估在两种环境中进行：真实世界的WidowX机器人平台和LIBERO模拟基准。真实世界评估包括9个多样任务，每个任务进行10次试验，评估指令理解、空间推理和多任务运动规划能力。模拟基准包括30个程序生成的解耦任务，涉及空间布局（LIBERO-Spatial）、物体（LIBERO-Object）、任务目标（LIBERO-Goal）和10个长视野纠缠任务（LIBERO-Long）。所有策略在相同设置下进行500次试验评估。主要评估指标为成功率，定义为成功完成任务的比例，计算公式为：% success rate := (100 E_{τ~D_eval} 1[task τ is successfully completed])%。</p>"
  },
  {
    "date": "2025-04-22",
    "title": "$π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization",
    "link": "http://arxiv.org/abs/2504.16054",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n- Physical Intelligence\n## 论文概述\n- 本文提出了一个名为 π₀.₅ 的视觉-语言-动作（VLA）模型，旨在通过在异构任务上进行协同训练，实现机器人在开放世界中的泛化能力。\n- 模型的核心思想是结合来自多种信息源的知识，包括多种机器人平台的数据、网络数据、高级语义预测以及人类语言指令。\n- 论文声称，π₀.₅ 是首个能够在训练中从未见过的全新家庭环境中，执行长达10到15分钟的长时程、灵巧操作任务（如清理厨房或卧室）的端到端学习系统。\n## 论文核心贡献点\n- 提出了一个新的 VLA 模型 π₀.₅ 及其相应的训练配方。\n- 设计了一个协同训练框架，该框架能够有效地整合和利用异构数据源，以提升模型的泛化能力。\n- 实现了一个层次化的推理架构，模型在执行任务时首先预测高级语义子任务，然后基于该子任务生成低级动作。\n- 通过详尽的实验，首次在真实、未见过的家庭环境中，展示了端到端学习系统完成长时程复杂操作任务的能力。\n## 论文方法描述\n- **模型架构**:\n - 模型基于一个统一的 Transformer，能够处理多模态输入（图像、文本、机器人状态）并输出多模态结果（文本子任务、连续动作）。\n - 引入了“动作专家”模块，类似于混合专家模型，专门用于生成连续的动作序列，以提高效率和性能。\n - 将策略分解为 p(动作, 子任务 \\|观测, 指令) = p(动作 \\|观测, 子任务) * p(子任务 \\|观测, 指令)，实现高层次与低层次推理的解耦。\n- **训练过程**:\n - **第一阶段：预训练**\n - 目标：利用大规模多样化的数据混合来适应模型。\n - 数据来源：包括移动机械臂数据（约400小时）、其他非移动机器人数据、实验室跨具身数据、高级语义子任务预测数据、人类语言指令以及网络多模态数据。\n - 动作表示：在此阶段使用离散令牌来表示动作。\n - **第二阶段：后训练**\n - 目标：专门针对移动操作任务进行微调。\n - 数据来源：专注于与任务最相关的移动机械臂数据和人类口头语言指令。\n - 动作表示：改用流匹配来表示连续的动作分布，并引入动作专家，以实现更精细的实时控制。\n- **推理过程**:\n - 采用层次化推理，模型在每一步首先预测一个高级语义子任务（如“拿起盘子”）。\n - 然后，模型以该子任务为条件，通过动作专家预测下一步的低级动作块。\n## 论文使用数据集和训练资源\n- **数据集**:\n - 未使用单一命名数据集，而是采用异构数据混合。\n - 核心数据包括在多个真实家庭中收集的约400小时的移动机械臂操作数据。\n - 辅助数据包括：其他机器人数据、实验室跨具身任务数据、高级语义子任务预测数据、人类语言指令和网络数据（如图像说明、问答、对象定位）。\n- **训练资源**:\n - 论文未明确说明硬件配置，但训练如此规模的视觉-语言-动作模型和数据集需要大量的计算资源（如大规模GPU集群）。\n## 论文使用的评估环境和评估指标\n- **评估环境**:\n - 主要在真实世界的、全新的家庭环境中进行评估，这些环境在训练期间模型从未见过。\n - 测试场景包括全新的厨房和卧室。\n - 评估任务是长时程、多阶段的复杂操作，例如“清理整个厨房”。\n- **评估指标**:\n - 主要指标是任务成功率，通过一个详细的任务评估标准来判断。\n - 评估模型在执行长达10-15分钟任务中的表现。\n - 包含了与其他视觉-语言-动作（VLA）模型的定量比较，以评估其相对性能和泛化能力。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<ul><li>Physical Intelligence</li></ul>\n<h2 class=\"section-title\">论文概述</h2>\n<ul><li>本文提出了一个名为 π₀.₅ 的视觉-语言-动作（VLA）模型，旨在通过在异构任务上进行协同训练，实现机器人在开放世界中的泛化能力。</li><li>模型的核心思想是结合来自多种信息源的知识，包括多种机器人平台的数据、网络数据、高级语义预测以及人类语言指令。</li><li>论文声称，π₀.₅ 是首个能够在训练中从未见过的全新家庭环境中，执行长达10到15分钟的长时程、灵巧操作任务（如清理厨房或卧室）的端到端学习系统。</li></ul>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ul><li>提出了一个新的 VLA 模型 π₀.₅ 及其相应的训练配方。</li><li>设计了一个协同训练框架，该框架能够有效地整合和利用异构数据源，以提升模型的泛化能力。</li><li>实现了一个层次化的推理架构，模型在执行任务时首先预测高级语义子任务，然后基于该子任务生成低级动作。</li><li>通过详尽的实验，首次在真实、未见过的家庭环境中，展示了端到端学习系统完成长时程复杂操作任务的能力。</li></ul>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ul><li><strong>模型架构</strong>:</li></ul>\n<p> - 模型基于一个统一的 Transformer，能够处理多模态输入（图像、文本、机器人状态）并输出多模态结果（文本子任务、连续动作）。</p>\n<p> - 引入了“动作专家”模块，类似于混合专家模型，专门用于生成连续的动作序列，以提高效率和性能。</p>\n<p> - 将策略分解为 p(动作, 子任务 \\|观测, 指令) = p(动作 \\|观测, 子任务) * p(子任务 \\|观测, 指令)，实现高层次与低层次推理的解耦。</p>\n<ul><li><strong>训练过程</strong>:</li></ul>\n<p> - <strong>第一阶段：预训练</strong></p>\n<p> - 目标：利用大规模多样化的数据混合来适应模型。</p>\n<p> - 数据来源：包括移动机械臂数据（约400小时）、其他非移动机器人数据、实验室跨具身数据、高级语义子任务预测数据、人类语言指令以及网络多模态数据。</p>\n<p> - 动作表示：在此阶段使用离散令牌来表示动作。</p>\n<p> - <strong>第二阶段：后训练</strong></p>\n<p> - 目标：专门针对移动操作任务进行微调。</p>\n<p> - 数据来源：专注于与任务最相关的移动机械臂数据和人类口头语言指令。</p>\n<p> - 动作表示：改用流匹配来表示连续的动作分布，并引入动作专家，以实现更精细的实时控制。</p>\n<ul><li><strong>推理过程</strong>:</li></ul>\n<p> - 采用层次化推理，模型在每一步首先预测一个高级语义子任务（如“拿起盘子”）。</p>\n<p> - 然后，模型以该子任务为条件，通过动作专家预测下一步的低级动作块。</p>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> - 未使用单一命名数据集，而是采用异构数据混合。</p>\n<p> - 核心数据包括在多个真实家庭中收集的约400小时的移动机械臂操作数据。</p>\n<p> - 辅助数据包括：其他机器人数据、实验室跨具身任务数据、高级语义子任务预测数据、人类语言指令和网络数据（如图像说明、问答、对象定位）。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - 论文未明确说明硬件配置，但训练如此规模的视觉-语言-动作模型和数据集需要大量的计算资源（如大规模GPU集群）。</p>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - 主要在真实世界的、全新的家庭环境中进行评估，这些环境在训练期间模型从未见过。</p>\n<p> - 测试场景包括全新的厨房和卧室。</p>\n<p> - 评估任务是长时程、多阶段的复杂操作，例如“清理整个厨房”。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - 主要指标是任务成功率，通过一个详细的任务评估标准来判断。</p>\n<p> - 评估模型在执行长达10-15分钟任务中的表现。</p>\n<p> - 包含了与其他视觉-语言-动作（VLA）模型的定量比较，以评估其相对性能和泛化能力。</p>"
  },
  {
    "date": "2025-04-01",
    "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning",
    "link": "http://arxiv.org/abs/2504.00907",
    "summary_markdown": "### 论文研究单位\nGeorgia Institute of Technology 和 Meta FAIR\n### 论文概述\n本文提出 Ask-to-Act 任务，研究具身智能体在部分可观测的家居环境中，如何通过最少且相关的澄清问题解决模糊指令，完成单或多物体重排任务。作者提出 AutoAsk 方法，利用强化学习（RL）微调多模态大语言模型（MLLM）作为视觉-语言-行动（VLA）策略，通过大型语言模型（LLM）生成奖励信号，无需大规模人类演示或手动设计奖励。实验表明，该方法在未见场景和任务上显著优于零样本基线（如 GPT-4o）和监督微调模型。\n### 论文核心贡献点\n- 引入 Ask-to-Act 任务，评估智能体通过提问解决模糊指令的能力，涵盖属性识别、空间推理、物体大小等七类子任务。\n- 提出 AutoAsk 方法，首次实现基于 LLM 生成奖励的在线 RL 训练 VLA 策略，使智能体能同时行动和提问。\n- 验证 LLM 生成奖励的有效性，消除对人工奖励或人类数据的依赖，在未见场景和任务上成功率提升 10.4%-16.5%。\n- 展示训练策略在问题预算下的可调性，平衡成功率和提问效率。\n### 论文方法描述\nAutoAsk 基于 LLaVA-OneVision 0.5B 架构，适配为 VLA 策略：\n- **策略架构**：输入包括任务指令、历史观察、动作和用户回复。视觉观察通过 Perceiver 模型下采样至 4 个 token，以支持长历史序列处理。\n- **训练机制**：使用 DD-PPO（分布式 PPO）进行在线 RL，训练 50M 步。奖励函数由 LLM（Llama-3）生成，包含成功奖励、子目标奖励、有用问题奖励、超预算惩罚和步数惩罚。\n- **奖励生成**：LLM 根据特权环境状态、任务指令和智能体动作，评估提问是否有效解决模糊性，输出二进制奖励。\n- **实现细节**：异步处理奖励请求，使用 8 个 A40 GPU，vLLM 服务器加速推理。\n### 论文使用数据集和训练资源\n- **数据集**：在 Habitat 3.0 中构建，使用 ReplicaCAD 数据集的 83 个场景（63 个训练，20 个评估）和 Google Scanned Objects 的 42 个物体类别。任务包括单物体和多物体重排，覆盖无模糊性、属性识别、空间推理等七类。\n- **训练资源**：使用 8 个 NVIDIA A40 GPU，分布式训练 5000 万步。奖励生成模型为 Llama-3 8B，通过 vLLM 服务异步处理。\n### 论文使用的评估环境和评估指标\n- **评估环境**：部分可观测的 Habitat 3.0 模拟器，智能体以 Spot 机器人形态执行任务。评估分两维度：Unseen Scenes（新场景和物体布局）和 Unseen Tasks（新模糊任务组合）。\n- **评估指标**：\n - Success Rate (SR)：任务完成率。\n - Ambiguity-Resolution Efficiency Score (ARS)：衡量任务成功的同时最小化提问数量，计算公式为 $\\frac{\\mathds{1}_{\\text{success}}}{1+\\text{abs}(q_{\\text{relevant}}-K)+q_{\\text{irrelevant}}}$。\n - Question Ratio (QR)：实际提问数与最小需求问题数的比率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Georgia Institute of Technology 和 Meta FAIR</p>\n<h3>论文概述</h3>\n<p>本文提出 Ask-to-Act 任务，研究具身智能体在部分可观测的家居环境中，如何通过最少且相关的澄清问题解决模糊指令，完成单或多物体重排任务。作者提出 AutoAsk 方法，利用强化学习（RL）微调多模态大语言模型（MLLM）作为视觉-语言-行动（VLA）策略，通过大型语言模型（LLM）生成奖励信号，无需大规模人类演示或手动设计奖励。实验表明，该方法在未见场景和任务上显著优于零样本基线（如 GPT-4o）和监督微调模型。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>引入 Ask-to-Act 任务，评估智能体通过提问解决模糊指令的能力，涵盖属性识别、空间推理、物体大小等七类子任务。</li><li>提出 AutoAsk 方法，首次实现基于 LLM 生成奖励的在线 RL 训练 VLA 策略，使智能体能同时行动和提问。</li><li>验证 LLM 生成奖励的有效性，消除对人工奖励或人类数据的依赖，在未见场景和任务上成功率提升 10.4%-16.5%。</li><li>展示训练策略在问题预算下的可调性，平衡成功率和提问效率。</li></ul>\n<h3>论文方法描述</h3>\n<p>AutoAsk 基于 LLaVA-OneVision 0.5B 架构，适配为 VLA 策略：</p>\n<ul><li><strong>策略架构</strong>：输入包括任务指令、历史观察、动作和用户回复。视觉观察通过 Perceiver 模型下采样至 4 个 token，以支持长历史序列处理。</li><li><strong>训练机制</strong>：使用 DD-PPO（分布式 PPO）进行在线 RL，训练 50M 步。奖励函数由 LLM（Llama-3）生成，包含成功奖励、子目标奖励、有用问题奖励、超预算惩罚和步数惩罚。</li><li><strong>奖励生成</strong>：LLM 根据特权环境状态、任务指令和智能体动作，评估提问是否有效解决模糊性，输出二进制奖励。</li><li><strong>实现细节</strong>：异步处理奖励请求，使用 8 个 A40 GPU，vLLM 服务器加速推理。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：在 Habitat 3.0 中构建，使用 ReplicaCAD 数据集的 83 个场景（63 个训练，20 个评估）和 Google Scanned Objects 的 42 个物体类别。任务包括单物体和多物体重排，覆盖无模糊性、属性识别、空间推理等七类。</li><li><strong>训练资源</strong>：使用 8 个 NVIDIA A40 GPU，分布式训练 5000 万步。奖励生成模型为 Llama-3 8B，通过 vLLM 服务异步处理。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：部分可观测的 Habitat 3.0 模拟器，智能体以 Spot 机器人形态执行任务。评估分两维度：Unseen Scenes（新场景和物体布局）和 Unseen Tasks（新模糊任务组合）。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - Success Rate (SR)：任务完成率。</p>\n<p> - Ambiguity-Resolution Efficiency Score (ARS)：衡量任务成功的同时最小化提问数量，计算公式为 $\\frac{\\mathds{1}_{\\text{success}}}{1+\\text{abs}(q_{\\text{relevant}}-K)+q_{\\text{irrelevant}}}$。</p>\n<p> - Question Ratio (QR)：实际提问数与最小需求问题数的比率。</p>"
  },
  {
    "date": "2025-03-30",
    "title": "OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model",
    "link": "http://arxiv.org/abs/2503.23463",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-27",
    "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2503.22020",
    "summary_markdown": "### 论文研究单位\nNVIDIA、斯坦福大学和麻省理工学院。\n### 论文概述\n该论文提出了CoT-VLA，一种视觉-语言-动作模型，它通过引入显式的视觉思维链（CoT）推理来增强模型的性能。CoT-VLA首先生成一个代表未来子目标的图像，然后基于当前观测和生成的子目标图像来生成一个短动作序列以实现该子目标。该方法允许模型在执行动作前进行“视觉思考”，从而提升在复杂操作任务中的表现。论文通过在模拟基准和真实世界实验中进行广泛评估，证明了视觉思维链推理能提升VLA性能，并且在多个机器人平台和任务上实现了最先进的性能。\n### 论文核心贡献点\n1. 引入了一种通过子目标图像生成作为机器人控制中间推理步骤的视觉思维链推理方法。\n2. 提出了CoT-VLA系统，该系统集成了视觉思维链推理，并采用了一种混合注意力机制，将用于像素和文本生成的因果注意力与用于动作预测的全注意力相结合。\n3. 在模拟和真实世界环境中进行了全面评估，证明视觉思维链推理提高了VLA性能，其系统在多个机器人平台和任务上实现了最先进的性能。\n### 论文方法描述\n1. **视觉思维链推理框架**：模型操作分为两个连续阶段。首先，预测一个子目标图像作为中间视觉推理步骤；然后，生成一个动作序列以实现该子目标状态。\n2. **基础视觉-语言模型**：基于VILA-U构建，这是一个能够理解和生成图像和文本标记的统一多模态基础模型。\n3. **训练程序**：包括视觉标记预测（使用因果注意力）和动作标记预测（使用全注意力）。训练目标结合了动作损失和视觉损失。\n4. **混合注意力机制**：对图像或文本生成使用因果注意力，对动作生成使用全注意力。\n5. **动作分块**：预测动作序列而不是单个动作，每个动作被离散化为256个区间。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - 机器人演示数据：Open X-Embodiment数据集（OpenX）的子集，包括第三人称摄像头视图和单臂末端执行器控制（7自由度）。\n - 无动作视频数据：EPIC-KITCHENS和Something-Something V2数据集。\n2. **训练资源**：使用7B的VILA-U模型作为基础模型，所有图像处理在256x256分辨率下进行。\n3. **训练阶段**：\n - **预训练阶段**：在机器人演示数据和无动作视频数据上对模型进行预训练。\n - **适应阶段**：在下游任务部署的机器人设置上收集的任务演示对模型进行微调。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - **模拟基准测试**：LIBERO模拟基准测试。\n - **真实世界实验**：Bridge-V2和Franka-Tabletop真实机器人实验。\n2. **评估指标**：\n - 在LIBERO基准测试中，报告了在不同任务套件（空间、对象、目标、长程）上的平均成功率和标准误差。\n - 在真实世界任务中，通过成功率来评估性能。\n - 与基线方法（如Diffusion Policy、Octo、OpenVLA）进行了比较。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>NVIDIA、斯坦福大学和麻省理工学院。</p>\n<h3>论文概述</h3>\n<p>该论文提出了CoT-VLA，一种视觉-语言-动作模型，它通过引入显式的视觉思维链（CoT）推理来增强模型的性能。CoT-VLA首先生成一个代表未来子目标的图像，然后基于当前观测和生成的子目标图像来生成一个短动作序列以实现该子目标。该方法允许模型在执行动作前进行“视觉思考”，从而提升在复杂操作任务中的表现。论文通过在模拟基准和真实世界实验中进行广泛评估，证明了视觉思维链推理能提升VLA性能，并且在多个机器人平台和任务上实现了最先进的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>引入了一种通过子目标图像生成作为机器人控制中间推理步骤的视觉思维链推理方法。</li><li>提出了CoT-VLA系统，该系统集成了视觉思维链推理，并采用了一种混合注意力机制，将用于像素和文本生成的因果注意力与用于动作预测的全注意力相结合。</li><li>在模拟和真实世界环境中进行了全面评估，证明视觉思维链推理提高了VLA性能，其系统在多个机器人平台和任务上实现了最先进的性能。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>视觉思维链推理框架</strong>：模型操作分为两个连续阶段。首先，预测一个子目标图像作为中间视觉推理步骤；然后，生成一个动作序列以实现该子目标状态。</li><li><strong>基础视觉-语言模型</strong>：基于VILA-U构建，这是一个能够理解和生成图像和文本标记的统一多模态基础模型。</li><li><strong>训练程序</strong>：包括视觉标记预测（使用因果注意力）和动作标记预测（使用全注意力）。训练目标结合了动作损失和视觉损失。</li><li><strong>混合注意力机制</strong>：对图像或文本生成使用因果注意力，对动作生成使用全注意力。</li><li><strong>动作分块</strong>：预测动作序列而不是单个动作，每个动作被离散化为256个区间。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - 机器人演示数据：Open X-Embodiment数据集（OpenX）的子集，包括第三人称摄像头视图和单臂末端执行器控制（7自由度）。</p>\n<p> - 无动作视频数据：EPIC-KITCHENS和Something-Something V2数据集。</p>\n<ol><li><strong>训练资源</strong>：使用7B的VILA-U模型作为基础模型，所有图像处理在256x256分辨率下进行。</li><li><strong>训练阶段</strong>：</li></ol>\n<p> - <strong>预训练阶段</strong>：在机器人演示数据和无动作视频数据上对模型进行预训练。</p>\n<p> - <strong>适应阶段</strong>：在下游任务部署的机器人设置上收集的任务演示对模型进行微调。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - <strong>模拟基准测试</strong>：LIBERO模拟基准测试。</p>\n<p> - <strong>真实世界实验</strong>：Bridge-V2和Franka-Tabletop真实机器人实验。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - 在LIBERO基准测试中，报告了在不同任务套件（空间、对象、目标、长程）上的平均成功率和标准误差。</p>\n<p> - 在真实世界任务中，通过成功率来评估性能。</p>\n<p> - 与基线方法（如Diffusion Policy、Octo、OpenVLA）进行了比较。</p>"
  },
  {
    "date": "2025-03-26",
    "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation",
    "link": "http://arxiv.org/abs/2503.20384",
    "summary_markdown": "### 论文研究单位\n南京大学、香港理工大学、北京大学计算机学院多媒体信息处理国家重点实验室、北京智源人工智能研究院、香港科技大学。\n### 论文概述\n该论文提出了一种名为MoLe-VLA的新型视觉-语言-动作模型，旨在通过动态跳过大型语言模型中的部分层来提高机器人操作的效率。该方法受到神经科学中“浅层脑假说”和混合专家模型的启发，将每个LLM层视为一个专家，并设计了一个时空感知路由器来根据机器人的当前状态选择性地激活相关层，以模仿大脑在认知和因果推理中不同的信号通路。此外，为了弥补因层跳过而损失的LLM认知能力，论文还提出了一种认知自知识蒸馏方法来增强模型对任务需求的理解并生成相关的动作序列。\n### 论文核心贡献点\n1. 受浅层脑假说启发，开发了MoLe框架，模仿人脑信号流，通过路由器实现动态层激活以提高模型效率。\n2. 提出了一种新型层决策路由器STAR，充分利用机器人输入的时空信息做出更准确的激活决策。\n3. 引入了一种自知识蒸馏范式CogKD，以恢复稀疏LLM中因层跳过而丢失的认知信息，提升整体性能。\n### 论文方法描述\n论文方法主要包括三个核心部分：\n1. **混合层架构**：将LLM的每一层视为一个独立的专家，设计路由器动态选择执行哪些层。输入嵌入`x_k`经过路由器生成二进制门控向量`G_mol(x)`，仅top-k个值为1的层`π_k`被执行，其余层被跳过，输出`h_k`根据公式`h_k = G_k * π_k(h_{k-1}) + (1 - G_k) * h_{k-1}`计算。\n2. **时空感知路由器（STAR）**：为克服传统路由器无法捕捉动态具身智能任务中关键时空信息的限制，STAR独立处理来自视觉输入的空间特征和来自文本输入的时间依赖，将其组合成统一表示，以对齐LLM层选择与当前环境需求。\n3. **认知自知识蒸馏（CogKD）**：为了补偿层跳过导致的认知表达能力下降，使用原始全层模型作为教师，MoLe层跳过模型作为学生。引入可学习的“认知token”来整合视觉token和语言指导，通过分析认知token与学生token的相似性来识别关键信息，并自适应地重新加权蒸馏过程。\n### 论文使用数据集和训练资源\n数据集：RLBench仿真环境数据集和真实世界任务数据集。\n训练资源：论文未明确提及具体硬件，但训练过程在多个VLA模型上进行了端到端的实现，使用了扩散动作头，通过最小化预测噪声与真实噪声之间的均方误差进行优化。优化目标包括任务损失`L_task`和可能的负载平衡损失。\n### 论文使用的评估环境和评估指标\n评估环境：RLBench模拟环境和真实世界环境（使用Franka机械臂设置）。\n评估指标：主要指标是任务成功率，计算为在十个任务上的平均成功率。此外，还评估了计算效率，包括推理延迟（或频率）和计算成本（如FLOPs或延迟倍数）。在真实世界中，评估了模型在一系列操作任务上的定性表现。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南京大学、香港理工大学、北京大学计算机学院多媒体信息处理国家重点实验室、北京智源人工智能研究院、香港科技大学。</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种名为MoLe-VLA的新型视觉-语言-动作模型，旨在通过动态跳过大型语言模型中的部分层来提高机器人操作的效率。该方法受到神经科学中“浅层脑假说”和混合专家模型的启发，将每个LLM层视为一个专家，并设计了一个时空感知路由器来根据机器人的当前状态选择性地激活相关层，以模仿大脑在认知和因果推理中不同的信号通路。此外，为了弥补因层跳过而损失的LLM认知能力，论文还提出了一种认知自知识蒸馏方法来增强模型对任务需求的理解并生成相关的动作序列。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>受浅层脑假说启发，开发了MoLe框架，模仿人脑信号流，通过路由器实现动态层激活以提高模型效率。</li><li>提出了一种新型层决策路由器STAR，充分利用机器人输入的时空信息做出更准确的激活决策。</li><li>引入了一种自知识蒸馏范式CogKD，以恢复稀疏LLM中因层跳过而丢失的认知信息，提升整体性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文方法主要包括三个核心部分：</p>\n<ol><li><strong>混合层架构</strong>：将LLM的每一层视为一个独立的专家，设计路由器动态选择执行哪些层。输入嵌入<code>x_k</code>经过路由器生成二进制门控向量<code>G_mol(x)</code>，仅top-k个值为1的层<code>π_k</code>被执行，其余层被跳过，输出<code>h_k</code>根据公式<code>h_k = G_k * π_k(h_{k-1}) + (1 - G_k) * h_{k-1}</code>计算。</li><li><strong>时空感知路由器（STAR）</strong>：为克服传统路由器无法捕捉动态具身智能任务中关键时空信息的限制，STAR独立处理来自视觉输入的空间特征和来自文本输入的时间依赖，将其组合成统一表示，以对齐LLM层选择与当前环境需求。</li><li><strong>认知自知识蒸馏（CogKD）</strong>：为了补偿层跳过导致的认知表达能力下降，使用原始全层模型作为教师，MoLe层跳过模型作为学生。引入可学习的“认知token”来整合视觉token和语言指导，通过分析认知token与学生token的相似性来识别关键信息，并自适应地重新加权蒸馏过程。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：RLBench仿真环境数据集和真实世界任务数据集。</p>\n<p>训练资源：论文未明确提及具体硬件，但训练过程在多个VLA模型上进行了端到端的实现，使用了扩散动作头，通过最小化预测噪声与真实噪声之间的均方误差进行优化。优化目标包括任务损失<code>L_task</code>和可能的负载平衡损失。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：RLBench模拟环境和真实世界环境（使用Franka机械臂设置）。</p>\n<p>评估指标：主要指标是任务成功率，计算为在十个任务上的平均成功率。此外，还评估了计算效率，包括推理延迟（或频率）和计算成本（如FLOPs或延迟倍数）。在真实世界中，评估了模型在一系列操作任务上的定性表现。</p>"
  },
  {
    "date": "2025-03-25",
    "title": "Gemini Robotics: Bringing AI into the Physical World",
    "link": "http://arxiv.org/abs/2503.20020",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-25",
    "title": "Boosting Robotic Manipulation Generalization with Minimal Costly Data",
    "link": "http://arxiv.org/abs/2503.19516",
    "summary_markdown": "### 论文研究单位\n美团\n### 论文概述\n机器人操纵任务中，视觉-语言-行动模型的性能依赖于大量多样化的演示数据，但数据收集成本高昂，特别是涉及物理交互的数据。论文发现，操纵任务中的空间推理阶段数据易于低成本收集，且对模型泛化至关重要。为解决数据成本和覆盖度不足的问题，论文提出了RoboTron-Craft，一个成本效益高的物理真实轨迹生成管道，以及RoboTron-Platter方法，该方法通过解耦任务阶段，利用大量低成本的空间推理阶段数据来增强模型泛化，从而最大化利用昂贵但稀缺的物理交互阶段数据。\n### 论文核心贡献点\n1. 提出了RoboTron-Craft，一个分阶段且具有成本效益的轨迹生成管道，并揭示了模型性能与轨迹多样性之间的缩放定律。\n2. 提出了RoboTron-Platter方法，利用额外的低成本空间推理阶段轨迹来提高模型在零样本场景中的泛化性能。\n3. 证明了空间推理阶段数据可以作为催化剂，最大化昂贵的物理交互阶段数据在VLA模型训练中的贡献。\n4. 实验表明该方法在零样本场景中将任务成功率提高了41%，并能有效将模型技能迁移到新目标物体上。\n### 论文方法描述\nRoboTron-Platter方法首先将机器人的完整任务轨迹根据末端执行器与目标的距离划分为两个阶段：空间推理阶段和物理交互阶段。然后，构建一个混合数据集，该数据集包含一定数量的完整轨迹和大量额外收集的独立空间推理阶段轨迹。通过调整这两类数据的比例，对VLA模型进行模仿学习训练。这种分阶段的训练策略使模型能更有效地学习空间搜索和物理操作两种不同模式的技能，从而提升泛化能力。空间推理阶段的数据通过在真实抓取姿态上添加随机偏移来生成，避免了耗时的物理仿真，大幅降低了数据收集成本。\n### 论文使用数据集和训练资源\n1. **数据集**:\n * **自建数据集**：通过RoboTron-Craft管道生成，包含多种几何形状的日常物品在不同场景下的拾取-放置任务轨迹。\n * **CALVIN数据集**：用于验证方法通用性的公开基准数据集。\n2. **仿真环境**:\n * **平台**：NVIDIA Isaac Sim，配合Isaac Lab框架。\n * **场景**：包含多个工作台的房间，目标物体随机散落，场景纹理可随机化。\n * **智能体**：一个7-DoF Franka Emika Panda机械臂（固定基座）。\n * **观察**：使用手腕相机、头部相机和桌子相机的RGB图像，深度图仅在训练中用作监督。\n3. **训练资源**:\n * 论文未明确指定GPU型号和训练时间等计算资源细节。\n### 论文使用的评估环境和评估指标\n1. **评估环境**:\n * 在RoboTron-Craft基准提供的基于Isaac Sim的评估环境中进行。\n * 评估场景分为测试场景（模型见过的场景类型）和零样本场景（全新场景）。\n * 评估模型在分布外（OOD）的新目标物体上的泛化能力。\n2. **评估指标**:\n * **任务成功率**：主要指标。一次尝试成功抓取并放置目标物体记为1分，搬运过程中掉落记为0.5分，否则为0分。报告平均成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>美团</p>\n<h3>论文概述</h3>\n<p>机器人操纵任务中，视觉-语言-行动模型的性能依赖于大量多样化的演示数据，但数据收集成本高昂，特别是涉及物理交互的数据。论文发现，操纵任务中的空间推理阶段数据易于低成本收集，且对模型泛化至关重要。为解决数据成本和覆盖度不足的问题，论文提出了RoboTron-Craft，一个成本效益高的物理真实轨迹生成管道，以及RoboTron-Platter方法，该方法通过解耦任务阶段，利用大量低成本的空间推理阶段数据来增强模型泛化，从而最大化利用昂贵但稀缺的物理交互阶段数据。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了RoboTron-Craft，一个分阶段且具有成本效益的轨迹生成管道，并揭示了模型性能与轨迹多样性之间的缩放定律。</li><li>提出了RoboTron-Platter方法，利用额外的低成本空间推理阶段轨迹来提高模型在零样本场景中的泛化性能。</li><li>证明了空间推理阶段数据可以作为催化剂，最大化昂贵的物理交互阶段数据在VLA模型训练中的贡献。</li><li>实验表明该方法在零样本场景中将任务成功率提高了41%，并能有效将模型技能迁移到新目标物体上。</li></ol>\n<h3>论文方法描述</h3>\n<p>RoboTron-Platter方法首先将机器人的完整任务轨迹根据末端执行器与目标的距离划分为两个阶段：空间推理阶段和物理交互阶段。然后，构建一个混合数据集，该数据集包含一定数量的完整轨迹和大量额外收集的独立空间推理阶段轨迹。通过调整这两类数据的比例，对VLA模型进行模仿学习训练。这种分阶段的训练策略使模型能更有效地学习空间搜索和物理操作两种不同模式的技能，从而提升泛化能力。空间推理阶段的数据通过在真实抓取姿态上添加随机偏移来生成，避免了耗时的物理仿真，大幅降低了数据收集成本。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>:</li></ol>\n<p> * <strong>自建数据集</strong>：通过RoboTron-Craft管道生成，包含多种几何形状的日常物品在不同场景下的拾取-放置任务轨迹。</p>\n<p> * <strong>CALVIN数据集</strong>：用于验证方法通用性的公开基准数据集。</p>\n<ol><li><strong>仿真环境</strong>:</li></ol>\n<p> * <strong>平台</strong>：NVIDIA Isaac Sim，配合Isaac Lab框架。</p>\n<p> * <strong>场景</strong>：包含多个工作台的房间，目标物体随机散落，场景纹理可随机化。</p>\n<p> * <strong>智能体</strong>：一个7-DoF Franka Emika Panda机械臂（固定基座）。</p>\n<p> * <strong>观察</strong>：使用手腕相机、头部相机和桌子相机的RGB图像，深度图仅在训练中用作监督。</p>\n<ol><li><strong>训练资源</strong>:</li></ol>\n<p> * 论文未明确指定GPU型号和训练时间等计算资源细节。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>:</li></ol>\n<p> * 在RoboTron-Craft基准提供的基于Isaac Sim的评估环境中进行。</p>\n<p> * 评估场景分为测试场景（模型见过的场景类型）和零样本场景（全新场景）。</p>\n<p> * 评估模型在分布外（OOD）的新目标物体上的泛化能力。</p>\n<ol><li><strong>评估指标</strong>:</li></ol>\n<p> * <strong>任务成功率</strong>：主要指标。一次尝试成功抓取并放置目标物体记为1分，搬运过程中掉落记为0.5分，否则为0分。报告平均成功率。</p>"
  },
  {
    "date": "2025-03-20",
    "title": "IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes",
    "link": "http://arxiv.org/abs/2503.17406",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-20",
    "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse",
    "link": "http://arxiv.org/abs/2503.16365",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-18",
    "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
    "link": "http://arxiv.org/abs/2503.14734",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-15",
    "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis",
    "link": "http://arxiv.org/abs/2503.14526",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-17",
    "title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation",
    "link": "http://arxiv.org/abs/2503.13446",
    "summary_markdown": "### 论文研究单位\n北京邮电大学, 南洋理工大学, 清华大学\n### 论文概述\n论文提出了一种名为MoManipVLA的策略迁移框架，用于将预训练的视觉-语言-动作（VLA）模型从固定基座操作迁移到移动操作任务。该框架利用VLA模型生成具有高泛化能力的末端执行器路径点，并设计了一个双层轨迹优化框架来生成物理可行的移动基座和机械臂的联合轨迹，从而实现跨任务和环境的零样本调整，使移动操作策略能够执行多样化的家庭任务。\n### 论文核心贡献点\n提出了一种高效的策略适配框架MoManipVLA，用于将固定基座的VLA模型迁移到移动操作任务。\n设计了移动运动规划目标，包括末端执行器的可达性、轨迹平滑性和碰撞避免，以最大化轨迹的物理可行性。\n提出了一个双层目标优化框架，上层优化预测基座移动路径点以增强后续的操纵策略空间，下层优化选择最优的末端执行器轨迹以完成操纵任务。\n在OVMM基准测试和真实世界实验中验证了方法的有效性和高效性，相比最先进的移动操作技术，成功率高4.2%，并且由于预训练VLA模型的强大泛化能力，真实世界部署仅需50个专家样本。\n### 论文方法描述\nMoManipVLA首先使用预训练的VLA模型来生成高泛化的末端执行器路径点。\n然后，设计运动规划目标来评估基座和机械臂轨迹的物理可行性，包括可达性成本（基于逆运动学求解迭代次数）、平滑性成本（关节角和基座姿态的连续变化）和碰撞成本（基于ESDF的距离查询）。\n最后，采用一个双层轨迹优化框架来高效地求解轨迹：上层优化预测基座移动的路径点以扩展操纵策略空间，下层优化在给定路径点下选择最优的末端执行器轨迹以满足VLA模型生成的目标。该框架通过联合优化移动基座和机械臂的动作，使得VLA模型为固定基座生成的路径点在移动操作场景中变得可行。\n### 论文使用数据集和训练资源\n论文在OVMM基准测试和真实世界环境中进行了实验。\n真实世界部署仅需50个专家样本进行训练，这得益于预训练VLA模型的强大泛化能力。\n### 论文使用的评估环境和评估指标\n评估环境：OVMM基准测试和真实世界环境。\n评估指标：任务成功率。实验结果表明，MoManipVLA在OVMM上实现了比最先进方法高4.2%的成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京邮电大学, 南洋理工大学, 清华大学</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为MoManipVLA的策略迁移框架，用于将预训练的视觉-语言-动作（VLA）模型从固定基座操作迁移到移动操作任务。该框架利用VLA模型生成具有高泛化能力的末端执行器路径点，并设计了一个双层轨迹优化框架来生成物理可行的移动基座和机械臂的联合轨迹，从而实现跨任务和环境的零样本调整，使移动操作策略能够执行多样化的家庭任务。</p>\n<h3>论文核心贡献点</h3>\n<p>提出了一种高效的策略适配框架MoManipVLA，用于将固定基座的VLA模型迁移到移动操作任务。</p>\n<p>设计了移动运动规划目标，包括末端执行器的可达性、轨迹平滑性和碰撞避免，以最大化轨迹的物理可行性。</p>\n<p>提出了一个双层目标优化框架，上层优化预测基座移动路径点以增强后续的操纵策略空间，下层优化选择最优的末端执行器轨迹以完成操纵任务。</p>\n<p>在OVMM基准测试和真实世界实验中验证了方法的有效性和高效性，相比最先进的移动操作技术，成功率高4.2%，并且由于预训练VLA模型的强大泛化能力，真实世界部署仅需50个专家样本。</p>\n<h3>论文方法描述</h3>\n<p>MoManipVLA首先使用预训练的VLA模型来生成高泛化的末端执行器路径点。</p>\n<p>然后，设计运动规划目标来评估基座和机械臂轨迹的物理可行性，包括可达性成本（基于逆运动学求解迭代次数）、平滑性成本（关节角和基座姿态的连续变化）和碰撞成本（基于ESDF的距离查询）。</p>\n<p>最后，采用一个双层轨迹优化框架来高效地求解轨迹：上层优化预测基座移动的路径点以扩展操纵策略空间，下层优化在给定路径点下选择最优的末端执行器轨迹以满足VLA模型生成的目标。该框架通过联合优化移动基座和机械臂的动作，使得VLA模型为固定基座生成的路径点在移动操作场景中变得可行。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>论文在OVMM基准测试和真实世界环境中进行了实验。</p>\n<p>真实世界部署仅需50个专家样本进行训练，这得益于预训练VLA模型的强大泛化能力。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：OVMM基准测试和真实世界环境。</p>\n<p>评估指标：任务成功率。实验结果表明，MoManipVLA在OVMM上实现了比最先进方法高4.2%的成功率。</p>"
  },
  {
    "date": "2025-03-13",
    "title": "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2503.10631",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-12",
    "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
    "link": "http://arxiv.org/abs/2503.09527",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-11",
    "title": "MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2503.08007",
    "summary_markdown": "### 论文研究单位\nZhejiang University, China; MiLAB, Westlake University, China; AIM Lab, Faculty of IT, Monash University, Australia; HKUST(GZ), China\n### 论文概述\n本文提出了MoRE（Mixture of Robotic Experts），一种用于四足机器人的新型视觉-语言-动作（VLA）模型。该模型旨在通过强化学习（RL）微调大规模VLA模型，以适应大量混合质量数据。MoRE将多个低秩适应（LoRA）模块作为专家集成到密集的多模态大语言模型中，形成稀疏激活的专家混合模型，有效适应各种下游任务。模型采用基于RL的训练目标作为Q函数，利用自动收集的混合质量数据，提升数据效率和模型性能。\n### 论文核心贡献点\n- 提出MoRE，首个探索MoE架构在大规模端到端VLA模型中应用的工作，在多任务设置中展示高成功率和泛化能力。\n- 引入基于RL的训练目标，包含自动收集的次优轨迹，有效增强大规模VLA模型的数据效率和性能。\n- 在仿真和现实世界中进行广泛实验，研究MoRE在各种设置下的性能。\n### 论文方法描述\n- **模型架构**：基于Fuyu 8B解码器-Only Transformer，支持任意分辨率和图像数量的输入作为MLLM主干，生成动作令牌。\n- **混合LoRA专家**：将多个LoRA模块作为专家集成到解码器层的FFN中，每个专家包含共享FFN参数和LoRA适配器，通过路由器动态选择专家进行任务适应。\n- **训练目标**：采用自回归离散Q学习目标，将任务建模为MDP，模型作为Q函数训练，利用任务的结构特性（如视界无关回报和关键点有限性）从混合质量数据中学习。\n### 论文使用数据集和训练资源\n- **数据集**：使用QUARD数据集的6个具有挑战性的任务，分为“简单”、“中等”和“困难”三个难度级别，包含广泛和次优数据。\n- **训练资源**：基于Fuyu 8B模型，使用LoRA微调技术，未指定具体硬件。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在QUARD数据集的仿真环境中进行测试，并在现实世界四足机器人上部署验证。\n- **评估指标**：主要使用成功率（success rate），每个任务评估25个情节的平均性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Zhejiang University, China; MiLAB, Westlake University, China; AIM Lab, Faculty of IT, Monash University, Australia; HKUST(GZ), China</p>\n<h3>论文概述</h3>\n<p>本文提出了MoRE（Mixture of Robotic Experts），一种用于四足机器人的新型视觉-语言-动作（VLA）模型。该模型旨在通过强化学习（RL）微调大规模VLA模型，以适应大量混合质量数据。MoRE将多个低秩适应（LoRA）模块作为专家集成到密集的多模态大语言模型中，形成稀疏激活的专家混合模型，有效适应各种下游任务。模型采用基于RL的训练目标作为Q函数，利用自动收集的混合质量数据，提升数据效率和模型性能。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出MoRE，首个探索MoE架构在大规模端到端VLA模型中应用的工作，在多任务设置中展示高成功率和泛化能力。</li><li>引入基于RL的训练目标，包含自动收集的次优轨迹，有效增强大规模VLA模型的数据效率和性能。</li><li>在仿真和现实世界中进行广泛实验，研究MoRE在各种设置下的性能。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>模型架构</strong>：基于Fuyu 8B解码器-Only Transformer，支持任意分辨率和图像数量的输入作为MLLM主干，生成动作令牌。</li><li><strong>混合LoRA专家</strong>：将多个LoRA模块作为专家集成到解码器层的FFN中，每个专家包含共享FFN参数和LoRA适配器，通过路由器动态选择专家进行任务适应。</li><li><strong>训练目标</strong>：采用自回归离散Q学习目标，将任务建模为MDP，模型作为Q函数训练，利用任务的结构特性（如视界无关回报和关键点有限性）从混合质量数据中学习。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：使用QUARD数据集的6个具有挑战性的任务，分为“简单”、“中等”和“困难”三个难度级别，包含广泛和次优数据。</li><li><strong>训练资源</strong>：基于Fuyu 8B模型，使用LoRA微调技术，未指定具体硬件。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在QUARD数据集的仿真环境中进行测试，并在现实世界四足机器人上部署验证。</li><li><strong>评估指标</strong>：主要使用成功率（success rate），每个任务评估25个情节的平均性能。</li></ul>"
  },
  {
    "date": "2025-03-10",
    "title": "PointVLA: Injecting the 3D World into Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2503.07511",
    "summary_markdown": "### 论文研究单位\n美的集团、上海大学、华东师范大学\n### 论文概述\nPointVLA提出了一种无需重新训练即可将3D点云注入预训练视觉-语言-动作模型（VLA）的框架。该方法冻结原始动作专家模块，通过轻量级模块化块注入3D特征，并利用跳块分析确定对性能影响最小的网络层进行特征注入，显著提升了模型的空间推理能力。\n### 论文核心贡献点\n1. **高效3D注入机制**：通过冻结预训练VLA并选择性注入3D特征，避免大规模重新训练。\n2. **跳块分析技术**：识别动作专家中可跳过的非关键层（如第11-31层），最小化对预训练表示的干扰。\n3. **独特优势**：\n - 少样本多任务学习（每任务仅需20个样本）。\n - 区分真实物体与照片（利用3D信息避免\"物体幻觉\"）。\n - 高度适应性（适应训练中未见的桌面高度变化）。\n - 长时任务处理（如动态传送带物体抓取）。\n### 论文方法描述\n1. **架构设计**：\n - 冻结预训练的VLA主干（如DexVLA）和动作专家（如ScaleDP-1B）。\n - 点云通过分层卷积编码器提取特征，与动作嵌入对齐后注入选定层。\n2. **跳块分析**：\n - 通过跳过单层/多层实验，确定可替换的层（如第11-31层）。\n - 仅训练5个新增注入块，保持计算高效。\n3. **训练流程**：\n - 微调VLM以学习新语言指令，冻结大部分预训练权重。\n - 使用DexVLA的stage-1预训练权重初始化。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - **真实世界**：\n - 双手UR5e平台（3个Realsense相机：2个D435i腕部相机 + 1个L515顶部相机）。\n - 双手AgileX平台（与UR5e相似配置）。\n - 4个少样本任务（充电手机、擦拭盘子、放置面包、运输水果），每任务20个样本。\n - **仿真**：RoboTwin平台（14自由度移动双手机器人）。\n2. **训练资源**：\n - 计算资源未明确说明。\n - 使用DexVLA的stage-1预训练权重，沿用其stage-2超参数（chunk size=50）。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 真实机器人（UR5e和AgileX）及RoboTwin仿真平台。\n2. **评估指标**：\n - **少样本多任务**：任务平均成功率（%）。\n - **长时任务（如装配线打包）**：平均成功步长。\n - **真实-照片区分**：是否误抓取屏幕中的物体图像。\n - **高度适应性**：桌面高度变化下的抓取成功率。\n - **仿真基准**：各任务成功率（20/50样本下，测试100次均值±标准差）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>美的集团、上海大学、华东师范大学</p>\n<h3>论文概述</h3>\n<p>PointVLA提出了一种无需重新训练即可将3D点云注入预训练视觉-语言-动作模型（VLA）的框架。该方法冻结原始动作专家模块，通过轻量级模块化块注入3D特征，并利用跳块分析确定对性能影响最小的网络层进行特征注入，显著提升了模型的空间推理能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>高效3D注入机制</strong>：通过冻结预训练VLA并选择性注入3D特征，避免大规模重新训练。</li><li><strong>跳块分析技术</strong>：识别动作专家中可跳过的非关键层（如第11-31层），最小化对预训练表示的干扰。</li><li><strong>独特优势</strong>：</li></ol>\n<p> - 少样本多任务学习（每任务仅需20个样本）。</p>\n<p> - 区分真实物体与照片（利用3D信息避免\"物体幻觉\"）。</p>\n<p> - 高度适应性（适应训练中未见的桌面高度变化）。</p>\n<p> - 长时任务处理（如动态传送带物体抓取）。</p>\n<h3>论文方法描述</h3>\n<ol><li><strong>架构设计</strong>：</li></ol>\n<p> - 冻结预训练的VLA主干（如DexVLA）和动作专家（如ScaleDP-1B）。</p>\n<p> - 点云通过分层卷积编码器提取特征，与动作嵌入对齐后注入选定层。</p>\n<ol><li><strong>跳块分析</strong>：</li></ol>\n<p> - 通过跳过单层/多层实验，确定可替换的层（如第11-31层）。</p>\n<p> - 仅训练5个新增注入块，保持计算高效。</p>\n<ol><li><strong>训练流程</strong>：</li></ol>\n<p> - 微调VLM以学习新语言指令，冻结大部分预训练权重。</p>\n<p> - 使用DexVLA的stage-1预训练权重初始化。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - <strong>真实世界</strong>：</p>\n<p> - 双手UR5e平台（3个Realsense相机：2个D435i腕部相机 + 1个L515顶部相机）。</p>\n<p> - 双手AgileX平台（与UR5e相似配置）。</p>\n<p> - 4个少样本任务（充电手机、擦拭盘子、放置面包、运输水果），每任务20个样本。</p>\n<p> - <strong>仿真</strong>：RoboTwin平台（14自由度移动双手机器人）。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> - 计算资源未明确说明。</p>\n<p> - 使用DexVLA的stage-1预训练权重，沿用其stage-2超参数（chunk size=50）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - 真实机器人（UR5e和AgileX）及RoboTwin仿真平台。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>少样本多任务</strong>：任务平均成功率（%）。</p>\n<p> - <strong>长时任务（如装配线打包）</strong>：平均成功步长。</p>\n<p> - <strong>真实-照片区分</strong>：是否误抓取屏幕中的物体图像。</p>\n<p> - <strong>高度适应性</strong>：桌面高度变化下的抓取成功率。</p>\n<p> - <strong>仿真基准</strong>：各任务成功率（20/50样本下，测试100次均值±标准差）。</p>"
  },
  {
    "date": "2025-03-06",
    "title": "Refined Policy Distillation: From VLA Generalists to RL Experts",
    "link": "http://arxiv.org/abs/2503.05833",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-06",
    "title": "VLA Model-Expert Collaboration for Bi-directional Manipulation Learning",
    "link": "http://arxiv.org/abs/2503.04163",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-05",
    "title": "OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction",
    "link": "http://arxiv.org/abs/2503.03734",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-05",
    "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning",
    "link": "http://arxiv.org/abs/2503.03480",
    "summary_markdown": "### 论文研究单位\n\nInstitute for Artificial Intelligence, Peking University; PKU-PsiBot Joint Lab; State Key Laboratory of General Artificial Intelligence, Peking University.\n### 论文概述\n\n该论文探讨了如何将安全约束明确地整合到视觉-语言-动作模型（VLA）中。VLA作为通用机器人策略具有巨大潜力，但在实际部署中存在极端安全挑战，可能对环境、机器人自身和人类造成伤害。作者提出了一种集成安全方法（ISA），系统地建模安全需求，主动激发多样化的不安全行为，通过安全强化学习有效约束VLA策略，并通过有针对性的评估来严格确保其安全性。该方法利用约束马尔可夫决策过程（CMDP）范式，从最小-最大角度优化VLA以应对激发的安全风险。实验表明，通过ISA对齐的策略实现了有效的安全-性能权衡，与最先进方法相比，安全违规累积成本降低了83.58%，同时任务成功率提高了3.85%，并在长尾风险缓解和极端故障场景处理上表现出强大的安全保证，以及对分布外扰动的鲁棒泛化能力。\n### 论文核心贡献点\n\n1. 集成安全方法（ISA）的探索：系统地研究了VLA安全对齐的集成方法，包括安全建模、风险激发、策略约束和安全保证四个关键方面。\n2. 环境：为弥补VLA安全评估的空白，引入了Safety-CHORES测试平台。该基准内置了细粒度的安全约束，融合了导航和操作的多样化长视野任务，并通过大规模程序生成的场景和专门针对安全关键组件的设计，比传统基准更有效地暴露VLA的脆弱性。\n3. 实验验证与关键发现：通过广泛的实验证明，ISA对齐的策略实现了有效的安全-性能权衡，平均安全性比最先进方法提高83.58%，同时维持任务性能（+3.85%）；具有强大的安全保证，特别是在缓解长尾风险和处理极端故障场景方面；以及学习到的安全行为对各种分布外扰动的鲁棒泛化能力。\n### 论文方法描述\n\n1. 建模安全：定义场景、规范和任务的表示，使用状态-动作谓词和轨迹级谓词来形式化安全约束。\n2. 激发风险：利用大规模程序生成的室内场景和3D资产库，结合安全关键组件（如角落、盲点、易碎品集合、关键点、危险设备）来系统化地激发多样的不安全行为。\n3. 约束策略：将安全谓词转换为成本信号，采用基于拉格朗日的SafeRL技术，通过最小-最大优化过程在满足安全约束的前提下最大化任务奖励，动态调整策略参数和拉格朗日乘子以平衡安全和性能。\n4. 保证安全：通过测试时安全、长尾安全和极端故障安全三个维度进行综合评估，验证模型的安全性能和鲁棒性。\n### 论文使用数据集和训练资源\n\n数据集：Safety-CHORES，基于AI2THOR模拟器构建，包含150K多样化室内场景（由ProcTHOR生成）和800K 3D资产（来自Objaverse），设计了包含安全关键组件的导航和操作任务。\n训练资源：使用SPOC-DINOv2作为初始预训练VLA模型，在复杂任务（如Safety-Fetch）上训练2500万步，简单任务（如Safety-ObjNav和Safety-PickUp）上训练1500万步。\n### 论文使用的评估环境和评估指标\n\n评估环境：主要在Safety-CHORES基准上进行评估，该基准包含Safety-ObjNav、Safety-PickUp和Safety-Fetch三类任务。同时，在AI2THOR、iTHOR、ProcTHOR等现有基准上进行对比实验。评估还考虑了分布外扰动（颜色、光照、材质变化）和极端故障场景（如任务不可能完成时）。\n评估指标：任务成功率（SR），衡量任务完成的程度；累积成本（CC），量化整个轨迹中所有安全违规的总成本，计算为每一步违反各类安全约束的成本之和。",
    "summary_html": "<h3>论文研究单位</h3>\n\n<p>Institute for Artificial Intelligence, Peking University; PKU-PsiBot Joint Lab; State Key Laboratory of General Artificial Intelligence, Peking University.</p>\n<h3>论文概述</h3>\n\n<p>该论文探讨了如何将安全约束明确地整合到视觉-语言-动作模型（VLA）中。VLA作为通用机器人策略具有巨大潜力，但在实际部署中存在极端安全挑战，可能对环境、机器人自身和人类造成伤害。作者提出了一种集成安全方法（ISA），系统地建模安全需求，主动激发多样化的不安全行为，通过安全强化学习有效约束VLA策略，并通过有针对性的评估来严格确保其安全性。该方法利用约束马尔可夫决策过程（CMDP）范式，从最小-最大角度优化VLA以应对激发的安全风险。实验表明，通过ISA对齐的策略实现了有效的安全-性能权衡，与最先进方法相比，安全违规累积成本降低了83.58%，同时任务成功率提高了3.85%，并在长尾风险缓解和极端故障场景处理上表现出强大的安全保证，以及对分布外扰动的鲁棒泛化能力。</p>\n<h3>论文核心贡献点</h3>\n\n<ol><li>集成安全方法（ISA）的探索：系统地研究了VLA安全对齐的集成方法，包括安全建模、风险激发、策略约束和安全保证四个关键方面。</li><li>环境：为弥补VLA安全评估的空白，引入了Safety-CHORES测试平台。该基准内置了细粒度的安全约束，融合了导航和操作的多样化长视野任务，并通过大规模程序生成的场景和专门针对安全关键组件的设计，比传统基准更有效地暴露VLA的脆弱性。</li><li>实验验证与关键发现：通过广泛的实验证明，ISA对齐的策略实现了有效的安全-性能权衡，平均安全性比最先进方法提高83.58%，同时维持任务性能（+3.85%）；具有强大的安全保证，特别是在缓解长尾风险和处理极端故障场景方面；以及学习到的安全行为对各种分布外扰动的鲁棒泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n\n<ol><li>建模安全：定义场景、规范和任务的表示，使用状态-动作谓词和轨迹级谓词来形式化安全约束。</li><li>激发风险：利用大规模程序生成的室内场景和3D资产库，结合安全关键组件（如角落、盲点、易碎品集合、关键点、危险设备）来系统化地激发多样的不安全行为。</li><li>约束策略：将安全谓词转换为成本信号，采用基于拉格朗日的SafeRL技术，通过最小-最大优化过程在满足安全约束的前提下最大化任务奖励，动态调整策略参数和拉格朗日乘子以平衡安全和性能。</li><li>保证安全：通过测试时安全、长尾安全和极端故障安全三个维度进行综合评估，验证模型的安全性能和鲁棒性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n\n<p>数据集：Safety-CHORES，基于AI2THOR模拟器构建，包含150K多样化室内场景（由ProcTHOR生成）和800K 3D资产（来自Objaverse），设计了包含安全关键组件的导航和操作任务。</p>\n<p>训练资源：使用SPOC-DINOv2作为初始预训练VLA模型，在复杂任务（如Safety-Fetch）上训练2500万步，简单任务（如Safety-ObjNav和Safety-PickUp）上训练1500万步。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n\n<p>评估环境：主要在Safety-CHORES基准上进行评估，该基准包含Safety-ObjNav、Safety-PickUp和Safety-Fetch三类任务。同时，在AI2THOR、iTHOR、ProcTHOR等现有基准上进行对比实验。评估还考虑了分布外扰动（颜色、光照、材质变化）和极端故障场景（如任务不可能完成时）。</p>\n<p>评估指标：任务成功率（SR），衡量任务完成的程度；累积成本（CC），量化整个轨迹中所有安全违规的总成本，计算为每一步违反各类安全约束的成本之和。</p>"
  },
  {
    "date": "2025-03-04",
    "title": "RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour",
    "link": "http://arxiv.org/abs/2503.02572",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-04",
    "title": "Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding",
    "link": "http://arxiv.org/abs/2503.02310",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-03-03",
    "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
    "link": "http://arxiv.org/abs/2503.01378",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-27",
    "title": "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success",
    "link": "http://arxiv.org/abs/2502.19645",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-26",
    "title": "ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration",
    "link": "http://arxiv.org/abs/2502.19250",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-24",
    "title": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design",
    "link": "http://arxiv.org/abs/2502.17034",
    "summary_markdown": "### 论文研究单位\nSkolkovo Institute of Science and Technology, Intelligent Space Robotics Laboratory, Center for Digital Engineering.\n### 论文概述\n该论文提出了一个名为 Evolution 6.0 的概念，这是一个由生成式人工智能驱动的自主机器人系统。当机器人缺乏完成人类请求任务所需的工具时，它可以自主设计所需工具并学习如何使用它们以实现目标。该系统整合了视觉语言模型（VLMs）、视觉语言动作模型（VLA）和文本到3D生成模型，用于工具设计和任务执行，旨在提高机器人在不可预测环境中的适应性和操作灵活性。\n### 论文核心贡献点\n- 提出了 Evolution 6.0 框架，实现机器人通过生成式设计自主进化和创造工具。\n- 构建了一个包含工具生成模块和动作生成模块的系统，实现了从场景理解到工具制造和任务执行的闭环。\n- 集成了 QwenVLM、OpenVLA 和 Llama-Mesh 模型，分别用于环境理解、动作规划和3D工具生成。\n- 通过实验验证了系统的有效性，工具生成成功率达到90%，动作生成在物理和视觉泛化上达到83.5%。\n### 论文方法描述\n系统包含两个核心模块：\n- **工具生成模块**:\n - 通过相机等传感器捕获机器人场景输入。\n - 使用 Qwen2-VL-2B-Instruct 模型分析环境并生成工具设计的文本描述（如“创建一个3D刀具模型”）。\n - 将描述输入到自回归语言模型 Llama-Mesh 中，生成3D网格格式的工具模型。\n - 对网格进行渲染和验证，然后转换为G-Code，通过3D打印机制造出实体工具。\n- **动作生成模块**:\n - 基于一个经过微调的 OpenVLA 模型。\n - 接收第三方视角的相机画面和自然语言任务指令作为输入。\n - 输出一个7维的动作向量（3个平移位移，3个角位移和1个夹爪动作），直接控制机器人机械臂。\n - 系统以5Hz的频率迭代运行，实时处理环境变化并调整动作。\n### 论文使用数据集和训练资源\n- **数据集**:\n - 使用 UR10 机械臂和 Logitech C920e 相机收集。\n - 包含20个数据片段，平均分为两个任务：切蛋糕和抓取并放置蛋糕。\n - 数据格式化为RLDS（强化学习数据集）结构，包含机器人位姿、状态和同步的视觉反馈。\n- **训练资源**:\n - 对 OpenVLA-7b 模型进行微调，采用参数高效的 LoRA 方法，rank为32。\n - 训练设置包括：批量大小为16，学习率为 5e-4，训练4000个梯度步。\n - 在一块 NVIDIA A100 GPU 上进行训练。\n - 推理时，QwenVLM 和 Llama-Mesh 模型被优化为 int8 精度，在一块 NVIDIA RTX 4090 24Gb GPU 上运行。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - 评估分为两个阶段。\n - 第一阶段评估工具生成模块，在10个不同的机器人场景中进行。\n - 第二阶段评估动作生成模块，在一台 UR-10 机器人实物上进行，通过第三方视角观察。\n - 第二阶段包含五种评估场景：与训练数据相同的场景、物理泛化（目标物体尺寸或颜色变化）、运动泛化（物体位置变化）、语义泛化（新的指令）和视觉泛化（场景变化或引入干扰物）。\n- **评估指标**:\n - **工具生成**: 使用成功率和平均推理时间作为指标。\n - **动作生成**: 使用任务成功率作为主要指标，分别在五种场景下进行评估。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Skolkovo Institute of Science and Technology, Intelligent Space Robotics Laboratory, Center for Digital Engineering.</p>\n<h3>论文概述</h3>\n<p>该论文提出了一个名为 Evolution 6.0 的概念，这是一个由生成式人工智能驱动的自主机器人系统。当机器人缺乏完成人类请求任务所需的工具时，它可以自主设计所需工具并学习如何使用它们以实现目标。该系统整合了视觉语言模型（VLMs）、视觉语言动作模型（VLA）和文本到3D生成模型，用于工具设计和任务执行，旨在提高机器人在不可预测环境中的适应性和操作灵活性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了 Evolution 6.0 框架，实现机器人通过生成式设计自主进化和创造工具。</li><li>构建了一个包含工具生成模块和动作生成模块的系统，实现了从场景理解到工具制造和任务执行的闭环。</li><li>集成了 QwenVLM、OpenVLA 和 Llama-Mesh 模型，分别用于环境理解、动作规划和3D工具生成。</li><li>通过实验验证了系统的有效性，工具生成成功率达到90%，动作生成在物理和视觉泛化上达到83.5%。</li></ul>\n<h3>论文方法描述</h3>\n<p>系统包含两个核心模块：</p>\n<ul><li><strong>工具生成模块</strong>:</li></ul>\n<p> - 通过相机等传感器捕获机器人场景输入。</p>\n<p> - 使用 Qwen2-VL-2B-Instruct 模型分析环境并生成工具设计的文本描述（如“创建一个3D刀具模型”）。</p>\n<p> - 将描述输入到自回归语言模型 Llama-Mesh 中，生成3D网格格式的工具模型。</p>\n<p> - 对网格进行渲染和验证，然后转换为G-Code，通过3D打印机制造出实体工具。</p>\n<ul><li><strong>动作生成模块</strong>:</li></ul>\n<p> - 基于一个经过微调的 OpenVLA 模型。</p>\n<p> - 接收第三方视角的相机画面和自然语言任务指令作为输入。</p>\n<p> - 输出一个7维的动作向量（3个平移位移，3个角位移和1个夹爪动作），直接控制机器人机械臂。</p>\n<p> - 系统以5Hz的频率迭代运行，实时处理环境变化并调整动作。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> - 使用 UR10 机械臂和 Logitech C920e 相机收集。</p>\n<p> - 包含20个数据片段，平均分为两个任务：切蛋糕和抓取并放置蛋糕。</p>\n<p> - 数据格式化为RLDS（强化学习数据集）结构，包含机器人位姿、状态和同步的视觉反馈。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - 对 OpenVLA-7b 模型进行微调，采用参数高效的 LoRA 方法，rank为32。</p>\n<p> - 训练设置包括：批量大小为16，学习率为 5e-4，训练4000个梯度步。</p>\n<p> - 在一块 NVIDIA A100 GPU 上进行训练。</p>\n<p> - 推理时，QwenVLM 和 Llama-Mesh 模型被优化为 int8 精度，在一块 NVIDIA RTX 4090 24Gb GPU 上运行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - 评估分为两个阶段。</p>\n<p> - 第一阶段评估工具生成模块，在10个不同的机器人场景中进行。</p>\n<p> - 第二阶段评估动作生成模块，在一台 UR-10 机器人实物上进行，通过第三方视角观察。</p>\n<p> - 第二阶段包含五种评估场景：与训练数据相同的场景、物理泛化（目标物体尺寸或颜色变化）、运动泛化（物体位置变化）、语义泛化（新的指令）和视觉泛化（场景变化或引入干扰物）。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>工具生成</strong>: 使用成功率和平均推理时间作为指标。</p>\n<p> - <strong>动作生成</strong>: 使用任务成功率作为主要指标，分别在五种场景下进行评估。</p>"
  },
  {
    "date": "2025-02-20",
    "title": "Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration",
    "link": "http://arxiv.org/abs/2502.14795",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-20",
    "title": "ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2502.14420",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-19",
    "title": "VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation",
    "link": "http://arxiv.org/abs/2502.13508",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-14",
    "title": "Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation",
    "link": "http://arxiv.org/abs/2502.10040",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-13",
    "title": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation",
    "link": "http://arxiv.org/abs/2502.09268",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-07",
    "title": "Survey on Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2502.06851",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-09",
    "title": "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control",
    "link": "http://arxiv.org/abs/2502.05855",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-08",
    "title": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation",
    "link": "http://arxiv.org/abs/2502.05485",
    "summary_markdown": "# 论文总结\n## 论文研究单位\nNVIDIA, University of Washington, University of Southern California\n## 论文概述\n本文提出了一种名为HAMSTER（Hierarchical Action Models with Separated TEpRpresentations）的分层视觉-语言-动作（VLA）模型，用于开放世界机器人操作。该模型通过高层视觉语言模型（VLM）预测粗粒度的2D路径作为中间表示，然后由低层3D感知策略根据路径生成精确动作，以提升泛化能力和操作鲁棒性。实验表明，该方法在七个泛化维度上平均成功率比OpenVLA提升20%，相当于相对增益50%。\n## 论文核心贡献点\n1. 提出分层VLA架构HAMSTER，将VLM的2D路径预测与3D低层策略解耦，降低对昂贵机器人数据的依赖。\n2. 证明高层VLM可利用离域数据（如无动作视频、仿真数据）进行微调，实现跨域泛化。\n3. 低层策略结合3D感知与路径引导，专注于局部几何控制，显著提升视觉和语义泛化能力。\n## 论文方法描述\n1. **高层VLM设计**：使用VILA-1.5-13B作为基础模型，输入RGB图像和语言指令，输出机器人末端执行器的2D路径序列（包含位置和夹爪状态）。\n2. **离域数据微调**：构建多源数据集，包括770k像素点预测任务（RoboPoint）、320k仿真轨迹（RLBench）和110k真实机器人轨迹（Bridge/DROID），通过监督损失最大化路径预测的对数似然。\n3. **路径简化**：采用Ramer-Douglas-Peucker算法压缩长路径点，保留关键点。\n4. **低层策略设计**：采用RVT-2或3D-DA架构，将2D路径以彩色轨迹叠加在输入图像上，作为额外条件通道，策略学习基于路径生成动作。\n5. **分层执行**：VLM以低频率生成路径，低层策略以高频率执行动作，支持异步操作。\n## 论文使用数据集和训练资源\n- **离域数据**：RoboPoint（770k样本）、RLBench仿真（320k样本）、Bridge/DROID真实数据（110k样本）。\n- **同域数据**：真实机器人收集的小规模任务特定数据集，包含100-200条轨迹，用于训练低层策略。\n- **计算资源**：高层VLM在8×A100 GPU上训练，低层策略在4×A100 GPU上训练。\n## 论文使用的评估环境和评估指标\n- **评估环境**：真实世界（Franka Emika机器人，配备RGB-D相机）和仿真环境（RLBench）。\n- **任务**：七类泛化任务，包括新物体、新场景、新背景、新机器人构型、新指令、长视野操作和动态干扰。\n- **评估指标**：任务成功率（Success Rate）、路径预测准确率（Path Accuracy）和跨域泛化性能（Cross-domain Generalization）。",
    "summary_html": "<h1>论文总结</h1>\n<h2 class=\"section-title\">论文研究单位</h2>\n<p>NVIDIA, University of Washington, University of Southern California</p>\n<h2 class=\"section-title\">论文概述</h2>\n<p>本文提出了一种名为HAMSTER（Hierarchical Action Models with Separated TEpRpresentations）的分层视觉-语言-动作（VLA）模型，用于开放世界机器人操作。该模型通过高层视觉语言模型（VLM）预测粗粒度的2D路径作为中间表示，然后由低层3D感知策略根据路径生成精确动作，以提升泛化能力和操作鲁棒性。实验表明，该方法在七个泛化维度上平均成功率比OpenVLA提升20%，相当于相对增益50%。</p>\n<h2 class=\"section-title\">论文核心贡献点</h2>\n<ol><li>提出分层VLA架构HAMSTER，将VLM的2D路径预测与3D低层策略解耦，降低对昂贵机器人数据的依赖。</li><li>证明高层VLM可利用离域数据（如无动作视频、仿真数据）进行微调，实现跨域泛化。</li><li>低层策略结合3D感知与路径引导，专注于局部几何控制，显著提升视觉和语义泛化能力。</li></ol>\n<h2 class=\"section-title\">论文方法描述</h2>\n<ol><li><strong>高层VLM设计</strong>：使用VILA-1.5-13B作为基础模型，输入RGB图像和语言指令，输出机器人末端执行器的2D路径序列（包含位置和夹爪状态）。</li><li><strong>离域数据微调</strong>：构建多源数据集，包括770k像素点预测任务（RoboPoint）、320k仿真轨迹（RLBench）和110k真实机器人轨迹（Bridge/DROID），通过监督损失最大化路径预测的对数似然。</li><li><strong>路径简化</strong>：采用Ramer-Douglas-Peucker算法压缩长路径点，保留关键点。</li><li><strong>低层策略设计</strong>：采用RVT-2或3D-DA架构，将2D路径以彩色轨迹叠加在输入图像上，作为额外条件通道，策略学习基于路径生成动作。</li><li><strong>分层执行</strong>：VLM以低频率生成路径，低层策略以高频率执行动作，支持异步操作。</li></ol>\n<h2 class=\"section-title\">论文使用数据集和训练资源</h2>\n<ul><li><strong>离域数据</strong>：RoboPoint（770k样本）、RLBench仿真（320k样本）、Bridge/DROID真实数据（110k样本）。</li><li><strong>同域数据</strong>：真实机器人收集的小规模任务特定数据集，包含100-200条轨迹，用于训练低层策略。</li><li><strong>计算资源</strong>：高层VLM在8×A100 GPU上训练，低层策略在4×A100 GPU上训练。</li></ul>\n<h2 class=\"section-title\">论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：真实世界（Franka Emika机器人，配备RGB-D相机）和仿真环境（RLBench）。</li><li><strong>任务</strong>：七类泛化任务，包括新物体、新场景、新背景、新机器人构型、新指令、长视野操作和动态干扰。</li><li><strong>评估指标</strong>：任务成功率（Success Rate）、路径预测准确率（Path Accuracy）和跨域泛化性能（Cross-domain Generalization）。</li></ul>"
  },
  {
    "date": "2025-02-08",
    "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy",
    "link": "http://arxiv.org/abs/2502.05450",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-06",
    "title": "Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture",
    "link": "http://arxiv.org/abs/2502.04558",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-04",
    "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2502.02175",
    "summary_markdown": "### 论文研究单位\nUniversity of Sydney, Australia; Shanghai Jiao Tong University, China.\n### 论文概述\n本文针对视觉-语言-动作（VLA）模型在实时机器人控制中计算成本高昂的问题，提出了一种名为VLA-Cache的训练无关推理加速方法。该方法利用机器人操作中连续帧之间的时间冗余，通过自适应地缓存和复用视觉上未发生显著变化的静态token的键值（KV）表示，来减少计算开销。同时，为保持动作的精确性，VLA-Cache会过滤掉对任务至关重要但视觉上静态的token，确保对这些区域进行重新计算。此外，还引入了一种分层自适应token复用策略，根据解码器各层的注意力集中程度动态调整复用比例。实验表明，VLA-Cache在多个模拟环境和真实机器人上实现了最高1.7倍的推理速度提升和15%的控制频率增加，而对任务成功率的影响微乎其微。\n### 论文核心贡献点\n1. 提出了一种训练无关、即插即用的VLA推理加速框架VLA-Cache，通过跨帧token缓存减少计算冗余。\n2. 设计了一种基于视觉相似性的静态token选择机制，并结合解码器注意力分数过滤掉任务相关的语义重要token，以平衡效率与精度。\n3. 引入了一种分层自适应的token复用策略，根据模型不同层的注意力集中程度动态调整token复用率，进一步优化计算效率。\n4. 在多个主流VLA模型（OpenVLA, CogAct, OpenVLA-OFT）、两个模拟基准（LIBERO, SIMPLER）和一个真实机器人系统（Kinova Jaco2）上进行了广泛验证，证明了方法的通用性和有效性。\n### 论文方法描述\nVLA-Cache的核心方法分为三个步骤：\n1. 静态Token选择：将图像划分为多个不重叠的块，计算当前帧与前一帧对应块之间的余弦相似度。选择相似度超过预设阈值τ且最稳定的前k个块作为静态视觉token候选集。\n2. 保留任务相关信息：认识到部分视觉上静态但对任务至关重要的token（如机械臂末端、目标物体）不应被复用。为此，该方法利用VLA解码器中语言指令对视觉token的交叉注意力分数来评估每个视觉token的任务相关性。过滤掉注意力分数高的任务相关token，得到最终可复用的token集。\n3. 分层自适应Token复用：观察到Transformer模型不同层的注意力分布不同，深层注意力更为集中。该方法基于每层注意力熵的变化计算一个复用比例α，对注意力更集中的层允许更高比例的token复用，从而在保证关键信息更新的同时最大化计算收益。\n在推理过程中，对于被判定为可复用的token，直接使用其前一时刻缓存的KV值；对于动态变化或任务相关的token，则重新计算其KV值。\n### 论文使用数据集和训练资源\n- **数据集与评估基准**:\n - LIBERO：一个包含四种任务套件（Spatial, Object, Goal, Long）的机器人操作模拟基准。\n - SIMPLER：一个包含Visual Matching和Variant Aggregation两种设置的模拟环境，用于缩小模拟与现实的差距。\n - 真实世界任务：在Kinova Jaco2机械臂上执行了四个自定义任务（PickPot, PlaceCube, PutSausage, WipeTable）。\n- **训练资源**:\n - VLA-Cache本身是一个训练无关的推理优化方法。\n - 实验使用了预训练好的VLA模型：OpenVLA, OpenVLA-OFT, CogAct。\n - 真实机器人实验中，使用LoRA对OpenVLA进行了微调。\n - 所有实验均在NVIDIA RTX 4090 GPU上进行。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - 模拟环境：LIBERO和SIMPLER模拟器。\n - 真实世界环境：配备一个前置摄像头的Kinova Jaco2七自由度机械臂。\n- **评估指标**:\n - Success Rate (SR)：任务成功率，衡量任务完成的百分比。\n - Control Frequency (Hz)：控制频率，衡量模型每秒能生成多少次动作，反映系统的实时响应能力。\n - FLOPs (Teraflops)：浮点运算次数，用于衡量理论计算量。\n - CUDA Latency (ms)：CUDA延迟，指在GPU上完成一次推理所需的时间，反映实际推理速度。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>University of Sydney, Australia; Shanghai Jiao Tong University, China.</p>\n<h3>论文概述</h3>\n<p>本文针对视觉-语言-动作（VLA）模型在实时机器人控制中计算成本高昂的问题，提出了一种名为VLA-Cache的训练无关推理加速方法。该方法利用机器人操作中连续帧之间的时间冗余，通过自适应地缓存和复用视觉上未发生显著变化的静态token的键值（KV）表示，来减少计算开销。同时，为保持动作的精确性，VLA-Cache会过滤掉对任务至关重要但视觉上静态的token，确保对这些区域进行重新计算。此外，还引入了一种分层自适应token复用策略，根据解码器各层的注意力集中程度动态调整复用比例。实验表明，VLA-Cache在多个模拟环境和真实机器人上实现了最高1.7倍的推理速度提升和15%的控制频率增加，而对任务成功率的影响微乎其微。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种训练无关、即插即用的VLA推理加速框架VLA-Cache，通过跨帧token缓存减少计算冗余。</li><li>设计了一种基于视觉相似性的静态token选择机制，并结合解码器注意力分数过滤掉任务相关的语义重要token，以平衡效率与精度。</li><li>引入了一种分层自适应的token复用策略，根据模型不同层的注意力集中程度动态调整token复用率，进一步优化计算效率。</li><li>在多个主流VLA模型（OpenVLA, CogAct, OpenVLA-OFT）、两个模拟基准（LIBERO, SIMPLER）和一个真实机器人系统（Kinova Jaco2）上进行了广泛验证，证明了方法的通用性和有效性。</li></ol>\n<h3>论文方法描述</h3>\n<p>VLA-Cache的核心方法分为三个步骤：</p>\n<ol><li>静态Token选择：将图像划分为多个不重叠的块，计算当前帧与前一帧对应块之间的余弦相似度。选择相似度超过预设阈值τ且最稳定的前k个块作为静态视觉token候选集。</li><li>保留任务相关信息：认识到部分视觉上静态但对任务至关重要的token（如机械臂末端、目标物体）不应被复用。为此，该方法利用VLA解码器中语言指令对视觉token的交叉注意力分数来评估每个视觉token的任务相关性。过滤掉注意力分数高的任务相关token，得到最终可复用的token集。</li><li>分层自适应Token复用：观察到Transformer模型不同层的注意力分布不同，深层注意力更为集中。该方法基于每层注意力熵的变化计算一个复用比例α，对注意力更集中的层允许更高比例的token复用，从而在保证关键信息更新的同时最大化计算收益。</li></ol>\n<p>在推理过程中，对于被判定为可复用的token，直接使用其前一时刻缓存的KV值；对于动态变化或任务相关的token，则重新计算其KV值。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集与评估基准</strong>:</li></ul>\n<p> - LIBERO：一个包含四种任务套件（Spatial, Object, Goal, Long）的机器人操作模拟基准。</p>\n<p> - SIMPLER：一个包含Visual Matching和Variant Aggregation两种设置的模拟环境，用于缩小模拟与现实的差距。</p>\n<p> - 真实世界任务：在Kinova Jaco2机械臂上执行了四个自定义任务（PickPot, PlaceCube, PutSausage, WipeTable）。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - VLA-Cache本身是一个训练无关的推理优化方法。</p>\n<p> - 实验使用了预训练好的VLA模型：OpenVLA, OpenVLA-OFT, CogAct。</p>\n<p> - 真实机器人实验中，使用LoRA对OpenVLA进行了微调。</p>\n<p> - 所有实验均在NVIDIA RTX 4090 GPU上进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - 模拟环境：LIBERO和SIMPLER模拟器。</p>\n<p> - 真实世界环境：配备一个前置摄像头的Kinova Jaco2七自由度机械臂。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - Success Rate (SR)：任务成功率，衡量任务完成的百分比。</p>\n<p> - Control Frequency (Hz)：控制频率，衡量模型每秒能生成多少次动作，反映系统的实时响应能力。</p>\n<p> - FLOPs (Teraflops)：浮点运算次数，用于衡量理论计算量。</p>\n<p> - CUDA Latency (ms)：CUDA延迟，指在GPU上完成一次推理所需的时间，反映实际推理速度。</p>"
  },
  {
    "date": "2025-02-03",
    "title": "Scalable, Training-Free Visual Language Robotics: A Modular Multi-Model Framework for Consumer-Grade GPUs",
    "link": "http://arxiv.org/abs/2502.01071",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-01-31",
    "title": "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent",
    "link": "http://arxiv.org/abs/2501.18867",
    "summary_markdown": "### 论文研究单位\n论文作者为 Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen。论文中未明确提及具体的研究单位或机构。\n### 论文概述\n该论文提出了UP-VLA (Unified Understanding and Prediction Model for Embodied Agent)，一种用于具身智能体的统一理解与预测模型。作者指出，现有的视觉-语言-动作 (VLA) 模型通常依赖于预训练的视觉-语言模型 (VLM)，虽然VLMs提供了丰富的语义知识，但它们往往关注高层语义内容而忽略了低层特征，限制了模型捕获对机器人控制至关重要的详细视觉和空间信息的能力。为了解决这一问题，UP-VLA引入了一种新的训练范式，将多模态理解和未来预测目标相结合。该模型旨在同时增强高层语义理解和低层空间理解，实验证明其在模拟和真实世界的操作任务中均取得了显著的性能提升，特别是在Calvin ABC-D基准上相比之前的最先进方法实现了33%的改进。\n### 论文核心贡献点\n1. 受近期对VLMs局限性研究的启发，将包含丰富细节信息和动态上下文的视频数据集集成到VLA模型的预训练中，以增强其能力。\n2. 引入了一种新的VLA模型训练范式，结合了视觉-语言理解和未来预测目标，使模型能够捕获具身智能体所需的高级语义和低层视觉模式。\n3. 在模拟和真实世界的操作任务中实现了成功率的显著提升。此外，通过消融研究验证了两种预训练方法的有效性。\n### 论文方法描述\nUP-VLA方法的核心是构建一个统一的模型，该模型能够处理多模态理解、视觉预测和动作学习。\n1. **模型骨干**:\n - 使用Phi-1.5作为基础大语言模型。\n - 对于多模态理解任务，使用CLIP-ViT作为连续视觉编码器，将图像特征投影到语言嵌入空间。\n - 对于视觉预测任务，使用VQ-GAN作为离散图像编码器，直接预测未来的离散图像token。\n\n2. **连接视觉预测与多模态理解**:\n - 通过统一的提示和注意力机制，使模型能够处理多种任务。\n - **多模态理解**: 图像的连续token被放置在语言token之前，并通过修改因果注意力掩码，使图像token之间可以相互关注。\n - **未来视觉预测**: 当前观测的离散图像token被放置在语言指令之后，使模型在关注所有先前信息后预测未来图像token。\n\n3. **通过联合预测与理解增强动作学习**:\n - 模型在执行动作时，不仅输出动作序列，还输出对未来观测的预测。\n - 输入指令被扩展，加入了模型自身生成的场景描述 `L' = [E_1(O_t'), π_θ^{MMU}(O_t, L_prompt), L]`，使得动作决策能够同时利用高层次的语义理解和对当前场景的低层次描述。\n - 最终动作通过一个小型策略头生成，该策略头由一个单层注意力模块 (MAP) 和一个线性层 (MLP) 组成。\n\n4. **训练策略**:\n - **训练流程**: 分为两个阶段。第一阶段是预测与理解预训练，混合使用机器人数据（Bridge数据集，25k演示）用于未来预测和图文对数据（LLaVA-tuning-665k）用于增强理解能力。第二阶段是带动作的微调，在下游具身任务上对模型进行微调，同时继续混合图文对数据以保留多模态理解能力。\n - **训练目标**: 包含三个建模目标：用于多模态理解的语言建模、用于视觉预测的图像建模、以及用于具身任务的动作建模。\n### 论文使用数据集和训练资源\n- **数据集**:\n - **预训练**: 使用了Bridge数据集（包含25k个机械臂演示轨迹）进行未来预测任务的训练；使用了LLaVA-tuning-665k数据集（包含665k个图像-文本对）进行多模态理解能力的训练。\n - **微调与评估**: 在Calvin数据集上进行下游任务的微调和评估。附录中提到了一个用于真实世界评估的Manipulation Dataset。\n- **训练资源**:\n - 论文未明确提及所使用的具体硬件资源（如GPU类型和数量）。模型的骨干网络从Show-o模型进行初始化。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - **模拟环境**: 在Calvin模拟环境中进行评估，具体测试了ABC->D和ABCD->D泛化基准。\n - **真实世界环境**: 在真实的机器人平台上进行评估，区分了`real-seen`（已见任务）、`real-unseen`（未见任务，测试语义泛化能力）和`real-precise`（需要精确控制的任务）三种设置。\n- **评估指标**:\n - **成功率**: 主要评估指标是任务执行的成功率。在Calvin基准上，衡量的是长序列任务的成功率；在真实世界任务中，衡量的是单个或序列任务的成功完成情况。论文报告的33%性能提升是在Calvin ABC-D基准上的成功率提升。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>论文作者为 Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen。论文中未明确提及具体的研究单位或机构。</p>\n<h3>论文概述</h3>\n<p>该论文提出了UP-VLA (Unified Understanding and Prediction Model for Embodied Agent)，一种用于具身智能体的统一理解与预测模型。作者指出，现有的视觉-语言-动作 (VLA) 模型通常依赖于预训练的视觉-语言模型 (VLM)，虽然VLMs提供了丰富的语义知识，但它们往往关注高层语义内容而忽略了低层特征，限制了模型捕获对机器人控制至关重要的详细视觉和空间信息的能力。为了解决这一问题，UP-VLA引入了一种新的训练范式，将多模态理解和未来预测目标相结合。该模型旨在同时增强高层语义理解和低层空间理解，实验证明其在模拟和真实世界的操作任务中均取得了显著的性能提升，特别是在Calvin ABC-D基准上相比之前的最先进方法实现了33%的改进。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>受近期对VLMs局限性研究的启发，将包含丰富细节信息和动态上下文的视频数据集集成到VLA模型的预训练中，以增强其能力。</li><li>引入了一种新的VLA模型训练范式，结合了视觉-语言理解和未来预测目标，使模型能够捕获具身智能体所需的高级语义和低层视觉模式。</li><li>在模拟和真实世界的操作任务中实现了成功率的显著提升。此外，通过消融研究验证了两种预训练方法的有效性。</li></ol>\n<h3>论文方法描述</h3>\n<p>UP-VLA方法的核心是构建一个统一的模型，该模型能够处理多模态理解、视觉预测和动作学习。</p>\n<ol><li><strong>模型骨干</strong>:</li></ol>\n<p> - 使用Phi-1.5作为基础大语言模型。</p>\n<p> - 对于多模态理解任务，使用CLIP-ViT作为连续视觉编码器，将图像特征投影到语言嵌入空间。</p>\n<p> - 对于视觉预测任务，使用VQ-GAN作为离散图像编码器，直接预测未来的离散图像token。</p>\n\n<ol><li><strong>连接视觉预测与多模态理解</strong>:</li></ol>\n<p> - 通过统一的提示和注意力机制，使模型能够处理多种任务。</p>\n<p> - <strong>多模态理解</strong>: 图像的连续token被放置在语言token之前，并通过修改因果注意力掩码，使图像token之间可以相互关注。</p>\n<p> - <strong>未来视觉预测</strong>: 当前观测的离散图像token被放置在语言指令之后，使模型在关注所有先前信息后预测未来图像token。</p>\n\n<ol><li><strong>通过联合预测与理解增强动作学习</strong>:</li></ol>\n<p> - 模型在执行动作时，不仅输出动作序列，还输出对未来观测的预测。</p>\n<p> - 输入指令被扩展，加入了模型自身生成的场景描述 <code>L' = [E_1(O_t'), π_θ^{MMU}(O_t, L_prompt), L]</code>，使得动作决策能够同时利用高层次的语义理解和对当前场景的低层次描述。</p>\n<p> - 最终动作通过一个小型策略头生成，该策略头由一个单层注意力模块 (MAP) 和一个线性层 (MLP) 组成。</p>\n\n<ol><li><strong>训练策略</strong>:</li></ol>\n<p> - <strong>训练流程</strong>: 分为两个阶段。第一阶段是预测与理解预训练，混合使用机器人数据（Bridge数据集，25k演示）用于未来预测和图文对数据（LLaVA-tuning-665k）用于增强理解能力。第二阶段是带动作的微调，在下游具身任务上对模型进行微调，同时继续混合图文对数据以保留多模态理解能力。</p>\n<p> - <strong>训练目标</strong>: 包含三个建模目标：用于多模态理解的语言建模、用于视觉预测的图像建模、以及用于具身任务的动作建模。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> - <strong>预训练</strong>: 使用了Bridge数据集（包含25k个机械臂演示轨迹）进行未来预测任务的训练；使用了LLaVA-tuning-665k数据集（包含665k个图像-文本对）进行多模态理解能力的训练。</p>\n<p> - <strong>微调与评估</strong>: 在Calvin数据集上进行下游任务的微调和评估。附录中提到了一个用于真实世界评估的Manipulation Dataset。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - 论文未明确提及所使用的具体硬件资源（如GPU类型和数量）。模型的骨干网络从Show-o模型进行初始化。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - <strong>模拟环境</strong>: 在Calvin模拟环境中进行评估，具体测试了ABC->D和ABCD->D泛化基准。</p>\n<p> - <strong>真实世界环境</strong>: 在真实的机器人平台上进行评估，区分了<code>real-seen</code>（已见任务）、<code>real-unseen</code>（未见任务，测试语义泛化能力）和<code>real-precise</code>（需要精确控制的任务）三种设置。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>成功率</strong>: 主要评估指标是任务执行的成功率。在Calvin基准上，衡量的是长序列任务的成功率；在真实世界任务中，衡量的是单个或序列任务的成功完成情况。论文报告的33%性能提升是在Calvin ABC-D基准上的成功率提升。</p>"
  },
  {
    "date": "2025-01-28",
    "title": "Improving Vision-Language-Action Model with Online Reinforcement Learning",
    "link": "http://arxiv.org/abs/2501.16664",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-01-25",
    "title": "An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation",
    "link": "http://arxiv.org/abs/2501.15068",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-01-16",
    "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2501.09747",
    "summary_markdown": "### 论文研究单位\nPhysical Intelligence, UC Berkeley, Stanford\n### 论文概述\n论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。\n### 论文核心贡献点\n1. 提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。\n2. 发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。\n3. 验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。\n4. 实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。\n### 论文方法描述\n1. 动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。\n2. DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。\n3. 系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。\n4. 展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。\n5. 逆向过程：通过逆BPE和逆DCT重建连续动作。\n### 论文使用数据集和训练资源\n1. 评估任务：\n - Libero模拟基准（空间、目标、长时任务）\n - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）\n - DROID零样本桌面操作（15Hz）\n2. 训练数据：\n - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。\n - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。\n3. 计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。\n### 论文使用的评估环境和评估指标\n1. 评估环境：\n - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。\n - 模拟环境：Libero任务套件。\n - 零样本测试：三个大学校园的全新桌面场景。\n2. 评估指标：\n - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。\n - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。\n - 训练效率：收敛速度和计算时间。\n - 泛化能力：零样本环境下的策略鲁棒性。</think>\n### 论文研究单位\nPhysical Intelligence, UC Berkeley, Stanford\n### 论文概述\n论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。\n### 论文核心贡献点\n1. 提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。\n2. 发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。\n3. 验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。\n4. 实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。\n### 论文方法描述\n1. 动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。\n2. DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。\n3. 系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。\n4. 展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。\n5. 逆向过程：通过逆BPE和逆DCT重建连续动作。\n### 论文使用数据集和训练资源\n1. 评估任务：\n - Libero模拟基准（空间、目标、长时任务）\n - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）\n - DROID零样本桌面操作（15Hz）\n2. 训练数据：\n - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。\n - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。\n3. 计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。\n### 论文使用的评估环境和评估指标\n1. 评估环境：\n - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。\n - 模拟环境：Libero任务套件。\n - 零样本测试：三个大学校园的全新桌面场景。\n2. 评估指标：\n - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。\n - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。\n - 训练效率：收敛速度和计算时间。\n - 泛化能力：零样本环境下的策略鲁棒性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Physical Intelligence, UC Berkeley, Stanford</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。</li><li>发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。</li><li>验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。</li><li>实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。</li><li>DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。</li><li>系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。</li><li>展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。</li><li>逆向过程：通过逆BPE和逆DCT重建连续动作。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li>评估任务：</li></ol>\n<p> - Libero模拟基准（空间、目标、长时任务）</p>\n<p> - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）</p>\n<p> - DROID零样本桌面操作（15Hz）</p>\n<ol><li>训练数据：</li></ol>\n<p> - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。</p>\n<p> - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。</p>\n<ol><li>计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li>评估环境：</li></ol>\n<p> - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。</p>\n<p> - 模拟环境：Libero任务套件。</p>\n<p> - 零样本测试：三个大学校园的全新桌面场景。</p>\n<ol><li>评估指标：</li></ol>\n<p> - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。</p>\n<p> - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。</p>\n<p> - 训练效率：收敛速度和计算时间。</p>\n<p> - 泛化能力：零样本环境下的策略鲁棒性。</think></p>\n<h3>论文研究单位</h3>\n<p>Physical Intelligence, UC Berkeley, Stanford</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。</li><li>发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。</li><li>验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。</li><li>实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。</li><li>DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。</li><li>系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。</li><li>展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。</li><li>逆向过程：通过逆BPE和逆DCT重建连续动作。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li>评估任务：</li></ol>\n<p> - Libero模拟基准（空间、目标、长时任务）</p>\n<p> - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）</p>\n<p> - DROID零样本桌面操作（15Hz）</p>\n<ol><li>训练数据：</li></ol>\n<p> - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。</p>\n<p> - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。</p>\n<ol><li>计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li>评估环境：</li></ol>\n<p> - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。</p>\n<p> - 模拟环境：Libero任务套件。</p>\n<p> - 零样本测试：三个大学校园的全新桌面场景。</p>\n<ol><li>评估指标：</li></ol>\n<p> - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。</p>\n<p> - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。</p>\n<p> - 训练效率：收敛速度和计算时间。</p>\n<p> - 泛化能力：零样本环境下的策略鲁棒性。</p>"
  },
  {
    "date": "2025-01-12",
    "title": "Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing",
    "link": "http://arxiv.org/abs/2501.06919",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-01-09",
    "title": "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation",
    "link": "http://arxiv.org/abs/2501.05014",
    "summary_markdown": "### 论文研究单位\n莫斯科斯科尔科沃科学技术学院\n### 论文概述\n该论文提出了UAV-VLA（视觉-语言-动作）系统，这是一个旨在通过简单文本指令为空中机器人生成大规模飞行任务的工具。该系统集成了卫星图像处理、视觉语言模型（VLM）和大型语言模型（GPT），能够将用户的自然语言请求转换为具体的飞行路径和动作计划。系统利用卫星图像提供的丰富上下文信息来增强决策和任务规划。实验表明，该方法生成的飞行路径长度与人工结果相差22%，在K-最近邻（KNN）方法下，地图上目标定位的平均欧氏距离误差为34.22米。同时，UAV-VLA系统生成所有飞行计划仅需5分24秒，比有经验的操作员快6.5倍。\n### 论文核心贡献点\n- 提出了一个大规模的视觉-语言-动作（VLA）系统，能够根据文本任务请求和卫星图像生成路径-动作集。\n- 引入了UAV-VLPA-nano-30基准数据集，用于在全球范围内评估VLA系统的性能。\n- 在UAV-VLPA-nano-30基准上验证了该系统，其性能可与人类专家生成的路径和动作计划相媲美。\n### 论文方法描述\n该方法包含三个核心模块：\n1. **目标提取GPT模块**：解析用户的语言指令，提取出一组目标任务。\n2. **目标搜索VLM模块**：利用Molmo模型在卫星图像中识别出这些目标，并输出它们在图像中的像素坐标。\n3. **动作生成GPT模块**：将像素坐标通过元数据转换为全球地理坐标，并结合任务细节，使用MAVProxy工具生成具体的无人机动作序列。\n整个流程实现了从自然语言指令到无人机可执行任务的端到端转换，包括指令解析、目标检测和坐标转换。\n### 论文使用数据集和训练资源\n- **数据集**：使用新提出的UAV-VLPA-nano-30基准数据集。该数据集包含30张来自USGS EarthExplorer平台的高分辨率卫星图像，覆盖美国多样化的城市、郊区和自然环境。图像分辨率为1.5米/像素，拍摄于春夏季节的白天。\n- **训练资源**：实验未涉及模型从零开始的训练，而是使用了预训练模型。具体评估环境为一台配备RTX 4090 GPU (24GB VRAM)和Intel Core i9-13900K CPU的PC。由于内存限制，使用了量化后的Molmo-7B-D BnB 4-bit模型进行目标搜索。\n### 论文使用的评估环境和评估指标\n- **评估环境**：系统生成的飞行计划与人类专家在Mission Planner软件中手动创建的计划进行比较。测试任务为：“创建一个飞行计划，让四轴飞行器在100米高度绕飞每个建筑物，然后返回起始点并降落”。\n- **评估指标**：\n - **路径长度**：比较系统生成路径与人工生成路径的总长度。\n - **定位误差**：使用均方根误差（RMSE）衡量生成路径点与人工路径点之间的空间偏差。采用了三种方法计算误差：顺序方法、动态时间规整（DTW）和K-最近邻（KNN）。\n - **效率**：比较系统生成任务所需时间与人类操作员所需时间。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>莫斯科斯科尔科沃科学技术学院</p>\n<h3>论文概述</h3>\n<p>该论文提出了UAV-VLA（视觉-语言-动作）系统，这是一个旨在通过简单文本指令为空中机器人生成大规模飞行任务的工具。该系统集成了卫星图像处理、视觉语言模型（VLM）和大型语言模型（GPT），能够将用户的自然语言请求转换为具体的飞行路径和动作计划。系统利用卫星图像提供的丰富上下文信息来增强决策和任务规划。实验表明，该方法生成的飞行路径长度与人工结果相差22%，在K-最近邻（KNN）方法下，地图上目标定位的平均欧氏距离误差为34.22米。同时，UAV-VLA系统生成所有飞行计划仅需5分24秒，比有经验的操作员快6.5倍。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个大规模的视觉-语言-动作（VLA）系统，能够根据文本任务请求和卫星图像生成路径-动作集。</li><li>引入了UAV-VLPA-nano-30基准数据集，用于在全球范围内评估VLA系统的性能。</li><li>在UAV-VLPA-nano-30基准上验证了该系统，其性能可与人类专家生成的路径和动作计划相媲美。</li></ul>\n<h3>论文方法描述</h3>\n<p>该方法包含三个核心模块：</p>\n<ol><li><strong>目标提取GPT模块</strong>：解析用户的语言指令，提取出一组目标任务。</li><li><strong>目标搜索VLM模块</strong>：利用Molmo模型在卫星图像中识别出这些目标，并输出它们在图像中的像素坐标。</li><li><strong>动作生成GPT模块</strong>：将像素坐标通过元数据转换为全球地理坐标，并结合任务细节，使用MAVProxy工具生成具体的无人机动作序列。</li></ol>\n<p>整个流程实现了从自然语言指令到无人机可执行任务的端到端转换，包括指令解析、目标检测和坐标转换。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：使用新提出的UAV-VLPA-nano-30基准数据集。该数据集包含30张来自USGS EarthExplorer平台的高分辨率卫星图像，覆盖美国多样化的城市、郊区和自然环境。图像分辨率为1.5米/像素，拍摄于春夏季节的白天。</li><li><strong>训练资源</strong>：实验未涉及模型从零开始的训练，而是使用了预训练模型。具体评估环境为一台配备RTX 4090 GPU (24GB VRAM)和Intel Core i9-13900K CPU的PC。由于内存限制，使用了量化后的Molmo-7B-D BnB 4-bit模型进行目标搜索。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：系统生成的飞行计划与人类专家在Mission Planner软件中手动创建的计划进行比较。测试任务为：“创建一个飞行计划，让四轴飞行器在100米高度绕飞每个建筑物，然后返回起始点并降落”。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>路径长度</strong>：比较系统生成路径与人工生成路径的总长度。</p>\n<p> - <strong>定位误差</strong>：使用均方根误差（RMSE）衡量生成路径点与人工路径点之间的空间偏差。采用了三种方法计算误差：顺序方法、动态时间规整（DTW）和K-最近邻（KNN）。</p>\n<p> - <strong>效率</strong>：比较系统生成任务所需时间与人类操作员所需时间。</p>"
  },
  {
    "date": "2025-01-08",
    "title": "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding",
    "link": "http://arxiv.org/abs/2501.04693",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-01-07",
    "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
    "link": "http://arxiv.org/abs/2501.03841",
    "summary_markdown": "### 论文研究单位\nCFCS, School of CS, Peking University; PKU-AgiBot Lab; AgiBot\n### 论文概述\n该论文提出了一种名为OmniManip的通用机器人操作系统，旨在解决非结构化环境下的机器人操控难题。现有视觉语言模型（VLM）虽擅长高层推理，但缺乏精细操控所需的细粒度3D空间理解能力，而将VLM微调为视觉-语言-动作模型（VLA）又面临数据成本高和泛化性差的问题。为应对这些挑战，论文提出了一种新颖的、以物体为中心的表示方法，通过将交互原语（点和方向）定义在物体的标准空间中，作为空间约束来桥接VLM的高层推理与底层操控。系统设计为一个双闭环架构：一个用于高层规划，通过原语重采样、交互渲染和VLM检查实现闭环；另一个用于底层执行，通过6D姿态跟踪实现闭环，从而在无需微调VLM的情况下实现鲁棒、实时的控制。\n### 论文核心贡献点\n1. 提出了一种新颖的以物体为中心的交互表示，有效桥接了VLM的高层常识推理与底层机器人操控之间的鸿沟。\n2. 首次提出了一个规划和执行双闭环的开放词汇操控系统，且整个过程无需对VLM进行微调。\n3. 通过大量实验证明了该方法在多样化操控任务上具有强大的零样本泛化能力，并展示了其在自动化大规模仿真数据生成方面的潜力。\n### 论文方法描述\n该方法将复杂的机器人任务分解为多个阶段，每个阶段由带有空间约束的物体交互原语来定义。\n1. **任务分解与原语定义**：利用视觉基础模型（VFM）和VLM从指令中识别相关物体并分解任务。交互原语被定义为物体标准空间中的交互点和交互方向。空间约束则定义了主动物体与被动物体之间交互原语的距离和角度关系。\n2. **原语与约束提取**：首先使用单视图3D生成网络和通用6D物体姿态估计模型对物体进行网格重建和标准化。然后，通过VLM结合视觉提示定位交互点（包括可见和不可见的点），并通过LLM对沿主轴采样的交互方向进行任务相关性评分，生成一个有序的、带有约束的交互原语列表。\n3. **双闭环系统**：\n - **闭环规划**：引入了一个基于重采样、渲染和检查的自校正机制（RRC）。系统渲染当前交互配置，交由VLM验证。根据验证结果（成功、失败或需优化），系统决定接受、尝试下一个约束或进入细化阶段进行更精细的重采样，从而有效缓解VLM的幻觉问题。\n - **闭环执行**：当空间约束确定后，任务执行被表述为一个优化问题，目标是在满足空间约束、碰撞避免和路径平滑等损失函数的条件下，计算最优的末端执行器目标姿态。通过6D姿态跟踪器实时更新姿态，形成执行闭环。\n### 论文使用数据集和训练资源\n该方法不依赖于特定任务的训练数据集，采用零样本学习方式。它利用了多种预训练模型作为组件，包括：\n- 视觉语言模型（VLM）和大型语言模型（LLM）用于高层推理和验证。\n- Omni6DPose用于通用6D物体姿态估计。\n- 单视图3D生成网络用于从单张图像生成物体网格。\n实验中使用的机器人硬件包括Franka Emika和Kinova Gen3机械臂，以及各种日常物体。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在真实世界的实验平台上进行评估，使用了Franka Emika和Kinova Gen3两种机械臂，并包含了12种不同的日常操作任务，涵盖了刚体物体操控（如倒茶、插花）和铰接物体操控（如开关抽屉）。\n- **评估指标**：主要采用任务成功率作为核心量化指标。对于每个任务，记录在多次尝试中成功完成的次数，并计算总体的平均成功率。例如，在12项任务上，OmniManip的闭环版本总成功率为68.3%，显著优于其他基线方法。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>CFCS, School of CS, Peking University; PKU-AgiBot Lab; AgiBot</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种名为OmniManip的通用机器人操作系统，旨在解决非结构化环境下的机器人操控难题。现有视觉语言模型（VLM）虽擅长高层推理，但缺乏精细操控所需的细粒度3D空间理解能力，而将VLM微调为视觉-语言-动作模型（VLA）又面临数据成本高和泛化性差的问题。为应对这些挑战，论文提出了一种新颖的、以物体为中心的表示方法，通过将交互原语（点和方向）定义在物体的标准空间中，作为空间约束来桥接VLM的高层推理与底层操控。系统设计为一个双闭环架构：一个用于高层规划，通过原语重采样、交互渲染和VLM检查实现闭环；另一个用于底层执行，通过6D姿态跟踪实现闭环，从而在无需微调VLM的情况下实现鲁棒、实时的控制。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新颖的以物体为中心的交互表示，有效桥接了VLM的高层常识推理与底层机器人操控之间的鸿沟。</li><li>首次提出了一个规划和执行双闭环的开放词汇操控系统，且整个过程无需对VLM进行微调。</li><li>通过大量实验证明了该方法在多样化操控任务上具有强大的零样本泛化能力，并展示了其在自动化大规模仿真数据生成方面的潜力。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法将复杂的机器人任务分解为多个阶段，每个阶段由带有空间约束的物体交互原语来定义。</p>\n<ol><li><strong>任务分解与原语定义</strong>：利用视觉基础模型（VFM）和VLM从指令中识别相关物体并分解任务。交互原语被定义为物体标准空间中的交互点和交互方向。空间约束则定义了主动物体与被动物体之间交互原语的距离和角度关系。</li><li><strong>原语与约束提取</strong>：首先使用单视图3D生成网络和通用6D物体姿态估计模型对物体进行网格重建和标准化。然后，通过VLM结合视觉提示定位交互点（包括可见和不可见的点），并通过LLM对沿主轴采样的交互方向进行任务相关性评分，生成一个有序的、带有约束的交互原语列表。</li><li><strong>双闭环系统</strong>：</li></ol>\n<p> - <strong>闭环规划</strong>：引入了一个基于重采样、渲染和检查的自校正机制（RRC）。系统渲染当前交互配置，交由VLM验证。根据验证结果（成功、失败或需优化），系统决定接受、尝试下一个约束或进入细化阶段进行更精细的重采样，从而有效缓解VLM的幻觉问题。</p>\n<p> - <strong>闭环执行</strong>：当空间约束确定后，任务执行被表述为一个优化问题，目标是在满足空间约束、碰撞避免和路径平滑等损失函数的条件下，计算最优的末端执行器目标姿态。通过6D姿态跟踪器实时更新姿态，形成执行闭环。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>该方法不依赖于特定任务的训练数据集，采用零样本学习方式。它利用了多种预训练模型作为组件，包括：</p>\n<ul><li>视觉语言模型（VLM）和大型语言模型（LLM）用于高层推理和验证。</li><li>Omni6DPose用于通用6D物体姿态估计。</li><li>单视图3D生成网络用于从单张图像生成物体网格。</li></ul>\n<p>实验中使用的机器人硬件包括Franka Emika和Kinova Gen3机械臂，以及各种日常物体。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在真实世界的实验平台上进行评估，使用了Franka Emika和Kinova Gen3两种机械臂，并包含了12种不同的日常操作任务，涵盖了刚体物体操控（如倒茶、插花）和铰接物体操控（如开关抽屉）。</li><li><strong>评估指标</strong>：主要采用任务成功率作为核心量化指标。对于每个任务，记录在多次尝试中成功完成的次数，并计算总体的平均成功率。例如，在12项任务上，OmniManip的闭环版本总成功率为68.3%，显著优于其他基线方法。</li></ul>"
  },
  {
    "date": "2025-01-07",
    "title": "Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis",
    "link": "http://arxiv.org/abs/2501.03565",
    "summary_markdown": "### 论文研究单位\n中国科学技术大学生命科学与医学部、苏州高等研究院、斯坦福大学、科大讯飞医疗事业部、中国科学技术大学附属第一医院放射科。\n### 论文概述\n针对3D医学图像零样本诊断中现有视觉-语言对齐方法存在的模态间隙问题，提出Bridged Semantic Alignment (BrgSA)框架。该框架通过大型语言模型对医疗报告进行语义总结，并设计跨模态知识交互模块，利用跨模态知识库作为语义桥梁，缩小图像与文本特征之间的间隙，提升对齐效果。\n### 论文核心贡献点\n- 提出BrgSA框架，包含语义报告总结和跨模态知识交互模块，有效弥合视觉与文本特征间隙。\n- 构建扩展基准数据集CT-RATE-LT，涵盖15种低频异常，用于评估长尾疾病的零样本诊断能力。\n- 在CT-RATE、RAD-ChestCT等数据集上实现state-of-the-art性能，显著提升低频异常诊断和报告-体素检索任务效果。\n### 论文方法描述\n- **语义总结**：利用大型语言模型（如GPT-4 Turbo）提取医疗报告关键信息，生成固定模板总结（如\"There is [abnormality]\"），并采用双输入策略（原始报告+总结）平衡信息完整性与语义一致性。\n- **跨模态知识交互（CMKI）**：引入跨模态知识库（CMKB）作为共享语义桥梁，通过注意力权重重建图像和文本特征（公式7），结合重构损失（MSE）和对比损失（InfoNCE）优化特征对齐（公式8-11）。\n### 论文使用数据集和训练资源\n- **数据集**：CT-RATE（47,149训练样本）、CT-RATE-LT（扩展15种低频异常）、RAD-ChestCT（3,630样本）、INSPECT（3,214 CTPA样本）。\n- **训练资源**：单块NVIDIA A800 GPU，PyTorch框架，学习率5e-5，批量大小64，CMKB大小K=2048，损失权重α=0.5、β=1、γ=1。\n### 论文使用的评估环境和评估指标\n- **评估环境**：单块NVIDIA A800 GPU。\n- **评估指标**：\n - 多标签分类：AUC、Accuracy、F1、Precision、mAP、Recall@1、Precision@3。\n - 检索任务：体素-体素检索用MAP@Q（Q={5,10,50}），报告-体素检索用Recall@P（P={5,10,50,100}）。\n - 统计显著性：AUC用DeLong测试，其他指标用bootstrap或t-test（p<0.001）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中国科学技术大学生命科学与医学部、苏州高等研究院、斯坦福大学、科大讯飞医疗事业部、中国科学技术大学附属第一医院放射科。</p>\n<h3>论文概述</h3>\n<p>针对3D医学图像零样本诊断中现有视觉-语言对齐方法存在的模态间隙问题，提出Bridged Semantic Alignment (BrgSA)框架。该框架通过大型语言模型对医疗报告进行语义总结，并设计跨模态知识交互模块，利用跨模态知识库作为语义桥梁，缩小图像与文本特征之间的间隙，提升对齐效果。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出BrgSA框架，包含语义报告总结和跨模态知识交互模块，有效弥合视觉与文本特征间隙。</li><li>构建扩展基准数据集CT-RATE-LT，涵盖15种低频异常，用于评估长尾疾病的零样本诊断能力。</li><li>在CT-RATE、RAD-ChestCT等数据集上实现state-of-the-art性能，显著提升低频异常诊断和报告-体素检索任务效果。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>语义总结</strong>：利用大型语言模型（如GPT-4 Turbo）提取医疗报告关键信息，生成固定模板总结（如\"There is [abnormality]\"），并采用双输入策略（原始报告+总结）平衡信息完整性与语义一致性。</li><li><strong>跨模态知识交互（CMKI）</strong>：引入跨模态知识库（CMKB）作为共享语义桥梁，通过注意力权重重建图像和文本特征（公式7），结合重构损失（MSE）和对比损失（InfoNCE）优化特征对齐（公式8-11）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：CT-RATE（47,149训练样本）、CT-RATE-LT（扩展15种低频异常）、RAD-ChestCT（3,630样本）、INSPECT（3,214 CTPA样本）。</li><li><strong>训练资源</strong>：单块NVIDIA A800 GPU，PyTorch框架，学习率5e-5，批量大小64，CMKB大小K=2048，损失权重α=0.5、β=1、γ=1。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：单块NVIDIA A800 GPU。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 多标签分类：AUC、Accuracy、F1、Precision、mAP、Recall@1、Precision@3。</p>\n<p> - 检索任务：体素-体素检索用MAP@Q（Q={5,10,50}），报告-体素检索用Recall@P（P={5,10,50,100}）。</p>\n<p> - 统计显著性：AUC用DeLong测试，其他指标用bootstrap或t-test（p<0.001）。</p>"
  },
  {
    "date": "2025-01-06",
    "title": "Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches",
    "link": "http://arxiv.org/abs/2501.03151",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-12-29",
    "title": "CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance",
    "link": "http://arxiv.org/abs/2412.20451",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-12-24",
    "title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks",
    "link": "http://arxiv.org/abs/2412.18194",
    "summary_markdown": "1. **拆解用户需求**\n\n* **角色：** 我需要扮演一名论文阅读专家。\n* **输入：** 一篇 Arxiv 论文的 HTML 内容。它已被截断，但涵盖了核心部分：标题、作者、摘要、介绍、相关工作、方法（VLABench 部分）、实验和结论。\n* **任务：** 总结论文的要点。\n* **格式：** 严格的 Markdown 格式。无加粗。无额外文本。\n* **结构：** 输出必须包含这些确切的标题：\n * 论文研究单位\n * 论文概述\n * 论文核心贡献点\n * 论文方法描述\n * 论文使用数据集和训练资源\n * 论文使用的评估环境和评估指标\n\n2. **初步梳理与信息提取（心智草稿）**\n\n* **论文研究单位:**\n * 查找作者单位。作者列表显示“School of Computer Science, Fudan University”以及相应的电子邮件 `@fudan.edu.cn`。这很直接。答案：复旦大学计算机科学学院。\n\n* **论文概述:**\n * 阅读摘要和引言。\n * 摘要：“我们提出了 VLABench，一个开源基准，用于评估通用的语言条件操作（LCM）任务学习...VLABench 提供了 100 个精心设计的任务类别...与之前的基准相比，VLABench 有四个关键方面...”\n * 引言：“语言条件操作是具身智能的一个基本挑战...为了更好地定义适合基础模型的语言条件操作任务的类型，并为进一步推进机器人学研究提供一个标准化的评估套件，我们引入了 VLABench。”\n * 核心思想：当前的语言条件操作基准不足以评估像视觉-语言-动作（VLA）模型这样的大型基础模型。它们缺乏长期推理、常识转移、复杂的自然语言指令以及对模型能力进行多方面评估的需求。因此，作者创建了 VLABench 来填补这一空白。\n * 我的概述总结：VLABench 是一个新的大规模基准，旨在评估语言条件机器人操作任务，特别是对基础模型（如 VLAs）的评估。它设计了 100 个类别、超过 2000 个物体的任务，重点关注长期推理、常识转移和隐式指令等要求，从视觉、语言、知识和行动等多个维度对模型进行评估。\n\n* **论文核心贡献点:**\n * 查找类似“我们贡献如下”的部分。引言的末尾有这样一个列表。\n * 要点 1：“我们提出了 VLABench，第一个旨在全面评估 VLAs 和 VLMs 在机器人操作任务中的能力的基准...”\n * 要点 2：“我们在标准化评估框架内定义了 100 个新颖的 LCM 任务，这些任务专为基础模型的能力量身定制...”\n * 要点 3：“我们提供了一个可扩展的数据构建框架和一个标准化的评估数据集。这种自动化数据构建方法促进了未来关于机器人预训练数据的研究。”\n * 要点 4：“我们的实验表明，当前的预训练 VLAs 尚未表现出在 LLMs 中观察到的强泛化能力...”\n * 这些是四个明确陈述的贡献。我将直接列出它们。\n\n* **论文方法描述:**\n * 这一节更宽泛一些。它关乎“VLABench”是如何构建的。\n * 查看 3. VLABench 章节：\n * **任务描述（3.1）：** 任务被分为 60 个基元任务和 40 个复合任务。基元任务评估网格/纹理理解、空间理解、常识、语义理解和物理定律等特定维度。复合任务需要多步骤规划和长期推理（平均超过 500 个时间步）。\n * **基准（3.2）：** 评估针对三种方法进行：VLA（对已见/未见物体进行评估）、基础模型工作流（零样本）和 VLM（使用领域特定语言进行非交互式和交互式评估）。\n * **仿真（3.3）：** 基于 Mujoco 和 dm_control。高度模块化。资产库有 163 个类别、2164 个物品，来自 RoboCasa、Objaverse 和文本到 3D 的生成。默认机器人是 Franka Emika Panda（7 自由度）。\n * **数据集构建（3.4）：** 为确保多样性而进行域随机化（物体位置、比例、纹理、光照）。通过一个利用技能库、启发式信息和 RRT/SLERP 轨迹规划的可扩展自动化框架生成轨迹。使用 GPT-4 进行指令增强以创建自然的、隐式的指令。\n * 我需要把这些内容整合成一个连贯的段落，涵盖任务设计、仿真环境、数据收集和评估方法论。\n\n* **论文使用数据集和训练资源:**\n * 这与前一点有重叠，但我会特别关注“是什么”和“有多少”。\n * **数据集：** 由论文的自动化框架构建。包含 100 个任务类别的轨迹。\n * **资产：** 163 个类别、2164 个 3D 物体。\n * **训练资源：** 论文提到使用此数据集微调了像 OpenVLA、Octo 和 RDT-1B 这样的预训练 VLA。他们使用“每个任务类别 100 条轨迹，总计 1600 条轨迹”进行微调。所以，训练资源是他们自定义的数据集和强大的 GPU（尽管具体型号未提及，但这是隐含的）。我将陈述他们创建的数据集，并列出用于微调的模型。\n\n* **论文使用的评估环境和评估指标:**\n * **环境：** 基于 Mujoco 和 dm_control 的仿真环境。在仿真中使用 Franka Emika Panda 机械臂。支持多相机视图和点云。\n * **指标：** 这是关键的。论文定义了几个指标。\n * **Progress Score (PS)：** 首要指标。公式为 `PS = α * (n_correct / N) + (1-α) * (m_done / M)`。它平衡了正确决策与任务进度。默认的 α 为 0.2。\n * **对于 VLM：** 更详细的指标：\n * Skill Recall Rate\n * Parameter Recall Rate\n * Skill & Parameter Recall Rate\n * Precise Matching Rate (PM)\n * 评估设置各不相同：VLA（已见/未见物体），工作流（零样本），VLM（交互式/非交互式）。\n * 我将清晰地列出环境（Mujoco，Franka Panda）和指标（PS 为主要指标，外加 VLM 的特定指标）。\n\n3. **最终审查和规范格式**\n\n* 我将把提取的要点组合在所需的 Markdown 标题下。\n* 我将使用项目符号以求清晰，特别是在“贡献”和“方法”部分。\n* 我将确保语言简洁准确，并直接基于论文内容。\n* 我将再次检查，确保没有加粗，也没有像“这是总结：”这样的额外文本。输出应该*就是*这个总结。\n* 结构将是：\n ```markdown\n### 论文研究单位\n ...\n### 论文概述\n ...\n### 论文核心贡献点\n - ...\n - ...\n### 论文方法描述\n ...\n### 论文使用数据集和训练资源\n ...\n### 论文使用的评估环境和评估指标\n ...\n ```\n* 这个结构满足了用户的所有要求。内容直接从源文本中提取和总结。我现在将生成最终输出。</think>\n### 论文研究单位\n复旦大学计算机科学学院\n### 论文概述\nVLABench是一个大规模开源基准，旨在评估通用语言条件操作（LCM）任务的学习，特别是针对基于基础模型的方法。该基准提供了100个精心设计的任务类别，包含超过2000个物体，具有强随机性。VLABench的核心特点是强调需要世界知识与常识迁移的任务、包含隐含人类意图的自然语言指令、需要多步推理的长视野任务，以及对动作策略和语言模型能力的双重评估。它旨在从视觉、语言、知识迁移、规划和行动等多个维度，全面评估模型的综合技能学习与泛化能力。\n### 论文核心贡献点\n- 提出了VLABench，首个专为全面评估视觉-语言-动作模型（VLA）和视觉-语言模型（VLM）在机器人操作任务中能力而设计的大规模基准。\n- 在标准化评估框架内定义了100个新颖的LCM任务，这些任务专为评估基础模型在语义、视觉、空间、物理、知识迁移和推理等方面的能力而设计。\n- 提供了一个可扩展的数据构建框架和一个标准化的评估数据集，该自动化数据构建方法有助于未来关于机器人预训练数据的研究。\n- 通过实验证明，当前最先进的预训练VLA尚未展现出在大型语言模型中观察到的强泛化能力，现有最先进的VLM在具身场景中也存在局限性。\n### 论文方法描述\nVLABench的构建方法包含以下几个部分：\n- **任务设计**：任务分为60个基元任务和40个复合任务。基元任务用于评估网格与纹理理解、空间理解、常识与世界知识、语义理解和物理定律等特定能力维度。复合任务则结合多种技能，需要长期任务规划和多步逻辑推理，平均轨迹长度超过500个时间步。\n- **仿真环境**：基于Mujoco和dm_control构建，因其轻量级、高性能和物理真实感而入选。框架高度模块化，支持多种机器人形态，标准评估使用7自由度的Franka Emika Panda机械臂。\n- **资产库构建**：构建了包含163个类别、共2164个物体的资产库。资产部分继承自RoboCasa和Objaverse，部分通过在线3D模型网站和生成式AI模型（如Tripo.AI和Runway.AI）创建。\n- **数据集构建**：为实现数据多样性和规模化，实现了多种域随机化（如物体位置、纹理、光照）。开发了一个高效的自动化数据收集管道，该管道利用自定义的技能库和先验信息（如点云、抓取点）通过RRT和SLERP算法生成轨迹，并使用GPT-4进行指令增强，以生成自然且隐含的指令。\n### 论文使用数据集和训练资源\n- **数据集**：通过上述自动化框架构建，为每个任务类别提供高质量的轨迹数据。\n- **训练资源**：使用该数据集对OpenVLA、Octo和RDT-1B等预训练VLA模型进行微调。实验中，从每个任务类别采样100条轨迹，总计1600条轨迹进行微调，以确保任务间的平衡表示。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在基于Mujoco的仿真环境中进行评估。标准评估设置使用Franka Emika Panda机械臂，其末端执行器的位姿通过欧几里得空间中的3D坐标和四元数表示，并通过逆运动学解析为关节角。\n- **评估指标**：\n - **Progress Score (PS)**：主要评估指标，计算公式为 `PS = α * (n_correct / N) + (1-α) * (m_done / M)`，其中α默认为0.2。该指标综合了正确决策的比率（`n_correct / N`）和任务进度的比率（`m_done / M`），用于更细致的评估。\n - **VLM评估指标**：采用了更详细的评分方法，包括技能召回率、参数召回率、技能与参数召回率和精确匹配率。\n- **评估设置**：针对VLA模型，设置了已见物体和未见物体两种泛化评估场景；对于基础模型工作流，进行零样本转移能力评估；对于VLM，则进行交互式和非交互式两种方式的综合能力评估。",
    "summary_html": "<ol><li><strong>拆解用户需求</strong></li></ol>\n\n<ul><li><strong>角色：</strong> 我需要扮演一名论文阅读专家。</li><li><strong>输入：</strong> 一篇 Arxiv 论文的 HTML 内容。它已被截断，但涵盖了核心部分：标题、作者、摘要、介绍、相关工作、方法（VLABench 部分）、实验和结论。</li><li><strong>任务：</strong> 总结论文的要点。</li><li><strong>格式：</strong> 严格的 Markdown 格式。无加粗。无额外文本。</li><li><strong>结构：</strong> 输出必须包含这些确切的标题：</li></ul>\n<p> * 论文研究单位</p>\n<p> * 论文概述</p>\n<p> * 论文核心贡献点</p>\n<p> * 论文方法描述</p>\n<p> * 论文使用数据集和训练资源</p>\n<p> * 论文使用的评估环境和评估指标</p>\n\n<ol><li><strong>初步梳理与信息提取（心智草稿）</strong></li></ol>\n\n<ul><li><strong>论文研究单位:</strong></li></ul>\n<p> * 查找作者单位。作者列表显示“School of Computer Science, Fudan University”以及相应的电子邮件 <code>@fudan.edu.cn</code>。这很直接。答案：复旦大学计算机科学学院。</p>\n\n<ul><li><strong>论文概述:</strong></li></ul>\n<p> * 阅读摘要和引言。</p>\n<p> * 摘要：“我们提出了 VLABench，一个开源基准，用于评估通用的语言条件操作（LCM）任务学习...VLABench 提供了 100 个精心设计的任务类别...与之前的基准相比，VLABench 有四个关键方面...”</p>\n<p> * 引言：“语言条件操作是具身智能的一个基本挑战...为了更好地定义适合基础模型的语言条件操作任务的类型，并为进一步推进机器人学研究提供一个标准化的评估套件，我们引入了 VLABench。”</p>\n<p> * 核心思想：当前的语言条件操作基准不足以评估像视觉-语言-动作（VLA）模型这样的大型基础模型。它们缺乏长期推理、常识转移、复杂的自然语言指令以及对模型能力进行多方面评估的需求。因此，作者创建了 VLABench 来填补这一空白。</p>\n<p> * 我的概述总结：VLABench 是一个新的大规模基准，旨在评估语言条件机器人操作任务，特别是对基础模型（如 VLAs）的评估。它设计了 100 个类别、超过 2000 个物体的任务，重点关注长期推理、常识转移和隐式指令等要求，从视觉、语言、知识和行动等多个维度对模型进行评估。</p>\n\n<ul><li><strong>论文核心贡献点:</strong></li></ul>\n<p> * 查找类似“我们贡献如下”的部分。引言的末尾有这样一个列表。</p>\n<p> * 要点 1：“我们提出了 VLABench，第一个旨在全面评估 VLAs 和 VLMs 在机器人操作任务中的能力的基准...”</p>\n<p> * 要点 2：“我们在标准化评估框架内定义了 100 个新颖的 LCM 任务，这些任务专为基础模型的能力量身定制...”</p>\n<p> * 要点 3：“我们提供了一个可扩展的数据构建框架和一个标准化的评估数据集。这种自动化数据构建方法促进了未来关于机器人预训练数据的研究。”</p>\n<p> * 要点 4：“我们的实验表明，当前的预训练 VLAs 尚未表现出在 LLMs 中观察到的强泛化能力...”</p>\n<p> * 这些是四个明确陈述的贡献。我将直接列出它们。</p>\n\n<ul><li><strong>论文方法描述:</strong></li></ul>\n<p> * 这一节更宽泛一些。它关乎“VLABench”是如何构建的。</p>\n<p> * 查看 3. VLABench 章节：</p>\n<p> * <strong>任务描述（3.1）：</strong> 任务被分为 60 个基元任务和 40 个复合任务。基元任务评估网格/纹理理解、空间理解、常识、语义理解和物理定律等特定维度。复合任务需要多步骤规划和长期推理（平均超过 500 个时间步）。</p>\n<p> * <strong>基准（3.2）：</strong> 评估针对三种方法进行：VLA（对已见/未见物体进行评估）、基础模型工作流（零样本）和 VLM（使用领域特定语言进行非交互式和交互式评估）。</p>\n<p> * <strong>仿真（3.3）：</strong> 基于 Mujoco 和 dm_control。高度模块化。资产库有 163 个类别、2164 个物品，来自 RoboCasa、Objaverse 和文本到 3D 的生成。默认机器人是 Franka Emika Panda（7 自由度）。</p>\n<p> * <strong>数据集构建（3.4）：</strong> 为确保多样性而进行域随机化（物体位置、比例、纹理、光照）。通过一个利用技能库、启发式信息和 RRT/SLERP 轨迹规划的可扩展自动化框架生成轨迹。使用 GPT-4 进行指令增强以创建自然的、隐式的指令。</p>\n<p> * 我需要把这些内容整合成一个连贯的段落，涵盖任务设计、仿真环境、数据收集和评估方法论。</p>\n\n<ul><li><strong>论文使用数据集和训练资源:</strong></li></ul>\n<p> * 这与前一点有重叠，但我会特别关注“是什么”和“有多少”。</p>\n<p> * <strong>数据集：</strong> 由论文的自动化框架构建。包含 100 个任务类别的轨迹。</p>\n<p> * <strong>资产：</strong> 163 个类别、2164 个 3D 物体。</p>\n<p> * <strong>训练资源：</strong> 论文提到使用此数据集微调了像 OpenVLA、Octo 和 RDT-1B 这样的预训练 VLA。他们使用“每个任务类别 100 条轨迹，总计 1600 条轨迹”进行微调。所以，训练资源是他们自定义的数据集和强大的 GPU（尽管具体型号未提及，但这是隐含的）。我将陈述他们创建的数据集，并列出用于微调的模型。</p>\n\n<ul><li><strong>论文使用的评估环境和评估指标:</strong></li></ul>\n<p> * <strong>环境：</strong> 基于 Mujoco 和 dm_control 的仿真环境。在仿真中使用 Franka Emika Panda 机械臂。支持多相机视图和点云。</p>\n<p> * <strong>指标：</strong> 这是关键的。论文定义了几个指标。</p>\n<p> * <strong>Progress Score (PS)：</strong> 首要指标。公式为 <code>PS = α * (n_correct / N) + (1-α) * (m_done / M)</code>。它平衡了正确决策与任务进度。默认的 α 为 0.2。</p>\n<p> * <strong>对于 VLM：</strong> 更详细的指标：</p>\n<p> * Skill Recall Rate</p>\n<p> * Parameter Recall Rate</p>\n<p> * Skill & Parameter Recall Rate</p>\n<p> * Precise Matching Rate (PM)</p>\n<p> * 评估设置各不相同：VLA（已见/未见物体），工作流（零样本），VLM（交互式/非交互式）。</p>\n<p> * 我将清晰地列出环境（Mujoco，Franka Panda）和指标（PS 为主要指标，外加 VLM 的特定指标）。</p>\n\n<ol><li><strong>最终审查和规范格式</strong></li></ol>\n\n<ul><li>我将把提取的要点组合在所需的 Markdown 标题下。</li><li>我将使用项目符号以求清晰，特别是在“贡献”和“方法”部分。</li><li>我将确保语言简洁准确，并直接基于论文内容。</li><li>我将再次检查，确保没有加粗，也没有像“这是总结：”这样的额外文本。输出应该*就是*这个总结。</li><li>结构将是：</li></ul>\n<p> ```markdown</p>\n<h3>论文研究单位</h3>\n<p> ...</p>\n<h3>论文概述</h3>\n<p> ...</p>\n<h3>论文核心贡献点</h3>\n<p> - ...</p>\n<p> - ...</p>\n<h3>论文方法描述</h3>\n<p> ...</p>\n<h3>论文使用数据集和训练资源</h3>\n<p> ...</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p> ...</p>\n<p> ```</p>\n<ul><li>这个结构满足了用户的所有要求。内容直接从源文本中提取和总结。我现在将生成最终输出。</think></li></ul>\n<h3>论文研究单位</h3>\n<p>复旦大学计算机科学学院</p>\n<h3>论文概述</h3>\n<p>VLABench是一个大规模开源基准，旨在评估通用语言条件操作（LCM）任务的学习，特别是针对基于基础模型的方法。该基准提供了100个精心设计的任务类别，包含超过2000个物体，具有强随机性。VLABench的核心特点是强调需要世界知识与常识迁移的任务、包含隐含人类意图的自然语言指令、需要多步推理的长视野任务，以及对动作策略和语言模型能力的双重评估。它旨在从视觉、语言、知识迁移、规划和行动等多个维度，全面评估模型的综合技能学习与泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了VLABench，首个专为全面评估视觉-语言-动作模型（VLA）和视觉-语言模型（VLM）在机器人操作任务中能力而设计的大规模基准。</li><li>在标准化评估框架内定义了100个新颖的LCM任务，这些任务专为评估基础模型在语义、视觉、空间、物理、知识迁移和推理等方面的能力而设计。</li><li>提供了一个可扩展的数据构建框架和一个标准化的评估数据集，该自动化数据构建方法有助于未来关于机器人预训练数据的研究。</li><li>通过实验证明，当前最先进的预训练VLA尚未展现出在大型语言模型中观察到的强泛化能力，现有最先进的VLM在具身场景中也存在局限性。</li></ul>\n<h3>论文方法描述</h3>\n<p>VLABench的构建方法包含以下几个部分：</p>\n<ul><li><strong>任务设计</strong>：任务分为60个基元任务和40个复合任务。基元任务用于评估网格与纹理理解、空间理解、常识与世界知识、语义理解和物理定律等特定能力维度。复合任务则结合多种技能，需要长期任务规划和多步逻辑推理，平均轨迹长度超过500个时间步。</li><li><strong>仿真环境</strong>：基于Mujoco和dm_control构建，因其轻量级、高性能和物理真实感而入选。框架高度模块化，支持多种机器人形态，标准评估使用7自由度的Franka Emika Panda机械臂。</li><li><strong>资产库构建</strong>：构建了包含163个类别、共2164个物体的资产库。资产部分继承自RoboCasa和Objaverse，部分通过在线3D模型网站和生成式AI模型（如Tripo.AI和Runway.AI）创建。</li><li><strong>数据集构建</strong>：为实现数据多样性和规模化，实现了多种域随机化（如物体位置、纹理、光照）。开发了一个高效的自动化数据收集管道，该管道利用自定义的技能库和先验信息（如点云、抓取点）通过RRT和SLERP算法生成轨迹，并使用GPT-4进行指令增强，以生成自然且隐含的指令。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：通过上述自动化框架构建，为每个任务类别提供高质量的轨迹数据。</li><li><strong>训练资源</strong>：使用该数据集对OpenVLA、Octo和RDT-1B等预训练VLA模型进行微调。实验中，从每个任务类别采样100条轨迹，总计1600条轨迹进行微调，以确保任务间的平衡表示。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在基于Mujoco的仿真环境中进行评估。标准评估设置使用Franka Emika Panda机械臂，其末端执行器的位姿通过欧几里得空间中的3D坐标和四元数表示，并通过逆运动学解析为关节角。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>Progress Score (PS)</strong>：主要评估指标，计算公式为 <code>PS = α * (n_correct / N) + (1-α) * (m_done / M)</code>，其中α默认为0.2。该指标综合了正确决策的比率（<code>n_correct / N</code>）和任务进度的比率（<code>m_done / M</code>），用于更细致的评估。</p>\n<p> - <strong>VLM评估指标</strong>：采用了更详细的评分方法，包括技能召回率、参数召回率、技能与参数召回率和精确匹配率。</p>\n<ul><li><strong>评估设置</strong>：针对VLA模型，设置了已见物体和未见物体两种泛化评估场景；对于基础模型工作流，进行零样本转移能力评估；对于VLM，则进行交互式和非交互式两种方式的综合能力评估。</li></ul>"
  },
  {
    "date": "2024-12-20",
    "title": "QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning",
    "link": "http://arxiv.org/abs/2412.15576",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-12-18",
    "title": "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2412.14058",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-12-18",
    "title": "RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation",
    "link": "http://arxiv.org/abs/2412.13877",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-12-16",
    "title": "Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning",
    "link": "http://arxiv.org/abs/2412.11974",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-12-13",
    "title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies",
    "link": "http://arxiv.org/abs/2412.10345",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-12-09",
    "title": "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks",
    "link": "http://arxiv.org/abs/2412.06224",
    "summary_markdown": "### 论文研究单位\n北京大学计算机学院CFCS、Galbot、北京智源研究院\n### 论文概述\nUni-NaVid是一个基于视频的视觉-语言-动作（VLA）模型，旨在统一多种具身导航任务。模型仅需接收自然语言指令和RGB视频流作为输入，端到端输出底层机器人动作，支持连续环境中的高效导航。通过收集360万条多任务导航样本并进行训练，Uni-NaVid在多个导航基准测试中实现了最先进性能，并在真实世界实验中展示了有效性和泛化能力。\n### 论文核心贡献点\n1. 提出首个统一处理多种具身导航任务（包括VLN、ObjectNav、EQA和Human Following）的VLA模型，仅需RGB视频输入。\n2. 设计在线视觉token合并机制，按时间分组压缩视觉信息（当前、短期、长期token），推理速度提升至5 Hz，满足实时部署需求。\n3. 引入前瞻性动作规划，一次性生成未来4步动作序列，减少推理延迟。\n4. 收集并构建包含360万导航样本和230万互联网VQA/视频字幕样本的多任务数据集，促进跨任务学习协同。\n5. 在模拟基准和真实机器人平台上实现SOTA性能，验证模型的泛化能力。\n### 论文方法描述\n模型架构分为三部分：\n1. **观察编码**：使用EVA-CLIP视觉编码器将每帧视频转换为256个视觉token（维度C）。\n2. **在线视觉token合并**：\n - **分组**：按时间分为当前token（帧T）、短期token（帧[T-B, T)）、长期token（帧[1, T-B)），其中B=64。\n - **空间压缩**：使用GridPool操作，当前token压缩率α_curr=2（输出64个token），短期token α_short=8（输出4个token），长期token α_long=16（输出1个token）。\n - **在线更新**：新帧到达时，仅处理最新帧和最旧短期帧，重用历史token。\n - **长期token优化**：基于余弦相似度（阈值τ=0.95）合并相似长期token，防止线性增长，合并公式为加权平均。\n3. **动作规划**：视觉token经MLP投影后与语言token拼接，输入LLM生成4个动作（FORWARD、TURN-LEFT、TURN-RIGHT、STOP），动作空间兼容连续环境（30度旋转或25cm移动）。\n### 论文使用数据集和训练资源\n- **导航数据集**：收集360万条样本，覆盖四个任务：\n - VLN-CE（R2R/RxR连续环境版）\n - ObjectNav（HM3D数据集）\n - EQA（MP3D-EQA）\n - Human Following（自建Habitat 3.0基准）\n- **辅助数据集**：230万互联网视频数据（VQA和视频字幕），增强场景理解和sim-to-real泛化。\n- **训练资源**：使用多GPU训练（具体型号未明确），训练策略联合优化多任务损失，未公开详细计算资源。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：Habitat仿真器，包含VLN-CE、HM3D、MP3D-EQA等基准。\n - **真实世界**：配备RGB摄像头的机器人平台，部署非阻塞导航系统。\n- **评估指标**：\n - **VLN**：SPL（路径长度加权成功率）、nDTW（标准化轨迹扭曲）。\n - **ObjectNav**：成功率（Success Rate）、SPL。\n - **EQA**：准确率（Accuracy）。\n - **Human Following**：距离保持误差（Distance Maintenance Error）。\n - **整体**：任务特定指标和推理速度（目标5 Hz）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学计算机学院CFCS、Galbot、北京智源研究院</p>\n<h3>论文概述</h3>\n<p>Uni-NaVid是一个基于视频的视觉-语言-动作（VLA）模型，旨在统一多种具身导航任务。模型仅需接收自然语言指令和RGB视频流作为输入，端到端输出底层机器人动作，支持连续环境中的高效导航。通过收集360万条多任务导航样本并进行训练，Uni-NaVid在多个导航基准测试中实现了最先进性能，并在真实世界实验中展示了有效性和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出首个统一处理多种具身导航任务（包括VLN、ObjectNav、EQA和Human Following）的VLA模型，仅需RGB视频输入。</li><li>设计在线视觉token合并机制，按时间分组压缩视觉信息（当前、短期、长期token），推理速度提升至5 Hz，满足实时部署需求。</li><li>引入前瞻性动作规划，一次性生成未来4步动作序列，减少推理延迟。</li><li>收集并构建包含360万导航样本和230万互联网VQA/视频字幕样本的多任务数据集，促进跨任务学习协同。</li><li>在模拟基准和真实机器人平台上实现SOTA性能，验证模型的泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<p>模型架构分为三部分：</p>\n<ol><li><strong>观察编码</strong>：使用EVA-CLIP视觉编码器将每帧视频转换为256个视觉token（维度C）。</li><li><strong>在线视觉token合并</strong>：</li></ol>\n<p> - <strong>分组</strong>：按时间分为当前token（帧T）、短期token（帧[T-B, T)）、长期token（帧[1, T-B)），其中B=64。</p>\n<p> - <strong>空间压缩</strong>：使用GridPool操作，当前token压缩率α_curr=2（输出64个token），短期token α_short=8（输出4个token），长期token α_long=16（输出1个token）。</p>\n<p> - <strong>在线更新</strong>：新帧到达时，仅处理最新帧和最旧短期帧，重用历史token。</p>\n<p> - <strong>长期token优化</strong>：基于余弦相似度（阈值τ=0.95）合并相似长期token，防止线性增长，合并公式为加权平均。</p>\n<ol><li><strong>动作规划</strong>：视觉token经MLP投影后与语言token拼接，输入LLM生成4个动作（FORWARD、TURN-LEFT、TURN-RIGHT、STOP），动作空间兼容连续环境（30度旋转或25cm移动）。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>导航数据集</strong>：收集360万条样本，覆盖四个任务：</li></ul>\n<p> - VLN-CE（R2R/RxR连续环境版）</p>\n<p> - ObjectNav（HM3D数据集）</p>\n<p> - EQA（MP3D-EQA）</p>\n<p> - Human Following（自建Habitat 3.0基准）</p>\n<ul><li><strong>辅助数据集</strong>：230万互联网视频数据（VQA和视频字幕），增强场景理解和sim-to-real泛化。</li><li><strong>训练资源</strong>：使用多GPU训练（具体型号未明确），训练策略联合优化多任务损失，未公开详细计算资源。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：Habitat仿真器，包含VLN-CE、HM3D、MP3D-EQA等基准。</p>\n<p> - <strong>真实世界</strong>：配备RGB摄像头的机器人平台，部署非阻塞导航系统。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>VLN</strong>：SPL（路径长度加权成功率）、nDTW（标准化轨迹扭曲）。</p>\n<p> - <strong>ObjectNav</strong>：成功率（Success Rate）、SPL。</p>\n<p> - <strong>EQA</strong>：准确率（Accuracy）。</p>\n<p> - <strong>Human Following</strong>：距离保持误差（Distance Maintenance Error）。</p>\n<p> - <strong>整体</strong>：任务特定指标和推理速度（目标5 Hz）。</p>"
  },
  {
    "date": "2024-12-05",
    "title": "NaVILA: Legged Robot Vision-Language-Action Model for Navigation",
    "link": "http://arxiv.org/abs/2412.04453",
    "summary_markdown": "### 论文研究单位\nUC San Diego, USC, NVIDIA\n### 论文概述\nNaVILA是一个用于腿式机器人导航的视觉-语言-动作模型，旨在解决视觉-语言导航问题。该模型采用两级框架：高级视觉-语言-动作（VLA）模型生成包含空间信息的中级动作（如“向前移动75cm”），低级视觉运动策略则执行这些动作。这种方法使机器人能够通过人类语言指令导航复杂场景，并在模拟和真实环境中实现了高性能，显著提升了现有基准的导航成功率。\n### 论文核心贡献点\n- 提出了一个两级框架NaVILA，统一了VLA模型与运动技能，通过中级语言动作桥接高级推理和低级控制。\n- VLA模型输出自然语言形式的中级动作，而不是直接预测低级关节动作，增强了跨机器人平台的可移植性。\n- 利用YouTube人类旅行视频数据训练模型，首次实现直接从真实视频中学习连续环境导航，提升泛化能力。\n- 开发了新基准VLN-CE-Isaac，基于Isaac Sim，包含更现实的场景、低级控制和物理交互。\n- 在真实机器人部署中取得88%的整体成功率，其中复杂指令成功率达75%，展示了在挑战环境中的鲁棒性。\n### 论文方法描述\n- **高级VLA模型**：基于VILA视觉-语言模型，处理单视图图像和历史帧，生成导航动作。设计特定导航提示，区分当前观察和历史帧，使用文本描述内存信息。采用监督微调（SFT）混合数据，包括仿真数据、真实视频数据和一般VQA数据。\n- **从人类视频学习**：处理2000个YouTube旅行视频，通过熵采样生成20K轨迹，使用MASt3R估计相机姿态，提取步进动作，并利用VLM和LLM生成自然语言指令。\n- **低级运动策略**：使用PPO算法训练视觉运动策略，输入为LiDAR点云生成的身高图和本体感觉数据（如关节位置、速度），输出为期望关节位置。策略在Isaac Sim中通过Isaac Lab以单阶段方式训练，避免策略蒸馏，提高效率。身高图处理包括体素网格化和最大滤波，以增强环境感知。\n- **训练推理**：VLA模型在VILA第二阶段基础上微调一个epoch，所有模块未冻结；推理时使用正则表达式解析器提取动作类型和参数。运动策略以高频率运行，支持实时控制。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 仿真导航数据：R2R-CE和RxR-CE，使用Habitat模拟器生成动作序列。\n - 真实视频数据：2000个YouTube人类旅行视频，处理成20K轨迹，通过MASt3R估计姿态。\n - 辅助导航数据：EnvDrop增强指令、ScanQA 3D扫描QA对、导航轨迹摘要任务。\n - 一般VQA数据：来自多个通用视觉问答数据集，以保持模型泛化能力。\n- **训练资源**：\n - VLA训练：基于VILA模型，在混合数据上微调一个epoch，使用标准GPU资源（具体型号未提及）。\n - 运动策略训练：在Isaac Sim中使用Isaac Lab，采用PPO算法，在RTX 4090 GPU上达到60K FPS训练吞吐量。\n - 推理：VLA模型运行在低频率，运动策略实时运行，整体系统部署在机器人硬件上。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 经典VLN基准：R2R-CE和RxR-CE的Val-Unseen分割。\n - 新基准：VLN-CE-Isaac，基于Isaac Sim，包含详细机器人关节运动和物理交互。\n - 真实世界场景：在Unitree Go2、Unitree H1和Booster T1机器人上测试，包括实验室、住宅和户外不平地形。\n- **评估指标**：\n - 导航误差（NE，数值越低越好）。\n - 目标成功率（OS，数值越高越好）。\n - 成功率（SR，数值越高越好）。\n - 路径长度加权成功率（SPL，数值越高越好）。\n - nDTW（用于RxR-CE，数值越高越好）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>UC San Diego, USC, NVIDIA</p>\n<h3>论文概述</h3>\n<p>NaVILA是一个用于腿式机器人导航的视觉-语言-动作模型，旨在解决视觉-语言导航问题。该模型采用两级框架：高级视觉-语言-动作（VLA）模型生成包含空间信息的中级动作（如“向前移动75cm”），低级视觉运动策略则执行这些动作。这种方法使机器人能够通过人类语言指令导航复杂场景，并在模拟和真实环境中实现了高性能，显著提升了现有基准的导航成功率。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个两级框架NaVILA，统一了VLA模型与运动技能，通过中级语言动作桥接高级推理和低级控制。</li><li>VLA模型输出自然语言形式的中级动作，而不是直接预测低级关节动作，增强了跨机器人平台的可移植性。</li><li>利用YouTube人类旅行视频数据训练模型，首次实现直接从真实视频中学习连续环境导航，提升泛化能力。</li><li>开发了新基准VLN-CE-Isaac，基于Isaac Sim，包含更现实的场景、低级控制和物理交互。</li><li>在真实机器人部署中取得88%的整体成功率，其中复杂指令成功率达75%，展示了在挑战环境中的鲁棒性。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>高级VLA模型</strong>：基于VILA视觉-语言模型，处理单视图图像和历史帧，生成导航动作。设计特定导航提示，区分当前观察和历史帧，使用文本描述内存信息。采用监督微调（SFT）混合数据，包括仿真数据、真实视频数据和一般VQA数据。</li><li><strong>从人类视频学习</strong>：处理2000个YouTube旅行视频，通过熵采样生成20K轨迹，使用MASt3R估计相机姿态，提取步进动作，并利用VLM和LLM生成自然语言指令。</li><li><strong>低级运动策略</strong>：使用PPO算法训练视觉运动策略，输入为LiDAR点云生成的身高图和本体感觉数据（如关节位置、速度），输出为期望关节位置。策略在Isaac Sim中通过Isaac Lab以单阶段方式训练，避免策略蒸馏，提高效率。身高图处理包括体素网格化和最大滤波，以增强环境感知。</li><li><strong>训练推理</strong>：VLA模型在VILA第二阶段基础上微调一个epoch，所有模块未冻结；推理时使用正则表达式解析器提取动作类型和参数。运动策略以高频率运行，支持实时控制。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 仿真导航数据：R2R-CE和RxR-CE，使用Habitat模拟器生成动作序列。</p>\n<p> - 真实视频数据：2000个YouTube人类旅行视频，处理成20K轨迹，通过MASt3R估计姿态。</p>\n<p> - 辅助导航数据：EnvDrop增强指令、ScanQA 3D扫描QA对、导航轨迹摘要任务。</p>\n<p> - 一般VQA数据：来自多个通用视觉问答数据集，以保持模型泛化能力。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - VLA训练：基于VILA模型，在混合数据上微调一个epoch，使用标准GPU资源（具体型号未提及）。</p>\n<p> - 运动策略训练：在Isaac Sim中使用Isaac Lab，采用PPO算法，在RTX 4090 GPU上达到60K FPS训练吞吐量。</p>\n<p> - 推理：VLA模型运行在低频率，运动策略实时运行，整体系统部署在机器人硬件上。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 经典VLN基准：R2R-CE和RxR-CE的Val-Unseen分割。</p>\n<p> - 新基准：VLN-CE-Isaac，基于Isaac Sim，包含详细机器人关节运动和物理交互。</p>\n<p> - 真实世界场景：在Unitree Go2、Unitree H1和Booster T1机器人上测试，包括实验室、住宅和户外不平地形。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 导航误差（NE，数值越低越好）。</p>\n<p> - 目标成功率（OS，数值越高越好）。</p>\n<p> - 成功率（SR，数值越高越好）。</p>\n<p> - 路径长度加权成功率（SPL，数值越高越好）。</p>\n<p> - nDTW（用于RxR-CE，数值越高越好）。</p>"
  },
  {
    "date": "2024-12-02",
    "title": "Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control",
    "link": "http://arxiv.org/abs/2412.01034",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-11-29",
    "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
    "link": "http://arxiv.org/abs/2412.00174",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-11-29",
    "title": "RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World",
    "link": "http://arxiv.org/abs/2412.00171",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-11-29",
    "title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2411.19650",
    "summary_markdown": "### 论文研究单位\n清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。\n### 论文概述\n该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。\n### 论文核心贡献点\n- 提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。\n- 系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。\n- 引入了一种简单而有效的自适应动作集成算法，用于时间融合。\n- 在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。\n### 论文方法描述\n模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。\n### 论文使用数据集和训练资源\n- 数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。\n- 预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。\n- 微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。\n- 训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。\n### 论文使用的评估环境和评估指标\n- 评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。\n- 评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。</think>\n### 论文研究单位\n清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。\n### 论文概述\n该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。\n### 论文核心贡献点\n- 提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。\n- 系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。\n- 引入了一种简单而有效的自适应动作集成算法，用于时间融合。\n- 在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。\n### 论文方法描述\n模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。\n### 论文使用数据集和训练资源\n- 数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。\n- 预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。\n- 微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。\n- 训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。\n### 论文使用的评估环境和评估指标\n- 评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。\n- 评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。</p>\n<h3>论文概述</h3>\n<p>该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。</li><li>系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。</li><li>引入了一种简单而有效的自适应动作集成算法，用于时间融合。</li><li>在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<p>模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。</li><li>预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。</li><li>微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。</li><li>训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。</li><li>评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。</think></li></ul>\n<h3>论文研究单位</h3>\n<p>清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。</p>\n<h3>论文概述</h3>\n<p>该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。</li><li>系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。</li><li>引入了一种简单而有效的自适应动作集成算法，用于时间融合。</li><li>在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<p>模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。</li><li>预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。</li><li>微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。</li><li>训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。</li><li>评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。</li></ul>"
  },
  {
    "date": "2024-11-28",
    "title": "GRAPE: Generalizing Robot Policy via Preference Alignment",
    "link": "http://arxiv.org/abs/2411.19309",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-11-18",
    "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
    "link": "http://arxiv.org/abs/2411.13587",
    "summary_markdown": "### 论文研究单位\nRochester Institute of Technology, University of Missouri - Kansas City, U.S. Naval Research Laboratory, Lamar University, Meta AI, University of Rochester, Rutgers University\n### 论文概述\n信息未在提供的HTML原文中。\n### 论文核心贡献点\n信息未在提供的HTML原文中。\n### 论文方法描述\n信息未在提供的HTML原文中。\n### 论文使用数据集和训练资源\n信息未在提供的HTML原文中。\n### 论文使用的评估环境和评估指标\n信息未在提供的HTML原文中。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Rochester Institute of Technology, University of Missouri - Kansas City, U.S. Naval Research Laboratory, Lamar University, Meta AI, University of Rochester, Rutgers University</p>\n<h3>论文概述</h3>\n<p>信息未在提供的HTML原文中。</p>\n<h3>论文核心贡献点</h3>\n<p>信息未在提供的HTML原文中。</p>\n<h3>论文方法描述</h3>\n<p>信息未在提供的HTML原文中。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>信息未在提供的HTML原文中。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>信息未在提供的HTML原文中。</p>"
  },
  {
    "date": "2024-11-15",
    "title": "Visual-Linguistic Agent: Towards Collaborative Contextual Object Reasoning",
    "link": "http://arxiv.org/abs/2411.10252",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-11-04",
    "title": "Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks",
    "link": "http://arxiv.org/abs/2411.05821",
    "summary_markdown": "### 论文研究单位\nMetarch.ai\n### 论文概述\n本文提出了一个全面的评估框架和基准测试套件，用于评估视觉-语言-动作（VLA）模型在机器人学习任务中的性能。研究对GPT-4o、OpenVLA和JAT三种最先进的VLM和VLA模型进行了详细分析，覆盖了来自Open-X-Embodiment集合的20个多样化数据集。研究揭示了当前VLA模型在不同任务和机器人平台上的性能变化显著，其中GPT-4o通过复杂的提示工程展现出最一致的性能，而所有模型在需要多步规划的复杂操作任务中均表现不佳，且模型性能对动作空间特性和环境因素高度敏感。\n### 论文核心贡献点\n1. **首个大规模VLA基准测试**：推出了MultiNet v0.1，这是首个针对大规模通用动作模型的基准测试。\n2. **系统性评估框架**：建立了一个全面的评估框架，包括数据集管理、模型配置和性能指标。\n3. **模型性能分析**：提供了对三种代表性VLA模型的详细性能分析，揭示了它们的优势和局限。\n4. **开源工具**：发布了开源软件基础设施，用于下载、管理和利用基准数据。\n5. **跨模态映射框架**：提出了一个通用框架，用于将VLM映射到其他模态类别，特别是动作空间。\n### 论文方法描述\n1. **数据集处理**：从Open-X-Embodiment数据集中筛选并处理了53个数据集，最终对20个数据集进行评估。数据集涵盖多种机器人平台和任务类型，使用RLDS格式存储。\n2. **模型配置**：\n - **JAT**：采用零样本设置，对图像进行4通道处理（RGB复制红色通道作为Alpha），观察值和动作拼接为单一张量。\n - **GPT-4o**：构建综合提示，包括浮点观察状态、主图像观察、自然语言指令和动作空间描述，对不兼容输出进行错误处理。\n - **OpenVLA**：标准化夹爪命令（二进制/三元离散化或连续归一化），处理特殊数据集（如UCSD和ETH），并排除终端张量。\n3. **评估指标**：主要使用平均均方误差（AMSE）、归一化AMSE（NAMSE）和完成率，通过比较预测动作与真实动作来评估模型性能。\n4. **推理基础设施**：JAT和GPT使用GCP e2-standard-8实例，OpenVLA使用配备NVIDIA L4 GPU的GCP g2-standard-8实例。\n### 论文使用数据集和训练资源\n1. **数据集**：Open-X-Embodiment数据集，包含来自21个机构的超过100万条真实机器人轨迹，涵盖22种不同的机器人形态。评估版本v0.1使用了53个数据集，完整训练数据约32TB。\n2. **训练资源**：未明确提及模型训练过程，但详细描述了推理阶段的计算资源，包括GCP实例类型和GPU配置（如NVIDIA L4）。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：基于Google Cloud Platform（GCP）的虚拟机实例，具体包括：\n - JAT和GPT：e2-standard-8实例（8 vCPU，32 GB内存）。\n - OpenVLA：g2-standard-8实例（NVIDIA L4 GPU，24 GB显存）。\n2. **评估指标**：\n - **平均均方误差（AMSE）**：计算数据集中所有时间步的MSE平均值，用于跨数据集性能比较。\n - **归一化AMSE（NAMSE）**：对每个模型的MSE进行最小-最大归一化后取平均，用于 equitable 跨数据集比较。\n - **完成率**：通过比较预测与真实最终动作评估任务完成情况，作为任务完成能力的近似度量。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Metarch.ai</p>\n<h3>论文概述</h3>\n<p>本文提出了一个全面的评估框架和基准测试套件，用于评估视觉-语言-动作（VLA）模型在机器人学习任务中的性能。研究对GPT-4o、OpenVLA和JAT三种最先进的VLM和VLA模型进行了详细分析，覆盖了来自Open-X-Embodiment集合的20个多样化数据集。研究揭示了当前VLA模型在不同任务和机器人平台上的性能变化显著，其中GPT-4o通过复杂的提示工程展现出最一致的性能，而所有模型在需要多步规划的复杂操作任务中均表现不佳，且模型性能对动作空间特性和环境因素高度敏感。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首个大规模VLA基准测试</strong>：推出了MultiNet v0.1，这是首个针对大规模通用动作模型的基准测试。</li><li><strong>系统性评估框架</strong>：建立了一个全面的评估框架，包括数据集管理、模型配置和性能指标。</li><li><strong>模型性能分析</strong>：提供了对三种代表性VLA模型的详细性能分析，揭示了它们的优势和局限。</li><li><strong>开源工具</strong>：发布了开源软件基础设施，用于下载、管理和利用基准数据。</li><li><strong>跨模态映射框架</strong>：提出了一个通用框架，用于将VLM映射到其他模态类别，特别是动作空间。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据集处理</strong>：从Open-X-Embodiment数据集中筛选并处理了53个数据集，最终对20个数据集进行评估。数据集涵盖多种机器人平台和任务类型，使用RLDS格式存储。</li><li><strong>模型配置</strong>：</li></ol>\n<p> - <strong>JAT</strong>：采用零样本设置，对图像进行4通道处理（RGB复制红色通道作为Alpha），观察值和动作拼接为单一张量。</p>\n<p> - <strong>GPT-4o</strong>：构建综合提示，包括浮点观察状态、主图像观察、自然语言指令和动作空间描述，对不兼容输出进行错误处理。</p>\n<p> - <strong>OpenVLA</strong>：标准化夹爪命令（二进制/三元离散化或连续归一化），处理特殊数据集（如UCSD和ETH），并排除终端张量。</p>\n<ol><li><strong>评估指标</strong>：主要使用平均均方误差（AMSE）、归一化AMSE（NAMSE）和完成率，通过比较预测动作与真实动作来评估模型性能。</li><li><strong>推理基础设施</strong>：JAT和GPT使用GCP e2-standard-8实例，OpenVLA使用配备NVIDIA L4 GPU的GCP g2-standard-8实例。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：Open-X-Embodiment数据集，包含来自21个机构的超过100万条真实机器人轨迹，涵盖22种不同的机器人形态。评估版本v0.1使用了53个数据集，完整训练数据约32TB。</li><li><strong>训练资源</strong>：未明确提及模型训练过程，但详细描述了推理阶段的计算资源，包括GCP实例类型和GPU配置（如NVIDIA L4）。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：基于Google Cloud Platform（GCP）的虚拟机实例，具体包括：</li></ol>\n<p> - JAT和GPT：e2-standard-8实例（8 vCPU，32 GB内存）。</p>\n<p> - OpenVLA：g2-standard-8实例（NVIDIA L4 GPU，24 GB显存）。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>平均均方误差（AMSE）</strong>：计算数据集中所有时间步的MSE平均值，用于跨数据集性能比较。</p>\n<p> - <strong>归一化AMSE（NAMSE）</strong>：对每个模型的MSE进行最小-最大归一化后取平均，用于 equitable 跨数据集比较。</p>\n<p> - <strong>完成率</strong>：通过比较预测与真实最终动作评估任务完成情况，作为任务完成能力的近似度量。</p>"
  },
  {
    "date": "2024-11-05",
    "title": "VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation",
    "link": "http://arxiv.org/abs/2411.03540",
    "summary_markdown": "### 论文研究单位\nRobotics Institute, Carnegie Mellon University\n### 论文概述\n论文介绍了VLA-3D，一个用于3D语义场景理解和导航任务的大规模真实世界数据集。该数据集旨在解决现有视觉-语言模型在室内导航任务中因空间推理和语义理解不足而面临的挑战，特别是在包含大量细粒度物体的复杂且变化的场景中。VLA-3D通过提供丰富的3D场景数据、语义关系和指称语言，帮助开发更鲁棒和交互式的具身智能体。\n### 论文核心贡献点\n1. 提出了VLA-3D，一个目前最大的真实世界3D视觉-语言-行动数据集，包含超过11.5K个室内场景、23.5M个语义关系和9.7M个指称语句。\n2. 为每个场景提供了大规模的密集场景图，有助于处理场景变化和识别相似物体，这是该数据集区别于其他数据集的关键特征之一。\n3. 包含了可导航的自由空间标注，使得模型不仅能引用物体，还能引用空间或路径。\n4. 生成的指称语言语句是视图无关、无歧义且最小化的，更贴近人类自然语言习惯。\n5. 公开了完整的数据集生成代码和可视化工具，以促进研究发展。\n### 论文方法描述\n数据集的构建包含三个主要步骤：\n1. **3D扫描处理**：整合ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes和Unity等数据源的3D扫描数据，生成点云。为每个物体提取语义类别、边界框和主要颜色，并计算水平可穿越的自由空间区域。\n2. **场景图生成**：基于物体的3D边界框，使用启发式方法为每个区域内的物体对或三元组计算八种类型的语义空间关系（如above, below, near, between等），构建结构化的场景图。\n3. **语言生成**：基于生成的场景图，使用模板方法合成指称语言语句。这些语句遵循视图无关、无歧义和最小化的原则，并利用关系谓词的同义词增加多样性。\n### 论文使用数据集和训练资源\n* **使用数据集**：该论文主要介绍了一个新的数据集VLA-3D，其本身由多个现有数据集构成，包括ScanNet、Matterport3D、Habitat-Matterport 3D (HM3D)、3RScan、ARKitScenes以及Unity生成的场景。最终数据集包含7635个场景，超过11.5K个区域（房间），超过28.6万个物体，以及477个不同的物体类别。\n* **训练资源**：论文未提及在VLA-3D数据集上从头训练模型的计算资源（如GPU数量、训练时间）。评估时直接使用了MVT和3D-VisTA模型的预训练检查点。\n### 论文使用的评估环境和评估指标\n* **评估环境**：在VLA-3D数据集的测试集上，对两个当前最先进的开源模型（MVT和3D-VisTA）的预训练权重进行了直接评估，以验证数据集的挑战性。评估内容是在未见过的复杂场景上执行指称物体定位任务。\n* **评估指标**：主要的评估指标是**准确率**，即模型根据语言描述正确识别出目标物体的百分比。论文还将模型在VLA-3D上的性能与它们在标准基准数据集Nr3D和Sr3D上的表现进行了对比。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Robotics Institute, Carnegie Mellon University</p>\n<h3>论文概述</h3>\n<p>论文介绍了VLA-3D，一个用于3D语义场景理解和导航任务的大规模真实世界数据集。该数据集旨在解决现有视觉-语言模型在室内导航任务中因空间推理和语义理解不足而面临的挑战，特别是在包含大量细粒度物体的复杂且变化的场景中。VLA-3D通过提供丰富的3D场景数据、语义关系和指称语言，帮助开发更鲁棒和交互式的具身智能体。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了VLA-3D，一个目前最大的真实世界3D视觉-语言-行动数据集，包含超过11.5K个室内场景、23.5M个语义关系和9.7M个指称语句。</li><li>为每个场景提供了大规模的密集场景图，有助于处理场景变化和识别相似物体，这是该数据集区别于其他数据集的关键特征之一。</li><li>包含了可导航的自由空间标注，使得模型不仅能引用物体，还能引用空间或路径。</li><li>生成的指称语言语句是视图无关、无歧义且最小化的，更贴近人类自然语言习惯。</li><li>公开了完整的数据集生成代码和可视化工具，以促进研究发展。</li></ol>\n<h3>论文方法描述</h3>\n<p>数据集的构建包含三个主要步骤：</p>\n<ol><li><strong>3D扫描处理</strong>：整合ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes和Unity等数据源的3D扫描数据，生成点云。为每个物体提取语义类别、边界框和主要颜色，并计算水平可穿越的自由空间区域。</li><li><strong>场景图生成</strong>：基于物体的3D边界框，使用启发式方法为每个区域内的物体对或三元组计算八种类型的语义空间关系（如above, below, near, between等），构建结构化的场景图。</li><li><strong>语言生成</strong>：基于生成的场景图，使用模板方法合成指称语言语句。这些语句遵循视图无关、无歧义和最小化的原则，并利用关系谓词的同义词增加多样性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>使用数据集</strong>：该论文主要介绍了一个新的数据集VLA-3D，其本身由多个现有数据集构成，包括ScanNet、Matterport3D、Habitat-Matterport 3D (HM3D)、3RScan、ARKitScenes以及Unity生成的场景。最终数据集包含7635个场景，超过11.5K个区域（房间），超过28.6万个物体，以及477个不同的物体类别。</li><li><strong>训练资源</strong>：论文未提及在VLA-3D数据集上从头训练模型的计算资源（如GPU数量、训练时间）。评估时直接使用了MVT和3D-VisTA模型的预训练检查点。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在VLA-3D数据集的测试集上，对两个当前最先进的开源模型（MVT和3D-VisTA）的预训练权重进行了直接评估，以验证数据集的挑战性。评估内容是在未见过的复杂场景上执行指称物体定位任务。</li><li><strong>评估指标</strong>：主要的评估指标是<strong>准确率</strong>，即模型根据语言描述正确识别出目标物体的百分比。论文还将模型在VLA-3D上的性能与它们在标准基准数据集Nr3D和Sr3D上的表现进行了对比。</li></ul>"
  },
  {
    "date": "2024-11-04",
    "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
    "link": "http://arxiv.org/abs/2411.02359",
    "summary_markdown": "### 论文研究单位\n清华大学自动化系与字节跳动\n### 论文概述\n论文提出了一种名为DeeR-VLA的动态推理框架，用于解决多模态大语言模型在机器人控制中的计算效率问题。该方法基于一个观察：机器人执行任务时，大多数情况是相对简单的，可以用较小的模型处理，只有少数复杂情况才需要完整的大模型。DeeR-VLA通过在多模态大语言模型中引入多个中间退出点，根据当前情况动态选择合适的模型规模，从而在保持性能的同时显著降低计算和内存需求。\n### 论文核心贡献点\n- 提出动态早期退出框架DeeR-VLA，首次将动态推理应用于机器人多模态大语言模型\n- 设计基于动作一致性的退出判据，替代传统基于置信度的方法\n- 开发预算执行算法，可根据计算和内存约束自动优化退出阈值\n- 在CALVIN基准上实现5.2-6.5倍计算减少和2-6倍GPU内存减少，同时保持竞争性能\n### 论文方法描述\n- 多退出架构：将LLM层分为N个连续组，每组后添加退出点，通过最大池化聚合特征\n- 动作预测头：使用轻量级LSTM处理历史信息，预测7自由度末端执行器动作（6连续+1离散）\n- 退出判据：比较相邻退出点的动作预测L2距离，当小于阈值时提前退出\n- 预算执行：将阈值选择建模为优化问题，在满足计算/内存约束下最大化任务成功率\n- 训练策略：随机采样所有退出点特征进行训练，使用辅助动作头优化模型\n### 论文使用数据集和训练资源\n- 数据集：CALVIN Long-Horizon Multi-Task Language Control (LH-MTLC)基准\n- 训练资源：8×NVIDIA A100 GPU，训练时间约24小时\n### 论文使用的评估环境和评估指标\n- 评估环境：CALVIN LH-MTLC的三个挑战子集\n- 评估指标：任务成功率、平均计算成本、峰值计算成本、GPU内存使用量",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学自动化系与字节跳动</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为DeeR-VLA的动态推理框架，用于解决多模态大语言模型在机器人控制中的计算效率问题。该方法基于一个观察：机器人执行任务时，大多数情况是相对简单的，可以用较小的模型处理，只有少数复杂情况才需要完整的大模型。DeeR-VLA通过在多模态大语言模型中引入多个中间退出点，根据当前情况动态选择合适的模型规模，从而在保持性能的同时显著降低计算和内存需求。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出动态早期退出框架DeeR-VLA，首次将动态推理应用于机器人多模态大语言模型</li><li>设计基于动作一致性的退出判据，替代传统基于置信度的方法</li><li>开发预算执行算法，可根据计算和内存约束自动优化退出阈值</li><li>在CALVIN基准上实现5.2-6.5倍计算减少和2-6倍GPU内存减少，同时保持竞争性能</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>多退出架构：将LLM层分为N个连续组，每组后添加退出点，通过最大池化聚合特征</li><li>动作预测头：使用轻量级LSTM处理历史信息，预测7自由度末端执行器动作（6连续+1离散）</li><li>退出判据：比较相邻退出点的动作预测L2距离，当小于阈值时提前退出</li><li>预算执行：将阈值选择建模为优化问题，在满足计算/内存约束下最大化任务成功率</li><li>训练策略：随机采样所有退出点特征进行训练，使用辅助动作头优化模型</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：CALVIN Long-Horizon Multi-Task Language Control (LH-MTLC)基准</li><li>训练资源：8×NVIDIA A100 GPU，训练时间约24小时</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：CALVIN LH-MTLC的三个挑战子集</li><li>评估指标：任务成功率、平均计算成本、峰值计算成本、GPU内存使用量</li></ul>"
  },
  {
    "date": "2024-11-01",
    "title": "CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision",
    "link": "http://arxiv.org/abs/2411.00508",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-10-21",
    "title": "The Duality of Generative AI and Reinforcement Learning in Robotics: A Review",
    "link": "http://arxiv.org/abs/2410.16411",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-10-21",
    "title": "VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making",
    "link": "http://arxiv.org/abs/2410.15885",
    "summary_markdown": "### 论文研究单位\n浙江大学计算机科学与技术学院、中国电信数字生活科技有限公司、浙江实验室、都柏林三一学院\n### 论文概述\n本文提出MIMO-VLA（VLASCD），一个统一的多输入多输出（MIMO）训练架构，用于同时进行对话生成和决策制定。研究发现现有的多输入单输出（MISO）架构（如LLMs和VLAs）在MIMO场景中存在根本限制，任务间会因共享输出通道产生相互排斥效应，导致优化不平衡和性能下降。MIMO-VLA通过并行多任务输出消除了任务间干扰，支持高效并行处理，在CARLA自动驾驶平台上验证了其有效性。\n### 论文核心贡献点\n1. 首次证明现有MISO模型（如LLMs和VLAs）在处理MIMO任务时存在根本性缺陷\n2. 提出MIMO-VLA统一MIMO训练架构，集成多项技术：\n - 生成连续动作值的计算模块和损失项\n - 利用丰富视觉信息的图像重建损失\n - 保持对话能力同时增强决策准确性的标签平滑策略\n3. 广泛实验证明MIMO-VLA在决策准确性上超越SOTA基线，同时完全保留实时对话功能\n### 论文方法描述\n模型架构：\n- 基于LLaMA-7B骨干网络，支持文本、图像和数值向量三种输入模态\n- 图像分割为补丁通过2D卷积编码，动作值通过MLP编码\n- 文本使用预训练嵌入层，所有模态嵌入按固定顺序拼接\n- 输出端支持文本响应和连续动作决策两种模态\n\n训练过程：\n- 使用LoRA微调，仅更新Q和V投影模块（占0.06%参数）\n- 联合训练三个损失函数：\n - 文本生成：采用标签平滑的交叉熵损失防止过拟合\n - 动作预测：直接预测连续动作值的MSE损失\n - 图像重建：像素级欧氏距离的辅助损失\n- 实现梯度空间隔离机制，不同损失影响不同位置编码\n### 论文使用数据集和训练资源\n数据集：\n- 在CARLA 0.9.10平台使用EGADS框架收集专家轨迹数据\n- 总量5.69GB，包含13,761帧驾驶场景\n- 每帧配有从50个预设问题中随机选择的问答对\n\n训练资源：\n- 使用LoRA技术高效微调大型语言模型\n- 在town03地图上采集训练数据\n- 动作空间为2维（加速度和转向），范围分别为[-3,3]和[-0.2,0.2]\n### 论文使用的评估环境和评估指标\n评估环境：\n- gym-carla环境（基于CARLA 0.9.10的OpenAI Gym兼容环境）\n- 训练集town03，测试集包括town03和town04（泛化测试）\n\n评估指标：\n对话能力评估：\n- GPT-4o对50个随机环境-问题对进行评分（0-10分）\n- 分级标准：不可接受(<3)、可接受(3≤score<6)、良好(≥6)\n\n决策能力评估：\n- 碰撞率(CR, %, ↓)\n- 偏离道路率(OR, %, ↓)\n- 任务完成率(ER, %, ↑)\n- 平均安全驾驶距离(ASD, 米, ↑)\n- 平均奖励(AR, 分数, ↑)\n- 驾驶分数(DS = ER × AR, ↑)",
    "summary_html": "<h3>论文研究单位</h3>\n<p>浙江大学计算机科学与技术学院、中国电信数字生活科技有限公司、浙江实验室、都柏林三一学院</p>\n<h3>论文概述</h3>\n<p>本文提出MIMO-VLA（VLASCD），一个统一的多输入多输出（MIMO）训练架构，用于同时进行对话生成和决策制定。研究发现现有的多输入单输出（MISO）架构（如LLMs和VLAs）在MIMO场景中存在根本限制，任务间会因共享输出通道产生相互排斥效应，导致优化不平衡和性能下降。MIMO-VLA通过并行多任务输出消除了任务间干扰，支持高效并行处理，在CARLA自动驾驶平台上验证了其有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>首次证明现有MISO模型（如LLMs和VLAs）在处理MIMO任务时存在根本性缺陷</li><li>提出MIMO-VLA统一MIMO训练架构，集成多项技术：</li></ol>\n<p> - 生成连续动作值的计算模块和损失项</p>\n<p> - 利用丰富视觉信息的图像重建损失</p>\n<p> - 保持对话能力同时增强决策准确性的标签平滑策略</p>\n<ol><li>广泛实验证明MIMO-VLA在决策准确性上超越SOTA基线，同时完全保留实时对话功能</li></ol>\n<h3>论文方法描述</h3>\n<p>模型架构：</p>\n<ul><li>基于LLaMA-7B骨干网络，支持文本、图像和数值向量三种输入模态</li><li>图像分割为补丁通过2D卷积编码，动作值通过MLP编码</li><li>文本使用预训练嵌入层，所有模态嵌入按固定顺序拼接</li><li>输出端支持文本响应和连续动作决策两种模态</li></ul>\n\n<p>训练过程：</p>\n<ul><li>使用LoRA微调，仅更新Q和V投影模块（占0.06%参数）</li><li>联合训练三个损失函数：</li></ul>\n<p> - 文本生成：采用标签平滑的交叉熵损失防止过拟合</p>\n<p> - 动作预测：直接预测连续动作值的MSE损失</p>\n<p> - 图像重建：像素级欧氏距离的辅助损失</p>\n<ul><li>实现梯度空间隔离机制，不同损失影响不同位置编码</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：</p>\n<ul><li>在CARLA 0.9.10平台使用EGADS框架收集专家轨迹数据</li><li>总量5.69GB，包含13,761帧驾驶场景</li><li>每帧配有从50个预设问题中随机选择的问答对</li></ul>\n\n<p>训练资源：</p>\n<ul><li>使用LoRA技术高效微调大型语言模型</li><li>在town03地图上采集训练数据</li><li>动作空间为2维（加速度和转向），范围分别为[-3,3]和[-0.2,0.2]</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：</p>\n<ul><li>gym-carla环境（基于CARLA 0.9.10的OpenAI Gym兼容环境）</li><li>训练集town03，测试集包括town03和town04（泛化测试）</li></ul>\n\n<p>评估指标：</p>\n<p>对话能力评估：</p>\n<ul><li>GPT-4o对50个随机环境-问题对进行评分（0-10分）</li><li>分级标准：不可接受(<3)、可接受(3≤score<6)、良好(≥6)</li></ul>\n\n<p>决策能力评估：</p>\n<ul><li>碰撞率(CR, %, ↓)</li><li>偏离道路率(OR, %, ↓)</li><li>任务完成率(ER, %, ↑)</li><li>平均安全驾驶距离(ASD, 米, ↑)</li><li>平均奖励(AR, 分数, ↑)</li><li>驾驶分数(DS = ER × AR, ↑)</li></ul>"
  },
  {
    "date": "2024-10-21",
    "title": "A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM",
    "link": "http://arxiv.org/abs/2410.15549",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-10-17",
    "title": "Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand",
    "link": "http://arxiv.org/abs/2410.14022",
    "summary_markdown": "### 论文研究单位\n瑞士洛桑联邦理工学院机械工程学院\n### 论文概述\n该论文提出了一种结合微调的视觉-语言-动作（VLA）模型与扩散模型的混合控制方法，用于实现人形手的灵巧操作。VLA模型负责基于语言指令的高级规划，具有高泛化性；扩散模型则处理低级交互，为特定物体和环境提供所需的精度和鲁棒性。通过在训练数据中加入切换信号，实现了两个模型之间基于事件的转换，应用于通过语言指令指定目标物体和放置位置的抓取-放置任务。该方法在13自由度的ADAPT Hand 2上进行了部署，这是首次将VLA模型应用于多指手。实验表明，该混合方法的成功率超过80%，而仅使用VLA模型的成功率低于40%。\n### 论文核心贡献点\n1. 提出了一种混合控制框架，结合了VLA模型的高级规划能力和扩散模型的低级精确控制能力。\n2. 设计了一种基于事件信号的模型切换机制，实现了VLA模型和扩散模型之间的无缝转换。\n3. 首次成功地将VLA模型应用于控制多指灵巧手。\n4. 在真实的抓取-放置任务中，验证了混合框架的有效性，成功率远超仅使用VLA模型的基线。\n5. 展示了系统具备多种期望行为：多模态抓取、错误恢复能力以及通过硬件柔顺性带来的交互稳定性。\n### 论文方法描述\n1. **框架设计**：系统在VLA模型和扩散策略模型之间切换。VLA模型（微调的openVLA）根据语言指令和视觉反馈控制手臂的运动，使手部接近目标物体，其输出中的一个标量信号（抓取百分比）被重新用作切换事件信号。扩散策略模型负责精确的抓取动作，当其完成抓取后，通过另一个事件信号将控制权交还给VLA模型。\n2. **机器人平台**：使用定制的ADAPT Hand 2，一个13自由度、具有串联弹性驱动（SEA）的拟人化灵巧手，安装在UR5机械臂上。这种柔顺性设计允许机器人安全地与环境和物体交互。\n3. **模型输入**：VLA模型的视觉输入是来自一个固定摄像头和一个手腕摄像头的两幅图像，经过缩放后垂直拼接成一幅图像输入。扩散策略同样使用这两个摄像头的图像作为输入。\n4. **模型选择**：扩散策略模型是针对特定物体训练的，因此系统使用一个基于语言输入的查找表来为每个物体选择对应的扩散策略模型。\n### 论文使用数据集和训练资源\n1. **数据集**：所有数据均通过真实机器人遥操作收集。\n * **VLA数据**：记录完整的抓取-放置任务，但仅使用抓取动作前后的轨迹进行训练。针对每个物体和目标位置的组合，记录了20次试验。\n * **扩散策略数据**：仅记录抓取动作部分。对每个物体（辣椒、磁带、纸），记录了30到40次试验，包含不同的抓取策略（如滑动抓取、直接抓取）和失败恢复的演示。\n2. **训练资源**：\n * **VLA模型训练**：在配备单个A100-80GB GPU的集群虚拟机上进行微调，直至动作精度超过95%。\n * **扩散策略训练**：在自定义GPU上训练1500个周期。\n * **推理运行**：在Nvidia RTX 4090 GPU上运行，实现约5Hz的控制频率。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n * **硬件**：ADAPT Hand 2灵巧手与UR5机械臂。\n * **任务**：对三种不同几何形状的物体（红辣椒、磁带、一张纸）执行抓取-放置任务，放置到两个指定位置（黄色或紫色盘子）之一。\n * **传感器**：一个固定于世界坐标的摄像头和一个安装在手腕的摄像头。\n2. **评估指标**：\n * **VLA接近精度**：测量VLA控制手部移动到目标物体附近后，手部中心与物体中心在xy平面的偏移距离。\n * **扩散策略成功率**：测试当手部初始位置与物体有不同偏移（5, 10, 15cm）时的抓取成功率。\n * **多模态抓取能力**：评估模型能否根据物体在桌面上的不同位置选择合适的抓取策略（如滑动抓取或直接抓取）。\n * **任务整体成功率**：采用一个5级评分系统来评估完整的抓取-放置任务：1.00（完全成功）、0.75（未能放在正确盘子）、0.50（未能放在错误盘子）、0.25（未能抓取正确物体）、0.00（接近了错误物体）。\n * **对比基准**：将提出的VLA+扩散模型与仅使用VLA模型（使用预编程的抓取序列作为1-DoF夹爪代理）的方法进行对比。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>瑞士洛桑联邦理工学院机械工程学院</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种结合微调的视觉-语言-动作（VLA）模型与扩散模型的混合控制方法，用于实现人形手的灵巧操作。VLA模型负责基于语言指令的高级规划，具有高泛化性；扩散模型则处理低级交互，为特定物体和环境提供所需的精度和鲁棒性。通过在训练数据中加入切换信号，实现了两个模型之间基于事件的转换，应用于通过语言指令指定目标物体和放置位置的抓取-放置任务。该方法在13自由度的ADAPT Hand 2上进行了部署，这是首次将VLA模型应用于多指手。实验表明，该混合方法的成功率超过80%，而仅使用VLA模型的成功率低于40%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种混合控制框架，结合了VLA模型的高级规划能力和扩散模型的低级精确控制能力。</li><li>设计了一种基于事件信号的模型切换机制，实现了VLA模型和扩散模型之间的无缝转换。</li><li>首次成功地将VLA模型应用于控制多指灵巧手。</li><li>在真实的抓取-放置任务中，验证了混合框架的有效性，成功率远超仅使用VLA模型的基线。</li><li>展示了系统具备多种期望行为：多模态抓取、错误恢复能力以及通过硬件柔顺性带来的交互稳定性。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>框架设计</strong>：系统在VLA模型和扩散策略模型之间切换。VLA模型（微调的openVLA）根据语言指令和视觉反馈控制手臂的运动，使手部接近目标物体，其输出中的一个标量信号（抓取百分比）被重新用作切换事件信号。扩散策略模型负责精确的抓取动作，当其完成抓取后，通过另一个事件信号将控制权交还给VLA模型。</li><li><strong>机器人平台</strong>：使用定制的ADAPT Hand 2，一个13自由度、具有串联弹性驱动（SEA）的拟人化灵巧手，安装在UR5机械臂上。这种柔顺性设计允许机器人安全地与环境和物体交互。</li><li><strong>模型输入</strong>：VLA模型的视觉输入是来自一个固定摄像头和一个手腕摄像头的两幅图像，经过缩放后垂直拼接成一幅图像输入。扩散策略同样使用这两个摄像头的图像作为输入。</li><li><strong>模型选择</strong>：扩散策略模型是针对特定物体训练的，因此系统使用一个基于语言输入的查找表来为每个物体选择对应的扩散策略模型。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：所有数据均通过真实机器人遥操作收集。</li></ol>\n<p> * <strong>VLA数据</strong>：记录完整的抓取-放置任务，但仅使用抓取动作前后的轨迹进行训练。针对每个物体和目标位置的组合，记录了20次试验。</p>\n<p> * <strong>扩散策略数据</strong>：仅记录抓取动作部分。对每个物体（辣椒、磁带、纸），记录了30到40次试验，包含不同的抓取策略（如滑动抓取、直接抓取）和失败恢复的演示。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> * <strong>VLA模型训练</strong>：在配备单个A100-80GB GPU的集群虚拟机上进行微调，直至动作精度超过95%。</p>\n<p> * <strong>扩散策略训练</strong>：在自定义GPU上训练1500个周期。</p>\n<p> * <strong>推理运行</strong>：在Nvidia RTX 4090 GPU上运行，实现约5Hz的控制频率。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> * <strong>硬件</strong>：ADAPT Hand 2灵巧手与UR5机械臂。</p>\n<p> * <strong>任务</strong>：对三种不同几何形状的物体（红辣椒、磁带、一张纸）执行抓取-放置任务，放置到两个指定位置（黄色或紫色盘子）之一。</p>\n<p> * <strong>传感器</strong>：一个固定于世界坐标的摄像头和一个安装在手腕的摄像头。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> * <strong>VLA接近精度</strong>：测量VLA控制手部移动到目标物体附近后，手部中心与物体中心在xy平面的偏移距离。</p>\n<p> * <strong>扩散策略成功率</strong>：测试当手部初始位置与物体有不同偏移（5, 10, 15cm）时的抓取成功率。</p>\n<p> * <strong>多模态抓取能力</strong>：评估模型能否根据物体在桌面上的不同位置选择合适的抓取策略（如滑动抓取或直接抓取）。</p>\n<p> * <strong>任务整体成功率</strong>：采用一个5级评分系统来评估完整的抓取-放置任务：1.00（完全成功）、0.75（未能放在正确盘子）、0.50（未能放在错误盘子）、0.25（未能抓取正确物体）、0.00（接近了错误物体）。</p>\n<p> * <strong>对比基准</strong>：将提出的VLA+扩散模型与仅使用VLA模型（使用预编程的抓取序列作为1-DoF夹爪代理）的方法进行对比。</p>"
  },
  {
    "date": "2024-10-15",
    "title": "Latent Action Pretraining from Videos",
    "link": "http://arxiv.org/abs/2410.11758",
    "summary_markdown": "好的，我已阅读并理解了论文内容。以下是根据要求生成的Markdown格式论文总结：\n\n```markdown\n### 论文研究单位\nKAIST、华盛顿大学、微软研究院、NVIDIA、艾伦人工智能研究所。\n### 论文概述\n论文提出了一种名为“潜动作预训练”（Latent Action Pretraining, LAPA）的无监督方法，用于在无需真实机器人动作标签的情况下预训练视觉-语言-动作（VLA）模型。该方法旨在从大规模无标签的互联网视频中学习机器人技能，克服现有VLA模型对人工遥操作数据的依赖，从而扩展数据来源和规模。LAPA通过学习离散的潜动作表示，并在少量机器人动作数据上微调，实现了对未见任务、对象和指令的泛化能力，性能超越了现有技术。\n### 论文核心贡献点\n1. 首次提出无监督的VLA预训练框架LAPA，利用无动作标签的视频数据。\n2. 实验表明LAPA在跨任务、跨环境和跨形态设置中显著优于现有基线方法，并在真实任务中超越当前最优VLA模型OpenVLA（+6.22%）。\n3. 证明了仅使用人类操作视频进行预训练的有效性，为利用网络规模数据构建机器人基础模型开辟了道路。\n4. 展示了LAPA可作为世界模型生成神经轨迹，支持闭环评估。\n### 论文方法描述\n方法分为三个阶段：\n1. **潜动作量化（Latent Action Quantization）**：使用基于VQ-VAE的目标训练编码器-解码器模型，从连续视频帧（当前帧xt和未来帧xt+H）中学习离散的潜动作表示zt，通过量化损失和码本实现动作标记化。\n2. **潜预训练（Latent Pretraining）**：将预训练的视觉-语言模型（VLM）的编码器作为逆动力学模型，预测语言指令和当前观察xt对应的潜动作zt，冻结视觉编码器并训练语言模型参数。\n3. **动作微调（Action Finetuning）**：在小规模机器人动作数据集上微调模型，替换潜动作头为真实动作头（如末端执行器增量动作），映射潜空间到机器人动作，冻结视觉编码器。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：BridgeV2（机器人轨迹）、Open-X-Embodiment（多机器人数据集）、Something-Something V2（人类操作视频）。\n - 微调：Language Table（仿真推积木）、SIMPLER（仿真7-DOF WidowX臂）、真实世界桌面操作任务（7-DOF Franka Panda臂，3任务）。\n- **训练资源**：\n - 潜动作量化模型：使用NSVQ避免梯度崩溃，码本替换技术。\n - VLM骨干：7B Large World Model（LWM-Chat-1M）。\n - 预训练计算：8 H100 GPU训练34小时（272 GPU小时），比OpenVLA（21,500 A100小时）高效30倍。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真：Language Table（2-DOF推任务）、SIMPLER（多任务仿真）。\n - 真实世界：Franka Emika Panda机械臂执行“拾取物体”、“覆盖物体”、“敲击物体”任务。\n- **评估指标**：\n - 平均成功率（Average Success Rate），标准差（StdErr），针对seen/unseen对象、组合和指令设置。\n - 任务部分成功率（Partial Success Criterion）。\n - 生成轨迹质量（通过闭环 rollout 评估）。\n```",
    "summary_html": "<p>好的，我已阅读并理解了论文内容。以下是根据要求生成的Markdown格式论文总结：</p>\n\n<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>KAIST、华盛顿大学、微软研究院、NVIDIA、艾伦人工智能研究所。</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为“潜动作预训练”（Latent Action Pretraining, LAPA）的无监督方法，用于在无需真实机器人动作标签的情况下预训练视觉-语言-动作（VLA）模型。该方法旨在从大规模无标签的互联网视频中学习机器人技能，克服现有VLA模型对人工遥操作数据的依赖，从而扩展数据来源和规模。LAPA通过学习离散的潜动作表示，并在少量机器人动作数据上微调，实现了对未见任务、对象和指令的泛化能力，性能超越了现有技术。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>首次提出无监督的VLA预训练框架LAPA，利用无动作标签的视频数据。</li><li>实验表明LAPA在跨任务、跨环境和跨形态设置中显著优于现有基线方法，并在真实任务中超越当前最优VLA模型OpenVLA（+6.22%）。</li><li>证明了仅使用人类操作视频进行预训练的有效性，为利用网络规模数据构建机器人基础模型开辟了道路。</li><li>展示了LAPA可作为世界模型生成神经轨迹，支持闭环评估。</li></ol>\n<h3>论文方法描述</h3>\n<p>方法分为三个阶段：</p>\n<ol><li><strong>潜动作量化（Latent Action Quantization）</strong>：使用基于VQ-VAE的目标训练编码器-解码器模型，从连续视频帧（当前帧xt和未来帧xt+H）中学习离散的潜动作表示zt，通过量化损失和码本实现动作标记化。</li><li><strong>潜预训练（Latent Pretraining）</strong>：将预训练的视觉-语言模型（VLM）的编码器作为逆动力学模型，预测语言指令和当前观察xt对应的潜动作zt，冻结视觉编码器并训练语言模型参数。</li><li><strong>动作微调（Action Finetuning）</strong>：在小规模机器人动作数据集上微调模型，替换潜动作头为真实动作头（如末端执行器增量动作），映射潜空间到机器人动作，冻结视觉编码器。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：BridgeV2（机器人轨迹）、Open-X-Embodiment（多机器人数据集）、Something-Something V2（人类操作视频）。</p>\n<p> - 微调：Language Table（仿真推积木）、SIMPLER（仿真7-DOF WidowX臂）、真实世界桌面操作任务（7-DOF Franka Panda臂，3任务）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 潜动作量化模型：使用NSVQ避免梯度崩溃，码本替换技术。</p>\n<p> - VLM骨干：7B Large World Model（LWM-Chat-1M）。</p>\n<p> - 预训练计算：8 H100 GPU训练34小时（272 GPU小时），比OpenVLA（21,500 A100小时）高效30倍。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真：Language Table（2-DOF推任务）、SIMPLER（多任务仿真）。</p>\n<p> - 真实世界：Franka Emika Panda机械臂执行“拾取物体”、“覆盖物体”、“敲击物体”任务。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 平均成功率（Average Success Rate），标准差（StdErr），针对seen/unseen对象、组合和指令设置。</p>\n<p> - 任务部分成功率（Partial Success Criterion）。</p>\n<p> - 生成轨迹质量（通过闭环 rollout 评估）。</p>\n<p>```</p>"
  },
  {
    "date": "2024-10-10",
    "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2410.08001",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-09-12",
    "title": "HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers",
    "link": "http://arxiv.org/abs/2410.05273",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-10-07",
    "title": "LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2410.05191",
    "summary_markdown": "```markdown\n### 论文研究单位\nUniversity of Alberta, Edmonton, AB, Canada\nThe University of Tokyo, Tokyo, Japan\n### 论文概述\n本文提出了LADEV，一个语言驱动的测试和评估平台，用于机器人操作中的视觉-语言-动作（VLA）模型。该平台通过自然语言输入自动生成仿真环境，并对任务指令进行转述，以全面评估VLA模型的性能。相比现有方法如SimplerEnv（需手动调整环境且忽略语言输入变化），LADEV提高了测试效率并填补了语言多样性评估的空白。实验使用超过4000个场景测试了七个VLA模型，证明了其有效性。\n### 论文核心贡献点\n- 提出语言驱动方法，从自然语言描述自动生成仿真环境，减少手动配置需求。\n- 实现任务指令转述机制，生成多样化的语言输入以评估模型的语言鲁棒性。\n- 引入批量评估方法，通过单一输入生成多个测试场景，支持大规模测试。\n- 对多个VLA模型进行全面评估，涵盖四个任务和4000+场景，建立性能基线。\n### 论文方法描述\n- **语言驱动的仿真环境生成**：\n 使用LLM将自然语言描述（对象数量、细节、环境设置）转换为仿真器兼容配置。过程分两步：对象配置（基于YCB和SimplerEnv数据集选择模型）和环境设置（调整光照和相机位置）。采用少样本上下文学习确保输出格式兼容。\n- **自然语言任务指令转述**：\n 生成阶段用LLM基于原始指令创建k个变体；验证阶段用sentence BERT计算语义相似度，超过阈值则保留有效指令。\n- **批量评估**：\n LLM生成n个场景描述和原始指令，每个指令转述为k个变体，共n×k个输入，实现高效大规模测试。\n### 论文使用数据集和训练资源\n- **数据集**：\n YCB对象数据集（65个对象，视为未见对象）和SimplerEnv默认数据集（18个对象，视为训练集覆盖对象）。\n- **训练资源**：\n 实验使用GPT-4o作为LLM，服务器配置为AMD 5955WX CPU和两个NVIDIA RTX A6000 GPU。平台未训练新模型，仅用于评估。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n 基于SimplerEnv，构建于SAPIEN仿真器和ManiSkill2基准。\n- **评估任务**：\n 四个机器人操作任务：拾取物体、移动物体A到物体B附近、将物体A放在物体B上、将物体A放入物体B内。\n- **评估指标**：\n 成功率（任务完成百分比），评估条件包括对象数量（1-5个）、任务指令（基本vs转述）、对象类型（SimplerEnv vs YCB）、环境条件（默认光照、变化光照、相机位置变化）。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>University of Alberta, Edmonton, AB, Canada</p>\n<p>The University of Tokyo, Tokyo, Japan</p>\n<h3>论文概述</h3>\n<p>本文提出了LADEV，一个语言驱动的测试和评估平台，用于机器人操作中的视觉-语言-动作（VLA）模型。该平台通过自然语言输入自动生成仿真环境，并对任务指令进行转述，以全面评估VLA模型的性能。相比现有方法如SimplerEnv（需手动调整环境且忽略语言输入变化），LADEV提高了测试效率并填补了语言多样性评估的空白。实验使用超过4000个场景测试了七个VLA模型，证明了其有效性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出语言驱动方法，从自然语言描述自动生成仿真环境，减少手动配置需求。</li><li>实现任务指令转述机制，生成多样化的语言输入以评估模型的语言鲁棒性。</li><li>引入批量评估方法，通过单一输入生成多个测试场景，支持大规模测试。</li><li>对多个VLA模型进行全面评估，涵盖四个任务和4000+场景，建立性能基线。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>语言驱动的仿真环境生成</strong>：</li></ul>\n<p> 使用LLM将自然语言描述（对象数量、细节、环境设置）转换为仿真器兼容配置。过程分两步：对象配置（基于YCB和SimplerEnv数据集选择模型）和环境设置（调整光照和相机位置）。采用少样本上下文学习确保输出格式兼容。</p>\n<ul><li><strong>自然语言任务指令转述</strong>：</li></ul>\n<p> 生成阶段用LLM基于原始指令创建k个变体；验证阶段用sentence BERT计算语义相似度，超过阈值则保留有效指令。</p>\n<ul><li><strong>批量评估</strong>：</li></ul>\n<p> LLM生成n个场景描述和原始指令，每个指令转述为k个变体，共n×k个输入，实现高效大规模测试。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> YCB对象数据集（65个对象，视为未见对象）和SimplerEnv默认数据集（18个对象，视为训练集覆盖对象）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> 实验使用GPT-4o作为LLM，服务器配置为AMD 5955WX CPU和两个NVIDIA RTX A6000 GPU。平台未训练新模型，仅用于评估。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> 基于SimplerEnv，构建于SAPIEN仿真器和ManiSkill2基准。</p>\n<ul><li><strong>评估任务</strong>：</li></ul>\n<p> 四个机器人操作任务：拾取物体、移动物体A到物体B附近、将物体A放在物体B上、将物体A放入物体B内。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> 成功率（任务完成百分比），评估条件包括对象数量（1-5个）、任务指令（基本vs转述）、对象类型（SimplerEnv vs YCB）、环境条件（默认光照、变化光照、相机位置变化）。</p>\n<p>```</p>"
  },
  {
    "date": "2024-10-02",
    "title": "Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust",
    "link": "http://arxiv.org/abs/2410.01971",
    "summary_markdown": "```markdown\n### 论文研究单位\n普林斯顿大学机械与航空航天工程系\n### 论文概述\n本文提出了一种名为“Bring Your Own VLA”（BYOVLA）的运行时干预方案，旨在提高视觉-语言-动作（VLA）模型在视觉干扰下的鲁棒性。VLAs虽然在大规模数据上训练，但对任务无关的视觉细节（如干扰物或背景颜色）非常敏感。该方法通过动态识别VLA模型敏感的输入图像区域，并使用自动化图像编辑工具对任务无关区域进行最小化修改来降低模型的敏感性，且无需对模型进行微调或访问模型权重。\n### 论文核心贡献点\n- 提出了BYOVLA，一种通用的、无需训练的运行时干预框架，适用于任何现成的VLA模型。\n- 设计了一种视觉敏感性探针，通过扰动图像不同区段并测量动作输出的变化，来直接评估VLA对特定区域的敏感度。\n- 在包含干扰物和背景变化的多种操作任务中，BYOVLA将任务成功率提高了20-40%，显著优于现有基线方法（如GradCAM）。\n- 方法仅需VLM和分割模型识别任务无关区域，并自动进行选择性图像编辑，无需重新训练VLA。\n### 论文方法描述\nBYOVLA方法包含三个主要步骤：\n1. **定位任务无关对象**：使用GPT-4o等视觉语言模型（VLM）根据语言指令识别初始图像中的任务无关区域，并通过Grounded-SAM2等分割模型获得像素级掩码。\n2. **应用视觉敏感性探针**：对于每个任务无关区域，通过添加高斯模糊（针对对象干扰）或高斯噪声（针对背景干扰）来扰动图像，计算扰动前后VLA输出动作序列的加权L2范数变化（公式2），量化模型对该区域的敏感度。\n3. **确定敏感度阈值并转换图像**：若某区域的敏感度值高于预设阈值（对象干扰为0.002m，背景干扰为0.001m），则进行干预。对于对象干扰，使用Inpaint Anything模型进行修复移除；对于背景干扰，直接修改RGB像素值。\n### 论文使用数据集和训练资源\n- **数据集**：BridgeV2数据集（用于确定敏感度阈值）和自定义的厨房环境任务。\n- **VLA模型**：Octo-Base和OpenVLA（作为BYOVLA的测试平台）。\n- **预训练模型**：GPT-4o（用于识别任务无关区域）、Grounded-SAM2（用于分割）、Inpaint Anything（用于图像修复）。\n- **训练资源**：未对VLA模型进行训练或微调；BYOVLA作为运行时模块，计算资源消耗主要来自VLM和图像编辑工具的推理开销。\n### 论文使用的评估环境和评估指标\n- **评估环境**：硬件实验在物理机器人平台上进行，包含语言指令的操控任务，环境设置包括任务无关的干扰对象（如额外物体）和背景变化（如颜色改变）。\n- **评估指标**：任务成功率（Task Success Rate），即完成指定语言指令的比例。实验比较了原始VLA、BYOVLA增强的VLA以及基线方法（如GradCAM）在干扰环境下的表现。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>普林斯顿大学机械与航空航天工程系</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为“Bring Your Own VLA”（BYOVLA）的运行时干预方案，旨在提高视觉-语言-动作（VLA）模型在视觉干扰下的鲁棒性。VLAs虽然在大规模数据上训练，但对任务无关的视觉细节（如干扰物或背景颜色）非常敏感。该方法通过动态识别VLA模型敏感的输入图像区域，并使用自动化图像编辑工具对任务无关区域进行最小化修改来降低模型的敏感性，且无需对模型进行微调或访问模型权重。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了BYOVLA，一种通用的、无需训练的运行时干预框架，适用于任何现成的VLA模型。</li><li>设计了一种视觉敏感性探针，通过扰动图像不同区段并测量动作输出的变化，来直接评估VLA对特定区域的敏感度。</li><li>在包含干扰物和背景变化的多种操作任务中，BYOVLA将任务成功率提高了20-40%，显著优于现有基线方法（如GradCAM）。</li><li>方法仅需VLM和分割模型识别任务无关区域，并自动进行选择性图像编辑，无需重新训练VLA。</li></ul>\n<h3>论文方法描述</h3>\n<p>BYOVLA方法包含三个主要步骤：</p>\n<ol><li><strong>定位任务无关对象</strong>：使用GPT-4o等视觉语言模型（VLM）根据语言指令识别初始图像中的任务无关区域，并通过Grounded-SAM2等分割模型获得像素级掩码。</li><li><strong>应用视觉敏感性探针</strong>：对于每个任务无关区域，通过添加高斯模糊（针对对象干扰）或高斯噪声（针对背景干扰）来扰动图像，计算扰动前后VLA输出动作序列的加权L2范数变化（公式2），量化模型对该区域的敏感度。</li><li><strong>确定敏感度阈值并转换图像</strong>：若某区域的敏感度值高于预设阈值（对象干扰为0.002m，背景干扰为0.001m），则进行干预。对于对象干扰，使用Inpaint Anything模型进行修复移除；对于背景干扰，直接修改RGB像素值。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：BridgeV2数据集（用于确定敏感度阈值）和自定义的厨房环境任务。</li><li><strong>VLA模型</strong>：Octo-Base和OpenVLA（作为BYOVLA的测试平台）。</li><li><strong>预训练模型</strong>：GPT-4o（用于识别任务无关区域）、Grounded-SAM2（用于分割）、Inpaint Anything（用于图像修复）。</li><li><strong>训练资源</strong>：未对VLA模型进行训练或微调；BYOVLA作为运行时模块，计算资源消耗主要来自VLM和图像编辑工具的推理开销。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：硬件实验在物理机器人平台上进行，包含语言指令的操控任务，环境设置包括任务无关的干扰对象（如额外物体）和背景变化（如颜色改变）。</li><li><strong>评估指标</strong>：任务成功率（Task Success Rate），即完成指定语言指令的比例。实验比较了原始VLA、BYOVLA增强的VLA以及基线方法（如GradCAM）在干扰环境下的表现。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2024-09-29",
    "title": "RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2409.19590",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-09-19",
    "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2409.12514",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-09-05",
    "title": "OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving",
    "link": "http://arxiv.org/abs/2409.03272",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-08-19",
    "title": "CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving",
    "link": "http://arxiv.org/abs/2408.10845",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-07-25",
    "title": "Unified Lexical Representation for Interpretable Visual-Language Alignment",
    "link": "http://arxiv.org/abs/2407.17827",
    "summary_markdown": "好的，我将以论文阅读专家的身份，根据提供的Arxiv论文HTML原文，总结论文的要点。我将遵循你的要求，只提供Markdown格式文本，不使用加粗，不输出其他内容，并将总结分为你指定的几个部分。\n\n1. **论文研究单位**:\n * 我将从作者信息中提取研究机构。根据HTML，作者单位是复旦大学（Fudan University）和亚马逊云科技（Amazon Web Services）。\n\n2. **论文概述**:\n * 我将概括论文的背景、问题陈述和主要目标。这篇论文介绍了一个名为LexVLA的框架，用于视觉-语言对齐（VLA），旨在通过学习统一的词汇表示来解决现有方法（如CLIP）的可解释性差和训练复杂的问题。\n\n3. **论文核心贡献点**:\n * 我将从引言的“We contribute”部分提取贡献点。\n * 贡献1：利用单模态预训练模型（用于视觉的DINOv2，用于文本的Llama 2），利用它们独特的优势（如局部特征和上下文预测能力）进行VLA。\n * 贡献2：提出了一种统一的词汇表示方法，但为视觉和文本模态使用独立的码本，以避免削弱预训练能力。\n * 贡献3：引入了过度使用惩罚（overuse penalty），以鼓励稀疏嵌入并防止无意义激活。\n * 贡献4：在更少的多模态训练数据下实现了卓越的检索性能，并使用提出的PatchDis指标获得了更好的可解释性。\n\n4. **论文方法描述**:\n * 我将总结方法的关键步骤和组件，基于第3节“LexVLA”。\n * **词汇表示**: 使用共享词汇表但为视觉和文本模态设置不同的码本。码本初始化为Llama 2的输出码本；文本码本被冻结，视觉码本被微调。表示被稀疏化，通过阈值化（值大于1/√V）。\n * **词汇编码器**:\n * **文本编码器**: 使用Llama 2，并通过上下文提示（如“关键词提取”）使其执行词汇预测，而不是简单的标题嵌入。\n * **视觉编码器**: 使用DINOv2作为冻结的骨干网络。特征通过一个适配器（自注意力+2个MLP）处理，然后映射到视觉词汇表示。\n * **训练**: 使用标准的对比学习目标，并加入过度使用惩罚。训练采用增量微调：对于文本，使用LoRA；对于视觉，只训练适配器和映射器。\n\n5. **论文使用数据集和训练资源**:\n * 我将从“实验”部分（第4节）中提取数据集和计算细节。\n * **数据集**: 训练在CC-12M上进行。在Flickr30K和MSCOCO上进行零样本跨模态检索评估。\n * **训练资源**: 文本编码器基于Llama 2（7B），视觉骨干网络为DINOv2（ViT-L/14）。使用单个8x A100 GPU进行训练。提及使用LoRA进行文本微调以减少资源消耗。\n\n6. **论文使用的评估环境和评估指标**:\n * 我将从“实验”部分提取评估设置和指标。\n * **评估环境**: 标准零样本跨模态检索设置（图像到文本和文本到图像检索）。\n * **评估指标**: 在Flickr30K和MSCOCO上的检索指标（R@1, R@5, R@10, mean Recall）。此外，引入了一个新的可解释性指标“PatchDis”来量化模型在未经过细粒度任务训练的情况下的块级对齐能力。</think>\n### 论文研究单位\n复旦大学 和 亚马逊云科技\n### 论文概述\n本文提出了一种名为LexVLA的视觉-语言对齐框架，旨在通过学习统一的词汇表示来解决现有方法（如CLIP）存在的可解释性差和训练复杂的问题。传统CLIP的潜在特征对齐缺乏表示清晰度和相似度评分的明确性，而词汇表示虽然自然且可解释，但难以学习。LexVLA利用单模态预训练模型的独特优势，通过简单的训练配置，实现了更优的跨模态检索性能和模型可解释性。\n### 论文核心贡献点\n1. 强调利用单模态预训练模型进行视觉-语言对齐，以受益于其无法通过对比目标学习的独特属性。例如，使用DINOv2因其局部化特征和使用Llama 2因其上下文能力。\n2. 有效学习统一词汇表示，为视觉和语言模态采用独特的码本，以避免削弱预训练能力。\n3. 提出过度使用惩罚，以鼓励稀疏嵌入并防止无意义激活。\n4. 以更少的多模态训练数据，享受卓越的检索性能，并在全局监督信号下，实现更好的块级可解释的VLA模型，使用提出的PatchDis指标在量上超越CLIP风格和词汇方法。\n### 论文方法描述\nLexVLA包含三个核心部分：\n1. **词汇表示**：使用共享词汇表但为文本和图像模态设置不同的码本。码本初始化为Llama 2的输出码本；文本码本被冻结以保留其语义，视觉码本则被微调以适应视觉特征。词汇表示被限制为非负且l2归一化，并通过值阈值化（大于1/√V）实现稀疏性。\n2. **词汇编码器**：\n * **词汇文本编码器**：使用Llama 2作为基础。并非直接对文本标题嵌入，而是通过上下文提示（例如，“...的关键词是：”）使其执行词汇预测任务。输出标记的嵌入与文本码本计算点积注意力，经elu1p激活和l2归一化后得到全局词汇表示。\n * **词汇视觉编码器**：使用DINOv2作为冻结的视觉骨干网络。输入图像被展平为块，通过DINOv2提取特征序列。特征通过一个适配器（包含自注意力层和两个多层感知机）进行处理，然后映射到视觉词汇表示。\n3. **训练LexVLA**：采用标准的对比学习目标来对齐图像-文本对。训练中加入了过度使用惩罚项，以防止频繁激活无意义词汇。训练采用增量微调策略：对于文本，使用LoRA适配器微调Llama 2；对于视觉，冻结DINOv2骨干，仅训练适配器和映射器，视觉码本用文本码本初始化并进行微调。\n### 论文使用数据集和训练资源\n**数据集**：模型在CC-12M数据集上进行训练。在Flickr30K和MSCOCO数据集上进行零样本跨模态检索评估。\n**训练资源**：文本编码器基于Llama 2（7B参数），视觉骨干网络为DINOv2（ViT-L/14架构）。训练在单个8x A100 GPU上进行。对文本模型使用LoRA进行微调以减少计算资源消耗。\n### 论文使用的评估环境和评估指标\n**评估环境**：在标准的零样本跨模态检索设置下评估模型性能，包括图像到文本检索和文本到图像检索任务。\n**评估指标**：主要在Flickr30K和MSCOCO数据集上使用检索指标，包括R@1, R@5, R@10和平均召回率。此外，论文引入了一个新的可解释性评估指标PatchDis，用于在未经过细粒度任务（如分割或检测）训练的VLA模型上量化其块级对齐能力。",
    "summary_html": "<p>好的，我将以论文阅读专家的身份，根据提供的Arxiv论文HTML原文，总结论文的要点。我将遵循你的要求，只提供Markdown格式文本，不使用加粗，不输出其他内容，并将总结分为你指定的几个部分。</p>\n\n<ol><li><strong>论文研究单位</strong>:</li></ol>\n<p> * 我将从作者信息中提取研究机构。根据HTML，作者单位是复旦大学（Fudan University）和亚马逊云科技（Amazon Web Services）。</p>\n\n<ol><li><strong>论文概述</strong>:</li></ol>\n<p> * 我将概括论文的背景、问题陈述和主要目标。这篇论文介绍了一个名为LexVLA的框架，用于视觉-语言对齐（VLA），旨在通过学习统一的词汇表示来解决现有方法（如CLIP）的可解释性差和训练复杂的问题。</p>\n\n<ol><li><strong>论文核心贡献点</strong>:</li></ol>\n<p> * 我将从引言的“We contribute”部分提取贡献点。</p>\n<p> * 贡献1：利用单模态预训练模型（用于视觉的DINOv2，用于文本的Llama 2），利用它们独特的优势（如局部特征和上下文预测能力）进行VLA。</p>\n<p> * 贡献2：提出了一种统一的词汇表示方法，但为视觉和文本模态使用独立的码本，以避免削弱预训练能力。</p>\n<p> * 贡献3：引入了过度使用惩罚（overuse penalty），以鼓励稀疏嵌入并防止无意义激活。</p>\n<p> * 贡献4：在更少的多模态训练数据下实现了卓越的检索性能，并使用提出的PatchDis指标获得了更好的可解释性。</p>\n\n<ol><li><strong>论文方法描述</strong>:</li></ol>\n<p> * 我将总结方法的关键步骤和组件，基于第3节“LexVLA”。</p>\n<p> * <strong>词汇表示</strong>: 使用共享词汇表但为视觉和文本模态设置不同的码本。码本初始化为Llama 2的输出码本；文本码本被冻结，视觉码本被微调。表示被稀疏化，通过阈值化（值大于1/√V）。</p>\n<p> * <strong>词汇编码器</strong>:</p>\n<p> * <strong>文本编码器</strong>: 使用Llama 2，并通过上下文提示（如“关键词提取”）使其执行词汇预测，而不是简单的标题嵌入。</p>\n<p> * <strong>视觉编码器</strong>: 使用DINOv2作为冻结的骨干网络。特征通过一个适配器（自注意力+2个MLP）处理，然后映射到视觉词汇表示。</p>\n<p> * <strong>训练</strong>: 使用标准的对比学习目标，并加入过度使用惩罚。训练采用增量微调：对于文本，使用LoRA；对于视觉，只训练适配器和映射器。</p>\n\n<ol><li><strong>论文使用数据集和训练资源</strong>:</li></ol>\n<p> * 我将从“实验”部分（第4节）中提取数据集和计算细节。</p>\n<p> * <strong>数据集</strong>: 训练在CC-12M上进行。在Flickr30K和MSCOCO上进行零样本跨模态检索评估。</p>\n<p> * <strong>训练资源</strong>: 文本编码器基于Llama 2（7B），视觉骨干网络为DINOv2（ViT-L/14）。使用单个8x A100 GPU进行训练。提及使用LoRA进行文本微调以减少资源消耗。</p>\n\n<ol><li><strong>论文使用的评估环境和评估指标</strong>:</li></ol>\n<p> * 我将从“实验”部分提取评估设置和指标。</p>\n<p> * <strong>评估环境</strong>: 标准零样本跨模态检索设置（图像到文本和文本到图像检索）。</p>\n<p> * <strong>评估指标</strong>: 在Flickr30K和MSCOCO上的检索指标（R@1, R@5, R@10, mean Recall）。此外，引入了一个新的可解释性指标“PatchDis”来量化模型在未经过细粒度任务训练的情况下的块级对齐能力。</think></p>\n<h3>论文研究单位</h3>\n<p>复旦大学 和 亚马逊云科技</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为LexVLA的视觉-语言对齐框架，旨在通过学习统一的词汇表示来解决现有方法（如CLIP）存在的可解释性差和训练复杂的问题。传统CLIP的潜在特征对齐缺乏表示清晰度和相似度评分的明确性，而词汇表示虽然自然且可解释，但难以学习。LexVLA利用单模态预训练模型的独特优势，通过简单的训练配置，实现了更优的跨模态检索性能和模型可解释性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>强调利用单模态预训练模型进行视觉-语言对齐，以受益于其无法通过对比目标学习的独特属性。例如，使用DINOv2因其局部化特征和使用Llama 2因其上下文能力。</li><li>有效学习统一词汇表示，为视觉和语言模态采用独特的码本，以避免削弱预训练能力。</li><li>提出过度使用惩罚，以鼓励稀疏嵌入并防止无意义激活。</li><li>以更少的多模态训练数据，享受卓越的检索性能，并在全局监督信号下，实现更好的块级可解释的VLA模型，使用提出的PatchDis指标在量上超越CLIP风格和词汇方法。</li></ol>\n<h3>论文方法描述</h3>\n<p>LexVLA包含三个核心部分：</p>\n<ol><li><strong>词汇表示</strong>：使用共享词汇表但为文本和图像模态设置不同的码本。码本初始化为Llama 2的输出码本；文本码本被冻结以保留其语义，视觉码本则被微调以适应视觉特征。词汇表示被限制为非负且l2归一化，并通过值阈值化（大于1/√V）实现稀疏性。</li><li><strong>词汇编码器</strong>：</li></ol>\n<p> * <strong>词汇文本编码器</strong>：使用Llama 2作为基础。并非直接对文本标题嵌入，而是通过上下文提示（例如，“...的关键词是：”）使其执行词汇预测任务。输出标记的嵌入与文本码本计算点积注意力，经elu1p激活和l2归一化后得到全局词汇表示。</p>\n<p> * <strong>词汇视觉编码器</strong>：使用DINOv2作为冻结的视觉骨干网络。输入图像被展平为块，通过DINOv2提取特征序列。特征通过一个适配器（包含自注意力层和两个多层感知机）进行处理，然后映射到视觉词汇表示。</p>\n<ol><li><strong>训练LexVLA</strong>：采用标准的对比学习目标来对齐图像-文本对。训练中加入了过度使用惩罚项，以防止频繁激活无意义词汇。训练采用增量微调策略：对于文本，使用LoRA适配器微调Llama 2；对于视觉，冻结DINOv2骨干，仅训练适配器和映射器，视觉码本用文本码本初始化并进行微调。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>：模型在CC-12M数据集上进行训练。在Flickr30K和MSCOCO数据集上进行零样本跨模态检索评估。</p>\n<p><strong>训练资源</strong>：文本编码器基于Llama 2（7B参数），视觉骨干网络为DINOv2（ViT-L/14架构）。训练在单个8x A100 GPU上进行。对文本模型使用LoRA进行微调以减少计算资源消耗。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>：在标准的零样本跨模态检索设置下评估模型性能，包括图像到文本检索和文本到图像检索任务。</p>\n<p><strong>评估指标</strong>：主要在Flickr30K和MSCOCO数据集上使用检索指标，包括R@1, R@5, R@10和平均召回率。此外，论文引入了一个新的可解释性评估指标PatchDis，用于在未经过细粒度任务（如分割或检测）训练的VLA模型上量化其块级对齐能力。</p>"
  },
  {
    "date": "2024-07-11",
    "title": "Robotic Control via Embodied Chain-of-Thought Reasoning",
    "link": "http://arxiv.org/abs/2407.08693",
    "summary_markdown": "### 论文研究单位\nUC Berkeley, University of Warsaw, Stanford University\n### 论文概述\n论文提出了一种名为\"具身思维链推理\"（Embodied Chain-of-Thought Reasoning, ECoT）的方法，用于提升视觉-语言-动作模型（VLAs）在机器人控制中的泛化能力。传统VLAs直接从观测映射到动作，而ECoT通过在预测动作前引入多步文本推理（包括任务规划、子任务分解、运动描述和视觉特征定位），迫使模型在行动前\"仔细思考\"和\"仔细观察\"。实验表明，ECoT在无需额外机器人数据的情况下，将OpenVLA模型在挑战性泛化任务上的绝对成功率提高了28%，并增强了策略的可解释性和人机交互能力。\n### 论文核心贡献点\n1. 提出ECoT方法：将视觉-语言-动作模型训练为在预测动作前执行多步推理，结合高级语义推理（如子任务规划）和低级具身推理（如物体边界框和夹爪位置）。\n2. 设计可扩展的合成数据生成管道：利用预训练模型（如Prismatic VLM、Grounding DINO、Gemini）自动为大规模机器人数据集标注推理链，无需人工标注。\n3. 实验验证ECoT有效性：在Bridge V2数据集上训练的模型，在真实机器人泛化任务中成功率显著超越基线（包括OpenVLA、RT-2-X），并提升策略可解释性和交互式纠错能力。\n4. 推理效率优化：提出同步/异步执行策略，减少推理延迟，使ECoT实用化。\n### 论文方法描述\n1. **推理步骤设计**：\n - 高级推理：任务重述（TASK）、高层计划（PLAN）、当前子任务（SUBTASK）。\n - 具身推理：低级运动指令（MOVE，如\"左移\"）、夹爪像素位置（GRIPPER）、物体边界框（OBJECTS）。\n - 强制模型关注多模态输入，将推理与视觉和机器人状态对齐。\n2. **数据生成流程**：\n - 使用Prismatic VLM生成场景描述。\n - Grounding DINO检测物体边界框，关联文本片段。\n - 从机器人本体感觉计算运动原语（729种模板化动作）。\n - OWLv2和SAM检测夹爪位置，拟合投影矩阵。\n - Gemini整合信息生成完整推理链（计划、子任务、解释）。\n - 在Bridge V2的250万+转换数据上运行，耗时7天。\n3. **训练与推理**：\n - 将推理和动作离散为tokens，微调OpenVLA（基于Llama 2-7B和Prismatic视觉编码器）。\n - 推理时采用N步冻结（如每5步更新高级推理）或异步并行（双实例）加速。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - 主要训练数据：Bridge V2数据集（60k演示，250万+转换）。\n - 评估任务：自定义泛化任务集（空间关系、未见过物体、新指令），共314次试验。\n - 泛化测试包括分布内（ID）和分布外（OOD）视角场景。\n2. **训练资源**：\n - 基础模型：OpenVLA（Prismatic VLM + Llama 2-7B）。\n - 数据生成依赖：Prismatic-7B、Grounding DINO、Gemini 1.0、OWLv2、SAM。\n - 计算资源：未明确说明，但强调生成管线可扩展（7天处理Bridge V2）。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 机器人平台：6-DoF WidowX机械臂（Bridge V2标准配置）。\n - 传感器：单视角第三人称摄像头。\n - 场景：真实世界环境，控制相机角度、光照和背景。\n2. **评估指标**：\n - 任务成功率：主要指标，报告平均值±标准误（StdErr）。\n - 泛化能力：按任务类型分类评估（ID/OOD物体、空间关系、新指令）。\n - 可解释性：定性分析推理链与失败案例。\n - 交互纠错：单次语言干预后的成功率提升。\n - 推理效率：比较不同加速方法的吞吐量（相对加速比）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>UC Berkeley, University of Warsaw, Stanford University</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为\"具身思维链推理\"（Embodied Chain-of-Thought Reasoning, ECoT）的方法，用于提升视觉-语言-动作模型（VLAs）在机器人控制中的泛化能力。传统VLAs直接从观测映射到动作，而ECoT通过在预测动作前引入多步文本推理（包括任务规划、子任务分解、运动描述和视觉特征定位），迫使模型在行动前\"仔细思考\"和\"仔细观察\"。实验表明，ECoT在无需额外机器人数据的情况下，将OpenVLA模型在挑战性泛化任务上的绝对成功率提高了28%，并增强了策略的可解释性和人机交互能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出ECoT方法：将视觉-语言-动作模型训练为在预测动作前执行多步推理，结合高级语义推理（如子任务规划）和低级具身推理（如物体边界框和夹爪位置）。</li><li>设计可扩展的合成数据生成管道：利用预训练模型（如Prismatic VLM、Grounding DINO、Gemini）自动为大规模机器人数据集标注推理链，无需人工标注。</li><li>实验验证ECoT有效性：在Bridge V2数据集上训练的模型，在真实机器人泛化任务中成功率显著超越基线（包括OpenVLA、RT-2-X），并提升策略可解释性和交互式纠错能力。</li><li>推理效率优化：提出同步/异步执行策略，减少推理延迟，使ECoT实用化。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>推理步骤设计</strong>：</li></ol>\n<p> - 高级推理：任务重述（TASK）、高层计划（PLAN）、当前子任务（SUBTASK）。</p>\n<p> - 具身推理：低级运动指令（MOVE，如\"左移\"）、夹爪像素位置（GRIPPER）、物体边界框（OBJECTS）。</p>\n<p> - 强制模型关注多模态输入，将推理与视觉和机器人状态对齐。</p>\n<ol><li><strong>数据生成流程</strong>：</li></ol>\n<p> - 使用Prismatic VLM生成场景描述。</p>\n<p> - Grounding DINO检测物体边界框，关联文本片段。</p>\n<p> - 从机器人本体感觉计算运动原语（729种模板化动作）。</p>\n<p> - OWLv2和SAM检测夹爪位置，拟合投影矩阵。</p>\n<p> - Gemini整合信息生成完整推理链（计划、子任务、解释）。</p>\n<p> - 在Bridge V2的250万+转换数据上运行，耗时7天。</p>\n<ol><li><strong>训练与推理</strong>：</li></ol>\n<p> - 将推理和动作离散为tokens，微调OpenVLA（基于Llama 2-7B和Prismatic视觉编码器）。</p>\n<p> - 推理时采用N步冻结（如每5步更新高级推理）或异步并行（双实例）加速。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：</li></ol>\n<p> - 主要训练数据：Bridge V2数据集（60k演示，250万+转换）。</p>\n<p> - 评估任务：自定义泛化任务集（空间关系、未见过物体、新指令），共314次试验。</p>\n<p> - 泛化测试包括分布内（ID）和分布外（OOD）视角场景。</p>\n<ol><li><strong>训练资源</strong>：</li></ol>\n<p> - 基础模型：OpenVLA（Prismatic VLM + Llama 2-7B）。</p>\n<p> - 数据生成依赖：Prismatic-7B、Grounding DINO、Gemini 1.0、OWLv2、SAM。</p>\n<p> - 计算资源：未明确说明，但强调生成管线可扩展（7天处理Bridge V2）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：</li></ol>\n<p> - 机器人平台：6-DoF WidowX机械臂（Bridge V2标准配置）。</p>\n<p> - 传感器：单视角第三人称摄像头。</p>\n<p> - 场景：真实世界环境，控制相机角度、光照和背景。</p>\n<ol><li><strong>评估指标</strong>：</li></ol>\n<p> - 任务成功率：主要指标，报告平均值±标准误（StdErr）。</p>\n<p> - 泛化能力：按任务类型分类评估（ID/OOD物体、空间关系、新指令）。</p>\n<p> - 可解释性：定性分析推理链与失败案例。</p>\n<p> - 交互纠错：单次语言干预后的成功率提升。</p>\n<p> - 推理效率：比较不同加速方法的吞吐量（相对加速比）。</p>"
  },
  {
    "date": "2024-07-10",
    "title": "Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs",
    "link": "http://arxiv.org/abs/2407.07775",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-06-27",
    "title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents",
    "link": "http://arxiv.org/abs/2407.00114",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-06-28",
    "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
    "link": "http://arxiv.org/abs/2406.20095",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-06-21",
    "title": "Learning Efficient and Robust Language-conditioned Manipulation using Textual-Visual Relevancy and Equivariant Language Mapping",
    "link": "http://arxiv.org/abs/2406.15677",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-06-13",
    "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2406.09246",
    "summary_markdown": "### 论文研究单位\n斯坦福大学、UC Berkeley、丰田研究院、Google Deepmind、Physical Intelligence、MIT。\n### 论文概述\n提出OpenVLA，一个7B参数的开源视觉-语言-动作模型（VLA），在970k真实机器人演示数据上训练。基于Llama 2语言模型，融合DINOv2和SigLIP视觉编码器，旨在解决现有VLAs闭源和微调效率低的问题。\n### 论文核心贡献点\n1. 开源高性能VLA模型，在29个任务上超越RT-2-X（55B参数）16.5%成功率。\n2. 首次系统探索VLA高效微调方法，支持LoRA参数微调。\n3. 实现量化推理，4-bit量化保持性能不变，内存减半。\n4. 发布完整代码库和训练基础设施，支持大规模训练。\n5. 在多机器人泛化和语言条件任务上表现最优。\n### 论文方法描述\n架构：采用Prismatic VLM框架，包含视觉编码器（DINOv2+SigLIP特征融合）、投影层（2层MLP）和LLM主干（Llama 2）。动作通过离散化为256个bin映射到语言token，训练目标为下一token预测。关键设计：微调视觉编码器、输入分辨率224×224px、训练27个epoch、固定学习率2e-5。\n### 论文使用数据集和训练资源\n数据集：Open X-Embodiment数据集，过滤后包含970k轨迹，覆盖多机器人形态。训练资源：64个A100 GPU训练14天，总计算量21,500 A100小时，批大小2048，采用FSDP并行。\n### 论文使用的评估环境和评估指标\n环境：真实机器人平台包括WidowX（BridgeData V2任务）和Google Robot（移动操作任务），模拟环境LIBERO。指标：任务成功率（success rate），测试视觉、运动、物理和语义泛化维度，以及多对象语言条件能力。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>斯坦福大学、UC Berkeley、丰田研究院、Google Deepmind、Physical Intelligence、MIT。</p>\n<h3>论文概述</h3>\n<p>提出OpenVLA，一个7B参数的开源视觉-语言-动作模型（VLA），在970k真实机器人演示数据上训练。基于Llama 2语言模型，融合DINOv2和SigLIP视觉编码器，旨在解决现有VLAs闭源和微调效率低的问题。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>开源高性能VLA模型，在29个任务上超越RT-2-X（55B参数）16.5%成功率。</li><li>首次系统探索VLA高效微调方法，支持LoRA参数微调。</li><li>实现量化推理，4-bit量化保持性能不变，内存减半。</li><li>发布完整代码库和训练基础设施，支持大规模训练。</li><li>在多机器人泛化和语言条件任务上表现最优。</li></ol>\n<h3>论文方法描述</h3>\n<p>架构：采用Prismatic VLM框架，包含视觉编码器（DINOv2+SigLIP特征融合）、投影层（2层MLP）和LLM主干（Llama 2）。动作通过离散化为256个bin映射到语言token，训练目标为下一token预测。关键设计：微调视觉编码器、输入分辨率224×224px、训练27个epoch、固定学习率2e-5。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：Open X-Embodiment数据集，过滤后包含970k轨迹，覆盖多机器人形态。训练资源：64个A100 GPU训练14天，总计算量21,500 A100小时，批大小2048，采用FSDP并行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>环境：真实机器人平台包括WidowX（BridgeData V2任务）和Google Robot（移动操作任务），模拟环境LIBERO。指标：任务成功率（success rate），测试视觉、运动、物理和语义泛化维度，以及多对象语言条件能力。</p>"
  },
  {
    "date": "2024-06-06",
    "title": "RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation",
    "link": "http://arxiv.org/abs/2406.04339",
    "summary_markdown": "### 论文研究单位\n北京大学、AI²Robotics、北京智源人工智能研究院（BAAI）\n### 论文概述\n该论文提出了RoboMamba，一个高效的视觉-语言-行动（VLA）模型，用于机器人推理和操作。RoboMamba基于Mamba架构，旨在解决现有VLA模型在复杂任务推理能力不足和计算成本高的问题。模型集成了视觉编码器和Mamba语言模型，通过两阶段训练策略，使其同时具备通用常识、机器人相关推理能力和低级操作技能，并保持了高效的微调和推理速度。\n### 论文核心贡献点\n1. 提出RoboMamba，一个高效的端到端机器人VLA模型，它将视觉编码器与线性复杂度的Mamba语言模型相结合。\n2. 设计了一个高效的微调策略，仅通过微调一个简单的策略头（约占模型总参数的0.1%）即可赋予模型SE(3)姿态预测能力。\n3. 实验表明，RoboMamba在多个通用和机器人推理基准上表现出色，并在模拟和真实世界实验中展示了令人印象深刻的姿态预测结果，推理速度是现有VLA模型的3倍。\n### 论文方法描述\nRoboMamba的架构包括CLIP视觉编码器、一个多层感知机（MLP）作为跨模态连接器，以及一个Mamba语言模型。训练分为两个阶段：\n1. **阶段一：通用和机器人相关推理训练**\n - **1.1 对齐预训练**：冻结CLIP编码器和Mamba，仅使用558K图像-文本对数据集训练MLP连接器，对齐视觉特征与语言嵌入。\n - **1.2 指令协同训练**：解冻Mamba和连接器，结合通用指令数据集（如LLaVA混合指令数据集）和机器人指令数据集（如RoboVQA）进行训练，提升模型的视觉常识和机器人相关推理能力。\n2. **阶段二：机器人操作微调**\n - 冻结RoboMamba的所有参数，仅添加一个简单的策略头（包含两个MLP，分别用于预测末端执行器的位置和方向），该策略头仅占模型总参数的0.1%。\n - 使用SAPIEN模拟器生成的10K末端执行器姿态预测数据集进行微调，采用位置L1损失和方向反余弦损失。该过程仅需几分钟即可在单个A100 GPU上完成。\n### 论文使用数据集和训练资源\n**数据集**:\n- **阶段一**：LLaVA-LCS 558K（对齐预训练）、LLaVA混合指令数据集、ShareGPT4V-SFT数据集、LLaVA-Next数据集（指令协同训练）、300K RoboVQA样本（指令协同训练）。\n- **阶段二**：使用SAPIEN模拟器和PartNet-Mobility数据集收集的10K末端执行器姿态预测数据，用于操作微调。\n- **评估**：推理评估在VQAv2、OKVQA、RoboVQA、GQA等多个基准上进行；操作评估在SAPIEN模拟器中进行，测试集包含1.1K样本（20个已见任务和10个未见任务）。\n**训练资源**:\n- 所有实验均在NVIDIA A100 GPU上进行。\n- 阶段一训练使用AdamW优化器，学习率4e-5，混合精度（fp16）。\n- 阶段二微调学习率1e-5，权重衰减0.1，使用fp32精度。\n### 论文使用的评估环境和评估指标\n**评估环境**:\n- **模拟环境**：使用SAPIEN物理模拟引擎，与PartNet-Mobility数据集中的可动物体进行交互。机器人模型为配备吸盘夹爪的Franka Panda机器人。\n- **真实世界环境**：使用真实的Franka Panda机器人对多个可动物体进行操作。\n**评估指标**:\n- **推理能力**：在多个视觉问答基准上使用标准指标（如BLEU-4用于RoboVQA）进行评估。\n- **操作能力**：\n - **成功率**：在模拟环境中，成功操作样本数占总测试样本数的比例。一个操作被视为成功，当且仅当物体交互前后的关节状态变化超过0.1米的阈值。\n - **位置误差（L1 loss）**：预测位置与真实位置之间的平均绝对误差。\n - **方向误差**：预测旋转矩阵与真实旋转矩阵之间的反余弦误差。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学、AI²Robotics、北京智源人工智能研究院（BAAI）</p>\n<h3>论文概述</h3>\n<p>该论文提出了RoboMamba，一个高效的视觉-语言-行动（VLA）模型，用于机器人推理和操作。RoboMamba基于Mamba架构，旨在解决现有VLA模型在复杂任务推理能力不足和计算成本高的问题。模型集成了视觉编码器和Mamba语言模型，通过两阶段训练策略，使其同时具备通用常识、机器人相关推理能力和低级操作技能，并保持了高效的微调和推理速度。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出RoboMamba，一个高效的端到端机器人VLA模型，它将视觉编码器与线性复杂度的Mamba语言模型相结合。</li><li>设计了一个高效的微调策略，仅通过微调一个简单的策略头（约占模型总参数的0.1%）即可赋予模型SE(3)姿态预测能力。</li><li>实验表明，RoboMamba在多个通用和机器人推理基准上表现出色，并在模拟和真实世界实验中展示了令人印象深刻的姿态预测结果，推理速度是现有VLA模型的3倍。</li></ol>\n<h3>论文方法描述</h3>\n<p>RoboMamba的架构包括CLIP视觉编码器、一个多层感知机（MLP）作为跨模态连接器，以及一个Mamba语言模型。训练分为两个阶段：</p>\n<ol><li><strong>阶段一：通用和机器人相关推理训练</strong></li></ol>\n<p> - <strong>1.1 对齐预训练</strong>：冻结CLIP编码器和Mamba，仅使用558K图像-文本对数据集训练MLP连接器，对齐视觉特征与语言嵌入。</p>\n<p> - <strong>1.2 指令协同训练</strong>：解冻Mamba和连接器，结合通用指令数据集（如LLaVA混合指令数据集）和机器人指令数据集（如RoboVQA）进行训练，提升模型的视觉常识和机器人相关推理能力。</p>\n<ol><li><strong>阶段二：机器人操作微调</strong></li></ol>\n<p> - 冻结RoboMamba的所有参数，仅添加一个简单的策略头（包含两个MLP，分别用于预测末端执行器的位置和方向），该策略头仅占模型总参数的0.1%。</p>\n<p> - 使用SAPIEN模拟器生成的10K末端执行器姿态预测数据集进行微调，采用位置L1损失和方向反余弦损失。该过程仅需几分钟即可在单个A100 GPU上完成。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>:</p>\n<ul><li><strong>阶段一</strong>：LLaVA-LCS 558K（对齐预训练）、LLaVA混合指令数据集、ShareGPT4V-SFT数据集、LLaVA-Next数据集（指令协同训练）、300K RoboVQA样本（指令协同训练）。</li><li><strong>阶段二</strong>：使用SAPIEN模拟器和PartNet-Mobility数据集收集的10K末端执行器姿态预测数据，用于操作微调。</li><li><strong>评估</strong>：推理评估在VQAv2、OKVQA、RoboVQA、GQA等多个基准上进行；操作评估在SAPIEN模拟器中进行，测试集包含1.1K样本（20个已见任务和10个未见任务）。</li></ul>\n<p><strong>训练资源</strong>:</p>\n<ul><li>所有实验均在NVIDIA A100 GPU上进行。</li><li>阶段一训练使用AdamW优化器，学习率4e-5，混合精度（fp16）。</li><li>阶段二微调学习率1e-5，权重衰减0.1，使用fp32精度。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>:</p>\n<ul><li><strong>模拟环境</strong>：使用SAPIEN物理模拟引擎，与PartNet-Mobility数据集中的可动物体进行交互。机器人模型为配备吸盘夹爪的Franka Panda机器人。</li><li><strong>真实世界环境</strong>：使用真实的Franka Panda机器人对多个可动物体进行操作。</li></ul>\n<p><strong>评估指标</strong>:</p>\n<ul><li><strong>推理能力</strong>：在多个视觉问答基准上使用标准指标（如BLEU-4用于RoboVQA）进行评估。</li><li><strong>操作能力</strong>：</li></ul>\n<p> - <strong>成功率</strong>：在模拟环境中，成功操作样本数占总测试样本数的比例。一个操作被视为成功，当且仅当物体交互前后的关节状态变化超过0.1米的阈值。</p>\n<p> - <strong>位置误差（L1 loss）</strong>：预测位置与真实位置之间的平均绝对误差。</p>\n<p> - <strong>方向误差</strong>：预测旋转矩阵与真实旋转矩阵之间的反余弦误差。</p>"
  },
  {
    "date": "2024-05-31",
    "title": "Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations",
    "link": "http://arxiv.org/abs/2406.00121",
    "summary_markdown": "### 论文研究单位\nCUHK, ByteDance Inc., University of California, Merced\n### 论文概述\n本文旨在解决现有基于文本的图像编辑方法在处理用户模糊、高层次的编辑意图（称为\"编辑提示\"）时效果不佳的问题，这种情况下存在从抽象意图到具体指令的\"创造力差距\"。为此，论文提出了一个新任务——图像编辑推荐（IER），其目标是从一张输入图像和一个简单的编辑提示出发，自动生成一组多样化且具创意的编辑指令。为了实现该任务，作者构建了一个名为Creativity-Vision Language Assistant (Creativity-VLA)的多模态框架，并引入了一个独特的\"token-for-localization\"机制，以同时支持全局和局部编辑推荐。实验证明，该模型能生成既富有创意又与图像内容和用户初始提示高度相关的编辑建议。\n### 论文核心贡献点\n1. 提出了图像编辑推荐（IER）任务，旨在将用户模糊的编辑意图转化为多样化且可执行的创意编辑指令。\n2. 构建了一个包含16,000个样本的专用创意指令数据集，该数据集通过结合GPT-4的思维链（CoT）提示技术与人工校对流程创建，旨在激发模型的发散性想象。\n3. 开发了Creativity-VLA模型，这是一个基于LLaVA的视觉语言助手，它将模型的输出解耦为文本建议和编辑位置，并引入\"token-for-localization\"机制，使模型能够为编辑指令推荐具体的图像区域，从而支持全局和局部两种编辑模式。\n### 论文方法描述\nCreativity-VLA模型基于LLaVA-7B构建。其核心创新在于将模型的输出解耦为两个部分：文本编辑建议（`O_sug`）和编辑位置（`O_loc`）。为实现这一点，模型在词汇表中引入了一个特殊标记`<EDIT>`。在生成过程中，模型首先输出文本建议，然后输出`<EDIT>`标记。该标记的嵌入向量会输入一个专门的定位解码器，该解码器由一个MLP投影层和一个三层Transformer组成。定位解码器将`<EDIT>`标记的嵌入与来自CLIP视觉编码器的图像特征进行交叉注意力计算，最终预测出执行编辑的边界框位置。模型采用端到端方式训练，总损失函数由文本生成的交叉熵损失和定位的L1损失与GIoU损失加权组成，其中视觉编码器参数被冻结。\n### 论文使用数据集和训练资源\n论文构建了一个包含16,000个编辑指令的内部数据集，内容涵盖人类、动物、室内外场景和商品特写等，其中全局和局部指令各占50%。数据集的构建分为四步：1）使用RAM、Grounding-DINO、SAM和BLIP-2进行详细的视觉理解；2）利用带思维链提示的GPT-4，根据编辑提示（如\"奢华\"）联想相关概念；3）提示GPT-4生成JSON格式的编辑指令；4）进行去重和人工整理，确保指令的有效性。训练时，模型基于LLaVA-7B架构，在自定义数据集上进行了3个epoch的微调。\n### 论文使用的评估环境和评估指标\n评估环境方面，论文使用InstructDiffusion作为全局编辑工具，GLIGEN作为局部编辑工具，以执行生成的编辑指令。对比基线包括直接编辑方法（MagicBrush, InstructDiffusion）和指令生成方法（LLaVA-v1.5, GPT-4V）。评估指标主要采用用户偏好评估，即由人工用户在提示对齐度、图像对齐度、视觉质量和多样性四个维度上对不同方法生成的编辑结果进行评分（分数越低表示越好）。此外，还使用CLIP分数作为辅助的自动化评估指标，用于衡量生成图像与文本指令之间的一致性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>CUHK, ByteDance Inc., University of California, Merced</p>\n<h3>论文概述</h3>\n<p>本文旨在解决现有基于文本的图像编辑方法在处理用户模糊、高层次的编辑意图（称为\"编辑提示\"）时效果不佳的问题，这种情况下存在从抽象意图到具体指令的\"创造力差距\"。为此，论文提出了一个新任务——图像编辑推荐（IER），其目标是从一张输入图像和一个简单的编辑提示出发，自动生成一组多样化且具创意的编辑指令。为了实现该任务，作者构建了一个名为Creativity-Vision Language Assistant (Creativity-VLA)的多模态框架，并引入了一个独特的\"token-for-localization\"机制，以同时支持全局和局部编辑推荐。实验证明，该模型能生成既富有创意又与图像内容和用户初始提示高度相关的编辑建议。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了图像编辑推荐（IER）任务，旨在将用户模糊的编辑意图转化为多样化且可执行的创意编辑指令。</li><li>构建了一个包含16,000个样本的专用创意指令数据集，该数据集通过结合GPT-4的思维链（CoT）提示技术与人工校对流程创建，旨在激发模型的发散性想象。</li><li>开发了Creativity-VLA模型，这是一个基于LLaVA的视觉语言助手，它将模型的输出解耦为文本建议和编辑位置，并引入\"token-for-localization\"机制，使模型能够为编辑指令推荐具体的图像区域，从而支持全局和局部两种编辑模式。</li></ol>\n<h3>论文方法描述</h3>\n<p>Creativity-VLA模型基于LLaVA-7B构建。其核心创新在于将模型的输出解耦为两个部分：文本编辑建议（<code>O_sug</code>）和编辑位置（<code>O_loc</code>）。为实现这一点，模型在词汇表中引入了一个特殊标记<code><EDIT></code>。在生成过程中，模型首先输出文本建议，然后输出<code><EDIT></code>标记。该标记的嵌入向量会输入一个专门的定位解码器，该解码器由一个MLP投影层和一个三层Transformer组成。定位解码器将<code><EDIT></code>标记的嵌入与来自CLIP视觉编码器的图像特征进行交叉注意力计算，最终预测出执行编辑的边界框位置。模型采用端到端方式训练，总损失函数由文本生成的交叉熵损失和定位的L1损失与GIoU损失加权组成，其中视觉编码器参数被冻结。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>论文构建了一个包含16,000个编辑指令的内部数据集，内容涵盖人类、动物、室内外场景和商品特写等，其中全局和局部指令各占50%。数据集的构建分为四步：1）使用RAM、Grounding-DINO、SAM和BLIP-2进行详细的视觉理解；2）利用带思维链提示的GPT-4，根据编辑提示（如\"奢华\"）联想相关概念；3）提示GPT-4生成JSON格式的编辑指令；4）进行去重和人工整理，确保指令的有效性。训练时，模型基于LLaVA-7B架构，在自定义数据集上进行了3个epoch的微调。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境方面，论文使用InstructDiffusion作为全局编辑工具，GLIGEN作为局部编辑工具，以执行生成的编辑指令。对比基线包括直接编辑方法（MagicBrush, InstructDiffusion）和指令生成方法（LLaVA-v1.5, GPT-4V）。评估指标主要采用用户偏好评估，即由人工用户在提示对齐度、图像对齐度、视觉质量和多样性四个维度上对不同方法生成的编辑结果进行评分（分数越低表示越好）。此外，还使用CLIP分数作为辅助的自动化评估指标，用于衡量生成图像与文本指令之间的一致性。</p>"
  },
  {
    "date": "2024-05-27",
    "title": "A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation",
    "link": "http://arxiv.org/abs/2405.17418",
    "summary_markdown": "```markdown\n### 论文研究单位\n北京大学计算机学院，多媒体信息处理国家重点实验室\n### 论文概述\n本文提出了一种自校正视觉-语言-动作模型（SC-VLA），用于机器人的系统操作。该框架模仿人类思维的快系统和慢系统：快系统直接预测末端执行器的SE(3)位姿以实现快速决策，慢系统通过思维链训练策略反思和纠正失败动作。此外，引入连续策略学习方法，将成功纠正的样本用于增强快系统的适应性，减少专家干预。模型在仿真和真实任务中均验证了其有效性和鲁棒性。\n### 论文核心贡献点\n1. 提出了SC-VLA框架，集成了快系统的直接动作预测能力和慢系统的反思纠错能力。\n2. 为慢系统设计了思维链训练策略，使模型能检测失败原因、请求专家反馈并逐步生成校正动作。\n3. 引入了连续策略学习方法，利用成功纠正的样本通过指数移动平均技术提升快系统的稳定性，降低对专家干预的依赖。\n### 论文方法描述\n1. **快系统**：使用LLaMA-Adapter V2作为基础模型，通过参数高效微调将位姿预测转化为语言建模问题，保留多模态大语言模型的推理能力。输入包括RGB图像和任务描述，输出为离散化的6-DoF位姿。\n2. **慢系统**：基于结束状态图像和机器人状态识别失败类型（位置、旋转或组合错误），并动态请求专家反馈：\n - 位置专家（如Where2Act）生成可供性图。\n - 旋转专家（如Anygrasp）预测潜在旋转。\n - 推理专家（如GPT-4V）细化接触点。\n 通过思维链推理整合错误原因和专家反馈，重新生成动作。\n3. **连续策略学习**：使用指数移动平均更新模型参数（α=0.999），结合成功纠正样本微调适配器，实现知识从慢系统到快系统的转移。\n### 论文使用数据集和训练资源\n1. **数据集**：在SAPIEN仿真器中生成12k成功样本、15k失败样本和60k校正提示；使用PartNet-Mobility数据集进行开环实验；RLBench任务收集100个episodes per task用于闭环训练。\n2. **训练资源**：基于80GB A100 GPU，使用Adam优化器（学习率2e-5）训练10个epochs，耗时约3小时。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：开环实验在SAPIEN仿真器中测试，使用PartNet-Mobility数据集；闭环实验在RLBench环境中进行；真实世界实验部署于Franka Panda机械臂，进行sim-to-real迁移测试。\n2. **评估指标**：任务成功率（success rate），基于预定义条件（如任务完成状态）评估，包括开环位姿准确率和闭环任务成功率。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>北京大学计算机学院，多媒体信息处理国家重点实验室</p>\n<h3>论文概述</h3>\n<p>本文提出了一种自校正视觉-语言-动作模型（SC-VLA），用于机器人的系统操作。该框架模仿人类思维的快系统和慢系统：快系统直接预测末端执行器的SE(3)位姿以实现快速决策，慢系统通过思维链训练策略反思和纠正失败动作。此外，引入连续策略学习方法，将成功纠正的样本用于增强快系统的适应性，减少专家干预。模型在仿真和真实任务中均验证了其有效性和鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了SC-VLA框架，集成了快系统的直接动作预测能力和慢系统的反思纠错能力。</li><li>为慢系统设计了思维链训练策略，使模型能检测失败原因、请求专家反馈并逐步生成校正动作。</li><li>引入了连续策略学习方法，利用成功纠正的样本通过指数移动平均技术提升快系统的稳定性，降低对专家干预的依赖。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>快系统</strong>：使用LLaMA-Adapter V2作为基础模型，通过参数高效微调将位姿预测转化为语言建模问题，保留多模态大语言模型的推理能力。输入包括RGB图像和任务描述，输出为离散化的6-DoF位姿。</li><li><strong>慢系统</strong>：基于结束状态图像和机器人状态识别失败类型（位置、旋转或组合错误），并动态请求专家反馈：</li></ol>\n<p> - 位置专家（如Where2Act）生成可供性图。</p>\n<p> - 旋转专家（如Anygrasp）预测潜在旋转。</p>\n<p> - 推理专家（如GPT-4V）细化接触点。</p>\n<p> 通过思维链推理整合错误原因和专家反馈，重新生成动作。</p>\n<ol><li><strong>连续策略学习</strong>：使用指数移动平均更新模型参数（α=0.999），结合成功纠正样本微调适配器，实现知识从慢系统到快系统的转移。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：在SAPIEN仿真器中生成12k成功样本、15k失败样本和60k校正提示；使用PartNet-Mobility数据集进行开环实验；RLBench任务收集100个episodes per task用于闭环训练。</li><li><strong>训练资源</strong>：基于80GB A100 GPU，使用Adam优化器（学习率2e-5）训练10个epochs，耗时约3小时。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：开环实验在SAPIEN仿真器中测试，使用PartNet-Mobility数据集；闭环实验在RLBench环境中进行；真实世界实验部署于Franka Panda机械臂，进行sim-to-real迁移测试。</li><li><strong>评估指标</strong>：任务成功率（success rate），基于预定义条件（如任务完成状态）评估，包括开环位姿准确率和闭环任务成功率。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2024-05-23",
    "title": "A Survey on Vision-Language-Action Models for Embodied AI",
    "link": "http://arxiv.org/abs/2405.14093",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-05-09",
    "title": "Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations",
    "link": "http://arxiv.org/abs/2405.06039",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  }
]