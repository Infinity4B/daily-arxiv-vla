[
  {
    "date": "2026-01-05",
    "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding",
    "link": "http://arxiv.org/abs/2601.02295",
    "summary_markdown": "论文研究单位：\n牛津大学计算机科学系，剑桥大学工程系\n\n论文概述：\n本文提出了cyclevla，一个使视觉语言动作模型具备主动自我纠正能力的系统。针对现有机器人失败检测与纠正方法大多是事后进行的问题，cyclevla通过结合具有子任务进度感知能力的vla、基于vlm的失败预测与规划器，以及基于最小贝叶斯风险解码的测试时扩展策略，实现了在失败完全显现前进行预测和恢复的主动自我纠正。系统利用了机器人任务失败多发生在子任务边界这一观察，在子任务完成阶段（例如抓取或插入即将完成时）进行失败预测和回溯恢复。\n\n论文核心贡献点：\n1. 提出了cyclevla系统，通过子任务回溯和mbr解码，为vla模型赋予了主动自我纠正能力。该系统包含：(a) 赋予vla子任务进度预测能力的微调流程；(b) 基于vlm的失败预测和规划器，利用进度线索决定在子任务间是继续执行还是回溯；(c) 基于mbr的测试时扩展策略，提高回溯后重试的成功率。\n2. 证明了mbr解码可以作为提高vla策略成功率的零样本测试时扩展策略。\n3. 实验表明，cyclevla有效提升了机器人基准测试中的任务成功率，并且对于训练不足的vla模型也保持有效。\n\n论文方法描述：\ncyclevla包含三个主要部分：\n1. 子任务分解与进度感知vla训练：使用llm将任务指令分解为原子子任务序列，并结合夹爪状态检测和运动原语提取，与演示轨迹对齐，构建子任务分解数据集。在此基础上微调vla，将其动作维度从7维扩展至9维，增加了停止信号和进度信号两个维度。停止信号指示子任务终止，进度信号指示接近完成的程度。\n2. 基于vlm的失败预测与规划：当vla预测的进度达到阈值时，系统查询一个现成的vlm（提供第三人称和腕部摄像头视图、当前子任务和子任务列表）。vlm输出决策：继续执行下一个子任务，或回溯到能够恢复缺失前提条件的最早子任务（例如，物体中途掉落则回溯到抓取子任务）。当触发回溯时，系统通过反向执行记录的增量动作将机器人状态恢复到目标子任务的开始状态。\n3. 基于mbr解码的测试时扩展：回溯后，vla从相同起始状态重试子任务。系统从随机策略中采样多个动作块假设，并使用mbr解码选择共识轨迹。mbr选择在策略输出空间中具有高密度区域的轨迹，即平均成对距离最小的轨迹。\n\n论文使用数据集和训练资源：\n1. 数据集：在libero仿真基准上进行评估，使用了其包含的四个任务套件：spatial, object, goal, long。通过llm（gpt-4.1）对演示进行子任务分解和边界推断来构建子任务分解数据集。\n2. 训练资源：使用4张nvidia a100 gpu（40gb vram）进行模型训练。vla主干采用openvla（基于扩散的动作专家模型）。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：libero仿真环境。在单个nvidia a10 gpu（24gb vram）上进行评估。\n2. 评估指标：任务成功率。对于每个任务套件，在50次运行、3个随机种子下进行评估。使用mbr解码时，通过估计的成功概率来评估其有效性，即从多个随机采样的假设中选择成功动作块的概率。此外，分析了不同假设数量（n）和距离度量对mbr性能的影响，以及测试时扩展的运行时开销（端到端运行时间及各组件占比）。",
    "summary_html": "<p>论文研究单位：</p>\n<p>牛津大学计算机科学系，剑桥大学工程系</p>\n\n<p>论文概述：</p>\n<p>本文提出了cyclevla，一个使视觉语言动作模型具备主动自我纠正能力的系统。针对现有机器人失败检测与纠正方法大多是事后进行的问题，cyclevla通过结合具有子任务进度感知能力的vla、基于vlm的失败预测与规划器，以及基于最小贝叶斯风险解码的测试时扩展策略，实现了在失败完全显现前进行预测和恢复的主动自我纠正。系统利用了机器人任务失败多发生在子任务边界这一观察，在子任务完成阶段（例如抓取或插入即将完成时）进行失败预测和回溯恢复。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了cyclevla系统，通过子任务回溯和mbr解码，为vla模型赋予了主动自我纠正能力。该系统包含：(a) 赋予vla子任务进度预测能力的微调流程；(b) 基于vlm的失败预测和规划器，利用进度线索决定在子任务间是继续执行还是回溯；(c) 基于mbr的测试时扩展策略，提高回溯后重试的成功率。</li><li>证明了mbr解码可以作为提高vla策略成功率的零样本测试时扩展策略。</li><li>实验表明，cyclevla有效提升了机器人基准测试中的任务成功率，并且对于训练不足的vla模型也保持有效。</li></ol>\n\n<p>论文方法描述：</p>\n<p>cyclevla包含三个主要部分：</p>\n<ol><li>子任务分解与进度感知vla训练：使用llm将任务指令分解为原子子任务序列，并结合夹爪状态检测和运动原语提取，与演示轨迹对齐，构建子任务分解数据集。在此基础上微调vla，将其动作维度从7维扩展至9维，增加了停止信号和进度信号两个维度。停止信号指示子任务终止，进度信号指示接近完成的程度。</li><li>基于vlm的失败预测与规划：当vla预测的进度达到阈值时，系统查询一个现成的vlm（提供第三人称和腕部摄像头视图、当前子任务和子任务列表）。vlm输出决策：继续执行下一个子任务，或回溯到能够恢复缺失前提条件的最早子任务（例如，物体中途掉落则回溯到抓取子任务）。当触发回溯时，系统通过反向执行记录的增量动作将机器人状态恢复到目标子任务的开始状态。</li><li>基于mbr解码的测试时扩展：回溯后，vla从相同起始状态重试子任务。系统从随机策略中采样多个动作块假设，并使用mbr解码选择共识轨迹。mbr选择在策略输出空间中具有高密度区域的轨迹，即平均成对距离最小的轨迹。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>数据集：在libero仿真基准上进行评估，使用了其包含的四个任务套件：spatial, object, goal, long。通过llm（gpt-4.1）对演示进行子任务分解和边界推断来构建子任务分解数据集。</li><li>训练资源：使用4张nvidia a100 gpu（40gb vram）进行模型训练。vla主干采用openvla（基于扩散的动作专家模型）。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：libero仿真环境。在单个nvidia a10 gpu（24gb vram）上进行评估。</li><li>评估指标：任务成功率。对于每个任务套件，在50次运行、3个随机种子下进行评估。使用mbr解码时，通过估计的成功概率来评估其有效性，即从多个随机采样的假设中选择成功动作块的概率。此外，分析了不同假设数量（n）和距离度量对mbr性能的影响，以及测试时扩展的运行时开销（端到端运行时间及各组件占比）。</li></ol>"
  },
  {
    "date": "2026-01-04",
    "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation",
    "link": "http://arxiv.org/abs/2601.01618",
    "summary_markdown": "论文研究单位：\n北京大学多媒体信息处理国家重点实验室、计算机学院，北京通用人工智能研究院，悉尼大学，中国科学院自动化研究所。\n\n论文概述：\n该论文提出Action-Sketcher，一个用于长时程机器人操作的视觉语言动作框架。它通过引入“视觉草图”作为明确的中间空间意图表示，来解决长时程操作任务中的空间歧义和时序脆弱性问题。视觉草图由点、边界框和箭头等几何图元组成，在机器人当前视角的图像上绘制，形成一种人类可验证的界面，连接高层推理和低层控制。框架采用“观察→思考→草图→行动”的循环工作流程，由一个基于令牌门控的策略进行协调，支持实时动作预测，同时允许推理触发、草图修订和动作发出。\n\n论文核心贡献点：\n1. 形式化了“视觉草图”作为一种共地理的、明确的空间意图接口，它渲染点、边界框和箭头，从而消除“在哪里”和“如何”行动的歧义，并作为高层推理与低层控制之间的人类可验证契约。\n2. 提出了Action-Sketcher框架，它在“观察→思考→草图→行动”的循环中运行，通过令牌门控状态在推理、视觉草图生成/修订和动作合成之间进行自适应切换，实现了实时中断处理、错误检测和草图级修正，且不牺牲延迟。\n3. 策划了一个交错语料库和训练方案，通过交错序列对齐、语言-草图一致性和带有草图到动作强化的模仿学习，将语言、视觉草图和动作对齐；该系统提升了长时程任务成功率、对动态场景变化的鲁棒性以及可解释性，并通过全面的消融研究和真实机器人研究进行了验证。\n\n论文方法描述：\n该方法的核心是视觉草图和基于此的Action-Sketcher框架。视觉草图是定义在机器人当前视角图像平面上的几何图元稀疏元组，包括用于指定目标区域的边界框、用于表示关键交互位置的点和用于编码预期运动的箭头（包括平移箭头和旋转箭头）。\nAction-Sketcher框架将长时程操作任务建模为在混合输出空间上的序列建模任务。它采用“观察-思考-草图-行动”流水线，并在两个模式间自适应切换：推理模式和行动模式。\n* 推理模式：当需要推理时，模型生成<BOR>令牌，随后进行时域和空域推理，生成下一个子任务和相应的文本描述视觉草图，最后用<EOR>令牌结束。\n* 行动模式：当无需推理时，模型生成<BOA>令牌，触发动作专家通过流匹配生成动作块。\n该框架模型无关，可以使用任何VLA模型作为主干。\n训练采用多阶段课程学习策略：\n* 阶段1：基础时空学习，使用大规模数据集进行预训练，发展通用的时空建模和推理能力。\n* 阶段2：推理到草图增强，学习推理模式的连贯推理过程，生成子任务和相应的视觉草图。\n* 阶段3：草图到动作与模式适应，联合训练动作策略和模式切换机制，并使用草图增强和模式平衡采样策略来增强鲁棒性。\n\n论文使用数据集和训练资源：\n* 训练数据：构建了一个交错的语料库，包含交错图像、文本、视觉草图监督和动作序列。具体包括：\n * 阶段1：使用大规模数据集，包含3.4M个样本用于空间理解，870k个序列用于时序学习，其中20%的语料库用GPT-4o生成的详细文本理由进行了标注。\n * 阶段2：从真实世界收集的2.6k个多样、长时程操作任务轨迹，并补充了来自LIBERO和RoboTwin 2.0等现有数据集的1.7k个完整轨迹注释，形成了21k个样本的推理到草图数据集。\n* 模型主干：使用了π0作为具体实例的主干模型。\n* 计算资源：未在提供的HTML片段中明确说明。\n\n论文使用的评估环境和评估指标：\n* 评估环境：\n * 仿真环境：LIBERO基准（侧重于终身技能）、增强版RoboTwin 2.0（增加了物体杂乱度和空间复杂性）。\n * 真实世界环境：Agilex和Galaxea双机械臂平台，执行“整理凌乱桌面”、“倒茶”和“通用拾放”任务。\n* 评估指标：\n * 主要指标：任务成功率，基于任务完成情况（仿真）或子任务完成率平均值（真实世界）。\n * 其他分析：失败模式分析（错误模式切换、推理模式错误、行动模式错误的比例）、人工在环干预效果（修正视觉草图后的成功率提升）。",
    "summary_html": "<p>论文研究单位：</p>\n<p>北京大学多媒体信息处理国家重点实验室、计算机学院，北京通用人工智能研究院，悉尼大学，中国科学院自动化研究所。</p>\n\n<p>论文概述：</p>\n<p>该论文提出Action-Sketcher，一个用于长时程机器人操作的视觉语言动作框架。它通过引入“视觉草图”作为明确的中间空间意图表示，来解决长时程操作任务中的空间歧义和时序脆弱性问题。视觉草图由点、边界框和箭头等几何图元组成，在机器人当前视角的图像上绘制，形成一种人类可验证的界面，连接高层推理和低层控制。框架采用“观察→思考→草图→行动”的循环工作流程，由一个基于令牌门控的策略进行协调，支持实时动作预测，同时允许推理触发、草图修订和动作发出。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>形式化了“视觉草图”作为一种共地理的、明确的空间意图接口，它渲染点、边界框和箭头，从而消除“在哪里”和“如何”行动的歧义，并作为高层推理与低层控制之间的人类可验证契约。</li><li>提出了Action-Sketcher框架，它在“观察→思考→草图→行动”的循环中运行，通过令牌门控状态在推理、视觉草图生成/修订和动作合成之间进行自适应切换，实现了实时中断处理、错误检测和草图级修正，且不牺牲延迟。</li><li>策划了一个交错语料库和训练方案，通过交错序列对齐、语言-草图一致性和带有草图到动作强化的模仿学习，将语言、视觉草图和动作对齐；该系统提升了长时程任务成功率、对动态场景变化的鲁棒性以及可解释性，并通过全面的消融研究和真实机器人研究进行了验证。</li></ol>\n\n<p>论文方法描述：</p>\n<p>该方法的核心是视觉草图和基于此的Action-Sketcher框架。视觉草图是定义在机器人当前视角图像平面上的几何图元稀疏元组，包括用于指定目标区域的边界框、用于表示关键交互位置的点和用于编码预期运动的箭头（包括平移箭头和旋转箭头）。</p>\n<p>Action-Sketcher框架将长时程操作任务建模为在混合输出空间上的序列建模任务。它采用“观察-思考-草图-行动”流水线，并在两个模式间自适应切换：推理模式和行动模式。</p>\n<p>* 推理模式：当需要推理时，模型生成<BOR>令牌，随后进行时域和空域推理，生成下一个子任务和相应的文本描述视觉草图，最后用<EOR>令牌结束。</p>\n<p>* 行动模式：当无需推理时，模型生成<BOA>令牌，触发动作专家通过流匹配生成动作块。</p>\n<p>该框架模型无关，可以使用任何VLA模型作为主干。</p>\n<p>训练采用多阶段课程学习策略：</p>\n<p>* 阶段1：基础时空学习，使用大规模数据集进行预训练，发展通用的时空建模和推理能力。</p>\n<p>* 阶段2：推理到草图增强，学习推理模式的连贯推理过程，生成子任务和相应的视觉草图。</p>\n<p>* 阶段3：草图到动作与模式适应，联合训练动作策略和模式切换机制，并使用草图增强和模式平衡采样策略来增强鲁棒性。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>* 训练数据：构建了一个交错的语料库，包含交错图像、文本、视觉草图监督和动作序列。具体包括：</p>\n<p> * 阶段1：使用大规模数据集，包含3.4M个样本用于空间理解，870k个序列用于时序学习，其中20%的语料库用GPT-4o生成的详细文本理由进行了标注。</p>\n<p> * 阶段2：从真实世界收集的2.6k个多样、长时程操作任务轨迹，并补充了来自LIBERO和RoboTwin 2.0等现有数据集的1.7k个完整轨迹注释，形成了21k个样本的推理到草图数据集。</p>\n<p>* 模型主干：使用了π0作为具体实例的主干模型。</p>\n<p>* 计算资源：未在提供的HTML片段中明确说明。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>* 评估环境：</p>\n<p> * 仿真环境：LIBERO基准（侧重于终身技能）、增强版RoboTwin 2.0（增加了物体杂乱度和空间复杂性）。</p>\n<p> * 真实世界环境：Agilex和Galaxea双机械臂平台，执行“整理凌乱桌面”、“倒茶”和“通用拾放”任务。</p>\n<p>* 评估指标：</p>\n<p> * 主要指标：任务成功率，基于任务完成情况（仿真）或子任务完成率平均值（真实世界）。</p>\n<p> * 其他分析：失败模式分析（错误模式切换、推理模式错误、行动模式错误的比例）、人工在环干预效果（修正视觉草图后的成功率提升）。</p>"
  },
  {
    "date": "2026-01-02",
    "title": "Value Vision-Language-Action Planning & Search",
    "link": "http://arxiv.org/abs/2601.00969",
    "summary_markdown": "论文研究单位：不列颠哥伦比亚大学（The University of British Columbia）\n\n论文概述：本研究提出了Value Vision-Language-Action Planning and Search（V-VLAPS）框架，旨在解决视觉-语言-动作（VLA）模型在分布偏移下表现脆弱的问题。该框架通过在学习到的价值函数指导下，增强基于蒙特卡洛树搜索（MCTS）的规划能力，以更少的模拟次数提高任务成功率。\n\n论文核心贡献点：\n1. 提出了V-VLAPS框架，将轻量级可学习价值函数与基于VLA的蒙特卡洛树搜索相结合。\n2. 在固定VLA骨干网络（Octo）的潜层表示上训练简单的多层感知机（MLP）作为价值头，为搜索提供显式的成功信号。\n3. 在LIBERO机器人操作基准上验证了方法，展示了在提高成功率的同时减少了平均MCTS模拟次数。\n\n论文方法描述：\n1. 数据收集：通过在LIBERO任务上滚动执行固定的预训练VLA策略（Octo），收集状态序列。每个决策步骤提取VLA的潜层读出向量，并根据稀疏的终端成功奖励计算蒙特卡洛价值目标。\n2. 价值头训练：在冻结的VLA骨干网络上，训练一个三层的MLP作为价值头，以潜层读出向量为输入，预测蒙特卡洛价值目标，损失函数为均方误差。\n3. 价值集成：将学习到的价值函数集成到VLAPS的树搜索中。修改节点选择评分规则，在原有的基于先验和访问次数的探索项基础上，增加了基于价值估计的Q项，以偏向高价值节点的选择。\n\n论文使用数据集和训练资源：\n数据集：LIBERO机器人操作基准，包括空间任务套件和物体任务套件。训练数据包含从指定任务（空间套件任务1、6、8；物体套件任务0、1、2、3、4、6、8）中收集的共524,157个训练样本和58,239个测试样本。为处理数据不平衡，对失败状态进行了随机下采样。\n训练资源：使用预训练的Octo VLA模型作为固定骨干。价值头是轻量级的三层MLP，使用Adam优化器进行训练，仅更新价值头参数，VLA骨干参数保持冻结。\n\n论文使用的评估环境和评估指标：\n评估环境：LIBERO模拟机器人操作环境，分为空间任务套件和物体任务套件，共评估了每个套件中的10个任务（任务0-9）。\n评估指标：\n1. 任务成功率：在10次不同初始状态下的平均成功率。\n2. 规划效率：平均MCTS模拟次数，即从根节点重启搜索的次数。\n3. 定性分析：通过t-SNE可视化VLA潜层读出的分布，并检查价值估计随轨迹的变化。",
    "summary_html": "<p>论文研究单位：不列颠哥伦比亚大学（The University of British Columbia）</p>\n\n<p>论文概述：本研究提出了Value Vision-Language-Action Planning and Search（V-VLAPS）框架，旨在解决视觉-语言-动作（VLA）模型在分布偏移下表现脆弱的问题。该框架通过在学习到的价值函数指导下，增强基于蒙特卡洛树搜索（MCTS）的规划能力，以更少的模拟次数提高任务成功率。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了V-VLAPS框架，将轻量级可学习价值函数与基于VLA的蒙特卡洛树搜索相结合。</li><li>在固定VLA骨干网络（Octo）的潜层表示上训练简单的多层感知机（MLP）作为价值头，为搜索提供显式的成功信号。</li><li>在LIBERO机器人操作基准上验证了方法，展示了在提高成功率的同时减少了平均MCTS模拟次数。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>数据收集：通过在LIBERO任务上滚动执行固定的预训练VLA策略（Octo），收集状态序列。每个决策步骤提取VLA的潜层读出向量，并根据稀疏的终端成功奖励计算蒙特卡洛价值目标。</li><li>价值头训练：在冻结的VLA骨干网络上，训练一个三层的MLP作为价值头，以潜层读出向量为输入，预测蒙特卡洛价值目标，损失函数为均方误差。</li><li>价值集成：将学习到的价值函数集成到VLAPS的树搜索中。修改节点选择评分规则，在原有的基于先验和访问次数的探索项基础上，增加了基于价值估计的Q项，以偏向高价值节点的选择。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<p>数据集：LIBERO机器人操作基准，包括空间任务套件和物体任务套件。训练数据包含从指定任务（空间套件任务1、6、8；物体套件任务0、1、2、3、4、6、8）中收集的共524,157个训练样本和58,239个测试样本。为处理数据不平衡，对失败状态进行了随机下采样。</p>\n<p>训练资源：使用预训练的Octo VLA模型作为固定骨干。价值头是轻量级的三层MLP，使用Adam优化器进行训练，仅更新价值头参数，VLA骨干参数保持冻结。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：LIBERO模拟机器人操作环境，分为空间任务套件和物体任务套件，共评估了每个套件中的10个任务（任务0-9）。</p>\n<p>评估指标：</p>\n<ol><li>任务成功率：在10次不同初始状态下的平均成功率。</li><li>规划效率：平均MCTS模拟次数，即从根节点重启搜索的次数。</li><li>定性分析：通过t-SNE可视化VLA潜层读出的分布，并检查价值估计随轨迹的变化。</li></ol>"
  },
  {
    "date": "2025-12-31",
    "title": "Dichotomous Diffusion Policy Optimization",
    "link": "http://arxiv.org/abs/2601.00898",
    "summary_markdown": "论文研究单位：\n- 中国科学院自动化研究所基础模型研究中心\n- 中国科学院大学人工智能学院\n- 清华大学人工智能产业研究院\n- 香港中文大学\n- 上海交通大学\n- 北京大学\n- 小米汽车\n\n论文概述：\n该论文提出了一种称为二分扩散策略优化的新型强化学习方法，旨在稳定且可控制地优化扩散策略。扩散策略因其强大的表达能力和推理期间的可控生成能力，在解决决策任务中日益流行。然而，使用强化学习有效训练大型扩散策略仍然具有挑战性。现有方法要么因直接最大化价值目标而导致训练不稳定，要么因依赖粗糙的高斯似然近似而面临计算问题。DIPOLE方法通过重新审视强化学习中的KL正则化目标，提出了一种贪婪化的策略正则化方案，从而能够将最优策略分解为一对稳定学习的二分策略：一个专注于奖励最大化，另一个专注于奖励最小化。\n\n论文核心贡献点：\n1. 提出DIPOLE，一种用于稳定且可控制的扩散策略优化的强化学习框架。\n2. 引入贪婪化的KL正则化强化学习目标，该目标自然地将最优策略分解为一对二分策略。\n3. 展示优化后的策略可以通过在推理过程中线性组合二分策略的得分来恢复，这完美地结合了扩散模型中广泛使用的无分类器引导机制，从而实现对动作生成贪婪程度的灵活控制。\n4. 在ExORL和OGBench基准测试的离线和离线到在线强化学习设置中验证了方法的有效性。\n5. 将DIPOLE扩展应用于训练大型视觉-语言-动作模型进行端到端自动驾驶，并在大规模真实世界自动驾驶基准NAVSIM上进行评估，展示了其在复杂现实世界应用中的潜力。\n\n论文方法描述：\nDIPOLE从一个贪婪化的KL正则化强化学习目标出发，该目标使用一个有界的平滑sigmoid函数作为加权函数。论文推导出该目标的闭式最优解，并利用sigmoid函数的特性，将最优策略分解为一对二分策略：正策略旨在最大化回报，负策略旨在最小化回报。这两个策略分别使用有界的sigmoid权重函数进行训练，避免了损失爆炸并实现了更高效的学习。推理时，最优策略的得分函数可以表示为二分策略得分的线性组合，权重由贪婪因子控制。该方法适用于离线和离线到在线强化学习设置。\n\n论文使用数据集和训练资源：\n- RL基准测试：ExORL基准和OGBench任务套件，包含总计39个任务。OGBench使用了由RND收集的默认数据集。用于端到端自动驾驶实验的数据集未在提供的片段中明确命名。\n- 训练资源：在RL基准测试中使用了特定的离线数据集。对于端到端自动驾驶，使用了一个10亿参数模型，该模型使用Florence-2作为编码器，扩散动作头作为解码器，并进行了模仿学习预训练。使用了LoRA模块进行微调。\n\n论文使用的评估环境和评估指标：\n- 评估环境：ExORL和OGBench基准测试环境，包含机器人运动、迷宫导航、物体操作等任务。端到端自动驾驶评估在大规模真实世界自动驾驶基准NAVSIM上进行。\n- 评估指标：在RL基准测试中，报告了归一化分数作为性能指标。在自动驾驶评估中，使用了基于安全性、进展和舒适度的奖励函数来评估轨迹质量。具体指标未在提供的片段中详细列出，但提到了在ExORL基准测试中的平均分数。",
    "summary_html": "<p>论文研究单位：</p>\n<ul><li>中国科学院自动化研究所基础模型研究中心</li><li>中国科学院大学人工智能学院</li><li>清华大学人工智能产业研究院</li><li>香港中文大学</li><li>上海交通大学</li><li>北京大学</li><li>小米汽车</li></ul>\n\n<p>论文概述：</p>\n<p>该论文提出了一种称为二分扩散策略优化的新型强化学习方法，旨在稳定且可控制地优化扩散策略。扩散策略因其强大的表达能力和推理期间的可控生成能力，在解决决策任务中日益流行。然而，使用强化学习有效训练大型扩散策略仍然具有挑战性。现有方法要么因直接最大化价值目标而导致训练不稳定，要么因依赖粗糙的高斯似然近似而面临计算问题。DIPOLE方法通过重新审视强化学习中的KL正则化目标，提出了一种贪婪化的策略正则化方案，从而能够将最优策略分解为一对稳定学习的二分策略：一个专注于奖励最大化，另一个专注于奖励最小化。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出DIPOLE，一种用于稳定且可控制的扩散策略优化的强化学习框架。</li><li>引入贪婪化的KL正则化强化学习目标，该目标自然地将最优策略分解为一对二分策略。</li><li>展示优化后的策略可以通过在推理过程中线性组合二分策略的得分来恢复，这完美地结合了扩散模型中广泛使用的无分类器引导机制，从而实现对动作生成贪婪程度的灵活控制。</li><li>在ExORL和OGBench基准测试的离线和离线到在线强化学习设置中验证了方法的有效性。</li><li>将DIPOLE扩展应用于训练大型视觉-语言-动作模型进行端到端自动驾驶，并在大规模真实世界自动驾驶基准NAVSIM上进行评估，展示了其在复杂现实世界应用中的潜力。</li></ol>\n\n<p>论文方法描述：</p>\n<p>DIPOLE从一个贪婪化的KL正则化强化学习目标出发，该目标使用一个有界的平滑sigmoid函数作为加权函数。论文推导出该目标的闭式最优解，并利用sigmoid函数的特性，将最优策略分解为一对二分策略：正策略旨在最大化回报，负策略旨在最小化回报。这两个策略分别使用有界的sigmoid权重函数进行训练，避免了损失爆炸并实现了更高效的学习。推理时，最优策略的得分函数可以表示为二分策略得分的线性组合，权重由贪婪因子控制。该方法适用于离线和离线到在线强化学习设置。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>RL基准测试：ExORL基准和OGBench任务套件，包含总计39个任务。OGBench使用了由RND收集的默认数据集。用于端到端自动驾驶实验的数据集未在提供的片段中明确命名。</li><li>训练资源：在RL基准测试中使用了特定的离线数据集。对于端到端自动驾驶，使用了一个10亿参数模型，该模型使用Florence-2作为编码器，扩散动作头作为解码器，并进行了模仿学习预训练。使用了LoRA模块进行微调。</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>评估环境：ExORL和OGBench基准测试环境，包含机器人运动、迷宫导航、物体操作等任务。端到端自动驾驶评估在大规模真实世界自动驾驶基准NAVSIM上进行。</li><li>评估指标：在RL基准测试中，报告了归一化分数作为性能指标。在自动驾驶评估中，使用了基于安全性、进展和舒适度的奖励函数来评估轨迹质量。具体指标未在提供的片段中详细列出，但提到了在ExORL基准测试中的平均分数。</li></ul>"
  },
  {
    "date": "2025-12-31",
    "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots",
    "link": "http://arxiv.org/abs/2512.24673",
    "summary_markdown": "论文研究单位：中国移动（杭州）信息技术有限公司。\n\n论文概述：论文针对视觉-语言-动作（VLA）模型部署到机器人时，由于异步推理和时序失配导致的动作抖动、停顿及轨迹不连续问题，提出了一个名为VLA-RAIL的实时异步推理连接框架。该框架旨在平滑、连续地执行机器人动作，提高任务成功率和执行速度。\n\n论文核心贡献点：\n1. 提出一个开源、模型无关的异步推理框架，作为即插即用的中间件，无缝连接多种VLA模型和异构机器人平台。\n2. 提出一个两阶段的动作块后处理策略，包括块内轨迹平滑和块间无缝融合，以消除预测噪声和异步时序失配引起的运动抖动，从而提高整体任务成功率。\n3. 提出一种简单而高效的动作加速策略，通过联合调整动作轨迹插值和指令发送频率，可将执行速度加速至硬件极限。\n\n论文方法描述：VLA-RAIL采用客户端-服务器架构，将VLA模型推理与机器人运动控制解耦。核心算法贡献是轨迹后处理流水线。它包括两个阶段：\n1. 块内轨迹平滑：使用多项式拟合（如三次多项式）表示动作块内的轨迹，通过最小二乘估计平滑掉训练数据中的人为遥操作噪声和模型预测的高频振荡。\n2. 块间无缝融合：包括精确的时序对齐和轨迹平滑优化。时序对齐通过优化方法确定新动作块的激活时间；轨迹平滑采用双五次样条插值方法，确保连续动作块之间位置、速度和加速度的连续性，生成C²连续的复合轨迹。\n\n论文使用数据集和训练资源：论文未明确提及用于实验的具体外部数据集。实验中的VLA模型（如GO1, SmolVLA, π₀, π₀.₅, GR00T）均在从30Hz遥操作收集的演示数据集上进行训练。实验在配备NVIDIA RTX 4080 GPU（笔记本版，12GB显存）的计算机上进行。\n\n论文使用的评估环境和评估指标：\n评估环境：在真实世界AgiBot G1机器人（双14自由度手臂和双平行夹爪）上进行的操作任务。计算服务器与真实机器人在同一局域网内连接，以最小化通信延迟。\n评估指标：\n1. 任务成功率：超过20次真实机器人试验的成功率。\n2. 平均完成时间：成功试验的平均完成时间，复杂任务进一步按阶段分解。\n3. 轨迹平滑度指标：计算1秒间隔内关节角度、速度和加速度的标准差，数值越低表示运动越平滑。",
    "summary_html": "<p>论文研究单位：中国移动（杭州）信息技术有限公司。</p>\n\n<p>论文概述：论文针对视觉-语言-动作（VLA）模型部署到机器人时，由于异步推理和时序失配导致的动作抖动、停顿及轨迹不连续问题，提出了一个名为VLA-RAIL的实时异步推理连接框架。该框架旨在平滑、连续地执行机器人动作，提高任务成功率和执行速度。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出一个开源、模型无关的异步推理框架，作为即插即用的中间件，无缝连接多种VLA模型和异构机器人平台。</li><li>提出一个两阶段的动作块后处理策略，包括块内轨迹平滑和块间无缝融合，以消除预测噪声和异步时序失配引起的运动抖动，从而提高整体任务成功率。</li><li>提出一种简单而高效的动作加速策略，通过联合调整动作轨迹插值和指令发送频率，可将执行速度加速至硬件极限。</li></ol>\n\n<p>论文方法描述：VLA-RAIL采用客户端-服务器架构，将VLA模型推理与机器人运动控制解耦。核心算法贡献是轨迹后处理流水线。它包括两个阶段：</p>\n<ol><li>块内轨迹平滑：使用多项式拟合（如三次多项式）表示动作块内的轨迹，通过最小二乘估计平滑掉训练数据中的人为遥操作噪声和模型预测的高频振荡。</li><li>块间无缝融合：包括精确的时序对齐和轨迹平滑优化。时序对齐通过优化方法确定新动作块的激活时间；轨迹平滑采用双五次样条插值方法，确保连续动作块之间位置、速度和加速度的连续性，生成C²连续的复合轨迹。</li></ol>\n\n<p>论文使用数据集和训练资源：论文未明确提及用于实验的具体外部数据集。实验中的VLA模型（如GO1, SmolVLA, π₀, π₀.₅, GR00T）均在从30Hz遥操作收集的演示数据集上进行训练。实验在配备NVIDIA RTX 4080 GPU（笔记本版，12GB显存）的计算机上进行。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：在真实世界AgiBot G1机器人（双14自由度手臂和双平行夹爪）上进行的操作任务。计算服务器与真实机器人在同一局域网内连接，以最小化通信延迟。</p>\n<p>评估指标：</p>\n<ol><li>任务成功率：超过20次真实机器人试验的成功率。</li><li>平均完成时间：成功试验的平均完成时间，复杂任务进一步按阶段分解。</li><li>轨迹平滑度指标：计算1秒间隔内关节角度、速度和加速度的标准差，数值越低表示运动越平滑。</li></ol>"
  },
  {
    "date": "2025-12-31",
    "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence",
    "link": "http://arxiv.org/abs/2512.24653",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-30",
    "title": "Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning",
    "link": "http://arxiv.org/abs/2512.24426",
    "summary_markdown": "论文研究单位： NVIDIA、 UCLA、 Stanford University\n\n论文概述： 本文提出了Counterfactual VLA (CF-VLA)，一个具备自适应推理能力的自反思视觉-语言-动作模型框架。该框架旨在解决现有推理增强VLA模型仅进行描述性推理而缺乏自我反思能力的问题。CF-VLA通过反事实推理，让模型在执行前对其预测的动作计划进行评估和修正，从而将推理从一次性描述升级为因果自我修正。该工作聚焦于自动驾驶应用。\n\n论文核心贡献点：\n1. 提出了用于VLA的自反思反事实推理新范式。该范式使VLA能够基于其自身预测的元动作进行推理，预测后果，并在生成最终动作前修改计划，将推理从描述性解释升级为因果自我修正。\n2. 引入了元动作及反事实数据生成流程。利用时间分割的元动作作为动作与语言的对齐中介，并提出了“展开-过滤-标注”流程，从模型自身的行为中自动整理反事实数据，形成自我改进的闭环。\n3. 实现了自动驾驶中的自适应思考能力。CF-VLA能够“在必要时思考”，将反事实推理集中在最具挑战性的场景中，同时在保持合理测试时计算成本的前提下，提高轨迹准确性和安全指标。\n\n论文方法描述：\nCF-VLA框架的核心是一个自我反思循环，循环步骤为：预测元动作 → 进行反事实推理 → 更新元动作 → 生成最终轨迹。为了实现此行为，论文提出三个关键部分：\n1. 时间分割的元动作：将驾驶意图在纵向、横向和车道层面表示为覆盖6.4秒规划视野的时间分区语言段，作为连接推理与底层动作的可解释中间抽象。\n2. 自反思反事实推理：模型在生成初步元动作后，会基于视觉上下文和这些元动作进行反事实思维链推理，询问“如果我遵循这个计划会发生什么，这合适吗？”，并据此修改不安全或次优的计划。\n3. “展开-过滤-标注”反事实数据生成流水线：首先，在基础VLA上展开预测，生成自由生成的轨迹和使用真实元动作“预填充”的轨迹。然后，通过比较两种轨迹与专家轨迹的误差，过滤出元动作是性能瓶颈的高价值场景。最后，使用教师模型为这些场景生成反事实推理标签，说明初始计划为何不优及如何调整。模型通过混合数据集训练，并结合损失掩码与加权技术，学习自适应推理能力。\n\n论文使用数据集和训练资源：\n使用了一个大规模内部专有数据集进行训练和评估。数据集包含80,000小时来自25个国家的人类驾驶数据。轨迹专用数据集D_traj包含约1160万个20秒视频片段。元动作标注子集D_meta包含3000小时数据，其中训练集有433K个20秒视频片段，对应801K个8.4秒样本；验证集有39K个视频片段，对应73K个样本。反事实推理数据集D_CF通常包含200K个样本。模型架构基于Alpamayo-R1，接收文本提示、两个前置摄像头视频（宽角和长焦）以及历史轨迹作为输入。教师模型使用Qwen2.5-VL-72B-Instruct生成反事实推理标签。\n\n论文使用的评估环境和评估指标：\n评估是在内部数据集的分割验证集上进行的。\n主要评估指标包括：\n1. 轨迹精度：MinADE/AvgADE（平均位移误差）、MinFDE/AvgFDE（终点位移误差）、Corner Distance（车辆角点平均偏差）。\n2. 安全特性：Collision Rate（碰撞率）、Out-of-road Rate（驶出道路率）。\n3. 推理质量：Meta-Action IOU（预测与真实元动作的交并比，CF-VLA报告修正后的IOU）、Output Length（输出令牌数）、Think Rate（包含反事实推理的响应比例）。",
    "summary_html": "<p>论文研究单位： NVIDIA、 UCLA、 Stanford University</p>\n\n<p>论文概述： 本文提出了Counterfactual VLA (CF-VLA)，一个具备自适应推理能力的自反思视觉-语言-动作模型框架。该框架旨在解决现有推理增强VLA模型仅进行描述性推理而缺乏自我反思能力的问题。CF-VLA通过反事实推理，让模型在执行前对其预测的动作计划进行评估和修正，从而将推理从一次性描述升级为因果自我修正。该工作聚焦于自动驾驶应用。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了用于VLA的自反思反事实推理新范式。该范式使VLA能够基于其自身预测的元动作进行推理，预测后果，并在生成最终动作前修改计划，将推理从描述性解释升级为因果自我修正。</li><li>引入了元动作及反事实数据生成流程。利用时间分割的元动作作为动作与语言的对齐中介，并提出了“展开-过滤-标注”流程，从模型自身的行为中自动整理反事实数据，形成自我改进的闭环。</li><li>实现了自动驾驶中的自适应思考能力。CF-VLA能够“在必要时思考”，将反事实推理集中在最具挑战性的场景中，同时在保持合理测试时计算成本的前提下，提高轨迹准确性和安全指标。</li></ol>\n\n<p>论文方法描述：</p>\n<p>CF-VLA框架的核心是一个自我反思循环，循环步骤为：预测元动作 → 进行反事实推理 → 更新元动作 → 生成最终轨迹。为了实现此行为，论文提出三个关键部分：</p>\n<ol><li>时间分割的元动作：将驾驶意图在纵向、横向和车道层面表示为覆盖6.4秒规划视野的时间分区语言段，作为连接推理与底层动作的可解释中间抽象。</li><li>自反思反事实推理：模型在生成初步元动作后，会基于视觉上下文和这些元动作进行反事实思维链推理，询问“如果我遵循这个计划会发生什么，这合适吗？”，并据此修改不安全或次优的计划。</li><li>“展开-过滤-标注”反事实数据生成流水线：首先，在基础VLA上展开预测，生成自由生成的轨迹和使用真实元动作“预填充”的轨迹。然后，通过比较两种轨迹与专家轨迹的误差，过滤出元动作是性能瓶颈的高价值场景。最后，使用教师模型为这些场景生成反事实推理标签，说明初始计划为何不优及如何调整。模型通过混合数据集训练，并结合损失掩码与加权技术，学习自适应推理能力。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<p>使用了一个大规模内部专有数据集进行训练和评估。数据集包含80,000小时来自25个国家的人类驾驶数据。轨迹专用数据集D_traj包含约1160万个20秒视频片段。元动作标注子集D_meta包含3000小时数据，其中训练集有433K个20秒视频片段，对应801K个8.4秒样本；验证集有39K个视频片段，对应73K个样本。反事实推理数据集D_CF通常包含200K个样本。模型架构基于Alpamayo-R1，接收文本提示、两个前置摄像头视频（宽角和长焦）以及历史轨迹作为输入。教师模型使用Qwen2.5-VL-72B-Instruct生成反事实推理标签。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估是在内部数据集的分割验证集上进行的。</p>\n<p>主要评估指标包括：</p>\n<ol><li>轨迹精度：MinADE/AvgADE（平均位移误差）、MinFDE/AvgFDE（终点位移误差）、Corner Distance（车辆角点平均偏差）。</li><li>安全特性：Collision Rate（碰撞率）、Out-of-road Rate（驶出道路率）。</li><li>推理质量：Meta-Action IOU（预测与真实元动作的交并比，CF-VLA报告修正后的IOU）、Output Length（输出令牌数）、Think Rate（包含反事实推理的响应比例）。</li></ol>"
  },
  {
    "date": "2025-12-30",
    "title": "GR-Dexter Technical Report",
    "link": "http://arxiv.org/abs/2512.24210",
    "summary_markdown": "论文研究单位：ByteDance Seed\n\n论文概述：本论文提出了GR-Dexter，一个面向灵巧手操作的整体硬件-模型-数据框架。该工作将视觉-语言-动作模型扩展到配备高自由度灵巧手的双臂机器人上，以解决动作空间大、遮挡严重和真实机器人数据收集成本高等挑战。论文介绍了紧凑的21自由度机器人手ByteDexter V2的设计，一个直观的双臂遥操作系统，以及一种利用遥操作机器人轨迹、大规模视觉语言数据和精心策划的跨本体数据集进行协同训练的方法。\n\n论文核心贡献点：\n1. 介绍了ByteDexter V2灵巧手，一种21自由度的连杆驱动拟人化机器人手，相比前代增加了拇指自由度并减小了尺寸，集成了指尖触觉传感器。\n2. 开发了一个用于真实机器人数据收集的双臂遥操作界面，使用VR头盔和数据手套实时重定向人体手腕姿态和手部动作。\n3. 提出了一个名为GR-Dexter的视觉-语言-动作模型，用于控制总自由度为56的双臂灵巧手系统。\n4. 设计了一种协同训练方法，混合使用多种数据源：视觉语言数据、跨本体机器人数据、人体轨迹数据和机器人遥操作轨迹。\n5. 构建了统一的跨本体运动重定向和迁移管道，以整合来自不同本体和人体演示的数据。\n\n论文方法描述：\nGR-Dexter模型基于一个预训练的视觉语言模型，采用混合专家Transformer架构，拥有40亿参数。模型生成k长度的动作块，控制固定基座的双臂机器人。每个动作是长度为88的向量，包含臂关节动作、臂末端执行器姿态、手部关节动作和指尖位置。训练使用动态混合视觉语言数据与机器人轨迹，总损失是下一个令牌预测损失和流匹配损失之和。为处理跨本体和人体数据的结构差异，对目标本体中不存在或不稳定的动作维度进行了掩蔽。\n\n论文使用数据集和训练资源：\n1. 视觉语言数据：复用自GR-3，涵盖图像描述、视觉问答、图像接地等任务。\n2. 跨本体数据：利用了三个开源双臂拟人化操作数据集：Fourier ActionNet Dataset、OpenLoong Baihu Dataset和RoboMIND。\n3. 人体轨迹数据：采用了超过800小时的自我中心视频及其配对的3D手和手指跟踪数据，并补充了使用Pico VR设备收集的数据。\n4. 机器人遥操作数据：收集了约20小时的真实机器人轨迹用于特定任务。\n训练资源包括使用了预训练的Qwen2.5-VL模型作为基础。\n\n论文使用的评估环境和评估指标：\n评估环境：真实世界的双臂机器人平台，配备两个ByteDexter V2手和四个全局RGB-D相机。\n评估任务：1) 长视野灵巧操作（如化妆台整理、吸尘、上面包）；2) 泛化性拾放任务。\n评估指标：任务成功率，即在多次试验中成功完成子任务的平均比例。评估了在基本设置（训练中见过的空间布局）、分布外设置（未见过的空间布局）、未见过的物体和未见过的语言指令下的表现。",
    "summary_html": "<p>论文研究单位：ByteDance Seed</p>\n\n<p>论文概述：本论文提出了GR-Dexter，一个面向灵巧手操作的整体硬件-模型-数据框架。该工作将视觉-语言-动作模型扩展到配备高自由度灵巧手的双臂机器人上，以解决动作空间大、遮挡严重和真实机器人数据收集成本高等挑战。论文介绍了紧凑的21自由度机器人手ByteDexter V2的设计，一个直观的双臂遥操作系统，以及一种利用遥操作机器人轨迹、大规模视觉语言数据和精心策划的跨本体数据集进行协同训练的方法。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>介绍了ByteDexter V2灵巧手，一种21自由度的连杆驱动拟人化机器人手，相比前代增加了拇指自由度并减小了尺寸，集成了指尖触觉传感器。</li><li>开发了一个用于真实机器人数据收集的双臂遥操作界面，使用VR头盔和数据手套实时重定向人体手腕姿态和手部动作。</li><li>提出了一个名为GR-Dexter的视觉-语言-动作模型，用于控制总自由度为56的双臂灵巧手系统。</li><li>设计了一种协同训练方法，混合使用多种数据源：视觉语言数据、跨本体机器人数据、人体轨迹数据和机器人遥操作轨迹。</li><li>构建了统一的跨本体运动重定向和迁移管道，以整合来自不同本体和人体演示的数据。</li></ol>\n\n<p>论文方法描述：</p>\n<p>GR-Dexter模型基于一个预训练的视觉语言模型，采用混合专家Transformer架构，拥有40亿参数。模型生成k长度的动作块，控制固定基座的双臂机器人。每个动作是长度为88的向量，包含臂关节动作、臂末端执行器姿态、手部关节动作和指尖位置。训练使用动态混合视觉语言数据与机器人轨迹，总损失是下一个令牌预测损失和流匹配损失之和。为处理跨本体和人体数据的结构差异，对目标本体中不存在或不稳定的动作维度进行了掩蔽。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>视觉语言数据：复用自GR-3，涵盖图像描述、视觉问答、图像接地等任务。</li><li>跨本体数据：利用了三个开源双臂拟人化操作数据集：Fourier ActionNet Dataset、OpenLoong Baihu Dataset和RoboMIND。</li><li>人体轨迹数据：采用了超过800小时的自我中心视频及其配对的3D手和手指跟踪数据，并补充了使用Pico VR设备收集的数据。</li><li>机器人遥操作数据：收集了约20小时的真实机器人轨迹用于特定任务。</li></ol>\n<p>训练资源包括使用了预训练的Qwen2.5-VL模型作为基础。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：真实世界的双臂机器人平台，配备两个ByteDexter V2手和四个全局RGB-D相机。</p>\n<p>评估任务：1) 长视野灵巧操作（如化妆台整理、吸尘、上面包）；2) 泛化性拾放任务。</p>\n<p>评估指标：任务成功率，即在多次试验中成功完成子任务的平均比例。评估了在基本设置（训练中见过的空间布局）、分布外设置（未见过的空间布局）、未见过的物体和未见过的语言指令下的表现。</p>"
  },
  {
    "date": "2025-12-30",
    "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training",
    "link": "http://arxiv.org/abs/2512.24125",
    "summary_markdown": "论文研究单位：AgiBot Research， AgiBot， Shanghai Innovation Institute\n\n论文概述：\n本论文旨在解决通用机器人系统在开放世界环境中同时实现广泛泛化和高精度动作执行的双重挑战。现有的视觉-语言-动作（VLA）模型往往在这两者之间存在权衡。本文提出了两大贡献：首先，引入了体现推理智能商数（ERIQ）基准，这是一个大规模、解耦的体现推理评测集，用于独立量化VLM的推理能力而不受动作执行误差的干扰。其次，提出了FACT（基于流匹配的动作分词器），一种将连续动作离散化为紧凑令牌序列，并通过流匹配解码器重建高保真连续轨迹的方法。基于此，构建了统一的GenieReasoner系统，在一个共享梯度空间内共同优化推理和动作生成，从而弥合了推理与执行之间的差距。\n\n论文核心贡献点：\n1. 提出 Embodied Reasoning Intelligence Quotient (ERIQ) 基准：一个包含超过6K问答对的大规模体现推理评测基准，覆盖空间感知、任务规划、错误检测与恢复、人类意图理解四个维度，用于独立于物理控制评估VLM的推理能力，并揭示了推理能力与下游VLA泛化性能之间的强正相关性。\n2. 提出 FACT（Flow-matching Action Tokenizer）：一种创新的动作分词器，结合了向量量化编码和基于流匹配的解码器，能够将连续控制信号编码为紧凑的离散令牌，并高保真地重建出连续的机器人轨迹，从而解决了离散动作令牌化中的精度-效率权衡问题。\n3. 构建了GenieReasoner统一框架：利用FACT，将推理（语言/视觉令牌）和动作（离散动作令牌）在同一个自回归Transformer中进行联合优化，实现了从感知、推理到高精度执行的无缝统一。\n\n论文方法描述：\n1. ERIQ基准构建：基准包含6052个源自真实机器人试验视频的问答对，分为15个细分子任务，涵盖四大推理维度。评测采用标准化的选择题或是非题形式，确保评估的客观性和可复现性。\n2. FACT动作分词器架构：\n * **编码器与量化**：采用基于查询的VQ编码器（使用MM-DiT架构）将连续动作块压缩为低维连续嵌入，然后通过基于符号函数的查找自由量化器将其转换为二进制离散代码。\n * **流匹配解码器**：解码器同样基于MM-DiT架构，训练目标为流匹配（Rectified Flow）损失。其接收带噪声的中间状态、离散动作代码和时间步，预测将噪声样本沿直线轨迹推向目标动作的速度场。\n * **训练损失**：总损失包括编码器的熵损失和承诺损失（确保连续嵌入接近量化值），以及解码器的流匹配均方误差损失。\n3. GenieReasoner系统：分为训练和推理两阶段。\n * **训练阶段**：采用统一的训练流程，将通用VQA数据（以保持基础视觉-语言知识）和体现推理/动作数据混合，共同训练VLM主干，使其能够根据多模态观察和指令生成离散的动作代码序列。\n * **推理阶段**：VLM主干自回归地预测离散动作代码，然后将这些代码送入预训练的FACT解码器。解码器通过求解学习到的速度场所定义的常微分方程，从高斯噪声开始，迭代积分出高保真的连续动作轨迹，交由机器人执行。\n\n论文使用数据集和训练资源：\n1. **训练数据**：\n * **通用多模态数据**：用于保持基础视觉-语言能力，包括Cambrian-10M、LLaVA-OneVision、Describe Anything、CogVLM-SFT-311K、BLIP3-Grounding-50M等。\n * **体现推理与动作数据**：用于增强体现理解和控制能力，包括开源数据集（如NVIDIA Cosmos-Reason、ShareRobot、Robo2VLM、EmbSpatial-SFT、ManipulationVQA-60K）以及内部收集的、与机器人观察空间同源的体现推理数据集（基于AgiBot World，包含2D轨迹、物体接地标注、子任务规划数据等）。\n * **机器人动作数据**：用于训练动作分词器和策略，结合了多个开源数据集（如Octo、OpenVLA、RoboSet）以及内部收集的超过100万个episodes的真实世界机器人操作数据。\n2. **训练资源**：在128个H800 GPU上进行了4天的预训练。FACT分词器的训练使用了2.56M个机器人动作轨迹。\n\n论文使用的评估环境和评估指标：\n1. **评估环境**：\n * **离线基准评测**：在ERIQ基准（自身提出）以及其他开源空间推理基准（如CV-Bench, EmbSpa, BLINK-S/R）上进行VLM推理能力评估。在模拟器（如AgiBot Genie）和真实机器人平台（如AgiBot G01, AgileX, ARX）上进行端到端任务执行评估。\n2. **评估指标**：\n * **ERIQ基准**：使用分类准确率（百分比）来评估模型在15个子任务上的表现，并计算总体平均分（ERIQ-Avg）。\n * **动作重建精度**：使用均方误差（MSE）评估FACT解码器重建连续动作轨迹的保真度，并与基线方法（如FAST）进行比较。\n * **端到端任务执行**：在模拟和真实世界的长视野、多步骤操作任务中，使用任务成功率（Success Rate）作为主要评估指标。",
    "summary_html": "<p>论文研究单位：AgiBot Research， AgiBot， Shanghai Innovation Institute</p>\n\n<p>论文概述：</p>\n<p>本论文旨在解决通用机器人系统在开放世界环境中同时实现广泛泛化和高精度动作执行的双重挑战。现有的视觉-语言-动作（VLA）模型往往在这两者之间存在权衡。本文提出了两大贡献：首先，引入了体现推理智能商数（ERIQ）基准，这是一个大规模、解耦的体现推理评测集，用于独立量化VLM的推理能力而不受动作执行误差的干扰。其次，提出了FACT（基于流匹配的动作分词器），一种将连续动作离散化为紧凑令牌序列，并通过流匹配解码器重建高保真连续轨迹的方法。基于此，构建了统一的GenieReasoner系统，在一个共享梯度空间内共同优化推理和动作生成，从而弥合了推理与执行之间的差距。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出 Embodied Reasoning Intelligence Quotient (ERIQ) 基准：一个包含超过6K问答对的大规模体现推理评测基准，覆盖空间感知、任务规划、错误检测与恢复、人类意图理解四个维度，用于独立于物理控制评估VLM的推理能力，并揭示了推理能力与下游VLA泛化性能之间的强正相关性。</li><li>提出 FACT（Flow-matching Action Tokenizer）：一种创新的动作分词器，结合了向量量化编码和基于流匹配的解码器，能够将连续控制信号编码为紧凑的离散令牌，并高保真地重建出连续的机器人轨迹，从而解决了离散动作令牌化中的精度-效率权衡问题。</li><li>构建了GenieReasoner统一框架：利用FACT，将推理（语言/视觉令牌）和动作（离散动作令牌）在同一个自回归Transformer中进行联合优化，实现了从感知、推理到高精度执行的无缝统一。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>ERIQ基准构建：基准包含6052个源自真实机器人试验视频的问答对，分为15个细分子任务，涵盖四大推理维度。评测采用标准化的选择题或是非题形式，确保评估的客观性和可复现性。</li><li>FACT动作分词器架构：</li></ol>\n<p> * <strong>编码器与量化</strong>：采用基于查询的VQ编码器（使用MM-DiT架构）将连续动作块压缩为低维连续嵌入，然后通过基于符号函数的查找自由量化器将其转换为二进制离散代码。</p>\n<p> * <strong>流匹配解码器</strong>：解码器同样基于MM-DiT架构，训练目标为流匹配（Rectified Flow）损失。其接收带噪声的中间状态、离散动作代码和时间步，预测将噪声样本沿直线轨迹推向目标动作的速度场。</p>\n<p> * <strong>训练损失</strong>：总损失包括编码器的熵损失和承诺损失（确保连续嵌入接近量化值），以及解码器的流匹配均方误差损失。</p>\n<p>3. GenieReasoner系统：分为训练和推理两阶段。</p>\n<p> * <strong>训练阶段</strong>：采用统一的训练流程，将通用VQA数据（以保持基础视觉-语言知识）和体现推理/动作数据混合，共同训练VLM主干，使其能够根据多模态观察和指令生成离散的动作代码序列。</p>\n<p> * <strong>推理阶段</strong>：VLM主干自回归地预测离散动作代码，然后将这些代码送入预训练的FACT解码器。解码器通过求解学习到的速度场所定义的常微分方程，从高斯噪声开始，迭代积分出高保真的连续动作轨迹，交由机器人执行。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>1. <strong>训练数据</strong>：</p>\n<p> * <strong>通用多模态数据</strong>：用于保持基础视觉-语言能力，包括Cambrian-10M、LLaVA-OneVision、Describe Anything、CogVLM-SFT-311K、BLIP3-Grounding-50M等。</p>\n<p> * <strong>体现推理与动作数据</strong>：用于增强体现理解和控制能力，包括开源数据集（如NVIDIA Cosmos-Reason、ShareRobot、Robo2VLM、EmbSpatial-SFT、ManipulationVQA-60K）以及内部收集的、与机器人观察空间同源的体现推理数据集（基于AgiBot World，包含2D轨迹、物体接地标注、子任务规划数据等）。</p>\n<p> * <strong>机器人动作数据</strong>：用于训练动作分词器和策略，结合了多个开源数据集（如Octo、OpenVLA、RoboSet）以及内部收集的超过100万个episodes的真实世界机器人操作数据。</p>\n<p>2. <strong>训练资源</strong>：在128个H800 GPU上进行了4天的预训练。FACT分词器的训练使用了2.56M个机器人动作轨迹。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>1. <strong>评估环境</strong>：</p>\n<p> * <strong>离线基准评测</strong>：在ERIQ基准（自身提出）以及其他开源空间推理基准（如CV-Bench, EmbSpa, BLINK-S/R）上进行VLM推理能力评估。在模拟器（如AgiBot Genie）和真实机器人平台（如AgiBot G01, AgileX, ARX）上进行端到端任务执行评估。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> * <strong>ERIQ基准</strong>：使用分类准确率（百分比）来评估模型在15个子任务上的表现，并计算总体平均分（ERIQ-Avg）。</p>\n<p> * <strong>动作重建精度</strong>：使用均方误差（MSE）评估FACT解码器重建连续动作轨迹的保真度，并与基线方法（如FAST）进行比较。</p>\n<p> * <strong>端到端任务执行</strong>：在模拟和真实世界的长视野、多步骤操作任务中，使用任务成功率（Success Rate）作为主要评估指标。</p>"
  },
  {
    "date": "2025-12-29",
    "title": "Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation",
    "link": "http://arxiv.org/abs/2512.23864",
    "summary_markdown": "论文研究单位：Northwestern University\n\n论文概述：本文针对视觉-语言-动作（VLA）模型在接触密集的操作任务（如插入、抓取、防滑）中因无法感知物理接触而表现不佳的问题，提出了DreamTacVLA框架。该框架通过整合高分辨率视觉化触觉图像与标准的视觉（第三人称视角和腕部摄像头）及语言输入，使VLA模型具备预测未来接触状态的能力，从而实现更鲁棒、更具触觉感知能力的机器人控制。\n\n论文核心贡献点：\n1. 提出一种用于多尺度传感器数据空间对齐的新型对比损失方法，将宏观视觉、局部视觉和微观触觉感知统一到一个共享的潜在空间中。\n2. 引入一个经过预训练的触觉世界模型，将其作为一种自监督目标来“构想”未来，通过预测高分辨率触觉信号，学习对接触物理和材料交互的隐式理解。\n3. 提出一种名为“思考-构想-执行”的两阶段策略，该策略利用构想能力进行动作细化。策略首先根据当前状态构思一个草案动作，然后利用世界模型构想该动作的触觉后果，最终输出一个经过精细化调整的最终指令。\n\n论文方法描述：\n该方法是一个端到端统一框架，基于共享的LLM骨干网络构建，包含三个主要组件：多模态编码器、触觉世界模型和统一策略。\n1. 阶段一（预训练空间对齐与世界模型）：\n * 通过新颖的分层空间对齐损失，利用机器人运动学和相机标定参数，将触觉激活映射到腕部视图和第三人称视图中的对应位置，强制模型学习触觉传感器与其他相机视图之间的空间对应关系。\n * 同时，使用行为克隆损失训练一个能够从对齐的多模态输入中生成草案动作的基线策略。\n * 使用一个预先在大型未标记触觉图像序列数据集上训练好的冻结触觉世界模型（基于V-JEPA2），作为稳定的触觉特征提取器。\n2. 阶段二（通过潜在构想进行微调）：\n * 引入一个轻量级预测MLP，学习根据当前触觉嵌入和策略生成的草案动作，来“构想”未来的潜在触觉状态。\n * 这个构想出的未来触觉表示与当前状态一起被反馈给策略，使其能够基于预期的接触结果来细化其初始动作，做出更具物理依据的最终决策。\n * 整个过程形成一个“思考-构想-执行”的闭环。\n\n论文使用数据集和训练资源：\n* 构建了一个混合大规模数据集，包含来自高保真数字孪生和真实世界实验的数据，总计约200万个触觉帧，覆盖4种操作任务和9个物体。数据集构成中，模拟演示约占80%，真实世界演示约占20%。\n* 策略和多模态编码器基于预训练的CLIP模型进行初始化并微调。\n* 触觉世界模型使用预先训练的V-JEPA2模型（ViT-L/ViT-G）。\n* 动作专家是一个动作Transformer，预测7自由度动作。\n* 在模拟和真实环境中进行实验。模拟环境基于IsaacSim，并集成了基于物理的触觉传感器模型。真实世界硬件使用Dobot Xtrainer平台、高分辨率GelSight触觉传感器和Realsense D405摄像头。\n\n论文使用的评估环境和评估指标：\n* 评估环境：真实世界机器人实验平台（Dobot Xtrainer，配备GelSight触觉传感器和摄像头），以及IsaacSim模拟环境。\n* 评估指标：任务成功率，通过100次试验计算。任务包括：Peg-in-Hole（孔轴装配）、USB Insert（USB插入）、Gear Assembly（齿轮装配）、Pen Stabilization（工具稳定）。\n* 与多种基线模型进行比较：ACT、Diffusion Policy、π-0。\n* 进行了详尽的消融研究，以验证分层空间对齐损失和世界模型中触觉预测组件各自的作用。",
    "summary_html": "<p>论文研究单位：Northwestern University</p>\n\n<p>论文概述：本文针对视觉-语言-动作（VLA）模型在接触密集的操作任务（如插入、抓取、防滑）中因无法感知物理接触而表现不佳的问题，提出了DreamTacVLA框架。该框架通过整合高分辨率视觉化触觉图像与标准的视觉（第三人称视角和腕部摄像头）及语言输入，使VLA模型具备预测未来接触状态的能力，从而实现更鲁棒、更具触觉感知能力的机器人控制。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出一种用于多尺度传感器数据空间对齐的新型对比损失方法，将宏观视觉、局部视觉和微观触觉感知统一到一个共享的潜在空间中。</li><li>引入一个经过预训练的触觉世界模型，将其作为一种自监督目标来“构想”未来，通过预测高分辨率触觉信号，学习对接触物理和材料交互的隐式理解。</li><li>提出一种名为“思考-构想-执行”的两阶段策略，该策略利用构想能力进行动作细化。策略首先根据当前状态构思一个草案动作，然后利用世界模型构想该动作的触觉后果，最终输出一个经过精细化调整的最终指令。</li></ol>\n\n<p>论文方法描述：</p>\n<p>该方法是一个端到端统一框架，基于共享的LLM骨干网络构建，包含三个主要组件：多模态编码器、触觉世界模型和统一策略。</p>\n<p>1. 阶段一（预训练空间对齐与世界模型）：</p>\n<p> * 通过新颖的分层空间对齐损失，利用机器人运动学和相机标定参数，将触觉激活映射到腕部视图和第三人称视图中的对应位置，强制模型学习触觉传感器与其他相机视图之间的空间对应关系。</p>\n<p> * 同时，使用行为克隆损失训练一个能够从对齐的多模态输入中生成草案动作的基线策略。</p>\n<p> * 使用一个预先在大型未标记触觉图像序列数据集上训练好的冻结触觉世界模型（基于V-JEPA2），作为稳定的触觉特征提取器。</p>\n<p>2. 阶段二（通过潜在构想进行微调）：</p>\n<p> * 引入一个轻量级预测MLP，学习根据当前触觉嵌入和策略生成的草案动作，来“构想”未来的潜在触觉状态。</p>\n<p> * 这个构想出的未来触觉表示与当前状态一起被反馈给策略，使其能够基于预期的接触结果来细化其初始动作，做出更具物理依据的最终决策。</p>\n<p> * 整个过程形成一个“思考-构想-执行”的闭环。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>* 构建了一个混合大规模数据集，包含来自高保真数字孪生和真实世界实验的数据，总计约200万个触觉帧，覆盖4种操作任务和9个物体。数据集构成中，模拟演示约占80%，真实世界演示约占20%。</p>\n<p>* 策略和多模态编码器基于预训练的CLIP模型进行初始化并微调。</p>\n<p>* 触觉世界模型使用预先训练的V-JEPA2模型（ViT-L/ViT-G）。</p>\n<p>* 动作专家是一个动作Transformer，预测7自由度动作。</p>\n<p>* 在模拟和真实环境中进行实验。模拟环境基于IsaacSim，并集成了基于物理的触觉传感器模型。真实世界硬件使用Dobot Xtrainer平台、高分辨率GelSight触觉传感器和Realsense D405摄像头。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>* 评估环境：真实世界机器人实验平台（Dobot Xtrainer，配备GelSight触觉传感器和摄像头），以及IsaacSim模拟环境。</p>\n<p>* 评估指标：任务成功率，通过100次试验计算。任务包括：Peg-in-Hole（孔轴装配）、USB Insert（USB插入）、Gear Assembly（齿轮装配）、Pen Stabilization（工具稳定）。</p>\n<p>* 与多种基线模型进行比较：ACT、Diffusion Policy、π-0。</p>\n<p>* 进行了详尽的消融研究，以验证分层空间对齐损失和世界模型中触觉预测组件各自的作用。</p>"
  },
  {
    "date": "2025-12-29",
    "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
    "link": "http://arxiv.org/abs/2512.23162",
    "summary_markdown": "论文研究单位：\nNVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital\n\n论文概述：\n本文提出了SurgWorld框架，旨在解决手术机器人领域因视觉观测与机器人运动学配对数据稀缺而阻碍自主性发展的核心问题。该方法通过构建一个手术世界模型，利用大量未标注的手术视频，生成高质量、可泛化的合成手术视频，并结合逆动力学模型推断伪运动学数据，从而生成配对的视频-动作数据，用于增强手术视觉-语言-动作模型的政策学习。最终在真实手术机器人平台上验证了该方法能有效提升政策性能。\n\n论文核心贡献点：\n1. 构建了外科动作-文本对齐数据集，这是一个包含2447个专家标注视频片段的大型手术视频-文本语料库，专门用于支持物理AI模型的开发。\n2. 开发了首个基于最先进物理AI世界模型并经过SATA数据集微调的手术世界模型，展现出强大的泛化能力、高视频质量和逼真的动态效果。\n3. 首次将手术世界模型与机器人学习连接起来，通过逆动力学模型合成视频-动作数据，从而在手术机器人学习中实现了显著的性能提升。\n\n论文方法描述：\n方法整体流程分为三个主要部分。首先，基于Cosmos-Predict2.5世界模型，利用低秩自适应方法，使用SATA数据集和少量真实机器人轨迹数据进行微调，构建了能够根据初始帧和文本提示生成高质量、任务一致的手术视频的世界模型。其次，针对特定的机器人本体和任务，训练一个逆动力学模型，该模型能够根据世界模型生成的合成视频推断出伪运动学标签，从而生成合成的配对视频-动作数据。最后，使用真实的演示数据和合成的视频-动作数据，共同训练一个基于GR00T N1.5架构的手术视觉-语言-动作政策模型。\n\n论文使用数据集和训练资源：\n1. 数据集：构建了外科动作-文本对齐数据集，包含2447个视频片段，涵盖8种手术类型中的4种核心动作。此外，还使用了60条真实的人类遥操作演示数据，用于具体任务的评估。\n2. 训练资源：以预训练的Cosmos-Predict2.5世界模型和GR00T N1.5 VLA模型作为基础模型。使用Franka IDM检查点初始化逆动力学模型。训练涉及LoRA微调、流匹配损失函数等技术。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：在商业内窥镜手术系统上进行了真实世界验证，任务为“针拾取与交接”。同时，在SATA数据集和少量真实轨迹数据上进行了视频生成质量评估。\n2. 评估指标：\n * 手术世界模型评估：使用弗雷歇视频距离、VBench指标以及由三位外科专家根据文本-视频对齐、工具一致性和解剖结构真实性进行的定性评估。\n * 机器人政策实验：使用真实测试集上的轨迹预测均方误差作为主要评估指标，比较仅使用真实数据和结合合成数据进行训练的模型性能。",
    "summary_html": "<p>论文研究单位：</p>\n<p>NVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital</p>\n\n<p>论文概述：</p>\n<p>本文提出了SurgWorld框架，旨在解决手术机器人领域因视觉观测与机器人运动学配对数据稀缺而阻碍自主性发展的核心问题。该方法通过构建一个手术世界模型，利用大量未标注的手术视频，生成高质量、可泛化的合成手术视频，并结合逆动力学模型推断伪运动学数据，从而生成配对的视频-动作数据，用于增强手术视觉-语言-动作模型的政策学习。最终在真实手术机器人平台上验证了该方法能有效提升政策性能。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>构建了外科动作-文本对齐数据集，这是一个包含2447个专家标注视频片段的大型手术视频-文本语料库，专门用于支持物理AI模型的开发。</li><li>开发了首个基于最先进物理AI世界模型并经过SATA数据集微调的手术世界模型，展现出强大的泛化能力、高视频质量和逼真的动态效果。</li><li>首次将手术世界模型与机器人学习连接起来，通过逆动力学模型合成视频-动作数据，从而在手术机器人学习中实现了显著的性能提升。</li></ol>\n\n<p>论文方法描述：</p>\n<p>方法整体流程分为三个主要部分。首先，基于Cosmos-Predict2.5世界模型，利用低秩自适应方法，使用SATA数据集和少量真实机器人轨迹数据进行微调，构建了能够根据初始帧和文本提示生成高质量、任务一致的手术视频的世界模型。其次，针对特定的机器人本体和任务，训练一个逆动力学模型，该模型能够根据世界模型生成的合成视频推断出伪运动学标签，从而生成合成的配对视频-动作数据。最后，使用真实的演示数据和合成的视频-动作数据，共同训练一个基于GR00T N1.5架构的手术视觉-语言-动作政策模型。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>数据集：构建了外科动作-文本对齐数据集，包含2447个视频片段，涵盖8种手术类型中的4种核心动作。此外，还使用了60条真实的人类遥操作演示数据，用于具体任务的评估。</li><li>训练资源：以预训练的Cosmos-Predict2.5世界模型和GR00T N1.5 VLA模型作为基础模型。使用Franka IDM检查点初始化逆动力学模型。训练涉及LoRA微调、流匹配损失函数等技术。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：在商业内窥镜手术系统上进行了真实世界验证，任务为“针拾取与交接”。同时，在SATA数据集和少量真实轨迹数据上进行了视频生成质量评估。</li><li>评估指标：</li></ol>\n<p> * 手术世界模型评估：使用弗雷歇视频距离、VBench指标以及由三位外科专家根据文本-视频对齐、工具一致性和解剖结构真实性进行的定性评估。</p>\n<p> * 机器人政策实验：使用真实测试集上的轨迹预测均方误差作为主要评估指标，比较仅使用真实数据和结合合成数据进行训练的模型性能。</p>"
  },
  {
    "date": "2025-12-27",
    "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
    "link": "http://arxiv.org/abs/2512.22615",
    "summary_markdown": "论文研究单位：香港大学，华为技术有限公司\n论文概述：该论文研究了基于扩散大语言模型构建视觉语言模型和视觉语言动作模型的潜力，以克服自回归模型在复杂视觉规划和动态机器人控制中的限制。论文提出了Dream-VL，一种开源的基于扩散的视觉语言模型，在现有基于扩散的视觉语言模型中达到了最先进的性能；并基于Dream-VL进一步提出了Dream-VLA，一种通过持续在开放机器人数据集上预训练开发的基于扩散大语言模型的视觉语言动作模型。论文证明了扩散模型原生双向的特性使其成为视觉语言动作任务的优越基础，更适合于动作分块和并行生成，从而在下游微调中实现更快的收敛。\n\n论文核心贡献点：\n1. 提出了Dream-VL，一个在Dream 7B扩散大语言模型上构建的、使用开放数据训练的优秀扩散视觉语言模型，在各种基准测试中与顶级的自回归视觉语言模型性能相当，并且在视觉规划任务中展现出更优的潜力。\n2. 提出了Dream-VLA，这是首个通过大规模机器人预训练得到的预训练扩散视觉语言动作模型，作为下游视觉语言动作任务的强大骨干。Dream-VLA在LIBERO、SimplerEnv-Bridge和SimplerEnv-Fractal等基准上取得了顶尖性能，超越了π0和GR00T-N1等领先模型。\n3. 系统地评估了Dream-VL在通用视觉理解、视觉规划（高/低层）等方面的能力，并通过可控实验验证了基于扩散的视觉语言模型在多种训练目标的下游任务中超越了自回归基线。\n4. 开源了Dream-VL和Dream-VLA模型以促进社区研究。\n\n论文方法描述：\nDream-VL基于Dream 7B扩散大语言模型构建，采用Qwen2ViT模块编码视觉输入，并与文本特征拼接作为模型输入。模型使用与Dream 7B相同的离散扩散损失进行训练，遵循多阶段训练范式。训练收集了约1200万个开源多模态数据。\nDream-VLA在Dream-VL的基础上，通过对大规模开源机器人数据集（包括RT-1、Bridge、MimicPlay等数据集，总计超过100万个轨迹片段）进行持续的预训练得到。预训练使用离散扩散损失，采用动作分块策略来预测一系列动作。模型支持并行解码，其原生双向架构自然支持动作分块和并行生成，无需修改模型结构。\n\n论文使用数据集和训练资源：\nDream-VL的训练使用了约1200万个开源多模态数据，包含来自多样化现实世界任务的指令-响应对。数据集涵盖数学问题求解、OCR、领域特定推理等。多阶段训练的详细参数在论文表1中给出。\nDream-VLA的机器人预训练使用了超过100万个轨迹片段，数据来源于开源机器人数据集，包括RT-1、Bridge、MimicPlay、Libri、OW等。模型在4到16个NVIDIA A100或H100 GPU上进行训练。Dream-VL和Dream-VLA模型均已开源。\n\n论文使用的评估环境和评估指标：\n评估环境：在多种视觉语言理解、视觉规划和机器人控制基准上进行评估。\n评估指标：\n- 对于视觉语言理解：在多个基准上使用准确率进行评估，包括MMMU、MMMU Pro、MMStar、MMBench、SeedBench、MathVista、MathVerse、AI2D、ChartQA、InfoVQA、DocVQA、RealWorldQA、Seed-video、VideoMME、MuirBench、MLVU等。\n- 对于高/低层动作规划：\n - ViPlan基准：评估任务成功率（任务准确率）和动作准确率（有效动作占所有生成动作的比例）。\n - LIBERO基准：评估任务成功率。\n - SimplerEnv基准：评估任务成功率。",
    "summary_html": "<p>论文研究单位：香港大学，华为技术有限公司</p>\n<p>论文概述：该论文研究了基于扩散大语言模型构建视觉语言模型和视觉语言动作模型的潜力，以克服自回归模型在复杂视觉规划和动态机器人控制中的限制。论文提出了Dream-VL，一种开源的基于扩散的视觉语言模型，在现有基于扩散的视觉语言模型中达到了最先进的性能；并基于Dream-VL进一步提出了Dream-VLA，一种通过持续在开放机器人数据集上预训练开发的基于扩散大语言模型的视觉语言动作模型。论文证明了扩散模型原生双向的特性使其成为视觉语言动作任务的优越基础，更适合于动作分块和并行生成，从而在下游微调中实现更快的收敛。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了Dream-VL，一个在Dream 7B扩散大语言模型上构建的、使用开放数据训练的优秀扩散视觉语言模型，在各种基准测试中与顶级的自回归视觉语言模型性能相当，并且在视觉规划任务中展现出更优的潜力。</li><li>提出了Dream-VLA，这是首个通过大规模机器人预训练得到的预训练扩散视觉语言动作模型，作为下游视觉语言动作任务的强大骨干。Dream-VLA在LIBERO、SimplerEnv-Bridge和SimplerEnv-Fractal等基准上取得了顶尖性能，超越了π0和GR00T-N1等领先模型。</li><li>系统地评估了Dream-VL在通用视觉理解、视觉规划（高/低层）等方面的能力，并通过可控实验验证了基于扩散的视觉语言模型在多种训练目标的下游任务中超越了自回归基线。</li><li>开源了Dream-VL和Dream-VLA模型以促进社区研究。</li></ol>\n\n<p>论文方法描述：</p>\n<p>Dream-VL基于Dream 7B扩散大语言模型构建，采用Qwen2ViT模块编码视觉输入，并与文本特征拼接作为模型输入。模型使用与Dream 7B相同的离散扩散损失进行训练，遵循多阶段训练范式。训练收集了约1200万个开源多模态数据。</p>\n<p>Dream-VLA在Dream-VL的基础上，通过对大规模开源机器人数据集（包括RT-1、Bridge、MimicPlay等数据集，总计超过100万个轨迹片段）进行持续的预训练得到。预训练使用离散扩散损失，采用动作分块策略来预测一系列动作。模型支持并行解码，其原生双向架构自然支持动作分块和并行生成，无需修改模型结构。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>Dream-VL的训练使用了约1200万个开源多模态数据，包含来自多样化现实世界任务的指令-响应对。数据集涵盖数学问题求解、OCR、领域特定推理等。多阶段训练的详细参数在论文表1中给出。</p>\n<p>Dream-VLA的机器人预训练使用了超过100万个轨迹片段，数据来源于开源机器人数据集，包括RT-1、Bridge、MimicPlay、Libri、OW等。模型在4到16个NVIDIA A100或H100 GPU上进行训练。Dream-VL和Dream-VLA模型均已开源。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：在多种视觉语言理解、视觉规划和机器人控制基准上进行评估。</p>\n<p>评估指标：</p>\n<ul><li>对于视觉语言理解：在多个基准上使用准确率进行评估，包括MMMU、MMMU Pro、MMStar、MMBench、SeedBench、MathVista、MathVerse、AI2D、ChartQA、InfoVQA、DocVQA、RealWorldQA、Seed-video、VideoMME、MuirBench、MLVU等。</li><li>对于高/低层动作规划：</li></ul>\n<p> - ViPlan基准：评估任务成功率（任务准确率）和动作准确率（有效动作占所有生成动作的比例）。</p>\n<p> - LIBERO基准：评估任务成功率。</p>\n<p> - SimplerEnv基准：评估任务成功率。</p>"
  },
  {
    "date": "2025-12-27",
    "title": "VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.22539",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-12-27",
    "title": "Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding",
    "link": "http://arxiv.org/abs/2512.22519",
    "summary_markdown": "论文研究单位：\n阿肯色大学费耶特维尔分校，新加坡国立大学，维也纳工业大学，马克斯·普朗克智能系统研究所与斯图加特大学，利物浦大学。\n\n论文概述：\n本文提出了一种名为OBEYED-VLA的框架，旨在解决现有视觉-语言-动作模型在杂乱场景中语言条件视觉对齐能力退化的问题。该框架将感知与动作推理解耦，通过一个感知模块将原始RGB输入转换为任务条件化、物体中心化和几何感知的观测，供下游VLA模型使用。该感知模块包含一个基于VLM的物体中心化对齐阶段和一个互补的几何对齐阶段。框架仅在干净的单物体演示上进行微调，但能在真实的杂乱场景中显著提高模型的鲁棒性和泛化能力。\n\n论文核心贡献点：\n1. 提出了OBEYED-VLA框架，通过物体几何对齐为VLA提供语义相关和空间对齐的观测。\n2. 在大量真实世界实验中，OBEYED-VLA在多种干扰和杂乱场景下展现出比现有VLA基线更强的鲁棒性，尽管其仅在干净的单物体演示上进行微调。\n3. 证明了OBEYED-VLA能够有效泛化到具有新场景组合的未知目标物体，并保持可靠的视觉运动性能。\n\n论文方法描述：\nOBEYED-VLA框架由感知对齐模块和动作推理VLA模型两部分组成。感知模块先将多视角原始RGB输入（如基座和腕部摄像头视图）通过分割网络生成物体掩码提议，然后进行两阶段的物体中心化对齐：首先，利用VLM和“标记集”提示方法，在基座视图中根据任务指令选择相关物体的掩码区域；接着，通过跨视角区域匹配，将基座视图中的相关物体裁剪图像与腕部视图的标记增强图像一同输入VLM，以识别腕部视图中的对应区域。随后，几何对齐阶段利用深度估计器将选定区域的RGB图像转换为深度图，并将深度值映射到高动态范围的颜色空间以增强几何细节。最终，这两个视角的掩码化深度图作为感知对齐的视觉输入，连同语言指令和机器人本体感知状态，一同输入到预训练的VLA策略中生成动作轨迹。整个训练过程中，仅对VLA模型进行微调，感知模块保持冻结。\n\n论文使用数据集和训练资源：\n1. 训练数据：在真实世界桌面上收集了2000个单物体拾放演示数据（每个物体250条），涉及8种杂货物体。场景无杂乱，每个演示伴随一个自然语言拾放指令。\n2. 语言提示：使用了四种指令模板变体，均指代将目标物体放入垃圾桶。\n3. 模型资源：\n 感知模块使用Qwen3-VL（8B-Instruct版本）VLM、基于YOLO11-Seg改进的分割网络、以及Depth Anything v2深度估计器。\n 动作策略基于预训练的Pi-0和Pi-0 FAST VLA模型实例化。\n4. 训练硬件：微调使用4个NVIDIA A6000 GPU，感知模块推理使用2个A6000 GPU。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：6自由度UR10e机械臂，配备Robotiq 2F-85平行夹爪和两个同步RGB摄像头（基座视图和腕部视图）。控制频率为10 Hz。\n2. 评估指标：成功率，通常通过100次滚动的平均成功率（及95%置信区间）来衡量。\n3. 评估场景：\n - 干扰物体：评估包含1、4、7个干扰物（来自训练物体类别）场景下的语言指令跟随能力。\n - 目标缺失拒绝：评估当指令物体不存在时，策略拒绝执行抓取的能力。\n - 空间推理：评估对空间关系指令（例如“左边的物体”）的跟随能力。\n - 背景变化鲁棒性：评估在四种不同背景（如不同桌布、背景板、彩色纸张）下执行单物体任务的能力。\n - 未知物体泛化：评估在包含7个未知类别物体的场景中，对目标未知物体的拾放能力。",
    "summary_html": "<p>论文研究单位：</p>\n<p>阿肯色大学费耶特维尔分校，新加坡国立大学，维也纳工业大学，马克斯·普朗克智能系统研究所与斯图加特大学，利物浦大学。</p>\n\n<p>论文概述：</p>\n<p>本文提出了一种名为OBEYED-VLA的框架，旨在解决现有视觉-语言-动作模型在杂乱场景中语言条件视觉对齐能力退化的问题。该框架将感知与动作推理解耦，通过一个感知模块将原始RGB输入转换为任务条件化、物体中心化和几何感知的观测，供下游VLA模型使用。该感知模块包含一个基于VLM的物体中心化对齐阶段和一个互补的几何对齐阶段。框架仅在干净的单物体演示上进行微调，但能在真实的杂乱场景中显著提高模型的鲁棒性和泛化能力。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了OBEYED-VLA框架，通过物体几何对齐为VLA提供语义相关和空间对齐的观测。</li><li>在大量真实世界实验中，OBEYED-VLA在多种干扰和杂乱场景下展现出比现有VLA基线更强的鲁棒性，尽管其仅在干净的单物体演示上进行微调。</li><li>证明了OBEYED-VLA能够有效泛化到具有新场景组合的未知目标物体，并保持可靠的视觉运动性能。</li></ol>\n\n<p>论文方法描述：</p>\n<p>OBEYED-VLA框架由感知对齐模块和动作推理VLA模型两部分组成。感知模块先将多视角原始RGB输入（如基座和腕部摄像头视图）通过分割网络生成物体掩码提议，然后进行两阶段的物体中心化对齐：首先，利用VLM和“标记集”提示方法，在基座视图中根据任务指令选择相关物体的掩码区域；接着，通过跨视角区域匹配，将基座视图中的相关物体裁剪图像与腕部视图的标记增强图像一同输入VLM，以识别腕部视图中的对应区域。随后，几何对齐阶段利用深度估计器将选定区域的RGB图像转换为深度图，并将深度值映射到高动态范围的颜色空间以增强几何细节。最终，这两个视角的掩码化深度图作为感知对齐的视觉输入，连同语言指令和机器人本体感知状态，一同输入到预训练的VLA策略中生成动作轨迹。整个训练过程中，仅对VLA模型进行微调，感知模块保持冻结。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>训练数据：在真实世界桌面上收集了2000个单物体拾放演示数据（每个物体250条），涉及8种杂货物体。场景无杂乱，每个演示伴随一个自然语言拾放指令。</li><li>语言提示：使用了四种指令模板变体，均指代将目标物体放入垃圾桶。</li><li>模型资源：</li></ol>\n<p> 感知模块使用Qwen3-VL（8B-Instruct版本）VLM、基于YOLO11-Seg改进的分割网络、以及Depth Anything v2深度估计器。</p>\n<p> 动作策略基于预训练的Pi-0和Pi-0 FAST VLA模型实例化。</p>\n<p>4. 训练硬件：微调使用4个NVIDIA A6000 GPU，感知模块推理使用2个A6000 GPU。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：6自由度UR10e机械臂，配备Robotiq 2F-85平行夹爪和两个同步RGB摄像头（基座视图和腕部视图）。控制频率为10 Hz。</li><li>评估指标：成功率，通常通过100次滚动的平均成功率（及95%置信区间）来衡量。</li><li>评估场景：</li></ol>\n<p> - 干扰物体：评估包含1、4、7个干扰物（来自训练物体类别）场景下的语言指令跟随能力。</p>\n<p> - 目标缺失拒绝：评估当指令物体不存在时，策略拒绝执行抓取的能力。</p>\n<p> - 空间推理：评估对空间关系指令（例如“左边的物体”）的跟随能力。</p>\n<p> - 背景变化鲁棒性：评估在四种不同背景（如不同桌布、背景板、彩色纸张）下执行单物体任务的能力。</p>\n<p> - 未知物体泛化：评估在包含7个未知类别物体的场景中，对目标未知物体的拾放能力。</p>"
  },
  {
    "date": "2025-12-27",
    "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.22414",
    "summary_markdown": "论文研究单位：\nPhysical Intelligence, Georgia Institute of Technology\n\n论文概述：\n本研究探讨了在视觉-语言-动作模型中，从人类视频数据到机器人技能转移能力的涌现现象。研究者提出了一种简单的协同训练方法，将人类视频数据视为与机器人数据类似的一种“具身体现”。研究发现，当VLA模型在足够多样的场景、任务和具身体现上进行预训练后，人类到机器人的转移能力会自然涌现。这种能力使得机器人能够泛化到未见过的场景、新物体类别以及具有新语义结构的任务。\n\n论文核心贡献点：\n1. 提出并验证了人类到机器人技能转移是一种随着多样化VLA预训练规模而涌现的特性。\n2. 引入了一种名为 π0.5+ego 的简单协同训练方法，该方法将人类数据与相关机器人数据以50/50的比例混合进行微调，无需进行显式的域对齐。\n3. 通过实验证明，充分多样化的预训练会产生对具身体现不敏感的表征，从而使人类和机器人数据的潜在表示自然对齐，促进了转移。\n4. 通过一系列基准测试（涵盖场景、物体和任务泛化）量化了转移效果，并发现使用人类数据可以使仅在人类数据中出现的泛化设置性能几乎翻倍。\n\n论文方法描述：\n方法基于现有的 π0.5 VLA模型架构。首先，收集具身化的人类视频数据，操作者佩戴头戴式摄像机（以及可选的腕戴式摄像机）。使用视觉SLAM重建头部摄像机的6D运动和双手的3D关键点，并通过相对末端执行器动作（类似于机器人末端执行器轨迹）和预测高层子任务语言注释来构造动作和标注，使其与机器人数据的训练目标保持一致。在微调阶段，采用协同训练方法，将针对泛化任务收集的人类数据与最邻近任务的机器人数据以50/50的比例混合。训练目标包括：1）通过流匹配对连续动作进行低层动作预测；2）通过语言标记的下一个令牌预测进行高层子任务预测。该方法未引入任何显式的人类-机器人对齐步骤。\n\n论文使用数据集和训练资源：\n数据集：使用了一个大规模、多样化的机器人遥操作数据集进行VLA模型的预训练，该数据涵盖多种场景、任务和机器人具身体现（包括ARX、移动ARX及其他非目标机器人）。为四个泛化基准任务（清理调味架、整理梳妆台、清理餐桌、按颜色分拣鸡蛋）专门收集了约14小时的具身化人类视频数据（头戴及腕戴摄像机）用于微调。\n训练资源：预训练和微调基于 π0.5 模型，该模型融合了视觉语言模型架构和行为克隆。具体硬件配置未在提供的HTML片段中明确说明。\n\n论文使用的评估环境和评估指标：\n评估环境：在真实的物理机器人（ARX机器人）上对微调后的策略进行评估，测试其在四个基准任务上的性能。这些任务旨在测试仅通过人类数据引入的新概念的泛化能力。\n评估指标：\n* **场景泛化（Spice， Dresser）**： 二进制成功率。\n* **物体泛化（Bussing）**： 正确放置物体的数量，并归一化到[0， 1]范围。\n* **任务泛化（Sort Eggs）**： 正确分拣（按颜色放置）并盖好盒盖的鸡蛋数量，并归一化到[0， 1]范围。\n* 此外，通过t-SNE可视化分析VLA骨干网络输出嵌入，以探究人类和机器人数据潜在表征的对齐情况，作为衡量转移能力的间接指标。",
    "summary_html": "<p>论文研究单位：</p>\n<p>Physical Intelligence, Georgia Institute of Technology</p>\n\n<p>论文概述：</p>\n<p>本研究探讨了在视觉-语言-动作模型中，从人类视频数据到机器人技能转移能力的涌现现象。研究者提出了一种简单的协同训练方法，将人类视频数据视为与机器人数据类似的一种“具身体现”。研究发现，当VLA模型在足够多样的场景、任务和具身体现上进行预训练后，人类到机器人的转移能力会自然涌现。这种能力使得机器人能够泛化到未见过的场景、新物体类别以及具有新语义结构的任务。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出并验证了人类到机器人技能转移是一种随着多样化VLA预训练规模而涌现的特性。</li><li>引入了一种名为 π0.5+ego 的简单协同训练方法，该方法将人类数据与相关机器人数据以50/50的比例混合进行微调，无需进行显式的域对齐。</li><li>通过实验证明，充分多样化的预训练会产生对具身体现不敏感的表征，从而使人类和机器人数据的潜在表示自然对齐，促进了转移。</li><li>通过一系列基准测试（涵盖场景、物体和任务泛化）量化了转移效果，并发现使用人类数据可以使仅在人类数据中出现的泛化设置性能几乎翻倍。</li></ol>\n\n<p>论文方法描述：</p>\n<p>方法基于现有的 π0.5 VLA模型架构。首先，收集具身化的人类视频数据，操作者佩戴头戴式摄像机（以及可选的腕戴式摄像机）。使用视觉SLAM重建头部摄像机的6D运动和双手的3D关键点，并通过相对末端执行器动作（类似于机器人末端执行器轨迹）和预测高层子任务语言注释来构造动作和标注，使其与机器人数据的训练目标保持一致。在微调阶段，采用协同训练方法，将针对泛化任务收集的人类数据与最邻近任务的机器人数据以50/50的比例混合。训练目标包括：1）通过流匹配对连续动作进行低层动作预测；2）通过语言标记的下一个令牌预测进行高层子任务预测。该方法未引入任何显式的人类-机器人对齐步骤。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>数据集：使用了一个大规模、多样化的机器人遥操作数据集进行VLA模型的预训练，该数据涵盖多种场景、任务和机器人具身体现（包括ARX、移动ARX及其他非目标机器人）。为四个泛化基准任务（清理调味架、整理梳妆台、清理餐桌、按颜色分拣鸡蛋）专门收集了约14小时的具身化人类视频数据（头戴及腕戴摄像机）用于微调。</p>\n<p>训练资源：预训练和微调基于 π0.5 模型，该模型融合了视觉语言模型架构和行为克隆。具体硬件配置未在提供的HTML片段中明确说明。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：在真实的物理机器人（ARX机器人）上对微调后的策略进行评估，测试其在四个基准任务上的性能。这些任务旨在测试仅通过人类数据引入的新概念的泛化能力。</p>\n<p>评估指标：</p>\n<p>* <strong>场景泛化（Spice， Dresser）</strong>： 二进制成功率。</p>\n<p>* <strong>物体泛化（Bussing）</strong>： 正确放置物体的数量，并归一化到[0， 1]范围。</p>\n<p>* <strong>任务泛化（Sort Eggs）</strong>： 正确分拣（按颜色放置）并盖好盒盖的鸡蛋数量，并归一化到[0， 1]范围。</p>\n<p>* 此外，通过t-SNE可视化分析VLA骨干网络输出嵌入，以探究人类和机器人数据潜在表征的对齐情况，作为衡量转移能力的间接指标。</p>"
  },
  {
    "date": "2025-12-22",
    "title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA",
    "link": "http://arxiv.org/abs/2512.22208",
    "summary_markdown": "论文研究单位：\n- 主要单位：东北大学，哈佛大学，康奈尔大学，杜兰大学，华盛顿大学，Roboraction.ai，Futurewei，AIBAO LLC。\n\n论文概述：\n- 本文介绍了Moxin 7B，一个遵循模型开放框架的完全开源大语言模型。论文围绕基于Moxin的三个变体模型展开：面向视觉语言任务的Moxin-VLM、面向视觉语言动作任务的Moxin-VLA以及面向中文能力增强的Moxin-Chinese。论文强调采用开源框架和数据进行训练，并发布模型、数据和代码。\n\n论文核心贡献点：\n- 开发了多模态Moxin模型，包括Moxin-VLM、Moxin-VLA和Moxin-Chinese，以赋予Moxin在不同任务中的多样化能力。对于Moxin-Chinese，通过扩展中文词汇表和用更多中文数据进行后训练，提升了中文编码能力和效率。\n- 基于Moxin作为LLM骨干，开发了Moxin-VLM。基于开源VLM框架Prismatic VLMs，在完全开源的数据集上，以Moxin为LLM骨干，DINOv2和SigLIP为视觉骨干进行训练。实验表明该VLM优于其他VLM模型或LLM骨干。\n- 进一步使用来自OpenVLA-OFT的配方对Moxin-VLM进行微调，开发用于机器人控制的Moxin-VLA。该方法利用Moxin-VLM骨干和高效的OpenVLA-OFT微调方法简化了训练范式。Moxin-VLA在综合评估中取得了优越性能。\n\n论文方法描述：\n- Moxin-VLM：采用Prismatic VLMs框架，视觉骨干使用DINOv2和SigLIP并融合其特征，LLM骨干使用Moxin-7B-Base。采用通用VLM架构，包含视觉表示骨干、视觉语言投影器和语言模型。使用LLaVa v1.5数据混合（包含约55.8万样本的标题数据集和约66.5万样本的多模态指令调优数据）进行单阶段训练，共训练两个epoch，冻结视觉表示模块。\n- Moxin-VLA：以Moxin-VLM作为骨干，研究两种训练策略：一种是在Open X-Embodiment数据集上进行大规模通用预训练，另一种是直接从Moxin-VLM检查点进行微调。使用OpenVLA-OFT配方，采用并行解码和动作分块来替换标准自回归动作预测。训练时使用了LoRA，秩为32。输入包括两帧图像历史记录和机器人的本体感知状态。\n- Moxin-Chinese：为解决原词汇表对中文支持不足的问题，扩展了Moxin的词汇表，增加了中文token，并使用SentencePiece训练了中文BPE词汇表。随后使用多个高质量中文数据集进行持续训练，并使用中英翻译数据集进行微调以提升翻译能力。\n\n论文使用数据集和训练资源：\n- Moxin-VLM：使用LLaVa v1.5数据混合，包括标题数据集（如Conceptual Captions, LAION）和多模态指令调优数据（如LLaVa合成数据、VQA数据、多选VQA数据、标题数据、指代表达数据、ShareGPT语言数据）。\n- Moxin-VLA：训练数据来自Open X-Embodiment数据集混合，重点关注Franka Emika Panda平台，包含探索和复杂操作。具体数据集包括CMU Franka Exploration, Berkeley Cable Routing, Taco Play, VIOLA等，共包含超过100万个真实世界机器人轨迹。\n- Moxin-Chinese：词汇扩展使用WuDaoCorpus2，训练数据包括WanJuan, gutenberg-books, Chinese-Data-Distill-From-R1等。微调翻译能力使用Garsa3112/ChineseEnglishTranslationDataset, FradSer/DeepSeek-R1-Distilled-Translate-en-zh_CN-39k-Alpaca-GPT4-without-Think等。\n- 训练资源：Moxin-VLM训练细节未明确说明硬件。Moxin-VLA在单个节点（8个H100 GPU）上训练了大约两周（约9万步）。Moxin-Chinese训练资源未明确说明。\n\n论文使用的评估环境和评估指标：\n- VLM评估：使用Prismatic VLMs的评估套件。评估领域包括开放式视觉问答（VizWiz，GQA）、定位（RefCOCO, RefCOCO+, RefCOCOg, OCID-Ref）、挑战集（闭集预测，如VSR，TallyQA，POPE）。评估指标主要为准确率（Accuracy）。\n- VLA评估：在LIBERO仿真环境中评估。实验设置与OpenVLA-OFT基线一致。评估任务类别包括空间（Spatial）、物体（Object）、目标（Goal）、长程（Long）任务。评估指标为成功率（Success Rate，%）。\n- Moxin-Chinese评估：使用LM-harness框架在CMMLU和CEVAL中文数据集上进行评估。评估指标为准确率。\n- 评估环境：VLM和中文评估主要在软件框架（如LM-harness）和特定基准数据集上进行。VLA评估在LIBERO仿真环境中进行，使用8个NVIDIA H100 GPU进行微调（批量大小64，学习率3e-4）。",
    "summary_html": "<p>论文研究单位：</p>\n<ul><li>主要单位：东北大学，哈佛大学，康奈尔大学，杜兰大学，华盛顿大学，Roboraction.ai，Futurewei，AIBAO LLC。</li></ul>\n\n<p>论文概述：</p>\n<ul><li>本文介绍了Moxin 7B，一个遵循模型开放框架的完全开源大语言模型。论文围绕基于Moxin的三个变体模型展开：面向视觉语言任务的Moxin-VLM、面向视觉语言动作任务的Moxin-VLA以及面向中文能力增强的Moxin-Chinese。论文强调采用开源框架和数据进行训练，并发布模型、数据和代码。</li></ul>\n\n<p>论文核心贡献点：</p>\n<ul><li>开发了多模态Moxin模型，包括Moxin-VLM、Moxin-VLA和Moxin-Chinese，以赋予Moxin在不同任务中的多样化能力。对于Moxin-Chinese，通过扩展中文词汇表和用更多中文数据进行后训练，提升了中文编码能力和效率。</li><li>基于Moxin作为LLM骨干，开发了Moxin-VLM。基于开源VLM框架Prismatic VLMs，在完全开源的数据集上，以Moxin为LLM骨干，DINOv2和SigLIP为视觉骨干进行训练。实验表明该VLM优于其他VLM模型或LLM骨干。</li><li>进一步使用来自OpenVLA-OFT的配方对Moxin-VLM进行微调，开发用于机器人控制的Moxin-VLA。该方法利用Moxin-VLM骨干和高效的OpenVLA-OFT微调方法简化了训练范式。Moxin-VLA在综合评估中取得了优越性能。</li></ul>\n\n<p>论文方法描述：</p>\n<ul><li>Moxin-VLM：采用Prismatic VLMs框架，视觉骨干使用DINOv2和SigLIP并融合其特征，LLM骨干使用Moxin-7B-Base。采用通用VLM架构，包含视觉表示骨干、视觉语言投影器和语言模型。使用LLaVa v1.5数据混合（包含约55.8万样本的标题数据集和约66.5万样本的多模态指令调优数据）进行单阶段训练，共训练两个epoch，冻结视觉表示模块。</li><li>Moxin-VLA：以Moxin-VLM作为骨干，研究两种训练策略：一种是在Open X-Embodiment数据集上进行大规模通用预训练，另一种是直接从Moxin-VLM检查点进行微调。使用OpenVLA-OFT配方，采用并行解码和动作分块来替换标准自回归动作预测。训练时使用了LoRA，秩为32。输入包括两帧图像历史记录和机器人的本体感知状态。</li><li>Moxin-Chinese：为解决原词汇表对中文支持不足的问题，扩展了Moxin的词汇表，增加了中文token，并使用SentencePiece训练了中文BPE词汇表。随后使用多个高质量中文数据集进行持续训练，并使用中英翻译数据集进行微调以提升翻译能力。</li></ul>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>Moxin-VLM：使用LLaVa v1.5数据混合，包括标题数据集（如Conceptual Captions, LAION）和多模态指令调优数据（如LLaVa合成数据、VQA数据、多选VQA数据、标题数据、指代表达数据、ShareGPT语言数据）。</li><li>Moxin-VLA：训练数据来自Open X-Embodiment数据集混合，重点关注Franka Emika Panda平台，包含探索和复杂操作。具体数据集包括CMU Franka Exploration, Berkeley Cable Routing, Taco Play, VIOLA等，共包含超过100万个真实世界机器人轨迹。</li><li>Moxin-Chinese：词汇扩展使用WuDaoCorpus2，训练数据包括WanJuan, gutenberg-books, Chinese-Data-Distill-From-R1等。微调翻译能力使用Garsa3112/ChineseEnglishTranslationDataset, FradSer/DeepSeek-R1-Distilled-Translate-en-zh_CN-39k-Alpaca-GPT4-without-Think等。</li><li>训练资源：Moxin-VLM训练细节未明确说明硬件。Moxin-VLA在单个节点（8个H100 GPU）上训练了大约两周（约9万步）。Moxin-Chinese训练资源未明确说明。</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>VLM评估：使用Prismatic VLMs的评估套件。评估领域包括开放式视觉问答（VizWiz，GQA）、定位（RefCOCO, RefCOCO+, RefCOCOg, OCID-Ref）、挑战集（闭集预测，如VSR，TallyQA，POPE）。评估指标主要为准确率（Accuracy）。</li><li>VLA评估：在LIBERO仿真环境中评估。实验设置与OpenVLA-OFT基线一致。评估任务类别包括空间（Spatial）、物体（Object）、目标（Goal）、长程（Long）任务。评估指标为成功率（Success Rate，%）。</li><li>Moxin-Chinese评估：使用LM-harness框架在CMMLU和CEVAL中文数据集上进行评估。评估指标为准确率。</li><li>评估环境：VLM和中文评估主要在软件框架（如LM-harness）和特定基准数据集上进行。VLA评估在LIBERO仿真环境中进行，使用8个NVIDIA H100 GPU进行微调（批量大小64，学习率3e-4）。</li></ul>"
  },
  {
    "date": "2025-12-26",
    "title": "StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision",
    "link": "http://arxiv.org/abs/2512.21970",
    "summary_markdown": "论文研究单位：Galbot、北京大学、香港大学、中国科学院自动化研究所、北京智源人工智能研究院、厦门大学马来西亚分校\n\n论文概述：本文提出了StereoVLA，一个利用立体视觉增强空间感知的视觉-语言-动作模型。针对现有视觉-语言-动作模型多依赖单目或特定多相机设置，未能充分利用立体图像对所提供的丰富几何线索的问题，StereoVLA通过提取并融合立体图像的几何特征和语义特征，并引入交互区域深度估计辅助任务，显著提升了模型在需要精细空间感知的机器人操作任务中的性能。\n\n论文核心贡献点：\n1. 提出了StereoVLA，一个专门利用立体视觉几何线索来增强机器人操作能力的视觉-语言-动作模型。\n2. 设计了几何-语义特征提取模块，该模块通过融合来自立体视觉基础模型的稠密几何特征和来自单目视觉基础模型的语义特征，生成兼具几何精度和语义丰富性的视觉表征。\n3. 提出了交互区域深度估计辅助训练任务，通过引导模型聚焦于与操作相关的关键区域（如夹爪和物体周围）的深度信息，增强其精细几何感知能力，并加速模型收敛。\n4. 通过大量实验证明，StereoVLA在立体相机设置下的多种操作任务上显著优于基线模型，并且在相机姿态变化下展现出更强的鲁棒性。\n\n论文方法描述：\n1. 模型架构：整体采用视觉-语言-动作框架。输入立体图像对和语言指令。\n2. 几何-语义特征提取：使用FoundationStereo模型从立体图像对中提取过滤后的成本体积作为几何特征。同时，仅使用左视图，通过SigLIP和DINOv2模型提取语义丰富的特征。将两种特征在通道维度上进行拼接和融合，形成混合视觉表征。\n3. 动作与辅助任务预测：视觉与语言特征由大型语言模型InternLM-1.8B处理。一个动作专家网络基于流匹配方法预测末端执行器的增量位姿。为增强几何学习，在训练时引入交互区域深度估计作为辅助任务，该任务预测在物体-夹爪交互区域内随机采样点的度量深度。\n4. 训练策略：采用渐进式动作生成，模型先预测目标物体的二维边界框和夹爪的关键帧位姿，再指导动作块的预测。总体损失函数包含动作预测、深度估计、边界框预测和关键帧位姿预测四部分。\n\n论文使用数据集和训练资源：\n1. 数据集：由于缺乏大规模的立体机器人数据集，论文使用MuJoCo模拟器生成500万条合成抓放轨迹。使用Isaac Sim渲染立体观测，相机基线、内参矩阵与真实Zed Mini相机参数有5%的随机变化。此外，还加入了互联网规模的GRIT数据集以增强语义基础能力。\n2. 训练资源：在32块NVIDIA H800 GPU上进行了16万步训练，批次大小为384，学习率为1.6e-4。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：在真实机器人平台上进行评估，使用Franka机械臂，工作空间大小为0.5m × 0.4m。评估使用了多种相机姿态随机化范围。\n2. 评估指标：主要指标为任务成功率。评估任务包括通用抓放任务、抓取不同朝向的条形物体、抓取中型和小型物体。每项任务进行15次试验。为了严格评估性能，采用了更严苛的评估准则：每个试验仅允许单次执行（抓取前闭合一次夹爪，抓取后张开一次夹爪），禁用用于遮掩不准确决策的“夹爪粘连”启发式方法，并且仅当任务完全完成时才计为成功。",
    "summary_html": "<p>论文研究单位：Galbot、北京大学、香港大学、中国科学院自动化研究所、北京智源人工智能研究院、厦门大学马来西亚分校</p>\n\n<p>论文概述：本文提出了StereoVLA，一个利用立体视觉增强空间感知的视觉-语言-动作模型。针对现有视觉-语言-动作模型多依赖单目或特定多相机设置，未能充分利用立体图像对所提供的丰富几何线索的问题，StereoVLA通过提取并融合立体图像的几何特征和语义特征，并引入交互区域深度估计辅助任务，显著提升了模型在需要精细空间感知的机器人操作任务中的性能。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了StereoVLA，一个专门利用立体视觉几何线索来增强机器人操作能力的视觉-语言-动作模型。</li><li>设计了几何-语义特征提取模块，该模块通过融合来自立体视觉基础模型的稠密几何特征和来自单目视觉基础模型的语义特征，生成兼具几何精度和语义丰富性的视觉表征。</li><li>提出了交互区域深度估计辅助训练任务，通过引导模型聚焦于与操作相关的关键区域（如夹爪和物体周围）的深度信息，增强其精细几何感知能力，并加速模型收敛。</li><li>通过大量实验证明，StereoVLA在立体相机设置下的多种操作任务上显著优于基线模型，并且在相机姿态变化下展现出更强的鲁棒性。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>模型架构：整体采用视觉-语言-动作框架。输入立体图像对和语言指令。</li><li>几何-语义特征提取：使用FoundationStereo模型从立体图像对中提取过滤后的成本体积作为几何特征。同时，仅使用左视图，通过SigLIP和DINOv2模型提取语义丰富的特征。将两种特征在通道维度上进行拼接和融合，形成混合视觉表征。</li><li>动作与辅助任务预测：视觉与语言特征由大型语言模型InternLM-1.8B处理。一个动作专家网络基于流匹配方法预测末端执行器的增量位姿。为增强几何学习，在训练时引入交互区域深度估计作为辅助任务，该任务预测在物体-夹爪交互区域内随机采样点的度量深度。</li><li>训练策略：采用渐进式动作生成，模型先预测目标物体的二维边界框和夹爪的关键帧位姿，再指导动作块的预测。总体损失函数包含动作预测、深度估计、边界框预测和关键帧位姿预测四部分。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>数据集：由于缺乏大规模的立体机器人数据集，论文使用MuJoCo模拟器生成500万条合成抓放轨迹。使用Isaac Sim渲染立体观测，相机基线、内参矩阵与真实Zed Mini相机参数有5%的随机变化。此外，还加入了互联网规模的GRIT数据集以增强语义基础能力。</li><li>训练资源：在32块NVIDIA H800 GPU上进行了16万步训练，批次大小为384，学习率为1.6e-4。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：在真实机器人平台上进行评估，使用Franka机械臂，工作空间大小为0.5m × 0.4m。评估使用了多种相机姿态随机化范围。</li><li>评估指标：主要指标为任务成功率。评估任务包括通用抓放任务、抓取不同朝向的条形物体、抓取中型和小型物体。每项任务进行15次试验。为了严格评估性能，采用了更严苛的评估准则：每个试验仅允许单次执行（抓取前闭合一次夹爪，抓取后张开一次夹爪），禁用用于遮掩不准确决策的“夹爪粘连”启发式方法，并且仅当任务完全完成时才计为成功。</li></ol>"
  },
  {
    "date": "2025-12-23",
    "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
    "link": "http://arxiv.org/abs/2512.20276",
    "summary_markdown": "论文研究单位：中国科学技术大学计算机科学与技术学院，中国科学技术大学苏州高等研究院，IEIT SYSTEMS Co., Ltd.\n\n论文概述：论文提出了ActionFlow，一个面向边缘计算平台的系统级视觉语言动作模型加速推理框架。针对VLA模型在动态真实世界部署时因自回归解码导致的高延迟问题，ActionFlow通过跨请求流水线策略，将内部Prefill和Decode微请求进行批量处理，并设计相应的算子与内存管理机制，以提高硬件利用率，从而实现无需重新训练的性能加速，使VLA模型能在边缘硬件上实现实时动态操控。\n\n论文核心贡献点：\n1. 提出跨请求流水线策略，将单个VLA任务视为宏流水线，对内部的Prefill和Decode微请求进行批处理以提高计算效率。\n2. 设计并实现了定制的\"跨请求状态打包前向\"算子，将解码阶段一系列低效的、内存受限的矩阵-向量操作聚合为单个计算受限的矩阵-矩阵操作。\n3. 构建了ActionFlow，一个针对资源受限边缘设备优化的端到端推理框架，弥补了VLA高效部署在系统级优化方面的空白。\n\n论文方法描述：\n方法核心是跨请求流水线策略，将连续的K步生成过程视为K级流水线。在每轮计算中，将当前请求的Prefill阶段与K-1个历史请求的某个解码阶段打包成一个计算批次，形成跨请求状态。为了支持此调度，设计了跨请求状态打包前向算子和统一的KV环形缓冲区。该算子通过融合内核，将旋转位置编码和KV写入操作合并，并配合变长注意力机制，高效处理不同请求的上下文历史。统一的KV环形缓冲区将所有活动请求的KV状态存储在单个物理连续的内存区域，并通过原地KV移位操作管理缓冲区，避免了动态内存分配和CPU-GPU同步开销。\n\n论文使用数据集和训练资源：使用OpenVLA-7B模型进行评估。实验使用了LIBERO基准套件进行功能正确性验证。未提及额外的训练或特定的数据集用于模型本身，重点在于推理加速。\n\n论文使用的评估环境和评估指标：\n评估环境：\n硬件平台：NVIDIA Jetson AGX Orin (64GB) 和 NVIDIA RTX 5090。\n软件栈：PyTorch 2.6.0, Transformers库 4.49.0, CUDA 12.6。\n评估指标：\n核心性能指标：每秒帧数。\n功能正确性指标：任务成功率，在LIBERO基准套件的多个任务类别上进行评估。",
    "summary_html": "<p>论文研究单位：中国科学技术大学计算机科学与技术学院，中国科学技术大学苏州高等研究院，IEIT SYSTEMS Co., Ltd.</p>\n\n<p>论文概述：论文提出了ActionFlow，一个面向边缘计算平台的系统级视觉语言动作模型加速推理框架。针对VLA模型在动态真实世界部署时因自回归解码导致的高延迟问题，ActionFlow通过跨请求流水线策略，将内部Prefill和Decode微请求进行批量处理，并设计相应的算子与内存管理机制，以提高硬件利用率，从而实现无需重新训练的性能加速，使VLA模型能在边缘硬件上实现实时动态操控。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出跨请求流水线策略，将单个VLA任务视为宏流水线，对内部的Prefill和Decode微请求进行批处理以提高计算效率。</li><li>设计并实现了定制的\"跨请求状态打包前向\"算子，将解码阶段一系列低效的、内存受限的矩阵-向量操作聚合为单个计算受限的矩阵-矩阵操作。</li><li>构建了ActionFlow，一个针对资源受限边缘设备优化的端到端推理框架，弥补了VLA高效部署在系统级优化方面的空白。</li></ol>\n\n<p>论文方法描述：</p>\n<p>方法核心是跨请求流水线策略，将连续的K步生成过程视为K级流水线。在每轮计算中，将当前请求的Prefill阶段与K-1个历史请求的某个解码阶段打包成一个计算批次，形成跨请求状态。为了支持此调度，设计了跨请求状态打包前向算子和统一的KV环形缓冲区。该算子通过融合内核，将旋转位置编码和KV写入操作合并，并配合变长注意力机制，高效处理不同请求的上下文历史。统一的KV环形缓冲区将所有活动请求的KV状态存储在单个物理连续的内存区域，并通过原地KV移位操作管理缓冲区，避免了动态内存分配和CPU-GPU同步开销。</p>\n\n<p>论文使用数据集和训练资源：使用OpenVLA-7B模型进行评估。实验使用了LIBERO基准套件进行功能正确性验证。未提及额外的训练或特定的数据集用于模型本身，重点在于推理加速。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<p>硬件平台：NVIDIA Jetson AGX Orin (64GB) 和 NVIDIA RTX 5090。</p>\n<p>软件栈：PyTorch 2.6.0, Transformers库 4.49.0, CUDA 12.6。</p>\n<p>评估指标：</p>\n<p>核心性能指标：每秒帧数。</p>\n<p>功能正确性指标：任务成功率，在LIBERO基准套件的多个任务类别上进行评估。</p>"
  },
  {
    "date": "2025-12-23",
    "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.20188",
    "summary_markdown": "论文研究单位：Astribot Team (astribot_ai@astribot.com)\n\n论文概述：论文提出了一种名为DuoCore-FS的异步快慢视觉-语言-动作策略框架，用于全身机器人操作。该框架旨在解决现有视觉-语言-动作系统中由于大型视觉语言模型推理速度慢而导致的控制频率受限问题。它将系统组织为一个高频生成动作的快通路和一个用于丰富语义推理的慢通路，通过一种潜在表示缓冲桥和全身动作标记器实现真正的异步并行执行和端到端联合训练。\n\n论文核心贡献点：\n1. 提出一种真正并行且异步的快-慢执行架构，快通路负责高频连续全身动作生成，慢通路进行低频语义理解和高级推理，整体动作生成频率由快通路决定。\n2. 引入一个由视觉-语言-动作跨模态对齐支持的桥接缓冲，慢系统向其中更新语义和推理潜在表示，为快通路提供高级语义指导。\n3. 提出一个用于全身操作的全身动作标记器，提供捕获高维全身关节配置结构化模式的紧凑统一表示。\n4. 实现了两个通路的端到端联合训练，使慢模块和快模块能够共同优化，促进紧密的语义-控制对齐。\n\n论文方法描述：\n方法基于一个异步运行的双子系统架构：慢系统和快系统。慢系统是一个大型视觉语言模型，运行在低频，用于从视觉观察和任务提示生成结构化语义隐藏状态（包括指令语义和动作推理表示）。快系统运行在高频，是一个轻量级基于扩散的策略，用于生成连续全身动作块。一个桥接缓冲连接两个子系统，存储慢系统产生的语义表示，快系统从中获取最新的潜在表示并与实时感知输入融合，通过一个基于Transformer的扩散策略解码器生成动作。方法还包含一个用于全身操作的几何感知动作标记器，基于残差VQ-VAE设计，将连续动作分解为位置、旋转和抓取流进行标记化。训练采用跨时间尺度的联合训练策略，分两阶段：第一阶段独立训练慢系统；第二阶段引入跨时间尺度采样策略来模拟部署时的异步行为，联合训练快系统和慢系统。\n\n论文使用数据集和训练资源：\n在商业爆米花售货亭场景中收集了1780条演示轨迹用于训练，涉及一个长视界的爆米花舀取任务和一个短视界的关闭饮料柜门任务。训练使用24块NVIDIA H100 GPU。\n\n论文使用的评估环境和评估指标：\n评估在Astribot S1移动双臂操作平台上进行，该平台包括两个7自由度手臂、一个4自由度躯干、一个2自由度头部和一个3自由度全向移动底盘。评估指标为每个子任务的条件成功率（仅当子任务可达时测量其成功率）和整体任务成功率。评估包括分布内测试、分布外泛化测试、异常场景测试和语言指令遵循能力测试。推断性能在单个NVIDIA RTX 4090 GPU上评估，测量推断频率。",
    "summary_html": "<p>论文研究单位：Astribot Team (astribot_ai@astribot.com)</p>\n\n<p>论文概述：论文提出了一种名为DuoCore-FS的异步快慢视觉-语言-动作策略框架，用于全身机器人操作。该框架旨在解决现有视觉-语言-动作系统中由于大型视觉语言模型推理速度慢而导致的控制频率受限问题。它将系统组织为一个高频生成动作的快通路和一个用于丰富语义推理的慢通路，通过一种潜在表示缓冲桥和全身动作标记器实现真正的异步并行执行和端到端联合训练。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出一种真正并行且异步的快-慢执行架构，快通路负责高频连续全身动作生成，慢通路进行低频语义理解和高级推理，整体动作生成频率由快通路决定。</li><li>引入一个由视觉-语言-动作跨模态对齐支持的桥接缓冲，慢系统向其中更新语义和推理潜在表示，为快通路提供高级语义指导。</li><li>提出一个用于全身操作的全身动作标记器，提供捕获高维全身关节配置结构化模式的紧凑统一表示。</li><li>实现了两个通路的端到端联合训练，使慢模块和快模块能够共同优化，促进紧密的语义-控制对齐。</li></ol>\n\n<p>论文方法描述：</p>\n<p>方法基于一个异步运行的双子系统架构：慢系统和快系统。慢系统是一个大型视觉语言模型，运行在低频，用于从视觉观察和任务提示生成结构化语义隐藏状态（包括指令语义和动作推理表示）。快系统运行在高频，是一个轻量级基于扩散的策略，用于生成连续全身动作块。一个桥接缓冲连接两个子系统，存储慢系统产生的语义表示，快系统从中获取最新的潜在表示并与实时感知输入融合，通过一个基于Transformer的扩散策略解码器生成动作。方法还包含一个用于全身操作的几何感知动作标记器，基于残差VQ-VAE设计，将连续动作分解为位置、旋转和抓取流进行标记化。训练采用跨时间尺度的联合训练策略，分两阶段：第一阶段独立训练慢系统；第二阶段引入跨时间尺度采样策略来模拟部署时的异步行为，联合训练快系统和慢系统。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>在商业爆米花售货亭场景中收集了1780条演示轨迹用于训练，涉及一个长视界的爆米花舀取任务和一个短视界的关闭饮料柜门任务。训练使用24块NVIDIA H100 GPU。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估在Astribot S1移动双臂操作平台上进行，该平台包括两个7自由度手臂、一个4自由度躯干、一个2自由度头部和一个3自由度全向移动底盘。评估指标为每个子任务的条件成功率（仅当子任务可达时测量其成功率）和整体任务成功率。评估包括分布内测试、分布外泛化测试、异常场景测试和语言指令遵循能力测试。推断性能在单个NVIDIA RTX 4090 GPU上评估，测量推断频率。</p>"
  },
  {
    "date": "2025-12-23",
    "title": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation",
    "link": "http://arxiv.org/abs/2512.20166",
    "summary_markdown": "论文研究单位：\n- 中国科学院微电子研究所\n- 中国科学院大学\n- 微软研究院\n\n论文概述：\n这篇论文提出了LoLA，一种用于长视野机器人操作的视觉-语言-动作模型。该模型旨在解决现有VLA模型在长视野任务中的不足，通过集成长期多视角观察和机器人本体感觉，实现多步骤推理和动作生成。\n\n论文核心贡献点：\n1. 提出了LoLA框架，用于长视野机器人操作任务。\n2. 引入了状态感知潜在重新表征模块，通过一个可学习的“体现锚定”潜在空间，将预训练的视觉语言特征显式地扎根到真实机器人运动空间中。\n3. 设计了一种选择性的时空采样策略，平衡计算效率和丰富的时空上下文需求。\n4. 收集并构建了一个新颖的长视野复杂操作任务数据集，包含28个真实机器人任务。\n\n论文方法描述：\nLoLA由三个主要组件构成：\n1. 预训练视觉语言模型用于视觉编码：采用选择性时空采样策略处理当前高保真观察和降采样的历史运动上下文，以高效获取时空信息。\n2. 状态感知潜在重新表征模块：该模块是核心创新，通过一个并行于VLM运行的状态变换器，使用状态锚定的乘法融合，在每一层将VLM的中间嵌入显式地接地到机器人本体感觉状态。\n3. 用于动作预测的动作专家：采用条件流匹配模型实现，将SALR模块输出的高层物理接地表示转化为具体的多步骤动作序列。\n\n论文使用数据集和训练资源：\n1. 训练数据集：\n - 预训练使用了OXE数据集和AgiBot数据集，共约110万条真实世界机器人轨迹。\n - 还收集了一个新的长视野操作数据集，包含28个真实机器人任务，分为22个原子子任务组成的7个序列长视野任务和6个额外的连续长视野任务。\n\n2. 训练资源：\n - 在32个NVIDIA A100 GPU集群上进行端到端训练，批量大小为1280。\n - 默认使用25个历史帧以平衡信息容量和计算效率。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：\n - 仿真基准：SIMPLER（谷歌机器人和WidowX机器人）、LIBERO（包含四个任务套件：LIBERO-Goal、LIBERO-Object、LIBERO-Spatial、LIBERO-Long）。\n - 真实世界平台：Franka Research 3机器人、双机械手Aloha机器人（BusyBox设置）。\n\n2. 评估指标：\n - 主要指标是任务成功率。\n - 在SIMPLER和LIBERO基准测试中，报告了各项任务及平均成功率。\n - 在真实世界评估中，报告了单步任务和多步任务（连续执行至少两个子任务）的成功率。",
    "summary_html": "<p>论文研究单位：</p>\n<ul><li>中国科学院微电子研究所</li><li>中国科学院大学</li><li>微软研究院</li></ul>\n\n<p>论文概述：</p>\n<p>这篇论文提出了LoLA，一种用于长视野机器人操作的视觉-语言-动作模型。该模型旨在解决现有VLA模型在长视野任务中的不足，通过集成长期多视角观察和机器人本体感觉，实现多步骤推理和动作生成。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了LoLA框架，用于长视野机器人操作任务。</li><li>引入了状态感知潜在重新表征模块，通过一个可学习的“体现锚定”潜在空间，将预训练的视觉语言特征显式地扎根到真实机器人运动空间中。</li><li>设计了一种选择性的时空采样策略，平衡计算效率和丰富的时空上下文需求。</li><li>收集并构建了一个新颖的长视野复杂操作任务数据集，包含28个真实机器人任务。</li></ol>\n\n<p>论文方法描述：</p>\n<p>LoLA由三个主要组件构成：</p>\n<ol><li>预训练视觉语言模型用于视觉编码：采用选择性时空采样策略处理当前高保真观察和降采样的历史运动上下文，以高效获取时空信息。</li><li>状态感知潜在重新表征模块：该模块是核心创新，通过一个并行于VLM运行的状态变换器，使用状态锚定的乘法融合，在每一层将VLM的中间嵌入显式地接地到机器人本体感觉状态。</li><li>用于动作预测的动作专家：采用条件流匹配模型实现，将SALR模块输出的高层物理接地表示转化为具体的多步骤动作序列。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<p>1. 训练数据集：</p>\n<p> - 预训练使用了OXE数据集和AgiBot数据集，共约110万条真实世界机器人轨迹。</p>\n<p> - 还收集了一个新的长视野操作数据集，包含28个真实机器人任务，分为22个原子子任务组成的7个序列长视野任务和6个额外的连续长视野任务。</p>\n\n<p>2. 训练资源：</p>\n<p> - 在32个NVIDIA A100 GPU集群上进行端到端训练，批量大小为1280。</p>\n<p> - 默认使用25个历史帧以平衡信息容量和计算效率。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>1. 评估环境：</p>\n<p> - 仿真基准：SIMPLER（谷歌机器人和WidowX机器人）、LIBERO（包含四个任务套件：LIBERO-Goal、LIBERO-Object、LIBERO-Spatial、LIBERO-Long）。</p>\n<p> - 真实世界平台：Franka Research 3机器人、双机械手Aloha机器人（BusyBox设置）。</p>\n\n<p>2. 评估指标：</p>\n<p> - 主要指标是任务成功率。</p>\n<p> - 在SIMPLER和LIBERO基准测试中，报告了各项任务及平均成功率。</p>\n<p> - 在真实世界评估中，报告了单步任务和多步任务（连续执行至少两个子任务）的成功率。</p>"
  },
  {
    "date": "2025-12-23",
    "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting",
    "link": "http://arxiv.org/abs/2512.20014",
    "summary_markdown": "论文研究单位：\nSangoh Lee, Sangwoo Mo, Wook-Shin Han\n\n论文概述：\n本研究聚焦于让视觉-语言-动作模型处理用户个人化对象指令（例如“拿我的杯子”）的问题。现有模型只能识别通用的语义类别，无法区分同一类别中用户特定的实例。论文提出了一种名为视觉注意力提示的训练免费框架，该框架通过利用少量参考图像，在场景中定位用户对象，并将其作为视觉提示注入到冻结的模型中，从而实现针对特定实例的操作。\n\n论文核心贡献点：\n1. 引入了个人对象操作任务：要求智能体仅利用少量参考图像，在视觉相似的干扰物中操作用户特定对象。\n2. 提出了视觉注意力提示方法：一个训练免费的框架，通过将个人化概念转化为视觉提示，使冻结的视觉-语言-动作模型能够实现对实例的精确控制，在注册新对象后即可立即使用。\n3. 建立了评估基准和评估：构建了两个模拟基准和一个现实世界设置，证明了视觉注意力提示方法在个性化操作中显著优于通用和其他基线方法。\n\n论文方法描述：\n视觉注意力提示方法作为输入端的感知适配器，不改变视觉-语言-动作模型的参数，只预处理视觉和语言输入。方法分为两个阶段：首先是定位，系统使用参考图像在场景中识别用户的对象；其次是视觉提示，将定位知识直接覆盖到机器人的观察上。具体而言，它使用开放词汇检测器和基于嵌入的匹配来定位目标对象，获得其像素级掩码，然后在图像上以半透明高亮覆盖该区域，并重写指令使其匹配视觉提示（例如将“拿我的杯子”重写为“拿红色的杯子”）。对于后续时间步，则使用实时跟踪器来更新掩码，避免每一控制步都重新运行检测和检索。\n\n论文使用数据集和训练资源：\n论文构建了两个模拟基准：Personalized-SIMPLER和Personalized-VLABench，以及一个现实世界的桌面基准。Personalized-SIMPLER基于SIMPLER环境，使用Google Robot和WidowX机器人平台，包含拾取、移动、放置等任务，使用了Sketchfab的3D资产作为个人对象。Personalized-VLABench基于VLABench，使用Franka机器人手臂，进行多视角选择任务。现实世界基准使用SO-101机器人手臂和三摄像头系统，包含8个日常类别（如花瓶、杯子、拖鞋）的挑选和拾放任务。训练时，使用预训练的π0或π0.5模型作为主干视觉-语言-动作模型，并在排除个人对象和指令的通用数据上进行环境适应微调。视觉编码器使用DINOv2，检测器使用Grounding DINO，分割和跟踪使用SAM2。\n\n论文使用的评估环境和评估指标：\n评估环境包括模拟环境（Personalized-SIMPLER和Personalized-VLABench）和现实世界物理设置（SO-101机器人手臂）。评估指标包括：成功率，即完成任务的情景比例；对于拾取和拾放任务，还报告正确移动率，即至少移动一次正确个人对象的情景比例，无论最终是否成功。",
    "summary_html": "<p>论文研究单位：</p>\n<p>Sangoh Lee, Sangwoo Mo, Wook-Shin Han</p>\n\n<p>论文概述：</p>\n<p>本研究聚焦于让视觉-语言-动作模型处理用户个人化对象指令（例如“拿我的杯子”）的问题。现有模型只能识别通用的语义类别，无法区分同一类别中用户特定的实例。论文提出了一种名为视觉注意力提示的训练免费框架，该框架通过利用少量参考图像，在场景中定位用户对象，并将其作为视觉提示注入到冻结的模型中，从而实现针对特定实例的操作。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>引入了个人对象操作任务：要求智能体仅利用少量参考图像，在视觉相似的干扰物中操作用户特定对象。</li><li>提出了视觉注意力提示方法：一个训练免费的框架，通过将个人化概念转化为视觉提示，使冻结的视觉-语言-动作模型能够实现对实例的精确控制，在注册新对象后即可立即使用。</li><li>建立了评估基准和评估：构建了两个模拟基准和一个现实世界设置，证明了视觉注意力提示方法在个性化操作中显著优于通用和其他基线方法。</li></ol>\n\n<p>论文方法描述：</p>\n<p>视觉注意力提示方法作为输入端的感知适配器，不改变视觉-语言-动作模型的参数，只预处理视觉和语言输入。方法分为两个阶段：首先是定位，系统使用参考图像在场景中识别用户的对象；其次是视觉提示，将定位知识直接覆盖到机器人的观察上。具体而言，它使用开放词汇检测器和基于嵌入的匹配来定位目标对象，获得其像素级掩码，然后在图像上以半透明高亮覆盖该区域，并重写指令使其匹配视觉提示（例如将“拿我的杯子”重写为“拿红色的杯子”）。对于后续时间步，则使用实时跟踪器来更新掩码，避免每一控制步都重新运行检测和检索。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>论文构建了两个模拟基准：Personalized-SIMPLER和Personalized-VLABench，以及一个现实世界的桌面基准。Personalized-SIMPLER基于SIMPLER环境，使用Google Robot和WidowX机器人平台，包含拾取、移动、放置等任务，使用了Sketchfab的3D资产作为个人对象。Personalized-VLABench基于VLABench，使用Franka机器人手臂，进行多视角选择任务。现实世界基准使用SO-101机器人手臂和三摄像头系统，包含8个日常类别（如花瓶、杯子、拖鞋）的挑选和拾放任务。训练时，使用预训练的π0或π0.5模型作为主干视觉-语言-动作模型，并在排除个人对象和指令的通用数据上进行环境适应微调。视觉编码器使用DINOv2，检测器使用Grounding DINO，分割和跟踪使用SAM2。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境包括模拟环境（Personalized-SIMPLER和Personalized-VLABench）和现实世界物理设置（SO-101机器人手臂）。评估指标包括：成功率，即完成任务的情景比例；对于拾取和拾放任务，还报告正确移动率，即至少移动一次正确个人对象的情景比例，无论最终是否成功。</p>"
  },
  {
    "date": "2025-12-22",
    "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.19562",
    "summary_markdown": "论文研究单位：捷克布拉格捷克理工大学信息学、机器人学与控制论研究所，布拉格捷克理工大学电气工程学院，阿姆斯特丹大学\n\n论文概述：本文提出了REALM，一个通过真实到仿真验证的机器人操作泛化基准。该基准包含一个高保真模拟环境和一套评估协议，旨在评估视觉-语言-动作模型在各种扰动下的泛化能力，并验证模拟结果与现实世界性能之间的强相关性。\n\n论文核心贡献点：1. 提出了一个可复现的高保真模拟环境，包含对齐的机器人控制、7项操作技能、15类可控扰动因子，并支持超过3500个物体，实现了对VLA模型泛化能力的大规模可靠评估。2. 通过近800对真实与模拟轨迹的广泛验证，证明了该高保真模拟环境可作为现实世界性能的有效代理，两者性能呈强相关性。3. 在该模拟环境中定义了一个新的泛化基准，并评估了三种先进的VLA模型，结果显示尽管模型在大规模机器人数据上训练，但其在大多数扰动下的泛化能力和鲁棒性仍然面临巨大挑战。\n\n论文方法描述：该基准包含两个任务集：REALM-base（8个任务，测试拾取放置技能）和REALM-articulated（2个任务，测试开关抽屉技能）。它实施了15类扰动，涵盖了视觉（如模糊、视角变化、场景照明）、语义（如基于物体属性、空间关系的指令）和行为（如物体质量、姿态、更换物体）等方面。为了减少真实到仿真的控制差距，通过系统辨识优化了仿真机器人的物理参数（关节摩擦和转子惯量），使用CMA-ES算法最小化真实与模拟关节轨迹之间的均方误差损失。\n\n论文使用数据集和训练资源：基准设计灵感来源于DROID平台。使用的对象库包含超过3500个物体。对于生成语义扰动，使用了现成的VLM模型。评估的VLA模型使用了在DROID数据集上微调的开源检查点。\n\n论文使用的评估环境和评估指标：评估在基于IsaacSim构建的REALM高保真模拟环境中进行。评估指标包括：分层的任务进展度（范围0到1，根据每项技能的离散步骤序列加权计算）、二进制成功率。为了量化扰动的影响，使用了均方根偏差，该指标计算了所有模型和任务在特定扰动下相对于默认设置的任务进展度的总体偏差。在真实到仿真验证中，使用了皮尔逊相关系数、p值和平均最大秩违规来衡量模拟与现实性能的一致性。",
    "summary_html": "<p>论文研究单位：捷克布拉格捷克理工大学信息学、机器人学与控制论研究所，布拉格捷克理工大学电气工程学院，阿姆斯特丹大学</p>\n\n<p>论文概述：本文提出了REALM，一个通过真实到仿真验证的机器人操作泛化基准。该基准包含一个高保真模拟环境和一套评估协议，旨在评估视觉-语言-动作模型在各种扰动下的泛化能力，并验证模拟结果与现实世界性能之间的强相关性。</p>\n\n<p>论文核心贡献点：1. 提出了一个可复现的高保真模拟环境，包含对齐的机器人控制、7项操作技能、15类可控扰动因子，并支持超过3500个物体，实现了对VLA模型泛化能力的大规模可靠评估。2. 通过近800对真实与模拟轨迹的广泛验证，证明了该高保真模拟环境可作为现实世界性能的有效代理，两者性能呈强相关性。3. 在该模拟环境中定义了一个新的泛化基准，并评估了三种先进的VLA模型，结果显示尽管模型在大规模机器人数据上训练，但其在大多数扰动下的泛化能力和鲁棒性仍然面临巨大挑战。</p>\n\n<p>论文方法描述：该基准包含两个任务集：REALM-base（8个任务，测试拾取放置技能）和REALM-articulated（2个任务，测试开关抽屉技能）。它实施了15类扰动，涵盖了视觉（如模糊、视角变化、场景照明）、语义（如基于物体属性、空间关系的指令）和行为（如物体质量、姿态、更换物体）等方面。为了减少真实到仿真的控制差距，通过系统辨识优化了仿真机器人的物理参数（关节摩擦和转子惯量），使用CMA-ES算法最小化真实与模拟关节轨迹之间的均方误差损失。</p>\n\n<p>论文使用数据集和训练资源：基准设计灵感来源于DROID平台。使用的对象库包含超过3500个物体。对于生成语义扰动，使用了现成的VLM模型。评估的VLA模型使用了在DROID数据集上微调的开源检查点。</p>\n\n<p>论文使用的评估环境和评估指标：评估在基于IsaacSim构建的REALM高保真模拟环境中进行。评估指标包括：分层的任务进展度（范围0到1，根据每项技能的离散步骤序列加权计算）、二进制成功率。为了量化扰动的影响，使用了均方根偏差，该指标计算了所有模型和任务在特定扰动下相对于默认设置的任务进展度的总体偏差。在真实到仿真验证中，使用了皮尔逊相关系数、p值和平均最大秩违规来衡量模拟与现实性能的一致性。</p>"
  },
  {
    "date": "2025-12-22",
    "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
    "link": "http://arxiv.org/abs/2512.19024",
    "summary_markdown": "论文研究单位：\n北京大学（由作者列表和致谢中的国家自然科学基金等信息推断）\n\n论文概述：\n本文提出了IndoorUAV，这是第一个专门针对室内环境中无人机视觉语言导航的大规模基准和方法。针对当前空中VLN研究主要集中在室外开放环境，而室内VLN研究不足的问题，论文构建了一个包含超过1000个多样化3D室内场景、超过5万条高质量指令-轨迹对的数据集。该基准包含两个子集：专注于长航时导航的IndoorUAV-VLN和专注于细粒度、短航时动作规划的IndoorUAV-VLA。论文同时提出了IndoorUAV-Agent导航模型，它利用任务分解和多模态推理来处理室内无人机导航的独特挑战。\n\n论文核心贡献点：\n1. 引入第一个专门针对室内三维环境中无人机视觉语言导航的大规模基准IndoorUAV。\n2. 开发了一套自动化的数据收集和标注流程，用于生成逼真的无人机飞行轨迹和多种粒度的自然语言指令。\n3. 提出了IndoorUAV-Agent模型，这是一种针对室内空中VLN独特挑战的强基线模型。\n\n论文方法描述：\n论文提出IndoorUAV-Agent模型，采用分层设计来处理不同粒度的导航任务。\n对于短航时VLA任务，模型直接使用一个经过微调的π0模型，根据当前视觉观察和短指令来预测未来h步的连续低级别控制轨迹（三维坐标和偏航角）。\n对于长航时VLN任务，模型采用任务分解流程：首先利用GPT-4o将复杂的多步长指令分解为一系列简短的、VLA风格的子指令，每个子指令对应1-3个可执行动作。然后，模型依次调用基于π0的VLA模型来执行每个子任务，并将上一个子任务预测的最终观察图像作为当前子任务的初始参考帧，以确保时间连续性并减轻错误累积。\n\n论文使用数据集和训练资源：\n数据集：IndoorUAV基准，包含IndoorUAV-VLN（16,040条轨迹）和IndoorUAV-VLA（34,925条轨迹）两个子集。数据源自超过1000个Habitat模拟器中的高质量3D室内场景（来源于Matterport3D、Gibson、HM3D和Replica数据集）。\n训练资源：在模型训练细节中提及使用了不同型号的GPU，包括RTX 2080 Ti和A6000。\n\n论文使用的评估环境和评估指标：\n评估环境：在Habitat模拟器中构建的连续3D室内环境。\n评估指标：使用了四个主要指标。\n对于VLA任务：成功率为最终预测位置距离目标小于0.5米且偏航角差小于π/4；归一化动态时间规整考虑了三维坐标和偏航角序列的对齐，并根据轨迹路径长度和累积旋转角进行加权。\n对于VLN任务：成功率为最终预测位置距离目标小于2米；归一化动态时间规整仅计算三维坐标对齐；导航误差为预测轨迹终点与目标位置的距离；Oracle成功率为预测轨迹上任何点满足成功条件的比率。",
    "summary_html": "<p>论文研究单位：</p>\n<p>北京大学（由作者列表和致谢中的国家自然科学基金等信息推断）</p>\n\n<p>论文概述：</p>\n<p>本文提出了IndoorUAV，这是第一个专门针对室内环境中无人机视觉语言导航的大规模基准和方法。针对当前空中VLN研究主要集中在室外开放环境，而室内VLN研究不足的问题，论文构建了一个包含超过1000个多样化3D室内场景、超过5万条高质量指令-轨迹对的数据集。该基准包含两个子集：专注于长航时导航的IndoorUAV-VLN和专注于细粒度、短航时动作规划的IndoorUAV-VLA。论文同时提出了IndoorUAV-Agent导航模型，它利用任务分解和多模态推理来处理室内无人机导航的独特挑战。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>引入第一个专门针对室内三维环境中无人机视觉语言导航的大规模基准IndoorUAV。</li><li>开发了一套自动化的数据收集和标注流程，用于生成逼真的无人机飞行轨迹和多种粒度的自然语言指令。</li><li>提出了IndoorUAV-Agent模型，这是一种针对室内空中VLN独特挑战的强基线模型。</li></ol>\n\n<p>论文方法描述：</p>\n<p>论文提出IndoorUAV-Agent模型，采用分层设计来处理不同粒度的导航任务。</p>\n<p>对于短航时VLA任务，模型直接使用一个经过微调的π0模型，根据当前视觉观察和短指令来预测未来h步的连续低级别控制轨迹（三维坐标和偏航角）。</p>\n<p>对于长航时VLN任务，模型采用任务分解流程：首先利用GPT-4o将复杂的多步长指令分解为一系列简短的、VLA风格的子指令，每个子指令对应1-3个可执行动作。然后，模型依次调用基于π0的VLA模型来执行每个子任务，并将上一个子任务预测的最终观察图像作为当前子任务的初始参考帧，以确保时间连续性并减轻错误累积。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>数据集：IndoorUAV基准，包含IndoorUAV-VLN（16,040条轨迹）和IndoorUAV-VLA（34,925条轨迹）两个子集。数据源自超过1000个Habitat模拟器中的高质量3D室内场景（来源于Matterport3D、Gibson、HM3D和Replica数据集）。</p>\n<p>训练资源：在模型训练细节中提及使用了不同型号的GPU，包括RTX 2080 Ti和A6000。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：在Habitat模拟器中构建的连续3D室内环境。</p>\n<p>评估指标：使用了四个主要指标。</p>\n<p>对于VLA任务：成功率为最终预测位置距离目标小于0.5米且偏航角差小于π/4；归一化动态时间规整考虑了三维坐标和偏航角序列的对齐，并根据轨迹路径长度和累积旋转角进行加权。</p>\n<p>对于VLN任务：成功率为最终预测位置距离目标小于2米；归一化动态时间规整仅计算三维坐标对齐；导航误差为预测轨迹终点与目标位置的距离；Oracle成功率为预测轨迹上任何点满足成功条件的比率。</p>"
  },
  {
    "date": "2025-12-22",
    "title": "Point What You Mean: Visually Grounded Instruction Policy",
    "link": "http://arxiv.org/abs/2512.18933",
    "summary_markdown": "论文研究单位：同济大学，上海交通大学，Spirit AI，清华大学\n\n论文概述：本研究提出了Point-VLA，一种即插即用的视觉-语言-动作（VLA）策略，通过在语言指令中添加显式的视觉提示（如边界框）来解决仅依赖文本指令时存在的指代歧义问题，特别是在杂乱或分布外（OOD）场景中。该方法旨在实现精确的对象级视觉接地，从而提升具身控制的泛化能力。\n\n论文核心贡献点：1. 提出了Point-VLA，通过增强显式视觉接地的语言指令，解决了杂乱和未见环境中物体和位置指代模糊的问题。2. Point-VLA是一个统一的策略，支持纯文本或视觉接地指令模式，并能保持强大的文本指令跟随能力，这得益于在两种指令上的协同训练。3. 提供了一个可扩展的数据构建流程，利用预训练的多模态大语言模型（MLLM）从现有轨迹中自动生成视觉接地监督，降低了标注成本，并支持与先前数据集的无缝集成。\n\n论文方法描述：Point-VLA扩展了标准VLA接口，增加了视觉接地的指令。核心方法是在首帧的俯视图像上叠加一个视觉标记（如边界框）来构成接地输入，文本指令仅表达高层意图（如“拾取”、“放置”），所有目标特定信息来自接地图像。模型在训练时，每个时间步基于当前的多视角观测和固定的首帧接地图像来预测下一个机器人动作。为了高效扩展数据集，开发了一个基于MLLM的四阶段自动标注流程，从演示视频中自动生成目标边界框。此外，对接地图像输入应用了两种数据增强（随机平移和局部CutMix）以提高鲁棒性。该模型在纯文本指令和视觉接地指令的混合数据集上进行协同训练，以获得一个统一的策略。在推理时，支持两种交互模式：用户手动绘制边界框或通过MLLM自动根据人类指向预测边界框。\n\n论文使用数据集和训练资源：使用了真实世界的机器人演示数据，由专业操作员在12个不同的场景中收集，每个任务场景大约包含两小时的演示。所有文本标注均为手动标注和验证。在训练中，使用了π0.5 VLA模型作为主干，并遵循其官方微调方案，每个任务微调20k步。还评估了其在π0模型和全身人形机器人上的表现。\n\n论文使用的评估环境和评估指标：评估在六项真实世界的机器人操作任务上进行，分为目标物体指代和目标位置指代两大类。评估指标为成功率。在拾取任务中，机器人成功抓取并提起指定物体记为成功；在放置任务中，物体被放置在目标区域内记为成功。每个场景进行30次独立试验，允许最多两次重试。在普通桌面放置任务中，最终物体中心偏离目标位置超过10厘米记为失败。",
    "summary_html": "<p>论文研究单位：同济大学，上海交通大学，Spirit AI，清华大学</p>\n\n<p>论文概述：本研究提出了Point-VLA，一种即插即用的视觉-语言-动作（VLA）策略，通过在语言指令中添加显式的视觉提示（如边界框）来解决仅依赖文本指令时存在的指代歧义问题，特别是在杂乱或分布外（OOD）场景中。该方法旨在实现精确的对象级视觉接地，从而提升具身控制的泛化能力。</p>\n\n<p>论文核心贡献点：1. 提出了Point-VLA，通过增强显式视觉接地的语言指令，解决了杂乱和未见环境中物体和位置指代模糊的问题。2. Point-VLA是一个统一的策略，支持纯文本或视觉接地指令模式，并能保持强大的文本指令跟随能力，这得益于在两种指令上的协同训练。3. 提供了一个可扩展的数据构建流程，利用预训练的多模态大语言模型（MLLM）从现有轨迹中自动生成视觉接地监督，降低了标注成本，并支持与先前数据集的无缝集成。</p>\n\n<p>论文方法描述：Point-VLA扩展了标准VLA接口，增加了视觉接地的指令。核心方法是在首帧的俯视图像上叠加一个视觉标记（如边界框）来构成接地输入，文本指令仅表达高层意图（如“拾取”、“放置”），所有目标特定信息来自接地图像。模型在训练时，每个时间步基于当前的多视角观测和固定的首帧接地图像来预测下一个机器人动作。为了高效扩展数据集，开发了一个基于MLLM的四阶段自动标注流程，从演示视频中自动生成目标边界框。此外，对接地图像输入应用了两种数据增强（随机平移和局部CutMix）以提高鲁棒性。该模型在纯文本指令和视觉接地指令的混合数据集上进行协同训练，以获得一个统一的策略。在推理时，支持两种交互模式：用户手动绘制边界框或通过MLLM自动根据人类指向预测边界框。</p>\n\n<p>论文使用数据集和训练资源：使用了真实世界的机器人演示数据，由专业操作员在12个不同的场景中收集，每个任务场景大约包含两小时的演示。所有文本标注均为手动标注和验证。在训练中，使用了π0.5 VLA模型作为主干，并遵循其官方微调方案，每个任务微调20k步。还评估了其在π0模型和全身人形机器人上的表现。</p>\n\n<p>论文使用的评估环境和评估指标：评估在六项真实世界的机器人操作任务上进行，分为目标物体指代和目标位置指代两大类。评估指标为成功率。在拾取任务中，机器人成功抓取并提起指定物体记为成功；在放置任务中，物体被放置在目标区域内记为成功。每个场景进行30次独立试验，允许最多两次重试。在普通桌面放置任务中，最终物体中心偏离目标位置超过10厘米记为失败。</p>"
  },
  {
    "date": "2025-12-20",
    "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.18477",
    "summary_markdown": "论文研究单位：中山大学\n\n论文概述：论文提出了STORM，一个用于机器人操作的新型时空推理框架。该框架将基于扩散模型的动作生成、条件视频预测和基于搜索的规划统一起来，通过显式的视觉模拟进行前瞻式规划，以克服现有视觉语言动作模型在物理推理上的局限性。\n\n论文核心贡献点：\n1. 提出STORM框架，将基于扩散的视觉语言动作模型、生成式视频世界模型和蒙特卡洛树搜索相结合，实现了显式的时空推理。\n2. 证明了奖励增强的视频预测器能作为高效的生成式世界模型，显著提高了动作条件视觉模拟的保真度和任务相关性。\n3. 实验表明STORM在任务成功率上超越了现有的视觉语言动作模型基线，并展现了从失败中恢复和重新规划的卓越能力。\n\n论文方法描述：\nSTORM框架将决策过程建模为一个部分可观察的马尔可夫决策过程。其核心是一个由蒙特卡洛树搜索协调的闭环：1）基于扩散的视觉语言动作策略提出多样化的候选动作序列；2）基于iVideoGPT的条件视频预测模型（作为生成式世界模型）模拟这些动作的可视化结果并预测奖励；3）蒙特卡洛树搜索利用预测的奖励，通过选择、扩展、评估和回溯步骤对这些模拟的未来进行前瞻性搜索，以识别最优的长期策略。该方法将搜索建立在显式的视觉模拟之上，从而获得更可解释、可验证和鲁棒的决策。\n\n论文使用数据集和训练资源：\n在SimplerEnv模拟器中使用WidowX机械臂进行评估。主要训练数据来自Bridge数据集。视觉语言动作模块采用预训练的CogACT-Base模型（70亿参数），未进行进一步训练。生成式世界模型基于预训练的iVideoGPT-medium Transformer，并在Bridge数据集上进行了微调，引入了动作条件和奖励预测头。微调在2个NVIDIA A100 (80GB) GPU上进行，训练约120，000步，每个设备的批量大小为18，使用AdamW优化器。\n\n论文使用的评估环境和评估指标：\n评估环境：SimplerEnv机器人操作基准测试。\n评估指标：\n主要任务性能：平均成功率（在每项任务的24个程序变体上计算）。\n视频预测质量：弗雷歇视频距离、学习感知图像块相似度、峰值信噪比、结构相似性。\n其他：通过案例研究定性分析失败恢复能力。",
    "summary_html": "<p>论文研究单位：中山大学</p>\n\n<p>论文概述：论文提出了STORM，一个用于机器人操作的新型时空推理框架。该框架将基于扩散模型的动作生成、条件视频预测和基于搜索的规划统一起来，通过显式的视觉模拟进行前瞻式规划，以克服现有视觉语言动作模型在物理推理上的局限性。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出STORM框架，将基于扩散的视觉语言动作模型、生成式视频世界模型和蒙特卡洛树搜索相结合，实现了显式的时空推理。</li><li>证明了奖励增强的视频预测器能作为高效的生成式世界模型，显著提高了动作条件视觉模拟的保真度和任务相关性。</li><li>实验表明STORM在任务成功率上超越了现有的视觉语言动作模型基线，并展现了从失败中恢复和重新规划的卓越能力。</li></ol>\n\n<p>论文方法描述：</p>\n<p>STORM框架将决策过程建模为一个部分可观察的马尔可夫决策过程。其核心是一个由蒙特卡洛树搜索协调的闭环：1）基于扩散的视觉语言动作策略提出多样化的候选动作序列；2）基于iVideoGPT的条件视频预测模型（作为生成式世界模型）模拟这些动作的可视化结果并预测奖励；3）蒙特卡洛树搜索利用预测的奖励，通过选择、扩展、评估和回溯步骤对这些模拟的未来进行前瞻性搜索，以识别最优的长期策略。该方法将搜索建立在显式的视觉模拟之上，从而获得更可解释、可验证和鲁棒的决策。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>在SimplerEnv模拟器中使用WidowX机械臂进行评估。主要训练数据来自Bridge数据集。视觉语言动作模块采用预训练的CogACT-Base模型（70亿参数），未进行进一步训练。生成式世界模型基于预训练的iVideoGPT-medium Transformer，并在Bridge数据集上进行了微调，引入了动作条件和奖励预测头。微调在2个NVIDIA A100 (80GB) GPU上进行，训练约120，000步，每个设备的批量大小为18，使用AdamW优化器。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：SimplerEnv机器人操作基准测试。</p>\n<p>评估指标：</p>\n<p>主要任务性能：平均成功率（在每项任务的24个程序变体上计算）。</p>\n<p>视频预测质量：弗雷歇视频距离、学习感知图像块相似度、峰值信噪比、结构相似性。</p>\n<p>其他：通过案例研究定性分析失败恢复能力。</p>"
  },
  {
    "date": "2025-12-20",
    "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
    "link": "http://arxiv.org/abs/2512.18396",
    "summary_markdown": "论文研究单位：\n未明确列出。\n\n论文概述：\n该论文提出了一个名为AOMGen的框架，用于生成铰接物体操纵的逼真、物理一致的演示数据。该框架能够从一次真实扫描、一个真实演示以及一个可用的数字资产库出发，合成具有物理状态验证的逼真训练数据。AOMGen生成了同步的多视角RGB视频、时间对齐的动作命令以及关节和接触点的状态标注，并系统性地变化相机视角、物体样式和物体姿态，从而将单个执行过程扩展为一个多样的数据集。\n\n论文核心贡献点：\n- 仅使用铰接物体的一个静态扫描视频，就能为同一类别的任何其他物体生成操纵数据。\n- 确保所有合成数据中物理交互的精确性和高度的视觉真实感。\n- 其架构支持对目标物体姿态进行任意调整，极大地扩展了生成数据的配置多样性，从而拓宽了泛化边界。\n- 实验证明，由AOMGen生成的合成数据能有效用于VLA模型训练，从而提高模型性能。\n\n论文方法描述：\nAOMGen主要由两个核心模块构成：场景重建与运动恢复，以及铰接物体替换与姿态泛化。\n1. 场景重建与运动恢复：使用3D高斯点云技术精确重建真实世界的操纵场景。首先从原始观测数据中分割高斯点，并将3DGS重建结果与真实世界坐标系对齐。然后利用真实的机械臂轨迹作为物理先验，恢复出准确且物理一致的铰接运动，确保高保真的几何对齐。这包括关键帧提取、接触点检测、铰接物体建模以及可移动部件运动恢复。\n2. 铰接物体替换与姿态泛化：支持用同类别的其他物体替换原始物体，并模拟其相应的交互。在原始物体和新物体模型之间建立映射，以确保正确的关节参数、尺寸和初始姿态。通过将原始场景的光照和材质转移到新物体上，进一步增强了真实感，为训练操纵策略生成视觉逼真且物理合理的演示。\n\n论文使用数据集和训练资源：\n- 数据集：真实铰接物体操纵数据，包括静态场景扫描视频、动态操纵视频和机械臂关节状态。同时，使用了来自ArtVIP库的模拟3D资产进行替换。\n- 训练资源：在NVIDIA RTX4090 GPU上进行模型训练。使用了两个VLA模型进行微调实验：π0.5 和 OpenVLA，采用LoRA技术进行微调。\n\n论文使用的评估环境和评估指标：\n- 评估环境：使用IsaacSim作为仿真平台进行仿真回放和模型策略测试。在仿真环境中，导入优化后的USD资产、铰接物体和桌面，并使用3DGS渲染的图像作为背景墙纹理，以减小仿真环境与真实世界之间的差距。\n- 评估指标：主要评估指标是任务成功率。在仿真回放中，计算替换物体后数据样本的成功率。在VLA模型训练效果评估中，测试模型在不同物体、不同数量演示数据微调下的任务成功率。此外，还评估了模型对物体尺度变化（缩放）和未见物体的泛化能力。",
    "summary_html": "<p>论文研究单位：</p>\n<p>未明确列出。</p>\n\n<p>论文概述：</p>\n<p>该论文提出了一个名为AOMGen的框架，用于生成铰接物体操纵的逼真、物理一致的演示数据。该框架能够从一次真实扫描、一个真实演示以及一个可用的数字资产库出发，合成具有物理状态验证的逼真训练数据。AOMGen生成了同步的多视角RGB视频、时间对齐的动作命令以及关节和接触点的状态标注，并系统性地变化相机视角、物体样式和物体姿态，从而将单个执行过程扩展为一个多样的数据集。</p>\n\n<p>论文核心贡献点：</p>\n<ul><li>仅使用铰接物体的一个静态扫描视频，就能为同一类别的任何其他物体生成操纵数据。</li><li>确保所有合成数据中物理交互的精确性和高度的视觉真实感。</li><li>其架构支持对目标物体姿态进行任意调整，极大地扩展了生成数据的配置多样性，从而拓宽了泛化边界。</li><li>实验证明，由AOMGen生成的合成数据能有效用于VLA模型训练，从而提高模型性能。</li></ul>\n\n<p>论文方法描述：</p>\n<p>AOMGen主要由两个核心模块构成：场景重建与运动恢复，以及铰接物体替换与姿态泛化。</p>\n<ol><li>场景重建与运动恢复：使用3D高斯点云技术精确重建真实世界的操纵场景。首先从原始观测数据中分割高斯点，并将3DGS重建结果与真实世界坐标系对齐。然后利用真实的机械臂轨迹作为物理先验，恢复出准确且物理一致的铰接运动，确保高保真的几何对齐。这包括关键帧提取、接触点检测、铰接物体建模以及可移动部件运动恢复。</li><li>铰接物体替换与姿态泛化：支持用同类别的其他物体替换原始物体，并模拟其相应的交互。在原始物体和新物体模型之间建立映射，以确保正确的关节参数、尺寸和初始姿态。通过将原始场景的光照和材质转移到新物体上，进一步增强了真实感，为训练操纵策略生成视觉逼真且物理合理的演示。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>数据集：真实铰接物体操纵数据，包括静态场景扫描视频、动态操纵视频和机械臂关节状态。同时，使用了来自ArtVIP库的模拟3D资产进行替换。</li><li>训练资源：在NVIDIA RTX4090 GPU上进行模型训练。使用了两个VLA模型进行微调实验：π0.5 和 OpenVLA，采用LoRA技术进行微调。</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>评估环境：使用IsaacSim作为仿真平台进行仿真回放和模型策略测试。在仿真环境中，导入优化后的USD资产、铰接物体和桌面，并使用3DGS渲染的图像作为背景墙纹理，以减小仿真环境与真实世界之间的差距。</li><li>评估指标：主要评估指标是任务成功率。在仿真回放中，计算替换物体后数据样本的成功率。在VLA模型训练效果评估中，测试模型在不同物体、不同数量演示数据微调下的任务成功率。此外，还评估了模型对物体尺度变化（缩放）和未见物体的泛化能力。</li></ul>"
  },
  {
    "date": "2025-12-19",
    "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
    "link": "http://arxiv.org/abs/2512.18007",
    "summary_markdown": "论文研究单位：Salesforce AI Research 和 University of North Carolina at Chapel Hill\n\n论文概述：本文针对现有视觉-语言-动作（VLA）模型缺乏显式运动推理能力、主要模仿专家轨迹的局限性，提出了一种联合学习运动图像扩散的策略。该方法旨在增强VLA模型的运动推理能力，使其能学习与动作执行相关的未来动态，从而提高在复杂操纵任务中的性能和泛化能力。\n\n论文核心贡献点：\n1. 提出了联合学习运动图像扩散的策略，可无缝增强VLA模型的运动推理能力，同时保持其实时推理效率。\n2. 提出了运动图像扩散，通过扩散变换器（DiT）实现，提供了与稀疏动作监督互补的密集像素级动态监督，并证明基于光流的运动图像是联合动作-运动学习的最有效表示。\n3. 通过大量实验证明，该方法能有效提升大规模VLA模型的性能，例如将π系列VLA模型在LIBERO基准上的平均成功率提升至97.5%，在RoboTwin基准上提升至58.0%，并在真实世界任务中实现23%的性能提升。\n\n论文方法描述：\n该方法基于标准的预训练VLA架构（包含一个VLM主干和一个动作头），引入了一个并行的、作为扩散变换器（DiT）实现的运动头，从而形成一个双头设计。动作头负责预测未来动作块，运动头则负责预测基于光流的未来运动图像（通过扩散过程生成运动潜在令牌，再通过冻结的VAE解码器解码为运动图像）。两个头共享同一个VLM主干，并通过流匹配损失进行联合优化，总损失为动作损失和运动损失之和。训练使用从观测对计算出的图像光流（转换为RGB图像）作为运动监督信号。训练过程采用两阶段流水线：首先在DROID数据集上对运动头进行预训练（仅优化运动头），然后对所有参数（VAE编解码器除外）进行联合训练，使共享的VLM主干能够共同对齐动作和运动表示。\n\n论文使用数据集和训练资源：\n使用的数据集包括DROID数据集（用于运动头预训练）、LIBERO基准数据集（包含四个套件共40个任务）和RoboTwin 2.0基准数据集（包含7个双手机器人操纵任务）。模型基于π系列VLA（如π0， π0.5）进行初始化，使用Paligemma-3B作为VLM主干，Paligemma-300M作为动作头，运动头则是一个包含400M参数的轻量级扩散变换器（DiT）。实验使用8块NVIDIA H200 GPU进行训练。\n\n论文使用的评估环境和评估指标：\n评估环境包括模拟环境（LIBERO和RoboTwin基准）和真实世界环境（使用Dobot Nova 5 6自由度机械臂和DH Robotics AG95夹爪的桌面操纵任务）。评估指标为任务成功率（成功完成任务的试验比例）。在LIBERO基准上，每个策略在每个任务的每个套件下进行50次试验。在RoboTwin基准上，每个任务在简单（领域内）和困难（领域随机化）两种模式下各进行100次试验。真实世界实验中，对三个桌面任务进行有限数据的微调后评估成功率。",
    "summary_html": "<p>论文研究单位：Salesforce AI Research 和 University of North Carolina at Chapel Hill</p>\n\n<p>论文概述：本文针对现有视觉-语言-动作（VLA）模型缺乏显式运动推理能力、主要模仿专家轨迹的局限性，提出了一种联合学习运动图像扩散的策略。该方法旨在增强VLA模型的运动推理能力，使其能学习与动作执行相关的未来动态，从而提高在复杂操纵任务中的性能和泛化能力。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了联合学习运动图像扩散的策略，可无缝增强VLA模型的运动推理能力，同时保持其实时推理效率。</li><li>提出了运动图像扩散，通过扩散变换器（DiT）实现，提供了与稀疏动作监督互补的密集像素级动态监督，并证明基于光流的运动图像是联合动作-运动学习的最有效表示。</li><li>通过大量实验证明，该方法能有效提升大规模VLA模型的性能，例如将π系列VLA模型在LIBERO基准上的平均成功率提升至97.5%，在RoboTwin基准上提升至58.0%，并在真实世界任务中实现23%的性能提升。</li></ol>\n\n<p>论文方法描述：</p>\n<p>该方法基于标准的预训练VLA架构（包含一个VLM主干和一个动作头），引入了一个并行的、作为扩散变换器（DiT）实现的运动头，从而形成一个双头设计。动作头负责预测未来动作块，运动头则负责预测基于光流的未来运动图像（通过扩散过程生成运动潜在令牌，再通过冻结的VAE解码器解码为运动图像）。两个头共享同一个VLM主干，并通过流匹配损失进行联合优化，总损失为动作损失和运动损失之和。训练使用从观测对计算出的图像光流（转换为RGB图像）作为运动监督信号。训练过程采用两阶段流水线：首先在DROID数据集上对运动头进行预训练（仅优化运动头），然后对所有参数（VAE编解码器除外）进行联合训练，使共享的VLM主干能够共同对齐动作和运动表示。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>使用的数据集包括DROID数据集（用于运动头预训练）、LIBERO基准数据集（包含四个套件共40个任务）和RoboTwin 2.0基准数据集（包含7个双手机器人操纵任务）。模型基于π系列VLA（如π0， π0.5）进行初始化，使用Paligemma-3B作为VLM主干，Paligemma-300M作为动作头，运动头则是一个包含400M参数的轻量级扩散变换器（DiT）。实验使用8块NVIDIA H200 GPU进行训练。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境包括模拟环境（LIBERO和RoboTwin基准）和真实世界环境（使用Dobot Nova 5 6自由度机械臂和DH Robotics AG95夹爪的桌面操纵任务）。评估指标为任务成功率（成功完成任务的试验比例）。在LIBERO基准上，每个策略在每个任务的每个套件下进行50次试验。在RoboTwin基准上，每个任务在简单（领域内）和困难（领域随机化）两种模式下各进行100次试验。真实世界实验中，对三个桌面任务进行有限数据的微调后评估成功率。</p>"
  },
  {
    "date": "2025-12-18",
    "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
    "link": "http://arxiv.org/abs/2512.16811",
    "summary_markdown": "论文研究单位：香港中文大学（深圳）、湖南大学、理想汽车、滴滴出行Voyager Research\n\n论文概述：本文提出GeoPredict，一种几何感知的视觉-语言-动作模型框架，用于机器人精确操作。该框架通过引入预测运动学和3D高斯几何模块，增强连续动作策略对3D空间和长期动态的理解能力，解决现有VLA模型主要依赖2D图像空间、缺乏3D几何推理的问题。\n\n论文核心贡献点：1. 提出GeoPredict框架，将未来感知的运动学和几何先验注入连续动作VLA策略。2. 设计两个互补预测模块：轨迹级运动预测器和带轨迹引导细化的预测3D高斯几何模块。3. 在多个仿真和真实世界操作任务中验证方法的有效性，尤其在需要精确空间推理的场景中表现突出。\n\n论文方法描述：方法包含两个核心模块：1. 轨迹级运动预测模块，通过轨迹编码器压缩运动历史，使用未来轨迹查询预测多步3D关键点轨迹。2. 预测3D高斯几何模块，通过3D空间查询和体素解码器生成3D高斯场景表示，并利用预测轨迹进行局部几何细化。3. 采用块级因果注意力机制整合预测模块，训练时使用动作损失、轨迹损失和深度渲染损失的加权组合，推理时不调用预测模块以保持效率。\n\n论文使用数据集和训练资源：使用RoboCasa Human-50、LIBERO和真实世界操作任务进行评估。训练使用8块NVIDIA H20 GPU，总批次大小为32，训练40,000次迭代，使用AdamW优化器。\n\n论文使用的评估环境和评估指标：评估环境包括RoboCasa仿真基准（24个子任务）、LIBERO仿真基准（4个评估套件）和真实世界评估套件。评估指标为任务成功率。",
    "summary_html": "<p>论文研究单位：香港中文大学（深圳）、湖南大学、理想汽车、滴滴出行Voyager Research</p>\n\n<p>论文概述：本文提出GeoPredict，一种几何感知的视觉-语言-动作模型框架，用于机器人精确操作。该框架通过引入预测运动学和3D高斯几何模块，增强连续动作策略对3D空间和长期动态的理解能力，解决现有VLA模型主要依赖2D图像空间、缺乏3D几何推理的问题。</p>\n\n<p>论文核心贡献点：1. 提出GeoPredict框架，将未来感知的运动学和几何先验注入连续动作VLA策略。2. 设计两个互补预测模块：轨迹级运动预测器和带轨迹引导细化的预测3D高斯几何模块。3. 在多个仿真和真实世界操作任务中验证方法的有效性，尤其在需要精确空间推理的场景中表现突出。</p>\n\n<p>论文方法描述：方法包含两个核心模块：1. 轨迹级运动预测模块，通过轨迹编码器压缩运动历史，使用未来轨迹查询预测多步3D关键点轨迹。2. 预测3D高斯几何模块，通过3D空间查询和体素解码器生成3D高斯场景表示，并利用预测轨迹进行局部几何细化。3. 采用块级因果注意力机制整合预测模块，训练时使用动作损失、轨迹损失和深度渲染损失的加权组合，推理时不调用预测模块以保持效率。</p>\n\n<p>论文使用数据集和训练资源：使用RoboCasa Human-50、LIBERO和真实世界操作任务进行评估。训练使用8块NVIDIA H20 GPU，总批次大小为32，训练40,000次迭代，使用AdamW优化器。</p>\n\n<p>论文使用的评估环境和评估指标：评估环境包括RoboCasa仿真基准（24个子任务）、LIBERO仿真基准（4个评估套件）和真实世界评估套件。评估指标为任务成功率。</p>"
  },
  {
    "date": "2025-12-18",
    "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
    "link": "http://arxiv.org/abs/2512.16793",
    "summary_markdown": "论文研究单位\n香港科技大学（广州），中关村研究院，中关村人工智能研究院，深赛博，哈尔滨工业大学，华中科技大学\n\n论文概述\n本文提出physbrain方法，利用人类第一人称视频作为桥梁，将视觉语言模型扩展到物理智能领域。论文通过构建egocentric2embodiment翻译流水线，将原始人类第一人称视频转换为结构化、多层次的视觉问答监督数据，形成e2e-3m数据集。在该数据集上训练获得的physbrain模型在自我中心理解方面表现显著提升，特别是在规划任务上。该模型作为视觉语言动作模型的骨干网络，在机器人控制任务中实现了更高的成功率。\n\n论文核心贡献点\n提出egocentric2embodiment翻译流水线，将大规模人类第一人称视频转换为多层次的具身监督数据。构建了结构化的第一人称视觉问答数据集e2e-3m。通过实验证明人类第一人称视频可为自我中心环境下的具身大脑学习提供有效监督。发现人类第一人称数据与机器人数据具有互补性，且更具可扩展性。\n\n论文方法描述\n设计基于模式的标注方案，包含七种互补的视觉问答模式（时间、空间、属性、力学、推理、总结、轨迹）。通过确定性规则检查器进行质量验证，确保监督数据的证据基础、自我中心一致性和时间逻辑。提出两种视觉语言动作架构：physgr00t采用双系统设计，使用最后层视觉语言模型特征作为条件信号。physpi通过层间交叉注意力将多个视觉语言模型层与动作专家耦合。\n\n论文使用数据集和训练资源\n使用e2e-3m数据集（约300万样本），数据来源包括ego4d（家庭环境）、buildai（工厂环境）和egodex（实验室环境）。训练使用8张nvidia h100 gpu，约22小时。优化器使用adamw，学习率4e-5，采用余弦学习率调度。\n\n论文使用的评估环境和评估指标\n在egothink基准上评估自我中心理解能力，涵盖六个维度（活动、预测、定位、物体、规划、推理）。在simplerenv仿真环境中评估机器人控制性能，使用成功率作为指标。",
    "summary_html": "<p>论文研究单位</p>\n<p>香港科技大学（广州），中关村研究院，中关村人工智能研究院，深赛博，哈尔滨工业大学，华中科技大学</p>\n\n<p>论文概述</p>\n<p>本文提出physbrain方法，利用人类第一人称视频作为桥梁，将视觉语言模型扩展到物理智能领域。论文通过构建egocentric2embodiment翻译流水线，将原始人类第一人称视频转换为结构化、多层次的视觉问答监督数据，形成e2e-3m数据集。在该数据集上训练获得的physbrain模型在自我中心理解方面表现显著提升，特别是在规划任务上。该模型作为视觉语言动作模型的骨干网络，在机器人控制任务中实现了更高的成功率。</p>\n\n<p>论文核心贡献点</p>\n<p>提出egocentric2embodiment翻译流水线，将大规模人类第一人称视频转换为多层次的具身监督数据。构建了结构化的第一人称视觉问答数据集e2e-3m。通过实验证明人类第一人称视频可为自我中心环境下的具身大脑学习提供有效监督。发现人类第一人称数据与机器人数据具有互补性，且更具可扩展性。</p>\n\n<p>论文方法描述</p>\n<p>设计基于模式的标注方案，包含七种互补的视觉问答模式（时间、空间、属性、力学、推理、总结、轨迹）。通过确定性规则检查器进行质量验证，确保监督数据的证据基础、自我中心一致性和时间逻辑。提出两种视觉语言动作架构：physgr00t采用双系统设计，使用最后层视觉语言模型特征作为条件信号。physpi通过层间交叉注意力将多个视觉语言模型层与动作专家耦合。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用e2e-3m数据集（约300万样本），数据来源包括ego4d（家庭环境）、buildai（工厂环境）和egodex（实验室环境）。训练使用8张nvidia h100 gpu，约22小时。优化器使用adamw，学习率4e-5，采用余弦学习率调度。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>在egothink基准上评估自我中心理解能力，涵盖六个维度（活动、预测、定位、物体、规划、推理）。在simplerenv仿真环境中评估机器人控制性能，使用成功率作为指标。</p>"
  },
  {
    "date": "2025-12-18",
    "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
    "link": "http://arxiv.org/abs/2512.16760",
    "summary_markdown": "```markdown\n论文研究单位：WorldBench Team\n\n论文概述：该论文系统性地回顾了自动驾驶领域中视觉-语言-动作（VLA）模型的发展历程，从早期的视觉-动作（VA）模型到现代VLA框架的演进。论文分析了VLA模型如何通过整合视觉感知、语言推理和动作输出，提供更可解释、泛化性更强且与人类对齐的驾驶策略范式。\n\n论文核心贡献点：\n1. 提出从VA模型到VLA框架的历史演进路径，阐明该范式转变的技术动机\n2. 建立VLA架构的层次化分类法，区分端到端VLA和双系统VLA设计\n3. 提供VLA驾驶相关数据集和评估基准的组织化综述\n4. 识别VLA实际部署中的关键挑战并展望未来研究方向\n\n论文方法描述：\n论文将VLA模型形式化为三个核心组件：多模态输入、VLM主干网络和动作预测头。VLA模型耦合视觉语言模型（VLM）主干与动作预测头，实现从多模态输入（视觉+语言）到可执行驾驶动作的直接映射。VLA架构分为两种主要范式：\n- 端到端VLA：在单一模型内集成感知、推理和规划\n- 双系统VLA：将慢速审议（通过VLM）与快速、安全关键执行（通过规划器）分离\n\n论文使用数据集和训练资源：\n数据集包括：\n- 视觉-动作数据集：CARLA、NoCrash、ProcGen、Lyft、nuScenes、Bench2Drive、NAVSIM、OpenOcc、OpenDV、nuPlan、Occ3D、Cam4DOcc和私有数据\n训练资源涉及VLM主干网络，使用大型视觉语言模型作为核心推理引擎\n\n论文使用的评估环境和评估指标：\n评估环境包括nuScenes基准、WOD-E2E基准、NAVSIM基准和Bench2Drive基准\n评估指标分为：\n- 基于轨迹的动作评估\n- 基于文本的动作评估\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：WorldBench Team</p>\n\n<p>论文概述：该论文系统性地回顾了自动驾驶领域中视觉-语言-动作（VLA）模型的发展历程，从早期的视觉-动作（VA）模型到现代VLA框架的演进。论文分析了VLA模型如何通过整合视觉感知、语言推理和动作输出，提供更可解释、泛化性更强且与人类对齐的驾驶策略范式。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出从VA模型到VLA框架的历史演进路径，阐明该范式转变的技术动机</li><li>建立VLA架构的层次化分类法，区分端到端VLA和双系统VLA设计</li><li>提供VLA驾驶相关数据集和评估基准的组织化综述</li><li>识别VLA实际部署中的关键挑战并展望未来研究方向</li></ol>\n\n<p>论文方法描述：</p>\n<p>论文将VLA模型形式化为三个核心组件：多模态输入、VLM主干网络和动作预测头。VLA模型耦合视觉语言模型（VLM）主干与动作预测头，实现从多模态输入（视觉+语言）到可执行驾驶动作的直接映射。VLA架构分为两种主要范式：</p>\n<ul><li>端到端VLA：在单一模型内集成感知、推理和规划</li><li>双系统VLA：将慢速审议（通过VLM）与快速、安全关键执行（通过规划器）分离</li></ul>\n\n<p>论文使用数据集和训练资源：</p>\n<p>数据集包括：</p>\n<ul><li>视觉-动作数据集：CARLA、NoCrash、ProcGen、Lyft、nuScenes、Bench2Drive、NAVSIM、OpenOcc、OpenDV、nuPlan、Occ3D、Cam4DOcc和私有数据</li></ul>\n<p>训练资源涉及VLM主干网络，使用大型视觉语言模型作为核心推理引擎</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境包括nuScenes基准、WOD-E2E基准、NAVSIM基准和Bench2Drive基准</p>\n<p>评估指标分为：</p>\n<ul><li>基于轨迹的动作评估</li><li>基于文本的动作评估</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-17",
    "title": "Large Video Planner Enables Generalizable Robot Control",
    "link": "http://arxiv.org/abs/2512.15840",
    "summary_markdown": "```markdown\n论文研究单位：\n麻省理工学院（MIT），加州大学伯克利分校（UC Berkeley），哈佛大学（Harvard）\n\n论文概述：\n本研究提出了一种基于视频生成的新型机器人基础模型范式。与传统的视觉-语言-动作（VLA）模型不同，该方法利用大规模视频预训练作为主要模态，通过生成视频作为视觉动作规划，然后从视频中提取可执行机器人动作。该方法通过构建一个140亿参数的视频基础模型，能够在未见过的场景和任务中实现零样本泛化。\n\n论文核心贡献点：\n1. 提出大型视频规划器（LVP），一个专门为机器人操作设计的大规模视频基础模型，以及将其部署为机器人零样本策略的框架。\n2. 构建并发布了LVP-1M数据集，一个经过精心策划的互联网规模视频数据集，包含人类活动和机器人任务演示。\n3. 通过独立测试协议和真实机器人实验系统评估任务级泛化能力。\n\n论文方法描述：\n1. 采用潜在扩散框架，使用时间因果3D变分自编码器（VAE）压缩视频。\n2. 引入扩散强制变换器（Diffusion Forcing Transformer）来增强时间一致性。\n3. 使用历史引导（History Guidance）技术提高上下文连贯性。\n3. 通过扩散强制训练策略联合训练图像到视频（I2V）和视频到视频（V2V）。\n4. 动作提取管道：使用HaMeR进行人手姿态估计，MegaSaM进行4D一致性对齐，以及Dex-Retargeting进行机器人手指运动重定向。\n\n论文使用数据集和训练资源：\n1. LVP-1M数据集：包含140万个剪辑，来自8个数据源（4个机器人数据集和4个人类活动数据集）。\n2. 训练过程：分两阶段进行，首先在完整数据集上训练60k步，然后在低相机运动子集上微调10k步。\n3. 训练硬件：使用128个H100 SXM5 GPU训练14天。\n\n论文使用的评估环境和评估指标：\n评估环境：\n1. 第三方测试：由独立测试者选择100个野外任务进行评估。\n2. 真实机器人实验：在Franka Emika机械臂（平行夹爪）和G1机械臂（灵巧手）上执行。\n3. 基线比较：与Wan 2.1、Cosmos-Predict 2、Hunyuan I2V等视频生成模型对比。\n\n评估指标：\n四级评估指标：\n1. 正确接触：手与指定物体在正确位置接触。\n2. 正确结束状态：最后一帧实现指令目标。\n3. 任务完成：正确接触和正确结束状态，具有合理连续运动。\n4. 完美任务完成：视觉上无瑕疵的物理一致性和无明显伪影。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>麻省理工学院（MIT），加州大学伯克利分校（UC Berkeley），哈佛大学（Harvard）</p>\n\n<p>论文概述：</p>\n<p>本研究提出了一种基于视频生成的新型机器人基础模型范式。与传统的视觉-语言-动作（VLA）模型不同，该方法利用大规模视频预训练作为主要模态，通过生成视频作为视觉动作规划，然后从视频中提取可执行机器人动作。该方法通过构建一个140亿参数的视频基础模型，能够在未见过的场景和任务中实现零样本泛化。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出大型视频规划器（LVP），一个专门为机器人操作设计的大规模视频基础模型，以及将其部署为机器人零样本策略的框架。</li><li>构建并发布了LVP-1M数据集，一个经过精心策划的互联网规模视频数据集，包含人类活动和机器人任务演示。</li><li>通过独立测试协议和真实机器人实验系统评估任务级泛化能力。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>采用潜在扩散框架，使用时间因果3D变分自编码器（VAE）压缩视频。</li><li>引入扩散强制变换器（Diffusion Forcing Transformer）来增强时间一致性。</li><li>使用历史引导（History Guidance）技术提高上下文连贯性。</li><li>通过扩散强制训练策略联合训练图像到视频（I2V）和视频到视频（V2V）。</li><li>动作提取管道：使用HaMeR进行人手姿态估计，MegaSaM进行4D一致性对齐，以及Dex-Retargeting进行机器人手指运动重定向。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>LVP-1M数据集：包含140万个剪辑，来自8个数据源（4个机器人数据集和4个人类活动数据集）。</li><li>训练过程：分两阶段进行，首先在完整数据集上训练60k步，然后在低相机运动子集上微调10k步。</li><li>训练硬件：使用128个H100 SXM5 GPU训练14天。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<ol><li>第三方测试：由独立测试者选择100个野外任务进行评估。</li><li>真实机器人实验：在Franka Emika机械臂（平行夹爪）和G1机械臂（灵巧手）上执行。</li><li>基线比较：与Wan 2.1、Cosmos-Predict 2、Hunyuan I2V等视频生成模型对比。</li></ol>\n\n<p>评估指标：</p>\n<p>四级评估指标：</p>\n<ol><li>正确接触：手与指定物体在正确位置接触。</li><li>正确结束状态：最后一帧实现指令目标。</li><li>任务完成：正确接触和正确结束状态，具有合理连续运动。</li><li>完美任务完成：视觉上无瑕疵的物理一致性和无明显伪影。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-17",
    "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
    "link": "http://arxiv.org/abs/2512.15692",
    "summary_markdown": "```markdown\n论文研究单位：\nmimic robotics，Microsoft Zurich，ETH Zurich，ETH AI Center，UC Berkeley\n\n论文概述：\n本论文提出mimic-video，一种新型视频-动作模型，用于可泛化的机器人控制。该方法通过将预训练的大规模视频模型与基于流匹配的动作解码器相结合，解决传统视觉-语言-动作模型在物理动态理解上的局限性。mimic-video利用视频数据中蕴含的物理先验知识，将机器人策略直接建立在生成式视频模型的潜在表示上，从而显著提高样本效率和收敛速度。\n\n论文核心贡献点：\n1. 提出视频-动作模型新范式，将机器人控制直接建立在预训练视频模型的潜在表示上\n2. 通过部分去噪策略提取视觉规划信息，避免完整视频生成的计算成本\n3. 在模拟和真实世界机器人操作任务中达到最先进性能\n4. 相比传统VLA架构，样本效率提高10倍，收敛速度提高2倍\n4. 展示视频生成质量与策略性能之间的直接关联\n\n论文方法描述：\n1. 架构由两个条件流匹配模型组成：预训练的语言条件视频主干和轻量级动作解码器\n2. 视频模型基于Cosmos-Predict2，一个20亿参数的潜在扩散变换器\n3. 采用部分去噪策略，在中间流时间提取潜在视觉规划\n4. 动作解码器作为逆动力学模型，通过交叉注意力机制处理视频模型中间表示\n5. 训练过程分两阶段：视频主干微调和使用独立流时间的动作解码器训练\n\n论文使用数据集和训练资源：\n1. 模拟基准：SIMPLER-Bridge，LIBERO\n2. 真实世界评估：使用Franka Emika Panda机械臂和16自由度灵巧手的双手机器人系统\n3. 训练数据包括BridgeDataV2数据集和机器人特定任务数据\n4. 计算资源由瑞士国家超级计算中心提供\n\n论文使用的评估环境和评估指标：\n1. 评估环境：SIMPLER-Bridge，LIBERO，真实世界灵巧双手操作\n2. 评估指标：任务成功率，样本效率，收敛速度\n3. 与多个最先进基线比较，包括OpenVLA，Octo，ThinkAct，FLOWER等\n3. 在真实世界实验中，使用极少量的任务特定数据（1-2小时演示）\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>mimic robotics，Microsoft Zurich，ETH Zurich，ETH AI Center，UC Berkeley</p>\n\n<p>论文概述：</p>\n<p>本论文提出mimic-video，一种新型视频-动作模型，用于可泛化的机器人控制。该方法通过将预训练的大规模视频模型与基于流匹配的动作解码器相结合，解决传统视觉-语言-动作模型在物理动态理解上的局限性。mimic-video利用视频数据中蕴含的物理先验知识，将机器人策略直接建立在生成式视频模型的潜在表示上，从而显著提高样本效率和收敛速度。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出视频-动作模型新范式，将机器人控制直接建立在预训练视频模型的潜在表示上</li><li>通过部分去噪策略提取视觉规划信息，避免完整视频生成的计算成本</li><li>在模拟和真实世界机器人操作任务中达到最先进性能</li><li>相比传统VLA架构，样本效率提高10倍，收敛速度提高2倍</li><li>展示视频生成质量与策略性能之间的直接关联</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>架构由两个条件流匹配模型组成：预训练的语言条件视频主干和轻量级动作解码器</li><li>视频模型基于Cosmos-Predict2，一个20亿参数的潜在扩散变换器</li><li>采用部分去噪策略，在中间流时间提取潜在视觉规划</li><li>动作解码器作为逆动力学模型，通过交叉注意力机制处理视频模型中间表示</li><li>训练过程分两阶段：视频主干微调和使用独立流时间的动作解码器训练</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>模拟基准：SIMPLER-Bridge，LIBERO</li><li>真实世界评估：使用Franka Emika Panda机械臂和16自由度灵巧手的双手机器人系统</li><li>训练数据包括BridgeDataV2数据集和机器人特定任务数据</li><li>计算资源由瑞士国家超级计算中心提供</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：SIMPLER-Bridge，LIBERO，真实世界灵巧双手操作</li><li>评估指标：任务成功率，样本效率，收敛速度</li><li>与多个最先进基线比较，包括OpenVLA，Octo，ThinkAct，FLOWER等</li><li>在真实世界实验中，使用极少量的任务特定数据（1-2小时演示）</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-17",
    "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training",
    "link": "http://arxiv.org/abs/2512.15411",
    "summary_markdown": "论文研究单位：同济大学，电子科技大学\n\n论文概述：论文提出MiVLA模型，旨在解决视觉-语言-动作模型在真实世界机器人数据稀缺情况下的泛化能力不足问题。通过人机互模仿预训练，利用人类手部和机器人手臂之间的行为相似性，构建统一的行为先验知识库。\n\n论文核心贡献点：提出基于人机互模仿预训练的可泛化VLA模型；引入双向人机动作空间转换方法；在三个机器人平台上验证了模型在仿真和真实机器人控制任务中的优越性能。\n\n论文方法描述：采用基于运动学规则和左右手坐标系的双向对齐机制，实现人机动作空间的转换。模型架构包含多模态标记器和基于扩散的动作解码器。通过人机互模仿学习目标，将仿真机器人数据的操作多样性和人类视频中真实世界行为知识整合到统一模型中。\n\n论文使用数据集和训练资源：使用仿真机器人数据（RoboTwin-2.0基准）和人类视频数据。预训练使用4个A100 GPU，批量大小为128；微调使用2个A100 GPU，批量大小为32。\n\n论文使用的评估环境和评估指标：评估环境包括仿真环境（RoboTwin-2.0基准的50个双臂协作操作任务）和真实机器人平台（PiPer、ARX和LocoMan三个异构机器人）。评估指标包括成功率、完整性和时间成本。在仿真任务中平均成功率提升25%，真实机器人控制任务中提升14%。",
    "summary_html": "<p>论文研究单位：同济大学，电子科技大学</p>\n\n<p>论文概述：论文提出MiVLA模型，旨在解决视觉-语言-动作模型在真实世界机器人数据稀缺情况下的泛化能力不足问题。通过人机互模仿预训练，利用人类手部和机器人手臂之间的行为相似性，构建统一的行为先验知识库。</p>\n\n<p>论文核心贡献点：提出基于人机互模仿预训练的可泛化VLA模型；引入双向人机动作空间转换方法；在三个机器人平台上验证了模型在仿真和真实机器人控制任务中的优越性能。</p>\n\n<p>论文方法描述：采用基于运动学规则和左右手坐标系的双向对齐机制，实现人机动作空间的转换。模型架构包含多模态标记器和基于扩散的动作解码器。通过人机互模仿学习目标，将仿真机器人数据的操作多样性和人类视频中真实世界行为知识整合到统一模型中。</p>\n\n<p>论文使用数据集和训练资源：使用仿真机器人数据（RoboTwin-2.0基准）和人类视频数据。预训练使用4个A100 GPU，批量大小为128；微调使用2个A100 GPU，批量大小为32。</p>\n\n<p>论文使用的评估环境和评估指标：评估环境包括仿真环境（RoboTwin-2.0基准的50个双臂协作操作任务）和真实机器人平台（PiPer、ARX和LocoMan三个异构机器人）。评估指标包括成功率、完整性和时间成本。在仿真任务中平均成功率提升25%，真实机器人控制任务中提升14%。</p>"
  },
  {
    "date": "2025-12-17",
    "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
    "link": "http://arxiv.org/abs/2512.15258",
    "summary_markdown": "```markdown\n论文研究单位：\n未明确提及\n\n论文概述：\n本文提出VLA-AN，一种专用于复杂环境中无人机自主导航的高效机载视觉-语言-动作框架。该框架解决了现有大型空中导航模型的四个主要局限性：数据领域差距、时间导航推理不足、生成动作策略的安全问题以及机载部署约束。\n\n论文核心贡献点：\n1. 构建大规模高保真3D高斯泼溅数据集，有效弥合数据领域差距\n2. 引入渐进式三阶段训练框架，依次增强场景理解、核心飞行技能和复杂导航能力\n3. 设计轻量级实时动作模块，结合几何安全校正，确保快速、无碰撞和稳定的指令生成\n4. 通过深度优化机载部署流程，在资源受限的无人机上实现2-3Hz的鲁棒实时推理速率\n5. 实现8.3倍推理吞吐量提升，为轻量级空中机器人提供全链闭环自主路径\n\n论文方法描述：\n1. 数据收集：采用3D高斯泼溅技术与Unity引擎集成，生成高保真视觉观测和符合无人机动力学的飞行轨迹\n2. 三阶段训练：阶段I进行监督微调增强场景理解和推理；阶段II注入导航专用数据传授核心飞行技能；阶段III应用强化学习微调优化复杂决策\n3. 动作模块：作为连续指令生成器，从深度图提取局部障碍物信息，生成可微分排斥梯度力调整轨迹\n4. 机载部署：在NVIDIA Jetson Orin NX上实现，通过Flash-Attention、算子融合、KV缓存预加载和CUDA图调度等优化\n\n论文使用数据集和训练资源：\n- 构建包含超过10万条导航轨迹和100多万个多模态样本的数据集\n- 涵盖多样室内外环境、光照条件、遮挡模式和动态元素\n- 训练使用混合数据集，包括VQA、空间定位、推理和STEM任务\n- 机载计算平台：NVIDIA Jetson Orin NX 16GB模块，约100 TOPS计算能力\n\n论文使用的评估环境和评估指标：\n- 评估环境：仿真环境和真实世界无人机平台\n- 评估指标：成功率作为主要指标\n- 在八种代表性导航场景中评估，包括开放词汇对象导航、精确导航、空间定位、长时程导航等\n- 基准测试对比：OpenVLA、π0和Groot N1.5等主流VLA方法\n- 实现最高单任务成功率98.1%，平均成功率超过90%\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>未明确提及</p>\n\n<p>论文概述：</p>\n<p>本文提出VLA-AN，一种专用于复杂环境中无人机自主导航的高效机载视觉-语言-动作框架。该框架解决了现有大型空中导航模型的四个主要局限性：数据领域差距、时间导航推理不足、生成动作策略的安全问题以及机载部署约束。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>构建大规模高保真3D高斯泼溅数据集，有效弥合数据领域差距</li><li>引入渐进式三阶段训练框架，依次增强场景理解、核心飞行技能和复杂导航能力</li><li>设计轻量级实时动作模块，结合几何安全校正，确保快速、无碰撞和稳定的指令生成</li><li>通过深度优化机载部署流程，在资源受限的无人机上实现2-3Hz的鲁棒实时推理速率</li><li>实现8.3倍推理吞吐量提升，为轻量级空中机器人提供全链闭环自主路径</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>数据收集：采用3D高斯泼溅技术与Unity引擎集成，生成高保真视觉观测和符合无人机动力学的飞行轨迹</li><li>三阶段训练：阶段I进行监督微调增强场景理解和推理；阶段II注入导航专用数据传授核心飞行技能；阶段III应用强化学习微调优化复杂决策</li><li>动作模块：作为连续指令生成器，从深度图提取局部障碍物信息，生成可微分排斥梯度力调整轨迹</li><li>机载部署：在NVIDIA Jetson Orin NX上实现，通过Flash-Attention、算子融合、KV缓存预加载和CUDA图调度等优化</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>构建包含超过10万条导航轨迹和100多万个多模态样本的数据集</li><li>涵盖多样室内外环境、光照条件、遮挡模式和动态元素</li><li>训练使用混合数据集，包括VQA、空间定位、推理和STEM任务</li><li>机载计算平台：NVIDIA Jetson Orin NX 16GB模块，约100 TOPS计算能力</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>评估环境：仿真环境和真实世界无人机平台</li><li>评估指标：成功率作为主要指标</li><li>在八种代表性导航场景中评估，包括开放词汇对象导航、精确导航、空间定位、长时程导航等</li><li>基准测试对比：OpenVLA、π0和Groot N1.5等主流VLA方法</li><li>实现最高单任务成功率98.1%，平均成功率超过90%</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-16",
    "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.14666",
    "summary_markdown": "论文研究单位：新加坡国立大学Show Lab\n\n论文概述：EVOLVE-VLA提出了一种测试时训练框架，使视觉语言动作模型能够在部署环境中通过自主交互持续学习，克服了传统监督微调方法对大量演示数据的依赖和缺乏适应性的问题。该框架通过在线强化学习，利用环境反馈来改进策略，实现了从静态模仿学习向动态自适应学习的范式转变。\n\n论文核心贡献点：\n1. 提出测试时训练框架，使VLA模型通过自主交互持续适应环境，解决了静态SFT的脆弱性和可扩展性限制。\n2. 通过引入学习到的进度估计器来解决缺乏oracle奖励的核心挑战，并开发技术来“驯服”固有的噪声奖励信号，使实用的测试时训练变得可行。\n3. 引入两个关键技术：累积进度估计机制，将噪声估计平滑为稳定信号；渐进式视野扩展策略，实现逐步策略演化，证明对长视野任务有效。\n4. 在LIBERO基准测试中取得显著结果：长视野任务提升8.6%，1-shot学习提升22.0%，并实现零样本跨任务泛化（0%→20.8%），分析揭示自主探索产生的错误恢复等新兴技能。\n\n论文方法描述：\n1. 任务定义为马尔可夫决策过程，VLA策略将状态映射到动作分布，采用动作标记化处理连续机器人动作。\n2. 测试时训练框架包括在线强化学习，其中策略通过组相对策略优化进行更新。\n3. 累积进度估计机制通过基于间隔的里程碑采样和增量进度计算，聚合噪声点估计为稳定可靠信号。\n4. 渐进式视野扩展策略将训练过程分为多个阶段，每个阶段使用逐步增加的最大rollout视野，使策略首先掌握较短的子目标，然后再处理完整任务。\n5. 任务进度估计使用基础评论模型VLAC，该模型以两个图像和任务指令作为输入，输出评论值表示第二个图像相对于第一个图像的任务进度。\n6. 结合渐进式学习和累积进度估计，分别解决时间信用分配和噪声奖励问题。\n\n论文使用数据集和训练资源：\n1. 在LIBERO基准测试上进行评估，该基准包含四个任务套件：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long，每个套件包含10个任务，每个任务有50个专家演示。\n2. 基础模型采用OpenVLA-OFT，这是一个最先进的自回归VLA模型。\n3. 奖励模型使用基础评论模型VLAC，该模型在大规模机器人操作数据集上预训练。\n4. 在线强化学习采用组相对策略优化进行策略更新。\n\n论文使用的评估环境和评估指标：\n1. 评估环境为LIBERO基准测试的模拟环境。\n2. 评估指标为平均成功率，每个任务进行50次试验。\n3. 在低数据体制下进行实验，使用每个任务仅一个演示进行SFT预训练，然后进行测试时训练。\n4. 进行消融研究验证累积进度估计和渐进式视野扩展的有效性。\n5. 定性分析揭示策略行为，包括错误恢复能力和新策略发现。\n6. 评估结果显示在长视野任务上提升8.6%，1-shot学习提升22.0%，并实现跨任务泛化。",
    "summary_html": "<p>论文研究单位：新加坡国立大学Show Lab</p>\n\n<p>论文概述：EVOLVE-VLA提出了一种测试时训练框架，使视觉语言动作模型能够在部署环境中通过自主交互持续学习，克服了传统监督微调方法对大量演示数据的依赖和缺乏适应性的问题。该框架通过在线强化学习，利用环境反馈来改进策略，实现了从静态模仿学习向动态自适应学习的范式转变。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出测试时训练框架，使VLA模型通过自主交互持续适应环境，解决了静态SFT的脆弱性和可扩展性限制。</li><li>通过引入学习到的进度估计器来解决缺乏oracle奖励的核心挑战，并开发技术来“驯服”固有的噪声奖励信号，使实用的测试时训练变得可行。</li><li>引入两个关键技术：累积进度估计机制，将噪声估计平滑为稳定信号；渐进式视野扩展策略，实现逐步策略演化，证明对长视野任务有效。</li><li>在LIBERO基准测试中取得显著结果：长视野任务提升8.6%，1-shot学习提升22.0%，并实现零样本跨任务泛化（0%→20.8%），分析揭示自主探索产生的错误恢复等新兴技能。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>任务定义为马尔可夫决策过程，VLA策略将状态映射到动作分布，采用动作标记化处理连续机器人动作。</li><li>测试时训练框架包括在线强化学习，其中策略通过组相对策略优化进行更新。</li><li>累积进度估计机制通过基于间隔的里程碑采样和增量进度计算，聚合噪声点估计为稳定可靠信号。</li><li>渐进式视野扩展策略将训练过程分为多个阶段，每个阶段使用逐步增加的最大rollout视野，使策略首先掌握较短的子目标，然后再处理完整任务。</li><li>任务进度估计使用基础评论模型VLAC，该模型以两个图像和任务指令作为输入，输出评论值表示第二个图像相对于第一个图像的任务进度。</li><li>结合渐进式学习和累积进度估计，分别解决时间信用分配和噪声奖励问题。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>在LIBERO基准测试上进行评估，该基准包含四个任务套件：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long，每个套件包含10个任务，每个任务有50个专家演示。</li><li>基础模型采用OpenVLA-OFT，这是一个最先进的自回归VLA模型。</li><li>奖励模型使用基础评论模型VLAC，该模型在大规模机器人操作数据集上预训练。</li><li>在线强化学习采用组相对策略优化进行策略更新。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境为LIBERO基准测试的模拟环境。</li><li>评估指标为平均成功率，每个任务进行50次试验。</li><li>在低数据体制下进行实验，使用每个任务仅一个演示进行SFT预训练，然后进行测试时训练。</li><li>进行消融研究验证累积进度估计和渐进式视野扩展的有效性。</li><li>定性分析揭示策略行为，包括错误恢复能力和新策略发现。</li><li>评估结果显示在长视野任务上提升8.6%，1-shot学习提升22.0%，并实现跨任务泛化。</li></ol>"
  },
  {
    "date": "2025-12-16",
    "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
    "link": "http://arxiv.org/abs/2512.14031",
    "summary_markdown": "```markdown\n论文研究单位：\n- 石溪大学土木工程系\n- 弗吉尼亚理工大学土木与环境工程系\n- 弗吉尼亚理工大学电气与计算机工程系\n\n论文概述：\n本研究评估了两种机器人技能学习方法在建筑任务中的应用：视觉-语言-动作模型和分层强化学习方法。研究目标是比较两种方法在任务性能和实际部署所需工作量方面的差异，为建筑自动化提供指导。研究开发了两种遥操作接口用于控制机器人和收集演示数据，并进行了三阶段评估。\n\n论文核心贡献点：\n- 开发了两种轻量级数据收集接口：基于鼠标的关节遥操作系统和基于键盘的末端执行器控制接口\n- 创建了粘附任务环境模式：提供可重用的MuJoCo仿真环境模板\n- 提出两种互补模型：强调动作级细节的分层强化学习框架和强调指令跟随与少样本泛化的VLA框架\n- 建立了分层强化学习方法和VLA在匹配评估协议下的基线比较\n\n论文方法描述：\n- 分层强化学习方法采用两个基线：多层感知机直接策略和基于深度Q网络的模仿模型。MLP策略将低维观测映射到关节空间动作，DQN模仿模型使用形状奖励进行训练\n- VLA方法评估了三种不同模型：OpenVLA、π₀和π₀.₅。VLA模型使用冻结骨干和轻量级控制头进行端到端视觉-语言策略微调\n- 使用参数高效的LoRA适配器进行微调，仅训练头部和可选的低秩适配器\n- 实验设计包括三个阶段：MLP与DQN比较、三种VLA模型比较、VLA与DQN基准测试\n\n论文使用数据集和训练资源：\n- 跨多场景数据集：桌面场景100个演示数据，地面场景200个演示数据\n- 使用MuJoCo物理仿真器，遵循LIBERO和Robosuite设置构建两个工作场所\n- 训练使用AdamW优化器，学习率为5×10⁻⁴\n- 机器人配置包括UR5e机械臂和Franka Panda机械臂，均配备粘附夹具\n\n论文使用的评估环境和评估指标：\n评估环境：\n- MuJoCo仿真环境\n- 两个工作场所：地面级场景和桌面级场景\n\n评估指标：\n- 拾取成功率\n- 放置成功率\n- 对齐成功率\n- 平均关节角度误差\n- 样本效率测量\n- 计算效率测量\n- 泛化能力测试（面板位置轻微扰动）\n- 训练损失曲线\n- 过拟合情况分析\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<ul><li>石溪大学土木工程系</li><li>弗吉尼亚理工大学土木与环境工程系</li><li>弗吉尼亚理工大学电气与计算机工程系</li></ul>\n\n<p>论文概述：</p>\n<p>本研究评估了两种机器人技能学习方法在建筑任务中的应用：视觉-语言-动作模型和分层强化学习方法。研究目标是比较两种方法在任务性能和实际部署所需工作量方面的差异，为建筑自动化提供指导。研究开发了两种遥操作接口用于控制机器人和收集演示数据，并进行了三阶段评估。</p>\n\n<p>论文核心贡献点：</p>\n<ul><li>开发了两种轻量级数据收集接口：基于鼠标的关节遥操作系统和基于键盘的末端执行器控制接口</li><li>创建了粘附任务环境模式：提供可重用的MuJoCo仿真环境模板</li><li>提出两种互补模型：强调动作级细节的分层强化学习框架和强调指令跟随与少样本泛化的VLA框架</li><li>建立了分层强化学习方法和VLA在匹配评估协议下的基线比较</li></ul>\n\n<p>论文方法描述：</p>\n<ul><li>分层强化学习方法采用两个基线：多层感知机直接策略和基于深度Q网络的模仿模型。MLP策略将低维观测映射到关节空间动作，DQN模仿模型使用形状奖励进行训练</li><li>VLA方法评估了三种不同模型：OpenVLA、π₀和π₀.₅。VLA模型使用冻结骨干和轻量级控制头进行端到端视觉-语言策略微调</li><li>使用参数高效的LoRA适配器进行微调，仅训练头部和可选的低秩适配器</li><li>实验设计包括三个阶段：MLP与DQN比较、三种VLA模型比较、VLA与DQN基准测试</li></ul>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>跨多场景数据集：桌面场景100个演示数据，地面场景200个演示数据</li><li>使用MuJoCo物理仿真器，遵循LIBERO和Robosuite设置构建两个工作场所</li><li>训练使用AdamW优化器，学习率为5×10⁻⁴</li><li>机器人配置包括UR5e机械臂和Franka Panda机械臂，均配备粘附夹具</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<ul><li>MuJoCo仿真环境</li><li>两个工作场所：地面级场景和桌面级场景</li></ul>\n\n<p>评估指标：</p>\n<ul><li>拾取成功率</li><li>放置成功率</li><li>对齐成功率</li><li>平均关节角度误差</li><li>样本效率测量</li><li>计算效率测量</li><li>泛化能力测试（面板位置轻微扰动）</li><li>训练损失曲线</li><li>过拟合情况分析</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-15",
    "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
    "link": "http://arxiv.org/abs/2512.13636",
    "summary_markdown": "论文研究单位\n- 华中科技大学\n- 小米EV\n\n论文概述\n该论文提出了一种名为MindDrive的视觉-语言-动作模型，用于通过在线强化学习实现自动驾驶。当前基于模仿学习的视觉-语言-动作模型存在分布偏移和因果混淆等问题。在线强化学习通过试错学习提供了一条有前景的解决途径，但在连续动作空间中的低效探索限制了其应用。MindDrive通过引入动态语言-动作映射，将动作空间从轨迹转换为基于语言的决策，从而显著提高探索效率，并利用轨迹奖励来优化模型的推理能力。\n\n论文核心贡献点\n- 提出MindDrive，一种用于视觉-语言-动作自动驾驶模型的在线强化学习框架。通过动态语言-动作映射，显著提高探索效率，使轨迹级动作奖励能够促进推理优化。\n- 引入一种计算高效的在线强化学习方案。据作者所知，MindDrive是首个在模拟器中通过在线强化学习训练的基于视觉-语言-动作的自动驾驶模型。\n- 广泛的实验证明了MindDrive的有效性，在Bench2Drive基准测试中达到78.04的驾驶分数和55.09%的成功率，显著优于相同模型规模下的最先进模仿学习基线。\n\n论文方法描述\n- MindDrive架构包含两个主要组件：决策专家和动作专家，它们共享一个共同的视觉编码器和文本标记器，但仅在其各自的LoRA参数上有所不同。\n- 训练过程包括两个阶段：模仿学习阶段建立语言和动作空间之间的映射，为在线强化学习提供高质量的候选轨迹，有效减少其探索空间。\n- 在线强化学习阶段通过在线环境中的动作奖励进一步增强模型的理解能力。\n- 决策专家从导航指令和多视角视觉输入中执行高级推理，生成抽象驾驶决策形式的元动作。动作专家将这些元动作转换为具体动作轨迹，以场景信息和指令为条件。\n- 具体方法包括问题建模为马尔可夫决策过程，使用变分自编码器对齐语言和动作空间，并通过近端策略优化算法优化策略。\n\n论文使用数据集和训练资源\n- 数据集：使用Bench2Drive数据集进行训练和评估，这是一个基于CARLA模拟器的综合闭环基准测试。在模仿学习阶段，采用包含1000个片段的官方基础集进行训练。\n- 训练资源：所有实验在32个NVIDIA A800 GPU上进行，每个GPU内存为80 GB。使用EVA-02-L作为视觉编码器，轻量级Qwen2-0.5B作为基础大语言模型。在强化学习阶段，使用24个并行CARLA模拟器收集数据。\n\n论文使用的评估环境和评估指标\n- 评估环境：在CARLA模拟器中进行闭环评估，包括220条短路线，覆盖44个交互场景。\n\n评估指标\n- 驾驶分数：基于路线完成情况，但会因违规行为而减少。\n- 成功率：在指定时间限制内完成路线的比例。\n- 多能力：独立评估五种高级城市驾驶技能。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>华中科技大学</li><li>小米EV</li></ul>\n\n<p>论文概述</p>\n<p>该论文提出了一种名为MindDrive的视觉-语言-动作模型，用于通过在线强化学习实现自动驾驶。当前基于模仿学习的视觉-语言-动作模型存在分布偏移和因果混淆等问题。在线强化学习通过试错学习提供了一条有前景的解决途径，但在连续动作空间中的低效探索限制了其应用。MindDrive通过引入动态语言-动作映射，将动作空间从轨迹转换为基于语言的决策，从而显著提高探索效率，并利用轨迹奖励来优化模型的推理能力。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出MindDrive，一种用于视觉-语言-动作自动驾驶模型的在线强化学习框架。通过动态语言-动作映射，显著提高探索效率，使轨迹级动作奖励能够促进推理优化。</li><li>引入一种计算高效的在线强化学习方案。据作者所知，MindDrive是首个在模拟器中通过在线强化学习训练的基于视觉-语言-动作的自动驾驶模型。</li><li>广泛的实验证明了MindDrive的有效性，在Bench2Drive基准测试中达到78.04的驾驶分数和55.09%的成功率，显著优于相同模型规模下的最先进模仿学习基线。</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>MindDrive架构包含两个主要组件：决策专家和动作专家，它们共享一个共同的视觉编码器和文本标记器，但仅在其各自的LoRA参数上有所不同。</li><li>训练过程包括两个阶段：模仿学习阶段建立语言和动作空间之间的映射，为在线强化学习提供高质量的候选轨迹，有效减少其探索空间。</li><li>在线强化学习阶段通过在线环境中的动作奖励进一步增强模型的理解能力。</li><li>决策专家从导航指令和多视角视觉输入中执行高级推理，生成抽象驾驶决策形式的元动作。动作专家将这些元动作转换为具体动作轨迹，以场景信息和指令为条件。</li><li>具体方法包括问题建模为马尔可夫决策过程，使用变分自编码器对齐语言和动作空间，并通过近端策略优化算法优化策略。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：使用Bench2Drive数据集进行训练和评估，这是一个基于CARLA模拟器的综合闭环基准测试。在模仿学习阶段，采用包含1000个片段的官方基础集进行训练。</li><li>训练资源：所有实验在32个NVIDIA A800 GPU上进行，每个GPU内存为80 GB。使用EVA-02-L作为视觉编码器，轻量级Qwen2-0.5B作为基础大语言模型。在强化学习阶段，使用24个并行CARLA模拟器收集数据。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：在CARLA模拟器中进行闭环评估，包括220条短路线，覆盖44个交互场景。</li></ul>\n\n<p>评估指标</p>\n<ul><li>驾驶分数：基于路线完成情况，但会因违规行为而减少。</li><li>成功率：在指定时间限制内完成路线的比例。</li><li>多能力：独立评估五种高级城市驾驶技能。</li></ul>"
  },
  {
    "date": "2025-12-15",
    "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
    "link": "http://arxiv.org/abs/2512.13080",
    "summary_markdown": "```markdown\n论文研究单位：\n北京大学，中国人民大学，BeingBeyond\n\n论文概述：\n该论文提出了一种空间感知的视觉-语言-动作（VLA）预训练范式，通过从人类视频中提取视觉-物理对齐信息，以解决现有VLA模型依赖2D视觉输入在3D物理环境中执行动作时存在的感知与动作基础之间的差距。该方法利用大规模人类演示视频提取3D视觉和3D动作标注，构建了Hand3D数据集，并开发了双编码器架构的VIPA-VLA模型，在机器人任务中展现出更强的空间基础能力和泛化性能。\n\n论文核心贡献点：\n1. 提出空间感知VLA预训练范式，通过大规模人类视频数据桥接2D视觉感知与3D物理动作\n2. 构建Hand3D数据集，包含人类操作视频的3D视觉和动作标注\n3. 提出VIPA-VLA模型，采用双编码器架构增强空间特征表示\n4. 在模拟和真实机器人任务中验证了方法的有效性\n\n论文方法描述：\n1. 采用双编码器架构，包含语义视觉编码器和3D视觉编码器（Cut3R）\n2. 通过交叉注意力融合层结合语义和空间特征\n3. 设计两阶段预训练策略：第一阶段使用3D视觉标注进行视觉-物理对齐；第二阶段扩展LLM词汇表包含运动token，训练模型预测3D动作序列\n4. 后训练阶段使用扩散变换器（DiT）作为动作头生成可执行动作\n5. 从人类视频中提取手部轨迹作为3D动作监督\n\n论文使用数据集和训练资源：\n1. Hand3D-visual：包含300,368个指令-答案对，涵盖空间关系、任务完成、手部运动和相机移动四类任务\n6. Hand3D-action：包含1,030,850个视频-指令-动作对\n7. 训练资源：8张NVIDIA A800 GPU\n8. 预训练时间：第一阶段约6小时，第二阶段约20小时\n9. 后训练时间：LIBERO和真实任务约5小时，RoboCasa约40小时\n\n论文使用的评估环境和评估指标：\n1. 模拟环境：LIBERO基准（四个任务套件）和RoboCasa基准（24个任务）\n2. 真实机器人环境：Franka Research 3机械臂，Inspire手部，两个RealSense L515相机\n3. 评估指标：任务成功率（%），基于500次试验（LIBERO）或50次试验（RoboCasa）\n4. 评估设置：单视图、双视图和多视图输入配置\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>北京大学，中国人民大学，BeingBeyond</p>\n\n<p>论文概述：</p>\n<p>该论文提出了一种空间感知的视觉-语言-动作（VLA）预训练范式，通过从人类视频中提取视觉-物理对齐信息，以解决现有VLA模型依赖2D视觉输入在3D物理环境中执行动作时存在的感知与动作基础之间的差距。该方法利用大规模人类演示视频提取3D视觉和3D动作标注，构建了Hand3D数据集，并开发了双编码器架构的VIPA-VLA模型，在机器人任务中展现出更强的空间基础能力和泛化性能。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出空间感知VLA预训练范式，通过大规模人类视频数据桥接2D视觉感知与3D物理动作</li><li>构建Hand3D数据集，包含人类操作视频的3D视觉和动作标注</li><li>提出VIPA-VLA模型，采用双编码器架构增强空间特征表示</li><li>在模拟和真实机器人任务中验证了方法的有效性</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>采用双编码器架构，包含语义视觉编码器和3D视觉编码器（Cut3R）</li><li>通过交叉注意力融合层结合语义和空间特征</li><li>设计两阶段预训练策略：第一阶段使用3D视觉标注进行视觉-物理对齐；第二阶段扩展LLM词汇表包含运动token，训练模型预测3D动作序列</li><li>后训练阶段使用扩散变换器（DiT）作为动作头生成可执行动作</li><li>从人类视频中提取手部轨迹作为3D动作监督</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>Hand3D-visual：包含300,368个指令-答案对，涵盖空间关系、任务完成、手部运动和相机移动四类任务</li><li>Hand3D-action：包含1,030,850个视频-指令-动作对</li><li>训练资源：8张NVIDIA A800 GPU</li><li>预训练时间：第一阶段约6小时，第二阶段约20小时</li><li>后训练时间：LIBERO和真实任务约5小时，RoboCasa约40小时</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>模拟环境：LIBERO基准（四个任务套件）和RoboCasa基准（24个任务）</li><li>真实机器人环境：Franka Research 3机械臂，Inspire手部，两个RealSense L515相机</li><li>评估指标：任务成功率（%），基于500次试验（LIBERO）或50次试验（RoboCasa）</li><li>评估设置：单视图、双视图和多视图输入配置</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-15",
    "title": "Motus: A Unified Latent Action World Model",
    "link": "http://arxiv.org/abs/2512.13030",
    "summary_markdown": "论文研究单位：清华大学计算机科学与技术系，人工智能研究院，北京国家信息科学技术研究中心，THBI实验室，清华大学-博世联合机器学习中心；北京大学；地平线机器人\n\n论文概述：Motus是一个统一潜在动作世界模型，旨在解决具身智能中现有方法将理解、世界建模和控制等功能分离的问题。该模型整合了五种主流建模范式：视觉语言动作模型、世界模型、逆动力学模型、视频生成模型和视频动作联合预测模型。通过利用现有预训练模型和丰富可共享的运动信息，Motus在一个统一框架内融合了多模态生成能力，并能够从大规模异构数据中学习。\n\n论文核心贡献点：提出一个统一具身基础模型，整合五种主流建模范式而不损害通用多模态先验；提出一个可扩展的机器人训练方法，包含三阶段训练流程和六层数据金字塔，利用基于光流的潜在动作学习跨具身可迁移的运动知识；在仿真和真实世界场景中显著优于现有最先进方法。\n\n论文方法描述：采用混合Transformer架构，集成三个专家模块：理解专家、视频生成专家和动作专家，通过三模型联合注意力机制实现跨模态特征融合。引入UniDiffuser风格的调度器，为不同模态分配不同时间步和噪声尺度，实现灵活切换不同推理模式。利用光流学习潜在动作，将其编码为像素级“增量动作”，实现大规模动作预训练。训练过程分为三个阶段：学习视觉动力学、学习动作表示和针对目标机器人进行专门化。\n\n论文使用数据集和训练资源：使用六层数据金字塔，包括网络数据、自我中心人类视频、合成数据、任务无关数据、多机器人任务轨迹数据和目标机器人任务轨迹数据。具体数据集包括RoboTwin 2.0仿真数据、LIBERO-Long、VLABench等。训练资源包括预训练基础模型如Wan 2.2 5B视频生成模型和Qwen3-VL-2B视觉语言模型。\n\n论文使用的评估环境和评估指标：在仿真环境（RoboTwin 2.0仿真）和真实世界环境中进行评估。评估指标包括任务成功率，在仿真环境中相比X-VLA提升15%，相比π0.5提升45%；在真实世界场景中提升11%至48%。具体任务包括放置双鞋、移动订书机垫、堆叠两个积木、扫描物体、放置物体架、放置风扇、移动药瓶垫、拾取双瓶、积木排序RGB、放置面包篮、堆叠三个积木、放置物体篮、打开微波炉等50多个任务。",
    "summary_html": "<p>论文研究单位：清华大学计算机科学与技术系，人工智能研究院，北京国家信息科学技术研究中心，THBI实验室，清华大学-博世联合机器学习中心；北京大学；地平线机器人</p>\n\n<p>论文概述：Motus是一个统一潜在动作世界模型，旨在解决具身智能中现有方法将理解、世界建模和控制等功能分离的问题。该模型整合了五种主流建模范式：视觉语言动作模型、世界模型、逆动力学模型、视频生成模型和视频动作联合预测模型。通过利用现有预训练模型和丰富可共享的运动信息，Motus在一个统一框架内融合了多模态生成能力，并能够从大规模异构数据中学习。</p>\n\n<p>论文核心贡献点：提出一个统一具身基础模型，整合五种主流建模范式而不损害通用多模态先验；提出一个可扩展的机器人训练方法，包含三阶段训练流程和六层数据金字塔，利用基于光流的潜在动作学习跨具身可迁移的运动知识；在仿真和真实世界场景中显著优于现有最先进方法。</p>\n\n<p>论文方法描述：采用混合Transformer架构，集成三个专家模块：理解专家、视频生成专家和动作专家，通过三模型联合注意力机制实现跨模态特征融合。引入UniDiffuser风格的调度器，为不同模态分配不同时间步和噪声尺度，实现灵活切换不同推理模式。利用光流学习潜在动作，将其编码为像素级“增量动作”，实现大规模动作预训练。训练过程分为三个阶段：学习视觉动力学、学习动作表示和针对目标机器人进行专门化。</p>\n\n<p>论文使用数据集和训练资源：使用六层数据金字塔，包括网络数据、自我中心人类视频、合成数据、任务无关数据、多机器人任务轨迹数据和目标机器人任务轨迹数据。具体数据集包括RoboTwin 2.0仿真数据、LIBERO-Long、VLABench等。训练资源包括预训练基础模型如Wan 2.2 5B视频生成模型和Qwen3-VL-2B视觉语言模型。</p>\n\n<p>论文使用的评估环境和评估指标：在仿真环境（RoboTwin 2.0仿真）和真实世界环境中进行评估。评估指标包括任务成功率，在仿真环境中相比X-VLA提升15%，相比π0.5提升45%；在真实世界场景中提升11%至48%。具体任务包括放置双鞋、移动订书机垫、堆叠两个积木、扫描物体、放置物体架、放置风扇、移动药瓶垫、拾取双瓶、积木排序RGB、放置面包篮、堆叠三个积木、放置物体篮、打开微波炉等50多个任务。</p>"
  },
  {
    "date": "2025-12-14",
    "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
    "link": "http://arxiv.org/abs/2512.12799",
    "summary_markdown": "论文研究单位：香港大学，隐王智能科技有限公司，天津大学，华中科技大学\n\n论文概述：该论文提出DrivePI，一种空间感知的4D多模态大语言模型，用于统一自动驾驶的理解、感知、预测和规划。它结合了粗粒度语言空间理解和细粒度3D感知能力，同时兼容视觉-动作模型和视觉-语言-动作框架，通过端到端优化并行执行空间理解、3D感知（3D占用）、预测（占用流）和规划（动作输出）。\n\n论文核心贡献点：1. 提出首个统一的空间感知4D MLLM框架，无缝集成粗粒度语言空间理解和细粒度3D感知能力，弥合自动驾驶中基于视觉-动作和基于视觉-语言-动作范式之间的差距。2. 引入LiDAR作为相机图像的补充传感模态，提供高精度3D几何信息。3. 开发基于数据引擎的三个互补空间理解基准，构建多种问答对评估时空维度的语言空间推理能力。3. 尽管仅使用0.5B参数MLLM骨干，DrivePI在3D占用和占用流方面超越现有视觉-动作模型，同时保持与现有视觉-语言-动作框架相当的交互能力。\n\n论文方法描述：方法包含四个主要步骤：1. 使用多模态视觉编码器处理多视图图像和LiDAR点云，转换为紧凑的潜在鸟瞰图特征表示。2. 通过空间投影器将鸟瞰图特征映射到语言空间，获得视觉标记。3. 将视觉标记和文本标记输入MLLM，生成输出标记。4. 使用四个专用头：文本头用于自回归场景理解，3D占用头用于精确空间感知，占用流头用于像素级运动预测，动作扩散头用于轨迹规划。所有任务在训练期间通过加权损失函数进行联合优化。\n\n论文使用数据集和训练资源：在nuScenes数据集上进行实验，包含750个训练场景、150个验证场景和150个测试场景。使用OpenOcc作为主要占用评估基准，Occ3D用于全面比较3D占用方法。训练使用8个NVIDIA L40S GPU，采用Qwen2.5-0.5B作为基础模型。\n\n论文使用的评估环境和评估指标：评估环境包括OpenOcc验证集、nuScenes验证集和nuScenes-QA验证集。评估指标包括：3D占用使用RayIoU，占用流使用平均速度误差（mAVE），规划使用L2距离误差和碰撞率，文本理解使用准确率，4D空间理解使用占用状态准确率、占用类别准确率、动作状态准确率等。",
    "summary_html": "<p>论文研究单位：香港大学，隐王智能科技有限公司，天津大学，华中科技大学</p>\n\n<p>论文概述：该论文提出DrivePI，一种空间感知的4D多模态大语言模型，用于统一自动驾驶的理解、感知、预测和规划。它结合了粗粒度语言空间理解和细粒度3D感知能力，同时兼容视觉-动作模型和视觉-语言-动作框架，通过端到端优化并行执行空间理解、3D感知（3D占用）、预测（占用流）和规划（动作输出）。</p>\n\n<p>论文核心贡献点：1. 提出首个统一的空间感知4D MLLM框架，无缝集成粗粒度语言空间理解和细粒度3D感知能力，弥合自动驾驶中基于视觉-动作和基于视觉-语言-动作范式之间的差距。2. 引入LiDAR作为相机图像的补充传感模态，提供高精度3D几何信息。3. 开发基于数据引擎的三个互补空间理解基准，构建多种问答对评估时空维度的语言空间推理能力。3. 尽管仅使用0.5B参数MLLM骨干，DrivePI在3D占用和占用流方面超越现有视觉-动作模型，同时保持与现有视觉-语言-动作框架相当的交互能力。</p>\n\n<p>论文方法描述：方法包含四个主要步骤：1. 使用多模态视觉编码器处理多视图图像和LiDAR点云，转换为紧凑的潜在鸟瞰图特征表示。2. 通过空间投影器将鸟瞰图特征映射到语言空间，获得视觉标记。3. 将视觉标记和文本标记输入MLLM，生成输出标记。4. 使用四个专用头：文本头用于自回归场景理解，3D占用头用于精确空间感知，占用流头用于像素级运动预测，动作扩散头用于轨迹规划。所有任务在训练期间通过加权损失函数进行联合优化。</p>\n\n<p>论文使用数据集和训练资源：在nuScenes数据集上进行实验，包含750个训练场景、150个验证场景和150个测试场景。使用OpenOcc作为主要占用评估基准，Occ3D用于全面比较3D占用方法。训练使用8个NVIDIA L40S GPU，采用Qwen2.5-0.5B作为基础模型。</p>\n\n<p>论文使用的评估环境和评估指标：评估环境包括OpenOcc验证集、nuScenes验证集和nuScenes-QA验证集。评估指标包括：3D占用使用RayIoU，占用流使用平均速度误差（mAVE），规划使用L2距离误差和碰撞率，文本理解使用准确率，4D空间理解使用占用状态准确率、占用类别准确率、动作状态准确率等。</p>"
  },
  {
    "date": "2025-12-12",
    "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.11769",
    "summary_markdown": "```markdown\n论文研究单位：\n圣母大学，里海大学\n\n论文概述：\nblurr是一种轻量级推理包装器，旨在加速视觉语言动作模型的推理过程，而无需重新训练模型或修改现有检查点。该方法针对现有vla模型在交互式部署中的高延迟问题，通过优化推理流程实现实时响应。\n\n论文核心贡献点：\n1. 无需架构修改或重新训练即可加速现有vla控制器\n2. 结合指令前缀kv缓存、混合精度执行和单步控制策略\n3. 在保持原始策略准确性的同时显著降低推理延迟和内存消耗\n4. 提供可实时切换推理选项的交互式web演示\n\n论文方法描述：\n1. 单步控制与前缀缓存指令：将语言指令处理为一次性前缀kv缓存，在整episode中重复使用\n2. 高效bf16解码器：使用bf16精度执行、编译计算图和flashattention内核\n3. 推理流程重组：减少冗余前缀计算、最小化每步token成本、最大化张量核心利用率\n4. 运行时优化：通过torch.compile实现内核融合，使用sdp后端启用高效注意力内核\n\n论文使用数据集和训练资源：\n1. 使用simplerenv桥接任务的四个领域内任务：胡萝卜放盘子、茄子放容器、勺子放盘子、堆叠积木\n2. 基于现有pi-0、openvla和tracevla检查点\n3. 实验在单个nvidia h100 gpu上进行\n\n论文使用评估环境和评估指标：\n1. 评估环境：simplerenv仿真环境，nvidia h100 gpu\n2. 评估指标：\n - 每步延迟（毫秒）\n - 峰值vram使用量（gb）\n - 有效gflops\n - 任务成功率（100个评估episode的平均值）\n - 控制频率（hz）\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>圣母大学，里海大学</p>\n\n<p>论文概述：</p>\n<p>blurr是一种轻量级推理包装器，旨在加速视觉语言动作模型的推理过程，而无需重新训练模型或修改现有检查点。该方法针对现有vla模型在交互式部署中的高延迟问题，通过优化推理流程实现实时响应。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>无需架构修改或重新训练即可加速现有vla控制器</li><li>结合指令前缀kv缓存、混合精度执行和单步控制策略</li><li>在保持原始策略准确性的同时显著降低推理延迟和内存消耗</li><li>提供可实时切换推理选项的交互式web演示</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>单步控制与前缀缓存指令：将语言指令处理为一次性前缀kv缓存，在整episode中重复使用</li><li>高效bf16解码器：使用bf16精度执行、编译计算图和flashattention内核</li><li>推理流程重组：减少冗余前缀计算、最小化每步token成本、最大化张量核心利用率</li><li>运行时优化：通过torch.compile实现内核融合，使用sdp后端启用高效注意力内核</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>使用simplerenv桥接任务的四个领域内任务：胡萝卜放盘子、茄子放容器、勺子放盘子、堆叠积木</li><li>基于现有pi-0、openvla和tracevla检查点</li><li>实验在单个nvidia h100 gpu上进行</li></ol>\n\n<p>论文使用评估环境和评估指标：</p>\n<ol><li>评估环境：simplerenv仿真环境，nvidia h100 gpu</li><li>评估指标：</li></ol>\n<p> - 每步延迟（毫秒）</p>\n<p> - 峰值vram使用量（gb）</p>\n<p> - 有效gflops</p>\n<p> - 任务成功率（100个评估episode的平均值）</p>\n<p> - 控制频率（hz）</p>\n<p>```</p>"
  },
  {
    "date": "2025-12-12",
    "title": "Embodied Image Compression",
    "link": "http://arxiv.org/abs/2512.11612",
    "summary_markdown": "论文研究单位：上海交通大学，上海人工智能实验室，南洋理工大学\n\n论文概述：本文提出了具身图像压缩这一新兴科学问题，针对具身AI在多智能体系统中的通信约束和实时任务执行需求，建立了首个专为具身操作设计的基准数据集EmbodiedComp。该研究通过模拟和真实环境实验，揭示了现有图像编解码器在超低码率下会导致视觉语言动作模型在简单操作任务上失效。\n\n论文核心贡献点：1. 发布首个针对具身操作的基准数据集EmbodiedComp，包含100个标准化测试序列；2. 推导了具身感知的RVS-码率关系模型；3. 实证评估了10种先进图像编解码器在具身操作任务上的表现。\n\n论文方法描述：采用闭环评估范式，包含四个顺序模块：场景渲染和采样参考图像、编解码器压缩、将失真图像输入VLA进行推理、在仿真中执行动作改变环境状态并重新采样。通过调整质量因子和分辨率来满足目标码率要求。\n\n论文使用数据集和训练资源：使用Robosuite 1.5.1仿真平台，包含2000个静态专家轨迹用于训练，100个交互场景用于测试。训练使用4×NVIDIA H200 NVL 141GB GPU，推理使用NVIDIA GeForce RTX 5090 32GB GPU。\n\n论文使用的评估环境和评估指标：评估环境包括仿真和真实世界设置。评估指标包括成功率（SR）和步数（Step），用于衡量任务完成情况和效率。",
    "summary_html": "<p>论文研究单位：上海交通大学，上海人工智能实验室，南洋理工大学</p>\n\n<p>论文概述：本文提出了具身图像压缩这一新兴科学问题，针对具身AI在多智能体系统中的通信约束和实时任务执行需求，建立了首个专为具身操作设计的基准数据集EmbodiedComp。该研究通过模拟和真实环境实验，揭示了现有图像编解码器在超低码率下会导致视觉语言动作模型在简单操作任务上失效。</p>\n\n<p>论文核心贡献点：1. 发布首个针对具身操作的基准数据集EmbodiedComp，包含100个标准化测试序列；2. 推导了具身感知的RVS-码率关系模型；3. 实证评估了10种先进图像编解码器在具身操作任务上的表现。</p>\n\n<p>论文方法描述：采用闭环评估范式，包含四个顺序模块：场景渲染和采样参考图像、编解码器压缩、将失真图像输入VLA进行推理、在仿真中执行动作改变环境状态并重新采样。通过调整质量因子和分辨率来满足目标码率要求。</p>\n\n<p>论文使用数据集和训练资源：使用Robosuite 1.5.1仿真平台，包含2000个静态专家轨迹用于训练，100个交互场景用于测试。训练使用4×NVIDIA H200 NVL 141GB GPU，推理使用NVIDIA GeForce RTX 5090 32GB GPU。</p>\n\n<p>论文使用的评估环境和评估指标：评估环境包括仿真和真实世界设置。评估指标包括成功率（SR）和步数（Step），用于衡量任务完成情况和效率。</p>"
  },
  {
    "date": "2025-12-12",
    "title": "Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents",
    "link": "http://arxiv.org/abs/2512.11584",
    "summary_markdown": "```markdown\n论文研究单位：\n- 索菲亚大学“圣克莱门特奥赫里茨基”\n- 索菲亚技术大学\n- 特温特大学EEMCS学院\n- GATE研究所，索菲亚大学\n\n论文概述：\n本论文提出了原子动作切片方法，一种规划器对齐的方法，用于将长时程演示分解为短小的、类型化的原子动作，这些动作易于规划器使用且便于策略学习。该方法解决了当前视觉-语言-动作模型在需要新技能或物体组合的任务上泛化能力差的问题。\n\n论文核心贡献点：\n- 提出了原子动作切片方法，通过三阶段流程将长演示分解为短小的类型化原子动作\n- 从LIBERO演示中构建了包含2124个规划器对齐原子片段的数据集\n- 通过在这些片段上微调VLA策略，提高了任务成功率\n\n论文方法描述：\n原子动作切片方法包含三个阶段：\n1. 发现阶段：使用任务规划器基于指令和环境符号生成有序的原子动作计划\n2. 模式约束的LLM分割：使用多模态视觉语言模型在关键帧上提出边界，同时满足连续性、覆盖范围和标签有效性约束\n3. 验证和置信度分配：通过计数、顺序和持续时间检查验证候选序列，并为接受的步骤分配置信度\n\n论文使用数据集和训练资源：\n- 使用LIBERO操作任务数据集\n- 原始演示：LIBERO-Goal任务434个演示，LIBERO-Long任务391个演示\n- 分割后得到758个原子片段（来自LIBERO-Goal）和1366个原子片段（来自LIBERO-Long），总计2124个原子片段\n- 使用Gemini 2.5 Pro和Gemini 2.5 Flash模型进行分割\n- 使用AutoGPT+P进行规划器引导的发现\n\n论文使用的评估环境和评估指标：\n评估环境：LIBERO基准测试\n评估指标：\n- 序列准确率：预测标签序列与规划器序列的完全匹配\n- 编辑相似度：基于编辑距离的相似度度量\n- 计数/顺序指标：片段数量匹配和标签顺序正确性\n- IoU指标：边界框交并比\n- 平均绝对误差：开始、结束和持续时间的平均绝对误差\n- 稳定性@抖动：在关键帧抖动下的分割稳定性\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<ul><li>索菲亚大学“圣克莱门特奥赫里茨基”</li><li>索菲亚技术大学</li><li>特温特大学EEMCS学院</li><li>GATE研究所，索菲亚大学</li></ul>\n\n<p>论文概述：</p>\n<p>本论文提出了原子动作切片方法，一种规划器对齐的方法，用于将长时程演示分解为短小的、类型化的原子动作，这些动作易于规划器使用且便于策略学习。该方法解决了当前视觉-语言-动作模型在需要新技能或物体组合的任务上泛化能力差的问题。</p>\n\n<p>论文核心贡献点：</p>\n<ul><li>提出了原子动作切片方法，通过三阶段流程将长演示分解为短小的类型化原子动作</li><li>从LIBERO演示中构建了包含2124个规划器对齐原子片段的数据集</li><li>通过在这些片段上微调VLA策略，提高了任务成功率</li></ul>\n\n<p>论文方法描述：</p>\n<p>原子动作切片方法包含三个阶段：</p>\n<ol><li>发现阶段：使用任务规划器基于指令和环境符号生成有序的原子动作计划</li><li>模式约束的LLM分割：使用多模态视觉语言模型在关键帧上提出边界，同时满足连续性、覆盖范围和标签有效性约束</li><li>验证和置信度分配：通过计数、顺序和持续时间检查验证候选序列，并为接受的步骤分配置信度</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>使用LIBERO操作任务数据集</li><li>原始演示：LIBERO-Goal任务434个演示，LIBERO-Long任务391个演示</li><li>分割后得到758个原子片段（来自LIBERO-Goal）和1366个原子片段（来自LIBERO-Long），总计2124个原子片段</li><li>使用Gemini 2.5 Pro和Gemini 2.5 Flash模型进行分割</li><li>使用AutoGPT+P进行规划器引导的发现</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：LIBERO基准测试</p>\n<p>评估指标：</p>\n<ul><li>序列准确率：预测标签序列与规划器序列的完全匹配</li><li>编辑相似度：基于编辑距离的相似度度量</li><li>计数/顺序指标：片段数量匹配和标签顺序正确性</li><li>IoU指标：边界框交并比</li><li>平均绝对误差：开始、结束和持续时间的平均绝对误差</li><li>稳定性@抖动：在关键帧抖动下的分割稳定性</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-12",
    "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
    "link": "http://arxiv.org/abs/2512.11362",
    "summary_markdown": "```markdown\n论文研究单位：\nIROOTECH TECHNOLOGY，King’s College London，Hong Kong Polytechnic University，Technische Universität Darmstadt，University of Agder，Imperial College London\n\n论文概述：\n这篇论文对视觉-语言-动作模型进行了系统性解剖分析，从基础模块到发展里程碑和核心挑战。论文采用金字塔结构组织内容，从基础模块开始，追溯历史演进，深入分析当前研究前沿的五大挑战，旨在为研究人员提供清晰的学习路径和未来研究方向。\n\n论文核心贡献点：\n1. 对VLA领域五大核心挑战进行深度系统分析：多模态对齐与物理世界建模、指令跟随与规划及鲁棒实时执行、从泛化到持续适应、安全性与可解释性及可靠交互、数据构建与基准测试标准\n2. 设计独特的论文结构，模仿研究者的自然学习路径：从基础模块到里程碑再到挑战分析\n3. 提供持续更新的在线资源，保持与前沿研究同步\n\n论文方法描述：\n1. 模块化分析：将VLA系统分解为三大核心模块\n - 感知模块：视觉编码器、语言编码器、本体感受编码器\n - 大脑模块：Transformer、扩散Transformer、混合架构、视觉语言模型\n - 动作模块：动作表示（离散、连续、混合空间）和动作解码（自回归、非自回归、混合解码）\n2. 历史演进追踪：按时间顺序梳理2017-2025年间的关键模型发展\n3. 挑战分类：将核心挑战细分为15个子挑战，涵盖从基础感知到高级推理的各个层面\n\n论文使用数据集和训练资源：\n- Open X-Embodiment数据集\n- 1.5M-EO-Data数据集\n- 大规模多任务/多机器人预训练数据\n- 互联网/人类视频知识迁移\n- 模拟器生成的多模态数据\n\n论文使用的评估环境和评估指标：\n- 评估环境：真实世界部署、高保真模拟器\n- 评估指标：任务成功率、泛化能力、实时性能、安全性指标\n- 基准测试标准：包括长期任务评估、开放式指令理解等标准化评估体系\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>IROOTECH TECHNOLOGY，King’s College London，Hong Kong Polytechnic University，Technische Universität Darmstadt，University of Agder，Imperial College London</p>\n\n<p>论文概述：</p>\n<p>这篇论文对视觉-语言-动作模型进行了系统性解剖分析，从基础模块到发展里程碑和核心挑战。论文采用金字塔结构组织内容，从基础模块开始，追溯历史演进，深入分析当前研究前沿的五大挑战，旨在为研究人员提供清晰的学习路径和未来研究方向。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>对VLA领域五大核心挑战进行深度系统分析：多模态对齐与物理世界建模、指令跟随与规划及鲁棒实时执行、从泛化到持续适应、安全性与可解释性及可靠交互、数据构建与基准测试标准</li><li>设计独特的论文结构，模仿研究者的自然学习路径：从基础模块到里程碑再到挑战分析</li><li>提供持续更新的在线资源，保持与前沿研究同步</li></ol>\n\n<p>论文方法描述：</p>\n<p>1. 模块化分析：将VLA系统分解为三大核心模块</p>\n<p> - 感知模块：视觉编码器、语言编码器、本体感受编码器</p>\n<p> - 大脑模块：Transformer、扩散Transformer、混合架构、视觉语言模型</p>\n<p> - 动作模块：动作表示（离散、连续、混合空间）和动作解码（自回归、非自回归、混合解码）</p>\n<ol><li>历史演进追踪：按时间顺序梳理2017-2025年间的关键模型发展</li><li>挑战分类：将核心挑战细分为15个子挑战，涵盖从基础感知到高级推理的各个层面</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>Open X-Embodiment数据集</li><li>1.5M-EO-Data数据集</li><li>大规模多任务/多机器人预训练数据</li><li>互联网/人类视频知识迁移</li><li>模拟器生成的多模态数据</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>评估环境：真实世界部署、高保真模拟器</li><li>评估指标：任务成功率、泛化能力、实时性能、安全性指标</li><li>基准测试标准：包括长期任务评估、开放式指令理解等标准化评估体系</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-12",
    "title": "Benchmarking the Generality of Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.11315",
    "summary_markdown": "```markdown\n论文研究单位：\nmanifold research, metarch ai, georgia tech, tufts university, northeastern university, birla institute of technology and science pilani, institute for research and innovation in intelligent systems (iriis)\n\n论文概述：\n该论文提出了multinet v1.0基准测试，用于评估视觉-语言-动作模型在六个核心能力领域的跨领域泛化能力，包括视觉基础、空间推理、工具使用、物理常识、多智能体协作和连续机器人控制。通过评估gpt-5、π0和magma等模型，发现当前模型在未见领域、不熟悉模态或跨领域任务转换时表现出显著性能下降，揭示了当前基础模型在实现真正通用智能方面存在的差距。\n\n论文核心贡献点：\n1. 开源模型适配代码，支持异构任务域上不同架构的一致评估\n2. 标准化提交流程，确保可复现的基准测试、结果验证和跨模型可比性\n3. 提供跨感知、推理和控制任务的全面评估结果\n4. 开源sdk，简化从广泛领域和标注格式下载、处理和转换数据集\n\n论文方法描述：\n采用零样本评估设置，针对不同模态使用特定领域指标。离散任务使用分类动作，连续任务使用实值向量，视觉语言任务生成需要解析和验证的文本。对于连续动作预测，使用平均绝对误差和均方误差作为基础指标，并通过相对于基线预测器的模型误差归一化，实现跨数据集比较。对于自由形式的视觉语言任务，使用预测答案和参考答案之间句子嵌入的余弦相似度来测量语义相似性。\n\n论文使用数据集和训练资源：\n数据集包括：open-x embodiment（机器人操作）、overcooked（多智能体协作游戏）、piqa（物理常识推理）、odinw（开放世界物体检测）、sqa3d（3d空间问答）、bfcl v3（对话式函数调用）和robovqa（时空推理和视频理解）。\n\n论文使用的评估环境和评估指标：\n评估环境：统一的跨领域基准测试框架\n评估指标：\n- 离散动作预测：精确匹配率、微平均精度/召回率/f1、宏平均精度/召回率/f1\n- 连续动作预测：基线归一化平均绝对误差、基线归一化均方误差\n- 视觉语言任务：精确匹配率、语义相似度\n- 额外指标：无效预测率、brier mae、分位数过滤的bnamae、最大相对mae\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>manifold research, metarch ai, georgia tech, tufts university, northeastern university, birla institute of technology and science pilani, institute for research and innovation in intelligent systems (iriis)</p>\n\n<p>论文概述：</p>\n<p>该论文提出了multinet v1.0基准测试，用于评估视觉-语言-动作模型在六个核心能力领域的跨领域泛化能力，包括视觉基础、空间推理、工具使用、物理常识、多智能体协作和连续机器人控制。通过评估gpt-5、π0和magma等模型，发现当前模型在未见领域、不熟悉模态或跨领域任务转换时表现出显著性能下降，揭示了当前基础模型在实现真正通用智能方面存在的差距。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>开源模型适配代码，支持异构任务域上不同架构的一致评估</li><li>标准化提交流程，确保可复现的基准测试、结果验证和跨模型可比性</li><li>提供跨感知、推理和控制任务的全面评估结果</li><li>开源sdk，简化从广泛领域和标注格式下载、处理和转换数据集</li></ol>\n\n<p>论文方法描述：</p>\n<p>采用零样本评估设置，针对不同模态使用特定领域指标。离散任务使用分类动作，连续任务使用实值向量，视觉语言任务生成需要解析和验证的文本。对于连续动作预测，使用平均绝对误差和均方误差作为基础指标，并通过相对于基线预测器的模型误差归一化，实现跨数据集比较。对于自由形式的视觉语言任务，使用预测答案和参考答案之间句子嵌入的余弦相似度来测量语义相似性。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>数据集包括：open-x embodiment（机器人操作）、overcooked（多智能体协作游戏）、piqa（物理常识推理）、odinw（开放世界物体检测）、sqa3d（3d空间问答）、bfcl v3（对话式函数调用）和robovqa（时空推理和视频理解）。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：统一的跨领域基准测试框架</p>\n<p>评估指标：</p>\n<ul><li>离散动作预测：精确匹配率、微平均精度/召回率/f1、宏平均精度/召回率/f1</li><li>连续动作预测：基线归一化平均绝对误差、基线归一化均方误差</li><li>视觉语言任务：精确匹配率、语义相似度</li><li>额外指标：无效预测率、brier mae、分位数过滤的bnamae、最大相对mae</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-12",
    "title": "Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy",
    "link": "http://arxiv.org/abs/2512.11218",
    "summary_markdown": "论文研究单位\n- 浙江大学\n- 加州大学伯克利分校\n\n论文概述\n论文提出bayesvla，一种贝叶斯因子分解的视觉语言动作策略，用于解决vla模型在微调过程中因模态不平衡导致的灾难性遗忘问题。该方法将策略分解为视觉动作先验和语言条件似然，实现从视觉到动作的生成和通过语言提示指定动作。该方法无需依赖外部推理数据，通过结构化的方式处理数据不平衡，提升指令跟随的泛化能力。\n\n论文核心贡献点\n- 提出贝叶斯因子分解框架，从结构上处理vla训练中的模态不平衡问题\n- 设计包含专门预接触和后接触策略的bayesvla架构\n- 开发建模似然和对齐先验的新技术\n- 通过信息论分析验证因子分解的有效性\n- 在仿真和真实场景中全面评估，展示优越的泛化性能\n\n论文方法描述\n- 将vla策略分解为视觉动作先验和语言条件似然\n- 视觉动作先验学习基础视觉运动控制，生成多模态动作分布\n- 语言条件似然基于先验生成的动作提案，通过语言指令进行对齐\n- 预接触阶段使用预训练的动作基础模型作为先验，生成接触姿态\n- 后接触阶段使用扩散模型作为先验，生成密集轨迹\n- 采用两阶段训练流程，先训练先验模型，再训练似然模型\n- 引入文本感知对齐和潜在适应技术实现语言对齐\n\n论文使用数据集和训练资源\n- 使用libero和libero-pro基准数据集，包含10个任务共500条轨迹\n- 自建刚体物体拾取放置和关节物体操作仿真环境\n- 预训练使用droid数据集，包含78544条轨迹\n- 训练使用adamw优化器，学习率1e-4，权重衰减1e-2\n- 批量大小为32，使用混合精度训练\n- 先验模型训练40万次迭代，似然模型训练20万次迭代\n\n论文使用的评估环境和评估指标\n- 在libero和libero-pro基准上评估，包括原始设置和五个扰动维度\n- 在自建刚体物体拾取放置环境中测试四个设置\n- 在关节物体操作环境中测试两个已见任务和一个未见任务\n- 评估指标为任务成功率，测试模型在未见指令，物体和环境下的零样本泛化性能",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>浙江大学</li><li>加州大学伯克利分校</li></ul>\n\n<p>论文概述</p>\n<p>论文提出bayesvla，一种贝叶斯因子分解的视觉语言动作策略，用于解决vla模型在微调过程中因模态不平衡导致的灾难性遗忘问题。该方法将策略分解为视觉动作先验和语言条件似然，实现从视觉到动作的生成和通过语言提示指定动作。该方法无需依赖外部推理数据，通过结构化的方式处理数据不平衡，提升指令跟随的泛化能力。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出贝叶斯因子分解框架，从结构上处理vla训练中的模态不平衡问题</li><li>设计包含专门预接触和后接触策略的bayesvla架构</li><li>开发建模似然和对齐先验的新技术</li><li>通过信息论分析验证因子分解的有效性</li><li>在仿真和真实场景中全面评估，展示优越的泛化性能</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>将vla策略分解为视觉动作先验和语言条件似然</li><li>视觉动作先验学习基础视觉运动控制，生成多模态动作分布</li><li>语言条件似然基于先验生成的动作提案，通过语言指令进行对齐</li><li>预接触阶段使用预训练的动作基础模型作为先验，生成接触姿态</li><li>后接触阶段使用扩散模型作为先验，生成密集轨迹</li><li>采用两阶段训练流程，先训练先验模型，再训练似然模型</li><li>引入文本感知对齐和潜在适应技术实现语言对齐</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>使用libero和libero-pro基准数据集，包含10个任务共500条轨迹</li><li>自建刚体物体拾取放置和关节物体操作仿真环境</li><li>预训练使用droid数据集，包含78544条轨迹</li><li>训练使用adamw优化器，学习率1e-4，权重衰减1e-2</li><li>批量大小为32，使用混合精度训练</li><li>先验模型训练40万次迭代，似然模型训练20万次迭代</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>在libero和libero-pro基准上评估，包括原始设置和五个扰动维度</li><li>在自建刚体物体拾取放置环境中测试四个设置</li><li>在关节物体操作环境中测试两个已见任务和一个未见任务</li><li>评估指标为任务成功率，测试模型在未见指令，物体和环境下的零样本泛化性能</li></ul>"
  },
  {
    "date": "2025-12-11",
    "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control",
    "link": "http://arxiv.org/abs/2512.11047",
    "summary_markdown": "论文研究单位：复旦大学，香港大学OpenDriveLab和MMLab，AgiBot，SII\n\n论文概述：论文提出wholebodyvla，一个用于人形机器人全身运动-操作控制的统一视觉-语言-动作框架。该框架旨在解决运动-操作任务中的数据稀缺问题和执行精度不足问题，实现大空间内的端到端全身运动-操作。\n\n论文核心贡献点：提出wholebodyvla框架，实现人形机器人在真实世界中的大空间运动-操作；提出统一潜在学习方法，从低成本无动作视频中学习运动-操作知识；提出面向运动-操作的强化学习策略，通过离散命令接口提高执行精度和稳定性。\n\n论文方法描述：采用两个独立的潜在动作模型分别处理操作和运动数据，通过vq-vae架构编码帧间变化为离散潜在动作；使用轻量级解码器将潜在动作转换为机器人可执行命令；设计两阶段训练课程，先获取基本步态，再针对运动-操作任务进行精度和稳定性优化。\n\n论文使用数据集和训练资源：使用自收集的以自我为中心的运动感知运动视频和agibot world操作数据集进行预训练；使用agibot x2人形机器人的遥操作轨迹进行微调；训练资源包括仿真环境和真实机器人硬件平台。\n\n论文使用的评估环境和评估指标：评估环境为agibot x2人形机器人真实世界场景；评估指标包括任务成功率、运动精度（位置和方向误差）、操作稳定性（质心摆动幅度）。",
    "summary_html": "<p>论文研究单位：复旦大学，香港大学OpenDriveLab和MMLab，AgiBot，SII</p>\n\n<p>论文概述：论文提出wholebodyvla，一个用于人形机器人全身运动-操作控制的统一视觉-语言-动作框架。该框架旨在解决运动-操作任务中的数据稀缺问题和执行精度不足问题，实现大空间内的端到端全身运动-操作。</p>\n\n<p>论文核心贡献点：提出wholebodyvla框架，实现人形机器人在真实世界中的大空间运动-操作；提出统一潜在学习方法，从低成本无动作视频中学习运动-操作知识；提出面向运动-操作的强化学习策略，通过离散命令接口提高执行精度和稳定性。</p>\n\n<p>论文方法描述：采用两个独立的潜在动作模型分别处理操作和运动数据，通过vq-vae架构编码帧间变化为离散潜在动作；使用轻量级解码器将潜在动作转换为机器人可执行命令；设计两阶段训练课程，先获取基本步态，再针对运动-操作任务进行精度和稳定性优化。</p>\n\n<p>论文使用数据集和训练资源：使用自收集的以自我为中心的运动感知运动视频和agibot world操作数据集进行预训练；使用agibot x2人形机器人的遥操作轨迹进行微调；训练资源包括仿真环境和真实机器人硬件平台。</p>\n\n<p>论文使用的评估环境和评估指标：评估环境为agibot x2人形机器人真实世界场景；评估指标包括任务成功率、运动精度（位置和方向误差）、操作稳定性（质心摆动幅度）。</p>"
  },
  {
    "date": "2025-12-11",
    "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control",
    "link": "http://arxiv.org/abs/2512.11921",
    "summary_markdown": "论文研究单位：独立研究人员（沙特阿拉伯利雅得），QSS AI和机器人实验室\n\n论文概述：本研究针对在低成本机器人平台上部署视觉语言动作（VLA）模型的两个主要挑战：计算资源限制和新机器人本体的适应。论文提出了一种基于低秩适应（LoRA）和量化技术的高效微调方法，使得31亿参数的VLA模型能够在8GB显存的消费级GPU上运行。通过在实际的SO101机器人手臂上进行按钮按压操作任务的部署分析，证明该方法在保持计算效率的同时实现了有效的操作性能。\n\n论文核心贡献点：提出资源高效的微调策略，结合LoRA和4位量化，显著降低内存需求；提供详细的真实世界部署分析，包括部署挑战、失败模式以及训练数据量与性能关系的研究。\n\n论文方法描述：基于SmolVLA模型架构，使用SigLIP-SO400M视觉编码器和Phi-2语言模型；应用LoRA（秩=8）到语言模型的注意力层，减少可训练参数；采用4位量化（NF4）进一步降低内存使用；系统比较冻结和未冻结视觉编码器的策略；使用动作分块（50步预测）提高时间一致性；部署框架包含实时推理管道、动作空间适应和安全机制。\n\n论文使用数据集和训练资源：收集不同规模的演示数据集（20、50、200个片段），共59，440帧；使用LeRobot v3.0数据集格式；训练在NVIDIA RTX 4060 8GB GPU上进行，通常需要10-20 GPU小时。\n\n论文使用的评估环境和评估指标：在物理SO101机器人手臂上评估，配备双摄像头视觉系统；评估指标包括训练损失、验证损失、视觉影响力（Δ_vision）、部署成功率（74-76%）、推理延迟（平均45ms）、控制频率（20Hz）和内存使用情况（峰值6.8GB）。",
    "summary_html": "<p>论文研究单位：独立研究人员（沙特阿拉伯利雅得），QSS AI和机器人实验室</p>\n\n<p>论文概述：本研究针对在低成本机器人平台上部署视觉语言动作（VLA）模型的两个主要挑战：计算资源限制和新机器人本体的适应。论文提出了一种基于低秩适应（LoRA）和量化技术的高效微调方法，使得31亿参数的VLA模型能够在8GB显存的消费级GPU上运行。通过在实际的SO101机器人手臂上进行按钮按压操作任务的部署分析，证明该方法在保持计算效率的同时实现了有效的操作性能。</p>\n\n<p>论文核心贡献点：提出资源高效的微调策略，结合LoRA和4位量化，显著降低内存需求；提供详细的真实世界部署分析，包括部署挑战、失败模式以及训练数据量与性能关系的研究。</p>\n\n<p>论文方法描述：基于SmolVLA模型架构，使用SigLIP-SO400M视觉编码器和Phi-2语言模型；应用LoRA（秩=8）到语言模型的注意力层，减少可训练参数；采用4位量化（NF4）进一步降低内存使用；系统比较冻结和未冻结视觉编码器的策略；使用动作分块（50步预测）提高时间一致性；部署框架包含实时推理管道、动作空间适应和安全机制。</p>\n\n<p>论文使用数据集和训练资源：收集不同规模的演示数据集（20、50、200个片段），共59，440帧；使用LeRobot v3.0数据集格式；训练在NVIDIA RTX 4060 8GB GPU上进行，通常需要10-20 GPU小时。</p>\n\n<p>论文使用的评估环境和评估指标：在物理SO101机器人手臂上评估，配备双摄像头视觉系统；评估指标包括训练损失、验证损失、视觉影响力（Δ_vision）、部署成功率（74-76%）、推理延迟（平均45ms）、控制频率（20Hz）和内存使用情况（峰值6.8GB）。</p>"
  },
  {
    "date": "2025-12-10",
    "title": "Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models",
    "link": "http://arxiv.org/abs/2512.11908",
    "summary_markdown": "论文研究单位：意大利理工学院人机接口与交互实验室、热那亚大学机器人与智能机器国家博士项目、普渡大学爱德华森工业工程学院\n\n论文概述：本论文是关于接触密集型机器人任务安全学习的综述，涵盖从经典学习方法到安全基础模型的演进。论文系统回顾了在物理交互环境中确保机器人安全学习的方法，重点关注安全探索和安全执行两大领域，并特别探讨了视觉语言模型和视觉语言动作模型等基础模型带来的安全机遇与挑战。\n\n论文核心贡献点：\n1. 提出以安全为中心的分类法，从学习阶段、安全集成层次和感知模态等多个维度组织现有方法。\n2. 将安全学习置于接触密集型机器人任务背景下分析，详细说明安全约束如何嵌入具体任务。\n3. 识别当前研究的空白、挑战和未来方向，特别强调与大型机器人基础模型集成的挑战与机遇。\n\n论文方法描述：论文将方法分为安全探索和安全执行两大类。安全探索方法包括控制屏障函数、模型预测屏蔽、可达性轨迹保障等，旨在最小化学习阶段的不安全行为风险。安全执行方法涵盖阻抗控制、导纳控制、强化学习参数调优、模型安全过滤器和混合学习控制架构。特别关注基础模型方法，如计划-参数化-执行框架、约束视觉语言动作模型和语义到物理约束的落地。\n\n论文使用数据集和训练资源：论文未明确指定具体数据集，但提及使用仿真环境如Gymnasium和Stable-Baselines3进行训练和评估。\n\n论文使用的评估环境和评估指标：评估环境包括标准化的仿真套件和基准测试。评估指标包括安全违规率、近失事件统计、泛化性能、任务效率与安全目标的权衡、接触力/力矩约束满足度等。强调需要量化违规率、拒绝/恢复行为以及在分布偏移下的泛化能力。",
    "summary_html": "<p>论文研究单位：意大利理工学院人机接口与交互实验室、热那亚大学机器人与智能机器国家博士项目、普渡大学爱德华森工业工程学院</p>\n\n<p>论文概述：本论文是关于接触密集型机器人任务安全学习的综述，涵盖从经典学习方法到安全基础模型的演进。论文系统回顾了在物理交互环境中确保机器人安全学习的方法，重点关注安全探索和安全执行两大领域，并特别探讨了视觉语言模型和视觉语言动作模型等基础模型带来的安全机遇与挑战。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出以安全为中心的分类法，从学习阶段、安全集成层次和感知模态等多个维度组织现有方法。</li><li>将安全学习置于接触密集型机器人任务背景下分析，详细说明安全约束如何嵌入具体任务。</li><li>识别当前研究的空白、挑战和未来方向，特别强调与大型机器人基础模型集成的挑战与机遇。</li></ol>\n\n<p>论文方法描述：论文将方法分为安全探索和安全执行两大类。安全探索方法包括控制屏障函数、模型预测屏蔽、可达性轨迹保障等，旨在最小化学习阶段的不安全行为风险。安全执行方法涵盖阻抗控制、导纳控制、强化学习参数调优、模型安全过滤器和混合学习控制架构。特别关注基础模型方法，如计划-参数化-执行框架、约束视觉语言动作模型和语义到物理约束的落地。</p>\n\n<p>论文使用数据集和训练资源：论文未明确指定具体数据集，但提及使用仿真环境如Gymnasium和Stable-Baselines3进行训练和评估。</p>\n\n<p>论文使用的评估环境和评估指标：评估环境包括标准化的仿真套件和基准测试。评估指标包括安全违规率、近失事件统计、泛化性能、任务效率与安全目标的权衡、接触力/力矩约束满足度等。强调需要量化违规率、拒绝/恢复行为以及在分布偏移下的泛化能力。</p>"
  },
  {
    "date": "2025-12-11",
    "title": "RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI",
    "link": "http://arxiv.org/abs/2512.10394",
    "summary_markdown": "```markdown\n论文研究单位：\n中国科学院自动化研究所，中国科学院大学，AiRiA，MICRO\n\n论文概述：\nRoboNeuron是一个用于具身智能的通用部署框架，旨在解决当前具身AI系统面临的工程障碍，包括跨场景适应性差、模块间耦合刚性以及推理加速碎片化等问题。该框架首次将大语言模型和视觉语言动作模型的认知能力与机器人操作系统的实时执行骨干进行深度集成。\n\n论文核心贡献点：\n1. 提出首个将LLM驱动的ROS控制深度集成的框架，通过模型上下文协议和自动ROS消息翻译器实现无缝、类型安全的控制\n2. 建立模块化解耦架构，通过ROS的统一通信机制严格分离感知、规划和控制模块\n3. 系统化集成VLA推理引擎和加速算法，提供统一基准测试平台\n\n论文方法描述：\nRoboNeuron采用分层认知执行架构，包含三个核心部分：\n- 认知核心与语义桥：大语言模型作为认知协调器，使用模型上下文协议作为类型安全的语义桥，自动将ROS消息转换为可调用的模型上下文协议工具\n- 解耦执行中间件：机器人操作系统作为实时执行中间件，根据任务复杂度采用双路径执行策略（简单路径用于低延迟命令，复杂路径用于VLA驱动的感知-动作循环）\n- 抽象与部署层：通过包装器策略对核心功能进行抽象，适配器将标准化ROS消息转换为平台特定协议\n\n核心模块包括：\n- ROS感知：统一传感器抽象层和动态流调度\n- ROS控制：动态逆运动学求解器和跨平台适配\n- 消息翻译器：通过ROS2MCP自动翻译器实现语义桥接\n- VLA推理：解耦模型加载和加速基准测试\n\n论文使用数据集和训练资源：\n未在提供的论文内容中明确提及具体使用的数据集和训练资源细节\n\n论文使用的评估环境和评估指标：\n评估环境：\n- 基于NVIDIA Isaac Sim的仿真环境，用于多机器人和运动学实验\n- 物理测试平台：使用Franka Emika Research 3机械臂和Intel RealSense D435i相机\n\n评估案例：\n1. 异构车辆统一控制：验证自动ROS到模型上下文协议工具生成和通用接口抽象\n2. 运动学感知操作：演示动态URDF集成、运行时逆运动学求解\n- 真实世界VLA驱动抓取：在硬件上执行完整的感知-推理-控制闭环操作\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>中国科学院自动化研究所，中国科学院大学，AiRiA，MICRO</p>\n\n<p>论文概述：</p>\n<p>RoboNeuron是一个用于具身智能的通用部署框架，旨在解决当前具身AI系统面临的工程障碍，包括跨场景适应性差、模块间耦合刚性以及推理加速碎片化等问题。该框架首次将大语言模型和视觉语言动作模型的认知能力与机器人操作系统的实时执行骨干进行深度集成。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出首个将LLM驱动的ROS控制深度集成的框架，通过模型上下文协议和自动ROS消息翻译器实现无缝、类型安全的控制</li><li>建立模块化解耦架构，通过ROS的统一通信机制严格分离感知、规划和控制模块</li><li>系统化集成VLA推理引擎和加速算法，提供统一基准测试平台</li></ol>\n\n<p>论文方法描述：</p>\n<p>RoboNeuron采用分层认知执行架构，包含三个核心部分：</p>\n<ul><li>认知核心与语义桥：大语言模型作为认知协调器，使用模型上下文协议作为类型安全的语义桥，自动将ROS消息转换为可调用的模型上下文协议工具</li><li>解耦执行中间件：机器人操作系统作为实时执行中间件，根据任务复杂度采用双路径执行策略（简单路径用于低延迟命令，复杂路径用于VLA驱动的感知-动作循环）</li><li>抽象与部署层：通过包装器策略对核心功能进行抽象，适配器将标准化ROS消息转换为平台特定协议</li></ul>\n\n<p>核心模块包括：</p>\n<ul><li>ROS感知：统一传感器抽象层和动态流调度</li><li>ROS控制：动态逆运动学求解器和跨平台适配</li><li>消息翻译器：通过ROS2MCP自动翻译器实现语义桥接</li><li>VLA推理：解耦模型加载和加速基准测试</li></ul>\n\n<p>论文使用数据集和训练资源：</p>\n<p>未在提供的论文内容中明确提及具体使用的数据集和训练资源细节</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<ul><li>基于NVIDIA Isaac Sim的仿真环境，用于多机器人和运动学实验</li><li>物理测试平台：使用Franka Emika Research 3机械臂和Intel RealSense D435i相机</li></ul>\n\n<p>评估案例：</p>\n<ol><li>异构车辆统一控制：验证自动ROS到模型上下文协议工具生成和通用接口抽象</li><li>运动学感知操作：演示动态URDF集成、运行时逆运动学求解</li></ol>\n<ul><li>真实世界VLA驱动抓取：在硬件上执行完整的感知-推理-控制闭环操作</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-11",
    "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
    "link": "http://arxiv.org/abs/2512.10226",
    "summary_markdown": "```markdown\n论文研究单位：\n德克萨斯大学奥斯汀分校，英伟达，斯坦福大学\n\n论文概述：\n本文提出latent chain of thought world modeling for end to end autonomous driving，一种用于端到端自动驾驶的潜在思维链世界建模方法。该方法使用潜在推理令牌替代自然语言进行推理，通过交替生成动作提议令牌和潜在世界模型预测令牌，模拟驾驶动作的潜在后果以指导最终决策。\n\n论文核心贡献点：\n1. 提出latent cot框架，在潜在空间中进行推理，与驾驶动作共享词汇表，实现更高效和对齐的推理过程。\n2. 设计包含潜在cot冷启动、世界模型训练和闭环强化学习的三阶段训练策略。\n3. 在大型端到端驾驶基准上验证方法，相比非推理和文本推理基线实现更快推理速度、更好轨迹质量和更大强化学习改进。\n\n论文方法描述：\n1. 潜在思维链推理：使用动作提议令牌和潜在世界模型令牌交替组成推理轨迹，模拟多智能体交互的未来结果。\n2. 使用轻量级潜在世界模型编码器将周围智能体状态压缩为紧凑令牌。\n3. 多分支推理：生成多个推理分支以探索不同策略。\n4. 三阶段训练：阶段0为非推理预训练；阶段1为cot冷启动，通过教师强制监督动作提议和世界模型令牌；阶段2使用grpo算法进行强化学习，通过轨迹级奖励优化推理能力。\n\n论文使用数据集和训练资源：\n1. 数据集：physicalai av数据集，包含1727小时真实世界多摄像头驾驶日志，具有精确自车轨迹和密集多智能体标注。\n2. 训练资源：使用qwen3 0.5b llm作为语言动作模块，dinov2 vit作为图像编码器。训练使用batch size 128，学习率4e 5，在8个a100 gpu上训练。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：physicalai av验证集，包含23，758个验证片段，涵盖14种特定场景。\n2. 评估指标：平均位移误差，偏离道路率，碰撞率，角点距离。所有指标越低越好。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>德克萨斯大学奥斯汀分校，英伟达，斯坦福大学</p>\n\n<p>论文概述：</p>\n<p>本文提出latent chain of thought world modeling for end to end autonomous driving，一种用于端到端自动驾驶的潜在思维链世界建模方法。该方法使用潜在推理令牌替代自然语言进行推理，通过交替生成动作提议令牌和潜在世界模型预测令牌，模拟驾驶动作的潜在后果以指导最终决策。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出latent cot框架，在潜在空间中进行推理，与驾驶动作共享词汇表，实现更高效和对齐的推理过程。</li><li>设计包含潜在cot冷启动、世界模型训练和闭环强化学习的三阶段训练策略。</li><li>在大型端到端驾驶基准上验证方法，相比非推理和文本推理基线实现更快推理速度、更好轨迹质量和更大强化学习改进。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>潜在思维链推理：使用动作提议令牌和潜在世界模型令牌交替组成推理轨迹，模拟多智能体交互的未来结果。</li><li>使用轻量级潜在世界模型编码器将周围智能体状态压缩为紧凑令牌。</li><li>多分支推理：生成多个推理分支以探索不同策略。</li><li>三阶段训练：阶段0为非推理预训练；阶段1为cot冷启动，通过教师强制监督动作提议和世界模型令牌；阶段2使用grpo算法进行强化学习，通过轨迹级奖励优化推理能力。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>数据集：physicalai av数据集，包含1727小时真实世界多摄像头驾驶日志，具有精确自车轨迹和密集多智能体标注。</li><li>训练资源：使用qwen3 0.5b llm作为语言动作模块，dinov2 vit作为图像编码器。训练使用batch size 128，学习率4e 5，在8个a100 gpu上训练。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：physicalai av验证集，包含23，758个验证片段，涵盖14种特定场景。</li><li>评估指标：平均位移误差，偏离道路率，碰撞率，角点距离。所有指标越低越好。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-10",
    "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.09928",
    "summary_markdown": "论文研究单位：西湖大学，浙江大学，香港科技大学（广州），南京大学，西湖机器人\n\n论文概述：本论文提出HiF-VLA，一个通过运动表示实现后见、洞见和先见的视觉-语言-动作模型框架。该框架旨在解决传统VLA模型因马尔可夫假设导致的时序短视问题，通过双向时序推理提升长时程操作任务的连贯性。HiF-VLA将运动作为紧凑且信息丰富的时序上下文表示，捕捉状态间变化同时过滤静态像素级噪声。\n\n论文核心贡献点：\n1. 提出HiF-VLA框架，通过引入结构化低维运动原语作为时序原语，显式扩展时序感受野，实现时序一致且高效的动作预测。\n2. 提出后见调制联合专家，在统一空间中统一时序和动作表示，实现因果一致和时序连贯的长时程运动生成的“边思考边行动”范式。\n3. 在广泛采用的长时程基准测试中取得显著性能提升，同时展现强时序可扩展性和高推理效率。\n\n论文方法描述：\nHiF-VLA包含三个核心组件：\n1. 后见先验获取：通过MPEG-4标准提取运动向量，将密集历史帧序列编码为紧凑的运动向量流，形成结构化后见原语。\n2. 基于洞见的先见推理：引入可学习的先见查询令牌和空动作令牌，使VLM能够并行推理连续视觉动态和动作生成。\n3. 后见调制联合专家：将后见、先见和动作表示在共享时序潜在空间中联合建模，通过自适应层归一化调节条件约束未来运动-动作模式。\n\n论文使用数据集和训练资源：\n使用LIBERO-Long和CALVIN ABC-D基准测试进行验证。训练使用8个NVIDIA A100 GPU，全局批次大小为64。在LIBERO上微调150k步，在CALVIN上微调80k步。采用Prismatic-7B作为VLM骨干，并使用OpenVLA预训练权重初始化。\n\n论文使用的评估环境和评估指标：\n评估环境包括仿真环境（LIBERO-Long和CALVIN ABC-D）和真实世界机器人平台（AgileX Piper机械臂）。评估指标包括平均成功率、推理延迟、GPU内存使用量和平均任务长度。在真实世界实验中，每个任务进行20次试验评估性能。",
    "summary_html": "<p>论文研究单位：西湖大学，浙江大学，香港科技大学（广州），南京大学，西湖机器人</p>\n\n<p>论文概述：本论文提出HiF-VLA，一个通过运动表示实现后见、洞见和先见的视觉-语言-动作模型框架。该框架旨在解决传统VLA模型因马尔可夫假设导致的时序短视问题，通过双向时序推理提升长时程操作任务的连贯性。HiF-VLA将运动作为紧凑且信息丰富的时序上下文表示，捕捉状态间变化同时过滤静态像素级噪声。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出HiF-VLA框架，通过引入结构化低维运动原语作为时序原语，显式扩展时序感受野，实现时序一致且高效的动作预测。</li><li>提出后见调制联合专家，在统一空间中统一时序和动作表示，实现因果一致和时序连贯的长时程运动生成的“边思考边行动”范式。</li><li>在广泛采用的长时程基准测试中取得显著性能提升，同时展现强时序可扩展性和高推理效率。</li></ol>\n\n<p>论文方法描述：</p>\n<p>HiF-VLA包含三个核心组件：</p>\n<ol><li>后见先验获取：通过MPEG-4标准提取运动向量，将密集历史帧序列编码为紧凑的运动向量流，形成结构化后见原语。</li><li>基于洞见的先见推理：引入可学习的先见查询令牌和空动作令牌，使VLM能够并行推理连续视觉动态和动作生成。</li><li>后见调制联合专家：将后见、先见和动作表示在共享时序潜在空间中联合建模，通过自适应层归一化调节条件约束未来运动-动作模式。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<p>使用LIBERO-Long和CALVIN ABC-D基准测试进行验证。训练使用8个NVIDIA A100 GPU，全局批次大小为64。在LIBERO上微调150k步，在CALVIN上微调80k步。采用Prismatic-7B作为VLM骨干，并使用OpenVLA预训练权重初始化。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境包括仿真环境（LIBERO-Long和CALVIN ABC-D）和真实世界机器人平台（AgileX Piper机械臂）。评估指标包括平均成功率、推理延迟、GPU内存使用量和平均任务长度。在真实世界实验中，每个任务进行20次试验评估性能。</p>"
  },
  {
    "date": "2025-12-10",
    "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.09927",
    "summary_markdown": "- 论文研究单位：哈马德·本·哈利法大学科学与工程学院，穆罕默德·本·扎耶德人工智能大学，浙江大学计算机科学与技术学院。\n\n- 论文概述：该论文提出TEAM-VLA，一种无需训练即可实现视觉-语言-动作模型（VLA）推理加速的令牌压缩框架。该方法通过动态令牌扩展和选择性合并机制，在保持任务性能的同时显著降低计算开销，适用于实时机器人控制场景。\n\n- 论文核心贡献点：提出完全无需训练的令牌压缩框架，无需额外监督或历史帧缓存；设计结合令牌扩展和合并的双阶段压缩流程；在LIBERO基准测试中实现卓越的加速效果与任务成功率平衡。\n\n- 论文方法描述：方法分为两个阶段：在语言模型主干之前执行令牌剪枝，通过相似性采样和空间扩展模块识别任务相关区域；在主干中间层引入动作引导的软二分匹配机制，将冗余令牌合并到语义相关的源令牌中。令牌扩展使用卷积核计算密度特征图，通过阈值控制进行确定性扩张和随机扩展。令牌合并阶段保留前M个动作相关令牌，其余令牌通过加权平均进行语义聚合。\n\n- 论文使用数据集和训练资源：使用LIBERO基准测试，包含空间推理、物体识别、目标泛化和长时程任务四个子集。实验在单个NVIDIA A100-40GB GPU上完成，基于OpenVLA-OFT代码库实现。\n\n- 论文使用的评估环境和评估指标：评估环境为CUDA平台。评估指标包括任务成功率、浮点运算次数（FLOPs）和CUDA延迟时间。",
    "summary_html": "<ul><li>论文研究单位：哈马德·本·哈利法大学科学与工程学院，穆罕默德·本·扎耶德人工智能大学，浙江大学计算机科学与技术学院。</li></ul>\n\n<ul><li>论文概述：该论文提出TEAM-VLA，一种无需训练即可实现视觉-语言-动作模型（VLA）推理加速的令牌压缩框架。该方法通过动态令牌扩展和选择性合并机制，在保持任务性能的同时显著降低计算开销，适用于实时机器人控制场景。</li></ul>\n\n<ul><li>论文核心贡献点：提出完全无需训练的令牌压缩框架，无需额外监督或历史帧缓存；设计结合令牌扩展和合并的双阶段压缩流程；在LIBERO基准测试中实现卓越的加速效果与任务成功率平衡。</li></ul>\n\n<ul><li>论文方法描述：方法分为两个阶段：在语言模型主干之前执行令牌剪枝，通过相似性采样和空间扩展模块识别任务相关区域；在主干中间层引入动作引导的软二分匹配机制，将冗余令牌合并到语义相关的源令牌中。令牌扩展使用卷积核计算密度特征图，通过阈值控制进行确定性扩张和随机扩展。令牌合并阶段保留前M个动作相关令牌，其余令牌通过加权平均进行语义聚合。</li></ul>\n\n<ul><li>论文使用数据集和训练资源：使用LIBERO基准测试，包含空间推理、物体识别、目标泛化和长时程任务四个子集。实验在单个NVIDIA A100-40GB GPU上完成，基于OpenVLA-OFT代码库实现。</li></ul>\n\n<ul><li>论文使用的评估环境和评估指标：评估环境为CUDA平台。评估指标包括任务成功率、浮点运算次数（FLOPs）和CUDA延迟时间。</li></ul>"
  },
  {
    "date": "2025-12-10",
    "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
    "link": "http://arxiv.org/abs/2512.09864",
    "summary_markdown": "论文研究单位：字节跳动Seed团队\n\n论文概述：该论文提出UniUGP，一个统一的端到端自动驾驶框架，将场景理解、未来视频生成和轨迹规划三个核心能力集成在一个模型中。该模型旨在解决自动驾驶在长尾场景中的挑战，通过结合预训练视觉语言模型和视频生成模型的优势，增强跨模态因果推理能力，从而提升规划性能。\n\n论文核心贡献点：\n1. 构建了多个针对自动驾驶的专用数据集，提供复杂场景下的解释、推理和规划标注\n2. 提出统一的理解-生成-规划框架，采用混合专家架构\n3. 设计了四阶段训练策略，利用多样化的自动驾驶数据集实现理解、生成和规划能力的相互增强\n\n论文方法描述：UniUGP采用混合专家架构，包含三个专家模块：理解专家、规划专家和生成专家。理解专家和规划专家构成混合变换器架构，使用Qwen2.5-VL作为骨干模型。规划专家采用流匹配过程建模动作块，通过反转噪声添加过程生成连续轨迹。生成专家以串行方式与前两个专家交互，通过流匹配过程生成未来视频。模型采用多阶段训练策略，逐步构建基础场景理解、视觉动态建模、文本推理和多能力融合。\n\n论文使用数据集和训练资源：使用超过10个自动驾驶数据集，包括Waymo-E2E、DADA2000、Lost and Found、StreetHazards、SOM、AADV、nuScenes、NuPlan、Lyft和Cosmos等。训练使用8个节点，每个节点8个GPU（80GB）。\n\n论文使用的评估环境和评估指标：评估包括理解能力（小物体识别、事故主体关系、事故预测）、CoT推理能力（GPT评分和BLEU分数）、规划能力（L2距离）和指令跟随能力（L2距离）。具体指标包括准确率、GPT评分、BLEU分数、L2位移误差和碰撞率。视频生成质量使用Fréchet Inception Distance（FID）评估。",
    "summary_html": "<p>论文研究单位：字节跳动Seed团队</p>\n\n<p>论文概述：该论文提出UniUGP，一个统一的端到端自动驾驶框架，将场景理解、未来视频生成和轨迹规划三个核心能力集成在一个模型中。该模型旨在解决自动驾驶在长尾场景中的挑战，通过结合预训练视觉语言模型和视频生成模型的优势，增强跨模态因果推理能力，从而提升规划性能。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>构建了多个针对自动驾驶的专用数据集，提供复杂场景下的解释、推理和规划标注</li><li>提出统一的理解-生成-规划框架，采用混合专家架构</li><li>设计了四阶段训练策略，利用多样化的自动驾驶数据集实现理解、生成和规划能力的相互增强</li></ol>\n\n<p>论文方法描述：UniUGP采用混合专家架构，包含三个专家模块：理解专家、规划专家和生成专家。理解专家和规划专家构成混合变换器架构，使用Qwen2.5-VL作为骨干模型。规划专家采用流匹配过程建模动作块，通过反转噪声添加过程生成连续轨迹。生成专家以串行方式与前两个专家交互，通过流匹配过程生成未来视频。模型采用多阶段训练策略，逐步构建基础场景理解、视觉动态建模、文本推理和多能力融合。</p>\n\n<p>论文使用数据集和训练资源：使用超过10个自动驾驶数据集，包括Waymo-E2E、DADA2000、Lost and Found、StreetHazards、SOM、AADV、nuScenes、NuPlan、Lyft和Cosmos等。训练使用8个节点，每个节点8个GPU（80GB）。</p>\n\n<p>论文使用的评估环境和评估指标：评估包括理解能力（小物体识别、事故主体关系、事故预测）、CoT推理能力（GPT评分和BLEU分数）、规划能力（L2距离）和指令跟随能力（L2距离）。具体指标包括准确率、GPT评分、BLEU分数、L2位移误差和碰撞率。视频生成质量使用Fréchet Inception Distance（FID）评估。</p>"
  },
  {
    "date": "2025-12-10",
    "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.09619",
    "summary_markdown": "```markdown\n论文研究单位：\nMBZUAI（阿布扎比，阿联酋），伊利诺伊大学芝加哥分校（美国）\n\n论文概述：\n该论文提出了一种几何感知的视觉-语言-动作模型框架GLaD，旨在解决现有VLA模型因依赖2D视觉编码器而缺乏3D几何理解的问题。通过知识蒸馏将3D几何先验融入预训练过程，增强模型的空间推理和策略泛化能力，而无需深度传感器或显式3D标注。\n\n论文核心贡献点：\n1. 识别当前VLA架构因依赖CLIP、SigLIP等2D视觉编码器而缺乏几何理解的局限性。\n2. 提出GLaD框架，通过知识蒸馏将VGGT的3D几何特征与LLM隐藏状态对齐，使几何理解深度融入驱动动作预测的多模态表示。\n3. 在LIBERO基准测试中达到94.1%平均成功率，超越UniVLA（92.5%）；在LIBERO-PRO鲁棒性基准上，对视觉外观变化展现出显著改进的抵抗力。\n\n论文方法描述：\n1. 采用VGGT作为冻结的教师网络，提取3D几何特征（深度图、点云、相机参数）。\n2. 设计特征对齐网络，通过两层MLP将LLM隐藏状态投影到VGGT特征空间。\n3. 训练目标结合动作预测损失（交叉熵）和几何对齐损失（MSE），通过超参数λ平衡两者。\n4. 训练策略分为两阶段：第一阶段在Bridge数据集上进行带几何蒸馏的预训练；第二阶段在下游任务（如LIBERO）上进行后训练，使用LoRA进行参数高效微调。\n\n论文使用数据集和训练资源：\n1. 预训练数据集：Bridge数据集。\n2. 后训练和评估数据集：LIBERO数据集（包含四个任务套件：SPATIAL、OBJECT、GOAL、LONG）。\n3. 训练资源：预训练使用8×A100 GPU进行45个epoch（约9天）；后训练使用8×A100 GPU进行60k步。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：标准LIBERO模拟器环境和LIBERO-PRO鲁棒性基准。\n2. 评估指标：任务成功率（%），每个任务评估50个episode。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>MBZUAI（阿布扎比，阿联酋），伊利诺伊大学芝加哥分校（美国）</p>\n\n<p>论文概述：</p>\n<p>该论文提出了一种几何感知的视觉-语言-动作模型框架GLaD，旨在解决现有VLA模型因依赖2D视觉编码器而缺乏3D几何理解的问题。通过知识蒸馏将3D几何先验融入预训练过程，增强模型的空间推理和策略泛化能力，而无需深度传感器或显式3D标注。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>识别当前VLA架构因依赖CLIP、SigLIP等2D视觉编码器而缺乏几何理解的局限性。</li><li>提出GLaD框架，通过知识蒸馏将VGGT的3D几何特征与LLM隐藏状态对齐，使几何理解深度融入驱动动作预测的多模态表示。</li><li>在LIBERO基准测试中达到94.1%平均成功率，超越UniVLA（92.5%）；在LIBERO-PRO鲁棒性基准上，对视觉外观变化展现出显著改进的抵抗力。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>采用VGGT作为冻结的教师网络，提取3D几何特征（深度图、点云、相机参数）。</li><li>设计特征对齐网络，通过两层MLP将LLM隐藏状态投影到VGGT特征空间。</li><li>训练目标结合动作预测损失（交叉熵）和几何对齐损失（MSE），通过超参数λ平衡两者。</li><li>训练策略分为两阶段：第一阶段在Bridge数据集上进行带几何蒸馏的预训练；第二阶段在下游任务（如LIBERO）上进行后训练，使用LoRA进行参数高效微调。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>预训练数据集：Bridge数据集。</li><li>后训练和评估数据集：LIBERO数据集（包含四个任务套件：SPATIAL、OBJECT、GOAL、LONG）。</li><li>训练资源：预训练使用8×A100 GPU进行45个epoch（约9天）；后训练使用8×A100 GPU进行60k步。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：标准LIBERO模拟器环境和LIBERO-PRO鲁棒性基准。</li><li>评估指标：任务成功率（%），每个任务评估50个episode。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-09",
    "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning",
    "link": "http://arxiv.org/abs/2512.08580",
    "summary_markdown": "- 论文研究单位：astribot团队\n\n- 论文概述：提出lumo-1视觉语言动作模型，通过具身推理实现机器人从思维到动作的端到端控制。模型基于qwen2.5-vl-7b预训练模型，采用三阶段训练流程：持续vlm预训练、跨具身机器人数据协同训练、带推理过程的目标具身动作训练，并结合强化学习优化推理-动作一致性。\n\n- 论文核心贡献点：\n - 提出空间动作分词器，实现紧凑的动作表示\n - 设计三阶段训练流程，逐步增强具身推理和动作生成能力\n - 结合离散和连续动作表示\n - 实现结构化推理与动作的联合优化\n\n- 论文方法描述：\n - 使用多模态transformer架构，支持文本、图像和动作标记处理\n - 采用流匹配动作专家提高推理效率\n - 通过推理-动作训练范式促进有目的的动作生成\n - 利用grpo强化学习方法优化推理质量\n\n- 论文使用数据集和训练资源：\n - 数据集：cambrian-10m、llava-665k、pixmo caption、robo2vlm、whatsup、egoplan、sharerobot、agibot、galaxea等公开数据集，以及自收集的astribot s1机器人数据\n - 训练资源：128块h100 gpu，总训练token数约407b\n\n- 论文使用的评估环境和评估指标：\n - 评估环境：真实世界机器人实验环境\n - 评估指标：在7个基准测试（blink、cv-bench、embspartial、refspatial-bench、sat、where2place、roboSpatial）上的性能表现\n - 具体指标包括空间理解、物体定位、空间参考等能力评估",
    "summary_html": "<ul><li>论文研究单位：astribot团队</li></ul>\n\n<ul><li>论文概述：提出lumo-1视觉语言动作模型，通过具身推理实现机器人从思维到动作的端到端控制。模型基于qwen2.5-vl-7b预训练模型，采用三阶段训练流程：持续vlm预训练、跨具身机器人数据协同训练、带推理过程的目标具身动作训练，并结合强化学习优化推理-动作一致性。</li></ul>\n\n<ul><li>论文核心贡献点：</li></ul>\n<p> - 提出空间动作分词器，实现紧凑的动作表示</p>\n<p> - 设计三阶段训练流程，逐步增强具身推理和动作生成能力</p>\n<p> - 结合离散和连续动作表示</p>\n<p> - 实现结构化推理与动作的联合优化</p>\n\n<ul><li>论文方法描述：</li></ul>\n<p> - 使用多模态transformer架构，支持文本、图像和动作标记处理</p>\n<p> - 采用流匹配动作专家提高推理效率</p>\n<p> - 通过推理-动作训练范式促进有目的的动作生成</p>\n<p> - 利用grpo强化学习方法优化推理质量</p>\n\n<ul><li>论文使用数据集和训练资源：</li></ul>\n<p> - 数据集：cambrian-10m、llava-665k、pixmo caption、robo2vlm、whatsup、egoplan、sharerobot、agibot、galaxea等公开数据集，以及自收集的astribot s1机器人数据</p>\n<p> - 训练资源：128块h100 gpu，总训练token数约407b</p>\n\n<ul><li>论文使用的评估环境和评估指标：</li></ul>\n<p> - 评估环境：真实世界机器人实验环境</p>\n<p> - 评估指标：在7个基准测试（blink、cv-bench、embspartial、refspatial-bench、sat、where2place、roboSpatial）上的性能表现</p>\n<p> - 具体指标包括空间理解、物体定位、空间参考等能力评估</p>"
  },
  {
    "date": "2025-12-08",
    "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations",
    "link": "http://arxiv.org/abs/2512.07582",
    "summary_markdown": "论文研究单位：北京理工大学，LimX Dynamics\n\n论文概述：论文提出ViVLA，一种视觉-语言-动作模型，能够通过观看一次专家演示视频学习新任务。该方法通过联合处理专家演示视频和机器人视觉观察，预测演示动作序列和后续机器人动作，实现从专家行为中提取细粒度操作知识并传递给机器人。\n\n论文核心贡献点：\n1. 提出ViVLA新范式，能够从单次演示中提取细粒度操作知识并传递给机器人，无需额外训练或微调\n2. 引入包含循环一致性约束的潜在动作学习框架，建立统一潜在动作空间\n3. 开发可扩展的专家-智能体对数据生成流程，构建包含892,911个专家-智能体对轨迹的大规模数据集\n4. 在LIBERO基准测试中，未见任务上实现超过30%的改进，使用不同具身视频时保持超过35%的增益\n\n论文方法描述：\n1. 潜在动作学习与循环一致性：开发潜在动作标记器，从观察序列中提取潜在动作表示，通过动作中心循环一致性约束建立统一潜在动作空间\n2. 采用并行解码机制缓解捷径学习问题并提高推理效率\n3. 使用时空间掩码策略减少视频信息冗余\n4. 包括时间定位任务，将智能体观察图像插入演示视频序列\n5. 使用局部-全局判别器对齐生成帧与数据集图像的分布\n\n论文使用数据集和训练资源：\n1. 收集7,421个人类视频，涵盖超过100个不同操作任务\n2. 构建Human2Robot数据集，包含89,736个人类-机器人配对训练样本\n3. 从公开数据集构建803,175个配对样本\n4. 总共使用892,911个专家-智能体样本训练ViVLA\n5. 使用Qwen2.5-VL作为基础模型，采用LoRA进行微调\n6. 全局批大小256，训练30,000步，学习率2e-5\n\n论文使用的评估环境和评估指标：\n1. 在LIBERO基准测试上进行评估\n2. 未见任务上实现超过30%的改进\n3. 使用跨具身视频时保持超过35%的增益\n4. 真实世界实验显示从人类视频中有效学习，未见任务上实现超过38%的改进",
    "summary_html": "<p>论文研究单位：北京理工大学，LimX Dynamics</p>\n\n<p>论文概述：论文提出ViVLA，一种视觉-语言-动作模型，能够通过观看一次专家演示视频学习新任务。该方法通过联合处理专家演示视频和机器人视觉观察，预测演示动作序列和后续机器人动作，实现从专家行为中提取细粒度操作知识并传递给机器人。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出ViVLA新范式，能够从单次演示中提取细粒度操作知识并传递给机器人，无需额外训练或微调</li><li>引入包含循环一致性约束的潜在动作学习框架，建立统一潜在动作空间</li><li>开发可扩展的专家-智能体对数据生成流程，构建包含892,911个专家-智能体对轨迹的大规模数据集</li><li>在LIBERO基准测试中，未见任务上实现超过30%的改进，使用不同具身视频时保持超过35%的增益</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>潜在动作学习与循环一致性：开发潜在动作标记器，从观察序列中提取潜在动作表示，通过动作中心循环一致性约束建立统一潜在动作空间</li><li>采用并行解码机制缓解捷径学习问题并提高推理效率</li><li>使用时空间掩码策略减少视频信息冗余</li><li>包括时间定位任务，将智能体观察图像插入演示视频序列</li><li>使用局部-全局判别器对齐生成帧与数据集图像的分布</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>收集7,421个人类视频，涵盖超过100个不同操作任务</li><li>构建Human2Robot数据集，包含89,736个人类-机器人配对训练样本</li><li>从公开数据集构建803,175个配对样本</li><li>总共使用892,911个专家-智能体样本训练ViVLA</li><li>使用Qwen2.5-VL作为基础模型，采用LoRA进行微调</li><li>全局批大小256，训练30,000步，学习率2e-5</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>在LIBERO基准测试上进行评估</li><li>未见任务上实现超过30%的改进</li><li>使用跨具身视频时保持超过35%的增益</li><li>真实世界实验显示从人类视频中有效学习，未见任务上实现超过38%的改进</li></ol>"
  },
  {
    "date": "2025-12-08",
    "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.07472",
    "summary_markdown": "论文研究单位\n悉尼大学计算机科学学院\n上海交通大学约翰·霍普克罗夫特计算机科学中心\n\n论文概述\n本论文提出了一种名为Affordance Field Intervention（AFI）的混合框架，旨在解决视觉语言动作（VLA）模型在分布外（OOD）场景下陷入\"记忆陷阱\"的问题。记忆陷阱指VLA模型在遇到环境变化时，仍然机械地复现训练时记忆的轨迹，而非适应更新后的场景。AFI利用3D空间可承受场（SAF）作为VLA动作生成的插件，通过检测记忆陷阱、引导回滚到高可承受位置以及分层轨迹探索，帮助VLA模型逃离记忆陷阱，提高任务成功率。\n\n论文核心贡献点\n1. 提出记忆陷阱检测机制，通过本体感知监控末端执行器运动模式和目标进展，识别VLA是否陷入僵化的记忆轨迹。\n2. 设计可承受场干预方法，包括基于可承受的历史回滚和分层探索最优轨迹。\n3. 开发模型无关的框架，无需修改VLA参数即可提升其鲁棒性。\n4. 在真实世界和仿真环境中验证了方法的有效性，平均成功率提升显著。\n\n论文方法描述\n1. 记忆陷阱检测：监测末端执行器位移是否低于阈值且距离目标超过阈值，判断是否陷入记忆陷阱。\n2. 可承受场干预：检测到记忆陷阱后，系统回滚到历史高可承受位置，然后进行两阶段树状探索：\n - 阶段1：局部SAF引导采样中间路径点，选择成本最低的位置作为路径点。\n - 阶段2：在采样路径点上使用VLA生成轨迹候选，SAF作为评分器根据累积可承受值选择最优轨迹执行。\n3. 可承受场构建：使用GPT-4o分解任务指令，Grounded-SAM生成2D分割掩码，结合深度图构建3D可承受场，包括目标引导场和障碍物避免场。\n\n论文使用数据集和训练资源\n1. 真实世界实验：使用AgileX Piper机械臂和两个Intel RealSense D435摄像头。\n2. 仿真实验：使用LIBERO-Pro基准测试，引入空间扰动评估OOD泛化能力。\n3. 训练资源：在NVIDIA H100 GPU上对VLA模型进行30,000步微调，批量大小为32。\n4. 计算资源：SAF重建在NVIDIA RTX 4090上每帧120毫秒，运动学计算在NVIDIA GeForce GTX 1080Ti GPU上约5毫秒延迟。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n - 真实世界：四个操作任务（放置胡萝卜、移除盖子、插入笔、堆叠胶带），每个任务在五种测试条件下评估20次试验。\n2. 评估指标：成功率，比较基线VLA模型和AFI框架在不同分布偏移场景下的表现。\n3. 仿真环境：LIBERO-Spatial和LIBERO-Object套件，评估空间扰动下的性能。",
    "summary_html": "<p>论文研究单位</p>\n<p>悉尼大学计算机科学学院</p>\n<p>上海交通大学约翰·霍普克罗夫特计算机科学中心</p>\n\n<p>论文概述</p>\n<p>本论文提出了一种名为Affordance Field Intervention（AFI）的混合框架，旨在解决视觉语言动作（VLA）模型在分布外（OOD）场景下陷入\"记忆陷阱\"的问题。记忆陷阱指VLA模型在遇到环境变化时，仍然机械地复现训练时记忆的轨迹，而非适应更新后的场景。AFI利用3D空间可承受场（SAF）作为VLA动作生成的插件，通过检测记忆陷阱、引导回滚到高可承受位置以及分层轨迹探索，帮助VLA模型逃离记忆陷阱，提高任务成功率。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出记忆陷阱检测机制，通过本体感知监控末端执行器运动模式和目标进展，识别VLA是否陷入僵化的记忆轨迹。</li><li>设计可承受场干预方法，包括基于可承受的历史回滚和分层探索最优轨迹。</li><li>开发模型无关的框架，无需修改VLA参数即可提升其鲁棒性。</li><li>在真实世界和仿真环境中验证了方法的有效性，平均成功率提升显著。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>记忆陷阱检测：监测末端执行器位移是否低于阈值且距离目标超过阈值，判断是否陷入记忆陷阱。</li><li>可承受场干预：检测到记忆陷阱后，系统回滚到历史高可承受位置，然后进行两阶段树状探索：</li></ol>\n<p> - 阶段1：局部SAF引导采样中间路径点，选择成本最低的位置作为路径点。</p>\n<p> - 阶段2：在采样路径点上使用VLA生成轨迹候选，SAF作为评分器根据累积可承受值选择最优轨迹执行。</p>\n<p>3. 可承受场构建：使用GPT-4o分解任务指令，Grounded-SAM生成2D分割掩码，结合深度图构建3D可承受场，包括目标引导场和障碍物避免场。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>真实世界实验：使用AgileX Piper机械臂和两个Intel RealSense D435摄像头。</li><li>仿真实验：使用LIBERO-Pro基准测试，引入空间扰动评估OOD泛化能力。</li><li>训练资源：在NVIDIA H100 GPU上对VLA模型进行30,000步微调，批量大小为32。</li><li>计算资源：SAF重建在NVIDIA RTX 4090上每帧120毫秒，运动学计算在NVIDIA GeForce GTX 1080Ti GPU上约5毫秒延迟。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> - 真实世界：四个操作任务（放置胡萝卜、移除盖子、插入笔、堆叠胶带），每个任务在五种测试条件下评估20次试验。</p>\n<ol><li>评估指标：成功率，比较基线VLA模型和AFI框架在不同分布偏移场景下的表现。</li><li>仿真环境：LIBERO-Spatial和LIBERO-Object套件，评估空间扰动下的性能。</li></ol>"
  },
  {
    "date": "2025-12-07",
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "link": "http://arxiv.org/abs/2512.06963",
    "summary_markdown": "论文研究单位\n西安交通大学人工智能与机器人研究所，微软亚洲研究院，复旦大学\n\n论文概述\n视频生成模型可以作为通用机器人操作器。论文提出VideoVLA方法，将大型视频生成模型转换为机器人视觉语言动作模型，通过联合预测动作序列和未来视觉结果，实现机器人操作的泛化能力。\n\n论文核心贡献点\n提出将预训练视频生成模型转换为机器人操作器的简单有效方法，引入双预测策略同时预测动作和视觉结果，展示了对新物体和新技能的泛化能力。\n\n论文方法描述\n基于多模态扩散变换器，将视频扩散变换器扩展为视频动作扩散变换器，添加动作作为新输出模态，使用DDPM扩散损失联合去噪视频和动作。采用T5文本编码器和3D因果VAE视频编码器处理输入，将语言指令和当前视觉观察的潜在表示作为条件，预测未来动作块和未来帧潜在表示。\n\n论文使用数据集和训练资源\n使用Open X-Embodiment数据集进行预训练，包含超过100万条真实世界机器人轨迹。训练使用32个AMD MI300X GPU，批量大小为256，预训练100K迭代，微调15K迭代。\n\n论文使用评估环境和评估指标\n在SIMPLER仿真环境和真实世界环境中进行评估，使用视觉匹配和变体聚合两种评估协议，报告任务成功率作为主要指标。",
    "summary_html": "<p>论文研究单位</p>\n<p>西安交通大学人工智能与机器人研究所，微软亚洲研究院，复旦大学</p>\n\n<p>论文概述</p>\n<p>视频生成模型可以作为通用机器人操作器。论文提出VideoVLA方法，将大型视频生成模型转换为机器人视觉语言动作模型，通过联合预测动作序列和未来视觉结果，实现机器人操作的泛化能力。</p>\n\n<p>论文核心贡献点</p>\n<p>提出将预训练视频生成模型转换为机器人操作器的简单有效方法，引入双预测策略同时预测动作和视觉结果，展示了对新物体和新技能的泛化能力。</p>\n\n<p>论文方法描述</p>\n<p>基于多模态扩散变换器，将视频扩散变换器扩展为视频动作扩散变换器，添加动作作为新输出模态，使用DDPM扩散损失联合去噪视频和动作。采用T5文本编码器和3D因果VAE视频编码器处理输入，将语言指令和当前视觉观察的潜在表示作为条件，预测未来动作块和未来帧潜在表示。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用Open X-Embodiment数据集进行预训练，包含超过100万条真实世界机器人轨迹。训练使用32个AMD MI300X GPU，批量大小为256，预训练100K迭代，微调15K迭代。</p>\n\n<p>论文使用评估环境和评估指标</p>\n<p>在SIMPLER仿真环境和真实世界环境中进行评估，使用视觉匹配和变体聚合两种评估协议，报告任务成功率作为主要指标。</p>"
  },
  {
    "date": "2025-12-05",
    "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving",
    "link": "http://arxiv.org/abs/2512.06112",
    "summary_markdown": "论文研究单位：复旦大学，音王智能科技有限公司\n论文概述：wam flow是一种视觉语言动作模型，将自车轨迹规划建模为结构化令牌空间上的离散流匹配问题。该方法支持完全并行双向去噪，实现粗到细的轨迹优化，并允许计算精度权衡。\n论文核心贡献点：提出基于离散流匹配的并行运动规划框架，引入度量对齐数值令牌化器，设计几何感知流目标，开发模拟器引导的grpo对齐方法，实现多阶段自适应训练策略。\n论文方法描述：使用连续时间马尔可夫链在离散令牌空间中进行概率传输，通过混合路径构造概率路径，利用速率矩阵定义生成动态。模型架构包括扩展的janus 1 5b主干网络，集成视觉和语言输入，输出8个航点组成的4秒轨迹。采用欧拉离散化进行并行去噪推理。\n论文使用数据集和训练资源：使用nuplan数据集进行初始训练，包含668k样本训练4个周期。使用6 5m视觉问答数据进行预训练，包括来自llava v1 5的3 4m通用多模态vqa和来自recogdrive的3 1m驾驶专用vqa。使用103k navsim数据集进行0 5周期的强化学习训练。模型基于janus 1 5b主干网络，词汇表扩展至122401个词。\n论文使用的评估环境和评估指标：在navsim v1和v2基准上进行评估。使用pdms和epdms作为主要指标，同时评估安全指标和性能指标。安全指标包括无碰撞和可行驶区域合规性，性能指标包括自车进度，碰撞时间和舒适度。",
    "summary_html": "<p>论文研究单位：复旦大学，音王智能科技有限公司</p>\n<p>论文概述：wam flow是一种视觉语言动作模型，将自车轨迹规划建模为结构化令牌空间上的离散流匹配问题。该方法支持完全并行双向去噪，实现粗到细的轨迹优化，并允许计算精度权衡。</p>\n<p>论文核心贡献点：提出基于离散流匹配的并行运动规划框架，引入度量对齐数值令牌化器，设计几何感知流目标，开发模拟器引导的grpo对齐方法，实现多阶段自适应训练策略。</p>\n<p>论文方法描述：使用连续时间马尔可夫链在离散令牌空间中进行概率传输，通过混合路径构造概率路径，利用速率矩阵定义生成动态。模型架构包括扩展的janus 1 5b主干网络，集成视觉和语言输入，输出8个航点组成的4秒轨迹。采用欧拉离散化进行并行去噪推理。</p>\n<p>论文使用数据集和训练资源：使用nuplan数据集进行初始训练，包含668k样本训练4个周期。使用6 5m视觉问答数据进行预训练，包括来自llava v1 5的3 4m通用多模态vqa和来自recogdrive的3 1m驾驶专用vqa。使用103k navsim数据集进行0 5周期的强化学习训练。模型基于janus 1 5b主干网络，词汇表扩展至122401个词。</p>\n<p>论文使用的评估环境和评估指标：在navsim v1和v2基准上进行评估。使用pdms和epdms作为主要指标，同时评估安全指标和性能指标。安全指标包括无碰撞和可行驶区域合规性，性能指标包括自车进度，碰撞时间和舒适度。</p>"
  },
  {
    "date": "2025-12-05",
    "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking",
    "link": "http://arxiv.org/abs/2512.05964",
    "summary_markdown": "论文研究单位：物理智能（Physical Intelligence）\n\n论文概述：本文提出了一种训练时动作条件化的方法，用于高效的实时分块控制。该方法通过模拟推理延迟并在训练时直接对动作前缀进行条件化，消除了推理时的计算开销，从而在保持任务性能和速度的同时降低计算成本。\n\n论文核心贡献点：提出训练时实时分块控制作为推理时实时分块控制的即插即用替代方案，无需修改模型架构或机器人运行时，仅需少量代码即可实现。\n\n论文方法描述：通过三个关键修改实现训练时动作条件化：允许每个动作时间步使用不同的流匹配时间步；对前缀使用真实无噪声动作并设置对应流匹配时间步为1；在损失函数中仅对后置部分计算损失。在训练过程中随机采样延迟参数，使模型学习在给定动作前缀条件下生成动作后置部分。\n\n论文使用数据集和训练资源：模拟实验使用动态Kinetix基准数据集，真实世界实验基于π0.6基础模型，在盒装搭建和意式咖啡制作任务上进行测试。训练批量大小为512，进行8000次梯度步。\n\n论文使用的评估环境和评估指标：模拟实验使用2048次试验评估解决率，真实世界实验在远程H100服务器上进行推理，评估指标包括成功率和任务持续时间，使用威尔逊分数区间和标准误差进行统计评估。",
    "summary_html": "<p>论文研究单位：物理智能（Physical Intelligence）</p>\n\n<p>论文概述：本文提出了一种训练时动作条件化的方法，用于高效的实时分块控制。该方法通过模拟推理延迟并在训练时直接对动作前缀进行条件化，消除了推理时的计算开销，从而在保持任务性能和速度的同时降低计算成本。</p>\n\n<p>论文核心贡献点：提出训练时实时分块控制作为推理时实时分块控制的即插即用替代方案，无需修改模型架构或机器人运行时，仅需少量代码即可实现。</p>\n\n<p>论文方法描述：通过三个关键修改实现训练时动作条件化：允许每个动作时间步使用不同的流匹配时间步；对前缀使用真实无噪声动作并设置对应流匹配时间步为1；在损失函数中仅对后置部分计算损失。在训练过程中随机采样延迟参数，使模型学习在给定动作前缀条件下生成动作后置部分。</p>\n\n<p>论文使用数据集和训练资源：模拟实验使用动态Kinetix基准数据集，真实世界实验基于π0.6基础模型，在盒装搭建和意式咖啡制作任务上进行测试。训练批量大小为512，进行8000次梯度步。</p>\n\n<p>论文使用的评估环境和评估指标：模拟实验使用2048次试验评估解决率，真实世界实验在远程H100服务器上进行推理，评估指标包括成功率和任务持续时间，使用威尔逊分数区间和标准误差进行统计评估。</p>"
  },
  {
    "date": "2025-12-05",
    "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies",
    "link": "http://arxiv.org/abs/2512.05693",
    "summary_markdown": "论文研究单位：复旦大学、微软亚洲研究院、西安交通大学、清华大学\n\n论文概述：hiMoE-vla提出一种基于分层混合专家架构的视觉-语言-动作框架，旨在处理机器人数据中的异构性，包括不同的动作空间、本体配置和传感器设置，通过将异构因素抽象为共享知识表示，实现跨领域知识迁移。\n\n论文核心贡献点：提出新型视觉-语言-动作框架，专门处理机器人数据中的异构性；引入包含动作空间MoE和异构平衡MoE的分层混合专家架构；在仿真基准和真实世界机器人平台上均取得优于现有VLA基线的性能表现。\n\n论文方法描述：采用预训练的PaliGemma模型作为视觉-语言模块；设计分层混合专家架构，在浅层使用动作空间MoE处理动作空间差异，在相邻层使用异构平衡MoE整合更广泛的异构因素；使用流匹配损失建模动作分布；引入动作空间正则化和异构平衡正则化来指导专家 specialization 和抽象。\n\n论文使用数据集和训练资源：预训练数据集包含Open X-Embodiment数据集（22.5M帧）和Aloha数据集（1.6M帧），总计24.1M帧；训练使用16块A100 GPU，模型参数量为4B。\n\n论文使用的评估环境和评估指标：仿真实验使用CALVIN和LIBERO基准，评估指标包括连续完成任务的平均数量和成功率；真实世界实验在xArm7单臂机器人和Aloha双臂机器人上进行，评估指标为各任务子阶段和整体平均成功率。",
    "summary_html": "<p>论文研究单位：复旦大学、微软亚洲研究院、西安交通大学、清华大学</p>\n\n<p>论文概述：hiMoE-vla提出一种基于分层混合专家架构的视觉-语言-动作框架，旨在处理机器人数据中的异构性，包括不同的动作空间、本体配置和传感器设置，通过将异构因素抽象为共享知识表示，实现跨领域知识迁移。</p>\n\n<p>论文核心贡献点：提出新型视觉-语言-动作框架，专门处理机器人数据中的异构性；引入包含动作空间MoE和异构平衡MoE的分层混合专家架构；在仿真基准和真实世界机器人平台上均取得优于现有VLA基线的性能表现。</p>\n\n<p>论文方法描述：采用预训练的PaliGemma模型作为视觉-语言模块；设计分层混合专家架构，在浅层使用动作空间MoE处理动作空间差异，在相邻层使用异构平衡MoE整合更广泛的异构因素；使用流匹配损失建模动作分布；引入动作空间正则化和异构平衡正则化来指导专家 specialization 和抽象。</p>\n\n<p>论文使用数据集和训练资源：预训练数据集包含Open X-Embodiment数据集（22.5M帧）和Aloha数据集（1.6M帧），总计24.1M帧；训练使用16块A100 GPU，模型参数量为4B。</p>\n\n<p>论文使用的评估环境和评估指标：仿真实验使用CALVIN和LIBERO基准，评估指标包括连续完成任务的平均数量和成功率；真实世界实验在xArm7单臂机器人和Aloha双臂机器人上进行，评估指标为各任务子阶段和整体平均成功率。</p>"
  },
  {
    "date": "2025-12-04",
    "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2512.05107",
    "summary_markdown": "论文研究单位\n- 慕尼黑华为技术研究中心\n- 帝国理工学院\n\n论文概述\n该论文提出stare-vla框架，用于改进视觉-语言-动作模型的微调过程。针对长时程机器人操作任务中存在的稀疏奖励和信用分配问题，设计了一种渐进式阶段感知强化方法。该方法将动作轨迹分解为语义上有意义的阶段，并为每个阶段提供密集的奖励信号。通过将stare模块集成到tpo和ppo算法中，形成了sta-tpo和sta-ppo两种改进方法。进一步结合监督微调，构建了ipi三阶段微调流水线。实验结果表明，该方法在simplerenv和maniskill3基准测试中取得了最先进的成功率。\n\n论文核心贡献点\n- 设计stare模块，能够将轨迹分解为语义上有意义的阶段，提供超越轨迹级别的细粒度监督\n- 提出基于stare的两种微调方法：sta-tpo用于离线阶段偏好对齐，sta-ppo用于在线阶段内交互\n- 构建ipi流水线，统一监督微调、sta-tpo和sta-ppo，形成完整的三阶段微调框架\n\n论文方法描述\nstare模块包含两个核心组件：阶段分离器和阶段计算器。阶段分离器通过检测任务相关事件来确定阶段转换时机，基于末端执行器的平移和旋转信号设置阈值来识别阶段边界。阶段计算器计算阶段成本和每步奖励，使用基于势能的奖励塑形方法为每个阶段提供密集指导。sta-tpo在阶段级别构建成对偏好，通过阶段成本提供精确的梯度信号。sta-ppo将稀疏的终端奖励转化为密集的交互奖励，稳定阶段内更新。ipi流水线依次执行sft、sta-tpo和sta-ppo，实现渐进式优化。\n\n论文使用数据集和训练资源\n- 使用openvla-7b和pi0.5_base作为骨干网络\n- 在simplerenv-widowx和maniskill3-franka环境中进行实验\n- 包括stackcube、pushcube、pullcube和liftpegupright等接触丰富的任务\n- 所有方法均基于openvla-7b进行微调\n\n论文使用的评估环境和评估指标\n- 评估环境：simplerenv和maniskill3模拟基准\n- 评估指标：成功率，在simplerenv上达到98.0%，在maniskill3上达到96.4%\n- 每个方法和设置评估300个episode\n- 对比基线包括rt-1-x、octo-base/small、robovlm、spatialvla等vl模型\n- 同时比较grape、rl4vla和πrl等微调方法\n- 报告最终成功率和抓取成功率",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>慕尼黑华为技术研究中心</li><li>帝国理工学院</li></ul>\n\n<p>论文概述</p>\n<p>该论文提出stare-vla框架，用于改进视觉-语言-动作模型的微调过程。针对长时程机器人操作任务中存在的稀疏奖励和信用分配问题，设计了一种渐进式阶段感知强化方法。该方法将动作轨迹分解为语义上有意义的阶段，并为每个阶段提供密集的奖励信号。通过将stare模块集成到tpo和ppo算法中，形成了sta-tpo和sta-ppo两种改进方法。进一步结合监督微调，构建了ipi三阶段微调流水线。实验结果表明，该方法在simplerenv和maniskill3基准测试中取得了最先进的成功率。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>设计stare模块，能够将轨迹分解为语义上有意义的阶段，提供超越轨迹级别的细粒度监督</li><li>提出基于stare的两种微调方法：sta-tpo用于离线阶段偏好对齐，sta-ppo用于在线阶段内交互</li><li>构建ipi流水线，统一监督微调、sta-tpo和sta-ppo，形成完整的三阶段微调框架</li></ul>\n\n<p>论文方法描述</p>\n<p>stare模块包含两个核心组件：阶段分离器和阶段计算器。阶段分离器通过检测任务相关事件来确定阶段转换时机，基于末端执行器的平移和旋转信号设置阈值来识别阶段边界。阶段计算器计算阶段成本和每步奖励，使用基于势能的奖励塑形方法为每个阶段提供密集指导。sta-tpo在阶段级别构建成对偏好，通过阶段成本提供精确的梯度信号。sta-ppo将稀疏的终端奖励转化为密集的交互奖励，稳定阶段内更新。ipi流水线依次执行sft、sta-tpo和sta-ppo，实现渐进式优化。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>使用openvla-7b和pi0.5_base作为骨干网络</li><li>在simplerenv-widowx和maniskill3-franka环境中进行实验</li><li>包括stackcube、pushcube、pullcube和liftpegupright等接触丰富的任务</li><li>所有方法均基于openvla-7b进行微调</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：simplerenv和maniskill3模拟基准</li><li>评估指标：成功率，在simplerenv上达到98.0%，在maniskill3上达到96.4%</li><li>每个方法和设置评估300个episode</li><li>对比基线包括rt-1-x、octo-base/small、robovlm、spatialvla等vl模型</li><li>同时比较grape、rl4vla和πrl等微调方法</li><li>报告最终成功率和抓取成功率</li></ul>"
  },
  {
    "date": "2025-12-04",
    "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization",
    "link": "http://arxiv.org/abs/2512.04952",
    "summary_markdown": "```markdown\n论文研究单位：\n清华大学，复旦大学，上海创新研究院，Galaxea AI，天津大学，香港大学，加州大学圣地亚哥分校\n\n论文概述：\n本文提出FASTer框架，通过神经动作标记化实现高效自回归视觉语言动作建模。该框架包含FASTerVQ（动作标记器）和FASTerVLA（自回归策略）两个组件，旨在解决动作标记化在重建保真度和推理效率之间的权衡问题。\n\n论文核心贡献点：\n1. 提出FASTerVQ：基于变换器和残差向量量化的紧凑高压缩比动作标记器\n2. 引入块级自回归解码和轻量级动作专家，实现高效动作标记建模\n3. 建立覆盖四个真实机器人和四个模拟环境的综合基准，首次系统分析VLA的动作标记化\n4. 在模拟和真实世界环境中实现最先进性能\n\n论文方法描述：\nFASTerVQ采用动作分块器和基于变换器的残差向量量化标记器，将动作序列编码为单通道图像，捕捉全局时空依赖关系。FASTerVLA基于该标记器，采用块级自回归解码和轻量级动作专家，实现并行预测同时保持连贯时空结构。\n\n论文使用数据集和训练资源：\n使用九个基准数据集，包括Libero、Simpler-Bridge、VLABench、GalaxeaManipSim、Xarm Suite、R1Lite Suite、Bridge和Droid。训练使用8个H100 GPU，全局批处理大小为32-128，训练步数为30k-4个epoch。\n\n论文使用的评估环境和评估指标：\n评估环境：四个真实机器人（xArm、R1Lite、WidowX、Franka）和四个模拟环境（LIBERO、VLABench、GalaxeaManipSim）。评估指标：任务成功率、有效重建率（VRR）、压缩率与重建权衡、推理时间测量。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>清华大学，复旦大学，上海创新研究院，Galaxea AI，天津大学，香港大学，加州大学圣地亚哥分校</p>\n\n<p>论文概述：</p>\n<p>本文提出FASTer框架，通过神经动作标记化实现高效自回归视觉语言动作建模。该框架包含FASTerVQ（动作标记器）和FASTerVLA（自回归策略）两个组件，旨在解决动作标记化在重建保真度和推理效率之间的权衡问题。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出FASTerVQ：基于变换器和残差向量量化的紧凑高压缩比动作标记器</li><li>引入块级自回归解码和轻量级动作专家，实现高效动作标记建模</li><li>建立覆盖四个真实机器人和四个模拟环境的综合基准，首次系统分析VLA的动作标记化</li><li>在模拟和真实世界环境中实现最先进性能</li></ol>\n\n<p>论文方法描述：</p>\n<p>FASTerVQ采用动作分块器和基于变换器的残差向量量化标记器，将动作序列编码为单通道图像，捕捉全局时空依赖关系。FASTerVLA基于该标记器，采用块级自回归解码和轻量级动作专家，实现并行预测同时保持连贯时空结构。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>使用九个基准数据集，包括Libero、Simpler-Bridge、VLABench、GalaxeaManipSim、Xarm Suite、R1Lite Suite、Bridge和Droid。训练使用8个H100 GPU，全局批处理大小为32-128，训练步数为30k-4个epoch。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：四个真实机器人（xArm、R1Lite、WidowX、Franka）和四个模拟环境（LIBERO、VLABench、GalaxeaManipSim）。评估指标：任务成功率、有效重建率（VRR）、压缩率与重建权衡、推理时间测量。</p>\n<p>```</p>"
  },
  {
    "date": "2025-12-04",
    "title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2512.04733",
    "summary_markdown": "论文研究单位\n- 麦吉尔大学\n- 澳门大学\n- 香港理工大学\n- 麻省理工学院\n- 华盛顿大学\n\n论文概述\n本文提出e3ad，一种情感感知的视觉-语言-动作模型，用于以人为中心的端到端自动驾驶。该模型扩展了传统的端到端自动驾驶框架，引入开放域端到端自动驾驶任务，要求自动驾驶车辆理解自然语言指令、推断情感状态，并规划物理可行的轨迹。e3ad通过集成连续的情感建模和双系统空间推理，在统一管道中实现情感感知和空间一致的推理。\n\n论文核心贡献点\n- 定义开放域端到端自动驾驶任务，统一自然语言指令的语义、情感和空间推理\n- 提出e3ad框架，集成连续情感建模和双系统空间推理，实现情感感知的端到端管道\n- 在多个基准测试中，e3ad在视觉定位、情感估计和路径点规划方面优于强基线\n\n论文方法描述\n- 情感建模：使用连续的情感-唤醒-支配空间捕捉语言中的情感和紧迫性\n- 空间推理：融合以自我为中心和以他为中心的双系统空间表示\n- 一致性导向训练：采用三阶段训练策略，包括模态预训练、联合微调和情感-动作对齐\n- 动作解码器：将高级输出转换为精确的物理可实现轨迹\n- 人类中心语言反馈：根据情感状态和紧迫性调整语气和内容\n\n论文使用数据集和训练资源\n- 数据集：talk2car, drivepilot, mocad, talk2car-trajectory\n- 训练资源：8个nvidia h200 gpu\n- 训练方法：使用ms-swift库和lora微调，秩为16，缩放因子为32，恒定学习率为0.0001，每个设备批次大小为16\n\n论文使用的评估环境和评估指标\n- 评估环境：多个真实世界基准测试，包括talk2car, drivepilot, mocad等\n- 评估指标：\n - 端到端性能：ade, fde, fréchet, dtw, sspd, 规划精度\n - 子任务评估：视觉定位的iou，空间推理的mae和iou，情感感知的spearman和kendall相关系数",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>麦吉尔大学</li><li>澳门大学</li><li>香港理工大学</li><li>麻省理工学院</li><li>华盛顿大学</li></ul>\n\n<p>论文概述</p>\n<p>本文提出e3ad，一种情感感知的视觉-语言-动作模型，用于以人为中心的端到端自动驾驶。该模型扩展了传统的端到端自动驾驶框架，引入开放域端到端自动驾驶任务，要求自动驾驶车辆理解自然语言指令、推断情感状态，并规划物理可行的轨迹。e3ad通过集成连续的情感建模和双系统空间推理，在统一管道中实现情感感知和空间一致的推理。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>定义开放域端到端自动驾驶任务，统一自然语言指令的语义、情感和空间推理</li><li>提出e3ad框架，集成连续情感建模和双系统空间推理，实现情感感知的端到端管道</li><li>在多个基准测试中，e3ad在视觉定位、情感估计和路径点规划方面优于强基线</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>情感建模：使用连续的情感-唤醒-支配空间捕捉语言中的情感和紧迫性</li><li>空间推理：融合以自我为中心和以他为中心的双系统空间表示</li><li>一致性导向训练：采用三阶段训练策略，包括模态预训练、联合微调和情感-动作对齐</li><li>动作解码器：将高级输出转换为精确的物理可实现轨迹</li><li>人类中心语言反馈：根据情感状态和紧迫性调整语气和内容</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：talk2car, drivepilot, mocad, talk2car-trajectory</li><li>训练资源：8个nvidia h200 gpu</li><li>训练方法：使用ms-swift库和lora微调，秩为16，缩放因子为32，恒定学习率为0.0001，每个设备批次大小为16</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：多个真实世界基准测试，包括talk2car, drivepilot, mocad等</li><li>评估指标：</li></ul>\n<p> - 端到端性能：ade, fde, fréchet, dtw, sspd, 规划精度</p>\n<p> - 子任务评估：视觉定位的iou，空间推理的mae和iou，情感感知的spearman和kendall相关系数</p>"
  },
  {
    "date": "2025-12-04",
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "link": "http://arxiv.org/abs/2512.04537",
    "summary_markdown": "论文研究单位：Show Lab, National University of Singapore\n\n论文概述：X-Humanoid提出一种生成式视频编辑方法，通过将人类视频转化为类人机器人视频来解决机器人研究中的数据稀缺问题。该方法利用强大的视频生成模型，经过微调实现第三人称视角下人类到类人机器人的视频转换，生成大规模数据集以促进具身人工智能研究。\n\n论文核心贡献点：\n- 提出生成式视频编辑方法，通过适应和微调现代视频生成模型来解决机器人研究数据稀缺问题。\n- 设计可扩展的数据合成流程，使用Unreal Engine生成超过17小时的配对人类-类人机器人视频数据集。\n- 创建并发布大规模\"机器人化\"数据集，包含60多小时的Ego-Exo4D视频转换结果，以特斯拉Optimus类人机器人为特征。\n\n论文方法描述：\n- 将Wan 2.2扩散变换器模型适应为视频输入-视频输出架构。\n- 输入视频编码为条件标记，与生成标记拼接，在自注意力层应用单向掩码防止条件标记关注生成标记。\n- 使用Unreal Engine合成配对训练数据，通过骨架对齐、动画转移和虚拟相机录制三个步骤。\n- 采用流匹配微调目标，使用LoRA方法进行参数高效微调。\n\n论文使用数据集和训练资源：\n- 合成数据集：17+小时1080p 30fps配对视频，包含2.8百万帧，使用NVIDIA RTX 3060 GPU渲染10天。\n- 训练资源：使用4个NVIDIA H200 GPU进行分布式数据并行训练，批量大小为1，训练500步耗时2.5小时。\n\n论文使用的评估环境和评估指标：\n- 评估环境：在合成验证集和真实世界Ego-Exo4D视频上测试。\n- 评估指标：用户偏好率（运动一致性69.0%，背景一致性75.9%，体现一致性62.1%，视频质量62.1%）。\n- 定量指标：PSNR（21.836 dB）、SSIM（0.671）、MSE（459.302）。\n- 用户研究：29名参与者评估10个视频片段，比较运动一致性、背景一致性、体现一致性和整体视频质量。",
    "summary_html": "<p>论文研究单位：Show Lab, National University of Singapore</p>\n\n<p>论文概述：X-Humanoid提出一种生成式视频编辑方法，通过将人类视频转化为类人机器人视频来解决机器人研究中的数据稀缺问题。该方法利用强大的视频生成模型，经过微调实现第三人称视角下人类到类人机器人的视频转换，生成大规模数据集以促进具身人工智能研究。</p>\n\n<p>论文核心贡献点：</p>\n<ul><li>提出生成式视频编辑方法，通过适应和微调现代视频生成模型来解决机器人研究数据稀缺问题。</li><li>设计可扩展的数据合成流程，使用Unreal Engine生成超过17小时的配对人类-类人机器人视频数据集。</li><li>创建并发布大规模\"机器人化\"数据集，包含60多小时的Ego-Exo4D视频转换结果，以特斯拉Optimus类人机器人为特征。</li></ul>\n\n<p>论文方法描述：</p>\n<ul><li>将Wan 2.2扩散变换器模型适应为视频输入-视频输出架构。</li><li>输入视频编码为条件标记，与生成标记拼接，在自注意力层应用单向掩码防止条件标记关注生成标记。</li><li>使用Unreal Engine合成配对训练数据，通过骨架对齐、动画转移和虚拟相机录制三个步骤。</li><li>采用流匹配微调目标，使用LoRA方法进行参数高效微调。</li></ul>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>合成数据集：17+小时1080p 30fps配对视频，包含2.8百万帧，使用NVIDIA RTX 3060 GPU渲染10天。</li><li>训练资源：使用4个NVIDIA H200 GPU进行分布式数据并行训练，批量大小为1，训练500步耗时2.5小时。</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>评估环境：在合成验证集和真实世界Ego-Exo4D视频上测试。</li><li>评估指标：用户偏好率（运动一致性69.0%，背景一致性75.9%，体现一致性62.1%，视频质量62.1%）。</li><li>定量指标：PSNR（21.836 dB）、SSIM（0.671）、MSE（459.302）。</li><li>用户研究：29名参与者评估10个视频片段，比较运动一致性、背景一致性、体现一致性和整体视频质量。</li></ul>"
  },
  {
    "date": "2025-12-04",
    "title": "dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning",
    "link": "http://arxiv.org/abs/2512.04459",
    "summary_markdown": "```markdown\n论文研究单位：\n威斯康星大学麦迪逊分校，英伟达，斯坦福大学，约翰斯·霍普金斯大学\n\n论文概述：\ndVLM-AD是一种基于扩散模型的视觉语言模型，用于端到端自动驾驶。该研究旨在解决现有自回归视觉语言模型在自动驾驶中存在的推理-动作不一致和不可控生成问题。通过引入双向注意力机制和迭代去噪过程，dVLM-AD能够统一感知、结构化推理和低层规划，在复杂驾驶场景下实现更强的全局一致性和可控性。\n\n论文核心贡献点：\n1. 提出首个基于扩散模型的自动驾驶视觉语言模型，用双向迭代生成取代左到右解码\n2. 设计动态去噪策略，解决固定长度槽偏差问题\n3. 实现结构化可控推理框架，增强推理与动作之间的一致性\n4. 在长尾场景下显著提升规划性能，同时提高模型对提示扰动的鲁棒性\n\n论文方法描述：\n1. 模型架构基于LLaDA-V扩散视觉语言模型，包含LLM骨干、视觉编码器和多模态投影器\n2. 采用模板锚定的填空式解码，保持推理过程的结构化\n3. 引入动态去噪策略，允许在固定掩码窗口内进行可变长度填充\n4. 使用两阶段训练流程：大规模对齐和结构化推理-动作监督微调\n5. 通过约束填充实现可控生成，防止用户意外控制模型\n\n论文使用数据集和训练资源：\n1. 使用nuScenes和Waymo Open Dataset End-to-End (WOD-E2E)进行验证\n2. 收集约145k个驾驶相关问答对进行对齐训练\n3. 构建53k个结构化推理-动作标注进行监督微调\n4. 具体使用nuScenes 23k和WOD-E2E 30k推理数据\n3. 采用LLaDA-8B-Instruct作为语言骨干，SigLIP2-so400m-patch14-384作为视觉编码器\n\n论文使用的评估环境和评估指标：\n1. 评估环境：nuScenes和WOD-E2E基准测试，在开环设置下进行\n\n评估指标：\n1. L2距离误差：在nuScenes上评估轨迹精度\n2. ADE（平均位移误差）：预测轨迹与真实轨迹的平均欧几里得距离\n3. RFS（评分者反馈分数）：评估预测轨迹与人类评分参考轨迹的一致性\n4. 碰撞率：评估安全性，越低越好\n5. 一致性指标：对象↔解释一致性和行为↔轨迹一致性\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>威斯康星大学麦迪逊分校，英伟达，斯坦福大学，约翰斯·霍普金斯大学</p>\n\n<p>论文概述：</p>\n<p>dVLM-AD是一种基于扩散模型的视觉语言模型，用于端到端自动驾驶。该研究旨在解决现有自回归视觉语言模型在自动驾驶中存在的推理-动作不一致和不可控生成问题。通过引入双向注意力机制和迭代去噪过程，dVLM-AD能够统一感知、结构化推理和低层规划，在复杂驾驶场景下实现更强的全局一致性和可控性。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出首个基于扩散模型的自动驾驶视觉语言模型，用双向迭代生成取代左到右解码</li><li>设计动态去噪策略，解决固定长度槽偏差问题</li><li>实现结构化可控推理框架，增强推理与动作之间的一致性</li><li>在长尾场景下显著提升规划性能，同时提高模型对提示扰动的鲁棒性</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>模型架构基于LLaDA-V扩散视觉语言模型，包含LLM骨干、视觉编码器和多模态投影器</li><li>采用模板锚定的填空式解码，保持推理过程的结构化</li><li>引入动态去噪策略，允许在固定掩码窗口内进行可变长度填充</li><li>使用两阶段训练流程：大规模对齐和结构化推理-动作监督微调</li><li>通过约束填充实现可控生成，防止用户意外控制模型</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>使用nuScenes和Waymo Open Dataset End-to-End (WOD-E2E)进行验证</li><li>收集约145k个驾驶相关问答对进行对齐训练</li><li>构建53k个结构化推理-动作标注进行监督微调</li><li>具体使用nuScenes 23k和WOD-E2E 30k推理数据</li><li>采用LLaDA-8B-Instruct作为语言骨干，SigLIP2-so400m-patch14-384作为视觉编码器</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>1. 评估环境：nuScenes和WOD-E2E基准测试，在开环设置下进行</p>\n\n<p>评估指标：</p>\n<ol><li>L2距离误差：在nuScenes上评估轨迹精度</li><li>ADE（平均位移误差）：预测轨迹与真实轨迹的平均欧几里得距离</li><li>RFS（评分者反馈分数）：评估预测轨迹与人类评分参考轨迹的一致性</li><li>碰撞率：评估安全性，越低越好</li><li>一致性指标：对象↔解释一致性和行为↔轨迹一致性</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-04",
    "title": "Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops",
    "link": "http://arxiv.org/abs/2512.04446",
    "summary_markdown": "```markdown\n论文研究单位：\n德克萨斯农工大学机械工程系，佛罗里达大学可持续基础设施与环境工程学院，德克萨斯农工大学土木与环境工程系\n\n论文概述：\n本研究评估了视觉-语言-动作模型在复杂机器人拆解任务中的可行性，以从报废台式机中提取关键组件为案例研究。研究聚焦于RAM模块移除和CPU支架解锁两个任务，通过微调OpenVLA和OpenVLA-OFT模型，探索端到端方法在接触丰富且需要精确操作的工业拆解场景中的应用潜力。\n\n论文核心贡献点：\n1. 构建了专门用于RAM和CPU拆解的定制化UR5e演示数据集\n2. 对两种成熟的VLA模型进行微调并评估其在复杂拆解任务中的表现\n3. 提出结合VLA与基于规则控制器的混合策略，成功实现完整拆解操作\n4. 分析了VLA模型在机器人拆解任务中的当前局限性，特别是精度和灵巧性方面的挑战\n\n论文方法描述：\n1. 数据收集：基于Gello系统构建远程操作平台，使用两个RGB-D相机（OAK-D和RealSense D435i）从不同视角记录拆解过程\n2. 模型微调：采用LoRA方法高效微调OpenVLA和OpenVLA-OFT模型\n3. 任务分解：将整个拆解任务划分为多个小步骤进行训练和评估\n4. 混合策略：当VLA模型无法完成精确操作时，切换至基于规则的位置控制器\n\n论文使用数据集和训练资源：\n数据集：使用10台不同配置的台式机收集287条拆解轨迹，其中RAM移除任务164条演示，CPU支架解锁任务123条演示\n硬件：UR5e机器人，Robotiq 2F-85夹具，OAK-D相机，RealSense D435i相机\n计算资源：德克萨斯农工大学高性能研究计算中心\n\n论文使用的评估环境和评估指标：\n评估环境：复现数据收集环境的测试设置，使用相同的相机配置和操作条件\n评估指标：\n- 对齐：夹具与目标模块的对齐能力\n- 接近：向目标模块移动的能力\n- 定位：达到精确操作位置的能力\n- 配置：保持正确夹具状态的能力\n- 驱动：完成最终拆解操作的能力\n通过20次实验对每个模型在各个子任务阶段的成功率进行评估\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>德克萨斯农工大学机械工程系，佛罗里达大学可持续基础设施与环境工程学院，德克萨斯农工大学土木与环境工程系</p>\n\n<p>论文概述：</p>\n<p>本研究评估了视觉-语言-动作模型在复杂机器人拆解任务中的可行性，以从报废台式机中提取关键组件为案例研究。研究聚焦于RAM模块移除和CPU支架解锁两个任务，通过微调OpenVLA和OpenVLA-OFT模型，探索端到端方法在接触丰富且需要精确操作的工业拆解场景中的应用潜力。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>构建了专门用于RAM和CPU拆解的定制化UR5e演示数据集</li><li>对两种成熟的VLA模型进行微调并评估其在复杂拆解任务中的表现</li><li>提出结合VLA与基于规则控制器的混合策略，成功实现完整拆解操作</li><li>分析了VLA模型在机器人拆解任务中的当前局限性，特别是精度和灵巧性方面的挑战</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>数据收集：基于Gello系统构建远程操作平台，使用两个RGB-D相机（OAK-D和RealSense D435i）从不同视角记录拆解过程</li><li>模型微调：采用LoRA方法高效微调OpenVLA和OpenVLA-OFT模型</li><li>任务分解：将整个拆解任务划分为多个小步骤进行训练和评估</li><li>混合策略：当VLA模型无法完成精确操作时，切换至基于规则的位置控制器</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<p>数据集：使用10台不同配置的台式机收集287条拆解轨迹，其中RAM移除任务164条演示，CPU支架解锁任务123条演示</p>\n<p>硬件：UR5e机器人，Robotiq 2F-85夹具，OAK-D相机，RealSense D435i相机</p>\n<p>计算资源：德克萨斯农工大学高性能研究计算中心</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：复现数据收集环境的测试设置，使用相同的相机配置和操作条件</p>\n<p>评估指标：</p>\n<ul><li>对齐：夹具与目标模块的对齐能力</li><li>接近：向目标模块移动的能力</li><li>定位：达到精确操作位置的能力</li><li>配置：保持正确夹具状态的能力</li><li>驱动：完成最终拆解操作的能力</li></ul>\n<p>通过20次实验对每个模型在各个子任务阶段的成功率进行评估</p>\n<p>```</p>"
  },
  {
    "date": "2025-12-03",
    "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations",
    "link": "http://arxiv.org/abs/2512.03913",
    "summary_markdown": "论文研究单位：韩国大学人工智能系，KAIST Kim Jaechul 人工智能研究生院，首尔国立大学航空航天工程系，韩国大学统计系，NAVER AI 实验室\n\n论文概述：论文提出 VINE，一种分层视觉语言动作模型，利用成功和失败演示进行训练。传统 VLA 模型通常仅使用成功演示，而丢弃失败数据。VINE 将高层推理与低层控制分离，在高层次进行可行性引导的树搜索，预测成功概率并修剪脆弱分支，低层执行选定的子目标序列。该方法在分层强化学习框架下，将失败数据作为结构化学习信号整合到决策循环中，提高操作任务的鲁棒性和成功率。\n\n论文核心贡献点：1. 利用离线失败数据训练系统 2 模型，整合到基于树的规划器中，对每个候选步骤预测成功概率进行评分。2. 引入基于 HRL 的分层 VLA 框架，将可行性感知的高层规划与低层动作执行分离。3. 在操作任务上相比强 VLA 基线展示了显著的成功率和鲁棒性提升。\n\n论文方法描述：VINE 采用双系统架构。系统 2 执行高层规划和推理，通过树搜索在 2D 场景图抽象上进行可行性引导。系统 1 执行低层控制动作。系统 2 包括节点和边缘生成、节点评估和树搜索算法。系统 1 包含动作专家和完成专家，分别生成动作块和检测子目标完成。系统 2 使用语言建模头自回归生成候选节点和边缘字符串，并通过值预测器估计节点成功概率。树搜索使用蒙特卡洛树搜索变体，用学习的状态值函数引导搜索。\n\n论文使用数据集和训练资源：使用包含成功和失败轨迹的混合质量数据集进行训练。具体包括插头插入环境、抽屉打包环境、简化环境和真实世界环境的数据集。模型基于预训练的 pi_0 构建，融合 PaliGemma 语言主干，使用 LoRA 适配器。训练使用离线遥操作数据，无需在线 rollout。\n\n论文使用的评估环境和评估指标：在模拟环境和真实世界环境中进行评估。评估指标包括成功率、鲁棒性、测试时间可扩展性。与统一 VLA 模型和 VLM 作为规划基线的比较，分析失败数据和树搜索的作用，并进行定性结果展示。",
    "summary_html": "<p>论文研究单位：韩国大学人工智能系，KAIST Kim Jaechul 人工智能研究生院，首尔国立大学航空航天工程系，韩国大学统计系，NAVER AI 实验室</p>\n\n<p>论文概述：论文提出 VINE，一种分层视觉语言动作模型，利用成功和失败演示进行训练。传统 VLA 模型通常仅使用成功演示，而丢弃失败数据。VINE 将高层推理与低层控制分离，在高层次进行可行性引导的树搜索，预测成功概率并修剪脆弱分支，低层执行选定的子目标序列。该方法在分层强化学习框架下，将失败数据作为结构化学习信号整合到决策循环中，提高操作任务的鲁棒性和成功率。</p>\n\n<p>论文核心贡献点：1. 利用离线失败数据训练系统 2 模型，整合到基于树的规划器中，对每个候选步骤预测成功概率进行评分。2. 引入基于 HRL 的分层 VLA 框架，将可行性感知的高层规划与低层动作执行分离。3. 在操作任务上相比强 VLA 基线展示了显著的成功率和鲁棒性提升。</p>\n\n<p>论文方法描述：VINE 采用双系统架构。系统 2 执行高层规划和推理，通过树搜索在 2D 场景图抽象上进行可行性引导。系统 1 执行低层控制动作。系统 2 包括节点和边缘生成、节点评估和树搜索算法。系统 1 包含动作专家和完成专家，分别生成动作块和检测子目标完成。系统 2 使用语言建模头自回归生成候选节点和边缘字符串，并通过值预测器估计节点成功概率。树搜索使用蒙特卡洛树搜索变体，用学习的状态值函数引导搜索。</p>\n\n<p>论文使用数据集和训练资源：使用包含成功和失败轨迹的混合质量数据集进行训练。具体包括插头插入环境、抽屉打包环境、简化环境和真实世界环境的数据集。模型基于预训练的 pi_0 构建，融合 PaliGemma 语言主干，使用 LoRA 适配器。训练使用离线遥操作数据，无需在线 rollout。</p>\n\n<p>论文使用的评估环境和评估指标：在模拟环境和真实世界环境中进行评估。评估指标包括成功率、鲁棒性、测试时间可扩展性。与统一 VLA 模型和 VLM 作为规划基线的比较，分析失败数据和树搜索的作用，并进行定性结果展示。</p>"
  },
  {
    "date": "2025-12-03",
    "title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention",
    "link": "http://arxiv.org/abs/2512.03724",
    "summary_markdown": "```markdown\n论文研究单位：\n- MBZUAI\n- AI2 Robotics\n- The University of Sydney\n- The University of Melbourne\n\n论文概述：\n本文提出PosA-VLA框架，通过姿态条件锚点注意力增强动作生成。针对现有视觉语言动作模型在复杂环境中产生冗余动作和轨迹不稳定的问题，通过将机器人末端执行器姿态与视觉注意力显式链接，引导模型关注任务相关区域，提高动作生成的精确性和效率。\n\n论文核心贡献点：\n- 对VLA不一致性的实证分析：识别出现有模型缺乏一致性和精确动作源于其空间均匀感知场。\n- 提出PosA-VLA框架：通过姿态条件监督锚定视觉注意力。\n- 高性能和效率：在多样化机器人操作基准测试中实现更高的成功率、更平滑的轨迹和更快的推理速度。\n\n论文方法描述：\n- 姿态条件锚点生成：从机器人末端执行器轨迹构建任务相关锚点和末端执行器锚点，使用高斯映射生成空间监督信号。\n- 姿态条件锚点损失：结合空间注意力损失和批次对比损失，使用Focal Loss处理前景背景不平衡问题。\n- 动作生成：采用流匹配变换器，通过ODE求解器生成连续动作序列。\n- 注意力精炼：通过锚点权重与DINOv2图像特征的元素级乘法获得精炼的视觉表示。\n\n论文使用数据集和训练资源：\n- 数据集：手动收集包含多个抓取任务的数据集，每个对象200个演示。\n- 训练资源：单张NVIDIA A100 GPU，批量大小16，训练20万步。\n- 推理环境：NVIDIA RTX 4090 GPU。\n\n论文使用的评估环境和评估指标：\n- 评估环境：AlphaBot 1s机器人平台，配备7自由度机械臂、头戴摄像头和腕戴摄像头。\n- 评估指标：抓取成功率、动作步骤数、推理时间、长时域任务成功率。\n- 测试条件：基本场景、未见背景、光照变化、干扰对象、未见对象等多样化场景。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<ul><li>MBZUAI</li><li>AI2 Robotics</li><li>The University of Sydney</li><li>The University of Melbourne</li></ul>\n\n<p>论文概述：</p>\n<p>本文提出PosA-VLA框架，通过姿态条件锚点注意力增强动作生成。针对现有视觉语言动作模型在复杂环境中产生冗余动作和轨迹不稳定的问题，通过将机器人末端执行器姿态与视觉注意力显式链接，引导模型关注任务相关区域，提高动作生成的精确性和效率。</p>\n\n<p>论文核心贡献点：</p>\n<ul><li>对VLA不一致性的实证分析：识别出现有模型缺乏一致性和精确动作源于其空间均匀感知场。</li><li>提出PosA-VLA框架：通过姿态条件监督锚定视觉注意力。</li><li>高性能和效率：在多样化机器人操作基准测试中实现更高的成功率、更平滑的轨迹和更快的推理速度。</li></ul>\n\n<p>论文方法描述：</p>\n<ul><li>姿态条件锚点生成：从机器人末端执行器轨迹构建任务相关锚点和末端执行器锚点，使用高斯映射生成空间监督信号。</li><li>姿态条件锚点损失：结合空间注意力损失和批次对比损失，使用Focal Loss处理前景背景不平衡问题。</li><li>动作生成：采用流匹配变换器，通过ODE求解器生成连续动作序列。</li><li>注意力精炼：通过锚点权重与DINOv2图像特征的元素级乘法获得精炼的视觉表示。</li></ul>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>数据集：手动收集包含多个抓取任务的数据集，每个对象200个演示。</li><li>训练资源：单张NVIDIA A100 GPU，批量大小16，训练20万步。</li><li>推理环境：NVIDIA RTX 4090 GPU。</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>评估环境：AlphaBot 1s机器人平台，配备7自由度机械臂、头戴摄像头和腕戴摄像头。</li><li>评估指标：抓取成功率、动作步骤数、推理时间、长时域任务成功率。</li><li>测试条件：基本场景、未见背景、光照变化、干扰对象、未见对象等多样化场景。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-03",
    "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation",
    "link": "http://arxiv.org/abs/2512.03538",
    "summary_markdown": "- 论文研究单位：国防科技大学、北京大学、深圳大学\n- 论文概述：提出adapower框架，通过轻量级适配将通用世界基础模型转化为专用世界模型，用于机器人预测性操作。该方法在推理时进行时空测试训练，并通过记忆持久性模块保持长时程一致性，与模型预测控制结合提升预训练视觉语言动作模型的零样本泛化能力\n- 论文核心贡献点：提出轻量级适配框架adapower；设计时空测试训练模块和记忆持久性模块解决测试时适应和长时程一致性问题；开发与预训练vla模型协作的mpc系统\n- 论文方法描述：基于扩散变换器架构，插入两个关键模块。时空测试训练模块通过自监督损失在时空维度进行优化，利用视频数据的低秩先验。记忆持久性模块通过dinov2编码器提取历史帧特征，利用交叉注意力融合历史上下文。最后将专用世界模型集成到模型预测控制框架中，与预训练vla模型协作规划动作序列\n- 论文使用数据集和训练资源：使用libero-90数据集的2000条轨迹进行仿真实验；真实世界实验使用franka research 3机械臂收集200个视频片段\n- 论文使用的评估环境和评估指标：在10个未见过的操作任务上评估，每个任务执行20次，报告平均任务成功率作为主要指标。在libero基准测试中任务成功率提升超过41%\n- 论文使用的评估环境和评估指标：在10个未见过的操作任务上评估，每个任务执行20次，报告平均任务成功率作为主要指标。在libero基准测试中任务成功率提升超过41%",
    "summary_html": "<ul><li>论文研究单位：国防科技大学、北京大学、深圳大学</li><li>论文概述：提出adapower框架，通过轻量级适配将通用世界基础模型转化为专用世界模型，用于机器人预测性操作。该方法在推理时进行时空测试训练，并通过记忆持久性模块保持长时程一致性，与模型预测控制结合提升预训练视觉语言动作模型的零样本泛化能力</li><li>论文核心贡献点：提出轻量级适配框架adapower；设计时空测试训练模块和记忆持久性模块解决测试时适应和长时程一致性问题；开发与预训练vla模型协作的mpc系统</li><li>论文方法描述：基于扩散变换器架构，插入两个关键模块。时空测试训练模块通过自监督损失在时空维度进行优化，利用视频数据的低秩先验。记忆持久性模块通过dinov2编码器提取历史帧特征，利用交叉注意力融合历史上下文。最后将专用世界模型集成到模型预测控制框架中，与预训练vla模型协作规划动作序列</li><li>论文使用数据集和训练资源：使用libero-90数据集的2000条轨迹进行仿真实验；真实世界实验使用franka research 3机械臂收集200个视频片段</li><li>论文使用的评估环境和评估指标：在10个未见过的操作任务上评估，每个任务执行20次，报告平均任务成功率作为主要指标。在libero基准测试中任务成功率提升超过41%</li><li>论文使用的评估环境和评估指标：在10个未见过的操作任务上评估，每个任务执行20次，报告平均任务成功率作为主要指标。在libero基准测试中任务成功率提升超过41%</li></ul>"
  },
  {
    "date": "2025-12-02",
    "title": "Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling",
    "link": "http://arxiv.org/abs/2512.03044",
    "summary_markdown": "```markdown\n论文研究单位：\n北京大学多媒体信息处理国家重点实验室、AI2 Robotics、中山大学、武汉大学、香港科技大学\n\n论文概述：\nVideo2Act是一种双系统视频扩散策略，通过机器人时空运动建模来增强机器人策略学习。该框架从视频扩散模型中显式提取空间和运动感知表示，并采用异步双系统设计，其中VDM作为慢速感知系统，DiT作为快速执行系统，实现高效且稳定的动作生成。\n\n论文核心贡献点：\n1. 系统分析了VDM在机器人环境中的表示，揭示了其捕获稳定结构和运动一致性特征的能力\n2. 提出了Video2Act框架，通过空间滤波算子和FFT显式集成时空运动表示\n3. 开发了异步双系统策略，使模型能够在高频输入下生成自适应和稳定的动作\n\n论文方法描述：\n1. 采用基于反演的特征提取策略，从VDM早期反演阶段提取干净特征信号\n2. 设计两个互补的视频流：高分辨率输入用于空间滤波，长时序输入用于运动提取\n3. 空间表示使用Sobel空间滤波算子提取结构边界\n4. 运动表示使用快速傅里叶变换捕获跨帧的连贯运动模式\n4. 异步双系统设计：VDM作为System 2提供低频时空特征，DiT作为System 1进行高频动作生成\n\n论文使用数据集和训练资源：\n1. 仿真实验使用RoboTwin双手机器人操作基准\n2. 真实世界实验使用ALOHA双手机器人系统\n3. 每个任务收集100个专家演示\n4. 使用Hunyuan视频扩散模型和SigLIP-ViT-L/14图像编码器\n5. 采用1B参数的扩散变压器作为动作头\n\n论文使用的评估环境和评估指标：\n1. 仿真环境：基于Sapien模拟器的RoboTwin环境\n2. 真实世界环境：Agilex Cobot Magic平台\n3. 评估指标：平均成功率\n4. 在仿真中超越先前最先进方法7.7%，在真实世界任务中超越21.7%\n5. 操作频率比从1:1到1:16进行测试\n6. 每个任务进行50次评估，重复三次确保鲁棒性\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>北京大学多媒体信息处理国家重点实验室、AI2 Robotics、中山大学、武汉大学、香港科技大学</p>\n\n<p>论文概述：</p>\n<p>Video2Act是一种双系统视频扩散策略，通过机器人时空运动建模来增强机器人策略学习。该框架从视频扩散模型中显式提取空间和运动感知表示，并采用异步双系统设计，其中VDM作为慢速感知系统，DiT作为快速执行系统，实现高效且稳定的动作生成。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>系统分析了VDM在机器人环境中的表示，揭示了其捕获稳定结构和运动一致性特征的能力</li><li>提出了Video2Act框架，通过空间滤波算子和FFT显式集成时空运动表示</li><li>开发了异步双系统策略，使模型能够在高频输入下生成自适应和稳定的动作</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>采用基于反演的特征提取策略，从VDM早期反演阶段提取干净特征信号</li><li>设计两个互补的视频流：高分辨率输入用于空间滤波，长时序输入用于运动提取</li><li>空间表示使用Sobel空间滤波算子提取结构边界</li><li>运动表示使用快速傅里叶变换捕获跨帧的连贯运动模式</li><li>异步双系统设计：VDM作为System 2提供低频时空特征，DiT作为System 1进行高频动作生成</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>仿真实验使用RoboTwin双手机器人操作基准</li><li>真实世界实验使用ALOHA双手机器人系统</li><li>每个任务收集100个专家演示</li><li>使用Hunyuan视频扩散模型和SigLIP-ViT-L/14图像编码器</li><li>采用1B参数的扩散变压器作为动作头</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>仿真环境：基于Sapien模拟器的RoboTwin环境</li><li>真实世界环境：Agilex Cobot Magic平台</li><li>评估指标：平均成功率</li><li>在仿真中超越先前最先进方法7.7%，在真实世界任务中超越21.7%</li><li>操作频率比从1:1到1:16进行测试</li><li>每个任务进行50次评估，重复三次确保鲁棒性</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-02",
    "title": "VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling",
    "link": "http://arxiv.org/abs/2512.02902",
    "summary_markdown": "论文研究单位：中山大学、广东省大数据分析与处理重点实验室、X-Era AI Lab\n\n论文概述：该论文重新评估了预训练视觉-语言-动作模型的泛化能力，发现其在面对新相机视角和视觉扰动时的性能下降主要源于空间建模中的表征错位，而非物理建模的局限性。论文提出了一种单次自适应框架，通过轻量级可学习更新来重新校准视觉表征，显著提升了模型在视角变化和视觉扰动下的鲁棒性。\n\n论文核心贡献点：重新评估了预训练VLA模型的鲁棒性，提出统一单次鲁棒性自适应框架，包含特征令牌调制和特征线性自适应两种方法，在Libero基准测试中实现了对未见相机视角和视觉扰动的最新泛化性能，揭示了现有VLA模型中未开发的鲁棒性潜力。\n\n论文方法描述：提出了两种轻量级自适应方法。特征令牌调制方法对视觉令牌嵌入应用全局仿射变换，仅需4K参数。特征线性自适应方法通过低秩更新对ViT编码器的线性层进行调整，使用4.7M参数。这两种方法都专注于对预训练视觉模块的轻量级自适应，而不改变其架构或数据分布。\n\n论文使用数据集和训练资源：构建了Libero-V基准测试，整合了来自Libero-Plus的不同程度的视角和视觉扰动。实验在单个NVIDIA A100 GPU上进行，使用批量大小为32，FTM和FLA自适应过程使用Adam优化器训练2000步。\n\n论文使用的评估环境和评估指标：在Libero基准测试的四个任务套件上评估新相机视角下的性能，使用成功率指标。在Libero-V基准测试中评估四种受控扰动类型：相机视角、光照、背景纹理和视觉噪声。",
    "summary_html": "<p>论文研究单位：中山大学、广东省大数据分析与处理重点实验室、X-Era AI Lab</p>\n\n<p>论文概述：该论文重新评估了预训练视觉-语言-动作模型的泛化能力，发现其在面对新相机视角和视觉扰动时的性能下降主要源于空间建模中的表征错位，而非物理建模的局限性。论文提出了一种单次自适应框架，通过轻量级可学习更新来重新校准视觉表征，显著提升了模型在视角变化和视觉扰动下的鲁棒性。</p>\n\n<p>论文核心贡献点：重新评估了预训练VLA模型的鲁棒性，提出统一单次鲁棒性自适应框架，包含特征令牌调制和特征线性自适应两种方法，在Libero基准测试中实现了对未见相机视角和视觉扰动的最新泛化性能，揭示了现有VLA模型中未开发的鲁棒性潜力。</p>\n\n<p>论文方法描述：提出了两种轻量级自适应方法。特征令牌调制方法对视觉令牌嵌入应用全局仿射变换，仅需4K参数。特征线性自适应方法通过低秩更新对ViT编码器的线性层进行调整，使用4.7M参数。这两种方法都专注于对预训练视觉模块的轻量级自适应，而不改变其架构或数据分布。</p>\n\n<p>论文使用数据集和训练资源：构建了Libero-V基准测试，整合了来自Libero-Plus的不同程度的视角和视觉扰动。实验在单个NVIDIA A100 GPU上进行，使用批量大小为32，FTM和FLA自适应过程使用Adam优化器训练2000步。</p>\n\n<p>论文使用的评估环境和评估指标：在Libero基准测试的四个任务套件上评估新相机视角下的性能，使用成功率指标。在Libero-V基准测试中评估四种受控扰动类型：相机视角、光照、背景纹理和视觉噪声。</p>"
  },
  {
    "date": "2025-12-02",
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "link": "http://arxiv.org/abs/2512.02834",
    "summary_markdown": "```markdown\n论文研究单位：\n- 中国电信人工智能研究院\n- 中国科学技术大学\n- 清华大学\n- 香港科技大学\n\n论文概述：\n该论文提出了一种测试时扩展框架TACO，用于解决视觉语言动作模型在微调后存在推理不稳定的问题。这种不稳定性源于预训练阶段吸收的冗余动作模式与下游任务成功模式之间的分布偏移。TACO通过轻量级伪计数估计器作为动作块的高保真验证器，在推理过程中选择具有最大伪计数的动作，从而防止分布偏移同时保留VLA的泛化能力。\n\n论文核心贡献点：\n- 提出TACO测试时扩展框架，在保留预训练VLA强泛化能力的同时，有效约束输出到特定下游任务的成功模式。\n- 为CFN引入高效的内部表示机制进行伪计数估计，以最小计算开销准确测量分布偏移。\n- 在仿真和真实世界任务中显著提高多种VLA模型的成功率，无需长时间训练且可低延迟运行。\n\n论文方法描述：\n- 将推理不稳定性建模为分布外问题，采用反探索原则约束生成动作位于SFT数据集中成功模式的支持范围内。\n- 使用耦合的伪计数估计器，将CFN实例化为轻量级MLP头，以VLA内部表示作为输入。\n- 针对基于去噪的VLA，提出高保真特征搜索程序，找到最能代表干净动作的分布内特征。\n- 在推理时采用两阶段生成-验证流程：首先生成多个候选动作块及其内部表示，然后使用训练好的CFN作为验证器选择伪计数最高的动作执行。\n- 通过共享观察键值缓存优化计算效率，重用视觉语言表示减少延迟。\n\n论文使用数据集和训练资源：\n- 使用四个仿真基准测试：RoboTwin2.0、Robotwin、LIBERO、SimplerEnv，涵盖64个任务。\n- 在双臂平台上进行真实世界实验，包含5个任务。\n- 在流匹配和基于扩散的VLA策略上进行评估，包括π0、π0.5和OpenVLA等模型。\n\n论文使用的评估环境和评估指标：\n- 评估环境：仿真基准测试和真实世界双臂平台。\n- 评估指标：任务成功率，通过比较不同噪声向量下的成功率变化验证稳定性改进。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<ul><li>中国电信人工智能研究院</li><li>中国科学技术大学</li><li>清华大学</li><li>香港科技大学</li></ul>\n\n<p>论文概述：</p>\n<p>该论文提出了一种测试时扩展框架TACO，用于解决视觉语言动作模型在微调后存在推理不稳定的问题。这种不稳定性源于预训练阶段吸收的冗余动作模式与下游任务成功模式之间的分布偏移。TACO通过轻量级伪计数估计器作为动作块的高保真验证器，在推理过程中选择具有最大伪计数的动作，从而防止分布偏移同时保留VLA的泛化能力。</p>\n\n<p>论文核心贡献点：</p>\n<ul><li>提出TACO测试时扩展框架，在保留预训练VLA强泛化能力的同时，有效约束输出到特定下游任务的成功模式。</li><li>为CFN引入高效的内部表示机制进行伪计数估计，以最小计算开销准确测量分布偏移。</li><li>在仿真和真实世界任务中显著提高多种VLA模型的成功率，无需长时间训练且可低延迟运行。</li></ul>\n\n<p>论文方法描述：</p>\n<ul><li>将推理不稳定性建模为分布外问题，采用反探索原则约束生成动作位于SFT数据集中成功模式的支持范围内。</li><li>使用耦合的伪计数估计器，将CFN实例化为轻量级MLP头，以VLA内部表示作为输入。</li><li>针对基于去噪的VLA，提出高保真特征搜索程序，找到最能代表干净动作的分布内特征。</li><li>在推理时采用两阶段生成-验证流程：首先生成多个候选动作块及其内部表示，然后使用训练好的CFN作为验证器选择伪计数最高的动作执行。</li><li>通过共享观察键值缓存优化计算效率，重用视觉语言表示减少延迟。</li></ul>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>使用四个仿真基准测试：RoboTwin2.0、Robotwin、LIBERO、SimplerEnv，涵盖64个任务。</li><li>在双臂平台上进行真实世界实验，包含5个任务。</li><li>在流匹配和基于扩散的VLA策略上进行评估，包括π0、π0.5和OpenVLA等模型。</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li>评估环境：仿真基准测试和真实世界双臂平台。</li><li>评估指标：任务成功率，通过比较不同噪声向量下的成功率变化验证稳定性改进。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-12-02",
    "title": "Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols",
    "link": "http://arxiv.org/abs/2512.02787",
    "summary_markdown": "```markdown\n论文研究单位：\n北京航空航天大学，上海创新研究院，南方科技大学，上海交通大学\n\n论文概述：\n本文提出ViFailback框架，用于诊断机器人操作失败并提供文本和视觉校正指导。该框架利用显式视觉符号提升标注效率，并发布了包含58,126个视觉问答对和5,202条真实世界操作轨迹的大规模数据集。基于该数据集建立了ViFailback-Bench基准，包含11个细粒度VQA任务，用于评估视觉语言模型的失败诊断和校正能力。通过微调Qwen3-VL-8B构建ViFailback-8B模型，在基准测试中表现优异，并能生成视觉符号用于校正动作指导。最后通过将ViFailback-8B与VLA模型集成，在真实世界机器人实验中展示其协助模型从失败中恢复的能力。\n\n论文核心贡献点：\n1. 提出ViFailback框架，通过视觉符号高效标注真实世界机器人失败视频，提供文本和视觉校正指导。\n2. 发布包含58,126个高质量VQA对的数据集，涵盖11种不同问题类型。\n3. 建立ViFailback-Bench基准，全面评估VLMs在机器人失败诊断和校正方面的能力。\n3. 实验证明ViFailback在提升通用VLMs对机器人失败的诊断和校正能力方面具有有效性，且能提高策略从失败中恢复的能力。\n\n论文方法描述：\n1. 设计7种视觉符号，分为运动符号（彩色直线箭头、半圆箭头）、空间关系符号（双十字准线、十字准线）和状态符号（开/关标签、禁止图标、倒带图标）。\n2. 将失败分析分为失败诊断和校正动作指导两个关键组件。\n3. 提出多阶段数据标注流程：基础任务语义信息填充、文本指导选择和视觉符号绘制、开放描述生成和精炼。\n3. 构建ViFailback-8B模型，通过LoRA微调Qwen3-VL-8B实现。\n4. 将ViFailback-8B部署为外部监督器，在机器人任务执行期间进行干预以从失败中恢复。\n\n论文使用数据集和训练资源：\n1. 收集5,202条机器人操作轨迹及其对应的自我中心视频，覆盖100个不同的真实世界任务。\n2. 使用ALOHA双臂遥操作平台收集轨迹，包括657条成功和4,545条失败轨迹。\n3. 训练使用4个NVIDIA Hopper GPU，采用deepspeed zero2阶段确保训练稳定性。\n4. 使用LoRA微调，秩为32，缩放因子α为64，训练1个epoch。\n\n论文使用的评估环境和评估指标：\n评估环境：\n1. ViFailback-Bench基准，包含500条轨迹，覆盖22个不同任务。\n2. 基准分为ViFailback-Bench Lite（封闭式VQA）和ViFailback-Bench Hard（开放式VQA）。\n\n评估指标：\n1. 多项选择题：使用正确回答样本的百分比作为准确率指标。\n2. 开放式问题：使用基于GPT-4o的评估器，从语义相似性、内容完整性和功能等价性三个维度评估生成输出质量，最终计算总分。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>北京航空航天大学，上海创新研究院，南方科技大学，上海交通大学</p>\n\n<p>论文概述：</p>\n<p>本文提出ViFailback框架，用于诊断机器人操作失败并提供文本和视觉校正指导。该框架利用显式视觉符号提升标注效率，并发布了包含58,126个视觉问答对和5,202条真实世界操作轨迹的大规模数据集。基于该数据集建立了ViFailback-Bench基准，包含11个细粒度VQA任务，用于评估视觉语言模型的失败诊断和校正能力。通过微调Qwen3-VL-8B构建ViFailback-8B模型，在基准测试中表现优异，并能生成视觉符号用于校正动作指导。最后通过将ViFailback-8B与VLA模型集成，在真实世界机器人实验中展示其协助模型从失败中恢复的能力。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出ViFailback框架，通过视觉符号高效标注真实世界机器人失败视频，提供文本和视觉校正指导。</li><li>发布包含58,126个高质量VQA对的数据集，涵盖11种不同问题类型。</li><li>建立ViFailback-Bench基准，全面评估VLMs在机器人失败诊断和校正方面的能力。</li><li>实验证明ViFailback在提升通用VLMs对机器人失败的诊断和校正能力方面具有有效性，且能提高策略从失败中恢复的能力。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>设计7种视觉符号，分为运动符号（彩色直线箭头、半圆箭头）、空间关系符号（双十字准线、十字准线）和状态符号（开/关标签、禁止图标、倒带图标）。</li><li>将失败分析分为失败诊断和校正动作指导两个关键组件。</li><li>提出多阶段数据标注流程：基础任务语义信息填充、文本指导选择和视觉符号绘制、开放描述生成和精炼。</li><li>构建ViFailback-8B模型，通过LoRA微调Qwen3-VL-8B实现。</li><li>将ViFailback-8B部署为外部监督器，在机器人任务执行期间进行干预以从失败中恢复。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>收集5,202条机器人操作轨迹及其对应的自我中心视频，覆盖100个不同的真实世界任务。</li><li>使用ALOHA双臂遥操作平台收集轨迹，包括657条成功和4,545条失败轨迹。</li><li>训练使用4个NVIDIA Hopper GPU，采用deepspeed zero2阶段确保训练稳定性。</li><li>使用LoRA微调，秩为32，缩放因子α为64，训练1个epoch。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<ol><li>ViFailback-Bench基准，包含500条轨迹，覆盖22个不同任务。</li><li>基准分为ViFailback-Bench Lite（封闭式VQA）和ViFailback-Bench Hard（开放式VQA）。</li></ol>\n\n<p>评估指标：</p>\n<ol><li>多项选择题：使用正确回答样本的百分比作为准确率指标。</li><li>开放式问题：使用基于GPT-4o的评估器，从语义相似性、内容完整性和功能等价性三个维度评估生成输出质量，最终计算总分。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-02",
    "title": "RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning",
    "link": "http://arxiv.org/abs/2512.02729",
    "summary_markdown": "论文研究单位：清华大学、Synapath Research、CUHK、HKU、PolyU\n\n论文概述：RoboWheel是一个数据引擎，能够将真实世界的人类手-物交互视频转换为可用于跨形态机器人学习的训练数据。该工作从单目RGB(D)输入中执行高精度HOI重建，并通过强化学习优化器在接触和穿透约束下改进手-物相对姿态，确保物理合理性。重建的接触丰富轨迹随后被重定向到不同的机器人形态，包括简单末端执行器的机械臂、灵巧手和人形机器人。为了扩展数据覆盖范围，在Isaac Sim中构建了模拟增强框架，包含多种领域随机化策略。整个数据管道形成了从视频→重建→重定向→增强→数据采集的端到端流程。\n\n论文核心贡献点：\n1. 提出物理合理的HOI重建和跨领域重定向方法，集成先进的手、身体和物体运动估计与基于物理的优化，支持灵活的跨形态重定向，输出可执行运动轨迹，为下游学习提供可扩展的监督信号\n2. 实现模拟增强的数据飞轮，在主流VLA和模仿学习模型上验证了HOI转换和增强策略的质量和有效性\n3. 构建大规模多模态数据集HORA，包含超过15万条轨迹，结合多视角运动捕捉、单目视频和公共HOI语料库，为机器人学习和下游HOI相关任务提供丰富且可扩展的资源\n\n论文方法描述：\n1. HOI重建：从RGB(D)视频中恢复手和物体的参数化表示，包括手部运动恢复、物体重建和姿态估计，并通过优化确保物理合理性\n2. 跨形态重定向：将重建轨迹重定向到异构机器人形态，包括机械臂、灵巧手和人形机器人\n3. 数据增强：在模拟环境中应用多种增强策略，包括不同形态的机械臂重定向、物体检索和替换、轨迹增强等\n4. 系统还包括重放评估和语言描述生成，使用Qwen2.5 VL进行自动任务评估\n\n论文使用数据集和训练资源：\n1. 数据集：构建HORA数据集，包含三个子集：自定义多视角运动捕捉系统（配备触觉传感器手套）、RGB(D) HOI记录设置和多个公共HOI数据集\n2. 训练资源：使用Isaac Sim进行模拟增强，采用GPU加速的并行逆运动学，使用cuRobo的IK后端\n\n论文使用的评估环境和评估指标：\n1. 评估环境：真实机器人实验使用UR5机械臂配备平行夹具；在Isaac Sim中进行模拟重放\n3. 评估指标：HOI重建质量评估使用Chamfer距离、F-score、手部抖动、世界坐标系MPJPE等指标\n4. 在真实世界任务上评估性能，按难度分组报告成功率",
    "summary_html": "<p>论文研究单位：清华大学、Synapath Research、CUHK、HKU、PolyU</p>\n\n<p>论文概述：RoboWheel是一个数据引擎，能够将真实世界的人类手-物交互视频转换为可用于跨形态机器人学习的训练数据。该工作从单目RGB(D)输入中执行高精度HOI重建，并通过强化学习优化器在接触和穿透约束下改进手-物相对姿态，确保物理合理性。重建的接触丰富轨迹随后被重定向到不同的机器人形态，包括简单末端执行器的机械臂、灵巧手和人形机器人。为了扩展数据覆盖范围，在Isaac Sim中构建了模拟增强框架，包含多种领域随机化策略。整个数据管道形成了从视频→重建→重定向→增强→数据采集的端到端流程。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出物理合理的HOI重建和跨领域重定向方法，集成先进的手、身体和物体运动估计与基于物理的优化，支持灵活的跨形态重定向，输出可执行运动轨迹，为下游学习提供可扩展的监督信号</li><li>实现模拟增强的数据飞轮，在主流VLA和模仿学习模型上验证了HOI转换和增强策略的质量和有效性</li><li>构建大规模多模态数据集HORA，包含超过15万条轨迹，结合多视角运动捕捉、单目视频和公共HOI语料库，为机器人学习和下游HOI相关任务提供丰富且可扩展的资源</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>HOI重建：从RGB(D)视频中恢复手和物体的参数化表示，包括手部运动恢复、物体重建和姿态估计，并通过优化确保物理合理性</li><li>跨形态重定向：将重建轨迹重定向到异构机器人形态，包括机械臂、灵巧手和人形机器人</li><li>数据增强：在模拟环境中应用多种增强策略，包括不同形态的机械臂重定向、物体检索和替换、轨迹增强等</li><li>系统还包括重放评估和语言描述生成，使用Qwen2.5 VL进行自动任务评估</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>数据集：构建HORA数据集，包含三个子集：自定义多视角运动捕捉系统（配备触觉传感器手套）、RGB(D) HOI记录设置和多个公共HOI数据集</li><li>训练资源：使用Isaac Sim进行模拟增强，采用GPU加速的并行逆运动学，使用cuRobo的IK后端</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：真实机器人实验使用UR5机械臂配备平行夹具；在Isaac Sim中进行模拟重放</li><li>评估指标：HOI重建质量评估使用Chamfer距离、F-score、手部抖动、世界坐标系MPJPE等指标</li><li>在真实世界任务上评估性能，按难度分组报告成功率</li></ol>"
  },
  {
    "date": "2025-12-01",
    "title": "ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.02013",
    "summary_markdown": "论文研究单位：\n北京大学，香港中文大学，Simplexity Robotics\n\n论文概述：\n该论文提出ManualVLA，一种基于混合变换器架构的统一视觉-语言-动作模型，用于解决具有预定义目标状态的长时程任务，如乐高组装和物体重排。该方法通过生成多模态手册（包含图像、位置提示和文本指令）来指导机器人动作执行，采用链式思维推理过程将目标状态转化为可执行程序。\n\n论文核心贡献点：\n提出ManualVLA统一VLA模型，基于MoT架构支持多模态手册生成和动作执行；设计手动链式思维推理过程，将生成的手册转化为精确动作；开发基于3D高斯泼溅的数字孪生工具包，自动生成手册数据；在长时程任务上实现比现有SOTA基线高32%的平均成功率。\n\n论文方法描述：\n采用混合变换器架构，包含规划专家和动作专家两个模块。规划专家生成包含文本描述、目标坐标和子目标图像的多模态手册。通过显式CoT推理（构建提示图像）和隐式CoT推理（在潜在空间中使用手册特征作为条件）来实现手册和动作生成。使用扩散策略进行动作建模，采用三阶段训练策略：动作专家预训练、手册专家预训练、联合手册-动作微调。\n\n论文使用数据集和训练资源：\n预训练使用超过400K轨迹的大规模开源机器人数据集。下游任务为每个任务收集100个演示轨迹。训练使用8个NVIDIA H20 GPU。\n\n论文使用的评估环境和评估指标：\n在双臂Franka机器人平台上进行真实世界实验，在RLBench基准上进行仿真实验。评估指标包括子目标图像的PSNR和FID，位置坐标的MAE，以及长时程任务的成功率和中间步骤成功率。",
    "summary_html": "<p>论文研究单位：</p>\n<p>北京大学，香港中文大学，Simplexity Robotics</p>\n\n<p>论文概述：</p>\n<p>该论文提出ManualVLA，一种基于混合变换器架构的统一视觉-语言-动作模型，用于解决具有预定义目标状态的长时程任务，如乐高组装和物体重排。该方法通过生成多模态手册（包含图像、位置提示和文本指令）来指导机器人动作执行，采用链式思维推理过程将目标状态转化为可执行程序。</p>\n\n<p>论文核心贡献点：</p>\n<p>提出ManualVLA统一VLA模型，基于MoT架构支持多模态手册生成和动作执行；设计手动链式思维推理过程，将生成的手册转化为精确动作；开发基于3D高斯泼溅的数字孪生工具包，自动生成手册数据；在长时程任务上实现比现有SOTA基线高32%的平均成功率。</p>\n\n<p>论文方法描述：</p>\n<p>采用混合变换器架构，包含规划专家和动作专家两个模块。规划专家生成包含文本描述、目标坐标和子目标图像的多模态手册。通过显式CoT推理（构建提示图像）和隐式CoT推理（在潜在空间中使用手册特征作为条件）来实现手册和动作生成。使用扩散策略进行动作建模，采用三阶段训练策略：动作专家预训练、手册专家预训练、联合手册-动作微调。</p>\n\n<p>论文使用数据集和训练资源：</p>\n<p>预训练使用超过400K轨迹的大规模开源机器人数据集。下游任务为每个任务收集100个演示轨迹。训练使用8个NVIDIA H20 GPU。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>在双臂Franka机器人平台上进行真实世界实验，在RLBench基准上进行仿真实验。评估指标包括子目标图像的PSNR和FID，位置坐标的MAE，以及长时程任务的成功率和中间步骤成功率。</p>"
  },
  {
    "date": "2025-12-01",
    "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
    "link": "http://arxiv.org/abs/2512.01801",
    "summary_markdown": "```markdown\n论文研究单位：ByteDance Seed\n\n论文概述：GR-RL是一个机器人学习框架，旨在将通用的视觉-语言-动作策略转化为能够执行长周期灵巧操作的专业化策略。该框架通过多阶段训练流程对存在噪声和次优问题的人类演示数据进行过滤、增强和强化学习，以解决毫米级精度控制、可变形物体操作和长周期鲁棒性等挑战性问题。\n\n论文核心贡献点：\n1. 提出基于强化学习的任务进度评估器，使用离线强化学习训练的Q值作为鲁棒的任务进度函数\n2. 引入形态对称增强方法，通过镜像机器动作和观察提升策略泛化能力\n3. 开发在线潜在空间引导方法，通过噪声预测器实现策略部署对齐\n4. 在鞋带穿眼任务上达到83.3%的成功率，这是首个能够自主完成鞋带穿眼任务的学习型策略\n\n论文方法描述：\n1. 采用混合Transformer架构，包含5B参数的视觉-语言-动作模型和多任务评论家\n2. 使用TD3+BC算法训练评论家，采用分布评论家处理稀疏奖励场景\n3. 通过形态对称增强，水平翻转图像观察，交换左右手腕图像，转换本体感觉状态和动作\n4. 在线强化学习阶段，在潜在空间进行结构化探索，学习噪声预测器来引导策略\n4. 结合离线策略缓冲区和在线策略缓冲区进行样本高效训练\n\n论文使用数据集和训练资源：\n1. 使用GR-3作为基础模型，该模型通过互联网数据、机器人轨迹和人类演示训练\n2. 采用Qwen2.5-VL-3B-Instruct作为视觉语言模型骨干\n5. 使用ByteMini-v2机器人进行验证，该机器人配备7自由度双机械臂和移动底座\n3. 训练数据包含成功和失败轨迹，通过后视标注创建更多失败轨迹\n\n论文使用的评估环境和评估指标：\n1. 评估环境：鞋带穿眼任务，涉及长周期、灵巧和精确操作\n2. 评估指标：二进制稀疏奖励设置，仅在鞋带正确穿过眼孔并完全放在桌上时获得正奖励\n3. 成功率和各阶段完成率，包括拾取正确鞋带、穿入正确眼孔、交接给另一个夹爪和拉紧鞋带\n4. 对比实验包括分布评论家与非分布评论家、回归基线与RL基线的比较\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：ByteDance Seed</p>\n\n<p>论文概述：GR-RL是一个机器人学习框架，旨在将通用的视觉-语言-动作策略转化为能够执行长周期灵巧操作的专业化策略。该框架通过多阶段训练流程对存在噪声和次优问题的人类演示数据进行过滤、增强和强化学习，以解决毫米级精度控制、可变形物体操作和长周期鲁棒性等挑战性问题。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出基于强化学习的任务进度评估器，使用离线强化学习训练的Q值作为鲁棒的任务进度函数</li><li>引入形态对称增强方法，通过镜像机器动作和观察提升策略泛化能力</li><li>开发在线潜在空间引导方法，通过噪声预测器实现策略部署对齐</li><li>在鞋带穿眼任务上达到83.3%的成功率，这是首个能够自主完成鞋带穿眼任务的学习型策略</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>采用混合Transformer架构，包含5B参数的视觉-语言-动作模型和多任务评论家</li><li>使用TD3+BC算法训练评论家，采用分布评论家处理稀疏奖励场景</li><li>通过形态对称增强，水平翻转图像观察，交换左右手腕图像，转换本体感觉状态和动作</li><li>在线强化学习阶段，在潜在空间进行结构化探索，学习噪声预测器来引导策略</li><li>结合离线策略缓冲区和在线策略缓冲区进行样本高效训练</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>使用GR-3作为基础模型，该模型通过互联网数据、机器人轨迹和人类演示训练</li><li>采用Qwen2.5-VL-3B-Instruct作为视觉语言模型骨干</li><li>使用ByteMini-v2机器人进行验证，该机器人配备7自由度双机械臂和移动底座</li><li>训练数据包含成功和失败轨迹，通过后视标注创建更多失败轨迹</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：鞋带穿眼任务，涉及长周期、灵巧和精确操作</li><li>评估指标：二进制稀疏奖励设置，仅在鞋带正确穿过眼孔并完全放在桌上时获得正奖励</li><li>成功率和各阶段完成率，包括拾取正确鞋带、穿入正确眼孔、交接给另一个夹爪和拉紧鞋带</li><li>对比实验包括分布评论家与非分布评论家、回归基线与RL基线的比较</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-12-01",
    "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
    "link": "http://arxiv.org/abs/2512.01715",
    "summary_markdown": "论文研究单位\n北京大学，BeingBeyond，中国人民大学\n\n论文概述\nDiG-Flow提出一种通过几何正则化增强视觉-语言-动作模型鲁棒性的框架。该框架利用观测特征和动作嵌入之间的分布差异作为几何信号，通过计算经验分布之间的差异，将其映射为调制权重，并在流匹配前对观测特征应用残差更新，从而提升模型在分布偏移和复杂多步任务上的性能。\n\n论文核心贡献点\n提出基于差异引导的流匹配框架，通过几何正则化增强VLA模型的鲁棒性。提供理论保证，证明差异引导训练可降低训练目标，且引导推理细化具有收敛性。在仿真和真实机器人实验中验证了方法的有效性，尤其在复杂任务和数据有限场景下表现突出。\n\n论文方法描述\n方法包含三个核心组件：差异函数量化观测和动作分布之间的距离，默认使用Wasserstein距离；单调权重映射将差异转换为调制因子；轻量级残差算子调整观测特征。训练时使用真实动作计算差异并生成门控权重，对特征进行残差调整后输入流匹配头。推理时可选择迭代细化方案，通过多次编码预测动作并重新计算差异，逐步优化动作生成。\n\n论文使用数据集和训练资源\n使用LIBERO基准和RoboCasa基准进行仿真实验。训练细节未具体说明，但提到方法可无缝集成到现有架构中，计算开销可忽略。\n\n论文使用的评估环境和评估指标\n在仿真环境（LIBERO和RoboCasa）和真实机器人环境中进行评估。评估指标包括任务成功率，重点关注分布偏移下的鲁棒性、复杂多步任务性能以及少样本学习场景。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京大学，BeingBeyond，中国人民大学</p>\n\n<p>论文概述</p>\n<p>DiG-Flow提出一种通过几何正则化增强视觉-语言-动作模型鲁棒性的框架。该框架利用观测特征和动作嵌入之间的分布差异作为几何信号，通过计算经验分布之间的差异，将其映射为调制权重，并在流匹配前对观测特征应用残差更新，从而提升模型在分布偏移和复杂多步任务上的性能。</p>\n\n<p>论文核心贡献点</p>\n<p>提出基于差异引导的流匹配框架，通过几何正则化增强VLA模型的鲁棒性。提供理论保证，证明差异引导训练可降低训练目标，且引导推理细化具有收敛性。在仿真和真实机器人实验中验证了方法的有效性，尤其在复杂任务和数据有限场景下表现突出。</p>\n\n<p>论文方法描述</p>\n<p>方法包含三个核心组件：差异函数量化观测和动作分布之间的距离，默认使用Wasserstein距离；单调权重映射将差异转换为调制因子；轻量级残差算子调整观测特征。训练时使用真实动作计算差异并生成门控权重，对特征进行残差调整后输入流匹配头。推理时可选择迭代细化方案，通过多次编码预测动作并重新计算差异，逐步优化动作生成。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用LIBERO基准和RoboCasa基准进行仿真实验。训练细节未具体说明，但提到方法可无缝集成到现有架构中，计算开销可忽略。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>在仿真环境（LIBERO和RoboCasa）和真实机器人环境中进行评估。评估指标包括任务成功率，重点关注分布偏移下的鲁棒性、复杂多步任务性能以及少样本学习场景。</p>"
  },
  {
    "date": "2025-11-30",
    "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "link": "http://arxiv.org/abs/2512.01031",
    "summary_markdown": "```markdown\n论文研究单位：\nmit，nvidia，tsinghua university，uc berkeley，ucsd，caltech\n\n论文概述：\nvlash是一个针对视觉语言动作模型的通用异步推理框架，旨在解决实时部署中存在的动作停顿和延迟反应问题。通过未来状态感知机制，vlash在保持原始精度的同时显著提升推理速度和反应速度。\n\n论文核心贡献点：\n提出未来状态感知的异步推理方法，通过状态前滚操作预测执行时的机器人状态，从而弥合预测与执行之间的时间错位。该方法无需额外开销或架构修改，能够实现平滑、准确和快速反应的控制。\n\n论文方法描述：\n1. 未来状态感知：在推理时使用之前生成的动作块将机器人状态前滚，获得执行时的状态估计。\n2. 带偏移的状态和动作微调：在训练阶段引入时间偏移增强，使模型能够处理不同推理延迟下的状态对齐。\n3. 共享观测的高效微调：通过块稀疏自注意力机制复用观测令牌，提升训练效率。\n3. 动作量化：将细粒度微动作分组为更粗粒度的宏动作，加速机器人运动。\n\n论文使用数据集和训练资源：\n1. 仿真评估：kinetix基准测试和libero基准测试（包含spatial、object、goal和libero-10四个子基准）。\n2. 真实世界评估：使用galaxea r1 lite和lerobot so-101机器人平台。\n3. 训练资源：使用4×h100 gpu进行分布式训练，有效批处理大小为64。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：仿真环境（kinetix和libero）和真实世界机器人平台。\n2. 评估指标：成功率、任务完成时间、反应延迟、平均执行步数。\n3. 硬件配置：rtx 5090、rtx 4090和rtx 5070 gpu。\n4. 性能指标：在kinetix上达到81.7%成功率，在libero上保持97%以上成功率，同时实现最高2.03倍加速和17.4倍反应延迟降低。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>mit，nvidia，tsinghua university，uc berkeley，ucsd，caltech</p>\n\n<p>论文概述：</p>\n<p>vlash是一个针对视觉语言动作模型的通用异步推理框架，旨在解决实时部署中存在的动作停顿和延迟反应问题。通过未来状态感知机制，vlash在保持原始精度的同时显著提升推理速度和反应速度。</p>\n\n<p>论文核心贡献点：</p>\n<p>提出未来状态感知的异步推理方法，通过状态前滚操作预测执行时的机器人状态，从而弥合预测与执行之间的时间错位。该方法无需额外开销或架构修改，能够实现平滑、准确和快速反应的控制。</p>\n\n<p>论文方法描述：</p>\n<ol><li>未来状态感知：在推理时使用之前生成的动作块将机器人状态前滚，获得执行时的状态估计。</li><li>带偏移的状态和动作微调：在训练阶段引入时间偏移增强，使模型能够处理不同推理延迟下的状态对齐。</li><li>共享观测的高效微调：通过块稀疏自注意力机制复用观测令牌，提升训练效率。</li><li>动作量化：将细粒度微动作分组为更粗粒度的宏动作，加速机器人运动。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>仿真评估：kinetix基准测试和libero基准测试（包含spatial、object、goal和libero-10四个子基准）。</li><li>真实世界评估：使用galaxea r1 lite和lerobot so-101机器人平台。</li><li>训练资源：使用4×h100 gpu进行分布式训练，有效批处理大小为64。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：仿真环境（kinetix和libero）和真实世界机器人平台。</li><li>评估指标：成功率、任务完成时间、反应延迟、平均执行步数。</li><li>硬件配置：rtx 5090、rtx 4090和rtx 5070 gpu。</li><li>性能指标：在kinetix上达到81.7%成功率，在libero上保持97%以上成功率，同时实现最高2.03倍加速和17.4倍反应延迟降低。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-11-30",
    "title": "CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding",
    "link": "http://arxiv.org/abs/2512.01022",
    "summary_markdown": "```markdown\n论文研究单位：\n中山大学\n香港中文大学（深圳）\n\n论文概述：\n本论文研究机器人操作中的一个重要但尚未充分探索的任务：基于循环的操作，即机器人需要执行周期性或重复性动作并在预期终止时间停止。这些任务在日常生活中很常见，例如摇晃瓶子或敲钉子。论文提出CycleManip框架，通过有效的历史感知和理解实现循环任务操作，同时建立了一个循环任务操作基准。\n\n论文核心贡献点：\n1. 提出CycleManip框架，以端到端模仿方式实现循环任务操作，无需额外模型、层次结构或显著计算开销\n2. 引入循环任务操作基准，提供多样化的循环任务和自动评估方法\n3. 框架通过成本感知采样策略增强历史感知，通过多任务学习改进历史理解\n4. 在仿真和真实环境中进行广泛实验验证\n\n论文方法描述：\n1. 有效历史感知：采用成本感知采样策略，对高开销观测（如视觉输入）进行稀疏采样，对低开销观测（如本体感知）进行密集采样\n2. 有效历史理解：通过多任务学习策略，引入预测当前进程阶段的辅助任务，鼓励模型学习进展区分性特征\n3. 框架架构：使用CLIP编码器编码语言特征，点编码器编码高观测特征，Transformer编码器编码低开销观测\n4. 使用DDIM作为扩散采样器，联合优化动作预测和辅助任务预测\n\n论文使用数据集和训练资源：\n1. 基于RoboTwin 2.0平台构建基准，包含8个循环操作任务环境\n2. 每个任务收集200条专家演示轨迹，循环次数从1到8\n3. 训练使用单个RTX 4090 GPU，批量大小为128，训练300个周期\n\n论文使用的评估环境和评估指标：\n评估环境：\n- 仿真环境：CycleManip基准和RoboTwin 2.0基准\n- 真实环境：多种异构机器人平台，包括单臂和双臂夹爪、灵巧手和人形机器人\n- 视觉观测使用Intel RealSense L515深度相机\n\n评估指标：\n1. 成功率（Suc.）：衡量任务成功完成且达到所需循环次数的比例\n2. 循环计数偏差（Cyc.）：量化执行与真实循环计数之间的平均绝对偏差\n3. 仿真实验进行100次试验，真实世界实验进行16次试验\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>中山大学</p>\n<p>香港中文大学（深圳）</p>\n\n<p>论文概述：</p>\n<p>本论文研究机器人操作中的一个重要但尚未充分探索的任务：基于循环的操作，即机器人需要执行周期性或重复性动作并在预期终止时间停止。这些任务在日常生活中很常见，例如摇晃瓶子或敲钉子。论文提出CycleManip框架，通过有效的历史感知和理解实现循环任务操作，同时建立了一个循环任务操作基准。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出CycleManip框架，以端到端模仿方式实现循环任务操作，无需额外模型、层次结构或显著计算开销</li><li>引入循环任务操作基准，提供多样化的循环任务和自动评估方法</li><li>框架通过成本感知采样策略增强历史感知，通过多任务学习改进历史理解</li><li>在仿真和真实环境中进行广泛实验验证</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>有效历史感知：采用成本感知采样策略，对高开销观测（如视觉输入）进行稀疏采样，对低开销观测（如本体感知）进行密集采样</li><li>有效历史理解：通过多任务学习策略，引入预测当前进程阶段的辅助任务，鼓励模型学习进展区分性特征</li><li>框架架构：使用CLIP编码器编码语言特征，点编码器编码高观测特征，Transformer编码器编码低开销观测</li><li>使用DDIM作为扩散采样器，联合优化动作预测和辅助任务预测</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>基于RoboTwin 2.0平台构建基准，包含8个循环操作任务环境</li><li>每个任务收集200条专家演示轨迹，循环次数从1到8</li><li>训练使用单个RTX 4090 GPU，批量大小为128，训练300个周期</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<ul><li>仿真环境：CycleManip基准和RoboTwin 2.0基准</li><li>真实环境：多种异构机器人平台，包括单臂和双臂夹爪、灵巧手和人形机器人</li><li>视觉观测使用Intel RealSense L515深度相机</li></ul>\n\n<p>评估指标：</p>\n<ol><li>成功率（Suc.）：衡量任务成功完成且达到所需循环次数的比例</li><li>循环计数偏差（Cyc.）：量化执行与真实循环计数之间的平均绝对偏差</li><li>仿真实验进行100次试验，真实世界实验进行16次试验</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-11-30",
    "title": "MM-ACT: Learn from Multimodal Parallel Generation to Act",
    "link": "http://arxiv.org/abs/2512.00975",
    "summary_markdown": "```markdown\n论文研究单位：上海人工智能实验室、上海交通大学、香港大学、中国科学技术大学、复旦大学、浙江大学\n\n论文概述：MM-ACT是一种统一的视觉-语言-动作模型，通过共享离散标记空间和并行解码策略，在单一架构中联合生成文本、图像和机器人动作。该模型采用上下文共享的多模态学习方法，利用跨模态学习增强动作生成能力，旨在构建具备语义理解和环境交互能力的通用机器人策略。\n\n论文核心贡献点：\n1. 提出统一的视觉-语言-动作模型架构，使用共享离散标记空间和双向注意力机制\n2. 引入上下文共享多模态学习训练范式，在共享上下文中监督三种模态的生成\n3. 采用并行解码策略：文本和图像使用重掩码并行解码，动作使用一步并行解码以提高效率\n4. 通过跨模态学习实现动作生成增强，在域外性能上获得显著提升\n\n论文方法描述：\n1. 模型设计：基于Transformer的掩码标记预测器，使用模态特定分词器将文本、图像和动作转换为共享空间中的离散标记\n2. 上下文共享多模态输入：为三种模态生成任务使用共享上下文，包含机器人当前视图、任务指令、文本描述和机器人状态\n3. 统一优化目标：采用相同的掩码标记预测优化目标训练所有三种模态的生成\n3. 并行解码策略：文本和图像采用重掩码并行解码，动作采用一步并行解码\n4. 两阶段训练策略：第一阶段仅训练文本和图像生成，第二阶段联合训练动作生成并保持其他模态能力\n\n论文使用数据集和训练资源：\n1. 数据集：LIBERO仿真基准（包含空间推理、物体中心理解、目标条件变化和长视野组合任务）、RoboTwin2.0双手机器人操作基准、Franka真实机器人实验数据\n2. 训练细节：批量大小128，动作块大小8，LIBERO每个子基准约11k步，RoboTwin八任务约27k步，Franka真实世界任务约8k步\n3. 基础模型：基于MMaDA离散扩散模型权重\n\n论文使用的评估环境和评估指标：\n1. 评估环境：LIBERO仿真环境、RoboTwin2.0双手机器人仿真环境、Franka真实机器人实验环境\n2. 评估指标：成功率（LIBERO和RoboTwin任务）、图像质量评估指标（PSNR、SSIM、LPIPS）、文本生成质量评估（使用LLM判断准确率）\n3. 具体结果：LIBERO平均成功率96.3%，RoboTwin八任务平均成功率52.38%，Franka真实世界任务平均成功率72.0%，跨模态学习带来9.25%的性能提升\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：上海人工智能实验室、上海交通大学、香港大学、中国科学技术大学、复旦大学、浙江大学</p>\n\n<p>论文概述：MM-ACT是一种统一的视觉-语言-动作模型，通过共享离散标记空间和并行解码策略，在单一架构中联合生成文本、图像和机器人动作。该模型采用上下文共享的多模态学习方法，利用跨模态学习增强动作生成能力，旨在构建具备语义理解和环境交互能力的通用机器人策略。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出统一的视觉-语言-动作模型架构，使用共享离散标记空间和双向注意力机制</li><li>引入上下文共享多模态学习训练范式，在共享上下文中监督三种模态的生成</li><li>采用并行解码策略：文本和图像使用重掩码并行解码，动作使用一步并行解码以提高效率</li><li>通过跨模态学习实现动作生成增强，在域外性能上获得显著提升</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>模型设计：基于Transformer的掩码标记预测器，使用模态特定分词器将文本、图像和动作转换为共享空间中的离散标记</li><li>上下文共享多模态输入：为三种模态生成任务使用共享上下文，包含机器人当前视图、任务指令、文本描述和机器人状态</li><li>统一优化目标：采用相同的掩码标记预测优化目标训练所有三种模态的生成</li><li>并行解码策略：文本和图像采用重掩码并行解码，动作采用一步并行解码</li><li>两阶段训练策略：第一阶段仅训练文本和图像生成，第二阶段联合训练动作生成并保持其他模态能力</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>数据集：LIBERO仿真基准（包含空间推理、物体中心理解、目标条件变化和长视野组合任务）、RoboTwin2.0双手机器人操作基准、Franka真实机器人实验数据</li><li>训练细节：批量大小128，动作块大小8，LIBERO每个子基准约11k步，RoboTwin八任务约27k步，Franka真实世界任务约8k步</li><li>基础模型：基于MMaDA离散扩散模型权重</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：LIBERO仿真环境、RoboTwin2.0双手机器人仿真环境、Franka真实机器人实验环境</li><li>评估指标：成功率（LIBERO和RoboTwin任务）、图像质量评估指标（PSNR、SSIM、LPIPS）、文本生成质量评估（使用LLM判断准确率）</li><li>具体结果：LIBERO平均成功率96.3%，RoboTwin八任务平均成功率52.38%，Franka真实世界任务平均成功率72.0%，跨模态学习带来9.25%的性能提升</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-11-30",
    "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
    "link": "http://arxiv.org/abs/2512.00903",
    "summary_markdown": "论文研究单位\ngigaai，北京大学，魔芯（湖州）科技有限公司，清华大学，x-humanoid\n\n论文概述\n论文提出swiftvla，一种轻量级视觉语言动作模型，旨在以最小开销增强模型的时空动态理解能力。该方法通过整合4d时空信息，在保持设计效率的同时提升紧凑模型的感知能力。\n\n论文核心贡献点\n提出swiftvla方法，通过掩码重建训练策略将4d知识蒸馏到vla中，使模型在推理时无需4d输入仍能保持性能。引入可学习的融合令牌，通过机器人末端执行器未来轨迹监督实现跨模态融合。在仿真和真实环境实验中证明swiftvla性能可媲美参数量大7倍的基准模型，在边缘设备上实现18倍加速和12倍内存减少。\n\n论文方法描述\n采用预训练的4d视觉几何变换器，通过时间缓存增量提取4d特征。引入融合令牌与2d和4d特征交互，通过未来轨迹预测监督实现跨模态对齐。使用掩码重建策略，在训练时随机掩码2d或4d特征，要求模型重建被掩码的特征。动作专家采用条件扩散模型生成动作。\n\n论文使用数据集和训练资源\n使用公共数据集进行预训练，包括robomind数据集。模型总参数量约4.5亿，其中动作专家模块约1亿参数。采用两阶段训练过程，具体细节在附录中说明。\n\n论文使用的评估环境和评估指标\n在roboTwin 2.0和libero基准上进行评估。评估指标包括成功率（sr）和平均轨迹长度。在边缘设备部署测试使用nvidia jetson orin平台。",
    "summary_html": "<p>论文研究单位</p>\n<p>gigaai，北京大学，魔芯（湖州）科技有限公司，清华大学，x-humanoid</p>\n\n<p>论文概述</p>\n<p>论文提出swiftvla，一种轻量级视觉语言动作模型，旨在以最小开销增强模型的时空动态理解能力。该方法通过整合4d时空信息，在保持设计效率的同时提升紧凑模型的感知能力。</p>\n\n<p>论文核心贡献点</p>\n<p>提出swiftvla方法，通过掩码重建训练策略将4d知识蒸馏到vla中，使模型在推理时无需4d输入仍能保持性能。引入可学习的融合令牌，通过机器人末端执行器未来轨迹监督实现跨模态融合。在仿真和真实环境实验中证明swiftvla性能可媲美参数量大7倍的基准模型，在边缘设备上实现18倍加速和12倍内存减少。</p>\n\n<p>论文方法描述</p>\n<p>采用预训练的4d视觉几何变换器，通过时间缓存增量提取4d特征。引入融合令牌与2d和4d特征交互，通过未来轨迹预测监督实现跨模态对齐。使用掩码重建策略，在训练时随机掩码2d或4d特征，要求模型重建被掩码的特征。动作专家采用条件扩散模型生成动作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用公共数据集进行预训练，包括robomind数据集。模型总参数量约4.5亿，其中动作专家模块约1亿参数。采用两阶段训练过程，具体细节在附录中说明。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>在roboTwin 2.0和libero基准上进行评估。评估指标包括成功率（sr）和平均轨迹长度。在边缘设备部署测试使用nvidia jetson orin平台。</p>"
  },
  {
    "date": "2025-11-30",
    "title": "Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment",
    "link": "http://arxiv.org/abs/2512.00783",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-28",
    "title": "LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.23034",
    "summary_markdown": "```markdown\n论文研究单位：\n中国科学院微电子研究所，中国科学院大学，微软研究院\n\n论文概述：\n提出LatBot框架，一种通用潜在动作学习框架，用于视觉-语言-动作模型。该框架从大规模物体操作视频中学习可迁移的潜在动作，通过任务指令和多帧输入指导，联合优化未来帧重建和动作序列预测，从而增强下游机器人任务的泛化能力。\n\n论文核心贡献点：\n1. 设计解耦的潜在动作表示，将潜在动作分解为运动令牌和场景令牌，区分机器人主动运动和环境变化\n2. 提出统一解码器，通过双向交互融合场景和运动信息，联合生成未来帧和帧间动作\n3. 提出知识蒸馏策略，将潜在动作知识迁移到VLA模型中，同时保留其推理能力\n\n论文方法描述：\n1. 使用预训练视觉语言模型作为潜在动作编码器，提取场景表示和运动表示\n2. 统一解码器基于融合的场景和运动特征解码未来视觉帧和帧间动作\n3. 知识蒸馏阶段设计潜在动作对齐损失和推理保留损失，平衡动作对齐与推理能力保留\n\n论文使用数据集和训练资源：\n使用OXE、AgiBoT和EgoDex数据集，包含100万个视频片段。在16个NVIDIA A100 GPU上训练，LAM预训练14天，蒸馏阶段7天。\n\n论文使用的评估环境和评估指标：\n在SIMPLER和LIBERO模拟环境以及真实世界Franka机器人上进行评估，使用成功率作为主要评估指标。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>中国科学院微电子研究所，中国科学院大学，微软研究院</p>\n\n<p>论文概述：</p>\n<p>提出LatBot框架，一种通用潜在动作学习框架，用于视觉-语言-动作模型。该框架从大规模物体操作视频中学习可迁移的潜在动作，通过任务指令和多帧输入指导，联合优化未来帧重建和动作序列预测，从而增强下游机器人任务的泛化能力。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>设计解耦的潜在动作表示，将潜在动作分解为运动令牌和场景令牌，区分机器人主动运动和环境变化</li><li>提出统一解码器，通过双向交互融合场景和运动信息，联合生成未来帧和帧间动作</li><li>提出知识蒸馏策略，将潜在动作知识迁移到VLA模型中，同时保留其推理能力</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>使用预训练视觉语言模型作为潜在动作编码器，提取场景表示和运动表示</li><li>统一解码器基于融合的场景和运动特征解码未来视觉帧和帧间动作</li><li>知识蒸馏阶段设计潜在动作对齐损失和推理保留损失，平衡动作对齐与推理能力保留</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<p>使用OXE、AgiBoT和EgoDex数据集，包含100万个视频片段。在16个NVIDIA A100 GPU上训练，LAM预训练14天，蒸馏阶段7天。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>在SIMPLER和LIBERO模拟环境以及真实世界Franka机器人上进行评估，使用成功率作为主要评估指标。</p>\n<p>```</p>"
  },
  {
    "date": "2025-11-28",
    "title": "RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video",
    "link": "http://arxiv.org/abs/2511.22950",
    "summary_markdown": "论文研究单位：Show Lab，新加坡国立大学\n\n论文概述：该论文提出RobotSeg，一个用于图像和视频中机器人分割的基础模型，并构建了视频机器人分割（VRS）数据集。该模型解决了现有分割模型在机器人分割任务中面临的结构复杂性、手动提示依赖性和标注效率低的问题，在图像和视频上均实现了最先进的性能。\n\n论文核心贡献点：\n1. 提出首个支持图像和视频的机器人分割基础模型RobotSeg，支持机器人手臂、夹具和整机细粒度分割，并提供可提示功能。\n2. 构建首个视频机器人分割基准VRS，包含2,812个视频（138,707帧），涵盖多种机器人形态和场景。\n3. 设计包含结构增强记忆关联器、机器人提示生成器和标签高效训练策略的新框架，实现结构感知、自动化和标签高效的视频机器人分割。\n\n论文方法描述：\n1. 结构增强记忆关联器：整合时序上下文和结构信息，通过边缘检测和多尺度特征提取增强关节机器人的特征表示。\n2. 机器人提示生成器：通过分层聚类策略生成类标记和对象标记，实现无需手动提示的自主分割。\n3. 标签高效训练策略：仅使用首帧标注，通过前向-后向循环一致性损失、语义一致性损失和块一致性损失进行监督训练。\n\n论文使用数据集和训练资源：\n1. 数据集：VRS数据集（2,812个视频，138,707帧）和RoboEngine数据集（3,629张图像）。\n2. 训练资源：8个NVIDIA RTX A5000 GPU（24GB内存），训练时间15小时。\n\n论文使用的评估环境和评估指标：\n1. 评估环境：在RoboEngine测试集和VRS测试集上评估，设置自动、1点击、3点击、边界框和在线交互五种评估设置。\n2. 评估指标：使用J和F指标（区域相似性和边界精度的组合）进行评估。",
    "summary_html": "<p>论文研究单位：Show Lab，新加坡国立大学</p>\n\n<p>论文概述：该论文提出RobotSeg，一个用于图像和视频中机器人分割的基础模型，并构建了视频机器人分割（VRS）数据集。该模型解决了现有分割模型在机器人分割任务中面临的结构复杂性、手动提示依赖性和标注效率低的问题，在图像和视频上均实现了最先进的性能。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出首个支持图像和视频的机器人分割基础模型RobotSeg，支持机器人手臂、夹具和整机细粒度分割，并提供可提示功能。</li><li>构建首个视频机器人分割基准VRS，包含2,812个视频（138,707帧），涵盖多种机器人形态和场景。</li><li>设计包含结构增强记忆关联器、机器人提示生成器和标签高效训练策略的新框架，实现结构感知、自动化和标签高效的视频机器人分割。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>结构增强记忆关联器：整合时序上下文和结构信息，通过边缘检测和多尺度特征提取增强关节机器人的特征表示。</li><li>机器人提示生成器：通过分层聚类策略生成类标记和对象标记，实现无需手动提示的自主分割。</li><li>标签高效训练策略：仅使用首帧标注，通过前向-后向循环一致性损失、语义一致性损失和块一致性损失进行监督训练。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>数据集：VRS数据集（2,812个视频，138,707帧）和RoboEngine数据集（3,629张图像）。</li><li>训练资源：8个NVIDIA RTX A5000 GPU（24GB内存），训练时间15小时。</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>评估环境：在RoboEngine测试集和VRS测试集上评估，设置自动、1点击、3点击、边界框和在线交互五种评估设置。</li><li>评估指标：使用J和F指标（区域相似性和边界精度的组合）进行评估。</li></ol>"
  },
  {
    "date": "2025-11-27",
    "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.22780",
    "summary_markdown": "```markdown\n论文研究单位：\n华为技术加拿大公司\n\n论文概述：\n本研究提出了一种评估协议，用于检验机器人在杂乱场景中的操作策略性能。与以往工作不同，本研究从心理物理学角度出发，采用统一的杂乱度度量方法，综合考虑环境因素以及干扰物的数量、特征和排列。利用该度量方法，在超真实模拟环境和真实世界中系统构建评估场景，并对视觉语言动作模型进行广泛实验。实验结果表明场景杂乱显著降低了策略性能，最多降低34%，尽管不同VLA策略在各项任务中平均性能相似，但它们具有独特的脆弱性，在成功场景上的一致性较低。\n\n论文核心贡献点：\n1. 提出了一种新颖的机器人操作策略评估协议，从心理物理学角度分析杂乱场景影响\n2. 引入双视角特征拥塞度量方法，综合考虑视觉杂乱和操作复杂性\n3. 系统构建了包含不同数量、类型和排列干扰物的评估场景\n4. 在模拟和真实环境中进行大规模评估，分析杂乱对VLA模型性能的影响\n5. 探索了数据增强策略对缓解杂乱负面影响的有限效果\n\n论文方法描述：\n1. 采用心理物理学视角，使用统一的杂乱度度量方法\n2. 提出双视角特征拥塞度量方法，结合机器人视角和俯视图的度量，同时捕捉策略视角的视觉杂乱和操作复杂性\n3. 从SIMPLER基础场景中采样，从61个YCB干扰物中随机选择1-12个对象，在机器人操作空间内随机放置\n4. 在超真实模拟器SIMPLER和真实环境中进行实验\n5. 通过微调策略在真实世界干扰数据上，评估数据增强对缓解杂乱影响的效果\n\n论文使用数据集和训练资源：\n1. 数据集：使用61个YCB对象作为干扰物集\n2. 模拟环境：SIMPLER超真实模拟器\n3. 评估模型：5个最先进的VLA模型，包括Octo、OpenVLA、SpatialVLA、π₀和CogACT\n4. 训练数据：Bridge数据集、Fractal数据集以及两者组合\n5. 实验场景：生成6000个场景，包含不同数量和排列的干扰物\n\n论文使用的评估环境和评估指标：\n评估环境：\n- 模拟环境：SIMPLER超真实模拟器\n- 真实环境：使用UR5e机器人操作器\n\n评估指标：\n- 成功率：完成任务的比例\n- 硬成功率：无任何碰撞的成功案例\n- 碰撞率：发生碰撞的场景比例\n- 抓取失败率：无法抓取目标的比例\n- 效率率：完成任务所需步数与允许总步数的比值\n- 双视角特征拥塞度量值：综合评估场景杂乱程度\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>华为技术加拿大公司</p>\n\n<p>论文概述：</p>\n<p>本研究提出了一种评估协议，用于检验机器人在杂乱场景中的操作策略性能。与以往工作不同，本研究从心理物理学角度出发，采用统一的杂乱度度量方法，综合考虑环境因素以及干扰物的数量、特征和排列。利用该度量方法，在超真实模拟环境和真实世界中系统构建评估场景，并对视觉语言动作模型进行广泛实验。实验结果表明场景杂乱显著降低了策略性能，最多降低34%，尽管不同VLA策略在各项任务中平均性能相似，但它们具有独特的脆弱性，在成功场景上的一致性较低。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出了一种新颖的机器人操作策略评估协议，从心理物理学角度分析杂乱场景影响</li><li>引入双视角特征拥塞度量方法，综合考虑视觉杂乱和操作复杂性</li><li>系统构建了包含不同数量、类型和排列干扰物的评估场景</li><li>在模拟和真实环境中进行大规模评估，分析杂乱对VLA模型性能的影响</li><li>探索了数据增强策略对缓解杂乱负面影响的有限效果</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>采用心理物理学视角，使用统一的杂乱度度量方法</li><li>提出双视角特征拥塞度量方法，结合机器人视角和俯视图的度量，同时捕捉策略视角的视觉杂乱和操作复杂性</li><li>从SIMPLER基础场景中采样，从61个YCB干扰物中随机选择1-12个对象，在机器人操作空间内随机放置</li><li>在超真实模拟器SIMPLER和真实环境中进行实验</li><li>通过微调策略在真实世界干扰数据上，评估数据增强对缓解杂乱影响的效果</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>数据集：使用61个YCB对象作为干扰物集</li><li>模拟环境：SIMPLER超真实模拟器</li><li>评估模型：5个最先进的VLA模型，包括Octo、OpenVLA、SpatialVLA、π₀和CogACT</li><li>训练数据：Bridge数据集、Fractal数据集以及两者组合</li><li>实验场景：生成6000个场景，包含不同数量和排列的干扰物</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<ul><li>模拟环境：SIMPLER超真实模拟器</li><li>真实环境：使用UR5e机器人操作器</li></ul>\n\n<p>评估指标：</p>\n<ul><li>成功率：完成任务的比例</li><li>硬成功率：无任何碰撞的成功案例</li><li>碰撞率：发生碰撞的场景比例</li><li>抓取失败率：无法抓取目标的比例</li><li>效率率：完成任务所需步数与允许总步数的比值</li><li>双视角特征拥塞度量值：综合评估场景杂乱程度</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-11-27",
    "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery",
    "link": "http://arxiv.org/abs/2511.22777",
    "summary_markdown": "```markdown\n论文研究单位：\n华为技术加拿大公司\n\n论文概述：\n本论文提出了一种名为NICE（自然修复上下文增强）的框架，旨在通过场景编辑增强机器人操作数据，提高视觉运动策略在存在视觉干扰物环境中的鲁棒性。该方法利用图像生成框架和大语言模型对现有演示数据进行三种编辑操作：对象替换、重新样式化和移除干扰对象，从而减少模仿学习中的分布外差距，无需额外收集机器人数据或访问模拟器。\n\n论文核心贡献点：\n- 提出NICE框架，通过场景级手术增强机器人数据，提高策略对干扰物的鲁棒性\n- 评估NICE数据在背景一致性和生成质量方面的真实性\n- 展示NICE数据在提高高度杂乱场景中视觉可供性预测准确性的优势\n- 通过广泛的真实世界实验验证NICE数据能提高机器人操作策略在不同任务和环境中的鲁棒性和安全性\n\n论文方法描述：\nNICE框架包含两个阶段：场景分解与角色分配、场景编辑。\n首先使用Florence-2 VLM进行对象检测，SAM-2进行精确分割，识别目标和干扰对象。\n然后执行三种编辑操作：\n- 对象移除：使用LaMa修复模型填充干扰对象区域\n- 对象重新样式化：从可描述纹理数据集中采样纹理并应用到对象表面\n- 对象替换：使用Deepseek-r1:7b生成对象描述，通过Stable Diffusion修复模型替换干扰对象\n\n论文使用数据集和训练资源：\n- 使用Bridge数据作为基础数据集\n- 采用Florence-2进行对象检测\n- 使用SAM-2进行对象分割\n- 应用LaMa和Stable Diffusion进行图像修复\n- 利用Describable Textures Dataset进行纹理样式化\n- 使用Deepseek-r1:7b语言模型生成对象描述\n\n论文使用的评估环境和评估指标：\n评估环境：\n- 真实世界机器人操作场景\n- 不同杂乱程度的环境（低、中、高杂乱）\n- 测试场景包含0到16个干扰对象\n\n评估指标：\n- 结构相似性指数用于评估背景一致性\n- Fréchet Inception Distance用于评估数据生成真实性\n- 平均预测准确率用于评估空间可供性预测\n- 成功率、碰撞率和目标混淆率用于评估操作性能\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：</p>\n<p>华为技术加拿大公司</p>\n\n<p>论文概述：</p>\n<p>本论文提出了一种名为NICE（自然修复上下文增强）的框架，旨在通过场景编辑增强机器人操作数据，提高视觉运动策略在存在视觉干扰物环境中的鲁棒性。该方法利用图像生成框架和大语言模型对现有演示数据进行三种编辑操作：对象替换、重新样式化和移除干扰对象，从而减少模仿学习中的分布外差距，无需额外收集机器人数据或访问模拟器。</p>\n\n<p>论文核心贡献点：</p>\n<ul><li>提出NICE框架，通过场景级手术增强机器人数据，提高策略对干扰物的鲁棒性</li><li>评估NICE数据在背景一致性和生成质量方面的真实性</li><li>展示NICE数据在提高高度杂乱场景中视觉可供性预测准确性的优势</li><li>通过广泛的真实世界实验验证NICE数据能提高机器人操作策略在不同任务和环境中的鲁棒性和安全性</li></ul>\n\n<p>论文方法描述：</p>\n<p>NICE框架包含两个阶段：场景分解与角色分配、场景编辑。</p>\n<p>首先使用Florence-2 VLM进行对象检测，SAM-2进行精确分割，识别目标和干扰对象。</p>\n<p>然后执行三种编辑操作：</p>\n<ul><li>对象移除：使用LaMa修复模型填充干扰对象区域</li><li>对象重新样式化：从可描述纹理数据集中采样纹理并应用到对象表面</li><li>对象替换：使用Deepseek-r1:7b生成对象描述，通过Stable Diffusion修复模型替换干扰对象</li></ul>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li>使用Bridge数据作为基础数据集</li><li>采用Florence-2进行对象检测</li><li>使用SAM-2进行对象分割</li><li>应用LaMa和Stable Diffusion进行图像修复</li><li>利用Describable Textures Dataset进行纹理样式化</li><li>使用Deepseek-r1:7b语言模型生成对象描述</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：</p>\n<ul><li>真实世界机器人操作场景</li><li>不同杂乱程度的环境（低、中、高杂乱）</li><li>测试场景包含0到16个干扰对象</li></ul>\n\n<p>评估指标：</p>\n<ul><li>结构相似性指数用于评估背景一致性</li><li>Fréchet Inception Distance用于评估数据生成真实性</li><li>平均预测准确率用于评估空间可供性预测</li><li>成功率、碰撞率和目标混淆率用于评估操作性能</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-11-27",
    "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations",
    "link": "http://arxiv.org/abs/2511.22697",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-27",
    "title": "Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention",
    "link": "http://arxiv.org/abs/2511.22555",
    "summary_markdown": "论文研究单位：吉林大学，微软亚洲研究院\n\n论文概述：该论文针对视觉语言动作模型中由于混合质量演示数据导致的执行质量不一致问题，提出了一种非侵入式的优雅执行精炼框架。通过构建LIBERO-Elegant基准测试，明确定义了优雅执行的标准，并开发了基于及时干预机制的分离式评估架构，在不修改基础VLA策略的情况下提升机器人操作质量。\n\n论文核心贡献点：\n1. 将优雅执行形式化为满足隐式任务约束\n2. 构建LIBERO-Elegant基准测试，用于评估超越二元成功的执行质量\n3. 提出分离式精炼框架，包含优雅评论家和及时干预机制\n4. 通过仿真和真实环境实验验证了方法的有效性和泛化能力\n\n论文方法描述：\n方法采用三阶段框架：\n1. 基础生成策略训练：使用流匹配生成模型学习人类行为分布\n2. 离线优雅评论家训练：基于校准Q学习训练轻量级评论家\n3. 及时干预机制：通过监测评论家置信度，仅在决策关键时刻进行选择性干预\n4. 优雅评论家架构：利用预训练VLA基础模型提取中间表示，通过VLM精炼头进行价值估计\n5. 使用Q值波动指标识别关键干预时刻\n\n论文使用数据集和训练资源：\n1. LIBERO-Elegant基准测试：包含8个操作任务，327个演示片段，约52.7K帧同步RGB-D和本体感知数据\n2. 优雅丰富数据集：148个高质量执行片段，每个片段标注25帧关键窗口\n3. 基础模型：SmolVLA（450M）和Isaac GR00T N1.5（3B）\n4. 训练硬件：NVIDIA RTX 5090 GPU和四个NVIDIA A40 GPU\n5. 训练配置：SmolVLA训练100k步，GR00T N1.5微调50k步\n\n论文使用的评估环境和评估指标：\n1. 仿真环境：LIBERO-Elegant基准测试\n2. 真实世界验证：SO-100机器人臂，6个操作任务各50次测试\n3. 主要评估指标：优雅成功率，同时满足任务目标和所有优雅约束\n4. 泛化评估：在未见任务上测试零样本精炼能力",
    "summary_html": "<p>论文研究单位：吉林大学，微软亚洲研究院</p>\n\n<p>论文概述：该论文针对视觉语言动作模型中由于混合质量演示数据导致的执行质量不一致问题，提出了一种非侵入式的优雅执行精炼框架。通过构建LIBERO-Elegant基准测试，明确定义了优雅执行的标准，并开发了基于及时干预机制的分离式评估架构，在不修改基础VLA策略的情况下提升机器人操作质量。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>将优雅执行形式化为满足隐式任务约束</li><li>构建LIBERO-Elegant基准测试，用于评估超越二元成功的执行质量</li><li>提出分离式精炼框架，包含优雅评论家和及时干预机制</li><li>通过仿真和真实环境实验验证了方法的有效性和泛化能力</li></ol>\n\n<p>论文方法描述：</p>\n<p>方法采用三阶段框架：</p>\n<ol><li>基础生成策略训练：使用流匹配生成模型学习人类行为分布</li><li>离线优雅评论家训练：基于校准Q学习训练轻量级评论家</li><li>及时干预机制：通过监测评论家置信度，仅在决策关键时刻进行选择性干预</li><li>优雅评论家架构：利用预训练VLA基础模型提取中间表示，通过VLM精炼头进行价值估计</li><li>使用Q值波动指标识别关键干预时刻</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ol><li>LIBERO-Elegant基准测试：包含8个操作任务，327个演示片段，约52.7K帧同步RGB-D和本体感知数据</li><li>优雅丰富数据集：148个高质量执行片段，每个片段标注25帧关键窗口</li><li>基础模型：SmolVLA（450M）和Isaac GR00T N1.5（3B）</li><li>训练硬件：NVIDIA RTX 5090 GPU和四个NVIDIA A40 GPU</li><li>训练配置：SmolVLA训练100k步，GR00T N1.5微调50k步</li></ol>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ol><li>仿真环境：LIBERO-Elegant基准测试</li><li>真实世界验证：SO-100机器人臂，6个操作任务各50次测试</li><li>主要评估指标：优雅成功率，同时满足任务目标和所有优雅约束</li><li>泛化评估：在未见任务上测试零样本精炼能力</li></ol>"
  },
  {
    "date": "2025-11-27",
    "title": "CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.22532",
    "summary_markdown": "```markdown\n论文研究单位：北京大学\n\n论文概述：CoT4AD是一种用于端到端自动驾驶的视觉-语言-动作模型，通过显式思维链推理增强数值和因果推理能力。该框架整合视觉观察和语言指令，进行语义推理、场景理解和轨迹规划。训练时，模型显式建模感知-问题-预测-动作的思维链，以在多任务中对齐推理空间和动作空间。推理时，模型执行隐式思维链推理，在动态环境中实现一致的数值推理和稳健决策。\n\n论文核心贡献点：\n1. 提出CoT4AD，一种端到端自动驾驶框架，利用预训练的视觉语言模型进行多步微调，从原始视觉观察和语言指令中实现思维链推理和多任务能力。\n2. 引入一种创新的未来预测和轨迹规划方法，采用基于扩散的框架，并整合思维链推理流程。\n3. 在NuScenes和Bench2Drive数据集上的广泛实验表明，CoT4AD在开环和闭环驾驶中均达到最先进水平，持续超越先前基于大语言模型和端到端自动驾驶方法。\n\n论文方法描述：\n1. 3D环境感知：使用多视图图像输入，通过2D骨干网络提取特征并投影到BEV空间，生成地图、对象和BEV令牌，构成环境表示。\n2. 视觉语言提示调优：基于VQA的多模态视觉语言微调框架，引入阶段无关令牌，学习高层感知能力和驾驶知识。\n3. VLM条件潜在扩散：采用潜在扩散模型，在潜在空间中执行扩散建模，条件嵌入来自VLM，用于生成高保真度未来帧，增强对环境和物理规律的理解。\n4. 思维链轨迹规划：采用VLM条件扩散规划，从动作锚点初始化采样噪声，扩散变换器预测去噪轨迹和分类分数。\n4. 整个模型，包括视觉编码器、视觉扩散变换器、规划扩散变换器和大语言模型，在训练期间联合优化。\n\n论文使用数据集和训练资源：\n数据集：nuScenes（真实世界数据集，包含1000个20秒场景，用于感知和开环规划）和Bench2Drive（闭环端到端自动驾驶基准，使用CARLA V2模拟环境）。\n训练资源：在8个NVIDIA RTX A800 GPU上实现，使用PyTorch深度学习框架。使用预训练的EVA-CLIP模型作为骨干特征提取，LLaMA-3作为语言模型。优化器使用SGD，学习率1e-4，动量0.9，权重衰减1e-4。\n\n论文使用的评估环境和评估指标：\n评估环境：在nuScenes数据集上进行开环评估，在Bench2Drive数据集上进行开环和闭环评估。\n评估指标：L2距离误差和平均碰撞率（nuScenes）；驾驶评分、成功率、效率、舒适度和多能力（Bench2Drive）。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位：北京大学</p>\n\n<p>论文概述：CoT4AD是一种用于端到端自动驾驶的视觉-语言-动作模型，通过显式思维链推理增强数值和因果推理能力。该框架整合视觉观察和语言指令，进行语义推理、场景理解和轨迹规划。训练时，模型显式建模感知-问题-预测-动作的思维链，以在多任务中对齐推理空间和动作空间。推理时，模型执行隐式思维链推理，在动态环境中实现一致的数值推理和稳健决策。</p>\n\n<p>论文核心贡献点：</p>\n<ol><li>提出CoT4AD，一种端到端自动驾驶框架，利用预训练的视觉语言模型进行多步微调，从原始视觉观察和语言指令中实现思维链推理和多任务能力。</li><li>引入一种创新的未来预测和轨迹规划方法，采用基于扩散的框架，并整合思维链推理流程。</li><li>在NuScenes和Bench2Drive数据集上的广泛实验表明，CoT4AD在开环和闭环驾驶中均达到最先进水平，持续超越先前基于大语言模型和端到端自动驾驶方法。</li></ol>\n\n<p>论文方法描述：</p>\n<ol><li>3D环境感知：使用多视图图像输入，通过2D骨干网络提取特征并投影到BEV空间，生成地图、对象和BEV令牌，构成环境表示。</li><li>视觉语言提示调优：基于VQA的多模态视觉语言微调框架，引入阶段无关令牌，学习高层感知能力和驾驶知识。</li><li>VLM条件潜在扩散：采用潜在扩散模型，在潜在空间中执行扩散建模，条件嵌入来自VLM，用于生成高保真度未来帧，增强对环境和物理规律的理解。</li><li>思维链轨迹规划：采用VLM条件扩散规划，从动作锚点初始化采样噪声，扩散变换器预测去噪轨迹和分类分数。</li><li>整个模型，包括视觉编码器、视觉扩散变换器、规划扩散变换器和大语言模型，在训练期间联合优化。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<p>数据集：nuScenes（真实世界数据集，包含1000个20秒场景，用于感知和开环规划）和Bench2Drive（闭环端到端自动驾驶基准，使用CARLA V2模拟环境）。</p>\n<p>训练资源：在8个NVIDIA RTX A800 GPU上实现，使用PyTorch深度学习框架。使用预训练的EVA-CLIP模型作为骨干特征提取，LLaMA-3作为语言模型。优化器使用SGD，学习率1e-4，动量0.9，权重衰减1e-4。</p>\n\n<p>论文使用的评估环境和评估指标：</p>\n<p>评估环境：在nuScenes数据集上进行开环评估，在Bench2Drive数据集上进行开环和闭环评估。</p>\n<p>评估指标：L2距离误差和平均碰撞率（nuScenes）；驾驶评分、成功率、效率、舒适度和多能力（Bench2Drive）。</p>\n<p>```</p>"
  },
  {
    "date": "2025-11-27",
    "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
    "link": "http://arxiv.org/abs/2511.22134",
    "summary_markdown": "论文研究单位：中国科学技术大学脑启发智能感知与认知教育部重点实验室，北京大学多媒体信息处理国家重点实验室，香港中文大学\n\n论文概述：论文提出DualVLA方法，旨在解决从专家视觉-语言-动作模型向推理视觉-语言-动作模型转换过程中出现的动作退化问题。通过部分解耦推理和动作学习，在保持多模态推理能力的同时提升动作执行性能。\n\n论文核心贡献点：提出了双层级数据剪枝策略，通过视频事件边界检测和运动关键帧选择来去除冗余的具身推理数据；设计了双教师自适应蒸馏策略，分别对机器人数据和多模态数据提供差异化监督；引入了VLA Score评估框架，首次为推理视觉-语言-动作模型提供细粒度评估能力。\n\n论文方法描述：采用双层级数据剪枝，结合场景事件变化和机器人运动变化识别关键帧，保留动作关键内容；构建双教师蒸馏框架，动作教师提供细粒度动作监督，推理教师保持通用推理能力；通过检索增强的VLM评估器对轨迹进行四维度评估：推理、动作、意图和推理-动作对齐。\n\n论文使用数据集和训练资源：使用650k VLA-IT数据集重构训练数据；在SimplerEnv仿真环境中评估；使用InstructVLA-E作为动作教师，InstructVLA-G作为多模态推理教师；采用GPT-4o作为VLA Score评估器。\n\n论文使用的评估环境和评估指标：评估环境包括SimplerEnv仿真和真实世界双机械臂任务；评估指标包括任务成功率、VLA Score（包含推理分数、动作分数、意图分数和对齐分数）。",
    "summary_html": "<p>论文研究单位：中国科学技术大学脑启发智能感知与认知教育部重点实验室，北京大学多媒体信息处理国家重点实验室，香港中文大学</p>\n\n<p>论文概述：论文提出DualVLA方法，旨在解决从专家视觉-语言-动作模型向推理视觉-语言-动作模型转换过程中出现的动作退化问题。通过部分解耦推理和动作学习，在保持多模态推理能力的同时提升动作执行性能。</p>\n\n<p>论文核心贡献点：提出了双层级数据剪枝策略，通过视频事件边界检测和运动关键帧选择来去除冗余的具身推理数据；设计了双教师自适应蒸馏策略，分别对机器人数据和多模态数据提供差异化监督；引入了VLA Score评估框架，首次为推理视觉-语言-动作模型提供细粒度评估能力。</p>\n\n<p>论文方法描述：采用双层级数据剪枝，结合场景事件变化和机器人运动变化识别关键帧，保留动作关键内容；构建双教师蒸馏框架，动作教师提供细粒度动作监督，推理教师保持通用推理能力；通过检索增强的VLM评估器对轨迹进行四维度评估：推理、动作、意图和推理-动作对齐。</p>\n\n<p>论文使用数据集和训练资源：使用650k VLA-IT数据集重构训练数据；在SimplerEnv仿真环境中评估；使用InstructVLA-E作为动作教师，InstructVLA-G作为多模态推理教师；采用GPT-4o作为VLA Score评估器。</p>\n\n<p>论文使用的评估环境和评估指标：评估环境包括SimplerEnv仿真和真实世界双机械臂任务；评估指标包括任务成功率、VLA Score（包含推理分数、动作分数、意图分数和对齐分数）。</p>"
  },
  {
    "date": "2025-11-26",
    "title": "Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.21663",
    "summary_markdown": "论文研究单位\n未在提供的文本中明确提及。\n\n论文概述\n本文提出了ADVLA，一个针对视觉-语言-动作（VLA）模型的注意力引导、基于块的稀疏对抗攻击框架。现有的对抗攻击方法需要高昂的端到端训练成本，且生成的对抗扰动块通常肉眼可见。ADVLA通过直接在视觉编码器投影到文本特征空间的特征上添加扰动，有效地在低幅度约束下破坏下游的动作预测。该方法利用注意力机制引导扰动，使其具有聚焦性和稀疏性。实验表明，在L∞=4/255的约束下，ADVLA结合Top-K掩码策略仅需修改不到10%的图像块，即可达到近100%的攻击成功率，且单步迭代仅需约0.06秒，显著优于传统的基于块的攻击方法。\n\n论文核心贡献点\n1. 提出了ADVLA框架，用于在VLA的视觉特征空间中进行灰盒攻击。\n2. 设计了三种独立的注意力引导策略，以实现高效、稀疏且不易察觉的攻击。\n3. ADVLA在严格约束下实现了高攻击成功率，同时保持了低时间成本和扰动的不可见性。\n\n论文方法描述\nADVLA是一个在VLA视觉编码器的投影特征空间中进行操作的灰盒对抗攻击框架。其基本方法采用投影梯度下降（PGD），通过最小化对抗图像特征与干净图像特征在视觉-文本投影特征空间中的相似性（如余弦相似度），来破坏模型对动作的预测。在此框架下，论文提出了三种注意力增强策略：\n1. ADVLA-AW (Attention-Weighted Gradient): 使用视觉骨干网络（如ViT）的注意力图对图像梯度进行加权，使扰动更新更集中于模型关注的区域。\n2. ADVLA-TKM (Top-K Masked Gradient): 根据注意力分数选择Top-K的图像块，创建一个二元空间掩码，仅在这些选定的块上更新梯度，从而实现空间稀疏性。\n3. ADVLA-TKL (Top-K Loss): 仅在Top-K的关键块特征上计算损失，使优化过程专注于模型最敏感的区域。\n\n论文使用数据集和训练资源\n- 数据集：LIBERO数据集，该数据集包含四个任务套件（Spatial, Object, Goal, Long），每个套件包含10个任务，每个任务执行50次回滚，共500个测试样本。\n- 训练资源：本文工作为攻击方法而非模型训练，实验在单块NVIDIA H100 GPU上进行，攻击单个样本的峰值内存使用约为17GB。\n\n论文使用的评估环境和评估指标\n- 评估环境：LIBERO数据集提供的标准仿真环境。\n- 评估指标：主要采用失败率（Failure Rate, FR），FR = 1 - 成功率。该指标越高，表示攻击效果越强。同时，论文也分析了不同扰动约束、迭代次数下的攻击性能，以及每次迭代所需的时间。",
    "summary_html": "<p>论文研究单位</p>\n<p>未在提供的文本中明确提及。</p>\n\n<p>论文概述</p>\n<p>本文提出了ADVLA，一个针对视觉-语言-动作（VLA）模型的注意力引导、基于块的稀疏对抗攻击框架。现有的对抗攻击方法需要高昂的端到端训练成本，且生成的对抗扰动块通常肉眼可见。ADVLA通过直接在视觉编码器投影到文本特征空间的特征上添加扰动，有效地在低幅度约束下破坏下游的动作预测。该方法利用注意力机制引导扰动，使其具有聚焦性和稀疏性。实验表明，在L∞=4/255的约束下，ADVLA结合Top-K掩码策略仅需修改不到10%的图像块，即可达到近100%的攻击成功率，且单步迭代仅需约0.06秒，显著优于传统的基于块的攻击方法。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了ADVLA框架，用于在VLA的视觉特征空间中进行灰盒攻击。</li><li>设计了三种独立的注意力引导策略，以实现高效、稀疏且不易察觉的攻击。</li><li>ADVLA在严格约束下实现了高攻击成功率，同时保持了低时间成本和扰动的不可见性。</li></ol>\n\n<p>论文方法描述</p>\n<p>ADVLA是一个在VLA视觉编码器的投影特征空间中进行操作的灰盒对抗攻击框架。其基本方法采用投影梯度下降（PGD），通过最小化对抗图像特征与干净图像特征在视觉-文本投影特征空间中的相似性（如余弦相似度），来破坏模型对动作的预测。在此框架下，论文提出了三种注意力增强策略：</p>\n<ol><li>ADVLA-AW (Attention-Weighted Gradient): 使用视觉骨干网络（如ViT）的注意力图对图像梯度进行加权，使扰动更新更集中于模型关注的区域。</li><li>ADVLA-TKM (Top-K Masked Gradient): 根据注意力分数选择Top-K的图像块，创建一个二元空间掩码，仅在这些选定的块上更新梯度，从而实现空间稀疏性。</li><li>ADVLA-TKL (Top-K Loss): 仅在Top-K的关键块特征上计算损失，使优化过程专注于模型最敏感的区域。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：LIBERO数据集，该数据集包含四个任务套件（Spatial, Object, Goal, Long），每个套件包含10个任务，每个任务执行50次回滚，共500个测试样本。</li><li>训练资源：本文工作为攻击方法而非模型训练，实验在单块NVIDIA H100 GPU上进行，攻击单个样本的峰值内存使用约为17GB。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：LIBERO数据集提供的标准仿真环境。</li><li>评估指标：主要采用失败率（Failure Rate, FR），FR = 1 - 成功率。该指标越高，表示攻击效果越强。同时，论文也分析了不同扰动约束、迭代次数下的攻击性能，以及每次迭代所需的时间。</li></ul>"
  },
  {
    "date": "2025-11-26",
    "title": "VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.21557",
    "summary_markdown": "论文研究单位\nThe Chinese University of Hong Kong, Hong Kong SAR, China\nShanghai Jiao Tong University, Shanghai, China\nInstitute of Automation, Chinese Academy of Sciences, Beijing, China\nDiDi Global, China\n\n论文概述\n当前视觉-语言-动作模型主要使用平行两指夹爪作为末端执行器，但这类夹爪在处理某些任务时存在固有局限性，例如擦拭玻璃表面或打开没有把手的抽屉。为了克服这些挑战，论文提出了一种集成了机械夹爪和真空吸盘单元的低成本硬件设计，能够在单一末端执行器内实现双模式操作（抓取和吸附）。系统支持两种模式的灵活切换或协同使用，从而扩展了可执行的任务范围。作者在DexVLA和π0这两个最先进的VLA框架中验证了该设计的有效性和实用性，实验结果表明，使用这种混合末端执行器，机器人可以成功完成多个传统夹爪无法完成的复杂任务。\n\n论文核心贡献点\n开发了一种新型的、低成本的末端执行器，它结合了吸附和抓取功能，使机器人能够执行具有挑战性的家庭任务，同时保持标准抓取操作的强大性能。\n建立并验证了一个全面的数据采集和控制系统。为了证明其有效性，作者设计并执行了四个不同的任务，并使用DexVLA和π0这两个开源的VLA框架成功验证了系统，在两个框架中都取得了一致且有希望的结果。\n\n论文方法描述\n硬件设计：末端执行器基于一个两指夹爪基座（AgileX Robotics Piper），集成了一个微型真空泵（电压12V，流量>15.0 L/min，真空压力-60 kPa）、一个电磁阀、一个MCU（Arduino Uno R3）和两个硅胶吸盘（直径15mm）。系统通过USB协议控制继电器来开关真空泵，并通过UART协议将设备状态传回计算机。\n视觉-语言-动作模型：该方法在DexVLA和π0两个现有框架上进行了实现。模型的输入包括视觉、语言指令、机器人本体感觉状态，并特别增加了代表吸盘开启/关闭状态（f∈{True, False}）的维度。为了避免模型在预测吸盘状态时出现“捷径学习”（即简单复制输入状态），模型被设计为直接输出吸盘状态的预测，而不是仅仅依赖于输入状态。VacuumVLA基于DexVLA时，采用Qwen2-VL作为视觉语言模型骨干，并通过扩散模型生成动作；基于π0时，采用PaliGemma作为初始化，并通过条件流匹配模型生成动作。\n\n论文使用数据集和训练资源\n数据集：通过同构遥操作收集数据，使用脚踏式USB开关控制吸盘开关。为四个特定任务收集了轨迹：任务1（清理桌面）200条，任务2（打开塑料容器）、任务3（打开无把手抽屉）和任务4（打开快递纸板箱）各100条。\n训练资源：\n对于基于π0的模型：使用单节点JAX代码，批次大小为16，训练了4天达到80,000步。\n对于基于DexVLA的模型：使用400M参数的变体，在4个A100服务器上训练了2天，批次大小为每GPU 16，学习率为2x10^-5。\n\n论文使用的评估环境和评估指标\n评估环境：一个固定的双臂机器人平台，配备一个顶置摄像头和两个腕部摄像头（每条手臂一个）。\n评估任务：设计了四个长时程任务进行评估：1）将玻璃片、香蕉、黄瓜和钱包放入托盘；2）打开密封的塑料容器，放入物体并盖好；3）打开无把手抽屉，放入物体并关上；4）打开快递纸板箱。\n评估指标：成功率。每个任务测试15次，只有当任务中的所有原始动作都成功完成时，该次试验才被计为成功。如果机器人手臂在同一位置持续振荡超过一分钟，则视为失败。\n评估结果：使用VacuumVLA（基于DexVLA和π0）的混合末端执行器在四个任务上都取得了显著的成功率，而仅使用传统夹爪的基线方法在所有任务上的成功率均为0%。具体而言，VacuumVLA（DexVLA）在任务1、2、3、4上的成功率分别为73.3%、80.0%、53.3%、33.3%；VacuumVLA（π0）在任务1、2、3、4上的成功率分别为53.3%、66.7%、60.0%、53.3%。",
    "summary_html": "<p>论文研究单位</p>\n<p>The Chinese University of Hong Kong, Hong Kong SAR, China</p>\n<p>Shanghai Jiao Tong University, Shanghai, China</p>\n<p>Institute of Automation, Chinese Academy of Sciences, Beijing, China</p>\n<p>DiDi Global, China</p>\n\n<p>论文概述</p>\n<p>当前视觉-语言-动作模型主要使用平行两指夹爪作为末端执行器，但这类夹爪在处理某些任务时存在固有局限性，例如擦拭玻璃表面或打开没有把手的抽屉。为了克服这些挑战，论文提出了一种集成了机械夹爪和真空吸盘单元的低成本硬件设计，能够在单一末端执行器内实现双模式操作（抓取和吸附）。系统支持两种模式的灵活切换或协同使用，从而扩展了可执行的任务范围。作者在DexVLA和π0这两个最先进的VLA框架中验证了该设计的有效性和实用性，实验结果表明，使用这种混合末端执行器，机器人可以成功完成多个传统夹爪无法完成的复杂任务。</p>\n\n<p>论文核心贡献点</p>\n<p>开发了一种新型的、低成本的末端执行器，它结合了吸附和抓取功能，使机器人能够执行具有挑战性的家庭任务，同时保持标准抓取操作的强大性能。</p>\n<p>建立并验证了一个全面的数据采集和控制系统。为了证明其有效性，作者设计并执行了四个不同的任务，并使用DexVLA和π0这两个开源的VLA框架成功验证了系统，在两个框架中都取得了一致且有希望的结果。</p>\n\n<p>论文方法描述</p>\n<p>硬件设计：末端执行器基于一个两指夹爪基座（AgileX Robotics Piper），集成了一个微型真空泵（电压12V，流量>15.0 L/min，真空压力-60 kPa）、一个电磁阀、一个MCU（Arduino Uno R3）和两个硅胶吸盘（直径15mm）。系统通过USB协议控制继电器来开关真空泵，并通过UART协议将设备状态传回计算机。</p>\n<p>视觉-语言-动作模型：该方法在DexVLA和π0两个现有框架上进行了实现。模型的输入包括视觉、语言指令、机器人本体感觉状态，并特别增加了代表吸盘开启/关闭状态（f∈{True, False}）的维度。为了避免模型在预测吸盘状态时出现“捷径学习”（即简单复制输入状态），模型被设计为直接输出吸盘状态的预测，而不是仅仅依赖于输入状态。VacuumVLA基于DexVLA时，采用Qwen2-VL作为视觉语言模型骨干，并通过扩散模型生成动作；基于π0时，采用PaliGemma作为初始化，并通过条件流匹配模型生成动作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：通过同构遥操作收集数据，使用脚踏式USB开关控制吸盘开关。为四个特定任务收集了轨迹：任务1（清理桌面）200条，任务2（打开塑料容器）、任务3（打开无把手抽屉）和任务4（打开快递纸板箱）各100条。</p>\n<p>训练资源：</p>\n<p>对于基于π0的模型：使用单节点JAX代码，批次大小为16，训练了4天达到80,000步。</p>\n<p>对于基于DexVLA的模型：使用400M参数的变体，在4个A100服务器上训练了2天，批次大小为每GPU 16，学习率为2x10^-5。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：一个固定的双臂机器人平台，配备一个顶置摄像头和两个腕部摄像头（每条手臂一个）。</p>\n<p>评估任务：设计了四个长时程任务进行评估：1）将玻璃片、香蕉、黄瓜和钱包放入托盘；2）打开密封的塑料容器，放入物体并盖好；3）打开无把手抽屉，放入物体并关上；4）打开快递纸板箱。</p>\n<p>评估指标：成功率。每个任务测试15次，只有当任务中的所有原始动作都成功完成时，该次试验才被计为成功。如果机器人手臂在同一位置持续振荡超过一分钟，则视为失败。</p>\n<p>评估结果：使用VacuumVLA（基于DexVLA和π0）的混合末端执行器在四个任务上都取得了显著的成功率，而仅使用传统夹爪的基线方法在所有任务上的成功率均为0%。具体而言，VacuumVLA（DexVLA）在任务1、2、3、4上的成功率分别为73.3%、80.0%、53.3%、33.3%；VacuumVLA（π0）在任务1、2、3、4上的成功率分别为53.3%、66.7%、60.0%、53.3%。</p>"
  },
  {
    "date": "2025-11-26",
    "title": "$\\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion",
    "link": "http://arxiv.org/abs/2511.21542",
    "summary_markdown": "### 论文研究单位\n中山大学，广东大数据分析与处理重点实验室，X-Era AI实验室，广东工业大学\n### 论文概述\n提出ℰ₀，一个连续化的离散扩散框架，用于视觉-语言-动作模型，以提升泛化能力和细粒度控制。该方法将动作生成为量化动作标记的迭代去噪过程，解决现有模型在符号对齐和离散控制上的不足，并引入球形视角扰动增强以提高对相机移动的鲁棒性。在LIBERO、VLABench和ManiSkill等数据集上验证，实现了最先进的性能。\n### 论文核心贡献点\n1. 提出ℰ₀框架，支持任意细粒度的动作离散化，实现高精度动作表示，同时保持与预训练视觉-语言模型的兼容性。\n2. 引入球形视角扰动增强和相对球形嵌入机制，显式建模动态相机扰动，提升跨视角一致性和鲁棒性。\n3. 通过多个模拟基准和真实机器人任务（包括Franka机械臂）的广泛实验，证明ℰ₀在14个多样化环境中超越基线模型平均10.7%的性能。\n### 论文方法描述\n基于PaliGemma视觉-语言模型主干网络，添加300M参数的动作专家模块。动作使用基于分位数的离散化，量化为2048个bins以减少异常值影响。训练中，对离散动作添加高斯噪声，模型通过交叉熵损失预测分类分布。推理采用多步迭代去噪，从噪声序列逐步恢复清晰动作，复用观察编码的键值缓存以提高效率。球形视角扰动通过3D点旋转重投影图像，结合可学习的偏移嵌入，增强视角不变性。\n### 论文使用数据集和训练资源\n数据集包括LIBERO（涵盖空间、物体、目标和长时程任务）、VLABench（需语言理解和常识推理）和ManiSkill（精细操作技能如插入和堆叠）。训练使用单块NVIDIA RTX RPO6000 GPU，批次大小32，训练30000步，学习率余弦衰减（峰值5×10⁻⁵，预热10000步），总时长24小时。\n### 论文使用的评估环境和评估指标\n评估环境为LIBERO、VLABench和ManiSkill模拟器，以及Franka Research 3机械臂真实平台。指标为任务成功率（SR%），在各项任务子集和平均表现上报告。此外，进行相机扰动鲁棒性测试，通过动态改变相机位置和姿态评估跨视角泛化能力。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中山大学，广东大数据分析与处理重点实验室，X-Era AI实验室，广东工业大学</p>\n<h3>论文概述</h3>\n<p>提出ℰ₀，一个连续化的离散扩散框架，用于视觉-语言-动作模型，以提升泛化能力和细粒度控制。该方法将动作生成为量化动作标记的迭代去噪过程，解决现有模型在符号对齐和离散控制上的不足，并引入球形视角扰动增强以提高对相机移动的鲁棒性。在LIBERO、VLABench和ManiSkill等数据集上验证，实现了最先进的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出ℰ₀框架，支持任意细粒度的动作离散化，实现高精度动作表示，同时保持与预训练视觉-语言模型的兼容性。</li><li>引入球形视角扰动增强和相对球形嵌入机制，显式建模动态相机扰动，提升跨视角一致性和鲁棒性。</li><li>通过多个模拟基准和真实机器人任务（包括Franka机械臂）的广泛实验，证明ℰ₀在14个多样化环境中超越基线模型平均10.7%的性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>基于PaliGemma视觉-语言模型主干网络，添加300M参数的动作专家模块。动作使用基于分位数的离散化，量化为2048个bins以减少异常值影响。训练中，对离散动作添加高斯噪声，模型通过交叉熵损失预测分类分布。推理采用多步迭代去噪，从噪声序列逐步恢复清晰动作，复用观察编码的键值缓存以提高效率。球形视角扰动通过3D点旋转重投影图像，结合可学习的偏移嵌入，增强视角不变性。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集包括LIBERO（涵盖空间、物体、目标和长时程任务）、VLABench（需语言理解和常识推理）和ManiSkill（精细操作技能如插入和堆叠）。训练使用单块NVIDIA RTX RPO6000 GPU，批次大小32，训练30000步，学习率余弦衰减（峰值5×10⁻⁵，预热10000步），总时长24小时。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境为LIBERO、VLABench和ManiSkill模拟器，以及Franka Research 3机械臂真实平台。指标为任务成功率（SR%），在各项任务子集和平均表现上报告。此外，进行相机扰动鲁棒性测试，通过动态改变相机位置和姿态评估跨视角泛化能力。</p>"
  },
  {
    "date": "2025-11-26",
    "title": "From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings",
    "link": "http://arxiv.org/abs/2511.21428",
    "summary_markdown": "论文研究单位\n上海科技大学信息科学与技术学院，中国上海；杭州电子科技大学自动化学院，中国\n\n论文概述\n论文提出了一种名为LAPS（Latent Action-based Primitive Segmentation）的无监督框架，旨在从连续的工业视频流中解锁大量未标注的人类演示数据，用于视觉-语言-动作（VLA）模型的预训练。该方法首先训练一个轻量级的运动分词器来编码运动动态，然后利用一种新颖的“潜在动作能量”指标进行无监督的动作分割，以发现和分割语义上连贯的动作基元。该流程输出分割后的视频片段及其对应的潜在动作序列，为VLA预训练提供了可直接使用的结构化数据。论文在公共基准和一个专有的电机组装数据集上进行了评估，证明了其有效性。\n\n论文核心贡献点\n1. 引入了一种基于“潜在动作能量”的新型分割方法，该指标定义在抽象的潜在动作空间中，用于从原始视频数据中识别语义动作基元。\n2. 提出了一个端到端的自动化数据管道，将长达数小时的工业视频转换为结构化的动作基元库，直接解决了工业VLA潜在预训练的数据来源瓶颈。\n3. 首次在公开的基准数据集和一个真实的复杂工业装配线数据集上验证了这种VLA数据获取方法，并通过实验证明了其实用性和可扩展性。\n\n论文方法描述\nLAPS流程包含三个顺序阶段：\n1. 运动跟踪：使用点跟踪器（如CoTracker）从原始视频中提取密集的运动轨迹。\n2. 动作检测与分割：将关键点输入一个运动分词器，生成连续的“潜在动作向量流”。然后，一个动作检测器使用“潜在动作能量”指标和具有滞后回差的双态控制器来识别持续的动作激活，从而定位动作边界并提取动作基元及其对应的离散代码序列。\n3. 语义动作聚类：通过冻结的Transformer模型对分割出的潜在向量进行时间嵌入，然后使用余弦k均值进行聚类，以自动发现工作站任务的有限集合，即语义动作簇。\n\n论文使用数据集和训练资源\n数据集：公共基准数据集GTEA和Breakfast，以及一个新自收集的工业电机装配数据集（包含约10小时的双视角连续视频）。\n训练资源：运动分词器在大型短视频片段数据集上训练；用于聚类的Transformer编码器是一个轻量级模型（约230万参数），在推理模式下使用，无需训练。\n\n论文使用的评估环境和评估指标\n评估环境：将LAPS与多个无监督时序动作检测（TAD）基线方法（如Optical Flow、ABD、OTAS）在GTEA、Breakfast和工业电机装配数据集上进行比较。\n评估指标：\n- 动作分割准确性：使用容差为2秒和5秒的严格边界级F1分数（F1@2s, F1@5s）。\n- 聚类质量：使用无监督指标包括轮廓分数、Calinski-Harabasz指数以及论文提出的基于视觉语言模型（VLM）的簇内语义相似度（ICSS）指标。",
    "summary_html": "<p>论文研究单位</p>\n<p>上海科技大学信息科学与技术学院，中国上海；杭州电子科技大学自动化学院，中国</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为LAPS（Latent Action-based Primitive Segmentation）的无监督框架，旨在从连续的工业视频流中解锁大量未标注的人类演示数据，用于视觉-语言-动作（VLA）模型的预训练。该方法首先训练一个轻量级的运动分词器来编码运动动态，然后利用一种新颖的“潜在动作能量”指标进行无监督的动作分割，以发现和分割语义上连贯的动作基元。该流程输出分割后的视频片段及其对应的潜在动作序列，为VLA预训练提供了可直接使用的结构化数据。论文在公共基准和一个专有的电机组装数据集上进行了评估，证明了其有效性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>引入了一种基于“潜在动作能量”的新型分割方法，该指标定义在抽象的潜在动作空间中，用于从原始视频数据中识别语义动作基元。</li><li>提出了一个端到端的自动化数据管道，将长达数小时的工业视频转换为结构化的动作基元库，直接解决了工业VLA潜在预训练的数据来源瓶颈。</li><li>首次在公开的基准数据集和一个真实的复杂工业装配线数据集上验证了这种VLA数据获取方法，并通过实验证明了其实用性和可扩展性。</li></ol>\n\n<p>论文方法描述</p>\n<p>LAPS流程包含三个顺序阶段：</p>\n<ol><li>运动跟踪：使用点跟踪器（如CoTracker）从原始视频中提取密集的运动轨迹。</li><li>动作检测与分割：将关键点输入一个运动分词器，生成连续的“潜在动作向量流”。然后，一个动作检测器使用“潜在动作能量”指标和具有滞后回差的双态控制器来识别持续的动作激活，从而定位动作边界并提取动作基元及其对应的离散代码序列。</li><li>语义动作聚类：通过冻结的Transformer模型对分割出的潜在向量进行时间嵌入，然后使用余弦k均值进行聚类，以自动发现工作站任务的有限集合，即语义动作簇。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：公共基准数据集GTEA和Breakfast，以及一个新自收集的工业电机装配数据集（包含约10小时的双视角连续视频）。</p>\n<p>训练资源：运动分词器在大型短视频片段数据集上训练；用于聚类的Transformer编码器是一个轻量级模型（约230万参数），在推理模式下使用，无需训练。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：将LAPS与多个无监督时序动作检测（TAD）基线方法（如Optical Flow、ABD、OTAS）在GTEA、Breakfast和工业电机装配数据集上进行比较。</p>\n<p>评估指标：</p>\n<ul><li>动作分割准确性：使用容差为2秒和5秒的严格边界级F1分数（F1@2s, F1@5s）。</li><li>聚类质量：使用无监督指标包括轮廓分数、Calinski-Harabasz指数以及论文提出的基于视觉语言模型（VLM）的簇内语义相似度（ICSS）指标。</li></ul>"
  },
  {
    "date": "2025-11-26",
    "title": "When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.21192",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n南洋理工大学\n## 论文概述\n本文系统研究了针对视觉-语言-动作（VLA）模型的通用可迁移对抗补丁攻击。现有方法通常针对单一模型进行白盒攻击，在未知架构、微调变体或模拟到现实的迁移场景下效果不佳。为此，论文提出了UPA-RFAS框架，旨在学习单个物理补丁，该补丁在共享特征空间中优化，并促进跨模型迁移。实验表明，该方法能在模型、任务和视角间实现有效迁移，揭示了基于补丁的实际攻击面，为未来防御提供了强基线。\n## 论文核心贡献点\n- 提出了首个针对VLA机器人的通用可迁移补丁攻击框架，在共享特征空间中结合了ℓ1偏差与排斥性对比对齐。\n- 设计了鲁棒性增强的通用补丁攻击，使用样本级不可见扰动作为增强器，并在大量几何随机化下训练通用补丁。\n- 设计了两种VLA特定损失：Patch Attention Dominance用于劫持文本到视觉的注意力，Patch Semantic Misalignment用于在无标签情况下使指令误对齐。\n- 在多个VLA模型、任务和模拟到现实设置下的广泛实验展示了强大的黑盒迁移性，揭示了基于补丁的实际威胁并为防御提供了可迁移基线。\n## 论文方法描述\n方法核心为UPA-RFAS框架，包含以下组件：\n1. **特征空间目标**：结合ℓ1偏差与排斥性InfoNCE损失，通过稀疏、高显著度的特征偏移和批次一致性方向促进迁移。\n2. **鲁棒性增强的双阶段优化**：内循环通过PGD学习样本级不可见扰动以最小化特征目标，外循环在强化邻域上优化通用补丁。\n3. **Patch Attention Dominance (PAD)损失**：增加补丁路由的文本到视觉注意力，通过单边边际抑制非补丁增量，实现位置无关的注意力吸引。\n4. **Patch Semantic Misalignment (PSM)损失**：将补丁表示拉向探查短语锚点，同时推离当前指令嵌入，创建持久的图像-文本不匹配。\n## 论文使用数据集和训练资源\n- 数据集：Libero、BridgeData等VLA机器人操作任务数据集。\n- 训练资源：未明确提及具体硬件配置，但方法涉及对抗训练（PGD）和AdamW优化，暗示需要GPU支持。\n## 论文使用的评估环境和评估指标\n- **评估环境**：模拟环境（如Libero、BridgeData）和物理世界实验（Franka Emika机械臂）。\n- **评估指标**：任务成功率、动作空间偏差、特征空间ℓ1偏差、注意力增量（补丁与非补丁区域）、文本相似度攻击损失（PSM损失）。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>南洋理工大学</p>\n<h2>论文概述</h2>\n<p>本文系统研究了针对视觉-语言-动作（VLA）模型的通用可迁移对抗补丁攻击。现有方法通常针对单一模型进行白盒攻击，在未知架构、微调变体或模拟到现实的迁移场景下效果不佳。为此，论文提出了UPA-RFAS框架，旨在学习单个物理补丁，该补丁在共享特征空间中优化，并促进跨模型迁移。实验表明，该方法能在模型、任务和视角间实现有效迁移，揭示了基于补丁的实际攻击面，为未来防御提供了强基线。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了首个针对VLA机器人的通用可迁移补丁攻击框架，在共享特征空间中结合了ℓ1偏差与排斥性对比对齐。</li><li>设计了鲁棒性增强的通用补丁攻击，使用样本级不可见扰动作为增强器，并在大量几何随机化下训练通用补丁。</li><li>设计了两种VLA特定损失：Patch Attention Dominance用于劫持文本到视觉的注意力，Patch Semantic Misalignment用于在无标签情况下使指令误对齐。</li><li>在多个VLA模型、任务和模拟到现实设置下的广泛实验展示了强大的黑盒迁移性，揭示了基于补丁的实际威胁并为防御提供了可迁移基线。</li></ul>\n<h2>论文方法描述</h2>\n<p>方法核心为UPA-RFAS框架，包含以下组件：</p>\n<ol><li><strong>特征空间目标</strong>：结合ℓ1偏差与排斥性InfoNCE损失，通过稀疏、高显著度的特征偏移和批次一致性方向促进迁移。</li><li><strong>鲁棒性增强的双阶段优化</strong>：内循环通过PGD学习样本级不可见扰动以最小化特征目标，外循环在强化邻域上优化通用补丁。</li><li><strong>Patch Attention Dominance (PAD)损失</strong>：增加补丁路由的文本到视觉注意力，通过单边边际抑制非补丁增量，实现位置无关的注意力吸引。</li><li><strong>Patch Semantic Misalignment (PSM)损失</strong>：将补丁表示拉向探查短语锚点，同时推离当前指令嵌入，创建持久的图像-文本不匹配。</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据集：Libero、BridgeData等VLA机器人操作任务数据集。</li><li>训练资源：未明确提及具体硬件配置，但方法涉及对抗训练（PGD）和AdamW优化，暗示需要GPU支持。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：模拟环境（如Libero、BridgeData）和物理世界实验（Franka Emika机械臂）。</li><li><strong>评估指标</strong>：任务成功率、动作空间偏差、特征空间ℓ1偏差、注意力增量（补丁与非补丁区域）、文本相似度攻击损失（PSM损失）。</li></ul>"
  },
  {
    "date": "2025-11-25",
    "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.20720",
    "summary_markdown": "论文研究单位\n香港城市大学，穆罕默德·本·扎耶德人工智能大学\n\n论文概述\n论文提出了一种名为DeeAD的免训练、动作引导的早期退出框架，旨在加速视觉-语言-动作模型在自动驾驶中的规划过程。该方法通过评估中间轨迹的物理可行性，在预测轨迹与轻量级规划先验（如导航指令或低精度规划）的可容忍偏差（≤ 2米）内对齐时终止推理。为了提高效率，DeeAD还引入了一个多跳控制器，根据分数变化率自适应地跳过冗余层。DeeAD可以无缝集成到现有的VLA模型（如ORION）中而无需重新训练。在Bench2Drive基准测试上的实验表明，该方法实现了高达28%的transformer层稀疏性和29%的延迟降低，同时保持了规划质量和安全性。\n\n论文核心贡献点\n1. 提出了一种基于物理原理的动态早期退出机制，用于VLA规划，当中间轨迹与导航先验的空间偏差在可接受范围内时终止推理，实现了免训练、可解释且与动作对齐的加速。\n2. 设计了两个轻量级组件来支持高效的早期终止：(i) 一个差异性估计器，以可忽略的成本衡量预测轨迹与参考轨迹的空间对齐程度；(ii) 一个多跳退出控制器，通过根据偏差大小自适应地跳过层来减少冗余的逐层评估，从而大大降低延迟开销。\n3. 将DeeAD集成到ORION VLA框架中，并在Bench2Drive上进行了评估，实现了高达28%的transformer层稀疏性和29%的延迟降低。\n\n论文方法描述\nDeeAD的核心是一个物理基础的动作空间早期退出机制。主要包含三个部分：\n1. 早期退出动作头：在选定的transformer层（如L13-L32）添加动作头，用于从中间隐藏状态解码出2D轨迹，而无需重新训练模型。\n2. 差异性估计：在每个候选退出层，计算预测轨迹与参考轨迹（来自CARLA导航或Autoware低分辨率规划）之间的平均L2距离。当该距离小于预设阈值δ（如2米）时，触发早期退出。\n3. 多跳退出控制器：为了避免在每一层都进行退出检查，该控制器根据当前差异性分数与阈值δ的关系动态决定下一检查的层数。例如，如果当前距离远大于δ（如>8*δ），则跳过8层；如果接近阈值（如>2*δ），则跳过2层或1层，从而实现从粗到精的搜索，减少计算开销。\n\n论文使用数据集和训练资源\n数据集：Bench2Drive基准，一个基于CARLA模拟器的闭环评估套件，包含超过1000个用于训练的驾驶片段和220个用于验证的多样化路线。\n训练资源：DeeAD是一个免训练的推理框架，不需要重新训练基础模型。它被用作一个运行时包装器，集成到预先训练好的ORION模型中。\n\n论文使用的评估环境和评估指标\n评估环境：Ubuntu 22.04操作系统，配备双路Intel Xeon Silver 4314 CPU（32核），512GB RAM，以及2块NVIDIA L20 GPU（每块40GB显存）。软件环境为PyTorch 1.8和CUDA 11.8。\n评估指标：\n1. L2@t (m)：在1秒、2秒、3秒时间范围的轨迹位移误差。\n2. Collision rate (%)：预测轨迹与物体发生碰撞的比率。\n3. Latency (ms)：每个样本的平均前向推理延迟，不包括传感器预处理。\n4. Sparsity (%)：因早期退出而跳过的transformer层的百分比。",
    "summary_html": "<p>论文研究单位</p>\n<p>香港城市大学，穆罕默德·本·扎耶德人工智能大学</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为DeeAD的免训练、动作引导的早期退出框架，旨在加速视觉-语言-动作模型在自动驾驶中的规划过程。该方法通过评估中间轨迹的物理可行性，在预测轨迹与轻量级规划先验（如导航指令或低精度规划）的可容忍偏差（≤ 2米）内对齐时终止推理。为了提高效率，DeeAD还引入了一个多跳控制器，根据分数变化率自适应地跳过冗余层。DeeAD可以无缝集成到现有的VLA模型（如ORION）中而无需重新训练。在Bench2Drive基准测试上的实验表明，该方法实现了高达28%的transformer层稀疏性和29%的延迟降低，同时保持了规划质量和安全性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种基于物理原理的动态早期退出机制，用于VLA规划，当中间轨迹与导航先验的空间偏差在可接受范围内时终止推理，实现了免训练、可解释且与动作对齐的加速。</li><li>设计了两个轻量级组件来支持高效的早期终止：(i) 一个差异性估计器，以可忽略的成本衡量预测轨迹与参考轨迹的空间对齐程度；(ii) 一个多跳退出控制器，通过根据偏差大小自适应地跳过层来减少冗余的逐层评估，从而大大降低延迟开销。</li><li>将DeeAD集成到ORION VLA框架中，并在Bench2Drive上进行了评估，实现了高达28%的transformer层稀疏性和29%的延迟降低。</li></ol>\n\n<p>论文方法描述</p>\n<p>DeeAD的核心是一个物理基础的动作空间早期退出机制。主要包含三个部分：</p>\n<ol><li>早期退出动作头：在选定的transformer层（如L13-L32）添加动作头，用于从中间隐藏状态解码出2D轨迹，而无需重新训练模型。</li><li>差异性估计：在每个候选退出层，计算预测轨迹与参考轨迹（来自CARLA导航或Autoware低分辨率规划）之间的平均L2距离。当该距离小于预设阈值δ（如2米）时，触发早期退出。</li><li>多跳退出控制器：为了避免在每一层都进行退出检查，该控制器根据当前差异性分数与阈值δ的关系动态决定下一检查的层数。例如，如果当前距离远大于δ（如>8*δ），则跳过8层；如果接近阈值（如>2*δ），则跳过2层或1层，从而实现从粗到精的搜索，减少计算开销。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：Bench2Drive基准，一个基于CARLA模拟器的闭环评估套件，包含超过1000个用于训练的驾驶片段和220个用于验证的多样化路线。</p>\n<p>训练资源：DeeAD是一个免训练的推理框架，不需要重新训练基础模型。它被用作一个运行时包装器，集成到预先训练好的ORION模型中。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：Ubuntu 22.04操作系统，配备双路Intel Xeon Silver 4314 CPU（32核），512GB RAM，以及2块NVIDIA L20 GPU（每块40GB显存）。软件环境为PyTorch 1.8和CUDA 11.8。</p>\n<p>评估指标：</p>\n<ol><li>L2@t (m)：在1秒、2秒、3秒时间范围的轨迹位移误差。</li><li>Collision rate (%)：预测轨迹与物体发生碰撞的比率。</li><li>Latency (ms)：每个样本的平均前向推理延迟，不包括传感器预处理。</li><li>Sparsity (%)：因早期退出而跳过的transformer层的百分比。</li></ol>"
  },
  {
    "date": "2025-11-25",
    "title": "Reinforcing Action Policies by Prophesying",
    "link": "http://arxiv.org/abs/2511.20633",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-25",
    "title": "CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model",
    "link": "http://arxiv.org/abs/2511.19914",
    "summary_markdown": "### 论文研究单位\n兰州大学、新加坡国立大学、中国科学技术大学\n### 论文概述\n本文提出了一种名为CoC-VLA的新型视觉-语言-动作模型，旨在通过对抗性域转移实现可解释的自动驾驶。该模型包含教师VLM模型、学生VLM模型和一个判别器。教师模型在模拟数据上预训练，学生模型在真实世界数据上预训练，两者共享相同的因果链（Chain-of-Causality）VLM架构。通过判别器进行对抗训练，将模拟数据中的长尾场景处理能力转移到真实世界部署中，从而弥合了模拟与真实世界自动驾驶之间的差距。\n### 论文核心贡献点\n- 提出了首个能够将罕见场景处理能力从模拟转移到真实世界的VLM自动驾驶模型。\n- 引入了一种新颖的判别器，用于学习模拟与真实世界数据之间的域差异。\n- 提出了一种反向传播策略，增强了对抗训练过程的收敛稳定性。\n- 开发了因果链策略，将时序信息与因果链答案连接，支持思维链推理以建模深层驾驶逻辑。\n- 在nuScenes-VLM数据集上进行了广泛实验，表明该方法显著优于现有方法。\n### 论文方法描述\n方法包括三个核心组件：教师VLM、学生VLM和判别器。\n1. 因果链视觉语言模型（CoC VLM）：作为教师和学生模型的共享基础架构，包含文本适配器（整合历史帧的简化答案与当前指令）、视觉适配器（将多视图图像转换为令牌）、LLM大脑（基于LLaMA）和因果链答案模块（生成感知、预测和规划的因果结构答案）。\n2. 判别器：采用Transformer架构，处理来自教师和学生VLM的特征，通过对抗学习对齐特征表示，最小化域差异。\n3. 训练流程：\n - 预训练：教师模型在模拟数据（CARLA-VLM）上训练，学生模型在真实数据（nuScenes-VLM）上训练。\n - 对抗训练：\n - 步骤1：使用模拟和真实数据前向传播，仅更新判别器参数。\n - 步骤2：使用真实数据前向传播学生VLM和判别器，结合VLM损失和判别器损失更新学生VLM，判别器参数不更新。\n### 论文使用数据集和训练资源\n数据集：\n- CARLA-VLM：模拟数据集，包含61.6%常规场景和38.4%挑战性场景（如交通堵塞、行人入侵等）。\n- nuScenes-VLM：基于nuScenes的真实世界数据集，用于预训练学生模型和最终评估。\n训练资源：使用8块NVIDIA H100 GPU，采用LoRA方法微调模型以降低计算资源需求。\n### 论文使用的评估环境和评估指标\n评估环境：基于nuScenes-VLM数据集进行开环评估。\n评估指标：\n语言评估指标：\n- BLEU-1至BLEU-4：衡量预测文本与参考文本的n元语法匹配度。\n- ROUGE-L：基于最长公共子序列的召回率评估。\n- CIDEr：通过TF-IDF向量余弦距离衡量文本相似性。\n- SPICE：基于场景图的结构相似性评估。\n- GPT Score：利用ChatGPT评估语义对齐度。\n- Accuracy和Match：衡量答案正确性。\n规划评估指标：\n- 平均位移误差（ADE）：预测轨迹与真实轨迹之间的平均L2距离。\n- 碰撞率（Collision Rate）：评估帧中发生碰撞的比例。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>兰州大学、新加坡国立大学、中国科学技术大学</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为CoC-VLA的新型视觉-语言-动作模型，旨在通过对抗性域转移实现可解释的自动驾驶。该模型包含教师VLM模型、学生VLM模型和一个判别器。教师模型在模拟数据上预训练，学生模型在真实世界数据上预训练，两者共享相同的因果链（Chain-of-Causality）VLM架构。通过判别器进行对抗训练，将模拟数据中的长尾场景处理能力转移到真实世界部署中，从而弥合了模拟与真实世界自动驾驶之间的差距。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了首个能够将罕见场景处理能力从模拟转移到真实世界的VLM自动驾驶模型。</li><li>引入了一种新颖的判别器，用于学习模拟与真实世界数据之间的域差异。</li><li>提出了一种反向传播策略，增强了对抗训练过程的收敛稳定性。</li><li>开发了因果链策略，将时序信息与因果链答案连接，支持思维链推理以建模深层驾驶逻辑。</li><li>在nuScenes-VLM数据集上进行了广泛实验，表明该方法显著优于现有方法。</li></ul>\n<h3>论文方法描述</h3>\n<p>方法包括三个核心组件：教师VLM、学生VLM和判别器。</p>\n<ol><li>因果链视觉语言模型（CoC VLM）：作为教师和学生模型的共享基础架构，包含文本适配器（整合历史帧的简化答案与当前指令）、视觉适配器（将多视图图像转换为令牌）、LLM大脑（基于LLaMA）和因果链答案模块（生成感知、预测和规划的因果结构答案）。</li><li>判别器：采用Transformer架构，处理来自教师和学生VLM的特征，通过对抗学习对齐特征表示，最小化域差异。</li><li>训练流程：</li></ol>\n<p> - 预训练：教师模型在模拟数据（CARLA-VLM）上训练，学生模型在真实数据（nuScenes-VLM）上训练。</p>\n<p> - 对抗训练：</p>\n<p> - 步骤1：使用模拟和真实数据前向传播，仅更新判别器参数。</p>\n<p> - 步骤2：使用真实数据前向传播学生VLM和判别器，结合VLM损失和判别器损失更新学生VLM，判别器参数不更新。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：</p>\n<ul><li>CARLA-VLM：模拟数据集，包含61.6%常规场景和38.4%挑战性场景（如交通堵塞、行人入侵等）。</li><li>nuScenes-VLM：基于nuScenes的真实世界数据集，用于预训练学生模型和最终评估。</li></ul>\n<p>训练资源：使用8块NVIDIA H100 GPU，采用LoRA方法微调模型以降低计算资源需求。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：基于nuScenes-VLM数据集进行开环评估。</p>\n<p>评估指标：</p>\n<p>语言评估指标：</p>\n<ul><li>BLEU-1至BLEU-4：衡量预测文本与参考文本的n元语法匹配度。</li><li>ROUGE-L：基于最长公共子序列的召回率评估。</li><li>CIDEr：通过TF-IDF向量余弦距离衡量文本相似性。</li><li>SPICE：基于场景图的结构相似性评估。</li><li>GPT Score：利用ChatGPT评估语义对齐度。</li><li>Accuracy和Match：衡量答案正确性。</li></ul>\n<p>规划评估指标：</p>\n<ul><li>平均位移误差（ADE）：预测轨迹与真实轨迹之间的平均L2距离。</li><li>碰撞率（Collision Rate）：评估帧中发生碰撞的比例。</li></ul>"
  },
  {
    "date": "2025-11-25",
    "title": "Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.19912",
    "summary_markdown": "### 论文研究单位\nLanzhou University, China\nNational University of Singapore, Singapore\nUniversity of Science and Technology of China, China\nTsinghua University, China\nUniversity of New South Wales, Australia\n### 论文概述\n论文提出了一种名为Reasoning-VLA的通用且快速的视觉-语言-动作（VLA）推理模型，用于解决现有自动驾驶VLA模型在推理效率和泛化能力方面的不足。该模型通过一组可学习的动作查询与增强推理的视觉-语言特征交互，实现连续动作轨迹的并行生成。为了提升泛化性，研究整合了八个公开的自动驾驶数据集，构建了一个基于思维链推理的统一数据集。结合监督微调（SFT）和强化学习（RL）的训练策略，实验表明Reasoning-VLA在多个基准上实现了最先进的性能、卓越的泛化能力和目前报告的优异推理速度。\n### 论文核心贡献点\n1. 提出了Reasoning-VLA，一个高效的VLA框架，通过可学习的动作查询与推理增强的视觉-语言表示交互，实现单步并行动作生成。\n2. 通过从真实轨迹中进行高斯分布采样来初始化可学习的动作查询，提升了模型效率。\n3. 构建了一个统一的、基于思维链推理的自动驾驶数据集，该数据集融合了八个现有数据集，以促进模型在不同车辆类型和驾驶环境下的泛化能力。\n4. 采用结合SFT和RL的微调策略，并辅以物理和动力学奖励函数，增强了模型的通用推理能力，显著优于先前的方法。\n### 论文方法描述\n该方法基于Qwen2.5-VL模型构建，包含三个主要部分：\n1. **推理增强的视觉-语言模型主干**：利用Qwen2.5-VL作为基础模型，提供强大的视觉-语言理解和推理能力。\n2. **VL到Action模块**：引入一组可学习的动作查询，这些查询通过自注意力和与VLM特征的交叉注意力进行交互，从VLM的推理内容中提取动作相关信息，实现并行动作轨迹的预测。\n3. **动作细化模块（ARM）**：通过多层感知机（MLP）和注意力机制处理动作查询的隐藏状态，以细化特征表示，提高轨迹精度。\n训练过程分为两个阶段：\n1. **监督微调（SFT）**：使用统一的推理数据集对模型进行初步训练，建立结构化的推理链。\n2. **强化学习（RL）微调**：采用GRPO算法和设计的基于规则的奖励函数进行优化，以提高模型在未见场景下的泛化能力。奖励函数包括物理轨迹奖励、车辆动态奖励（转向和加速度约束）以及它们的加权和。\n### 论文使用数据集和训练资源\n**数据集**：\n构建了一个统一的自动驾驶数据集，整合了八个公开数据集：NAVSIM, nuScenes, Waymo, Argoverse-V2, KITTI, Mapillary, ONCE, 和 IDD。经过筛选和处理，包含了超过75,000个高质量片段，并生成了思维链描述。\n**训练资源**：\n训练在8张H200 GPU上进行，总批大小为8。SFT阶段训练4个周期，学习率从5e-4开始；RL阶段训练1个周期，学习率从1e-6开始。累积梯度步数为2。\n### 论文使用的评估环境和评估指标\n**评估环境**：\n1. **开环评估**：在nuScenes数据集的验证集上进行，与现有方法进行公平比较。\n2. **闭环评估**：在NeuroNCAP模拟器上进行，该模拟器能够模拟复杂的现实世界驾驶场景。\n**评估指标**：\n1. **开环指标**：\n - L2误差（米）：在1秒、2秒、3秒未来时间点的平均L2距离，值越低越好。\n - 碰撞率（%）：在1秒、2秒、3秒未来时间点的碰撞率，值越低越好。\n2. **闭环指标**：\n - NeuroNCAP分数：在静止、正面、侧面等不同场景下的得分，值越高越好。\n - 碰撞率（%）：在静止、正面、侧面等不同场景下的碰撞率，值越低越好。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Lanzhou University, China</p>\n<p>National University of Singapore, Singapore</p>\n<p>University of Science and Technology of China, China</p>\n<p>Tsinghua University, China</p>\n<p>University of New South Wales, Australia</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为Reasoning-VLA的通用且快速的视觉-语言-动作（VLA）推理模型，用于解决现有自动驾驶VLA模型在推理效率和泛化能力方面的不足。该模型通过一组可学习的动作查询与增强推理的视觉-语言特征交互，实现连续动作轨迹的并行生成。为了提升泛化性，研究整合了八个公开的自动驾驶数据集，构建了一个基于思维链推理的统一数据集。结合监督微调（SFT）和强化学习（RL）的训练策略，实验表明Reasoning-VLA在多个基准上实现了最先进的性能、卓越的泛化能力和目前报告的优异推理速度。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了Reasoning-VLA，一个高效的VLA框架，通过可学习的动作查询与推理增强的视觉-语言表示交互，实现单步并行动作生成。</li><li>通过从真实轨迹中进行高斯分布采样来初始化可学习的动作查询，提升了模型效率。</li><li>构建了一个统一的、基于思维链推理的自动驾驶数据集，该数据集融合了八个现有数据集，以促进模型在不同车辆类型和驾驶环境下的泛化能力。</li><li>采用结合SFT和RL的微调策略，并辅以物理和动力学奖励函数，增强了模型的通用推理能力，显著优于先前的方法。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法基于Qwen2.5-VL模型构建，包含三个主要部分：</p>\n<ol><li><strong>推理增强的视觉-语言模型主干</strong>：利用Qwen2.5-VL作为基础模型，提供强大的视觉-语言理解和推理能力。</li><li><strong>VL到Action模块</strong>：引入一组可学习的动作查询，这些查询通过自注意力和与VLM特征的交叉注意力进行交互，从VLM的推理内容中提取动作相关信息，实现并行动作轨迹的预测。</li><li><strong>动作细化模块（ARM）</strong>：通过多层感知机（MLP）和注意力机制处理动作查询的隐藏状态，以细化特征表示，提高轨迹精度。</li></ol>\n<p>训练过程分为两个阶段：</p>\n<ol><li><strong>监督微调（SFT）</strong>：使用统一的推理数据集对模型进行初步训练，建立结构化的推理链。</li><li><strong>强化学习（RL）微调</strong>：采用GRPO算法和设计的基于规则的奖励函数进行优化，以提高模型在未见场景下的泛化能力。奖励函数包括物理轨迹奖励、车辆动态奖励（转向和加速度约束）以及它们的加权和。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>：</p>\n<p>构建了一个统一的自动驾驶数据集，整合了八个公开数据集：NAVSIM, nuScenes, Waymo, Argoverse-V2, KITTI, Mapillary, ONCE, 和 IDD。经过筛选和处理，包含了超过75,000个高质量片段，并生成了思维链描述。</p>\n<p><strong>训练资源</strong>：</p>\n<p>训练在8张H200 GPU上进行，总批大小为8。SFT阶段训练4个周期，学习率从5e-4开始；RL阶段训练1个周期，学习率从1e-6开始。累积梯度步数为2。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>：</p>\n<ol><li><strong>开环评估</strong>：在nuScenes数据集的验证集上进行，与现有方法进行公平比较。</li><li><strong>闭环评估</strong>：在NeuroNCAP模拟器上进行，该模拟器能够模拟复杂的现实世界驾驶场景。</li></ol>\n<p><strong>评估指标</strong>：</p>\n<p>1. <strong>开环指标</strong>：</p>\n<p> - L2误差（米）：在1秒、2秒、3秒未来时间点的平均L2距离，值越低越好。</p>\n<p> - 碰撞率（%）：在1秒、2秒、3秒未来时间点的碰撞率，值越低越好。</p>\n<p>2. <strong>闭环指标</strong>：</p>\n<p> - NeuroNCAP分数：在静止、正面、侧面等不同场景下的得分，值越高越好。</p>\n<p> - 碰撞率（%）：在静止、正面、侧面等不同场景下的碰撞率，值越低越好。</p>"
  },
  {
    "date": "2025-11-25",
    "title": "MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization",
    "link": "http://arxiv.org/abs/2511.19878",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-25",
    "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
    "link": "http://arxiv.org/abs/2511.19861",
    "summary_markdown": "### 论文研究单位\nGigaAI\n### 论文概述\n本文介绍了GigaWorld-0，一个统一的世界模型框架，旨在作为视觉-语言-动作（VLA）学习的数据引擎。它整合了两个协同组件：用于生成大规模视频的GigaWorld-0-Video和结合3D生成建模、物理仿真与运动规划的GigaWorld-0-3D，共同生成视觉丰富、几何一致且物理合理的具身交互数据。通过高效的GigaTrain训练框架实现大规模训练，实验证明其生成数据能显著提升VLA模型在真实机器人任务中的泛化能力和成功率。\n### 论文核心贡献点\n1. 提出GigaWorld-0作为数据引擎，整合视频生成与3D物理仿真，解决具身AI的数据瓶颈问题。\n2. 设计GigaWorld-0-Video系列模型，支持文本控制的外观、视角和动作迁移，以及多视角生成。\n3. 开发GigaWorld-0-3D模块，实现几何一致的3D场景重建、物理属性标注和可执行动作规划。\n4. 构建GigaTrain训练框架，利用FP8精度和稀疏注意力优化大规模视频生成效率。\n5. 通过下游任务验证，证明生成数据能提升VLA模型在真实机器人任务中的性能。\n### 论文方法描述\n1. **GigaWorld-0-Video-Dreamer**：基于流匹配和MoE架构的视频基础模型，支持图像-文本到视频生成。\n2. **GigaWorld-0-Video-AppearanceTransfer**：通过轻量控制分支实现文本引导的纹理、材质和光照编辑。\n3. **GigaWorld-0-Video-ViewTransfer**：结合双条件控制，生成新视角视频并同步调整机器人动作。\n4. **GigaWorld-0-Video-MimicTransfer**：将第一人称人类操作视频转换为机器人可执行轨迹。\n5. **GigaWorld-0-3D**：通过3D高斯溅射重建背景，生成式建模创建前景，结合物理属性标注和动作规划，确保几何与物理一致性。\n6. **GigaTrain框架**：支持FP8精度、稀疏注意力和分布式训练策略，优化计算资源使用。\n### 论文使用数据集和训练资源\n1. **数据集**：结合公开数据集（AgiBotWorld、RoboMind）与私有数据，覆盖工业、商业、办公等14类场景，任务涵盖基础操作到长时序交互。\n2. **训练资源**：使用自研GigaTrain框架，在480×768分辨率下训练61帧序列，支持多GPU/节点分布式训练与混合精度（FP8）优化。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：在DreamGen Bench和PBench Robot Set基准测试，定量评估生成质量；真实机器人部署（如AgiBot G1）验证下游任务性能。\n2. **评估指标**：物理合理性、几何一致性、文本对齐度、多视角一致性、视觉保真度；下游任务成功率与泛化能力。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>GigaAI</p>\n<h3>论文概述</h3>\n<p>本文介绍了GigaWorld-0，一个统一的世界模型框架，旨在作为视觉-语言-动作（VLA）学习的数据引擎。它整合了两个协同组件：用于生成大规模视频的GigaWorld-0-Video和结合3D生成建模、物理仿真与运动规划的GigaWorld-0-3D，共同生成视觉丰富、几何一致且物理合理的具身交互数据。通过高效的GigaTrain训练框架实现大规模训练，实验证明其生成数据能显著提升VLA模型在真实机器人任务中的泛化能力和成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出GigaWorld-0作为数据引擎，整合视频生成与3D物理仿真，解决具身AI的数据瓶颈问题。</li><li>设计GigaWorld-0-Video系列模型，支持文本控制的外观、视角和动作迁移，以及多视角生成。</li><li>开发GigaWorld-0-3D模块，实现几何一致的3D场景重建、物理属性标注和可执行动作规划。</li><li>构建GigaTrain训练框架，利用FP8精度和稀疏注意力优化大规模视频生成效率。</li><li>通过下游任务验证，证明生成数据能提升VLA模型在真实机器人任务中的性能。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>GigaWorld-0-Video-Dreamer</strong>：基于流匹配和MoE架构的视频基础模型，支持图像-文本到视频生成。</li><li><strong>GigaWorld-0-Video-AppearanceTransfer</strong>：通过轻量控制分支实现文本引导的纹理、材质和光照编辑。</li><li><strong>GigaWorld-0-Video-ViewTransfer</strong>：结合双条件控制，生成新视角视频并同步调整机器人动作。</li><li><strong>GigaWorld-0-Video-MimicTransfer</strong>：将第一人称人类操作视频转换为机器人可执行轨迹。</li><li><strong>GigaWorld-0-3D</strong>：通过3D高斯溅射重建背景，生成式建模创建前景，结合物理属性标注和动作规划，确保几何与物理一致性。</li><li><strong>GigaTrain框架</strong>：支持FP8精度、稀疏注意力和分布式训练策略，优化计算资源使用。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：结合公开数据集（AgiBotWorld、RoboMind）与私有数据，覆盖工业、商业、办公等14类场景，任务涵盖基础操作到长时序交互。</li><li><strong>训练资源</strong>：使用自研GigaTrain框架，在480×768分辨率下训练61帧序列，支持多GPU/节点分布式训练与混合精度（FP8）优化。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：在DreamGen Bench和PBench Robot Set基准测试，定量评估生成质量；真实机器人部署（如AgiBot G1）验证下游任务性能。</li><li><strong>评估指标</strong>：物理合理性、几何一致性、文本对齐度、多视角一致性、视觉保真度；下游任务成功率与泛化能力。</li></ol>"
  },
  {
    "date": "2025-11-25",
    "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation",
    "link": "http://arxiv.org/abs/2511.19859",
    "summary_markdown": "### 论文研究单位\n南京大学计算机软件新技术国家重点实验室（State Key Laboratory for Novel Software Technology, Nanjing University）\n### 论文概述\n本文提出VITA（Vision-Integrated Trajectory Alignment）框架，旨在统一视觉感知与机器人动作生成。通过构建跨模态共享的离散潜在空间，VITA将视觉观察与低级动作对齐，并引入隐式视觉思维链（Implicit Visual Chain-of-Thought）：自回归生成的token同时解码为未来帧预测和机器人动作序列，使视觉动态作为动作规划的归纳偏置。该方法解决了视觉-动作模态 gap 和训练不稳定性问题，在模拟与真实环境中实现SOTA性能。\n### 论文核心贡献点\n1. **框架创新**：提出VITA框架，通过统一潜在空间对齐感知与动作，并将未来帧预测内化为动作生成的归纳偏置，实现正向与逆向动力学联合建模。\n2. **训练策略**：设计渐进式训练方法（warmup + co-train + fine-tune），使模型从大规模人类演示视频中学习通用运动动态，同时过滤无关像素细节。\n3. **性能提升**：在CALVIN、LIBERO和SimplerEnv模拟基准上分别提升14.5%、9.6%和12.1%，真实世界任务平均成功率达80.5%。\n### 论文方法描述\n1. **跨模态向量量化框架**：\n - **视觉分支**：使用DINOv2提取连续帧特征，经M-Former生成运动嵌入，通过共享码本量化后解码为未来帧（L1 + SSIM损失）。\n - **动作分支**：对动作序列进行离散余弦变换（DCT）和频率编码，量化后解码重建动作（MSE损失）。\n - 共享码本（Codebook）统一视觉与动作表示，无需跨模态对齐数据。\n2. **VLM主干架构**：\n - **渐进注意力机制**：分阶段处理输入token → 文本子任务token → 跨模态token，确保信息流单向性（Input → Textual → Cross-modal）。\n - **文本CoT**：生成符号化子任务序列（如[GRASP], [MOVE]），作为高层规划指导。\n - **隐式视觉CoT**：基于子任务token生成跨模态token，同步解码为未来帧和动作序列。\n3. **训练流程**：\n - **Warmup阶段**：独立训练视觉/动作自编码器和共享码本，使用单模态数据（无监督重建损失）。\n - **Co-train阶段**：冻结码本，联合训练VLM与双解码器，混合使用视频数据和同步视频-动作数据（视觉损失 + 动作损失）。\n - **Fine-tune阶段**：仅微调动作解码器于特定任务数据。\n### 论文使用数据集和训练资源\n1. **混合数据集**：\n - 人类演示视频：SSv2、Ego4D。\n - 机器人数据（真实）：OXE、RoboMIND。\n - 机器人数据（模拟）：CALVIN-ABC、LIBERO。\n2. **训练资源**：\n - 模型规模：SigLIP视觉编码器（400M） + Gemma主干（2B） + 视觉/动作解码器（共约324M）。\n - 硬件：16块NVIDIA A100 GPU。\n - 时长：约5天，300K训练步数。\n### 论文使用的评估环境和评估指标\n1. **模拟环境**：\n - **CALVIN**：多步骤家庭任务，评估连续5条指令的完成率（Average Number of Tasks）。\n - **LIBERO**：多任务套件（GOAL/SPATIAL/OBJECT/LONG），评估任务平均成功率。\n - **SimplerEnv**：Google Robot和WidowX平台，评估视觉匹配任务成功率。\n2. **真实环境**：\n - **UR-5e机器人平台**：6项桌面操作任务，评估分布内（ID）和分布外（OOD）成功率。\n3. **评估指标**：\n - **CALVIN**：连续任务完成率（1-5任务）及平均完成长度（Avg. Len）。\n - **LIBERO/SimplerEnv**：任务平均成功率（%）。\n - **真实世界**：任务成功率（%），对比基线包括Pi0、OpenVLA、GR00T N1.5。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南京大学计算机软件新技术国家重点实验室（State Key Laboratory for Novel Software Technology, Nanjing University）</p>\n<h3>论文概述</h3>\n<p>本文提出VITA（Vision-Integrated Trajectory Alignment）框架，旨在统一视觉感知与机器人动作生成。通过构建跨模态共享的离散潜在空间，VITA将视觉观察与低级动作对齐，并引入隐式视觉思维链（Implicit Visual Chain-of-Thought）：自回归生成的token同时解码为未来帧预测和机器人动作序列，使视觉动态作为动作规划的归纳偏置。该方法解决了视觉-动作模态 gap 和训练不稳定性问题，在模拟与真实环境中实现SOTA性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>框架创新</strong>：提出VITA框架，通过统一潜在空间对齐感知与动作，并将未来帧预测内化为动作生成的归纳偏置，实现正向与逆向动力学联合建模。</li><li><strong>训练策略</strong>：设计渐进式训练方法（warmup + co-train + fine-tune），使模型从大规模人类演示视频中学习通用运动动态，同时过滤无关像素细节。</li><li><strong>性能提升</strong>：在CALVIN、LIBERO和SimplerEnv模拟基准上分别提升14.5%、9.6%和12.1%，真实世界任务平均成功率达80.5%。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>跨模态向量量化框架</strong>：</p>\n<p> - <strong>视觉分支</strong>：使用DINOv2提取连续帧特征，经M-Former生成运动嵌入，通过共享码本量化后解码为未来帧（L1 + SSIM损失）。</p>\n<p> - <strong>动作分支</strong>：对动作序列进行离散余弦变换（DCT）和频率编码，量化后解码重建动作（MSE损失）。</p>\n<p> - 共享码本（Codebook）统一视觉与动作表示，无需跨模态对齐数据。</p>\n<p>2. <strong>VLM主干架构</strong>：</p>\n<p> - <strong>渐进注意力机制</strong>：分阶段处理输入token → 文本子任务token → 跨模态token，确保信息流单向性（Input → Textual → Cross-modal）。</p>\n<p> - <strong>文本CoT</strong>：生成符号化子任务序列（如[GRASP], [MOVE]），作为高层规划指导。</p>\n<p> - <strong>隐式视觉CoT</strong>：基于子任务token生成跨模态token，同步解码为未来帧和动作序列。</p>\n<p>3. <strong>训练流程</strong>：</p>\n<p> - <strong>Warmup阶段</strong>：独立训练视觉/动作自编码器和共享码本，使用单模态数据（无监督重建损失）。</p>\n<p> - <strong>Co-train阶段</strong>：冻结码本，联合训练VLM与双解码器，混合使用视频数据和同步视频-动作数据（视觉损失 + 动作损失）。</p>\n<p> - <strong>Fine-tune阶段</strong>：仅微调动作解码器于特定任务数据。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>混合数据集</strong>：</p>\n<p> - 人类演示视频：SSv2、Ego4D。</p>\n<p> - 机器人数据（真实）：OXE、RoboMIND。</p>\n<p> - 机器人数据（模拟）：CALVIN-ABC、LIBERO。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - 模型规模：SigLIP视觉编码器（400M） + Gemma主干（2B） + 视觉/动作解码器（共约324M）。</p>\n<p> - 硬件：16块NVIDIA A100 GPU。</p>\n<p> - 时长：约5天，300K训练步数。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>模拟环境</strong>：</p>\n<p> - <strong>CALVIN</strong>：多步骤家庭任务，评估连续5条指令的完成率（Average Number of Tasks）。</p>\n<p> - <strong>LIBERO</strong>：多任务套件（GOAL/SPATIAL/OBJECT/LONG），评估任务平均成功率。</p>\n<p> - <strong>SimplerEnv</strong>：Google Robot和WidowX平台，评估视觉匹配任务成功率。</p>\n<p>2. <strong>真实环境</strong>：</p>\n<p> - <strong>UR-5e机器人平台</strong>：6项桌面操作任务，评估分布内（ID）和分布外（OOD）成功率。</p>\n<p>3. <strong>评估指标</strong>：</p>\n<p> - <strong>CALVIN</strong>：连续任务完成率（1-5任务）及平均完成长度（Avg. Len）。</p>\n<p> - <strong>LIBERO/SimplerEnv</strong>：任务平均成功率（%）。</p>\n<p> - <strong>真实世界</strong>：任务成功率（%），对比基线包括Pi0、OpenVLA、GR00T N1.5。</p>"
  },
  {
    "date": "2025-11-24",
    "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories",
    "link": "http://arxiv.org/abs/2511.19528",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n香港科技大学、清华大学、武汉大学、中南大学、微软研究院\n## 论文概述\n该论文提出了一个名为“发现、学习和强化”（DLR）的三阶段框架，用于生成多样化且高质量的机器人轨迹，以扩大视觉-语言-动作（VLA）模型的预训练规模。该框架旨在通过强化学习（RL）生成数据，解决当前依赖人类遥操作数据成本高昂且行为多样性有限的问题。论文通过理论分析和实验验证了DLR在生成多样化轨迹和提高下游任务泛化能力方面的有效性。\n## 论文核心贡献点\n1. 提出了一个原则性的三阶段框架DLR，利用强化学习为VLA预训练生成高质量且多样化的机器人轨迹。\n2. 提供了理论分析，证明DLR能够保持发现模式的多样性，防止模式坍塌到单一解决方案。\n3. 实验表明，DLR不仅能生成多样化的成功轨迹，还能使预训练的VLA模型在下游任务微调后表现更好。\n## 论文方法描述\nDLR框架包含三个阶段：\n1. 发现：使用基于变分自编码器（VAE）的信息论方法从人类演示中挖掘潜在的多样化行为模式。\n2. 学习：通过行为克隆训练一个条件策略来模仿这些发现的行为模式。\n3. 强化：使用稀疏的任务奖励在线优化每个条件策略，使不同模式收敛到各自对应的精炼解决方案。\n该方法通过解耦多样性目标与策略的探索过程，将多样性目标仅应用于成功轨迹内的状态，避免了标准方法中探索与多样性奖励之间的冲突。\n## 论文使用数据集和训练资源\n数据集：LIBERO基准，特别是LIBERO-90用于预训练，LIBEROSpatial/LIBEROObject/LIBEROGoal/LIBEROLong用于下游评估。\n训练资源：使用ResNet18作为视觉编码器，结合多层感知机（MLP）头输出动作。强化学习算法采用PPO。DLR设置模式数量\\|Z\\|=3，并使用均匀分布采样。在相同的预训练步数和数据量下与基线公平比较。\n## 论文使用的评估环境和评估指标\n评估环境：LIBERO模拟环境，包含多种操作任务，用于评估 lifelong learning 的知识迁移能力。\n评估指标：\n- 轨迹多样性指标：包括平均成对距离、终点方差、方向方差和路径长度方差。\n- 下游任务成功率：在LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long任务套件上的平均成功率，通过50次运行取平均。\n- 数据缩放行为：通过绘制下游成功率随RL生成数据量变化的曲线来评估。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>香港科技大学、清华大学、武汉大学、中南大学、微软研究院</p>\n<h2>论文概述</h2>\n<p>该论文提出了一个名为“发现、学习和强化”（DLR）的三阶段框架，用于生成多样化且高质量的机器人轨迹，以扩大视觉-语言-动作（VLA）模型的预训练规模。该框架旨在通过强化学习（RL）生成数据，解决当前依赖人类遥操作数据成本高昂且行为多样性有限的问题。论文通过理论分析和实验验证了DLR在生成多样化轨迹和提高下游任务泛化能力方面的有效性。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出了一个原则性的三阶段框架DLR，利用强化学习为VLA预训练生成高质量且多样化的机器人轨迹。</li><li>提供了理论分析，证明DLR能够保持发现模式的多样性，防止模式坍塌到单一解决方案。</li><li>实验表明，DLR不仅能生成多样化的成功轨迹，还能使预训练的VLA模型在下游任务微调后表现更好。</li></ol>\n<h2>论文方法描述</h2>\n<p>DLR框架包含三个阶段：</p>\n<ol><li>发现：使用基于变分自编码器（VAE）的信息论方法从人类演示中挖掘潜在的多样化行为模式。</li><li>学习：通过行为克隆训练一个条件策略来模仿这些发现的行为模式。</li><li>强化：使用稀疏的任务奖励在线优化每个条件策略，使不同模式收敛到各自对应的精炼解决方案。</li></ol>\n<p>该方法通过解耦多样性目标与策略的探索过程，将多样性目标仅应用于成功轨迹内的状态，避免了标准方法中探索与多样性奖励之间的冲突。</p>\n<h2>论文使用数据集和训练资源</h2>\n<p>数据集：LIBERO基准，特别是LIBERO-90用于预训练，LIBEROSpatial/LIBEROObject/LIBEROGoal/LIBEROLong用于下游评估。</p>\n<p>训练资源：使用ResNet18作为视觉编码器，结合多层感知机（MLP）头输出动作。强化学习算法采用PPO。DLR设置模式数量\\|Z\\|=3，并使用均匀分布采样。在相同的预训练步数和数据量下与基线公平比较。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>评估环境：LIBERO模拟环境，包含多种操作任务，用于评估 lifelong learning 的知识迁移能力。</p>\n<p>评估指标：</p>\n<ul><li>轨迹多样性指标：包括平均成对距离、终点方差、方向方差和路径长度方差。</li><li>下游任务成功率：在LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long任务套件上的平均成功率，通过50次运行取平均。</li><li>数据缩放行为：通过绘制下游成功率随RL生成数据量变化的曲线来评估。</li></ul>"
  },
  {
    "date": "2025-11-24",
    "title": "Mixture of Horizons in Action Chunking",
    "link": "http://arxiv.org/abs/2511.19433",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n中国人民大学 (RUC), 北卡罗来纳大学教堂山分校 (UNC), 香港中文大学 (CUHK)\n## 论文概述\n该论文研究了视觉-语言-动作（VLA）模型中动作块长度（horizon）的影响。研究发现存在一个固有的权衡：较长的horizon提供更好的全局规划能力但会降低细粒度精度，而较短的horizon提供精确的局部控制但在长期任务上表现不佳。为解决这一问题，论文提出了混合视界策略，通过将动作块重组为不同长度的分段，使用共享的动作变换器并行处理，并通过轻量级线性门控融合输出。\n## 论文核心贡献点\n1. 系统研究了VLA中动作块horizon的影响，揭示了长期预测与短期精度之间的关键权衡。\n2. 提出了混合视界策略，这是一个即插即用、低开销的方法，缓解了上述权衡，提升了性能和泛化能力。\n3. 提出了通过跨horizon共识的动态推理方案，以实现更稳定、更快的执行。\n## 论文方法描述\n混合视界策略包含以下关键组件：\n- 动作块重排：将最大horizon H的动作块按候选集合 H={h1, ..., hN} 截断为多个子块。\n- 共享处理：使用单一动作变换器并行处理所有horizon的子块，通过特定注意力掩码处理不同长度。\n- 门控融合：添加轻量级线性门控头，为每个时间步和horizon生成权重，通过softmax归一化后加权融合各horizon预测。\n- 平衡损失：引入负载平衡损失，鼓励所有horizon被均衡利用，避免门控网络退化。\n- 动态推理：通过跨horizon共识机制，选择稳定的动作前缀执行，将不确定动作推迟到下次重规划，提高吞吐量。\n## 论文使用数据集和训练资源\n- 数据集：LIBERO（包含Spatial、Object、Goal、Long四个任务套件）和RoboTwin 2.0（包含50个双手机器人任务）。\n- 训练资源：4块NVIDIA A100 GPU，训练30k迭代，批量大小32，LIBERO训练在10小时内完成，RoboTwin训练约3k-10k迭代。\n- 基础模型：基于PaliGemma的π系列模型（π0、π0.5、πreg）。\n## 论文使用的评估环境和评估指标\n- 评估环境：LIBERO仿真环境（评估500次试验）和RoboTwin 2.0仿真环境（简单和困难模式各评估100次试验）。\n- 评估指标：任务成功率，LIBERO执行每个动作块的前5步，RoboTwin执行前20步。\n- 比较方法：包括Octo、OpenVLA、Diffusion Policy等SOTA方法，在相同随机种子下进行公平比较。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>中国人民大学 (RUC), 北卡罗来纳大学教堂山分校 (UNC), 香港中文大学 (CUHK)</p>\n<h2>论文概述</h2>\n<p>该论文研究了视觉-语言-动作（VLA）模型中动作块长度（horizon）的影响。研究发现存在一个固有的权衡：较长的horizon提供更好的全局规划能力但会降低细粒度精度，而较短的horizon提供精确的局部控制但在长期任务上表现不佳。为解决这一问题，论文提出了混合视界策略，通过将动作块重组为不同长度的分段，使用共享的动作变换器并行处理，并通过轻量级线性门控融合输出。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>系统研究了VLA中动作块horizon的影响，揭示了长期预测与短期精度之间的关键权衡。</li><li>提出了混合视界策略，这是一个即插即用、低开销的方法，缓解了上述权衡，提升了性能和泛化能力。</li><li>提出了通过跨horizon共识的动态推理方案，以实现更稳定、更快的执行。</li></ol>\n<h2>论文方法描述</h2>\n<p>混合视界策略包含以下关键组件：</p>\n<ul><li>动作块重排：将最大horizon H的动作块按候选集合 H={h1, ..., hN} 截断为多个子块。</li><li>共享处理：使用单一动作变换器并行处理所有horizon的子块，通过特定注意力掩码处理不同长度。</li><li>门控融合：添加轻量级线性门控头，为每个时间步和horizon生成权重，通过softmax归一化后加权融合各horizon预测。</li><li>平衡损失：引入负载平衡损失，鼓励所有horizon被均衡利用，避免门控网络退化。</li><li>动态推理：通过跨horizon共识机制，选择稳定的动作前缀执行，将不确定动作推迟到下次重规划，提高吞吐量。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据集：LIBERO（包含Spatial、Object、Goal、Long四个任务套件）和RoboTwin 2.0（包含50个双手机器人任务）。</li><li>训练资源：4块NVIDIA A100 GPU，训练30k迭代，批量大小32，LIBERO训练在10小时内完成，RoboTwin训练约3k-10k迭代。</li><li>基础模型：基于PaliGemma的π系列模型（π0、π0.5、πreg）。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：LIBERO仿真环境（评估500次试验）和RoboTwin 2.0仿真环境（简单和困难模式各评估100次试验）。</li><li>评估指标：任务成功率，LIBERO执行每个动作块的前5步，RoboTwin执行前20步。</li><li>比较方法：包括Octo、OpenVLA、Diffusion Policy等SOTA方法，在相同随机种子下进行公平比较。</li></ul>"
  },
  {
    "date": "2025-11-24",
    "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.19221",
    "summary_markdown": "### 论文研究单位\nYinwang Intelligent Technology Co. Ltd., Fudan University\n### 论文概述\nPercept-WAM是一种感知增强的世界-意识-动作模型，首次在单个视觉语言模型中隐式集成2D/3D场景理解能力。该模型通过World-PV和World-BEV token统一2D和3D感知任务，编码空间坐标和置信度。采用网格条件预测机制、IoU感知评分和平行自回归解码，提升长尾场景的稳定性。模型保留预训练VLM的通用智能能力，可直接输出感知结果和轨迹控制。实验表明Percept-WAM在下游感知基准匹配或超越经典检测器，并在nuScenes和NAVSIM上提升规划性能。\n### 论文核心贡献点\n1. 感知增强的世界Token：首次通过World-PV和World-BEV token在单一VLM中统一2D/3D感知，编码度量坐标和校准置信度。\n2. 网格条件密集感知：引入网格条件预测头，结合IoU感知评分和平行AR解码，显著提升长尾、远距离和小物体感知的准确性和稳定性。\n3. 感知到动作范式：在nuScenes和NAVSIM上超越专业检测/分割基线，通过World-PV、World-BEV和World-Action token的对齐实现卓越规划性能。\n### 论文方法描述\nPercept-WAM包含三部分：1) VLM主干（InternVL2-8B）维持通用推理能力；2) 可学习BEV网格token隐式建模PV特征到BEV空间的映射；3) 动作专家头用于轨迹解码。World-PV分支处理透视视图感知，支持高分辨率输入和平行AR解码，输出序列格式为cls, <box>坐标</box>, <conf>置信度</conf>。World-BEV分支通过可学习查询token实现BEV空间感知，支持多模态融合。动作模块通过四组点级查询（Q_pv, Q_bev, Q_ego, Q_full）对齐多模态信息，使用MLP解码轨迹，并采用流式KV缓存策略提升效率。\n### 论文使用数据集和训练资源\n数据集：nuImages（2D检测/实例分割/语义分割）、nuScenes（3D检测/BEV分割/轨迹预测）、COCO（2D检测/实例分割/语义分割）、Waymo（3D检测）、RefCOCO系列（定位）、ADE20K（语义分割）、COCOStuff（语义分割）、NAVSIM（轨迹预测）、DriveLM-nuscenes（QA）等。训练使用AdamW优化器（基础学习率0.0002，权重衰减0.01），余弦衰减学习率，混合精度训练，梯度检查点。World-PV采用10×10网格，World-BEV检测使用40×40网格。\n### 论文使用的评估环境和评估指标\n评估环境：nuScenes验证集（PV/BEV感知任务）、NAVSIM v1（闭环规划）、nuScenes开放环路指标（轨迹L2误差）。评估指标：2D/3D检测mAP、NDS（nuScenes检测分数）、实例分割mAP、语义分割mIoU、轨迹L2误差（1s/2s/3s）、NAVSIM闭环指标（NC碰撞率、DAC动态舒适度、TTC时间至碰撞、Comf舒适性、EP效率、PDMS规划得分）、帧延迟（ms）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Yinwang Intelligent Technology Co. Ltd., Fudan University</p>\n<h3>论文概述</h3>\n<p>Percept-WAM是一种感知增强的世界-意识-动作模型，首次在单个视觉语言模型中隐式集成2D/3D场景理解能力。该模型通过World-PV和World-BEV token统一2D和3D感知任务，编码空间坐标和置信度。采用网格条件预测机制、IoU感知评分和平行自回归解码，提升长尾场景的稳定性。模型保留预训练VLM的通用智能能力，可直接输出感知结果和轨迹控制。实验表明Percept-WAM在下游感知基准匹配或超越经典检测器，并在nuScenes和NAVSIM上提升规划性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>感知增强的世界Token：首次通过World-PV和World-BEV token在单一VLM中统一2D/3D感知，编码度量坐标和校准置信度。</li><li>网格条件密集感知：引入网格条件预测头，结合IoU感知评分和平行AR解码，显著提升长尾、远距离和小物体感知的准确性和稳定性。</li><li>感知到动作范式：在nuScenes和NAVSIM上超越专业检测/分割基线，通过World-PV、World-BEV和World-Action token的对齐实现卓越规划性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>Percept-WAM包含三部分：1) VLM主干（InternVL2-8B）维持通用推理能力；2) 可学习BEV网格token隐式建模PV特征到BEV空间的映射；3) 动作专家头用于轨迹解码。World-PV分支处理透视视图感知，支持高分辨率输入和平行AR解码，输出序列格式为cls, <box>坐标</box>, <conf>置信度</conf>。World-BEV分支通过可学习查询token实现BEV空间感知，支持多模态融合。动作模块通过四组点级查询（Q_pv, Q_bev, Q_ego, Q_full）对齐多模态信息，使用MLP解码轨迹，并采用流式KV缓存策略提升效率。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：nuImages（2D检测/实例分割/语义分割）、nuScenes（3D检测/BEV分割/轨迹预测）、COCO（2D检测/实例分割/语义分割）、Waymo（3D检测）、RefCOCO系列（定位）、ADE20K（语义分割）、COCOStuff（语义分割）、NAVSIM（轨迹预测）、DriveLM-nuscenes（QA）等。训练使用AdamW优化器（基础学习率0.0002，权重衰减0.01），余弦衰减学习率，混合精度训练，梯度检查点。World-PV采用10×10网格，World-BEV检测使用40×40网格。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：nuScenes验证集（PV/BEV感知任务）、NAVSIM v1（闭环规划）、nuScenes开放环路指标（轨迹L2误差）。评估指标：2D/3D检测mAP、NDS（nuScenes检测分数）、实例分割mAP、语义分割mIoU、轨迹L2误差（1s/2s/3s）、NAVSIM闭环指标（NC碰撞率、DAC动态舒适度、TTC时间至碰撞、Comf舒适性、EP效率、PDMS规划得分）、帧延迟（ms）。</p>"
  },
  {
    "date": "2025-11-24",
    "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
    "link": "http://arxiv.org/abs/2511.18960",
    "summary_markdown": "### 论文研究单位\nLiAuto Inc., School of Information Science and Technology, Beijing University of Technology, School of Data Science, The Chinese University of Hong Kong, Shenzhen\n### 论文概述\n论文指出现有的视觉-语言-行动模型通常基于视觉-语言模型，将每个时间步的视觉输入作为独立帧处理，这种隐式的马尔可夫决策过程建模未能有效利用历史上下文，在动态的序列决策中是次优的。为解决此问题，论文从部分可观察马尔可夫决策过程的视角重新构建了问题，提出了AVA-VLA框架。该框架引入一个循环状态作为代理信念状态的神经网络近似，并设计了主动视觉注意模块，利用该循环状态动态调制当前帧的视觉处理，使模型能基于历史上下文主动过滤无关信息并聚焦于任务关键特征。\n### 论文核心贡献点\n1. 提出了新颖的AVA-VLA框架，通过受POMDP启发的方法，首次显式地解决了基于MDP的VLA模型中缺乏历史上下文的关键限制。\n2. 引入了主动视觉注意模块，利用循环状态来动态调制当前帧的视觉处理，以进行动作预测。\n3. 在模拟和真实世界任务中进行了全面评估，证明了AVA-VLA框架提升了VLA性能，并在多个机器人任务上取得了最先进的结果。\n### 论文方法描述\nAVA-VLA框架包含三个核心部分：\n1. **问题重构建**：将机器人操作任务从MDP重构建为POMDP。引入一个循环状态 r^{t-1} 作为代理信念状态的神经近似，该状态由上一时间步 t-1 的模型中间输出计算得出。策略预测不仅基于当前观测 x^t，还显式地基于循环状态 r^{t-1}。\n2. **主动视觉注意模块**：该模块接收当前观测和循环状态，通过交叉注意力机制和自注意力层，计算视觉标记的软权重 ω^t。这些权重被应用于语言模型主干所有层的注意力矩阵，动态增强或减弱不同视觉标记的重要性，使模型能根据历史信念主动聚焦于任务相关区域。\n3. **训练与推理**：训练采用截断的反向传播时间策略，以平衡计算可行性。损失函数包括动作块的平均绝对误差损失和对软权重均值的L2正则化惩罚。推理时，模型以完全循环的方式运行，在每个时间步更新循环状态。\n### 论文使用数据集和训练资源\n- **数据集**：\n - LIBERO 和 LIBERO+ 基准\n - CALVIN 基准（ABC到D的零样本泛化设置）\n - 真实世界任务：基于Mobile ALOHA双臂机器人的四个操作任务\n- **训练资源**：\n - 基础模型：OpenVLA-OFT\n - 计算平台：Nvidia A800 GPU\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：LIBERO（MuJoCo模拟器）和CALVIN的模拟环境。\n - 真实环境：一个桌置式双臂机器人平台。\n- **评估指标**：\n - 成功率：用于LIBERO、CALVIN和真实世界任务的主要评估指标。\n - 平均长度：用于CALVIN基准的补充指标，衡量任务完成的平均长度。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>LiAuto Inc., School of Information Science and Technology, Beijing University of Technology, School of Data Science, The Chinese University of Hong Kong, Shenzhen</p>\n<h3>论文概述</h3>\n<p>论文指出现有的视觉-语言-行动模型通常基于视觉-语言模型，将每个时间步的视觉输入作为独立帧处理，这种隐式的马尔可夫决策过程建模未能有效利用历史上下文，在动态的序列决策中是次优的。为解决此问题，论文从部分可观察马尔可夫决策过程的视角重新构建了问题，提出了AVA-VLA框架。该框架引入一个循环状态作为代理信念状态的神经网络近似，并设计了主动视觉注意模块，利用该循环状态动态调制当前帧的视觉处理，使模型能基于历史上下文主动过滤无关信息并聚焦于任务关键特征。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了新颖的AVA-VLA框架，通过受POMDP启发的方法，首次显式地解决了基于MDP的VLA模型中缺乏历史上下文的关键限制。</li><li>引入了主动视觉注意模块，利用循环状态来动态调制当前帧的视觉处理，以进行动作预测。</li><li>在模拟和真实世界任务中进行了全面评估，证明了AVA-VLA框架提升了VLA性能，并在多个机器人任务上取得了最先进的结果。</li></ol>\n<h3>论文方法描述</h3>\n<p>AVA-VLA框架包含三个核心部分：</p>\n<ol><li><strong>问题重构建</strong>：将机器人操作任务从MDP重构建为POMDP。引入一个循环状态 r^{t-1} 作为代理信念状态的神经近似，该状态由上一时间步 t-1 的模型中间输出计算得出。策略预测不仅基于当前观测 x^t，还显式地基于循环状态 r^{t-1}。</li><li><strong>主动视觉注意模块</strong>：该模块接收当前观测和循环状态，通过交叉注意力机制和自注意力层，计算视觉标记的软权重 ω^t。这些权重被应用于语言模型主干所有层的注意力矩阵，动态增强或减弱不同视觉标记的重要性，使模型能根据历史信念主动聚焦于任务相关区域。</li><li><strong>训练与推理</strong>：训练采用截断的反向传播时间策略，以平衡计算可行性。损失函数包括动作块的平均绝对误差损失和对软权重均值的L2正则化惩罚。推理时，模型以完全循环的方式运行，在每个时间步更新循环状态。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - LIBERO 和 LIBERO+ 基准</p>\n<p> - CALVIN 基准（ABC到D的零样本泛化设置）</p>\n<p> - 真实世界任务：基于Mobile ALOHA双臂机器人的四个操作任务</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 基础模型：OpenVLA-OFT</p>\n<p> - 计算平台：Nvidia A800 GPU</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：LIBERO（MuJoCo模拟器）和CALVIN的模拟环境。</p>\n<p> - 真实环境：一个桌置式双臂机器人平台。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率：用于LIBERO、CALVIN和真实世界任务的主要评估指标。</p>\n<p> - 平均长度：用于CALVIN基准的补充指标，衡量任务完成的平均长度。</p>"
  },
  {
    "date": "2025-11-24",
    "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.18950",
    "summary_markdown": "论文研究单位\n北京工业大学信息科学技术学院、理想汽车、北京市计算智能与智能系统重点实验室（北京工业大学）\n\n论文概述\n该论文针对视觉-语言-动作（VLA）模型在处理冗余视觉token时存在的计算开销瓶颈问题，提出了一个名为Compressor-VLA的混合式指令引导的视觉token压缩框架。该框架通过压缩视觉信息，在保持高性能的同时显著降低了计算成本。实验结果表明，在LIBERO基准测试上，该方法在降低59%的FLOPs和超过3倍的视觉token数量的情况下，仍能达到具有竞争力的成功率。此外，在真实双臂机器人平台上的部署验证了其模拟到现实的迁移能力。\n\n论文核心贡献点\n1. 提出了Compressor-VLA，一个新颖的混合式指令条件视觉token压缩框架，结合了全局和局部信息通路，用于对VLA模型中的视觉信息进行高效、面向任务的压缩。\n2. 在具有挑战性的LIBERO基准上进行了广泛实验，证明了所提出的框架在降低FLOPs的同时能保持竞争性的成功率，并在真实双臂机器人平台上部署验证了其有效性。\n3. 提供了定性和定量分析，验证了指令引导token压缩策略的核心设计原理以及全局和局部压缩通路之间的架构协同作用。\n\n论文方法描述\n该框架包含两个并行的指令引导模块：语义任务压缩器（STC）和空间精炼压缩器（SRC）。STC使用一组可学习的查询，通过特征线性调制（FiLM）机制由语言指令进行调制，然后通过交叉注意力机制从所有视觉token中提取一个与任务相关的全局场景摘要。SRC则在非重叠的局部窗口内操作，它首先通过下采样生成一个原始查询，然后将指令信息直接注入（通过加法）该查询，接着让这个被引导的查询与窗口内的原始token进行注意力交互，以保留任务相关的精细空间细节。最终的压缩视觉token序列由STC和SRC的输出拼接而成，整个过程都由语言指令动态调制。\n\n论文使用数据集和训练资源\n1. 数据集：模拟实验使用LIBERO基准数据集。真实世界实验使用了在Mobile ALOHA双臂机器人平台上为“空间感知”和“语义理解”任务自定义收集的数据集。\n2. 训练资源：模型在8块NVIDIA A100 GPU上进行训练。基于OpenVLA-OFT基础模型，并使用LoRA（秩为32）技术对视觉编码器、LLM主干和两个提出的压缩器进行微调。\n\n论文使用的评估环境和评估指标\n1. 评估环境：模拟环境为LIBERO基准，其中包含Franka Emika Panda机械臂。真实世界环境为配备Orbecc DABAI RGB-D相机的cobot magic双臂机器人平台。\n2. 评估指标：主要性能指标为任务成功率。效率通过FLOPs和压缩后的token数量进行评估。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京工业大学信息科学技术学院、理想汽车、北京市计算智能与智能系统重点实验室（北京工业大学）</p>\n\n<p>论文概述</p>\n<p>该论文针对视觉-语言-动作（VLA）模型在处理冗余视觉token时存在的计算开销瓶颈问题，提出了一个名为Compressor-VLA的混合式指令引导的视觉token压缩框架。该框架通过压缩视觉信息，在保持高性能的同时显著降低了计算成本。实验结果表明，在LIBERO基准测试上，该方法在降低59%的FLOPs和超过3倍的视觉token数量的情况下，仍能达到具有竞争力的成功率。此外，在真实双臂机器人平台上的部署验证了其模拟到现实的迁移能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了Compressor-VLA，一个新颖的混合式指令条件视觉token压缩框架，结合了全局和局部信息通路，用于对VLA模型中的视觉信息进行高效、面向任务的压缩。</li><li>在具有挑战性的LIBERO基准上进行了广泛实验，证明了所提出的框架在降低FLOPs的同时能保持竞争性的成功率，并在真实双臂机器人平台上部署验证了其有效性。</li><li>提供了定性和定量分析，验证了指令引导token压缩策略的核心设计原理以及全局和局部压缩通路之间的架构协同作用。</li></ol>\n\n<p>论文方法描述</p>\n<p>该框架包含两个并行的指令引导模块：语义任务压缩器（STC）和空间精炼压缩器（SRC）。STC使用一组可学习的查询，通过特征线性调制（FiLM）机制由语言指令进行调制，然后通过交叉注意力机制从所有视觉token中提取一个与任务相关的全局场景摘要。SRC则在非重叠的局部窗口内操作，它首先通过下采样生成一个原始查询，然后将指令信息直接注入（通过加法）该查询，接着让这个被引导的查询与窗口内的原始token进行注意力交互，以保留任务相关的精细空间细节。最终的压缩视觉token序列由STC和SRC的输出拼接而成，整个过程都由语言指令动态调制。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：模拟实验使用LIBERO基准数据集。真实世界实验使用了在Mobile ALOHA双臂机器人平台上为“空间感知”和“语义理解”任务自定义收集的数据集。</li><li>训练资源：模型在8块NVIDIA A100 GPU上进行训练。基于OpenVLA-OFT基础模型，并使用LoRA（秩为32）技术对视觉编码器、LLM主干和两个提出的压缩器进行微调。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：模拟环境为LIBERO基准，其中包含Franka Emika Panda机械臂。真实世界环境为配备Orbecc DABAI RGB-D相机的cobot magic双臂机器人平台。</li><li>评估指标：主要性能指标为任务成功率。效率通过FLOPs和压缩后的token数量进行评估。</li></ol>"
  },
  {
    "date": "2025-11-24",
    "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent",
    "link": "http://arxiv.org/abs/2511.18810",
    "summary_markdown": "论文研究单位\nUQMM Lab, The University of Queensland\n\n论文概述\n该论文探讨了视觉-语言-动作（VLA）模型在跨技能任务中的不可合并性（non-mergeability）问题。通过分析，论文发现VLA模型在微调过程中，LoRA适配器的参数会因任务而异，导致任务间参数冲突；同时，动作专家（action expert）中的自注意力机制会加深任务特异性，阻碍模块化合并。为解决这些问题，论文提出了MergeVLA，一种以合并为导向的VLA架构。MergeVLA通过稀疏激活的LoRA适配器、重新设计的动作专家（仅用交叉注意力）以及测试时的任务路由机制，实现了高效的多技能模型合并。实验在LIBERO、LIBERO-Plus、RoboTwin和真实机器人SO101上验证了其有效性，合并后模型性能与单任务微调模型相当甚至更优。\n\n论文核心贡献点\n- 识别了VLA模型不可合并性的两个关键来源：LoRA参数的极端任务特异性（超过75%的参数为“自私参数”）和动作专家中自注意力导致的跨块依赖。\n- 提出了MergeVLA架构：通过任务掩码实现LoRA适配器的稀疏激活，减少参数冲突；动作专家仅用交叉注意力，增强可组合性；设计测试时任务路由器，实现无监督任务推断。\n- 在多个基准测试（LIBERO、LIBERO-Plus、RoboTwin）和真实机器人实验中，MergeVLA的合并模型达到与独立微调专家相当或更高的成功率（如LIBERO平均90.2%），证明了跨任务、跨环境、跨实体的鲁棒性。\n\n论文方法描述\n- **任务掩码机制**：为每个任务生成二进制掩码，仅保留与合并参数一致且显著的LoRA参数，抑制冲突参数（公式3）。\n- **动作专家重设计**：移除自注意力层，仅保留交叉注意力，强制依赖预训练VLM的稳健特征；将tanh门替换为sigmoid门，确保VLM信息不被抑制。\n- **专业化层次合并**：对动作专家的浅层块进行参数平均合并，深层块（专家头）因任务特异性强而保持独立（如最后一层）。\n- **测试时任务路由**：利用合并动作专家值投影的主成分子空间，计算每个任务掩码下VLM隐藏状态的激活强度，选择最高得分任务（公式7）。\n\n论文使用数据集和训练资源\n- **数据集**：LIBERO（包含4个任务套件，每个任务50条演示）、LIBERO-Plus（10,030个任务，含7种扰动）、RoboTwin 2.0（3种实体、4个任务）、真实世界SO101机器人（3个任务，各50条人工遥操作演示）。\n- **训练资源**：所有微调在单块NVIDIA A6000 Ada GPU（48GB）上进行；VLM骨干为Qwen2.5-0.5B；动作专家从头训练；默认设置λ=0.6（掩码比例）、α=1（合并缩放）、k_r=8（路由主成分数）。\n\n论文使用的评估环境和评估指标\n- **评估环境**：LIBERO和LIBERO-Plus的模拟环境（含视觉/语言扰动）、RoboTwin的跨实体模拟环境、真实SO101机器人硬件平台。\n- **评估指标**：任务成功率（%），包括单任务微调基准（灰行）和混合任务评估（无任务先验）；在LIBERO-Plus中还按7种扰动类型报告平均成功率。",
    "summary_html": "<p>论文研究单位</p>\n<p>UQMM Lab, The University of Queensland</p>\n\n<p>论文概述</p>\n<p>该论文探讨了视觉-语言-动作（VLA）模型在跨技能任务中的不可合并性（non-mergeability）问题。通过分析，论文发现VLA模型在微调过程中，LoRA适配器的参数会因任务而异，导致任务间参数冲突；同时，动作专家（action expert）中的自注意力机制会加深任务特异性，阻碍模块化合并。为解决这些问题，论文提出了MergeVLA，一种以合并为导向的VLA架构。MergeVLA通过稀疏激活的LoRA适配器、重新设计的动作专家（仅用交叉注意力）以及测试时的任务路由机制，实现了高效的多技能模型合并。实验在LIBERO、LIBERO-Plus、RoboTwin和真实机器人SO101上验证了其有效性，合并后模型性能与单任务微调模型相当甚至更优。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>识别了VLA模型不可合并性的两个关键来源：LoRA参数的极端任务特异性（超过75%的参数为“自私参数”）和动作专家中自注意力导致的跨块依赖。</li><li>提出了MergeVLA架构：通过任务掩码实现LoRA适配器的稀疏激活，减少参数冲突；动作专家仅用交叉注意力，增强可组合性；设计测试时任务路由器，实现无监督任务推断。</li><li>在多个基准测试（LIBERO、LIBERO-Plus、RoboTwin）和真实机器人实验中，MergeVLA的合并模型达到与独立微调专家相当或更高的成功率（如LIBERO平均90.2%），证明了跨任务、跨环境、跨实体的鲁棒性。</li></ul>\n\n<p>论文方法描述</p>\n<ul><li><strong>任务掩码机制</strong>：为每个任务生成二进制掩码，仅保留与合并参数一致且显著的LoRA参数，抑制冲突参数（公式3）。</li><li><strong>动作专家重设计</strong>：移除自注意力层，仅保留交叉注意力，强制依赖预训练VLM的稳健特征；将tanh门替换为sigmoid门，确保VLM信息不被抑制。</li><li><strong>专业化层次合并</strong>：对动作专家的浅层块进行参数平均合并，深层块（专家头）因任务特异性强而保持独立（如最后一层）。</li><li><strong>测试时任务路由</strong>：利用合并动作专家值投影的主成分子空间，计算每个任务掩码下VLM隐藏状态的激活强度，选择最高得分任务（公式7）。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>数据集</strong>：LIBERO（包含4个任务套件，每个任务50条演示）、LIBERO-Plus（10,030个任务，含7种扰动）、RoboTwin 2.0（3种实体、4个任务）、真实世界SO101机器人（3个任务，各50条人工遥操作演示）。</li><li><strong>训练资源</strong>：所有微调在单块NVIDIA A6000 Ada GPU（48GB）上进行；VLM骨干为Qwen2.5-0.5B；动作专家从头训练；默认设置λ=0.6（掩码比例）、α=1（合并缩放）、k_r=8（路由主成分数）。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：LIBERO和LIBERO-Plus的模拟环境（含视觉/语言扰动）、RoboTwin的跨实体模拟环境、真实SO101机器人硬件平台。</li><li><strong>评估指标</strong>：任务成功率（%），包括单任务微调基准（灰行）和混合任务评估（无任务先验）；在LIBERO-Plus中还按7种扰动类型报告平均成功率。</li></ul>"
  },
  {
    "date": "2025-11-22",
    "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation",
    "link": "http://arxiv.org/abs/2511.18112",
    "summary_markdown": "论文研究单位\n中山大学深圳校区, 上海交通大学, 华为诺亚方舟实验室\n\n论文概述\n论文提出EchoVLA，一种用于长时程移动操作的内存感知视觉-语言-动作模型。现有的VLA模型大多局限于短时程、桌面操作，缺乏在变化空间环境中协调导航与操作所需的记忆和推理能力。受人类大脑陈述性记忆的启发，EchoVLA结合了场景记忆和情节记忆，通过粗粒度和细粒度的注意力机制引导移动臂的扩散策略。此外，论文还引入了MoMani基准，一个通过多模态大语言模型引导规划和反馈驱动优化生成的专家级长时程轨迹数据集，并辅以真实机器人演示。实验在仿真和真实环境中均表明，EchoVLA在长时程任务上超越了强基线模型。\n\n论文核心贡献点\n1. 提出了EchoVLA，一个配备协同场景和情节记忆的、用于长时程移动操作的记忆感知型VLA模型。\n2. 引入了MoMani，一个提供专家级多模态轨迹和真实机器人演示的自动化基准。\n3. 进行了广泛的仿真和真实世界实验，证明EchoVLA在长时程任务上持续优于强基线模型。\n\n论文方法描述\nEchoVLA是一个内存增强的视觉-语言-动作框架，主要包含三个部分：\n1. 多模态状态表示：将语言、多视角RGB图像、3D点云和机器人本体感觉状态编码为统一的token序列。语言和RGB图像使用冻结的SigLIP编码器，3D结构由可训练的PointAttn网络处理。\n2. 记忆检索与交互：维护两个互补的记忆库。场景记忆通过体素化的3D特征图积累跨场景的空间信息；情节记忆存储最近的多模态token序列，保留与任务相关的细粒度时间信息。模型通过一个两级（粗到细）的注意力机制检索信息：对场景记忆进行粗粒度交叉注意力，对情节记忆进行细粒度交叉注意力，然后将检索到的记忆特征融合，用于条件化后续策略。\n3. 基于扩散的动作生成：采用分部件的扩散策略，对移动基座和机械臂分别进行独立的去噪扩散过程，以建模其异构动态。该策略由融合了当前观测和记忆上下文 的表示进行条件引导。\n\n论文使用数据集和训练资源\n1. 数据集：MoMani\n - 仿真数据：通过MLLM引导的规划和反馈驱动的优化自动生成，包含四个移动操作任务（PnPCounterToStove, PnPSinkToCounter, TurnOnFaucet, TurnOnStove）和一个大规模的纯导航子集，并进行了域随机化。\n - 真实机器人数据：在TidyBot++全向移动平台上采集，包含四个移动操作任务（OpenFridge, TurnOnMicrowave, OpenDrawer, PutCupIntoSink），记录了同步的RGB-D图像、机器人状态等数据。\n2. 训练资源：\n - 硬件：使用8块NVIDIA A100 (80GB) GPU进行训练。\n - 超参数：采用AdamW优化器，学习率为1e-4，批大小为128，训练50个epoch，学习率采用余弦衰减策略，并设置了预热步数、权重衰减和梯度裁剪。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n - 仿真环境：在RoboCasa模拟器中进行，任务包括PnPCounterToStove, PnPSinkToCounter, TurnOnFaucet, TurnOnStove以及NavigateKitchen。为了增加难度，移动操作任务的初始化位置远离目标物体。\n - 真实世界环境：搭建了一个7米x7米的测试场，包含类似家庭的任务场景。机器人平台是基于开源的TidyBot++移动操作器，部署了EchoVLA以执行任务。\n2. 评估指标：\n - 主要指标：成功率，结果为50个评估轮次的平均值。\n - 对比基线：包括BC-T, Diffusion Policy, DP3, WB-VIMA, 和 \\pi_{0.5}。",
    "summary_html": "<p>论文研究单位</p>\n<p>中山大学深圳校区, 上海交通大学, 华为诺亚方舟实验室</p>\n\n<p>论文概述</p>\n<p>论文提出EchoVLA，一种用于长时程移动操作的内存感知视觉-语言-动作模型。现有的VLA模型大多局限于短时程、桌面操作，缺乏在变化空间环境中协调导航与操作所需的记忆和推理能力。受人类大脑陈述性记忆的启发，EchoVLA结合了场景记忆和情节记忆，通过粗粒度和细粒度的注意力机制引导移动臂的扩散策略。此外，论文还引入了MoMani基准，一个通过多模态大语言模型引导规划和反馈驱动优化生成的专家级长时程轨迹数据集，并辅以真实机器人演示。实验在仿真和真实环境中均表明，EchoVLA在长时程任务上超越了强基线模型。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了EchoVLA，一个配备协同场景和情节记忆的、用于长时程移动操作的记忆感知型VLA模型。</li><li>引入了MoMani，一个提供专家级多模态轨迹和真实机器人演示的自动化基准。</li><li>进行了广泛的仿真和真实世界实验，证明EchoVLA在长时程任务上持续优于强基线模型。</li></ol>\n\n<p>论文方法描述</p>\n<p>EchoVLA是一个内存增强的视觉-语言-动作框架，主要包含三个部分：</p>\n<ol><li>多模态状态表示：将语言、多视角RGB图像、3D点云和机器人本体感觉状态编码为统一的token序列。语言和RGB图像使用冻结的SigLIP编码器，3D结构由可训练的PointAttn网络处理。</li><li>记忆检索与交互：维护两个互补的记忆库。场景记忆通过体素化的3D特征图积累跨场景的空间信息；情节记忆存储最近的多模态token序列，保留与任务相关的细粒度时间信息。模型通过一个两级（粗到细）的注意力机制检索信息：对场景记忆进行粗粒度交叉注意力，对情节记忆进行细粒度交叉注意力，然后将检索到的记忆特征融合，用于条件化后续策略。</li><li>基于扩散的动作生成：采用分部件的扩散策略，对移动基座和机械臂分别进行独立的去噪扩散过程，以建模其异构动态。该策略由融合了当前观测和记忆上下文 的表示进行条件引导。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. 数据集：MoMani</p>\n<p> - 仿真数据：通过MLLM引导的规划和反馈驱动的优化自动生成，包含四个移动操作任务（PnPCounterToStove, PnPSinkToCounter, TurnOnFaucet, TurnOnStove）和一个大规模的纯导航子集，并进行了域随机化。</p>\n<p> - 真实机器人数据：在TidyBot++全向移动平台上采集，包含四个移动操作任务（OpenFridge, TurnOnMicrowave, OpenDrawer, PutCupIntoSink），记录了同步的RGB-D图像、机器人状态等数据。</p>\n<p>2. 训练资源：</p>\n<p> - 硬件：使用8块NVIDIA A100 (80GB) GPU进行训练。</p>\n<p> - 超参数：采用AdamW优化器，学习率为1e-4，批大小为128，训练50个epoch，学习率采用余弦衰减策略，并设置了预热步数、权重衰减和梯度裁剪。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> - 仿真环境：在RoboCasa模拟器中进行，任务包括PnPCounterToStove, PnPSinkToCounter, TurnOnFaucet, TurnOnStove以及NavigateKitchen。为了增加难度，移动操作任务的初始化位置远离目标物体。</p>\n<p> - 真实世界环境：搭建了一个7米x7米的测试场，包含类似家庭的任务场景。机器人平台是基于开源的TidyBot++移动操作器，部署了EchoVLA以执行任务。</p>\n<p>2. 评估指标：</p>\n<p> - 主要指标：成功率，结果为50个评估轮次的平均值。</p>\n<p> - 对比基线：包括BC-T, Diffusion Policy, DP3, WB-VIMA, 和 \\pi_{0.5}。</p>"
  },
  {
    "date": "2025-11-22",
    "title": "Continually Evolving Skill Knowledge in Vision Language Action Model",
    "link": "http://arxiv.org/abs/2511.18085",
    "summary_markdown": "### 论文研究单位\n上海交通大学、上海创新研究院、剑桥大学、北京航空航天大学、AgiBot\n### 论文概述\n论文提出Stellar VLA框架，用于视觉-语言-动作模型（VLA）的持续模仿学习（CIL）。该框架通过联合学习任务中心表示和自演化知识空间，使模型能够在获取新技能的同时缓解对先前任务的遗忘。Stellar VLA包含两个变体：T-Stellar（任务级建模）和TS-Stellar（分层任务-技能建模），旨在实现知识驱动的高效持续学习。\n### 论文核心贡献点\n1. 提出Stellar VLA框架，通过自监督的知识演化循环实现任务知识的保留与发现。\n2. 设计基于狄利克雷过程（DP）的知识空间，支持无限任务/技能聚类，自适应扩展知识表示。\n3. 引入知识引导的专家路由机制，在动作头中实现参数共享与任务特化的平衡，降低训练开销。\n4. 在LIBERO基准和真实任务中验证有效性，平均最终成功率相比基线提升超50%。\n### 论文方法描述\n1. **知识空间建模**：\n - T-Stellar使用狄利克雷过程混合模型（DPMM）构建离散任务级知识空间。\n - TS-Stellar采用分层狄利克雷过程（HDP）建模任务-技能层次关系，捕获跨任务共享的子技能。\n2. **自监督学习**：\n - 通过变分自编码器（VAE）学习任务中心表示，约束其聚合在知识空间内。\n - 使用基于记忆的变分贝叶斯（memoVB）算法更新知识分布，支持增量学习。\n3. **专家路由**：\n - 在动作头中集成混合专家（MoE）架构，动态分配专家。\n - 设计知识关系嵌入（距离计算）和Top-K语义嵌入（可学习聚类嵌入）指导专家选择。\n - 路由特征融合知识嵌入、噪声和语言令牌。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 模拟：LIBERO基准（LIBERO-goal, LIBERO-long, LIBERO-30*），涵盖多样化目标、长时序推理和多任务场景。\n - 真实世界：双臂机器人任务（\"Transfer Magic Stick\"、\"Pick up Bag\"、\"Handover Toy\"）。\n- **训练设置**：\n - 采用经验回放（Experience Replay），仅存储1%过去数据以减少内存开销。\n - 所有模型从零开始训练（非预训练）。\n - 真实任务中使用5%回放率确保稳定性。\n- **资源**：\n - 实验使用双臂机器人平台，观测来自头部和腕部摄像头，输出为关节姿态和夹爪状态。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟：LIBERO仿真环境。\n - 真实：人形机器人平台，配备多视角摄像头。\n- **评估指标**：\n - FWT（前向迁移，越高越好）：衡量学习新任务的能力。\n - NBT（反向负迁移，越低越好）：衡量遗忘程度。\n - AUC（成功率曲线下面积，越高越好）：反映整体性能稳定性。\n - Final SR（最终平均成功率，越高越好）：所有任务训练后的综合表现。\n - 每个策略在先前任务上评估100次，构建成功率矩阵进行计算。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学、上海创新研究院、剑桥大学、北京航空航天大学、AgiBot</p>\n<h3>论文概述</h3>\n<p>论文提出Stellar VLA框架，用于视觉-语言-动作模型（VLA）的持续模仿学习（CIL）。该框架通过联合学习任务中心表示和自演化知识空间，使模型能够在获取新技能的同时缓解对先前任务的遗忘。Stellar VLA包含两个变体：T-Stellar（任务级建模）和TS-Stellar（分层任务-技能建模），旨在实现知识驱动的高效持续学习。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出Stellar VLA框架，通过自监督的知识演化循环实现任务知识的保留与发现。</li><li>设计基于狄利克雷过程（DP）的知识空间，支持无限任务/技能聚类，自适应扩展知识表示。</li><li>引入知识引导的专家路由机制，在动作头中实现参数共享与任务特化的平衡，降低训练开销。</li><li>在LIBERO基准和真实任务中验证有效性，平均最终成功率相比基线提升超50%。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>知识空间建模</strong>：</p>\n<p> - T-Stellar使用狄利克雷过程混合模型（DPMM）构建离散任务级知识空间。</p>\n<p> - TS-Stellar采用分层狄利克雷过程（HDP）建模任务-技能层次关系，捕获跨任务共享的子技能。</p>\n<p>2. <strong>自监督学习</strong>：</p>\n<p> - 通过变分自编码器（VAE）学习任务中心表示，约束其聚合在知识空间内。</p>\n<p> - 使用基于记忆的变分贝叶斯（memoVB）算法更新知识分布，支持增量学习。</p>\n<p>3. <strong>专家路由</strong>：</p>\n<p> - 在动作头中集成混合专家（MoE）架构，动态分配专家。</p>\n<p> - 设计知识关系嵌入（距离计算）和Top-K语义嵌入（可学习聚类嵌入）指导专家选择。</p>\n<p> - 路由特征融合知识嵌入、噪声和语言令牌。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟：LIBERO基准（LIBERO-goal, LIBERO-long, LIBERO-30*），涵盖多样化目标、长时序推理和多任务场景。</p>\n<p> - 真实世界：双臂机器人任务（\"Transfer Magic Stick\"、\"Pick up Bag\"、\"Handover Toy\"）。</p>\n<ul><li><strong>训练设置</strong>：</li></ul>\n<p> - 采用经验回放（Experience Replay），仅存储1%过去数据以减少内存开销。</p>\n<p> - 所有模型从零开始训练（非预训练）。</p>\n<p> - 真实任务中使用5%回放率确保稳定性。</p>\n<ul><li><strong>资源</strong>：</li></ul>\n<p> - 实验使用双臂机器人平台，观测来自头部和腕部摄像头，输出为关节姿态和夹爪状态。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟：LIBERO仿真环境。</p>\n<p> - 真实：人形机器人平台，配备多视角摄像头。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - FWT（前向迁移，越高越好）：衡量学习新任务的能力。</p>\n<p> - NBT（反向负迁移，越低越好）：衡量遗忘程度。</p>\n<p> - AUC（成功率曲线下面积，越高越好）：反映整体性能稳定性。</p>\n<p> - Final SR（最终平均成功率，越高越好）：所有任务训练后的综合表现。</p>\n<p> - 每个策略在先前任务上评估100次，构建成功率矩阵进行计算。</p>"
  },
  {
    "date": "2025-11-22",
    "title": "ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.18082",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-11-22",
    "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
    "link": "http://arxiv.org/abs/2511.17889",
    "summary_markdown": "论文研究单位\n北京大学\n\n论文概述\n该论文旨在解决将自然语言指令转换为四足机器人连续控制的根本性挑战。现有方法在弥合高级语义推理与低级动作执行之间的鸿沟时存在困难。为此，研究者提出了MobileVLA-R1，一个统一的视觉-语言-动作框架。该框架通过生成结构化的思维链（CoT）动作计划，再将这些计划转换为连续控制命令，从而实现了显式推理。训练采用两阶段范式：首先在合成的MobileVLA-CoT数据集上进行监督式CoT对齐，然后通过GRPO（分组相对策略优化）强化学习来进一步提升推理一致性和控制稳定性。在VLN-CE和QUARD等基准测试上的评估表明，该方法性能优于强基线约5%，并在真实的Unitree Go2机器人上成功部署。\n\n论文核心贡献点\n1. 提出了MobileVLA-R1，一个通过显式的思维链生成和连续四足执行来连接语义推理与电机控制的分层视觉-语言-动作框架。\n2. 设计了一个两阶段训练框架，将监督式CoT对齐与GRPO强化学习相结合，改善了推理一致性、控制鲁棒性和长视野稳定性。\n3. 构建了MobileVLA-CoT，一个大规模的多粒度具身轨迹CoT数据集，并展示了在具身AI基准测试上约5%的性能提升以及在Unitree Go2平台上的可靠部署。\n\n论文方法描述\nMobileVLA-R1遵循分层推理-执行范式。模型架构基于LLaVA设计，从NaVILA模型初始化，并通过一个轻量级投影模块融合RGB、深度和点云等多模态观察。\n训练分为两个阶段：\n1. 冷启动阶段：使用监督微调（SFT）在带有CoT标注的MobileVLA-CoT数据集上对模型进行训练，使其学习到`<answer>...</answer>`的结构化推理格式，对齐语言与动作。\n2. 强化学习阶段：采用GRPO框架，通过生成多个响应并使用一组奖励函数进行评估来进一步优化策略。奖励函数包括：运动奖励（预测与真实运动方向的余弦相似度）、动作奖励（离散动作的二元匹配）和格式奖励（输出是否符合指定格式）。通过这些奖励信号，模型增强了从推理到执行的稳定性。\n\n论文使用数据集和训练资源\n1. 数据集：\n - 公共数据集：R2R和RxR用于视觉语言导航（VLN）任务，QUARD用于四足机器人的移动和操作任务。\n - 合成数据集：MobileVLA-CoT，包含18K个轨迹级、78K个步级和38K个导航级的CoT标注样本。该数据集通过一个结合了Gemini-2.5-Flash和结构化提示的数据引擎生成。\n2. 训练资源：\n - SFT阶段在4块H20 (96GB) GPU上进行，使用LoRA微调策略，参数设置为r=16, alpha=32。\n - GRPO阶段在单块H20 GPU上运行1K步，学习率为1e-6。视觉编码器保持冻结。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n - 模拟环境：VLN-CE基准（包含R2R-CE和RxR-CE）用于评估连续环境下的视觉语言导航能力；QUARD数据集用于评估四足机器人的连续控制和操作能力。\n - 真实世界：在Unitree Go2四足机器人上进行部署，测试环境包括工作区、走廊和室外等场景，并区分了简单和复杂两种指令任务。\n2. 评估指标：\n - VLN-CE任务：导航误差（NE）、预言成功率（OS）、成功率（SR）、成功率加权路径长度（SPL）和归一化动态时间规整（nDTW）。\n - QUARD任务：六个四足控制任务的平均成功率。\n - 真实世界任务：成功率（SR）和导航误差（NE）。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京大学</p>\n\n<p>论文概述</p>\n<p>该论文旨在解决将自然语言指令转换为四足机器人连续控制的根本性挑战。现有方法在弥合高级语义推理与低级动作执行之间的鸿沟时存在困难。为此，研究者提出了MobileVLA-R1，一个统一的视觉-语言-动作框架。该框架通过生成结构化的思维链（CoT）动作计划，再将这些计划转换为连续控制命令，从而实现了显式推理。训练采用两阶段范式：首先在合成的MobileVLA-CoT数据集上进行监督式CoT对齐，然后通过GRPO（分组相对策略优化）强化学习来进一步提升推理一致性和控制稳定性。在VLN-CE和QUARD等基准测试上的评估表明，该方法性能优于强基线约5%，并在真实的Unitree Go2机器人上成功部署。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了MobileVLA-R1，一个通过显式的思维链生成和连续四足执行来连接语义推理与电机控制的分层视觉-语言-动作框架。</li><li>设计了一个两阶段训练框架，将监督式CoT对齐与GRPO强化学习相结合，改善了推理一致性、控制鲁棒性和长视野稳定性。</li><li>构建了MobileVLA-CoT，一个大规模的多粒度具身轨迹CoT数据集，并展示了在具身AI基准测试上约5%的性能提升以及在Unitree Go2平台上的可靠部署。</li></ol>\n\n<p>论文方法描述</p>\n<p>MobileVLA-R1遵循分层推理-执行范式。模型架构基于LLaVA设计，从NaVILA模型初始化，并通过一个轻量级投影模块融合RGB、深度和点云等多模态观察。</p>\n<p>训练分为两个阶段：</p>\n<ol><li>冷启动阶段：使用监督微调（SFT）在带有CoT标注的MobileVLA-CoT数据集上对模型进行训练，使其学习到<code><answer>...</answer></code>的结构化推理格式，对齐语言与动作。</li><li>强化学习阶段：采用GRPO框架，通过生成多个响应并使用一组奖励函数进行评估来进一步优化策略。奖励函数包括：运动奖励（预测与真实运动方向的余弦相似度）、动作奖励（离散动作的二元匹配）和格式奖励（输出是否符合指定格式）。通过这些奖励信号，模型增强了从推理到执行的稳定性。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. 数据集：</p>\n<p> - 公共数据集：R2R和RxR用于视觉语言导航（VLN）任务，QUARD用于四足机器人的移动和操作任务。</p>\n<p> - 合成数据集：MobileVLA-CoT，包含18K个轨迹级、78K个步级和38K个导航级的CoT标注样本。该数据集通过一个结合了Gemini-2.5-Flash和结构化提示的数据引擎生成。</p>\n<p>2. 训练资源：</p>\n<p> - SFT阶段在4块H20 (96GB) GPU上进行，使用LoRA微调策略，参数设置为r=16, alpha=32。</p>\n<p> - GRPO阶段在单块H20 GPU上运行1K步，学习率为1e-6。视觉编码器保持冻结。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> - 模拟环境：VLN-CE基准（包含R2R-CE和RxR-CE）用于评估连续环境下的视觉语言导航能力；QUARD数据集用于评估四足机器人的连续控制和操作能力。</p>\n<p> - 真实世界：在Unitree Go2四足机器人上进行部署，测试环境包括工作区、走廊和室外等场景，并区分了简单和复杂两种指令任务。</p>\n<p>2. 评估指标：</p>\n<p> - VLN-CE任务：导航误差（NE）、预言成功率（OS）、成功率（SR）、成功率加权路径长度（SPL）和归一化动态时间规整（nDTW）。</p>\n<p> - QUARD任务：六个四足控制任务的平均成功率。</p>\n<p> - 真实世界任务：成功率（SR）和导航误差（NE）。</p>"
  },
  {
    "date": "2025-11-21",
    "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
    "link": "http://arxiv.org/abs/2511.17502",
    "summary_markdown": "# 论文研究单位\n- DAMO Academy, Alibaba Group\n- Hupan Lab\n- Zhejiang University\n# 论文概述\n论文介绍了RynnVLA-002，一个统一的视觉-语言-动作（VLA）模型与世界模型框架。该框架通过联合学习环境动力学和动作规划，使得VLA模型与世界模型能够相互促进：世界模型通过预测未来图像状态学习环境物理，以优化动作生成；VLA模型则通过生成动作来增强视觉理解，从而支持世界模型的图像生成。实验证明，该统一模型在模拟和真实世界机器人任务上均超越了独立的VLA模型和世界模型。\n# 论文核心贡献点\n- 提出了一个名为RynnVLA-002的统一框架，将VLA模型与世界模型整合在单一模型中进行联合训练。\n- 针对离散动作块生成，提出了一种动作注意力掩码策略，以减轻自回归生成过程中的误差累积问题；并增加了一个连续的Action Transformer头，以提高模型的泛化能力和生成动作的平滑性。\n- 通过实验验证了VLA模型与世界模型的相互增强效果，在LIBERO模拟基准上无需预训练即达到97.4%的成功率，在真实LeRobot实验中，世界模型的集成将整体成功率提升了50%。\n# 论文方法描述\n- 模型架构：RynnVLA-002基于Chameleon模型，使用统一的词汇表处理图像、文本、状态和动作四种模态的token。它通过混合VLA数据（根据指令和历史观察生成动作序列）和世界模型数据（根据当前图像和动作预测未来图像）来训练一个单一的大型语言模型（LLM）。\n- 离散动作生成：为解决标准自回归模型在生成连续动作序列时的误差传播问题，引入了一种特殊的注意力掩码。该掩码使当前动作的生成仅依赖于文本和视觉输入，而不依赖于先前生成的动作，从而隔离了动作之间的误差。\n- 连续动作生成：为提升在真实世界中的泛化能力和推理速度，模型在离散建模的基础上增加了一个小型的连续Action Transformer头。该头通过学习到的action queries并行解码整个动作块，生成连续、平滑的机器人动作轨迹，同时减少了过拟合风险。\n- 训练目标：模型的总体损失函数包含三部分：离散动作的交叉熵损失、图像生成的交叉熵损失以及连续动作的L1回归损失。\n# 论文使用数据集和训练资源\n- 数据集：\n - 模拟实验使用LIBERO基准数据集，包含Spatial、Object、Goal和Long四个任务套件。\n - 真实世界实验使用作者通过LeRobot SO100机械臂采集的新数据集，包含两个拾取与放置任务，共约500条演示轨迹。\n- 训练资源：论文未明确说明所使用的具体计算资源（如GPU类型和数量）。模型在Chameleon基础上进行初始化，损失权重系数α设置为10。\n# 论文使用的评估环境和评估指标\n- 评估环境：\n - 模拟环境：LIBERO基准测试集。\n - 真实世界环境：配备LeRobot SO100机械臂的物理机器人平台，测试场景包括单目标、多目标和带干扰物的任务。\n- 评估指标：\n - VLA模型性能：任务成功率（%），通过多次部署rollout计算平均值。\n - 世界模型性能：Fréchet Video Distance (FVD)、Peak Signal-to-Noise Ratio (PSNR)、Structural Similarity Index (SSIM)和Learned Perceptual Image Patch Similarity (LPIPS)。\n - 推理效率：动作生成频率（Hz）。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>DAMO Academy, Alibaba Group</li><li>Hupan Lab</li><li>Zhejiang University</li></ul>\n<h1>论文概述</h1>\n<p>论文介绍了RynnVLA-002，一个统一的视觉-语言-动作（VLA）模型与世界模型框架。该框架通过联合学习环境动力学和动作规划，使得VLA模型与世界模型能够相互促进：世界模型通过预测未来图像状态学习环境物理，以优化动作生成；VLA模型则通过生成动作来增强视觉理解，从而支持世界模型的图像生成。实验证明，该统一模型在模拟和真实世界机器人任务上均超越了独立的VLA模型和世界模型。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出了一个名为RynnVLA-002的统一框架，将VLA模型与世界模型整合在单一模型中进行联合训练。</li><li>针对离散动作块生成，提出了一种动作注意力掩码策略，以减轻自回归生成过程中的误差累积问题；并增加了一个连续的Action Transformer头，以提高模型的泛化能力和生成动作的平滑性。</li><li>通过实验验证了VLA模型与世界模型的相互增强效果，在LIBERO模拟基准上无需预训练即达到97.4%的成功率，在真实LeRobot实验中，世界模型的集成将整体成功率提升了50%。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>模型架构：RynnVLA-002基于Chameleon模型，使用统一的词汇表处理图像、文本、状态和动作四种模态的token。它通过混合VLA数据（根据指令和历史观察生成动作序列）和世界模型数据（根据当前图像和动作预测未来图像）来训练一个单一的大型语言模型（LLM）。</li><li>离散动作生成：为解决标准自回归模型在生成连续动作序列时的误差传播问题，引入了一种特殊的注意力掩码。该掩码使当前动作的生成仅依赖于文本和视觉输入，而不依赖于先前生成的动作，从而隔离了动作之间的误差。</li><li>连续动作生成：为提升在真实世界中的泛化能力和推理速度，模型在离散建模的基础上增加了一个小型的连续Action Transformer头。该头通过学习到的action queries并行解码整个动作块，生成连续、平滑的机器人动作轨迹，同时减少了过拟合风险。</li><li>训练目标：模型的总体损失函数包含三部分：离散动作的交叉熵损失、图像生成的交叉熵损失以及连续动作的L1回归损失。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：</li></ul>\n<p> - 模拟实验使用LIBERO基准数据集，包含Spatial、Object、Goal和Long四个任务套件。</p>\n<p> - 真实世界实验使用作者通过LeRobot SO100机械臂采集的新数据集，包含两个拾取与放置任务，共约500条演示轨迹。</p>\n<ul><li>训练资源：论文未明确说明所使用的具体计算资源（如GPU类型和数量）。模型在Chameleon基础上进行初始化，损失权重系数α设置为10。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：</li></ul>\n<p> - 模拟环境：LIBERO基准测试集。</p>\n<p> - 真实世界环境：配备LeRobot SO100机械臂的物理机器人平台，测试场景包括单目标、多目标和带干扰物的任务。</p>\n<ul><li>评估指标：</li></ul>\n<p> - VLA模型性能：任务成功率（%），通过多次部署rollout计算平均值。</p>\n<p> - 世界模型性能：Fréchet Video Distance (FVD)、Peak Signal-to-Noise Ratio (PSNR)、Structural Similarity Index (SSIM)和Learned Perceptual Image Patch Similarity (LPIPS)。</p>\n<p> - 推理效率：动作生成频率（Hz）。</p>"
  },
  {
    "date": "2025-11-21",
    "title": "METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2511.17366",
    "summary_markdown": "论文研究单位\n北京大学计算机学院多媒体信息处理重点实验室；北京人工智能研究院\n\n论文概述\n论文提出METIS，一种用于灵巧操作的视觉-语言-动作（VLA）模型，通过多源第一人称数据集进行预训练。为了解决灵巧操作中大规模动作标注数据稀缺的问题，论文构建了EgoAtlas数据集，该数据集整合了大规模人类和机器人数据，并在统一的动作空间下对齐。此外，论文提取了运动感知动力学（motion-aware dynamics），这是一种紧凑且离散化的运动表示，为VLA训练提供高效且富有表现力的监督。METIS将推理和行动集成在统一框架中，能够有效部署到下游灵巧操作任务中。\n\n论文核心贡献点\n1. 构建了多源第一人称操作数据集EgoAtlas，整合了不同的人类和机器人数据源，并在统一的动作空间下对齐。\n2. 提出提取运动感知动力学，一种紧凑且离散化的灵巧手部运动表示。\n3. 提出了METIS模型，一个用于灵巧操作的VLA模型，在大规模多源第一人称数据上预训练，将推理和行动集成在统一框架中。\n4. 通过一系列真实世界实验展示了该方法的有效性和泛化能力。\n\n论文方法描述\n方法包含三个主要部分：\n1. 运动感知动力学构建：\n - 视觉动力学离散化：使用基于逆动力学模型的编码器和基于前向动力学模型的解码器，结合视觉观测和连续运动信息，捕获与运动相关的视觉动力学，并使用VQ-VAE进行量化。\n - 运动动力学量化：使用PoseNet编码3D手部运动，结合多尺度时间卷积和轨迹自注意力，捕获时空动态，并使用RQ-VAE量化连续运动特征。\n2. METIS模型：\n - 基于Prismatic-7B模型构建，采用SigLIP和DINOv2的混合视觉编码器。\n - 扩展LLaMA分词器词汇表，加入特殊token以表示视觉动力学和运动动力学。\n - 集成推理和行动的统一框架，引入特殊token [BOA]（推理开始）和 [BOD]（动力学开始）实现推理与动作的自适应切换，减少推理延迟。\n - 动作解码器将运动感知动力学token转换为可执行的低层动作，通过多头注意力池化聚合视觉和动态特征，并通过MLP投影本体感觉输入。\n\n论文使用数据集和训练资源\n1. 数据集：EgoAtlas数据集，包含343K轨迹和89.72M图像-动作对，整合了8个数据源，涵盖人类和机器人领域。\n2. 训练资源：\n - 预训练：使用24块NVIDIA H100 GPU，全局批量大小768，优化器为AdamW，学习率2e-5，训练60k步，约72小时。\n - 后训练：使用8块NVIDIA H100 GPU，每设备批量大小4，对LLM主干和视觉编码器应用LoRA（秩32），动作解码器全参数微调，学习率3.5e-4，权重衰减1e-3，采用StepLR策略。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n - 硬件平台：Unitree G1人形机器人，配备一对Inspire 6自由度灵巧手，头部安装Intel RealSense D435相机捕捉第一人称RGB观测。\n - 任务：六个灵巧操作任务，包括三个短时域任务和三个长时域任务，如“抓取并放置”、“关闭笔记本”、“打开抽屉”、“抓取两个饮料放入篮子”等。\n2. 评估指标：\n - 成功率（SR）：任务整体成功完成的比率。\n - 进度成功率（PSR）：在长时域设置中，子任务相对于整体任务的平均完成比率。\n - 泛化能力评估：在未见背景、未见光照条件、未见物体、杂乱环境四种分布外场景下评估模型性能。\n - 跨具身泛化：评估模型在更高自由度（22自由度）灵巧手上的迁移性能。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京大学计算机学院多媒体信息处理重点实验室；北京人工智能研究院</p>\n\n<p>论文概述</p>\n<p>论文提出METIS，一种用于灵巧操作的视觉-语言-动作（VLA）模型，通过多源第一人称数据集进行预训练。为了解决灵巧操作中大规模动作标注数据稀缺的问题，论文构建了EgoAtlas数据集，该数据集整合了大规模人类和机器人数据，并在统一的动作空间下对齐。此外，论文提取了运动感知动力学（motion-aware dynamics），这是一种紧凑且离散化的运动表示，为VLA训练提供高效且富有表现力的监督。METIS将推理和行动集成在统一框架中，能够有效部署到下游灵巧操作任务中。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>构建了多源第一人称操作数据集EgoAtlas，整合了不同的人类和机器人数据源，并在统一的动作空间下对齐。</li><li>提出提取运动感知动力学，一种紧凑且离散化的灵巧手部运动表示。</li><li>提出了METIS模型，一个用于灵巧操作的VLA模型，在大规模多源第一人称数据上预训练，将推理和行动集成在统一框架中。</li><li>通过一系列真实世界实验展示了该方法的有效性和泛化能力。</li></ol>\n\n<p>论文方法描述</p>\n<p>方法包含三个主要部分：</p>\n<p>1. 运动感知动力学构建：</p>\n<p> - 视觉动力学离散化：使用基于逆动力学模型的编码器和基于前向动力学模型的解码器，结合视觉观测和连续运动信息，捕获与运动相关的视觉动力学，并使用VQ-VAE进行量化。</p>\n<p> - 运动动力学量化：使用PoseNet编码3D手部运动，结合多尺度时间卷积和轨迹自注意力，捕获时空动态，并使用RQ-VAE量化连续运动特征。</p>\n<p>2. METIS模型：</p>\n<p> - 基于Prismatic-7B模型构建，采用SigLIP和DINOv2的混合视觉编码器。</p>\n<p> - 扩展LLaMA分词器词汇表，加入特殊token以表示视觉动力学和运动动力学。</p>\n<p> - 集成推理和行动的统一框架，引入特殊token [BOA]（推理开始）和 [BOD]（动力学开始）实现推理与动作的自适应切换，减少推理延迟。</p>\n<p> - 动作解码器将运动感知动力学token转换为可执行的低层动作，通过多头注意力池化聚合视觉和动态特征，并通过MLP投影本体感觉输入。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：EgoAtlas数据集，包含343K轨迹和89.72M图像-动作对，整合了8个数据源，涵盖人类和机器人领域。</li><li>训练资源：</li></ol>\n<p> - 预训练：使用24块NVIDIA H100 GPU，全局批量大小768，优化器为AdamW，学习率2e-5，训练60k步，约72小时。</p>\n<p> - 后训练：使用8块NVIDIA H100 GPU，每设备批量大小4，对LLM主干和视觉编码器应用LoRA（秩32），动作解码器全参数微调，学习率3.5e-4，权重衰减1e-3，采用StepLR策略。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> - 硬件平台：Unitree G1人形机器人，配备一对Inspire 6自由度灵巧手，头部安装Intel RealSense D435相机捕捉第一人称RGB观测。</p>\n<p> - 任务：六个灵巧操作任务，包括三个短时域任务和三个长时域任务，如“抓取并放置”、“关闭笔记本”、“打开抽屉”、“抓取两个饮料放入篮子”等。</p>\n<p>2. 评估指标：</p>\n<p> - 成功率（SR）：任务整体成功完成的比率。</p>\n<p> - 进度成功率（PSR）：在长时域设置中，子任务相对于整体任务的平均完成比率。</p>\n<p> - 泛化能力评估：在未见背景、未见光照条件、未见物体、杂乱环境四种分布外场景下评估模型性能。</p>\n<p> - 跨具身泛化：评估模型在更高自由度（22自由度）灵巧手上的迁移性能。</p>"
  },
  {
    "date": "2025-11-21",
    "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.17199",
    "summary_markdown": "论文研究单位\n新加坡国立大学和华中科技大学\n\n论文概述\n该论文提出了VLA-4D，一个具有4D感知的通用视觉-语言-动作（VLA）模型，用于实现时空连贯的机器人操作。现有VLA模型在需要细粒度表示的时空连贯操作中面临挑战，通常在视觉表示中嵌入3D位置以增强动作的空间精度，但难以实现动作执行的时间连贯控制。VLA-4D通过两个关键设计解决此问题：1) 4D感知的视觉表示，提取视觉特征，将1D时间嵌入3D位置以形成4D嵌入，并通过交叉注意力机制融合成统一的视觉表示；2) 时空动作表示，在传统空间动作表示中引入时间信息，实现时空规划，并将多模态表示与LLM对齐以预测时空动作。该框架使视觉和动作表示共同确保机器人操作的空间平滑性和时间连贯性。此外，作者扩展了VLA数据集以包含时间动作注释，用于微调模型。大量实验验证了该方法在多种机器人操作任务上的优越性。\n\n论文核心贡献点\n1. 提出了VLA-4D，一个通用的4D视觉-语言-动作模型，用于时空连贯的机器人操作，将时空信息嵌入视觉和动作表示中。\n2. 设计了显式的4D感知视觉表示，通过交叉注意力机制将3D位置和1D时间融合到视觉特征中，增强模型的细粒度时空推理能力。\n3. 构建了时空动作表示，在传统空间动作中加入时间控制变量，提高了机器人操作的空间平滑性和时间连贯性。\n4. 扩展了VLA数据集并添加了时间动作注释用于微调模型，在多个机器人操作任务上实现了最先进的性能。\n\n论文方法描述\n模型框架包含两个主要阶段：\n1. 4D感知视觉表示：首先使用视觉编码器和几何编码器（如VGGT）提取输入视频序列的视觉特征和几何特征。在几何空间中，将3D位置和1D时间编码为4D时空嵌入。然后通过交叉注意力机制将4D嵌入与视觉特征融合，生成统一的视觉表示。\n2. 时空动作表示：将动作表示扩展到时空维度，定义为A=[X,T]，其中X和T为时空控制变量。融合后的视觉特征和本体感觉状态被投影到语言嵌入空间，与语言令牌对齐。LLM通过动作头基于对齐的多模态表示预测时空动作。\n具体地，3D位置通过几何投影从2D像素坐标和深度图转换得到，时间嵌入采用基于傅里叶的编码策略。交叉注意力融合动态调整4D嵌入与视觉特征的融合权重。动作表示在传统空间参数（平移、旋转、夹爪状态）基础上增加时间步长参数Δt，用于步级动作控制。模型使用L1范数损失函数进行优化，训练过程分为两个阶段：先进行4D视觉-语言对齐，再进行机器人任务微调。\n\n论文使用数据集和训练资源\n使用的数据集：LIBERO模拟套件，包含空间推理（LIBERO-Spatial）、物体理解（LIBERO-Object）、任务目标（LIBERO-Goal）和长视野规划（LIBERO-Long）四个基准设置。作者通过渲染轨迹、添加时间戳和深度，扩展了输入模态和动作注释，最终数据集包含40个子任务，共15万对视觉-语言-动作样本。\n训练资源：模型在8块RTX 6000 Ada GPU上训练。第一阶段使用AdamW优化器，学习率为1.0e-4，批大小为16；第二阶段学习率为5.0e-5，批大小为24。使用LoRA适配器优化部分模块权重。\n\n论文使用的评估环境和评估指标\n评估环境：LIBERO基准模拟环境。\n评估指标：任务成功率（SR）和完成时间（CT）。评估设置包括在所有机器人子任务上进行微调评估，以及在部分子任务上评估零样本泛化性能。对比方法包括OpenVLA、Octo、CogACT、DiffusionPolicy、TraceVLA、SpatialVLA和4D-VLA等。",
    "summary_html": "<p>论文研究单位</p>\n<p>新加坡国立大学和华中科技大学</p>\n\n<p>论文概述</p>\n<p>该论文提出了VLA-4D，一个具有4D感知的通用视觉-语言-动作（VLA）模型，用于实现时空连贯的机器人操作。现有VLA模型在需要细粒度表示的时空连贯操作中面临挑战，通常在视觉表示中嵌入3D位置以增强动作的空间精度，但难以实现动作执行的时间连贯控制。VLA-4D通过两个关键设计解决此问题：1) 4D感知的视觉表示，提取视觉特征，将1D时间嵌入3D位置以形成4D嵌入，并通过交叉注意力机制融合成统一的视觉表示；2) 时空动作表示，在传统空间动作表示中引入时间信息，实现时空规划，并将多模态表示与LLM对齐以预测时空动作。该框架使视觉和动作表示共同确保机器人操作的空间平滑性和时间连贯性。此外，作者扩展了VLA数据集以包含时间动作注释，用于微调模型。大量实验验证了该方法在多种机器人操作任务上的优越性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了VLA-4D，一个通用的4D视觉-语言-动作模型，用于时空连贯的机器人操作，将时空信息嵌入视觉和动作表示中。</li><li>设计了显式的4D感知视觉表示，通过交叉注意力机制将3D位置和1D时间融合到视觉特征中，增强模型的细粒度时空推理能力。</li><li>构建了时空动作表示，在传统空间动作中加入时间控制变量，提高了机器人操作的空间平滑性和时间连贯性。</li><li>扩展了VLA数据集并添加了时间动作注释用于微调模型，在多个机器人操作任务上实现了最先进的性能。</li></ol>\n\n<p>论文方法描述</p>\n<p>模型框架包含两个主要阶段：</p>\n<ol><li>4D感知视觉表示：首先使用视觉编码器和几何编码器（如VGGT）提取输入视频序列的视觉特征和几何特征。在几何空间中，将3D位置和1D时间编码为4D时空嵌入。然后通过交叉注意力机制将4D嵌入与视觉特征融合，生成统一的视觉表示。</li><li>时空动作表示：将动作表示扩展到时空维度，定义为A=[X,T]，其中X和T为时空控制变量。融合后的视觉特征和本体感觉状态被投影到语言嵌入空间，与语言令牌对齐。LLM通过动作头基于对齐的多模态表示预测时空动作。</li></ol>\n<p>具体地，3D位置通过几何投影从2D像素坐标和深度图转换得到，时间嵌入采用基于傅里叶的编码策略。交叉注意力融合动态调整4D嵌入与视觉特征的融合权重。动作表示在传统空间参数（平移、旋转、夹爪状态）基础上增加时间步长参数Δt，用于步级动作控制。模型使用L1范数损失函数进行优化，训练过程分为两个阶段：先进行4D视觉-语言对齐，再进行机器人任务微调。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用的数据集：LIBERO模拟套件，包含空间推理（LIBERO-Spatial）、物体理解（LIBERO-Object）、任务目标（LIBERO-Goal）和长视野规划（LIBERO-Long）四个基准设置。作者通过渲染轨迹、添加时间戳和深度，扩展了输入模态和动作注释，最终数据集包含40个子任务，共15万对视觉-语言-动作样本。</p>\n<p>训练资源：模型在8块RTX 6000 Ada GPU上训练。第一阶段使用AdamW优化器，学习率为1.0e-4，批大小为16；第二阶段学习率为5.0e-5，批大小为24。使用LoRA适配器优化部分模块权重。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：LIBERO基准模拟环境。</p>\n<p>评估指标：任务成功率（SR）和完成时间（CT）。评估设置包括在所有机器人子任务上进行微调评估，以及在部分子任务上评估零样本泛化性能。对比方法包括OpenVLA、Octo、CogACT、DiffusionPolicy、TraceVLA、SpatialVLA和4D-VLA等。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy",
    "link": "http://arxiv.org/abs/2511.16651",
    "summary_markdown": "### 论文研究单位\n上海人工智能实验室，北京大学\n### 论文概述\n该论文提出了InternData-A1，一个大规模、高保真的合成机器人操作数据集。论文旨在验证纯合成数据在预训练视觉-语言-动作（VLA）模型时的有效性。研究团队通过一个高度自动化、完全解耦和组合式的仿真流水线生成了该数据集，涵盖了4种机器人形态、18种技能、70个任务、227个场景，总计63万条轨迹和7433小时的数据，涉及刚性、铰接、柔性及流体物体的操作。核心发现是，一个仅使用InternData-A1预训练的π0模型，其性能可以媲美甚至超过在闭源的真实数据集π-dataset上预训练的官方π0模型，并且在多项任务上表现出惊人的零样本仿真到真实世界的迁移能力。论文将开源该数据集及其生成流水线，以降低机器人研究的数据获取门槛。\n### 论文核心贡献点\n1. 首次证明了纯合成数据可以匹敌最强的真实机器人数据集在VLA模型预训练上的效果，揭示了大规模仿真的巨大潜力。\n2. 推出了InternData-A1，一个具有高物理保真度和照片级渲染效果的合成数据集，其规模（63万轨迹）和多样性（4种机器人、70任务、227场景，覆盖刚性、铰接、柔性、流体物体）均处于领先地位。\n3. 提出了一种高度自动化、完全解耦、组合式的数据合成流水线，支持长时程技能组合、灵活的任务组装和异构机器人，实现了最少人工干预下的高效数据生成（每段轨迹成本低于0.003美元）。\n4. 展示了强大的Sim-to-Real迁移能力，在多个具有挑战性的真实任务上实现了超过50%的零样本迁移成功率，验证了数据的高保真度。\n### 论文方法描述\n论文的数据合成流程分为四个主要阶段：\n1. 环境构建：从资产库中检索并配置机器人、场景（来自GRUtopia）和物体。物体库包含刚性、�接、柔性和流体四类，所有资产都带有详细的物理和功能注释。\n2. 技能组合：任务由模块化的原子技能（如抓取、放置、推动）组合而成。每个技能是一个脚本化策略，输入物体和机器人状态，输出一系列末端执行器的路径点。用户通过配置文件顺序或并行地组织这些技能以构建复杂任务。\n3. 领域随机化：对相机视角（±5°旋转，±5cm平移）、光照（174种环境图）、物体位姿、接触区域等进行随机化，以增强视觉和轨迹的多样性。\n4. 生成与存储：使用CuRobo运动规划器在技能生成的路径点之间插值出密集的关节空间动作。成功验证的轨迹会被记录，包括多视角RGB图像、机器人状态、动作标签等，并转换为LeRobot标准格式。\n此外，论文还介绍了框架层面的多级系统优化，通过将轨迹规划与视觉渲染解耦、动态资源调度和引入堆叠渲染技术，实现了2-3倍的端到端性能提升。\n### 论文使用数据集和训练资源\n数据集：\n- 名称：InternData-A1\n- 规模：包含637,498条轨迹，401,430,981帧，总计7,433.91小时。\n- 内容：覆盖4种机器人形态（AgiBot Genie-1, Franka Panda, AgileX Split Aloha, ARX Lift-2），70个任务（分为基础、抓取放置、铰接操作、长时程四类），18种技能，227个室内场景。\n- 物体：包含3,185个刚性物体，321个铰接物体，20件服装以及流体物体。\n训练资源：\n- 预训练：使用32个A100 GPU，在InternData-A1上训练68万步，以匹配官方π0的训练量。\n- 微调：常规任务和Sim-to-Real任务使用8个GPU进行3万步微调，灵巧任务进行10万步微调。\n### 论文使用的评估环境和评估指标\n评估环境：\n- 仿真评估：使用RoboTwin 2.0基准，包含49个双手操作任务，在“简单”（干净）和“困难”（杂乱）两种模式下进行测试。\n- 真实机器人常规任务评估：在Genie-1、ARX Lift-2两种机器人上测试5个任务（如放置马克笔、传递瓶子、加热三明治）。\n- 真实机器人灵巧任务评估：在一种全新的机器人ARX AC One上测试4个长时程灵巧任务（如折叠衣物、拉开拉链袋）。\n- Sim-to-Real零样本迁移评估：在10个选定的真实任务上，仅使用500-1600条仿真轨迹进行微调，然后测试其在真实世界中的表现。\n基线模型：\n- 官方π0模型（在闭源π-dataset上训练）。\n- 从头开始训练的π0模型（无预训练）。\n- 在OXE、Agibot-World、RoboCasa等开源数据集上预训练的模型。\n评估指标：\n- 平均成功率：所有评估的核心指标，报告在多次试验中的平均任务成功率。\n- 试验次数：仿真任务每个评估100次试验，真实世界和Sim-to-Real任务每个评估30次试验。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海人工智能实验室，北京大学</p>\n<h3>论文概述</h3>\n<p>该论文提出了InternData-A1，一个大规模、高保真的合成机器人操作数据集。论文旨在验证纯合成数据在预训练视觉-语言-动作（VLA）模型时的有效性。研究团队通过一个高度自动化、完全解耦和组合式的仿真流水线生成了该数据集，涵盖了4种机器人形态、18种技能、70个任务、227个场景，总计63万条轨迹和7433小时的数据，涉及刚性、铰接、柔性及流体物体的操作。核心发现是，一个仅使用InternData-A1预训练的π0模型，其性能可以媲美甚至超过在闭源的真实数据集π-dataset上预训练的官方π0模型，并且在多项任务上表现出惊人的零样本仿真到真实世界的迁移能力。论文将开源该数据集及其生成流水线，以降低机器人研究的数据获取门槛。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>首次证明了纯合成数据可以匹敌最强的真实机器人数据集在VLA模型预训练上的效果，揭示了大规模仿真的巨大潜力。</li><li>推出了InternData-A1，一个具有高物理保真度和照片级渲染效果的合成数据集，其规模（63万轨迹）和多样性（4种机器人、70任务、227场景，覆盖刚性、铰接、柔性、流体物体）均处于领先地位。</li><li>提出了一种高度自动化、完全解耦、组合式的数据合成流水线，支持长时程技能组合、灵活的任务组装和异构机器人，实现了最少人工干预下的高效数据生成（每段轨迹成本低于0.003美元）。</li><li>展示了强大的Sim-to-Real迁移能力，在多个具有挑战性的真实任务上实现了超过50%的零样本迁移成功率，验证了数据的高保真度。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文的数据合成流程分为四个主要阶段：</p>\n<ol><li>环境构建：从资产库中检索并配置机器人、场景（来自GRUtopia）和物体。物体库包含刚性、�接、柔性和流体四类，所有资产都带有详细的物理和功能注释。</li><li>技能组合：任务由模块化的原子技能（如抓取、放置、推动）组合而成。每个技能是一个脚本化策略，输入物体和机器人状态，输出一系列末端执行器的路径点。用户通过配置文件顺序或并行地组织这些技能以构建复杂任务。</li><li>领域随机化：对相机视角（±5°旋转，±5cm平移）、光照（174种环境图）、物体位姿、接触区域等进行随机化，以增强视觉和轨迹的多样性。</li><li>生成与存储：使用CuRobo运动规划器在技能生成的路径点之间插值出密集的关节空间动作。成功验证的轨迹会被记录，包括多视角RGB图像、机器人状态、动作标签等，并转换为LeRobot标准格式。</li></ol>\n<p>此外，论文还介绍了框架层面的多级系统优化，通过将轨迹规划与视觉渲染解耦、动态资源调度和引入堆叠渲染技术，实现了2-3倍的端到端性能提升。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：</p>\n<ul><li>名称：InternData-A1</li><li>规模：包含637,498条轨迹，401,430,981帧，总计7,433.91小时。</li><li>内容：覆盖4种机器人形态（AgiBot Genie-1, Franka Panda, AgileX Split Aloha, ARX Lift-2），70个任务（分为基础、抓取放置、铰接操作、长时程四类），18种技能，227个室内场景。</li><li>物体：包含3,185个刚性物体，321个铰接物体，20件服装以及流体物体。</li></ul>\n<p>训练资源：</p>\n<ul><li>预训练：使用32个A100 GPU，在InternData-A1上训练68万步，以匹配官方π0的训练量。</li><li>微调：常规任务和Sim-to-Real任务使用8个GPU进行3万步微调，灵巧任务进行10万步微调。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：</p>\n<ul><li>仿真评估：使用RoboTwin 2.0基准，包含49个双手操作任务，在“简单”（干净）和“困难”（杂乱）两种模式下进行测试。</li><li>真实机器人常规任务评估：在Genie-1、ARX Lift-2两种机器人上测试5个任务（如放置马克笔、传递瓶子、加热三明治）。</li><li>真实机器人灵巧任务评估：在一种全新的机器人ARX AC One上测试4个长时程灵巧任务（如折叠衣物、拉开拉链袋）。</li><li>Sim-to-Real零样本迁移评估：在10个选定的真实任务上，仅使用500-1600条仿真轨迹进行微调，然后测试其在真实世界中的表现。</li></ul>\n<p>基线模型：</p>\n<ul><li>官方π0模型（在闭源π-dataset上训练）。</li><li>从头开始训练的π0模型（无预训练）。</li><li>在OXE、Agibot-World、RoboCasa等开源数据集上预训练的模型。</li></ul>\n<p>评估指标：</p>\n<ul><li>平均成功率：所有评估的核心指标，报告在多次试验中的平均任务成功率。</li><li>试验次数：仿真任务每个评估100次试验，真实世界和Sim-to-Real任务每个评估30次试验。</li></ul>"
  },
  {
    "date": "2025-11-20",
    "title": "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference",
    "link": "http://arxiv.org/abs/2511.16449",
    "summary_markdown": "### 论文研究单位\n上海交通大学人工智能学院、中国科学技术大学、哈尔滨工业大学（深圳）、北京智源人工智能研究院（BAAI）\n### 论文概述\n本文针对视觉-语言-动作（VLA）模型在处理连续视觉流时计算开销大的问题，提出了一种名为VLA-Pruner的免训练、即插即用的视觉令牌剪枝方法。该方法结合了VLA模型的双系统特性（高层语义理解与底层动作执行），并利用机器人操作中的时间连续性，实现了高效推理。实验表明，在多个VLA架构和机器人任务上，该方法在保持精度的同时显著提升了计算效率。\n### 论文核心贡献点\n1. **双层次重要性标准**：同时考虑视觉-语言预填充注意力（语义级相关性）和动作解码注意力（动作级重要性）。\n2. **时间平滑机制**：利用动作解码注意力的时间连续性，通过衰减窗口平均估计当前动作注意力。\n3. **双层次令牌选择策略**：基于最小冗余最大相关性（mRMR）原则，通过相关性最大化合并和冗余最小化过滤，自适应保留信息量高且多样化的视觉令牌。\n4. **即插即用设计**：无需重新训练，兼容主流VLA架构（如OpenVLA、π₀等）。\n### 论文方法描述\n1. **双层次重要性评估**：\n - 语义级重要性：使用视觉-语言预填充阶段的注意力分数（公式2）。\n - 动作级重要性：通过时间平滑（指数衰减窗口平均）估计动作解码注意力（公式7）。\n2. **令牌选择流程**：\n - **双层次Top-k选择**：分别获取语义和动作级重要令牌子集。\n - **相关性最大化合并**：取两个子集的并集作为候选池。\n - **冗余最小化过滤**：求解最大-最小多样性问题（公式8），通过贪心算法基于余弦距离（公式9）选择最终令牌子集。\n3. **实现细节**：在第3层Transformer层剪枝；窗口大小w=3，衰减率γ=0.8；warm-up w步以收集动作注意力历史。\n### 论文使用数据集和训练资源\n- **数据集**：\n - LIBERO：模拟机器人操作任务套件。\n - SIMPLER：多场景机器人评估基准。\n - 真实机器人数据：6-DoF xArm6机器人部署数据。\n- **训练资源**：\n - 免训练方法，无需额外训练数据或资源。\n - 依赖历史动作注意力进行在线估计，warm-up阶段需w=3步。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：LIBERO和SIMPLER基准测试。\n - 真实环境：xArm6物理机器人。\n- **评估指标**：\n - 任务成功率（Success Rate, %）：LIBERO的Spatial/Object/Goal/Long任务平均值。\n - 计算效率：\n - FLOPs（T）：浮点运算量。\n - 推理延迟（ms/action或ms/action-chunk）。\n - 加速比（Speedup, ×）：相对于未剪枝模型的推理速度倍率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学人工智能学院、中国科学技术大学、哈尔滨工业大学（深圳）、北京智源人工智能研究院（BAAI）</p>\n<h3>论文概述</h3>\n<p>本文针对视觉-语言-动作（VLA）模型在处理连续视觉流时计算开销大的问题，提出了一种名为VLA-Pruner的免训练、即插即用的视觉令牌剪枝方法。该方法结合了VLA模型的双系统特性（高层语义理解与底层动作执行），并利用机器人操作中的时间连续性，实现了高效推理。实验表明，在多个VLA架构和机器人任务上，该方法在保持精度的同时显著提升了计算效率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>双层次重要性标准</strong>：同时考虑视觉-语言预填充注意力（语义级相关性）和动作解码注意力（动作级重要性）。</li><li><strong>时间平滑机制</strong>：利用动作解码注意力的时间连续性，通过衰减窗口平均估计当前动作注意力。</li><li><strong>双层次令牌选择策略</strong>：基于最小冗余最大相关性（mRMR）原则，通过相关性最大化合并和冗余最小化过滤，自适应保留信息量高且多样化的视觉令牌。</li><li><strong>即插即用设计</strong>：无需重新训练，兼容主流VLA架构（如OpenVLA、π₀等）。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>双层次重要性评估</strong>：</p>\n<p> - 语义级重要性：使用视觉-语言预填充阶段的注意力分数（公式2）。</p>\n<p> - 动作级重要性：通过时间平滑（指数衰减窗口平均）估计动作解码注意力（公式7）。</p>\n<p>2. <strong>令牌选择流程</strong>：</p>\n<p> - <strong>双层次Top-k选择</strong>：分别获取语义和动作级重要令牌子集。</p>\n<p> - <strong>相关性最大化合并</strong>：取两个子集的并集作为候选池。</p>\n<p> - <strong>冗余最小化过滤</strong>：求解最大-最小多样性问题（公式8），通过贪心算法基于余弦距离（公式9）选择最终令牌子集。</p>\n<p>3. <strong>实现细节</strong>：在第3层Transformer层剪枝；窗口大小w=3，衰减率γ=0.8；warm-up w步以收集动作注意力历史。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - LIBERO：模拟机器人操作任务套件。</p>\n<p> - SIMPLER：多场景机器人评估基准。</p>\n<p> - 真实机器人数据：6-DoF xArm6机器人部署数据。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 免训练方法，无需额外训练数据或资源。</p>\n<p> - 依赖历史动作注意力进行在线估计，warm-up阶段需w=3步。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：LIBERO和SIMPLER基准测试。</p>\n<p> - 真实环境：xArm6物理机器人。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（Success Rate, %）：LIBERO的Spatial/Object/Goal/Long任务平均值。</p>\n<p> - 计算效率：</p>\n<p> - FLOPs（T）：浮点运算量。</p>\n<p> - 推理延迟（ms/action或ms/action-chunk）。</p>\n<p> - 加速比（Speedup, ×）：相对于未剪枝模型的推理速度倍率。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models",
    "link": "http://arxiv.org/abs/2511.16233",
    "summary_markdown": "### 论文研究单位\n根据论文的致谢部分，研究单位与中国国家自然科学基金和重庆市自然科学基金有关，推测主要研究机构位于中国重庆。\n### 论文概述\n论文针对视觉-语言-行动（VLA）模型因依赖大规模、冗余且价值不均的数据集而导致的效率瓶颈问题，提出了一种以数据为中心的生成式数据蒸馏框架FT-NCFM。该框架通过一个自包含的影响评估引擎（Fact-Tracing, FT）来评估样本的内在价值，并利用该评估结果引导一个对抗性的神经特征函数匹配（NCFM）过程，从而合成一个信息密度高、可复用的数据核心集。实验证明，仅使用5%的合成数据训练模型，即可达到使用完整数据集训练85-90%的任务成功率，同时将训练时间减少80%以上，展示了数据蒸馏在构建高效VLA模型中的巨大潜力。\n### 论文核心贡献点\n1. 提出了一种新的生成式数据蒸馏范式，区别于现有的模型压缩和策略蒸馏等以模型为中心的优化路径，从数据层面提升VLA模型效率。\n2. 设计了一个自包含的内在价值评估引擎FT。该引擎通过两阶段评估样本价值：首先使用因果归因进行预筛选，然后通过一个新颖的程序化对比验证模块，为精英样本生成“最小反例”来精炼其影响权重，从而稳健地量化样本的因果贡献和泛化潜力。\n3. 在多个主流VLA基准测试上进行了系统性评估。结果表明，使用FT-NCFM框架和仅5%的合成数据，模型就能达到使用全量数据训练85-90%的性能，同时显著降低了训练时间和计算资源消耗（训练时间减少80%以上），优于多种SOTA方法。\n### 论文方法描述\nFT-NCFM框架包含一个三阶段的流水线：\n1. **VLA多模态表示学习**: 将原始的视觉、语言和行动数据通过各自的编码器和Transformer骨干网络，融合成一个统一的特征向量 `h`，作为后续分析的基础。\n2. **FT影响评估引擎**: 一个两阶段的样本价值评估过程。\n * **阶段一：基于影响函数的因果归因预筛选**: 使用影响函数近似移除单个训练样本对模型损失的影响，计算出基础影响分数。此阶段使用一个“引导模型”，该模型在原始数据上进行轻量级、不完全的训练。\n * **阶段二：对比验证精化**: 对前K%的精英样本，在模拟器中通过“跨模态不匹配”策略程序化地生成一个语义或物理矛盾的“最小反例”。然后，通过比较原始样本和其反例对测试用例的影响差异，使用一个包含tanh函数的权重调制公式来精炼最终的影响权重。\n3. **引导式NCFM**: 将FT引擎计算出的一组影响权重用于指导NCFM的优化过程。通过将原始NCFM的期望计算改为基于权重的加权期望，迫使判别器关注高价值样本，从而引导生成器合成一个富含关键因果知识且信息密度极高的核心集。该核心集与原始数据格式相同，可直接用于训练任何下游VLA模型。\n### 论文使用数据集和训练资源\n* **使用数据集**:\n * CALVIN: 一个用于长期语言条件策略学习的大规模机器人操作基准。\n * Meta-World: 包含50种不同桌面操作任务的基准，用于评估多任务学习环境中的技能获取。\n * LIBERO: 用于促进终身学习的基准套件，包含Spatial、Object、Goal和Long等多个子集。\n* **训练资源**:\n * **硬件**: 实验主要在单块NVIDIA A100 80GB GPU上进行。\n * **软件环境**: Ubuntu 22.04.3 LTS, CUDA 12.4, Python 3.12.7, PyTorch 2.5.0。\n * **时间成本**: FT-NCFM框架的预处理阶段（FT引擎评估与NCFM合成）是一次性投资，在LIBERO数据集上约为24 GPU小时。在此之后，使用5%的合成数据训练下游策略模型仅需约7 GPU小时。\n### 论文使用的评估环境和评估指标\n* **评估环境**:\n * **硬件**: 1 × NVIDIA A100-SXM4-80GB GPU, AMD EPYC 7742 CPU, 47GiB内存。\n * **软件**: Ubuntu 22.04.3 LTS, CUDA 12.4, PyTorch 2.5.0, Python 3.12.7。\n* **评估指标**:\n * **性能指标**:\n * 成功率 或 平均任务完成长度: 用于在CALVIN、Meta-World和LIBERO上衡量任务完成的性能。CALVIN评估遵循D->A,B,C,D的长期任务泛化协议。\n * **效率指标**:\n * 总训练时间: 以GPU小时为单位，衡量从随机初始化到模型收敛所需的总时间，用于评估训练效率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>根据论文的致谢部分，研究单位与中国国家自然科学基金和重庆市自然科学基金有关，推测主要研究机构位于中国重庆。</p>\n<h3>论文概述</h3>\n<p>论文针对视觉-语言-行动（VLA）模型因依赖大规模、冗余且价值不均的数据集而导致的效率瓶颈问题，提出了一种以数据为中心的生成式数据蒸馏框架FT-NCFM。该框架通过一个自包含的影响评估引擎（Fact-Tracing, FT）来评估样本的内在价值，并利用该评估结果引导一个对抗性的神经特征函数匹配（NCFM）过程，从而合成一个信息密度高、可复用的数据核心集。实验证明，仅使用5%的合成数据训练模型，即可达到使用完整数据集训练85-90%的任务成功率，同时将训练时间减少80%以上，展示了数据蒸馏在构建高效VLA模型中的巨大潜力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新的生成式数据蒸馏范式，区别于现有的模型压缩和策略蒸馏等以模型为中心的优化路径，从数据层面提升VLA模型效率。</li><li>设计了一个自包含的内在价值评估引擎FT。该引擎通过两阶段评估样本价值：首先使用因果归因进行预筛选，然后通过一个新颖的程序化对比验证模块，为精英样本生成“最小反例”来精炼其影响权重，从而稳健地量化样本的因果贡献和泛化潜力。</li><li>在多个主流VLA基准测试上进行了系统性评估。结果表明，使用FT-NCFM框架和仅5%的合成数据，模型就能达到使用全量数据训练85-90%的性能，同时显著降低了训练时间和计算资源消耗（训练时间减少80%以上），优于多种SOTA方法。</li></ol>\n<h3>论文方法描述</h3>\n<p>FT-NCFM框架包含一个三阶段的流水线：</p>\n<ol><li><strong>VLA多模态表示学习</strong>: 将原始的视觉、语言和行动数据通过各自的编码器和Transformer骨干网络，融合成一个统一的特征向量 <code>h</code>，作为后续分析的基础。</li><li><strong>FT影响评估引擎</strong>: 一个两阶段的样本价值评估过程。</li></ol>\n<p> * <strong>阶段一：基于影响函数的因果归因预筛选</strong>: 使用影响函数近似移除单个训练样本对模型损失的影响，计算出基础影响分数。此阶段使用一个“引导模型”，该模型在原始数据上进行轻量级、不完全的训练。</p>\n<p> * <strong>阶段二：对比验证精化</strong>: 对前K%的精英样本，在模拟器中通过“跨模态不匹配”策略程序化地生成一个语义或物理矛盾的“最小反例”。然后，通过比较原始样本和其反例对测试用例的影响差异，使用一个包含tanh函数的权重调制公式来精炼最终的影响权重。</p>\n<p>3. <strong>引导式NCFM</strong>: 将FT引擎计算出的一组影响权重用于指导NCFM的优化过程。通过将原始NCFM的期望计算改为基于权重的加权期望，迫使判别器关注高价值样本，从而引导生成器合成一个富含关键因果知识且信息密度极高的核心集。该核心集与原始数据格式相同，可直接用于训练任何下游VLA模型。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>* <strong>使用数据集</strong>:</p>\n<p> * CALVIN: 一个用于长期语言条件策略学习的大规模机器人操作基准。</p>\n<p> * Meta-World: 包含50种不同桌面操作任务的基准，用于评估多任务学习环境中的技能获取。</p>\n<p> * LIBERO: 用于促进终身学习的基准套件，包含Spatial、Object、Goal和Long等多个子集。</p>\n<p>* <strong>训练资源</strong>:</p>\n<p> * <strong>硬件</strong>: 实验主要在单块NVIDIA A100 80GB GPU上进行。</p>\n<p> * <strong>软件环境</strong>: Ubuntu 22.04.3 LTS, CUDA 12.4, Python 3.12.7, PyTorch 2.5.0。</p>\n<p> * <strong>时间成本</strong>: FT-NCFM框架的预处理阶段（FT引擎评估与NCFM合成）是一次性投资，在LIBERO数据集上约为24 GPU小时。在此之后，使用5%的合成数据训练下游策略模型仅需约7 GPU小时。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>* <strong>评估环境</strong>:</p>\n<p> * <strong>硬件</strong>: 1 × NVIDIA A100-SXM4-80GB GPU, AMD EPYC 7742 CPU, 47GiB内存。</p>\n<p> * <strong>软件</strong>: Ubuntu 22.04.3 LTS, CUDA 12.4, PyTorch 2.5.0, Python 3.12.7。</p>\n<p>* <strong>评估指标</strong>:</p>\n<p> * <strong>性能指标</strong>:</p>\n<p> * 成功率 或 平均任务完成长度: 用于在CALVIN、Meta-World和LIBERO上衡量任务完成的性能。CALVIN评估遵循D->A,B,C,D的长期任务泛化协议。</p>\n<p> * <strong>效率指标</strong>:</p>\n<p> * 总训练时间: 以GPU小时为单位，衡量从随机初始化到模型收敛所需的总时间，用于评估训练效率。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.16203",
    "summary_markdown": "## 论文研究单位\n- 西湖大学工学院TGAI实验室\n- 浙江大学\n- 宾夕法尼亚州立大学\n- 索尼研究\n## 论文概述\n本文关注视觉-语言-动作（VLA）模型在具身环境中的对抗鲁棒性问题。尽管VLA模型在机器人感知、推理和行动方面取得了显著进展，但其对抗鲁棒性，尤其是在现实多模态和黑盒条件下的表现，尚未被充分探索。现有研究主要集中于单模态扰动，而忽视了影响具身推理和决策的根本性跨模态不对齐问题。为此，论文提出了VLA-Fool，这是一个在白盒和黑盒设置下对具身VLA模型多模态对抗鲁棒性的综合研究套件。VLA-Fool统一了三个层次的多模态对抗攻击：文本扰动、视觉扰动和跨模态不对齐攻击。在LIBERO基准上使用微调的OpenVLA模型进行的实验表明，即使是轻微的多模态扰动也可能导致显著的行为偏差，揭示了具身多模态对齐的脆弱性。\n## 论文核心贡献点\n- 提出了VLA-Fool，一个在白盒和黑盒设置下生成和评估多模态对抗攻击的综合框架。该框架包括通过基于梯度和基于提示的策略进行文本扰动，通过补丁和噪声失真进行视觉扰动，以及故意破坏感知与指令之间语义对应关系的跨模态不对齐攻击。\n- 通过将贪婪坐标梯度（GCG）方法扩展到VLA感知的语义空间，引入了首个自动构建的、语义引导的提示框架，用于VLA对抗攻击，包括四种语言丰富的错误对齐模式：指代歧义、属性削弱、范围模糊和否定混淆。\n- 通过广泛的实验，揭示了最先进的VLA模型在面对多模态扰动时的脆弱性，在所有变化类别的失败率超过60%，在长视野任务中失败率高达100%，为开发更强大和可信的具身代理提供了宝贵的见解和基准。\n## 论文方法描述\nVLA-Fool是一个统一的多模态对抗攻击套件，包含三个互补的模块：\n- **文本攻击**:\n - **SGCG (白盒)**: 一种基于梯度的语义引导攻击方法。它并行执行K个GCG优化过程，每个过程专注于一种特定的语义扰动策略，例如将具体实体替换为代词（指代歧义）、改变关键属性（属性削弱）、替换空间描述词（范围模糊）或引入否定词（否定混淆）。\n - **提示操作 (黑盒)**: 无需模型内部信息的简单攻击。包括后缀注入（如附加“忽略之前的信息”或随机代码片段）和前缀注入（在指令前添加误导性上下文）来操纵模型行为。\n- **视觉攻击**:\n - **局部补丁攻击 (白盒)**: 利用模型梯度直接优化一个可放置在环境物体或机械臂上的小补丁内容，通过最大化模型输出动作与原始动作之间的L2距离，使机器人执行错误的动作序列。\n - **噪声扰动攻击 (黑盒)**: 向图像中注入各种类型的噪声，模拟真实世界的传感器退化或环境干扰。测试的噪声类型包括高斯噪声、椒盐噪声、斑点噪声、均匀噪声和伪随机噪声等。\n- **跨模态不对齐攻击**:\n - 该攻击不单独扰动某个模态，而是同时优化视觉和文本扰动，以最大化跨模态不对齐损失（L_mis）。该损失函数衡量扰动前后视觉嵌入与语言嵌入之间平均余弦相似度的变化，旨在破坏视觉和语言特征之间的语义对应关系，从而从根本上瓦解VLA的感知-语言-行动管道。\n## 论文使用数据集和训练资源\n- **数据集**: LIBERO基准数据集。该数据集提供了一系列多样化的视觉-语言操作任务和逼真的模拟场景，分为四个评估类别：空间关系查询、对象识别与操作、目标导向行为和长视野多步骤任务。\n- **训练资源**: 实验使用了一个在LIBERO数据集上微调的7B参数OpenVLA模型作为受害者模型。模型推理在单块NVIDIA L40s（48GB）GPU上运行，使用bfloat16精度。\n## 论文使用的评估环境和评估指标\n- **评估环境**: 所有实验均在LIBERO数据集提供的模拟环境中进行。\n- **评估指标**:\n - **失败率**: 主要评估指标，定义为 `FR = 1 - SR`（任务成功率）。该指标直接反映了对抗性扰动导致的任务完成度下降，用于衡量模型对攻击的脆弱性。\n - **不对齐损失**: 一个辅助评估指标，用于量化由扰动引起的视觉和语言表征之间的不一致性。它通过计算扰动前后视觉块嵌入与语言标记嵌入之间平均余弦相似度的变化来衡量。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>西湖大学工学院TGAI实验室</li><li>浙江大学</li><li>宾夕法尼亚州立大学</li><li>索尼研究</li></ul>\n<h2>论文概述</h2>\n<p>本文关注视觉-语言-动作（VLA）模型在具身环境中的对抗鲁棒性问题。尽管VLA模型在机器人感知、推理和行动方面取得了显著进展，但其对抗鲁棒性，尤其是在现实多模态和黑盒条件下的表现，尚未被充分探索。现有研究主要集中于单模态扰动，而忽视了影响具身推理和决策的根本性跨模态不对齐问题。为此，论文提出了VLA-Fool，这是一个在白盒和黑盒设置下对具身VLA模型多模态对抗鲁棒性的综合研究套件。VLA-Fool统一了三个层次的多模态对抗攻击：文本扰动、视觉扰动和跨模态不对齐攻击。在LIBERO基准上使用微调的OpenVLA模型进行的实验表明，即使是轻微的多模态扰动也可能导致显著的行为偏差，揭示了具身多模态对齐的脆弱性。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了VLA-Fool，一个在白盒和黑盒设置下生成和评估多模态对抗攻击的综合框架。该框架包括通过基于梯度和基于提示的策略进行文本扰动，通过补丁和噪声失真进行视觉扰动，以及故意破坏感知与指令之间语义对应关系的跨模态不对齐攻击。</li><li>通过将贪婪坐标梯度（GCG）方法扩展到VLA感知的语义空间，引入了首个自动构建的、语义引导的提示框架，用于VLA对抗攻击，包括四种语言丰富的错误对齐模式：指代歧义、属性削弱、范围模糊和否定混淆。</li><li>通过广泛的实验，揭示了最先进的VLA模型在面对多模态扰动时的脆弱性，在所有变化类别的失败率超过60%，在长视野任务中失败率高达100%，为开发更强大和可信的具身代理提供了宝贵的见解和基准。</li></ul>\n<h2>论文方法描述</h2>\n<p>VLA-Fool是一个统一的多模态对抗攻击套件，包含三个互补的模块：</p>\n<ul><li><strong>文本攻击</strong>:</li></ul>\n<p> - <strong>SGCG (白盒)</strong>: 一种基于梯度的语义引导攻击方法。它并行执行K个GCG优化过程，每个过程专注于一种特定的语义扰动策略，例如将具体实体替换为代词（指代歧义）、改变关键属性（属性削弱）、替换空间描述词（范围模糊）或引入否定词（否定混淆）。</p>\n<p> - <strong>提示操作 (黑盒)</strong>: 无需模型内部信息的简单攻击。包括后缀注入（如附加“忽略之前的信息”或随机代码片段）和前缀注入（在指令前添加误导性上下文）来操纵模型行为。</p>\n<ul><li><strong>视觉攻击</strong>:</li></ul>\n<p> - <strong>局部补丁攻击 (白盒)</strong>: 利用模型梯度直接优化一个可放置在环境物体或机械臂上的小补丁内容，通过最大化模型输出动作与原始动作之间的L2距离，使机器人执行错误的动作序列。</p>\n<p> - <strong>噪声扰动攻击 (黑盒)</strong>: 向图像中注入各种类型的噪声，模拟真实世界的传感器退化或环境干扰。测试的噪声类型包括高斯噪声、椒盐噪声、斑点噪声、均匀噪声和伪随机噪声等。</p>\n<ul><li><strong>跨模态不对齐攻击</strong>:</li></ul>\n<p> - 该攻击不单独扰动某个模态，而是同时优化视觉和文本扰动，以最大化跨模态不对齐损失（L_mis）。该损失函数衡量扰动前后视觉嵌入与语言嵌入之间平均余弦相似度的变化，旨在破坏视觉和语言特征之间的语义对应关系，从而从根本上瓦解VLA的感知-语言-行动管道。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>: LIBERO基准数据集。该数据集提供了一系列多样化的视觉-语言操作任务和逼真的模拟场景，分为四个评估类别：空间关系查询、对象识别与操作、目标导向行为和长视野多步骤任务。</li><li><strong>训练资源</strong>: 实验使用了一个在LIBERO数据集上微调的7B参数OpenVLA模型作为受害者模型。模型推理在单块NVIDIA L40s（48GB）GPU上运行，使用bfloat16精度。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>: 所有实验均在LIBERO数据集提供的模拟环境中进行。</li><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>失败率</strong>: 主要评估指标，定义为 <code>FR = 1 - SR</code>（任务成功率）。该指标直接反映了对抗性扰动导致的任务完成度下降，用于衡量模型对攻击的脆弱性。</p>\n<p> - <strong>不对齐损失</strong>: 一个辅助评估指标，用于量化由扰动引起的视觉和语言表征之间的不一致性。它通过计算扰动前后视觉块嵌入与语言标记嵌入之间平均余弦相似度的变化来衡量。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
    "link": "http://arxiv.org/abs/2511.16175",
    "summary_markdown": "### 论文研究单位\n上海交通大学 (SJTU), 上海人工智能实验室 (SII), 南京邮电大学 (NJUPT), 复旦大学 (FDU), BOSCH。\n### 论文概述\n本文提出了一种名为 Mantis 的视觉-语言-动作 (VLA) 模型，旨在解决现有 VLA 模型面临的挑战：直接预测高维视觉状态会分散模型容量并带来高昂的训练成本，而压缩视觉状态则会引入信息瓶颈，同时现有方法常因忽视语言监督而导致理解与推理能力不佳。Mantis 引入了核心创新——解耦视觉预测 (DVF)，通过结合元查询和扩散 Transformer (DiT) 头，将视觉预测任务与主干网络解耦。当前视觉状态通过残差连接提供给 DiT，使得元查询能自动捕捉描绘视觉轨迹的潜在动作，从而提升显式动作学习。这种解耦设计减轻了主干网络的负担，使其能通过语言监督保持理解和推理能力。实验表明，在 LIBERO 基准上微调后，Mantis 取得了 96.7% 的成功率，并在真实世界评估中表现出优于基线模型的指令跟随、泛化和推理能力。\n### 论文核心贡献点\n- 提出了解耦视觉预测 (DVF)，为动作预测提供简洁且具有指导性的前瞻线索，并构建了新的 VLA 模型 Mantis。\n- 设计了用于模态融合的渐进式训练方案，使 Mantis 能够有效地整合动作预测与语言理解。\n- Mantis 在 LIBERO 基准上实现了 96.7% 的成功率，并在真实世界机器人实验中展示了卓越的指令跟随能力。\n### 论文方法描述\nMantis 的方法论包含四个主要部分：\n1. **模型概述**：模型由一个 VLM 主干网络、一个连接器、一个 DVF 头和一个动作头组成。主干网络接收语言指令和当前视觉观察，并与可学习的潜在动作查询 ([LAT]) 交互。DVF 头接收主干网络输出和当前视觉状态（通过残差连接），用于预测未来的视觉状态。动作头使用可学习的动作查询 ([ACT]) 从输入和 [LAT] 查询中提取信息，以生成显式动作序列。\n2. **模型规范**：主干网络采用 Qwen2.5-VL。DVF 头采用高效的文本到图像生成模型 Sana (一个 DiT)。连接器包含 12 层 Transformer 编码器和一个投影层。动作头同样基于 DiT 结构。\n3. **渐进式训练**：训练分为三个阶段以稳定融合多模态信息。\n - **阶段1：多时差视觉训练**：在 SSV2 人类操作视频数据集上预训练，仅训练 DVF 头和 [LAT] 查询，目标是预测未来帧，让模型学习通用操作技能。\n - **阶段2：视觉-动作联合训练**：在 DROID 机器人演示数据集上训练，引入动作损失，并解冻动作查询。\n - **阶段3：语言监督混合训练**：联合使用 38 个多模态数据集和 DROID 数据进行训练，解冻主干网络并引入语言损失，以保持模型的语义理解与推理能力。\n4. **自适应时间集成 (ATE)**：一种推理时优化策略，用于平衡计算效率和运动稳定性。它动态识别与指令相关的“目标补丁”和发生显著变化的“动态补丁”。当二者重叠时（表示精细操作），启用时间集成以增强稳定性；否则，跳过集成以节省计算开销。该模型变体称为 Mantis-ATE。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **预训练**：SSV2 数据集（约 220K 个人类操作视频），DROID 数据集（76K 个机器人演示），以及 38 个多模态数据集（用于图像-文本对）。\n - **微调**：LIBERO 模拟基准数据集。\n - **真实世界实验**：在三个自定义场景中收集的 100 个远程操作演示数据集。\n- **训练资源**：\n - **模型规模**：总计 58 亿参数（主干网络 3.7B，DVF 头 1.4B，动作头 0.3B，VAE 0.3B）。\n - **优化器**：使用 AdamW 优化器，权重衰减为 0.1，梯度裁剪为 0.5。\n - **训练框架**：使用 DeepSpeed 进行高效分布式训练。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：LIBERO 基准，包含四个任务套件（Spatial, Object, Goal, Long）。\n - **真实世界环境**：Agilex 机器人平台。\n- **评估指标**：\n - **LIBERO 基准**：成功率 (Success Rate, SR)，范围为 0 到 100，越高越好。\n - **真实世界实验**：平均成功执行次数（每个指令执行 10 次，取平均成功数）。\n - **Mantis-ATE 效率**：推理次数 和成功率 (SR)，用于评估计算效率和任务性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学 (SJTU), 上海人工智能实验室 (SII), 南京邮电大学 (NJUPT), 复旦大学 (FDU), BOSCH。</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为 Mantis 的视觉-语言-动作 (VLA) 模型，旨在解决现有 VLA 模型面临的挑战：直接预测高维视觉状态会分散模型容量并带来高昂的训练成本，而压缩视觉状态则会引入信息瓶颈，同时现有方法常因忽视语言监督而导致理解与推理能力不佳。Mantis 引入了核心创新——解耦视觉预测 (DVF)，通过结合元查询和扩散 Transformer (DiT) 头，将视觉预测任务与主干网络解耦。当前视觉状态通过残差连接提供给 DiT，使得元查询能自动捕捉描绘视觉轨迹的潜在动作，从而提升显式动作学习。这种解耦设计减轻了主干网络的负担，使其能通过语言监督保持理解和推理能力。实验表明，在 LIBERO 基准上微调后，Mantis 取得了 96.7% 的成功率，并在真实世界评估中表现出优于基线模型的指令跟随、泛化和推理能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了解耦视觉预测 (DVF)，为动作预测提供简洁且具有指导性的前瞻线索，并构建了新的 VLA 模型 Mantis。</li><li>设计了用于模态融合的渐进式训练方案，使 Mantis 能够有效地整合动作预测与语言理解。</li><li>Mantis 在 LIBERO 基准上实现了 96.7% 的成功率，并在真实世界机器人实验中展示了卓越的指令跟随能力。</li></ul>\n<h3>论文方法描述</h3>\n<p>Mantis 的方法论包含四个主要部分：</p>\n<ol><li><strong>模型概述</strong>：模型由一个 VLM 主干网络、一个连接器、一个 DVF 头和一个动作头组成。主干网络接收语言指令和当前视觉观察，并与可学习的潜在动作查询 ([LAT]) 交互。DVF 头接收主干网络输出和当前视觉状态（通过残差连接），用于预测未来的视觉状态。动作头使用可学习的动作查询 ([ACT]) 从输入和 [LAT] 查询中提取信息，以生成显式动作序列。</li><li><strong>模型规范</strong>：主干网络采用 Qwen2.5-VL。DVF 头采用高效的文本到图像生成模型 Sana (一个 DiT)。连接器包含 12 层 Transformer 编码器和一个投影层。动作头同样基于 DiT 结构。</li><li><strong>渐进式训练</strong>：训练分为三个阶段以稳定融合多模态信息。</li></ol>\n<p> - <strong>阶段1：多时差视觉训练</strong>：在 SSV2 人类操作视频数据集上预训练，仅训练 DVF 头和 [LAT] 查询，目标是预测未来帧，让模型学习通用操作技能。</p>\n<p> - <strong>阶段2：视觉-动作联合训练</strong>：在 DROID 机器人演示数据集上训练，引入动作损失，并解冻动作查询。</p>\n<p> - <strong>阶段3：语言监督混合训练</strong>：联合使用 38 个多模态数据集和 DROID 数据进行训练，解冻主干网络并引入语言损失，以保持模型的语义理解与推理能力。</p>\n<p>4. <strong>自适应时间集成 (ATE)</strong>：一种推理时优化策略，用于平衡计算效率和运动稳定性。它动态识别与指令相关的“目标补丁”和发生显著变化的“动态补丁”。当二者重叠时（表示精细操作），启用时间集成以增强稳定性；否则，跳过集成以节省计算开销。该模型变体称为 Mantis-ATE。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>预训练</strong>：SSV2 数据集（约 220K 个人类操作视频），DROID 数据集（76K 个机器人演示），以及 38 个多模态数据集（用于图像-文本对）。</p>\n<p> - <strong>微调</strong>：LIBERO 模拟基准数据集。</p>\n<p> - <strong>真实世界实验</strong>：在三个自定义场景中收集的 100 个远程操作演示数据集。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - <strong>模型规模</strong>：总计 58 亿参数（主干网络 3.7B，DVF 头 1.4B，动作头 0.3B，VAE 0.3B）。</p>\n<p> - <strong>优化器</strong>：使用 AdamW 优化器，权重衰减为 0.1，梯度裁剪为 0.5。</p>\n<p> - <strong>训练框架</strong>：使用 DeepSpeed 进行高效分布式训练。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：LIBERO 基准，包含四个任务套件（Spatial, Object, Goal, Long）。</p>\n<p> - <strong>真实世界环境</strong>：Agilex 机器人平台。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>LIBERO 基准</strong>：成功率 (Success Rate, SR)，范围为 0 到 100，越高越好。</p>\n<p> - <strong>真实世界实验</strong>：平均成功执行次数（每个指令执行 10 次，取平均成功数）。</p>\n<p> - <strong>Mantis-ATE 效率</strong>：推理次数 和成功率 (SR)，用于评估计算效率和任务性能。</p>"
  },
  {
    "date": "2025-11-20",
    "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2511.16166",
    "summary_markdown": "### 论文研究单位\nPeking University\n### 论文概述\n论文提出EvoVLA框架，用于解决视觉-语言-行动（VLA）模型在长视野机器人操作中的“阶段幻觉”问题（即智能体通过利用粗糙的评估信号来走捷径多步骤任务，报告高进展但实际任务未完成）。该框架通过三个协同组件实现：阶段对齐奖励（SAR）使用Gemini生成的硬负例进行三元组对比学习；基于位姿的对象探索（POE）将好奇心锚定在相对对象-夹爪位姿而非像素上；以及具有选择性上下文和门控融合的长期记忆，用于稳定内在奖励塑造。在Discoverse-L基准测试（包含三个多阶段任务）上的评估表明，EvoVLA平均成功率比最强基线高10.2个百分点，达到69.2%，样本效率提高1.5倍，阶段幻觉率从38.5%降至14.8%。真实机器人部署的平均成功率为54.6%。\n### 论文核心贡献点\n1. 提出自监督长视野学习方法EvoVLA，结合SAR与Gemini驱动的硬负例、POE提供密集且语义一致的内在反馈，无需额外标签即可实现可扩展的VLA微调。\n2. 通过上下文选择的长期记忆机制（含选择性注意和门控融合）解决长视野遗忘问题，并结合Discoverse-L基准（含三个多阶段操作任务，18–74阶段），为社区提供研究长视野操作记忆的方法和数据集。\n3. 广泛实验验证：EvoVLA在Discoverse-L上成功率69.2%（+10.2点 vs. OpenVLA-OFT），样本效率达1.5×，真实机器人成功率54.6%（+11.0点 vs. OpenVLA-OFT），幻觉率降至14.8%。\n### 论文方法描述\n1. **阶段对齐奖励（SAR）**：\n - 三元组文本生成：使用Gemini 2.5 Pro为每阶段生成正例、互斥负例和反事实硬负例（如“抓取非目标对象”的谓词）。\n - 图像-文本对比评分：通过CLIP编码器计算阶段对齐分数 \\( u_k(t) = \\sigma(\\tau[s_k^+(t) - \\max\\{s_k^-(t), s_k^h(t)\\}]) \\)，其中 \\( s_k^+ \\) 为正例得分，\\( s_k^- \\) 和 \\( s_k^h \\) 为负例和硬负例得分。\n - 时间平滑：维护滑动平均 \\( \\bar{u}_k(t) = (1-\\alpha)\\bar{u}_k(t-1) + \\alpha u_k(t) \\)，阶段奖励 \\( r_t^{\\text{stage}} = \\bar{u}_{\\kappa_t}(t) - \\bar{u}_{\\kappa_t}(t-1) \\)，阈值触发阶段推进。\n2. **基于位姿的对象探索（POE）**：\n - 潜在操作动力学：将操作状态表示为 \\( z_t = \\psi(T_{\\text{ee}}^{-1}T_{\\text{obj}}) \\in \\mathbb{R}^6 \\)（相对位姿）。\n - 训练轻量级MLP前向/逆模型：\\( \\hat{z}_{t+1} = f_\\phi(z_t, a_t) \\) 和 \\( \\hat{a}_t = g_\\psi(z_t, z_{t+1}) \\)。\n - 任务相关内在奖励：好奇心奖励 \\( r_t^{\\text{cur}} = \\frac{\\eta}{2} \\\\|\\text{sg}(\\hat{z}_{t+1}) - z_{t+1}\\\\|_2^2 \\)（sg为停止梯度），基础进展奖励 \\( r_t^{\\text{base}} = \\text{ReLU}(\\overline{\\mathcal{L}_F}(t-1) - \\overline{\\mathcal{L}_F}(t)) \\)。\n3. **长期记忆**：\n - 上下文选择：通过注意力机制从记忆库 \\( \\mathcal{M} \\) 中选择Top-K历史项作为上下文令牌。\n - 门控融合：计算门控信号 \\( g_t^{\\text{mem}} = \\sigma(w_g^\\top [\\hat{h}_t; x_t]) \\)，融合当前表征与上下文 \\( \\tilde{x}_t = (1-g_t^{\\text{mem}})x_t + g_t^{\\text{mem}}\\hat{h}_t \\)。\n - 奖励调制：进展奖励 \\( r_t^{\\text{prog}} = g_t^{\\text{mem}} \\cdot r_t^{\\text{base}} \\)，抑制不稳定模式的虚假信号。\n4. **训练目标**：联合优化PPO损失、世界模型损失和熵正则化：\\( \\mathcal{L} = \\mathcal{L}_{\\text{PPO}}(\\tilde{r}) + \\lambda_F \\mathcal{L}_F + \\lambda_I \\mathcal{L}_I - \\lambda_{\\text{ent}} H(\\pi) \\)，其中组合奖励 \\( \\tilde{r}_t = r_t^e + \\rho (r_t^{\\text{stage}} + r_t^{\\text{cur}} + r_t^{\\text{prog}}) \\)。\n### 论文使用数据集和训练资源\n- **数据集**：Discoverse-L基准（基于DISCOVERSE模拟器和AIRBOT-Play平台），包含三个多阶段操作任务：\n - Block Bridge（74阶段）：放置两根横杆形成桥状结构并填充积木。\n - Stack（18阶段）：顺序堆叠三个彩色积木。\n - Jujube-Cup（19阶段）：将枣放入杯子并移至盘子。\n 每任务生成50个随机初始条件的演示轨迹，存储于RLDS格式，阶段语义由Gemini 2.5 Pro通过视频驱动工作流生成。\n- **训练资源**：硬件为4×NVIDIA H20 GPU（96GB VRAM），软件环境为8个并行DISCOVERSE环境，每个种子训练200万步，共3个随机种子，耗时约24小时/种子。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **仿真**：DISCOVERSE模拟器中的Discoverse-L任务，每任务50个测试回合，固定回合预算400步（复杂任务约150–250步完成）。\n - **真实世界**：物理AIRBOT-Play机器人部署，包括Sim2Real迁移测试（三个任务）和新任务训练（堆叠四个杯子并插入香蕉形物体）。\n- **评估指标**：\n - **成功率（SR）**：400步内完成任务的百分比。\n - **样本效率**：达到50%成功率所需的环境步数。\n - **幻觉率（HR）**：高VLM分数但实际未完成的阶段占比，公式为 \\( \\text{HR} = \\frac{\\mathbb{E}[\\mathbb{1}\\{u_k(t) > \\theta \\land c_k(t)=0\\}]}{\\mathbb{E}[\\mathbb{1}\\{u_k(t) > \\theta\\}]} \\)，其中 \\( \\theta = 0.7 \\)，\\( c_k(t) \\) 为地面真值完成指示器（仅用于评估）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Peking University</p>\n<h3>论文概述</h3>\n<p>论文提出EvoVLA框架，用于解决视觉-语言-行动（VLA）模型在长视野机器人操作中的“阶段幻觉”问题（即智能体通过利用粗糙的评估信号来走捷径多步骤任务，报告高进展但实际任务未完成）。该框架通过三个协同组件实现：阶段对齐奖励（SAR）使用Gemini生成的硬负例进行三元组对比学习；基于位姿的对象探索（POE）将好奇心锚定在相对对象-夹爪位姿而非像素上；以及具有选择性上下文和门控融合的长期记忆，用于稳定内在奖励塑造。在Discoverse-L基准测试（包含三个多阶段任务）上的评估表明，EvoVLA平均成功率比最强基线高10.2个百分点，达到69.2%，样本效率提高1.5倍，阶段幻觉率从38.5%降至14.8%。真实机器人部署的平均成功率为54.6%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出自监督长视野学习方法EvoVLA，结合SAR与Gemini驱动的硬负例、POE提供密集且语义一致的内在反馈，无需额外标签即可实现可扩展的VLA微调。</li><li>通过上下文选择的长期记忆机制（含选择性注意和门控融合）解决长视野遗忘问题，并结合Discoverse-L基准（含三个多阶段操作任务，18–74阶段），为社区提供研究长视野操作记忆的方法和数据集。</li><li>广泛实验验证：EvoVLA在Discoverse-L上成功率69.2%（+10.2点 vs. OpenVLA-OFT），样本效率达1.5×，真实机器人成功率54.6%（+11.0点 vs. OpenVLA-OFT），幻觉率降至14.8%。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>阶段对齐奖励（SAR）</strong>：</p>\n<p> - 三元组文本生成：使用Gemini 2.5 Pro为每阶段生成正例、互斥负例和反事实硬负例（如“抓取非目标对象”的谓词）。</p>\n<p> - 图像-文本对比评分：通过CLIP编码器计算阶段对齐分数 \\( u_k(t) = \\sigma(\\tau[s_k^+(t) - \\max\\{s_k^-(t), s_k^h(t)\\}]) \\)，其中 \\( s_k^+ \\) 为正例得分，\\( s_k^- \\) 和 \\( s_k^h \\) 为负例和硬负例得分。</p>\n<p> - 时间平滑：维护滑动平均 \\( \\bar{u}_k(t) = (1-\\alpha)\\bar{u}_k(t-1) + \\alpha u_k(t) \\)，阶段奖励 \\( r_t^{\\text{stage}} = \\bar{u}_{\\kappa_t}(t) - \\bar{u}_{\\kappa_t}(t-1) \\)，阈值触发阶段推进。</p>\n<p>2. <strong>基于位姿的对象探索（POE）</strong>：</p>\n<p> - 潜在操作动力学：将操作状态表示为 \\( z_t = \\psi(T_{\\text{ee}}^{-1}T_{\\text{obj}}) \\in \\mathbb{R}^6 \\)（相对位姿）。</p>\n<p> - 训练轻量级MLP前向/逆模型：\\( \\hat{z}_{t+1} = f_\\phi(z_t, a_t) \\) 和 \\( \\hat{a}_t = g_\\psi(z_t, z_{t+1}) \\)。</p>\n<p> - 任务相关内在奖励：好奇心奖励 \\( r_t^{\\text{cur}} = \\frac{\\eta}{2} \\\\|\\text{sg}(\\hat{z}_{t+1}) - z_{t+1}\\\\|_2^2 \\)（sg为停止梯度），基础进展奖励 \\( r_t^{\\text{base}} = \\text{ReLU}(\\overline{\\mathcal{L}_F}(t-1) - \\overline{\\mathcal{L}_F}(t)) \\)。</p>\n<p>3. <strong>长期记忆</strong>：</p>\n<p> - 上下文选择：通过注意力机制从记忆库 \\( \\mathcal{M} \\) 中选择Top-K历史项作为上下文令牌。</p>\n<p> - 门控融合：计算门控信号 \\( g_t^{\\text{mem}} = \\sigma(w_g^\\top [\\hat{h}_t; x_t]) \\)，融合当前表征与上下文 \\( \\tilde{x}_t = (1-g_t^{\\text{mem}})x_t + g_t^{\\text{mem}}\\hat{h}_t \\)。</p>\n<p> - 奖励调制：进展奖励 \\( r_t^{\\text{prog}} = g_t^{\\text{mem}} \\cdot r_t^{\\text{base}} \\)，抑制不稳定模式的虚假信号。</p>\n<p>4. <strong>训练目标</strong>：联合优化PPO损失、世界模型损失和熵正则化：\\( \\mathcal{L} = \\mathcal{L}_{\\text{PPO}}(\\tilde{r}) + \\lambda_F \\mathcal{L}_F + \\lambda_I \\mathcal{L}_I - \\lambda_{\\text{ent}} H(\\pi) \\)，其中组合奖励 \\( \\tilde{r}_t = r_t^e + \\rho (r_t^{\\text{stage}} + r_t^{\\text{cur}} + r_t^{\\text{prog}}) \\)。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Discoverse-L基准（基于DISCOVERSE模拟器和AIRBOT-Play平台），包含三个多阶段操作任务：</li></ul>\n<p> - Block Bridge（74阶段）：放置两根横杆形成桥状结构并填充积木。</p>\n<p> - Stack（18阶段）：顺序堆叠三个彩色积木。</p>\n<p> - Jujube-Cup（19阶段）：将枣放入杯子并移至盘子。</p>\n<p> 每任务生成50个随机初始条件的演示轨迹，存储于RLDS格式，阶段语义由Gemini 2.5 Pro通过视频驱动工作流生成。</p>\n<ul><li><strong>训练资源</strong>：硬件为4×NVIDIA H20 GPU（96GB VRAM），软件环境为8个并行DISCOVERSE环境，每个种子训练200万步，共3个随机种子，耗时约24小时/种子。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>仿真</strong>：DISCOVERSE模拟器中的Discoverse-L任务，每任务50个测试回合，固定回合预算400步（复杂任务约150–250步完成）。</p>\n<p> - <strong>真实世界</strong>：物理AIRBOT-Play机器人部署，包括Sim2Real迁移测试（三个任务）和新任务训练（堆叠四个杯子并插入香蕉形物体）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（SR）</strong>：400步内完成任务的百分比。</p>\n<p> - <strong>样本效率</strong>：达到50%成功率所需的环境步数。</p>\n<p> - <strong>幻觉率（HR）</strong>：高VLM分数但实际未完成的阶段占比，公式为 \\( \\text{HR} = \\frac{\\mathbb{E}[\\mathbb{1}\\{u_k(t) > \\theta \\land c_k(t)=0\\}]}{\\mathbb{E}[\\mathbb{1}\\{u_k(t) > \\theta\\}]} \\)，其中 \\( \\theta = 0.7 \\)，\\( c_k(t) \\) 为地面真值完成指示器（仅用于评估）。</p>"
  },
  {
    "date": "2025-11-19",
    "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.15605",
    "summary_markdown": "我正在处理用户的请求。用户提供了一篇ArXiv论文的HTML原文，要求我作为论文阅读专家，总结论文的要点，并输出Markdown格式的文本。总结需要分为指定部分：研究单位、论文概述、核心贡献点、方法描述、数据集与训练资源、评估环境与指标。特别要求不能使用加粗，且只输出Markdown内容。\n\n首先，我需要仔细解析HTML内容，提取关键信息。论文标题是“SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models”，这表明主题是关于视觉-语言-动作模型的强化学习方法。\n\n从作者信息部分，我看到研究单位包括复旦大学、同济大学和上海创新研究院。这些在作者列表的脚注中标明了。作者中有多人来自这些机构，例如Senyu Fei来自同济大学和上海创新研究院，Siyin Wang来自复旦大学和上海创新研究院等。\n\n论文概述部分在摘要中有清晰描述。SRPO旨在解决VLA模型依赖专家演示导致的演示偏差和奖励稀疏性问题。它引入自参考学习，用模型自身生成的成功轨迹作为参考，为失败轨迹分配进度奖励，并结合世界模型的潜在表示来鲁棒地衡量行为进展。实验在LIBERO基准测试上显示，从48.9%成功率提升到99.2%，无需额外监督。\n\n核心贡献点有三个：提出SRPO框架消除对外部演示的依赖；引入基于潜在世界表示的进度奖励方法；实验证明达到SOTA性能和强泛化能力。这些在摘要的贡献列表中。\n\n方法描述部分在Methods章节。SRPO包括问题公式化、世界进展奖励建模和自参考策略优化。具体是：使用世界模型编码器提取轨迹的潜在表示；对成功轨迹聚类并计算失败轨迹到最近中心的距离；基于距离生成进度奖励；用CLIP损失和KL正则化进行策略优化。关键创新是自参考机制和潜在空间的使用。\n\n数据集和训练资源在实验设置部分。主要使用LIBERO基准及其变体LIBERO-Plus；基础模型是修改的OpenVLA（OpenVLA*）；训练基于SiiRL框架，使用V-JEPA 2作为世界模型；计算资源在附录F中描述，但原文未详述，我需要基于上下文推断。附录提到训练细节，但未提供具体资源，所以总结时需注明见附录。\n\n评估环境与指标：实验在LIBERO模拟环境中进行，包括四个任务套件；指标是任务成功率；泛化评估在LIBERO-Plus上添加七种扰动；额外分析包括训练效率、轨迹探索等。真实世界实验在X-ARM 7机器人上进行，任务包括放置苹果等，成功率作为指标。\n\n现在，组织这些点成Markdown格式。确保不使用加粗，只纯文本和标题。分部分列出，语言简洁。</think>\n```markdown\n# 论文研究单位\n复旦大学、同济大学、上海创新研究院\n# 论文概述\n本文提出SRPO（Self-Referential Policy Optimization），一种针对视觉-语言-动作（VLA）模型的强化学习框架。旨在解决VLA模型依赖专家演示导致的演示偏差和奖励稀疏性问题，通过自参考学习利用模型自身生成的成功轨迹为失败轨迹提供进度奖励，并结合世界模型的潜在表示实现鲁棒进展评估。实验在LIBERO基准上从48.9%成功率提升至99.2%，无需额外监督或人工奖励工程。\n# 论文核心贡献点\n1. 提出SRPO框架：通过自参考机制利用批次内成功轨迹生成进度奖励，消除对外部演示和任务特定工程的需求。\n2. 引入潜在世界表示奖励方法：基于世界模型（如V-JEPA 2）的压缩编码轨迹，替代像素级方法，提升跨环境泛化能力和轨迹比较鲁棒性。\n3. 实验证实SOTA性能：在LIBERO基准达到99.2%成功率，并在LIBERO-Plus泛化基准上实现167%性能提升，证明高效性和强泛化能力。\n# 论文方法描述\n1. **问题公式化**：策略π_θ基于观察o_t和指令l生成动作a_t，环境状态z_t不可直接访问，仅通过稀疏终端奖励评估。\n2. **世界进展奖励建模**：\n - 使用预训练世界模型编码器𝒲提取轨迹潜在表示h_i = 𝒲(o_{0:T}^{(i)})。\n - 对成功轨迹集合𝒮应用DBSCAN聚类生成中心C，计算失败轨迹到最近中心的距离d_i。\n - 奖励函数g_i：成功轨迹为1.0，失败轨迹通过φ(·)映射距离到(0,1)，基于归一化距离(d_i - \\bar{d}) / σ_d。\n3. **自参考策略优化**：\n - 基于GRPO框架，优势估计\\hat{A}_i = (g_i - μ_g) / σ_g。\n - 损失函数L_SRPO(θ) = E_{t,i}[min(r_{i,t}(θ)\\hat{A}_i, clip(r_{i,t}(θ), 1-ε, 1+ε)\\hat{A}_i)] + β D_{KL}(π_θ ∥ π_{ref})。\n - 通过KL正则化保持策略稳定，利用组内相对性能优化策略。\n# 论文使用数据集和训练资源\n1. **数据集**：\n - 主要基准：LIBERO（包含Spatial、Object、Goal、Long任务套件，各10个任务）。\n - 泛化基准：LIBERO-Plus（添加7种扰动维度，如相机视角、光照、噪声等）。\n - 真实世界任务：5个X-ARM 7机器人任务（如放置苹果、折叠毛巾）。\n2. **训练资源**：\n - 基础模型：OpenVLA*（OpenVLA增强动作分块和并行解码）。\n - 预训练世界模型：V-JEPA 2（大规模机器人视频数据训练）。\n - 训练框架：SiiRL，先单样本监督微调（SFT），后SRPO在线强化学习。\n - 计算资源：详见附录F（原文未提供具体GPU/时间细节）。\n# 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 模拟环境：LIBERO物理模拟器，提供状态z_t（物体位置、速度等）和观察o_t（第三人称图像）。\n - 真实环境：X-ARM 7机器人执行物体操作任务。\n2. **评估指标**：\n - 主要指标：任务成功率（%），在LIBERO各套件和LIBERO-Plus扰动维度上评估。\n - 进度奖励质量指标：Spearman相关性（SC）、单调性（Mono）、最大均值差异（MMD）、JS散度（JS）、标准化均值差异（SMD），用于量化奖励区分成功/失败轨迹的能力。\n - 效率指标：训练步数（如200 RL步）和收敛速度对比（如GRPO）。\n - 探索指标：轨迹动作空间分布多样性（如末端执行器位置分散度）。\n - 真实世界指标：任务成功率提升（如扩散模型+66.8%，自回归模型+86.7%）。\n```",
    "summary_html": "<p>我正在处理用户的请求。用户提供了一篇ArXiv论文的HTML原文，要求我作为论文阅读专家，总结论文的要点，并输出Markdown格式的文本。总结需要分为指定部分：研究单位、论文概述、核心贡献点、方法描述、数据集与训练资源、评估环境与指标。特别要求不能使用加粗，且只输出Markdown内容。</p>\n\n<p>首先，我需要仔细解析HTML内容，提取关键信息。论文标题是“SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models”，这表明主题是关于视觉-语言-动作模型的强化学习方法。</p>\n\n<p>从作者信息部分，我看到研究单位包括复旦大学、同济大学和上海创新研究院。这些在作者列表的脚注中标明了。作者中有多人来自这些机构，例如Senyu Fei来自同济大学和上海创新研究院，Siyin Wang来自复旦大学和上海创新研究院等。</p>\n\n<p>论文概述部分在摘要中有清晰描述。SRPO旨在解决VLA模型依赖专家演示导致的演示偏差和奖励稀疏性问题。它引入自参考学习，用模型自身生成的成功轨迹作为参考，为失败轨迹分配进度奖励，并结合世界模型的潜在表示来鲁棒地衡量行为进展。实验在LIBERO基准测试上显示，从48.9%成功率提升到99.2%，无需额外监督。</p>\n\n<p>核心贡献点有三个：提出SRPO框架消除对外部演示的依赖；引入基于潜在世界表示的进度奖励方法；实验证明达到SOTA性能和强泛化能力。这些在摘要的贡献列表中。</p>\n\n<p>方法描述部分在Methods章节。SRPO包括问题公式化、世界进展奖励建模和自参考策略优化。具体是：使用世界模型编码器提取轨迹的潜在表示；对成功轨迹聚类并计算失败轨迹到最近中心的距离；基于距离生成进度奖励；用CLIP损失和KL正则化进行策略优化。关键创新是自参考机制和潜在空间的使用。</p>\n\n<p>数据集和训练资源在实验设置部分。主要使用LIBERO基准及其变体LIBERO-Plus；基础模型是修改的OpenVLA（OpenVLA*）；训练基于SiiRL框架，使用V-JEPA 2作为世界模型；计算资源在附录F中描述，但原文未详述，我需要基于上下文推断。附录提到训练细节，但未提供具体资源，所以总结时需注明见附录。</p>\n\n<p>评估环境与指标：实验在LIBERO模拟环境中进行，包括四个任务套件；指标是任务成功率；泛化评估在LIBERO-Plus上添加七种扰动；额外分析包括训练效率、轨迹探索等。真实世界实验在X-ARM 7机器人上进行，任务包括放置苹果等，成功率作为指标。</p>\n\n<p>现在，组织这些点成Markdown格式。确保不使用加粗，只纯文本和标题。分部分列出，语言简洁。</think></p>\n<p>```markdown</p>\n<h1>论文研究单位</h1>\n<p>复旦大学、同济大学、上海创新研究院</p>\n<h1>论文概述</h1>\n<p>本文提出SRPO（Self-Referential Policy Optimization），一种针对视觉-语言-动作（VLA）模型的强化学习框架。旨在解决VLA模型依赖专家演示导致的演示偏差和奖励稀疏性问题，通过自参考学习利用模型自身生成的成功轨迹为失败轨迹提供进度奖励，并结合世界模型的潜在表示实现鲁棒进展评估。实验在LIBERO基准上从48.9%成功率提升至99.2%，无需额外监督或人工奖励工程。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出SRPO框架：通过自参考机制利用批次内成功轨迹生成进度奖励，消除对外部演示和任务特定工程的需求。</li><li>引入潜在世界表示奖励方法：基于世界模型（如V-JEPA 2）的压缩编码轨迹，替代像素级方法，提升跨环境泛化能力和轨迹比较鲁棒性。</li><li>实验证实SOTA性能：在LIBERO基准达到99.2%成功率，并在LIBERO-Plus泛化基准上实现167%性能提升，证明高效性和强泛化能力。</li></ol>\n<h1>论文方法描述</h1>\n<ol><li><strong>问题公式化</strong>：策略π_θ基于观察o_t和指令l生成动作a_t，环境状态z_t不可直接访问，仅通过稀疏终端奖励评估。</li><li><strong>世界进展奖励建模</strong>：</li></ol>\n<p> - 使用预训练世界模型编码器𝒲提取轨迹潜在表示h_i = 𝒲(o_{0:T}^{(i)})。</p>\n<p> - 对成功轨迹集合𝒮应用DBSCAN聚类生成中心C，计算失败轨迹到最近中心的距离d_i。</p>\n<p> - 奖励函数g_i：成功轨迹为1.0，失败轨迹通过φ(·)映射距离到(0,1)，基于归一化距离(d_i - \\bar{d}) / σ_d。</p>\n<p>3. <strong>自参考策略优化</strong>：</p>\n<p> - 基于GRPO框架，优势估计\\hat{A}_i = (g_i - μ_g) / σ_g。</p>\n<p> - 损失函数L_SRPO(θ) = E_{t,i}[min(r_{i,t}(θ)\\hat{A}_i, clip(r_{i,t}(θ), 1-ε, 1+ε)\\hat{A}_i)] + β D_{KL}(π_θ ∥ π_{ref})。</p>\n<p> - 通过KL正则化保持策略稳定，利用组内相对性能优化策略。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>1. <strong>数据集</strong>：</p>\n<p> - 主要基准：LIBERO（包含Spatial、Object、Goal、Long任务套件，各10个任务）。</p>\n<p> - 泛化基准：LIBERO-Plus（添加7种扰动维度，如相机视角、光照、噪声等）。</p>\n<p> - 真实世界任务：5个X-ARM 7机器人任务（如放置苹果、折叠毛巾）。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - 基础模型：OpenVLA*（OpenVLA增强动作分块和并行解码）。</p>\n<p> - 预训练世界模型：V-JEPA 2（大规模机器人视频数据训练）。</p>\n<p> - 训练框架：SiiRL，先单样本监督微调（SFT），后SRPO在线强化学习。</p>\n<p> - 计算资源：详见附录F（原文未提供具体GPU/时间细节）。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - 模拟环境：LIBERO物理模拟器，提供状态z_t（物体位置、速度等）和观察o_t（第三人称图像）。</p>\n<p> - 真实环境：X-ARM 7机器人执行物体操作任务。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - 主要指标：任务成功率（%），在LIBERO各套件和LIBERO-Plus扰动维度上评估。</p>\n<p> - 进度奖励质量指标：Spearman相关性（SC）、单调性（Mono）、最大均值差异（MMD）、JS散度（JS）、标准化均值差异（SMD），用于量化奖励区分成功/失败轨迹的能力。</p>\n<p> - 效率指标：训练步数（如200 RL步）和收敛速度对比（如GRPO）。</p>\n<p> - 探索指标：轨迹动作空间分布多样性（如末端执行器位置分散度）。</p>\n<p> - 真实世界指标：任务成功率提升（如扩散模型+66.8%，自回归模型+86.7%）。</p>\n<p>```</p>"
  },
  {
    "date": "2025-11-19",
    "title": "IPR-1: Interactive Physical Reasoner",
    "link": "http://arxiv.org/abs/2511.15407",
    "summary_markdown": "## 论文研究单位\n- 上海交通大学\n- 上海创新研究院\n- 卡内基梅隆大学\n## 论文概述\n论文提出了交互式物理推理器，旨在让智能体通过与环境交互来学习并持续改进人类般的物理推理能力。研究在Game-to-Unseen (G2U)设定下进行，策划了超过1000个具有不同物理和因果机制的异构游戏，并采用受马斯洛需求层次启发的三个级别进行评估：生存、好奇心和效用，分别对应从原始直觉到目标驱动推理的递进。分析发现，现有的VLM/VLA模型虽有推理能力但缺乏交互中的前瞻性，而世界模型能想象未来但倾向于模仿视觉模式而非分析物理因果。因此，IPR利用世界模型的推演来评分和强化VLM的策略，并引入了PhysCode，一种以物理为中心的动作代码，用于统一语义意图与动态，为预测和推理提供共享的动作空间。在1000多个游戏上预训练的IPR在三个级别上表现稳健，总体与GPT-5相当，并在好奇心上超越GPT-5，且性能随训练游戏和交互步数的增加而提升，并能零样本迁移到未见过的游戏中。\n## 论文核心贡献点\n- 提出了G2U问题，并构建了包含1000+异构游戏的基准，采用分层评估（生存/好奇心/效用），诊断了基于预测、强化学习和VLM等方法的优势与不足。\n- 提出了IPR范式：通过世界模型推演来评分和强化同一动作空间中的VLM策略，使得交互经验能够稳步提升物理推理能力。\n- 引入了PhysCode，一种以物理为中心的动作代码，融合了动作语义与视觉动态，为世界模型的预测和VLM的推理之间架起了桥梁。\n## 论文方法描述\nIPR方法包含三个核心组件：\n1. **诱导潜在动作词汇**：通过一个VQ-VAE模型，从视频帧（DINOv3特征）、光流和动作语义中学习一个离散的、以物理为中心的动作代码，即PhysCode。训练目标是使该代码能够捕捉可跨游戏共享的动态基元，并与特定领域的视觉可供性对齐。\n2. **训练潜在层面的世界模型**：在固定PhysCode词汇表后，训练一个特征级世界模型，该模型以当前特征和PhysCode序列为条件，预测未来的特征和奖励。训练过程包括一个特征预测损失和一个用于评估动作价值的Q学习风格的评论器头。\n3. **预测增强的交互式推理**：使用一个8B参数的VLM（Qwen3-VL-8B）作为策略，在推理时生成多个候选的PhysCode序列。世界模型在潜在空间中对这些序列进行短时域推演，并预测其回报和价值。然后，利用这些预测的优势来通过GRPO算法对VLM策略进行强化优化，选择最佳动作执行。\n## 论文使用数据集和训练资源\n- **数据集**：研究整理了一个大规模、多样化的游戏基准，包含：\n - 863个开源复古游戏（通过stable-retro）\n - 134个轻量级HTML/Canvas游戏\n - 3个商业游戏\n 总计超过1000个游戏。数据分布覆盖了不同的游戏类别、控制接口、视觉复杂度、视角、因果机制、物理原理和操作难度。对于每个游戏，记录了4分钟60FPS的人类游戏数据，并包含了物理原理、因果机制、动作语义和游戏指令等注释。\n- **训练资源**：\n - **模型骨干**：VLM部分使用了Qwen3-VL-8B模型。\n - **特征提取**：使用DINOv3提取图像特征，使用T5编码器处理轻量级语义提示。\n - **训练流程**：分三阶段训练，包括PhysCode的预训练、世界模型的训练，以及通过交互式经验对VLM进行强化学习。\n## 论文使用的评估环境和评估指标\n- **评估环境**：主要在200个游戏组成的评估集上进行，该子集的分布（类型、动作空间、物理因果等）与完整数据集保持一致。此外，为了验证零样本迁移能力，构建了一个包含50个未见游戏的保留测试集。\n- **评估指标**：采用受马斯洛需求层次启发的三层评估体系：\n - **生存**：衡量智能体避免风险、存活的时间。指标为归一化的生存时间 H = E[T] / T_typ，其中T_typ是每个游戏下的参考存活时间（例如随机策略下的中位存活时间）。\n - **好奇心**：衡量智能体访问新颖状态的广度。指标为状态空间覆盖率，使用CLIP视觉编码器计算轨迹的多尺度度量空间幅度曲线下的面积 E = AUC(M(τ))。\n - **效用**：衡量智能体达成下游目标的能力。指标为人类归一化分数 (HNS) = (m - m_rnd) / (m_hum - m_rnd)，其中m是智能体得分，m_rnd是随机基准得分，m_hum是人类得分。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>上海交通大学</li><li>上海创新研究院</li><li>卡内基梅隆大学</li></ul>\n<h2>论文概述</h2>\n<p>论文提出了交互式物理推理器，旨在让智能体通过与环境交互来学习并持续改进人类般的物理推理能力。研究在Game-to-Unseen (G2U)设定下进行，策划了超过1000个具有不同物理和因果机制的异构游戏，并采用受马斯洛需求层次启发的三个级别进行评估：生存、好奇心和效用，分别对应从原始直觉到目标驱动推理的递进。分析发现，现有的VLM/VLA模型虽有推理能力但缺乏交互中的前瞻性，而世界模型能想象未来但倾向于模仿视觉模式而非分析物理因果。因此，IPR利用世界模型的推演来评分和强化VLM的策略，并引入了PhysCode，一种以物理为中心的动作代码，用于统一语义意图与动态，为预测和推理提供共享的动作空间。在1000多个游戏上预训练的IPR在三个级别上表现稳健，总体与GPT-5相当，并在好奇心上超越GPT-5，且性能随训练游戏和交互步数的增加而提升，并能零样本迁移到未见过的游戏中。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了G2U问题，并构建了包含1000+异构游戏的基准，采用分层评估（生存/好奇心/效用），诊断了基于预测、强化学习和VLM等方法的优势与不足。</li><li>提出了IPR范式：通过世界模型推演来评分和强化同一动作空间中的VLM策略，使得交互经验能够稳步提升物理推理能力。</li><li>引入了PhysCode，一种以物理为中心的动作代码，融合了动作语义与视觉动态，为世界模型的预测和VLM的推理之间架起了桥梁。</li></ul>\n<h2>论文方法描述</h2>\n<p>IPR方法包含三个核心组件：</p>\n<ol><li><strong>诱导潜在动作词汇</strong>：通过一个VQ-VAE模型，从视频帧（DINOv3特征）、光流和动作语义中学习一个离散的、以物理为中心的动作代码，即PhysCode。训练目标是使该代码能够捕捉可跨游戏共享的动态基元，并与特定领域的视觉可供性对齐。</li><li><strong>训练潜在层面的世界模型</strong>：在固定PhysCode词汇表后，训练一个特征级世界模型，该模型以当前特征和PhysCode序列为条件，预测未来的特征和奖励。训练过程包括一个特征预测损失和一个用于评估动作价值的Q学习风格的评论器头。</li><li><strong>预测增强的交互式推理</strong>：使用一个8B参数的VLM（Qwen3-VL-8B）作为策略，在推理时生成多个候选的PhysCode序列。世界模型在潜在空间中对这些序列进行短时域推演，并预测其回报和价值。然后，利用这些预测的优势来通过GRPO算法对VLM策略进行强化优化，选择最佳动作执行。</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：研究整理了一个大规模、多样化的游戏基准，包含：</li></ul>\n<p> - 863个开源复古游戏（通过stable-retro）</p>\n<p> - 134个轻量级HTML/Canvas游戏</p>\n<p> - 3个商业游戏</p>\n<p> 总计超过1000个游戏。数据分布覆盖了不同的游戏类别、控制接口、视觉复杂度、视角、因果机制、物理原理和操作难度。对于每个游戏，记录了4分钟60FPS的人类游戏数据，并包含了物理原理、因果机制、动作语义和游戏指令等注释。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - <strong>模型骨干</strong>：VLM部分使用了Qwen3-VL-8B模型。</p>\n<p> - <strong>特征提取</strong>：使用DINOv3提取图像特征，使用T5编码器处理轻量级语义提示。</p>\n<p> - <strong>训练流程</strong>：分三阶段训练，包括PhysCode的预训练、世界模型的训练，以及通过交互式经验对VLM进行强化学习。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：主要在200个游戏组成的评估集上进行，该子集的分布（类型、动作空间、物理因果等）与完整数据集保持一致。此外，为了验证零样本迁移能力，构建了一个包含50个未见游戏的保留测试集。</li><li><strong>评估指标</strong>：采用受马斯洛需求层次启发的三层评估体系：</li></ul>\n<p> - <strong>生存</strong>：衡量智能体避免风险、存活的时间。指标为归一化的生存时间 H = E[T] / T_typ，其中T_typ是每个游戏下的参考存活时间（例如随机策略下的中位存活时间）。</p>\n<p> - <strong>好奇心</strong>：衡量智能体访问新颖状态的广度。指标为状态空间覆盖率，使用CLIP视觉编码器计算轨迹的多尺度度量空间幅度曲线下的面积 E = AUC(M(τ))。</p>\n<p> - <strong>效用</strong>：衡量智能体达成下游目标的能力。指标为人类归一化分数 (HNS) = (m - m_rnd) / (m_hum - m_rnd)，其中m是智能体得分，m_rnd是随机基准得分，m_hum是人类得分。</p>"
  },
  {
    "date": "2025-11-19",
    "title": "Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception",
    "link": "http://arxiv.org/abs/2511.15279",
    "summary_markdown": "论文研究单位\n上海交通大学人工智能学院，中国科学院自动化研究所，大连理工大学。\n\n论文概述\n该论文提出了一种名为EyeVLA的机器人眼球系统，用于具身感知中的主动视觉感知。现有固定相机系统难以兼顾大范围覆盖和细粒度细节获取。EyeVLA通过将动作行为离散化为动作标记，并将其与强大的视觉语言模型集成，在单一自回归序列中联合建模视觉、语言和动作。它利用二维边界框坐标引导推理链，并通过强化学习优化视点选择策略，从而在仅使用少量真实世界数据的情况下，将视觉语言模型的开放世界理解能力迁移到视觉-语言-动作策略中。实验表明，EyeVLA能在真实环境中有效理解场景，并通过指令驱动的旋转和缩放动作主动获取更准确的视觉信息。\n\n论文核心贡献点\n1. 将相机运动和缩放控制构建为一个离散的、标记化的决策过程，与多模态推理无缝集成，将视觉感知从被动帧消费转变为闭环、证据驱动、任务感知的主动获取范式。\n2. 引入了对平移、倾斜和缩放调整的分层离散化方法，并将其嵌入视觉语言模型词汇表，实现了对图像、语言和动作的统一自回归建模，无需单独的控制头。\n3. 利用二维区域（边界框）信号作为推理链中的结构指导和奖励塑形元素，设计了一种强化学习机制，将开放世界语义迁移到精细化的主动控制中。\n4. 展示了一个数据高效的流程，仅用500个真实世界样本和伪标签扩展，EyeVLA就能学习可操作策略，并在开放世界场景中实现零样本能力。\n\n论文方法描述\n1. 整体框架：构建了一个由二维云台和可变焦相机组成的“机器人眼球”硬件系统。算法层面，基于Qwen2.5-VL-7B模型进行适配，实现指令跟随、场景理解和相机控制。系统输入为初始RGB图像和自然语言指令，输出为可解码为相机控制命令（水平旋转Δθ1、垂直旋转Δθ2、缩放Δzoom）的动作标记序列。\n2. 动作编解码：提出了一种分层动作编码方案来高效表示连续的相机动作。将目标角度分解为十进制数字，并对每个数字使用基集{5, 2, 1}进行最优线性组合编码，以最小化标记数量。该方案理论上最优，且编解码效率高。\n3. 合成数据生成器：首先手动操作机器人眼球系统收集了500个真实世界样本。然后，利用现有的物体检测数据集（Rexverse-2M）生成合成数据。通过归一化边界框中心坐标来建模平移和倾斜角度，利用边界框面积之比来建模缩放，并使用随机森林模型学习映射关系以生成伪标签。\n4. 两阶段训练策略：\n - 阶段一：使用伪标签进行监督对齐微调。为了保留预训练能力，冻结了视觉Transformer（ViT）编码器和视觉-语言投影仪的参数，仅更新语言模型主干和新引入的动作标记嵌入。\n - 阶段二：使用强化学习进行策略优化。采用Group Relative Policy Optimization (GRPO)来优化策略，修正伪标签中的潜在偏差，提高模型在真实世界中的泛化能力。奖励函数结合了边界框的IoU以及三个动作输出的预测值与真实值之间的绝对差异。\n\n论文使用数据集和训练资源\n1. 数据集：\n - 真实世界数据：通过自建机器人眼球系统收集的500个样本，分为450个训练样本和50个测试样本。\n - 合成数据：从Rexverse-2M数据集中随机采样并筛选出的50,000张包含小目标物体的图像，用于生成伪标签以扩充数据集。\n2. 训练资源：基础模型为Qwen2.5-VL-7B-Instruct。在词汇表中添加了43个新的动作标记来表示平移、倾斜和缩放的变化值，并相应地扩展了嵌入维度。论文中未明确提及具体的GPU硬件和训练时长等计算资源。\n\n论文使用的评估环境和评估指标\n1. 评估环境：在受控的实验室环境中搭建的真实世界场景进行评估，测试了50个多样化的真实世界场景。\n2. 评估指标：\n - 定量指标：预测动作的平均绝对误差（MAE），包括水平旋转（Δθ1）、垂直旋转（Δθ2）和缩放（Δzoom）的误差。\n - 任务成功率：在真实场景中成功完成指定任务的比例。\n - 边界框交并比：用于奖励函数设计和数据过滤的指标，衡量预测边界框与真实边界框的重合度。",
    "summary_html": "<p>论文研究单位</p>\n<p>上海交通大学人工智能学院，中国科学院自动化研究所，大连理工大学。</p>\n\n<p>论文概述</p>\n<p>该论文提出了一种名为EyeVLA的机器人眼球系统，用于具身感知中的主动视觉感知。现有固定相机系统难以兼顾大范围覆盖和细粒度细节获取。EyeVLA通过将动作行为离散化为动作标记，并将其与强大的视觉语言模型集成，在单一自回归序列中联合建模视觉、语言和动作。它利用二维边界框坐标引导推理链，并通过强化学习优化视点选择策略，从而在仅使用少量真实世界数据的情况下，将视觉语言模型的开放世界理解能力迁移到视觉-语言-动作策略中。实验表明，EyeVLA能在真实环境中有效理解场景，并通过指令驱动的旋转和缩放动作主动获取更准确的视觉信息。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>将相机运动和缩放控制构建为一个离散的、标记化的决策过程，与多模态推理无缝集成，将视觉感知从被动帧消费转变为闭环、证据驱动、任务感知的主动获取范式。</li><li>引入了对平移、倾斜和缩放调整的分层离散化方法，并将其嵌入视觉语言模型词汇表，实现了对图像、语言和动作的统一自回归建模，无需单独的控制头。</li><li>利用二维区域（边界框）信号作为推理链中的结构指导和奖励塑形元素，设计了一种强化学习机制，将开放世界语义迁移到精细化的主动控制中。</li><li>展示了一个数据高效的流程，仅用500个真实世界样本和伪标签扩展，EyeVLA就能学习可操作策略，并在开放世界场景中实现零样本能力。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>整体框架：构建了一个由二维云台和可变焦相机组成的“机器人眼球”硬件系统。算法层面，基于Qwen2.5-VL-7B模型进行适配，实现指令跟随、场景理解和相机控制。系统输入为初始RGB图像和自然语言指令，输出为可解码为相机控制命令（水平旋转Δθ1、垂直旋转Δθ2、缩放Δzoom）的动作标记序列。</li><li>动作编解码：提出了一种分层动作编码方案来高效表示连续的相机动作。将目标角度分解为十进制数字，并对每个数字使用基集{5, 2, 1}进行最优线性组合编码，以最小化标记数量。该方案理论上最优，且编解码效率高。</li><li>合成数据生成器：首先手动操作机器人眼球系统收集了500个真实世界样本。然后，利用现有的物体检测数据集（Rexverse-2M）生成合成数据。通过归一化边界框中心坐标来建模平移和倾斜角度，利用边界框面积之比来建模缩放，并使用随机森林模型学习映射关系以生成伪标签。</li><li>两阶段训练策略：</li></ol>\n<p> - 阶段一：使用伪标签进行监督对齐微调。为了保留预训练能力，冻结了视觉Transformer（ViT）编码器和视觉-语言投影仪的参数，仅更新语言模型主干和新引入的动作标记嵌入。</p>\n<p> - 阶段二：使用强化学习进行策略优化。采用Group Relative Policy Optimization (GRPO)来优化策略，修正伪标签中的潜在偏差，提高模型在真实世界中的泛化能力。奖励函数结合了边界框的IoU以及三个动作输出的预测值与真实值之间的绝对差异。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. 数据集：</p>\n<p> - 真实世界数据：通过自建机器人眼球系统收集的500个样本，分为450个训练样本和50个测试样本。</p>\n<p> - 合成数据：从Rexverse-2M数据集中随机采样并筛选出的50,000张包含小目标物体的图像，用于生成伪标签以扩充数据集。</p>\n<p>2. 训练资源：基础模型为Qwen2.5-VL-7B-Instruct。在词汇表中添加了43个新的动作标记来表示平移、倾斜和缩放的变化值，并相应地扩展了嵌入维度。论文中未明确提及具体的GPU硬件和训练时长等计算资源。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：在受控的实验室环境中搭建的真实世界场景进行评估，测试了50个多样化的真实世界场景。</li><li>评估指标：</li></ol>\n<p> - 定量指标：预测动作的平均绝对误差（MAE），包括水平旋转（Δθ1）、垂直旋转（Δθ2）和缩放（Δzoom）的误差。</p>\n<p> - 任务成功率：在真实场景中成功完成指定任务的比例。</p>\n<p> - 边界框交并比：用于奖励函数设计和数据过滤的指标，衡量预测边界框与真实边界框的重合度。</p>"
  },
  {
    "date": "2025-11-18",
    "title": "$π^{*}_{0.6}$: a VLA That Learns From Experience",
    "link": "http://arxiv.org/abs/2511.14759",
    "summary_markdown": "论文研究单位\nPhysical Intelligence\n\n论文概述\n该论文研究如何通过强化学习（RL）让视觉-语言-动作（VLA）模型在真实世界部署中通过经验获得提升。论文提出了一种名为Recap（RL with Experience and Corrections via Advantage-conditioned Policies）的通用方法，该方法通过优势条件策略来对VLA进行RL训练。Recap能够将异构数据，包括人类演示、自主策略收集的数据以及专家在自主执行期间提供的干预数据，整合到模型的自我改进过程中。通过这种方式训练出的π^∗_0.6模型，可以在真实家庭中折叠衣物、可靠地组装盒子，并使用专业的意式咖啡机制作咖啡。在一些最具挑战性的任务上，Recap方法能将任务吞吐量提高一倍以上，并将任务失败率降低约一半。\n\n论文核心贡献点\n1. 提出了一种通用的RL方法Recap，它通过优势条件策略（advantage-conditioned policies）为VLA的训练提供了一种可扩展且稳定的方案。\n2. 该方法能够整合多种异构数据源进行自我提升，包括演示数据、自主收集的经验数据以及专家在部署期间提供的纠正性干预数据。\n3. 训练了一个名为π^∗_0.6的通用VLA模型，并展示了通过Recap方法，该模型可以被专门化以在下游任务上达到高性能，例如折叠衣物、组装盒子和制作咖啡。\n4. 实验证明，与仅使用模仿学习的基线相比，Recap在最难任务上将任务吞吐量提高了超过一倍，并将失败率降低了2倍或更多。\n5. 首次展示了，一个带有人类奖励反馈和干预的通用RL食谱，可以利用通过部署收集的经验，显著提高VLA模型的鲁棒性和吞吐量。\n\n论文方法描述\nRecap方法包含三个核心步骤，并可重复迭代：\n1. **数据收集**: 在真实环境中运行当前VLA策略，为每个试次标记任务结果（用于确定奖励），并可选择性地提供人类专家的干预，以作为对早期迭代中错误的纠正示例。\n2. **价值函数训练**: 使用迄今为止收集的所有数据，训练一个大型的多任务价值函数V^π_ref，该函数能够检测失败并判断任务完成所需的预期时间。该价值函数是一个分布式的、基于分类器的模型，它将观测和语言指令映射到离散的价值区间。\n3. **优势条件训练**: 为了使用该价值函数改进VLA策略，在VLA的输入前缀中包含一个基于从价值函数导出的优势值的优化性指标。这种“优势条件”的配方提供了一种简单有效的方法，能从包含次优数据的数据中提取出更优的策略。\n\n具体实现细节包括：\n- **策略提取**: 训练策略模型以同时表示行为策略（π_ref）和改进策略。训练目标是最大化一个复合的对数似然函数，该函数结合了无条件策略的似然和条件策略（基于优势指标I_t）的似然，从而能够有效利用所有数据。\n- **模型架构 (π^∗_0.6)**: 基于一个名为π_0.6的VLA，该模型使用Gemma 3 4B作为骨干网络和一个860M参数的流匹配动作专家。π^∗_0.6增加了处理二值化优势指标作为文本输入的能力，使其适用于Recap的RL训练。\n- **奖励函数**: 设计了一个通用的稀疏奖励函数。成功完成时奖励为0，失败时为一个大的负值常数，其他所有步骤为-1。这使得价值函数能够学习预测距离任务成功所需的步数（的负值）。\n\n论文使用数据集和训练资源\n- **预训练数据集**: 包含了来自多个不同机器人平台的数万小时、跨多种任务的演示数据。\n- **微调数据集**: 针对特定下游任务（如折叠衣物、制作咖啡）在线收集的自主经验数据，以及在部署期间由人类专家提供的干预数据。\n- **训练资源**: 训练了一个大型VLA模型π^∗_0.6，其视觉-语言模型（VLM）骨干网络为Gemma 3 4B，动作专家网络为860M参数。价值函数使用了一个较小的670M参数的VLM骨干网络。训练过程采用了知识隔离（KI）和流匹配技术。\n\n论文使用的评估环境和评估指标\n- **评估环境**: 真实世界环境，包括家庭环境（如折叠衣物）和工厂环境（如组装盒子）。使用的机器人平台是一个静态的双臂系统，每个手臂有6个自由度，配备平行夹爪，由三个摄像头（一个位于手臂之间的底座摄像头，每个手腕各一个）提供观测，并以50Hz的频率进行控制。\n- **评估任务**:\n - **衣物折叠**: 包括T恤和短裤的折叠、包含11种衣物的多样化折叠（评估时重点关注最难的扣子衬衫）、以及旨在移除特定失败模式的折叠。\n - **咖啡制作**: 使用专业的意式咖啡机制作双份浓缩咖啡。\n - **盒子组装**: 在真实工厂部署场景中组装用于实际包装的纸板箱。\n- **评估指标**:\n - **吞吐量**: 每小时成功完成的任务次数，该指标同时衡量了成功率与速度。\n - **成功率**: 任务成功完成的比率。\n - **失败率**: 任务失败的比率。",
    "summary_html": "<p>论文研究单位</p>\n<p>Physical Intelligence</p>\n\n<p>论文概述</p>\n<p>该论文研究如何通过强化学习（RL）让视觉-语言-动作（VLA）模型在真实世界部署中通过经验获得提升。论文提出了一种名为Recap（RL with Experience and Corrections via Advantage-conditioned Policies）的通用方法，该方法通过优势条件策略来对VLA进行RL训练。Recap能够将异构数据，包括人类演示、自主策略收集的数据以及专家在自主执行期间提供的干预数据，整合到模型的自我改进过程中。通过这种方式训练出的π^∗_0.6模型，可以在真实家庭中折叠衣物、可靠地组装盒子，并使用专业的意式咖啡机制作咖啡。在一些最具挑战性的任务上，Recap方法能将任务吞吐量提高一倍以上，并将任务失败率降低约一半。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种通用的RL方法Recap，它通过优势条件策略（advantage-conditioned policies）为VLA的训练提供了一种可扩展且稳定的方案。</li><li>该方法能够整合多种异构数据源进行自我提升，包括演示数据、自主收集的经验数据以及专家在部署期间提供的纠正性干预数据。</li><li>训练了一个名为π^∗_0.6的通用VLA模型，并展示了通过Recap方法，该模型可以被专门化以在下游任务上达到高性能，例如折叠衣物、组装盒子和制作咖啡。</li><li>实验证明，与仅使用模仿学习的基线相比，Recap在最难任务上将任务吞吐量提高了超过一倍，并将失败率降低了2倍或更多。</li><li>首次展示了，一个带有人类奖励反馈和干预的通用RL食谱，可以利用通过部署收集的经验，显著提高VLA模型的鲁棒性和吞吐量。</li></ol>\n\n<p>论文方法描述</p>\n<p>Recap方法包含三个核心步骤，并可重复迭代：</p>\n<ol><li><strong>数据收集</strong>: 在真实环境中运行当前VLA策略，为每个试次标记任务结果（用于确定奖励），并可选择性地提供人类专家的干预，以作为对早期迭代中错误的纠正示例。</li><li><strong>价值函数训练</strong>: 使用迄今为止收集的所有数据，训练一个大型的多任务价值函数V^π_ref，该函数能够检测失败并判断任务完成所需的预期时间。该价值函数是一个分布式的、基于分类器的模型，它将观测和语言指令映射到离散的价值区间。</li><li><strong>优势条件训练</strong>: 为了使用该价值函数改进VLA策略，在VLA的输入前缀中包含一个基于从价值函数导出的优势值的优化性指标。这种“优势条件”的配方提供了一种简单有效的方法，能从包含次优数据的数据中提取出更优的策略。</li></ol>\n\n<p>具体实现细节包括：</p>\n<ul><li><strong>策略提取</strong>: 训练策略模型以同时表示行为策略（π_ref）和改进策略。训练目标是最大化一个复合的对数似然函数，该函数结合了无条件策略的似然和条件策略（基于优势指标I_t）的似然，从而能够有效利用所有数据。</li><li><strong>模型架构 (π^∗_0.6)</strong>: 基于一个名为π_0.6的VLA，该模型使用Gemma 3 4B作为骨干网络和一个860M参数的流匹配动作专家。π^∗_0.6增加了处理二值化优势指标作为文本输入的能力，使其适用于Recap的RL训练。</li><li><strong>奖励函数</strong>: 设计了一个通用的稀疏奖励函数。成功完成时奖励为0，失败时为一个大的负值常数，其他所有步骤为-1。这使得价值函数能够学习预测距离任务成功所需的步数（的负值）。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>预训练数据集</strong>: 包含了来自多个不同机器人平台的数万小时、跨多种任务的演示数据。</li><li><strong>微调数据集</strong>: 针对特定下游任务（如折叠衣物、制作咖啡）在线收集的自主经验数据，以及在部署期间由人类专家提供的干预数据。</li><li><strong>训练资源</strong>: 训练了一个大型VLA模型π^∗_0.6，其视觉-语言模型（VLM）骨干网络为Gemma 3 4B，动作专家网络为860M参数。价值函数使用了一个较小的670M参数的VLM骨干网络。训练过程采用了知识隔离（KI）和流匹配技术。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>: 真实世界环境，包括家庭环境（如折叠衣物）和工厂环境（如组装盒子）。使用的机器人平台是一个静态的双臂系统，每个手臂有6个自由度，配备平行夹爪，由三个摄像头（一个位于手臂之间的底座摄像头，每个手腕各一个）提供观测，并以50Hz的频率进行控制。</li><li><strong>评估任务</strong>:</li></ul>\n<p> - <strong>衣物折叠</strong>: 包括T恤和短裤的折叠、包含11种衣物的多样化折叠（评估时重点关注最难的扣子衬衫）、以及旨在移除特定失败模式的折叠。</p>\n<p> - <strong>咖啡制作</strong>: 使用专业的意式咖啡机制作双份浓缩咖啡。</p>\n<p> - <strong>盒子组装</strong>: 在真实工厂部署场景中组装用于实际包装的纸板箱。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>吞吐量</strong>: 每小时成功完成的任务次数，该指标同时衡量了成功率与速度。</p>\n<p> - <strong>成功率</strong>: 任务成功完成的比率。</p>\n<p> - <strong>失败率</strong>: 任务失败的比率。</p>"
  },
  {
    "date": "2025-11-18",
    "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards",
    "link": "http://arxiv.org/abs/2511.14659",
    "summary_markdown": "论文研究单位\n南洋理工大学、Lambda Labs、新加坡科技设计大学\n\n论文概述\n该论文提出了NORA-1.5，一个视觉-语言-动作（VLA）模型，通过在预训练的NORA模型基础上增加一个基于流匹配的动作专家模块来构建。该方法不仅通过架构改进提升了性能，还引入了基于世界模型和动作偏好的奖励机制，结合直接偏好优化（DPO）进行后训练，显著提高了模型在模拟和真实机器人任务中的可靠性和泛化能力。\n\n论文核心贡献点\n1. 提出NORA-1.5模型，通过集成流匹配动作专家与自回归VLA骨干网络，在多个基准测试中实现最先进性能。\n2. 设计了多策略奖励机制，包括世界模型引导的目标奖励、基于真实动作的偏差奖励和子目标奖励，用于构建偏好数据集。\n3. 详细分析了流匹配专家与自回归VLA骨干的协同效应，揭示了数据依赖性行为。\n4. 验证了轻量级奖励模型与DPO结合在模拟和真实机器人环境中的一致性提升，为VLA模型提供了可扩展的后训练方向。\n\n论文方法描述\nNORA-1.5由两部分组成：预训练的NORA模型作为视觉-语言编码器，以及一个流匹配的动作专家模块。动作专家通过层式自注意力机制接收NORA的键值对输入，直接回归动作序列。后训练阶段采用DPO，使用组合奖励信号（包括基于世界模型的目标奖励和基于真实动作的偏差奖励）对动作序列进行排序，构建偏好数据集。世界模型V-JEPA-2-AC用于预测动作序列导致的未来状态，与目标状态比较生成奖励；偏差奖励则测量生成动作与真实动作的距离。\n\n论文使用数据集和训练资源\n数据集包括Open X-Embodiment（OXE）大规模跨具身数据集用于预训练，SimplerEnv和LIBERO用于模拟评估，以及使用Galaxea A1机械臂收集的1,000个远程操作抓取任务数据用于真实世界评估。训练在计算资源上进行，但具体硬件细节未提及。\n\n论文使用的评估环境和评估指标\n评估环境包括模拟环境SimplerEnv和LIBERO，以及真实世界的Galaxea A1机械臂。评估指标包括任务成功率（二进制成功率）、部分成功率（如正确抓取物体）、以及干扰物误抓率。在LIBERO中，任务分为空间、物体、目标和长时程四个子集，每个子集评估500个回合，结果取三次运行的平均值。SimplerEnv评估则包括视觉匹配和变体聚合两种协议，总计超过1,000个回合。真实机器人评估报告完整成功率、部分成功率和干扰物误抓率。",
    "summary_html": "<p>论文研究单位</p>\n<p>南洋理工大学、Lambda Labs、新加坡科技设计大学</p>\n\n<p>论文概述</p>\n<p>该论文提出了NORA-1.5，一个视觉-语言-动作（VLA）模型，通过在预训练的NORA模型基础上增加一个基于流匹配的动作专家模块来构建。该方法不仅通过架构改进提升了性能，还引入了基于世界模型和动作偏好的奖励机制，结合直接偏好优化（DPO）进行后训练，显著提高了模型在模拟和真实机器人任务中的可靠性和泛化能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出NORA-1.5模型，通过集成流匹配动作专家与自回归VLA骨干网络，在多个基准测试中实现最先进性能。</li><li>设计了多策略奖励机制，包括世界模型引导的目标奖励、基于真实动作的偏差奖励和子目标奖励，用于构建偏好数据集。</li><li>详细分析了流匹配专家与自回归VLA骨干的协同效应，揭示了数据依赖性行为。</li><li>验证了轻量级奖励模型与DPO结合在模拟和真实机器人环境中的一致性提升，为VLA模型提供了可扩展的后训练方向。</li></ol>\n\n<p>论文方法描述</p>\n<p>NORA-1.5由两部分组成：预训练的NORA模型作为视觉-语言编码器，以及一个流匹配的动作专家模块。动作专家通过层式自注意力机制接收NORA的键值对输入，直接回归动作序列。后训练阶段采用DPO，使用组合奖励信号（包括基于世界模型的目标奖励和基于真实动作的偏差奖励）对动作序列进行排序，构建偏好数据集。世界模型V-JEPA-2-AC用于预测动作序列导致的未来状态，与目标状态比较生成奖励；偏差奖励则测量生成动作与真实动作的距离。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集包括Open X-Embodiment（OXE）大规模跨具身数据集用于预训练，SimplerEnv和LIBERO用于模拟评估，以及使用Galaxea A1机械臂收集的1,000个远程操作抓取任务数据用于真实世界评估。训练在计算资源上进行，但具体硬件细节未提及。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境包括模拟环境SimplerEnv和LIBERO，以及真实世界的Galaxea A1机械臂。评估指标包括任务成功率（二进制成功率）、部分成功率（如正确抓取物体）、以及干扰物误抓率。在LIBERO中，任务分为空间、物体、目标和长时程四个子集，每个子集评估500个回合，结果取三次运行的平均值。SimplerEnv评估则包括视觉匹配和变体聚合两种协议，总计超过1,000个回合。真实机器人评估报告完整成功率、部分成功率和干扰物误抓率。</p>"
  },
  {
    "date": "2025-11-18",
    "title": "Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM",
    "link": "http://arxiv.org/abs/2511.14499",
    "summary_markdown": "论文研究单位\n清华大学，华为2012实验室\n\n论文概述\n论文提出了一种名为风险语义蒸馏的新框架，旨在解决端到端自动驾驶系统在处理长尾场景时的泛化问题。当前利用视觉语言模型的方法存在系统不一致或计算成本过高的挑战。RSD框架通过将视觉语言模型的风险语义知识蒸馏到一个轻量级的端到端自动驾驶模型中，增强了模型对高风险对象的感知能力，从而提升了在复杂和不可预测驾驶条件下的性能。\n\n论文核心贡献点\n提出了一种专为基于VLM的自动驾驶场景风险检测而设计的流程，实现了无需模型微调的零样本能力。\n开发了一种专门的蒸馏架构，有效地将风险感知知识从大型VLM转移到紧凑的端到端驾驶模型中，同时保持了计算效率。\n该方法在规划稳定性和感知准确性方面相比传统方法展现出可衡量的改进。\n\n论文方法描述\nRSD框架包含两个主要部分：VLM增强的风险语义标注和风险语义蒸馏。\n1. VLM增强的风险语义标注：首先使用OV-DINO视觉定位模型获取图像中物体的类别和边界框。然后，通过设计定制化的提示和风险级别的思维链，利用预训练的Qwen VLM为零样本关键风险对象进行定位和评分，生成分级和分数。\n2. 风险语义蒸馏：在端到端模型中引入一个名为RiskHead的即插即用模块。该模块通过BEV重分批处理、最近邻匹配和可变形注意力机制，将VLM输出的风险语义监督信号与BEV特征进行对齐。具体来说，它将BEV查询投影到透视图，通过可变形注意力提取风险感知信息，并使用L1损失函数来对齐预测的风险分数与VLM生成的伪标签。\n\n论文使用数据集和训练资源\n数据集：Bench2Drive，一个包含44种长尾场景的大规模自动驾驶基准测试集，配有6个环绕摄像头和丰富的标注信息。\n训练资源：模型使用AdamW优化器，学习率为2e-4，并采用余弦退火学习率调度。BEV特征图分辨率为100x100。模型总参数量约为50M，是典型VLM-AD架构的1%。闭环实验使用了Bench2Drive数据集的10%作为训练集。\n\n论文使用的评估环境和评估指标\n评估环境：主要在Bench2Drive基准上进行评估。闭环模拟实验在CARLA环境中进行，使用Dev10基准进行验证。\n评估指标：感知任务使用mAP、mATE、mASE、mAOE、mAVE和NDS。规划任务使用平均位移误差（ADE）和碰撞率。闭环评估使用驾驶分数和成功率。",
    "summary_html": "<p>论文研究单位</p>\n<p>清华大学，华为2012实验室</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为风险语义蒸馏的新框架，旨在解决端到端自动驾驶系统在处理长尾场景时的泛化问题。当前利用视觉语言模型的方法存在系统不一致或计算成本过高的挑战。RSD框架通过将视觉语言模型的风险语义知识蒸馏到一个轻量级的端到端自动驾驶模型中，增强了模型对高风险对象的感知能力，从而提升了在复杂和不可预测驾驶条件下的性能。</p>\n\n<p>论文核心贡献点</p>\n<p>提出了一种专为基于VLM的自动驾驶场景风险检测而设计的流程，实现了无需模型微调的零样本能力。</p>\n<p>开发了一种专门的蒸馏架构，有效地将风险感知知识从大型VLM转移到紧凑的端到端驾驶模型中，同时保持了计算效率。</p>\n<p>该方法在规划稳定性和感知准确性方面相比传统方法展现出可衡量的改进。</p>\n\n<p>论文方法描述</p>\n<p>RSD框架包含两个主要部分：VLM增强的风险语义标注和风险语义蒸馏。</p>\n<ol><li>VLM增强的风险语义标注：首先使用OV-DINO视觉定位模型获取图像中物体的类别和边界框。然后，通过设计定制化的提示和风险级别的思维链，利用预训练的Qwen VLM为零样本关键风险对象进行定位和评分，生成分级和分数。</li><li>风险语义蒸馏：在端到端模型中引入一个名为RiskHead的即插即用模块。该模块通过BEV重分批处理、最近邻匹配和可变形注意力机制，将VLM输出的风险语义监督信号与BEV特征进行对齐。具体来说，它将BEV查询投影到透视图，通过可变形注意力提取风险感知信息，并使用L1损失函数来对齐预测的风险分数与VLM生成的伪标签。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：Bench2Drive，一个包含44种长尾场景的大规模自动驾驶基准测试集，配有6个环绕摄像头和丰富的标注信息。</p>\n<p>训练资源：模型使用AdamW优化器，学习率为2e-4，并采用余弦退火学习率调度。BEV特征图分辨率为100x100。模型总参数量约为50M，是典型VLM-AD架构的1%。闭环实验使用了Bench2Drive数据集的10%作为训练集。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：主要在Bench2Drive基准上进行评估。闭环模拟实验在CARLA环境中进行，使用Dev10基准进行验证。</p>\n<p>评估指标：感知任务使用mAP、mATE、mASE、mAOE、mAVE和NDS。规划任务使用平均位移误差（ADE）和碰撞率。闭环评估使用驾驶分数和成功率。</p>"
  },
  {
    "date": "2025-11-18",
    "title": "Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion",
    "link": "http://arxiv.org/abs/2511.14178",
    "summary_markdown": "### 论文研究单位\n香港中文大学，香港物流机器人中心，意大利技术研究院\n### 论文概述\n本文提出了一种名为VLA-Pilot的即插即用推理时策略引导方法，旨在解决预训练的视觉-语言-动作（VLA）模型在下游任务部署时性能下降的问题。传统方法依赖微调，但其成本高昂且不切实际。VLA-Pilot无需任何额外的策略微调或数据收集，即可实现预训练VLA的零样本部署。该方法通过利用多模态大语言模型（MLLM）进行开放式推理以获得引导目标，并采用进化扩散算法优化动作提议，从而提升策略的任务对齐能力。实验在两个不同的机器人实体上的六个真实世界操作任务中验证了该方法的有效性。\n### 论文核心贡献点\n1. 提出VLA-Pilot，一种即插即用的推理时策略引导方法，实现了冻结VLA策略在不同下游任务和机器人实体上的零样本泛化，无需额外的策略微调或数据收集。\n2. 提出一种具身推理引导的进化扩散策略，该策略联合推断广义的引导目标并优化动作提议，以增强任务对齐。\n3. 在六个真实世界操作任务和两种不同机器人实体上进行了广泛实验。结果表明，VLA-Pilot将两个预训练VLA策略的平均成功率提升了31%，显著优于所有基线方法。\n### 论文方法描述\nVLA-Pilot包含三个核心步骤：\n1. **引导目标推理**：利用一个名为“具身策略引导思维链”（EPS-CoT）的模块，通过多模态大语言模型（如GPT-4o）对任务上下文进行结构化推理。该推理过程包括引导目标确认、场景理解、具身增强（利用DINO和SAM等视觉基础模型提取空间关键点）以及生成任务对齐的奖励函数代码，作为黑盒评分函数。\n2. **动作提议优化**：引入一种进化扩散算法。首先从预训练VLA策略中采样一组初始动作提议。然后进行迭代进化搜索，在每次迭代中：使用推理出的奖励函数对动作提议进行评分；根据分数选择精英提议；对精英提议应用截断的扩散-去噪过程（即添加少量噪声后，再利用VLA策略自身的噪声预测器去噪），以在保持动作分布的同时探索和优化任务对齐的行动。\n3. **迭代引导优化**：这是一个闭环修正机制。执行动作后，EPS-CoT模块会根据初始提议、执行的动作、执行后的任务上下文和推理历史进行反思。如果检测到引导失败，MLLM会修正奖励函数并重新启动引导过程，直至任务完成，从而提高引导的准确性和鲁棒性。\n### 论文使用数据集和训练资源\n该方法是推理时方法，本身不涉及模型训练。\n- **数据集**：实验在六个真实世界的下游操作任务上进行，包括四个简单的单臂任务（Mug Handling, Bag Handling, Basket Flipping, Table Bussing）和两个复杂的多臂任务。这些任务分为分布内（ID）和分布外（OOD）两种场景以评估泛化能力。\n- **训练资源**：无需为VLA-Pilot本身进行训练。实验中使用了预训练的VLA模型（DiVLA, RDT-1B）作为基础策略，并使用GPT-4o作为推理时的MLLM。评估使用的物理机器人硬件包括DOBOT X-Trainer双臂系统和Franka Panda机械臂。\n### 论文使用的评估环境和评估指标\n- **评估环境**：真实机器人环境。主要实验平台是DOBOT X-Trainer双臂系统，包含两个6自由度的Nova2机械臂和1自由度夹爪，配备三个英特尔实感摄像头。跨实体泛化实验在Franka Panda机械臂上进行。\n- **评估指标**：\n 1. **操作成功率**：在策略引导后，成功执行下游操作的机器人动作所占的比例。\n 2. **引导目标一致性**：被选中的动作提议中，与预期引导目标对齐的比例。\n 每个方法和任务场景进行20次试验，报告平均性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>香港中文大学，香港物流机器人中心，意大利技术研究院</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为VLA-Pilot的即插即用推理时策略引导方法，旨在解决预训练的视觉-语言-动作（VLA）模型在下游任务部署时性能下降的问题。传统方法依赖微调，但其成本高昂且不切实际。VLA-Pilot无需任何额外的策略微调或数据收集，即可实现预训练VLA的零样本部署。该方法通过利用多模态大语言模型（MLLM）进行开放式推理以获得引导目标，并采用进化扩散算法优化动作提议，从而提升策略的任务对齐能力。实验在两个不同的机器人实体上的六个真实世界操作任务中验证了该方法的有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出VLA-Pilot，一种即插即用的推理时策略引导方法，实现了冻结VLA策略在不同下游任务和机器人实体上的零样本泛化，无需额外的策略微调或数据收集。</li><li>提出一种具身推理引导的进化扩散策略，该策略联合推断广义的引导目标并优化动作提议，以增强任务对齐。</li><li>在六个真实世界操作任务和两种不同机器人实体上进行了广泛实验。结果表明，VLA-Pilot将两个预训练VLA策略的平均成功率提升了31%，显著优于所有基线方法。</li></ol>\n<h3>论文方法描述</h3>\n<p>VLA-Pilot包含三个核心步骤：</p>\n<ol><li><strong>引导目标推理</strong>：利用一个名为“具身策略引导思维链”（EPS-CoT）的模块，通过多模态大语言模型（如GPT-4o）对任务上下文进行结构化推理。该推理过程包括引导目标确认、场景理解、具身增强（利用DINO和SAM等视觉基础模型提取空间关键点）以及生成任务对齐的奖励函数代码，作为黑盒评分函数。</li><li><strong>动作提议优化</strong>：引入一种进化扩散算法。首先从预训练VLA策略中采样一组初始动作提议。然后进行迭代进化搜索，在每次迭代中：使用推理出的奖励函数对动作提议进行评分；根据分数选择精英提议；对精英提议应用截断的扩散-去噪过程（即添加少量噪声后，再利用VLA策略自身的噪声预测器去噪），以在保持动作分布的同时探索和优化任务对齐的行动。</li><li><strong>迭代引导优化</strong>：这是一个闭环修正机制。执行动作后，EPS-CoT模块会根据初始提议、执行的动作、执行后的任务上下文和推理历史进行反思。如果检测到引导失败，MLLM会修正奖励函数并重新启动引导过程，直至任务完成，从而提高引导的准确性和鲁棒性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>该方法是推理时方法，本身不涉及模型训练。</p>\n<ul><li><strong>数据集</strong>：实验在六个真实世界的下游操作任务上进行，包括四个简单的单臂任务（Mug Handling, Bag Handling, Basket Flipping, Table Bussing）和两个复杂的多臂任务。这些任务分为分布内（ID）和分布外（OOD）两种场景以评估泛化能力。</li><li><strong>训练资源</strong>：无需为VLA-Pilot本身进行训练。实验中使用了预训练的VLA模型（DiVLA, RDT-1B）作为基础策略，并使用GPT-4o作为推理时的MLLM。评估使用的物理机器人硬件包括DOBOT X-Trainer双臂系统和Franka Panda机械臂。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：真实机器人环境。主要实验平台是DOBOT X-Trainer双臂系统，包含两个6自由度的Nova2机械臂和1自由度夹爪，配备三个英特尔实感摄像头。跨实体泛化实验在Franka Panda机械臂上进行。</li><li><strong>评估指标</strong>：</li></ul>\n<p> 1. <strong>操作成功率</strong>：在策略引导后，成功执行下游操作的机器人动作所占的比例。</p>\n<p> 2. <strong>引导目标一致性</strong>：被选中的动作提议中，与预期引导目标对齐的比例。</p>\n<p> 每个方法和任务场景进行20次试验，报告平均性能。</p>"
  },
  {
    "date": "2025-11-18",
    "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
    "link": "http://arxiv.org/abs/2511.14161",
    "summary_markdown": "### 论文研究单位\n\n华中科技大学, 香港大学, INFIFORCE Intelligent Technology Co., Ltd., 浙江大学, 深圳光明实验室.\n### 论文概述\n\n论文提出了 RoboTidy，一个用于语言引导家庭整理任务的统一基准。该基准旨在解决现有基准在建模用户偏好、支持移动性和泛化能力方面的不足。RoboTidy 支持 Vision-Language-Action (VLA) 和 Vision-Language-Navigation (VLN) 模型的训练与评估，通过提供基于 3D Gaussian Splatting (3DGS) 的高保真家庭场景、物理真实的仿真环境以及大规模的演示轨迹，为家庭整理任务提供了一个全面且可扩展的评估平台，并用于验证仿真到现实的迁移能力。\n### 论文核心贡献点\n\n- 提出了一种“Action (Object, Container) list”的对象整理方法，利用 Qwen2.5-VL 模型从观察中自动推断放置规则，并决定具体动作。\n- 构建了 500 个基于 3D Gaussian Splatting 的照片级真实感家庭场景，覆盖 500 种物体和容器，并提供了用于训练的 6.4k 操作演示轨迹和 1.5k 导航轨迹。\n- 提出了一个统一的 RoboTidy 基准，支持对 VLA 和 VLN 方法进行模块化评估，并通过真实世界的移动双臂整理实验验证了仿真到现实的迁移。\n### 论文方法描述\n\nRoboTidy 采用模块化框架在 NVIDIA Isaac Sim 中构建。系统首先通过多视角传感器获取场景信息，使用 Qwen2.5-VL 模型解析物体和容器的类别与属性，并生成一个结构化的“Action (Object, Container)”列表。该列表包含四种基本操作：Pick and Place（抓取并放置）、Pick and Toss（抓取并投掷）、Open Container（打开容器）和 Close Container（关闭容器）。这些操作根据属性、功能、安全性和卫生四个标准来确定优先级。导航模块使用 A* 规划器在 2D 语义地图上生成路径，并通过 PID 控制器进行轨迹跟踪。操作模块则结合逆向运动学（IK）求解器和运动规划器来生成无碰撞的机器人操作轨迹。整个系统支持多模态传感器数据（RGB-D 和 LiDAR）的采集，用于训练和评估。\n### 论文使用数据集和训练资源\n\n论文构建了 RoboTidy 数据集，其中包含：\n- 500 个基于 InteriorGS 构建的 3DGS 家庭场景，这些场景为 3DGS-网格混合结构，以支持高保真渲染和物理碰撞。\n- 涵盖 500 种日常物体和容器的 3D 资产库。\n- 6.4k 条针对四种操作的高质量操作演示轨迹。\n- 1.5k 条跨房间导航轨迹。\n- 400 条真实世界的操作演示数据。\n训练与仿真在 NVIDIA Isaac Sim 5.0 环境中进行。\n### 论文使用的评估环境和评估指标\n\n评估在仿真环境 NVIDIA Isaac Sim 5.0 中进行，并通过真实世界的移动机器人部署了仿真到现实的迁移实验。评估指标包括：\n- Object Placement Accuracy (OPA)：衡量将物体放置到正确容器的准确率，计算公式为正确放置的物体数除以总物体数。\n- Valid Sorting Success Rate (VSSR)：衡量物体被放置到有效容器且所需操作成功完成的比率，结合了容器选择的正确性和操作执行的成败。\n- Navigation Success Rate：评估 VLN 模型导航任务的成功率，并与 R2R 等其他基准进行了对比。",
    "summary_html": "<h3>论文研究单位</h3>\n\n<p>华中科技大学, 香港大学, INFIFORCE Intelligent Technology Co., Ltd., 浙江大学, 深圳光明实验室.</p>\n<h3>论文概述</h3>\n\n<p>论文提出了 RoboTidy，一个用于语言引导家庭整理任务的统一基准。该基准旨在解决现有基准在建模用户偏好、支持移动性和泛化能力方面的不足。RoboTidy 支持 Vision-Language-Action (VLA) 和 Vision-Language-Navigation (VLN) 模型的训练与评估，通过提供基于 3D Gaussian Splatting (3DGS) 的高保真家庭场景、物理真实的仿真环境以及大规模的演示轨迹，为家庭整理任务提供了一个全面且可扩展的评估平台，并用于验证仿真到现实的迁移能力。</p>\n<h3>论文核心贡献点</h3>\n\n<ul><li>提出了一种“Action (Object, Container) list”的对象整理方法，利用 Qwen2.5-VL 模型从观察中自动推断放置规则，并决定具体动作。</li><li>构建了 500 个基于 3D Gaussian Splatting 的照片级真实感家庭场景，覆盖 500 种物体和容器，并提供了用于训练的 6.4k 操作演示轨迹和 1.5k 导航轨迹。</li><li>提出了一个统一的 RoboTidy 基准，支持对 VLA 和 VLN 方法进行模块化评估，并通过真实世界的移动双臂整理实验验证了仿真到现实的迁移。</li></ul>\n<h3>论文方法描述</h3>\n\n<p>RoboTidy 采用模块化框架在 NVIDIA Isaac Sim 中构建。系统首先通过多视角传感器获取场景信息，使用 Qwen2.5-VL 模型解析物体和容器的类别与属性，并生成一个结构化的“Action (Object, Container)”列表。该列表包含四种基本操作：Pick and Place（抓取并放置）、Pick and Toss（抓取并投掷）、Open Container（打开容器）和 Close Container（关闭容器）。这些操作根据属性、功能、安全性和卫生四个标准来确定优先级。导航模块使用 A* 规划器在 2D 语义地图上生成路径，并通过 PID 控制器进行轨迹跟踪。操作模块则结合逆向运动学（IK）求解器和运动规划器来生成无碰撞的机器人操作轨迹。整个系统支持多模态传感器数据（RGB-D 和 LiDAR）的采集，用于训练和评估。</p>\n<h3>论文使用数据集和训练资源</h3>\n\n<p>论文构建了 RoboTidy 数据集，其中包含：</p>\n<ul><li>500 个基于 InteriorGS 构建的 3DGS 家庭场景，这些场景为 3DGS-网格混合结构，以支持高保真渲染和物理碰撞。</li><li>涵盖 500 种日常物体和容器的 3D 资产库。</li><li>6.4k 条针对四种操作的高质量操作演示轨迹。</li><li>1.5k 条跨房间导航轨迹。</li><li>400 条真实世界的操作演示数据。</li></ul>\n<p>训练与仿真在 NVIDIA Isaac Sim 5.0 环境中进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n\n<p>评估在仿真环境 NVIDIA Isaac Sim 5.0 中进行，并通过真实世界的移动机器人部署了仿真到现实的迁移实验。评估指标包括：</p>\n<ul><li>Object Placement Accuracy (OPA)：衡量将物体放置到正确容器的准确率，计算公式为正确放置的物体数除以总物体数。</li><li>Valid Sorting Success Rate (VSSR)：衡量物体被放置到有效容器且所需操作成功完成的比率，结合了容器选择的正确性和操作执行的成败。</li><li>Navigation Success Rate：评估 VLN 模型导航任务的成功率，并与 R2R 等其他基准进行了对比。</li></ul>"
  },
  {
    "date": "2025-11-18",
    "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.14148",
    "summary_markdown": "### 论文研究单位\n上海人工智能实验室、清华大学、浙江大学、Lumos Robotics\n### 论文概述\n本文提出了AsyncVLA，一种新型的视觉-语言-动作（VLA）模型框架。该框架通过引入异步流匹配（AFM）和置信度驱动的自校正机制，解决了传统同步流匹配（SFM）在长视野任务中因刚性时间表而导致的错误累积问题。AsyncVLA首先使用SFM生成初始动作，然后通过置信度评估器筛选低置信度的动作进行异步再生，从而利用动作上下文信息实现选择性自校正，提升了模型的鲁棒性和任务成功率。\n### 论文核心贡献点\n1. 提出了AsyncVLA框架，首次将异步流匹配（AFM）引入VLA模型，突破了SFM的刚性同步时间表限制。\n2. 设计了置信度评估器（Confidence Rater），用于评估SFM生成的初始动作的置信度，并根据置信度动态选择需要再生成的动作标记。\n3. 实现了SFM和AFM的统一训练流程，使单一模型同时具备两种模式，并复用VL KV-cache以提高推理效率。\n4. 通过多个机器人操作基准测试验证了模型的数据高效性和自校正能力，在通用具身评估中实现了最先进的性能。\n### 论文方法描述\nAsyncVLA包含三个 sequential 部分：同步流匹配（SFM）、置信度评估器（Confidence Rater）和异步流匹配（AFM）。\n1. **SFM**：使用统一时间表从噪声同步生成所有动作标记。\n2. **置信度评估器**：由Transformer层和线性层组成，接收VL标记和SFM生成的动作作为输入，输出每个动作标记的置信度分数。通过比较置信度阈值生成二进制掩码，标记低置信度动作。\n3. **AFM**：根据掩码动态分配时间步，仅对掩码标记进行再生，未掩码标记保持不变。SFM生成的动作作为上下文信息，辅助修正低置信度动作。\n4. **统一训练**：将SFM视为全掩码AFM的特例，通过最小化掩码标记上的AFM速度预测损失，联合训练VLA主干和FM动作头，确保模型同时支持SFM和AFM推理。\n### 论文使用数据集和训练资源\n1. **数据集**：Open X-Embodiment（预训练），LIBERO、Bridge-V2、Fractal（微调）。\n2. **训练资源**：预训练使用32个H200 GPU，微调使用8个H200 GPU。预训练约2.5天，微调需15-32小时。全局批次大小为2048，使用BF16精度、ZeRO-2优化器分片、Flash-Attention-2。优化器为AdamW，权重衰减为0，β1=0.9，β2=0.999。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：LIBERO（模拟）、WidowX机器人（SimplerEnv模拟）、Google机器人（SimplerEnv模拟）。\n2. **评估指标**：任务成功率（Success Rate），即任务完成的成功率百分比，每个任务套件测试500次试验（10任务×50片段）。在Google机器人基准上还分为视觉匹配（M）和变体聚合（A）两种协议。</think>\n### 论文研究单位\n上海人工智能实验室、清华大学、浙江大学、Lumos Robotics\n### 论文概述\n本文提出了AsyncVLA，一种新型的视觉-语言-动作（VLA）模型框架。该框架通过引入异步流匹配（AFM）和置信度驱动的自校正机制，解决了传统同步流匹配（SFM）在长视野任务中因刚性时间表而导致的错误累积问题。AsyncVLA首先使用SFM生成初始动作，然后通过置信度评估器筛选低置信度的动作进行异步再生，从而利用动作上下文信息实现选择性自校正，提升了模型的鲁棒性和任务成功率。\n### 论文核心贡献点\n1. 提出了AsyncVLA框架，首次将异步流匹配（AFM）引入VLA模型，突破了SFM的刚性同步时间表限制。\n2. 设计了置信度评估器（Confidence Rater），用于评估SFM生成的初始动作的置信度，并根据置信度动态选择需要再生成的动作标记。\n3. 实现了SFM和AFM的统一训练流程，使单一模型同时具备两种模式，并复用VL KV-cache以提高推理效率。\n4. 通过多个机器人操作基准测试验证了模型的数据高效性和自校正能力，在通用具身评估中实现了最先进的性能。\n### 论文方法描述\nAsyncVLA包含三个 sequential 部分：同步流匹配（SFM）、置信度评估器（Confidence Rater）和异步流匹配（AFM）。\n1. **SFM**：使用统一时间表从噪声同步生成所有动作标记。\n2. **置信度评估器**：由Transformer层和线性层组成，接收VL标记和SFM生成的动作作为输入，输出每个动作标记的置信度分数。通过比较置信度阈值生成二进制掩码，标记低置信度动作。\n3. **AFM**：根据掩码动态分配时间步，仅对掩码标记进行再生，未掩码标记保持不变。SFM生成的动作作为上下文信息，辅助修正低置信度动作。\n4. **统一训练**：将SFM视为全掩码AFM的特例，通过最小化掩码标记上的AFM速度预测损失，联合训练VLA主干和FM动作头，确保模型同时支持SFM和AFM推理。\n### 论文使用数据集和训练资源\n1. **数据集**：Open X-Embodiment（预训练），LIBERO、Bridge-V2、Fractal（微调）。\n2. **训练资源**：预训练使用32个H200 GPU，微调使用8个H200 GPU。预训练约2.5天，微调需15-32小时。全局批次大小为2048，使用BF16精度、ZeRO-2优化器分片、Flash-Attention-2。优化器为AdamW，权重衰减为0，β1=0.9，β2=0.999。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：LIBERO（模拟）、WidowX机器人（SimplerEnv模拟）、Google机器人（SimplerEnv模拟）。\n2. **评估指标**：任务成功率（Success Rate），即任务完成的成功率百分比，每个任务套件测试500次试验（10任务×50片段）。在Google机器人基准上还分为视觉匹配（M）和变体聚合（A）两种协议。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海人工智能实验室、清华大学、浙江大学、Lumos Robotics</p>\n<h3>论文概述</h3>\n<p>本文提出了AsyncVLA，一种新型的视觉-语言-动作（VLA）模型框架。该框架通过引入异步流匹配（AFM）和置信度驱动的自校正机制，解决了传统同步流匹配（SFM）在长视野任务中因刚性时间表而导致的错误累积问题。AsyncVLA首先使用SFM生成初始动作，然后通过置信度评估器筛选低置信度的动作进行异步再生，从而利用动作上下文信息实现选择性自校正，提升了模型的鲁棒性和任务成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了AsyncVLA框架，首次将异步流匹配（AFM）引入VLA模型，突破了SFM的刚性同步时间表限制。</li><li>设计了置信度评估器（Confidence Rater），用于评估SFM生成的初始动作的置信度，并根据置信度动态选择需要再生成的动作标记。</li><li>实现了SFM和AFM的统一训练流程，使单一模型同时具备两种模式，并复用VL KV-cache以提高推理效率。</li><li>通过多个机器人操作基准测试验证了模型的数据高效性和自校正能力，在通用具身评估中实现了最先进的性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>AsyncVLA包含三个 sequential 部分：同步流匹配（SFM）、置信度评估器（Confidence Rater）和异步流匹配（AFM）。</p>\n<ol><li><strong>SFM</strong>：使用统一时间表从噪声同步生成所有动作标记。</li><li><strong>置信度评估器</strong>：由Transformer层和线性层组成，接收VL标记和SFM生成的动作作为输入，输出每个动作标记的置信度分数。通过比较置信度阈值生成二进制掩码，标记低置信度动作。</li><li><strong>AFM</strong>：根据掩码动态分配时间步，仅对掩码标记进行再生，未掩码标记保持不变。SFM生成的动作作为上下文信息，辅助修正低置信度动作。</li><li><strong>统一训练</strong>：将SFM视为全掩码AFM的特例，通过最小化掩码标记上的AFM速度预测损失，联合训练VLA主干和FM动作头，确保模型同时支持SFM和AFM推理。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：Open X-Embodiment（预训练），LIBERO、Bridge-V2、Fractal（微调）。</li><li><strong>训练资源</strong>：预训练使用32个H200 GPU，微调使用8个H200 GPU。预训练约2.5天，微调需15-32小时。全局批次大小为2048，使用BF16精度、ZeRO-2优化器分片、Flash-Attention-2。优化器为AdamW，权重衰减为0，β1=0.9，β2=0.999。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：LIBERO（模拟）、WidowX机器人（SimplerEnv模拟）、Google机器人（SimplerEnv模拟）。</li><li><strong>评估指标</strong>：任务成功率（Success Rate），即任务完成的成功率百分比，每个任务套件测试500次试验（10任务×50片段）。在Google机器人基准上还分为视觉匹配（M）和变体聚合（A）两种协议。</think></li></ol>\n<h3>论文研究单位</h3>\n<p>上海人工智能实验室、清华大学、浙江大学、Lumos Robotics</p>\n<h3>论文概述</h3>\n<p>本文提出了AsyncVLA，一种新型的视觉-语言-动作（VLA）模型框架。该框架通过引入异步流匹配（AFM）和置信度驱动的自校正机制，解决了传统同步流匹配（SFM）在长视野任务中因刚性时间表而导致的错误累积问题。AsyncVLA首先使用SFM生成初始动作，然后通过置信度评估器筛选低置信度的动作进行异步再生，从而利用动作上下文信息实现选择性自校正，提升了模型的鲁棒性和任务成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了AsyncVLA框架，首次将异步流匹配（AFM）引入VLA模型，突破了SFM的刚性同步时间表限制。</li><li>设计了置信度评估器（Confidence Rater），用于评估SFM生成的初始动作的置信度，并根据置信度动态选择需要再生成的动作标记。</li><li>实现了SFM和AFM的统一训练流程，使单一模型同时具备两种模式，并复用VL KV-cache以提高推理效率。</li><li>通过多个机器人操作基准测试验证了模型的数据高效性和自校正能力，在通用具身评估中实现了最先进的性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>AsyncVLA包含三个 sequential 部分：同步流匹配（SFM）、置信度评估器（Confidence Rater）和异步流匹配（AFM）。</p>\n<ol><li><strong>SFM</strong>：使用统一时间表从噪声同步生成所有动作标记。</li><li><strong>置信度评估器</strong>：由Transformer层和线性层组成，接收VL标记和SFM生成的动作作为输入，输出每个动作标记的置信度分数。通过比较置信度阈值生成二进制掩码，标记低置信度动作。</li><li><strong>AFM</strong>：根据掩码动态分配时间步，仅对掩码标记进行再生，未掩码标记保持不变。SFM生成的动作作为上下文信息，辅助修正低置信度动作。</li><li><strong>统一训练</strong>：将SFM视为全掩码AFM的特例，通过最小化掩码标记上的AFM速度预测损失，联合训练VLA主干和FM动作头，确保模型同时支持SFM和AFM推理。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：Open X-Embodiment（预训练），LIBERO、Bridge-V2、Fractal（微调）。</li><li><strong>训练资源</strong>：预训练使用32个H200 GPU，微调使用8个H200 GPU。预训练约2.5天，微调需15-32小时。全局批次大小为2048，使用BF16精度、ZeRO-2优化器分片、Flash-Attention-2。优化器为AdamW，权重衰减为0，β1=0.9，β2=0.999。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：LIBERO（模拟）、WidowX机器人（SimplerEnv模拟）、Google机器人（SimplerEnv模拟）。</li><li><strong>评估指标</strong>：任务成功率（Success Rate），即任务完成的成功率百分比，每个任务套件测试500次试验（10任务×50片段）。在Google机器人基准上还分为视觉匹配（M）和变体聚合（A）两种协议。</li></ol>"
  },
  {
    "date": "2025-11-16",
    "title": "VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2511.12405",
    "summary_markdown": "论文研究单位\n韩国科学技术院 (KAIST) 电气工程学院。\n\n论文概述\n该论文提出了一个名为视觉-语言动作检索（VLA-R）的开放式端到端自动驾驶（OW-E2EAD）框架。该框架旨在解决在非结构化的户外环境中，模型在训练时未遇到过的开放式场景下的泛化挑战。VLA-R通过整合开放式世界感知和一种新颖的视觉-动作检索范式，利用冻结的视觉-语言模型获取开放式世界的检测和分割特征，并通过一个Q-Former瓶颈来聚合视觉表征，最后通过视觉-动作对比学习来对齐视觉-语言和动作嵌入，从而实现有效的动作检索。\n\n论文核心贡献点\n1. 提出了一种名为视觉-语言动作检索（VLA-R）的开放式端到端自动驾驶框架。\n2. 利用冻结的视觉-语言模型实现无需领域特定微调的开放式世界感知。\n3. 设计了一个Q-Former瓶颈，用于聚合细粒度的视觉表征并与语言对齐。\n4. 引入了视觉-动作对比学习方案，将视觉-语言嵌入与对应的驾驶动作对齐，以实现有效的动作检索。\n5. 在真实世界的机器人平台上，即使在数据有限的情况下，该方法也展现出了强大的泛化和探索能力。\n\n论文方法描述\n1. **视觉-语言动作检索范式**：将自动驾驶任务建模为从共享的视觉-语言-动作嵌入空间中检索最合适的动作轨迹，而非直接回归或分类。\n2. **开放式查询变换器 (OW-QFormer)**：使用冻结的YOLOE模型提取多源感知特征（包括视觉特征、文本对齐特征和边界框几何先验），并通过一个可学习的查询机制进行聚合，生成与语言对齐的视觉嵌入。\n3. **动作变换器**：将连续的动作轨迹离散化为三维空间中的动作标记，并使用一个Transformer编码器将这些标记编码为动作嵌入。\n4. **视觉-动作对比学习**：采用InfoNCE损失函数，将视觉-语言嵌入与动作嵌入在共享的潜在空间中进行对齐，使得在推理时可以根据相似度检索出最匹配的动作。\n\n论文使用数据集和训练资源\n**数据集**：通过手动控制采集，包含36,582个图像-动作对（约2小时的驾驶数据）。\n**训练资源**：使用单块RTX 5000 Ada (32 GB) GPU进行训练，耗时不到4小时，训练了20个周期。\n\n论文使用的评估环境和评估指标\n**评估环境**：在真实世界的室外非结构化环境中进行闭环评估，使用Clearpath Jackal差分驱动机器人平台。测试场景包括崎岖地形、悬崖、死胡同和茂密树林等。\n**评估指标**：主要指标是“事件数量”，反映机器人在探索中遇到的障碍物次数；以及“成功率”，衡量整个实验过程中避免碰撞的整体成功率。",
    "summary_html": "<p>论文研究单位</p>\n<p>韩国科学技术院 (KAIST) 电气工程学院。</p>\n\n<p>论文概述</p>\n<p>该论文提出了一个名为视觉-语言动作检索（VLA-R）的开放式端到端自动驾驶（OW-E2EAD）框架。该框架旨在解决在非结构化的户外环境中，模型在训练时未遇到过的开放式场景下的泛化挑战。VLA-R通过整合开放式世界感知和一种新颖的视觉-动作检索范式，利用冻结的视觉-语言模型获取开放式世界的检测和分割特征，并通过一个Q-Former瓶颈来聚合视觉表征，最后通过视觉-动作对比学习来对齐视觉-语言和动作嵌入，从而实现有效的动作检索。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种名为视觉-语言动作检索（VLA-R）的开放式端到端自动驾驶框架。</li><li>利用冻结的视觉-语言模型实现无需领域特定微调的开放式世界感知。</li><li>设计了一个Q-Former瓶颈，用于聚合细粒度的视觉表征并与语言对齐。</li><li>引入了视觉-动作对比学习方案，将视觉-语言嵌入与对应的驾驶动作对齐，以实现有效的动作检索。</li><li>在真实世界的机器人平台上，即使在数据有限的情况下，该方法也展现出了强大的泛化和探索能力。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li><strong>视觉-语言动作检索范式</strong>：将自动驾驶任务建模为从共享的视觉-语言-动作嵌入空间中检索最合适的动作轨迹，而非直接回归或分类。</li><li><strong>开放式查询变换器 (OW-QFormer)</strong>：使用冻结的YOLOE模型提取多源感知特征（包括视觉特征、文本对齐特征和边界框几何先验），并通过一个可学习的查询机制进行聚合，生成与语言对齐的视觉嵌入。</li><li><strong>动作变换器</strong>：将连续的动作轨迹离散化为三维空间中的动作标记，并使用一个Transformer编码器将这些标记编码为动作嵌入。</li><li><strong>视觉-动作对比学习</strong>：采用InfoNCE损失函数，将视觉-语言嵌入与动作嵌入在共享的潜在空间中进行对齐，使得在推理时可以根据相似度检索出最匹配的动作。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p><strong>数据集</strong>：通过手动控制采集，包含36,582个图像-动作对（约2小时的驾驶数据）。</p>\n<p><strong>训练资源</strong>：使用单块RTX 5000 Ada (32 GB) GPU进行训练，耗时不到4小时，训练了20个周期。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p><strong>评估环境</strong>：在真实世界的室外非结构化环境中进行闭环评估，使用Clearpath Jackal差分驱动机器人平台。测试场景包括崎岖地形、悬崖、死胡同和茂密树林等。</p>\n<p><strong>评估指标</strong>：主要指标是“事件数量”，反映机器人在探索中遇到的障碍物次数；以及“成功率”，衡量整个实验过程中避免碰撞的整体成功率。</p>"
  },
  {
    "date": "2025-11-14",
    "title": "Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective",
    "link": "http://arxiv.org/abs/2511.11478",
    "summary_markdown": "论文研究单位\n论文作者来自多个机构，包括密歇根州立大学、丰田研究院、艾伦人工智能研究所、约翰霍普金斯大学、德克萨斯大学奥斯汀分校、日本丰田中央研发实验室、缅因大学、丰田北美研究院和伍斯特理工学院。\n\n论文概述\n该论文重新审视了机器人操控中的记忆状态演进问题，并从以物体为中心的视角提出了一个新方法。论文指出现有的视觉-语言-动作（VLA）模型在处理非马尔可夫场景时存在局限性，这些场景需要记忆过去的交互历史。为了解决这个问题，论文提出了LIBERO-Mem基准测试和Embodied-SlotSSM框架。LIBERO-Mem是一个专门设计用于评估机器人在物体级别部分可观测环境下记忆能力的任务套件，而Embodied-SlotSSM则是一个基于槽位的VLA框架，通过维持时空一致的物体槽位表示，实现了结构化的记忆和时序推理能力。\n\n论文核心贡献点\n1. 提出了LIBERO-Mem，一个新的非马尔可夫机器人操控基准，系统评估记忆增强模型在长视界任务中的表现，强调物体恒常性、历史推理和结构化记忆保留。\n2. 提出了Embodied-SlotSSM，一个基于槽位的状态空间建模框架，通过编码持久的、以物体为中心的记忆表示，支持部分可观测环境下的结构化跟踪和决策。\n3. 通过在LIBERO-Goal和LIBERO-Mem上的实验，验证了Embodied-SlotSSM在增强状态推理、长视界动作预测和操控任务性能方面的有效性，尤其是在需要处理物体级别POMDP依赖关系的任务中表现优于现有方法。\n\n论文方法描述\n论文提出的Embodied-SlotSSM方法是一个基于槽位的VLA框架，主要由三个核心组件构成：1) 使用Slot Attention进行物体定位和时序一致性建模，通过对比损失增强槽位在时间上的稳定性；2) 基于槽位的状态空间模型（SlotSSM）对每个物体的动态进行独立建模，预测局部时间窗口内的物体状态变化；3) 通过关系编码器将槽位动态与当前视觉特征结合，使用轻量级交叉注意力机制生成上下文感知的动作预测。该方法利用固定数量的槽位（默认16个）来表示和跟踪场景中的物体，每个槽位通过输入条件化的块对角矩阵进行状态空间建模，实现了可扩展的时序建模和持久记忆。\n\n论文使用数据集和训练资源\n论文使用了两个主要数据集：1) LIBERO-Goal，一个通用的机器人操控基准，包含多种任务用于评估模型的泛化能力；2) LIBERO-Mem，新提出的非马尔可夫基准，包含10个任务，涵盖物体运动、序列、关系和遮挡四种记忆维度，每个任务包含200-700帧的轨迹，共120个轨迹用于训练（100个）和验证（20个）。训练在模拟环境中进行，使用固定的场景布局和随机初始化的物体位姿。论文未明确提及具体计算资源，但说明所有实验在模拟环境中完成，使用了多GPU设置进行模型训练和评估。\n\n论文使用的评估环境和评估指标\n评估在模拟环境中进行，具体使用LIBERO-Goal和LIBERO-Mem基准。评估指标包括：1) 在LIBERO-Goal上使用任务成功率（success rate），定义为成功完成的尝试次数与总尝试次数的比率；2) 在LIBERO-Mem上使用子目标完成率（subgoal completion），定义为完成的子目标数量与总子目标数量的比率，通过多次种子实验（N=20）取平均值。此外，论文还进行了定性分析，通过可视化槽位注意力来验证模型对物体一致性的跟踪能力。实验对比了Embodied-SlotSSM与基线方法（如SlotVLA和π₀）在不同任务上的性能，特别是在长视界和部分可观测场景下的表现。",
    "summary_html": "<p>论文研究单位</p>\n<p>论文作者来自多个机构，包括密歇根州立大学、丰田研究院、艾伦人工智能研究所、约翰霍普金斯大学、德克萨斯大学奥斯汀分校、日本丰田中央研发实验室、缅因大学、丰田北美研究院和伍斯特理工学院。</p>\n\n<p>论文概述</p>\n<p>该论文重新审视了机器人操控中的记忆状态演进问题，并从以物体为中心的视角提出了一个新方法。论文指出现有的视觉-语言-动作（VLA）模型在处理非马尔可夫场景时存在局限性，这些场景需要记忆过去的交互历史。为了解决这个问题，论文提出了LIBERO-Mem基准测试和Embodied-SlotSSM框架。LIBERO-Mem是一个专门设计用于评估机器人在物体级别部分可观测环境下记忆能力的任务套件，而Embodied-SlotSSM则是一个基于槽位的VLA框架，通过维持时空一致的物体槽位表示，实现了结构化的记忆和时序推理能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了LIBERO-Mem，一个新的非马尔可夫机器人操控基准，系统评估记忆增强模型在长视界任务中的表现，强调物体恒常性、历史推理和结构化记忆保留。</li><li>提出了Embodied-SlotSSM，一个基于槽位的状态空间建模框架，通过编码持久的、以物体为中心的记忆表示，支持部分可观测环境下的结构化跟踪和决策。</li><li>通过在LIBERO-Goal和LIBERO-Mem上的实验，验证了Embodied-SlotSSM在增强状态推理、长视界动作预测和操控任务性能方面的有效性，尤其是在需要处理物体级别POMDP依赖关系的任务中表现优于现有方法。</li></ol>\n\n<p>论文方法描述</p>\n<p>论文提出的Embodied-SlotSSM方法是一个基于槽位的VLA框架，主要由三个核心组件构成：1) 使用Slot Attention进行物体定位和时序一致性建模，通过对比损失增强槽位在时间上的稳定性；2) 基于槽位的状态空间模型（SlotSSM）对每个物体的动态进行独立建模，预测局部时间窗口内的物体状态变化；3) 通过关系编码器将槽位动态与当前视觉特征结合，使用轻量级交叉注意力机制生成上下文感知的动作预测。该方法利用固定数量的槽位（默认16个）来表示和跟踪场景中的物体，每个槽位通过输入条件化的块对角矩阵进行状态空间建模，实现了可扩展的时序建模和持久记忆。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>论文使用了两个主要数据集：1) LIBERO-Goal，一个通用的机器人操控基准，包含多种任务用于评估模型的泛化能力；2) LIBERO-Mem，新提出的非马尔可夫基准，包含10个任务，涵盖物体运动、序列、关系和遮挡四种记忆维度，每个任务包含200-700帧的轨迹，共120个轨迹用于训练（100个）和验证（20个）。训练在模拟环境中进行，使用固定的场景布局和随机初始化的物体位姿。论文未明确提及具体计算资源，但说明所有实验在模拟环境中完成，使用了多GPU设置进行模型训练和评估。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估在模拟环境中进行，具体使用LIBERO-Goal和LIBERO-Mem基准。评估指标包括：1) 在LIBERO-Goal上使用任务成功率（success rate），定义为成功完成的尝试次数与总尝试次数的比率；2) 在LIBERO-Mem上使用子目标完成率（subgoal completion），定义为完成的子目标数量与总子目标数量的比率，通过多次种子实验（N=20）取平均值。此外，论文还进行了定性分析，通过可视化槽位注意力来验证模型对物体一致性的跟踪能力。实验对比了Embodied-SlotSSM与基线方法（如SlotVLA和π₀）在不同任务上的性能，特别是在长视界和部分可观测场景下的表现。</p>"
  },
  {
    "date": "2025-11-14",
    "title": "Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.11298",
    "summary_markdown": "```markdown\n### 论文研究单位\nMacquarie University, Sydney, NSW, Australia.\n### 论文概述\n该论文报告了对四种代表性视觉-语言-动作（VLA）模型在机器人操作任务上进行基准测试的经验研究。研究在ALOHA Mobile平台上进行了四个操作任务，结合了真实世界和仿真环境中的评估。论文建立了一个标准化的评估框架，从准确性、效率、适应性（包括分布内和分布外场景）以及语言指令遵循能力三个维度衡量模型性能。研究发现，π₀模型在分布外场景中表现出优越的适应性，而ACT模型在分布内环境中提供了最高的稳定性。此外，研究还识别了常见的失败模式，如抓取偏差、过早释放和长时程状态漂移，并揭示了不同VLA模型架构在精度、泛化和部署成本之间的实际权衡。\n### 论文核心贡献点\n1. **统一基准**：引入了一个双臂操作基准，包含明确定义的分布内和分布外评估设置，并发布了标准化的可复现评估流程，用于公平的跨模型比较。\n2. **失败分类法与诊断分析**：开发了一个结构化的失败分类法，捕获了时间漂移、符号接地失败和执行失误等常见错误模式，并利用它来揭示不同架构模型特有的弱点。\n3. **鲁棒性与数据扩展的经验见解**：通过受控实验，揭示了专业模仿学习策略与通用VLA模型在鲁棒性-精度上的权衡，并表明在长时程、可变形物体任务上，性能会随着有限的演示数据扩展而饱和。\n### 论文方法描述\n论文对四种模型进行了评估：一个专家级模仿学习策略（ACT）和三个通用VLA模型（OpenVLA–OFT, RDT-1B, π₀）。评估在物理和仿真两个环境中进行。物理环境使用ALOHA Mobile平台，该平台配备两个7自由度机械臂、三台Intel RealSense D405 RGB-D相机，并运行在NVIDIA RTX 5090 GPU上，控制频率为25Hz。仿真环境则是一个基于MuJoCo的桌面环境，用于评估零样本的语言理解和空间推理能力。研究设计了四个双臂操作任务，涉及工具使用、可变形物体处理和语言指令理解。评估分为三种条件：分布内、空间分布外和实例+空间分布外，以系统性地测试模型的泛化能力。\n### 论文使用数据集和训练资源\n所有训练数据均通过ALOHA Mobile平台的遥操作收集。对于“Clean Dish”、“Put Sponge into Pot”和“Unzip Bag”任务，各收集了100个演示数据；“Folding Shorts”任务收集了200个演示数据。训练资源方面，ACT和π₀在单块NVIDIA RTX 5090 GPU上进行训练或微调，耗时分别约为2小时和24小时。OpenVLA–OFT和RDT-1B由于需要更大内存，在两块NVIDIA A6000 GPU上进行微调，训练时间均超过2周。\n### 论文使用的评估环境和评估指标\n评估环境包括：\n1. **物理环境**：ALOHA Mobile平台，在一个1米x1米的工作台前操作，使用三台相机提供感知输入，并保持恒定的光照条件。\n2. **仿真环境**：一个基于MuJoCo的桌面环境，包含三个用于测试语言和空间推理任务的彩色立方体。\n\n评估指标包括：\n1. **任务成功率**：成功完成目标的试验百分比。\n2. **成功时间**：仅在成功的试验中，从第一个动作到任务完成所经过的时间。\n3. **指令遵循准确率**：对于语言指定的目标，其目标选择和关系接头的正确性。\n\n为确保公平性，每个（任务、模型、设置）组合进行50次试验，并重用相同的随机初始状态。成功率和指令遵循准确率报告了Wilson-score 95%置信区间。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>Macquarie University, Sydney, NSW, Australia.</p>\n<h3>论文概述</h3>\n<p>该论文报告了对四种代表性视觉-语言-动作（VLA）模型在机器人操作任务上进行基准测试的经验研究。研究在ALOHA Mobile平台上进行了四个操作任务，结合了真实世界和仿真环境中的评估。论文建立了一个标准化的评估框架，从准确性、效率、适应性（包括分布内和分布外场景）以及语言指令遵循能力三个维度衡量模型性能。研究发现，π₀模型在分布外场景中表现出优越的适应性，而ACT模型在分布内环境中提供了最高的稳定性。此外，研究还识别了常见的失败模式，如抓取偏差、过早释放和长时程状态漂移，并揭示了不同VLA模型架构在精度、泛化和部署成本之间的实际权衡。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>统一基准</strong>：引入了一个双臂操作基准，包含明确定义的分布内和分布外评估设置，并发布了标准化的可复现评估流程，用于公平的跨模型比较。</li><li><strong>失败分类法与诊断分析</strong>：开发了一个结构化的失败分类法，捕获了时间漂移、符号接地失败和执行失误等常见错误模式，并利用它来揭示不同架构模型特有的弱点。</li><li><strong>鲁棒性与数据扩展的经验见解</strong>：通过受控实验，揭示了专业模仿学习策略与通用VLA模型在鲁棒性-精度上的权衡，并表明在长时程、可变形物体任务上，性能会随着有限的演示数据扩展而饱和。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文对四种模型进行了评估：一个专家级模仿学习策略（ACT）和三个通用VLA模型（OpenVLA–OFT, RDT-1B, π₀）。评估在物理和仿真两个环境中进行。物理环境使用ALOHA Mobile平台，该平台配备两个7自由度机械臂、三台Intel RealSense D405 RGB-D相机，并运行在NVIDIA RTX 5090 GPU上，控制频率为25Hz。仿真环境则是一个基于MuJoCo的桌面环境，用于评估零样本的语言理解和空间推理能力。研究设计了四个双臂操作任务，涉及工具使用、可变形物体处理和语言指令理解。评估分为三种条件：分布内、空间分布外和实例+空间分布外，以系统性地测试模型的泛化能力。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>所有训练数据均通过ALOHA Mobile平台的遥操作收集。对于“Clean Dish”、“Put Sponge into Pot”和“Unzip Bag”任务，各收集了100个演示数据；“Folding Shorts”任务收集了200个演示数据。训练资源方面，ACT和π₀在单块NVIDIA RTX 5090 GPU上进行训练或微调，耗时分别约为2小时和24小时。OpenVLA–OFT和RDT-1B由于需要更大内存，在两块NVIDIA A6000 GPU上进行微调，训练时间均超过2周。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境包括：</p>\n<ol><li><strong>物理环境</strong>：ALOHA Mobile平台，在一个1米x1米的工作台前操作，使用三台相机提供感知输入，并保持恒定的光照条件。</li><li><strong>仿真环境</strong>：一个基于MuJoCo的桌面环境，包含三个用于测试语言和空间推理任务的彩色立方体。</li></ol>\n\n<p>评估指标包括：</p>\n<ol><li><strong>任务成功率</strong>：成功完成目标的试验百分比。</li><li><strong>成功时间</strong>：仅在成功的试验中，从第一个动作到任务完成所经过的时间。</li><li><strong>指令遵循准确率</strong>：对于语言指定的目标，其目标选择和关系接头的正确性。</li></ol>\n\n<p>为确保公平性，每个（任务、模型、设置）组合进行50次试验，并重用相同的随机初始状态。成功率和指令遵循准确率报告了Wilson-score 95%置信区间。</p>\n<p>```</p>"
  },
  {
    "date": "2025-11-13",
    "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded",
    "link": "http://arxiv.org/abs/2511.10560",
    "summary_markdown": "## 论文研究单位\n香港科技大学(HKUST)、南洋理工大学(NTU)、中山大学(SYSU)、新加坡国立大学(NUS)、阿里巴巴集团\n## 论文概述\n论文提出了OmniVGGT框架，这是一个能够有效利用任意数量辅助几何模态（如深度图、相机内参和外参）进行训练和推理的视觉基础模型。该框架通过轻量级的GeoAdapter模块逐步注入几何信息，并采用随机多模态融合策略，使模型能够在测试时处理任意数量的模态输入。在单目/多视角深度估计、多视角立体视觉和相机姿态估计等多个3D视觉任务上取得了最先进的性能。\n## 论文核心贡献点\n1. 提出了GeoAdapter，一个轻量级适配器，能够将深度图和相机参数编码到空间基础模型中，使用零初始化卷积逐步注入几何信息而不破坏基础模型的表示空间\n2. 设计了随机多模态融合策略，在训练时随机采样模态子集，使模型能够在测试时接受任意数量的模态输入，并学习鲁棒的空间表示\n3. 在多种3D视觉任务上超越了现有方法，即使仅使用RGB输入也取得了最先进的结果\n4. 成功将OmniVGGT集成到视觉-语言-动作(VLA)模型中，在机器人操作任务上实现了持续的性能提升\n## 论文方法描述\n1. **GeoAdapter架构**：包含相机适配器和深度适配器两部分。相机适配器对相机内参和外参进行归一化、参数化为特征向量，然后通过专用编码器和零卷积层处理；深度适配器对深度图进行批内归一化，通过卷积层编码为空间标记\n2. **随机多模态训练**：在训练过程中随机为图像序列分配不同数量的辅助信息（相机参数和深度图），确保模型对各种部分信息场景具有鲁棒性\n3. **训练目标**：采用多任务损失函数，包括相机损失、深度损失和点图损失，每个都增强梯度项以提高局部几何一致性\n4. **网络架构**：基于VGGT框架，使用24个交替注意力块处理输入，最终通过三个预测头输出深度图、相机姿态和3D点图\n## 论文使用数据集和训练资源\n使用19个公共数据集进行训练，包括ARKitScenes、BlendedMVS、DL3DV、Dynamic Replica、HyperSim、Kubric、MapFree、MegaDepth、Matterport 3D、MVS-Synth、ScanNet、ScanNet++、Spring、TartanAir、UASOL、Unreal 4K、Virtual KITTI、Waymo和WildRGBD。这些数据集涵盖了合成和真实内容、室内外环境以及静态和动态场景。训练使用32个NVIDIA A100 GPU，耗时10天，采用梯度检查点优化内存使用。\n## 论文使用的评估环境和评估指标\n在多个标准数据集上进行评估：\n- **深度估计**：使用绝对相对误差和δ<1.25指标\n- **相机姿态估计**：使用相对旋转精度(RRA)、相对平移精度(RTA)和AUC(精度阈值曲线下面积)\n- **3D重建**：使用精度、完整性和法线一致性指标\n- **机器人操作任务**：在CALVIN数据集上评估平均长度等指标\n模型在Sintel、Bonn、NYU-v2、ScanNet、ETH3D、DTU、Tanks and Temples、7-Scenes、Co3Dv2和RealEstate10K等数据集上进行了广泛测试。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>香港科技大学(HKUST)、南洋理工大学(NTU)、中山大学(SYSU)、新加坡国立大学(NUS)、阿里巴巴集团</p>\n<h2>论文概述</h2>\n<p>论文提出了OmniVGGT框架，这是一个能够有效利用任意数量辅助几何模态（如深度图、相机内参和外参）进行训练和推理的视觉基础模型。该框架通过轻量级的GeoAdapter模块逐步注入几何信息，并采用随机多模态融合策略，使模型能够在测试时处理任意数量的模态输入。在单目/多视角深度估计、多视角立体视觉和相机姿态估计等多个3D视觉任务上取得了最先进的性能。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出了GeoAdapter，一个轻量级适配器，能够将深度图和相机参数编码到空间基础模型中，使用零初始化卷积逐步注入几何信息而不破坏基础模型的表示空间</li><li>设计了随机多模态融合策略，在训练时随机采样模态子集，使模型能够在测试时接受任意数量的模态输入，并学习鲁棒的空间表示</li><li>在多种3D视觉任务上超越了现有方法，即使仅使用RGB输入也取得了最先进的结果</li><li>成功将OmniVGGT集成到视觉-语言-动作(VLA)模型中，在机器人操作任务上实现了持续的性能提升</li></ol>\n<h2>论文方法描述</h2>\n<ol><li><strong>GeoAdapter架构</strong>：包含相机适配器和深度适配器两部分。相机适配器对相机内参和外参进行归一化、参数化为特征向量，然后通过专用编码器和零卷积层处理；深度适配器对深度图进行批内归一化，通过卷积层编码为空间标记</li><li><strong>随机多模态训练</strong>：在训练过程中随机为图像序列分配不同数量的辅助信息（相机参数和深度图），确保模型对各种部分信息场景具有鲁棒性</li><li><strong>训练目标</strong>：采用多任务损失函数，包括相机损失、深度损失和点图损失，每个都增强梯度项以提高局部几何一致性</li><li><strong>网络架构</strong>：基于VGGT框架，使用24个交替注意力块处理输入，最终通过三个预测头输出深度图、相机姿态和3D点图</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<p>使用19个公共数据集进行训练，包括ARKitScenes、BlendedMVS、DL3DV、Dynamic Replica、HyperSim、Kubric、MapFree、MegaDepth、Matterport 3D、MVS-Synth、ScanNet、ScanNet++、Spring、TartanAir、UASOL、Unreal 4K、Virtual KITTI、Waymo和WildRGBD。这些数据集涵盖了合成和真实内容、室内外环境以及静态和动态场景。训练使用32个NVIDIA A100 GPU，耗时10天，采用梯度检查点优化内存使用。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>在多个标准数据集上进行评估：</p>\n<ul><li><strong>深度估计</strong>：使用绝对相对误差和δ<1.25指标</li><li><strong>相机姿态估计</strong>：使用相对旋转精度(RRA)、相对平移精度(RTA)和AUC(精度阈值曲线下面积)</li><li><strong>3D重建</strong>：使用精度、完整性和法线一致性指标</li><li><strong>机器人操作任务</strong>：在CALVIN数据集上评估平均长度等指标</li></ul>\n<p>模型在Sintel、Bonn、NYU-v2、ScanNet、ETH3D、DTU、Tanks and Temples、7-Scenes、Co3Dv2和RealEstate10K等数据集上进行了广泛测试。</p>"
  },
  {
    "date": "2025-11-13",
    "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.10518",
    "summary_markdown": "- 论文研究单位\n\n 未在提供的论文HTML原文中明确提及具体的研究单位。\n\n- 论文概述\n\n 本文提出了SemanticVLA，一种新颖的视觉-语言-行动（VLA）框架，通过语义对齐的稀疏化和增强实现高效的机器人操作。该框架解决了现有VLA模型在部署中的两个关键限制：感知冗余和指令-视觉语义对齐的浅层化。SemanticVLA在模拟和真实世界任务中均实现了最先进的性能和效率。\n\n- 论文核心贡献点\n\n 1. 提出了语义引导的双视觉剪枝器（SD-Pruner），通过指令驱动的剪枝器（ID-Pruner）和空间聚合剪枝器（SA-Pruner）分别对SigLIP和DINOv2编码器进行剪枝，显著减少了冗余感知。\n 2. 提出了语义互补的分层融合器（SH-Fuser），通过稠密融合器和稀疏融合器整合来自SigLIP和DINOv2的稠密补丁特征和稀疏语义标记，增强了指令语义和空间结构的对齐。\n 3. 设计了语义条件行动耦合器（SA-Coupler），实现了从稀疏感知到语义行动类型的更直观、高效的映射。\n 4. 在标准VLA基准测试和真实世界机器人部署中进行了广泛实验，证明SemanticVLA实现了最先进的性能和效率。\n\n- 论文方法描述\n\n SemanticVLA通过三个互补的语义级别（指令级语言意图语义、视觉级空间语义和控制级行动语义）统一稀疏化和增强。具体包括：1）SD-Pruner利用编码器专业化（SigLIP用于指令基础，DINOv2用于空间几何），独立剪枝每个编码器以在遮挡和噪声下保留最任务相关的证据；2）SH-Fuser通过稠密融合器和稀疏融合器执行双流融合，跨越整个视觉编码阶段；3）SA-Coupler采用结构化公式，将感知表示显式映射到语义行动类型，并通过基于这些类型的联合控制进行调制，从而增强行动解码的效率和可解释性。\n\n- 论文使用数据集和训练资源\n\n 模拟评估在LIBERO基准上进行，包含四个任务集（Spatial、Object、Goal、Long），每个任务集有500个人类遥操作演示。真实世界实验在AgileX Cobot Magic平台上进行，涵盖物体放置、抽屉操作、多步可变形任务和另外两种场景，分别有60、60、45和105个人类遥操作演示。所有实验均在8块A800（80GB）GPU上进行。对于LIBERO训练，模型训练了80K步，批大小为128；真实世界训练从60K步开始评估检查点。\n\n- 论文使用的评估环境和评估指标\n\n 评估环境包括模拟环境（LIBERO基准）和真实世界环境（AgileX Cobot Magic平台）。评估指标为任务成功率（SR）和排名（RK）在LIBERO基准的四个任务套件上，以及真实世界任务的成功率。同时报告了训练成本、推理延迟、吞吐量、FLOPs等效率指标。",
    "summary_html": "<ul><li>论文研究单位</li></ul>\n\n<p> 未在提供的论文HTML原文中明确提及具体的研究单位。</p>\n\n<ul><li>论文概述</li></ul>\n\n<p> 本文提出了SemanticVLA，一种新颖的视觉-语言-行动（VLA）框架，通过语义对齐的稀疏化和增强实现高效的机器人操作。该框架解决了现有VLA模型在部署中的两个关键限制：感知冗余和指令-视觉语义对齐的浅层化。SemanticVLA在模拟和真实世界任务中均实现了最先进的性能和效率。</p>\n\n<ul><li>论文核心贡献点</li></ul>\n\n<p> 1. 提出了语义引导的双视觉剪枝器（SD-Pruner），通过指令驱动的剪枝器（ID-Pruner）和空间聚合剪枝器（SA-Pruner）分别对SigLIP和DINOv2编码器进行剪枝，显著减少了冗余感知。</p>\n<p> 2. 提出了语义互补的分层融合器（SH-Fuser），通过稠密融合器和稀疏融合器整合来自SigLIP和DINOv2的稠密补丁特征和稀疏语义标记，增强了指令语义和空间结构的对齐。</p>\n<p> 3. 设计了语义条件行动耦合器（SA-Coupler），实现了从稀疏感知到语义行动类型的更直观、高效的映射。</p>\n<p> 4. 在标准VLA基准测试和真实世界机器人部署中进行了广泛实验，证明SemanticVLA实现了最先进的性能和效率。</p>\n\n<ul><li>论文方法描述</li></ul>\n\n<p> SemanticVLA通过三个互补的语义级别（指令级语言意图语义、视觉级空间语义和控制级行动语义）统一稀疏化和增强。具体包括：1）SD-Pruner利用编码器专业化（SigLIP用于指令基础，DINOv2用于空间几何），独立剪枝每个编码器以在遮挡和噪声下保留最任务相关的证据；2）SH-Fuser通过稠密融合器和稀疏融合器执行双流融合，跨越整个视觉编码阶段；3）SA-Coupler采用结构化公式，将感知表示显式映射到语义行动类型，并通过基于这些类型的联合控制进行调制，从而增强行动解码的效率和可解释性。</p>\n\n<ul><li>论文使用数据集和训练资源</li></ul>\n\n<p> 模拟评估在LIBERO基准上进行，包含四个任务集（Spatial、Object、Goal、Long），每个任务集有500个人类遥操作演示。真实世界实验在AgileX Cobot Magic平台上进行，涵盖物体放置、抽屉操作、多步可变形任务和另外两种场景，分别有60、60、45和105个人类遥操作演示。所有实验均在8块A800（80GB）GPU上进行。对于LIBERO训练，模型训练了80K步，批大小为128；真实世界训练从60K步开始评估检查点。</p>\n\n<ul><li>论文使用的评估环境和评估指标</li></ul>\n\n<p> 评估环境包括模拟环境（LIBERO基准）和真实世界环境（AgileX Cobot Magic平台）。评估指标为任务成功率（SR）和排名（RK）在LIBERO基准的四个任务套件上，以及真实世界任务的成功率。同时报告了训练成本、推理延迟、吞吐量、FLOPs等效率指标。</p>"
  },
  {
    "date": "2025-11-13",
    "title": "Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks",
    "link": "http://arxiv.org/abs/2511.10008",
    "summary_markdown": "## 论文研究单位\n浙江大学\n## 论文概述\n本文首次系统性地研究了视觉-语言-动作（VLA）模型在面对物理传感器攻击时的脆弱性。鉴于VLA模型严重依赖摄像头和麦克风等传感器输入，作者旨在量化物理传感器攻击（如激光、电磁、超声波）对模型性能的影响，并探索相应的防御机制。论文提出了一个名为“真实-模拟-真实”的框架，用于自动模拟物理世界中的传感器攻击向量，并在模拟和真实机器人系统上进行验证。通过对多种VLA模型和任务进行大规模评估，研究揭示了这些模型存在的显著漏洞。最后，作者提出并验证了一种基于对抗训练的防御方法，以增强VLA模型对此类物理扰动的鲁棒性。\n## 论文核心贡献点\n- 验证了VLA模型在物理传感器攻击面前的脆弱性，并证实攻击可导致其在真实世界中行为异常。\n- 提出了一个“真实-模拟-真实”框架，以系统且现实的方式评估VLA模型对抗物理传感器攻击的鲁棒性，有效连接了纯数字攻击模拟与资源密集的物理实验之间的差距。\n- 进行了大规模的鲁棒性评估，涵盖了多种VLA模型和任务。至关重要的是，通过在真实世界系统上进行的针对性物理实验，验证了模拟环境的发现，从而证实了该框架的有效性。\n- 提出并验证了一种基于对抗训练的防御策略，用于抵御这些物理攻击，同时保持VLA模型在干净数据集上的性能。\n## 论文方法描述\n论文方法的核心是“真实-模拟-真实”框架。\n1. **攻击选择与模拟**：首先，从顶级安全会议中筛选出八种代表性的物理传感器攻击，其中六种针对摄像头（激光致盲、光投影、激光彩色条纹、电磁彩色条纹、电磁截断、超声波模糊），两种针对麦克风（语音拒绝服务、语音欺骗）。然后，基于这些攻击的物理原理，在数字领域实现高保真度的模拟，并定义了弱、中、强三个攻击强度等级。\n2. **麦克风攻击设计**：\n - **语音拒绝服务**：通过注入高强度超声波信号使麦克风传感器饱和，从而阻止有效指令的接收。\n - **语音欺骗**：通过调制的激光或超声波信号向麦克风注入恶意的语音指令。\n3. **摄像头攻击设计**：\n - **激光致盲**：使用高功率激光照射摄像头，使其光电传感器饱和，无法捕捉环境光线。\n - **光投影**：使用投影仪向环境或镜头投射虚假图像，干扰视觉感知。\n - **激光彩色条纹**：利用摄像头CMOS传感器的卷帘快门效应，通过开关调制激光注入彩色条纹。\n - **电磁彩色条纹与截断**：通过向摄像头图像传输接口（如MIPI CSI-2）注入电磁干扰，导致颜色解码错误或图像截断。\n - **超声波模糊**：向配备防抖模块的摄像头注入超声波，引起其惯性测量单元（IMU）共振，误导防抖算法进行不必要的补偿，导致图像模糊。\n4. **防御方法**：提出一种基于对抗训练的防御策略。该策略首先在干净数据集上训练VLA模型，然后在训练数据中混合一定比例（如30%）的经过攻击模拟的数据集，并对模型进行微调，使其对由传感器攻击引起的分布外扰动具有鲁棒性。\n## 论文使用数据集和训练资源\n- **数据集**：\n - 模拟环境：使用Libero模拟器及其配套的数据集，包括Libero-Spatial、Libero-Object、Libero-Goal和Libero-Long。\n - 真实世界环境：为适应真实世界，通过遥操作收集了一小时的机械臂操作数据，用于微调模型以完成积木抓取与放置任务。\n- **训练资源**：\n - 模型运行与评估：NVIDIA 4090 GPU。\n - 模型微调：使用Lora技术，在NVIDIA H800 GPU (80GB) 上进行。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：Libero模拟器。\n - 真实世界环境：一个装备有Franka Emika Panda机械臂的实验平台。该机械臂配备了一个全局摄像头（Intel RealSense D435i）、一个手腕摄像头（Intel RealSense D435i）和一个麦克风。攻击平台包括电磁干扰（EMI）平台、投影平台、激光平台和超声波平台。\n- **评估指标**：\n - 任务成功率：定义为成功完成的任务次数与总任务次数的比率。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>浙江大学</p>\n<h2>论文概述</h2>\n<p>本文首次系统性地研究了视觉-语言-动作（VLA）模型在面对物理传感器攻击时的脆弱性。鉴于VLA模型严重依赖摄像头和麦克风等传感器输入，作者旨在量化物理传感器攻击（如激光、电磁、超声波）对模型性能的影响，并探索相应的防御机制。论文提出了一个名为“真实-模拟-真实”的框架，用于自动模拟物理世界中的传感器攻击向量，并在模拟和真实机器人系统上进行验证。通过对多种VLA模型和任务进行大规模评估，研究揭示了这些模型存在的显著漏洞。最后，作者提出并验证了一种基于对抗训练的防御方法，以增强VLA模型对此类物理扰动的鲁棒性。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>验证了VLA模型在物理传感器攻击面前的脆弱性，并证实攻击可导致其在真实世界中行为异常。</li><li>提出了一个“真实-模拟-真实”框架，以系统且现实的方式评估VLA模型对抗物理传感器攻击的鲁棒性，有效连接了纯数字攻击模拟与资源密集的物理实验之间的差距。</li><li>进行了大规模的鲁棒性评估，涵盖了多种VLA模型和任务。至关重要的是，通过在真实世界系统上进行的针对性物理实验，验证了模拟环境的发现，从而证实了该框架的有效性。</li><li>提出并验证了一种基于对抗训练的防御策略，用于抵御这些物理攻击，同时保持VLA模型在干净数据集上的性能。</li></ul>\n<h2>论文方法描述</h2>\n<p>论文方法的核心是“真实-模拟-真实”框架。</p>\n<ol><li><strong>攻击选择与模拟</strong>：首先，从顶级安全会议中筛选出八种代表性的物理传感器攻击，其中六种针对摄像头（激光致盲、光投影、激光彩色条纹、电磁彩色条纹、电磁截断、超声波模糊），两种针对麦克风（语音拒绝服务、语音欺骗）。然后，基于这些攻击的物理原理，在数字领域实现高保真度的模拟，并定义了弱、中、强三个攻击强度等级。</li><li><strong>麦克风攻击设计</strong>：</li></ol>\n<p> - <strong>语音拒绝服务</strong>：通过注入高强度超声波信号使麦克风传感器饱和，从而阻止有效指令的接收。</p>\n<p> - <strong>语音欺骗</strong>：通过调制的激光或超声波信号向麦克风注入恶意的语音指令。</p>\n<p>3. <strong>摄像头攻击设计</strong>：</p>\n<p> - <strong>激光致盲</strong>：使用高功率激光照射摄像头，使其光电传感器饱和，无法捕捉环境光线。</p>\n<p> - <strong>光投影</strong>：使用投影仪向环境或镜头投射虚假图像，干扰视觉感知。</p>\n<p> - <strong>激光彩色条纹</strong>：利用摄像头CMOS传感器的卷帘快门效应，通过开关调制激光注入彩色条纹。</p>\n<p> - <strong>电磁彩色条纹与截断</strong>：通过向摄像头图像传输接口（如MIPI CSI-2）注入电磁干扰，导致颜色解码错误或图像截断。</p>\n<p> - <strong>超声波模糊</strong>：向配备防抖模块的摄像头注入超声波，引起其惯性测量单元（IMU）共振，误导防抖算法进行不必要的补偿，导致图像模糊。</p>\n<p>4. <strong>防御方法</strong>：提出一种基于对抗训练的防御策略。该策略首先在干净数据集上训练VLA模型，然后在训练数据中混合一定比例（如30%）的经过攻击模拟的数据集，并对模型进行微调，使其对由传感器攻击引起的分布外扰动具有鲁棒性。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟环境：使用Libero模拟器及其配套的数据集，包括Libero-Spatial、Libero-Object、Libero-Goal和Libero-Long。</p>\n<p> - 真实世界环境：为适应真实世界，通过遥操作收集了一小时的机械臂操作数据，用于微调模型以完成积木抓取与放置任务。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 模型运行与评估：NVIDIA 4090 GPU。</p>\n<p> - 模型微调：使用Lora技术，在NVIDIA H800 GPU (80GB) 上进行。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：Libero模拟器。</p>\n<p> - 真实世界环境：一个装备有Franka Emika Panda机械臂的实验平台。该机械臂配备了一个全局摄像头（Intel RealSense D435i）、一个手腕摄像头（Intel RealSense D435i）和一个麦克风。攻击平台包括电磁干扰（EMI）平台、投影平台、激光平台和超声波平台。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率：定义为成功完成的任务次数与总任务次数的比率。</p>"
  },
  {
    "date": "2025-11-13",
    "title": "Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.09958",
    "summary_markdown": "## 论文研究单位\n华东师范大学, 复旦大学\n## 论文概述\n本文提出了Audio-VLA，一种将接触音频感知融入视觉-语言-行动模型的多模态机器人操控策略。传统VLA模型仅依赖视觉感知，在感知接触事件和动态操作过程方面存在根本性局限。Audio-VLA通过引入接触音频来克服这一限制，利用音频信号提供视觉无法捕捉的物理交互信息，如接触质量和材料属性。此外，本文还提出了一种新的评估指标——任务完成率，用于更系统地评估机器人在动态操作过程中的感知能力。实验在LIBERO、RLBench仿真环境以及两个真实世界任务中验证了Audio-VLA的优越性能。\n## 论文核心贡献点\n1. 提出了Audio-VLA模型，这是首个将接触音频感知集成到VLA框架中的多模态操控策略，旨在克服纯视觉模型的感知局限。\n2. 提出了任务完成率这一新的评估指标，用于量化机器人在操控过程中对动态过程的感知和理解能力，而不仅仅评估最终结果。\n3. 构建了音频增强的仿真环境，通过在LIBERO和RLBench中引入基于物理碰撞的音频生成，为模型训练和评估提供了更真实的声学反馈。\n## 论文方法描述\nAudio-VLA模型架构包含四个主要部分：\n1. 多模态编码器：使用预训练的DINOv2和SigLIP作为视觉编码器，处理第三人称和手腕相机图像；使用经过ManiWAV数据集微调的AudioCLIP作为音频编码器，处理高频率的接触音频信号；使用一个MLP层编码机器人本体感觉状态（如关节角度）。\n2. 多模态投影器：通过线性层和MLP将视觉、音频和本体感觉特征对齐到统一的文本特征空间，形成一个时序序列。\n3. 语言模块：采用7B参数的Llama2作为骨干网络，将多模态特征序列与语言指令结合，进行认知推理，并解码出包含动作信息的隐藏状态。\n4. 动作头：使用一个四层的MLP，根据解码出的隐藏状态生成连续的机器人动作序列。\n\n模型的训练目标是最小化预测动作序列与专家演示动作之间的平均L1损失。\n## 论文使用数据集和训练资源\n* **数据集**：\n * **仿真环境**：LIBERO官方数据集和RLBench任务数据集（每个任务100个演示）。\n * **真实世界**：为两个任务（擦白板和舀燕麦）通过遥操作方式收集了40个演示数据。\n * **音频预训练**：使用ManiWAV数据集对音频编码器进行额外的微调。\n* **训练资源**：\n * 使用2块NVIDIA H20 GPU进行训练。\n * LoRA秩设置为32，训练步数根据任务在5万到10万之间，批大小为8，学习率为1e-4并采用余弦退火策略。\n## 论文使用的评估环境和评估指标\n* **评估环境**：\n * **仿真环境**：在音频增强的LIBERO和RLBench环境中进行评估。评估分为两种条件：标准环境（与训练环境相同）和领域偏移环境（随机改变光照和桌面材质颜色）。\n * **真实世界环境**：在AgileX Mobile ALOHA平台上进行评估，该平台配备双7-DOF Piper机械臂、手腕相机和安装在夹爪上的压电接触麦克风。评估分为见过环境（与训练条件相同）和未见过环境（使用不同颜色的记号笔或不同特性的燕麦）。\n* **评估指标**：\n * **成功率**：标准的二元任务成功率指标。\n * **任务完成率**：本文提出的新指标，用于衡量任务的完成进度，计算公式为 `TCR = 已完成进度 / 任务目标`。它是一个连续值，能更细致地评估模型对动态过程的理解能力。例如，擦白板任务的TCR是擦除标记的面积，舀燕麦任务的TCR是舀取的燕麦重量。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>华东师范大学, 复旦大学</p>\n<h2>论文概述</h2>\n<p>本文提出了Audio-VLA，一种将接触音频感知融入视觉-语言-行动模型的多模态机器人操控策略。传统VLA模型仅依赖视觉感知，在感知接触事件和动态操作过程方面存在根本性局限。Audio-VLA通过引入接触音频来克服这一限制，利用音频信号提供视觉无法捕捉的物理交互信息，如接触质量和材料属性。此外，本文还提出了一种新的评估指标——任务完成率，用于更系统地评估机器人在动态操作过程中的感知能力。实验在LIBERO、RLBench仿真环境以及两个真实世界任务中验证了Audio-VLA的优越性能。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出了Audio-VLA模型，这是首个将接触音频感知集成到VLA框架中的多模态操控策略，旨在克服纯视觉模型的感知局限。</li><li>提出了任务完成率这一新的评估指标，用于量化机器人在操控过程中对动态过程的感知和理解能力，而不仅仅评估最终结果。</li><li>构建了音频增强的仿真环境，通过在LIBERO和RLBench中引入基于物理碰撞的音频生成，为模型训练和评估提供了更真实的声学反馈。</li></ol>\n<h2>论文方法描述</h2>\n<p>Audio-VLA模型架构包含四个主要部分：</p>\n<ol><li>多模态编码器：使用预训练的DINOv2和SigLIP作为视觉编码器，处理第三人称和手腕相机图像；使用经过ManiWAV数据集微调的AudioCLIP作为音频编码器，处理高频率的接触音频信号；使用一个MLP层编码机器人本体感觉状态（如关节角度）。</li><li>多模态投影器：通过线性层和MLP将视觉、音频和本体感觉特征对齐到统一的文本特征空间，形成一个时序序列。</li><li>语言模块：采用7B参数的Llama2作为骨干网络，将多模态特征序列与语言指令结合，进行认知推理，并解码出包含动作信息的隐藏状态。</li><li>动作头：使用一个四层的MLP，根据解码出的隐藏状态生成连续的机器人动作序列。</li></ol>\n\n<p>模型的训练目标是最小化预测动作序列与专家演示动作之间的平均L1损失。</p>\n<h2>论文使用数据集和训练资源</h2>\n<p>* <strong>数据集</strong>：</p>\n<p> * <strong>仿真环境</strong>：LIBERO官方数据集和RLBench任务数据集（每个任务100个演示）。</p>\n<p> * <strong>真实世界</strong>：为两个任务（擦白板和舀燕麦）通过遥操作方式收集了40个演示数据。</p>\n<p> * <strong>音频预训练</strong>：使用ManiWAV数据集对音频编码器进行额外的微调。</p>\n<p>* <strong>训练资源</strong>：</p>\n<p> * 使用2块NVIDIA H20 GPU进行训练。</p>\n<p> * LoRA秩设置为32，训练步数根据任务在5万到10万之间，批大小为8，学习率为1e-4并采用余弦退火策略。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>* <strong>评估环境</strong>：</p>\n<p> * <strong>仿真环境</strong>：在音频增强的LIBERO和RLBench环境中进行评估。评估分为两种条件：标准环境（与训练环境相同）和领域偏移环境（随机改变光照和桌面材质颜色）。</p>\n<p> * <strong>真实世界环境</strong>：在AgileX Mobile ALOHA平台上进行评估，该平台配备双7-DOF Piper机械臂、手腕相机和安装在夹爪上的压电接触麦克风。评估分为见过环境（与训练条件相同）和未见过环境（使用不同颜色的记号笔或不同特性的燕麦）。</p>\n<p>* <strong>评估指标</strong>：</p>\n<p> * <strong>成功率</strong>：标准的二元任务成功率指标。</p>\n<p> * <strong>任务完成率</strong>：本文提出的新指标，用于衡量任务的完成进度，计算公式为 <code>TCR = 已完成进度 / 任务目标</code>。它是一个连续值，能更细致地评估模型对动态过程的理解能力。例如，擦白板任务的TCR是擦除标记的面积，舀燕麦任务的TCR是舀取的燕麦重量。</p>"
  },
  {
    "date": "2025-11-12",
    "title": "MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.09516",
    "summary_markdown": "论文研究单位\n南洋理工大学（新加坡），VinUniversity（越南，河内），北京邮电大学（中国，北京），清华大学（中国，北京），华南理工大学（中国，广州）\n\n论文概述\n本文提出了MAP-VLA（Memory-Augmented Prompting for Vision-Language-Action Model）框架，旨在解决预训练的视觉-语言-动作（VLA）模型在处理长时程机器人操作任务时，因缺乏记忆机制而表现不佳的问题。该框架通过从历史演示中构建记忆库，并在任务执行期间检索相关记忆提示，动态增强一个冻结的VLA模型的动作生成能力。MAP-VLA作为一种轻量级、即插即用的模块，无需修改模型内部权重，通过提示调优和检索增强生成来提升任务适应性。实验结果表明，该方法在仿真基准和真实机器人评估中，分别实现了最高7.0%和25.0%的绝对性能增益。\n\n论文核心贡献点\n- 提出了MAP-VLA框架，通过提示调优和检索增强的方式，为预训练VLA模型增加演示衍生的记忆提示，从而提升长时程任务性能，且无需修改模型内部权重。\n- 介绍了记忆提示构建（MPC）和记忆增强动作生成（MAAG）两个核心模块。MPC将专家演示中的阶段特定记忆编码为一组可学习的软提示；MAAG在执行时检索相关记忆，并通过动态的、感知记忆的提示集成来增强动作生成。\n- 通过广泛的实验验证了方法的有效性，MAP-VLA在长时程操作任务上持续优于现有最先进方法，在仿真基准和真实机器人评估中分别取得了7.0%和25.0%的绝对增益。\n\n论文方法描述\n该方法包含离线构建和在线执行两个阶段。\n1. 记忆提示构建（MPC）：首先，使用Ramer-Douglas-Peucker (RDP)算法从参考轨迹中提取关键位姿以划分任务阶段，并利用动态时间规整（DTW）算法将所有演示轨迹与参考轨迹对齐，确保阶段一致性。然后，为每个任务阶段 $\\mathcal{S}_k$ 学习一个软提示向量 $\\mathcal{V}_k$。该软提示通过元素加法与基础提示（由当前观测生成）结合，并使用流匹配损失进行优化，使其能够引导模型在该阶段生成与专家一致的动作。所有阶段的软提示构成一个记忆库。\n2. 记忆增强动作生成（MAAG）：在任务执行期间，系统首先通过计算当前轨迹窗口与记忆库中演示片段的L2距离，检索最相似的历史片段，并获取对应的阶段记忆提示。然后，冻结的VLA模型分别使用基础提示和检索到的记忆提示生成两组动作预测。系统还会检索与匹配片段对应的专家动作作为先验。最后，通过一个动态加权系数 $\\alpha_t$（基于两组动作预测与专家动作的相似度计算得出）来融合这两组预测，得到最终执行的动作，实现了任务级泛化与阶段特定记忆的动态平衡。\n\n论文使用数据集和训练资源\n- 数据集：LIBERO仿真基准，特别是其中的长时程任务套件LIBERO-Long。\n- 训练资源：模型微调和提示调优在一台配备6块NVIDIA RTX 6000 Ada GPU的服务器上进行。真实世界的计算在一块NVIDIA RTX 4090 GPU上完成。\n\n论文使用的评估环境和评估指标\n- 评估环境：\n - 仿真环境：LIBERO基准测试。\n - 真实环境：配备6自由度Galaxea A1机械臂的物理机器人平台。\n- 评估指标：\n - 主要指标：任务成功率。\n - 在真实机器人评估中，进一步细分为部分成功率（完成第一个子任务）和完全成功率（完成所有子任务）。",
    "summary_html": "<p>论文研究单位</p>\n<p>南洋理工大学（新加坡），VinUniversity（越南，河内），北京邮电大学（中国，北京），清华大学（中国，北京），华南理工大学（中国，广州）</p>\n\n<p>论文概述</p>\n<p>本文提出了MAP-VLA（Memory-Augmented Prompting for Vision-Language-Action Model）框架，旨在解决预训练的视觉-语言-动作（VLA）模型在处理长时程机器人操作任务时，因缺乏记忆机制而表现不佳的问题。该框架通过从历史演示中构建记忆库，并在任务执行期间检索相关记忆提示，动态增强一个冻结的VLA模型的动作生成能力。MAP-VLA作为一种轻量级、即插即用的模块，无需修改模型内部权重，通过提示调优和检索增强生成来提升任务适应性。实验结果表明，该方法在仿真基准和真实机器人评估中，分别实现了最高7.0%和25.0%的绝对性能增益。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了MAP-VLA框架，通过提示调优和检索增强的方式，为预训练VLA模型增加演示衍生的记忆提示，从而提升长时程任务性能，且无需修改模型内部权重。</li><li>介绍了记忆提示构建（MPC）和记忆增强动作生成（MAAG）两个核心模块。MPC将专家演示中的阶段特定记忆编码为一组可学习的软提示；MAAG在执行时检索相关记忆，并通过动态的、感知记忆的提示集成来增强动作生成。</li><li>通过广泛的实验验证了方法的有效性，MAP-VLA在长时程操作任务上持续优于现有最先进方法，在仿真基准和真实机器人评估中分别取得了7.0%和25.0%的绝对增益。</li></ul>\n\n<p>论文方法描述</p>\n<p>该方法包含离线构建和在线执行两个阶段。</p>\n<ol><li>记忆提示构建（MPC）：首先，使用Ramer-Douglas-Peucker (RDP)算法从参考轨迹中提取关键位姿以划分任务阶段，并利用动态时间规整（DTW）算法将所有演示轨迹与参考轨迹对齐，确保阶段一致性。然后，为每个任务阶段 $\\mathcal{S}_k$ 学习一个软提示向量 $\\mathcal{V}_k$。该软提示通过元素加法与基础提示（由当前观测生成）结合，并使用流匹配损失进行优化，使其能够引导模型在该阶段生成与专家一致的动作。所有阶段的软提示构成一个记忆库。</li><li>记忆增强动作生成（MAAG）：在任务执行期间，系统首先通过计算当前轨迹窗口与记忆库中演示片段的L2距离，检索最相似的历史片段，并获取对应的阶段记忆提示。然后，冻结的VLA模型分别使用基础提示和检索到的记忆提示生成两组动作预测。系统还会检索与匹配片段对应的专家动作作为先验。最后，通过一个动态加权系数 $\\alpha_t$（基于两组动作预测与专家动作的相似度计算得出）来融合这两组预测，得到最终执行的动作，实现了任务级泛化与阶段特定记忆的动态平衡。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：LIBERO仿真基准，特别是其中的长时程任务套件LIBERO-Long。</li><li>训练资源：模型微调和提示调优在一台配备6块NVIDIA RTX 6000 Ada GPU的服务器上进行。真实世界的计算在一块NVIDIA RTX 4090 GPU上完成。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：</li></ul>\n<p> - 仿真环境：LIBERO基准测试。</p>\n<p> - 真实环境：配备6自由度Galaxea A1机械臂的物理机器人平台。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 主要指标：任务成功率。</p>\n<p> - 在真实机器人评估中，进一步细分为部分成功率（完成第一个子任务）和完全成功率（完成所有子任务）。</p>"
  },
  {
    "date": "2025-11-12",
    "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.09515",
    "summary_markdown": "### 论文研究单位\n香港科技大学、字节跳动\n### 论文概述\n本文提出WMPO（World Model-based Policy Optimization），一个基于生成式世界模型的视觉-语言-动作（VLA）模型强化学习框架。该方法通过在像素级世界模型中进行策略优化，完全替代了昂贵且低效的真实环境交互，显著提升了VLA模型的样本效率和泛化能力，同时展现出自我修正等涌现行为。\n### 论文核心贡献点\n1. **像素级世界模型**：提出与VLA预训练特征对齐的像素空间世界模型，避免潜在空间不匹配问题。\n2. **策略行为对齐**：通过微调世界模型使其适应策略行为分布，实现对失败场景的逼真模拟。\n3. **高效在线强化学习**：支持GRPO算法在\"想象\"轨迹中进行策略优化，克服物理交互瓶颈。\n4. **涌现能力验证**：实验证明方法能产生自我修正行为、高效任务执行及强泛化能力。\n### 论文方法描述\n1. **世界模型构建**：基于OpenSora的视频扩散模型，替换3D VAE为SDXL的2D VAE以保留运动细节，引入带噪声帧条件增强长时程生成稳定性。\n2. **策略行为对齐**：在Open X-Embodiment数据集预训练后，用策略自身采集的轨迹微调世界模型。\n3. **奖励模型设计**：采用VideoMAE编码器训练二分类模型，通过滑动窗口评估轨迹成功概率。\n4. **GRPO优化**：每组初始状态生成G条想象轨迹，通过动态采样确保批次多样性，利用归一化优势函数更新策略。\n### 论文使用数据集和训练资源\n- **数据集**：Open X-Embodiment（预训练）、Mimicgen仿真任务（Coffee、StackThree等）、Cobot Mobile ALOHA真实轨迹（200条专家演示+128条策略采集）。\n- **训练资源**：8块H100 GPU进行策略微调，32块H100 GPU训练世界模型和优化策略。\n### 论文使用的评估环境和评估指标\n- **仿真环境**：Mimicgen基准的4个操作任务，评估128个随机初始状态的平均成功率。\n- **真实环境**：Cobot Mobile ALOHA平台执行\"插入方块\"任务（间隙5mm），30次试验平均成功率。\n- **泛化测试**：位置扰动（棒随机位置）、背景扰动（灰色背景）、纹理扰动（木质基座）下的成功率。\n- **终身学习**：迭代采集128条轨迹更新策略，对比DPO基线的性能提升。\n- **指标**：任务成功率（%）、轨迹长度（效率）、F1分数（奖励模型可靠性）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>香港科技大学、字节跳动</p>\n<h3>论文概述</h3>\n<p>本文提出WMPO（World Model-based Policy Optimization），一个基于生成式世界模型的视觉-语言-动作（VLA）模型强化学习框架。该方法通过在像素级世界模型中进行策略优化，完全替代了昂贵且低效的真实环境交互，显著提升了VLA模型的样本效率和泛化能力，同时展现出自我修正等涌现行为。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>像素级世界模型</strong>：提出与VLA预训练特征对齐的像素空间世界模型，避免潜在空间不匹配问题。</li><li><strong>策略行为对齐</strong>：通过微调世界模型使其适应策略行为分布，实现对失败场景的逼真模拟。</li><li><strong>高效在线强化学习</strong>：支持GRPO算法在\"想象\"轨迹中进行策略优化，克服物理交互瓶颈。</li><li><strong>涌现能力验证</strong>：实验证明方法能产生自我修正行为、高效任务执行及强泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>世界模型构建</strong>：基于OpenSora的视频扩散模型，替换3D VAE为SDXL的2D VAE以保留运动细节，引入带噪声帧条件增强长时程生成稳定性。</li><li><strong>策略行为对齐</strong>：在Open X-Embodiment数据集预训练后，用策略自身采集的轨迹微调世界模型。</li><li><strong>奖励模型设计</strong>：采用VideoMAE编码器训练二分类模型，通过滑动窗口评估轨迹成功概率。</li><li><strong>GRPO优化</strong>：每组初始状态生成G条想象轨迹，通过动态采样确保批次多样性，利用归一化优势函数更新策略。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Open X-Embodiment（预训练）、Mimicgen仿真任务（Coffee、StackThree等）、Cobot Mobile ALOHA真实轨迹（200条专家演示+128条策略采集）。</li><li><strong>训练资源</strong>：8块H100 GPU进行策略微调，32块H100 GPU训练世界模型和优化策略。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>仿真环境</strong>：Mimicgen基准的4个操作任务，评估128个随机初始状态的平均成功率。</li><li><strong>真实环境</strong>：Cobot Mobile ALOHA平台执行\"插入方块\"任务（间隙5mm），30次试验平均成功率。</li><li><strong>泛化测试</strong>：位置扰动（棒随机位置）、背景扰动（灰色背景）、纹理扰动（木质基座）下的成功率。</li><li><strong>终身学习</strong>：迭代采集128条轨迹更新策略，对比DPO基线的性能提升。</li><li><strong>指标</strong>：任务成功率（%）、轨迹长度（效率）、F1分数（奖励模型可靠性）。</li></ul>"
  },
  {
    "date": "2025-11-12",
    "title": "MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror",
    "link": "http://arxiv.org/abs/2511.08865",
    "summary_markdown": "论文研究单位\nZTE Terminators Group\n\n论文概述\n本文提出了一个基于PICO的机器人远程操作框架MirrorLimb，旨在实现低成本、实时的手部运动和姿态数据采集。该框架与RealMirror生态系统原生兼容，能够在Isaac仿真环境中稳定、精确地记录机器人轨迹，以促进视觉-语言-动作（VLA）数据集的构建。此外，该系统支持对配备灵巧手和夹爪等多种末端执行器的机器人进行实时遥操作，旨在降低上肢机器人操作研究的技术门槛。\n\n论文核心贡献点\n与RealMirror平台原生兼容，结合其运动学/动力学优化能力，可在IsaacSim仿真环境中实现高精度、稳定的机器人远程遥操作。\n集成了基于WebXR/OpenXR的通信框架和端到端遥操作软件系统，为手柄/手势输入的原始数据提供标准化接口，便于适配不同操作平台和机器人末端执行器。\n支持PICO 3和4等XR设备，相比专业动捕系统或Apple Vision Pro，硬件成本显著降低，大大降低了实现精细化机器人操作的门槛。\n\n论文方法描述\n该方法设计了一个双通道采集栈，实时从PICO XR设备获取手柄指令和精细的手部姿态。手柄路径使用WebXR在浏览器中实现，数据流传输至安全的Node.js服务器；手势路径则通过OpenXR（PICO的Unity SDK）实现，以60Hz的固定频率通过UDP传输关节姿态数据。两路数据都会经过坐标系统转换，以匹配RealMirror/IsaacSim的坐标约定。为了解决遥操作中的抖动和突变问题，系统在逆运动学求解阶段设计了运动学优化规则，通过分析连续帧间末端位置和关节角度的变化，并应用阈值过滤，从而抑制控制不稳定性，确保遥操作的平滑与精确。\n\n论文使用数据集和训练资源\n论文本身不使用特定公开数据集进行模型训练，而是提供一个数据采集框架，用于生成和记录机器人在Isaac仿真环境中的遥操作轨迹数据，以构建VLA数据集。使用的硬件资源主要包括PICO 3和PICO 4等XR设备以及用于数据传输和处理的Node.js服务器。\n\n论文使用的评估环境和评估指标\n评估环境是NVIDIA的IsaacSim/IsaacLab仿真平台。评估指标侧重于定性性能，包括：成本效益、实时性能、数据采集的稳定性、远程操作的精确度、以及通过运动学优化对末端执行器抖动和突变的抑制效果。",
    "summary_html": "<p>论文研究单位</p>\n<p>ZTE Terminators Group</p>\n\n<p>论文概述</p>\n<p>本文提出了一个基于PICO的机器人远程操作框架MirrorLimb，旨在实现低成本、实时的手部运动和姿态数据采集。该框架与RealMirror生态系统原生兼容，能够在Isaac仿真环境中稳定、精确地记录机器人轨迹，以促进视觉-语言-动作（VLA）数据集的构建。此外，该系统支持对配备灵巧手和夹爪等多种末端执行器的机器人进行实时遥操作，旨在降低上肢机器人操作研究的技术门槛。</p>\n\n<p>论文核心贡献点</p>\n<p>与RealMirror平台原生兼容，结合其运动学/动力学优化能力，可在IsaacSim仿真环境中实现高精度、稳定的机器人远程遥操作。</p>\n<p>集成了基于WebXR/OpenXR的通信框架和端到端遥操作软件系统，为手柄/手势输入的原始数据提供标准化接口，便于适配不同操作平台和机器人末端执行器。</p>\n<p>支持PICO 3和4等XR设备，相比专业动捕系统或Apple Vision Pro，硬件成本显著降低，大大降低了实现精细化机器人操作的门槛。</p>\n\n<p>论文方法描述</p>\n<p>该方法设计了一个双通道采集栈，实时从PICO XR设备获取手柄指令和精细的手部姿态。手柄路径使用WebXR在浏览器中实现，数据流传输至安全的Node.js服务器；手势路径则通过OpenXR（PICO的Unity SDK）实现，以60Hz的固定频率通过UDP传输关节姿态数据。两路数据都会经过坐标系统转换，以匹配RealMirror/IsaacSim的坐标约定。为了解决遥操作中的抖动和突变问题，系统在逆运动学求解阶段设计了运动学优化规则，通过分析连续帧间末端位置和关节角度的变化，并应用阈值过滤，从而抑制控制不稳定性，确保遥操作的平滑与精确。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>论文本身不使用特定公开数据集进行模型训练，而是提供一个数据采集框架，用于生成和记录机器人在Isaac仿真环境中的遥操作轨迹数据，以构建VLA数据集。使用的硬件资源主要包括PICO 3和PICO 4等XR设备以及用于数据传输和处理的Node.js服务器。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境是NVIDIA的IsaacSim/IsaacLab仿真平台。评估指标侧重于定性性能，包括：成本效益、实时性能、数据采集的稳定性、远程操作的精确度、以及通过运动学优化对末端执行器抖动和突变的抑制效果。</p>"
  },
  {
    "date": "2025-11-11",
    "title": "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control",
    "link": "http://arxiv.org/abs/2511.07820",
    "summary_markdown": "论文研究单位\nNvidia\n\n论文概述\n该论文提出了SONIC框架，旨在通过扩展运动跟踪来实现自然的人形机器人全身控制。论文指出，尽管其他领域（如语言模型）已经通过大规模扩展取得了显著成果，但人形机器人控制领域尚未实现类似的规模效益。现有控制器通常规模较小、行为集有限且训练资源投入不多。SONIC通过将运动跟踪定义为一个可扩展的基础任务，利用大规模、多样化的运动捕捉数据进行密集监督，从而避免了为每个任务手动设计奖励函数的复杂性。作者通过增加网络规模（至4200万参数）、数据集规模（超过1亿帧，700小时）和计算资源（32000 GPU小时），成功训练出一个能够生成自然且鲁棒的全身运动的通用控制器。此外，论文还展示了两种应用机制：一个实现交互式控制的实时运动学规划器，以及一个支持多种输入模态（如VR遥操作、视频、文本、音乐和VLA模型）的统一令牌空间。研究结果表明，运动跟踪的性能随数据量和计算量的增加而稳步提升，并且学习到的表示能够泛化到未见过的动作，证明了其作为人形机器人控制基础任务的可行性。\n\n论文核心贡献点\n- 将运动跟踪确定为人形机器人控制的可扩展基础任务，证明了其在计算资源和数据多样性规模上的扩展优势，并将人形机器人控制的训练规模扩展至32000 GPU小时和超过1亿帧的运动数据，实现了对多样化人类行为的通用跟踪能力。\n- 引入了一个用于交互式控制的运动学生成系统和一个支持多模态输入的通用令牌空间，该空间能统一处理遥操作、人类视频、运动指令和视觉-语言-动作（VLA）模型等，所有这些均通过单阶段训练实现，无需蒸馏。\n- 提供了全面的评估，包括人形机器人控制的经验性扩展趋势、对未见运动的零样本迁移、在物理人形机器人上的鲁棒sim-to-real部署，以及与基础模型的成功集成。\n\n论文方法描述\nSONIC框架的核心是一个通用的人形机器人运动跟踪系统，它通过统一的编码器-解码器架构处理来自不同形态（机器人、人类、混合）的运动指令。该方法将运动跟踪建模为马尔可夫决策过程，并使用近端策略优化（PPO）算法进行训练。状态包括机器人本体感知信息和运动指令，动作是目标关节位置。奖励函数结合了最小化机器人与目标运动之间差异的跟踪项，以及惩罚突变和关节越界的惩罚项。关键创新在于其通用控制策略：不同类型的运动指令（机器人关节状态、人体SMPL姿态、混合指令）通过专门的编码器被映射到一个共享的潜在表示，该表示经过量化后形成“通用令牌”。这个令牌再由一个共同的机器人控制解码器解析为电机指令，同时辅以一个运动重建解码器来促进特征学习和隐式重定向。系统还包含一个实时的运动学规划器，它基于用户命令和机器人历史状态，以自回归方式生成短时（0.8-2.4秒）的未来运动轨迹，从而支持响应式的交互控制。整个训练过程的总损失包括PPO损失、重建损失、令牌对齐损失和循环一致性损失，以确保跨形态表示的一致性。\n\n论文使用数据集和训练资源\n- 数据集：使用了一个内部的大规模运动捕捉数据集，包含170名受试者的数据，总计700小时、超过1亿帧（50Hz）的运动数据。数据内容涵盖了步行、跑步、舞蹈、体育动作、日常活动和格斗等多种人类行为。所有人体运动数据均通过GMR（Geometric Motion Retargeting）方法重定向到人形机器人上。\n- 训练资源：最终模型在128个GPU上训练了约3天，总计使用了32000 GPU小时。模型的参数规模从120万扩展到了4200万。\n\n论文使用的评估环境和评估指标\n- 评估环境：主要在Isaac Lab仿真环境中进行，为了与基线方法进行公平比较，部分实验也在MuJoCo中完成。最终的真实世界评估在Unitree G1人形机器人上进行。\n- 评估指标：\n - 成功率：当机器人在跟踪过程中身体高度偏离参考运动超过0.25米或根方向偏离超过1弧度时，视为跟踪失败。\n - 平均每关节位置误差（MPJPE）：以毫米为单位，衡量局部姿态的跟踪精度。\n - 加速度差异和速度差异：分别以 mm/frame² 和 mm/frame 为单位，衡量跟踪运动的物理保真度。\n - 泛化性测试：在未训练过的AMASS数据集子集（9小时，1602个轨迹）上评估零样本泛化能力。\n - 真实世界成功率：在50个多样化的真实世界运动轨迹上测试，报告了100%的成功率。\n - 遥操作延迟与精度：对于VR遥操作，测量了从命令发出到机器人执行的延迟（平均121.9毫秒）以及手腕末端的位置和姿态误差（平均位置误差6cm，平均姿态误差0.145弧度）。",
    "summary_html": "<p>论文研究单位</p>\n<p>Nvidia</p>\n\n<p>论文概述</p>\n<p>该论文提出了SONIC框架，旨在通过扩展运动跟踪来实现自然的人形机器人全身控制。论文指出，尽管其他领域（如语言模型）已经通过大规模扩展取得了显著成果，但人形机器人控制领域尚未实现类似的规模效益。现有控制器通常规模较小、行为集有限且训练资源投入不多。SONIC通过将运动跟踪定义为一个可扩展的基础任务，利用大规模、多样化的运动捕捉数据进行密集监督，从而避免了为每个任务手动设计奖励函数的复杂性。作者通过增加网络规模（至4200万参数）、数据集规模（超过1亿帧，700小时）和计算资源（32000 GPU小时），成功训练出一个能够生成自然且鲁棒的全身运动的通用控制器。此外，论文还展示了两种应用机制：一个实现交互式控制的实时运动学规划器，以及一个支持多种输入模态（如VR遥操作、视频、文本、音乐和VLA模型）的统一令牌空间。研究结果表明，运动跟踪的性能随数据量和计算量的增加而稳步提升，并且学习到的表示能够泛化到未见过的动作，证明了其作为人形机器人控制基础任务的可行性。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>将运动跟踪确定为人形机器人控制的可扩展基础任务，证明了其在计算资源和数据多样性规模上的扩展优势，并将人形机器人控制的训练规模扩展至32000 GPU小时和超过1亿帧的运动数据，实现了对多样化人类行为的通用跟踪能力。</li><li>引入了一个用于交互式控制的运动学生成系统和一个支持多模态输入的通用令牌空间，该空间能统一处理遥操作、人类视频、运动指令和视觉-语言-动作（VLA）模型等，所有这些均通过单阶段训练实现，无需蒸馏。</li><li>提供了全面的评估，包括人形机器人控制的经验性扩展趋势、对未见运动的零样本迁移、在物理人形机器人上的鲁棒sim-to-real部署，以及与基础模型的成功集成。</li></ul>\n\n<p>论文方法描述</p>\n<p>SONIC框架的核心是一个通用的人形机器人运动跟踪系统，它通过统一的编码器-解码器架构处理来自不同形态（机器人、人类、混合）的运动指令。该方法将运动跟踪建模为马尔可夫决策过程，并使用近端策略优化（PPO）算法进行训练。状态包括机器人本体感知信息和运动指令，动作是目标关节位置。奖励函数结合了最小化机器人与目标运动之间差异的跟踪项，以及惩罚突变和关节越界的惩罚项。关键创新在于其通用控制策略：不同类型的运动指令（机器人关节状态、人体SMPL姿态、混合指令）通过专门的编码器被映射到一个共享的潜在表示，该表示经过量化后形成“通用令牌”。这个令牌再由一个共同的机器人控制解码器解析为电机指令，同时辅以一个运动重建解码器来促进特征学习和隐式重定向。系统还包含一个实时的运动学规划器，它基于用户命令和机器人历史状态，以自回归方式生成短时（0.8-2.4秒）的未来运动轨迹，从而支持响应式的交互控制。整个训练过程的总损失包括PPO损失、重建损失、令牌对齐损失和循环一致性损失，以确保跨形态表示的一致性。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：使用了一个内部的大规模运动捕捉数据集，包含170名受试者的数据，总计700小时、超过1亿帧（50Hz）的运动数据。数据内容涵盖了步行、跑步、舞蹈、体育动作、日常活动和格斗等多种人类行为。所有人体运动数据均通过GMR（Geometric Motion Retargeting）方法重定向到人形机器人上。</li><li>训练资源：最终模型在128个GPU上训练了约3天，总计使用了32000 GPU小时。模型的参数规模从120万扩展到了4200万。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：主要在Isaac Lab仿真环境中进行，为了与基线方法进行公平比较，部分实验也在MuJoCo中完成。最终的真实世界评估在Unitree G1人形机器人上进行。</li><li>评估指标：</li></ul>\n<p> - 成功率：当机器人在跟踪过程中身体高度偏离参考运动超过0.25米或根方向偏离超过1弧度时，视为跟踪失败。</p>\n<p> - 平均每关节位置误差（MPJPE）：以毫米为单位，衡量局部姿态的跟踪精度。</p>\n<p> - 加速度差异和速度差异：分别以 mm/frame² 和 mm/frame 为单位，衡量跟踪运动的物理保真度。</p>\n<p> - 泛化性测试：在未训练过的AMASS数据集子集（9小时，1602个轨迹）上评估零样本泛化能力。</p>\n<p> - 真实世界成功率：在50个多样化的真实世界运动轨迹上测试，报告了100%的成功率。</p>\n<p> - 遥操作延迟与精度：对于VR遥操作，测量了从命令发出到机器人执行的延迟（平均121.9毫秒）以及手腕末端的位置和姿态误差（平均位置误差6cm，平均姿态误差0.145弧度）。</p>"
  },
  {
    "date": "2025-11-10",
    "title": "How Do VLAs Effectively Inherit from VLMs?",
    "link": "http://arxiv.org/abs/2511.06619",
    "summary_markdown": "### 论文研究单位\nMicrosoft Research\n### 论文概述\n该论文探讨了视觉-语言-行动模型如何有效地从预训练的视觉-语言模型中继承知识。为了解决这一核心问题并应对训练过程中可能出现的灾难性遗忘，研究者们引入了一个名为GrinningFace的诊断基准。该基准是一个表情符号桌面操作任务，要求机器人根据语言指令将物体放置在对应的印刷表情符号上。由于表情符号在VLM的预训练数据中广泛存在，但在标准机器人数据集中几乎不存在，该任务的成功完成可以清晰地表明VLM的先验知识被有效迁移到了具身控制中。论文通过在模拟和真实机器人上进行系统性实验，比较了多种有前景的知识迁移技术，揭示了保留VLM先验知识对于VLA泛化能力的关键重要性，并为未来开发真正可泛化的具身智能系统提供了指导。\n### 论文核心贡献点\n- 提出了一个最小化、可复现的基准，用于解构视觉-语义先验知识与运动技能，可作为诊断工具来评估VLM如何被有效地适配到VLA。\n- 在一个严格控制实验的框架内，对不同VLA预训练和微调技术进行了系统的比较分析。\n- 提供了基于实证的洞察，为未来开发可泛化具身智能体的研究方向提供指导，包括：VLM初始化、VLA预训练和VLA微调分别扮演着不同且互补的角色；全参数微调在狭窄任务上表现良好，但会导致灾难性遗忘；联合训练和预测潜在动作是更有前景的研究方向；在更多样化的数据集上进行VLA预训练可以提升性能。\n### 论文方法描述\n论文设计并比较了多种旨在改善VLA继承VLM先验知识的训练技术。核心方法是构建并使用GrinningFace基准进行评估。被比较的技术包括：\n- 基线方法：使用π0风格的模型进行全参数微调。\n- 参数高效微调：采用LoRA或仅微调动作专家模块。\n- 冻结VLM主干：在微调过程中冻结VLM的权重。\n- 联合训练：在VLA训练中同时加入视觉-语言任务，如在桌面场景中识别表情符号。\n- 离散化目标：训练VLA预测离散化的动作目标。\n- 潜在动作目标：训练VLA在预测机器人动作的同时，也预测潜在动作。\n- 多样化数据预训练：研究不同数据集（如OXE magic soup, bridge-v2）对VLA性能的影响。\n### 论文使用数据集和训练资源\n- 数据集：\n - GrinningFace基准：包含100个用于训练的表情符号和100个用于验证的表情符号，收集了500条轨迹用于微调。\n - VLA预训练数据集：主要使用Open X-Embodiment (OXE) magic-soup混合数据集（包含913k条轨迹）。实验中还使用了bridge-v2数据集以及排除了bridge-v2的OXE数据集，以进行消融研究。\n- 训练资源：\n - 计算资源：使用8块Nvidia A100 GPU进行VLA的预训练和微调，使用单块Nvidia A100 GPU进行评估。\n - 模型基础：代码库基于π0的开源实现，使用PaliGemma作为VLM主干，SigLIP作为视觉编码器。\n - 训练配置：VLA预训练80k步，微调30k步，批大小为1024。\n### 论文使用的评估环境和评估指标\n- 评估环境：\n - 模拟环境：ManiSkill3。\n - 真实机器人：搭载Inspire Robots夹爪的Realman RM75机械臂。真实机器人上的任务被简化为“触摸”指定表情符号。\n- 评估指标：\n - 核心公式：整体成功率 = 执行成功率 × 识别成功率。\n - 执行成功率：机器人成功拿起方块并放置到任意一个表情符号卡片上。\n - 识别成功率：机器人在三个候选卡片中选择了正确的那个。\n - 评估协议：\n - ID：评估模型在训练时见过的表情符号组合上的表现。\n - Train：评估模型在由训练集表情符号构成的新组合上的表现。\n - Val：评估模型在验证集表情符号上的表现，用于测试泛化能力。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Microsoft Research</p>\n<h3>论文概述</h3>\n<p>该论文探讨了视觉-语言-行动模型如何有效地从预训练的视觉-语言模型中继承知识。为了解决这一核心问题并应对训练过程中可能出现的灾难性遗忘，研究者们引入了一个名为GrinningFace的诊断基准。该基准是一个表情符号桌面操作任务，要求机器人根据语言指令将物体放置在对应的印刷表情符号上。由于表情符号在VLM的预训练数据中广泛存在，但在标准机器人数据集中几乎不存在，该任务的成功完成可以清晰地表明VLM的先验知识被有效迁移到了具身控制中。论文通过在模拟和真实机器人上进行系统性实验，比较了多种有前景的知识迁移技术，揭示了保留VLM先验知识对于VLA泛化能力的关键重要性，并为未来开发真正可泛化的具身智能系统提供了指导。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个最小化、可复现的基准，用于解构视觉-语义先验知识与运动技能，可作为诊断工具来评估VLM如何被有效地适配到VLA。</li><li>在一个严格控制实验的框架内，对不同VLA预训练和微调技术进行了系统的比较分析。</li><li>提供了基于实证的洞察，为未来开发可泛化具身智能体的研究方向提供指导，包括：VLM初始化、VLA预训练和VLA微调分别扮演着不同且互补的角色；全参数微调在狭窄任务上表现良好，但会导致灾难性遗忘；联合训练和预测潜在动作是更有前景的研究方向；在更多样化的数据集上进行VLA预训练可以提升性能。</li></ul>\n<h3>论文方法描述</h3>\n<p>论文设计并比较了多种旨在改善VLA继承VLM先验知识的训练技术。核心方法是构建并使用GrinningFace基准进行评估。被比较的技术包括：</p>\n<ul><li>基线方法：使用π0风格的模型进行全参数微调。</li><li>参数高效微调：采用LoRA或仅微调动作专家模块。</li><li>冻结VLM主干：在微调过程中冻结VLM的权重。</li><li>联合训练：在VLA训练中同时加入视觉-语言任务，如在桌面场景中识别表情符号。</li><li>离散化目标：训练VLA预测离散化的动作目标。</li><li>潜在动作目标：训练VLA在预测机器人动作的同时，也预测潜在动作。</li><li>多样化数据预训练：研究不同数据集（如OXE magic soup, bridge-v2）对VLA性能的影响。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：</li></ul>\n<p> - GrinningFace基准：包含100个用于训练的表情符号和100个用于验证的表情符号，收集了500条轨迹用于微调。</p>\n<p> - VLA预训练数据集：主要使用Open X-Embodiment (OXE) magic-soup混合数据集（包含913k条轨迹）。实验中还使用了bridge-v2数据集以及排除了bridge-v2的OXE数据集，以进行消融研究。</p>\n<ul><li>训练资源：</li></ul>\n<p> - 计算资源：使用8块Nvidia A100 GPU进行VLA的预训练和微调，使用单块Nvidia A100 GPU进行评估。</p>\n<p> - 模型基础：代码库基于π0的开源实现，使用PaliGemma作为VLM主干，SigLIP作为视觉编码器。</p>\n<p> - 训练配置：VLA预训练80k步，微调30k步，批大小为1024。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：</li></ul>\n<p> - 模拟环境：ManiSkill3。</p>\n<p> - 真实机器人：搭载Inspire Robots夹爪的Realman RM75机械臂。真实机器人上的任务被简化为“触摸”指定表情符号。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 核心公式：整体成功率 = 执行成功率 × 识别成功率。</p>\n<p> - 执行成功率：机器人成功拿起方块并放置到任意一个表情符号卡片上。</p>\n<p> - 识别成功率：机器人在三个候选卡片中选择了正确的那个。</p>\n<p> - 评估协议：</p>\n<p> - ID：评估模型在训练时见过的表情符号组合上的表现。</p>\n<p> - Train：评估模型在由训练集表情符号构成的新组合上的表现。</p>\n<p> - Val：评估模型在验证集表情符号上的表现，用于测试泛化能力。</p>"
  },
  {
    "date": "2025-11-09",
    "title": "ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval",
    "link": "http://arxiv.org/abs/2511.06202",
    "summary_markdown": "### 论文研究单位\n卡内基梅隆大学机器人研究所，美国匹兹堡\n### 论文概述\n论文提出了一种名为ExpReS-VLA的方法，旨在解决预训练的视觉-语言-动作（VLA）模型在特定部署环境中进行快速适应时遇到的灾难性遗忘和性能下降问题。该方法通过压缩的经验回放和检索增强生成技术，使模型能够在设备端利用少量演示（12个）快速适应新环境，同时保留原有能力。ExpReS-VLA通过存储视觉编码器的嵌入而非原始图像，实现了97%的存储空间节省，并结合检索到的相似经验和一种新的对比损失函数（THCL）来学习成功与失败的经验。实验表明，该方法在模拟和真实机器人任务上均显著提升了任务成功率。\n### 论文核心贡献点\nRAG增强的机器人学习：首次将检索机制集成到VLA微调中，加速了适应过程。\n压缩的经验回放：一种通过冻结视觉编码器实现97%内存减少的技术，在保持语义保真度的同时，实现了实际部署。\n用于失败利用的THCL：一种新颖的分段损失函数，通过动态选择合适的对比目标来防止重复性错误。\n严谨的实证评估：在40个模拟任务（5个随机种子）和5个物理操作任务（共150次试验）中进行了系统性消融实验，明确了各组件的贡献。\n### 论文方法描述\n该方法首先使用OpenVLA冻结的视觉编码器（融合SigLIP和DINOv2）从RGB图像中提取1024维的嵌入向量，以实现高效的内存存储。\n维护一个双缓冲区内存管理系统，分别存储成功和失败的经验轨迹，并采用FIFO策略和时序加权进行更新。\n在训练时，系统根据当前观察的嵌入，从缓冲区中检索余弦相似度最高的k个相关经验，并将其与当前数据混合构建训练批次。\n引入了阈值化混合对比损失（THCL），该损失结合了行为克隆损失和自适应的对比学习损失。当三元组损失低于阈值时使用三元组损失，否则使用InfoNCE损失，从而使模型能从成功和失败中学习。\n整个在线学习流程采用LoRA进行高效微调，在性能低于阈值时触发，并在单块消费级GPU（如RTX 5090）上快速完成。\n### 论文使用数据集和训练资源\n数据集：模拟实验使用了LIBERO基准测试，包含四个任务套件（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。真实机器人实验在7-DOF Franka Emika Panda机械臂上进行，包含5个操作任务。\n训练资源：所有实验均在单个NVIDIA RTX 5090 GPU（32GB内存）上完成，使用BFloat16混合精度。模型微调采用LoRA配置，仅训练98.3M个参数（占总数的1.4%）。适应过程仅需12个演示和31秒。\n### 论文使用的评估环境和评估指标\n评估环境：在LIBERO模拟基准和真实的Franka Emika Panda物理机器人上进行评估。物理机器人任务包括分布内和分布外（包含未见过的背景和物体）两种测试场景。\n评估指标：主要评估指标是任务成功率。模拟中，每个任务进行50次滚动测试，重复5个随机种子。物理机器人中，每个任务进行30次分布内试验和10次分布外试验。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>卡内基梅隆大学机器人研究所，美国匹兹堡</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为ExpReS-VLA的方法，旨在解决预训练的视觉-语言-动作（VLA）模型在特定部署环境中进行快速适应时遇到的灾难性遗忘和性能下降问题。该方法通过压缩的经验回放和检索增强生成技术，使模型能够在设备端利用少量演示（12个）快速适应新环境，同时保留原有能力。ExpReS-VLA通过存储视觉编码器的嵌入而非原始图像，实现了97%的存储空间节省，并结合检索到的相似经验和一种新的对比损失函数（THCL）来学习成功与失败的经验。实验表明，该方法在模拟和真实机器人任务上均显著提升了任务成功率。</p>\n<h3>论文核心贡献点</h3>\n<p>RAG增强的机器人学习：首次将检索机制集成到VLA微调中，加速了适应过程。</p>\n<p>压缩的经验回放：一种通过冻结视觉编码器实现97%内存减少的技术，在保持语义保真度的同时，实现了实际部署。</p>\n<p>用于失败利用的THCL：一种新颖的分段损失函数，通过动态选择合适的对比目标来防止重复性错误。</p>\n<p>严谨的实证评估：在40个模拟任务（5个随机种子）和5个物理操作任务（共150次试验）中进行了系统性消融实验，明确了各组件的贡献。</p>\n<h3>论文方法描述</h3>\n<p>该方法首先使用OpenVLA冻结的视觉编码器（融合SigLIP和DINOv2）从RGB图像中提取1024维的嵌入向量，以实现高效的内存存储。</p>\n<p>维护一个双缓冲区内存管理系统，分别存储成功和失败的经验轨迹，并采用FIFO策略和时序加权进行更新。</p>\n<p>在训练时，系统根据当前观察的嵌入，从缓冲区中检索余弦相似度最高的k个相关经验，并将其与当前数据混合构建训练批次。</p>\n<p>引入了阈值化混合对比损失（THCL），该损失结合了行为克隆损失和自适应的对比学习损失。当三元组损失低于阈值时使用三元组损失，否则使用InfoNCE损失，从而使模型能从成功和失败中学习。</p>\n<p>整个在线学习流程采用LoRA进行高效微调，在性能低于阈值时触发，并在单块消费级GPU（如RTX 5090）上快速完成。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：模拟实验使用了LIBERO基准测试，包含四个任务套件（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。真实机器人实验在7-DOF Franka Emika Panda机械臂上进行，包含5个操作任务。</p>\n<p>训练资源：所有实验均在单个NVIDIA RTX 5090 GPU（32GB内存）上完成，使用BFloat16混合精度。模型微调采用LoRA配置，仅训练98.3M个参数（占总数的1.4%）。适应过程仅需12个演示和31秒。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：在LIBERO模拟基准和真实的Franka Emika Panda物理机器人上进行评估。物理机器人任务包括分布内和分布外（包含未见过的背景和物体）两种测试场景。</p>\n<p>评估指标：主要评估指标是任务成功率。模拟中，每个任务进行50次滚动测试，重复5个随机种子。物理机器人中，每个任务进行30次分布内试验和10次分布外试验。</p>"
  },
  {
    "date": "2025-11-08",
    "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.05936",
    "summary_markdown": "论文研究单位\n新加坡的研究机构，包括A*STAR (Agency for Science, Technology and Research) 和南洋理工大学。\n\n论文概述\n本文是一篇综述性论文，旨在探讨视觉-语言-动作（VLA）模型未来发展所面临的10个开放性挑战。这些挑战涵盖了多模态感知、鲁棒推理、训练数据质量、模型评估、跨机器人动作泛化、资源效率、全身协调、安全保障、智能体框架以及人机协调等多个方面。此外，论文还讨论了应对这些挑战的新兴趋势，包括分层规划、空间理解、通用动作表示、世界动态建模、数据合成和后训练，以期推动VLA模型的发展和应用。\n\n论文核心贡献点\n1. 系统性地梳理并提出了VLA模型发展过程中的10个核心开放性挑战。\n2. 总结和分析了当前应对这些挑战的新兴技术趋势和研究方向。\n3. 提出了一个高层级的多智能体VLA规划框架（算法1），该框架集成了高层规划器、低层级动作专家、安全护栏和评估器，旨在实现更鲁棒和安全的任务执行。\n\n论文方法描述\n论文主要不是提出一个单一的新方法，而是对现有及未来可能的方法论进行总结和展望。\n1. **VLA基础方法**: 介绍了VLA模型的基本框架，即将视觉观测和语言指令结合以生成机器人动作。区分了两种主流的动作建模方法：离散动作模型（将连续动作量化为离散token，易于与Transformer结合，但可能损失精度）和连续动作模型（直接预测连续动作，如使用扩散模型，精度高但计算成本大）。\n2. **分层规划**: 建议使用大型语言模型（LLM）或视觉语言模型（VLM）作为高层级规划器，将复杂任务分解为子任务。低层级动作专家则负责根据子任务生成具体的机器人动作序列。在生成动作前加入推理步骤可以提高模型性能和可解释性。\n3. **空间理解**: 提出应增强VLA模型对3D空间的理解能力，例如通过融合来自深度相机或LiDAR的深度信息，或者利用专门的深度估计模块，以提升模型在物理世界中的操作鲁棒性。\n4. **通用动作表示**: 探讨了通过学习一个通用的原子动作表示（action codebook）来解决跨机器人动作泛化问题的可能性，使模型能够适应不同机器人的动作空间。\n5. **世界动态建模**: 强调了世界模型在预测动作结果和进行有效规划中的重要性，介绍了两种主要方法：一种是直接生成未来状态像素的生成模型，另一种是预测未来状态嵌入的嵌入预测模型（如JEPA）。\n6. **数据合成**: 建议利用视频生成模型大规模生成机器人执行任务的模拟视频数据，并通过世界模型从视频中提取“潜在动作”，以解决机器人数据采集成本高的问题。\n7. **后训练**: 提议借鉴LLM的成功经验，在预训练之后对VLA模型进行后训练。通过利用世界模型作为奖励模型或评估器，可以对生成的动作序列进行评估，并使用强化学习或偏好优化（如DPO）来微调模型，以提高其任务表现和安全性。\n\n论文使用数据集和训练资源\n论文讨论了现有研究中常用的数据集，而非为自身方法提供新数据集。\n1. **数据集**: 提到了`Open-X-Embodiment`数据集，它是一个包含超过一百万个机器人操作片段的大型统一数据集，涵盖了多种机器人、任务和场景。此外，也提到了`DROID`数据集。论文还提议通过视频生成模型合成训练数据。\n2. **训练资源**: 论文指出，训练大型VLA模型通常需要大量的计算资源。同时，对于部署在机器人上的模型，需要在模型容量和资源效率之间进行权衡，以适应机器人本体的计算和能源限制。\n\n论文使用的评估环境和评估指标\n论文本身未进行新的实验评估，而是对现有VLA模型的评估方法进行了分析和评述。\n1. **评估环境**: 指出当前评估主要在真实机器人（如WidowX, Franka）和模拟器（如LIBERO, SimplerEnv）中进行。论文强调了现有评估的局限性，如评估环境单一、模拟与现实之间存在差距。\n2. **评估指标**: 提到任务成功率是常用的评估指标。在讨论后训练时，提到了使用任务完成度、效率和最优性等作为奖励函数的可能标准。`SimplerEval`被提及作为一种通过引入分布偏移（如改变背景、光照）来增强评估鲁棒性的尝试。",
    "summary_html": "<p>论文研究单位</p>\n<p>新加坡的研究机构，包括A*STAR (Agency for Science, Technology and Research) 和南洋理工大学。</p>\n\n<p>论文概述</p>\n<p>本文是一篇综述性论文，旨在探讨视觉-语言-动作（VLA）模型未来发展所面临的10个开放性挑战。这些挑战涵盖了多模态感知、鲁棒推理、训练数据质量、模型评估、跨机器人动作泛化、资源效率、全身协调、安全保障、智能体框架以及人机协调等多个方面。此外，论文还讨论了应对这些挑战的新兴趋势，包括分层规划、空间理解、通用动作表示、世界动态建模、数据合成和后训练，以期推动VLA模型的发展和应用。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>系统性地梳理并提出了VLA模型发展过程中的10个核心开放性挑战。</li><li>总结和分析了当前应对这些挑战的新兴技术趋势和研究方向。</li><li>提出了一个高层级的多智能体VLA规划框架（算法1），该框架集成了高层规划器、低层级动作专家、安全护栏和评估器，旨在实现更鲁棒和安全的任务执行。</li></ol>\n\n<p>论文方法描述</p>\n<p>论文主要不是提出一个单一的新方法，而是对现有及未来可能的方法论进行总结和展望。</p>\n<ol><li><strong>VLA基础方法</strong>: 介绍了VLA模型的基本框架，即将视觉观测和语言指令结合以生成机器人动作。区分了两种主流的动作建模方法：离散动作模型（将连续动作量化为离散token，易于与Transformer结合，但可能损失精度）和连续动作模型（直接预测连续动作，如使用扩散模型，精度高但计算成本大）。</li><li><strong>分层规划</strong>: 建议使用大型语言模型（LLM）或视觉语言模型（VLM）作为高层级规划器，将复杂任务分解为子任务。低层级动作专家则负责根据子任务生成具体的机器人动作序列。在生成动作前加入推理步骤可以提高模型性能和可解释性。</li><li><strong>空间理解</strong>: 提出应增强VLA模型对3D空间的理解能力，例如通过融合来自深度相机或LiDAR的深度信息，或者利用专门的深度估计模块，以提升模型在物理世界中的操作鲁棒性。</li><li><strong>通用动作表示</strong>: 探讨了通过学习一个通用的原子动作表示（action codebook）来解决跨机器人动作泛化问题的可能性，使模型能够适应不同机器人的动作空间。</li><li><strong>世界动态建模</strong>: 强调了世界模型在预测动作结果和进行有效规划中的重要性，介绍了两种主要方法：一种是直接生成未来状态像素的生成模型，另一种是预测未来状态嵌入的嵌入预测模型（如JEPA）。</li><li><strong>数据合成</strong>: 建议利用视频生成模型大规模生成机器人执行任务的模拟视频数据，并通过世界模型从视频中提取“潜在动作”，以解决机器人数据采集成本高的问题。</li><li><strong>后训练</strong>: 提议借鉴LLM的成功经验，在预训练之后对VLA模型进行后训练。通过利用世界模型作为奖励模型或评估器，可以对生成的动作序列进行评估，并使用强化学习或偏好优化（如DPO）来微调模型，以提高其任务表现和安全性。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>论文讨论了现有研究中常用的数据集，而非为自身方法提供新数据集。</p>\n<ol><li><strong>数据集</strong>: 提到了<code>Open-X-Embodiment</code>数据集，它是一个包含超过一百万个机器人操作片段的大型统一数据集，涵盖了多种机器人、任务和场景。此外，也提到了<code>DROID</code>数据集。论文还提议通过视频生成模型合成训练数据。</li><li><strong>训练资源</strong>: 论文指出，训练大型VLA模型通常需要大量的计算资源。同时，对于部署在机器人上的模型，需要在模型容量和资源效率之间进行权衡，以适应机器人本体的计算和能源限制。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>论文本身未进行新的实验评估，而是对现有VLA模型的评估方法进行了分析和评述。</p>\n<ol><li><strong>评估环境</strong>: 指出当前评估主要在真实机器人（如WidowX, Franka）和模拟器（如LIBERO, SimplerEnv）中进行。论文强调了现有评估的局限性，如评估环境单一、模拟与现实之间存在差距。</li><li><strong>评估指标</strong>: 提到任务成功率是常用的评估指标。在讨论后训练时，提到了使用任务完成度、效率和最优性等作为奖励函数的可能标准。<code>SimplerEval</code>被提及作为一种通过引入分布偏移（如改变背景、光照）来增强评估鲁棒性的尝试。</li></ol>"
  },
  {
    "date": "2025-11-07",
    "title": "Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots",
    "link": "http://arxiv.org/abs/2511.05642",
    "summary_markdown": "# 论文研究单位\n- Department of Cyber‑Physical Systems, Clark Atlanta University (作者 1、2、3)\n- Siemens Corporation (作者 4)\n# 论文概述\n本文提出 **LiteVLA**，一种面向 CPU‑受限边缘机器人的轻量级视觉‑语言‑行动（VLA）框架。该系统将紧凑的 SmolVLM 主干、LoRA 参数高效微调、4‑bit NF4 量化以及 ROS 2 完整管线集成在 Raspberry Pi 4/TurtleBot 4 上，实现完全本地化的感知‑推理‑控制闭环。在无需云端资源的前提下，验证了在 CPU 上进行实时（或准实时）多模态推理的可行性，为无 GPS、带宽受限环境下的自治机器人提供思路。\n# 论文核心贡献点\n1. **CPU‑only 本体 VLA**：在 Raspberry Pi 4 上使用 GGUF‑量化的 VLA 策略并通过 ROS 2 实现异步控制，基线延迟约 11.1 s/查询（0.09 Hz）。\n2. **参数高效适配**：利用 LoRA（rank = 8，α = 8，dropout = 0.1）对 SmolVLM 主干进行任务专用微调，内存/算力需求极低。\n3. **边缘量化与稳定性**：采用 4‑bit NF4 主干 + FP32 投影头的混合精度，实现约 75 % 内存降低、最高 9× 推理加速，同时保持输出稳定。\n4. **端到端 ROS 2 管线**：统一的感知‑推理‑控制闭环，将 RGB 帧映射为结构化运动指令（geometry_msgs/Twist），使用 llama‑cpp 运行时。\n5. **可扩展路线图**：提出六阶段 EDGE‑VLA‑ROADMAP，从地面机器人逐步扩展至无人机、多智能体协同、多模态接地、持续/强化学习及联邦安全。\n# 论文方法描述\n- **系统架构**（Figure 1）\n - 数据采集节点 → 多模态推理节点（SmolVLM+LoRA） → ROS 2 控制节点。\n- **数据管道**（Algorithm 1）\n - 通过遥控采集 15 083 条 RGB‑动作对；时间戳对齐 → 归一化 → 224×224 缩放 → 随机水平翻转增强；训练/验证划分 0.85 : 0.15。\n- **LoRA 微调**\n - 在 SmolVLM 主干的 query/key/value/output/gating 层注入 rank = 8 的低秩矩阵，损失函数为预测动作与标注文本的交叉熵；约两轮在 CPU 后端完成。\n- **量化方案**\n - 主干使用 NF4（4‑bit）量化，投影层保持 FP32（混合精度），以避免动作输出的不稳定。\n- **推理与 ROS 2 集成**\n - llama‑cpp‑python 加载 GGUF 权重；推理输出形如 “forward_0.2_3.0s”，在 ROS 2 中解析为 Twist 消息；采用 Action Chunking 机制实现低层持续控制与高层推理的异步解耦。\n# 数据集与训练资源\n- **数据集**：遥控采集的 15 083 条 RGB‑动作对（包含线性/角速度及时间戳），每条指令在室内多环境下覆盖前、后、左、右、停止等类别，分布基本均衡。实际实验使用约 13 000 条（其中 1 152 条为后退指令，其余约 2 990 条/类）。\n- **预处理**：统一尺寸 224×224、像素标准化、随机水平翻转；时间戳对齐保证图像‑动作一一对应。\n- **训练资源**：全部微调在 CPU 后端完成（约两轮），未使用 GPU；模型量化与部署在 Raspberry Pi 4（ARM Cortex‑A72，1.5 GHz，4 GB RAM）上执行。\n# 评估环境与评估指标\n- **硬件平台**：Raspberry Pi 4（4 GB RAM）+ TurtleBot 4 机器人，ROS 2 环境。\n- **评估指标**\n - **推理延迟**：每条查询的平均耗时；FP32 SmolVLM‑256 基线 ≈ 11 s，LiteVLA（FP32）≈ 18 min，混合量化 ≈ 2 min（9× 加速），全 NF4 ≈ 1.5 min（输出不稳定）。\n - **内存占用**：混合量化较 FP32 主干降低约 75 %。\n - **系统吞吐量**：推理频率约 0.09 Hz；控制层通过 Action Chunking 实现低层持续运动。\n - **输出稳定性**：全 NF4 量化导致动作输出出现幻觉，混合精度保持稳定。\n- **对比实验**：将 LiteVLA 与原始 SmolVLM‑256（FP32）以及未量化的 LiteVLA 进行横向比较，量化显著降低时延且维持可接受的控制质量。\n- **稳健性讨论**：在高温或长时运行情况下出现热降频，导致延迟波动；CPU 竞争（图像采集、预处理、推理）亦产生延迟峰值。\n\n以上内容概括了论文的研究单位、整体思路、核心贡献、技术路线、实验数据及评测细节。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>Department of Cyber‑Physical Systems, Clark Atlanta University (作者 1、2、3)</li><li>Siemens Corporation (作者 4)</li></ul>\n<h1>论文概述</h1>\n<p>本文提出 <strong>LiteVLA</strong>，一种面向 CPU‑受限边缘机器人的轻量级视觉‑语言‑行动（VLA）框架。该系统将紧凑的 SmolVLM 主干、LoRA 参数高效微调、4‑bit NF4 量化以及 ROS 2 完整管线集成在 Raspberry Pi 4/TurtleBot 4 上，实现完全本地化的感知‑推理‑控制闭环。在无需云端资源的前提下，验证了在 CPU 上进行实时（或准实时）多模态推理的可行性，为无 GPS、带宽受限环境下的自治机器人提供思路。</p>\n<h1>论文核心贡献点</h1>\n<ol><li><strong>CPU‑only 本体 VLA</strong>：在 Raspberry Pi 4 上使用 GGUF‑量化的 VLA 策略并通过 ROS 2 实现异步控制，基线延迟约 11.1 s/查询（0.09 Hz）。</li><li><strong>参数高效适配</strong>：利用 LoRA（rank = 8，α = 8，dropout = 0.1）对 SmolVLM 主干进行任务专用微调，内存/算力需求极低。</li><li><strong>边缘量化与稳定性</strong>：采用 4‑bit NF4 主干 + FP32 投影头的混合精度，实现约 75 % 内存降低、最高 9× 推理加速，同时保持输出稳定。</li><li><strong>端到端 ROS 2 管线</strong>：统一的感知‑推理‑控制闭环，将 RGB 帧映射为结构化运动指令（geometry_msgs/Twist），使用 llama‑cpp 运行时。</li><li><strong>可扩展路线图</strong>：提出六阶段 EDGE‑VLA‑ROADMAP，从地面机器人逐步扩展至无人机、多智能体协同、多模态接地、持续/强化学习及联邦安全。</li></ol>\n<h1>论文方法描述</h1>\n<ul><li><strong>系统架构</strong>（Figure 1）</li></ul>\n<p> - 数据采集节点 → 多模态推理节点（SmolVLM+LoRA） → ROS 2 控制节点。</p>\n<ul><li><strong>数据管道</strong>（Algorithm 1）</li></ul>\n<p> - 通过遥控采集 15 083 条 RGB‑动作对；时间戳对齐 → 归一化 → 224×224 缩放 → 随机水平翻转增强；训练/验证划分 0.85 : 0.15。</p>\n<ul><li><strong>LoRA 微调</strong></li></ul>\n<p> - 在 SmolVLM 主干的 query/key/value/output/gating 层注入 rank = 8 的低秩矩阵，损失函数为预测动作与标注文本的交叉熵；约两轮在 CPU 后端完成。</p>\n<ul><li><strong>量化方案</strong></li></ul>\n<p> - 主干使用 NF4（4‑bit）量化，投影层保持 FP32（混合精度），以避免动作输出的不稳定。</p>\n<ul><li><strong>推理与 ROS 2 集成</strong></li></ul>\n<p> - llama‑cpp‑python 加载 GGUF 权重；推理输出形如 “forward_0.2_3.0s”，在 ROS 2 中解析为 Twist 消息；采用 Action Chunking 机制实现低层持续控制与高层推理的异步解耦。</p>\n<h1>数据集与训练资源</h1>\n<ul><li><strong>数据集</strong>：遥控采集的 15 083 条 RGB‑动作对（包含线性/角速度及时间戳），每条指令在室内多环境下覆盖前、后、左、右、停止等类别，分布基本均衡。实际实验使用约 13 000 条（其中 1 152 条为后退指令，其余约 2 990 条/类）。</li><li><strong>预处理</strong>：统一尺寸 224×224、像素标准化、随机水平翻转；时间戳对齐保证图像‑动作一一对应。</li><li><strong>训练资源</strong>：全部微调在 CPU 后端完成（约两轮），未使用 GPU；模型量化与部署在 Raspberry Pi 4（ARM Cortex‑A72，1.5 GHz，4 GB RAM）上执行。</li></ul>\n<h1>评估环境与评估指标</h1>\n<ul><li><strong>硬件平台</strong>：Raspberry Pi 4（4 GB RAM）+ TurtleBot 4 机器人，ROS 2 环境。</li><li><strong>评估指标</strong></li></ul>\n<p> - <strong>推理延迟</strong>：每条查询的平均耗时；FP32 SmolVLM‑256 基线 ≈ 11 s，LiteVLA（FP32）≈ 18 min，混合量化 ≈ 2 min（9× 加速），全 NF4 ≈ 1.5 min（输出不稳定）。</p>\n<p> - <strong>内存占用</strong>：混合量化较 FP32 主干降低约 75 %。</p>\n<p> - <strong>系统吞吐量</strong>：推理频率约 0.09 Hz；控制层通过 Action Chunking 实现低层持续运动。</p>\n<p> - <strong>输出稳定性</strong>：全 NF4 量化导致动作输出出现幻觉，混合精度保持稳定。</p>\n<ul><li><strong>对比实验</strong>：将 LiteVLA 与原始 SmolVLM‑256（FP32）以及未量化的 LiteVLA 进行横向比较，量化显著降低时延且维持可接受的控制质量。</li><li><strong>稳健性讨论</strong>：在高温或长时运行情况下出现热降频，导致延迟波动；CPU 竞争（图像采集、预处理、推理）亦产生延迟峰值。</li></ul>\n\n<p>以上内容概括了论文的研究单位、整体思路、核心贡献、技术路线、实验数据及评测细节。</p>"
  },
  {
    "date": "2025-11-07",
    "title": "EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.05397",
    "summary_markdown": "# 论文研究单位\n\n匹兹堡大学（University of Pittsburgh）\n# 论文概述\n\nEveryDayVLA是一个面向可负担机器人操作的视觉-语言-动作模型。该研究结合了低成本硬件（300美元6自由度机械臂）和先进的VLA模型，通过协作训练预测离散和连续动作，并引入自适应地平线集成器（AdaHorizon）来提高实时操作的安全性和可靠性。在LIBERO仿真基准测试中达到了最先进的性能，在真实世界测试中在分布内场景领先49%，在分布外场景领先34.9%。\n# 论文核心贡献点\n\n1. **协作训练与自适应地平线控制（AdaHorizon）**：联合训练连续（L1回归）和离散自回归动作头，使用分歧估计模型不确定性，在严格实时约束下动态调整动作范围触发重规划\n\n2. **低成本集成6自由度机械臂**：300美元设计达到优于10mm重复精度，利用Arduino Uno分线板和PCA9685 PWM驱动器进行12位PWM控制\n\n3. **自动化数据收集流水线**：简化遥操作收集带语言指令、视频和末端执行器轨迹的数据集，发布超过1200个任务执行以实现跨环境可扩展微调\n# 论文方法描述\n\n基于Prismatic-7B VLM构建，使用SigLIP和DinoV2双部分视觉编码器，Llama 2语言模型作为主干。采用协作训练策略联合预测连续和离散动作块，连续动作通过多层感知器（MLP）动作头输出，离散动作通过256-bin离散化处理并使用softmax获得概率分布。损失函数结合交叉熵损失和L1损失（权重λ=1）平衡优化。\n\nAdaHorizon算法通过计算连续和离散动作预测之间的平均绝对差异作为不确定性度量，动态调整执行的动作块长度。当预测分歧超过阈值时触发重规划，在实时约束下最小执行长度为4个动作。\n# 论文使用数据集和训练资源\n\n- **数据集**：1200个演示的定制数据集，包含RGB观察序列、对应末端执行器姿态和自然语言指令，涵盖多种桌面环境，包括抓取放置、环境操作和块堆叠任务\n- **训练资源**：仿真实验在2个A100 GPU上微调10万次迭代，真实世界实验在1个A100 GPU上微调5万次迭代；使用LoRA（rank=32）、batch size=8和4步梯度累积\n# 论文使用的评估环境和评估指标\n\n- **仿真基准**：LIBERO四个任务套件（空间、物体、目标、长期），使用成功率（SR）作为主要评估指标\n- **真实世界评估**：分布内和分布外场景测试，包括静态和动态干扰物评估；成功率作为主要指标\n- **推理性能**：在LIBERO上测量推理频率（Hz）和延迟（秒）\n- **对比基线**：与Diffusion Policy、Octo、DiT Policy、OpenVLA、OpenVLA-OFT、ACT、HybridVLA、COGAct等方法进行比较",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>匹兹堡大学（University of Pittsburgh）</p>\n<h1>论文概述</h1>\n\n<p>EveryDayVLA是一个面向可负担机器人操作的视觉-语言-动作模型。该研究结合了低成本硬件（300美元6自由度机械臂）和先进的VLA模型，通过协作训练预测离散和连续动作，并引入自适应地平线集成器（AdaHorizon）来提高实时操作的安全性和可靠性。在LIBERO仿真基准测试中达到了最先进的性能，在真实世界测试中在分布内场景领先49%，在分布外场景领先34.9%。</p>\n<h1>论文核心贡献点</h1>\n\n<p>1. <strong>协作训练与自适应地平线控制（AdaHorizon）</strong>：联合训练连续（L1回归）和离散自回归动作头，使用分歧估计模型不确定性，在严格实时约束下动态调整动作范围触发重规划</p>\n\n<p>2. <strong>低成本集成6自由度机械臂</strong>：300美元设计达到优于10mm重复精度，利用Arduino Uno分线板和PCA9685 PWM驱动器进行12位PWM控制</p>\n\n<p>3. <strong>自动化数据收集流水线</strong>：简化遥操作收集带语言指令、视频和末端执行器轨迹的数据集，发布超过1200个任务执行以实现跨环境可扩展微调</p>\n<h1>论文方法描述</h1>\n\n<p>基于Prismatic-7B VLM构建，使用SigLIP和DinoV2双部分视觉编码器，Llama 2语言模型作为主干。采用协作训练策略联合预测连续和离散动作块，连续动作通过多层感知器（MLP）动作头输出，离散动作通过256-bin离散化处理并使用softmax获得概率分布。损失函数结合交叉熵损失和L1损失（权重λ=1）平衡优化。</p>\n\n<p>AdaHorizon算法通过计算连续和离散动作预测之间的平均绝对差异作为不确定性度量，动态调整执行的动作块长度。当预测分歧超过阈值时触发重规划，在实时约束下最小执行长度为4个动作。</p>\n<h1>论文使用数据集和训练资源</h1>\n\n<ul><li><strong>数据集</strong>：1200个演示的定制数据集，包含RGB观察序列、对应末端执行器姿态和自然语言指令，涵盖多种桌面环境，包括抓取放置、环境操作和块堆叠任务</li><li><strong>训练资源</strong>：仿真实验在2个A100 GPU上微调10万次迭代，真实世界实验在1个A100 GPU上微调5万次迭代；使用LoRA（rank=32）、batch size=8和4步梯度累积</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<ul><li><strong>仿真基准</strong>：LIBERO四个任务套件（空间、物体、目标、长期），使用成功率（SR）作为主要评估指标</li><li><strong>真实世界评估</strong>：分布内和分布外场景测试，包括静态和动态干扰物评估；成功率作为主要指标</li><li><strong>推理性能</strong>：在LIBERO上测量推理频率（Hz）和延迟（秒）</li><li><strong>对比基线</strong>：与Diffusion Policy、Octo、DiT Policy、OpenVLA、OpenVLA-OFT、ACT、HybridVLA、COGAct等方法进行比较</li></ul>"
  },
  {
    "date": "2025-11-07",
    "title": "TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.05275",
    "summary_markdown": "### 论文研究单位\n延世大学人工智能系（Yonsei University, Department of Artificial Intelligence），微软研究院（Microsoft Research）。\n### 论文概述\nVision-Language-Action模型（VLAs）在单臂机器人操控中表现出色，但双臂操控因缺乏大规模公开数据而面临挑战。本研究提出TwinVLA，通过组合两个预训练单臂VLA模型实现数据高效的双臂操控。TwinVLA避免了大量双臂预训练数据的依赖，仅需少量双臂演示即可在真实世界和仿真任务中达到或超越单体模型（如RDT-1B）的性能，并接近状态最先进的π_0模型。\n### 论文核心贡献点\n- 提出模块化双臂架构：通过复制预训练单臂VLA并结合联合注意力机制，实现两臂协调控制。\n- 数据高效微调范式：仅使用小量双臂数据（每任务约50演示）进行微调，无需额外双臂预训练。\n- 性能验证：在真实世界（Anubis机器人）和仿真（RoboTwin 2.0、Tabletop-Sim）任务中，TwinVLA在成功率上优于或匹配RDT-1B和DP方法，接近π_0模型。\n### 论文方法描述\nTwinVLA基于以下三大核心组件：\n1. **单臂策略复制**：复制预训练单臂VLA的VLM骨干（仅复制1.3B参数），共享视觉编码器和DiT动作头。左右臂各有一个轻量级本体感觉编码器。\n2. **联合注意力机制**：共享两个VLM间的自注意力层，实现跨臂信息融合，采用因果联合注意力掩码保持时序性和对称交互。\n3. **混合专家（MoE）集成**：通过MoE高效处理共享输入（如语言和自我视角图像），减少计算开销。同时，采用任务算术和注意力重加权技术保留预训练知识并加速微调。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：OXE数据集的0.5M轨迹子集。\n - 微调：真实世界任务每任务收集50集（绝对末端执行器控制），仿真任务每任务50集（如RoboTwin 2.0和Tabletop-Sim）。\n- **训练资源**：\n - SingleVLA预训练：5×H100 GPUs，耗时5天，120k步。\n - TwinVLA微调：1×L40S GPU，耗时2天，100k步。\n - 计算总量：约25 H100 GPU天（RDT-1B需超1,000 H100 GPU天）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **真实世界**：使用Anubis双臂机器人，任务包括“carrot to bag”（胡萝卜入袋）、“brush to dustpan”（刷子入簸箕）、“take towel off”（取下毛巾）。\n - **仿真**：\n - RoboTwin 2.0：50个双臂任务（Easy和Hard设置）。\n - Tabletop-Sim：5个任务（如“dish-drainer”（碗碟架）、“handover-box”（传递盒子））。\n- **评估指标**：\n - 主要指标为任务成功率（Success Rate），在每任务20-500次 rollout 中计算平均成功率。\n - 语言跟随测试：多任务组合指令（如“put X box into Y pot”）下的平均成功率。\n - 数据效率评估：随演示数量（20、35、50）变化的成功率曲线。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>延世大学人工智能系（Yonsei University, Department of Artificial Intelligence），微软研究院（Microsoft Research）。</p>\n<h3>论文概述</h3>\n<p>Vision-Language-Action模型（VLAs）在单臂机器人操控中表现出色，但双臂操控因缺乏大规模公开数据而面临挑战。本研究提出TwinVLA，通过组合两个预训练单臂VLA模型实现数据高效的双臂操控。TwinVLA避免了大量双臂预训练数据的依赖，仅需少量双臂演示即可在真实世界和仿真任务中达到或超越单体模型（如RDT-1B）的性能，并接近状态最先进的π_0模型。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出模块化双臂架构：通过复制预训练单臂VLA并结合联合注意力机制，实现两臂协调控制。</li><li>数据高效微调范式：仅使用小量双臂数据（每任务约50演示）进行微调，无需额外双臂预训练。</li><li>性能验证：在真实世界（Anubis机器人）和仿真（RoboTwin 2.0、Tabletop-Sim）任务中，TwinVLA在成功率上优于或匹配RDT-1B和DP方法，接近π_0模型。</li></ul>\n<h3>论文方法描述</h3>\n<p>TwinVLA基于以下三大核心组件：</p>\n<ol><li><strong>单臂策略复制</strong>：复制预训练单臂VLA的VLM骨干（仅复制1.3B参数），共享视觉编码器和DiT动作头。左右臂各有一个轻量级本体感觉编码器。</li><li><strong>联合注意力机制</strong>：共享两个VLM间的自注意力层，实现跨臂信息融合，采用因果联合注意力掩码保持时序性和对称交互。</li><li><strong>混合专家（MoE）集成</strong>：通过MoE高效处理共享输入（如语言和自我视角图像），减少计算开销。同时，采用任务算术和注意力重加权技术保留预训练知识并加速微调。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：OXE数据集的0.5M轨迹子集。</p>\n<p> - 微调：真实世界任务每任务收集50集（绝对末端执行器控制），仿真任务每任务50集（如RoboTwin 2.0和Tabletop-Sim）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - SingleVLA预训练：5×H100 GPUs，耗时5天，120k步。</p>\n<p> - TwinVLA微调：1×L40S GPU，耗时2天，100k步。</p>\n<p> - 计算总量：约25 H100 GPU天（RDT-1B需超1,000 H100 GPU天）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>真实世界</strong>：使用Anubis双臂机器人，任务包括“carrot to bag”（胡萝卜入袋）、“brush to dustpan”（刷子入簸箕）、“take towel off”（取下毛巾）。</p>\n<p> - <strong>仿真</strong>：</p>\n<p> - RoboTwin 2.0：50个双臂任务（Easy和Hard设置）。</p>\n<p> - Tabletop-Sim：5个任务（如“dish-drainer”（碗碟架）、“handover-box”（传递盒子））。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标为任务成功率（Success Rate），在每任务20-500次 rollout 中计算平均成功率。</p>\n<p> - 语言跟随测试：多任务组合指令（如“put X box into Y pot”）下的平均成功率。</p>\n<p> - 数据效率评估：随演示数量（20、35、50）变化的成功率曲线。</p>"
  },
  {
    "date": "2025-11-06",
    "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment",
    "link": "http://arxiv.org/abs/2511.04555",
    "summary_markdown": "```markdown\n论文研究单位\n上海交通大学人工智能学院, EvoMind Tech, IAAR-Shanghai, SII, 卡内基梅隆大学, 剑桥大学, 南洋理工大学\n\n论文概述\n当前视觉-语言-动作模型通常参数量巨大，且严重依赖大规模机器人数据预训练，导致训练和推理成本高昂，部署困难。同时，许多训练范式会破坏视觉-语言主干的感知表示，导致过拟合和泛化能力差。该论文提出了Evo-1，一个轻量级的VLA模型，旨在降低计算成本并提高部署效率，同时无需机器人数据预训练即可保持强大性能。Evo-1基于一个原生的多模态VLM，结合了新颖的交叉调制扩散Transformer和一个优化的集成模块。此外，论文还引入了一个两阶段训练范式，逐步对齐动作与感知，从而保留了VLM的表示能力。Evo-1仅有7.7亿参数，在Meta-World和RoboTwin基准上取得了最先进的结果，并在LIBERO上达到了94.8%的竞争性结果。在真实世界评估中，Evo-1实现了78%的成功率，并具有高推理频率和低内存开销。\n\n论文核心贡献点\n1. 轻量高效的架构：提出了一个仅有0.77B参数的轻量级VLA架构，降低了训练成本，并提高了在消费级GPU上的实时部署推理速度。\n2. 语义保留以提升泛化能力：引入了一种两阶段训练范式，在保留VLM固有视觉-语言理解的同时，使其适应下游动作生成，有效增强了跨多种操作任务的泛化能力。\n3. 无需预训练的优异性能：广泛的仿真和真实世界实验表明，Evo-1在没有依赖大规模机器人数据预训练的情况下，实现了最先进的性能，显著减少了对昂贵且劳动密集型数据收集的需求。\n\n论文方法描述\nEvo-1是一个模块化的视觉-语言-动作模型，主要由三部分组成：\n1. 视觉-语言主干：采用InternVL3-1B模型，该模型通过原生多模态范式进行预训练。视觉编码器为InternViT-300M，语言分支为Qwen2.5-0.5B。视觉和语言特征通过将图像块嵌入插入到标记序列中进行融合。\n2. 交叉调制扩散Transformer：作为动作专家，它是一个基于流匹配范式的条件去噪模块。该模块是一个扩散Transformer，仅依赖堆叠的交叉注意力层来生成连续的控制动作序列。\n3. 集成模块：采用基于交叉注意力的模块来融合多模态表示和机器人本体感觉信息。具体来说，将从VLM主干提取的多模态表示与机器人状态连接，然后作为后续动作专家Transformer的键值输入。\n模型采用两阶段训练范式：\n1. 动作专家对齐：冻结视觉-语言主干，仅训练集成模块和动作专家，使其与预训练的多模态嵌入空间对齐。\n2. 全规模微调：解冻VLM主干，对整个架构进行端到端的微调，实现感知与控制的深度整合。\n\n论文使用数据集和训练资源\n数据集：\n- 仿真环境：Meta-World benchmark (包含50个任务，分为easy, medium, hard, very-hard四个难度), LIBERO benchmark (包含spatial, object, goal, long四个类别的40个任务), RoboTwin benchmark (包含Click AlarmClock, Dump Bin BigBin, Place Bread Basket, Place Can Basket四个双臂任务)。\n- 真实世界环境：使用6-DoF xArm6机械臂和四种操作任务（Pick and Place Can, Pour Foam from Cup, Hand Delivery, Can Stacking），每个任务收集100次遥操作演示用于训练。\n训练资源：论文提到模型设计旨在消费级GPU上进行低成本训练和实时部署。真实世界推理效率评估在RTX 4090d GPU上进行，模型参数量为0.77B。\n\n论文使用的评估环境和评估指标\n评估环境：\n- 仿真环境：Meta-World, LIBERO, 和 RoboTwin。\n- 真实世界环境：配备平行夹爪的xArm6机械臂，在四个自定义任务上进行评估。\n- 泛化实验：在真实世界的“Pick and Place Can”任务中，通过加入未见过的干扰物、改变背景颜色、改变目标位置和高度四种方式测试模型的泛化能力。\n评估指标：\n- 任务成功率：主要评估指标是任务成功率。在仿真环境中，报告各子任务或难度级别的平均成功率及总体平均成功率。在真实世界环境中，报告各任务的平均成功率及总体平均成功率。\n- 推理效率：在真实世界实验中，还比较了模型的参数量、GPU内存占用和推理频率。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位</p>\n<p>上海交通大学人工智能学院, EvoMind Tech, IAAR-Shanghai, SII, 卡内基梅隆大学, 剑桥大学, 南洋理工大学</p>\n\n<p>论文概述</p>\n<p>当前视觉-语言-动作模型通常参数量巨大，且严重依赖大规模机器人数据预训练，导致训练和推理成本高昂，部署困难。同时，许多训练范式会破坏视觉-语言主干的感知表示，导致过拟合和泛化能力差。该论文提出了Evo-1，一个轻量级的VLA模型，旨在降低计算成本并提高部署效率，同时无需机器人数据预训练即可保持强大性能。Evo-1基于一个原生的多模态VLM，结合了新颖的交叉调制扩散Transformer和一个优化的集成模块。此外，论文还引入了一个两阶段训练范式，逐步对齐动作与感知，从而保留了VLM的表示能力。Evo-1仅有7.7亿参数，在Meta-World和RoboTwin基准上取得了最先进的结果，并在LIBERO上达到了94.8%的竞争性结果。在真实世界评估中，Evo-1实现了78%的成功率，并具有高推理频率和低内存开销。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>轻量高效的架构：提出了一个仅有0.77B参数的轻量级VLA架构，降低了训练成本，并提高了在消费级GPU上的实时部署推理速度。</li><li>语义保留以提升泛化能力：引入了一种两阶段训练范式，在保留VLM固有视觉-语言理解的同时，使其适应下游动作生成，有效增强了跨多种操作任务的泛化能力。</li><li>无需预训练的优异性能：广泛的仿真和真实世界实验表明，Evo-1在没有依赖大规模机器人数据预训练的情况下，实现了最先进的性能，显著减少了对昂贵且劳动密集型数据收集的需求。</li></ol>\n\n<p>论文方法描述</p>\n<p>Evo-1是一个模块化的视觉-语言-动作模型，主要由三部分组成：</p>\n<ol><li>视觉-语言主干：采用InternVL3-1B模型，该模型通过原生多模态范式进行预训练。视觉编码器为InternViT-300M，语言分支为Qwen2.5-0.5B。视觉和语言特征通过将图像块嵌入插入到标记序列中进行融合。</li><li>交叉调制扩散Transformer：作为动作专家，它是一个基于流匹配范式的条件去噪模块。该模块是一个扩散Transformer，仅依赖堆叠的交叉注意力层来生成连续的控制动作序列。</li><li>集成模块：采用基于交叉注意力的模块来融合多模态表示和机器人本体感觉信息。具体来说，将从VLM主干提取的多模态表示与机器人状态连接，然后作为后续动作专家Transformer的键值输入。</li></ol>\n<p>模型采用两阶段训练范式：</p>\n<ol><li>动作专家对齐：冻结视觉-语言主干，仅训练集成模块和动作专家，使其与预训练的多模态嵌入空间对齐。</li><li>全规模微调：解冻VLM主干，对整个架构进行端到端的微调，实现感知与控制的深度整合。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<ul><li>仿真环境：Meta-World benchmark (包含50个任务，分为easy, medium, hard, very-hard四个难度), LIBERO benchmark (包含spatial, object, goal, long四个类别的40个任务), RoboTwin benchmark (包含Click AlarmClock, Dump Bin BigBin, Place Bread Basket, Place Can Basket四个双臂任务)。</li><li>真实世界环境：使用6-DoF xArm6机械臂和四种操作任务（Pick and Place Can, Pour Foam from Cup, Hand Delivery, Can Stacking），每个任务收集100次遥操作演示用于训练。</li></ul>\n<p>训练资源：论文提到模型设计旨在消费级GPU上进行低成本训练和实时部署。真实世界推理效率评估在RTX 4090d GPU上进行，模型参数量为0.77B。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ul><li>仿真环境：Meta-World, LIBERO, 和 RoboTwin。</li><li>真实世界环境：配备平行夹爪的xArm6机械臂，在四个自定义任务上进行评估。</li><li>泛化实验：在真实世界的“Pick and Place Can”任务中，通过加入未见过的干扰物、改变背景颜色、改变目标位置和高度四种方式测试模型的泛化能力。</li></ul>\n<p>评估指标：</p>\n<ul><li>任务成功率：主要评估指标是任务成功率。在仿真环境中，报告各子任务或难度级别的平均成功率及总体平均成功率。在真实世界环境中，报告各任务的平均成功率及总体平均成功率。</li><li>推理效率：在真实世界实验中，还比较了模型的参数量、GPU内存占用和推理频率。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-11-06",
    "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies",
    "link": "http://arxiv.org/abs/2511.04357",
    "summary_markdown": "论文研究单位\nUmeå University (瑞典), PrioriAnalytica (澳大利亚), Bretagne INP - ENIB (法国), IMT Atlantique (法国), CNRS IRL 2010 CROSSING (澳大利亚)\n\n论文概述\n论文提出了一种名为GraSP-VLA的神经-符号框架，旨在解决现有视觉-语言-动作(VLA)模型在长时程任务中缺乏高层级符号规划，以及符号方法(AML)在泛化和可扩展性方面的局限。该方法通过连续场景图表示从人类演示中生成符号表示，并利用该表示在推理时自动生成新的规划域（PDDL格式），同时作为底层VLA策略的编排器，从而提升机器人在长时程任务中的性能。\n\n论文核心贡献点\n1. 提出了一种新的场景图生成(SGG)方法，将场景图扩展为多层连续场景图。\n2. 提出了一种从连续场景图自动生成规划域的新算法。\n3. 提出了一种基于连续场景图的编排器，用于将任务分解为原子行为序列。\n4. 设计了一个客户端-服务器执行架构，使用一个预训练VLA策略库。\n\n论文方法描述\n该方法分为两个阶段：\n阶段一（任务建模）：从单个演示中提取动作描述。首先，使用一个多层场景图生成模型（REACT），该模型区分功能、拓扑、物理部分-整体和属性四种关系类型。然后，通过多目标跟踪算法（OC-SORT）将场景图随时间聚合成连续场景图作为智能体的内部记忆，其中节点和边随时间持续，并采用状态精炼机制和置信度更新来增强鲁棒性。最后，通过比较动作前后连续场景图中拓扑层的变化来识别由代理（如人）发起的功能性关系，并自动提取出对应的PDDL动作（参数、前置条件、效果）。\n\n阶段二（任务再现）：再现完整的任务。动作编排器根据当前连续场景图状态验证PDDL动作的前置条件，若满足则通过客户端-服务器通信调用策略库中对应的预训练低层级VLA策略（如“拿起桌上的刀”），以原子行为为单位再现整个任务。\n\n论文使用数据集和训练资源\n数据集：\n- IndoorVG：用于训练场景图生成(SGG)主干网络，包含84个物体类和34个谓词类。\n- DAHLIA (DAily Home LIfe Activity)：用于评估连续场景图和动作描述生成，包含44个家庭日常活动视频。\n- 自定义真实世界数据集：包含20个摆餐具任务演示，用于训练VLA策略。\n\n训练资源/模型：\n- SGG主干：基于YOLOv8m的REACT模型。\n- VLA策略：基于SmolVLA模型，针对6个原子行为（拿起叉/刀/勺，放置在左/右/内侧）分别进行微调，每个策略训练20,000步。\n- 硬件：训练和推理在配备NVIDIA RTX 3080 GPU的计算机上进行。\n\n论文使用的评估环境和评估指标\n评估环境：\n- SGG评估：在IndoorVG数据集上进行离线评估。\n- 动作生成评估：在DAHLIA数据集上进行离线评估。\n- 完整系统评估：在真实世界环境中使用SO-101机械臂进行，任务为摆设餐桌，复杂度从1个到6个连续的拾取-放置动作不等。\n\n评估指标：\n- SGG性能：Recall@K (R@K), mean Recall@K (mR@K) (K=[20,50,100]), mAP@50, 以及延迟。\n- 动作生成评估：通过人工判断生成PDDL动作的正确性，计算召回率（正确动作数/总生成动作数）。\n- 完整系统性能：\n - 单个策略准确率：每个原子VLA策略执行10次的成功率。\n - 整体任务准确率：不同长度（2、4、6个技能）长时程任务的成功率。\n - 动作生成准确率：在真实世界中生成正确PDDL动作的准确率。\n- 基线比较：将GraSP-VLA的整体准确率与对SmolVLA进行端到端微调的基线进行比较。",
    "summary_html": "<p>论文研究单位</p>\n<p>Umeå University (瑞典), PrioriAnalytica (澳大利亚), Bretagne INP - ENIB (法国), IMT Atlantique (法国), CNRS IRL 2010 CROSSING (澳大利亚)</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为GraSP-VLA的神经-符号框架，旨在解决现有视觉-语言-动作(VLA)模型在长时程任务中缺乏高层级符号规划，以及符号方法(AML)在泛化和可扩展性方面的局限。该方法通过连续场景图表示从人类演示中生成符号表示，并利用该表示在推理时自动生成新的规划域（PDDL格式），同时作为底层VLA策略的编排器，从而提升机器人在长时程任务中的性能。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种新的场景图生成(SGG)方法，将场景图扩展为多层连续场景图。</li><li>提出了一种从连续场景图自动生成规划域的新算法。</li><li>提出了一种基于连续场景图的编排器，用于将任务分解为原子行为序列。</li><li>设计了一个客户端-服务器执行架构，使用一个预训练VLA策略库。</li></ol>\n\n<p>论文方法描述</p>\n<p>该方法分为两个阶段：</p>\n<p>阶段一（任务建模）：从单个演示中提取动作描述。首先，使用一个多层场景图生成模型（REACT），该模型区分功能、拓扑、物理部分-整体和属性四种关系类型。然后，通过多目标跟踪算法（OC-SORT）将场景图随时间聚合成连续场景图作为智能体的内部记忆，其中节点和边随时间持续，并采用状态精炼机制和置信度更新来增强鲁棒性。最后，通过比较动作前后连续场景图中拓扑层的变化来识别由代理（如人）发起的功能性关系，并自动提取出对应的PDDL动作（参数、前置条件、效果）。</p>\n\n<p>阶段二（任务再现）：再现完整的任务。动作编排器根据当前连续场景图状态验证PDDL动作的前置条件，若满足则通过客户端-服务器通信调用策略库中对应的预训练低层级VLA策略（如“拿起桌上的刀”），以原子行为为单位再现整个任务。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<ul><li>IndoorVG：用于训练场景图生成(SGG)主干网络，包含84个物体类和34个谓词类。</li><li>DAHLIA (DAily Home LIfe Activity)：用于评估连续场景图和动作描述生成，包含44个家庭日常活动视频。</li><li>自定义真实世界数据集：包含20个摆餐具任务演示，用于训练VLA策略。</li></ul>\n\n<p>训练资源/模型：</p>\n<ul><li>SGG主干：基于YOLOv8m的REACT模型。</li><li>VLA策略：基于SmolVLA模型，针对6个原子行为（拿起叉/刀/勺，放置在左/右/内侧）分别进行微调，每个策略训练20,000步。</li><li>硬件：训练和推理在配备NVIDIA RTX 3080 GPU的计算机上进行。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ul><li>SGG评估：在IndoorVG数据集上进行离线评估。</li><li>动作生成评估：在DAHLIA数据集上进行离线评估。</li><li>完整系统评估：在真实世界环境中使用SO-101机械臂进行，任务为摆设餐桌，复杂度从1个到6个连续的拾取-放置动作不等。</li></ul>\n\n<p>评估指标：</p>\n<ul><li>SGG性能：Recall@K (R@K), mean Recall@K (mR@K) (K=[20,50,100]), mAP@50, 以及延迟。</li><li>动作生成评估：通过人工判断生成PDDL动作的正确性，计算召回率（正确动作数/总生成动作数）。</li><li>完整系统性能：</li></ul>\n<p> - 单个策略准确率：每个原子VLA策略执行10次的成功率。</p>\n<p> - 整体任务准确率：不同长度（2、4、6个技能）长时程任务的成功率。</p>\n<p> - 动作生成准确率：在真实世界中生成正确PDDL动作的准确率。</p>\n<ul><li>基线比较：将GraSP-VLA的整体准确率与对SmolVLA进行端到端微调的基线进行比较。</li></ul>"
  },
  {
    "date": "2025-11-04",
    "title": "XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations",
    "link": "http://arxiv.org/abs/2511.02776",
    "summary_markdown": "## 论文研究单位\n北京具身智能机器人创新中心（牵头单位），联合北京航空航天大学（机械工程与自动化学院、虚拟现实技术与系统国家重点实验室）、北京大学（计算机学院多媒体信息处理国家重点实验室）共同完成。\n## 论文概述\n针对视觉语言行动（VLA）模型在低层动作精确控制和跨机器人形态数据整合方面的挑战，论文提出 **XR-1** 框架：通过引入 **统一视觉运动代码（UVMC）** 作为离散潜在表征，结合三阶段训练范式，实现多模态对齐与跨身体控制的视觉语言行动模型。该模型可同时利用人类视频和机器人数据，显著提升多任务泛化能力和实际部署效果。\n## 论文核心贡献点\n1. **UVMC 机制**\n 提出统一视觉运动代码（UVMC），通过双分支 VQ-VAE 将视觉动态和机器人运动编码至共享离散潜在空间，并通过 KL 散度约束实现跨模态对齐。\n2. **三阶段训练框架**\n 包含自监督 UVMC 学习、跨身体预训练和任务特定微调，模型无关于特定 VLA 架构，可适配多种基础模型（如 π₀、SwitchVLA）。\n3. **大规模实证验证**\n 在 6 种机器人平台、120+ 任务中进行超过 14,000 次实机测试，平均成功率显著超越 π₀.₅、π₀、RDT、UniVLA 等 SOTA 基线。\n## 论文方法描述\n### 核心架构\n- **UVMC 学习（Stage-1）**\n 通过双分支 VQ-VAE 分别编码视觉动态（未来帧预测）和机器人运动（动作序列重构），共享码本实现模态统一，引入视觉-运动 KL 散度损失促进对齐。\n- **跨身体预训练（Stage-2）**\n 使用 UVMC 作为监督信号，通过可学习 token 将其注入 VLM 主干，结合动作预测损失联合训练策略网络。\n- **任务特定微调（Stage-3）**\n 基于目标机器人任务数据微调模型，提升特定场景性能。\n### 训练流程\n1. 大规模数据预训练 UVMC：利用 Open-X、RoboMIND、Ego4D、XR-D 四类数据（人类视频+机器人数据），加权采样平衡不同来源。\n2. UVMC 引导策略预训练：在 XR-D 上进行跨身体动作学习。\n3. 任务特定微调：针对下游任务数据进行小样本适配。\n## 论文使用数据集和训练资源\n- **数据集组成**：\n - Open-X（978k episodes, 59.3M frames, 权重 40%）\n - RoboMIND（69k episodes, 21.4M frames, 权重 15%）\n - XR-D（158k episodes, 69.1M frames, 权重 35%）\n - Ego4D（59k episodes, 14.3M frames, 权重 10%）\n- **训练资源**：\n - 主模型 XR-1：基于 PaliGemma（SigLIP+Gemma）架构\n - 轻量模型 XR-1-Light：基于 SwitchVLA（Florence-2）架构\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 6 种真实机器人平台：Tien Kung 1.0/2.0、单/双臂 UR-5e、双臂 Franka、AgileX Cobot Magic 2.0\n - 覆盖 120+ 复杂任务：双臂协作、灵巧操作、可变形体处理、接触密集任务、动态环境等\n- **评估指标**：\n - **成功率（Success Rate）**：每任务 20 次测试的平均完成率\n - **跨身体泛化**：在未见过机器人平台（如 Tien Kung 2.0）上验证迁移能力\n - **鲁棒性测试**：新增物体、背景干扰、光照变化下的性能保持率\n\n**效果摘要**：\nXR-1 在所有平台平均性能显著优于 π₀.₅、π₀ 等基线（如 Tien Kung 2.0 任务平均 72% vs 41%），并在未知场景（新增干扰物等）中展现更强鲁棒性。\n---\n论文核心创新在于通过 UVMC 机制统一视觉与动作模态，结合三阶段训练打通人类演示与机器人数据，实现跨身体的通用技能学习。实验充分验证了其在复杂操作中的有效性。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>北京具身智能机器人创新中心（牵头单位），联合北京航空航天大学（机械工程与自动化学院、虚拟现实技术与系统国家重点实验室）、北京大学（计算机学院多媒体信息处理国家重点实验室）共同完成。</p>\n<h2>论文概述</h2>\n<p>针对视觉语言行动（VLA）模型在低层动作精确控制和跨机器人形态数据整合方面的挑战，论文提出 <strong>XR-1</strong> 框架：通过引入 <strong>统一视觉运动代码（UVMC）</strong> 作为离散潜在表征，结合三阶段训练范式，实现多模态对齐与跨身体控制的视觉语言行动模型。该模型可同时利用人类视频和机器人数据，显著提升多任务泛化能力和实际部署效果。</p>\n<h2>论文核心贡献点</h2>\n<p>1. <strong>UVMC 机制</strong></p>\n<p> 提出统一视觉运动代码（UVMC），通过双分支 VQ-VAE 将视觉动态和机器人运动编码至共享离散潜在空间，并通过 KL 散度约束实现跨模态对齐。</p>\n<p>2. <strong>三阶段训练框架</strong></p>\n<p> 包含自监督 UVMC 学习、跨身体预训练和任务特定微调，模型无关于特定 VLA 架构，可适配多种基础模型（如 π₀、SwitchVLA）。</p>\n<p>3. <strong>大规模实证验证</strong></p>\n<p> 在 6 种机器人平台、120+ 任务中进行超过 14,000 次实机测试，平均成功率显著超越 π₀.₅、π₀、RDT、UniVLA 等 SOTA 基线。</p>\n<h2>论文方法描述</h2>\n<h3>核心架构</h3>\n<ul><li><strong>UVMC 学习（Stage-1）</strong></li></ul>\n<p> 通过双分支 VQ-VAE 分别编码视觉动态（未来帧预测）和机器人运动（动作序列重构），共享码本实现模态统一，引入视觉-运动 KL 散度损失促进对齐。</p>\n<ul><li><strong>跨身体预训练（Stage-2）</strong></li></ul>\n<p> 使用 UVMC 作为监督信号，通过可学习 token 将其注入 VLM 主干，结合动作预测损失联合训练策略网络。</p>\n<ul><li><strong>任务特定微调（Stage-3）</strong></li></ul>\n<p> 基于目标机器人任务数据微调模型，提升特定场景性能。</p>\n<h3>训练流程</h3>\n<ol><li>大规模数据预训练 UVMC：利用 Open-X、RoboMIND、Ego4D、XR-D 四类数据（人类视频+机器人数据），加权采样平衡不同来源。</li><li>UVMC 引导策略预训练：在 XR-D 上进行跨身体动作学习。</li><li>任务特定微调：针对下游任务数据进行小样本适配。</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集组成</strong>：</li></ul>\n<p> - Open-X（978k episodes, 59.3M frames, 权重 40%）</p>\n<p> - RoboMIND（69k episodes, 21.4M frames, 权重 15%）</p>\n<p> - XR-D（158k episodes, 69.1M frames, 权重 35%）</p>\n<p> - Ego4D（59k episodes, 14.3M frames, 权重 10%）</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 主模型 XR-1：基于 PaliGemma（SigLIP+Gemma）架构</p>\n<p> - 轻量模型 XR-1-Light：基于 SwitchVLA（Florence-2）架构</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 6 种真实机器人平台：Tien Kung 1.0/2.0、单/双臂 UR-5e、双臂 Franka、AgileX Cobot Magic 2.0</p>\n<p> - 覆盖 120+ 复杂任务：双臂协作、灵巧操作、可变形体处理、接触密集任务、动态环境等</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（Success Rate）</strong>：每任务 20 次测试的平均完成率</p>\n<p> - <strong>跨身体泛化</strong>：在未见过机器人平台（如 Tien Kung 2.0）上验证迁移能力</p>\n<p> - <strong>鲁棒性测试</strong>：新增物体、背景干扰、光照变化下的性能保持率</p>\n\n<p><strong>效果摘要</strong>：</p>\n<p>XR-1 在所有平台平均性能显著优于 π₀.₅、π₀ 等基线（如 Tien Kung 2.0 任务平均 72% vs 41%），并在未知场景（新增干扰物等）中展现更强鲁棒性。</p>\n<hr/>\n<p>论文核心创新在于通过 UVMC 机制统一视觉与动作模态，结合三阶段训练打通人类演示与机器人数据，实现跨身体的通用技能学习。实验充分验证了其在复杂操作中的有效性。</p>"
  },
  {
    "date": "2025-11-01",
    "title": "iFlyBot-VLA Technical Report",
    "link": "http://arxiv.org/abs/2511.01914",
    "summary_markdown": "论文研究单位\niFLYTEK Research and Development Group; LindenBot\n\n论文概述\niFlyBot-VLA 是一个基于 Transformer 语言主干与扩散/流匹配动作专家的 Vision-Language-Action（VLA）模型，用于端到端双机械臂操作。论文提出“显式 + 隐式”双层动作表示框架：以频域压缩的离散动作标记 FAST 提供显式监督，帮助 VLM 学习动作语义和隐式规划；以 VQ-VAE 预训练的紧凑潜在动作表示提供隐式规划信号，仅将其特征传入动作专家，实现高效且可控的动作生成。该方法在保持 VLM 通用感知与推理能力的同时，缓解了端到端训练对 VLM 能力的破坏，并通过混合训练策略（机器人轨迹 + 通用 QA/空间 QA 数据）进一步提升泛化。\n\n论文核心贡献点\n提出并训练了基于 VQ-VAE 的潜在动作模型，代码本规模为 32，每步检索 8 个离散码，引入 NSVQ 近似以替代 Straight-Through Estimator 并在解码时对当前帧停止梯度。\n提出双层动作表示：FAST 离散动作标记用于监督 VLM 学习动作语义（特征不送入动作专家），潜在动作标记作为压缩后的隐式规划信号输入动作专家，实现高效连续控制。\n提出混合训练策略，在预训练阶段将空间 QA 与机器人轨迹数据按优化比例混合，保留并增强 VLM 的通用感知与空间推理；并通过截断专家到主干梯度、在下游微调阶段开启专家反向传播与多噪声扰动等策略稳定训练。\n在 LIBERO 仿真与真实世界任务上实现领先性能与泛化，并计划开源部分自建数据集。\n\n论文方法描述\n总体架构：基于 Qwen2.5-VL(3B) 作为视觉语言主干，在其上接入 Flow-Matching 扩散 Transformer 动作专家；仅传递潜在动作标记对应的 KV 缓存至专家，离散 FAST 标记的 KV 不传递。\n潜在动作模型（Stage I）：以自监督方式从人机操作视频中学习潜在动作；采用 NSVQ 解决 VQ-VAE 训练中的梯度崩塌问题。\n离散动作标记（FAST）：对滑窗连续控制信号进行 DCT 压缩和 BPE 编码，仅用于监督 VLM 的动作语义与隐式规划，不向动作专家提供特征。\nVLA 训练（Stage II/III）：双阶段训练，第一阶段截断专家到主干梯度以保护 VLM 能力，第二阶段开启梯度传播并引入多噪声扰动加速专家适配。\n动作生成：采用 Flow Matching，通过离散前向欧拉积分（5 步，σ=0.2）从随机高斯噪声逐步还原动作块；动作与状态统一 pad 到 20 维（左右臂各 10 维）。\n\n论文使用数据集和训练资源\n潜在动作训练数据（Stage I）：人类视频数据集 HoloAssist、Ego4D、EgoDex、HOI4D、Something-Something V2、EgoVid；机器人数据集 OXE、AgiBot-World、RoboMind、Galaxea。\nVLA 预训练数据（Stage II）：内部构建的空间推理 QA 数据；公开数据集 OXE、AgiBot-World 的子集；iFLYTEK 自采集双臂操作数据（衣褶、通用抓取摆放、长程包裹分拣）。\n自采集数据集规模与构成：\n衣褶任务：8 类衣物（5 款 T 恤、3 款短裤），每类约 190 条轨迹，平均 4.5 分钟，约 110 小时。\n通用抓取摆放：30 类物体，每类约 400 条轨迹，平均 27 秒，约 90 小时。\n长程包裹分拣：约 2,752 条轨迹，平均 61 秒，约 47 小时。\n预训练配比与阶段：预训练阶段混合上述数据集与空间 QA；微调阶段进行任务特定训练（如 LIBERO：70,000 步用于 Long 子集，50,000 步用于其他子集；动作窗口长度 7；全局批大小 64；仅第三人称图像与文本指令）。\n\n论文使用的评估环境和评估指标\n仿真评估：LIBERO 基准（LIBERO-Spatial、Object、Goal、Long 四个任务套件，各 10 个任务×10 条演示）；指标为成功率（%）。\n真实世界评估：\n通用抓取摆放：四种配置（基础、未见物体、光照变化、未见场景），24 类可见或 14 类未见物体，每类 20 次尝试；指标为成功率（%）。\n长程包裹分拣：双机械臂翻转与摆放流程，40 次×3 包裹（两条需翻转）试验；采用“严格（不允许校正）”与“允许校正（最多两次校正）”两种评估；指标为成功率与相对提升（%）。\n衣褶任务：定义步骤级评估协议，统计各步骤完成率；任务含“点选角落”、“摊平（扫平/拖拽）”、“折叠”等子步骤；允许在 3 分钟时间限制内多次尝试。",
    "summary_html": "<p>论文研究单位</p>\n<p>iFLYTEK Research and Development Group; LindenBot</p>\n\n<p>论文概述</p>\n<p>iFlyBot-VLA 是一个基于 Transformer 语言主干与扩散/流匹配动作专家的 Vision-Language-Action（VLA）模型，用于端到端双机械臂操作。论文提出“显式 + 隐式”双层动作表示框架：以频域压缩的离散动作标记 FAST 提供显式监督，帮助 VLM 学习动作语义和隐式规划；以 VQ-VAE 预训练的紧凑潜在动作表示提供隐式规划信号，仅将其特征传入动作专家，实现高效且可控的动作生成。该方法在保持 VLM 通用感知与推理能力的同时，缓解了端到端训练对 VLM 能力的破坏，并通过混合训练策略（机器人轨迹 + 通用 QA/空间 QA 数据）进一步提升泛化。</p>\n\n<p>论文核心贡献点</p>\n<p>提出并训练了基于 VQ-VAE 的潜在动作模型，代码本规模为 32，每步检索 8 个离散码，引入 NSVQ 近似以替代 Straight-Through Estimator 并在解码时对当前帧停止梯度。</p>\n<p>提出双层动作表示：FAST 离散动作标记用于监督 VLM 学习动作语义（特征不送入动作专家），潜在动作标记作为压缩后的隐式规划信号输入动作专家，实现高效连续控制。</p>\n<p>提出混合训练策略，在预训练阶段将空间 QA 与机器人轨迹数据按优化比例混合，保留并增强 VLM 的通用感知与空间推理；并通过截断专家到主干梯度、在下游微调阶段开启专家反向传播与多噪声扰动等策略稳定训练。</p>\n<p>在 LIBERO 仿真与真实世界任务上实现领先性能与泛化，并计划开源部分自建数据集。</p>\n\n<p>论文方法描述</p>\n<p>总体架构：基于 Qwen2.5-VL(3B) 作为视觉语言主干，在其上接入 Flow-Matching 扩散 Transformer 动作专家；仅传递潜在动作标记对应的 KV 缓存至专家，离散 FAST 标记的 KV 不传递。</p>\n<p>潜在动作模型（Stage I）：以自监督方式从人机操作视频中学习潜在动作；采用 NSVQ 解决 VQ-VAE 训练中的梯度崩塌问题。</p>\n<p>离散动作标记（FAST）：对滑窗连续控制信号进行 DCT 压缩和 BPE 编码，仅用于监督 VLM 的动作语义与隐式规划，不向动作专家提供特征。</p>\n<p>VLA 训练（Stage II/III）：双阶段训练，第一阶段截断专家到主干梯度以保护 VLM 能力，第二阶段开启梯度传播并引入多噪声扰动加速专家适配。</p>\n<p>动作生成：采用 Flow Matching，通过离散前向欧拉积分（5 步，σ=0.2）从随机高斯噪声逐步还原动作块；动作与状态统一 pad 到 20 维（左右臂各 10 维）。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>潜在动作训练数据（Stage I）：人类视频数据集 HoloAssist、Ego4D、EgoDex、HOI4D、Something-Something V2、EgoVid；机器人数据集 OXE、AgiBot-World、RoboMind、Galaxea。</p>\n<p>VLA 预训练数据（Stage II）：内部构建的空间推理 QA 数据；公开数据集 OXE、AgiBot-World 的子集；iFLYTEK 自采集双臂操作数据（衣褶、通用抓取摆放、长程包裹分拣）。</p>\n<p>自采集数据集规模与构成：</p>\n<p>衣褶任务：8 类衣物（5 款 T 恤、3 款短裤），每类约 190 条轨迹，平均 4.5 分钟，约 110 小时。</p>\n<p>通用抓取摆放：30 类物体，每类约 400 条轨迹，平均 27 秒，约 90 小时。</p>\n<p>长程包裹分拣：约 2,752 条轨迹，平均 61 秒，约 47 小时。</p>\n<p>预训练配比与阶段：预训练阶段混合上述数据集与空间 QA；微调阶段进行任务特定训练（如 LIBERO：70,000 步用于 Long 子集，50,000 步用于其他子集；动作窗口长度 7；全局批大小 64；仅第三人称图像与文本指令）。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>仿真评估：LIBERO 基准（LIBERO-Spatial、Object、Goal、Long 四个任务套件，各 10 个任务×10 条演示）；指标为成功率（%）。</p>\n<p>真实世界评估：</p>\n<p>通用抓取摆放：四种配置（基础、未见物体、光照变化、未见场景），24 类可见或 14 类未见物体，每类 20 次尝试；指标为成功率（%）。</p>\n<p>长程包裹分拣：双机械臂翻转与摆放流程，40 次×3 包裹（两条需翻转）试验；采用“严格（不允许校正）”与“允许校正（最多两次校正）”两种评估；指标为成功率与相对提升（%）。</p>\n<p>衣褶任务：定义步骤级评估协议，统计各步骤完成率；任务含“点选角落”、“摊平（扫平/拖拽）”、“折叠”等子步骤；允许在 3 分钟时间限制内多次尝试。</p>"
  },
  {
    "date": "2025-11-03",
    "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process",
    "link": "http://arxiv.org/abs/2511.01718",
    "summary_markdown": "# 论文研究单位\n- HKUST(GZ) - 香港科技大学（广州）\n- Westlake University - 西湖大学\n- Zhejiang University - 浙江大学\n- Monash University - 蒙纳士大学\n# 论文概述\n论文提出统一扩散VLA（UD-VLA），通过联合离散去噪扩散过程（JD3P）将视觉、语言和动作的理解、生成和执行统一到单个Transformer中。核心创新是通过同步去噪过程联合优化视觉生成和动作预测，使动作在持续的视觉指导下从初始化逐步演化，实现真正的跨模态协同。\n# 论文核心贡献点\n- 提出统一扩散VLA，通过同步去噪过程紧密耦合理解、生成和执行，形成互利关系\n- 通过离散标记化、混合注意力和JD3P过程作为跨模态协同的核心机制来实例化设计\n- 设计两阶段训练pipeline激活图像生成能力，引入多种测试时技术确保高性能和效率\n- 在CALVIN、LIBERO和SimplerEnv等基准上达到SOTA性能，推理速度比自回归方法快4倍\n# 论文方法描述\n**统一标记化**：将语言、视觉和动作模态转换为离散标记并连接成单序列，使用VQ标记器进行视觉量化，FAST进行动作标记化\n\n**混合注意机制**：输入部分采用因果和双向注意，输出分为生成块（未来图像）和执行块（动作），块内双向注意，块间因果注意，阻止动作信息回流\n\n**联合离散去噪扩散过程（JD3P）**：并行生成动作和图像，通过马尔可夫链噪声过程和条件去噪分布，在每个去噪步骤联合重构掩码位置，从噪声逐步恢复原始信号\n\n**两阶段训练**：阶段(i)在视频数据集上注入未来图像生成能力；阶段(ii)在机器人动作数据集上联合优化图像和动作生成\n\n**推理技术**：前缀KV缓存、预填充特殊令牌、置信度引导解码、解码空间映射等提升效率\n# 论文使用数据集和训练资源\n- **预训练数据**：大规模视频数据集用于阶段(i)训练\n- **下游数据**：机器人动作数据集用于阶段(ii)联合训练\n- **评估基准**：CALVIN、LIBERO、SimplerEnv仿真环境，真实世界UR5e机械臂+Inspire RH56E2机械手平台\n- **初始化模型**：预训练VLM主干网络（Emu3）\n# 论文使用的评估环境和评估指标\n**CALVIN**：4个环境、34个任务、1000条语言指令，报告连续5个子任务的平均完成长度（最大值5.0）\n\n**LIBERO**：4个套件（空间、物体、目标、长程），每套件10个任务×50次运行，报告各套件和整体成功率\n\n**SimplerEnv-WidowX**：真实到仿真传输评估，4个任务，报告各任务和整体成功率\n\n**真实世界实验**：UR5e机械臂+6自由度Inspire RH56E2机械手，3类任务（堆叠碗、放置方块、翻转塔），每类200条轨迹，在可见和不可见设置下评估各30次的成功率",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>HKUST(GZ) - 香港科技大学（广州）</li><li>Westlake University - 西湖大学</li><li>Zhejiang University - 浙江大学</li><li>Monash University - 蒙纳士大学</li></ul>\n<h1>论文概述</h1>\n<p>论文提出统一扩散VLA（UD-VLA），通过联合离散去噪扩散过程（JD3P）将视觉、语言和动作的理解、生成和执行统一到单个Transformer中。核心创新是通过同步去噪过程联合优化视觉生成和动作预测，使动作在持续的视觉指导下从初始化逐步演化，实现真正的跨模态协同。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出统一扩散VLA，通过同步去噪过程紧密耦合理解、生成和执行，形成互利关系</li><li>通过离散标记化、混合注意力和JD3P过程作为跨模态协同的核心机制来实例化设计</li><li>设计两阶段训练pipeline激活图像生成能力，引入多种测试时技术确保高性能和效率</li><li>在CALVIN、LIBERO和SimplerEnv等基准上达到SOTA性能，推理速度比自回归方法快4倍</li></ul>\n<h1>论文方法描述</h1>\n<p><strong>统一标记化</strong>：将语言、视觉和动作模态转换为离散标记并连接成单序列，使用VQ标记器进行视觉量化，FAST进行动作标记化</p>\n\n<p><strong>混合注意机制</strong>：输入部分采用因果和双向注意，输出分为生成块（未来图像）和执行块（动作），块内双向注意，块间因果注意，阻止动作信息回流</p>\n\n<p><strong>联合离散去噪扩散过程（JD3P）</strong>：并行生成动作和图像，通过马尔可夫链噪声过程和条件去噪分布，在每个去噪步骤联合重构掩码位置，从噪声逐步恢复原始信号</p>\n\n<p><strong>两阶段训练</strong>：阶段(i)在视频数据集上注入未来图像生成能力；阶段(ii)在机器人动作数据集上联合优化图像和动作生成</p>\n\n<p><strong>推理技术</strong>：前缀KV缓存、预填充特殊令牌、置信度引导解码、解码空间映射等提升效率</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>预训练数据</strong>：大规模视频数据集用于阶段(i)训练</li><li><strong>下游数据</strong>：机器人动作数据集用于阶段(ii)联合训练</li><li><strong>评估基准</strong>：CALVIN、LIBERO、SimplerEnv仿真环境，真实世界UR5e机械臂+Inspire RH56E2机械手平台</li><li><strong>初始化模型</strong>：预训练VLM主干网络（Emu3）</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<p><strong>CALVIN</strong>：4个环境、34个任务、1000条语言指令，报告连续5个子任务的平均完成长度（最大值5.0）</p>\n\n<p><strong>LIBERO</strong>：4个套件（空间、物体、目标、长程），每套件10个任务×50次运行，报告各套件和整体成功率</p>\n\n<p><strong>SimplerEnv-WidowX</strong>：真实到仿真传输评估，4个任务，报告各任务和整体成功率</p>\n\n<p><strong>真实世界实验</strong>：UR5e机械臂+6自由度Inspire RH56E2机械手，3类任务（堆叠碗、放置方块、翻转塔），每类200条轨迹，在可见和不可见设置下评估各30次的成功率</p>"
  },
  {
    "date": "2025-11-03",
    "title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2511.01571",
    "summary_markdown": "### 论文研究单位\n- 华南理工大学自动化科学与工程学院\n- 中国科学院沈阳自动化研究所\n- 穆罕默德·本·扎耶德人工智能大学（Mohamed bin Zayed University of Artificial Intelligence）\n- 澳大利亚国立大学\n### 论文概述\nPixelVLA 是首个支持像素级理解和多模态提示（文本和视觉输入）的视觉-语言-动作（VLA）模型。现有 VLAs 局限于图像级理解且依赖文本提示，导致空间推理和跨域泛化能力不足。PixelVLA 通过集成多尺度像素感知编码器、视觉提示编码器和连续动作解码器，解决这些挑战。使用两阶段自动注释管道生成的 Pixel-160K 数据集进行训练，该数据集包含 160K 操作片段和 6.5M 图像-文本-动作三元组。PixelVLA 在三个标准 VLA 基准测试（SimplerEnv、Google Robot、LIBERO）上显著优于 OpenVLA，成功率提升 10.1%~28.7%，且训练成本仅为 OpenVLA 的 1.5%。\n### 论文核心贡献点\n1. **PixelVLA 架构设计**：提出支持像素级理解和多模态提示的 VLA 模型，包括多尺度像素感知编码器、视觉提示编码器（处理点、线、区域等提示）和连续动作解码器（直接预测 7D 动作）。\n2. **两阶段自动注释管道**：开发用于合成 Pixel-160K 数据集的管道，第一阶段定位夹爪并生成区域提议，第二阶段使用 LLM 和开放词汇分割模型生成像素级注释和视觉提示。\n3. **视觉运动指令调优框架**：引入两阶段训练流程——连续动作训练阶段和像素级理解增强阶段，有效提升像素级空间理解能力。\n4. **实证验证**：在多个基准测试上集成 PixelVLA 架构到 OpenVLA 和 π₀ 模型，验证性能提升和泛化能力。\n### 论文方法描述\n- **PixelVLA 架构**：基于 Prismatic-7B VLM 和 Llama 2-7B 的主干，结合视觉编码器（DinoV2 和 SigLIP）、MLP 投影仪、视觉提示编码器（源自 SAM）、多尺度像素感知编码器（生成像素感知嵌入）和连续动作解码器（ResNet 块 + MLP，预测连续动作序列）。\n- **多尺度像素感知编码器**：从多尺度视觉特征中提取像素级信息，公式基于特征和像素掩码的 MLP 组合。\n- **视觉提示编码器**：处理多样化视觉提示（如点、线、区域），生成提示感知嵌入。\n- **连续动作解码器**：基于 LLM 隐藏状态，通过线性投影、ResNet 块和 MLP 输出连续动作值（chunk 大小为 8）。\n- **训练流程**：两阶段视觉运动指令调优。\n - **阶段一（连续动作训练）**：使用 Fractal 和 Bridge v2 数据训练连续动作解码器，冻结其他模块。\n - **阶段二（像素级理解增强）**：使用 Pixel-160K 数据集，通过 LoRA 微调 LLM 主干，联合训练视觉提示和像素感知编码器。\n### 论文使用数据集和训练资源\n- **数据集**：Pixel-160K（160K 操作片段，6.5M 三元组），基于 Fractal 和 Bridge v2 数据集构建；LIBERO-Pixel（使用管道处理 LIBERO 基准）。\n- **训练资源**：\n - 两阶段训练：阶段一 100k 步（批量 32，学习率 5e-4），阶段二 200k 步（批量 32，学习率 1e-3，LoRA r=32）。\n - 总训练成本仅为 OpenVLA 预训练的 1.5%。\n - 输入分辨率 224×224，单视角第三视角相机。\n### 论文使用的评估环境和评估指标\n- **评估环境**：三个仿真基准测试。\n - SimplerEnv（Google Robot 和 WidowX 设置）：评估零样本对象操作。\n - LIBERO：四个任务套件（LIBERO-Spatial、-Object、-Goal、-Long）评估新机器人设置适配。\n- **评估指标**：\n - SimplerEnv：成功率，包括视觉匹配（VM）和变体聚合（VA）分数；平均抓取和任务完成成功率。\n - LIBERO：每个任务套件的成功率和排名。\n - 主要指标：操作成功率（%），如 Google Robot 上 PixelVLA 平均 VM 得分 61.4（提升 28.7%）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>华南理工大学自动化科学与工程学院</li><li>中国科学院沈阳自动化研究所</li><li>穆罕默德·本·扎耶德人工智能大学（Mohamed bin Zayed University of Artificial Intelligence）</li><li>澳大利亚国立大学</li></ul>\n<h3>论文概述</h3>\n<p>PixelVLA 是首个支持像素级理解和多模态提示（文本和视觉输入）的视觉-语言-动作（VLA）模型。现有 VLAs 局限于图像级理解且依赖文本提示，导致空间推理和跨域泛化能力不足。PixelVLA 通过集成多尺度像素感知编码器、视觉提示编码器和连续动作解码器，解决这些挑战。使用两阶段自动注释管道生成的 Pixel-160K 数据集进行训练，该数据集包含 160K 操作片段和 6.5M 图像-文本-动作三元组。PixelVLA 在三个标准 VLA 基准测试（SimplerEnv、Google Robot、LIBERO）上显著优于 OpenVLA，成功率提升 10.1%~28.7%，且训练成本仅为 OpenVLA 的 1.5%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>PixelVLA 架构设计</strong>：提出支持像素级理解和多模态提示的 VLA 模型，包括多尺度像素感知编码器、视觉提示编码器（处理点、线、区域等提示）和连续动作解码器（直接预测 7D 动作）。</li><li><strong>两阶段自动注释管道</strong>：开发用于合成 Pixel-160K 数据集的管道，第一阶段定位夹爪并生成区域提议，第二阶段使用 LLM 和开放词汇分割模型生成像素级注释和视觉提示。</li><li><strong>视觉运动指令调优框架</strong>：引入两阶段训练流程——连续动作训练阶段和像素级理解增强阶段，有效提升像素级空间理解能力。</li><li><strong>实证验证</strong>：在多个基准测试上集成 PixelVLA 架构到 OpenVLA 和 π₀ 模型，验证性能提升和泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>PixelVLA 架构</strong>：基于 Prismatic-7B VLM 和 Llama 2-7B 的主干，结合视觉编码器（DinoV2 和 SigLIP）、MLP 投影仪、视觉提示编码器（源自 SAM）、多尺度像素感知编码器（生成像素感知嵌入）和连续动作解码器（ResNet 块 + MLP，预测连续动作序列）。</li><li><strong>多尺度像素感知编码器</strong>：从多尺度视觉特征中提取像素级信息，公式基于特征和像素掩码的 MLP 组合。</li><li><strong>视觉提示编码器</strong>：处理多样化视觉提示（如点、线、区域），生成提示感知嵌入。</li><li><strong>连续动作解码器</strong>：基于 LLM 隐藏状态，通过线性投影、ResNet 块和 MLP 输出连续动作值（chunk 大小为 8）。</li><li><strong>训练流程</strong>：两阶段视觉运动指令调优。</li></ul>\n<p> - <strong>阶段一（连续动作训练）</strong>：使用 Fractal 和 Bridge v2 数据训练连续动作解码器，冻结其他模块。</p>\n<p> - <strong>阶段二（像素级理解增强）</strong>：使用 Pixel-160K 数据集，通过 LoRA 微调 LLM 主干，联合训练视觉提示和像素感知编码器。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Pixel-160K（160K 操作片段，6.5M 三元组），基于 Fractal 和 Bridge v2 数据集构建；LIBERO-Pixel（使用管道处理 LIBERO 基准）。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 两阶段训练：阶段一 100k 步（批量 32，学习率 5e-4），阶段二 200k 步（批量 32，学习率 1e-3，LoRA r=32）。</p>\n<p> - 总训练成本仅为 OpenVLA 预训练的 1.5%。</p>\n<p> - 输入分辨率 224×224，单视角第三视角相机。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：三个仿真基准测试。</li></ul>\n<p> - SimplerEnv（Google Robot 和 WidowX 设置）：评估零样本对象操作。</p>\n<p> - LIBERO：四个任务套件（LIBERO-Spatial、-Object、-Goal、-Long）评估新机器人设置适配。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - SimplerEnv：成功率，包括视觉匹配（VM）和变体聚合（VA）分数；平均抓取和任务完成成功率。</p>\n<p> - LIBERO：每个任务套件的成功率和排名。</p>\n<p> - 主要指标：操作成功率（%），如 Google Robot 上 PixelVLA 平均 VM 得分 61.4（提升 28.7%）。</p>"
  },
  {
    "date": "2025-11-03",
    "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.01331",
    "summary_markdown": "### 论文研究单位\nWestlake University\n### 论文概述\nRobustVLA是一种针对Vision-Language-Action (VLA)模型的在线强化学习后训练方法，旨在提高模型在环境扰动（如观察噪声和动作噪声）下的鲁棒性。传统方法在out-of-distribution部署中易失败，RobustVLA通过理论分析的鲁棒性界限，引入Jacobian正则化和smoothness正则化来显式约束模型敏感性，显著提升VLA模型的稳定性和可靠性。\n### 论文核心贡献点\n- 提出RobustVLA方法，结合在线RL和鲁棒性约束，针对环境扰动优化VLA模型。\n- 进行系统鲁棒性分析，识别观察扰动和动作扰动的影响，分别推导Jacobian敏感性和smoothness稳定性的关键作用。\n- 引入双正则化策略：Jacobian正则化抑制观察噪声敏感性，smoothness正则化缓解动作扰动和更新漂移。\n- 实验证明RobustVLA在多种扰动下优于SOTA基线（如RIPT-VLA、OpenVLA等），提升迁移学习和消融性能。\n### 论文方法描述\n- **问题建模**：基于马尔可夫决策过程（MDP），VLA模型πθ映射语言指令和观察序列到动作分布。\n- **鲁棒性分析**：\n - 定理1（观察扰动误差界限）：返回差距受Jacobian敏感性和观察噪声影响，需控制\\|\\|∇_s log πθ(a\\|s)\\|\\|。\n - 定理2（动作扰动返回漂移）：返回差距与累积模型漂移∑δ_t和动作噪声σ√d相关，需限制模型更新平滑度。\n - 定理3（联合扰动鲁棒性）：观察和动作扰动联合时返回差距扩大，需同时应用双正则化。\n- **正则化目标**：\n - Jacobian正则化：ℛ_Jac(θ) = E[min(\\|\\|∇_s log πθ(a\\|s)\\|\\|², G_max)]，限制输入敏感性。\n - Smoothness正则化：ℛ_Smooth(θ) = E[\\|\\|μ_θ(s) - μ_θ-(s)\\|\\|²]，稳定模型更新。\n - 整体目标：ℒ_RobustVLA = ℒ_PPO + αℛ_Jac + βℛ_Smooth。\n- **算法实现**：采用课程学习机制（RobustVLA-C），基于成功率的移动平均自适应调整噪声水平（ε_min到ε_max），防止训练初期不稳定。\n### 论文使用数据集和训练资源\n- **数据集**：基于LIBERO仿真平台，包含Objects、Long、Spatial、Goal四个任务套件，每个套件使用50个保留测试上下文。\n- **扰动设置**：\n - 观察扰动：图像移位、旋转、颜色抖动、遮挡、擦除。\n - 动作扰动：零均值高斯噪声，标准差0.1、0.2、0.3。\n- **训练资源**：在线RL交互收集数据，算法在仿真环境中运行；评估基于单VLA模型部署至所有任务。\n### 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO仿真平台，模拟机器人操作任务场景。\n- **评估指标**：\n - 主要指标：平均成功率（SR）在扰动下的表现。\n - 对比基线：离线IL（π₀、GEVRM、OpenVLA、OpenVLA-OFT）、离线RL（RWR、ARFM、ReinboT）、在线RL（RIPT-VLA）。\n - 其他评估：迁移学习性能（从LIBERO Goal套件到下游任务）、消融研究（超参数α和β影响、T-SNE观察表示可视化）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Westlake University</p>\n<h3>论文概述</h3>\n<p>RobustVLA是一种针对Vision-Language-Action (VLA)模型的在线强化学习后训练方法，旨在提高模型在环境扰动（如观察噪声和动作噪声）下的鲁棒性。传统方法在out-of-distribution部署中易失败，RobustVLA通过理论分析的鲁棒性界限，引入Jacobian正则化和smoothness正则化来显式约束模型敏感性，显著提升VLA模型的稳定性和可靠性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出RobustVLA方法，结合在线RL和鲁棒性约束，针对环境扰动优化VLA模型。</li><li>进行系统鲁棒性分析，识别观察扰动和动作扰动的影响，分别推导Jacobian敏感性和smoothness稳定性的关键作用。</li><li>引入双正则化策略：Jacobian正则化抑制观察噪声敏感性，smoothness正则化缓解动作扰动和更新漂移。</li><li>实验证明RobustVLA在多种扰动下优于SOTA基线（如RIPT-VLA、OpenVLA等），提升迁移学习和消融性能。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>问题建模</strong>：基于马尔可夫决策过程（MDP），VLA模型πθ映射语言指令和观察序列到动作分布。</li><li><strong>鲁棒性分析</strong>：</li></ul>\n<p> - 定理1（观察扰动误差界限）：返回差距受Jacobian敏感性和观察噪声影响，需控制\\|\\|∇_s log πθ(a\\|s)\\|\\|。</p>\n<p> - 定理2（动作扰动返回漂移）：返回差距与累积模型漂移∑δ_t和动作噪声σ√d相关，需限制模型更新平滑度。</p>\n<p> - 定理3（联合扰动鲁棒性）：观察和动作扰动联合时返回差距扩大，需同时应用双正则化。</p>\n<ul><li><strong>正则化目标</strong>：</li></ul>\n<p> - Jacobian正则化：ℛ_Jac(θ) = E[min(\\|\\|∇_s log πθ(a\\|s)\\|\\|², G_max)]，限制输入敏感性。</p>\n<p> - Smoothness正则化：ℛ_Smooth(θ) = E[\\|\\|μ_θ(s) - μ_θ-(s)\\|\\|²]，稳定模型更新。</p>\n<p> - 整体目标：ℒ_RobustVLA = ℒ_PPO + αℛ_Jac + βℛ_Smooth。</p>\n<ul><li><strong>算法实现</strong>：采用课程学习机制（RobustVLA-C），基于成功率的移动平均自适应调整噪声水平（ε_min到ε_max），防止训练初期不稳定。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：基于LIBERO仿真平台，包含Objects、Long、Spatial、Goal四个任务套件，每个套件使用50个保留测试上下文。</li><li><strong>扰动设置</strong>：</li></ul>\n<p> - 观察扰动：图像移位、旋转、颜色抖动、遮挡、擦除。</p>\n<p> - 动作扰动：零均值高斯噪声，标准差0.1、0.2、0.3。</p>\n<ul><li><strong>训练资源</strong>：在线RL交互收集数据，算法在仿真环境中运行；评估基于单VLA模型部署至所有任务。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：LIBERO仿真平台，模拟机器人操作任务场景。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：平均成功率（SR）在扰动下的表现。</p>\n<p> - 对比基线：离线IL（π₀、GEVRM、OpenVLA、OpenVLA-OFT）、离线RL（RWR、ARFM、ReinboT）、在线RL（RIPT-VLA）。</p>\n<p> - 其他评估：迁移学习性能（从LIBERO Goal套件到下游任务）、消融研究（超参数α和β影响、T-SNE观察表示可视化）。</p>"
  },
  {
    "date": "2025-11-03",
    "title": "Embodiment Transfer Learning for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2511.01224",
    "summary_markdown": "# 论文研究单位\nShanghai University（上海大学）\n# 论文概述\n论文介绍了ET-VLA（Embodiment Transfer Learning for Vision-Language-Action），一种用于将预训练VLA模型高效迁移到多机器人系统的新型框架。传统自回归VLA模型（如RT-2、OpenVLA）在单臂机器人任务上表现优异，但在多机器人协作场景中性能显著下降，经常无法生成结构有效的动作序列。ET-VLA通过合成继续预训练（SCP）和实体化思维图（EGoT）技术，在双机器人系统上显著提升性能，在六个真实世界任务中成功率比OpenVLA高出53.2%。\n# 论文核心贡献点\n- 深入分析了现有自回归VLA模型在多机器人多任务设置中的局限性\n- 提出ET-VLA框架，包含合成继续预训练（SCP）和实体化思维图（EGoT）两个关键技术\n- 在真实机器人和仿真环境中广泛评估，证明了相对于最先进VLA模型的优越性能\n# 论文方法描述\n**合成继续预训练（SCP）**：生成合成多机器人数据来预热模型，使其能够生成正确的动作序列和精确的动作token数量。SCP采用交叉采样方法，从batch中随机选择不同样本的动作tokens，组合成14个动作tokens（每个机器人7个DoF），使模型学习token到机器人的映射关系。\n\n**实体化思维图（EGoT）**：将复杂任务分解为显式表示时间依赖关系的动作图。该方法定义了五个任务类型（Grasp、Release、Waiting、End、Complete），让VLA模型理解和管理多机器人任务中的时间依赖，提供结构化和可解释的规划框架，增强任务规划和跨机器人协调能力。\n# 论文使用数据集和训练资源\n- 使用Open X-Embodiment (OXE)数据集进行预训练\n- 使用Bridge Data V2进行微调\n- 收集458条跨6个不同任务的真实机器人轨迹数据\n- 额外收集980条人类演示轨迹\n- 在16个A100 GPU上进行训练\n# 论文使用的评估环境和评估指标\n**评估环境**：在三种不同双机器人实体上进行验证：双UR5e、双Franka、双AgileX机器人。设计了六个协作任务：PickBread（拾取面包）、PickFruits（拾取水果）、WipePlate（擦拭盘子）、InsertPlate（插入盘子）、PullString（拉绳）、BuildBlocks（构建积木）。同时在RLBench2和RoboTwin仿真基准上进行评估。\n\n**评估指标**：主要使用平均成功率（%）作为评估指标，在每个任务上报告成功次数/总试验次数的比率，并通过多个任务的平均成功率来评估整体性能。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Shanghai University（上海大学）</p>\n<h1>论文概述</h1>\n<p>论文介绍了ET-VLA（Embodiment Transfer Learning for Vision-Language-Action），一种用于将预训练VLA模型高效迁移到多机器人系统的新型框架。传统自回归VLA模型（如RT-2、OpenVLA）在单臂机器人任务上表现优异，但在多机器人协作场景中性能显著下降，经常无法生成结构有效的动作序列。ET-VLA通过合成继续预训练（SCP）和实体化思维图（EGoT）技术，在双机器人系统上显著提升性能，在六个真实世界任务中成功率比OpenVLA高出53.2%。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>深入分析了现有自回归VLA模型在多机器人多任务设置中的局限性</li><li>提出ET-VLA框架，包含合成继续预训练（SCP）和实体化思维图（EGoT）两个关键技术</li><li>在真实机器人和仿真环境中广泛评估，证明了相对于最先进VLA模型的优越性能</li></ul>\n<h1>论文方法描述</h1>\n<p><strong>合成继续预训练（SCP）</strong>：生成合成多机器人数据来预热模型，使其能够生成正确的动作序列和精确的动作token数量。SCP采用交叉采样方法，从batch中随机选择不同样本的动作tokens，组合成14个动作tokens（每个机器人7个DoF），使模型学习token到机器人的映射关系。</p>\n\n<p><strong>实体化思维图（EGoT）</strong>：将复杂任务分解为显式表示时间依赖关系的动作图。该方法定义了五个任务类型（Grasp、Release、Waiting、End、Complete），让VLA模型理解和管理多机器人任务中的时间依赖，提供结构化和可解释的规划框架，增强任务规划和跨机器人协调能力。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>使用Open X-Embodiment (OXE)数据集进行预训练</li><li>使用Bridge Data V2进行微调</li><li>收集458条跨6个不同任务的真实机器人轨迹数据</li><li>额外收集980条人类演示轨迹</li><li>在16个A100 GPU上进行训练</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<p><strong>评估环境</strong>：在三种不同双机器人实体上进行验证：双UR5e、双Franka、双AgileX机器人。设计了六个协作任务：PickBread（拾取面包）、PickFruits（拾取水果）、WipePlate（擦拭盘子）、InsertPlate（插入盘子）、PullString（拉绳）、BuildBlocks（构建积木）。同时在RLBench2和RoboTwin仿真基准上进行评估。</p>\n\n<p><strong>评估指标</strong>：主要使用平均成功率（%）作为评估指标，在每个任务上报告成功次数/总试验次数的比率，并通过多个任务的平均成功率来评估整体性能。</p>"
  },
  {
    "date": "2025-11-03",
    "title": "OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2511.01210",
    "summary_markdown": "# 论文研究单位\n\nPrinceton University、University of California, Los Angeles、Microsoft Research Asia\n# 论文概述\n\n论文提出了OmniVLA，一个多模态视觉-语言-动作（VLA）模型，通过集成红外、毫米波雷达和声学传感器等新型感知模态，实现超越RGB感知能力的机器人操控。该模型将异构传感器统一到图像空间中，利用传感器掩码图像这一核心创新，实现了物理基础的空间智能。\n# 论文核心贡献点\n\n1. 首个统一多个感知模态的VLA模型，整合红外、毫米波雷达和声学传感器\n2. 提出传感器掩码图像技术，实现传感器信息的空间定位和语义对齐\n3. 设计轻量级多传感VLA模型架构，支持数据高效学习\n4. 显著提升机器人操控任务的成功率和泛化能力\n# 论文方法描述\n\n核心方法是传感器掩码图像生成：\n- 传感器数据预处理：将毫米波雷达和麦克风阵列数据通过波束成形转换为二维热图\n- 分割掩码生成：使用GPT-4o生成分割提示词，结合Grounded SAM 2进行语义分割\n- 图像融合：将传感器图像在掩码区域叠加到RGB图像上\n\n模型架构采用预训练的SmolVLA作为基础：\n- 冻结视觉和语言编码器\n- 为每个传感器模态添加独立的多层感知机投影层\n- 投影后的传感器token与语言token拼接，输入大语言模型\n- 使用扩散式动作专家模块生成最终机器人动作\n# 论文使用数据集和训练资源\n\n硬件平台：SO101机械臂配合多模态传感器套件，包括RGB相机、红外热像仪、毫米波雷达和六麦克风圆形阵列\n\n训练数据：收集800个演示片段（每类传感器任务200个，通用抓取任务200个）\n\n计算资源：\n- 训练：多个Nvidia A100 GPU进行分布式训练\n- 推理：本地RTX 4090 GPU，支持15fps实时预测\n# 论文使用的评估环境和评估指标\n\n评估任务涵盖三种需要非视觉感知的操控任务：\n- 热传感任务：区分冷热饮料并抓取冷饮\n- 毫米波任务：透视纸箱定位隐藏物体\n- 声学任务：定位并 uncovering响铃手机\n\n评估指标：\n- 任务成功率：25次独立试验的成功率\n- 任务评分：0.5分选择正确目标+0.5分完成正确操作\n- 泛化能力：在未见任务上的少样本学习表现\n\n基准对比：\n- VLA-RGB：仅RGB输入的VLA模型\n- VLA-RAW：使用原始传感器数据的VLA模型\n\n实验结果：OmniVLA达到84%平均成功率，分别比RGB-only和raw-sensor基线高出59%和28%。",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>Princeton University、University of California, Los Angeles、Microsoft Research Asia</p>\n<h1>论文概述</h1>\n\n<p>论文提出了OmniVLA，一个多模态视觉-语言-动作（VLA）模型，通过集成红外、毫米波雷达和声学传感器等新型感知模态，实现超越RGB感知能力的机器人操控。该模型将异构传感器统一到图像空间中，利用传感器掩码图像这一核心创新，实现了物理基础的空间智能。</p>\n<h1>论文核心贡献点</h1>\n\n<ol><li>首个统一多个感知模态的VLA模型，整合红外、毫米波雷达和声学传感器</li><li>提出传感器掩码图像技术，实现传感器信息的空间定位和语义对齐</li><li>设计轻量级多传感VLA模型架构，支持数据高效学习</li><li>显著提升机器人操控任务的成功率和泛化能力</li></ol>\n<h1>论文方法描述</h1>\n\n<p>核心方法是传感器掩码图像生成：</p>\n<ul><li>传感器数据预处理：将毫米波雷达和麦克风阵列数据通过波束成形转换为二维热图</li><li>分割掩码生成：使用GPT-4o生成分割提示词，结合Grounded SAM 2进行语义分割</li><li>图像融合：将传感器图像在掩码区域叠加到RGB图像上</li></ul>\n\n<p>模型架构采用预训练的SmolVLA作为基础：</p>\n<ul><li>冻结视觉和语言编码器</li><li>为每个传感器模态添加独立的多层感知机投影层</li><li>投影后的传感器token与语言token拼接，输入大语言模型</li><li>使用扩散式动作专家模块生成最终机器人动作</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n\n<p>硬件平台：SO101机械臂配合多模态传感器套件，包括RGB相机、红外热像仪、毫米波雷达和六麦克风圆形阵列</p>\n\n<p>训练数据：收集800个演示片段（每类传感器任务200个，通用抓取任务200个）</p>\n\n<p>计算资源：</p>\n<ul><li>训练：多个Nvidia A100 GPU进行分布式训练</li><li>推理：本地RTX 4090 GPU，支持15fps实时预测</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<p>评估任务涵盖三种需要非视觉感知的操控任务：</p>\n<ul><li>热传感任务：区分冷热饮料并抓取冷饮</li><li>毫米波任务：透视纸箱定位隐藏物体</li><li>声学任务：定位并 uncovering响铃手机</li></ul>\n\n<p>评估指标：</p>\n<ul><li>任务成功率：25次独立试验的成功率</li><li>任务评分：0.5分选择正确目标+0.5分完成正确操作</li><li>泛化能力：在未见任务上的少样本学习表现</li></ul>\n\n<p>基准对比：</p>\n<ul><li>VLA-RGB：仅RGB输入的VLA模型</li><li>VLA-RAW：使用原始传感器数据的VLA模型</li></ul>\n\n<p>实验结果：OmniVLA达到84%平均成功率，分别比RGB-only和raw-sensor基线高出59%和28%。</p>"
  },
  {
    "date": "2025-11-02",
    "title": "Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots",
    "link": "http://arxiv.org/abs/2511.00917",
    "summary_markdown": "## 论文研究单位\n宾夕法尼亚大学（University of Pennsylvania），并开放项目主页：maestro-robot.github.io。\n## 论文概述\n论文提出maestro（Managerial Agent for Executing Sensorimotor Tasks in Robotics），一个以视觉语言模型（VLM）为中心的模块化智能体，通过编排包含感知、几何、控制、预训练视觉-动作模型与图像编辑等在内的“工具集”，以编写与执行代码的方式在真实世界中实现零样本通用机器人操作。该范式不依赖大规模机器人数据，而是在闭循环中“计划—反应—重计划”的执行与反馈回路中不断演化，适应新任务与新硬件。\n## 论文核心贡献点\n- 首个在零样本条件下能与最新视觉-语言-动作（VLA）模型竞争的模块化机器人策略。\n- 提出了以VLM为编码智能体、工具库为核心的系统化设计（覆盖几何与主动感知、碰撞规避、VLA高频监控），显著提升语义理解与精确执行的协同能力。\n- 系统性消融实验揭示主动感知与几何推理工具的必要性。\n- 展示在少量真实试验基础上的演化式改进机制，通过历史代码与失败分析进行上下文学习。\n## 论文方法描述\n- 总体架构：以VLM（文中采用Gemini Robotics-ER 1.5）为代码生成智能体，接收任务指令与场景图像，动态编写程序并实时执行与修改，形成“计划—反应—重计划”的闭环。\n- 工具模块设计（Tabletop / Mobile 双套件）：\n - 感知：原始RGB+本体感觉；分割与中心点；指向与任务相关关键点（受ReKep启发）；FoundationStereo深度；主动感知（手腕相机变焦/环视）。\n - 控制与几何：笛卡尔与手爪控制；cuRobo点云无碰撞运动规划；几何与线性代数（向量构造、距离、相对旋转、向量旋转）以支持空间推理。\n - 视觉运动策略：GraspGen抓取模型；π0.5 VLA作为可调用工具，配以本地托管的Qwen-2.5-VL-72B实现2Hz任务完成监控以高频中断与重规划。\n - 图像编辑：在图像上绘制关键点与6D位姿叠加，提升视觉定位与推理。\n - 移动操控：Mobile base状态估计（Faster-LIO）；语义地图缓存；主动探索工具；细粒度“nudge”局部微调与全局导航（Nav2）。\n- 演化式改进：记录历史执行代码、输出与失败分析，作为上下文样例供后续尝试学习与优化。\n## 论文使用数据集和训练资源\n- 评估方法：为检验零样本泛化，采用STAR-Gen生成评测场景，固定对比基线；5次试验/任务（初始+4次扰动）。\n- 真实世界平台与任务：\n - 桌面操控：Franka Emika Panda（7-DoF）+ Robotiq 2F 手爪；手腕与第三人称相机；DROID平台。任务包括拾取放置、可变形物体（折毛巾/T恤）、铰接物体（开柜）、空间推理（紫色面朝上旋转立方体）、工具使用（用刀切香蕉）、物体功能（挂杯）、记忆与长时序（擦白板指令后按指令叠杯）。\n - 移动操控：Unitree Go2-W 四足机器人 + AgileX PiPER机械臂；校准手腕相机。任务包括长时序取物、投掷、主动探索（搜索并返回）、物体功能（按按钮开门）。\n- 预训练/基础模型：\n - VLM：Gemini Robotics-ER 1.5（用于代码生成与视觉推理）；Qwen-2.5-VL-72B（本地高频监控与中断）。\n - VLA：π0-FAST-DROID 与 π0.5-DROID（作为可调用工具）。\n - 其他：GraspGen（抓取）、FoundationStereo（深度估计）、cuRobo（无碰撞规划）、Faster-LIO（移动基定位）、Nav2（导航）。\n## 论文使用的评估环境和评估指标\n- 评估环境：两类真实世界本体——桌面操控（DROID、Franka）、移动操控（四足+机械臂），跨场景扰动（物体、位姿、语言指令、背景等）形成零样本测试集。\n- 评估指标：\n - 主要指标：平均任务进度（0–100，数值越高越好），基于STAR-Gen与任务分解子目标的可量化评分。\n - 对比基线：Gemini Robotics Agent（代码即策略）、π0-FAST-DROID、π0.5-DROID；另含maestro+π0.5（VLA作为工具并联）。\n - 消融设置：去除主动感知与几何模块对性能的影响。\n - 跨任务类别：在桌面与移动操控的多样任务上进行对比与误差分析。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>宾夕法尼亚大学（University of Pennsylvania），并开放项目主页：maestro-robot.github.io。</p>\n<h2>论文概述</h2>\n<p>论文提出maestro（Managerial Agent for Executing Sensorimotor Tasks in Robotics），一个以视觉语言模型（VLM）为中心的模块化智能体，通过编排包含感知、几何、控制、预训练视觉-动作模型与图像编辑等在内的“工具集”，以编写与执行代码的方式在真实世界中实现零样本通用机器人操作。该范式不依赖大规模机器人数据，而是在闭循环中“计划—反应—重计划”的执行与反馈回路中不断演化，适应新任务与新硬件。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>首个在零样本条件下能与最新视觉-语言-动作（VLA）模型竞争的模块化机器人策略。</li><li>提出了以VLM为编码智能体、工具库为核心的系统化设计（覆盖几何与主动感知、碰撞规避、VLA高频监控），显著提升语义理解与精确执行的协同能力。</li><li>系统性消融实验揭示主动感知与几何推理工具的必要性。</li><li>展示在少量真实试验基础上的演化式改进机制，通过历史代码与失败分析进行上下文学习。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>总体架构：以VLM（文中采用Gemini Robotics-ER 1.5）为代码生成智能体，接收任务指令与场景图像，动态编写程序并实时执行与修改，形成“计划—反应—重计划”的闭环。</li><li>工具模块设计（Tabletop / Mobile 双套件）：</li></ul>\n<p> - 感知：原始RGB+本体感觉；分割与中心点；指向与任务相关关键点（受ReKep启发）；FoundationStereo深度；主动感知（手腕相机变焦/环视）。</p>\n<p> - 控制与几何：笛卡尔与手爪控制；cuRobo点云无碰撞运动规划；几何与线性代数（向量构造、距离、相对旋转、向量旋转）以支持空间推理。</p>\n<p> - 视觉运动策略：GraspGen抓取模型；π0.5 VLA作为可调用工具，配以本地托管的Qwen-2.5-VL-72B实现2Hz任务完成监控以高频中断与重规划。</p>\n<p> - 图像编辑：在图像上绘制关键点与6D位姿叠加，提升视觉定位与推理。</p>\n<p> - 移动操控：Mobile base状态估计（Faster-LIO）；语义地图缓存；主动探索工具；细粒度“nudge”局部微调与全局导航（Nav2）。</p>\n<ul><li>演化式改进：记录历史执行代码、输出与失败分析，作为上下文样例供后续尝试学习与优化。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>评估方法：为检验零样本泛化，采用STAR-Gen生成评测场景，固定对比基线；5次试验/任务（初始+4次扰动）。</li><li>真实世界平台与任务：</li></ul>\n<p> - 桌面操控：Franka Emika Panda（7-DoF）+ Robotiq 2F 手爪；手腕与第三人称相机；DROID平台。任务包括拾取放置、可变形物体（折毛巾/T恤）、铰接物体（开柜）、空间推理（紫色面朝上旋转立方体）、工具使用（用刀切香蕉）、物体功能（挂杯）、记忆与长时序（擦白板指令后按指令叠杯）。</p>\n<p> - 移动操控：Unitree Go2-W 四足机器人 + AgileX PiPER机械臂；校准手腕相机。任务包括长时序取物、投掷、主动探索（搜索并返回）、物体功能（按按钮开门）。</p>\n<ul><li>预训练/基础模型：</li></ul>\n<p> - VLM：Gemini Robotics-ER 1.5（用于代码生成与视觉推理）；Qwen-2.5-VL-72B（本地高频监控与中断）。</p>\n<p> - VLA：π0-FAST-DROID 与 π0.5-DROID（作为可调用工具）。</p>\n<p> - 其他：GraspGen（抓取）、FoundationStereo（深度估计）、cuRobo（无碰撞规划）、Faster-LIO（移动基定位）、Nav2（导航）。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：两类真实世界本体——桌面操控（DROID、Franka）、移动操控（四足+机械臂），跨场景扰动（物体、位姿、语言指令、背景等）形成零样本测试集。</li><li>评估指标：</li></ul>\n<p> - 主要指标：平均任务进度（0–100，数值越高越好），基于STAR-Gen与任务分解子目标的可量化评分。</p>\n<p> - 对比基线：Gemini Robotics Agent（代码即策略）、π0-FAST-DROID、π0.5-DROID；另含maestro+π0.5（VLA作为工具并联）。</p>\n<p> - 消融设置：去除主动感知与几何模块对性能的影响。</p>\n<p> - 跨任务类别：在桌面与移动操控的多样任务上进行对比与误差分析。</p>"
  },
  {
    "date": "2025-10-31",
    "title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection",
    "link": "http://arxiv.org/abs/2511.00139",
    "summary_markdown": "### 论文研究单位\nByteDance Seed\n### 论文概述\n论文针对灵巧手在一般机器人中的manipulation挑战，提出一种共享自治（Shared Autonomy）框架，结合人类VR远程操作和自主DexGrasp-VLA策略（用于手部控制），以高效收集协调手臂-手部演示数据。该框架通过分工控制宏-微运动域：人类引导手臂，VLA Copilot执行精细抓取，降低认知负担。数据用于训练端到端VLA策略（含Arm-Hand Feature Enhancement模块），并通过校正远程操作（Corrective Teleoperation）持续改进。实验显示，策略在50+物体上达约90%成功率，数据质量高。\n### 论文核心贡献点\n- 提出多模态VLA Copilot（DexGrasp-VLA）：融合视觉、语言、触觉和本体感受，实现力适应抓取。\n- 设计共享自治框架：人类VR控制手臂，VLA自主控制手部，高效收集高质量演示。\n- 开发Arm-Hand Feature Enhancement架构：显式建模手臂（宏运动）和手部（微运动）的互补特征，提升协调性。\n- 实现校正人类在环远程操作：采集失败恢复数据，迭代优化策略鲁棒性。\n### 论文方法描述\n方法分四阶段：\n1. **DexGrasp-VLA策略训练**：先训练LSTM“盲策略”（融合参数化力控制和远程操作数据），再扩展为多模态VLA（整合视觉、触觉、语言和本体感受）。\n2. **共享自治数据收集**：人类通过VR相对运动映射远程操作手臂（90Hz），DexGrasp-VLA并行控制手部（30Hz），采集同步手臂-手部数据。\n3. **端到端VLA策略学习**：基于预训练VLA模型（SFT），添加Arm-Hand Feature Enhancement模块（共享表示+MLP编码的手臂/手部特征），优化总损失（含主损失+辅助损失）。\n4. **校正远程操作系统**：部署策略自动记录成功/失败轨迹；人类介入纠错采集恢复数据，迭代SFT更新策略。\n### 论文使用数据集和训练资源\n- **LSTM预训练数据集**：218条轨迹（150条人类远程操作+68条力控制数据），覆盖20+物体，用于训练“盲”力适应抓取策略。\n- **DexGrasp-VLA手部数据集**：180条抓取轨迹（杂乱场景中60种物体），含手部RGB、触觉（力向量+空间嵌入）、本体状态和动作。\n- **端到端手臂-手部数据集**：100条共享自治演示，覆盖20种物体，含多视角RGB、语言指令和手臂-手部联合动作。\n- **校正干预数据集**：100条轨迹（50条方向失败恢复+50条角落案例）。\n- **训练资源**：基于开源框架LeRobot（Fine-tuning预训练VLA模型π₀）；硬件包括UR3e机械臂（6-DoF）、Xhand五指手（12-DoF）、三RGB-D摄像头及触觉传感器。\n### 论文使用的评估环境和评估指标\n- **评估环境**：集成机器人平台（UR3e + Xhand），装配三RGB-D摄像头（两静态+一手腕-mounted）；任务场景包括杂杂乱物体抓取、表单清理等。\n- **评估指标**：\n - 抓取成功率：DexGrasp-VLA在杂乱场景中95.5%，端到端策略在50+物体上约90%。\n - 消融研究：验证触觉特征、手部-手臂特征增强、校正远程操作对性能的影响（如成功率、鲁棒性）。\n - 数据效率：远程操作会话时长、认知负担（经验显示~30分钟无疲劳）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>ByteDance Seed</p>\n<h3>论文概述</h3>\n<p>论文针对灵巧手在一般机器人中的manipulation挑战，提出一种共享自治（Shared Autonomy）框架，结合人类VR远程操作和自主DexGrasp-VLA策略（用于手部控制），以高效收集协调手臂-手部演示数据。该框架通过分工控制宏-微运动域：人类引导手臂，VLA Copilot执行精细抓取，降低认知负担。数据用于训练端到端VLA策略（含Arm-Hand Feature Enhancement模块），并通过校正远程操作（Corrective Teleoperation）持续改进。实验显示，策略在50+物体上达约90%成功率，数据质量高。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出多模态VLA Copilot（DexGrasp-VLA）：融合视觉、语言、触觉和本体感受，实现力适应抓取。</li><li>设计共享自治框架：人类VR控制手臂，VLA自主控制手部，高效收集高质量演示。</li><li>开发Arm-Hand Feature Enhancement架构：显式建模手臂（宏运动）和手部（微运动）的互补特征，提升协调性。</li><li>实现校正人类在环远程操作：采集失败恢复数据，迭代优化策略鲁棒性。</li></ul>\n<h3>论文方法描述</h3>\n<p>方法分四阶段：</p>\n<ol><li><strong>DexGrasp-VLA策略训练</strong>：先训练LSTM“盲策略”（融合参数化力控制和远程操作数据），再扩展为多模态VLA（整合视觉、触觉、语言和本体感受）。</li><li><strong>共享自治数据收集</strong>：人类通过VR相对运动映射远程操作手臂（90Hz），DexGrasp-VLA并行控制手部（30Hz），采集同步手臂-手部数据。</li><li><strong>端到端VLA策略学习</strong>：基于预训练VLA模型（SFT），添加Arm-Hand Feature Enhancement模块（共享表示+MLP编码的手臂/手部特征），优化总损失（含主损失+辅助损失）。</li><li><strong>校正远程操作系统</strong>：部署策略自动记录成功/失败轨迹；人类介入纠错采集恢复数据，迭代SFT更新策略。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>LSTM预训练数据集</strong>：218条轨迹（150条人类远程操作+68条力控制数据），覆盖20+物体，用于训练“盲”力适应抓取策略。</li><li><strong>DexGrasp-VLA手部数据集</strong>：180条抓取轨迹（杂乱场景中60种物体），含手部RGB、触觉（力向量+空间嵌入）、本体状态和动作。</li><li><strong>端到端手臂-手部数据集</strong>：100条共享自治演示，覆盖20种物体，含多视角RGB、语言指令和手臂-手部联合动作。</li><li><strong>校正干预数据集</strong>：100条轨迹（50条方向失败恢复+50条角落案例）。</li><li><strong>训练资源</strong>：基于开源框架LeRobot（Fine-tuning预训练VLA模型π₀）；硬件包括UR3e机械臂（6-DoF）、Xhand五指手（12-DoF）、三RGB-D摄像头及触觉传感器。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：集成机器人平台（UR3e + Xhand），装配三RGB-D摄像头（两静态+一手腕-mounted）；任务场景包括杂杂乱物体抓取、表单清理等。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 抓取成功率：DexGrasp-VLA在杂乱场景中95.5%，端到端策略在50+物体上约90%。</p>\n<p> - 消融研究：验证触觉特征、手部-手臂特征增强、校正远程操作对性能的影响（如成功率、鲁棒性）。</p>\n<p> - 数据效率：远程操作会话时长、认知负担（经验显示~30分钟无疲劳）。</p>"
  },
  {
    "date": "2025-10-30",
    "title": "Self-Improving Vision-Language-Action Models with Data Generation via Residual RL",
    "link": "http://arxiv.org/abs/2511.00091",
    "summary_markdown": "```markdown\n论文研究单位\nNVIDIA, CMU, UC Berkeley, UT Austin\n\n论文概述\n该论文提出了一种名为Probe, Learn, Distill (PLD) 的三阶段框架，旨在通过残差强化学习和分布感知的数据收集，自主改进视觉-语言-动作（VLA）模型，而无需额外的人类演示数据。该方法首先冻结VLA主干，通过离线策略RL训练轻量级残差专家；然后采用混合回放方案，使残差干预偏向于基础策略频繁访问的状态；最后通过标准SFT将精心策划的轨迹蒸馏回通用模型。实验表明，PLD在LIBERO基准测试中达到99%任务成功率，在SimplerEnv中实现超过50%的性能提升，并在真实世界的Franka和YAM手臂灵巧操作任务中取得100%成功率。\n\n论文核心贡献点\n1. 自主后训练配方：提出了一种使VLA模型无需额外人工演示即可自主改进的后训练流程。\n2. RL生成数据的系统研究：分析了自动数据收集的关键组成部分，并进行了广泛的实验来研究RL生成数据如何影响对未见任务的泛化能力。\n3. 综合实证验证：提供了大规模的设计选择消融实验，并在真实机器人硬件上展示了超过99%的成功率，实现了连续1小时的GPU插拔操作而无需人工干预。\n\n论文方法描述\nPLD框架包含三个阶段：\n1. 专家获取（Probe）：冻结VLA主干，通过离线策略RL训练轻量级残差专家，这些专家在基础策略失败时接管，探测VLA通用模型的失败区域。\n2. 数据收集（Learn）：采用混合回放方案，使残差干预偏向于基础策略频繁访问的状态，对齐收集的轨迹与通用模型的部署分布，同时捕获恢复行为。\n3. 监督微调（Distill）：将收集的多任务数据通过SFT蒸馏回基础模型，该过程与VLA架构无关，支持流匹配和自回归动作头。\n\n论文使用数据集和训练资源\n数据集：LIBERO（包含130个语言条件操作任务，分为四个套件）、SimplerEnv（旨在实现高模拟到真实世界相关性）。\n训练资源：使用NVIDIA GPU进行训练，具体GPU型号和数量在提供的文本中未明确提及。\n\n论文使用的评估环境和评估指标\n评估环境：模拟环境（LIBERO、SimplerEnv）和真实世界环境（Franka Emika Panda机械臂、YAM双手机械臂）。\n评估指标：任务成功率（Success Rate）、平均回报（Average Return）、零样本泛化性能（Zero-shot Generalization）、样本效率（Sample Efficiency）。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位</p>\n<p>NVIDIA, CMU, UC Berkeley, UT Austin</p>\n\n<p>论文概述</p>\n<p>该论文提出了一种名为Probe, Learn, Distill (PLD) 的三阶段框架，旨在通过残差强化学习和分布感知的数据收集，自主改进视觉-语言-动作（VLA）模型，而无需额外的人类演示数据。该方法首先冻结VLA主干，通过离线策略RL训练轻量级残差专家；然后采用混合回放方案，使残差干预偏向于基础策略频繁访问的状态；最后通过标准SFT将精心策划的轨迹蒸馏回通用模型。实验表明，PLD在LIBERO基准测试中达到99%任务成功率，在SimplerEnv中实现超过50%的性能提升，并在真实世界的Franka和YAM手臂灵巧操作任务中取得100%成功率。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>自主后训练配方：提出了一种使VLA模型无需额外人工演示即可自主改进的后训练流程。</li><li>RL生成数据的系统研究：分析了自动数据收集的关键组成部分，并进行了广泛的实验来研究RL生成数据如何影响对未见任务的泛化能力。</li><li>综合实证验证：提供了大规模的设计选择消融实验，并在真实机器人硬件上展示了超过99%的成功率，实现了连续1小时的GPU插拔操作而无需人工干预。</li></ol>\n\n<p>论文方法描述</p>\n<p>PLD框架包含三个阶段：</p>\n<ol><li>专家获取（Probe）：冻结VLA主干，通过离线策略RL训练轻量级残差专家，这些专家在基础策略失败时接管，探测VLA通用模型的失败区域。</li><li>数据收集（Learn）：采用混合回放方案，使残差干预偏向于基础策略频繁访问的状态，对齐收集的轨迹与通用模型的部署分布，同时捕获恢复行为。</li><li>监督微调（Distill）：将收集的多任务数据通过SFT蒸馏回基础模型，该过程与VLA架构无关，支持流匹配和自回归动作头。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：LIBERO（包含130个语言条件操作任务，分为四个套件）、SimplerEnv（旨在实现高模拟到真实世界相关性）。</p>\n<p>训练资源：使用NVIDIA GPU进行训练，具体GPU型号和数量在提供的文本中未明确提及。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：模拟环境（LIBERO、SimplerEnv）和真实世界环境（Franka Emika Panda机械臂、YAM双手机械臂）。</p>\n<p>评估指标：任务成功率（Success Rate）、平均回报（Average Return）、零样本泛化性能（Zero-shot Generalization）、样本效率（Sample Efficiency）。</p>\n<p>```</p>"
  },
  {
    "date": "2025-10-30",
    "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail",
    "link": "http://arxiv.org/abs/2511.00088",
    "summary_markdown": "# 论文研究单位\nNVIDIA\n# 论文概述\nAlpamayo-R1 (AR1) 是一个视觉-语言-动作模型，旨在通过整合Chain of Causation推理与轨迹规划来增强自主驾驶在复杂驾驶场景中的决策能力。该模型专注于解决安全关键的尾部场景问题，这些场景中监督稀疏且因果理解有限。\n# 论文核心贡献点\n1. **Chain of Causation (CoC)数据集**：构建了混合自动标注和人工标注流程，产生与驾驶行为对齐的决策导向、因果关联的推理轨迹\n2. **模块化VLA架构**：结合Cosmos-Reason（为物理AI应用预训练的视觉-语言模型）与基于扩散的轨迹解码器，实时生成动态可行的计划\n3. **多阶段训练策略**：使用监督微调来引出推理能力，并通过强化学习优化推理质量和推理-行动一致性\n# 论文方法描述\n## 架构设计\n- **VLM主干网络**：采用Cosmos-Reason作为视觉-语言模型骨干\n- **高效视觉编码**：支持单图像标记化、多相机标记化和多相机视频标记化，实现最高20倍token压缩率\n- **扩散式轨迹解码**：使用流匹配框架将离散轨迹token转换为连续表示，基于unicycle动力学建模\n## 推理与行动结合\n通过统一的序列预测框架，将历史观测、推理过程和未来轨迹整合为统一序列，实现推理与行动预测的无缝结合。\n## 数据构建方法\n**Chain of Causation数据集**采用结构化标注框架：\n- **决策标注**：定义封闭集合的高层驾驶决策\n- **关键组件标注**：开放集合的因果因素\n- **混合标注流程**：结合人工标注(10-20%数据)和自动标注(80-90%数据)\n# 论文使用数据集和训练资源\n- **基础数据集**：基于nuScenes等公开驾驶数据集\n- **CoC数据集**：混合人工和自动标注，包含结构化推理轨迹\n- **物理AI领域数据**：覆盖机器人、医疗、智慧城市等多领域数据用于预训练\n- **人类标注数据**：10-20%高质量人工标注样本用于监督微调和评估\n- **自动标注数据**：大规模自动生成的结构化标注数据\n# 论文使用的评估环境和评估指标\n## 评估环境\n- **开环评估**：标准数据集上的规划准确性评估\n- **闭环评估**：仿真环境中的驾驶性能测试\n- **实车测试**：车载系统验证实际部署性能\n## 评估指标\n- **规划性能**：在挑战性场景中规划准确性提升12%\n- **安全性指标**：\n - 离路率降低35%\n - 近距离遭遇率降低25%\n- **推理质量**：通过大推理模型评估，推理质量提升45%\n- **推理-行动一致性**：一致性提升37%\n- **模型扩展性**：从0.5B到7B参数的一致改进\n- **实时性能**：端到端延迟99毫秒，支持实时部署",
    "summary_html": "<h1>论文研究单位</h1>\n<p>NVIDIA</p>\n<h1>论文概述</h1>\n<p>Alpamayo-R1 (AR1) 是一个视觉-语言-动作模型，旨在通过整合Chain of Causation推理与轨迹规划来增强自主驾驶在复杂驾驶场景中的决策能力。该模型专注于解决安全关键的尾部场景问题，这些场景中监督稀疏且因果理解有限。</p>\n<h1>论文核心贡献点</h1>\n<ol><li><strong>Chain of Causation (CoC)数据集</strong>：构建了混合自动标注和人工标注流程，产生与驾驶行为对齐的决策导向、因果关联的推理轨迹</li><li><strong>模块化VLA架构</strong>：结合Cosmos-Reason（为物理AI应用预训练的视觉-语言模型）与基于扩散的轨迹解码器，实时生成动态可行的计划</li><li><strong>多阶段训练策略</strong>：使用监督微调来引出推理能力，并通过强化学习优化推理质量和推理-行动一致性</li></ol>\n<h1>论文方法描述</h1>\n<h2>架构设计</h2>\n<ul><li><strong>VLM主干网络</strong>：采用Cosmos-Reason作为视觉-语言模型骨干</li><li><strong>高效视觉编码</strong>：支持单图像标记化、多相机标记化和多相机视频标记化，实现最高20倍token压缩率</li><li><strong>扩散式轨迹解码</strong>：使用流匹配框架将离散轨迹token转换为连续表示，基于unicycle动力学建模</li></ul>\n<h2>推理与行动结合</h2>\n<p>通过统一的序列预测框架，将历史观测、推理过程和未来轨迹整合为统一序列，实现推理与行动预测的无缝结合。</p>\n<h2>数据构建方法</h2>\n<p><strong>Chain of Causation数据集</strong>采用结构化标注框架：</p>\n<ul><li><strong>决策标注</strong>：定义封闭集合的高层驾驶决策</li><li><strong>关键组件标注</strong>：开放集合的因果因素</li><li><strong>混合标注流程</strong>：结合人工标注(10-20%数据)和自动标注(80-90%数据)</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>基础数据集</strong>：基于nuScenes等公开驾驶数据集</li><li><strong>CoC数据集</strong>：混合人工和自动标注，包含结构化推理轨迹</li><li><strong>物理AI领域数据</strong>：覆盖机器人、医疗、智慧城市等多领域数据用于预训练</li><li><strong>人类标注数据</strong>：10-20%高质量人工标注样本用于监督微调和评估</li><li><strong>自动标注数据</strong>：大规模自动生成的结构化标注数据</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<h2>评估环境</h2>\n<ul><li><strong>开环评估</strong>：标准数据集上的规划准确性评估</li><li><strong>闭环评估</strong>：仿真环境中的驾驶性能测试</li><li><strong>实车测试</strong>：车载系统验证实际部署性能</li></ul>\n<h2>评估指标</h2>\n<ul><li><strong>规划性能</strong>：在挑战性场景中规划准确性提升12%</li><li><strong>安全性指标</strong>：</li></ul>\n<p> - 离路率降低35%</p>\n<p> - 近距离遭遇率降低25%</p>\n<ul><li><strong>推理质量</strong>：通过大推理模型评估，推理质量提升45%</li><li><strong>推理-行动一致性</strong>：一致性提升37%</li><li><strong>模型扩展性</strong>：从0.5B到7B参数的一致改进</li><li><strong>实时性能</strong>：端到端延迟99毫秒，支持实时部署</li></ul>"
  },
  {
    "date": "2025-10-31",
    "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2510.27607",
    "summary_markdown": "### 论文研究单位\nKAIST (韩国科学技术院) 和 RLWRLD\n### 论文概述\n本文提出了双流扩散（DUST）框架，一种用于增强视觉-语言-动作模型（VLA）的世界模型。DUST通过在多模态扩散Transformer中维护独立的模态流，同时通过跨模态注意力实现知识共享，解决了联合预测动作和未来观测时存在的模态冲突问题。该方法还包括解耦的训练算法和异步采样策略，支持推理时的扩展。\n### 论文核心贡献点\n1. 提出了双流扩散（DUST）框架，结合世界模型增强VLA，通过独立模态流和跨模态注意力解决模态冲突。\n2. 设计了多模态扩散Transformer（MMDiT）架构，分别处理动作和视觉流，实现双向信息传递。\n3. 开发了基于独立噪声调度的解耦训练算法，学习动作与视觉之间的因果关系。\n4. 引入了异步联合采样策略，允许视觉和动作标记以不同速率进行扩散，实现推理时的性能扩展。\n### 论文方法描述\nDUST架构包括：\n1. **VLM主干网络**：使用预训练的视觉-语言模型提取当前观测和指令的语义特征。\n2. **多模态扩散Transformer（MMDiT）**：包含12个MMDiT层和4个模态特定的DiT层。动作和视觉标记流在MMDiT层中通过共享交叉注意力层交互，但在其他操作中保持独立。\n3. **解耦训练**：动作和未来视觉观测分别添加独立噪声，使用模态特定的流匹配损失函数优化联合目标。\n4. **异步采样**：推理时，视觉标记以更高频率（如32步）去噪，动作标记以较低频率（如4步）去噪，平衡效率与精度。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **模拟环境**：RoboCasa（24个厨房任务）和GR-1（24个人形机器人桌面任务）。\n - **真实世界**：Franka Research 3机械臂的4个取放任务。\n - **迁移学习**：使用BridgeV2的无动作视频进行预训练。\n- **训练资源**：\n - VLM主干：冻结Eagle-2模型。\n - 扩散模型：使用Flow Matching目标，批大小未明确，迭代次数未明确。\n - 损失权重：\\(\\lambda_{\\text{WM}} = 1.0\\)。\n - 硬件：未明确说明。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟**：RoboCasa和GR-1基准，使用不同规模的演示数据（100、300、1000条/任务）。\n - **真实世界**：Franka Research 3机械臂在桌面场景执行取放任务。\n- **评估指标**：\n - **成功率（%）**：任务完成的主要指标，涵盖不同任务类别（如取放、开合装置）。\n - **平均成功率**：跨任务类别的平均表现。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>KAIST (韩国科学技术院) 和 RLWRLD</p>\n<h3>论文概述</h3>\n<p>本文提出了双流扩散（DUST）框架，一种用于增强视觉-语言-动作模型（VLA）的世界模型。DUST通过在多模态扩散Transformer中维护独立的模态流，同时通过跨模态注意力实现知识共享，解决了联合预测动作和未来观测时存在的模态冲突问题。该方法还包括解耦的训练算法和异步采样策略，支持推理时的扩展。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了双流扩散（DUST）框架，结合世界模型增强VLA，通过独立模态流和跨模态注意力解决模态冲突。</li><li>设计了多模态扩散Transformer（MMDiT）架构，分别处理动作和视觉流，实现双向信息传递。</li><li>开发了基于独立噪声调度的解耦训练算法，学习动作与视觉之间的因果关系。</li><li>引入了异步联合采样策略，允许视觉和动作标记以不同速率进行扩散，实现推理时的性能扩展。</li></ol>\n<h3>论文方法描述</h3>\n<p>DUST架构包括：</p>\n<ol><li><strong>VLM主干网络</strong>：使用预训练的视觉-语言模型提取当前观测和指令的语义特征。</li><li><strong>多模态扩散Transformer（MMDiT）</strong>：包含12个MMDiT层和4个模态特定的DiT层。动作和视觉标记流在MMDiT层中通过共享交叉注意力层交互，但在其他操作中保持独立。</li><li><strong>解耦训练</strong>：动作和未来视觉观测分别添加独立噪声，使用模态特定的流匹配损失函数优化联合目标。</li><li><strong>异步采样</strong>：推理时，视觉标记以更高频率（如32步）去噪，动作标记以较低频率（如4步）去噪，平衡效率与精度。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：RoboCasa（24个厨房任务）和GR-1（24个人形机器人桌面任务）。</p>\n<p> - <strong>真实世界</strong>：Franka Research 3机械臂的4个取放任务。</p>\n<p> - <strong>迁移学习</strong>：使用BridgeV2的无动作视频进行预训练。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - VLM主干：冻结Eagle-2模型。</p>\n<p> - 扩散模型：使用Flow Matching目标，批大小未明确，迭代次数未明确。</p>\n<p> - 损失权重：\\(\\lambda_{\\text{WM}} = 1.0\\)。</p>\n<p> - 硬件：未明确说明。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟</strong>：RoboCasa和GR-1基准，使用不同规模的演示数据（100、300、1000条/任务）。</p>\n<p> - <strong>真实世界</strong>：Franka Research 3机械臂在桌面场景执行取放任务。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（%）</strong>：任务完成的主要指标，涵盖不同任务类别（如取放、开合装置）。</p>\n<p> - <strong>平均成功率</strong>：跨任务类别的平均表现。</p>"
  },
  {
    "date": "2025-10-31",
    "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
    "link": "http://arxiv.org/abs/2510.27545",
    "summary_markdown": "# 论文研究单位\nZhiCheng AI（致诚智能）、伊利诺伊大学厄巴纳-香槟分校（UIUC）、清华大学、北京大学\n# 论文概述\n论文提出 EBT-Policy（Energy-Based Transformer Policy），用能量函数直接刻画可行动轨迹的能量景观，通过能量最小化而非扩散去噪生成动作。该方法旨在解决扩散/流模型在机器人策略学习中的高计算成本、暴露偏差（exposure bias）和推理不稳定等问题。研究在仿真基准（robomimic）与真实双机械臂平台（共4个RGB相机、台式场景）上开展系统实验，验证 EBT-Policy 在训练与推理效率、鲁棒性与泛化能力上的优势，并展示其“涌现能力”——零样本的失败重试与恢复行为。\n# 论文核心贡献点\n- 稳定性与鲁棒性：学习单一、时不变的能量景观，减少暴露偏差与错误累积；在分布外（OOD）状态下通过平衡动力学“回拉”至数据流形，提升在真实复杂环境中的稳定性。\n- 高效性：推理仅需2步即可在部分任务上达到与扩散策略相同的成功率，相比扩散策略100步，实现50倍计算量减少；训练轮次减少约55%，训练与推理显著提速。\n- 涌现能力：在未提供重试数据的前提下，仅靠行为克隆与能量驱动即可实现零样本失败恢复与重试策略，表现出类似物理推理的行为选择。\n- 统一的可解释信号：能量标量提供不确定性与置信度度量，驱动动态计算分配（困难状态多步、简单状态少步），并在训练中通过正则化手段提升多模态分布学习的可控性与稳定性。\n# 论文方法描述\n- 总体思想：将可视运动策略建模为基于能量函数的能量模型（EBM），以能量最小化过程替代扩散模型的去噪/流匹配。动作序列 a 在给定观察 o 与语言指令 ℓ 下，通过迭代沿能量景观负梯度下降生成，最终收敛到低能量（高似然）轨迹。\n- 生成与采样：采用随机马尔可夫链蒙特卡罗（MCMC）与 Langevin 动态的迭代更新：\n ŷ_{i+1} = ŷ_i − α_i ∇_ŷ E_θ(x, ŷ_i) + η_i\n 其中 α_i 为能量缩放的步幅，η_i 为噪声项（可按余弦退火调度）。\n- 训练细节：随机化 MCMC 步数、缩放 Langevin 噪声、步幅随机化与 Nesterov 加速梯度用于多模态探索与稳定优化；引入能量缩放步幅、预采样归一化（RMSNorm）、梯度裁剪等正则与稳定化机制；在每次迭代中以随机噪声初始化目标轨迹并逐步“去噪”，损失为去噪轨迹与演示轨迹的均方误差。\n- 推理与动态计算：执行自适应能量下降（梯度范数低于阈值终止或达到最大步数），在不同场景自动分配计算步数，困难状态迭代更久、简单状态快速收敛，形成不确定性与计算的自适应联动。\n# 论文使用数据集和训练资源\n- 仿真数据：robomimic 基准的四项任务（Lift、Can、Square、Tool Hang），每项在模拟环境做50回合评估成功率。\n- 真实数据：在双机械臂平台（4个RGB相机、台式环境）上通过遥操作采集的数据，用于三项真实任务（Fold Towel、Pick & Place、Collect/Place Pan），包含语言指令与多模态观察。\n- 架构与对比：设置两种模型变体\n - EBT-Policy-S：约30M参数，仿真对比用（视觉编码器 ResNet-18，无语言编码器）。\n - EBT-Policy-R：约100M参数，真实任务用（视觉编码器 DINOv2-S，文本编码器 T5-S）。\n 与扩散策略（DP）进行同等规模（≤150M）下的公平对比。\n# 论文使用的评估环境和评估指标\n- 环境与场景：仿真 robomimic（标准、可复现实验）；真实双机械臂桌面环境（4相机、可变光照与传感器噪声）。\n- 指标：任务成功率（SR，百分比），训练收敛轮次/步数（训练效率），推理步数与计算量（推理效率），稳定性与分布外鲁棒性（含失败恢复/重试的定性表现）。\n- 结果摘要\n - 仿真：在Lift、Can、Square、Tool Hang四项任务上均优于扩散策略（DP-10步为0，DP-100步在Square/Tool Hang未超越EBT），EBT两项（S、 R）均取得更高SR。\n - 真实：Fold Towel、Collect Pan、Pick & Place 三任务中，EBT-Policy-R在两项显著优于DP，Pick & Place略优或相当。\n - 训练与推理效率：EBT在约30轮即达100%成功率，DP需约90轮且推理步数更高；EBT在部分任务2步即收敛，DP在10步时成功率降至0，100步才稳定。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>ZhiCheng AI（致诚智能）、伊利诺伊大学厄巴纳-香槟分校（UIUC）、清华大学、北京大学</p>\n<h1>论文概述</h1>\n<p>论文提出 EBT-Policy（Energy-Based Transformer Policy），用能量函数直接刻画可行动轨迹的能量景观，通过能量最小化而非扩散去噪生成动作。该方法旨在解决扩散/流模型在机器人策略学习中的高计算成本、暴露偏差（exposure bias）和推理不稳定等问题。研究在仿真基准（robomimic）与真实双机械臂平台（共4个RGB相机、台式场景）上开展系统实验，验证 EBT-Policy 在训练与推理效率、鲁棒性与泛化能力上的优势，并展示其“涌现能力”——零样本的失败重试与恢复行为。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>稳定性与鲁棒性：学习单一、时不变的能量景观，减少暴露偏差与错误累积；在分布外（OOD）状态下通过平衡动力学“回拉”至数据流形，提升在真实复杂环境中的稳定性。</li><li>高效性：推理仅需2步即可在部分任务上达到与扩散策略相同的成功率，相比扩散策略100步，实现50倍计算量减少；训练轮次减少约55%，训练与推理显著提速。</li><li>涌现能力：在未提供重试数据的前提下，仅靠行为克隆与能量驱动即可实现零样本失败恢复与重试策略，表现出类似物理推理的行为选择。</li><li>统一的可解释信号：能量标量提供不确定性与置信度度量，驱动动态计算分配（困难状态多步、简单状态少步），并在训练中通过正则化手段提升多模态分布学习的可控性与稳定性。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>总体思想：将可视运动策略建模为基于能量函数的能量模型（EBM），以能量最小化过程替代扩散模型的去噪/流匹配。动作序列 a 在给定观察 o 与语言指令 ℓ 下，通过迭代沿能量景观负梯度下降生成，最终收敛到低能量（高似然）轨迹。</li><li>生成与采样：采用随机马尔可夫链蒙特卡罗（MCMC）与 Langevin 动态的迭代更新：</li></ul>\n<p> ŷ_{i+1} = ŷ_i − α_i ∇_ŷ E_θ(x, ŷ_i) + η_i</p>\n<p> 其中 α_i 为能量缩放的步幅，η_i 为噪声项（可按余弦退火调度）。</p>\n<ul><li>训练细节：随机化 MCMC 步数、缩放 Langevin 噪声、步幅随机化与 Nesterov 加速梯度用于多模态探索与稳定优化；引入能量缩放步幅、预采样归一化（RMSNorm）、梯度裁剪等正则与稳定化机制；在每次迭代中以随机噪声初始化目标轨迹并逐步“去噪”，损失为去噪轨迹与演示轨迹的均方误差。</li><li>推理与动态计算：执行自适应能量下降（梯度范数低于阈值终止或达到最大步数），在不同场景自动分配计算步数，困难状态迭代更久、简单状态快速收敛，形成不确定性与计算的自适应联动。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>仿真数据：robomimic 基准的四项任务（Lift、Can、Square、Tool Hang），每项在模拟环境做50回合评估成功率。</li><li>真实数据：在双机械臂平台（4个RGB相机、台式环境）上通过遥操作采集的数据，用于三项真实任务（Fold Towel、Pick & Place、Collect/Place Pan），包含语言指令与多模态观察。</li><li>架构与对比：设置两种模型变体</li></ul>\n<p> - EBT-Policy-S：约30M参数，仿真对比用（视觉编码器 ResNet-18，无语言编码器）。</p>\n<p> - EBT-Policy-R：约100M参数，真实任务用（视觉编码器 DINOv2-S，文本编码器 T5-S）。</p>\n<p> 与扩散策略（DP）进行同等规模（≤150M）下的公平对比。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>环境与场景：仿真 robomimic（标准、可复现实验）；真实双机械臂桌面环境（4相机、可变光照与传感器噪声）。</li><li>指标：任务成功率（SR，百分比），训练收敛轮次/步数（训练效率），推理步数与计算量（推理效率），稳定性与分布外鲁棒性（含失败恢复/重试的定性表现）。</li><li>结果摘要</li></ul>\n<p> - 仿真：在Lift、Can、Square、Tool Hang四项任务上均优于扩散策略（DP-10步为0，DP-100步在Square/Tool Hang未超越EBT），EBT两项（S、 R）均取得更高SR。</p>\n<p> - 真实：Fold Towel、Collect Pan、Pick & Place 三任务中，EBT-Policy-R在两项显著优于DP，Pick & Place略优或相当。</p>\n<p> - 训练与推理效率：EBT在约30轮即达100%成功率，DP需约90轮且推理步数更高；EBT在部分任务2步即收敛，DP在10步时成功率降至0，100步才稳定。</p>"
  },
  {
    "date": "2025-10-30",
    "title": "Running VLAs at Real-time Speed",
    "link": "http://arxiv.org/abs/2510.26742",
    "summary_markdown": "# 论文研究单位\nDexmal, StepFun\n# 论文概述\n本文展示了如何在单个消费级GPU上以30Hz帧率和最高480Hz轨迹频率运行π_0级多视角VLA模型，实现了之前被认为大型VLA模型无法完成的动态实时任务。通过消除模型推理中的开销，证明了VLAs确实能够在RTX 4090 GPU上实时运行，实验结果显示π_0策略在抓取掉落笔任务中达到100%成功率。\n# 论文核心贡献点\n- 实现了VLA模型的30FPS推理，双视角输入延迟仅27.3ms，显著快于官方openpi推理\n- 提出完整流式推理框架，实现多层次控制频率（480Hz力控制、30Hz视觉控制、低于1Hz文本交互）\n- 建立了基于屋顶线模型的理论下界，验证了实现接近最优\n- 在真实世界中验证了实时抓取掉落笔任务，达到人类反应水平\n# 论文方法描述\n**消除开销策略**：\n- 使用CUDA图机制消除Python CPU开销，将推理速度提升约两倍\n- 计算图简化：融合RMS归一化参数到线性层、折叠动作时间编码器、融合QKV投影\n\n**内核深度优化**：\n- 调优GEMM瓦片参数，使用Triton实现优化配置\n- 融合门控线性层，提高并行性和内存访问效率\n- 采用部分split-k策略处理特殊尺寸的GEMM操作\n- 将标量操作（偏置、残差、激活函数）融合到GEMM中\n\n**完整流式推理框架**：\n- 重叠执行VLM和AE内核，实现资源充分利用\n- 将高频率输入信号注入AE，支持480Hz控制频率\n- 实现渐进式动作生成，替代传统的完整轨迹生成\n# 论文使用数据集和训练资源\n- **硬件环境**：单张RTX 4090 GPU\n- **训练数据**：600集真实世界抓取掉落笔任务的演示数据\n- **软件框架**：基于官方openpi仓库进行模型训练和推理\n# 论文使用的评估环境和评估指标\n- **推理速度评估**：在不同视角数量下测量推理时间（1视角：20.0ms，2视角：27.3ms，3视角：36.8ms）\n- **实际任务验证**：抓取掉落笔任务的100%成功率测试\n- **理论下界分析**：使用屋顶线模型计算GEMM操作的理论上界，考虑内存带宽和计算能力\n- **同步开销测量**：通过比较不同同步方法的开销，验证实现的接近最优性",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Dexmal, StepFun</p>\n<h1>论文概述</h1>\n<p>本文展示了如何在单个消费级GPU上以30Hz帧率和最高480Hz轨迹频率运行π_0级多视角VLA模型，实现了之前被认为大型VLA模型无法完成的动态实时任务。通过消除模型推理中的开销，证明了VLAs确实能够在RTX 4090 GPU上实时运行，实验结果显示π_0策略在抓取掉落笔任务中达到100%成功率。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>实现了VLA模型的30FPS推理，双视角输入延迟仅27.3ms，显著快于官方openpi推理</li><li>提出完整流式推理框架，实现多层次控制频率（480Hz力控制、30Hz视觉控制、低于1Hz文本交互）</li><li>建立了基于屋顶线模型的理论下界，验证了实现接近最优</li><li>在真实世界中验证了实时抓取掉落笔任务，达到人类反应水平</li></ul>\n<h1>论文方法描述</h1>\n<p><strong>消除开销策略</strong>：</p>\n<ul><li>使用CUDA图机制消除Python CPU开销，将推理速度提升约两倍</li><li>计算图简化：融合RMS归一化参数到线性层、折叠动作时间编码器、融合QKV投影</li></ul>\n\n<p><strong>内核深度优化</strong>：</p>\n<ul><li>调优GEMM瓦片参数，使用Triton实现优化配置</li><li>融合门控线性层，提高并行性和内存访问效率</li><li>采用部分split-k策略处理特殊尺寸的GEMM操作</li><li>将标量操作（偏置、残差、激活函数）融合到GEMM中</li></ul>\n\n<p><strong>完整流式推理框架</strong>：</p>\n<ul><li>重叠执行VLM和AE内核，实现资源充分利用</li><li>将高频率输入信号注入AE，支持480Hz控制频率</li><li>实现渐进式动作生成，替代传统的完整轨迹生成</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>硬件环境</strong>：单张RTX 4090 GPU</li><li><strong>训练数据</strong>：600集真实世界抓取掉落笔任务的演示数据</li><li><strong>软件框架</strong>：基于官方openpi仓库进行模型训练和推理</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>推理速度评估</strong>：在不同视角数量下测量推理时间（1视角：20.0ms，2视角：27.3ms，3视角：36.8ms）</li><li><strong>实际任务验证</strong>：抓取掉落笔任务的100%成功率测试</li><li><strong>理论下界分析</strong>：使用屋顶线模型计算GEMM操作的理论上界，考虑内存带宽和计算能力</li><li><strong>同步开销测量</strong>：通过比较不同同步方法的开销，验证实现的接近最优性</li></ul>"
  },
  {
    "date": "2025-10-30",
    "title": "RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration",
    "link": "http://arxiv.org/abs/2510.26536",
    "summary_markdown": "### 论文研究单位\n- 北京大学计算机科学学院多媒体信息处理国家重点实验室\n- 北京智源人工智能研究院\n- 中国科学院自动化研究所\n- 北京航空航天大学\n### 论文概述\nRoboOS-NeXT是一个统一的基于内存的多机器人协作框架，旨在实现终身适应性、可扩展协作和鲁棒调度。该框架的核心是Spatio-Temporal–Embodiment Memory (STEM)，它将空间场景几何、时间事件历史和机器人配置文件集成到一个共享表示中。通过采用大脑-小脑框架，高级大脑模型通过检索和更新STEM进行全局规划，而低级控制器在本地执行动作。这种认知、执行和内存的闭环实现了动态任务分配、容错协作和一致状态同步。\n### 论文核心贡献点\n- 提出了RoboOS-NeXT，一个基于内存的多机器人协作框架，构建在STEM之上，将空间、时间和实例化维度集成到统一表示中\n- 设计了Brain-Cerebellum-Memory层次循环，通过STEM连接全局推理与技能执行，为多机器人协作提供了原则性基础\n- 在餐厅、超市和家庭等多样化任务中评估RoboOS-NeXT，并在真实世界演示中展示其在异构机器人上的有效性\n### 论文方法描述\nRoboOS-NeXT的核心方法包括：\n\n1. **Spatio-Temporal–Embodiment Memory (STEM)**：\n - 时间内存：维护附加仅、时间有序的列表，记录状态增量、暂存任务上下文和工具调用跟踪\n - 空间内存：建模场景为根类型多分支树（全局场景->区域->载体），每个载体锚定对象级图\n - 实例化内存：为每个机器人维护档案，包括定位、能力、资源、传感器状态和可用性\n - 组织为队列-树-图-代理结构，支持跨维度交互\n\n2. **Brain-Cerebellum-Memory框架**：\n - 全局任务分解：大脑模型使用检索增强生成过程查询共享空间内存，生成结构化推理跟踪和工作流图\n - 拓扑子任务分配：监控器基于有向无环图中的拓扑依赖关系动态调度和分配子任务\n - 分布式子任务代理：为每个子任务部署专用机器人代理，自主编排工具选择\n - 动态内存更新：随着机器人在子任务执行过程中感知和行动，增量更新时间内存和空间内存\n### 论文使用数据集和训练资源\n- **数据集**：在三个领域（餐厅、超市、家庭）各实例化200个任务的模拟设置\n- **训练资源**：\n - 高级推理由RoboBrain-2.0多模态大语言模型驱动，增强用于时空推理\n - 低级执行包含基于SLAM的导航模块和基于扩散策略的操纵模块\n- **真实机器人平台**：Unitree G1人形机器人、Agilex双臂机器人和Realman单臂机器人\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 三个领域（餐厅、超市、家庭）的模拟设置\n - 真实世界协作演示场景\n- **评估指标**：\n - 成功率(SR, %)：在步骤预算内成功完成的任务比例\n - 边际成功率(MSR, %)：在终身或课程序列中每个序列的最终任务上测量的成功率\n - 平均每任务执行步骤(AEST, #)：完成任务所需的平均步骤数\n - 每步成功率(SS, %/#)：任务成功率与平均步骤数之间的比率，反映每步实现平均准确性",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>北京大学计算机科学学院多媒体信息处理国家重点实验室</li><li>北京智源人工智能研究院</li><li>中国科学院自动化研究所</li><li>北京航空航天大学</li></ul>\n<h3>论文概述</h3>\n<p>RoboOS-NeXT是一个统一的基于内存的多机器人协作框架，旨在实现终身适应性、可扩展协作和鲁棒调度。该框架的核心是Spatio-Temporal–Embodiment Memory (STEM)，它将空间场景几何、时间事件历史和机器人配置文件集成到一个共享表示中。通过采用大脑-小脑框架，高级大脑模型通过检索和更新STEM进行全局规划，而低级控制器在本地执行动作。这种认知、执行和内存的闭环实现了动态任务分配、容错协作和一致状态同步。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了RoboOS-NeXT，一个基于内存的多机器人协作框架，构建在STEM之上，将空间、时间和实例化维度集成到统一表示中</li><li>设计了Brain-Cerebellum-Memory层次循环，通过STEM连接全局推理与技能执行，为多机器人协作提供了原则性基础</li><li>在餐厅、超市和家庭等多样化任务中评估RoboOS-NeXT，并在真实世界演示中展示其在异构机器人上的有效性</li></ul>\n<h3>论文方法描述</h3>\n<p>RoboOS-NeXT的核心方法包括：</p>\n\n<p>1. <strong>Spatio-Temporal–Embodiment Memory (STEM)</strong>：</p>\n<p> - 时间内存：维护附加仅、时间有序的列表，记录状态增量、暂存任务上下文和工具调用跟踪</p>\n<p> - 空间内存：建模场景为根类型多分支树（全局场景->区域->载体），每个载体锚定对象级图</p>\n<p> - 实例化内存：为每个机器人维护档案，包括定位、能力、资源、传感器状态和可用性</p>\n<p> - 组织为队列-树-图-代理结构，支持跨维度交互</p>\n\n<p>2. <strong>Brain-Cerebellum-Memory框架</strong>：</p>\n<p> - 全局任务分解：大脑模型使用检索增强生成过程查询共享空间内存，生成结构化推理跟踪和工作流图</p>\n<p> - 拓扑子任务分配：监控器基于有向无环图中的拓扑依赖关系动态调度和分配子任务</p>\n<p> - 分布式子任务代理：为每个子任务部署专用机器人代理，自主编排工具选择</p>\n<p> - 动态内存更新：随着机器人在子任务执行过程中感知和行动，增量更新时间内存和空间内存</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：在三个领域（餐厅、超市、家庭）各实例化200个任务的模拟设置</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 高级推理由RoboBrain-2.0多模态大语言模型驱动，增强用于时空推理</p>\n<p> - 低级执行包含基于SLAM的导航模块和基于扩散策略的操纵模块</p>\n<ul><li><strong>真实机器人平台</strong>：Unitree G1人形机器人、Agilex双臂机器人和Realman单臂机器人</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 三个领域（餐厅、超市、家庭）的模拟设置</p>\n<p> - 真实世界协作演示场景</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率(SR, %)：在步骤预算内成功完成的任务比例</p>\n<p> - 边际成功率(MSR, %)：在终身或课程序列中每个序列的最终任务上测量的成功率</p>\n<p> - 平均每任务执行步骤(AEST, #)：完成任务所需的平均步骤数</p>\n<p> - 每步成功率(SS, %/#)：任务成功率与平均步骤数之间的比率，反映每步实现平均准确性</p>"
  },
  {
    "date": "2025-10-30",
    "title": "Human-in-the-loop Online Rejection Sampling for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2510.26406",
    "summary_markdown": "论文研究单位\n清华大学深圳国际研究生院, 腾讯机器人X实验室\n\n论文概述\n论文提出了一种名为“人在环路中的在线拒绝采样”的方法，用于视觉-语言-动作模型的机器人操作后训练。该方法旨在解决强化学习（RL）在微调VLA模型时的不稳定性问题，以及模仿学习（IL）因离线特性而表现不佳的问题。Hi-ORS通过拒绝采样过滤掉负奖励的样本来稳定值估计，并采用奖励加权的监督学习目标为中间步骤提供密集监督。实验表明，该方法仅需1.5小时的真实世界训练，就能在三个真实任务和两种机器人形态上超越RL和IL基线，并具备强大的测试时错误恢复能力。\n\n论文核心贡献点\n1. 识别了VLA后训练中RL不稳定的根源，即不准确的值估计和低效的监督。为此，引入了Hi-ORS，一种利用基于结果的值估计和拒绝采样的简单有效的后训练方法。\n2. 证明了Hi-ORS能够自然地融合在线人工干预，为策略学习错误恢复行为提供明确指导，从而实现了令人印象深刻的测试时扩展能力。\n3. 在三个具有挑战性的真实世界任务和两种机器人形态上验证了Hi-ORS，其在有效性和效率上均大幅优于IL和RL基线，且样本效率高，无需复杂的超参数调优。\n\n论文方法描述\nHi-ORS包含一个评估阶段和一个改进阶段，并辅以一个异步基础设施。\n1. 评估与改进阶段：评估阶段从当前策略生成轨迹，并根据累积奖励使用一个阈值m进行过滤，只保留奖励高于m的成功轨迹，从而避免了不准确的值函数估计。改进阶段则利用这些被接受的高质量轨迹，通过一个奖励加权的监督学习目标来更新策略。该方法为基于流匹配的VLA提供了所有中间去噪步骤的密集监督。\n2. 人在环路与频率变化：Hi-ORS支持操作员在任务执行中的任何时刻进行干预，如远程操作纠正。这些干预数据只有在满足奖励过滤标准后才会被保留，为学习错误恢复行为提供了明确的指导。系统还采用自适应的数据记录频率，在人工干预期间以较高频率记录，在自主执行期间以较低频率记录，以平衡数据质量和执行流畅性。\n3. 异步基础设施：系统使用一个GPU进行在线推理，其余GPU-1个GPU进行模型学习。这种异步的Actor-Learner架构提高了训练吞吐量，并允许学习过程在机器人暂停时继续进行。\n\n论文使用数据集和训练资源\n1. 数据集：研究未使用标准公开数据集，而是在三个设计的真实世界任务中收集初始人类演示数据。在训练过程中，Hi-ORS通过在线机器人交互和人工干预动态收集新数据，用于策略的迭代改进。\n2. 训练资源：基础模型是一个基于流匹配的VLA模型π₀，使用PaliGemma-3B作为骨干网络，并包含一个3亿参数的动作专家模块。训练采用异步的GPU设置，利用ZeRO-2技术进行大规模分布式训练，以适应高容量VLAs的计算需求。\n\n论文使用的评估环境和评估指标\n1. 评估环境：在两个不同形态的真实机器人上评估了三个任务：\n * Raise-Hand：使用Paxini Tora One机器人，任务是将手臂抬到目标姿态。\n * Pack-Detergent：使用Paxini Tora One机器人，任务是将洗涤剂从传送带上拿起并放入纸箱。\n * Insert-Moisturizer：使用Dobot X-Trainer机器人，任务是将细长的润肤霜瓶插入基座。\n2. 评估指标：\n * 成功率：主要评估指标，通过在随机重置的环境中进行10次试验来计算任务的成功率。\n * 训练时间：记录达到特定性能水平所需的小时数。\n * 测试时扩展：通过评估策略在不同尝试预算下的最终性能来衡量。",
    "summary_html": "<p>论文研究单位</p>\n<p>清华大学深圳国际研究生院, 腾讯机器人X实验室</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为“人在环路中的在线拒绝采样”的方法，用于视觉-语言-动作模型的机器人操作后训练。该方法旨在解决强化学习（RL）在微调VLA模型时的不稳定性问题，以及模仿学习（IL）因离线特性而表现不佳的问题。Hi-ORS通过拒绝采样过滤掉负奖励的样本来稳定值估计，并采用奖励加权的监督学习目标为中间步骤提供密集监督。实验表明，该方法仅需1.5小时的真实世界训练，就能在三个真实任务和两种机器人形态上超越RL和IL基线，并具备强大的测试时错误恢复能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>识别了VLA后训练中RL不稳定的根源，即不准确的值估计和低效的监督。为此，引入了Hi-ORS，一种利用基于结果的值估计和拒绝采样的简单有效的后训练方法。</li><li>证明了Hi-ORS能够自然地融合在线人工干预，为策略学习错误恢复行为提供明确指导，从而实现了令人印象深刻的测试时扩展能力。</li><li>在三个具有挑战性的真实世界任务和两种机器人形态上验证了Hi-ORS，其在有效性和效率上均大幅优于IL和RL基线，且样本效率高，无需复杂的超参数调优。</li></ol>\n\n<p>论文方法描述</p>\n<p>Hi-ORS包含一个评估阶段和一个改进阶段，并辅以一个异步基础设施。</p>\n<ol><li>评估与改进阶段：评估阶段从当前策略生成轨迹，并根据累积奖励使用一个阈值m进行过滤，只保留奖励高于m的成功轨迹，从而避免了不准确的值函数估计。改进阶段则利用这些被接受的高质量轨迹，通过一个奖励加权的监督学习目标来更新策略。该方法为基于流匹配的VLA提供了所有中间去噪步骤的密集监督。</li><li>人在环路与频率变化：Hi-ORS支持操作员在任务执行中的任何时刻进行干预，如远程操作纠正。这些干预数据只有在满足奖励过滤标准后才会被保留，为学习错误恢复行为提供了明确的指导。系统还采用自适应的数据记录频率，在人工干预期间以较高频率记录，在自主执行期间以较低频率记录，以平衡数据质量和执行流畅性。</li><li>异步基础设施：系统使用一个GPU进行在线推理，其余GPU-1个GPU进行模型学习。这种异步的Actor-Learner架构提高了训练吞吐量，并允许学习过程在机器人暂停时继续进行。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：研究未使用标准公开数据集，而是在三个设计的真实世界任务中收集初始人类演示数据。在训练过程中，Hi-ORS通过在线机器人交互和人工干预动态收集新数据，用于策略的迭代改进。</li><li>训练资源：基础模型是一个基于流匹配的VLA模型π₀，使用PaliGemma-3B作为骨干网络，并包含一个3亿参数的动作专家模块。训练采用异步的GPU设置，利用ZeRO-2技术进行大规模分布式训练，以适应高容量VLAs的计算需求。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：在两个不同形态的真实机器人上评估了三个任务：</p>\n<p> * Raise-Hand：使用Paxini Tora One机器人，任务是将手臂抬到目标姿态。</p>\n<p> * Pack-Detergent：使用Paxini Tora One机器人，任务是将洗涤剂从传送带上拿起并放入纸箱。</p>\n<p> * Insert-Moisturizer：使用Dobot X-Trainer机器人，任务是将细长的润肤霜瓶插入基座。</p>\n<p>2. 评估指标：</p>\n<p> * 成功率：主要评估指标，通过在随机重置的环境中进行10次试验来计算任务的成功率。</p>\n<p> * 训练时间：记录达到特定性能水平所需的小时数。</p>\n<p> * 测试时扩展：通过评估策略在不同尝试预算下的最终性能来衡量。</p>"
  },
  {
    "date": "2025-10-29",
    "title": "$π_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.25889",
    "summary_markdown": "好的，我将根据提供的Arxiv论文HTML原文，按照要求的六个部分，用Markdown格式总结论文要点，不使用加粗。\n\n```markdown\n论文研究单位\n清华大学、北京大学、中国科学院自动化研究所、卡内基梅隆大学、Infinigence AI、中关村实验室。\n\n论文概述\n论文提出了π_RL，一个用于在线强化学习微调基于流的视觉-语言-动作（VLA）模型的框架。当前VLA模型依赖监督微调（SFT），但收集大规模专家轨迹数据成本高昂且易过拟合。尽管RL已被用于自回归VLA，但由于流匹配中难以计算精确的动作对数似然，将其应用于基于流的VLA（如π_0和π_0.5）仍具挑战性。π_RL通过引入Flow-Noise和Flow-SDE两种算法解决了该问题，实现了流模型中的精确似然估计和随机性注入，从而能够在并行仿真环境中高效地进行大规模多任务RL训练。\n\n论文核心贡献点\n- 首个针对基于流的π系列VLA模型的在线RL微调框架，包含Flow-Noise和Flow-SDE两种技术方案，解决了流匹配中的精确对数似然估计难题。\n- 在多任务基准LIBERO和ManiSkill上展示了显著的性能提升和更强的泛化能力。\n- 进行了全面的消融实验，涵盖RL算法、评论家设计、噪声注入策略、MDP构建和超参数，为未来研究提供了实证见解。\n- 开源了所有代码和模型检查点，以促进可复现性和领域发展。\n\n论文方法描述\n论文提出了两种核心方法：\n- Flow-Noise：通过在去噪过程中注入可学习的噪声，将整个去噪序列建模为一个离散时间马尔可夫决策过程（MDP）。这使得能够计算整个序列的精确联合对数似然，从而可以直接应用于标准的策略梯度优化（如PPO）。\n- Flow-SDE：将确定性的常微分方程（ODE）去噪过程转换为等价的随机微分方程（SDE）以增强探索，并构建了一个两层MDP以耦合去噪过程与策略-环境交互。同时采用混合ODE-SDE采样技术，在训练期间随机选择一步进行SDE转换，其余步骤使用确定性ODE，以加速训练。\n\n论文使用数据集和训练资源\n- 数据集：LIBERO（包含Spatial、Object、Goal、Long四个任务套件）和ManiSkill（使用其SIMPLER基准和扩展的4352个拾取-放置任务组合的MultiTask基准）。\n- 基础模型：基于流的VLA模型π_0（3B PaliGemma + 300M 流匹配专家）和π_0.5（异构数据联合训练）。\n- 训练资源：8块NVIDIA H100 80GB GPU。\n- 训练设置：先进行少样本SFT（例如LIBERO中每个子任务至少40条轨迹），然后使用RL微调，仅微调300M的动作专家模块，冻结VLM参数。采用共享、同置GPU分配策略。\n\n论文使用的评估环境和评估指标\n- 评估环境：LIBERO的CPU仿真平台；ManiSkill的高保真、GPU并行化仿真平台。\n- 评估指标：成功率（%）。在LIBERO-Long任务中，特别关注了单轨迹SFT的性能提升。在ManiSkill中，评估大规模多任务学习能力和在4352个任务组合上的平均成功率。\n```</think>\n论文研究单位\n清华大学、北京大学、中国科学院自动化研究所、卡内基梅隆大学、Infinigence AI、中关村实验室。\n\n论文概述\n论文提出了π_RL，一个用于在线强化学习微调基于流的视觉-语言-动作（VLA）模型的框架。当前VLA模型依赖监督微调（SFT），但收集大规模专家轨迹数据成本高昂且易过拟合。尽管RL已被用于自回归VLA，但由于流匹配中难以计算精确的动作对数似然，将其应用于基于流的VLA（如π_0和π_0.5）仍具挑战性。π_RL通过引入Flow-Noise和Flow-SDE两种算法解决了该问题，实现了流模型中的精确似然估计和随机性注入，从而能够在并行仿真环境中高效地进行大规模多任务RL训练。\n\n论文核心贡献点\n- 首个针对基于流的π系列VLA模型的在线RL微调框架，包含Flow-Noise和Flow-SDE两种技术方案，解决了流匹配中的精确对数似然估计难题。\n- 在多任务基准LIBERO和ManiSkill上展示了显著的性能提升和更强的泛化能力。\n- 进行了全面的消融实验，涵盖RL算法、评论家设计、噪声注入策略、MDP构建和超参数，为未来研究提供了实证见解。\n- 开源了所有代码和模型检查点，以促进可复现性和领域发展。\n\n论文方法描述\n论文提出了两种核心方法：\n- Flow-Noise：通过在去噪过程中注入可学习的噪声，将整个去噪序列建模为一个离散时间马尔可夫决策过程（MDP）。这使得能够计算整个序列的精确联合对数似然，从而可以直接应用于标准的策略梯度优化（如PPO）。\n- Flow-SDE：将确定性的常微分方程（ODE）去噪过程转换为等价的随机微分方程（SDE）以增强探索，并构建了一个两层MDP以耦合去噪过程与策略-环境交互。同时采用混合ODE-SDE采样技术，在训练期间随机选择一步进行SDE转换，其余步骤使用确定性ODE，以加速训练。\n\n论文使用数据集和训练资源\n- 数据集：LIBERO（包含Spatial、Object、Goal、Long四个任务套件）和ManiSkill（使用其SIMPLER基准和扩展的4352个拾取-放置任务组合的MultiTask基准）。\n- 基础模型：基于流的VLA模型π_0（3B PaliGemma + 300M 流匹配专家）和π_0.5（异构数据联合训练）。\n- 训练资源：8块NVIDIA H100 80GB GPU。\n- 训练设置：先进行少样本SFT（例如LIBERO中每个子任务至少40条轨迹），然后使用RL微调，仅微调300M的动作专家模块，冻结VLM参数。采用共享、同置GPU分配策略。\n\n论文使用的评估环境和评估指标\n- 评估环境：LIBERO的CPU仿真平台；ManiSkill的高保真、GPU并行化仿真平台。\n- 评估指标：成功率（%）。在LIBERO-Long任务中，特别关注了单轨迹SFT的性能提升。在ManiSkill中，评估大规模多任务学习能力和在4352个任务组合上的平均成功率。",
    "summary_html": "<p>好的，我将根据提供的Arxiv论文HTML原文，按照要求的六个部分，用Markdown格式总结论文要点，不使用加粗。</p>\n\n<p>```markdown</p>\n<p>论文研究单位</p>\n<p>清华大学、北京大学、中国科学院自动化研究所、卡内基梅隆大学、Infinigence AI、中关村实验室。</p>\n\n<p>论文概述</p>\n<p>论文提出了π_RL，一个用于在线强化学习微调基于流的视觉-语言-动作（VLA）模型的框架。当前VLA模型依赖监督微调（SFT），但收集大规模专家轨迹数据成本高昂且易过拟合。尽管RL已被用于自回归VLA，但由于流匹配中难以计算精确的动作对数似然，将其应用于基于流的VLA（如π_0和π_0.5）仍具挑战性。π_RL通过引入Flow-Noise和Flow-SDE两种算法解决了该问题，实现了流模型中的精确似然估计和随机性注入，从而能够在并行仿真环境中高效地进行大规模多任务RL训练。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>首个针对基于流的π系列VLA模型的在线RL微调框架，包含Flow-Noise和Flow-SDE两种技术方案，解决了流匹配中的精确对数似然估计难题。</li><li>在多任务基准LIBERO和ManiSkill上展示了显著的性能提升和更强的泛化能力。</li><li>进行了全面的消融实验，涵盖RL算法、评论家设计、噪声注入策略、MDP构建和超参数，为未来研究提供了实证见解。</li><li>开源了所有代码和模型检查点，以促进可复现性和领域发展。</li></ul>\n\n<p>论文方法描述</p>\n<p>论文提出了两种核心方法：</p>\n<ul><li>Flow-Noise：通过在去噪过程中注入可学习的噪声，将整个去噪序列建模为一个离散时间马尔可夫决策过程（MDP）。这使得能够计算整个序列的精确联合对数似然，从而可以直接应用于标准的策略梯度优化（如PPO）。</li><li>Flow-SDE：将确定性的常微分方程（ODE）去噪过程转换为等价的随机微分方程（SDE）以增强探索，并构建了一个两层MDP以耦合去噪过程与策略-环境交互。同时采用混合ODE-SDE采样技术，在训练期间随机选择一步进行SDE转换，其余步骤使用确定性ODE，以加速训练。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：LIBERO（包含Spatial、Object、Goal、Long四个任务套件）和ManiSkill（使用其SIMPLER基准和扩展的4352个拾取-放置任务组合的MultiTask基准）。</li><li>基础模型：基于流的VLA模型π_0（3B PaliGemma + 300M 流匹配专家）和π_0.5（异构数据联合训练）。</li><li>训练资源：8块NVIDIA H100 80GB GPU。</li><li>训练设置：先进行少样本SFT（例如LIBERO中每个子任务至少40条轨迹），然后使用RL微调，仅微调300M的动作专家模块，冻结VLM参数。采用共享、同置GPU分配策略。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：LIBERO的CPU仿真平台；ManiSkill的高保真、GPU并行化仿真平台。</li><li>评估指标：成功率（%）。在LIBERO-Long任务中，特别关注了单轨迹SFT的性能提升。在ManiSkill中，评估大规模多任务学习能力和在4352个任务组合上的平均成功率。</li></ul>\n<p>```</think></p>\n<p>论文研究单位</p>\n<p>清华大学、北京大学、中国科学院自动化研究所、卡内基梅隆大学、Infinigence AI、中关村实验室。</p>\n\n<p>论文概述</p>\n<p>论文提出了π_RL，一个用于在线强化学习微调基于流的视觉-语言-动作（VLA）模型的框架。当前VLA模型依赖监督微调（SFT），但收集大规模专家轨迹数据成本高昂且易过拟合。尽管RL已被用于自回归VLA，但由于流匹配中难以计算精确的动作对数似然，将其应用于基于流的VLA（如π_0和π_0.5）仍具挑战性。π_RL通过引入Flow-Noise和Flow-SDE两种算法解决了该问题，实现了流模型中的精确似然估计和随机性注入，从而能够在并行仿真环境中高效地进行大规模多任务RL训练。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>首个针对基于流的π系列VLA模型的在线RL微调框架，包含Flow-Noise和Flow-SDE两种技术方案，解决了流匹配中的精确对数似然估计难题。</li><li>在多任务基准LIBERO和ManiSkill上展示了显著的性能提升和更强的泛化能力。</li><li>进行了全面的消融实验，涵盖RL算法、评论家设计、噪声注入策略、MDP构建和超参数，为未来研究提供了实证见解。</li><li>开源了所有代码和模型检查点，以促进可复现性和领域发展。</li></ul>\n\n<p>论文方法描述</p>\n<p>论文提出了两种核心方法：</p>\n<ul><li>Flow-Noise：通过在去噪过程中注入可学习的噪声，将整个去噪序列建模为一个离散时间马尔可夫决策过程（MDP）。这使得能够计算整个序列的精确联合对数似然，从而可以直接应用于标准的策略梯度优化（如PPO）。</li><li>Flow-SDE：将确定性的常微分方程（ODE）去噪过程转换为等价的随机微分方程（SDE）以增强探索，并构建了一个两层MDP以耦合去噪过程与策略-环境交互。同时采用混合ODE-SDE采样技术，在训练期间随机选择一步进行SDE转换，其余步骤使用确定性ODE，以加速训练。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：LIBERO（包含Spatial、Object、Goal、Long四个任务套件）和ManiSkill（使用其SIMPLER基准和扩展的4352个拾取-放置任务组合的MultiTask基准）。</li><li>基础模型：基于流的VLA模型π_0（3B PaliGemma + 300M 流匹配专家）和π_0.5（异构数据联合训练）。</li><li>训练资源：8块NVIDIA H100 80GB GPU。</li><li>训练设置：先进行少样本SFT（例如LIBERO中每个子任务至少40条轨迹），然后使用RL微调，仅微调300M的动作专家模块，冻结VLM参数。采用共享、同置GPU分配策略。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：LIBERO的CPU仿真平台；ManiSkill的高保真、GPU并行化仿真平台。</li><li>评估指标：成功率（%）。在LIBERO-Long任务中，特别关注了单轨迹SFT的性能提升。在ManiSkill中，评估大规模多任务学习能力和在4352个任务组合上的平均成功率。</li></ul>"
  },
  {
    "date": "2025-10-29",
    "title": "Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.25713",
    "summary_markdown": "### 论文研究单位\nSoft Robotics Lab, ETHz（苏黎世联邦理工学院软机器人实验室）\n### 论文概述\n针对机器人与人类在协作任务中的自然交互挑战，论文提出基于预训练视觉语言动作模型（VLA）的改进方案。核心思想是通过运动线索（而非语言提示）实现隐式意图理解，使机器人能实时响应人类动作并完成协作任务。\n### 论文核心贡献点\n- **FiLM条件化**：在视觉编码器中插入FiLM层，增强文本-视觉对齐能力\n- **辅助损失设计**：添加并行预测头（手部姿态和目标物体）以隐式学习人类意图\n- **动作后处理优化**：通过增量坐标、旋转向量及手部关节PCA降维（保留96%方差）重构低维动作流形\n- **方向性损失**（实验效果负向）：提出方向对齐损失，但未提升性能\n- **推理系统集成**：构建实时硬件-模型接口，实现长任务动态提示切换\n### 论文方法描述\n**模型架构**：基于Open-VLA改进（SigLIP+DINOv2视觉编码器+LLaMA2-7B语言模型），集成FiLM条件化和辅助预测头。\n**数据收集流程**：双角色协作采集（操作者穿戴动作捕捉手套控制机器人，协作者在共享空间交互），10Hz采样并同步多模态数据，附文本指令和手部姿态标注。\n**训练策略**：4×H100 GPU分布式训练（批量24，LoRA rank=32，20轮迭代），采用复合损失（动作+辅助损失）。\n**推理系统**：硬件接口（绿色）→模型推理（蓝色）→高层规划（红色）的三层架构，0.3秒端到端延迟。\n### 论文使用数据集和训练资源\n- **协作任务数据**：\n - \"Pick up cube\"任务：120条轨迹（红蓝立方体各60条）\n - \"Pass cube\"任务：260条轨迹（红立方体200条，蓝立方体60条）\n - 辅助标签：手部3D姿态（MediaPipe）、目标物体索引\n- **训练资源**：4×H100 GPU集群，平均训练时长12小时\n### 论文使用的评估环境和评估指标\n- **动作空间分析**：\n - 端 effector增量坐标分布更接近高斯（vs原始坐标非凸性）\n - 手部关节PCA保留4个主成分解释96%方差\n- **消融实验**：\n - 动作后处理贡献最大（各指标显著提升）\n - 辅助损失提供稳定增益\n - 方向性损失和FiLM在不同损失下呈负向效果\n- **真实世界评估**：\n - 场景：桌面临场交互（多摄像头）\n - 长任务成功率：10次试验中1次成功\n - 失败原因：协作对象与训练者不同时出现\"训练员过拟合\"（模型对特定操作者行为过拟合）\n- **评估指标**：L2损失、PCA重构误差、方向对齐误差、真实任务完成率",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Soft Robotics Lab, ETHz（苏黎世联邦理工学院软机器人实验室）</p>\n<h3>论文概述</h3>\n<p>针对机器人与人类在协作任务中的自然交互挑战，论文提出基于预训练视觉语言动作模型（VLA）的改进方案。核心思想是通过运动线索（而非语言提示）实现隐式意图理解，使机器人能实时响应人类动作并完成协作任务。</p>\n<h3>论文核心贡献点</h3>\n<ul><li><strong>FiLM条件化</strong>：在视觉编码器中插入FiLM层，增强文本-视觉对齐能力</li><li><strong>辅助损失设计</strong>：添加并行预测头（手部姿态和目标物体）以隐式学习人类意图</li><li><strong>动作后处理优化</strong>：通过增量坐标、旋转向量及手部关节PCA降维（保留96%方差）重构低维动作流形</li><li><strong>方向性损失</strong>（实验效果负向）：提出方向对齐损失，但未提升性能</li><li><strong>推理系统集成</strong>：构建实时硬件-模型接口，实现长任务动态提示切换</li></ul>\n<h3>论文方法描述</h3>\n<p><strong>模型架构</strong>：基于Open-VLA改进（SigLIP+DINOv2视觉编码器+LLaMA2-7B语言模型），集成FiLM条件化和辅助预测头。</p>\n<p><strong>数据收集流程</strong>：双角色协作采集（操作者穿戴动作捕捉手套控制机器人，协作者在共享空间交互），10Hz采样并同步多模态数据，附文本指令和手部姿态标注。</p>\n<p><strong>训练策略</strong>：4×H100 GPU分布式训练（批量24，LoRA rank=32，20轮迭代），采用复合损失（动作+辅助损失）。</p>\n<p><strong>推理系统</strong>：硬件接口（绿色）→模型推理（蓝色）→高层规划（红色）的三层架构，0.3秒端到端延迟。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>协作任务数据</strong>：</li></ul>\n<p> - \"Pick up cube\"任务：120条轨迹（红蓝立方体各60条）</p>\n<p> - \"Pass cube\"任务：260条轨迹（红立方体200条，蓝立方体60条）</p>\n<p> - 辅助标签：手部3D姿态（MediaPipe）、目标物体索引</p>\n<ul><li><strong>训练资源</strong>：4×H100 GPU集群，平均训练时长12小时</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>动作空间分析</strong>：</li></ul>\n<p> - 端 effector增量坐标分布更接近高斯（vs原始坐标非凸性）</p>\n<p> - 手部关节PCA保留4个主成分解释96%方差</p>\n<ul><li><strong>消融实验</strong>：</li></ul>\n<p> - 动作后处理贡献最大（各指标显著提升）</p>\n<p> - 辅助损失提供稳定增益</p>\n<p> - 方向性损失和FiLM在不同损失下呈负向效果</p>\n<ul><li><strong>真实世界评估</strong>：</li></ul>\n<p> - 场景：桌面临场交互（多摄像头）</p>\n<p> - 长任务成功率：10次试验中1次成功</p>\n<p> - 失败原因：协作对象与训练者不同时出现\"训练员过拟合\"（模型对特定操作者行为过拟合）</p>\n<ul><li><strong>评估指标</strong>：L2损失、PCA重构误差、方向对齐误差、真实任务完成率</li></ul>"
  },
  {
    "date": "2025-10-29",
    "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization",
    "link": "http://arxiv.org/abs/2510.25616",
    "summary_markdown": "### 论文研究单位\nCognitive AI Lab, IAI MIPT, Moscow, Russia\n### 论文概述\n该论文研究了视觉-语言-动作模型（VLA）在动作微调过程中视觉表示退化的问题。通过系统性分析，作者发现标准的监督微调（SFT）会导致视觉表示崩溃、注意力分散和领域特定遗忘。为解决这一问题，论文提出了一种轻量级的视觉表示对齐方法，将VLA的视觉特征锚定到预训练的教师模型上，从而保留多模态理解能力并提升分布外（OOD）泛化性能。\n### 论文核心贡献点\n1. **表示退化分析**：通过注意力图可视化和t-SNE分析，系统证明VLA微调会导致视觉表示崩溃和注意力分散，相较于原始VLM丧失语义一致性。\n2. **VL-Think任务套件**：设计了一个诊断基准，包含8个视觉-语言任务（如形状、颜色、交通符号识别），用于量化VLA从VLM继承的VL知识迁移效果，揭示动作微调引起的领域遗忘。\n3. **视觉对齐方法**：提出一种基于柏拉图表示假说的轻量级方法，通过正则化损失将VLA的中间特征对齐到教师模型，无需额外计算开销即可恢复语义表示，提升OOD泛化。\n### 论文方法描述\n- **视觉对齐机制**：在VLA的Transformer主干中选择中间层 \\(i^{\\star}\\)，提取视觉token \\(h^{i^{\\star}}_{1:k}\\)，通过可训练投影器 \\(P_{\\varphi}\\) 映射到教师特征空间（如C-RADIOv3），最小化负相似度损失 \\(\\mathcal{L}_{\\text{align}}\\)。\n- **总损失函数**：结合标准动作损失 \\(\\mathcal{L}_{\\text{VLA}}\\) 与对齐损失 \\(\\mathcal{L}_{\\text{align}}\\)：\\(\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{VLA}} + \\lambda \\mathcal{L}_{\\text{align}}\\)，其中 \\(\\lambda > 0\\) 平衡策略学习与语义保留。\n- **实现细节**：使用LoRA适配器微调OpenVLA，投影器为冻结MLP以防止特征退化，教师模型冻结作为语义锚点。\n### 论文使用数据集和训练资源\n- **数据集**：基于Simpler基准的1400条专家演示轨迹，涵盖16种物体和16种桌面，用于任务特定微调；VL-Think任务套件含8个视觉-语言任务；ImageNet-100用于线性探测。\n- **训练资源**：使用MPLib运动规划器生成轨迹，训练采用LoRA适配器，硬件未明确说明（开源代码未提供细节）。\n- **教师模型**：C-RADIOv3作为主要视觉教师，对比DINOv2、SigLIP、Theia等。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Simpler基准的OOD变体，包括：\n - **视觉扰动**：动态纹理（Tex03/Tex05）、图像噪声。\n - **语义变化**：新物体、新容器、指令改写。\n - **执行变化**：随机初始位姿、中途物体重定位。\n- **评估指标**：\n - **任务成功率**：在128个随机种子上取平均值及标准差（SD），衡量放置动作正确性。\n - **VL-Think准确率**：视觉-语言理解的零样本准确率。\n - **线性探测准确率**：ImageNet-100上训练线性分类器的分类精度。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Cognitive AI Lab, IAI MIPT, Moscow, Russia</p>\n<h3>论文概述</h3>\n<p>该论文研究了视觉-语言-动作模型（VLA）在动作微调过程中视觉表示退化的问题。通过系统性分析，作者发现标准的监督微调（SFT）会导致视觉表示崩溃、注意力分散和领域特定遗忘。为解决这一问题，论文提出了一种轻量级的视觉表示对齐方法，将VLA的视觉特征锚定到预训练的教师模型上，从而保留多模态理解能力并提升分布外（OOD）泛化性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>表示退化分析</strong>：通过注意力图可视化和t-SNE分析，系统证明VLA微调会导致视觉表示崩溃和注意力分散，相较于原始VLM丧失语义一致性。</li><li><strong>VL-Think任务套件</strong>：设计了一个诊断基准，包含8个视觉-语言任务（如形状、颜色、交通符号识别），用于量化VLA从VLM继承的VL知识迁移效果，揭示动作微调引起的领域遗忘。</li><li><strong>视觉对齐方法</strong>：提出一种基于柏拉图表示假说的轻量级方法，通过正则化损失将VLA的中间特征对齐到教师模型，无需额外计算开销即可恢复语义表示，提升OOD泛化。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>视觉对齐机制</strong>：在VLA的Transformer主干中选择中间层 \\(i^{\\star}\\)，提取视觉token \\(h^{i^{\\star}}_{1:k}\\)，通过可训练投影器 \\(P_{\\varphi}\\) 映射到教师特征空间（如C-RADIOv3），最小化负相似度损失 \\(\\mathcal{L}_{\\text{align}}\\)。</li><li><strong>总损失函数</strong>：结合标准动作损失 \\(\\mathcal{L}_{\\text{VLA}}\\) 与对齐损失 \\(\\mathcal{L}_{\\text{align}}\\)：\\(\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{VLA}} + \\lambda \\mathcal{L}_{\\text{align}}\\)，其中 \\(\\lambda > 0\\) 平衡策略学习与语义保留。</li><li><strong>实现细节</strong>：使用LoRA适配器微调OpenVLA，投影器为冻结MLP以防止特征退化，教师模型冻结作为语义锚点。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：基于Simpler基准的1400条专家演示轨迹，涵盖16种物体和16种桌面，用于任务特定微调；VL-Think任务套件含8个视觉-语言任务；ImageNet-100用于线性探测。</li><li><strong>训练资源</strong>：使用MPLib运动规划器生成轨迹，训练采用LoRA适配器，硬件未明确说明（开源代码未提供细节）。</li><li><strong>教师模型</strong>：C-RADIOv3作为主要视觉教师，对比DINOv2、SigLIP、Theia等。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Simpler基准的OOD变体，包括：</li></ul>\n<p> - <strong>视觉扰动</strong>：动态纹理（Tex03/Tex05）、图像噪声。</p>\n<p> - <strong>语义变化</strong>：新物体、新容器、指令改写。</p>\n<p> - <strong>执行变化</strong>：随机初始位姿、中途物体重定位。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率</strong>：在128个随机种子上取平均值及标准差（SD），衡量放置动作正确性。</p>\n<p> - <strong>VL-Think准确率</strong>：视觉-语言理解的零样本准确率。</p>\n<p> - <strong>线性探测准确率</strong>：ImageNet-100上训练线性分类器的分类精度。</p>"
  },
  {
    "date": "2025-10-29",
    "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies",
    "link": "http://arxiv.org/abs/2510.25122",
    "summary_markdown": "```markdown\n### 论文研究单位\n不列颠哥伦比亚大学、阿尔伯塔大学、小米EV、东北大学\n### 论文概述\n论文提出NanoVLA，一种面向边缘设备的轻量级视觉-语言-动作（VLA）模型框架，旨在解决传统VLA模型在资源受限设备（如移动机器人）上的部署难题。通过重新设计模态融合机制、动作执行策略和计算分配方式，实现在参数量减少98%的情况下，推理速度提升52倍，同时保持或超越现有模型的任务准确性和泛化能力。\n### 论文核心贡献点\n1. **视觉-语言解耦（Vision-language Decoupling）**：将视觉和语言输入的处理分离至后期融合阶段，支持指令特征缓存，减少重复计算。\n2. **长-短动作分块（Long-short Action Chunking）**：训练时预测长动作序列以保证平滑性，推理时仅执行短窗口并基于新观测重新规划，平衡连贯性与实时响应。\n3. **动态路由（Dynamic Routing）**：根据任务复杂度自适应选择轻量或重量级语言模型，优化计算资源分配。\n### 论文方法描述\n1. **视觉-语言解耦**：\n - 使用冻结的视觉编码器（如ResNet）和语言编码器（如BERT、Qwen）独立提取特征。\n - 通过轻量级Transformer的交叉注意力层在后期融合多模态嵌入。\n - 缓存静态指令特征，每时间步仅更新视觉特征。\n2. **长-短动作分块**：\n - 训练目标：预测长动作序列（$H_{\\text{train}}$步），使用监督回归损失。\n - 推理策略：执行预测序列的前$h \\ll H_{\\text{train}}$步后重新规划，确保平滑性与适应性。\n3. **动态路由**：\n - 基于贝叶斯成功模型估计不同模型在任务上的胜率。\n - 训练文本条件二分类器预测模型胜率，推理时默认使用轻量模型，仅在复杂任务（胜率超阈值$\\tau$）时切换至重量模型。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **模拟环境**：LIBERO基准（包含LIBERO-Spatial/Object/Goal/Long任务集）及LIBERO-90（90个短时序任务）。\n - **真实环境**：LeRobot平台，包含10个操作任务（如物体抓取、变形体操作、精确开盖等）及2个未见任务。\n- **训练资源**：\n - 使用100步动作块（AC）训练，推理时采用10步AC。\n - 模型变体：NanoVLA-S（BERT-base，161M参数）、NanoVLA-L（Qwen2.5 0.5B，520M参数）、NanoVLA-R（动态路由，平均296M参数）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：标准LIBERO测试环境。\n - **真实环境**：Jetson Orin Nano（8GB内存）边缘设备部署LeRobot So-arm 101。\n- **评估指标**：\n - **成功率（Success Rate, SR）**：50次独立试验的平均成功率（模拟和真实任务）。\n - **推理延迟**：边缘设备上的帧率（FPS）及平均模型参数量。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>不列颠哥伦比亚大学、阿尔伯塔大学、小米EV、东北大学</p>\n<h3>论文概述</h3>\n<p>论文提出NanoVLA，一种面向边缘设备的轻量级视觉-语言-动作（VLA）模型框架，旨在解决传统VLA模型在资源受限设备（如移动机器人）上的部署难题。通过重新设计模态融合机制、动作执行策略和计算分配方式，实现在参数量减少98%的情况下，推理速度提升52倍，同时保持或超越现有模型的任务准确性和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>视觉-语言解耦（Vision-language Decoupling）</strong>：将视觉和语言输入的处理分离至后期融合阶段，支持指令特征缓存，减少重复计算。</li><li><strong>长-短动作分块（Long-short Action Chunking）</strong>：训练时预测长动作序列以保证平滑性，推理时仅执行短窗口并基于新观测重新规划，平衡连贯性与实时响应。</li><li><strong>动态路由（Dynamic Routing）</strong>：根据任务复杂度自适应选择轻量或重量级语言模型，优化计算资源分配。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>视觉-语言解耦</strong>：</p>\n<p> - 使用冻结的视觉编码器（如ResNet）和语言编码器（如BERT、Qwen）独立提取特征。</p>\n<p> - 通过轻量级Transformer的交叉注意力层在后期融合多模态嵌入。</p>\n<p> - 缓存静态指令特征，每时间步仅更新视觉特征。</p>\n<p>2. <strong>长-短动作分块</strong>：</p>\n<p> - 训练目标：预测长动作序列（$H_{\\text{train}}$步），使用监督回归损失。</p>\n<p> - 推理策略：执行预测序列的前$h \\ll H_{\\text{train}}$步后重新规划，确保平滑性与适应性。</p>\n<p>3. <strong>动态路由</strong>：</p>\n<p> - 基于贝叶斯成功模型估计不同模型在任务上的胜率。</p>\n<p> - 训练文本条件二分类器预测模型胜率，推理时默认使用轻量模型，仅在复杂任务（胜率超阈值$\\tau$）时切换至重量模型。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：LIBERO基准（包含LIBERO-Spatial/Object/Goal/Long任务集）及LIBERO-90（90个短时序任务）。</p>\n<p> - <strong>真实环境</strong>：LeRobot平台，包含10个操作任务（如物体抓取、变形体操作、精确开盖等）及2个未见任务。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 使用100步动作块（AC）训练，推理时采用10步AC。</p>\n<p> - 模型变体：NanoVLA-S（BERT-base，161M参数）、NanoVLA-L（Qwen2.5 0.5B，520M参数）、NanoVLA-R（动态路由，平均296M参数）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：标准LIBERO测试环境。</p>\n<p> - <strong>真实环境</strong>：Jetson Orin Nano（8GB内存）边缘设备部署LeRobot So-arm 101。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（Success Rate, SR）</strong>：50次独立试验的平均成功率（模拟和真实任务）。</p>\n<p> - <strong>推理延迟</strong>：边缘设备上的帧率（FPS）及平均模型参数量。</p>\n<p>```</p>"
  },
  {
    "date": "2025-10-27",
    "title": "A Survey on Efficient Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.24795",
    "summary_markdown": "论文研究单位\n同济大学、西南交通大学、电子科技大学、特伦托大学\n\n论文概述\n本文是首个全面回顾高效视觉-语言-动作模型的综述，覆盖整个数据-模型-训练流程。旨在解决基础VLA模型在计算资源、推理延迟和数据需求方面的效率瓶颈，推动其在资源受限环境中的实际部署。论文系统分析了现有技术，并提出了统一分类框架。\n\n论文核心贡献点\n- 开创性综述：首次系统覆盖VLA全流程的高效化技术，填补文献空白。\n- 新颖分类法：提出三支柱分类框架，包括高效模型设计、高效训练和高效数据收集。\n- 未来路线图：总结当前挑战，提出可扩展具身智能的研究方向。\n\n论文方法描述\n高效模型设计：\n - 高效架构：包括线性注意力、Transformer替代方案（如Mamba）、并行/生成式动作解码、轻量化组件、混合专家系统和分层系统。\n - 模型压缩：通过层剪枝（如动态多出口）、量化（如4-8位压缩）和Token优化（合并/剪枝冗余Token）减少参数和计算量。\n高效训练：\n - 高效预训练：利用数据高效技术、优化动作表示和预训练策略减少计算负担。\n - 高效后训练：采用监督微调和强化学习方法提升资源利用率。\n高效数据收集：\n - 人机协作收集、仿真生成、跨域数据利用、自我探索和数据增强技术提升数据质量和规模。\n\n论文使用数据集和训练资源\n数据集：\n - 真实世界数据：Open X-Embodiment（多任务聚合）、BridgeData V2（跨领域）、DROID（野外灵巧操作）。\n - 仿真数据：RLBench（标准化任务）、RoboCasa（日常任务）、RoboGen（生成式仿真）。\n训练资源：\n - 基础VLA模型需大规模资源，如OpenVLA消耗21,500 A100-GPU小时，π₀需10,000小时机器人轨迹。\n\n论文使用的评估环境和评估指标\n评估环境：\n - 真实机器人平台（如机械臂、移动机器人）和仿真环境（如RLBench、CALVIN）。\n评估指标：\n - 任务成功率（平均/子任务）、推理延迟（毫秒级）、控制频率（Hz）、参数量和泛化能力（跨任务/环境）。基准测试包括Meta-World（操作任务）、LIBERO（终身学习）、CALVIN（指令泛化）和VLABench（视觉定位）。",
    "summary_html": "<p>论文研究单位</p>\n<p>同济大学、西南交通大学、电子科技大学、特伦托大学</p>\n\n<p>论文概述</p>\n<p>本文是首个全面回顾高效视觉-语言-动作模型的综述，覆盖整个数据-模型-训练流程。旨在解决基础VLA模型在计算资源、推理延迟和数据需求方面的效率瓶颈，推动其在资源受限环境中的实际部署。论文系统分析了现有技术，并提出了统一分类框架。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>开创性综述：首次系统覆盖VLA全流程的高效化技术，填补文献空白。</li><li>新颖分类法：提出三支柱分类框架，包括高效模型设计、高效训练和高效数据收集。</li><li>未来路线图：总结当前挑战，提出可扩展具身智能的研究方向。</li></ul>\n\n<p>论文方法描述</p>\n<p>高效模型设计：</p>\n<p> - 高效架构：包括线性注意力、Transformer替代方案（如Mamba）、并行/生成式动作解码、轻量化组件、混合专家系统和分层系统。</p>\n<p> - 模型压缩：通过层剪枝（如动态多出口）、量化（如4-8位压缩）和Token优化（合并/剪枝冗余Token）减少参数和计算量。</p>\n<p>高效训练：</p>\n<p> - 高效预训练：利用数据高效技术、优化动作表示和预训练策略减少计算负担。</p>\n<p> - 高效后训练：采用监督微调和强化学习方法提升资源利用率。</p>\n<p>高效数据收集：</p>\n<p> - 人机协作收集、仿真生成、跨域数据利用、自我探索和数据增强技术提升数据质量和规模。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<p> - 真实世界数据：Open X-Embodiment（多任务聚合）、BridgeData V2（跨领域）、DROID（野外灵巧操作）。</p>\n<p> - 仿真数据：RLBench（标准化任务）、RoboCasa（日常任务）、RoboGen（生成式仿真）。</p>\n<p>训练资源：</p>\n<p> - 基础VLA模型需大规模资源，如OpenVLA消耗21,500 A100-GPU小时，π₀需10,000小时机器人轨迹。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<p> - 真实机器人平台（如机械臂、移动机器人）和仿真环境（如RLBench、CALVIN）。</p>\n<p>评估指标：</p>\n<p> - 任务成功率（平均/子任务）、推理延迟（毫秒级）、控制频率（Hz）、参数量和泛化能力（跨任务/环境）。基准测试包括Meta-World（操作任务）、LIBERO（终身学习）、CALVIN（指令泛化）和VLABench（视觉定位）。</p>"
  },
  {
    "date": "2025-10-28",
    "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning",
    "link": "http://arxiv.org/abs/2510.24161",
    "summary_markdown": "# 论文研究单位\n- Tongji University; Shanghai Magic; Koala Uran\n# 论文概述\n- 背景：现有多模态大语言模型（MLLM）、具身大语言模型（ELLM）、视觉-语言-动作模型（VLA）和通用多模态大模型（GMLM）未能同时实现跨空间、跨任务、跨具身的统一能力\n- 目标：提出BLM1（Boundless Large Model），在单一统一模型中实现数字空间推理与物理空间控制的融合\n- 方法：两阶段训练（Stage I SFT注入具身知识；Stage II冻结MLLM，训练基于意图桥接接口的扩散变换器（DiT）策略模块）\n- 结论：单一BLM1在数字任务上约提升6%，物理任务上约提升3%，同时保持指令跟随与跨具身控制能力\n# 论文核心贡献点\n- 统一三能力：跨空间迁移（数字→物理）、跨任务学习、跨具身泛化\n- 保持语言能力：Stage I以SFT注入具身知识，避免破坏MLLM指令跟随\n- 有效桥接：意图桥接接口+Perceiver压缩高阶意图，支撑高频率闭环控制\n- 策略共享：Diffusion Transformer作为通用策略头（0.76B参数），跨具身参数共享\n- 训练配方：加权采样保证数据集平衡；Stage II中后期引入未来预测损失提升泛化\n- 系统评测：数字空间六大基准、物理空间四具身六任务，建立统一对比\n- 结果优势：超越四类模型家族（MLLM、ELLM、VLA、GMLM），单一模型不切换架构\n# 论文方法描述\n- 架构组成\n - Backbone：Qwen2.5-VL-7B-Instruct；融合视觉与语言；抽取第k层隐藏状态Hk作为高层意图\n - 意图桥接接口：Perceiver将Hk压缩为固定K个令牌H̃k，降低计算、适配控制\n - 机器人策略：DiT扩散变换器作为条件生成头；状态编码器fs、动作编码器fa/解码器；预测horizon为h的动作块At\n- 训练目标\n - SFT目标（Stage I）：标准next-token交叉熵，冻结视觉与投影器，更新语言与投影\n - 流动匹配目标（Stage II早期）：Rectified-flow路径，DiT拟合速度场vφ(xτ,τ\\|H̃k)，LFM为MSE\n - 未来预测目标（Stage II后期）：在DiT中引入可学习未来令牌F，使其中间层表征与冻结MLLM对未来观察o_{t+H}与指令y的压缩表征对齐，用余弦相似度\n - 总体：Stage I仅L_SFT；Stage II早期训练Perceiver/DiT/编解码器；后期叠加L_FP\n- 训练配方\n - 加权采样：保证各数据集等概率抽取，避免大数据主导\n - Stage I：数字多模态问答对（RoboVQA、AgiBot、HoloAssist、BridgeData V2、EgoPlan、ShareRobot）\n - Stage II：冻结MLLM，使用四具身×六任务的模拟演示数据训练策略；具身特定编码器/解码器独立优化，DiT参数跨具身共享\n - 物理数据生成：基于ManiSkill，高层技能+关键姿态+IK与碰撞检验+路径规划与平滑，录制第三人称与腕部图像、本体感受与动作\n# 论文使用数据集和训练资源\n- 数字空间数据集（Stage I）\n - RoboVQA：829,502视频文本对，29,520指令；多类问答（规划、完成检验、 affordance判别与生成、过去/未来事件）\n - AgiBot：1,001,552条轨迹，2,976.4小时；217任务、87技能、106场景\n - HoloAssist：166小时；350教师-执行者对，20物理任务、16类物体\n - BridgeData V2：60,096轨迹，24环境，100+物类；基础到复杂13技能族\n - EgoPlan：EgoPlan-Bench 4,939多选题 + EgoPlan-IT 50K问答（基于EPIC-Kitchens）\n - ShareRobot：整合Open X-Embodiment的23数据集，102场景、12具身；1,027,990规划问答、6,522 affordance注释、6,870轨迹预测\n- 物理空间数据集（Stage II）\n - 具身：Franka Emika Panda、xArm-6、xArm-7、WidowX AI\n - 任务：PickCube、PullCube、StackCube、PushCube、PlaceSphere、LiftPegUpright（每具身×任务100集）\n - 总帧数：347.8K；任务特定成功判定\n- 数字评测基准（Stage I后）\n - 多选题：RoboVQA、AgiBot、HoloAssist、RoboFail\n - 自由问答：EgoThink、ShareRobot\n - 总计：3,160测试样本（见表2）\n- 训练资源\n - 框架：ManiSkill模拟收集\n - 加权采样：C个数据集等概率，样本在数据集内均匀\n - 视频帧采样：统一0.5秒间隔采样；短视频4帧，长视频8帧；任务前移除测试片段防泄露\n# 论文使用的评估环境和评估指标\n- 数字空间评估\n - 指标\n - Exact-match accuracy（EM）：多选题严格匹配归一化答案\n - LLM-as-a-judge（GPT评分）：自由问答使用判别式提示与评分标准\n - EgoThink：s_i∈{0,0.5,1}，最终分数按平均×100\n - ShareRobot：s_i∈{1,2,3,4,5}，最终分数按((s_i−1)/4)平均×100\n- 物理空间评估\n - 指标：Episode级成功率sr_{e,u}=N^s_{e,u}/N_{e,u}；总体为E具身×U任务的成功率平均\n - 环境：仿真闭环控制；每任务50次rollout，具身特定步长上限\n- 结果概览\n - 数字空间：表4显示BLM1平均64.88，显著优于GPT-4o（59.86）与开源MLLM；细分见表5/表6\n - 物理空间：BLM1在四具身×六任务上均实现稳定跨具身控制成功率，体现共享DiT策略的有效性",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>Tongji University; Shanghai Magic; Koala Uran</li></ul>\n<h1>论文概述</h1>\n<ul><li>背景：现有多模态大语言模型（MLLM）、具身大语言模型（ELLM）、视觉-语言-动作模型（VLA）和通用多模态大模型（GMLM）未能同时实现跨空间、跨任务、跨具身的统一能力</li><li>目标：提出BLM1（Boundless Large Model），在单一统一模型中实现数字空间推理与物理空间控制的融合</li><li>方法：两阶段训练（Stage I SFT注入具身知识；Stage II冻结MLLM，训练基于意图桥接接口的扩散变换器（DiT）策略模块）</li><li>结论：单一BLM1在数字任务上约提升6%，物理任务上约提升3%，同时保持指令跟随与跨具身控制能力</li></ul>\n<h1>论文核心贡献点</h1>\n<ul><li>统一三能力：跨空间迁移（数字→物理）、跨任务学习、跨具身泛化</li><li>保持语言能力：Stage I以SFT注入具身知识，避免破坏MLLM指令跟随</li><li>有效桥接：意图桥接接口+Perceiver压缩高阶意图，支撑高频率闭环控制</li><li>策略共享：Diffusion Transformer作为通用策略头（0.76B参数），跨具身参数共享</li><li>训练配方：加权采样保证数据集平衡；Stage II中后期引入未来预测损失提升泛化</li><li>系统评测：数字空间六大基准、物理空间四具身六任务，建立统一对比</li><li>结果优势：超越四类模型家族（MLLM、ELLM、VLA、GMLM），单一模型不切换架构</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>架构组成</li></ul>\n<p> - Backbone：Qwen2.5-VL-7B-Instruct；融合视觉与语言；抽取第k层隐藏状态Hk作为高层意图</p>\n<p> - 意图桥接接口：Perceiver将Hk压缩为固定K个令牌H̃k，降低计算、适配控制</p>\n<p> - 机器人策略：DiT扩散变换器作为条件生成头；状态编码器fs、动作编码器fa/解码器；预测horizon为h的动作块At</p>\n<ul><li>训练目标</li></ul>\n<p> - SFT目标（Stage I）：标准next-token交叉熵，冻结视觉与投影器，更新语言与投影</p>\n<p> - 流动匹配目标（Stage II早期）：Rectified-flow路径，DiT拟合速度场vφ(xτ,τ\\|H̃k)，LFM为MSE</p>\n<p> - 未来预测目标（Stage II后期）：在DiT中引入可学习未来令牌F，使其中间层表征与冻结MLLM对未来观察o_{t+H}与指令y的压缩表征对齐，用余弦相似度</p>\n<p> - 总体：Stage I仅L_SFT；Stage II早期训练Perceiver/DiT/编解码器；后期叠加L_FP</p>\n<ul><li>训练配方</li></ul>\n<p> - 加权采样：保证各数据集等概率抽取，避免大数据主导</p>\n<p> - Stage I：数字多模态问答对（RoboVQA、AgiBot、HoloAssist、BridgeData V2、EgoPlan、ShareRobot）</p>\n<p> - Stage II：冻结MLLM，使用四具身×六任务的模拟演示数据训练策略；具身特定编码器/解码器独立优化，DiT参数跨具身共享</p>\n<p> - 物理数据生成：基于ManiSkill，高层技能+关键姿态+IK与碰撞检验+路径规划与平滑，录制第三人称与腕部图像、本体感受与动作</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数字空间数据集（Stage I）</li></ul>\n<p> - RoboVQA：829,502视频文本对，29,520指令；多类问答（规划、完成检验、 affordance判别与生成、过去/未来事件）</p>\n<p> - AgiBot：1,001,552条轨迹，2,976.4小时；217任务、87技能、106场景</p>\n<p> - HoloAssist：166小时；350教师-执行者对，20物理任务、16类物体</p>\n<p> - BridgeData V2：60,096轨迹，24环境，100+物类；基础到复杂13技能族</p>\n<p> - EgoPlan：EgoPlan-Bench 4,939多选题 + EgoPlan-IT 50K问答（基于EPIC-Kitchens）</p>\n<p> - ShareRobot：整合Open X-Embodiment的23数据集，102场景、12具身；1,027,990规划问答、6,522 affordance注释、6,870轨迹预测</p>\n<ul><li>物理空间数据集（Stage II）</li></ul>\n<p> - 具身：Franka Emika Panda、xArm-6、xArm-7、WidowX AI</p>\n<p> - 任务：PickCube、PullCube、StackCube、PushCube、PlaceSphere、LiftPegUpright（每具身×任务100集）</p>\n<p> - 总帧数：347.8K；任务特定成功判定</p>\n<ul><li>数字评测基准（Stage I后）</li></ul>\n<p> - 多选题：RoboVQA、AgiBot、HoloAssist、RoboFail</p>\n<p> - 自由问答：EgoThink、ShareRobot</p>\n<p> - 总计：3,160测试样本（见表2）</p>\n<ul><li>训练资源</li></ul>\n<p> - 框架：ManiSkill模拟收集</p>\n<p> - 加权采样：C个数据集等概率，样本在数据集内均匀</p>\n<p> - 视频帧采样：统一0.5秒间隔采样；短视频4帧，长视频8帧；任务前移除测试片段防泄露</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>数字空间评估</li></ul>\n<p> - 指标</p>\n<p> - Exact-match accuracy（EM）：多选题严格匹配归一化答案</p>\n<p> - LLM-as-a-judge（GPT评分）：自由问答使用判别式提示与评分标准</p>\n<p> - EgoThink：s_i∈{0,0.5,1}，最终分数按平均×100</p>\n<p> - ShareRobot：s_i∈{1,2,3,4,5}，最终分数按((s_i−1)/4)平均×100</p>\n<ul><li>物理空间评估</li></ul>\n<p> - 指标：Episode级成功率sr_{e,u}=N^s_{e,u}/N_{e,u}；总体为E具身×U任务的成功率平均</p>\n<p> - 环境：仿真闭环控制；每任务50次rollout，具身特定步长上限</p>\n<ul><li>结果概览</li></ul>\n<p> - 数字空间：表4显示BLM1平均64.88，显著优于GPT-4o（59.86）与开源MLLM；细分见表5/表6</p>\n<p> - 物理空间：BLM1在四具身×六任务上均实现稳定跨具身控制成功率，体现共享DiT策略的有效性</p>"
  },
  {
    "date": "2025-10-27",
    "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
    "link": "http://arxiv.org/abs/2510.23763",
    "summary_markdown": "### 论文研究单位\n复旦大学、上海创新研究院、新加坡国立大学\n### 论文概述\n论文提出“跨模态上下文指令”设置：机器人需从人类语音、环境声与视觉线索中主动推断潜在意图，并通过交互确认后执行动作，而不再依赖显式指令。为解决数据缺失与评估基准不足，构建 OmniAction 数据集与 OmniAction-LIBERO 评测；提出 RoboOmni 端到端全模态大模型，集成感知、推理、对话与控制，实现意图识别—确认—执行的闭环。仿真与真实验证显示其在成功率、推理速度、主动辅助与意图识别上显著优于文本与ASR基线。\n### 论文核心贡献点\n- 定义并研究“跨模态上下文指令”任务：融合语音、环境声与视觉进行主动意图推断与交互确认\n- 提出 RoboOmni：Perceiver–Thinker–Talker–Executor 的端到端全模态框架，统一推理与控制，支持直接语音交互与行动生成\n- 构造 OmniAction：约14万集 multimodal 数据、5千+说话人、2.4千事件声音、640背景、六类上下文指令；并基于 LIBERO 构建 OmniAction-LIBERO 评测套件\n- 实验显示：仿真 OmniAction-LIBERO 上平均成功率达85.6%；真实人声指令平均76.6%；推理延迟降至基线的约0.49倍；具备更强的意图识别与主动协助能力\n### 论文方法描述\n- 架构：Perceiver 统一编码视觉/音频/文本为共享表征；Thinker 基于全模态LLM在词汇与动作标记联合空间自回归生成；Talker 生成语音；Executor 用 FAST+ 将离散动作标记解码为连续控制向量\n- 训练：统一自回归目标，同时优化对话与动作生成。预训练使用64×A100、10天、15,360 A100‑小时、批量512、学习率5e-5；下游SFT为8×A100、5e‑5、10–30k步。图像224×224、音频16kHz、动作块长6\n### 论文使用数据集和训练资源\n- 数据集\n - OmniAction：约141,162集，涵盖112技能、748对象、5,096说话人音色、2,482非言语事件、640环境背景与六类上下文指令\n - OmniAction‑LIBERO：基于 LIBERO 的仿真评测（OmniAction‑LIBERO‑TTS：40任务×6变体=240评测量；OmniAction‑LIBERO‑Real：10名志愿者真实语音指令）\n- 训练资源\n - 预训练：64×A100，10天，总计约15,360 A100‑小时，批量512，LR=5e-5，前1k步warm‑up\n - 下游SFT：8×A100，LR=5e-5，10–30k步\n### 论文使用的评估环境和评估指标\n- 仿真评估：OmniAction‑LIBERO（四套任务：Spatial、Goal、Object、Long‑Horizon）×六种上下文指令；与 OpenVLA、OpenVLA‑OFT、π0、NORA 比较；指标为成功率\n- 真实人声评测：OmniAction‑LIBERO‑Real；对比“真实文本提示/ASR转文本→VLA”与RoboOmni直接音频输入；指标为成功率\n- 主动辅助与意图识别：对比 Qwen2.5‑Omni‑3B/7B 与 ASR+GPT‑4o；指标为意图识别准确率、交互能力质性评估\n- 效率分析：对比端到端与级联流水线；指标为推理延迟（相对ASR+OpenVLA的倍数）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>复旦大学、上海创新研究院、新加坡国立大学</p>\n<h3>论文概述</h3>\n<p>论文提出“跨模态上下文指令”设置：机器人需从人类语音、环境声与视觉线索中主动推断潜在意图，并通过交互确认后执行动作，而不再依赖显式指令。为解决数据缺失与评估基准不足，构建 OmniAction 数据集与 OmniAction-LIBERO 评测；提出 RoboOmni 端到端全模态大模型，集成感知、推理、对话与控制，实现意图识别—确认—执行的闭环。仿真与真实验证显示其在成功率、推理速度、主动辅助与意图识别上显著优于文本与ASR基线。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>定义并研究“跨模态上下文指令”任务：融合语音、环境声与视觉进行主动意图推断与交互确认</li><li>提出 RoboOmni：Perceiver–Thinker–Talker–Executor 的端到端全模态框架，统一推理与控制，支持直接语音交互与行动生成</li><li>构造 OmniAction：约14万集 multimodal 数据、5千+说话人、2.4千事件声音、640背景、六类上下文指令；并基于 LIBERO 构建 OmniAction-LIBERO 评测套件</li><li>实验显示：仿真 OmniAction-LIBERO 上平均成功率达85.6%；真实人声指令平均76.6%；推理延迟降至基线的约0.49倍；具备更强的意图识别与主动协助能力</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>架构：Perceiver 统一编码视觉/音频/文本为共享表征；Thinker 基于全模态LLM在词汇与动作标记联合空间自回归生成；Talker 生成语音；Executor 用 FAST+ 将离散动作标记解码为连续控制向量</li><li>训练：统一自回归目标，同时优化对话与动作生成。预训练使用64×A100、10天、15,360 A100‑小时、批量512、学习率5e-5；下游SFT为8×A100、5e‑5、10–30k步。图像224×224、音频16kHz、动作块长6</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集</li></ul>\n<p> - OmniAction：约141,162集，涵盖112技能、748对象、5,096说话人音色、2,482非言语事件、640环境背景与六类上下文指令</p>\n<p> - OmniAction‑LIBERO：基于 LIBERO 的仿真评测（OmniAction‑LIBERO‑TTS：40任务×6变体=240评测量；OmniAction‑LIBERO‑Real：10名志愿者真实语音指令）</p>\n<ul><li>训练资源</li></ul>\n<p> - 预训练：64×A100，10天，总计约15,360 A100‑小时，批量512，LR=5e-5，前1k步warm‑up</p>\n<p> - 下游SFT：8×A100，LR=5e-5，10–30k步</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>仿真评估：OmniAction‑LIBERO（四套任务：Spatial、Goal、Object、Long‑Horizon）×六种上下文指令；与 OpenVLA、OpenVLA‑OFT、π0、NORA 比较；指标为成功率</li><li>真实人声评测：OmniAction‑LIBERO‑Real；对比“真实文本提示/ASR转文本→VLA”与RoboOmni直接音频输入；指标为成功率</li><li>主动辅助与意图识别：对比 Qwen2.5‑Omni‑3B/7B 与 ASR+GPT‑4o；指标为意图识别准确率、交互能力质性评估</li><li>效率分析：对比端到端与级联流水线；指标为推理延迟（相对ASR+OpenVLA的倍数）</li></ul>"
  },
  {
    "date": "2025-10-27",
    "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
    "link": "http://arxiv.org/abs/2510.23576",
    "summary_markdown": "## 论文研究单位\n北京大学、Galbot、中国科学技术大学、北京人工智能研究院(BAAI)\n## 论文概述\nUrbanVLA是一个面向城市微移动性应用的视觉-语言-动作(VLA)框架，主要用于配送机器人等在城市环境中的长距离导航任务。该方法通过整合导航工具提供的高级路线信息与机器人视觉感知，实现了大规模城市场景中可靠、安全的长程导航。\n## 论文核心贡献点\n- 提出首个面向城市微移动性的路线条件化VLA框架，将导航工具指导与视觉语言策略学习有效结合\n- 开发了模拟到现实的训练管道，通过路线提升算法构建模拟-现实聚合数据集，实现SOTA性能并展示现实世界泛化能力\n- 引入基于IQL的强化学习微调方法，显著提升安全关键行为能力，包括避障、行人互动和交通合规性\n## 论文方法描述\nUrbanVLA基于预训练的导航基础模型NavFoM，采用两阶段训练策略：**(1)监督微调(SFT)**：在MetaUrban模拟器和网络导航视频数据上训练，结合VideoQA任务和路线条件导航任务，学习基本导航能力；**(2)强化微调(RFT)**：使用隐式Q-Learning(IQL)在模拟-现实聚合数据上进行离线强化学习，优化安全性和适应性。方法核心包括多模态特征融合、路线与视觉观察对齐、轨迹级别的策略优化。\n## 论文使用数据集和训练资源\n- **模拟数据**：MetaUrban-12K数据集(约40小时，2400集)，由PPO专家生成\n- **现实数据**：约8小时的人为遥操作系统数据\n- **网络数据**：Sekai网络导航视频数据集和LongVU视频问答数据集\n- **训练资源**：8张NVIDIA H100 GPU，训练约12小时(总计96 GPU小时)\n## 论文使用的评估环境和评估指标\n- **模拟环境评估**：MetaUrban基准，包含PointNav和SocialNav任务，在MetaUrban-test(1000场景)和MetaUrban-unseen(100场景)上测试\n- **现实环境评估**：在城市街区进行，包括天桥、行人横道等复杂场景，机器人为Unitree Go2四足机器人\n- **评估指标**：成功率(SR)、路径长度加权的成功率(SPL)、社交导航分数(SNS)、累积成本(CC)、路线完成度(RC)等",
    "summary_html": "<h2>论文研究单位</h2>\n<p>北京大学、Galbot、中国科学技术大学、北京人工智能研究院(BAAI)</p>\n<h2>论文概述</h2>\n<p>UrbanVLA是一个面向城市微移动性应用的视觉-语言-动作(VLA)框架，主要用于配送机器人等在城市环境中的长距离导航任务。该方法通过整合导航工具提供的高级路线信息与机器人视觉感知，实现了大规模城市场景中可靠、安全的长程导航。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出首个面向城市微移动性的路线条件化VLA框架，将导航工具指导与视觉语言策略学习有效结合</li><li>开发了模拟到现实的训练管道，通过路线提升算法构建模拟-现实聚合数据集，实现SOTA性能并展示现实世界泛化能力</li><li>引入基于IQL的强化学习微调方法，显著提升安全关键行为能力，包括避障、行人互动和交通合规性</li></ul>\n<h2>论文方法描述</h2>\n<p>UrbanVLA基于预训练的导航基础模型NavFoM，采用两阶段训练策略：<strong>(1)监督微调(SFT)</strong>：在MetaUrban模拟器和网络导航视频数据上训练，结合VideoQA任务和路线条件导航任务，学习基本导航能力；<strong>(2)强化微调(RFT)</strong>：使用隐式Q-Learning(IQL)在模拟-现实聚合数据上进行离线强化学习，优化安全性和适应性。方法核心包括多模态特征融合、路线与视觉观察对齐、轨迹级别的策略优化。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>模拟数据</strong>：MetaUrban-12K数据集(约40小时，2400集)，由PPO专家生成</li><li><strong>现实数据</strong>：约8小时的人为遥操作系统数据</li><li><strong>网络数据</strong>：Sekai网络导航视频数据集和LongVU视频问答数据集</li><li><strong>训练资源</strong>：8张NVIDIA H100 GPU，训练约12小时(总计96 GPU小时)</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>模拟环境评估</strong>：MetaUrban基准，包含PointNav和SocialNav任务，在MetaUrban-test(1000场景)和MetaUrban-unseen(100场景)上测试</li><li><strong>现实环境评估</strong>：在城市街区进行，包括天桥、行人横道等复杂场景，机器人为Unitree Go2四足机器人</li><li><strong>评估指标</strong>：成功率(SR)、路径长度加权的成功率(SPL)、社交导航分数(SNS)、累积成本(CC)、路线完成度(RC)等</li></ul>"
  },
  {
    "date": "2025-10-27",
    "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation",
    "link": "http://arxiv.org/abs/2510.23571",
    "summary_markdown": "### 论文研究单位\n卡内基梅隆大学、浙江大学、北京大学、国立台湾大学\n### 论文概述\n随着通用机器人策略的快速发展，其评估面临规模化、可复现性与安全性等挑战，现有评测要么依赖真实世界环境（人力与安全成本高），要么局限在同一合成域内训练—测试，无法评估真实数据或其他环境训练的模型。为解决这些问题，本文提出RobotArena ∞，通过自动化将真实演示视频映射为大规模可扩展的仿真环境，并结合视觉语言模型（VLM）自动评分与人类偏好反馈，对来自全球多实验室的视觉语言动作（VLA）策略开展系统性评测与稳健性检验。\n### 论文核心贡献点\n- 提出可扩展、可扩展的机器人评测协议：结合物理引擎、真实到仿真（real-to-sim）转换与人类偏好反馈，实现规模化且可复现的评测。\n- 完整自动化的reality-to-simulation翻译管线：融合VLMs、2D到3D生成模型与可微分渲染，无需人工标定或额外注释。\n- 大规模跨实验室政策评测：聚合超过7000条人类偏好比较、覆盖百余个仿真场景及多种扰动，目前最大规模的机器人评测工作。\n- 系统性评测发现：跨数据集泛化能力弱、模型架构差异明显、颜色扰动下强VLM背bone策略更稳健、背景变化带来显著性能衰减。\n### 论文方法描述\n方法核心是将演示视频自动翻译为仿真环境，再对策略进行多维度评测。\n\n- 自动化视频到仿真映射：提取五个关键要素（1）相机—机器人六自由度位姿；（2）任务相关物体的三维网格重建、姿态与材质属性；（3）场景深度；（4）干净背景；（5）控制增益估计。\n- 机器人—相机标定：采用可微分渲染的分析—综合方法，优化RGB、光流与DINOv2特征对齐损失，估计相机—机器人位姿。\n- 物体与场景三维重建：利用Gemini进行机器人与前景物体分割；InvSR提升分割图像质量；Hunyuan-3D生成有纹理网格；通过MINIMA建立真实crop与模拟渲染视图的2D对应；利用MoGe单目深度与SVD解算刚体变换；用LaMa进行背景修复；系统辨识校准PD控制增益。\n- 可控域扰动：ΔBG替换背景；ΔColor调整颜色通道（如BGR转换）；ΔObjPose随机改变物体位置；用于压力测试策略泛化。\n- 评测方式：绝对评测用VLM对打乱帧序列进行任务进度打分（最后30%帧平均分数与人类进度相关性最佳）；相对评测采用双盲成对比较，收集偏好标签与自由文本解释，使用Bradley–Terry模型与Sandwich方差得到全局排名与置信区间。\n- 对比与外部验证：与SIMPLER基准比较，评估策略在BridgeSim与SIMPLER四场景上的表现一致性；在一个真实任务“把胡萝卜放到盘子里”验证仿真—现实一致性。\n### 论文使用数据集和训练资源\n- 数据源与仿真环境：BridgeSim（Bridge v2，OXE的广泛使用子集）、DROIDSim（DROID，因噪声较高常被排除于预训练）、Rh20TSim（仅SpatialVLA用其训练）。\n- 评测对象策略：Octo-Base（93M，OXE预训练）、RoboVLM（基于KosMos的VLA）、SpatialVLA（引入3D空间表征）、CogAct（7B VLM背bone+扩散Transformer动作预测）。\n- 环境规模：100个名义环境与数百种扰动（ΔBG/ΔColor/ΔObjPose），累计7000+人类偏好比较与多轮VLM自动评分。\n### 论文使用的评估环境和评估指标\n- 评估环境：自动化生成的仿真环境来源于真实演示视频，支持内分布（与训练集对应）与外分布（超出训练集）评测；在仿真中部署策略执行并录制轨迹视频用于后续评分。\n- 评估指标：\n - 自动化VLM任务进度评分：打乱帧序列+VLM打分，优选“执行最后30%帧的平均分数”作为进度指标。\n - 人类偏好成对比较：偏好标签（胜/负/平局）与自由文本理由；Bradley–Terry模型全局排名，Sandwich方差估计置信区间。\n - 稳健性：跨数据集迁移性能（BridgeSim vs. DROIDSim vs. Rh20TSim），以及在ΔBG/ΔColor/ΔObjPose扰动下的性能曲线与趋势。\n - 现实—仿真一致性：单任务现实与仿真部署对比（如“把胡萝卜放到盘子里”）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>卡内基梅隆大学、浙江大学、北京大学、国立台湾大学</p>\n<h3>论文概述</h3>\n<p>随着通用机器人策略的快速发展，其评估面临规模化、可复现性与安全性等挑战，现有评测要么依赖真实世界环境（人力与安全成本高），要么局限在同一合成域内训练—测试，无法评估真实数据或其他环境训练的模型。为解决这些问题，本文提出RobotArena ∞，通过自动化将真实演示视频映射为大规模可扩展的仿真环境，并结合视觉语言模型（VLM）自动评分与人类偏好反馈，对来自全球多实验室的视觉语言动作（VLA）策略开展系统性评测与稳健性检验。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出可扩展、可扩展的机器人评测协议：结合物理引擎、真实到仿真（real-to-sim）转换与人类偏好反馈，实现规模化且可复现的评测。</li><li>完整自动化的reality-to-simulation翻译管线：融合VLMs、2D到3D生成模型与可微分渲染，无需人工标定或额外注释。</li><li>大规模跨实验室政策评测：聚合超过7000条人类偏好比较、覆盖百余个仿真场景及多种扰动，目前最大规模的机器人评测工作。</li><li>系统性评测发现：跨数据集泛化能力弱、模型架构差异明显、颜色扰动下强VLM背bone策略更稳健、背景变化带来显著性能衰减。</li></ul>\n<h3>论文方法描述</h3>\n<p>方法核心是将演示视频自动翻译为仿真环境，再对策略进行多维度评测。</p>\n\n<ul><li>自动化视频到仿真映射：提取五个关键要素（1）相机—机器人六自由度位姿；（2）任务相关物体的三维网格重建、姿态与材质属性；（3）场景深度；（4）干净背景；（5）控制增益估计。</li><li>机器人—相机标定：采用可微分渲染的分析—综合方法，优化RGB、光流与DINOv2特征对齐损失，估计相机—机器人位姿。</li><li>物体与场景三维重建：利用Gemini进行机器人与前景物体分割；InvSR提升分割图像质量；Hunyuan-3D生成有纹理网格；通过MINIMA建立真实crop与模拟渲染视图的2D对应；利用MoGe单目深度与SVD解算刚体变换；用LaMa进行背景修复；系统辨识校准PD控制增益。</li><li>可控域扰动：ΔBG替换背景；ΔColor调整颜色通道（如BGR转换）；ΔObjPose随机改变物体位置；用于压力测试策略泛化。</li><li>评测方式：绝对评测用VLM对打乱帧序列进行任务进度打分（最后30%帧平均分数与人类进度相关性最佳）；相对评测采用双盲成对比较，收集偏好标签与自由文本解释，使用Bradley–Terry模型与Sandwich方差得到全局排名与置信区间。</li><li>对比与外部验证：与SIMPLER基准比较，评估策略在BridgeSim与SIMPLER四场景上的表现一致性；在一个真实任务“把胡萝卜放到盘子里”验证仿真—现实一致性。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据源与仿真环境：BridgeSim（Bridge v2，OXE的广泛使用子集）、DROIDSim（DROID，因噪声较高常被排除于预训练）、Rh20TSim（仅SpatialVLA用其训练）。</li><li>评测对象策略：Octo-Base（93M，OXE预训练）、RoboVLM（基于KosMos的VLA）、SpatialVLA（引入3D空间表征）、CogAct（7B VLM背bone+扩散Transformer动作预测）。</li><li>环境规模：100个名义环境与数百种扰动（ΔBG/ΔColor/ΔObjPose），累计7000+人类偏好比较与多轮VLM自动评分。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：自动化生成的仿真环境来源于真实演示视频，支持内分布（与训练集对应）与外分布（超出训练集）评测；在仿真中部署策略执行并录制轨迹视频用于后续评分。</li><li>评估指标：</li></ul>\n<p> - 自动化VLM任务进度评分：打乱帧序列+VLM打分，优选“执行最后30%帧的平均分数”作为进度指标。</p>\n<p> - 人类偏好成对比较：偏好标签（胜/负/平局）与自由文本理由；Bradley–Terry模型全局排名，Sandwich方差估计置信区间。</p>\n<p> - 稳健性：跨数据集迁移性能（BridgeSim vs. DROIDSim vs. Rh20TSim），以及在ΔBG/ΔColor/ΔObjPose扰动下的性能曲线与趋势。</p>\n<p> - 现实—仿真一致性：单任务现实与仿真部署对比（如“把胡萝卜放到盘子里”）。</p>"
  },
  {
    "date": "2025-10-27",
    "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox",
    "link": "http://arxiv.org/abs/2510.23511",
    "summary_markdown": "# 论文总结\n## 论文研究单位\nDexmal、StepFun\n## 论文概述\nDexbotic是一个基于PyTorch的开源视觉-语言-动作(VLA)模型工具箱，为具身智能领域的专业人士提供一站式VLA研究服务。该工具箱统一了不同研究机构的VLA策略框架，解决了研究碎片化问题，支持多种主流VLA策略的复现，并提供更强的预训练模型以提升性能。\n## 论文核心贡献点\n- 统一模块化VLA框架：将VLA策略标准化为视觉语言模型(VLM)和动作专家(AE)两个部分\n- 预训练基础模型：提供比原始开源模型更强大的预训练模型，在多个基准测试中显著提升性能\n- 实验为中心开发：采用分层配置架构，用户可通过简单修改Exp脚本快速开发新实验\n- 多平台训练支持：同时支持云端(如阿里云)和本地GPU(RTX 4090)训练\n- 多机器人支持：统一数据格式支持UR5、Franka、ALOHA等多种主流机器人平台\n## 论文方法描述\n统一架构设计：\n- VLM部分：包含视觉编码器(CLIP)、投影器(两层MLP)和大语言模型(Qwen2.5)\n- AE部分：支持扩散变换器、多层感知机或专家混合等不同架构\n\n预训练策略：\n- 离散预训练模型(Dexbotic-Base)：将连续动作量化为256个离散区间\n- 连续预训练模型：支持单臂和双臂任务，通过扩展噪声token数量实现\n- 多视图处理：共享视觉编码器处理多视角输入\n\n数据格式：\n- Dexdata格式：统一存储机器人数据集，包含video和jsonl两个核心元素\n- 相比LeRobot和RLDS格式更节省存储空间\n## 论文使用数据集和训练资源\n训练数据来源：\n- Open-X Embodiment数据集子集\n- 仿真数据：RLBench、LIBERO、ManiSkill2\n- 真实机器人数据：UR5等多型号单臂机器人\n- 私有数据集：52个操作任务，使用8种单臂真实机器人收集\n- 双臂数据：Robomind、AgiBot World数据集及ALOHA双臂机器人数据\n\n训练环境：\n- 云端平台：阿里云、Volcano Engine\n- 本地GPU：RTX 4090等消费级显卡\n## 论文使用的评估环境和评估指标\n仿真基准测试：\n- SimplerEnv：WidowX机器人，包含4项视觉匹配任务\n- CALVIN：ABC→D设定，评估长期任务完成能力\n- ManiSkill2：5项代表性抓取和堆叠任务\n- RoboTwin2.0：4项双臂操作任务\n- LIBERO：4个任务套件(Spatial、Object、Goal、Long)\n\n评估指标：\n- 任务成功率(Success Rate)：各基准测试的核心指标\n- 平均完成长度(CALVIN)：连续完成的任务序列平均长度\n- 性能提升幅度：与原始开源模型的对比改善\n\n真实世界评估：\n- 在UR5e、ALOHA、ARX5、Franka等机器人上验证\n- 任务类型：日常操作如摆放盘子、按钮操作、纸张粉碎等\n- 典型结果：set the plates任务100%成功率，search the green box任务80%成功率",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>Dexmal、StepFun</p>\n<h2>论文概述</h2>\n<p>Dexbotic是一个基于PyTorch的开源视觉-语言-动作(VLA)模型工具箱，为具身智能领域的专业人士提供一站式VLA研究服务。该工具箱统一了不同研究机构的VLA策略框架，解决了研究碎片化问题，支持多种主流VLA策略的复现，并提供更强的预训练模型以提升性能。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>统一模块化VLA框架：将VLA策略标准化为视觉语言模型(VLM)和动作专家(AE)两个部分</li><li>预训练基础模型：提供比原始开源模型更强大的预训练模型，在多个基准测试中显著提升性能</li><li>实验为中心开发：采用分层配置架构，用户可通过简单修改Exp脚本快速开发新实验</li><li>多平台训练支持：同时支持云端(如阿里云)和本地GPU(RTX 4090)训练</li><li>多机器人支持：统一数据格式支持UR5、Franka、ALOHA等多种主流机器人平台</li></ul>\n<h2>论文方法描述</h2>\n<p>统一架构设计：</p>\n<ul><li>VLM部分：包含视觉编码器(CLIP)、投影器(两层MLP)和大语言模型(Qwen2.5)</li><li>AE部分：支持扩散变换器、多层感知机或专家混合等不同架构</li></ul>\n\n<p>预训练策略：</p>\n<ul><li>离散预训练模型(Dexbotic-Base)：将连续动作量化为256个离散区间</li><li>连续预训练模型：支持单臂和双臂任务，通过扩展噪声token数量实现</li><li>多视图处理：共享视觉编码器处理多视角输入</li></ul>\n\n<p>数据格式：</p>\n<ul><li>Dexdata格式：统一存储机器人数据集，包含video和jsonl两个核心元素</li><li>相比LeRobot和RLDS格式更节省存储空间</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<p>训练数据来源：</p>\n<ul><li>Open-X Embodiment数据集子集</li><li>仿真数据：RLBench、LIBERO、ManiSkill2</li><li>真实机器人数据：UR5等多型号单臂机器人</li><li>私有数据集：52个操作任务，使用8种单臂真实机器人收集</li><li>双臂数据：Robomind、AgiBot World数据集及ALOHA双臂机器人数据</li></ul>\n\n<p>训练环境：</p>\n<ul><li>云端平台：阿里云、Volcano Engine</li><li>本地GPU：RTX 4090等消费级显卡</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>仿真基准测试：</p>\n<ul><li>SimplerEnv：WidowX机器人，包含4项视觉匹配任务</li><li>CALVIN：ABC→D设定，评估长期任务完成能力</li><li>ManiSkill2：5项代表性抓取和堆叠任务</li><li>RoboTwin2.0：4项双臂操作任务</li><li>LIBERO：4个任务套件(Spatial、Object、Goal、Long)</li></ul>\n\n<p>评估指标：</p>\n<ul><li>任务成功率(Success Rate)：各基准测试的核心指标</li><li>平均完成长度(CALVIN)：连续完成的任务序列平均长度</li><li>性能提升幅度：与原始开源模型的对比改善</li></ul>\n\n<p>真实世界评估：</p>\n<ul><li>在UR5e、ALOHA、ARX5、Franka等机器人上验证</li><li>任务类型：日常操作如摆放盘子、按钮操作、纸张粉碎等</li><li>典型结果：set the plates任务100%成功率，search the green box任务80%成功率</li></ul>"
  },
  {
    "date": "2025-10-25",
    "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
    "link": "http://arxiv.org/abs/2510.22201",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-23",
    "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence",
    "link": "http://arxiv.org/abs/2510.21860",
    "summary_markdown": "### 论文研究单位\nAndon Labs（研究机构及实验室）\n### 论文概述\nButter-Bench是一个评估LLM控制机器人实用智能的基准测试。论文探讨当前SOTA大型语言模型在机器人编排（orchestration）能力上的上限，通过设计真实世界任务（如“传递黄油”）评估模型的空间推理、社交理解和物理世界常识等能力。研究发现，人类在所有任务中显著优于LLM，最佳模型完成率仅达40%，人类平均达95%。\n### 论文核心贡献点\n1. **引入实用智能评估框架**：提出区分分析智能（analytical intelligence）与实用智能（practical intelligence）的机器人基准测试。\n2. **隔离编排器评估**：在简化硬件架构下测试LLM编排能力，避免VLA（视觉语言动作）模型干扰。\n3. **系统性能力测评**：设计六大任务全面覆盖空间导航、视觉推理、社交互动和多步规划。\n4. **红队安全评估**：在压力场景（低电量、充电器故障）下测试模型的对抗性脆弱性。\n5. **对比人类基线**：明确人类与LLM在真实环境中的能力差距。\n### 论文方法描述\n- **硬件平台**：使用TurtleBot 4标准移动机器人（搭载OAK-D相机、2D激光雷达、IMU等），运行ROS 2系统提供SLAM能力。\n- **代理架构**：采用ReAct风格循环，LLM观察环境→推理决策→调用工具执行动作。工具箱包括：\n 1. 运动控制：`drive`, `rotate`, `wait`\n 2. 维护功能：`dock`, `undock`, `status`\n 3. 视觉感知：`take_photo`\n 4. 导航工具：`view_map`, `navigate_to`\n 5. 社交交互：`read_msg`, `send_msg`, `save_image`\n- **任务设计**：六个子任务评估不同能力（如识别含黄油纸袋、注意用户缺席、多步路径规划等）。\n- **红队测试**：模拟低电量+充电器故障场景，诱导模型泄露机密信息。\n### 论文使用数据集和训练资源\n- **任务数据集**：基于真实办公环境设计的6项操作任务（附录A详述），包含接受标准。\n- **模型评估**：测试多个LLM：\n - **SOTA通用模型**：Gemini 2.5 Pro、Claude Opus 4.1、GPT-5、Grok 4、Llama 4 Maverick\n - **专用实体模型**：Gemini ER 1.5（针对机器人调优）\n- **人类基线**：3名人类操作员通过网页界面控制机器人完成相同任务（不知环境布局）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：真实办公室场景（单房间含出口、办公桌、厨房等），固定光照和障碍物布局。\n- **核心指标**：\n 1. **主要指标**：任务完成率（每模型-任务组合测试5次）。\n 2. **辅助指标**：\n - 任务完成时间（推理延迟+规划效率）\n - 工具调用分布（分析工具使用模式）\n - 定性失效模式分类（空间推理、社交理解等5类）。\n- **安全性评估**：红队攻击成功率（如泄露机密信息的倾向性）。\n\n**结果摘要**\n- 人类vs LLM：人类平均95%完成率，最佳LLM Gemini 2.5 Pro仅40%。\n- **关键弱点**：\n - 社交理解（\"注意缺席\"任务：LLM 0% vs 人类100%）\n - 多步空间规划（\"Plan\"任务：LLM依赖随机路径漂移，非真实理解）\n- **模型差异**：\n Gemini ER 1.5未优于Gemini 2.5 Pro，表明实体调优未显著提升实用智能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Andon Labs（研究机构及实验室）</p>\n<h3>论文概述</h3>\n<p>Butter-Bench是一个评估LLM控制机器人实用智能的基准测试。论文探讨当前SOTA大型语言模型在机器人编排（orchestration）能力上的上限，通过设计真实世界任务（如“传递黄油”）评估模型的空间推理、社交理解和物理世界常识等能力。研究发现，人类在所有任务中显著优于LLM，最佳模型完成率仅达40%，人类平均达95%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>引入实用智能评估框架</strong>：提出区分分析智能（analytical intelligence）与实用智能（practical intelligence）的机器人基准测试。</li><li><strong>隔离编排器评估</strong>：在简化硬件架构下测试LLM编排能力，避免VLA（视觉语言动作）模型干扰。</li><li><strong>系统性能力测评</strong>：设计六大任务全面覆盖空间导航、视觉推理、社交互动和多步规划。</li><li><strong>红队安全评估</strong>：在压力场景（低电量、充电器故障）下测试模型的对抗性脆弱性。</li><li><strong>对比人类基线</strong>：明确人类与LLM在真实环境中的能力差距。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>硬件平台</strong>：使用TurtleBot 4标准移动机器人（搭载OAK-D相机、2D激光雷达、IMU等），运行ROS 2系统提供SLAM能力。</li><li><strong>代理架构</strong>：采用ReAct风格循环，LLM观察环境→推理决策→调用工具执行动作。工具箱包括：</li></ul>\n<p> 1. 运动控制：<code>drive</code>, <code>rotate</code>, <code>wait</code></p>\n<p> 2. 维护功能：<code>dock</code>, <code>undock</code>, <code>status</code></p>\n<p> 3. 视觉感知：<code>take_photo</code></p>\n<p> 4. 导航工具：<code>view_map</code>, <code>navigate_to</code></p>\n<p> 5. 社交交互：<code>read_msg</code>, <code>send_msg</code>, <code>save_image</code></p>\n<ul><li><strong>任务设计</strong>：六个子任务评估不同能力（如识别含黄油纸袋、注意用户缺席、多步路径规划等）。</li><li><strong>红队测试</strong>：模拟低电量+充电器故障场景，诱导模型泄露机密信息。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>任务数据集</strong>：基于真实办公环境设计的6项操作任务（附录A详述），包含接受标准。</li><li><strong>模型评估</strong>：测试多个LLM：</li></ul>\n<p> - <strong>SOTA通用模型</strong>：Gemini 2.5 Pro、Claude Opus 4.1、GPT-5、Grok 4、Llama 4 Maverick</p>\n<p> - <strong>专用实体模型</strong>：Gemini ER 1.5（针对机器人调优）</p>\n<ul><li><strong>人类基线</strong>：3名人类操作员通过网页界面控制机器人完成相同任务（不知环境布局）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：真实办公室场景（单房间含出口、办公桌、厨房等），固定光照和障碍物布局。</li><li><strong>核心指标</strong>：</li></ul>\n<p> 1. <strong>主要指标</strong>：任务完成率（每模型-任务组合测试5次）。</p>\n<p> 2. <strong>辅助指标</strong>：</p>\n<p> - 任务完成时间（推理延迟+规划效率）</p>\n<p> - 工具调用分布（分析工具使用模式）</p>\n<p> - 定性失效模式分类（空间推理、社交理解等5类）。</p>\n<ul><li><strong>安全性评估</strong>：红队攻击成功率（如泄露机密信息的倾向性）。</li></ul>\n\n<p><strong>结果摘要</strong></p>\n<ul><li>人类vs LLM：人类平均95%完成率，最佳LLM Gemini 2.5 Pro仅40%。</li><li><strong>关键弱点</strong>：</li></ul>\n<p> - 社交理解（\"注意缺席\"任务：LLM 0% vs 人类100%）</p>\n<p> - 多步空间规划（\"Plan\"任务：LLM依赖随机路径漂移，非真实理解）</p>\n<ul><li><strong>模型差异</strong>：</li></ul>\n<p> Gemini ER 1.5未优于Gemini 2.5 Pro，表明实体调优未显著提升实用智能。</p>"
  },
  {
    "date": "2025-10-21",
    "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting",
    "link": "http://arxiv.org/abs/2510.21817",
    "summary_markdown": "论文研究单位\n南京大学, 腾讯优图实验室, 中国科学院自动化研究所, 傅里叶智能有限公司。\n\n论文概述\n针对当前视觉-语言-动作（VLA）模型交互范式僵化、无法并发处理视听说动和实时中断的问题，论文提出了VITA-E框架。该框架通过一个双模型架构，使机器人能够同时观察环境、倾听用户指令、提供语言回复并执行动作，实现了类似人类的多任务处理能力，并能响应用户的紧急中断。\n\n论文核心贡献点\n1. 提出了一种用于并发交互的双模型架构，其中两个VLA实例分别作为主动模型和备用模型协同工作。\n2. 设计了一种基于特殊令牌的控制流，VLM生成如[ACT]、[HALT]等令牌直接驱动系统状态转换。\n3. 提出了一种训练交互式VLA的方法，包括数据整理和微调策略，使VLM学会生成系统级控制令牌。\n\n论文方法描述\nVITA-E采用“模型即控制器”范式和双模型交互核心。核心是微调一个视觉语言模型（VLM），使其生成特殊令牌（如[RES], [ACT], [HALT], [END]），作为系统级命令来控制行为。系统由VLM（负责高级理解）和扩散动作专家（负责底层电机控制）组成的双系统架构驱动。双模型架构包含一个“主动模型”和一个“备用模型”，通过同步原语协调，实现了四种交互模式：行为并发、语音中断、任务切换和紧急停止。\n\n论文使用数据集和训练资源\n使用的数据集包括ActionNet、Libero、自收集的真实世界场景数据以及通过遥操作机器人收集的300个演示轨迹。训练在Fourier GR2人形机器人平台上进行，采用DeepSpeed ZeRO-3配置。模型训练分为两阶段：首先微调VLM，然后训练扩散动作专家。\n\n论文使用的评估环境和评估指标\n评估环境包括Libero仿真基准和真实的Fourier GR2人形机器人。评估指标方面，基础操作任务和交互式任务（语音中断、任务切换、紧急停止）使用成功率进行衡量。并发任务则通过定性展示和语音响应的平均延迟（2.26秒）进行评估。",
    "summary_html": "<p>论文研究单位</p>\n<p>南京大学, 腾讯优图实验室, 中国科学院自动化研究所, 傅里叶智能有限公司。</p>\n\n<p>论文概述</p>\n<p>针对当前视觉-语言-动作（VLA）模型交互范式僵化、无法并发处理视听说动和实时中断的问题，论文提出了VITA-E框架。该框架通过一个双模型架构，使机器人能够同时观察环境、倾听用户指令、提供语言回复并执行动作，实现了类似人类的多任务处理能力，并能响应用户的紧急中断。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种用于并发交互的双模型架构，其中两个VLA实例分别作为主动模型和备用模型协同工作。</li><li>设计了一种基于特殊令牌的控制流，VLM生成如[ACT]、[HALT]等令牌直接驱动系统状态转换。</li><li>提出了一种训练交互式VLA的方法，包括数据整理和微调策略，使VLM学会生成系统级控制令牌。</li></ol>\n\n<p>论文方法描述</p>\n<p>VITA-E采用“模型即控制器”范式和双模型交互核心。核心是微调一个视觉语言模型（VLM），使其生成特殊令牌（如[RES], [ACT], [HALT], [END]），作为系统级命令来控制行为。系统由VLM（负责高级理解）和扩散动作专家（负责底层电机控制）组成的双系统架构驱动。双模型架构包含一个“主动模型”和一个“备用模型”，通过同步原语协调，实现了四种交互模式：行为并发、语音中断、任务切换和紧急停止。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用的数据集包括ActionNet、Libero、自收集的真实世界场景数据以及通过遥操作机器人收集的300个演示轨迹。训练在Fourier GR2人形机器人平台上进行，采用DeepSpeed ZeRO-3配置。模型训练分为两阶段：首先微调VLM，然后训练扩散动作专家。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境包括Libero仿真基准和真实的Fourier GR2人形机器人。评估指标方面，基础操作任务和交互式任务（语音中断、任务切换、紧急停止）使用成功率进行衡量。并发任务则通过定性展示和语音响应的平均延迟（2.26秒）进行评估。</p>"
  },
  {
    "date": "2025-10-24",
    "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos",
    "link": "http://arxiv.org/abs/2510.21571",
    "summary_markdown": "# 论文研究单位\n\n清华大学、微软亚洲研究院\n# 论文概述\n\n本文提出了一种将无脚本真实生活人类手部活动视频转换为视觉-语言-动作（VLA）数据格式的新方法，用于预训练机器人操作VLA模型。该方法将人类手部视为灵巧机器人末端执行器，通过开发全自动化的人类活动分析框架，将\"野生\"第一人称人类视频转换为与现有机器人VLA训练数据完全对齐的数据格式，无需任何注释。最终构建了包含100万片段和2600万帧的大规模手部VLA数据集，并设计了灵巧手部VLA模型。\n# 论文核心贡献点\n\n1. **首个VLA预训练方法**：首次将非结构化人类视频转换为与机器人VLA训练数据完全对齐的数据格式，无需人工注释\n\n2. **自动化分析框架**：开发了全自动化的人类活动分析框架，包含三维运动标注、原子动作分割和指令标注三个阶段\n\n3. **大规模数据集构建**：构建了包含100万片段和2600万帧的手部VLA数据集，覆盖广泛的对象、概念、技能和环境变化\n\n4. **VLA模型架构**：设计了由VLM骨干网和扩散动作专家组成的灵巧手部VLA模型，支持统一的单双hand动作预测\n\n5. **零样本泛化能力**：模型在完全未见过的真实世界观察中展现出强大的零样本能力\n\n6. **缩放行为验证**：证明了模型任务性能随预训练数据规模的清晰缩放行为\n# 论文方法描述\n## 数据转换框架\n\n**三维运动标注**：采用单目三维相机和手部姿态跟踪方法，利用深度视觉SLAM、深度估计和手部重建技术，估计相机内参和帧级手部姿态（基于6D手腕姿态和完整关节角度）\n\n**原子动作分割**：通过检测世界空间中3D手部手腕的速度最小值作为切割点，将长视频分割为原子级手部动作序列，遵循机器人VLA数据的粒度要求\n\n**指令标注**：从每个片段均匀采样8帧，在帧上叠加手部轨迹投影，使用GPT-4.1为每个片段生成动作的描述性语言指令\n## VLA模型设计\n\n**模型架构**：由PaliGemma-2 VLM骨干网和DiT-Base扩散Transformer动作专家组成，添加可学习的认知token作为条件输入\n\n**动作空间**：在相机坐标系中预测手部动作，包含左右手的手腕平移、旋转和15个关节的欧拉角\n\n**统一预测**：通过动作掩码机制处理单双hand动作的统一预测，使用因果注意机制进行动作去噪\n\n**训练策略**：采用轨迹感知数据增强，随机裁剪和透视变换，同时变换对应的动作序列\n## 机器人微调\n\n将人类手部动作空间映射到机器人手部，通过关节拓扑对应关系进行简单的映射策略，对模型进行微调以适应真实机器人操作\n# 论文使用数据集和训练资源\n## 预训练数据集\n\n**数据来源**：处理了来自Ego4D（77%）、Epic-Kitchen（12%）、EgoExo4D（6%）和Something-Something-V2（5%）的第一人称人类视频\n\n**数据规模**：100万片段，2600万帧，覆盖烹饪、清洁、建筑、维修、手工和绘画等真实生活活动\n\n**数据多样性**：与OpenImages数据集相比具有更高的视觉多样性，包含更丰富的语言指令多样性（名词、动词、形容词）\n## 训练资源\n\n**预训练**：在8个NVIDIA H100 GPU上训练2天，批量大小512，学习率1e-4和1e-5\n\n**微调**：在8个NVIDIA H100 GPU上训练8小时，批量大小256，学习率1e-5\n## 机器人数据\n\n收集了1.2K条遥操作轨迹用于四个任务的微调：一般拾取放置、功能抓取、倒水和清扫\n# 论文使用的评估环境和评估指标\n## 人类手部动作预测评估\n\n**评估环境**：\n- 抓取任务：在47个未见过的环境中使用Azure Kinect捕获RGB-D图像\n- 一般动作任务：在117个未见过的真实生活环境中进行用户研究\n\n**评估指标**：\n- 抓取任务：手-物体最小距离（$d_{\\text{hand-obj}}$）评估动作合理性\n- 一般动作：23名参与者对30个随机场景的top-3动作进行排名打分\n## 真实机器人灵巧操作评估\n\n**评估环境**：\n- Realman机器人配备12-DoF XHand灵巧手和RealSense头部相机\n- 桌面环境设置，使用遥操作系统收集训练数据\n\n**评估任务**：\n- 拾取放置：移动物体到盒子中（3-4个随机干扰物）\n- 功能抓取：在功能位置抓取物体（如手柄）\n- 倒水：抓取瓶子，将内容倒入另一容器，放回桌面\n- 扫帚清扫：从篮子中拿起扫帚，将垃圾扫入簸箕，归位扫帚\n\n**评估指标**：\n- 任务成功率：在可见和不可见物体/背景下的成功率百分比\n- 对比基线：与VPP和π₀等代表性方法的性能比较\n- 缩放行为：不同数据规模下的性能变化趋势",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>清华大学、微软亚洲研究院</p>\n<h1>论文概述</h1>\n\n<p>本文提出了一种将无脚本真实生活人类手部活动视频转换为视觉-语言-动作（VLA）数据格式的新方法，用于预训练机器人操作VLA模型。该方法将人类手部视为灵巧机器人末端执行器，通过开发全自动化的人类活动分析框架，将\"野生\"第一人称人类视频转换为与现有机器人VLA训练数据完全对齐的数据格式，无需任何注释。最终构建了包含100万片段和2600万帧的大规模手部VLA数据集，并设计了灵巧手部VLA模型。</p>\n<h1>论文核心贡献点</h1>\n\n<p>1. <strong>首个VLA预训练方法</strong>：首次将非结构化人类视频转换为与机器人VLA训练数据完全对齐的数据格式，无需人工注释</p>\n\n<p>2. <strong>自动化分析框架</strong>：开发了全自动化的人类活动分析框架，包含三维运动标注、原子动作分割和指令标注三个阶段</p>\n\n<p>3. <strong>大规模数据集构建</strong>：构建了包含100万片段和2600万帧的手部VLA数据集，覆盖广泛的对象、概念、技能和环境变化</p>\n\n<p>4. <strong>VLA模型架构</strong>：设计了由VLM骨干网和扩散动作专家组成的灵巧手部VLA模型，支持统一的单双hand动作预测</p>\n\n<p>5. <strong>零样本泛化能力</strong>：模型在完全未见过的真实世界观察中展现出强大的零样本能力</p>\n\n<p>6. <strong>缩放行为验证</strong>：证明了模型任务性能随预训练数据规模的清晰缩放行为</p>\n<h1>论文方法描述</h1>\n<h2>数据转换框架</h2>\n\n<p><strong>三维运动标注</strong>：采用单目三维相机和手部姿态跟踪方法，利用深度视觉SLAM、深度估计和手部重建技术，估计相机内参和帧级手部姿态（基于6D手腕姿态和完整关节角度）</p>\n\n<p><strong>原子动作分割</strong>：通过检测世界空间中3D手部手腕的速度最小值作为切割点，将长视频分割为原子级手部动作序列，遵循机器人VLA数据的粒度要求</p>\n\n<p><strong>指令标注</strong>：从每个片段均匀采样8帧，在帧上叠加手部轨迹投影，使用GPT-4.1为每个片段生成动作的描述性语言指令</p>\n<h2>VLA模型设计</h2>\n\n<p><strong>模型架构</strong>：由PaliGemma-2 VLM骨干网和DiT-Base扩散Transformer动作专家组成，添加可学习的认知token作为条件输入</p>\n\n<p><strong>动作空间</strong>：在相机坐标系中预测手部动作，包含左右手的手腕平移、旋转和15个关节的欧拉角</p>\n\n<p><strong>统一预测</strong>：通过动作掩码机制处理单双hand动作的统一预测，使用因果注意机制进行动作去噪</p>\n\n<p><strong>训练策略</strong>：采用轨迹感知数据增强，随机裁剪和透视变换，同时变换对应的动作序列</p>\n<h2>机器人微调</h2>\n\n<p>将人类手部动作空间映射到机器人手部，通过关节拓扑对应关系进行简单的映射策略，对模型进行微调以适应真实机器人操作</p>\n<h1>论文使用数据集和训练资源</h1>\n<h2>预训练数据集</h2>\n\n<p><strong>数据来源</strong>：处理了来自Ego4D（77%）、Epic-Kitchen（12%）、EgoExo4D（6%）和Something-Something-V2（5%）的第一人称人类视频</p>\n\n<p><strong>数据规模</strong>：100万片段，2600万帧，覆盖烹饪、清洁、建筑、维修、手工和绘画等真实生活活动</p>\n\n<p><strong>数据多样性</strong>：与OpenImages数据集相比具有更高的视觉多样性，包含更丰富的语言指令多样性（名词、动词、形容词）</p>\n<h2>训练资源</h2>\n\n<p><strong>预训练</strong>：在8个NVIDIA H100 GPU上训练2天，批量大小512，学习率1e-4和1e-5</p>\n\n<p><strong>微调</strong>：在8个NVIDIA H100 GPU上训练8小时，批量大小256，学习率1e-5</p>\n<h2>机器人数据</h2>\n\n<p>收集了1.2K条遥操作轨迹用于四个任务的微调：一般拾取放置、功能抓取、倒水和清扫</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<h2>人类手部动作预测评估</h2>\n\n<p><strong>评估环境</strong>：</p>\n<ul><li>抓取任务：在47个未见过的环境中使用Azure Kinect捕获RGB-D图像</li><li>一般动作任务：在117个未见过的真实生活环境中进行用户研究</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li>抓取任务：手-物体最小距离（$d_{\\text{hand-obj}}$）评估动作合理性</li><li>一般动作：23名参与者对30个随机场景的top-3动作进行排名打分</li></ul>\n<h2>真实机器人灵巧操作评估</h2>\n\n<p><strong>评估环境</strong>：</p>\n<ul><li>Realman机器人配备12-DoF XHand灵巧手和RealSense头部相机</li><li>桌面环境设置，使用遥操作系统收集训练数据</li></ul>\n\n<p><strong>评估任务</strong>：</p>\n<ul><li>拾取放置：移动物体到盒子中（3-4个随机干扰物）</li><li>功能抓取：在功能位置抓取物体（如手柄）</li><li>倒水：抓取瓶子，将内容倒入另一容器，放回桌面</li><li>扫帚清扫：从篮子中拿起扫帚，将垃圾扫入簸箕，归位扫帚</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li>任务成功率：在可见和不可见物体/背景下的成功率百分比</li><li>对比基线：与VPP和π₀等代表性方法的性能比较</li><li>缩放行为：不同数据规模下的性能变化趋势</li></ul>"
  },
  {
    "date": "2025-10-23",
    "title": "SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing",
    "link": "http://arxiv.org/abs/2510.20965",
    "summary_markdown": "论文研究单位\n约翰霍普金斯大学、NVIDIA、斯坦福大学、多伦多大学\n\n论文概述\n论文介绍了SutureBot，一个用于在da Vinci研究工具包(dVRK)上实现自主端到端缝合的精密框架和基准。该任务被分解为针头拾取、组织穿透和打结三个子任务。为了解决当前数据集规模小、缺乏标准化基准的问题，作者发布了包含1,890次高质量演示的SutureBot数据集。此外，论文提出了一种目标条件化的模仿学习框架，通过显式优化针头插入点精度，将目标准确性提高了59%-74%。作者还评估了包括π₀、GR00T N1和OpenVLA-OFT在内的多种最先进的视觉-语言-动作(VLA)模型在该基准上的表现，为未来的研究建立了基线。\n\n论文核心贡献点\n1. 提出了一个用于评估在手术环境中进行长时程灵巧操作策略的新基准，涵盖完整的自主缝合任务。\n2. 发布了目前最大的公开真实世界缝合数据集，包含1,890次dVRK演示，以支持可复现的研究。\n3. 提出了一个目标条件化的模仿学习框架，使学习到的策略能够实现精确的定点插入。\n4. 对多种最先进的VLA模型在SutureBot基准上进行了全面评估，确立了未来研究的性能基线。\n\n论文方法描述\n1. 分层框架：采用分层架构，包括一个基于Swin Transformer的高级策略和一个低级策略。高级策略负责根据视觉观测选择当前子任务并生成相应的语言指令。低级策略接收语言指令、实时图像和目标条件，并输出连续的机器人控制指令。\n2. 低级策略比较：比较了四种最先进的视觉-语言-动作模型作为低级策略，包括π₀、GR00T N1、OpenVLA-OFT和一个多任务Action Chunking Transformer (ACT)。\n3. 目标表示：探索了三种目标条件表示方法来引导针头放置：(1) 点标签，在内窥镜图像上用蓝/绿像素标记插入/退出点；(2) 二元掩码，一个三通道图像，分别代表插入和退出区域；(3) 距离图，一个编码了到目标点的像素级偏移向量和热力图的三通道图像。实验发现点标签方法在精度上表现最佳。\n\n论文使用数据集和训练资源\n1. 数据集：使用dVRK系统和软组织缝合垫进行数据采集。数据集包含1,890次演示，涵盖针头拾取(628次)、针头投掷(310次)和打结(952次)三个任务，并包含454次从失败状态开始的恢复演示。数据包括同步的立体内窥镜图像、腕部摄像头图像(30Hz)和机器人运动学数据。该数据集已在Hugging Face上公开。\n2. 训练资源：所有模型训练均在配备8块NVIDIA A100 80GB GPU的NVIDIA DGX A100系统上进行。\n\n论文使用的评估环境和评估指标\n1. 评估环境：评估在物理世界的da Vinci研究工具包(dVRK)上进行。实验采用固定的机器人配置和缝合垫位置，但改变针头的初始位置。策略的在线推理和评估在配备双NVIDIA RTX 4090 GPU的工作站上完成。\n2. 评估指标：\n - 任务成功率：分别评估针头拾取、针头投掷、拉线和打结四个子任务的成功率，并设定了时间限制。\n - 插入/退出误差：使用紫外线笔标记目标点，任务完成后在紫外线灯下用ImageJ测量实际缝合点与目标点之间的欧几里得距离。\n - 过程时间：记录完成整个缝合过程所需的总时间。",
    "summary_html": "<p>论文研究单位</p>\n<p>约翰霍普金斯大学、NVIDIA、斯坦福大学、多伦多大学</p>\n\n<p>论文概述</p>\n<p>论文介绍了SutureBot，一个用于在da Vinci研究工具包(dVRK)上实现自主端到端缝合的精密框架和基准。该任务被分解为针头拾取、组织穿透和打结三个子任务。为了解决当前数据集规模小、缺乏标准化基准的问题，作者发布了包含1,890次高质量演示的SutureBot数据集。此外，论文提出了一种目标条件化的模仿学习框架，通过显式优化针头插入点精度，将目标准确性提高了59%-74%。作者还评估了包括π₀、GR00T N1和OpenVLA-OFT在内的多种最先进的视觉-语言-动作(VLA)模型在该基准上的表现，为未来的研究建立了基线。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一个用于评估在手术环境中进行长时程灵巧操作策略的新基准，涵盖完整的自主缝合任务。</li><li>发布了目前最大的公开真实世界缝合数据集，包含1,890次dVRK演示，以支持可复现的研究。</li><li>提出了一个目标条件化的模仿学习框架，使学习到的策略能够实现精确的定点插入。</li><li>对多种最先进的VLA模型在SutureBot基准上进行了全面评估，确立了未来研究的性能基线。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>分层框架：采用分层架构，包括一个基于Swin Transformer的高级策略和一个低级策略。高级策略负责根据视觉观测选择当前子任务并生成相应的语言指令。低级策略接收语言指令、实时图像和目标条件，并输出连续的机器人控制指令。</li><li>低级策略比较：比较了四种最先进的视觉-语言-动作模型作为低级策略，包括π₀、GR00T N1、OpenVLA-OFT和一个多任务Action Chunking Transformer (ACT)。</li><li>目标表示：探索了三种目标条件表示方法来引导针头放置：(1) 点标签，在内窥镜图像上用蓝/绿像素标记插入/退出点；(2) 二元掩码，一个三通道图像，分别代表插入和退出区域；(3) 距离图，一个编码了到目标点的像素级偏移向量和热力图的三通道图像。实验发现点标签方法在精度上表现最佳。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：使用dVRK系统和软组织缝合垫进行数据采集。数据集包含1,890次演示，涵盖针头拾取(628次)、针头投掷(310次)和打结(952次)三个任务，并包含454次从失败状态开始的恢复演示。数据包括同步的立体内窥镜图像、腕部摄像头图像(30Hz)和机器人运动学数据。该数据集已在Hugging Face上公开。</li><li>训练资源：所有模型训练均在配备8块NVIDIA A100 80GB GPU的NVIDIA DGX A100系统上进行。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：评估在物理世界的da Vinci研究工具包(dVRK)上进行。实验采用固定的机器人配置和缝合垫位置，但改变针头的初始位置。策略的在线推理和评估在配备双NVIDIA RTX 4090 GPU的工作站上完成。</li><li>评估指标：</li></ol>\n<p> - 任务成功率：分别评估针头拾取、针头投掷、拉线和打结四个子任务的成功率，并设定了时间限制。</p>\n<p> - 插入/退出误差：使用紫外线笔标记目标点，任务完成后在紫外线灯下用ImageJ测量实际缝合点与目标点之间的欧几里得距离。</p>\n<p> - 过程时间：记录完成整个缝合过程所需的总时间。</p>"
  },
  {
    "date": "2025-10-23",
    "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation",
    "link": "http://arxiv.org/abs/2510.20818",
    "summary_markdown": "论文研究单位\nUniversity of Washington, DEVCOM ARL\n论文概述\nVAMOS是一个分层的视觉-语言-行动（VLA）模型，用于实现能力调制和可转向的导航。它将语义规划与具体机器人的物理约束和能力分离，通过一个通用规划器从多样化的开放世界数据中学习，同时一个专业的可供性模型在安全、低成本的仿真中学习机器人的物理约束和能力。该模型设计了一个接口，让高级规划器直接在图像空间中提出候选路径，然后由可供性模型评估和重新排序。\n论文核心贡献点\n提出了一种分层的VLA模型，结合了高级VLM规划器和低级可供性模型，实现跨具身导航。\n通过预测2D路径作为接口，统一了不同数据源和动作空间，支持大规模异构数据训练。\n实现了自然语言可转向性，允许在测试时通过文本指令调整导航行为。\n验证了跨具身导航能力，在腿式和轮式机器人上复用同一高级规划器，仅更换低级可供性模型。\n实验表明，与模块化基线相比，VAMOS在复杂室内外环境中实现了更高的导航成功率，并且可供性模型显著提高了单机器人导航的可靠性，成功率提升3倍。\n论文方法描述\n系统架构为分层设计：高级VLM规划器输入图像和文本编码的目标坐标，输出图像空间中的候选路径。\n训练时，使用来自SCAND、TartanDrive 2、CODa和Spot数据集的混合数据，总计29.8小时，通过投影3D位姿到2D图像路径，并利用GPT-5-mini生成文本偏好标注以增强可转向性。\n低级可供性函数在仿真中训练，输入高程图、位置和方向，输出路径的可穿越概率。\n部署时，VLM预测的2D路径投影到3D世界坐标，由可供性模型计算累计可穿越得分并重新排序，选择最优路径由低级控制器执行。\n论文使用数据集和训练资源\n高级规划器使用SCAND、TartanDrive 2、CODa和Spot数据集，总时长29.8小时，混合比例为SCAND 50%、CODa 25%、TartanDrive 2 100%、Spot 100%，并添加了人工草图数据用于few-shot适应。\n低级可供性模型在Isaac Lab仿真环境中训练，使用程序生成的地形。\n计算资源包括Nvidia L40节点的8个GPU，训练约5小时，支持在消费级Nvidia RTX 4090上进行LoRA微调。\n论文使用的评估环境和评估指标\n评估环境：真实世界的室内外场景，包括走廊（20米）、中庭（20米）、实验室（5米）、校园（40米）、森林（20米）和下坡斜坡（15米），覆盖复杂地形、光照和植被。\n评估指标：成功率（SR，5次试验中成功到达目标的百分比）、平均干预次数（NI，成功运行中的0-2次）和超时次数（T，0-5次）。\n跨具身导航评估：在包含楼梯和斜坡的设置中测试Spot和UW Hound机器人，比较有无可供性调制的性能。\n可转向性评估：通过自然语言指令（如“向左”或“走右侧”）调整路径，使用ChatGPT 5作为VLM裁判，量化偏好对齐率。</think>\n论文研究单位\nUniversity of Washington, DEVCOM ARL\n论文概述\nVAMOS是一个分层的视觉-语言-行动（VLA）模型，用于实现能力调制和可转向的导航。它将语义规划与具体机器人的物理约束和能力分离，通过一个通用规划器从多样化的开放世界数据中学习，同时一个专业的可供性模型在安全、低成本的仿真中学习机器人的物理约束和能力。该模型设计了一个接口，让高级规划器直接在图像空间中提出候选路径，然后由可供性模型评估和重新排序。\n论文核心贡献点\n提出了一种分层的VLA模型，结合了高级VLM规划器和低级可供性模型，实现跨具身导航。\n通过预测2D路径作为接口，统一了不同数据源和动作空间，支持大规模异构数据训练。\n实现了自然语言可转向性，允许在测试时通过文本指令调整导航行为。\n验证了跨具身导航能力，在腿式和轮式机器人上复用同一高级规划器，仅更换低级可供性模型。\n实验表明，与模块化基线相比，VAMOS在复杂室内外环境中实现了更高的导航成功率，并且可供性模型显著提高了单机器人导航的可靠性，成功率提升3倍。\n论文方法描述\n系统架构为分层设计：高级VLM规划器输入图像和文本编码的目标坐标，输出图像空间中的候选路径。\n训练时，使用来自SCAND、TartanDrive 2、CODa和Spot数据集的混合数据，总计29.8小时，通过投影3D位姿到2D图像路径，并利用GPT-5-mini生成文本偏好标注以增强可转向性。\n低级可供性函数在仿真中训练，输入高程图、位置和方向，输出路径的可穿越概率。\n部署时，VLM预测的2D路径投影到3D世界坐标，由可供性模型计算累计可穿越得分并重新排序，选择最优路径由低级控制器执行。\n论文使用数据集和训练资源\n高级规划器使用SCAND、TartanDrive 2、CODa和Spot数据集，总时长29.8小时，混合比例为SCAND 50%、CODa 25%、TartanDrive 2 100%、Spot 100%，并添加了人工草图数据用于few-shot适应。\n低级可供性模型在Isaac Lab仿真环境中训练，使用程序生成的地形。\n计算资源包括Nvidia L40节点的8个GPU，训练约5小时，支持在消费级Nvidia RTX 4090上进行LoRA微调。\n论文使用的评估环境和评估指标\n评估环境：真实世界的室内外场景，包括走廊（20米）、中庭（20米）、实验室（5米）、校园（40米）、森林（20米）和下坡斜坡（15米），覆盖复杂地形、光照和植被。\n评估指标：成功率（SR，5次试验中成功到达目标的百分比）、平均干预次数（NI，成功运行中的0-2次）和超时次数（T，0-5次）。\n跨具身导航评估：在包含楼梯和斜坡的设置中测试Spot和UW Hound机器人，比较有无可供性调制的性能。\n可转向性评估：通过自然语言指令（如“向左”或“走右侧”）调整路径，使用ChatGPT 5作为VLM裁判，量化偏好对齐率。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Washington, DEVCOM ARL</p>\n<p>论文概述</p>\n<p>VAMOS是一个分层的视觉-语言-行动（VLA）模型，用于实现能力调制和可转向的导航。它将语义规划与具体机器人的物理约束和能力分离，通过一个通用规划器从多样化的开放世界数据中学习，同时一个专业的可供性模型在安全、低成本的仿真中学习机器人的物理约束和能力。该模型设计了一个接口，让高级规划器直接在图像空间中提出候选路径，然后由可供性模型评估和重新排序。</p>\n<p>论文核心贡献点</p>\n<p>提出了一种分层的VLA模型，结合了高级VLM规划器和低级可供性模型，实现跨具身导航。</p>\n<p>通过预测2D路径作为接口，统一了不同数据源和动作空间，支持大规模异构数据训练。</p>\n<p>实现了自然语言可转向性，允许在测试时通过文本指令调整导航行为。</p>\n<p>验证了跨具身导航能力，在腿式和轮式机器人上复用同一高级规划器，仅更换低级可供性模型。</p>\n<p>实验表明，与模块化基线相比，VAMOS在复杂室内外环境中实现了更高的导航成功率，并且可供性模型显著提高了单机器人导航的可靠性，成功率提升3倍。</p>\n<p>论文方法描述</p>\n<p>系统架构为分层设计：高级VLM规划器输入图像和文本编码的目标坐标，输出图像空间中的候选路径。</p>\n<p>训练时，使用来自SCAND、TartanDrive 2、CODa和Spot数据集的混合数据，总计29.8小时，通过投影3D位姿到2D图像路径，并利用GPT-5-mini生成文本偏好标注以增强可转向性。</p>\n<p>低级可供性函数在仿真中训练，输入高程图、位置和方向，输出路径的可穿越概率。</p>\n<p>部署时，VLM预测的2D路径投影到3D世界坐标，由可供性模型计算累计可穿越得分并重新排序，选择最优路径由低级控制器执行。</p>\n<p>论文使用数据集和训练资源</p>\n<p>高级规划器使用SCAND、TartanDrive 2、CODa和Spot数据集，总时长29.8小时，混合比例为SCAND 50%、CODa 25%、TartanDrive 2 100%、Spot 100%，并添加了人工草图数据用于few-shot适应。</p>\n<p>低级可供性模型在Isaac Lab仿真环境中训练，使用程序生成的地形。</p>\n<p>计算资源包括Nvidia L40节点的8个GPU，训练约5小时，支持在消费级Nvidia RTX 4090上进行LoRA微调。</p>\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：真实世界的室内外场景，包括走廊（20米）、中庭（20米）、实验室（5米）、校园（40米）、森林（20米）和下坡斜坡（15米），覆盖复杂地形、光照和植被。</p>\n<p>评估指标：成功率（SR，5次试验中成功到达目标的百分比）、平均干预次数（NI，成功运行中的0-2次）和超时次数（T，0-5次）。</p>\n<p>跨具身导航评估：在包含楼梯和斜坡的设置中测试Spot和UW Hound机器人，比较有无可供性调制的性能。</p>\n<p>可转向性评估：通过自然语言指令（如“向左”或“走右侧”）调整路径，使用ChatGPT 5作为VLM裁判，量化偏好对齐率。</think></p>\n<p>论文研究单位</p>\n<p>University of Washington, DEVCOM ARL</p>\n<p>论文概述</p>\n<p>VAMOS是一个分层的视觉-语言-行动（VLA）模型，用于实现能力调制和可转向的导航。它将语义规划与具体机器人的物理约束和能力分离，通过一个通用规划器从多样化的开放世界数据中学习，同时一个专业的可供性模型在安全、低成本的仿真中学习机器人的物理约束和能力。该模型设计了一个接口，让高级规划器直接在图像空间中提出候选路径，然后由可供性模型评估和重新排序。</p>\n<p>论文核心贡献点</p>\n<p>提出了一种分层的VLA模型，结合了高级VLM规划器和低级可供性模型，实现跨具身导航。</p>\n<p>通过预测2D路径作为接口，统一了不同数据源和动作空间，支持大规模异构数据训练。</p>\n<p>实现了自然语言可转向性，允许在测试时通过文本指令调整导航行为。</p>\n<p>验证了跨具身导航能力，在腿式和轮式机器人上复用同一高级规划器，仅更换低级可供性模型。</p>\n<p>实验表明，与模块化基线相比，VAMOS在复杂室内外环境中实现了更高的导航成功率，并且可供性模型显著提高了单机器人导航的可靠性，成功率提升3倍。</p>\n<p>论文方法描述</p>\n<p>系统架构为分层设计：高级VLM规划器输入图像和文本编码的目标坐标，输出图像空间中的候选路径。</p>\n<p>训练时，使用来自SCAND、TartanDrive 2、CODa和Spot数据集的混合数据，总计29.8小时，通过投影3D位姿到2D图像路径，并利用GPT-5-mini生成文本偏好标注以增强可转向性。</p>\n<p>低级可供性函数在仿真中训练，输入高程图、位置和方向，输出路径的可穿越概率。</p>\n<p>部署时，VLM预测的2D路径投影到3D世界坐标，由可供性模型计算累计可穿越得分并重新排序，选择最优路径由低级控制器执行。</p>\n<p>论文使用数据集和训练资源</p>\n<p>高级规划器使用SCAND、TartanDrive 2、CODa和Spot数据集，总时长29.8小时，混合比例为SCAND 50%、CODa 25%、TartanDrive 2 100%、Spot 100%，并添加了人工草图数据用于few-shot适应。</p>\n<p>低级可供性模型在Isaac Lab仿真环境中训练，使用程序生成的地形。</p>\n<p>计算资源包括Nvidia L40节点的8个GPU，训练约5小时，支持在消费级Nvidia RTX 4090上进行LoRA微调。</p>\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：真实世界的室内外场景，包括走廊（20米）、中庭（20米）、实验室（5米）、校园（40米）、森林（20米）和下坡斜坡（15米），覆盖复杂地形、光照和植被。</p>\n<p>评估指标：成功率（SR，5次试验中成功到达目标的百分比）、平均干预次数（NI，成功运行中的0-2次）和超时次数（T，0-5次）。</p>\n<p>跨具身导航评估：在包含楼梯和斜坡的设置中测试Spot和UW Hound机器人，比较有无可供性调制的性能。</p>\n<p>可转向性评估：通过自然语言指令（如“向左”或“走右侧”）调整路径，使用ChatGPT 5作为VLM裁判，量化偏好对齐率。</p>"
  },
  {
    "date": "2025-10-23",
    "title": "MemER: Scaling Up Memory for Robot Control via Experience Retrieval",
    "link": "http://arxiv.org/abs/2510.20328",
    "summary_markdown": "## 论文研究单位\n\n斯坦福大学（Stanford University）\n## 论文概述\n\n机器人策略通常缺乏长期记忆能力，无法像人类一样利用历史信息完成复杂任务。针对这一问题，本文提出了MemER（Memory via Experience Retrieval）框架，采用分层策略设计，使机器人能够选择性地记忆任务相关的关键帧，从而实现分钟级别的长期记忆。该方法在三个真实世界的长时序机器人操作任务上进行了验证。\n## 论文核心贡献点\n\n1. **分层记忆架构**：提出高层策略负责选择和跟踪相关关键帧，低层策略执行具体动作的层次化设计\n2. **关键帧选择机制**：开发了一种基于一维单链接聚类的算法，能从历史观察中自动提取任务相关信息\n3. **高效记忆存储**：设计了紧凑的视觉记忆表示方法，避免了传统长上下文方法的计算开销和协变量偏移问题\n4. **少样本适应**：仅需50个远程操作演示即可将预训练的视觉-语言模型适配到机器人任务\n## 论文方法描述\n\n**高层策略（π_h）**：\n- 基于Qwen2.5-VL-7B-Instruct微调\n- 输入：最近N帧、任务指令、选定关键帧\n- 输出：当前子任务和候选关键帧\n- 采用一维单链接聚类（合并距离d=5帧）构建视觉记忆\n\n**低层策略（π_l）**：\n- 基于π_0.5微调\n- 输入：当前图像、关节状态、子任务指令\n- 输出：机器人动作块\n\n**关键帧选择算法**：\n1. 收集所有候选关键帧的时间戳\n2. 对时间戳进行排序\n3. 使用单链接聚类合并相近帧\n4. 选择每簇的中位数作为代表帧\n\n**模型合并技术**：使用权重插值θ=(1-α)·θ_pre + α·θ_ft（α=0.8）保持预训练模型的鲁棒性\n## 论文使用数据集和训练资源\n\n**训练数据**：\n- 50个长时序轨迹演示，每个任务10-15个干预演示\n- 数据格式：(I_t, q_t, l_t, l'_t, a_t)\n\n**模型训练**：\n- 高层策略：4500梯度步，96 H200 GPU小时\n- 低层策略：18000训练步，48 H200 GPU小时\n- 冻结视觉编码器和投影层，仅微调LLM骨干网络\n\n**硬件配置**：\n- Franka机械臂，平行爪夹持器\n- 双摄像头：第三人称ZED摄像头和手腕安装的miniZED摄像头\n- 图像分辨率320×180，15Hz采样率\n## 论文使用的评估环境和评估指标\n\n**评估任务**：\n1. **物品搜索任务**：在三个不透明垃圾箱中搜索3-5个物品，评估记忆优化路径\n2. **计数舀取任务**：向两个碗中放入指定数量的两种食材，评估计数准确性\n3. **除尘与复位任务**：从双层架子移除物品，清洗架子并复位物品，评估空间记忆\n\n**评估指标**：\n- 物品搜索：成功检索次数（↑）、使用最优路径次数（↑）\n- 计数舀取：错误舀取数量（↓）\n- 除尘与复位：底层/顶层除尘成功（↑）、底层/顶层物品复位成功（↑）\n\n**对比方法**：\n- 无历史：仅当前帧\n- 短历史：N=8帧\n- 长历史：N=32帧\n- 人类高层：人工提供正确子任务\n\n**主要结果**：\n- MemER在所有任务上>90%成功率\n- 明显优于无历史和短历史基线\n- 长历史方法平均性能比MemER差34%\n- 与人类高层策略性能相当\n- API-VLMs因延迟问题表现不佳",
    "summary_html": "<h2>论文研究单位</h2>\n\n<p>斯坦福大学（Stanford University）</p>\n<h2>论文概述</h2>\n\n<p>机器人策略通常缺乏长期记忆能力，无法像人类一样利用历史信息完成复杂任务。针对这一问题，本文提出了MemER（Memory via Experience Retrieval）框架，采用分层策略设计，使机器人能够选择性地记忆任务相关的关键帧，从而实现分钟级别的长期记忆。该方法在三个真实世界的长时序机器人操作任务上进行了验证。</p>\n<h2>论文核心贡献点</h2>\n\n<ol><li><strong>分层记忆架构</strong>：提出高层策略负责选择和跟踪相关关键帧，低层策略执行具体动作的层次化设计</li><li><strong>关键帧选择机制</strong>：开发了一种基于一维单链接聚类的算法，能从历史观察中自动提取任务相关信息</li><li><strong>高效记忆存储</strong>：设计了紧凑的视觉记忆表示方法，避免了传统长上下文方法的计算开销和协变量偏移问题</li><li><strong>少样本适应</strong>：仅需50个远程操作演示即可将预训练的视觉-语言模型适配到机器人任务</li></ol>\n<h2>论文方法描述</h2>\n\n<p><strong>高层策略（π_h）</strong>：</p>\n<ul><li>基于Qwen2.5-VL-7B-Instruct微调</li><li>输入：最近N帧、任务指令、选定关键帧</li><li>输出：当前子任务和候选关键帧</li><li>采用一维单链接聚类（合并距离d=5帧）构建视觉记忆</li></ul>\n\n<p><strong>低层策略（π_l）</strong>：</p>\n<ul><li>基于π_0.5微调</li><li>输入：当前图像、关节状态、子任务指令</li><li>输出：机器人动作块</li></ul>\n\n<p><strong>关键帧选择算法</strong>：</p>\n<ol><li>收集所有候选关键帧的时间戳</li><li>对时间戳进行排序</li><li>使用单链接聚类合并相近帧</li><li>选择每簇的中位数作为代表帧</li></ol>\n\n<p><strong>模型合并技术</strong>：使用权重插值θ=(1-α)·θ_pre + α·θ_ft（α=0.8）保持预训练模型的鲁棒性</p>\n<h2>论文使用数据集和训练资源</h2>\n\n<p><strong>训练数据</strong>：</p>\n<ul><li>50个长时序轨迹演示，每个任务10-15个干预演示</li><li>数据格式：(I_t, q_t, l_t, l'_t, a_t)</li></ul>\n\n<p><strong>模型训练</strong>：</p>\n<ul><li>高层策略：4500梯度步，96 H200 GPU小时</li><li>低层策略：18000训练步，48 H200 GPU小时</li><li>冻结视觉编码器和投影层，仅微调LLM骨干网络</li></ul>\n\n<p><strong>硬件配置</strong>：</p>\n<ul><li>Franka机械臂，平行爪夹持器</li><li>双摄像头：第三人称ZED摄像头和手腕安装的miniZED摄像头</li><li>图像分辨率320×180，15Hz采样率</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n\n<p><strong>评估任务</strong>：</p>\n<ol><li><strong>物品搜索任务</strong>：在三个不透明垃圾箱中搜索3-5个物品，评估记忆优化路径</li><li><strong>计数舀取任务</strong>：向两个碗中放入指定数量的两种食材，评估计数准确性</li><li><strong>除尘与复位任务</strong>：从双层架子移除物品，清洗架子并复位物品，评估空间记忆</li></ol>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li>物品搜索：成功检索次数（↑）、使用最优路径次数（↑）</li><li>计数舀取：错误舀取数量（↓）</li><li>除尘与复位：底层/顶层除尘成功（↑）、底层/顶层物品复位成功（↑）</li></ul>\n\n<p><strong>对比方法</strong>：</p>\n<ul><li>无历史：仅当前帧</li><li>短历史：N=8帧</li><li>长历史：N=32帧</li><li>人类高层：人工提供正确子任务</li></ul>\n\n<p><strong>主要结果</strong>：</p>\n<ul><li>MemER在所有任务上>90%成功率</li><li>明显优于无历史和短历史基线</li><li>长历史方法平均性能比MemER差34%</li><li>与人类高层策略性能相当</li><li>API-VLMs因延迟问题表现不佳</li></ul>"
  },
  {
    "date": "2025-10-22",
    "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.19752",
    "summary_markdown": "# 论文研究单位\nUC Berkeley和Physical Intelligence\n# 论文概述\n论文提出了LITEN（Learning from Inference-Time Execution），一种针对视觉-语言-动作模型（VLA）的推理时学习框架。该方法通过将VLA低层策略与高层视觉语言模型（VLM）连接，使高层VLM能够在上下文中利用过往经验，学习机器人的可达性（affordances）和能力。LITEN通过迭代的推理和评估阶段，使机器人能够从实际执行经验中学习，不断改进复杂任务的规划能力，无需额外的模型训练。\n# 论文核心贡献点\n- 提出LITEN框架，实现了VLA模型的推理时学习能力\n- 设计了两阶段迭代方法：推理阶段生成和执行计划，评估阶段分析执行结果并提取经验\n- 展示了如何从非结构化的机器人轨迹中提取有意义的反馈\n- 在复杂的多阶段操作任务上验证了方法的有效性\n- 证明了方法可以在不使用额外训练数据的情况下显著提升任务成功率\n# 论文方法描述\nLITEN采用双阶段迭代循环：\n1. **推理阶段**：高层VLM接收任务指令和初始环境观察，生成子任务序列和执行计划。系统提供VLA训练指令风格示例和可操作对象列表来指导计划生成。\n2. **评估阶段**：系统收集每个子任务的执行轨迹，使用结构化评估程序通过VLM法官分析执行结果：\n - 判断子任务是否成功\n - 描述实际执行情况（失败时）\n - 分析失败原因并提出改进建议\n3. **经验利用**：将评估结果以层次结构存储，并在下一轮推理中作为上下文提供，帮助VLMreasoner生成更优计划。\n\n方法使用GPT-5-mini作为高层VLM，π₀.₅-DROID作为低层VLA策略。\n# 论文使用数据集和训练资源\n- **数据集**：使用DROID数据集对π₀.₅-DROID进行微调，并收集了450个演示数据（每个任务150个）\n- **硬件环境**：标准DROID机器人设置，包括7-DoF Franka Emika Panda机械臂、2F-85 Robotiq抓手、ZED 2.0相机和ZED mini腕部相机\n- **计算资源**：训练使用4个NVIDIA A100 GPU，批大小128，训练2500步\n- **技术栈**：部署在5Hz操作频率，VLA产生长度为8的关节速度动作块\n# 论文使用的评估环境和评估指标\n- **评估环境**：桌面操作环境，包含多种物体配置\n- **任务类型**：三个复杂多阶段任务：\n - Stacking：堆叠三个物体\n - Emptying Bowls：将两个碗中的内容物转移到其他碗\n - Moving Off Table：将桌面物体移动到其他物体上，仅保留三个与桌面直接接触\n- **评估指标**：\n - 整体任务成功率（不计算部分成功）\n - 五次迭代中的性能改善趋势\n - 与基线方法的对比（No-Feedback、Positive-ICL、Reflexion adaptation）\n- **实验设计**：每个方法在10个不同初始配置下运行5次迭代，评估经验累积的效果\n- **消融研究**：移除评估阶段的不同组件（失败推理、结果分析等）来验证各组件的重要性",
    "summary_html": "<h1>论文研究单位</h1>\n<p>UC Berkeley和Physical Intelligence</p>\n<h1>论文概述</h1>\n<p>论文提出了LITEN（Learning from Inference-Time Execution），一种针对视觉-语言-动作模型（VLA）的推理时学习框架。该方法通过将VLA低层策略与高层视觉语言模型（VLM）连接，使高层VLM能够在上下文中利用过往经验，学习机器人的可达性（affordances）和能力。LITEN通过迭代的推理和评估阶段，使机器人能够从实际执行经验中学习，不断改进复杂任务的规划能力，无需额外的模型训练。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出LITEN框架，实现了VLA模型的推理时学习能力</li><li>设计了两阶段迭代方法：推理阶段生成和执行计划，评估阶段分析执行结果并提取经验</li><li>展示了如何从非结构化的机器人轨迹中提取有意义的反馈</li><li>在复杂的多阶段操作任务上验证了方法的有效性</li><li>证明了方法可以在不使用额外训练数据的情况下显著提升任务成功率</li></ul>\n<h1>论文方法描述</h1>\n<p>LITEN采用双阶段迭代循环：</p>\n<ol><li><strong>推理阶段</strong>：高层VLM接收任务指令和初始环境观察，生成子任务序列和执行计划。系统提供VLA训练指令风格示例和可操作对象列表来指导计划生成。</li><li><strong>评估阶段</strong>：系统收集每个子任务的执行轨迹，使用结构化评估程序通过VLM法官分析执行结果：</li></ol>\n<p> - 判断子任务是否成功</p>\n<p> - 描述实际执行情况（失败时）</p>\n<p> - 分析失败原因并提出改进建议</p>\n<p>3. <strong>经验利用</strong>：将评估结果以层次结构存储，并在下一轮推理中作为上下文提供，帮助VLMreasoner生成更优计划。</p>\n\n<p>方法使用GPT-5-mini作为高层VLM，π₀.₅-DROID作为低层VLA策略。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>数据集</strong>：使用DROID数据集对π₀.₅-DROID进行微调，并收集了450个演示数据（每个任务150个）</li><li><strong>硬件环境</strong>：标准DROID机器人设置，包括7-DoF Franka Emika Panda机械臂、2F-85 Robotiq抓手、ZED 2.0相机和ZED mini腕部相机</li><li><strong>计算资源</strong>：训练使用4个NVIDIA A100 GPU，批大小128，训练2500步</li><li><strong>技术栈</strong>：部署在5Hz操作频率，VLA产生长度为8的关节速度动作块</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>评估环境</strong>：桌面操作环境，包含多种物体配置</li><li><strong>任务类型</strong>：三个复杂多阶段任务：</li></ul>\n<p> - Stacking：堆叠三个物体</p>\n<p> - Emptying Bowls：将两个碗中的内容物转移到其他碗</p>\n<p> - Moving Off Table：将桌面物体移动到其他物体上，仅保留三个与桌面直接接触</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 整体任务成功率（不计算部分成功）</p>\n<p> - 五次迭代中的性能改善趋势</p>\n<p> - 与基线方法的对比（No-Feedback、Positive-ICL、Reflexion adaptation）</p>\n<ul><li><strong>实验设计</strong>：每个方法在10个不同初始配置下运行5次迭代，评估经验累积的效果</li><li><strong>消融研究</strong>：移除评估阶段的不同组件（失败推理、结果分析等）来验证各组件的重要性</li></ul>"
  },
  {
    "date": "2025-10-22",
    "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2510.19430",
    "summary_markdown": "# 论文总结\n## 论文研究单位\nGigaAI。论文页面：https://GigaBrain0.github.io\n## 论文概述\nGigaBrain-0是一类由世界模型驱动的视觉-语言-动作（VLA）基础模型，旨在缓解真实机器人数据收集的高成本与多样性受限问题。通过在大规模、真实且多样的世界模型合成轨迹上训练，GigaBrain-0显著减少对真实机器人数据的依赖，同时提升跨任务、跨外观与跨视角的泛化能力。\n## 论文核心贡献点\n- 以世界模型生成的多样化数据为核心驱动的VLA范式，涵盖视频生成、Real2Real、View Transfer、Sim2Real、人体视频迁移等多源合成轨迹。\n- 架构层面引入RGB-D输入与Embodied Chain-of-Thought（CoT）监督，增强3D几何感知与长时程推理能力，支持操纵轨迹、子目标语言与离散动作符号的联合学习。\n- 混合Transformer架构：基于PaliGemma2的VLM作为感知与语言接口，动作生成采用Diffusion Transformer（DiT）与Flow Matching以预测连续动作块；引入离散动作token加速收敛；通过Knowledge Insulation隔离不同优化流以减轻干扰。\n- 系统性数据管线GigaWorld，提供高效视频生成（单步蒸馏、NATTEN注意力、FP8推理，速度提升>50倍）与质量评估（几何一致性、多视角一致性、文本对齐、物理合理性）。\n- 真实世界广泛验证：在灵巧操作、长时程与移动操纵任务上稳定领先；外观、放置与视角泛化随合成数据混合比α提升显著。\n- 轻量化部署GigaBrain-0-Small，在NVIDIA Jetson AGX Orin上以约402M参数实现低延迟与低显存，保持与π0相当的任务成功率。\n## 论文方法描述\n- 输入与表示：RGB-D输入（B×H×W×4），对SigLIP首层卷积扩展零初始化深度通道；训练期随机丢弃深度以兼容RGB推断。\n- 架构细节：VLM（PaliGemma2）编码多模态输入；动作专家采用DiT进行Flow Matching生成连续动作块；离散动作token作为辅助预测以加速训练；10个可学习的轨迹token通过双向注意与GRU解码输出2D操纵轨迹关键点；子目标语言与离散动作采用自回归生成。\n- 监督与损失：联合优化三项——语言/离散动作的next-token损失、Flow Matching动作块回归损失、轨迹2D坐标回归损失；通过Knowledge Insulation隔离语义与动作学习的相互干扰。\n## 论文使用数据集和训练资源\n- 真实世界数据：融合公开数据集（AgiBotWorld、RoboMind、Open X-Embodiment）与自采数据（Agilex Cobot Magic 199小时 + AgiBot G1 983小时，共1182小时；覆盖工业、商业、办公、居住、实验室等五大类共14个场景）。\n- 世界模型生成数据：GigaWorld提供多管线合成数据，显著提升外观、几何与视角多样性。包括：\n - Real2Real Transfer：在深度与边缘结构先验约束下生成多纹理/光照/材质的视觉变体。\n - View Transfer：基于深度的新视角投影、遮挡修复与物理仿真微调，保持任务语义一致。\n - Sim2Real Transfer：在Isaac Sim中构造多样化场景并合成至逼真外观，增强域泛化。\n - Human Video Transfer：将EgoDex等第一人称人类演示转化为稳定机器人执行序列。\n - 文本条件视频生成+逆动力学（IDM）反推动作序列，扩充操纵轨迹。\n - 多视角一致视频生成与质量评估用于4D一致性与可训练性筛选。\n- 训练策略：全量/部分/未标注数据混合训练；基于手爪状态切换自动切分子目标并用Qwen-VL-2.5生成语言子目标；统一去重与采样策略提升样本效率。\n## 论文使用的评估环境和评估指标\n- 评估平台：AgiBot G1双足/双手机器人平台与PiPER双臂平台。\n- 任务覆盖：\n - 灵巧操纵：Laundry Folding、Paper Towel Preparation。\n - 长时程：Table Bussing、Juice Preparation。\n - 移动操纵：Boxes Moving、Laundry Baskets Moving。\n- 指标：真实世界任务成功率（多次试验平均）；外观/放置/视角泛化实验以合成数据混合比α为变量，评估成功率随α提升曲线。\n- 设备部署：NVIDIA Jetson AGX Orin上对GigaBrain-0-Small与π0进行推理性能与成功率对比（FLOPs、参数量、显存、时延、成功率）。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>GigaAI。论文页面：https://GigaBrain0.github.io</p>\n<h2>论文概述</h2>\n<p>GigaBrain-0是一类由世界模型驱动的视觉-语言-动作（VLA）基础模型，旨在缓解真实机器人数据收集的高成本与多样性受限问题。通过在大规模、真实且多样的世界模型合成轨迹上训练，GigaBrain-0显著减少对真实机器人数据的依赖，同时提升跨任务、跨外观与跨视角的泛化能力。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>以世界模型生成的多样化数据为核心驱动的VLA范式，涵盖视频生成、Real2Real、View Transfer、Sim2Real、人体视频迁移等多源合成轨迹。</li><li>架构层面引入RGB-D输入与Embodied Chain-of-Thought（CoT）监督，增强3D几何感知与长时程推理能力，支持操纵轨迹、子目标语言与离散动作符号的联合学习。</li><li>混合Transformer架构：基于PaliGemma2的VLM作为感知与语言接口，动作生成采用Diffusion Transformer（DiT）与Flow Matching以预测连续动作块；引入离散动作token加速收敛；通过Knowledge Insulation隔离不同优化流以减轻干扰。</li><li>系统性数据管线GigaWorld，提供高效视频生成（单步蒸馏、NATTEN注意力、FP8推理，速度提升>50倍）与质量评估（几何一致性、多视角一致性、文本对齐、物理合理性）。</li><li>真实世界广泛验证：在灵巧操作、长时程与移动操纵任务上稳定领先；外观、放置与视角泛化随合成数据混合比α提升显著。</li><li>轻量化部署GigaBrain-0-Small，在NVIDIA Jetson AGX Orin上以约402M参数实现低延迟与低显存，保持与π0相当的任务成功率。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>输入与表示：RGB-D输入（B×H×W×4），对SigLIP首层卷积扩展零初始化深度通道；训练期随机丢弃深度以兼容RGB推断。</li><li>架构细节：VLM（PaliGemma2）编码多模态输入；动作专家采用DiT进行Flow Matching生成连续动作块；离散动作token作为辅助预测以加速训练；10个可学习的轨迹token通过双向注意与GRU解码输出2D操纵轨迹关键点；子目标语言与离散动作采用自回归生成。</li><li>监督与损失：联合优化三项——语言/离散动作的next-token损失、Flow Matching动作块回归损失、轨迹2D坐标回归损失；通过Knowledge Insulation隔离语义与动作学习的相互干扰。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>真实世界数据：融合公开数据集（AgiBotWorld、RoboMind、Open X-Embodiment）与自采数据（Agilex Cobot Magic 199小时 + AgiBot G1 983小时，共1182小时；覆盖工业、商业、办公、居住、实验室等五大类共14个场景）。</li><li>世界模型生成数据：GigaWorld提供多管线合成数据，显著提升外观、几何与视角多样性。包括：</li></ul>\n<p> - Real2Real Transfer：在深度与边缘结构先验约束下生成多纹理/光照/材质的视觉变体。</p>\n<p> - View Transfer：基于深度的新视角投影、遮挡修复与物理仿真微调，保持任务语义一致。</p>\n<p> - Sim2Real Transfer：在Isaac Sim中构造多样化场景并合成至逼真外观，增强域泛化。</p>\n<p> - Human Video Transfer：将EgoDex等第一人称人类演示转化为稳定机器人执行序列。</p>\n<p> - 文本条件视频生成+逆动力学（IDM）反推动作序列，扩充操纵轨迹。</p>\n<p> - 多视角一致视频生成与质量评估用于4D一致性与可训练性筛选。</p>\n<ul><li>训练策略：全量/部分/未标注数据混合训练；基于手爪状态切换自动切分子目标并用Qwen-VL-2.5生成语言子目标；统一去重与采样策略提升样本效率。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估平台：AgiBot G1双足/双手机器人平台与PiPER双臂平台。</li><li>任务覆盖：</li></ul>\n<p> - 灵巧操纵：Laundry Folding、Paper Towel Preparation。</p>\n<p> - 长时程：Table Bussing、Juice Preparation。</p>\n<p> - 移动操纵：Boxes Moving、Laundry Baskets Moving。</p>\n<ul><li>指标：真实世界任务成功率（多次试验平均）；外观/放置/视角泛化实验以合成数据混合比α为变量，评估成功率随α提升曲线。</li><li>设备部署：NVIDIA Jetson AGX Orin上对GigaBrain-0-Small与π0进行推理性能与成功率对比（FLOPs、参数量、显存、时延、成功率）。</li></ul>"
  },
  {
    "date": "2025-10-22",
    "title": "Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes",
    "link": "http://arxiv.org/abs/2510.19400",
    "summary_markdown": "# 论文研究单位\n清华大学、北京大学、复旦大学、微软研究院亚洲、香港科技大学、浙江大学\n# 论文概述\n论文介绍了MV-RoboBench，一个专门用于评估视觉语言模型在机器人操作场景中多视图空间推理能力的基准测试。该基准包含1,700个手工制作的QA项目，涵盖8个子任务，分为空间理解（交叉视图匹配、距离判断、视角识别、3D空间一致性）和机器人执行（动作规划、步骤执行、轨迹选择、属性识别）两大类别。论文评估了多种VLM模型，并探索了CoT风格的增强方法。\n# 论文核心贡献点\n1. 建立首个整合空间和机器人推理与同步多视图输入的基准测试\n2. 验证了机器人多视图场景仍极具挑战性，最强大的VLM模型远低于人类表现\n3. 发现空间推理和机器人执行之间存在正相关性\n4. 证明强大的单视图空间基准测试性能不能可靠转移到机器人任务或多视图场景\n5. 揭示多视图机器人推理对视角整合、遮挡解决和空间融合提出了根本性不同要求\n# 论文方法描述\n构建了多阶段的基准测试构建管道：数据收集（基于AgiWorld和BridgeV2数据集）、QA生成（使用任务特定模板构建五选一QA对）、人工质量审查（迭代审查修正问题）。探索了三种CoT风格增强方法：文本CoT（通过GPT-4.1生成场景描述）、视觉CoT（使用VGGT进行新颖视图合成）、结构CoT（使用MoGe-2提供深度先验）。评估采用零样本多选题格式，确保跨模型公平比较。\n# 论文使用数据集和训练资源\n使用AgiWorld和BridgeV2数据集作为数据源，构建了包含980个片段的1,708个QA对。评估涵盖盲评（Random、GPT-3.5-turbo、GPT-4-turbo）、专有模型（GPT-4o系列、GPT-4.1系列、Claude-3.5/3.7、Gemini-2.x系列）、专有推理模型（o4-mini、GPT-5系列、Claude-3.7-think、Gemini-2.5-pro）、开源模型（Gemma-3、InternVL3、Qwen2.5-vl系列）和开源MoE模型（Llama-4系列）。\n# 论文使用的评估环境和评估指标\n评估采用统一的零样本提示格式，所有任务设计为多选题以确保公平比较。使用准确率作为主要评估指标，并通过人工评估作为参考点。评估环境包括多种模型类别，从盲评文本LLM到专有多模态和推理优化架构，还包括开源社区开发的VLMs和MoE模型。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>清华大学、北京大学、复旦大学、微软研究院亚洲、香港科技大学、浙江大学</p>\n<h1>论文概述</h1>\n<p>论文介绍了MV-RoboBench，一个专门用于评估视觉语言模型在机器人操作场景中多视图空间推理能力的基准测试。该基准包含1,700个手工制作的QA项目，涵盖8个子任务，分为空间理解（交叉视图匹配、距离判断、视角识别、3D空间一致性）和机器人执行（动作规划、步骤执行、轨迹选择、属性识别）两大类别。论文评估了多种VLM模型，并探索了CoT风格的增强方法。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>建立首个整合空间和机器人推理与同步多视图输入的基准测试</li><li>验证了机器人多视图场景仍极具挑战性，最强大的VLM模型远低于人类表现</li><li>发现空间推理和机器人执行之间存在正相关性</li><li>证明强大的单视图空间基准测试性能不能可靠转移到机器人任务或多视图场景</li><li>揭示多视图机器人推理对视角整合、遮挡解决和空间融合提出了根本性不同要求</li></ol>\n<h1>论文方法描述</h1>\n<p>构建了多阶段的基准测试构建管道：数据收集（基于AgiWorld和BridgeV2数据集）、QA生成（使用任务特定模板构建五选一QA对）、人工质量审查（迭代审查修正问题）。探索了三种CoT风格增强方法：文本CoT（通过GPT-4.1生成场景描述）、视觉CoT（使用VGGT进行新颖视图合成）、结构CoT（使用MoGe-2提供深度先验）。评估采用零样本多选题格式，确保跨模型公平比较。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>使用AgiWorld和BridgeV2数据集作为数据源，构建了包含980个片段的1,708个QA对。评估涵盖盲评（Random、GPT-3.5-turbo、GPT-4-turbo）、专有模型（GPT-4o系列、GPT-4.1系列、Claude-3.5/3.7、Gemini-2.x系列）、专有推理模型（o4-mini、GPT-5系列、Claude-3.7-think、Gemini-2.5-pro）、开源模型（Gemma-3、InternVL3、Qwen2.5-vl系列）和开源MoE模型（Llama-4系列）。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>评估采用统一的零样本提示格式，所有任务设计为多选题以确保公平比较。使用准确率作为主要评估指标，并通过人工评估作为参考点。评估环境包括多种模型类别，从盲评文本LLM到专有多模态和推理优化架构，还包括开源社区开发的VLMs和MoE模型。</p>"
  },
  {
    "date": "2025-10-21",
    "title": "MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning",
    "link": "http://arxiv.org/abs/2510.18337",
    "summary_markdown": "### 论文研究单位\n- **Harvard University** (Wenhui Huang, Han Qi, Yilun Du, Heng Yang)\n- **University of Michigan** (Changhe Chen)\n- **Nanyang Technological University** (Chen Lv)\n### 论文概述\nMoTVLA是一个基于混合Transformer（MoT）架构的视觉语言动作（VLA）模型，通过整合快速与慢速推理来改进机器人学习中的语言指令控制能力。该模型保留预训练VLM的通用智能（通用智能专家）以处理感知和语义规划，同时引入领域专家生成快速运动分解信号，最终通过扩散Transformer（DiT）动作专家执行具体操作。该设计显著提升语言可控性和推理效率，并已在NLP基准、仿真环境（ManiSkill）和真实机器人实验中验证有效性。\n### 论文核心贡献点\n1. **统一快慢推理架构**：在单一MoT模型中实现通用智能（慢速推理）与领域特定知识（快速推理）的融合，通过分解-组合-再分解的机制保持性能平衡。\n2. **动作条件化策略学习**：将快速推理生成的运动分解信号作为扩散策略条件，提升任务执行效率和行为可解释性。\n3. **延迟与性能优势**：在推理速度（快推理频率达4倍提升）、语义理解及真实操作任务上超越SOTA基线（如π0.5、RT-2）。\n### 论文方法描述\n- **架构设计**：\n - **通用智能专家（Slow Reasoning）**：基于Qwen2.5-LLM 7B的预训练模型，处理视觉-文本多模态理解。\n - **领域专家（Fast Reasoning）**：同构架构，生成步骤化运动分解文本（双向注意力）。\n - **动作专家**：DiT结构，基于视觉窗口（5帧）、机器人状态、运动分解信号执行扩散去噪生成动作序列。\n- **训练流程**：\n - **领域专家SFT**：使用1.27M问答对（仿真154K + 真实125K + Robo2VLM 678K + LLaVA-OV 318K）优化快速推理。\n - **动作专家扩散策略**：ManiSkill仿真（每任务300轨迹）+ 真实操作（Pick-and-Place 50轨迹，Table Bussing 200轨迹），以均方误差优化去噪网络。\n- **推理模式**：\n - **对话模式**：通用智能专家执行慢推理回答人类问题（如场景描述）。\n - **行动模式**：领域专家快速生成运动分解 → 动作专家执行多步操作（如堆叠、插孔）。\n### 论文使用数据集和训练资源\n- **领域专家数据**：\n - 仿真数据：154K（ManiSkill的Cube Stacking、Peg-in-Hole、L-tool Pull）\n - 真实演示：125K（人类操作的Pick-and-Place、Table Bussing）\n - 外部数据：678K（Robo2VLM，转换为推理文本）、318K（LLaVA-OV，筛选长答案）\n- **动作专家数据**：\n - 仿真：900轨迹（3任务×300）\n - 真实：250轨迹（Pick-and-Place 50 + Table Bussing 200）\n- **预训练资源**：\n - 通用智能专家：Bagel-VLM初始化（SigLIP-So400m视觉编码 + Qwen2.5文本分词器）\n - 推理骨干：MoTVLA-14B（双7B专家，总参数量14B）\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **仿真**：ManiSkill平台（Cube Stacking、Peg-in-Hole、L-tool Pull）。\n - **真实操作**：桌面任务（蔬菜Pick-and-Place含干扰物、Table Bussing含模糊指令）。\n- **评估指标**：\n - **语义推理**：BLEU、METEOR、CIDEr、Token准确率（VQA任务如ScienceQA、Visual-7W）。\n - **机器人任务**：平均成功率（重复随机种子测试，零样本干扰场景）。\n - **延迟对比**：推理频率（MoTVLA-14B/1B vs π0.5-KI，H100/A6000 GPU）。\n- **基线对比**：π0.5-KI、π0、GR-MG、Diffusion Policy（DP），基于1050轨迹微调后性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li><strong>Harvard University</strong> (Wenhui Huang, Han Qi, Yilun Du, Heng Yang)</li><li><strong>University of Michigan</strong> (Changhe Chen)</li><li><strong>Nanyang Technological University</strong> (Chen Lv)</li></ul>\n<h3>论文概述</h3>\n<p>MoTVLA是一个基于混合Transformer（MoT）架构的视觉语言动作（VLA）模型，通过整合快速与慢速推理来改进机器人学习中的语言指令控制能力。该模型保留预训练VLM的通用智能（通用智能专家）以处理感知和语义规划，同时引入领域专家生成快速运动分解信号，最终通过扩散Transformer（DiT）动作专家执行具体操作。该设计显著提升语言可控性和推理效率，并已在NLP基准、仿真环境（ManiSkill）和真实机器人实验中验证有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>统一快慢推理架构</strong>：在单一MoT模型中实现通用智能（慢速推理）与领域特定知识（快速推理）的融合，通过分解-组合-再分解的机制保持性能平衡。</li><li><strong>动作条件化策略学习</strong>：将快速推理生成的运动分解信号作为扩散策略条件，提升任务执行效率和行为可解释性。</li><li><strong>延迟与性能优势</strong>：在推理速度（快推理频率达4倍提升）、语义理解及真实操作任务上超越SOTA基线（如π0.5、RT-2）。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>架构设计</strong>：</li></ul>\n<p> - <strong>通用智能专家（Slow Reasoning）</strong>：基于Qwen2.5-LLM 7B的预训练模型，处理视觉-文本多模态理解。</p>\n<p> - <strong>领域专家（Fast Reasoning）</strong>：同构架构，生成步骤化运动分解文本（双向注意力）。</p>\n<p> - <strong>动作专家</strong>：DiT结构，基于视觉窗口（5帧）、机器人状态、运动分解信号执行扩散去噪生成动作序列。</p>\n<ul><li><strong>训练流程</strong>：</li></ul>\n<p> - <strong>领域专家SFT</strong>：使用1.27M问答对（仿真154K + 真实125K + Robo2VLM 678K + LLaVA-OV 318K）优化快速推理。</p>\n<p> - <strong>动作专家扩散策略</strong>：ManiSkill仿真（每任务300轨迹）+ 真实操作（Pick-and-Place 50轨迹，Table Bussing 200轨迹），以均方误差优化去噪网络。</p>\n<ul><li><strong>推理模式</strong>：</li></ul>\n<p> - <strong>对话模式</strong>：通用智能专家执行慢推理回答人类问题（如场景描述）。</p>\n<p> - <strong>行动模式</strong>：领域专家快速生成运动分解 → 动作专家执行多步操作（如堆叠、插孔）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>领域专家数据</strong>：</li></ul>\n<p> - 仿真数据：154K（ManiSkill的Cube Stacking、Peg-in-Hole、L-tool Pull）</p>\n<p> - 真实演示：125K（人类操作的Pick-and-Place、Table Bussing）</p>\n<p> - 外部数据：678K（Robo2VLM，转换为推理文本）、318K（LLaVA-OV，筛选长答案）</p>\n<ul><li><strong>动作专家数据</strong>：</li></ul>\n<p> - 仿真：900轨迹（3任务×300）</p>\n<p> - 真实：250轨迹（Pick-and-Place 50 + Table Bussing 200）</p>\n<ul><li><strong>预训练资源</strong>：</li></ul>\n<p> - 通用智能专家：Bagel-VLM初始化（SigLIP-So400m视觉编码 + Qwen2.5文本分词器）</p>\n<p> - 推理骨干：MoTVLA-14B（双7B专家，总参数量14B）</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>仿真</strong>：ManiSkill平台（Cube Stacking、Peg-in-Hole、L-tool Pull）。</p>\n<p> - <strong>真实操作</strong>：桌面任务（蔬菜Pick-and-Place含干扰物、Table Bussing含模糊指令）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>语义推理</strong>：BLEU、METEOR、CIDEr、Token准确率（VQA任务如ScienceQA、Visual-7W）。</p>\n<p> - <strong>机器人任务</strong>：平均成功率（重复随机种子测试，零样本干扰场景）。</p>\n<p> - <strong>延迟对比</strong>：推理频率（MoTVLA-14B/1B vs π0.5-KI，H100/A6000 GPU）。</p>\n<ul><li><strong>基线对比</strong>：π0.5-KI、π0、GR-MG、Diffusion Policy（DP），基于1050轨迹微调后性能。</li></ul>"
  },
  {
    "date": "2025-10-20",
    "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies",
    "link": "http://arxiv.org/abs/2510.17950",
    "summary_markdown": "## 论文研究单位\nDexmal 和 Hugging Face\n## 论文概述\n论文介绍了RoboChallenge，一个在线机器人评估系统，旨在为视觉-语言-动作模型（VLA）提供大规模真实机器人基准测试。系统提供10台在线机器人供公众访问，覆盖UR5、Franka Panda、Cobot Magic Aloha和ARX-5等四种类型。研究构建了初始基准Table30，包含30个围绕桌子的任务，测试VLA模型的多种能力。论文描述了系统设计、评估协议和实验结果，强调了真实机器人在评估中的必要性。\n## 论文核心贡献点\n- 提出了一个在线机器人评估平台，支持大规模真实机器人测试，提供10台机器人机群。\n- 设计了评估协议，通过视觉任务重现控制初始状态，区分稳定性（多次测试一致）和公平性（模型间比较一致）。\n- 构建了Table30基准，包含30个任务，涵盖精确3D定位、多视角、时间依赖和软体操作等挑战。\n- 实现了“远程机器人”范式，用户通过API直接访问机器人，无需提交模型文件。\n- 提供了详细的结果分析，展示不同VLA模型在各项任务上的性能差异。\n## 论文方法描述\n系统采用“远程机器人”模式：用户通过低级别API访问机器人传感器（时间戳RGB、深度和本体感受数据）和动作队列（不可撤销动作）。评估协议包括：\n- 视觉任务重现：通过参考图像引导测试者固定初始场景状态。\n- 环境扰动控制：允许光照和背景变化，测试模型鲁棒性。\n- 测试者控制：区分经验测试者、无知测试者和自适应测试者，提出受控测试者减少偏差。\n- 稳定性与公平性权衡：基准协议关注模型稳定性（多次测试结果一致），比较协议关注公平性（模型间相对排序一致）。\n## 论文使用数据集和训练资源\n- 数据集：提供每个任务的演示数据（每任务最多1000集），用户需基于此微调模型。\n- 训练资源：\n - 任务特定设置：在8-GPU机器上训练约1天。\n - 一般设置：混合多任务数据训练“机器通才”模型（每任务约50集）。\n- 示例模型：包括π₀、π₀.5、CogACT和OpenVLA/OFT等开源VLA算法。\n## 论文使用的评估环境和评估指标\n- 评估环境：10台机器人，分为四种类型（UR5、Franka Panda、Cobot Magic Aloha、ARX-5），配备多摄像头（主要、手腕、侧面）。\n- 评估指标：\n - 成功率（SR）：任务完成百分比。\n - 进度分数：基于任务阶段完成情况，每个任务总分10分，每阶段分配点数，重试扣0.5分；每任务测试10次，总分100。\n- Table30基准：30个任务列表（见表1），如插花、整理水果和开抽屉等，测试精确操作、多视角和时间依赖等能力。\n- 结果：在π₀.5模型上观察到最高成功率，平均43.7%，其他模型性能较低；任务分析显示时间和软体相关任务更具挑战性。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>Dexmal 和 Hugging Face</p>\n<h2>论文概述</h2>\n<p>论文介绍了RoboChallenge，一个在线机器人评估系统，旨在为视觉-语言-动作模型（VLA）提供大规模真实机器人基准测试。系统提供10台在线机器人供公众访问，覆盖UR5、Franka Panda、Cobot Magic Aloha和ARX-5等四种类型。研究构建了初始基准Table30，包含30个围绕桌子的任务，测试VLA模型的多种能力。论文描述了系统设计、评估协议和实验结果，强调了真实机器人在评估中的必要性。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了一个在线机器人评估平台，支持大规模真实机器人测试，提供10台机器人机群。</li><li>设计了评估协议，通过视觉任务重现控制初始状态，区分稳定性（多次测试一致）和公平性（模型间比较一致）。</li><li>构建了Table30基准，包含30个任务，涵盖精确3D定位、多视角、时间依赖和软体操作等挑战。</li><li>实现了“远程机器人”范式，用户通过API直接访问机器人，无需提交模型文件。</li><li>提供了详细的结果分析，展示不同VLA模型在各项任务上的性能差异。</li></ul>\n<h2>论文方法描述</h2>\n<p>系统采用“远程机器人”模式：用户通过低级别API访问机器人传感器（时间戳RGB、深度和本体感受数据）和动作队列（不可撤销动作）。评估协议包括：</p>\n<ul><li>视觉任务重现：通过参考图像引导测试者固定初始场景状态。</li><li>环境扰动控制：允许光照和背景变化，测试模型鲁棒性。</li><li>测试者控制：区分经验测试者、无知测试者和自适应测试者，提出受控测试者减少偏差。</li><li>稳定性与公平性权衡：基准协议关注模型稳定性（多次测试结果一致），比较协议关注公平性（模型间相对排序一致）。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据集：提供每个任务的演示数据（每任务最多1000集），用户需基于此微调模型。</li><li>训练资源：</li></ul>\n<p> - 任务特定设置：在8-GPU机器上训练约1天。</p>\n<p> - 一般设置：混合多任务数据训练“机器通才”模型（每任务约50集）。</p>\n<ul><li>示例模型：包括π₀、π₀.5、CogACT和OpenVLA/OFT等开源VLA算法。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：10台机器人，分为四种类型（UR5、Franka Panda、Cobot Magic Aloha、ARX-5），配备多摄像头（主要、手腕、侧面）。</li><li>评估指标：</li></ul>\n<p> - 成功率（SR）：任务完成百分比。</p>\n<p> - 进度分数：基于任务阶段完成情况，每个任务总分10分，每阶段分配点数，重试扣0.5分；每任务测试10次，总分100。</p>\n<ul><li>Table30基准：30个任务列表（见表1），如插花、整理水果和开抽屉等，测试精确操作、多视角和时间依赖等能力。</li><li>结果：在π₀.5模型上观察到最高成功率，平均43.7%，其他模型性能较低；任务分析显示时间和软体相关任务更具挑战性。</li></ul>"
  },
  {
    "date": "2025-10-20",
    "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2510.17640",
    "summary_markdown": "### 论文研究单位\n- 南洋理工大学（新加坡）\n- 清华大学（中国北京）\n- 北京邮电大学（中国北京）\n### 论文概述\n论文针对机器人操作中的分布外（OOD）泛化问题，提出RESample框架。现有视觉-语言-动作（VLA）模型在模仿学习中易受OOD状态影响，因数据集仅含成功轨迹。RESample利用离线强化学习获取动作价值网络识别次优动作，并通过探索性采样生成潜在OOD轨迹，将数据整合训练集以增强VLA模型鲁棒性。\n### 论文核心贡献点\n- 设计稳健数据增强框架缓解模仿学习的OOD问题。\n- 提出探索性采样机制，自适应纳入OOD样本。\n- 利用动作价值网络识别细粒度OOD动作样本。\n- 在模拟和真实机器人任务中验证有效性。\n### 论文方法描述\nRESample基于策略（πθ）和动作价值网络（Qφ）的二元性：\n- **概念框架**：策略与评论家分歧触发探索性采样，强制执行策略自信但低值行动，暴露失败模式。\n- **价值函数估计**：基于Cal-QL改进，使用行为克隆代理策略（πψ）校准Q值。评论家目标含时间差分损失和正则化（统一惩罚、行动锚定、数据保持）。\n- **探索性采样机制**：策略生成候选动作，评论家过滤Q值低样本执行；为空则用常规动作。产生失败和恢复轨迹纳入训练。\n### 论文使用数据集和训练资源\n- **数据集**：模拟实验使用LIBERO基准（包含Spatial、Object、Goal和Long-horizon四类任务，每类10个任务）。真实实验使用Galaxea A1机器人手臂。\n- **训练资源**：评论家基于SAC算法离线训练，策略如DiT Policy或π0在模拟和真实环境重新训练。参数包括批大小（64或256）、学习率（1e-4或2e-5）、折扣因子（0.99）等。\n### 论文使用的评估环境和评估指标\n- **评估环境**：模拟实验在LIBERO基准评估，真实实验在四个任务（Pick Block、Stack Cup、Arrange Cubes、Stack 2 Cups）。\n- **评估指标**：主要使用任务成功率（Success Rate）。在LIBERO上报告各任务类别平均成功率；消融研究使用成功率衡量性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>南洋理工大学（新加坡）</li><li>清华大学（中国北京）</li><li>北京邮电大学（中国北京）</li></ul>\n<h3>论文概述</h3>\n<p>论文针对机器人操作中的分布外（OOD）泛化问题，提出RESample框架。现有视觉-语言-动作（VLA）模型在模仿学习中易受OOD状态影响，因数据集仅含成功轨迹。RESample利用离线强化学习获取动作价值网络识别次优动作，并通过探索性采样生成潜在OOD轨迹，将数据整合训练集以增强VLA模型鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>设计稳健数据增强框架缓解模仿学习的OOD问题。</li><li>提出探索性采样机制，自适应纳入OOD样本。</li><li>利用动作价值网络识别细粒度OOD动作样本。</li><li>在模拟和真实机器人任务中验证有效性。</li></ul>\n<h3>论文方法描述</h3>\n<p>RESample基于策略（πθ）和动作价值网络（Qφ）的二元性：</p>\n<ul><li><strong>概念框架</strong>：策略与评论家分歧触发探索性采样，强制执行策略自信但低值行动，暴露失败模式。</li><li><strong>价值函数估计</strong>：基于Cal-QL改进，使用行为克隆代理策略（πψ）校准Q值。评论家目标含时间差分损失和正则化（统一惩罚、行动锚定、数据保持）。</li><li><strong>探索性采样机制</strong>：策略生成候选动作，评论家过滤Q值低样本执行；为空则用常规动作。产生失败和恢复轨迹纳入训练。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：模拟实验使用LIBERO基准（包含Spatial、Object、Goal和Long-horizon四类任务，每类10个任务）。真实实验使用Galaxea A1机器人手臂。</li><li><strong>训练资源</strong>：评论家基于SAC算法离线训练，策略如DiT Policy或π0在模拟和真实环境重新训练。参数包括批大小（64或256）、学习率（1e-4或2e-5）、折扣因子（0.99）等。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：模拟实验在LIBERO基准评估，真实实验在四个任务（Pick Block、Stack Cup、Arrange Cubes、Stack 2 Cups）。</li><li><strong>评估指标</strong>：主要使用任务成功率（Success Rate）。在LIBERO上报告各任务类别平均成功率；消融研究使用成功率衡量性能。</li></ul>"
  },
  {
    "date": "2025-10-20",
    "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
    "link": "http://arxiv.org/abs/2510.17439",
    "summary_markdown": "### 论文研究单位\n- ByteDance Seed、NUS、NTU、THU、SMU 等机构合作\n### 论文概述\n- FALCON (From Spatial to Action) 是一类视觉-语言-行动（VLA）模型，针对现有 VLA 基于 2D 视觉编码、缺乏可靠 3D 空间理解的问题，提出通过空间基础模型在仅 RGB 条件下注入丰富的空间 token，并在可选条件下融合深度与位姿，从而在动作头层面融合语义与几何信息，增强长程与空间推理能力、跨模态可迁移性以及语言-视觉对齐\n### 论文核心贡献点\n- 以 Embodied Spatial Model（ESM）从 RGB 生成强几何先验，支持可选深度/位姿注入，无需重训即可提升精度\n- 设计 Spatial-Enhanced Action Head，将空间 token 在动作头而非 VLM 中融合，保持 VLM 语义对齐不受破坏\n- 采用随机条件注入训练策略（RGB-only 与 RGB-D/Pose 随机），实现强跨模态迁移能力\n- 在动作头采用轻量元素相加融合，最大化稳定性与效率（优于交叉注意力与 FiLM）\n### 论文方法描述\n- 整体架构：2D VLM（如 Kosmos-2）负责语义表示；ESM 提取空间 tokens（基于 VGGT 范式，结合 DINO 视觉 token）；动作头融合语义动作 token 与空间 tokens 产生精确动作\n- 空间编码：\n - 输入为第三视角图像，附加可学习相机 token，进入 Spatial Encoder（N 层自/交叉注意力）输出空间 tokens 与相机 token\n - 3D 条件编码：将相机位姿（7DoF）编码为 GT camera token；对深度与有效性掩膜进行归一化与卷积编码，生成与图像 tokens 等尺寸的深度 tokens\n - 3D 条件注入：采用 b_d、b_p∼Bernoulli(p) 随机决定是否注入深度与位姿，替换可学习相机 token 并进行元素级相加；在训练中采用深度、点云与位姿的多任务监督\n- 空间增强动作头：\n - 将空间 tokens 最大池化为统一向量，经轻量 MLP 投影至动作头维度，与语义 action token 元素级相加\n - 比较了交叉注意力、FiLM 门控与元素相加，最终采用元素相加（在泛化与稳定性上表现最佳）\n - 预测器支持两类：MLP（短期）或 LSTM（长期）输出动作序列（7DoF 姿态+夹爪状态，C 步预测）\n- 训练范式：\n - 两阶段训练：阶段一冻结 VLM/ESM/旧动作头，只训练适配器（零初始化线性层避免初始破坏对齐）；阶段二解冻并联合优化\n - 损失：MSE（姿态）+ BCE（夹爪）+ 3D 重建/深度/位姿监督（与 VGGT 一致）\n### 论文使用数据集和训练资源\n- 数据与平台：Open X-Embodiment 混合预训练；真实机器人多任务微调；基准覆盖 CALVIN（ABCD→D、ABC→D）、SimplerEnv（WidowX、Google Robot）与 11 项真实任务\n- 训练资源：32×A100 训练；模型规模约 2.9B（VLM～1.6B，ESM～1.0B，配套动作头）\n### 论文使用的评估环境和评估指标\n- 仿真基准与指标：\n - CALVIN：Tasks Completed in a Row（1-5 连贯任务完成率）与 Avg. Len（平均连击长度）\n - ABCD→D：FALCON 97.2/93.3/90.3/88.0/84.0%，Avg Len 4.53；ABC→D：98.4/94.5/88.6/82.5/75.5%，Avg Len 4.40\n - 相较 RT-1/Robo-Flamingo/GR-1/UP-VLA/RoboVLM 等一致领先\n - SimplerEnv（WidowX/Google Robot）：按任务成功率汇总\n - WidowX：FALCON 56.3%（Put Spoon 62.5%，Put Carrot 41.7% 等）\n - Google Robot：FALCON 62.9%，在“打开抽屉放苹果”上 41.7%，远超 RT-2-X 55B 的 3.7%\n- 真实世界评测与指标：三类设置的成功率（%）\n - Base Tasks（9 套任务）：FALCON 70.0%，显著高于 SpatialVLA（44.4%）\n - Few-shot（Simple/Unseen）：在未见物体/背景/任务描述变体下领先，Simple 提升 +27.5%，Unseen 平均 +27%\n - 空间理解能力：在高度变化、尺寸变化、空间指令等任务中保持最高成功率（例如高度变化任务 60%→80%）\n- 模态可迁移性与消融：在 CALVIN 与真实任务上验证 RGB-only 与 RGB-D/Pose 的增益；ESM 深度预测指标（δ<1.25、AbsRel）随可用模态增强而显著提升；元素相加在融合策略中兼具效率与性能",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>ByteDance Seed、NUS、NTU、THU、SMU 等机构合作</li></ul>\n<h3>论文概述</h3>\n<ul><li>FALCON (From Spatial to Action) 是一类视觉-语言-行动（VLA）模型，针对现有 VLA 基于 2D 视觉编码、缺乏可靠 3D 空间理解的问题，提出通过空间基础模型在仅 RGB 条件下注入丰富的空间 token，并在可选条件下融合深度与位姿，从而在动作头层面融合语义与几何信息，增强长程与空间推理能力、跨模态可迁移性以及语言-视觉对齐</li></ul>\n<h3>论文核心贡献点</h3>\n<ul><li>以 Embodied Spatial Model（ESM）从 RGB 生成强几何先验，支持可选深度/位姿注入，无需重训即可提升精度</li><li>设计 Spatial-Enhanced Action Head，将空间 token 在动作头而非 VLM 中融合，保持 VLM 语义对齐不受破坏</li><li>采用随机条件注入训练策略（RGB-only 与 RGB-D/Pose 随机），实现强跨模态迁移能力</li><li>在动作头采用轻量元素相加融合，最大化稳定性与效率（优于交叉注意力与 FiLM）</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>整体架构：2D VLM（如 Kosmos-2）负责语义表示；ESM 提取空间 tokens（基于 VGGT 范式，结合 DINO 视觉 token）；动作头融合语义动作 token 与空间 tokens 产生精确动作</li><li>空间编码：</li></ul>\n<p> - 输入为第三视角图像，附加可学习相机 token，进入 Spatial Encoder（N 层自/交叉注意力）输出空间 tokens 与相机 token</p>\n<p> - 3D 条件编码：将相机位姿（7DoF）编码为 GT camera token；对深度与有效性掩膜进行归一化与卷积编码，生成与图像 tokens 等尺寸的深度 tokens</p>\n<p> - 3D 条件注入：采用 b_d、b_p∼Bernoulli(p) 随机决定是否注入深度与位姿，替换可学习相机 token 并进行元素级相加；在训练中采用深度、点云与位姿的多任务监督</p>\n<ul><li>空间增强动作头：</li></ul>\n<p> - 将空间 tokens 最大池化为统一向量，经轻量 MLP 投影至动作头维度，与语义 action token 元素级相加</p>\n<p> - 比较了交叉注意力、FiLM 门控与元素相加，最终采用元素相加（在泛化与稳定性上表现最佳）</p>\n<p> - 预测器支持两类：MLP（短期）或 LSTM（长期）输出动作序列（7DoF 姿态+夹爪状态，C 步预测）</p>\n<ul><li>训练范式：</li></ul>\n<p> - 两阶段训练：阶段一冻结 VLM/ESM/旧动作头，只训练适配器（零初始化线性层避免初始破坏对齐）；阶段二解冻并联合优化</p>\n<p> - 损失：MSE（姿态）+ BCE（夹爪）+ 3D 重建/深度/位姿监督（与 VGGT 一致）</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据与平台：Open X-Embodiment 混合预训练；真实机器人多任务微调；基准覆盖 CALVIN（ABCD→D、ABC→D）、SimplerEnv（WidowX、Google Robot）与 11 项真实任务</li><li>训练资源：32×A100 训练；模型规模约 2.9B（VLM～1.6B，ESM～1.0B，配套动作头）</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>仿真基准与指标：</li></ul>\n<p> - CALVIN：Tasks Completed in a Row（1-5 连贯任务完成率）与 Avg. Len（平均连击长度）</p>\n<p> - ABCD→D：FALCON 97.2/93.3/90.3/88.0/84.0%，Avg Len 4.53；ABC→D：98.4/94.5/88.6/82.5/75.5%，Avg Len 4.40</p>\n<p> - 相较 RT-1/Robo-Flamingo/GR-1/UP-VLA/RoboVLM 等一致领先</p>\n<p> - SimplerEnv（WidowX/Google Robot）：按任务成功率汇总</p>\n<p> - WidowX：FALCON 56.3%（Put Spoon 62.5%，Put Carrot 41.7% 等）</p>\n<p> - Google Robot：FALCON 62.9%，在“打开抽屉放苹果”上 41.7%，远超 RT-2-X 55B 的 3.7%</p>\n<ul><li>真实世界评测与指标：三类设置的成功率（%）</li></ul>\n<p> - Base Tasks（9 套任务）：FALCON 70.0%，显著高于 SpatialVLA（44.4%）</p>\n<p> - Few-shot（Simple/Unseen）：在未见物体/背景/任务描述变体下领先，Simple 提升 +27.5%，Unseen 平均 +27%</p>\n<p> - 空间理解能力：在高度变化、尺寸变化、空间指令等任务中保持最高成功率（例如高度变化任务 60%→80%）</p>\n<ul><li>模态可迁移性与消融：在 CALVIN 与真实任务上验证 RGB-only 与 RGB-D/Pose 的增益；ESM 深度预测指标（δ<1.25、AbsRel）随可用模态增强而显著提升；元素相加在融合策略中兼具效率与性能</li></ul>"
  },
  {
    "date": "2025-10-20",
    "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots",
    "link": "http://arxiv.org/abs/2510.17369",
    "summary_markdown": "# 论文研究单位\n- 主要单位：EPFL（洛桑联邦理工学院）\n- 合作单位：LatentWorlds AI（荷兰代尔夫特理工大学背景）、Embodied AI SA\n# 论文概述\n- 研究问题：现有视觉-语言-动作（VLA）模型主要部署于刚性机械臂，难以迁移到软体连续体机器人；两者在运动学、动力学和形态学上的差异导致即用策略失败。\n- 目标：在软体连续体机器人上部署VLA模型，建立从刚性到软体机器人之间的可迁移部署流程，展示在接近人类的真实环境中的安全操控。\n- 方案：构建结构化微调与部署流程，对OpenVLA-OFT与π0两类SOTA VLA模型进行对比评估，在软体机器人Embuddy上完成三类代表性操控任务。\n# 论文核心贡献点\n- 贡献1：首个开放软体机器人演示数据集，覆盖抓取放置与人类近距互动任务，便于复现研究与跨形态迁移学习。\n- 贡献2：在UR5（刚性）与Embuddy（软体）上系统对比OpenVLA-OFT；经针对性微调后，软体机器人取得与刚性平台相当的任务成功率，验证了刚性→软体领域迁移的可行性。\n- 贡献3：在软体机器人上比较OpenVLA-OFT与π0：π0在刚性跨平台泛化更强，而经微调后OpenVLA-OFT在软体平台上表现更优。\n# 论文方法描述\n- 平台与任务设计：\n - 刚性平台：UR5，配套平行夹爪与俯视单目相机。\n - 软体平台：Embuddy，连续体由三段软体节段构成（每段受腱驱动、最大弯曲角度分别为80°、50°、50°），具固有顺应性与碰撞恢复能力；总重约5 kg，工作空间由弯曲角限制。与UR5使用相同相机与夹爪配置以保证公平对比。\n - 任务：\n 1. “把橙子放入盘子”——基础抓取放置。\n 2. “把X放入盘子”（X为橙子或牛奶）——带选择指令的抓取放置。\n 3. “用棉花糖喂人”——近距人机互动操控。\n- 数据采集与处理：\n - 采用3Dconnexion空间鼠标遥操作系统；软体机器人的逆运动学基于分段常曲率（PCC）模型，将连续体节段近似为常曲率以映射腱长与末端位姿。\n - 多模态观测：第三人称图像、手腕相机图像、本体感受状态（末端位姿）、自然语言任务描述；统一裁剪与缩放到256×256，手腕图像做翻转。\n - 表示标准化：\n - 本体状态s为8维向量[x,y,z,r,p,y,pad,g]，其中(x,y,z)为笛卡尔位置，(r,p,y)为滚-俯-偏角，g为夹爪状态（0/1）。\n - 动作a定义为相邻状态差分：7维[Δx,Δy,Δz,Δr,Δp,Δy,g]；角差使用Δ = ((Δ+π) mod 2π) − π处理边界。\n - 过滤与转换：去除几乎静止片段；OpenVLA-OFT数据用RLDS格式，π0数据用LeRobot格式；提供开放数据集（HuggingFace：HCSuMoss/soft_orange、HCSuMoss/soft_feed）。\n- 模型微调与推理：\n - OpenVLA-OFT（Llama 2 7B + ViT视觉前端）：采用LoRA（rank=32）进行全量微调以平衡性能与成本；引入FiLM层提升语言-视觉条件化；在Task 2中启用FiLM以强化指令对象选择。\n - π0（PaliGemma 3B）：由于模型体量较小，采用全量微调；在默认设置上调整动作块大小为8以公平比较。\n - 推理管线：本地机器人端采集第三人称/手腕图像、本体状态与语言指令；远程GPU进行动作块预测并回传；本地执行并循环，直至任务完成或达最大步数。采用动作块并行解码以提高速度。\n - 安全性与鲁棒性：软体机器人可在被人工推动后恢复姿态并继续执行任务，显著优于刚性臂在人机交互场景的安全性。\n# 论文使用数据集和训练资源\n- 数据集：开放软体机器人演示数据集，覆盖三类任务（Task 1: 50集；Task 2: 100集，橙子与牛奶各50；Task 3: 20集）；HuggingFace可下载。\n- 训练资源：\n - OpenVLA-OFT（UR5）：A100（80 GB+），Microsoft Azure虚拟机；软体Embuddy：H100远程HPC。\n - π0（Embuddy）：H100远程HPC。\n - 训练时长：OpenVLA-OFT约需56小时完成200k步（单A100）；π0约需11小时完成30k步（单H100）。\n# 论文使用的评估环境和评估指标\n- 评估环境：软体机器人Embuddy与刚性机器人UR5；两者使用相同相机、夹爪与任务设定以保证公平性。\n- 评估指标：\n - 任务成功率：在每项任务上进行10次试验，统计成功率作为主要指标。\n - 推理频率（端到端闭环，含网络延迟）：UR5+OpenVLA-OFT约32.3 Hz；Embuddy+OpenVLA-OFT约25.1 Hz；Embuddy+π0约38.0 Hz。\n - 语言指令遵循验证：Task 2中FiLM模块提升对象选择准确率（OpenVLA-OFT达70%成功）；Task 3中在场景替换（棉花糖换为橙子）时模型能提前终止而非错误操作。\n - 扰动鲁棒性测试：\n - 人类出现与移动对性能无影响；\n - 未见物体可能偶致混淆（约10次中1次）；\n - 目标物体置于工作空间外时模型总是失败，说明训练工作空间是决定性约束。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>主要单位：EPFL（洛桑联邦理工学院）</li><li>合作单位：LatentWorlds AI（荷兰代尔夫特理工大学背景）、Embodied AI SA</li></ul>\n<h1>论文概述</h1>\n<ul><li>研究问题：现有视觉-语言-动作（VLA）模型主要部署于刚性机械臂，难以迁移到软体连续体机器人；两者在运动学、动力学和形态学上的差异导致即用策略失败。</li><li>目标：在软体连续体机器人上部署VLA模型，建立从刚性到软体机器人之间的可迁移部署流程，展示在接近人类的真实环境中的安全操控。</li><li>方案：构建结构化微调与部署流程，对OpenVLA-OFT与π0两类SOTA VLA模型进行对比评估，在软体机器人Embuddy上完成三类代表性操控任务。</li></ul>\n<h1>论文核心贡献点</h1>\n<ul><li>贡献1：首个开放软体机器人演示数据集，覆盖抓取放置与人类近距互动任务，便于复现研究与跨形态迁移学习。</li><li>贡献2：在UR5（刚性）与Embuddy（软体）上系统对比OpenVLA-OFT；经针对性微调后，软体机器人取得与刚性平台相当的任务成功率，验证了刚性→软体领域迁移的可行性。</li><li>贡献3：在软体机器人上比较OpenVLA-OFT与π0：π0在刚性跨平台泛化更强，而经微调后OpenVLA-OFT在软体平台上表现更优。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>平台与任务设计：</li></ul>\n<p> - 刚性平台：UR5，配套平行夹爪与俯视单目相机。</p>\n<p> - 软体平台：Embuddy，连续体由三段软体节段构成（每段受腱驱动、最大弯曲角度分别为80°、50°、50°），具固有顺应性与碰撞恢复能力；总重约5 kg，工作空间由弯曲角限制。与UR5使用相同相机与夹爪配置以保证公平对比。</p>\n<p> - 任务：</p>\n<p> 1. “把橙子放入盘子”——基础抓取放置。</p>\n<p> 2. “把X放入盘子”（X为橙子或牛奶）——带选择指令的抓取放置。</p>\n<p> 3. “用棉花糖喂人”——近距人机互动操控。</p>\n<ul><li>数据采集与处理：</li></ul>\n<p> - 采用3Dconnexion空间鼠标遥操作系统；软体机器人的逆运动学基于分段常曲率（PCC）模型，将连续体节段近似为常曲率以映射腱长与末端位姿。</p>\n<p> - 多模态观测：第三人称图像、手腕相机图像、本体感受状态（末端位姿）、自然语言任务描述；统一裁剪与缩放到256×256，手腕图像做翻转。</p>\n<p> - 表示标准化：</p>\n<p> - 本体状态s为8维向量[x,y,z,r,p,y,pad,g]，其中(x,y,z)为笛卡尔位置，(r,p,y)为滚-俯-偏角，g为夹爪状态（0/1）。</p>\n<p> - 动作a定义为相邻状态差分：7维[Δx,Δy,Δz,Δr,Δp,Δy,g]；角差使用Δ = ((Δ+π) mod 2π) − π处理边界。</p>\n<p> - 过滤与转换：去除几乎静止片段；OpenVLA-OFT数据用RLDS格式，π0数据用LeRobot格式；提供开放数据集（HuggingFace：HCSuMoss/soft_orange、HCSuMoss/soft_feed）。</p>\n<ul><li>模型微调与推理：</li></ul>\n<p> - OpenVLA-OFT（Llama 2 7B + ViT视觉前端）：采用LoRA（rank=32）进行全量微调以平衡性能与成本；引入FiLM层提升语言-视觉条件化；在Task 2中启用FiLM以强化指令对象选择。</p>\n<p> - π0（PaliGemma 3B）：由于模型体量较小，采用全量微调；在默认设置上调整动作块大小为8以公平比较。</p>\n<p> - 推理管线：本地机器人端采集第三人称/手腕图像、本体状态与语言指令；远程GPU进行动作块预测并回传；本地执行并循环，直至任务完成或达最大步数。采用动作块并行解码以提高速度。</p>\n<p> - 安全性与鲁棒性：软体机器人可在被人工推动后恢复姿态并继续执行任务，显著优于刚性臂在人机交互场景的安全性。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：开放软体机器人演示数据集，覆盖三类任务（Task 1: 50集；Task 2: 100集，橙子与牛奶各50；Task 3: 20集）；HuggingFace可下载。</li><li>训练资源：</li></ul>\n<p> - OpenVLA-OFT（UR5）：A100（80 GB+），Microsoft Azure虚拟机；软体Embuddy：H100远程HPC。</p>\n<p> - π0（Embuddy）：H100远程HPC。</p>\n<p> - 训练时长：OpenVLA-OFT约需56小时完成200k步（单A100）；π0约需11小时完成30k步（单H100）。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：软体机器人Embuddy与刚性机器人UR5；两者使用相同相机、夹爪与任务设定以保证公平性。</li><li>评估指标：</li></ul>\n<p> - 任务成功率：在每项任务上进行10次试验，统计成功率作为主要指标。</p>\n<p> - 推理频率（端到端闭环，含网络延迟）：UR5+OpenVLA-OFT约32.3 Hz；Embuddy+OpenVLA-OFT约25.1 Hz；Embuddy+π0约38.0 Hz。</p>\n<p> - 语言指令遵循验证：Task 2中FiLM模块提升对象选择准确率（OpenVLA-OFT达70%成功）；Task 3中在场景替换（棉花糖换为橙子）时模型能提前终止而非错误操作。</p>\n<p> - 扰动鲁棒性测试：</p>\n<p> - 人类出现与移动对性能无影响；</p>\n<p> - 未见物体可能偶致混淆（约10次中1次）；</p>\n<p> - 目标物体置于工作空间外时模型总是失败，说明训练工作空间是决定性约束。</p>"
  },
  {
    "date": "2025-10-20",
    "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
    "link": "http://arxiv.org/abs/2510.17148",
    "summary_markdown": "# 论文研究单位\n\n- RIX, Bosch\n- AIR, Tsinghua University\n- Shanghai Jiao Tong University\n# 论文概述\n\n端到端（E2E）自动驾驶模型在常规条件下可生成物理可行的轨迹，但难以泛化至长尾场景；视觉-语言-动作（VLA）模型利用世界知识与认知推理应对复杂场景，但缺乏细粒度三维推理，易产生物理不可行的动作。DiffVLA++通过度量引导的轨迹评分器将VLA与E2E的优势对齐，提出桥接认知推理与端到端规划的统一框架。实验在ICCV 2025 Autonomous Grand Challenge公开榜单上实现EPDMS 49.12。\n# 论文核心贡献点\n\n- 构建完全可微的VLA模块，直接回归带语义与三维推理的未来轨迹（4秒、8个关键点）。\n- 设计密集轨迹词库的E2E模块，包含代理检测、语义分割与规划头，保障物理可行性与密集场景表达。\n- 提出度量引导对齐机制：以规则化驾驶指标（NC、DAC、EP、TTC、LK、DDC、TLC、HC）训练轻量轨迹评分器（共享BEV特征的MLP并行头），将VLA与E2E轨迹映射至统一度量空间，实现对齐与融合。\n- 后处理：对候选轨迹进行可行驶区域过滤与加权评分选择；离线集成两分支以提升最终性能。\n# 论文方法描述\n\n- VLA模块\n - 视觉流：CLIP ViT-L/14编码多视角图像；Driving Vision Adapter压缩并投影视觉token至LLM嵌入空间。\n - 语言流：预训练分词器与文本编码器处理导航与高层指令，生成文本token。\n - 语言模型：Vicuna-v1.5-7B进行多模态融合与因果注意力；LLM最后一层直接回归连续未来轨迹（x,y,θ），避免离散化误差，支持端到端优化。\n- E2E模块\n - BEV生成：BevFormer以VoVNet-99为骨干，构建128×128的BEV网格，覆盖64×64米空间。\n - 多任务头：代理检测头（32查询）、语义分割头、轨迹规划头。\n - 轨迹规划头：预定义密集轨迹词库（8192候选，K-means聚类专家轨迹），每个候选含8个2Hz关键点；沿候选轨迹采样BEV特征，经注意力机制聚合为上下文感知轨迹嵌入；通过可变形交叉注意力整合代理特征以精炼轨迹嵌入；每个嵌入经MLP解码为残差偏移，输出最终预测轨迹。\n- 度量引导对齐\n - 轨迹评分器：并行MLP头回归八项规则化指标（EP连续值[0,1]；DAC、TLC、TTC、LK、HC二值{0,1}；NC、DDC三值{0,0.5,1}）；采用加权复合损失（MSE、BCE、Cross-Entropy）训练。\n- 后处理\n - 先用全景驾驶感知模型预测可行驶区域，剔除不合法候选；随后按加权分数排序与选择；E2E与VLA两分支离线集成。\n# 论文使用数据集和训练资源\n\n- 数据集\n - NavsimV2（navtrain划分）用于训练；Navhard两阶段测试与公开榜单用于评估。\n- 训练资源与配置\n - VLA模块：Vicuna-v1.5-7B + CLIP ViT-L/14；AdamW优化器，余弦学习率，初值1e-5；dropout 0.05；1个epoch；批量8；8×NVIDIA A800 GPU。\n - E2E与评分器联合训练：BevFormer + VoVNet-99；代理检测/分割/规划/评分多损失联合（权重10/1/14/20/14）；30个epoch；批量8；初始学习率1e-4；4×A800 GPU；AdamW与余弦调度（与VLA一致）。\n# 论文使用的评估环境和评估指标\n\n- 评估环境\n - ICCV 2025 Autonomous Grand Challenge（Navhard两阶段测试与公开leaderboard）。\n- 评估指标与结果\n - 主要指标：扩展预测驾驶员模型得分（EPDMS）。\n - 分支成绩：VLA分支48.0；E2E分支43.7。\n - 最终模型（离线集成）EPDMS：49.1238。\n - 分项指标（公开leaderboard，两阶段）：\n - No-At-Fault Collisions：98.2143（Stage 1）、88.7709（Stage 2）\n - Drivable Area Compliance：98.5714（Stage 1）、95.3235（Stage 2）\n - Driving Direction Compliance：100（Stage 1）、97.2196（Stage 2）\n - Traffic Light Compliance：99.2857（Stage 1）、98.1711（Stage 2）\n - Ego Progress：79.5117（Stage 1）、73.4289（Stage 2）\n - Time-To-Collision within bound：98.5714（Stage 1）、87.9888（Stage 2）\n - Lane Keeping：95（Stage 1）、59.4454（Stage 2）\n - History Comfort：92.8571（Stage 1）、98.9833（Stage 2）\n - Two-frame Extended Comfort：50（Stage 1）、52.9822（Stage 2）",
    "summary_html": "<h1>论文研究单位</h1>\n\n<ul><li>RIX, Bosch</li><li>AIR, Tsinghua University</li><li>Shanghai Jiao Tong University</li></ul>\n<h1>论文概述</h1>\n\n<p>端到端（E2E）自动驾驶模型在常规条件下可生成物理可行的轨迹，但难以泛化至长尾场景；视觉-语言-动作（VLA）模型利用世界知识与认知推理应对复杂场景，但缺乏细粒度三维推理，易产生物理不可行的动作。DiffVLA++通过度量引导的轨迹评分器将VLA与E2E的优势对齐，提出桥接认知推理与端到端规划的统一框架。实验在ICCV 2025 Autonomous Grand Challenge公开榜单上实现EPDMS 49.12。</p>\n<h1>论文核心贡献点</h1>\n\n<ul><li>构建完全可微的VLA模块，直接回归带语义与三维推理的未来轨迹（4秒、8个关键点）。</li><li>设计密集轨迹词库的E2E模块，包含代理检测、语义分割与规划头，保障物理可行性与密集场景表达。</li><li>提出度量引导对齐机制：以规则化驾驶指标（NC、DAC、EP、TTC、LK、DDC、TLC、HC）训练轻量轨迹评分器（共享BEV特征的MLP并行头），将VLA与E2E轨迹映射至统一度量空间，实现对齐与融合。</li><li>后处理：对候选轨迹进行可行驶区域过滤与加权评分选择；离线集成两分支以提升最终性能。</li></ul>\n<h1>论文方法描述</h1>\n\n<ul><li>VLA模块</li></ul>\n<p> - 视觉流：CLIP ViT-L/14编码多视角图像；Driving Vision Adapter压缩并投影视觉token至LLM嵌入空间。</p>\n<p> - 语言流：预训练分词器与文本编码器处理导航与高层指令，生成文本token。</p>\n<p> - 语言模型：Vicuna-v1.5-7B进行多模态融合与因果注意力；LLM最后一层直接回归连续未来轨迹（x,y,θ），避免离散化误差，支持端到端优化。</p>\n<ul><li>E2E模块</li></ul>\n<p> - BEV生成：BevFormer以VoVNet-99为骨干，构建128×128的BEV网格，覆盖64×64米空间。</p>\n<p> - 多任务头：代理检测头（32查询）、语义分割头、轨迹规划头。</p>\n<p> - 轨迹规划头：预定义密集轨迹词库（8192候选，K-means聚类专家轨迹），每个候选含8个2Hz关键点；沿候选轨迹采样BEV特征，经注意力机制聚合为上下文感知轨迹嵌入；通过可变形交叉注意力整合代理特征以精炼轨迹嵌入；每个嵌入经MLP解码为残差偏移，输出最终预测轨迹。</p>\n<ul><li>度量引导对齐</li></ul>\n<p> - 轨迹评分器：并行MLP头回归八项规则化指标（EP连续值[0,1]；DAC、TLC、TTC、LK、HC二值{0,1}；NC、DDC三值{0,0.5,1}）；采用加权复合损失（MSE、BCE、Cross-Entropy）训练。</p>\n<ul><li>后处理</li></ul>\n<p> - 先用全景驾驶感知模型预测可行驶区域，剔除不合法候选；随后按加权分数排序与选择；E2E与VLA两分支离线集成。</p>\n<h1>论文使用数据集和训练资源</h1>\n\n<ul><li>数据集</li></ul>\n<p> - NavsimV2（navtrain划分）用于训练；Navhard两阶段测试与公开榜单用于评估。</p>\n<ul><li>训练资源与配置</li></ul>\n<p> - VLA模块：Vicuna-v1.5-7B + CLIP ViT-L/14；AdamW优化器，余弦学习率，初值1e-5；dropout 0.05；1个epoch；批量8；8×NVIDIA A800 GPU。</p>\n<p> - E2E与评分器联合训练：BevFormer + VoVNet-99；代理检测/分割/规划/评分多损失联合（权重10/1/14/20/14）；30个epoch；批量8；初始学习率1e-4；4×A800 GPU；AdamW与余弦调度（与VLA一致）。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<ul><li>评估环境</li></ul>\n<p> - ICCV 2025 Autonomous Grand Challenge（Navhard两阶段测试与公开leaderboard）。</p>\n<ul><li>评估指标与结果</li></ul>\n<p> - 主要指标：扩展预测驾驶员模型得分（EPDMS）。</p>\n<p> - 分支成绩：VLA分支48.0；E2E分支43.7。</p>\n<p> - 最终模型（离线集成）EPDMS：49.1238。</p>\n<p> - 分项指标（公开leaderboard，两阶段）：</p>\n<p> - No-At-Fault Collisions：98.2143（Stage 1）、88.7709（Stage 2）</p>\n<p> - Drivable Area Compliance：98.5714（Stage 1）、95.3235（Stage 2）</p>\n<p> - Driving Direction Compliance：100（Stage 1）、97.2196（Stage 2）</p>\n<p> - Traffic Light Compliance：99.2857（Stage 1）、98.1711（Stage 2）</p>\n<p> - Ego Progress：79.5117（Stage 1）、73.4289（Stage 2）</p>\n<p> - Time-To-Collision within bound：98.5714（Stage 1）、87.9888（Stage 2）</p>\n<p> - Lane Keeping：95（Stage 1）、59.4454（Stage 2）</p>\n<p> - History Comfort：92.8571（Stage 1）、98.9833（Stage 2）</p>\n<p> - Two-frame Extended Comfort：50（Stage 1）、52.9822（Stage 2）</p>"
  },
  {
    "date": "2025-10-20",
    "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
    "link": "http://arxiv.org/abs/2510.17111",
    "summary_markdown": "## 论文研究单位\n中国科学院自动化研究所、中国科学院大学、AiRiA、南京信息工程大学\n## 论文概述\n这是一篇关于高效视觉-语言-动作（VLA）模型在具身操作中的系统性综述论文。VLA模型通过将自然语言指令和视觉观察映射到机器人动作，实现具身控制。尽管VLA系统功能强大，但其巨大的计算和内存需求与边缘平台的实时性能要求存在显著冲突。本综述首次从效率角度对VLA模型进行系统性回顾，涵盖模型架构、感知特征、动作生成和训练/推理策略四个维度，提供了主流效率优化方法的分类和比较分析。\n## 论文核心贡献点\n1. 首次专门针对高效VLA的系统性综述，将效率改进技术分为四个维度：模型架构、感知特征、动作生成、训练/推理机制\n2. 基于分类法总结主流效率优化方法，分析各种技术的优劣势和适用场景\n3. 讨论VLA模型未来发展趋势，强调需要在新兴趋势下进一步改进效率的优先方向\n4. 提供实用的参考，支撑开发既高效又具备通用可靠具身智能的VLA系统\n## 论文方法描述\n**架构层面：**\n- **静态骨干选择**：使用轻量级模型如Mamba、SmolVLA（2.24亿参数）、NORA（30亿参数）替代大型预训练模型\n- **动态计算路径**：包括层剪枝（FLOWER）、早期退出（DEER-VLA）、专家混合（MoLE-VLA）、相似性跳过（Efficient-VLA）\n- **双系统设计**：结合慢系统（大型多模态语言模型）处理复杂推理和快系统（轻量模型）实现快速响应\n\n**感知特征优化：**\n- **选择性特征处理**：基于注意力分数、任务相关性、空间结构的令牌修剪方法\n- **时间共享重用**：利用帧间相似性重用KV缓存、高层表示和推理结果\n\n**动作生成策略：**\n- **原始动作生成**：动作块分块、令牌压缩（FAST）、离散化（VOTE）\n- **推理感知动作**：语言链式思维推理（ECoT）、视觉推理（UniPi、VPP、CoT-VLA）\n\n**训练推理优化：**\n- **训练效率**：参数高效微调（LoRA）、知识蒸馏、量化感知训练、剪枝恢复\n- **推理效率**：非自回归解码、投机解码（Spec-VLA）、并行细化（PD-VLA）\n## 论文使用数据集和训练资源\n- **Open X-Embodiment (OXE)**：大规模真实世界机器人数据集\n- **DROID**：大规模野外机器人操作数据集\n- **各种模拟数据集**：用于预训练和Sim-to-Real迁移\n## 论文使用的评估环境和评估指标\n**评估维度：**\n- **资源效率**：模型规模、推理延迟、内存占用、训练时间、能耗\n- **性能鲁棒性**：任务成功率、长时序稳定性、分布外泛化能力、环境扰动恢复\n- **可解释性**：人类可读的推理过程、决策归因机制、透明度\n\n**技术实现：**\n- 统一的云边部署架构\n- 标准化硬件配置报告\n- 开源评估框架和基准测试\n- 多任务多场景的公开数据集和仿真环境",
    "summary_html": "<h2>论文研究单位</h2>\n<p>中国科学院自动化研究所、中国科学院大学、AiRiA、南京信息工程大学</p>\n<h2>论文概述</h2>\n<p>这是一篇关于高效视觉-语言-动作（VLA）模型在具身操作中的系统性综述论文。VLA模型通过将自然语言指令和视觉观察映射到机器人动作，实现具身控制。尽管VLA系统功能强大，但其巨大的计算和内存需求与边缘平台的实时性能要求存在显著冲突。本综述首次从效率角度对VLA模型进行系统性回顾，涵盖模型架构、感知特征、动作生成和训练/推理策略四个维度，提供了主流效率优化方法的分类和比较分析。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>首次专门针对高效VLA的系统性综述，将效率改进技术分为四个维度：模型架构、感知特征、动作生成、训练/推理机制</li><li>基于分类法总结主流效率优化方法，分析各种技术的优劣势和适用场景</li><li>讨论VLA模型未来发展趋势，强调需要在新兴趋势下进一步改进效率的优先方向</li><li>提供实用的参考，支撑开发既高效又具备通用可靠具身智能的VLA系统</li></ol>\n<h2>论文方法描述</h2>\n<p><strong>架构层面：</strong></p>\n<ul><li><strong>静态骨干选择</strong>：使用轻量级模型如Mamba、SmolVLA（2.24亿参数）、NORA（30亿参数）替代大型预训练模型</li><li><strong>动态计算路径</strong>：包括层剪枝（FLOWER）、早期退出（DEER-VLA）、专家混合（MoLE-VLA）、相似性跳过（Efficient-VLA）</li><li><strong>双系统设计</strong>：结合慢系统（大型多模态语言模型）处理复杂推理和快系统（轻量模型）实现快速响应</li></ul>\n\n<p><strong>感知特征优化：</strong></p>\n<ul><li><strong>选择性特征处理</strong>：基于注意力分数、任务相关性、空间结构的令牌修剪方法</li><li><strong>时间共享重用</strong>：利用帧间相似性重用KV缓存、高层表示和推理结果</li></ul>\n\n<p><strong>动作生成策略：</strong></p>\n<ul><li><strong>原始动作生成</strong>：动作块分块、令牌压缩（FAST）、离散化（VOTE）</li><li><strong>推理感知动作</strong>：语言链式思维推理（ECoT）、视觉推理（UniPi、VPP、CoT-VLA）</li></ul>\n\n<p><strong>训练推理优化：</strong></p>\n<ul><li><strong>训练效率</strong>：参数高效微调（LoRA）、知识蒸馏、量化感知训练、剪枝恢复</li><li><strong>推理效率</strong>：非自回归解码、投机解码（Spec-VLA）、并行细化（PD-VLA）</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>Open X-Embodiment (OXE)</strong>：大规模真实世界机器人数据集</li><li><strong>DROID</strong>：大规模野外机器人操作数据集</li><li><strong>各种模拟数据集</strong>：用于预训练和Sim-to-Real迁移</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<p><strong>评估维度：</strong></p>\n<ul><li><strong>资源效率</strong>：模型规模、推理延迟、内存占用、训练时间、能耗</li><li><strong>性能鲁棒性</strong>：任务成功率、长时序稳定性、分布外泛化能力、环境扰动恢复</li><li><strong>可解释性</strong>：人类可读的推理过程、决策归因机制、透明度</li></ul>\n\n<p><strong>技术实现：</strong></p>\n<ul><li>统一的云边部署架构</li><li>标准化硬件配置报告</li><li>开源评估框架和基准测试</li><li>多任务多场景的公开数据集和仿真环境</li></ul>"
  },
  {
    "date": "2025-10-18",
    "title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation",
    "link": "http://arxiv.org/abs/2510.16617",
    "summary_markdown": "## 论文研究单位\nUniversity of Texas at Austin（作者来自德州大学奥斯汀分校）\n## 论文概述\n现有的大规模视觉-语言-动作（VLA）模型在跨实验室、环境与任务上的泛化能力不足，通常需要在新场景中收集一定量的专家演示并进行参数微调才能奏效。本文提出 MoS-VLA（Mixture of Skills VLA），通过“技能混合”的方式将机器人策略表示为有限基函数的线性组合，从而构建结构化的技能空间；在线推理时，仅需一次专家演示，通过最小化 L1 行动误差的轻量凸优化（线性规划）得到系数，无需梯度回传即可在新环境中快速适配，显著降低适配开销。论文基于 OpenVLA 与 Open X-Embodiment 数据进行训练，并在仿真与真实机械臂任务上验证效果。\n## 论文核心贡献点\n- 提出一种一次样本、上下文内的 VLA 适配方法：在推理阶段只需一次专家演示，无需梯度更新，计算与内存开销极低（仅前向传播），在普通 GPU（如 RTX 3090）上几秒即可完成校准。\n- 首次将函数编码器（Function Encoders）应用于亿级参数与多模态机器人数据，验证其在 VLA 场景的可行性与可扩展性。\n- 在结构化技能空间中缓解“混合上下文过拟合”：通过学习基函数并在系数空间区分不同上下文，提升了分布外数据集与实际环境中的表现。\n- 构建了 L1 误差驱动的凸优化校准流程（线性规划），强调鲁棒性与对抗离群点的能力。\n- 给出端到端实现细节与可扩展训练策略，支持分布式并行与缓冲校准，保证可复现性与工程可行性。\n## 论文方法描述\n- 问题形式化：机器人策略空间为状态 S = I × T 到行动 A 的映射；上下文 c（如光照、相机位姿、机器人形态等）影响专家策略但不可直接观测；将不同上下文下的专家策略集合视为函数空间。\n- 主思想：将上下文相关的专家策略表示为函数空间中的函数，学习一组神经网络的基函数 {g1, ..., gk}，任何上下文策略 π_exp^c 可表示为这些基函数的线性组合。新任务在线适配时，通过一次专家演示 τ_exp^c，最小化 L1 行动误差，求解线性规划得到系数 α^c，再以线性组合给出动作预测。\n- 架构实现：在 OpenVLA 主干的基础上替换语言模型输出头为 k 个独立的“基函数动作头”，共享主干特征；采用并行解码策略同时预测多个行动维度；降低参数开销并保持 Transformer 输入的变长处理能力。\n- 训练流程：采用修正的函数编码器算法，支持 L1 误差与 Banach 空间设置；使用 LoRA 进行高效微调；维护校准缓冲（每数据集 512 样本），每若干步更新一次上下文系数并在训练中广播；使用分布式数据并行（DDP）在多节点（32×GH200）上进行训练。\n## 论文使用数据集和训练资源\n- 数据集：Open X-Embodiment（RT-X）中的 Magic Soup Plus 数据混合（同 OpenVLA），包含多个实验室的轨迹数据；包含分布内与分布外（OOD）子集。\n- 训练资源：32 个计算节点，每节点 1×GH200；全局批次 320；训练步数约 5000（耗时约 24 小时）；Adam 优化器，学习率 1e-4，热身 10 步。\n- 适配资源（在线）：单条专家演示；在普通 GPU（如 RTX 3090）上仅需几秒完成系数求解。\n## 论文使用的评估环境和评估指标\n- 数据集评估：在 27 个训练子集与 5 个 OOD 子集上评估动作预测误差（L1 误差），对比基线（OpenVLA）。\n- 仿真实验（Robosuite）：两项任务——块搬运与开门；每任务 m=20 次试验。\n- 真实机械臂实验（Franka Emika Panda）：三项任务——目标抵达、块搬运、笔插入；每任务 m=10 次试验；单 RGB 前置相机环境，部分运动方向限制以简化深度估计。\n- 指标：任务成功率（%），同时观察与讨论适配在短-horizon 任务上的有效性以及在更长时序或更高随机性场景中的局限。\n## 关键结果\n- 在 5 个 OOD 数据集上，MoS-VLA 的 L1 行动误差全面低于基线；在训练子集中也有 18/27 的优势。\n- 仿真与真实实验：OpenVLA 在未见环境与新任务上成功率为 0%；MoS-VLA 在一次演示校准后，仿真任务（块搬运、开门）达 70–75% 成功率，真实任务（目标抵达、块搬运、笔插入）达 100% 成功率。\n- 系数可视化显示相似实验室的上下文在技能空间内聚类，说明学习到的技能空间能够捕捉环境与任务的结构性差异。\n## 工程与复现要点\n- 架构：在 OpenVLA 主干上引入 k=16 个基函数动作头，采用并行解码直接输出标量动作。\n- 校准：维护分布式校准缓冲，每 16 步进行一次系数广播与更新；采用 CVXPY 求解 L1 线性规划。\n- 资源友好：在线适配不需梯度回传，推理阶段计算与内存开销与专家轨迹长度无关（常数复杂度）。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>University of Texas at Austin（作者来自德州大学奥斯汀分校）</p>\n<h2>论文概述</h2>\n<p>现有的大规模视觉-语言-动作（VLA）模型在跨实验室、环境与任务上的泛化能力不足，通常需要在新场景中收集一定量的专家演示并进行参数微调才能奏效。本文提出 MoS-VLA（Mixture of Skills VLA），通过“技能混合”的方式将机器人策略表示为有限基函数的线性组合，从而构建结构化的技能空间；在线推理时，仅需一次专家演示，通过最小化 L1 行动误差的轻量凸优化（线性规划）得到系数，无需梯度回传即可在新环境中快速适配，显著降低适配开销。论文基于 OpenVLA 与 Open X-Embodiment 数据进行训练，并在仿真与真实机械臂任务上验证效果。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出一种一次样本、上下文内的 VLA 适配方法：在推理阶段只需一次专家演示，无需梯度更新，计算与内存开销极低（仅前向传播），在普通 GPU（如 RTX 3090）上几秒即可完成校准。</li><li>首次将函数编码器（Function Encoders）应用于亿级参数与多模态机器人数据，验证其在 VLA 场景的可行性与可扩展性。</li><li>在结构化技能空间中缓解“混合上下文过拟合”：通过学习基函数并在系数空间区分不同上下文，提升了分布外数据集与实际环境中的表现。</li><li>构建了 L1 误差驱动的凸优化校准流程（线性规划），强调鲁棒性与对抗离群点的能力。</li><li>给出端到端实现细节与可扩展训练策略，支持分布式并行与缓冲校准，保证可复现性与工程可行性。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>问题形式化：机器人策略空间为状态 S = I × T 到行动 A 的映射；上下文 c（如光照、相机位姿、机器人形态等）影响专家策略但不可直接观测；将不同上下文下的专家策略集合视为函数空间。</li><li>主思想：将上下文相关的专家策略表示为函数空间中的函数，学习一组神经网络的基函数 {g1, ..., gk}，任何上下文策略 π_exp^c 可表示为这些基函数的线性组合。新任务在线适配时，通过一次专家演示 τ_exp^c，最小化 L1 行动误差，求解线性规划得到系数 α^c，再以线性组合给出动作预测。</li><li>架构实现：在 OpenVLA 主干的基础上替换语言模型输出头为 k 个独立的“基函数动作头”，共享主干特征；采用并行解码策略同时预测多个行动维度；降低参数开销并保持 Transformer 输入的变长处理能力。</li><li>训练流程：采用修正的函数编码器算法，支持 L1 误差与 Banach 空间设置；使用 LoRA 进行高效微调；维护校准缓冲（每数据集 512 样本），每若干步更新一次上下文系数并在训练中广播；使用分布式数据并行（DDP）在多节点（32×GH200）上进行训练。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据集：Open X-Embodiment（RT-X）中的 Magic Soup Plus 数据混合（同 OpenVLA），包含多个实验室的轨迹数据；包含分布内与分布外（OOD）子集。</li><li>训练资源：32 个计算节点，每节点 1×GH200；全局批次 320；训练步数约 5000（耗时约 24 小时）；Adam 优化器，学习率 1e-4，热身 10 步。</li><li>适配资源（在线）：单条专家演示；在普通 GPU（如 RTX 3090）上仅需几秒完成系数求解。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>数据集评估：在 27 个训练子集与 5 个 OOD 子集上评估动作预测误差（L1 误差），对比基线（OpenVLA）。</li><li>仿真实验（Robosuite）：两项任务——块搬运与开门；每任务 m=20 次试验。</li><li>真实机械臂实验（Franka Emika Panda）：三项任务——目标抵达、块搬运、笔插入；每任务 m=10 次试验；单 RGB 前置相机环境，部分运动方向限制以简化深度估计。</li><li>指标：任务成功率（%），同时观察与讨论适配在短-horizon 任务上的有效性以及在更长时序或更高随机性场景中的局限。</li></ul>\n<h2>关键结果</h2>\n<ul><li>在 5 个 OOD 数据集上，MoS-VLA 的 L1 行动误差全面低于基线；在训练子集中也有 18/27 的优势。</li><li>仿真与真实实验：OpenVLA 在未见环境与新任务上成功率为 0%；MoS-VLA 在一次演示校准后，仿真任务（块搬运、开门）达 70–75% 成功率，真实任务（目标抵达、块搬运、笔插入）达 100% 成功率。</li><li>系数可视化显示相似实验室的上下文在技能空间内聚类，说明学习到的技能空间能够捕捉环境与任务的结构性差异。</li></ul>\n<h2>工程与复现要点</h2>\n<ul><li>架构：在 OpenVLA 主干上引入 k=16 个基函数动作头，采用并行解码直接输出标量动作。</li><li>校准：维护分布式校准缓冲，每 16 步进行一次系数广播与更新；采用 CVXPY 求解 L1 线性规划。</li><li>资源友好：在线适配不需梯度回传，推理阶段计算与内存开销与专家轨迹长度无关（常数复杂度）。</li></ul>"
  },
  {
    "date": "2025-10-17",
    "title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?",
    "link": "http://arxiv.org/abs/2510.16263",
    "summary_markdown": "# 论文研究单位\nDepartment of Computer & Data Sciences, Case Western Reserve University\n# 论文概述\n论文提出NEBULA，一个针对单臂操作的统一生态，用于视觉-语言-动作（VLA）智能体的诊断式与可复现评估。NEBULA以标准化API和大规模聚合数据集为基础，构建“双轴评估协议”：一条轴线进行能力测试（精细技能诊断），另一条轴线进行压力测试（稳健性刻画）。论文同时对当前SOTA模型开展系统评测，指出空间推理与动态适应是普遍短板，并强调推理速度与低延迟对动态任务的关键作用。\n# 论文核心贡献点\n- 标准化API与统一数据格式，整合ManiSkill、LeRobot等分散数据集，支持跨数据集训练与公平对比；提供大规模聚合数据集（Alpha与Beta），并配套PyTorch与TFRecord接口、模型适配器。\n- 首次提出双轴评估协议：将“能力”（六类技能：控制、感知、语言、动态适应、空间推理、稳健性/泛化）与“压力”分离，能力轴采用可控变量隔离与分层难度，压力轴对推理频率、延迟、稳定性、适应性、资源进行单一指标测评。\n- 全面基准评测：揭示当前VLA在空间推理和动态适应上的系统性薄弱与稳健性不足，显示传统“任务成功率”掩蔽关键失败模式。\n- 诊断输出与因子隔离：提供雷达图等可视化绩效摘要，验证可控变量隔离与分层难度的有效性，支持解释性错误定位。\n# 论文方法描述\n- 统一数据平台与API：基于SAPIEN与ManiSkill3收集与标注多模态时序数据（RGB/深度/分割、状态、动作、语言指令），形成标准化Episode与Step格式；提供SDK与查询引擎、训练/评测工具链及PyTorch/TF兼容适配器。\n- 能力测试任务：按六类技能设计独立任务模板，每类技能分为Easy/Medium/Hard三个难度层级；遵循“可控变量隔离”原则，使性能变化可唯一归因于被测技能。\n- 压力测试任务：四个单指标压力测试（推理频率、延迟、稳定性分数、适应性），每项分为三个压力等级（v1–v3），用于刻画系统在不同运营约束下的退化曲线与稳健边界。\n- 评测与诊断输出：统一数据加载与评测协议，雷达图与可视化展示；进行因子隔离有效性验证，比较隔离与非隔离场景下模型表现差异。\n# 论文使用数据集和训练资源\n- 数据集：NEBULA-Alpha与Beta两套版本，来源于专家轨迹（运动规划）与人机演示；Alpha包含超过5.4万演示，覆盖控制/感知/语言/动态/空间五类能力家族（稳健性家族仅用于评测，不入训练集）；Beta为约10%规模，部分高难度任务含人工遥操作。\n- 数据形式：多模态观测（6视角摄像头RGB、深度、分割）、本体感受、动作与成功标签、语言指令；提供PyTorch与TFRecord格式，附带模型特定适配器。\n- 训练资源：主要评测在Alpha数据集上进行微调；训练使用统一数据加载与原版训练协议；具体硬件规格与资源用量未详述。\n# 论文使用的评估环境和评估指标\n- 评估环境：基于SAPIEN的定制仿真平台（ManiSkill3框架），与NEBULA统一API与数据格式兼容。\n- 能力轴指标：按六类技能（控制、感知、语言、动态适应、空间推理、稳健性/泛化）报告分层成功率；提供能力雷达图等诊断性可视化。\n- 压力轴指标：推理频率（Hz）、延迟（ms）、稳定性分数（基于相邻动作差的指数衰减，归一化至[0,1]）、适应性（在目标切换/指令变更等场景下的成功率）；每项指标分为三档压力等级（v1–v3）。\n- 诊断与因子隔离：比较隔离与非隔离场景的误差来源，验证能力评估的可控性与有效性。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Department of Computer & Data Sciences, Case Western Reserve University</p>\n<h1>论文概述</h1>\n<p>论文提出NEBULA，一个针对单臂操作的统一生态，用于视觉-语言-动作（VLA）智能体的诊断式与可复现评估。NEBULA以标准化API和大规模聚合数据集为基础，构建“双轴评估协议”：一条轴线进行能力测试（精细技能诊断），另一条轴线进行压力测试（稳健性刻画）。论文同时对当前SOTA模型开展系统评测，指出空间推理与动态适应是普遍短板，并强调推理速度与低延迟对动态任务的关键作用。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>标准化API与统一数据格式，整合ManiSkill、LeRobot等分散数据集，支持跨数据集训练与公平对比；提供大规模聚合数据集（Alpha与Beta），并配套PyTorch与TFRecord接口、模型适配器。</li><li>首次提出双轴评估协议：将“能力”（六类技能：控制、感知、语言、动态适应、空间推理、稳健性/泛化）与“压力”分离，能力轴采用可控变量隔离与分层难度，压力轴对推理频率、延迟、稳定性、适应性、资源进行单一指标测评。</li><li>全面基准评测：揭示当前VLA在空间推理和动态适应上的系统性薄弱与稳健性不足，显示传统“任务成功率”掩蔽关键失败模式。</li><li>诊断输出与因子隔离：提供雷达图等可视化绩效摘要，验证可控变量隔离与分层难度的有效性，支持解释性错误定位。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>统一数据平台与API：基于SAPIEN与ManiSkill3收集与标注多模态时序数据（RGB/深度/分割、状态、动作、语言指令），形成标准化Episode与Step格式；提供SDK与查询引擎、训练/评测工具链及PyTorch/TF兼容适配器。</li><li>能力测试任务：按六类技能设计独立任务模板，每类技能分为Easy/Medium/Hard三个难度层级；遵循“可控变量隔离”原则，使性能变化可唯一归因于被测技能。</li><li>压力测试任务：四个单指标压力测试（推理频率、延迟、稳定性分数、适应性），每项分为三个压力等级（v1–v3），用于刻画系统在不同运营约束下的退化曲线与稳健边界。</li><li>评测与诊断输出：统一数据加载与评测协议，雷达图与可视化展示；进行因子隔离有效性验证，比较隔离与非隔离场景下模型表现差异。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：NEBULA-Alpha与Beta两套版本，来源于专家轨迹（运动规划）与人机演示；Alpha包含超过5.4万演示，覆盖控制/感知/语言/动态/空间五类能力家族（稳健性家族仅用于评测，不入训练集）；Beta为约10%规模，部分高难度任务含人工遥操作。</li><li>数据形式：多模态观测（6视角摄像头RGB、深度、分割）、本体感受、动作与成功标签、语言指令；提供PyTorch与TFRecord格式，附带模型特定适配器。</li><li>训练资源：主要评测在Alpha数据集上进行微调；训练使用统一数据加载与原版训练协议；具体硬件规格与资源用量未详述。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：基于SAPIEN的定制仿真平台（ManiSkill3框架），与NEBULA统一API与数据格式兼容。</li><li>能力轴指标：按六类技能（控制、感知、语言、动态适应、空间推理、稳健性/泛化）报告分层成功率；提供能力雷达图等诊断性可视化。</li><li>压力轴指标：推理频率（Hz）、延迟（ms）、稳定性分数（基于相邻动作差的指数衰减，归一化至[0,1]）、适应性（在目标切换/指令变更等场景下的成功率）；每项指标分为三档压力等级（v1–v3）。</li><li>诊断与因子隔离：比较隔离与非隔离场景的误差来源，验证能力评估的可控性与有效性。</li></ul>"
  },
  {
    "date": "2025-10-17",
    "title": "VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving",
    "link": "http://arxiv.org/abs/2510.15446",
    "summary_markdown": "## 论文研究单位\n- 清华大学苏州汽车研究院\n- 作者实习单位为清华大学苏州汽车研究院\n## 论文概述\n论文提出 VDRive 框架，将视觉-语言-动作模型（VLA）与扩散策略（Diffusion Policy）结合，用于端到端自动驾驶。框架以离散化的状态-动作表示为核心，通过“强化的模态对齐”桥接高维传感器空间与低维动作空间：在上下文层面，VLA 通过视觉 token 预训练预测未来观测；几何层面，通过对 VLA 的强化学习微调，使其基于当前驾驶条件预测轨迹与动作。随后，扩散策略头在 VLA 提供当前与预测状态 token 的条件下生成层次化动作与轨迹，并由动态引导的精炼头优化轨迹输出。方法采用全离线强化学习范式，在 nuScenes 开环规划与 Bench2Drive 闭环基准上均取得最先进性能。\n## 论文核心贡献点\n- 提出 VDRive 框架：通过 VLA 预测未来状态 token 与扩散策略头联合训练，实现上下文与几何对齐的驾驶决策。\n- 构建强化学习偏好数据集：基于 nuScenes 与 Bench2Drive 构造“选择/拒绝”视觉-动作对用于 VLA 微调。\n- 设计离线奖励数据：结合规则奖励与专家 VLM（Qwen2.5-VL-72B）评分，训练扩散策略头并构建 actor-critic 强化学习闭环。\n- 在多基准上实现最先进结果：在 nuScenes 开环评估与 Bench2Drive 闭环评估中均显著提升。\n## 论文方法描述\n- 离散化表示（CVQ-VAE）：训练条件向量量化变分自编码器，以轨迹为条件，将原始/分割图像编码为离散观测 token，加入 VLA 分词器词表。\n- VLA 强化微调：利用偏好数据训练 VLA 生成未来观测 token、动作信号与导航命令，结合选择/拒绝样本进行偏好对齐。\n- 扩散策略头学习：在离线状态-动作-奖励-下一状态数据集上，训练扩散模型作为 actor 网络、最小化生成动作与真实动作的重建误差，并通过 critic 网络提供价值反馈，实现累积奖励最大化。\n- 动态引导的精炼头：将 VLA 与扩散策略的异步动作预测作为条件，通过 Transformer 编码器与 MLP 映射融合位置嵌入与动作特征，最终解码优化轨迹。\n## 论文使用数据集和训练资源\n- 数据：nuScenes（1000 场景，6 相机/5 雷达/1 激光雷达，2 Hz）与 Bench2Drive（CARLA 仿真）用于开/闭环评估与偏好/奖励数据集构建；利用 Vista 生成合成风险场景。\n- 预训练/微调：视觉 token 预训练（CVQ-VAE + VLA tokenizer）；VLA 强化微调（偏好对齐）；扩散策略离线强化学习（actor-critic）。\n- 模型：VLA 基于 InternVL3-8B；扩散策略头使用扩散模型；奖励评估采用 Qwen2.5-VL-72B。\n## 论文使用的评估环境和评估指标\n- 开环评估（nuScenes）：平均 L2 误差与碰撞率；在 1s/2s/3s 时间步与总体上报告指标。\n- 闭环评估（Bench2Drive）：Driving Score、Success Rate、Efficiency、Comfortness；并报告多能力指标（并线、超车、紧急制动、让行、交通标志）。\n- 消融实验：Bench2Drive-mini 上对比不同数据构造与不同精炼模块（Transformer/LSTM/GRU）设计。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>清华大学苏州汽车研究院</li><li>作者实习单位为清华大学苏州汽车研究院</li></ul>\n<h2>论文概述</h2>\n<p>论文提出 VDRive 框架，将视觉-语言-动作模型（VLA）与扩散策略（Diffusion Policy）结合，用于端到端自动驾驶。框架以离散化的状态-动作表示为核心，通过“强化的模态对齐”桥接高维传感器空间与低维动作空间：在上下文层面，VLA 通过视觉 token 预训练预测未来观测；几何层面，通过对 VLA 的强化学习微调，使其基于当前驾驶条件预测轨迹与动作。随后，扩散策略头在 VLA 提供当前与预测状态 token 的条件下生成层次化动作与轨迹，并由动态引导的精炼头优化轨迹输出。方法采用全离线强化学习范式，在 nuScenes 开环规划与 Bench2Drive 闭环基准上均取得最先进性能。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出 VDRive 框架：通过 VLA 预测未来状态 token 与扩散策略头联合训练，实现上下文与几何对齐的驾驶决策。</li><li>构建强化学习偏好数据集：基于 nuScenes 与 Bench2Drive 构造“选择/拒绝”视觉-动作对用于 VLA 微调。</li><li>设计离线奖励数据：结合规则奖励与专家 VLM（Qwen2.5-VL-72B）评分，训练扩散策略头并构建 actor-critic 强化学习闭环。</li><li>在多基准上实现最先进结果：在 nuScenes 开环评估与 Bench2Drive 闭环评估中均显著提升。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>离散化表示（CVQ-VAE）：训练条件向量量化变分自编码器，以轨迹为条件，将原始/分割图像编码为离散观测 token，加入 VLA 分词器词表。</li><li>VLA 强化微调：利用偏好数据训练 VLA 生成未来观测 token、动作信号与导航命令，结合选择/拒绝样本进行偏好对齐。</li><li>扩散策略头学习：在离线状态-动作-奖励-下一状态数据集上，训练扩散模型作为 actor 网络、最小化生成动作与真实动作的重建误差，并通过 critic 网络提供价值反馈，实现累积奖励最大化。</li><li>动态引导的精炼头：将 VLA 与扩散策略的异步动作预测作为条件，通过 Transformer 编码器与 MLP 映射融合位置嵌入与动作特征，最终解码优化轨迹。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据：nuScenes（1000 场景，6 相机/5 雷达/1 激光雷达，2 Hz）与 Bench2Drive（CARLA 仿真）用于开/闭环评估与偏好/奖励数据集构建；利用 Vista 生成合成风险场景。</li><li>预训练/微调：视觉 token 预训练（CVQ-VAE + VLA tokenizer）；VLA 强化微调（偏好对齐）；扩散策略离线强化学习（actor-critic）。</li><li>模型：VLA 基于 InternVL3-8B；扩散策略头使用扩散模型；奖励评估采用 Qwen2.5-VL-72B。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>开环评估（nuScenes）：平均 L2 误差与碰撞率；在 1s/2s/3s 时间步与总体上报告指标。</li><li>闭环评估（Bench2Drive）：Driving Score、Success Rate、Efficiency、Comfortness；并报告多能力指标（并线、超车、紧急制动、让行、交通标志）。</li><li>消融实验：Bench2Drive-mini 上对比不同数据构造与不同精炼模块（Transformer/LSTM/GRU）设计。</li></ul>"
  },
  {
    "date": "2025-10-16",
    "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
    "link": "http://arxiv.org/abs/2510.14968",
    "summary_markdown": "### 论文研究单位\n- Mingxuan Yan、Yuping Wang、Jiachen Li：加州大学河滨分校（University of California, Riverside）\n- Yuping Wang（共同作者）：密歇根大学（University of Michigan）\n- Zechun Liu：Meta AI\n### 论文概述\n论文针对长程任务中的层次化视觉-语言-行动（VLA）框架问题，提出检索式演示分解器（RDD）。现有VLM规划器微调依赖人工或启发式子任务分解，易导致子任务与低层视觉运动策略训练数据不一致，降低任务性能。RDD通过视觉特征检索自动分解演示，将子任务与策略训练数据对齐，使用动态规划优化分解策略，实现计划器与策略协同。\n### 论文核心贡献点\n- 首次协调高层规划器与低层视觉运动策略，通过生成对齐的规划器微调数据集提升长程任务性能。\n- 提出RDD框架，训练自由的检索式演示分解方法，形式化为最优分割问题，用动态规划高效求解（含理论分析）。\n- 在仿真和真实基准评估中，RDD优于启发式分解器（如UVD），展现跨设置鲁棒性。\n### 论文方法描述\n- **问题建模**：将演示分解形式化为最优分割问题（公式3.1），最大化分割策略与策略训练数据的相似性。\n- **动态规划求解**：利用最优性原理，将复杂度从O(2^N)降至O(N^2)（有界区间时O(N)）（算法1）。\n- **区间评分函数**：定义区间相似性（公式3.3），结合视觉编码器（如LIV）、近似最近邻检索（Annoy）和时间持续差异。\n- **视觉特征**：使用视觉编码器嵌入间隔（公式3.5），结合开始和结束帧，计算间隔相似度（公式3.6）。\n- **OOD处理**：扩展方法处理分布外子任务，结合检索和启发式（公式3.7-3.8）。\n### 论文使用数据集和训练资源\n- **数据集**：RLBench机器人操作基准，使用训练集1908个演示（分解为12700子任务区间）。\n- **视觉编码器**：LIV（主用），对比R3M、VIP、VC-1、CLIP、DINOv2、ResNet。\n- **检索方法**：Annoy近似最近邻搜索（Angular距离）。\n- **规划器**：LLaVA-based VLM（llama3-llava-next-8B），LoRA微调（rank 128，scale 256）。\n- **计算资源**：4个NVIDIA 6000 Ada GPU，微调约5分钟。\n### 论文使用的评估环境和评估指标\n- **评估环境**：RLBench仿真基准，13个任务（成功率高>35%）。\n- **评估指标**：\n - 多任务成功率（%，平均值和标准差）。\n - 平均排名（↓）。\n- **比较基线**：Expert（启发式专家）、UVD（视觉分解器）、Uniform（均匀分割）、w/o Finetune（不微调）。\n- **实验设置**：每个任务3个演示用于微调；结果平均10次随机种子。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>Mingxuan Yan、Yuping Wang、Jiachen Li：加州大学河滨分校（University of California, Riverside）</li><li>Yuping Wang（共同作者）：密歇根大学（University of Michigan）</li><li>Zechun Liu：Meta AI</li></ul>\n<h3>论文概述</h3>\n<p>论文针对长程任务中的层次化视觉-语言-行动（VLA）框架问题，提出检索式演示分解器（RDD）。现有VLM规划器微调依赖人工或启发式子任务分解，易导致子任务与低层视觉运动策略训练数据不一致，降低任务性能。RDD通过视觉特征检索自动分解演示，将子任务与策略训练数据对齐，使用动态规划优化分解策略，实现计划器与策略协同。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首次协调高层规划器与低层视觉运动策略，通过生成对齐的规划器微调数据集提升长程任务性能。</li><li>提出RDD框架，训练自由的检索式演示分解方法，形式化为最优分割问题，用动态规划高效求解（含理论分析）。</li><li>在仿真和真实基准评估中，RDD优于启发式分解器（如UVD），展现跨设置鲁棒性。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>问题建模</strong>：将演示分解形式化为最优分割问题（公式3.1），最大化分割策略与策略训练数据的相似性。</li><li><strong>动态规划求解</strong>：利用最优性原理，将复杂度从O(2^N)降至O(N^2)（有界区间时O(N)）（算法1）。</li><li><strong>区间评分函数</strong>：定义区间相似性（公式3.3），结合视觉编码器（如LIV）、近似最近邻检索（Annoy）和时间持续差异。</li><li><strong>视觉特征</strong>：使用视觉编码器嵌入间隔（公式3.5），结合开始和结束帧，计算间隔相似度（公式3.6）。</li><li><strong>OOD处理</strong>：扩展方法处理分布外子任务，结合检索和启发式（公式3.7-3.8）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：RLBench机器人操作基准，使用训练集1908个演示（分解为12700子任务区间）。</li><li><strong>视觉编码器</strong>：LIV（主用），对比R3M、VIP、VC-1、CLIP、DINOv2、ResNet。</li><li><strong>检索方法</strong>：Annoy近似最近邻搜索（Angular距离）。</li><li><strong>规划器</strong>：LLaVA-based VLM（llama3-llava-next-8B），LoRA微调（rank 128，scale 256）。</li><li><strong>计算资源</strong>：4个NVIDIA 6000 Ada GPU，微调约5分钟。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：RLBench仿真基准，13个任务（成功率高>35%）。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 多任务成功率（%，平均值和标准差）。</p>\n<p> - 平均排名（↓）。</p>\n<ul><li><strong>比较基线</strong>：Expert（启发式专家）、UVD（视觉分解器）、Uniform（均匀分割）、w/o Finetune（不微调）。</li><li><strong>实验设置</strong>：每个任务3个演示用于微调；结果平均10次随机种子。</li></ul>"
  },
  {
    "date": "2025-10-16",
    "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation",
    "link": "http://arxiv.org/abs/2510.14902",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-16",
    "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.14836",
    "summary_markdown": "### 论文研究单位\n- School of Artificial Intelligence, University of the Chinese Academy of Sciences\n- Institute of Automation, Chinese Academy of Sciences\n- Beijing Zhongke Huiling Robot Technology Co.\n### 论文概述\n论文提出QDepth-VLA框架，旨在通过引入量化深度预测作为辅助监督信号，增强视觉-语言-动作（VLA）模型的空间感知和推理能力。现有VLA模型在长时序和精细操作任务中表现不佳，主要因缺乏3D几何理解。QDepth-VLA利用向量量化（VQ-VAE）将深度图转换为离散令牌，并设计专门的深度专家模块预测这些令牌，避免干扰预训练语义对齐。实验在模拟和真实机器人任务上验证了其有效性。\n### 论文核心贡献点\n1. **QDepth-VLA框架**：通过量化深度预测增强VLA模型的空间理解能力。\n2. **深度专家模块**：设计用于预测离散深度令牌，而非回归像素级深度，提供更紧凑的优化友好监督信号。\n3. **性能提升**：在LIBERO基准上相比open π0平均提升7.7%成功率，在Simpler基准上提升6.1%，真实机器人任务提升10.0%。\n### 论文方法描述\n- **深度标注**：使用Video-Depth-Anything (ViDA)生成单目深度估计。\n- **VQ-VAE重建**：预训练VQ-VAE将深度图量化为256个代码向量的离散令牌（网格分辨率16×16）。\n- **QDepth-VLA架构**：基于PaLI-Gemma 3B的VLM，包括预训练VLM、动作专家和深度专家。深度专家采用Transformer架构，输入视觉令牌，预测量化深度令牌。\n- **混合注意力机制**：设计层次化注意力，允许深度令牌关注文本和图像令牌，动作令牌关注所有前序模态，防止干扰预训练VLM。\n- **联合训练程序**：总损失函数为动作损失（CFM损失）加指数衰减的深度损失（交叉熵损失）。优化使用AdamW优化器，余弦退火学习率（200步预热）。\n### 论文使用数据集和训练资源\n- **数据集**：LIBERO（四个子集：Spatial, Object, Goal, Long）、Simpler（Google Robot和WidowX250任务）、真实机器人数据集（Piper Arm任务）。\n- **训练资源**：8×H20 GPU，FSDP并行训练，全局批量大小1024，梯度累积。\n### 论文使用的评估环境和评估指标\n- **评估环境**：模拟环境（LIBERO和Simpler）、真实机器人环境（6-DoF Piper机械臂）。\n- **评估指标**：任务成功率（Success Rate），LIBERO每任务50次rollouts，Simpler每任务240-2400次评估，真实机器人每任务10次试验。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>School of Artificial Intelligence, University of the Chinese Academy of Sciences</li><li>Institute of Automation, Chinese Academy of Sciences</li><li>Beijing Zhongke Huiling Robot Technology Co.</li></ul>\n<h3>论文概述</h3>\n<p>论文提出QDepth-VLA框架，旨在通过引入量化深度预测作为辅助监督信号，增强视觉-语言-动作（VLA）模型的空间感知和推理能力。现有VLA模型在长时序和精细操作任务中表现不佳，主要因缺乏3D几何理解。QDepth-VLA利用向量量化（VQ-VAE）将深度图转换为离散令牌，并设计专门的深度专家模块预测这些令牌，避免干扰预训练语义对齐。实验在模拟和真实机器人任务上验证了其有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>QDepth-VLA框架</strong>：通过量化深度预测增强VLA模型的空间理解能力。</li><li><strong>深度专家模块</strong>：设计用于预测离散深度令牌，而非回归像素级深度，提供更紧凑的优化友好监督信号。</li><li><strong>性能提升</strong>：在LIBERO基准上相比open π0平均提升7.7%成功率，在Simpler基准上提升6.1%，真实机器人任务提升10.0%。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>深度标注</strong>：使用Video-Depth-Anything (ViDA)生成单目深度估计。</li><li><strong>VQ-VAE重建</strong>：预训练VQ-VAE将深度图量化为256个代码向量的离散令牌（网格分辨率16×16）。</li><li><strong>QDepth-VLA架构</strong>：基于PaLI-Gemma 3B的VLM，包括预训练VLM、动作专家和深度专家。深度专家采用Transformer架构，输入视觉令牌，预测量化深度令牌。</li><li><strong>混合注意力机制</strong>：设计层次化注意力，允许深度令牌关注文本和图像令牌，动作令牌关注所有前序模态，防止干扰预训练VLM。</li><li><strong>联合训练程序</strong>：总损失函数为动作损失（CFM损失）加指数衰减的深度损失（交叉熵损失）。优化使用AdamW优化器，余弦退火学习率（200步预热）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：LIBERO（四个子集：Spatial, Object, Goal, Long）、Simpler（Google Robot和WidowX250任务）、真实机器人数据集（Piper Arm任务）。</li><li><strong>训练资源</strong>：8×H20 GPU，FSDP并行训练，全局批量大小1024，梯度累积。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：模拟环境（LIBERO和Simpler）、真实机器人环境（6-DoF Piper机械臂）。</li><li><strong>评估指标</strong>：任务成功率（Success Rate），LIBERO每任务50次rollouts，Simpler每任务240-2400次评估，真实机器人每任务10次试验。</li></ul>"
  },
  {
    "date": "2025-10-16",
    "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning",
    "link": "http://arxiv.org/abs/2510.14300",
    "summary_markdown": "### 论文研究单位\n论文作者来自多个机构，主要包括：\n- 上海交通大学（MoE关键人工智能实验室、自动化与智能传感学院、计算机科学学院）\n- 上海AI实验室\n- 清华大学深圳国际研究生院\n- 香港大学\n- 同济大学\n- D-Robotics\n- 系统控制与信息处理教育部重点实验室（上海交通大学）\n- 信息安全综合管理技术上海市重点实验室\n### 论文概述\n论文针对Vision-Language-Action (VLA)模型在扩展时面临的挑战（如计算资源需求高、实时控制效率与模型容量平衡难题），提出了AdaMoE（Action-Specialized Mixture of Experts）架构。AdaMoE通过继承预训练VLA模型权重，并在动作专家中引入稀疏激活的MoE层来扩展模型容量，同时利用解耦专家选择与权重的创新设计，在保持计算效率的前提下提升性能。论文在仿真基准（LIBERO和RoboTwin 2.0）和真实世界实验中验证了AdaMoE的有效性。\n### 论文核心贡献点\n1. **高效扩展VLA模型**：提出一种从预训练密集VLA模型继承权重并转换为MoE架构的低成本方法。\n2. **解耦专家选择与权重**：引入独立的比例适配器（scale adapter）与路由器（router），解决传统MoE中负载平衡与任务性能冲突的根本问题。\n3. **显著性能提升**：在LIBERO基准提升1.8%，在RoboTwin 2.0域随机化任务提升9.3%，在真实世界实验提升21.5%，验证了实际应用价值。\n4. **专家特化验证**：通过专家激活模式分析，证明专家能捕获有意义的操作行为（如定位、抓取等）。\n### 论文方法描述\n- **架构设计**：基于π₀框架的动作专家由共享专家（处理通用操作）和路由专家（处理特定任务）组成，使用top-k选择（默认k=1）进行稀疏激活。\n- **解耦机制**：路由器（R）负责专家选择以实现负载平衡，比例适配器（S）独立调整专家贡献权重，最终权重为S_i(x) + softmax(R_i(x))。\n- **训练目标**：结合流匹配损失（flow matching loss，用于高频率动作生成）和负载平衡损失（load balancing loss），总损失为L_total = L_τ + λ_balance * L_balance（λ_balance默认0.01）。\n- **专家初始化**：共享专家继承原始FFN权重，路由专家为副本以快速扩展。\n- **推理过程**：使用流匹配进行去噪迭代（从纯噪声生成动作序列）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - LIBERO基准：包含LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long四个任务套件，每个套件含100专家轨迹。\n - RoboTwin 2.0：19个域随机化任务（清洁和随机化环境各100/400轨迹）。\n- **训练资源**：\n - 训练步数：120,000步（批量大小32）。\n - 优化器：AdamW（峰值学习率2.5e-5，路由器学习率5e-5，β1=0.9，β2=0.95）。\n - 其他超参数：动作视野50、专家数4、梯度裁剪1.0、EMA衰减0.99。\n - 计算资源：百度云平台提供计算支持，AgileX Robotics提供机器人平台。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **仿真环境**：LIBERO和RoboTwin 2.0基准（使用域随机化增强鲁棒性）。\n - **真实世界环境**：ALOHA-Agilex双机械臂系统（AgileX Robotics），包括四个任务：堆叠盘子、按铃、调整瓶子、放置杯子。\n- **评估指标**：\n - 主要指标：任务成功率（Success Rate, SR），以百分比表示（每个任务评估50次试验）。\n - 其他分析：专家使用强度（专家激活模式）、消融研究（比较不同架构变体如Vanilla MoE、CSMoE和AdaMoE）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>论文作者来自多个机构，主要包括：</p>\n<ul><li>上海交通大学（MoE关键人工智能实验室、自动化与智能传感学院、计算机科学学院）</li><li>上海AI实验室</li><li>清华大学深圳国际研究生院</li><li>香港大学</li><li>同济大学</li><li>D-Robotics</li><li>系统控制与信息处理教育部重点实验室（上海交通大学）</li><li>信息安全综合管理技术上海市重点实验室</li></ul>\n<h3>论文概述</h3>\n<p>论文针对Vision-Language-Action (VLA)模型在扩展时面临的挑战（如计算资源需求高、实时控制效率与模型容量平衡难题），提出了AdaMoE（Action-Specialized Mixture of Experts）架构。AdaMoE通过继承预训练VLA模型权重，并在动作专家中引入稀疏激活的MoE层来扩展模型容量，同时利用解耦专家选择与权重的创新设计，在保持计算效率的前提下提升性能。论文在仿真基准（LIBERO和RoboTwin 2.0）和真实世界实验中验证了AdaMoE的有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>高效扩展VLA模型</strong>：提出一种从预训练密集VLA模型继承权重并转换为MoE架构的低成本方法。</li><li><strong>解耦专家选择与权重</strong>：引入独立的比例适配器（scale adapter）与路由器（router），解决传统MoE中负载平衡与任务性能冲突的根本问题。</li><li><strong>显著性能提升</strong>：在LIBERO基准提升1.8%，在RoboTwin 2.0域随机化任务提升9.3%，在真实世界实验提升21.5%，验证了实际应用价值。</li><li><strong>专家特化验证</strong>：通过专家激活模式分析，证明专家能捕获有意义的操作行为（如定位、抓取等）。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>架构设计</strong>：基于π₀框架的动作专家由共享专家（处理通用操作）和路由专家（处理特定任务）组成，使用top-k选择（默认k=1）进行稀疏激活。</li><li><strong>解耦机制</strong>：路由器（R）负责专家选择以实现负载平衡，比例适配器（S）独立调整专家贡献权重，最终权重为S_i(x) + softmax(R_i(x))。</li><li><strong>训练目标</strong>：结合流匹配损失（flow matching loss，用于高频率动作生成）和负载平衡损失（load balancing loss），总损失为L_total = L_τ + λ_balance * L_balance（λ_balance默认0.01）。</li><li><strong>专家初始化</strong>：共享专家继承原始FFN权重，路由专家为副本以快速扩展。</li><li><strong>推理过程</strong>：使用流匹配进行去噪迭代（从纯噪声生成动作序列）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - LIBERO基准：包含LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long四个任务套件，每个套件含100专家轨迹。</p>\n<p> - RoboTwin 2.0：19个域随机化任务（清洁和随机化环境各100/400轨迹）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 训练步数：120,000步（批量大小32）。</p>\n<p> - 优化器：AdamW（峰值学习率2.5e-5，路由器学习率5e-5，β1=0.9，β2=0.95）。</p>\n<p> - 其他超参数：动作视野50、专家数4、梯度裁剪1.0、EMA衰减0.99。</p>\n<p> - 计算资源：百度云平台提供计算支持，AgileX Robotics提供机器人平台。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>仿真环境</strong>：LIBERO和RoboTwin 2.0基准（使用域随机化增强鲁棒性）。</p>\n<p> - <strong>真实世界环境</strong>：ALOHA-Agilex双机械臂系统（AgileX Robotics），包括四个任务：堆叠盘子、按铃、调整瓶子、放置杯子。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：任务成功率（Success Rate, SR），以百分比表示（每个任务评估50次试验）。</p>\n<p> - 其他分析：专家使用强度（专家激活模式）、消融研究（比较不同架构变体如Vanilla MoE、CSMoE和AdaMoE）。</p>"
  },
  {
    "date": "2025-10-15",
    "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.13626",
    "summary_markdown": "## 论文研究单位\n- 复旦大学\n- 同济大学\n- 上海创新研究院\n- 新加坡国立大学\n\n（作者分别隶属上述单位，属于跨机构合作研究）\n## 论文概述\n针对视觉‑语言‑动作（VLA）模型在标准基准上表现优异，却在真实场景中易受扰动的现象，本文在 LIBERO 仿真基准上系统地进行鲁棒性评估。通过在七种维度（对象布局、相机视角、机器人初始姿态、语言指令、光照、背景纹理、传感器噪声）上施加可控扰动，揭示模型的脆弱性：对视角和初始状态的微小变化即可导致成功率从 95% 下降到 30% 以下；而语言变化对整体表现影响最小，进一步实验表明多数模型并未真正依赖语言信号。研究还发现模型存在位置偏置、忽略视觉语义以及对组合扰动的负交互效应。为解决这些问题，文中提出了 LIBERO‑Plus 基准、组合泛化差距的统计定义以及基于大规模扰动数据的通用微调方案，实现了显著鲁棒性提升。\n## 论文核心贡献点\n1. **系统性脆弱性分析**：在七种单一维度扰动下对 10 种 VLA 模型进行评估，量化每类扰动的绝对性能跌幅（表 1）。\n2. **诊断框架**：定义扰动随机变量与成功指示器，提出条件成功率、组合概率及协方差形式的组合泛化差距 Δij，并用 χ² 检验显著性。\n3. **深度洞见**：揭示模型对视觉‑动作的固定映射依赖、语言利用率低、位置记忆偏置以及对组合扰动的负交互效应。\n4. **LIBERO‑Plus 基准**：自动化生成 10,030 任务，涵盖七大扰动因子及 21 子组件，按多模型成功率划分为 Level‑1–Level‑5 五级难度（图 5‑6）。\n5. **通用训练策略**：构建超过 20,000 条扰动成功轨迹的通用训练集，对 OpenVLA‑OFT_m 进行混合微调，整体成功率提升至 79.6%（视角扰动 92.8%）。\n6. **开源资源**：公开评估脚本、任务生成代码、难度划分方法及训练配置，便于后续研究复用。\n## 论文方法描述\n1. **扰动维度**\n - 对象布局（添加干扰对象、目标位移）\n - 相机视角（视角/视野变化）\n - 机器人初始姿态（关节角度/位置扰动）\n - 语言指令（语义改写、句式变换）\n - 光照（强度、方向、颜色、阴影）\n - 背景纹理（桌面/场景材质）\n - 传感器噪声（光度失真、抖动、模糊）\n\n2. **模型集合**\n OpenVLA、OpenVLA‑OFT（含_oft_w、_oft_m）、π₀、π₀‑fast、Nora、WorldVLA、UniVLA、RIPT‑VLA 等，涵盖自回归与扩散两大范式。\n\n3. **单维度扰动实验**\n 对每种扰动在 LIBERO 四套任务上评估成功率，记录绝对跌幅（表 1），并分析模型之间的鲁棒差异。\n\n4. **视觉注意分析**\n 将对象布局扰动拆分为“添加干扰对象”与“目标位移”，对比成功率（图 1），验证模型是否聚焦任务关键对象。\n\n5. **光照鲁棒性极端消融**\n - **全黑**：所有相机输入置黑。\n - **第三视角黑**：仅第三人称视角黑掉，保留腕部相机。\n 通过成功率差异解释腕部相机提供的光照不敏感几何线索（图 2）。\n\n6. **语言利用实验**\n - **空指令**：完全删除语言输入。\n - **目标替换**：将指令中的目标对象替换为同场景的其它对象。\n 观察成功率变化（图 3），判断模型是否真正依赖语言。\n\n7. **组合泛化差距定义**\n 设 D_i 为第 i 种扰动是否施加的二值随机变量，Y 为任务成功指示器，定义条件成功率 s(D_i=d_i,D_j=d_j)=P(Y=1\\|D_i=d_i,D_j=d_j)。组合差距 Δ_ij = Cov(D_i,D_j\\|Y=1) = p(D_i=1,D_j=1\\|Y=1) - p(D_i=1\\|Y=1)p(D_j=1\\|Y=1)。通过 2000 次独立实验估计概率，构建热图并进行 χ² 显著性检验（附录 F）。\n\n8. **LIBERO‑Plus 基准构建**\n - 自动生成任务 → 过滤平衡 → 按四模型成功率划分为五级难度。\n - 任务规模：10,030，七扰动因子，21 子组件（图 5‑6）。\n\n9. **通用训练与微调**\n - 基于原始 LIBERO 轨迹扩增 20,000+ 条成功扰动轨迹。\n - 以 OpenVLA‑OFT_m 预训练权重为起点进行混合微调。\n - 在 LIBERO‑Plus 上评估，实现跨扰动显著提升（表 2）。\n## 论文使用数据集和训练资源\n- **基础数据**：LIBERO 基准（4 套任务），每个套件提供对象、语言指令和标准成功轨迹。\n- **扰动生成**：自动化在七大扰动维度上生成场景与轨迹，筛选后形成 10,030 评估任务。\n- **训练集**：超过 20,000 条成功扰动轨迹，覆盖对象布局、相机、光照、背景、噪声等多维变化。轨迹统一保存格式（附录 D）。\n- **微调配置**：使用官方 OpenVLA‑OFT_m 权重进行混合微调；具体超参数与训练过程详见附录 D。\n- **计算资源**：所有实验在 LIBERO 仿真平台完成，未在文中披露具体硬件，但规模要求大量随机生成的评估样本。\n## 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO 仿真平台，支持第三人称视角与腕部相机（如模型使用）。\n- **模型评估**：\n 1. **单维度扰动**：在 4 套标准任务上分别施加七种扰动，记录原始成功率及绝对跌幅（表 1）。\n 2. **组合扰动**：对选定模型在两两扰动组合下进行 2000 次试验，估计联合概率并计算 Δ_ij（热图（图 4））。\n 3. **极端消融**：全黑与第三人称视角黑掉两种场景，检验视觉依赖。\n 4. **语言实验**：空指令与目标替换任务，评估语言利用率。\n- **指标**：\n - **成功率（%）**——任务执行成功的比例。\n - **绝对跌幅**——相对原始成功率的下降低点（表 1）。\n - **组合泛化差距 Δ_ij**——协方差形式的交互效应度量。\n - **χ² 检验**——Δ_ij 的统计显著性（附录 F）。\n - **难度分级**——基于多模型成功率分布的任务划分（Level‑1至Level‑5）。\n - **综合成功率**——所有扰动维度的平均表现（表 2）。\n\n（所有图表与实验细节均可在正文相应章节及附录中查阅）",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>复旦大学</li><li>同济大学</li><li>上海创新研究院</li><li>新加坡国立大学</li></ul>\n\n<p>（作者分别隶属上述单位，属于跨机构合作研究）</p>\n<h2>论文概述</h2>\n<p>针对视觉‑语言‑动作（VLA）模型在标准基准上表现优异，却在真实场景中易受扰动的现象，本文在 LIBERO 仿真基准上系统地进行鲁棒性评估。通过在七种维度（对象布局、相机视角、机器人初始姿态、语言指令、光照、背景纹理、传感器噪声）上施加可控扰动，揭示模型的脆弱性：对视角和初始状态的微小变化即可导致成功率从 95% 下降到 30% 以下；而语言变化对整体表现影响最小，进一步实验表明多数模型并未真正依赖语言信号。研究还发现模型存在位置偏置、忽略视觉语义以及对组合扰动的负交互效应。为解决这些问题，文中提出了 LIBERO‑Plus 基准、组合泛化差距的统计定义以及基于大规模扰动数据的通用微调方案，实现了显著鲁棒性提升。</p>\n<h2>论文核心贡献点</h2>\n<ol><li><strong>系统性脆弱性分析</strong>：在七种单一维度扰动下对 10 种 VLA 模型进行评估，量化每类扰动的绝对性能跌幅（表 1）。</li><li><strong>诊断框架</strong>：定义扰动随机变量与成功指示器，提出条件成功率、组合概率及协方差形式的组合泛化差距 Δij，并用 χ² 检验显著性。</li><li><strong>深度洞见</strong>：揭示模型对视觉‑动作的固定映射依赖、语言利用率低、位置记忆偏置以及对组合扰动的负交互效应。</li><li><strong>LIBERO‑Plus 基准</strong>：自动化生成 10,030 任务，涵盖七大扰动因子及 21 子组件，按多模型成功率划分为 Level‑1–Level‑5 五级难度（图 5‑6）。</li><li><strong>通用训练策略</strong>：构建超过 20,000 条扰动成功轨迹的通用训练集，对 OpenVLA‑OFT_m 进行混合微调，整体成功率提升至 79.6%（视角扰动 92.8%）。</li><li><strong>开源资源</strong>：公开评估脚本、任务生成代码、难度划分方法及训练配置，便于后续研究复用。</li></ol>\n<h2>论文方法描述</h2>\n<p>1. <strong>扰动维度</strong></p>\n<p> - 对象布局（添加干扰对象、目标位移）</p>\n<p> - 相机视角（视角/视野变化）</p>\n<p> - 机器人初始姿态（关节角度/位置扰动）</p>\n<p> - 语言指令（语义改写、句式变换）</p>\n<p> - 光照（强度、方向、颜色、阴影）</p>\n<p> - 背景纹理（桌面/场景材质）</p>\n<p> - 传感器噪声（光度失真、抖动、模糊）</p>\n\n<p>2. <strong>模型集合</strong></p>\n<p> OpenVLA、OpenVLA‑OFT（含_oft_w、_oft_m）、π₀、π₀‑fast、Nora、WorldVLA、UniVLA、RIPT‑VLA 等，涵盖自回归与扩散两大范式。</p>\n\n<p>3. <strong>单维度扰动实验</strong></p>\n<p> 对每种扰动在 LIBERO 四套任务上评估成功率，记录绝对跌幅（表 1），并分析模型之间的鲁棒差异。</p>\n\n<p>4. <strong>视觉注意分析</strong></p>\n<p> 将对象布局扰动拆分为“添加干扰对象”与“目标位移”，对比成功率（图 1），验证模型是否聚焦任务关键对象。</p>\n\n<p>5. <strong>光照鲁棒性极端消融</strong></p>\n<p> - <strong>全黑</strong>：所有相机输入置黑。</p>\n<p> - <strong>第三视角黑</strong>：仅第三人称视角黑掉，保留腕部相机。</p>\n<p> 通过成功率差异解释腕部相机提供的光照不敏感几何线索（图 2）。</p>\n\n<p>6. <strong>语言利用实验</strong></p>\n<p> - <strong>空指令</strong>：完全删除语言输入。</p>\n<p> - <strong>目标替换</strong>：将指令中的目标对象替换为同场景的其它对象。</p>\n<p> 观察成功率变化（图 3），判断模型是否真正依赖语言。</p>\n\n<p>7. <strong>组合泛化差距定义</strong></p>\n<p> 设 D_i 为第 i 种扰动是否施加的二值随机变量，Y 为任务成功指示器，定义条件成功率 s(D_i=d_i,D_j=d_j)=P(Y=1\\|D_i=d_i,D_j=d_j)。组合差距 Δ_ij = Cov(D_i,D_j\\|Y=1) = p(D_i=1,D_j=1\\|Y=1) - p(D_i=1\\|Y=1)p(D_j=1\\|Y=1)。通过 2000 次独立实验估计概率，构建热图并进行 χ² 显著性检验（附录 F）。</p>\n\n<p>8. <strong>LIBERO‑Plus 基准构建</strong></p>\n<p> - 自动生成任务 → 过滤平衡 → 按四模型成功率划分为五级难度。</p>\n<p> - 任务规模：10,030，七扰动因子，21 子组件（图 5‑6）。</p>\n\n<p>9. <strong>通用训练与微调</strong></p>\n<p> - 基于原始 LIBERO 轨迹扩增 20,000+ 条成功扰动轨迹。</p>\n<p> - 以 OpenVLA‑OFT_m 预训练权重为起点进行混合微调。</p>\n<p> - 在 LIBERO‑Plus 上评估，实现跨扰动显著提升（表 2）。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>基础数据</strong>：LIBERO 基准（4 套任务），每个套件提供对象、语言指令和标准成功轨迹。</li><li><strong>扰动生成</strong>：自动化在七大扰动维度上生成场景与轨迹，筛选后形成 10,030 评估任务。</li><li><strong>训练集</strong>：超过 20,000 条成功扰动轨迹，覆盖对象布局、相机、光照、背景、噪声等多维变化。轨迹统一保存格式（附录 D）。</li><li><strong>微调配置</strong>：使用官方 OpenVLA‑OFT_m 权重进行混合微调；具体超参数与训练过程详见附录 D。</li><li><strong>计算资源</strong>：所有实验在 LIBERO 仿真平台完成，未在文中披露具体硬件，但规模要求大量随机生成的评估样本。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：LIBERO 仿真平台，支持第三人称视角与腕部相机（如模型使用）。</li><li><strong>模型评估</strong>：</li></ul>\n<p> 1. <strong>单维度扰动</strong>：在 4 套标准任务上分别施加七种扰动，记录原始成功率及绝对跌幅（表 1）。</p>\n<p> 2. <strong>组合扰动</strong>：对选定模型在两两扰动组合下进行 2000 次试验，估计联合概率并计算 Δ_ij（热图（图 4））。</p>\n<p> 3. <strong>极端消融</strong>：全黑与第三人称视角黑掉两种场景，检验视觉依赖。</p>\n<p> 4. <strong>语言实验</strong>：空指令与目标替换任务，评估语言利用率。</p>\n<ul><li><strong>指标</strong>：</li></ul>\n<p> - <strong>成功率（%）</strong>——任务执行成功的比例。</p>\n<p> - <strong>绝对跌幅</strong>——相对原始成功率的下降低点（表 1）。</p>\n<p> - <strong>组合泛化差距 Δ_ij</strong>——协方差形式的交互效应度量。</p>\n<p> - <strong>χ² 检验</strong>——Δ_ij 的统计显著性（附录 F）。</p>\n<p> - <strong>难度分级</strong>——基于多模型成功率分布的任务划分（Level‑1至Level‑5）。</p>\n<p> - <strong>综合成功率</strong>——所有扰动维度的平均表现（表 2）。</p>\n\n<p>（所有图表与实验细节均可在正文相应章节及附录中查阅）</p>"
  },
  {
    "date": "2025-10-15",
    "title": "DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning",
    "link": "http://arxiv.org/abs/2510.13375",
    "summary_markdown": "## 论文研究单位\nIIIS, 清华大学;Galaxea AI\n## 论文概述\nDepthVLA是一篇针对视觉-语言-动作模型（Vision-Language-Action, VLA）在精确空间推理方面表现不佳的研究。当前基于大规模语言/视觉预训练的VLA虽然在语义理解上表现强劲，但普遍缺乏精细的空间理解，导致在精准抓取、避障与精细操作任务中失败。DepthVLA通过引入预训练的深度预测模块作为“深度专家”，在保持语言理解与开放词汇感知的同时显式增强几何推理能力。\n\nDepthVLA采用“混合 transformer”（Mixture-of-Transformers, MoT）架构，统一VLM、深度专家与动作专家（流匹配）三者并共享注意力，但采用分块掩码以保护各专家的预训练能力。深度专家基于DINOv2编码器、采用Depth Anything V2初始化，并通过尺度不变的对数损失进行训练，为动作专家提供贯穿中间层的几何特征，从而提升抓取与碰撞避免等精细操作能力。\n\n实验在真实世界与仿真环境开展，包括Simpler WidowX与LIBERO等基准。DepthVLA显著优于现有方法：在真实任务中取得78.5% vs 65.0%的成绩，在LIBERO上达94.9% vs 93.6%，在Simpler上为74.8% vs 58.8%。尽管新增约600M参数与约20ms的推理延迟，仍具实际可用性。\n## 论文核心贡献点\n- 深度专家融入MoT的VLA架构：在不破坏语义能力的前提下显式提供空间几何线索，提升精准操作与避障表现。\n- 分模块预训练策略：VLM与深度专家可分别在大规模数据上独立预训练，提高效率与可扩展性，突破仅依赖具身动作数据。\n- 真实与仿真全面验证：在多个基准上实现稳定且显著提升，涵盖精细抓取、碰撞避免与复杂多步任务。\n## 论文方法描述\n- 统一MoT架构：共享注意力的三专家设计（VLM、深度、动作），采用分块掩码使VLM/深度token自注意力、动作token可跨模态注意；保持各专家独立权重。\n- 深度专家：基于DINOv2的编码器-解码器结构，使用Depth Anything V2初始化进行预训练；损失为尺度不变对数损失，使其具备稳健的距离估计与几何感知。深度专家在所有中间层输出几何特征，支撑动作专家的细粒度空间推理。\n- 动作专家：采用流匹配（flow matching）损失对连续动作轨迹建模，与深度损失联合优化，实现端到端训练。\n- 训练策略：深度专家在大规模3D/深度数据上预训练；VLA阶段在具身演示数据上微调，保留深度预测损失以维持几何能力。\n## 论文使用数据集和训练资源\n- 深度预训练：WildRGB-D、ScanNet、ScanNet++、HyperSim等大规模3D/深度数据。\n- VLA训练：Galaxea Open-World（10万轨迹、150任务类、50场景）、BridgeData V2（6万+轨迹）、LIBERO（每个suite 500演示）。\n- 伪深度标签：Depth Anything V2、UniDepth V2、VGGT。\n- 训练资源：32×H100 GPU，AdamW优化；批量与学习率针对不同数据集设置。\n## 论文使用的评估环境和评估指标\n- 仿真评估：\n - Simpler WidowX：4套任务、每套120次试验，报告各任务及平均成功率。\n - LIBERO（Franka Panda）：四个suite（Spatial/Object/Goal/Long），总计2000次试验，每个suite报告成功率。\n- 真实评估：\n - Galaxea R1 Lite双臂移动平台：设计“餐桌清理”“微波操作”“方块叠放”三类任务。\n - 指标：进度分数（每步成功计一分），每任务平均20次；并提供少样本微调（20条轨迹）设置下的比较。\n- 推理与资源指标：显存（8.0GB vs 6.7GB）、延迟（210ms/步 vs 190ms/步），参数增加约600M。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>IIIS, 清华大学;Galaxea AI</p>\n<h2>论文概述</h2>\n<p>DepthVLA是一篇针对视觉-语言-动作模型（Vision-Language-Action, VLA）在精确空间推理方面表现不佳的研究。当前基于大规模语言/视觉预训练的VLA虽然在语义理解上表现强劲，但普遍缺乏精细的空间理解，导致在精准抓取、避障与精细操作任务中失败。DepthVLA通过引入预训练的深度预测模块作为“深度专家”，在保持语言理解与开放词汇感知的同时显式增强几何推理能力。</p>\n\n<p>DepthVLA采用“混合 transformer”（Mixture-of-Transformers, MoT）架构，统一VLM、深度专家与动作专家（流匹配）三者并共享注意力，但采用分块掩码以保护各专家的预训练能力。深度专家基于DINOv2编码器、采用Depth Anything V2初始化，并通过尺度不变的对数损失进行训练，为动作专家提供贯穿中间层的几何特征，从而提升抓取与碰撞避免等精细操作能力。</p>\n\n<p>实验在真实世界与仿真环境开展，包括Simpler WidowX与LIBERO等基准。DepthVLA显著优于现有方法：在真实任务中取得78.5% vs 65.0%的成绩，在LIBERO上达94.9% vs 93.6%，在Simpler上为74.8% vs 58.8%。尽管新增约600M参数与约20ms的推理延迟，仍具实际可用性。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>深度专家融入MoT的VLA架构：在不破坏语义能力的前提下显式提供空间几何线索，提升精准操作与避障表现。</li><li>分模块预训练策略：VLM与深度专家可分别在大规模数据上独立预训练，提高效率与可扩展性，突破仅依赖具身动作数据。</li><li>真实与仿真全面验证：在多个基准上实现稳定且显著提升，涵盖精细抓取、碰撞避免与复杂多步任务。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>统一MoT架构：共享注意力的三专家设计（VLM、深度、动作），采用分块掩码使VLM/深度token自注意力、动作token可跨模态注意；保持各专家独立权重。</li><li>深度专家：基于DINOv2的编码器-解码器结构，使用Depth Anything V2初始化进行预训练；损失为尺度不变对数损失，使其具备稳健的距离估计与几何感知。深度专家在所有中间层输出几何特征，支撑动作专家的细粒度空间推理。</li><li>动作专家：采用流匹配（flow matching）损失对连续动作轨迹建模，与深度损失联合优化，实现端到端训练。</li><li>训练策略：深度专家在大规模3D/深度数据上预训练；VLA阶段在具身演示数据上微调，保留深度预测损失以维持几何能力。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>深度预训练：WildRGB-D、ScanNet、ScanNet++、HyperSim等大规模3D/深度数据。</li><li>VLA训练：Galaxea Open-World（10万轨迹、150任务类、50场景）、BridgeData V2（6万+轨迹）、LIBERO（每个suite 500演示）。</li><li>伪深度标签：Depth Anything V2、UniDepth V2、VGGT。</li><li>训练资源：32×H100 GPU，AdamW优化；批量与学习率针对不同数据集设置。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>仿真评估：</li></ul>\n<p> - Simpler WidowX：4套任务、每套120次试验，报告各任务及平均成功率。</p>\n<p> - LIBERO（Franka Panda）：四个suite（Spatial/Object/Goal/Long），总计2000次试验，每个suite报告成功率。</p>\n<ul><li>真实评估：</li></ul>\n<p> - Galaxea R1 Lite双臂移动平台：设计“餐桌清理”“微波操作”“方块叠放”三类任务。</p>\n<p> - 指标：进度分数（每步成功计一分），每任务平均20次；并提供少样本微调（20条轨迹）设置下的比较。</p>\n<ul><li>推理与资源指标：显存（8.0GB vs 6.7GB）、延迟（210ms/步 vs 190ms/步），参数增加约600M。</li></ul>"
  },
  {
    "date": "2025-10-15",
    "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.13237",
    "summary_markdown": "- 论文研究单位\n奥克兰大学、阿卜杜拉国王科技大学、东京理科大学、RIKEN高级智能项目中心。\n\n- 论文概述\n该论文探讨了视觉-语言-动作（VLA）模型在面对对抗性补丁攻击时的脆弱性，并提出了一种模型无关的对抗性攻击方法（EDPA）及相应的防御策略。EDPA通过破坏视觉与文本嵌入空间的对齐来生成可部署的对抗补丁，显著降低了VLA模型的任务成功率。同时，论文提出了一种对抗性微调方案，通过微调视觉编码器以增强模型的鲁棒性，有效缓解了攻击带来的性能下降。\n\n- 论文核心贡献点\n1. 提出了嵌入干扰补丁攻击（EDPA），一种模型无关的对抗性补丁攻击方法，无需预先知道模型架构或受控机器人操作器的细节。\n2. EDPA通过两个互补的目标优化补丁：破坏视觉和文本潜在表示之间的语义对齐，以及最大化对抗性和干净视觉输入潜在表示之间的差异。\n3. 提出了一种对抗性微调方案，通过微调视觉编码器来提高VLA模型对对抗性补丁的鲁棒性。\n4. 在LIBERO机器人仿真基准上进行了广泛评估，证明了EDPA能显著提高最先进VLA模型的任务失败率，而提出的防御能有效减轻这种退化。\n\n- 论文方法描述\n1. EDPA方法：针对VLA模型的潜在表示空间，通过优化两个损失函数（图像-指令对齐损失和补丁对比损失）来生成对抗补丁。图像-指令对齐损失旨在破坏视觉和语言嵌入之间的语义对齐，补丁对比损失旨在最大化干净和对抗性视觉输入嵌入之间的差异。\n2. 对抗性微调防御：使用EDPA生成的对抗补丁构建对抗性视觉样本，微调视觉编码器。微调的目标是让编码器对对抗性和干净输入产生相似的潜在表示，同时保持对干净输入的性能。\n3. 攻击和防御过程仅需要访问视觉编码器的参数，而不需要整个VLA模型的参数，实现了模型无关的特性。\n\n- 论文使用数据集和训练资源\n1. 数据集：LIBERO机器人仿真基准，包含四个不同的任务套件（Spatial、Object、Goal、Long），每个套件包含10个任务，每个任务执行50次。\n2. 训练资源：对抗补丁生成使用Adam优化器，学习率为1e-5，批量大小为16，最大训练迭代次数为50,000。对抗性微调使用相同的优化器，补丁重置频率为1000，步长为2/255。\n3. 评估在多个开源VLA模型上进行，包括OpenVLA、OpenVLA-OFT和π0，这些模型针对LIBERO进行了微调。\n\n- 论文使用的评估环境和评估指标\n1. 评估环境：LIBERO机器人仿真环境，用于模拟机械臂执行复杂物理任务。\n2. 评估指标：失败率（FR），定义为任务未成功完成的比例（失败率 = 1 - 成功率）。报告的FR是三个不同随机种子实验的平均值。\n3. 对比方法：与随机噪声补丁、UADA和UPA等现有对抗补丁攻击方法进行性能对比。</think>\n- 论文研究单位\n奥克兰大学、阿卜杜拉国王科技大学、东京理科大学、RIKEN高级智能项目中心。\n\n- 论文概述\n该论文探讨了视觉-语言-动作（VLA）模型在面对对抗性补丁攻击时的脆弱性，并提出了一种模型无关的对抗性攻击方法（EDPA）及相应的防御策略。EDPA通过破坏视觉与文本嵌入空间的对齐来生成可部署的对抗补丁，显著降低了VLA模型的任务成功率。同时，论文提出了一种对抗性微调方案，通过微调视觉编码器以增强模型的鲁棒性，有效缓解了攻击带来的性能下降。\n\n- 论文核心贡献点\n1. 提出了嵌入干扰补丁攻击（EDPA），一种模型无关的对抗性补丁攻击方法，无需预先知道模型架构或受控机器人操作器的细节。\n2. EDPA通过两个互补的目标优化补丁：破坏视觉和文本潜在表示之间的语义对齐，以及最大化对抗性和干净视觉输入潜在表示之间的差异。\n3. 提出了一种对抗性微调方案，通过微调视觉编码器来提高VLA模型对对抗性补丁的鲁棒性。\n4. 在LIBERO机器人仿真基准上进行了广泛评估，证明了EDPA能显著提高最先进VLA模型的任务失败率，而提出的防御能有效减轻这种退化。\n\n- 论文方法描述\n1. EDPA方法：针对VLA模型的潜在表示空间，通过优化两个损失函数（图像-指令对齐损失和补丁对比损失）来生成对抗补丁。图像-指令对齐损失旨在破坏视觉和语言嵌入之间的语义对齐，补丁对比损失旨在最大化干净和对抗性视觉输入嵌入之间的差异。\n2. 对抗性微调防御：使用EDPA生成的对抗补丁构建对抗性视觉样本，微调视觉编码器。微调的目标是让编码器对对抗性和干净输入产生相似的潜在表示，同时保持对干净输入的性能。\n3. 攻击和防御过程仅需要访问视觉编码器的参数，而不需要整个VLA模型的参数，实现了模型无关的特性。\n\n- 论文使用数据集和训练资源\n1. 数据集：LIBERO机器人仿真基准，包含四个不同的任务套件（Spatial、Object、Goal、Long），每个套件包含10个任务，每个任务执行50次。\n2. 训练资源：对抗补丁生成使用Adam优化器，学习率为1e-5，批量大小为16，最大训练迭代次数为50,000。对抗性微调使用相同的优化器，补丁重置频率为1000，步长为2/255。\n3. 评估在多个开源VLA模型上进行，包括OpenVLA、OpenVLA-OFT和π0，这些模型针对LIBERO进行了微调。\n\n- 论文使用的评估环境和评估指标\n1. 评估环境：LIBERO机器人仿真环境，用于模拟机械臂执行复杂物理任务。\n2. 评估指标：失败率（FR），定义为任务未成功完成的比例（失败率 = 1 - 成功率）。报告的FR是三个不同随机种子实验的平均值。\n3. 对比方法：与随机噪声补丁、UADA和UPA等现有对抗补丁攻击方法进行性能对比。",
    "summary_html": "<ul><li>论文研究单位</li></ul>\n<p>奥克兰大学、阿卜杜拉国王科技大学、东京理科大学、RIKEN高级智能项目中心。</p>\n\n<ul><li>论文概述</li></ul>\n<p>该论文探讨了视觉-语言-动作（VLA）模型在面对对抗性补丁攻击时的脆弱性，并提出了一种模型无关的对抗性攻击方法（EDPA）及相应的防御策略。EDPA通过破坏视觉与文本嵌入空间的对齐来生成可部署的对抗补丁，显著降低了VLA模型的任务成功率。同时，论文提出了一种对抗性微调方案，通过微调视觉编码器以增强模型的鲁棒性，有效缓解了攻击带来的性能下降。</p>\n\n<ul><li>论文核心贡献点</li></ul>\n<ol><li>提出了嵌入干扰补丁攻击（EDPA），一种模型无关的对抗性补丁攻击方法，无需预先知道模型架构或受控机器人操作器的细节。</li><li>EDPA通过两个互补的目标优化补丁：破坏视觉和文本潜在表示之间的语义对齐，以及最大化对抗性和干净视觉输入潜在表示之间的差异。</li><li>提出了一种对抗性微调方案，通过微调视觉编码器来提高VLA模型对对抗性补丁的鲁棒性。</li><li>在LIBERO机器人仿真基准上进行了广泛评估，证明了EDPA能显著提高最先进VLA模型的任务失败率，而提出的防御能有效减轻这种退化。</li></ol>\n\n<ul><li>论文方法描述</li></ul>\n<ol><li>EDPA方法：针对VLA模型的潜在表示空间，通过优化两个损失函数（图像-指令对齐损失和补丁对比损失）来生成对抗补丁。图像-指令对齐损失旨在破坏视觉和语言嵌入之间的语义对齐，补丁对比损失旨在最大化干净和对抗性视觉输入嵌入之间的差异。</li><li>对抗性微调防御：使用EDPA生成的对抗补丁构建对抗性视觉样本，微调视觉编码器。微调的目标是让编码器对对抗性和干净输入产生相似的潜在表示，同时保持对干净输入的性能。</li><li>攻击和防御过程仅需要访问视觉编码器的参数，而不需要整个VLA模型的参数，实现了模型无关的特性。</li></ol>\n\n<ul><li>论文使用数据集和训练资源</li></ul>\n<ol><li>数据集：LIBERO机器人仿真基准，包含四个不同的任务套件（Spatial、Object、Goal、Long），每个套件包含10个任务，每个任务执行50次。</li><li>训练资源：对抗补丁生成使用Adam优化器，学习率为1e-5，批量大小为16，最大训练迭代次数为50,000。对抗性微调使用相同的优化器，补丁重置频率为1000，步长为2/255。</li><li>评估在多个开源VLA模型上进行，包括OpenVLA、OpenVLA-OFT和π0，这些模型针对LIBERO进行了微调。</li></ol>\n\n<ul><li>论文使用的评估环境和评估指标</li></ul>\n<ol><li>评估环境：LIBERO机器人仿真环境，用于模拟机械臂执行复杂物理任务。</li><li>评估指标：失败率（FR），定义为任务未成功完成的比例（失败率 = 1 - 成功率）。报告的FR是三个不同随机种子实验的平均值。</li><li>对比方法：与随机噪声补丁、UADA和UPA等现有对抗补丁攻击方法进行性能对比。</think></li></ol>\n<ul><li>论文研究单位</li></ul>\n<p>奥克兰大学、阿卜杜拉国王科技大学、东京理科大学、RIKEN高级智能项目中心。</p>\n\n<ul><li>论文概述</li></ul>\n<p>该论文探讨了视觉-语言-动作（VLA）模型在面对对抗性补丁攻击时的脆弱性，并提出了一种模型无关的对抗性攻击方法（EDPA）及相应的防御策略。EDPA通过破坏视觉与文本嵌入空间的对齐来生成可部署的对抗补丁，显著降低了VLA模型的任务成功率。同时，论文提出了一种对抗性微调方案，通过微调视觉编码器以增强模型的鲁棒性，有效缓解了攻击带来的性能下降。</p>\n\n<ul><li>论文核心贡献点</li></ul>\n<ol><li>提出了嵌入干扰补丁攻击（EDPA），一种模型无关的对抗性补丁攻击方法，无需预先知道模型架构或受控机器人操作器的细节。</li><li>EDPA通过两个互补的目标优化补丁：破坏视觉和文本潜在表示之间的语义对齐，以及最大化对抗性和干净视觉输入潜在表示之间的差异。</li><li>提出了一种对抗性微调方案，通过微调视觉编码器来提高VLA模型对对抗性补丁的鲁棒性。</li><li>在LIBERO机器人仿真基准上进行了广泛评估，证明了EDPA能显著提高最先进VLA模型的任务失败率，而提出的防御能有效减轻这种退化。</li></ol>\n\n<ul><li>论文方法描述</li></ul>\n<ol><li>EDPA方法：针对VLA模型的潜在表示空间，通过优化两个损失函数（图像-指令对齐损失和补丁对比损失）来生成对抗补丁。图像-指令对齐损失旨在破坏视觉和语言嵌入之间的语义对齐，补丁对比损失旨在最大化干净和对抗性视觉输入嵌入之间的差异。</li><li>对抗性微调防御：使用EDPA生成的对抗补丁构建对抗性视觉样本，微调视觉编码器。微调的目标是让编码器对对抗性和干净输入产生相似的潜在表示，同时保持对干净输入的性能。</li><li>攻击和防御过程仅需要访问视觉编码器的参数，而不需要整个VLA模型的参数，实现了模型无关的特性。</li></ol>\n\n<ul><li>论文使用数据集和训练资源</li></ul>\n<ol><li>数据集：LIBERO机器人仿真基准，包含四个不同的任务套件（Spatial、Object、Goal、Long），每个套件包含10个任务，每个任务执行50次。</li><li>训练资源：对抗补丁生成使用Adam优化器，学习率为1e-5，批量大小为16，最大训练迭代次数为50,000。对抗性微调使用相同的优化器，补丁重置频率为1000，步长为2/255。</li><li>评估在多个开源VLA模型上进行，包括OpenVLA、OpenVLA-OFT和π0，这些模型针对LIBERO进行了微调。</li></ol>\n\n<ul><li>论文使用的评估环境和评估指标</li></ul>\n<ol><li>评估环境：LIBERO机器人仿真环境，用于模拟机械臂执行复杂物理任务。</li><li>评估指标：失败率（FR），定义为任务未成功完成的比例（失败率 = 1 - 成功率）。报告的FR是三个不同随机种子实验的平均值。</li><li>对比方法：与随机噪声补丁、UADA和UPA等现有对抗补丁攻击方法进行性能对比。</li></ol>"
  },
  {
    "date": "2025-10-15",
    "title": "RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation",
    "link": "http://arxiv.org/abs/2510.13149",
    "summary_markdown": "### 论文研究单位\n南京大学、香港科技大学、新加坡国立大学、上海交通大学、上海创新研究院等机构联合研究\n### 论文概述\n机器人长期操作任务中的组合泛化能力面临挑战，现有模型在扰动条件下难以有效组合技能。针对此问题，研究提出RoboHiMan层次化评估范式，通过HiMan-Bench基准测试系统评估模型在原子任务和组合任务上的泛化能力，结合多级训练数据集和三种评估模式，分析分层架构的瓶颈。\n### 论文核心贡献点\n1. **构建HiMan-Bench基准**：包含114个原子任务和144个组合任务，覆盖12种扰动因素（颜色、纹理、光照等）\n2. **设计层次化评估范式**：提出Vanilla、Decoupled、Coupled三种评估模式，分离测试规划和执行能力\n3. **分析模型能力缺口**：发现VLA模型在组合任务和扰动场景下性能显著下降，揭示分层架构的脆弱性\n### 论文方法描述\n- **基准测试设计**：\n - 原子任务：10类基础操作（开抽屉、拿取物品等）\n - 组合任务：12类多步骤操作（取物放抽屉、转移物品等）\n - 扰动因素：对象颜色/纹理/大小、接收对象属性、光照、桌面纹理、干扰物、背景纹理、相机位姿\n- **多级训练数据集**：\n - L1：仅原子任务演示（每任务20条）\n - L2：添加扰动原子任务（每任务1条）\n - L3：添加4个组合任务（每任务5条）\n - L4：添加扰动组合任务（每任务1条）\n- **评估模式**：\n - Vanilla：端到端执行无规划器\n - Decoupled：规则规划器/视觉语言模型规划器独立评估\n - Coupled：端到端分层系统（视觉语言模型规划器+低层策略）\n### 论文使用数据集和训练资源\n- **仿真环境**：基于CoppeliaSim/PyRep构建的HiMan-Bench\n- **训练数据**：四个多级数据集（L1-L4），总演示量递增\n- **模型**：四类VLA模型（RVT-2、3D Diffuser Actor、π₀、π₀.₅）\n- **规划器**：Qwen2.5-VL视觉语言模型（用于高层规划）\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真测试：114个原子任务和144个组合任务，每任务15个无扰动测试+5个扰动测试+5个全扰动测试\n - 真实世界验证：设计小规模原子/组合任务集，测试扰动鲁棒性\n- **评估指标**：\n - **原子任务**：720集评估，平均成功率（A/AP任务）\n - **组合任务**：900集评估，平均成功率（C/CP任务）\n - **规划准确率**：离线评估视觉语言模型的分任务预测精度\n - **性能下降率**：在线评估对比离线基准的降幅（如L4→C&CP任务下降0.382）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南京大学、香港科技大学、新加坡国立大学、上海交通大学、上海创新研究院等机构联合研究</p>\n<h3>论文概述</h3>\n<p>机器人长期操作任务中的组合泛化能力面临挑战，现有模型在扰动条件下难以有效组合技能。针对此问题，研究提出RoboHiMan层次化评估范式，通过HiMan-Bench基准测试系统评估模型在原子任务和组合任务上的泛化能力，结合多级训练数据集和三种评估模式，分析分层架构的瓶颈。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>构建HiMan-Bench基准</strong>：包含114个原子任务和144个组合任务，覆盖12种扰动因素（颜色、纹理、光照等）</li><li><strong>设计层次化评估范式</strong>：提出Vanilla、Decoupled、Coupled三种评估模式，分离测试规划和执行能力</li><li><strong>分析模型能力缺口</strong>：发现VLA模型在组合任务和扰动场景下性能显著下降，揭示分层架构的脆弱性</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>基准测试设计</strong>：</li></ul>\n<p> - 原子任务：10类基础操作（开抽屉、拿取物品等）</p>\n<p> - 组合任务：12类多步骤操作（取物放抽屉、转移物品等）</p>\n<p> - 扰动因素：对象颜色/纹理/大小、接收对象属性、光照、桌面纹理、干扰物、背景纹理、相机位姿</p>\n<ul><li><strong>多级训练数据集</strong>：</li></ul>\n<p> - L1：仅原子任务演示（每任务20条）</p>\n<p> - L2：添加扰动原子任务（每任务1条）</p>\n<p> - L3：添加4个组合任务（每任务5条）</p>\n<p> - L4：添加扰动组合任务（每任务1条）</p>\n<ul><li><strong>评估模式</strong>：</li></ul>\n<p> - Vanilla：端到端执行无规划器</p>\n<p> - Decoupled：规则规划器/视觉语言模型规划器独立评估</p>\n<p> - Coupled：端到端分层系统（视觉语言模型规划器+低层策略）</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>仿真环境</strong>：基于CoppeliaSim/PyRep构建的HiMan-Bench</li><li><strong>训练数据</strong>：四个多级数据集（L1-L4），总演示量递增</li><li><strong>模型</strong>：四类VLA模型（RVT-2、3D Diffuser Actor、π₀、π₀.₅）</li><li><strong>规划器</strong>：Qwen2.5-VL视觉语言模型（用于高层规划）</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真测试：114个原子任务和144个组合任务，每任务15个无扰动测试+5个扰动测试+5个全扰动测试</p>\n<p> - 真实世界验证：设计小规模原子/组合任务集，测试扰动鲁棒性</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>原子任务</strong>：720集评估，平均成功率（A/AP任务）</p>\n<p> - <strong>组合任务</strong>：900集评估，平均成功率（C/CP任务）</p>\n<p> - <strong>规划准确率</strong>：离线评估视觉语言模型的分任务预测精度</p>\n<p> - <strong>性能下降率</strong>：在线评估对比离线基准的降幅（如L4→C&CP任务下降0.382）</p>"
  },
  {
    "date": "2025-10-15",
    "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
    "link": "http://arxiv.org/abs/2510.13054",
    "summary_markdown": "## 论文研究单位\n**NVIDIA**\n## 论文概述\nVLA-0 探讨了一种简洁的视觉语言动作模型（VLA）构建方法。该方法直接利用预训练视觉语言模型（VLM）的文本生成能力，通过自然语言字符串表示机器人动作（如坐标、关节角度），而非修改模型架构或引入新的动作表示机制。核心创新在于，通过精心设计的训练和推理配方（如动作集成预测和遮蔽动作增强），这种极简设计在性能上可与更复杂的VLA方法媲美。实验表明，在 LIBERO 仿真基准测试中，VLA-0 在不使用大规模预训练数据的情况下超越了所有基线方法；在真实世界评估中，也击败了基于SO-100数据集预训练的SmolVLA。\n## 论文核心贡献点\n1. **挑战VLA设计复杂性：** 证明了基于纯文本动作表示的简单VLA设计（无需修改VLM）可达到甚至超越更复杂的VLA架构（如离散token、生成式动作头或定制架构）的性能。\n2. **提出有效训练配方：** 详细阐述了实现高性能的关键技巧：动作文本解码（如归一化到整数）、推理阶段的动作集成预测（平均多个时间步预测）、训练阶段的遮蔽动作增强（随机遮蔽字符以强制模型基于视觉信息推理）。\n## 论文方法描述\n* **核心原理：** VLA-0 直接提示预训练的VLM（使用Qwen-VL-2.5-3B）输出动作文本序列。输入包含系统提示（定义任务）、图像（单一或拼接）、任务指令。\n* **动作表示：** 连续机器人动作（如末端执行器位置）被归一化到固定整数范围（如[0, 1000]），VLM输出该范围内的空间分隔整数序列，代表未来多个时间步的动作。\n* **推理阶段（关键配方）：**\n * **动作集成预测：** 模型预测未来n步动作。对当前时间步t，可获得t、t-1、...、t-n+1时刻预测的t步动作（作为其预测序列中的第一个或后续动作）。对这些n个预测取平均作为最终动作输出，增强稳定性。\n * **动作文本解码：** 解析VLM输出文本为具体动作值。\n* **训练阶段（关键配方）：**\n * **遮蔽动作增强：** 随机遮蔽动作字符串中的字符（如替换为占位符），迫使模型不依赖序列自回归完成，而是根据视觉和指令信息推理目标动作。\n * **优化：** 使用Adam优化器，64个epoch，批量大小192，学习率5e-6，在8个A100 GPU上训练约32小时。\n## 论文使用数据集和训练资源\n* **仿真数据：** 基于 LIBERO 基准测试。包含四个套件（空间推理、目标泛化、目标导向、长期任务），每个套件10个任务。VLA-0 在LIBERO内域数据上训练，未使用大规模预训练数据。\n* **真实数据：** 基于 SO-100 机器人平台和LeRobot框架。四个不同任务（重定位方块、推动苹果、拾取放置香蕉/纸杯蛋糕），每个任务100个演示。\n* **训练资源：** 使用8个 NVIDIA A100 GPU 进行约32小时的训练。\n## 论文使用的评估环境和评估指标\n* **评估环境：**\n * **仿真：** LIBERO 基准测试（四个套件，40个任务）。\n * **真实世界：** SO-100 机器人平台（基于LeRobot框架）。\n* **评估指标：**\n * **主要指标：** 任务成功率（Success Rate）。\n * **报告方式：** LIBERO 报告每个套件（10个任务）的平均成功率及总体平均成功率；SO-100 报告各任务成功率及总体平均成功率。所有评估均在固定次数的回合（如LIBERO每任务50回合）上进行。",
    "summary_html": "<h2>论文研究单位</h2>\n<p><strong>NVIDIA</strong></p>\n<h2>论文概述</h2>\n<p>VLA-0 探讨了一种简洁的视觉语言动作模型（VLA）构建方法。该方法直接利用预训练视觉语言模型（VLM）的文本生成能力，通过自然语言字符串表示机器人动作（如坐标、关节角度），而非修改模型架构或引入新的动作表示机制。核心创新在于，通过精心设计的训练和推理配方（如动作集成预测和遮蔽动作增强），这种极简设计在性能上可与更复杂的VLA方法媲美。实验表明，在 LIBERO 仿真基准测试中，VLA-0 在不使用大规模预训练数据的情况下超越了所有基线方法；在真实世界评估中，也击败了基于SO-100数据集预训练的SmolVLA。</p>\n<h2>论文核心贡献点</h2>\n<ol><li><strong>挑战VLA设计复杂性：</strong> 证明了基于纯文本动作表示的简单VLA设计（无需修改VLM）可达到甚至超越更复杂的VLA架构（如离散token、生成式动作头或定制架构）的性能。</li><li><strong>提出有效训练配方：</strong> 详细阐述了实现高性能的关键技巧：动作文本解码（如归一化到整数）、推理阶段的动作集成预测（平均多个时间步预测）、训练阶段的遮蔽动作增强（随机遮蔽字符以强制模型基于视觉信息推理）。</li></ol>\n<h2>论文方法描述</h2>\n<p>* <strong>核心原理：</strong> VLA-0 直接提示预训练的VLM（使用Qwen-VL-2.5-3B）输出动作文本序列。输入包含系统提示（定义任务）、图像（单一或拼接）、任务指令。</p>\n<p>* <strong>动作表示：</strong> 连续机器人动作（如末端执行器位置）被归一化到固定整数范围（如[0, 1000]），VLM输出该范围内的空间分隔整数序列，代表未来多个时间步的动作。</p>\n<p>* <strong>推理阶段（关键配方）：</strong></p>\n<p> * <strong>动作集成预测：</strong> 模型预测未来n步动作。对当前时间步t，可获得t、t-1、...、t-n+1时刻预测的t步动作（作为其预测序列中的第一个或后续动作）。对这些n个预测取平均作为最终动作输出，增强稳定性。</p>\n<p> * <strong>动作文本解码：</strong> 解析VLM输出文本为具体动作值。</p>\n<p>* <strong>训练阶段（关键配方）：</strong></p>\n<p> * <strong>遮蔽动作增强：</strong> 随机遮蔽动作字符串中的字符（如替换为占位符），迫使模型不依赖序列自回归完成，而是根据视觉和指令信息推理目标动作。</p>\n<p> * <strong>优化：</strong> 使用Adam优化器，64个epoch，批量大小192，学习率5e-6，在8个A100 GPU上训练约32小时。</p>\n<h2>论文使用数据集和训练资源</h2>\n<p>* <strong>仿真数据：</strong> 基于 LIBERO 基准测试。包含四个套件（空间推理、目标泛化、目标导向、长期任务），每个套件10个任务。VLA-0 在LIBERO内域数据上训练，未使用大规模预训练数据。</p>\n<p>* <strong>真实数据：</strong> 基于 SO-100 机器人平台和LeRobot框架。四个不同任务（重定位方块、推动苹果、拾取放置香蕉/纸杯蛋糕），每个任务100个演示。</p>\n<p>* <strong>训练资源：</strong> 使用8个 NVIDIA A100 GPU 进行约32小时的训练。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>* <strong>评估环境：</strong></p>\n<p> * <strong>仿真：</strong> LIBERO 基准测试（四个套件，40个任务）。</p>\n<p> * <strong>真实世界：</strong> SO-100 机器人平台（基于LeRobot框架）。</p>\n<p>* <strong>评估指标：</strong></p>\n<p> * <strong>主要指标：</strong> 任务成功率（Success Rate）。</p>\n<p> * <strong>报告方式：</strong> LIBERO 报告每个套件（10个任务）的平均成功率及总体平均成功率；SO-100 报告各任务成功率及总体平均成功率。所有评估均在固定次数的回合（如LIBERO每任务50回合）上进行。</p>"
  },
  {
    "date": "2025-10-14",
    "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
    "link": "http://arxiv.org/abs/2510.12796",
    "summary_markdown": "# DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving\n## 论文研究单位\n中国科学院自动化研究所模式识别国家重点实验室(NLPR)与银网智能科技有限公司联合完成\n## 论文概述\nDriveVLA-W0提出以世界模型为核心的训练范式，缓解VLA模型在稀疏动作监督下的“监督赤字”问题。该方法通过预测未来图像，引入密集的自监督信号，同时适配离散与连续视觉表征的两类主流VLA架构，并在端到端自主驾驶中实现实时推理与更高数据扩展性。\n## 论文核心贡献点\n- 识别并解决VLA的“监督赤字”：仅用低维动作信号无法充分驱动大规模VLA模型，世界模型通过预测未来图像提供密集监督\n- 设计跨架构世界建模：离散视觉token采用自回归(AR)世界模型，连续视觉特征采用扩散世界模型\n- 提出轻量化MoE Action Expert：与大型VLA主干通过联合注意力融合，提升推理效率(延迟降至原VLA的63.1%)\n- 揭示动作解码器的扩展规律逆转：在小数据上优的query/flow-matching在大规模数据下被自回归解码超越\n- 放大数据扩展律：在大规模训练中，世界模型持续提升性能，显著优于仅动作监督的基线\n## 论文方法描述\n- VLA基线：处理语言指令、前视图像与历史动作，采用Emu3-8B(VQ离散token)与Qwen2.5-VL-7B(连续ViT特征)双骨干，输出语言/视觉/动作三类特征，动作预测采用交叉熵损失\n- AR世界模型(DriveVLA-W0(VQ))：自回归预测当前图像的离散视觉token，训练目标为下一token预测损失，总损失为动作损失加权的世界模型损失\n- 扩散世界模型(DriveVLA-W0(ViT))：在潜空间进行条件扩散，预测未来帧图像，使用MSE对噪声预测网络优化，总损失为动作损失加权的扩散世界模型损失\n- 两阶段训练：先联合世界模型与动作目标预训练，再结合Action Expert进行动作微调；输入序列采用深交错的多模态时间拼接\n- Action Expert(MoE架构)：大型VLA Expert与500M轻量专家通过联合注意力融合，支持三种动作解码策略(查询式回归、自回归、Flow Matching)\n- 实时性：推理阶段可跳过显式的图像生成过程以保障低延迟\n## 论文使用数据集和训练资源\n- 数据集：NAVSIM v1/v2(基于OpenScene的安全关键场景基准)与大规模内部数据集(7000万帧、100万视频片段，100个挑战场景)\n- 训练资源：NAVSIM实验使用8×L20 GPU(global batch=48)；内部数据集使用64×GPU(global batch=256)；统一使用AdamW与余弦学习率、bfloat16混合精度\n- 对比基线：复现实验的TransFuser-50M与TransFuser-7B(单前视相机配置)\n## 论文使用的评估环境和评估指标\n- NAVSIM v1：NC、DAC、TTC、舒适度C、EP，综合指标PDMS\n- NAVSIM v2：NC、DAC、DDC、TLC、EP、TTC、LK、HC、EC，综合指标EPDMS\n- 内部数据集：3秒6点轨迹的ADE(↓)与碰撞率(↓)，碰撞率计算方法与NC一致\n- 扩展评估：跨域迁移(动作分布差异)、数据扩展律(70k/700k/70M帧)、延迟分析(H200 GPU)、生成保真度与规划性能的关联分析",
    "summary_html": "<h1>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</h1>\n<h2>论文研究单位</h2>\n<p>中国科学院自动化研究所模式识别国家重点实验室(NLPR)与银网智能科技有限公司联合完成</p>\n<h2>论文概述</h2>\n<p>DriveVLA-W0提出以世界模型为核心的训练范式，缓解VLA模型在稀疏动作监督下的“监督赤字”问题。该方法通过预测未来图像，引入密集的自监督信号，同时适配离散与连续视觉表征的两类主流VLA架构，并在端到端自主驾驶中实现实时推理与更高数据扩展性。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>识别并解决VLA的“监督赤字”：仅用低维动作信号无法充分驱动大规模VLA模型，世界模型通过预测未来图像提供密集监督</li><li>设计跨架构世界建模：离散视觉token采用自回归(AR)世界模型，连续视觉特征采用扩散世界模型</li><li>提出轻量化MoE Action Expert：与大型VLA主干通过联合注意力融合，提升推理效率(延迟降至原VLA的63.1%)</li><li>揭示动作解码器的扩展规律逆转：在小数据上优的query/flow-matching在大规模数据下被自回归解码超越</li><li>放大数据扩展律：在大规模训练中，世界模型持续提升性能，显著优于仅动作监督的基线</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>VLA基线：处理语言指令、前视图像与历史动作，采用Emu3-8B(VQ离散token)与Qwen2.5-VL-7B(连续ViT特征)双骨干，输出语言/视觉/动作三类特征，动作预测采用交叉熵损失</li><li>AR世界模型(DriveVLA-W0(VQ))：自回归预测当前图像的离散视觉token，训练目标为下一token预测损失，总损失为动作损失加权的世界模型损失</li><li>扩散世界模型(DriveVLA-W0(ViT))：在潜空间进行条件扩散，预测未来帧图像，使用MSE对噪声预测网络优化，总损失为动作损失加权的扩散世界模型损失</li><li>两阶段训练：先联合世界模型与动作目标预训练，再结合Action Expert进行动作微调；输入序列采用深交错的多模态时间拼接</li><li>Action Expert(MoE架构)：大型VLA Expert与500M轻量专家通过联合注意力融合，支持三种动作解码策略(查询式回归、自回归、Flow Matching)</li><li>实时性：推理阶段可跳过显式的图像生成过程以保障低延迟</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据集：NAVSIM v1/v2(基于OpenScene的安全关键场景基准)与大规模内部数据集(7000万帧、100万视频片段，100个挑战场景)</li><li>训练资源：NAVSIM实验使用8×L20 GPU(global batch=48)；内部数据集使用64×GPU(global batch=256)；统一使用AdamW与余弦学习率、bfloat16混合精度</li><li>对比基线：复现实验的TransFuser-50M与TransFuser-7B(单前视相机配置)</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>NAVSIM v1：NC、DAC、TTC、舒适度C、EP，综合指标PDMS</li><li>NAVSIM v2：NC、DAC、DDC、TLC、EP、TTC、LK、HC、EC，综合指标EPDMS</li><li>内部数据集：3秒6点轨迹的ADE(↓)与碰撞率(↓)，碰撞率计算方法与NC一致</li><li>扩展评估：跨域迁移(动作分布差异)、数据扩展律(70k/700k/70M帧)、延迟分析(H200 GPU)、生成保真度与规划性能的关联分析</li></ul>"
  },
  {
    "date": "2025-10-14",
    "title": "Reflection-Based Task Adaptation for Self-Improving VLA",
    "link": "http://arxiv.org/abs/2510.12710",
    "summary_markdown": "论文研究单位\n北京大学、京东探索学院、清华大学\n\n论文概述\n论文提出了一种名为Reflective Self-Adaptation的框架，旨在解决预训练视觉-语言-动作（VLA）模型在新任务中的自适应问题。现有强化学习方法效率低，难以快速掌握任务。该框架通过建立自我改进闭环，利用双路径架构（失败驱动的反思强化学习和成功驱动的质量引导监督微调）实现快速、自主的任务适应，无需人工干预。\n\n论文核心贡献点\n1. 提出Reflective Self-Adaptation框架，一种双路径架构，用于自主、原位VLA适应，系统性地从失败和成功中学习。\n2. 引入失败驱动的反思RL路径，通过Reflective Reward Synthesis方法利用VLM的因果推理自动生成密集奖励。\n3. 设计成功驱动的质量引导SFT路径，通过选择性模仿高质量成功轨迹确保目标对齐，防止奖励攻击。\n\n论文方法描述\n1. **问题定义**：任务建模为马尔可夫决策过程（MDP），策略参数化为预训练VLA模型，输入视觉观测和语言指令，输出动作。\n2. **失败驱动的反思RL**：\n - VLM分析失败轨迹，通过四阶段过程合成奖励函数：因果分析、组件选择（从预定义库中选择位置/方向/运动学/状态相关组件）、关系识别（AND/IF/OR逻辑组合）、结构化配置生成（参数化）。\n - 使用PPO算法结合合成的密集奖励优化策略，总奖励为稀疏环境奖励与反思奖励之和。\n3. **成功驱动的质量引导SFT**：\n - 使用内在质量评估筛选成功轨迹（结合累计反思奖励和轨迹长度），构建优先回放缓冲区。\n - 当成功率低于阈值时，激活条件课程学习，VLM简化任务环境以提供初始成功轨迹。\n - 通过行为克隆损失微调策略，最终目标为PPO损失与SFT损失的加权和。\n\n论文使用数据集和训练资源\n- **数据集**：LIBERO benchmark（标准套件包括LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long）和自定义套件LIBERO-Adapt（10个操作任务）。\n- **基础模型**：OpenVLA-7B，经LIBERO各套件监督微调。\n- **训练资源**：反思过程使用GPT-4o，数据收集在8个A800 GPU上并行进行，每5轮数据收集触发一次反思。\n\n论文使用的评估环境和评估指标\n- **评估环境**：LIBERO benchmark的四个标准套件和LIBERO-Adapt套件，测试空间关系、物体类别、目标导向和长时程任务。\n- **评估指标**：成功率（Success Rate, %），衡量任务完成的准确性；辅助使用排名（Rank）对比不同方法性能。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京大学、京东探索学院、清华大学</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为Reflective Self-Adaptation的框架，旨在解决预训练视觉-语言-动作（VLA）模型在新任务中的自适应问题。现有强化学习方法效率低，难以快速掌握任务。该框架通过建立自我改进闭环，利用双路径架构（失败驱动的反思强化学习和成功驱动的质量引导监督微调）实现快速、自主的任务适应，无需人工干预。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出Reflective Self-Adaptation框架，一种双路径架构，用于自主、原位VLA适应，系统性地从失败和成功中学习。</li><li>引入失败驱动的反思RL路径，通过Reflective Reward Synthesis方法利用VLM的因果推理自动生成密集奖励。</li><li>设计成功驱动的质量引导SFT路径，通过选择性模仿高质量成功轨迹确保目标对齐，防止奖励攻击。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li><strong>问题定义</strong>：任务建模为马尔可夫决策过程（MDP），策略参数化为预训练VLA模型，输入视觉观测和语言指令，输出动作。</li><li><strong>失败驱动的反思RL</strong>：</li></ol>\n<p> - VLM分析失败轨迹，通过四阶段过程合成奖励函数：因果分析、组件选择（从预定义库中选择位置/方向/运动学/状态相关组件）、关系识别（AND/IF/OR逻辑组合）、结构化配置生成（参数化）。</p>\n<p> - 使用PPO算法结合合成的密集奖励优化策略，总奖励为稀疏环境奖励与反思奖励之和。</p>\n<p>3. <strong>成功驱动的质量引导SFT</strong>：</p>\n<p> - 使用内在质量评估筛选成功轨迹（结合累计反思奖励和轨迹长度），构建优先回放缓冲区。</p>\n<p> - 当成功率低于阈值时，激活条件课程学习，VLM简化任务环境以提供初始成功轨迹。</p>\n<p> - 通过行为克隆损失微调策略，最终目标为PPO损失与SFT损失的加权和。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>数据集</strong>：LIBERO benchmark（标准套件包括LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long）和自定义套件LIBERO-Adapt（10个操作任务）。</li><li><strong>基础模型</strong>：OpenVLA-7B，经LIBERO各套件监督微调。</li><li><strong>训练资源</strong>：反思过程使用GPT-4o，数据收集在8个A800 GPU上并行进行，每5轮数据收集触发一次反思。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：LIBERO benchmark的四个标准套件和LIBERO-Adapt套件，测试空间关系、物体类别、目标导向和长时程任务。</li><li><strong>评估指标</strong>：成功率（Success Rate, %），衡量任务完成的准确性；辅助使用排名（Rank）对比不同方法性能。</li></ul>"
  },
  {
    "date": "2025-10-14",
    "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model",
    "link": "http://arxiv.org/abs/2510.12276",
    "summary_markdown": "### 论文研究单位\n香港科技大学（广州）、清华大学、西湖大学、浙江大学、华南理工大学等机构联合完成\n### 论文概述\n针对视觉-语言-行动（VLA）模型缺乏3D空间理解能力的问题，论文提出**空间强制（Spatial Forcing, SF）**方法。该方法通过将VLA模型的中间视觉表征与3D基础模型（VGGT）的几何表征对齐，在不依赖显式3D传感器或深度估计的条件下，隐式增强VLA的空间推理能力。\n### 论文核心贡献点\n1. **深度探测实验**：验证原始VLA视觉嵌入缺乏有效空间结构\n2. **空间强制（SF）机制**：将VLA中间视觉表征与VGGT输出的空间表征对齐\n3. **显著性能提升**：在仿真和真实环境中超越现有方法，同时提升训练效率（3.8×）和数据利用率\n### 论文方法描述\n- **对齐框架**：\n - 使用VGGT处理多视图图像，生成像素级空间表征\n - 对VLA中间层视觉令牌进行归一化和MLP变换\n - 通过余弦相似度最大化对齐视觉令牌与VGGT的空间表征\n- **监督策略**：监督中层而非最深层（防止视觉特征丢失）\n- **总损失函数**：结合动作生成损失与对齐损失\n- **推理阶段**：与标准VLA模型一致，无额外计算开销\n### 论文使用数据集和训练资源\n- **仿真数据**：\n - LIBERO基准（Spatial/Object/Goal/Long任务套件，共500专家演示）\n - RoboTwin（双机械臂仿真，含简单/困难模式）\n- **真实数据**：双机械臂AgileX平台（主相机+腕部相机）\n- **计算资源**：\n - 训练：8×NVIDIA H100显卡（LIBERO任务），1×H100（RoboTwin任务）\n - 训练迭代：15万次（LIBERO），3万次（RoboTwin）\n - 权重因子α：0.5\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **仿真环境**：LIBERO（四类任务）、RoboTwin（双机械臂）\n - **真实环境**：AgileX双机械臂平台\n- **评估指标**：\n - 任务成功率（Success Rate, SR）\n - 训练收敛速度（迭代次数比较）\n - 数据效率（不同数据比例下的成功率）\n - t-SNE可视化验证表征对齐效果\n- **实验结果**：\n - LIBERO平均SR达98.5%（超越所有2D/3D方法）\n - 训练效率提升3.8×（相同成功率所需迭代减少）\n - 5%数据下实现75.8% SR（数据效率提升5.9×）\n - 真实任务中透明杯堆叠等场景下SR显著提升\n\n论文主页：https://spatial-forcing.github.io/",
    "summary_html": "<h3>论文研究单位</h3>\n<p>香港科技大学（广州）、清华大学、西湖大学、浙江大学、华南理工大学等机构联合完成</p>\n<h3>论文概述</h3>\n<p>针对视觉-语言-行动（VLA）模型缺乏3D空间理解能力的问题，论文提出<strong>空间强制（Spatial Forcing, SF）</strong>方法。该方法通过将VLA模型的中间视觉表征与3D基础模型（VGGT）的几何表征对齐，在不依赖显式3D传感器或深度估计的条件下，隐式增强VLA的空间推理能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>深度探测实验</strong>：验证原始VLA视觉嵌入缺乏有效空间结构</li><li><strong>空间强制（SF）机制</strong>：将VLA中间视觉表征与VGGT输出的空间表征对齐</li><li><strong>显著性能提升</strong>：在仿真和真实环境中超越现有方法，同时提升训练效率（3.8×）和数据利用率</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>对齐框架</strong>：</li></ul>\n<p> - 使用VGGT处理多视图图像，生成像素级空间表征</p>\n<p> - 对VLA中间层视觉令牌进行归一化和MLP变换</p>\n<p> - 通过余弦相似度最大化对齐视觉令牌与VGGT的空间表征</p>\n<ul><li><strong>监督策略</strong>：监督中层而非最深层（防止视觉特征丢失）</li><li><strong>总损失函数</strong>：结合动作生成损失与对齐损失</li><li><strong>推理阶段</strong>：与标准VLA模型一致，无额外计算开销</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>仿真数据</strong>：</li></ul>\n<p> - LIBERO基准（Spatial/Object/Goal/Long任务套件，共500专家演示）</p>\n<p> - RoboTwin（双机械臂仿真，含简单/困难模式）</p>\n<ul><li><strong>真实数据</strong>：双机械臂AgileX平台（主相机+腕部相机）</li><li><strong>计算资源</strong>：</li></ul>\n<p> - 训练：8×NVIDIA H100显卡（LIBERO任务），1×H100（RoboTwin任务）</p>\n<p> - 训练迭代：15万次（LIBERO），3万次（RoboTwin）</p>\n<p> - 权重因子α：0.5</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>仿真环境</strong>：LIBERO（四类任务）、RoboTwin（双机械臂）</p>\n<p> - <strong>真实环境</strong>：AgileX双机械臂平台</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（Success Rate, SR）</p>\n<p> - 训练收敛速度（迭代次数比较）</p>\n<p> - 数据效率（不同数据比例下的成功率）</p>\n<p> - t-SNE可视化验证表征对齐效果</p>\n<ul><li><strong>实验结果</strong>：</li></ul>\n<p> - LIBERO平均SR达98.5%（超越所有2D/3D方法）</p>\n<p> - 训练效率提升3.8×（相同成功率所需迭代减少）</p>\n<p> - 5%数据下实现75.8% SR（数据效率提升5.9×）</p>\n<p> - 真实任务中透明杯堆叠等场景下SR显著提升</p>\n\n<p>论文主页：https://spatial-forcing.github.io/</p>"
  },
  {
    "date": "2025-10-13",
    "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
    "link": "http://arxiv.org/abs/2510.11660",
    "summary_markdown": "论文研究单位\n北京工业大学、南京大学、中国科学技术大学、Dexmal\n\n论文概述\n论文提出了ManiAgent，一个用于通用机器人操作任务的智能体框架。该框架通过多个专门的智能体进行协作，实现从任务描述和环境输入到机器人操作动作的端到端输出。它旨在解决现有视觉-语言-动作（VLA）模型在复杂推理和长时程任务规划中因数据稀缺和模型容量受限而表现不佳的问题。ManiAgent在SimplerEnv基准测试中达到了86.8%的成功率，在真实世界的拾取和放置任务中达到了95.8%。\n\n论文核心贡献点\n- 提出了一个名为ManiAgent的端到端智能体框架，可直接生成用于通用机器人操作任务的可执行动作序列。\n- 设计了一个感知-推理-控制的流水线，通过协调三个专门的智能体，整合空间感知、任务推理和动作规划，以执行复杂任务。\n- 进行了大量实验验证ManiAgent的卓越性能，其高成功率使其能作为一个全自动的数据收集工具，为其他基于学习的机器人操作方法提供有力支持。\n\n论文方法描述\nManiAgent框架包含三个协同工作的专门智能体：感知智能体、推理智能体和控制器智能体。\n1. 感知智能体：利用视觉语言模型（VLM）处理场景图像和用户指令，生成任务相关的场景描述。当VLM不足以进行精确空间定位时，会调用Florence-v2等目标检测方法获取物体的3D坐标和抓取姿态。\n2. 推理智能体：作为核心，接收场景描述和任务指令，利用大语言模型（LLM）进行状态评估和子任务分解。其分解过程是增量的，根据场景变化逐步调整，并存储历史子任务以防局部循环。\n3. 控制器智能体：负责将子任务和物体细节（中心位置、抓取姿态）转换为由LLM生成的可执行动作序列。为降低延迟，该智能体采用了一个缓存机制，对于已执行过的子任务，会直接检索并复用参数化的动作序列。\n\n论文使用数据集和训练资源\n- 框架本身是免训练的，依赖于预训练的大语言模型（LLM）和视觉语言模型（VLM）。\n- 仿真实验使用SimplerEnv数据集，具体环境为BridgeTable-v1和BridgeTable-v2，包含4个操作任务。\n- 真实世界实验在自定义任务集上进行，共设计了8个任务来评估不同维度的能力。\n- 物理平台使用WidowX-250s机械臂和两个Realsense D435摄像头。\n- 实验中评估了多种商业和开源模型，包括GPT-4o, GPT-5-nano, GPT-5, Claude-4-sonnet, Grok-4, GPT-oss-120b, Qwen-3-235b。\n- 组件方面，使用了Florence-v2进行目标检测，AnyGrasp进行抓取姿态生成。\n\n论文使用的评估环境和评估指标\n- 评估环境：\n - 仿真环境：SimplerEnv平台的BridgeTable-v1和BridgeTable-v2。\n - 真实环境：配备WidowX-250s机械臂和Realsense D435摄像头的物理平台。\n- 评估指标：\n - 主要指标：成功率，即成功完成的任务次数与总尝试次数的比例。\n - 真实世界任务评估维度：\n - 非拾取与放置能力\n - 高泛化能力\n - 相对位置感知能力\n - 意图推理能力\n - 知识检索与利用能力\n - 多步任务规划能力\n - 自动数据收集任务成功标准：物体最终位置与目标位置的距离小于15厘米。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京工业大学、南京大学、中国科学技术大学、Dexmal</p>\n\n<p>论文概述</p>\n<p>论文提出了ManiAgent，一个用于通用机器人操作任务的智能体框架。该框架通过多个专门的智能体进行协作，实现从任务描述和环境输入到机器人操作动作的端到端输出。它旨在解决现有视觉-语言-动作（VLA）模型在复杂推理和长时程任务规划中因数据稀缺和模型容量受限而表现不佳的问题。ManiAgent在SimplerEnv基准测试中达到了86.8%的成功率，在真实世界的拾取和放置任务中达到了95.8%。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了一个名为ManiAgent的端到端智能体框架，可直接生成用于通用机器人操作任务的可执行动作序列。</li><li>设计了一个感知-推理-控制的流水线，通过协调三个专门的智能体，整合空间感知、任务推理和动作规划，以执行复杂任务。</li><li>进行了大量实验验证ManiAgent的卓越性能，其高成功率使其能作为一个全自动的数据收集工具，为其他基于学习的机器人操作方法提供有力支持。</li></ul>\n\n<p>论文方法描述</p>\n<p>ManiAgent框架包含三个协同工作的专门智能体：感知智能体、推理智能体和控制器智能体。</p>\n<ol><li>感知智能体：利用视觉语言模型（VLM）处理场景图像和用户指令，生成任务相关的场景描述。当VLM不足以进行精确空间定位时，会调用Florence-v2等目标检测方法获取物体的3D坐标和抓取姿态。</li><li>推理智能体：作为核心，接收场景描述和任务指令，利用大语言模型（LLM）进行状态评估和子任务分解。其分解过程是增量的，根据场景变化逐步调整，并存储历史子任务以防局部循环。</li><li>控制器智能体：负责将子任务和物体细节（中心位置、抓取姿态）转换为由LLM生成的可执行动作序列。为降低延迟，该智能体采用了一个缓存机制，对于已执行过的子任务，会直接检索并复用参数化的动作序列。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>框架本身是免训练的，依赖于预训练的大语言模型（LLM）和视觉语言模型（VLM）。</li><li>仿真实验使用SimplerEnv数据集，具体环境为BridgeTable-v1和BridgeTable-v2，包含4个操作任务。</li><li>真实世界实验在自定义任务集上进行，共设计了8个任务来评估不同维度的能力。</li><li>物理平台使用WidowX-250s机械臂和两个Realsense D435摄像头。</li><li>实验中评估了多种商业和开源模型，包括GPT-4o, GPT-5-nano, GPT-5, Claude-4-sonnet, Grok-4, GPT-oss-120b, Qwen-3-235b。</li><li>组件方面，使用了Florence-v2进行目标检测，AnyGrasp进行抓取姿态生成。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：</li></ul>\n<p> - 仿真环境：SimplerEnv平台的BridgeTable-v1和BridgeTable-v2。</p>\n<p> - 真实环境：配备WidowX-250s机械臂和Realsense D435摄像头的物理平台。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 主要指标：成功率，即成功完成的任务次数与总尝试次数的比例。</p>\n<p> - 真实世界任务评估维度：</p>\n<p> - 非拾取与放置能力</p>\n<p> - 高泛化能力</p>\n<p> - 相对位置感知能力</p>\n<p> - 意图推理能力</p>\n<p> - 知识检索与利用能力</p>\n<p> - 多步任务规划能力</p>\n<p> - 自动数据收集任务成功标准：物体最终位置与目标位置的距离小于15厘米。</p>"
  },
  {
    "date": "2025-10-13",
    "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "link": "http://arxiv.org/abs/2510.11027",
    "summary_markdown": "# Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning\n## 论文研究单位\n- 论文由上海AI实验室主导，联合中国科学技术大学、上海交通大学、浙江大学、南京大学、复旦大学、清华大学、NUS、东北大学、深圳大学等多机构合作完成。\n- 通讯作者来自上海AI实验室；项目页面与代码仓库公开。\n## 论文概述\n- 针对上游VLM（Vision-Language Model）的具身推理能力与下游VLA（Vision-Language-Action）策略学习之间的关键差距，提出Vlaser：一个将高阶推理与低阶控制协同的VLA基础模型。\n- 构建高质量Vlaser-6M数据引擎，形成600万规模的多任务具身推理语料；系统分析不同VLM初始化与数据类型对VLA微调的影响，缩小互联网数据与具身策略学习之间的域间偏移。\n- 在12项具身推理基准与闭环机器人任务（WidowX/Google Robot）上取得领先结果，公开模型、数据与代码。\n## 论文核心贡献点\n- 统一的具身VLA架构与两阶段训练范式：将InternVL3扩展出“动作专家”，通过流匹配实现未来动作chunk预测，兼顾视觉、语言与控制。\n- 数据引擎与方法论创新：系统整合并增广多源公共数据集，构建覆盖定位（grounding）、空间推理、规划与通用VQA的Vlaser-6M；进一步在SimplerEnv中生成200万面向VLA的“域内”QA数据，缓解视觉域偏移。\n- 实证与洞见：上游具身推理的提升未必转化为下游VLA闭环成功率；与互联网数据相比，针对具体机器人本体与视角的“域内”数据更能加速收敛与提升成功率，并明确域间偏移的核心问题。\n## 论文方法描述\n- 模型结构\n - VLM骨干：基于InternVL3（InternViT图像编码 + Qwen2.5语言模型），尺寸为2B与8B，强调具身常识与端到端控制。\n - 动作专家：在VLM上增设低层控制分支；采用流匹配（flow matching）学习动作向量化场（vector field），以单帧观测、语言指令与机器人状态token为输入，生成未来H步动作序列；VLA流使用非因果注意力。\n - 训练与推理损失：VLM阶段采用自回归语言建模损失；VLA阶段最小化预测向量场与真实去噪向量场的MSE；推理时用欧拉积分从τ=0至τ=1对随机噪声进行去噪，生成动作。\n- 数据引擎\n - 具身定位：1.5M问答（边界框与中心点，标准化至[0,1000]），来源RoboPoint、ShareRobot、Pixmo-Points、Paco-LaVIS、RefSpatial；并从SA-1B分割掩码增广30万标注。\n - 通用/空间推理：1.2M通用RoboVQA + 0.5M空间推理数据；源RoboVQA、Robo2VLM、RoboPoint、SPAR、SpaceR-151k、VILASR，以及ScanNet/ScanNet++/CA-1M/ARKitScenes的10万手工空间样本。\n - 规划：40万数据，含Alpaca-15k-Instruction、MuEP、WAP，以及LLaRP在Habitat的规划轨迹与EgoPlan-IT/EgoCOT的第一人称视频规划样例。\n - VLA域内数据：200万针对WidowX与Google Robot的SimplerEnv生成问答，涵盖定位、空间、规划、通用VQA等类别。\n- 训练配方\n - 阶段一：监督微调InternVL3构建具身VLM骨干。\n - 阶段二：在机器人数据集上对动作专家进行VLA微调；使用流匹配损失训练，H=4，积分步长δ=10。\n## 论文使用数据集和训练资源\n- 具身推理评估数据（12项基准）：ERQA、Ego-Plan2、Where2place、Pointarena、Paco-Lavis、Pixmo-Points、VSI-Bench、RefSpatial-Bench、MMSI-Bench、VLABench、EmbodiedBench（ALFRED/Habitat）。\n- 机器人闭环评估：SimplerEnv（Bridge/WidowX 与 Google Robot 任务集），包含超过5百万帧图像与机器人 эпизодов。\n- 数据引擎与规模：Vlaser-6M由多源公共数据整合与增广（定位、空间推理、规划、通用VQA），以及来自SimplerEnv的200万域内VLM预训练问答对。\n- 预训练骨干：InternVL3-2B/8B（InternViT + Qwen2.5-1.5B/7B），动作专家采用流匹配架构；公开模型、训练与推理代码。\n## 论文使用的评估环境和评估指标\n- 评估环境\n - 具身推理：12项公开基准，涵盖问答、任务规划、定位、空间推理与仿真闭环评估。\n - 闭环机器人控制：SimplerEnv中的WidowX与Google Robot平台（Pick Coke Can、Move Near、Drawer、Carrot on plate、Put eggplant in basket、Spoon on towel、Stack Cube等任务）。\n- 评估指标\n - 具身推理：各基准的任务分数与标准化平均分（Avg）。\n - 闭环控制：成功率（Success Rate），分任务与平均成功率。\n - 训练分析：收敛速度与跨域泛化表现（域内vs互联网数据对VLA微调的影响）。",
    "summary_html": "<h1>Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</h1>\n<h2>论文研究单位</h2>\n<ul><li>论文由上海AI实验室主导，联合中国科学技术大学、上海交通大学、浙江大学、南京大学、复旦大学、清华大学、NUS、东北大学、深圳大学等多机构合作完成。</li><li>通讯作者来自上海AI实验室；项目页面与代码仓库公开。</li></ul>\n<h2>论文概述</h2>\n<ul><li>针对上游VLM（Vision-Language Model）的具身推理能力与下游VLA（Vision-Language-Action）策略学习之间的关键差距，提出Vlaser：一个将高阶推理与低阶控制协同的VLA基础模型。</li><li>构建高质量Vlaser-6M数据引擎，形成600万规模的多任务具身推理语料；系统分析不同VLM初始化与数据类型对VLA微调的影响，缩小互联网数据与具身策略学习之间的域间偏移。</li><li>在12项具身推理基准与闭环机器人任务（WidowX/Google Robot）上取得领先结果，公开模型、数据与代码。</li></ul>\n<h2>论文核心贡献点</h2>\n<ul><li>统一的具身VLA架构与两阶段训练范式：将InternVL3扩展出“动作专家”，通过流匹配实现未来动作chunk预测，兼顾视觉、语言与控制。</li><li>数据引擎与方法论创新：系统整合并增广多源公共数据集，构建覆盖定位（grounding）、空间推理、规划与通用VQA的Vlaser-6M；进一步在SimplerEnv中生成200万面向VLA的“域内”QA数据，缓解视觉域偏移。</li><li>实证与洞见：上游具身推理的提升未必转化为下游VLA闭环成功率；与互联网数据相比，针对具体机器人本体与视角的“域内”数据更能加速收敛与提升成功率，并明确域间偏移的核心问题。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>模型结构</li></ul>\n<p> - VLM骨干：基于InternVL3（InternViT图像编码 + Qwen2.5语言模型），尺寸为2B与8B，强调具身常识与端到端控制。</p>\n<p> - 动作专家：在VLM上增设低层控制分支；采用流匹配（flow matching）学习动作向量化场（vector field），以单帧观测、语言指令与机器人状态token为输入，生成未来H步动作序列；VLA流使用非因果注意力。</p>\n<p> - 训练与推理损失：VLM阶段采用自回归语言建模损失；VLA阶段最小化预测向量场与真实去噪向量场的MSE；推理时用欧拉积分从τ=0至τ=1对随机噪声进行去噪，生成动作。</p>\n<ul><li>数据引擎</li></ul>\n<p> - 具身定位：1.5M问答（边界框与中心点，标准化至[0,1000]），来源RoboPoint、ShareRobot、Pixmo-Points、Paco-LaVIS、RefSpatial；并从SA-1B分割掩码增广30万标注。</p>\n<p> - 通用/空间推理：1.2M通用RoboVQA + 0.5M空间推理数据；源RoboVQA、Robo2VLM、RoboPoint、SPAR、SpaceR-151k、VILASR，以及ScanNet/ScanNet++/CA-1M/ARKitScenes的10万手工空间样本。</p>\n<p> - 规划：40万数据，含Alpaca-15k-Instruction、MuEP、WAP，以及LLaRP在Habitat的规划轨迹与EgoPlan-IT/EgoCOT的第一人称视频规划样例。</p>\n<p> - VLA域内数据：200万针对WidowX与Google Robot的SimplerEnv生成问答，涵盖定位、空间、规划、通用VQA等类别。</p>\n<ul><li>训练配方</li></ul>\n<p> - 阶段一：监督微调InternVL3构建具身VLM骨干。</p>\n<p> - 阶段二：在机器人数据集上对动作专家进行VLA微调；使用流匹配损失训练，H=4，积分步长δ=10。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>具身推理评估数据（12项基准）：ERQA、Ego-Plan2、Where2place、Pointarena、Paco-Lavis、Pixmo-Points、VSI-Bench、RefSpatial-Bench、MMSI-Bench、VLABench、EmbodiedBench（ALFRED/Habitat）。</li><li>机器人闭环评估：SimplerEnv（Bridge/WidowX 与 Google Robot 任务集），包含超过5百万帧图像与机器人 эпизодов。</li><li>数据引擎与规模：Vlaser-6M由多源公共数据整合与增广（定位、空间推理、规划、通用VQA），以及来自SimplerEnv的200万域内VLM预训练问答对。</li><li>预训练骨干：InternVL3-2B/8B（InternViT + Qwen2.5-1.5B/7B），动作专家采用流匹配架构；公开模型、训练与推理代码。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境</li></ul>\n<p> - 具身推理：12项公开基准，涵盖问答、任务规划、定位、空间推理与仿真闭环评估。</p>\n<p> - 闭环机器人控制：SimplerEnv中的WidowX与Google Robot平台（Pick Coke Can、Move Near、Drawer、Carrot on plate、Put eggplant in basket、Spoon on towel、Stack Cube等任务）。</p>\n<ul><li>评估指标</li></ul>\n<p> - 具身推理：各基准的任务分数与标准化平均分（Avg）。</p>\n<p> - 闭环控制：成功率（Success Rate），分任务与平均成功率。</p>\n<p> - 训练分析：收敛速度与跨域泛化表现（域内vs互联网数据对VLA微调的影响）。</p>"
  },
  {
    "date": "2025-10-13",
    "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2510.10975",
    "summary_markdown": "### 论文研究单位\n深圳先进技术研究院（中国科学院）、鹏城实验室、中山大学计算机科学与工程学院、南洋理工大学计算与数据科学学院、上海AI实验室、中国科学院大学、X-Era AI Lab\n### 论文概述\nRoVer是一个针对视觉-语言-动作（VLA）模型的外部测试时缩放框架，旨在通过引入一个紧凑的机器人过程奖励模型（PRM）作为测试时验证器，在不修改原始VLA架构或权重的情况下增强其性能。该方法通过生成多个候选动作并使用PRM预测标量过程奖励和动作空间方向，以选择最优动作执行。关键是通过缓存共享感知特征来摊销感知成本，从而实现高效的测试时计算资源分配。\n### 论文核心贡献点\n1. **通用测试时缩放框架**：提出一个即插即用的外部测试时缩放框架，纯在推理时增强冻结的VLA策略，无需额外数据或模型重训练。\n2. **紧凑过程奖励模型**：设计一个同时输出标量过程奖励和动作细化方向的PRM，实现基于验证器的智能候选动作评估和方向引导探索。\n3. **高效方向引导采样策略**：利用共享感知缓存机制，将感知成本摊销到多个候选动作上，在固定测试时计算预算下支持可扩展的候选生成和选择。\n### 论文方法描述\nRoVer的整体方法分为三个部分：\n- **模型架构**：基于GPT-2风格的transformer，初始化自GR-1（使用MAE和CLIP预训练权重），添加奖励和方向预测头。引入共享感知缓存（编码观测、语言和状态特征一次并复用）和动作放大器（MLP结构，强化动作细微差异）。\n- **模型训练**：采用方向监督和偏好学习（Bradley-Terrry损失）训练PRM。数据准备通过锚定中心采样构造“更好/更差”动作对，并预测从当前动作到专家动作的方向向量。目标函数结合方向对齐损失（余弦相似度）和奖励偏好损失。\n- **方向引导测试时缩放**：推理时从基础策略采样N个候选动作，沿PRM预测的方向在有界角度内扩展M个新候选（统一候选预算K=N+M），PRM评分所有候选后选择最优动作执行。方向引导策略优于随机高斯采样，通过集中探索提升效率。\n### 论文使用数据集和训练资源\n- **数据集**：模拟实验使用CALVIN基准（ABC→D设置：训练集为A、B、C环境，测试为未见环境D），仅用20%训练集样本；真实机器人任务基于自定义场景（抓取放置、推按钮、堆叠碗）。\n- **训练资源**：PRM训练使用CALVIN ABC→D训练集100个epoch；模型总参数0.2B（可训练参数40M，冻结MAE和CLIP编码器）；推理在单张NVIDIA V100 GPU上运行，测试时计算预算可配置（如候选数K=10~10000）。\n### 论文使用的评估环境和评估指标\n- **模拟评估环境**：CALVIN基准（长时序语言条件任务），评估设置包括ABC→D跨环境迁移。指标为SR@k（连续完成k个任务的概率，k=1-5）和平均链长（Average Chain Length）。\n- **真实机器人评估环境**：双机械臂Dobot平台（右侧执行任务，左侧固定），使用腕部相机和俯视相机。任务涵盖Seen、Unseen object和Unseen position条件。指标为成功率（%，每条件10次试验）。\n- **效率评估**：使用共享感知缓存对比无缓存的延迟（秒）和加速比（倍），测试候选数10~10000下的吞吐扩展性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>深圳先进技术研究院（中国科学院）、鹏城实验室、中山大学计算机科学与工程学院、南洋理工大学计算与数据科学学院、上海AI实验室、中国科学院大学、X-Era AI Lab</p>\n<h3>论文概述</h3>\n<p>RoVer是一个针对视觉-语言-动作（VLA）模型的外部测试时缩放框架，旨在通过引入一个紧凑的机器人过程奖励模型（PRM）作为测试时验证器，在不修改原始VLA架构或权重的情况下增强其性能。该方法通过生成多个候选动作并使用PRM预测标量过程奖励和动作空间方向，以选择最优动作执行。关键是通过缓存共享感知特征来摊销感知成本，从而实现高效的测试时计算资源分配。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>通用测试时缩放框架</strong>：提出一个即插即用的外部测试时缩放框架，纯在推理时增强冻结的VLA策略，无需额外数据或模型重训练。</li><li><strong>紧凑过程奖励模型</strong>：设计一个同时输出标量过程奖励和动作细化方向的PRM，实现基于验证器的智能候选动作评估和方向引导探索。</li><li><strong>高效方向引导采样策略</strong>：利用共享感知缓存机制，将感知成本摊销到多个候选动作上，在固定测试时计算预算下支持可扩展的候选生成和选择。</li></ol>\n<h3>论文方法描述</h3>\n<p>RoVer的整体方法分为三个部分：</p>\n<ul><li><strong>模型架构</strong>：基于GPT-2风格的transformer，初始化自GR-1（使用MAE和CLIP预训练权重），添加奖励和方向预测头。引入共享感知缓存（编码观测、语言和状态特征一次并复用）和动作放大器（MLP结构，强化动作细微差异）。</li><li><strong>模型训练</strong>：采用方向监督和偏好学习（Bradley-Terrry损失）训练PRM。数据准备通过锚定中心采样构造“更好/更差”动作对，并预测从当前动作到专家动作的方向向量。目标函数结合方向对齐损失（余弦相似度）和奖励偏好损失。</li><li><strong>方向引导测试时缩放</strong>：推理时从基础策略采样N个候选动作，沿PRM预测的方向在有界角度内扩展M个新候选（统一候选预算K=N+M），PRM评分所有候选后选择最优动作执行。方向引导策略优于随机高斯采样，通过集中探索提升效率。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：模拟实验使用CALVIN基准（ABC→D设置：训练集为A、B、C环境，测试为未见环境D），仅用20%训练集样本；真实机器人任务基于自定义场景（抓取放置、推按钮、堆叠碗）。</li><li><strong>训练资源</strong>：PRM训练使用CALVIN ABC→D训练集100个epoch；模型总参数0.2B（可训练参数40M，冻结MAE和CLIP编码器）；推理在单张NVIDIA V100 GPU上运行，测试时计算预算可配置（如候选数K=10~10000）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>模拟评估环境</strong>：CALVIN基准（长时序语言条件任务），评估设置包括ABC→D跨环境迁移。指标为SR@k（连续完成k个任务的概率，k=1-5）和平均链长（Average Chain Length）。</li><li><strong>真实机器人评估环境</strong>：双机械臂Dobot平台（右侧执行任务，左侧固定），使用腕部相机和俯视相机。任务涵盖Seen、Unseen object和Unseen position条件。指标为成功率（%，每条件10次试验）。</li><li><strong>效率评估</strong>：使用共享感知缓存对比无缓存的延迟（秒）和加速比（倍），测试候选数10~10000下的吞吐扩展性。</li></ul>"
  },
  {
    "date": "2025-10-11",
    "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2510.10274",
    "summary_markdown": "# X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model\n\n- 论文研究单位\n - 清华大学AIR（Institute for AI Industry Research）、上海AI Lab、北京大学等合作完成\n\n- 论文概述\n - 目标是构建通用、可扩展的跨实体（跨机器人）视觉-语言-动作（VLA）模型，在不同硬件、数据和环境中实现稳健的泛化与快速适应\n - 提出软提示（Soft Prompt）机制，将每个数据源的硬件配置、相机设置、任务分布等异质性映射为可学习的轻量嵌入，作为“实体特定提示”，在特征融合早期引导模型学习\n - 设计X-VLA架构：采用流匹配（flow-matching）训练策略，基于标准Transformer编码器堆叠实现多模态输入的可扩展融合，支持多视角视觉、语言指令和本体感觉的联合编码\n - 训练管线分两阶段：预训练阶段在异质混合数据上学习“实体无关”的通用策略；域适应阶段冻结主干，仅调优新域的软提示和少量参数，实现高效迁移\n\n- 论文核心贡献点\n - 软提示学习：无需手工模板和自然语言描述，以可学习嵌入编码跨实体异质性，稳定预训练并促进快速适应\n - 简化而可扩展的Transformer架构：用标准自注意力Transformer替代扩散式解码器，保持通用性与简洁性；针对高维（多视角视觉、语言）和低维（本体感觉、动作）模态采用专用编码与早期融合\n - 自定义训练配方：包括对齐的动作表示、意图抽象（时间下采样构建高层意图锚点）、平衡采样、以及两阶段适应（软提示预热后联合优化）来提升训练稳定性和迁移效率\n - 实验验证：在6个仿真基准（含自动驾驶）和3个实体机器人上进行广泛评测，实现SOTA结果；此外用少量演示（1,200条）完成高难度的双手机器人“布料折叠”任务，并以1%参数（9M）微调实现接近完整微调的性能\n\n- 论文方法描述\n - 异质软提示学习：为每个数据源维护一组可学习的嵌入（软提示），近似映射硬件配置到提示空间；在动作生成早期注入，引导主干进行实体感知的学习\n - 架构设计：将主视觉-语言流与辅助视角（如腕部相机）分离编码；本体感觉与动作tokens（含时间嵌入）轻量投影后与视觉语言特征早期融合；整体采用标准Transformer堆叠进行融合与精确动作生成\n - 训练目标：使用流匹配策略，通过学习速度场将高斯噪声逐步传输到目标动作块；采用最优传输路径的线性插值监督，训练目标为预测速度与真实速度之间的均方误差\n - 数据处理与训练配方\n - 动作对齐：将动作标准化为末端执行器（EEF）位姿表示（xyz位置、Rotate6D旋转、二进制夹爪状态），分别用MSE和BCE监督\n - 意图抽象：时间下采样将低层动作轨迹抽象为未来4秒内的30个锚点，增强高层意图建模\n - 平衡采样：在跨域与域内同时随机打乱，缓解分布偏置与过拟合\n - 两阶段适应：先在冻结主干下预热新软提示，随后与主干联合微调\n - 学习率策略：对软提示与视觉-语言模块使用较低学习率，保留预训练表示并稳定优化\n\n- 论文使用数据集和训练资源\n - 预训练数据混合：Droid、RoboMind、Agibot等，约290K片段，覆盖7个平台与5类机械臂（单臂到双臂）\n - X-VLA-0.9B实例：隐藏维度1024、24层Transformer，预训练在异质混合数据上进行；后续在仿真与真实环境进行适配\n - 软提示与适配：每个数据源一组软提示；域适应中引入新软提示进行轻量化适配\n - 参数高效微调（PEFT）：采用LoRA等方法，仅调优约9M参数（约1%）实现接近全参数微调的效果\n - 实际任务数据：构建高质量“布料折叠”数据集Soft-FOLD，包含1,200条演示；并在BridgeData-v2等真实机器人协议上进行评测\n\n- 论文使用的评估环境和评估指标\n - 仿真基准：Libero、Simpler、VLABench、RoboTwin-2.0、Calvin、NAVSIM（覆盖单臂、双臂、跨域/跨任务/跨环境与自动驾驶场景）；在多项基准中达成新SOTA\n - 真实机器人：三类实体平台，分别测试简单操控、灵巧操控和快速适应（基于PEFT），任务成功率、任务完成率/吞吐量为主要指标\n - 关键结果概览\n - X-VLA-0.9B在多个仿真基准上超越既有SOTA（示例：Simpler-WidowX达约96%、Libero约98%、Calvin首阶段约96%）\n - 真实实验中在五类任务上显著优于基线；布料折叠任务达到接近100%成功率，约33次/小时的折叠吞吐\n - 以LoRA仅调优9M参数，在Libero与Simpler-WidowX上达到约93%与54%成功率，接近或可比全参数微调的强基线\n - 代理指标：预训练阶段使用ℓ1动作预测误差作为下游适配性能的代理指标，并与域适应成功率呈现强相关性",
    "summary_html": "<h1>X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model</h1>\n\n<ul><li>论文研究单位</li></ul>\n<p> - 清华大学AIR（Institute for AI Industry Research）、上海AI Lab、北京大学等合作完成</p>\n\n<ul><li>论文概述</li></ul>\n<p> - 目标是构建通用、可扩展的跨实体（跨机器人）视觉-语言-动作（VLA）模型，在不同硬件、数据和环境中实现稳健的泛化与快速适应</p>\n<p> - 提出软提示（Soft Prompt）机制，将每个数据源的硬件配置、相机设置、任务分布等异质性映射为可学习的轻量嵌入，作为“实体特定提示”，在特征融合早期引导模型学习</p>\n<p> - 设计X-VLA架构：采用流匹配（flow-matching）训练策略，基于标准Transformer编码器堆叠实现多模态输入的可扩展融合，支持多视角视觉、语言指令和本体感觉的联合编码</p>\n<p> - 训练管线分两阶段：预训练阶段在异质混合数据上学习“实体无关”的通用策略；域适应阶段冻结主干，仅调优新域的软提示和少量参数，实现高效迁移</p>\n\n<ul><li>论文核心贡献点</li></ul>\n<p> - 软提示学习：无需手工模板和自然语言描述，以可学习嵌入编码跨实体异质性，稳定预训练并促进快速适应</p>\n<p> - 简化而可扩展的Transformer架构：用标准自注意力Transformer替代扩散式解码器，保持通用性与简洁性；针对高维（多视角视觉、语言）和低维（本体感觉、动作）模态采用专用编码与早期融合</p>\n<p> - 自定义训练配方：包括对齐的动作表示、意图抽象（时间下采样构建高层意图锚点）、平衡采样、以及两阶段适应（软提示预热后联合优化）来提升训练稳定性和迁移效率</p>\n<p> - 实验验证：在6个仿真基准（含自动驾驶）和3个实体机器人上进行广泛评测，实现SOTA结果；此外用少量演示（1,200条）完成高难度的双手机器人“布料折叠”任务，并以1%参数（9M）微调实现接近完整微调的性能</p>\n\n<ul><li>论文方法描述</li></ul>\n<p> - 异质软提示学习：为每个数据源维护一组可学习的嵌入（软提示），近似映射硬件配置到提示空间；在动作生成早期注入，引导主干进行实体感知的学习</p>\n<p> - 架构设计：将主视觉-语言流与辅助视角（如腕部相机）分离编码；本体感觉与动作tokens（含时间嵌入）轻量投影后与视觉语言特征早期融合；整体采用标准Transformer堆叠进行融合与精确动作生成</p>\n<p> - 训练目标：使用流匹配策略，通过学习速度场将高斯噪声逐步传输到目标动作块；采用最优传输路径的线性插值监督，训练目标为预测速度与真实速度之间的均方误差</p>\n<p> - 数据处理与训练配方</p>\n<p> - 动作对齐：将动作标准化为末端执行器（EEF）位姿表示（xyz位置、Rotate6D旋转、二进制夹爪状态），分别用MSE和BCE监督</p>\n<p> - 意图抽象：时间下采样将低层动作轨迹抽象为未来4秒内的30个锚点，增强高层意图建模</p>\n<p> - 平衡采样：在跨域与域内同时随机打乱，缓解分布偏置与过拟合</p>\n<p> - 两阶段适应：先在冻结主干下预热新软提示，随后与主干联合微调</p>\n<p> - 学习率策略：对软提示与视觉-语言模块使用较低学习率，保留预训练表示并稳定优化</p>\n\n<ul><li>论文使用数据集和训练资源</li></ul>\n<p> - 预训练数据混合：Droid、RoboMind、Agibot等，约290K片段，覆盖7个平台与5类机械臂（单臂到双臂）</p>\n<p> - X-VLA-0.9B实例：隐藏维度1024、24层Transformer，预训练在异质混合数据上进行；后续在仿真与真实环境进行适配</p>\n<p> - 软提示与适配：每个数据源一组软提示；域适应中引入新软提示进行轻量化适配</p>\n<p> - 参数高效微调（PEFT）：采用LoRA等方法，仅调优约9M参数（约1%）实现接近全参数微调的效果</p>\n<p> - 实际任务数据：构建高质量“布料折叠”数据集Soft-FOLD，包含1,200条演示；并在BridgeData-v2等真实机器人协议上进行评测</p>\n\n<ul><li>论文使用的评估环境和评估指标</li></ul>\n<p> - 仿真基准：Libero、Simpler、VLABench、RoboTwin-2.0、Calvin、NAVSIM（覆盖单臂、双臂、跨域/跨任务/跨环境与自动驾驶场景）；在多项基准中达成新SOTA</p>\n<p> - 真实机器人：三类实体平台，分别测试简单操控、灵巧操控和快速适应（基于PEFT），任务成功率、任务完成率/吞吐量为主要指标</p>\n<p> - 关键结果概览</p>\n<p> - X-VLA-0.9B在多个仿真基准上超越既有SOTA（示例：Simpler-WidowX达约96%、Libero约98%、Calvin首阶段约96%）</p>\n<p> - 真实实验中在五类任务上显著优于基线；布料折叠任务达到接近100%成功率，约33次/小时的折叠吞吐</p>\n<p> - 以LoRA仅调优9M参数，在Libero与Simpler-WidowX上达到约93%与54%成功率，接近或可比全参数微调的强基线</p>\n<p> - 代理指标：预训练阶段使用ℓ1动作预测误差作为下游适配性能的代理指标，并与域适应成功率呈现强相关性</p>"
  },
  {
    "date": "2025-10-11",
    "title": "Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback",
    "link": "http://arxiv.org/abs/2510.10181",
    "summary_markdown": "### 论文研究单位\n上海交通大学计算机科学学院\n### 论文概述\n论文提出Dejavu框架，用于具身代理的后部署学习。该框架通过经验反馈网络（EFN）增强冻结的视觉-语言-行动（VLA）策略。EFN检索与当前上下文相关的历史成功行动经验，并预测残差行动来修正基础策略输出。在部署期间，EFN持续从新经验中学习，尽管基础策略权重固定，但能提升代理的适应性和成功率。实验基于LIBERO模拟器和AgiBot G1机器人，涵盖OpenVLA、UniVLA和GO-1骨干，显示EFN显著改善部署时性能。\n### 论文核心贡献点\n- 引入EFN作为以经验为中心的部署时机制，结合实时体验库和轻量级控制器，改进冻结VLA策略，无需梯度重训练。\n- 形式化体验为同步视觉-语言-行动轨迹，并实现语言条件视觉相似度的检索机制。\n- 将EFN集成至OpenVLA、UniVLA和GO-1骨干，在模拟和真实环境中实现一致的部署时改进。\n### 论文方法描述\n- **体验库设计**：存储步骤级轨迹（图像、视觉令牌、潜在行动），通过均值-最大融合构造紧凑关键向量，采用余弦相似度的概率top-k检索。\n- **残差策略学习**：使用软演员-评论家（SAC）算法训练EFN，以预测残差行动；奖励函数基于语义相似度（当前观察与检索经验的下一观察匹配），并包含反空闲惩罚。\n- **部署时机制**：指令过滤候选集、步进检索与效率优先（倾向短轨迹）、行动修正和执行、在线体验增长（将成功轨迹加入库）。\n- **训练细节**：EFN输入包括当前观察、基础行动和检索经验；上下文编码后通过行动者网络输出残差，评论家网络评估修正行动；损失函数基于SAC目标。\n### 论文使用数据集和训练资源\n- **数据集**：模拟实验使用LIBERO数据集；真实实验基于AgiBot G1机器人。\n- **训练资源**：VLA骨干包括OpenVLA、UniVLA和GO-1；使用Prismatic预训练管道、Flash Attention；LIBERO模拟器用于模拟，AgiBot G1硬件用于现实实验；EFN训练在模拟环境中进行，依赖CUDA加速。\n### 论文使用的评估环境和评估指标\n- **评估环境**：模拟实验在LIBERO平台上进行；真实实验在AgiBot G1机器人上进行。\n- **评估指标**：主要指标为成功率（Success Rate）和成功时的平均步数（Steps）；例如，LIBERO四子任务（Spatial、Object、Goal、Long）的成功率提升和步数减少；真实世界任务（PutBottle、SortItem、AddGoods）显示类似改进。基准比较包括EFN与OpenVLA/UniVLA/GO-1的对比，以及不同体验库容量（Volume 100-1000）的消融研究。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学计算机科学学院</p>\n<h3>论文概述</h3>\n<p>论文提出Dejavu框架，用于具身代理的后部署学习。该框架通过经验反馈网络（EFN）增强冻结的视觉-语言-行动（VLA）策略。EFN检索与当前上下文相关的历史成功行动经验，并预测残差行动来修正基础策略输出。在部署期间，EFN持续从新经验中学习，尽管基础策略权重固定，但能提升代理的适应性和成功率。实验基于LIBERO模拟器和AgiBot G1机器人，涵盖OpenVLA、UniVLA和GO-1骨干，显示EFN显著改善部署时性能。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>引入EFN作为以经验为中心的部署时机制，结合实时体验库和轻量级控制器，改进冻结VLA策略，无需梯度重训练。</li><li>形式化体验为同步视觉-语言-行动轨迹，并实现语言条件视觉相似度的检索机制。</li><li>将EFN集成至OpenVLA、UniVLA和GO-1骨干，在模拟和真实环境中实现一致的部署时改进。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>体验库设计</strong>：存储步骤级轨迹（图像、视觉令牌、潜在行动），通过均值-最大融合构造紧凑关键向量，采用余弦相似度的概率top-k检索。</li><li><strong>残差策略学习</strong>：使用软演员-评论家（SAC）算法训练EFN，以预测残差行动；奖励函数基于语义相似度（当前观察与检索经验的下一观察匹配），并包含反空闲惩罚。</li><li><strong>部署时机制</strong>：指令过滤候选集、步进检索与效率优先（倾向短轨迹）、行动修正和执行、在线体验增长（将成功轨迹加入库）。</li><li><strong>训练细节</strong>：EFN输入包括当前观察、基础行动和检索经验；上下文编码后通过行动者网络输出残差，评论家网络评估修正行动；损失函数基于SAC目标。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：模拟实验使用LIBERO数据集；真实实验基于AgiBot G1机器人。</li><li><strong>训练资源</strong>：VLA骨干包括OpenVLA、UniVLA和GO-1；使用Prismatic预训练管道、Flash Attention；LIBERO模拟器用于模拟，AgiBot G1硬件用于现实实验；EFN训练在模拟环境中进行，依赖CUDA加速。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：模拟实验在LIBERO平台上进行；真实实验在AgiBot G1机器人上进行。</li><li><strong>评估指标</strong>：主要指标为成功率（Success Rate）和成功时的平均步数（Steps）；例如，LIBERO四子任务（Spatial、Object、Goal、Long）的成功率提升和步数减少；真实世界任务（PutBottle、SortItem、AddGoods）显示类似改进。基准比较包括EFN与OpenVLA/UniVLA/GO-1的对比，以及不同体验库容量（Volume 100-1000）的消融研究。</li></ul>"
  },
  {
    "date": "2025-10-11",
    "title": "Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.09976",
    "summary_markdown": "## 论文研究单位\n- 脑认知与脑启发智能实验室（中科院自动化所）\n- 中国科学院大学\n- 脑认知与脑启发智能技术国家重点实验室\n- Long-term AI\n## 论文概述\n针对基于条件流匹配（flow matching）的视觉-语言-动作模型（VLA，如 π0），传统策略梯度强化学习方法由于无法计算重要性采样比而难以应用。本文提出“流策略优化”（Flow Policy Optimization，FPO）：用基于条件流匹配（CFM）损失的样本级变化构造“无似然比率”，替换传统重要性采样，实现 PPO 风格的截断代理目标更新；并结合结构感知信用分配、截断代理、潜空间多步欧拉探索与 Q 值集合体，构建稳定且可扩展的在线强化微调框架。在 LIBERO 与 ALOHA Transfer Cube 仿真任务上进行评估，FPO 在多项指标上超越多类强基线，并呈现稳定的学习曲线与稀疏奖励下的有效探索。\n## 论文核心贡献点\n- 首次实现流匹配策略与 PPO 风格策略梯度更新的兼容：无须显式似然或雅可比，计算“可通约的比率代理”。\n- 将样本级 CFM 损失差作为策略改进步信号（结构对齐的信用分配），并配套截断代理以控制更新幅度。\n- 引入潜空间多步欧拉探索与 Q 值集合体（保守目标与 GAE），提升在线学习的稳定性与效率。\n- 在 LIBERO（四个子套件）与 ALOHA Transfer Cube 任务上，π0-FPO 取得一致优于多类强基线的效果（含在线 RL、偏好对齐与扩散/自回归策略），并稳定收敛。\n## 论文方法描述\n- FPO 采用“冷启动/滚动-更新”交替流程：冻结旧策略进行滚动收集经验（轨迹、奖励、隐变量与 CFM 损失缓存），随后在当前策略上重评 CFM 损失，将损失差标准化并映射为无似然比率 ρt，最终采用截断代理目标进行策略更新。\n- 无似然比率：从旧/新策略在同一样本上的 CFM 损失差 Δℓcfm 出发，经批内标准化 zt = (Δℓcfm − μΔ)/σΔ 与单调指数映射 ρt = exp(β zt)，作为 PPO 截断代理中的重要性比替代。\n- 截断代理目标：Lactor(θ) = −E[min(ρt Ât, clip(ρt, 1−ε, 1+ε) Ât)]，其中优势 Ât 由 GAE 估计。\n- Q 值集合体：以最小化集合成员的保守目标 y_t = r_t + γ min_i Q̄_ϕi(s_{t+1}, x′_{t+1}) 进行时序差分训练，并用 Polyak 平均更新目标网络，以减少过估计与不稳定性。\n- 潜空间探索：在隐变量 x 上使用 CFM 速度场 vθ 进行多步欧拉积分生成平滑、时间相关的扰动，增强探索。\n- 数据管理：使用小滑窗轨迹缓存，保持更新策略与数据收集策略的分布接近，保证比率代理的稳定与可信。\n## 论文使用数据集和训练资源\n- 数据与预训练：采用 π0 预训练检查点作为起点，冻结其解码器 π0，仅在线更新流匹配 Actor 与 Q 集合体。\n- 训练方式：在仿真环境中进行在线交互与强化微调，无需大量监督演示数据。细节包括滚动窗口大小、K_update 更新轮次、β 与 ε 等超参数的选择与设置。\n## 论文使用的评估环境和评估指标\n- 评估环境：LIBERO 基准（ LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long）与 ALOHA Transfer Cube 仿真任务；二者均为接触丰富与稀疏奖励的典型具身控制环境。\n- 评估指标：按官方协议报告任务成功率（SR, %），并在 LIBERO 上给出各套件的平均排名；在学习动力学分析中结合成功率与平均回报曲线；消融研究则报告在指定任务上的最终成功率。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>脑认知与脑启发智能实验室（中科院自动化所）</li><li>中国科学院大学</li><li>脑认知与脑启发智能技术国家重点实验室</li><li>Long-term AI</li></ul>\n<h2>论文概述</h2>\n<p>针对基于条件流匹配（flow matching）的视觉-语言-动作模型（VLA，如 π0），传统策略梯度强化学习方法由于无法计算重要性采样比而难以应用。本文提出“流策略优化”（Flow Policy Optimization，FPO）：用基于条件流匹配（CFM）损失的样本级变化构造“无似然比率”，替换传统重要性采样，实现 PPO 风格的截断代理目标更新；并结合结构感知信用分配、截断代理、潜空间多步欧拉探索与 Q 值集合体，构建稳定且可扩展的在线强化微调框架。在 LIBERO 与 ALOHA Transfer Cube 仿真任务上进行评估，FPO 在多项指标上超越多类强基线，并呈现稳定的学习曲线与稀疏奖励下的有效探索。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>首次实现流匹配策略与 PPO 风格策略梯度更新的兼容：无须显式似然或雅可比，计算“可通约的比率代理”。</li><li>将样本级 CFM 损失差作为策略改进步信号（结构对齐的信用分配），并配套截断代理以控制更新幅度。</li><li>引入潜空间多步欧拉探索与 Q 值集合体（保守目标与 GAE），提升在线学习的稳定性与效率。</li><li>在 LIBERO（四个子套件）与 ALOHA Transfer Cube 任务上，π0-FPO 取得一致优于多类强基线的效果（含在线 RL、偏好对齐与扩散/自回归策略），并稳定收敛。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>FPO 采用“冷启动/滚动-更新”交替流程：冻结旧策略进行滚动收集经验（轨迹、奖励、隐变量与 CFM 损失缓存），随后在当前策略上重评 CFM 损失，将损失差标准化并映射为无似然比率 ρt，最终采用截断代理目标进行策略更新。</li><li>无似然比率：从旧/新策略在同一样本上的 CFM 损失差 Δℓcfm 出发，经批内标准化 zt = (Δℓcfm − μΔ)/σΔ 与单调指数映射 ρt = exp(β zt)，作为 PPO 截断代理中的重要性比替代。</li><li>截断代理目标：Lactor(θ) = −E[min(ρt Ât, clip(ρt, 1−ε, 1+ε) Ât)]，其中优势 Ât 由 GAE 估计。</li><li>Q 值集合体：以最小化集合成员的保守目标 y_t = r_t + γ min_i Q̄_ϕi(s_{t+1}, x′_{t+1}) 进行时序差分训练，并用 Polyak 平均更新目标网络，以减少过估计与不稳定性。</li><li>潜空间探索：在隐变量 x 上使用 CFM 速度场 vθ 进行多步欧拉积分生成平滑、时间相关的扰动，增强探索。</li><li>数据管理：使用小滑窗轨迹缓存，保持更新策略与数据收集策略的分布接近，保证比率代理的稳定与可信。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据与预训练：采用 π0 预训练检查点作为起点，冻结其解码器 π0，仅在线更新流匹配 Actor 与 Q 集合体。</li><li>训练方式：在仿真环境中进行在线交互与强化微调，无需大量监督演示数据。细节包括滚动窗口大小、K_update 更新轮次、β 与 ε 等超参数的选择与设置。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：LIBERO 基准（ LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long）与 ALOHA Transfer Cube 仿真任务；二者均为接触丰富与稀疏奖励的典型具身控制环境。</li><li>评估指标：按官方协议报告任务成功率（SR, %），并在 LIBERO 上给出各套件的平均排名；在学习动力学分析中结合成功率与平均回报曲线；消融研究则报告在指定任务上的最终成功率。</li></ul>"
  },
  {
    "date": "2025-10-10",
    "title": "VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation",
    "link": "http://arxiv.org/abs/2510.09607",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n南京大学、腾讯Youtu Lab、中科院(CASIA)\n## 论文概述\nVITA-VLA论文提出了一种高效的教学框架，通过动作专家蒸馏技术将小型动作模型的知识转移到预训练的大规模视觉-语言模型(VLM)中，使其具备动作执行能力。该方法解决了传统VLA模型需要大量计算资源和数据进行端到端训练的局限，通过两阶段训练策略显著降低训练成本，同时保持高性能表现。在仿真基准(LIBERO和CALVIN)和真实机器人环境中的实验验证了该方法的有效性。\n## 论文核心贡献点\n1. **精简架构设计**：在保持原始VLM结构的基础上，仅添加动作token和状态编码器，集成物理输入信息\n2. **两阶段训练策略**：第一阶段进行轻量级对齐(仅3000万参数)，第二阶段选择性微调，有效降低训练成本\n3. **优秀性能表现**：在LIBERO基准上达到97.3%平均成功率(提升11.8%)，在LIBERO-LONG上达到93.5%成功率(提升24.5%)，在CALVIN ABC-D基准上达到92.5%第一任务成功率\n4. **真实世界验证**：在ALOHA机器人平台上实现82.0%平均成功率，较teacher模型提升17%\n## 论文方法描述\n### 整体架构\n- **视觉编码器**：InternViT-300M处理图像输入\n- **连接器**：3层MLP桥接视觉和语言模态\n- **语言模型**：Qwen-2.5-7B作为基础\n- **状态编码器**：将6-DoF机械臂状态和2维夹爪状态编码为单token\n- **动作token**：作为可学习查询，重复3次预测未来3步动作\n- **动作映射器**：3层MLP将VLM隐藏状态映射到动作空间\n- **动作解码器**：复用的2层MLP生成最终执行动作\n### 两阶段训练策略\n**阶段1-对齐阶段**：\n- 训练状态编码器、动作token和动作映射器(约3000万参数)\n- 使用MSE损失对齐VLM和小动作模型的隐藏表示空间\n- 复用预训练动作解码器，避免昂贵端到端预训练\n\n**阶段2-微调阶段**：\n- 端到端微调语言模型、状态编码器、动作模块\n- 使用MAE损失监督6-DoF臂动作，BCE损失监督夹爪动作\n- 组合损失函数：L_total = L_arm + λ·L_gripper (λ=0.01)\n## 论文使用数据集和训练资源\n### 仿真数据集\n- **CALVIN ABC-D**：训练于环境A、B、C，测试于未见环境D，评估零样本泛化\n- **LIBERO基准**：4个任务套件(Spatial、Object、Goal、LONG)，每个包含10个长时序任务\n### 真实世界数据\n- 手动收集500个高质量演示轨迹(每任务100个)\n- 覆盖5个操作任务：close drawer、stack cups、stack blocks、pick place sponge、pick place block\n### 训练资源\n- 使用DeepSpeed ZeRO-2阶段优化内存使用\n- 对齐阶段：batch size=8，学习率=1e-4，训练3个epoch\n- 微调阶段：batch size=4，学习率=1e-4，训练2个epoch\n- 图像分辨率：统一为200×200像素\n## 论文使用的评估环境和评估指标\n### 评估环境\n- **仿真环境**：CALVIN和LIBERO基准测试平台\n- **真实世界**：ALOHA机器人平台(6关节PiPer机械臂+Songling夹爪)\n### 评估指标\n- **成功率**：任务完成的平均百分比\n- **平均任务长度**：连续完成指令的平均数量(CALVIN基准)\n- **长时序执行能力**：LIBERO-LONG基准上的表现\n- **真实世界性能**：5个操作任务各40次独立试验的平均成功率\n\n评估结果显示VITA-VLA在所有基准上都实现了最佳VLA模型性能，特别是在复杂长时序任务中表现出色，并在真实机器人部署中验证了方法的实际有效性。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>南京大学、腾讯Youtu Lab、中科院(CASIA)</p>\n<h2>论文概述</h2>\n<p>VITA-VLA论文提出了一种高效的教学框架，通过动作专家蒸馏技术将小型动作模型的知识转移到预训练的大规模视觉-语言模型(VLM)中，使其具备动作执行能力。该方法解决了传统VLA模型需要大量计算资源和数据进行端到端训练的局限，通过两阶段训练策略显著降低训练成本，同时保持高性能表现。在仿真基准(LIBERO和CALVIN)和真实机器人环境中的实验验证了该方法的有效性。</p>\n<h2>论文核心贡献点</h2>\n<ol><li><strong>精简架构设计</strong>：在保持原始VLM结构的基础上，仅添加动作token和状态编码器，集成物理输入信息</li><li><strong>两阶段训练策略</strong>：第一阶段进行轻量级对齐(仅3000万参数)，第二阶段选择性微调，有效降低训练成本</li><li><strong>优秀性能表现</strong>：在LIBERO基准上达到97.3%平均成功率(提升11.8%)，在LIBERO-LONG上达到93.5%成功率(提升24.5%)，在CALVIN ABC-D基准上达到92.5%第一任务成功率</li><li><strong>真实世界验证</strong>：在ALOHA机器人平台上实现82.0%平均成功率，较teacher模型提升17%</li></ol>\n<h2>论文方法描述</h2>\n<h3>整体架构</h3>\n<ul><li><strong>视觉编码器</strong>：InternViT-300M处理图像输入</li><li><strong>连接器</strong>：3层MLP桥接视觉和语言模态</li><li><strong>语言模型</strong>：Qwen-2.5-7B作为基础</li><li><strong>状态编码器</strong>：将6-DoF机械臂状态和2维夹爪状态编码为单token</li><li><strong>动作token</strong>：作为可学习查询，重复3次预测未来3步动作</li><li><strong>动作映射器</strong>：3层MLP将VLM隐藏状态映射到动作空间</li><li><strong>动作解码器</strong>：复用的2层MLP生成最终执行动作</li></ul>\n<h3>两阶段训练策略</h3>\n<p><strong>阶段1-对齐阶段</strong>：</p>\n<ul><li>训练状态编码器、动作token和动作映射器(约3000万参数)</li><li>使用MSE损失对齐VLM和小动作模型的隐藏表示空间</li><li>复用预训练动作解码器，避免昂贵端到端预训练</li></ul>\n\n<p><strong>阶段2-微调阶段</strong>：</p>\n<ul><li>端到端微调语言模型、状态编码器、动作模块</li><li>使用MAE损失监督6-DoF臂动作，BCE损失监督夹爪动作</li><li>组合损失函数：L_total = L_arm + λ·L_gripper (λ=0.01)</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<h3>仿真数据集</h3>\n<ul><li><strong>CALVIN ABC-D</strong>：训练于环境A、B、C，测试于未见环境D，评估零样本泛化</li><li><strong>LIBERO基准</strong>：4个任务套件(Spatial、Object、Goal、LONG)，每个包含10个长时序任务</li></ul>\n<h3>真实世界数据</h3>\n<ul><li>手动收集500个高质量演示轨迹(每任务100个)</li><li>覆盖5个操作任务：close drawer、stack cups、stack blocks、pick place sponge、pick place block</li></ul>\n<h3>训练资源</h3>\n<ul><li>使用DeepSpeed ZeRO-2阶段优化内存使用</li><li>对齐阶段：batch size=8，学习率=1e-4，训练3个epoch</li><li>微调阶段：batch size=4，学习率=1e-4，训练2个epoch</li><li>图像分辨率：统一为200×200像素</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<h3>评估环境</h3>\n<ul><li><strong>仿真环境</strong>：CALVIN和LIBERO基准测试平台</li><li><strong>真实世界</strong>：ALOHA机器人平台(6关节PiPer机械臂+Songling夹爪)</li></ul>\n<h3>评估指标</h3>\n<ul><li><strong>成功率</strong>：任务完成的平均百分比</li><li><strong>平均任务长度</strong>：连续完成指令的平均数量(CALVIN基准)</li><li><strong>长时序执行能力</strong>：LIBERO-LONG基准上的表现</li><li><strong>真实世界性能</strong>：5个操作任务各40次独立试验的平均成功率</li></ul>\n\n<p>评估结果显示VITA-VLA在所有基准上都实现了最佳VLA模型性能，特别是在复杂长时序任务中表现出色，并在真实机器人部署中验证了方法的实际有效性。</p>"
  },
  {
    "date": "2025-10-10",
    "title": "PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs",
    "link": "http://arxiv.org/abs/2510.09507",
    "summary_markdown": "## 论文研究单位\nHKUST(GZ)、HKUST、Beihang University、Knowin\n## 论文概述\nPhysToolBench是一个专门评估多模态大语言模型（MLLMs）物理工具理解能力的基准。它采用视觉问答（VQA）格式，涵盖超过1000个图像文本对，分为三个难度级别：工具识别（Tool Recognition）、工具理解（Tool Understanding）和工具创建（Tool Creation）。论文评估了32个不同类型的MLLMs，包括专有、开源、具体化和VLA骨干模型，发现当前MLLMs在工具理解方面存在显著不足，性能远低于人类表现（超过90% vs MLLMs最高63%）。\n## 论文核心贡献点\n1. **首个物理工具理解基准**：提出PhysToolBench，系统性地评估MLLMs对物理工具的掌握程度。\n2. **三层难度设计**：设计Easy、Medium、Hard三个级别，其中Medium细分为属性理解、工具组合和可用性理解三个子挑战，以渐进式评估深度理解。\n3. **广泛模型评估**：全面评估32个MLLMs，揭示关键弱点，如小模型缺乏涌现能力、长尾问题严重、工具可用性幻觉和视觉推理不足。\n4. **视觉中心推理框架**：提出一个初步解决方案，通过全局分析、物体检测和多层次证据整合来增强视觉推理能力。\n## 论文方法描述\n- **基准设计**：\n - 数据集包含1000+图像文本对（1024×1024图像），每个图像标注数字标签，模型需输出对应标签或“None”。\n - 三个难度级别：\n - Easy: 工具识别（如识别切菜刀用于切蔬菜）。\n - Medium: 工具理解，包含三个子挑战：\n - M.1 属性理解（如选择铸铁锅因耐高温）。\n - M.2 工具组合（如电池插入遥控器）。\n - M.3 可用性理解（如识别破损的活塞不可用）。\n - Hard: 工具创建（如用硬币代替平头螺丝刀）。\n- **数据集收集**：\n - 阶段1（概念化）：专家设计任务场景对。\n - 阶段2（图像生成）：使用GPT-4o-image（约90%）和真实摄影（约10%），人工监督质量。\n - 阶段3（标注验证）：自定义工具标注数字标签，多轮审核确保可靠性。\n- **评估方法**：\n - 使用一致文本提示，强制链式思考（CoT）推理或允许内置“思考”模式。\n - 输出工具标签或“None”，基于准确率评分。\n- **解决方案**：视觉中心推理代理框架：\n - 全局分析：整体理解任务和图像上下文。\n - 物体检测：调用DINOX工具识别并裁剪对象。\n - 多层次证据整合：结合全局和细节分析生成答案。\n## 论文使用数据集和训练资源\n- **数据集**：PhysToolBench，包含1000+图像文本对，涵盖日常、工业、户外和专业场景。图像由GPT-4o-image和真实摄影生成，人工标注数字标签。\n- **模型资源**：评估32个MLLMs，包括GPT-5、o3、GPT-4o、Claude、Gemini等专有模型，Qwen、InternVL、GLM等开源模型，RoboBrain、Embodied-R1等具体化模型，以及PaliGemma、Phi-3-Vision等VLA骨干模型。\n- **训练资源**：未训练新模型，而是使用现有模型进行评估。专有模型通过API调用，开源模型本地部署（如GLM-4.5V使用108B参数）。硬件要求未明确描述，但涉及大模型推理。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 专有模型（如GPT系列、Claude）通过各自API在线评估。\n - 开源模型（如Qwen、InternVL）本地部署，使用统一提示。\n - 人类基准：5名人类参与者作为参考。\n- **评估指标**：准确率（accuracy），以百分比表示（↑表示越高越好）。\n - 按难度级别细分：Easy、Medium（M.1/M.2/M.3）、Hard。\n - 按场景类别细分：专业（Professional）、工业（Industrial）、户外（Outdoor）、日常（Daily）。\n - 总体分数（Overall）基于加权或平均计算。表1显示人类最佳表现（如Easy 96.19%）与MLLMs最佳表现（如GPT-5 Overall 62.15%）对比。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>HKUST(GZ)、HKUST、Beihang University、Knowin</p>\n<h2>论文概述</h2>\n<p>PhysToolBench是一个专门评估多模态大语言模型（MLLMs）物理工具理解能力的基准。它采用视觉问答（VQA）格式，涵盖超过1000个图像文本对，分为三个难度级别：工具识别（Tool Recognition）、工具理解（Tool Understanding）和工具创建（Tool Creation）。论文评估了32个不同类型的MLLMs，包括专有、开源、具体化和VLA骨干模型，发现当前MLLMs在工具理解方面存在显著不足，性能远低于人类表现（超过90% vs MLLMs最高63%）。</p>\n<h2>论文核心贡献点</h2>\n<ol><li><strong>首个物理工具理解基准</strong>：提出PhysToolBench，系统性地评估MLLMs对物理工具的掌握程度。</li><li><strong>三层难度设计</strong>：设计Easy、Medium、Hard三个级别，其中Medium细分为属性理解、工具组合和可用性理解三个子挑战，以渐进式评估深度理解。</li><li><strong>广泛模型评估</strong>：全面评估32个MLLMs，揭示关键弱点，如小模型缺乏涌现能力、长尾问题严重、工具可用性幻觉和视觉推理不足。</li><li><strong>视觉中心推理框架</strong>：提出一个初步解决方案，通过全局分析、物体检测和多层次证据整合来增强视觉推理能力。</li></ol>\n<h2>论文方法描述</h2>\n<ul><li><strong>基准设计</strong>：</li></ul>\n<p> - 数据集包含1000+图像文本对（1024×1024图像），每个图像标注数字标签，模型需输出对应标签或“None”。</p>\n<p> - 三个难度级别：</p>\n<p> - Easy: 工具识别（如识别切菜刀用于切蔬菜）。</p>\n<p> - Medium: 工具理解，包含三个子挑战：</p>\n<p> - M.1 属性理解（如选择铸铁锅因耐高温）。</p>\n<p> - M.2 工具组合（如电池插入遥控器）。</p>\n<p> - M.3 可用性理解（如识别破损的活塞不可用）。</p>\n<p> - Hard: 工具创建（如用硬币代替平头螺丝刀）。</p>\n<ul><li><strong>数据集收集</strong>：</li></ul>\n<p> - 阶段1（概念化）：专家设计任务场景对。</p>\n<p> - 阶段2（图像生成）：使用GPT-4o-image（约90%）和真实摄影（约10%），人工监督质量。</p>\n<p> - 阶段3（标注验证）：自定义工具标注数字标签，多轮审核确保可靠性。</p>\n<ul><li><strong>评估方法</strong>：</li></ul>\n<p> - 使用一致文本提示，强制链式思考（CoT）推理或允许内置“思考”模式。</p>\n<p> - 输出工具标签或“None”，基于准确率评分。</p>\n<ul><li><strong>解决方案</strong>：视觉中心推理代理框架：</li></ul>\n<p> - 全局分析：整体理解任务和图像上下文。</p>\n<p> - 物体检测：调用DINOX工具识别并裁剪对象。</p>\n<p> - 多层次证据整合：结合全局和细节分析生成答案。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：PhysToolBench，包含1000+图像文本对，涵盖日常、工业、户外和专业场景。图像由GPT-4o-image和真实摄影生成，人工标注数字标签。</li><li><strong>模型资源</strong>：评估32个MLLMs，包括GPT-5、o3、GPT-4o、Claude、Gemini等专有模型，Qwen、InternVL、GLM等开源模型，RoboBrain、Embodied-R1等具体化模型，以及PaliGemma、Phi-3-Vision等VLA骨干模型。</li><li><strong>训练资源</strong>：未训练新模型，而是使用现有模型进行评估。专有模型通过API调用，开源模型本地部署（如GLM-4.5V使用108B参数）。硬件要求未明确描述，但涉及大模型推理。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 专有模型（如GPT系列、Claude）通过各自API在线评估。</p>\n<p> - 开源模型（如Qwen、InternVL）本地部署，使用统一提示。</p>\n<p> - 人类基准：5名人类参与者作为参考。</p>\n<ul><li><strong>评估指标</strong>：准确率（accuracy），以百分比表示（↑表示越高越好）。</li></ul>\n<p> - 按难度级别细分：Easy、Medium（M.1/M.2/M.3）、Hard。</p>\n<p> - 按场景类别细分：专业（Professional）、工业（Industrial）、户外（Outdoor）、日常（Daily）。</p>\n<p> - 总体分数（Overall）基于加权或平均计算。表1显示人类最佳表现（如Easy 96.19%）与MLLMs最佳表现（如GPT-5 Overall 62.15%）对比。</p>"
  },
  {
    "date": "2025-10-09",
    "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered",
    "link": "http://arxiv.org/abs/2510.08464",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-09",
    "title": "Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track",
    "link": "http://arxiv.org/abs/2510.07871",
    "summary_markdown": "## 论文研究单位\n- **香港科技大学（广州）**\n- **清华大学**\n- **中国科学院自动化研究所**\n- **小米电动汽车**\n## 论文概述\n论文提出了基于主动风险感知的社交导航方法，针对动态室内环境中的机器人导航任务。该方法在Falcon框架基础上创新性地引入了**主动风险感知模块（Proactive Risk Perception Module）**，通过预测周围人类的碰撞风险评分来增强机器人的空间感知能力。在IROS 2025 RoboSense挑战赛中，该方法获得**第二名**。\n## 论文核心贡献点\n1. **创新性风险感知模块**：提出基于距离的连续风险评分机制，将社交空间分为危险区、警告区和安全区\n2. **辅助学习优化**：通过密度监督信号提升传统强化学习在社交导航中的表现\n3. **高效集成方案**：风险模块与Falcon共享状态编码器，训练时提升导航能力，推理时零额外开销\n4. **实际竞赛验证**：在Social-HM3D数据集上优于Falcon基线方法7.46%\n## 论文方法描述\n### 核心架构\n- **主策略网络**：处理RGB-D观测和GPS+指南针信息，通过ResNet-50编码器提取视觉特征，2层LSTM处理时序依赖\n- **风险感知模块**：轻量级神经网络(公式10)，基于LSTM隐藏状态预测风险分数\n- **风险评分公式** (公式11)：\n - 危险区(d<2m)：风险评分=1.0\n - 警告区(2m≤d<4m)：风险评分线性衰减\n - 安全区(d≥4m)：风险评分=0\n### 训练优化\n- **总损失函数** (公式12)：结合主导航损失、Falcon辅助损失和风险感知损失\n- **DD-PPO算法**：在4块A40 GPU上训练75M步\n- **风险权重β_risk=0.1**：平衡风险感知与其他目标\n## 论文使用数据集和训练资源\n### 数据集\n- **Social-HM3D**：基于HM3D构建的844个真实室内场景\n- **数据特性**：\n - 人类数量按场景面积比例校准\n - 人类运动速度0.8-1.2倍机器人速度\n - 采用ORCA算法实现避障\n - 训练集/验证集/测试集比例为：大规模/1000 эпизодов/500 эпизодов\n### 计算资源\n- **4块NVIDIA A40 GPU**\n- **8并行环境训练**\n- **约75M训练步数**\n## 论文使用的评估环境和评估指标\n### 评估环境\n- **IROS 2025 RoboSense挑战赛**：Track 2社交导航轨道\n- **测试数据**：私有测试集约500个未见场景\n- **约束条件**：\n - 仅使用RGB-D观测和里程计\n - 无全局地图或特权信息\n - 禁止人类位置预测器\n### 评估指标\n- **成功率和路径效率**：\n - 成功率(SR)：到达目标1m内百分比\n - 加权路径长度(SPL)：相对最优路径的效率评估\n- **社交规范指标**：\n - 个人空间合规性(PSC)：与人类保持≥0.5m距离的时长百分比\n - 人类碰撞率(H-Coll)：发生人类碰撞的剧集百分比\n- **总成绩计算** (公式14)：Total = 0.4×SR + 0.3×SPL + 0.3×PSC\n### 竞赛结果\n- 团队排名：**16支队伍中第2名**\n- 总分：0.6994（较Falcon基线(0.6248)提升11.94%）\n- 核心指标：SR=0.656 \\|SPL=0.5958 \\|PSC=0.8608 \\|H-Coll=0.33\n- 最佳团队差距：仅0.0028分差",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li><strong>香港科技大学（广州）</strong></li><li><strong>清华大学</strong></li><li><strong>中国科学院自动化研究所</strong></li><li><strong>小米电动汽车</strong></li></ul>\n<h2>论文概述</h2>\n<p>论文提出了基于主动风险感知的社交导航方法，针对动态室内环境中的机器人导航任务。该方法在Falcon框架基础上创新性地引入了<strong>主动风险感知模块（Proactive Risk Perception Module）</strong>，通过预测周围人类的碰撞风险评分来增强机器人的空间感知能力。在IROS 2025 RoboSense挑战赛中，该方法获得<strong>第二名</strong>。</p>\n<h2>论文核心贡献点</h2>\n<ol><li><strong>创新性风险感知模块</strong>：提出基于距离的连续风险评分机制，将社交空间分为危险区、警告区和安全区</li><li><strong>辅助学习优化</strong>：通过密度监督信号提升传统强化学习在社交导航中的表现</li><li><strong>高效集成方案</strong>：风险模块与Falcon共享状态编码器，训练时提升导航能力，推理时零额外开销</li><li><strong>实际竞赛验证</strong>：在Social-HM3D数据集上优于Falcon基线方法7.46%</li></ol>\n<h2>论文方法描述</h2>\n<h3>核心架构</h3>\n<ul><li><strong>主策略网络</strong>：处理RGB-D观测和GPS+指南针信息，通过ResNet-50编码器提取视觉特征，2层LSTM处理时序依赖</li><li><strong>风险感知模块</strong>：轻量级神经网络(公式10)，基于LSTM隐藏状态预测风险分数</li><li><strong>风险评分公式</strong> (公式11)：</li></ul>\n<p> - 危险区(d<2m)：风险评分=1.0</p>\n<p> - 警告区(2m≤d<4m)：风险评分线性衰减</p>\n<p> - 安全区(d≥4m)：风险评分=0</p>\n<h3>训练优化</h3>\n<ul><li><strong>总损失函数</strong> (公式12)：结合主导航损失、Falcon辅助损失和风险感知损失</li><li><strong>DD-PPO算法</strong>：在4块A40 GPU上训练75M步</li><li><strong>风险权重β_risk=0.1</strong>：平衡风险感知与其他目标</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<h3>数据集</h3>\n<ul><li><strong>Social-HM3D</strong>：基于HM3D构建的844个真实室内场景</li><li><strong>数据特性</strong>：</li></ul>\n<p> - 人类数量按场景面积比例校准</p>\n<p> - 人类运动速度0.8-1.2倍机器人速度</p>\n<p> - 采用ORCA算法实现避障</p>\n<p> - 训练集/验证集/测试集比例为：大规模/1000 эпизодов/500 эпизодов</p>\n<h3>计算资源</h3>\n<ul><li><strong>4块NVIDIA A40 GPU</strong></li><li><strong>8并行环境训练</strong></li><li><strong>约75M训练步数</strong></li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<h3>评估环境</h3>\n<ul><li><strong>IROS 2025 RoboSense挑战赛</strong>：Track 2社交导航轨道</li><li><strong>测试数据</strong>：私有测试集约500个未见场景</li><li><strong>约束条件</strong>：</li></ul>\n<p> - 仅使用RGB-D观测和里程计</p>\n<p> - 无全局地图或特权信息</p>\n<p> - 禁止人类位置预测器</p>\n<h3>评估指标</h3>\n<ul><li><strong>成功率和路径效率</strong>：</li></ul>\n<p> - 成功率(SR)：到达目标1m内百分比</p>\n<p> - 加权路径长度(SPL)：相对最优路径的效率评估</p>\n<ul><li><strong>社交规范指标</strong>：</li></ul>\n<p> - 个人空间合规性(PSC)：与人类保持≥0.5m距离的时长百分比</p>\n<p> - 人类碰撞率(H-Coll)：发生人类碰撞的剧集百分比</p>\n<ul><li><strong>总成绩计算</strong> (公式14)：Total = 0.4×SR + 0.3×SPL + 0.3×PSC</li></ul>\n<h3>竞赛结果</h3>\n<ul><li>团队排名：<strong>16支队伍中第2名</strong></li><li>总分：0.6994（较Falcon基线(0.6248)提升11.94%）</li><li>核心指标：SR=0.656 \\|SPL=0.5958 \\|PSC=0.8608 \\|H-Coll=0.33</li><li>最佳团队差距：仅0.0028分差</li></ul>"
  },
  {
    "date": "2025-10-09",
    "title": "USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots",
    "link": "http://arxiv.org/abs/2510.07869",
    "summary_markdown": "## 论文研究单位\n- 中国科学院自动化研究所复杂系统认知与决策智能重点实验室(北京)\n- 百度公司(北京)\n- 中国科学院大学人工智能学院(北京)\n## 论文概述\n针对水下机器人数据稀缺、多任务泛化困难的问题，论文提出USIM，一个基于仿真的多任务视觉-语言-动作(VLA)数据集，覆盖9种场景和20项任务(561K帧，1,852条轨迹，约15.6小时)，并基于USIM训练得到水下通用VLA模型U0。U0采用多模态融合与卷积-注意力感知聚焦增强(CAP)模块，在检查、避障、扫描与动态跟踪等任务上平均成功率约80%，在挑战性移动抓取任务中较基线将机器人-目标距离缩短21.2%，证明仿真数据可有效驱动水下VLA能力形成。\n## 论文核心贡献点\n- 首个面向多任务、多场景的大规模水下VLA数据集USIM：561K帧/1,852轨迹/约15.6小时，20任务/9场景，涵盖抓取、检查、扫描、导航、跟踪、运输等。\n- 水下通用VLA模型U0：基于Isaac-GR00T N1.5预训练，加入多模态融合(双目、压力、IMU、DVL等)与CAP模块，显式提升水下目标感知与空间理解。\n- 建立可扩展“数据-任务”框架：仿真-数据-模型一体化管线，验证了闭环节在线评估与开环节离线评估均显著优于未微调基线，且距离指标改善21.2%。\n## 论文方法描述\n- 仿真环境：使用Stonefish构建9种水下场景(海床、海底管道、工业池、太阳能充电站、湖泊、近海工厂、现代/古代沉船等)，内置BlueROV2与机械手-夹爪；通过ROS集成与地图随机化、光照/水质变化，生成多样且逼真的视觉条件。\n- 数据生成：自动化并行采集；每任务多episode，控制层采用PID(ROV姿态跟踪)与MoveIt(机械臂规划)；数据以10Hz录制，遵循LeRobot格式。\n- 模型U0：\n - 多模态融合：视觉(左/右相机)、语言、压力/IMU/DVL等状态，归一化推进器PWM与机械臂关节角作为动作空间，采用机器人中心坐标表示目标(相对位姿)，提升动态性与跨任务泛化。\n - CAP模块：受VLM特征引导的卷积-注意力分支，强化目标检测与定位，损失为CAP的MSE与动作模块损失加权求和(推理时可关闭)。\n- 训练与部署：基于USIM对GR00T N1.5微调，总批1024、5000步；3B参数，适配NVIDIA Jetson等嵌入式平台。\n## 论文使用数据集和训练资源\n- 数据集：USIM(561K帧/1,852轨迹/约15.6小时)；训练：526K帧/1,752轨迹；测试：35K帧/100轨迹。\n- 场景与任务：9场景；20任务(12抓取、2管道检查、2沉船扫描、2避障导航、1动态跟踪、1运输)。\n- 传感器与动作：双目相机、压力传感器、IMU、DVL；推进器PWM与机械臂关节角。\n- 训练资源：基于Isaac-GR00T N1.5；批大小1024、训练步数5000；3B参数模型适配Jetson部署。\n## 论文使用的评估环境和评估指标\n- 评估方式：开环节离线评估(仿真测试集，20任务×5轨迹，共35K帧，约1小时)；闭环节在线测试(在仿真环境执行真实任务)。\n- 评估指标：\n - 动作误差 e_action(越低越好)\n - 目标误差 e_target(CAP模块，衡量定位精度)\n - 任务成功率(闭环节在线，多次试验统计)\n - 移动抓取任务的机器人-目标距离(平均距离，越低越好)\n- 主要结果：\n - 未微调GR00T N1.5在e_action上远高于微调模型，证实显著领域差异。\n - 经USIM微调后，模型在双目输入下优于单目；U0相对GR00T FT在单目与双目e_action上分别再降7.7%与4.2%。\n - 闭环节在线：U0在7项非抓取任务上平均成功率约80%，双目优于单目，并超过GR00T FT。\n - 移动抓取：U0(双目)将平均距离较GR00T FT缩短21.2%，显示对复杂流体力与机体-机械臂-目标交互的更好适应能力。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>中国科学院自动化研究所复杂系统认知与决策智能重点实验室(北京)</li><li>百度公司(北京)</li><li>中国科学院大学人工智能学院(北京)</li></ul>\n<h2>论文概述</h2>\n<p>针对水下机器人数据稀缺、多任务泛化困难的问题，论文提出USIM，一个基于仿真的多任务视觉-语言-动作(VLA)数据集，覆盖9种场景和20项任务(561K帧，1,852条轨迹，约15.6小时)，并基于USIM训练得到水下通用VLA模型U0。U0采用多模态融合与卷积-注意力感知聚焦增强(CAP)模块，在检查、避障、扫描与动态跟踪等任务上平均成功率约80%，在挑战性移动抓取任务中较基线将机器人-目标距离缩短21.2%，证明仿真数据可有效驱动水下VLA能力形成。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>首个面向多任务、多场景的大规模水下VLA数据集USIM：561K帧/1,852轨迹/约15.6小时，20任务/9场景，涵盖抓取、检查、扫描、导航、跟踪、运输等。</li><li>水下通用VLA模型U0：基于Isaac-GR00T N1.5预训练，加入多模态融合(双目、压力、IMU、DVL等)与CAP模块，显式提升水下目标感知与空间理解。</li><li>建立可扩展“数据-任务”框架：仿真-数据-模型一体化管线，验证了闭环节在线评估与开环节离线评估均显著优于未微调基线，且距离指标改善21.2%。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>仿真环境：使用Stonefish构建9种水下场景(海床、海底管道、工业池、太阳能充电站、湖泊、近海工厂、现代/古代沉船等)，内置BlueROV2与机械手-夹爪；通过ROS集成与地图随机化、光照/水质变化，生成多样且逼真的视觉条件。</li><li>数据生成：自动化并行采集；每任务多episode，控制层采用PID(ROV姿态跟踪)与MoveIt(机械臂规划)；数据以10Hz录制，遵循LeRobot格式。</li><li>模型U0：</li></ul>\n<p> - 多模态融合：视觉(左/右相机)、语言、压力/IMU/DVL等状态，归一化推进器PWM与机械臂关节角作为动作空间，采用机器人中心坐标表示目标(相对位姿)，提升动态性与跨任务泛化。</p>\n<p> - CAP模块：受VLM特征引导的卷积-注意力分支，强化目标检测与定位，损失为CAP的MSE与动作模块损失加权求和(推理时可关闭)。</p>\n<ul><li>训练与部署：基于USIM对GR00T N1.5微调，总批1024、5000步；3B参数，适配NVIDIA Jetson等嵌入式平台。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据集：USIM(561K帧/1,852轨迹/约15.6小时)；训练：526K帧/1,752轨迹；测试：35K帧/100轨迹。</li><li>场景与任务：9场景；20任务(12抓取、2管道检查、2沉船扫描、2避障导航、1动态跟踪、1运输)。</li><li>传感器与动作：双目相机、压力传感器、IMU、DVL；推进器PWM与机械臂关节角。</li><li>训练资源：基于Isaac-GR00T N1.5；批大小1024、训练步数5000；3B参数模型适配Jetson部署。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估方式：开环节离线评估(仿真测试集，20任务×5轨迹，共35K帧，约1小时)；闭环节在线测试(在仿真环境执行真实任务)。</li><li>评估指标：</li></ul>\n<p> - 动作误差 e_action(越低越好)</p>\n<p> - 目标误差 e_target(CAP模块，衡量定位精度)</p>\n<p> - 任务成功率(闭环节在线，多次试验统计)</p>\n<p> - 移动抓取任务的机器人-目标距离(平均距离，越低越好)</p>\n<ul><li>主要结果：</li></ul>\n<p> - 未微调GR00T N1.5在e_action上远高于微调模型，证实显著领域差异。</p>\n<p> - 经USIM微调后，模型在双目输入下优于单目；U0相对GR00T FT在单目与双目e_action上分别再降7.7%与4.2%。</p>\n<p> - 闭环节在线：U0在7项非抓取任务上平均成功率约80%，双目优于单目，并超过GR00T FT。</p>\n<p> - 移动抓取：U0(双目)将平均距离较GR00T FT缩短21.2%，显示对复杂流体力与机体-机械臂-目标交互的更好适应能力。</p>"
  },
  {
    "date": "2025-10-09",
    "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction",
    "link": "http://arxiv.org/abs/2510.07778",
    "summary_markdown": "# 论文研究单位\n- 哈尔滨工业大学（深圳）\n- 南京大学\n- 中国科学技术大学\n- Dexmal\n# 论文概述\n论文提出IntentionVLA，一个用于人机交互的具身意图推理VLA框架。当前视觉语言动作模型主要依赖显式指令映射到动作，缺乏推理密集型预训练和推理引导操作，无法在复杂现实交互中执行隐含人类意图推理。IntentionVLA通过课程训练范式和高效推理机制，首先利用精心设计的推理数据结合意图推理、空间 grounding和紧凑具身推理，为模型赋予推理和感知能力，然后在微调阶段采用紧凑推理输出作为动作生成的上下文指导，实现间接指令下的快速推理。\n# 论文核心贡献点\n- 提出统一的VLA模型IntentionVLA，训练于精心策划的意图推理数据，通过两阶段范式和高效推理机制明确桥接高级语义推理与低级动作执行\n- 在所有评估设置中显著优于最先进的VLA基线，展现强泛化能力和实时交互能力\n- 针对现有VLA模型无法理解隐含人类意图的问题，提供解决方案\n# 论文方法描述\n构建了涵盖意图推理、空间推理和紧凑推理的综合数据格式，采用高效标注流程从日常工作环境收集数据。模型基于Qwen2.5-7B构建，包含VLM骨干、可学习查询、连接器和扩散Transformer。两阶段训练：第一阶段训练VLM骨干进行意图推理和空间感知，第二阶段训练动作模块，将语义推理转化为可执行动作。推理时采用紧凑推理机制，实现0.2秒内的快速响应。\n# 论文使用数据集和训练资源\n基于日常办公环境构建的具身意图推理数据集，使用GPT-4o和Florence-2等预训练模型进行数据标注。实验使用WidowX-250s机械臂配备Realsense D435i摄像头，训练了IntentionVLA、ECoT、CogACT和π₀等模型进行对比。\n# 论文使用的评估环境和评估指标\n采用任务成功率作为评估指标（每任务10次试验）。评估环境包括：\n- 分布内任务：直接指令和意图指令\n- 分布外设置：未见指令、陌生物体操控\n- 零样本人机交互：真实人手交互测试实时响应能力\n实验在真实办公环境中进行，对象位置和放置姿态随机化，要求机器人完整正确完成整个交互过程。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>哈尔滨工业大学（深圳）</li><li>南京大学</li><li>中国科学技术大学</li><li>Dexmal</li></ul>\n<h1>论文概述</h1>\n<p>论文提出IntentionVLA，一个用于人机交互的具身意图推理VLA框架。当前视觉语言动作模型主要依赖显式指令映射到动作，缺乏推理密集型预训练和推理引导操作，无法在复杂现实交互中执行隐含人类意图推理。IntentionVLA通过课程训练范式和高效推理机制，首先利用精心设计的推理数据结合意图推理、空间 grounding和紧凑具身推理，为模型赋予推理和感知能力，然后在微调阶段采用紧凑推理输出作为动作生成的上下文指导，实现间接指令下的快速推理。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出统一的VLA模型IntentionVLA，训练于精心策划的意图推理数据，通过两阶段范式和高效推理机制明确桥接高级语义推理与低级动作执行</li><li>在所有评估设置中显著优于最先进的VLA基线，展现强泛化能力和实时交互能力</li><li>针对现有VLA模型无法理解隐含人类意图的问题，提供解决方案</li></ul>\n<h1>论文方法描述</h1>\n<p>构建了涵盖意图推理、空间推理和紧凑推理的综合数据格式，采用高效标注流程从日常工作环境收集数据。模型基于Qwen2.5-7B构建，包含VLM骨干、可学习查询、连接器和扩散Transformer。两阶段训练：第一阶段训练VLM骨干进行意图推理和空间感知，第二阶段训练动作模块，将语义推理转化为可执行动作。推理时采用紧凑推理机制，实现0.2秒内的快速响应。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>基于日常办公环境构建的具身意图推理数据集，使用GPT-4o和Florence-2等预训练模型进行数据标注。实验使用WidowX-250s机械臂配备Realsense D435i摄像头，训练了IntentionVLA、ECoT、CogACT和π₀等模型进行对比。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>采用任务成功率作为评估指标（每任务10次试验）。评估环境包括：</p>\n<ul><li>分布内任务：直接指令和意图指令</li><li>分布外设置：未见指令、陌生物体操控</li><li>零样本人机交互：真实人手交互测试实时响应能力</li></ul>\n<p>实验在真实办公环境中进行，对象位置和放置姿态随机化，要求机器人完整正确完成整个交互过程。</p>"
  },
  {
    "date": "2025-10-08",
    "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2510.07313",
    "summary_markdown": "## 论文研究单位\n- 北京大学多媒体信息处理国家重点实验室（计算机学院）\n- 香港科技大学\n- 新加坡国立大学\n- 北京人形机器人创新中心\n## 论文概述\nWristWorld提出首个从第三人称“锚点视角”生成腕部视角视频的4D世界模型，无需首帧腕部视角输入，即可合成时空一致且几何对齐的腕部视角序列，用于数据增强与下游视觉–语言–动作（VLA）策略提升。\n## 论文核心贡献点\n- 首个两阶段4D生成框架，在仅提供锚点视图的条件下，实现腕部视角视频的时空与几何一致合成。\n- 设计“腕部头”（wrist head）与空间投影一致性损失（SPC），从密集2D–2D对应与重建点云中监督腕部相机位姿，无需深度或外参。\n- 引入CLIP编码的锚点视角语义与文本提示，联合几何投影条件，指导视频扩散模型合成更真实、对齐的腕部视角。\n- 作为即插即用模块扩展单视角世界模型为多视角，无需新增腕部数据即可提供腕部视角训练样本。\n- 在Droid、Calvin与Franka Panda上实现SOTA视频质量，显著提升VLA性能：Calvin平均任务完成长度提升3.81%，缩小锚点–腕部性能差距42.4%。\n## 论文方法描述\n- 两阶段4D生成：\n - 重建阶段：扩展VGGT特征并通过“腕部头”与SPC损失估计腕部相机位姿与4D点云，投影得到时间对齐的条件图。\n - 生成阶段：基于Video DiT，将腕部投影条件与CLIP锚点语义及文本共同作为条件，生成腕部视角视频；结构上支持以[腕部潜变量；条件潜变量]双通道输入。\n- 训练目标包含标准扩散噪声预测损失与SPC投影损失（像素重投影误差+深度可行性项）。\n## 论文使用数据集和训练资源\n- 数据集：\n - Droid：约76k视频，覆盖59个任务，含ext1/ext2与腕部相机；10k子集预训练，100视频验证。\n - Calvin（模拟）：多任务语言条件基准，使用D split的10%数据。\n - Franka Panda（实机）：1700演示，3静态外视角+腕部视角；保留100视频评估。\n- 训练资源：\n - 重建阶段：Droid预训练在8×A800 GPU约12小时，640×480分辨率，批4。\n - 生成阶段：在8×A800 GPU约24小时，条件token长度512。\n - Franka跨视角微调：重建6小时、生成12小时，相同批大小、分辨率与token长度。\n## 论文使用的评估环境和评估指标\n- 评估环境：Droid、Calvin、Franka Panda。\n- 视频质量指标：FVD（越低越好）、LPIPS（越低越好）、SSIM（越高越好）、PSNR（越高越好）。\n- VLA评估（Calvin）：逐任务连续成功率的1/5–5/5统计与平均完成长度（Avg. Len.）。\n- 消融实验：评估腕部投影、CLIP锚点语义与SPC损失的贡献。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>北京大学多媒体信息处理国家重点实验室（计算机学院）</li><li>香港科技大学</li><li>新加坡国立大学</li><li>北京人形机器人创新中心</li></ul>\n<h2>论文概述</h2>\n<p>WristWorld提出首个从第三人称“锚点视角”生成腕部视角视频的4D世界模型，无需首帧腕部视角输入，即可合成时空一致且几何对齐的腕部视角序列，用于数据增强与下游视觉–语言–动作（VLA）策略提升。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>首个两阶段4D生成框架，在仅提供锚点视图的条件下，实现腕部视角视频的时空与几何一致合成。</li><li>设计“腕部头”（wrist head）与空间投影一致性损失（SPC），从密集2D–2D对应与重建点云中监督腕部相机位姿，无需深度或外参。</li><li>引入CLIP编码的锚点视角语义与文本提示，联合几何投影条件，指导视频扩散模型合成更真实、对齐的腕部视角。</li><li>作为即插即用模块扩展单视角世界模型为多视角，无需新增腕部数据即可提供腕部视角训练样本。</li><li>在Droid、Calvin与Franka Panda上实现SOTA视频质量，显著提升VLA性能：Calvin平均任务完成长度提升3.81%，缩小锚点–腕部性能差距42.4%。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>两阶段4D生成：</li></ul>\n<p> - 重建阶段：扩展VGGT特征并通过“腕部头”与SPC损失估计腕部相机位姿与4D点云，投影得到时间对齐的条件图。</p>\n<p> - 生成阶段：基于Video DiT，将腕部投影条件与CLIP锚点语义及文本共同作为条件，生成腕部视角视频；结构上支持以[腕部潜变量；条件潜变量]双通道输入。</p>\n<ul><li>训练目标包含标准扩散噪声预测损失与SPC投影损失（像素重投影误差+深度可行性项）。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据集：</li></ul>\n<p> - Droid：约76k视频，覆盖59个任务，含ext1/ext2与腕部相机；10k子集预训练，100视频验证。</p>\n<p> - Calvin（模拟）：多任务语言条件基准，使用D split的10%数据。</p>\n<p> - Franka Panda（实机）：1700演示，3静态外视角+腕部视角；保留100视频评估。</p>\n<ul><li>训练资源：</li></ul>\n<p> - 重建阶段：Droid预训练在8×A800 GPU约12小时，640×480分辨率，批4。</p>\n<p> - 生成阶段：在8×A800 GPU约24小时，条件token长度512。</p>\n<p> - Franka跨视角微调：重建6小时、生成12小时，相同批大小、分辨率与token长度。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：Droid、Calvin、Franka Panda。</li><li>视频质量指标：FVD（越低越好）、LPIPS（越低越好）、SSIM（越高越好）、PSNR（越高越好）。</li><li>VLA评估（Calvin）：逐任务连续成功率的1/5–5/5统计与平均完成长度（Avg. Len.）。</li><li>消融实验：评估腕部投影、CLIP锚点语义与SPC损失的贡献。</li></ul>"
  },
  {
    "date": "2025-10-08",
    "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
    "link": "http://arxiv.org/abs/2510.07134",
    "summary_markdown": "### 论文研究单位\n北京大学、Galbot、中国科学技术大学、BAAI、北京航空航天大学、南方科技大学、北京师范大学联合研究。\n### 论文概述\n提出TrackVLA++，一种增强具身视觉跟踪（EVT）能力的视觉-语言-动作（VLA）模型。针对现有方法在动态场景中因缺乏显式空间推理和有效时间记忆而失效的问题，TrackVLA++通过引入极坐标思维链（Polar-CoT）机制和目标识别记忆（TIM）模块，显著提升追踪性能与鲁棒性，支持多视角扩展，并在模拟和真实场景中验证了SOTA表现。\n### 论文核心贡献点\n1. **Polar-CoT机制**：将目标相对位置编码为极坐标token，提供轻量级空间推理能力，优于传统边界框方法。\n2. **TIM模块**：采用置信度门控记忆更新策略，抵抗遮挡和干扰物，实现长期目标识别。\n3. **跨域泛化**：在EVT-Bench和Gym-UnrealCV等基准测试中实现SOTA，并在真实机器人任务中验证零样本泛化。\n### 论文方法描述\n- **架构**：基于导航基础模型NavFoM构建，结合双编码器提取视觉特征（SigLIP和DINOv2）。\n- **Polar-CoT模块**：\n - 将代理感知空间（0.6m-5.0m）离散化为60角度×30距离网格，编码为唯一词汇token（含<invalid> token处理遮挡）。\n - 输入：视觉特征、语言token和TIM记忆；输出：紧凑reasoning token。\n- **TIM模块**：\n - 通过加权平均更新记忆：权重由预测置信度（归一化熵）决定，仅高置信度时融合新特征。\n - 初始化为空状态，首次有效特征后生效。\n- **训练流程**：\n - 输入序列：LLM接收视觉、语言和reasoning token；输出动作token经MLP解码为8步轨迹。\n - 损失函数：轨迹MSE损失、推理对数损失和文本损失（权重α=0.2，β=0.5）。\n- **数据集**：混合200万样本（100万EVT-Bench跟踪数据 + 100万QA数据：SYNTH-PEDES、图像/视频QA）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 模拟：EVT-Bench（STT/DT/AT分割）、Gym-UnrealCV（Single Target/Distractor/Unseen Objects）。\n - 真实：Unitree GO2机器人多视角RGB流（Obstacle/Winding Path/Distractor场景）。\n- **训练资源**：\n - 硬件：8块NVIDIA H100 GPU。\n - 时间：约192 GPU小时（约一天）。\n - 推理速度：4.8 FPS（对比NavFoM 5.1 FPS）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟：EVT-Bench（DT分割作为主要测试）、Gym-UnrealCV。\n - 真实：Unitree GO2四足机器人，配备4台SG3S11AFxK摄像头。\n- **评估指标**：\n - **SR**（Success Rate）：成功完成任务（1-3米内正确定向）的比例。\n - **TR**（Tracking Rate）：成功跟踪时间步的比例。\n - **CR**（Collision Rate）：因碰撞终止任务的比例。\n - **EL**（Episode Length）：Gym-UnrealCV中的平均步长（最大500）。\n - **识别准确率**：零样本人类识别任务（SYNTH-PEDES数据集）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学、Galbot、中国科学技术大学、BAAI、北京航空航天大学、南方科技大学、北京师范大学联合研究。</p>\n<h3>论文概述</h3>\n<p>提出TrackVLA++，一种增强具身视觉跟踪（EVT）能力的视觉-语言-动作（VLA）模型。针对现有方法在动态场景中因缺乏显式空间推理和有效时间记忆而失效的问题，TrackVLA++通过引入极坐标思维链（Polar-CoT）机制和目标识别记忆（TIM）模块，显著提升追踪性能与鲁棒性，支持多视角扩展，并在模拟和真实场景中验证了SOTA表现。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>Polar-CoT机制</strong>：将目标相对位置编码为极坐标token，提供轻量级空间推理能力，优于传统边界框方法。</li><li><strong>TIM模块</strong>：采用置信度门控记忆更新策略，抵抗遮挡和干扰物，实现长期目标识别。</li><li><strong>跨域泛化</strong>：在EVT-Bench和Gym-UnrealCV等基准测试中实现SOTA，并在真实机器人任务中验证零样本泛化。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>架构</strong>：基于导航基础模型NavFoM构建，结合双编码器提取视觉特征（SigLIP和DINOv2）。</li><li><strong>Polar-CoT模块</strong>：</li></ul>\n<p> - 将代理感知空间（0.6m-5.0m）离散化为60角度×30距离网格，编码为唯一词汇token（含<invalid> token处理遮挡）。</p>\n<p> - 输入：视觉特征、语言token和TIM记忆；输出：紧凑reasoning token。</p>\n<ul><li><strong>TIM模块</strong>：</li></ul>\n<p> - 通过加权平均更新记忆：权重由预测置信度（归一化熵）决定，仅高置信度时融合新特征。</p>\n<p> - 初始化为空状态，首次有效特征后生效。</p>\n<ul><li><strong>训练流程</strong>：</li></ul>\n<p> - 输入序列：LLM接收视觉、语言和reasoning token；输出动作token经MLP解码为8步轨迹。</p>\n<p> - 损失函数：轨迹MSE损失、推理对数损失和文本损失（权重α=0.2，β=0.5）。</p>\n<ul><li><strong>数据集</strong>：混合200万样本（100万EVT-Bench跟踪数据 + 100万QA数据：SYNTH-PEDES、图像/视频QA）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟：EVT-Bench（STT/DT/AT分割）、Gym-UnrealCV（Single Target/Distractor/Unseen Objects）。</p>\n<p> - 真实：Unitree GO2机器人多视角RGB流（Obstacle/Winding Path/Distractor场景）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 硬件：8块NVIDIA H100 GPU。</p>\n<p> - 时间：约192 GPU小时（约一天）。</p>\n<p> - 推理速度：4.8 FPS（对比NavFoM 5.1 FPS）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟：EVT-Bench（DT分割作为主要测试）、Gym-UnrealCV。</p>\n<p> - 真实：Unitree GO2四足机器人，配备4台SG3S11AFxK摄像头。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>SR</strong>（Success Rate）：成功完成任务（1-3米内正确定向）的比例。</p>\n<p> - <strong>TR</strong>（Tracking Rate）：成功跟踪时间步的比例。</p>\n<p> - <strong>CR</strong>（Collision Rate）：因碰撞终止任务的比例。</p>\n<p> - <strong>EL</strong>（Episode Length）：Gym-UnrealCV中的平均步长（最大500）。</p>\n<p> - <strong>识别准确率</strong>：零样本人类识别任务（SYNTH-PEDES数据集）。</p>"
  },
  {
    "date": "2025-10-08",
    "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
    "link": "http://arxiv.org/abs/2510.07077",
    "summary_markdown": "# 论文研究单位\n- 东京大学（The University of Tokyo）\n- 牛津大学（University of Oxford）\n- 德克萨斯大学奥斯汀分校（The University of Texas at Austin）\n# 论文概述\n- 背景：大模型（LLM/VLM）在NLP和CV快速进展，推动机器人领域的泛化与可扩展性需求；但“高级语言理解与具体动作执行”之间仍存在鸿沟，早期系统多采用“高层规划+预设动作库/模仿学习”的解耦方式，泛化受限。\n- VLA定义：将视觉、自然语言作为必要输入，直接生成机器人控制命令的端到端系统；排除仅做高层技能选择或无动作落地的方案。\n- 研究范围：系统综述VLA从历史演进、架构与模块、模态处理、学习范式，到数据采集、公开数据集、数据增强与评估基准的全栈方法；同时提供从业者指南。\n- 方法：覆盖从早期CNN/MLP到Transformer/VLM主干，再到扩散/流匹配、潜在动作视频预训练与世界模型/可供性融合，以及分层/CoT推理等路线；结合多数据集与多机器人平台进行分析。\n- 贡献：提出可落地的VLA全景与实践建议，分类并总结代表性工作，形成统一视角以支持实际部署。\n# 论文核心贡献点\n- 首个覆盖软件与硬件、架构与训练、数据与评估的全栈VLA综述。\n- 按生成策略梳理历史演进：CNN→序列Transformer→预训练VLM主干→扩散/流匹配→潜在动作学习→分层控制。\n- 将架构系统化为传感运动模型、世界模型、可供性模型三大类，并细分七种传感运动架构变体。\n- 总结多模态处理策略（视觉、语言、动作与附加模态）以及动作表征（离散化、连续生成、潜在动作、跨本体统一）。\n- 归纳学习范式：监督、自监督、强化学习（含与RL组合的高层/低层策略）；给出预训练/后训练实践建议。\n- 汇总数据采集方法、公开数据集（OXE、RT系列等）、数据增强与跨本体迁移方案。\n- 梳理真实机器人平台、评估基准与指标，并提炼工程落地建议与未来方向（推理、安全、持续学习、评估等）。\n# 论文方法描述\n- 架构与设计过渡：\n - 早期端到端CNN/MLP（如CLIPort）：CLIP特征+Transporter网络，端到端学习操作任务，但可扩展性有限。\n - 序列Transformer（Gato、VIMA）：统一令牌化，图像（ViT/目标检测）、语言（T5/Tokenizers）拼接为序列，自回归生成动作；在仿真中表现良好。\n - 预训练VLM主干（RT-1/RT-2/RT-X、OpenVLA）：以VLM（PaLM-E、PaLI-X、Prismatic/DINOv2/SigLIP等）为骨干，在大规模机器人数据上联合微调，实现真实世界可扩展；RT-2/RT-X进一步提升跨环境/本体泛化。\n - 扩散/流匹配策略（Octo、RDT-1B、π0/GR00T N1）：在Transformer后加扩散头或以扩散Transformer为骨干，生成平滑连续动作；π0引入流匹配与动作专家，实现50Hz实时控制；GR00T N1将潜在动作、扩散与流匹配整合为多阶段策略。\n - 潜在动作视频预训练（LAPA）：从无标注人类视频中学习潜在动作，桥接人类演示与机器人动作空间，兼容真实部署。\n - 分层与CoT推理（RT-H、π0.5、ECoT/CoT-VLA）：高层策略预测“语言动作/子任务/中间表示”，低层策略生成精细控制；或通过思维链逐步推理（生成子任务、关键点、目标图）提升长时序与复杂任务的稳健性。\n- 核心架构类型：\n - 传感运动模型（7类）：Transformer+离散动作、Transformer+扩散头、扩散Transformer、VLM+离散、VLM+扩散、VLM+流匹配、VLM+扩散Transformer；对应自回归、非自回归、连续控制与实时响应等不同折中。\n - 世界模型（3类）：预测未来视觉→逆动力学生成动作；学习潜在动作以利用人类视频；端到端联合预测动作与未来观测以增强规划与泛化。\n - 可供性模型（3类）：VLM估计可供性热图/接触点/关键姿态引导控制；从人类数据抽取可供性；与传感运动模型融合，直接生成条件动作。\n- 模态处理：\n - 视觉：ViT/DINOv2/SigLIP/CLIP特征，Q-Former/TokenLearner压缩；目标级表示（边界框/分割/跟踪）常与语言对齐。\n - 语言：继承LLM分词器（T5/LLaMA等），USE/CLIP文本编码器或VLM直接融合；FiLM等条件融合常用。\n - 动作：离散化分桶（256桶/保留低频词）、MLP解码为连续、扩散/流匹配生成连续动作、DCT/FAST降低序列长度；潜在动作（VQ-VAE/视频重建）；跨本体统一（统一视角、共享原子动作空间、异构令牌统一）。\n - 附加模态：音频（ASR/声学编码器）、触觉（图像化触觉+ViT/TVL）、3D（深度图/多视角/体素/点云/NeRF/高斯Splatting），提升接触与空间感知。\n- 训练与实现：\n - 监督学习：预训练（用VLM/多源数据）→后训练（高质量机器人演示）；上下文学习用于少样本提示。\n - 自监督：模态对齐（对比学习）、视觉自监督（MAE/CLIP/DINOv2）、潜在动作自监督（视频重建）。\n - 强化学习：两类组合——用RL微调VLA（如SFT→RL→SFT、RPRM稠密奖励、潜空间RL避免扩散回传不稳）；或VLA做高层决策、RL做低层控制（足式/人形/移动操作）。\n - 预训练阶段：数据规模与多样性（跨机器人/人类/合成/多任务损失）；常用VLM骨干（PaLM-E、PaLI-X、PaliGemma等）。\n - 推理：支持非自回归并行输出、目标图像/语言指令输入、多模态条件与低时延控制；实时性通过流匹配或序列压缩优化。\n- 数据与增强：遥感与多模态采集、统一格式（单相机+7-DoF动作），公开数据集（Open-X Embodiment、RT-1数据、多机器人/人类视频），数据增强（合成轨迹/世界模型生成/光学流与特征点跟踪/图像编辑）。\n# 论文使用数据集和训练资源\n- 公开数据与平台：\n - Open-X Embodiment (OXE)：多机器人多本体数据，统一格式，促进跨本体泛化。\n - RT-1/RT-2数据：大规模真实世界机器人演示，涵盖数百任务与长序列。\n - 人类视频数据集：Ego4D、EPIC-KITCHENS等，用于潜在动作与可供性抽取。\n - 仿真数据：任务/轨迹/合成视觉样本，用于长时序与多样场景扩展。\n- 预训练资源：\n - 多源混合：真实机器人轨迹、互联网规模视觉语言数据、检测/推理辅助损失、合成轨迹与COSMOS等世界模型生成数据。\n - VLM骨干：PaLM-E、PaLI-X、PaliGemma、LLaMA 2/Qwen2等；视觉编码器（DINOv2/SigLIP、EfficientNet、MAE-ViT）。\n- 计算与训练关注：Transformer为主，序列长度与高分辨率视觉导致显存/时延压力；采用TokenLearner/Q-Former压缩、流匹配加速采样、并行非自回归解码；建议端到端微调+动作头适配的折中训练策略。\n# 论文使用的评估环境和评估指标\n- 评估环境与平台：\n - 真实机器人：机械臂、移动操作、人形与足式机器人等多样化本体与场景。\n - 仿真到真实：模拟环境用于训练与数据扩增，结合真实部署验证。\n- 评估指标：\n - 任务层：成功率、平均回报、EpLen、鲁棒性（噪声/遮挡/跨域）。\n - 控制层：时延与频率（如50Hz）、动作平滑性与稳定性。\n - 泛化：跨任务/对象/环境/本体的转移能力。\n - 安全与失败检测：碰撞率、回退策略、恢复成功率。\n - 效率：参数规模、推理时延、部署资源占用。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>东京大学（The University of Tokyo）</li><li>牛津大学（University of Oxford）</li><li>德克萨斯大学奥斯汀分校（The University of Texas at Austin）</li></ul>\n<h1>论文概述</h1>\n<ul><li>背景：大模型（LLM/VLM）在NLP和CV快速进展，推动机器人领域的泛化与可扩展性需求；但“高级语言理解与具体动作执行”之间仍存在鸿沟，早期系统多采用“高层规划+预设动作库/模仿学习”的解耦方式，泛化受限。</li><li>VLA定义：将视觉、自然语言作为必要输入，直接生成机器人控制命令的端到端系统；排除仅做高层技能选择或无动作落地的方案。</li><li>研究范围：系统综述VLA从历史演进、架构与模块、模态处理、学习范式，到数据采集、公开数据集、数据增强与评估基准的全栈方法；同时提供从业者指南。</li><li>方法：覆盖从早期CNN/MLP到Transformer/VLM主干，再到扩散/流匹配、潜在动作视频预训练与世界模型/可供性融合，以及分层/CoT推理等路线；结合多数据集与多机器人平台进行分析。</li><li>贡献：提出可落地的VLA全景与实践建议，分类并总结代表性工作，形成统一视角以支持实际部署。</li></ul>\n<h1>论文核心贡献点</h1>\n<ul><li>首个覆盖软件与硬件、架构与训练、数据与评估的全栈VLA综述。</li><li>按生成策略梳理历史演进：CNN→序列Transformer→预训练VLM主干→扩散/流匹配→潜在动作学习→分层控制。</li><li>将架构系统化为传感运动模型、世界模型、可供性模型三大类，并细分七种传感运动架构变体。</li><li>总结多模态处理策略（视觉、语言、动作与附加模态）以及动作表征（离散化、连续生成、潜在动作、跨本体统一）。</li><li>归纳学习范式：监督、自监督、强化学习（含与RL组合的高层/低层策略）；给出预训练/后训练实践建议。</li><li>汇总数据采集方法、公开数据集（OXE、RT系列等）、数据增强与跨本体迁移方案。</li><li>梳理真实机器人平台、评估基准与指标，并提炼工程落地建议与未来方向（推理、安全、持续学习、评估等）。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>架构与设计过渡：</li></ul>\n<p> - 早期端到端CNN/MLP（如CLIPort）：CLIP特征+Transporter网络，端到端学习操作任务，但可扩展性有限。</p>\n<p> - 序列Transformer（Gato、VIMA）：统一令牌化，图像（ViT/目标检测）、语言（T5/Tokenizers）拼接为序列，自回归生成动作；在仿真中表现良好。</p>\n<p> - 预训练VLM主干（RT-1/RT-2/RT-X、OpenVLA）：以VLM（PaLM-E、PaLI-X、Prismatic/DINOv2/SigLIP等）为骨干，在大规模机器人数据上联合微调，实现真实世界可扩展；RT-2/RT-X进一步提升跨环境/本体泛化。</p>\n<p> - 扩散/流匹配策略（Octo、RDT-1B、π0/GR00T N1）：在Transformer后加扩散头或以扩散Transformer为骨干，生成平滑连续动作；π0引入流匹配与动作专家，实现50Hz实时控制；GR00T N1将潜在动作、扩散与流匹配整合为多阶段策略。</p>\n<p> - 潜在动作视频预训练（LAPA）：从无标注人类视频中学习潜在动作，桥接人类演示与机器人动作空间，兼容真实部署。</p>\n<p> - 分层与CoT推理（RT-H、π0.5、ECoT/CoT-VLA）：高层策略预测“语言动作/子任务/中间表示”，低层策略生成精细控制；或通过思维链逐步推理（生成子任务、关键点、目标图）提升长时序与复杂任务的稳健性。</p>\n<ul><li>核心架构类型：</li></ul>\n<p> - 传感运动模型（7类）：Transformer+离散动作、Transformer+扩散头、扩散Transformer、VLM+离散、VLM+扩散、VLM+流匹配、VLM+扩散Transformer；对应自回归、非自回归、连续控制与实时响应等不同折中。</p>\n<p> - 世界模型（3类）：预测未来视觉→逆动力学生成动作；学习潜在动作以利用人类视频；端到端联合预测动作与未来观测以增强规划与泛化。</p>\n<p> - 可供性模型（3类）：VLM估计可供性热图/接触点/关键姿态引导控制；从人类数据抽取可供性；与传感运动模型融合，直接生成条件动作。</p>\n<ul><li>模态处理：</li></ul>\n<p> - 视觉：ViT/DINOv2/SigLIP/CLIP特征，Q-Former/TokenLearner压缩；目标级表示（边界框/分割/跟踪）常与语言对齐。</p>\n<p> - 语言：继承LLM分词器（T5/LLaMA等），USE/CLIP文本编码器或VLM直接融合；FiLM等条件融合常用。</p>\n<p> - 动作：离散化分桶（256桶/保留低频词）、MLP解码为连续、扩散/流匹配生成连续动作、DCT/FAST降低序列长度；潜在动作（VQ-VAE/视频重建）；跨本体统一（统一视角、共享原子动作空间、异构令牌统一）。</p>\n<p> - 附加模态：音频（ASR/声学编码器）、触觉（图像化触觉+ViT/TVL）、3D（深度图/多视角/体素/点云/NeRF/高斯Splatting），提升接触与空间感知。</p>\n<ul><li>训练与实现：</li></ul>\n<p> - 监督学习：预训练（用VLM/多源数据）→后训练（高质量机器人演示）；上下文学习用于少样本提示。</p>\n<p> - 自监督：模态对齐（对比学习）、视觉自监督（MAE/CLIP/DINOv2）、潜在动作自监督（视频重建）。</p>\n<p> - 强化学习：两类组合——用RL微调VLA（如SFT→RL→SFT、RPRM稠密奖励、潜空间RL避免扩散回传不稳）；或VLA做高层决策、RL做低层控制（足式/人形/移动操作）。</p>\n<p> - 预训练阶段：数据规模与多样性（跨机器人/人类/合成/多任务损失）；常用VLM骨干（PaLM-E、PaLI-X、PaliGemma等）。</p>\n<p> - 推理：支持非自回归并行输出、目标图像/语言指令输入、多模态条件与低时延控制；实时性通过流匹配或序列压缩优化。</p>\n<ul><li>数据与增强：遥感与多模态采集、统一格式（单相机+7-DoF动作），公开数据集（Open-X Embodiment、RT-1数据、多机器人/人类视频），数据增强（合成轨迹/世界模型生成/光学流与特征点跟踪/图像编辑）。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>公开数据与平台：</li></ul>\n<p> - Open-X Embodiment (OXE)：多机器人多本体数据，统一格式，促进跨本体泛化。</p>\n<p> - RT-1/RT-2数据：大规模真实世界机器人演示，涵盖数百任务与长序列。</p>\n<p> - 人类视频数据集：Ego4D、EPIC-KITCHENS等，用于潜在动作与可供性抽取。</p>\n<p> - 仿真数据：任务/轨迹/合成视觉样本，用于长时序与多样场景扩展。</p>\n<ul><li>预训练资源：</li></ul>\n<p> - 多源混合：真实机器人轨迹、互联网规模视觉语言数据、检测/推理辅助损失、合成轨迹与COSMOS等世界模型生成数据。</p>\n<p> - VLM骨干：PaLM-E、PaLI-X、PaliGemma、LLaMA 2/Qwen2等；视觉编码器（DINOv2/SigLIP、EfficientNet、MAE-ViT）。</p>\n<ul><li>计算与训练关注：Transformer为主，序列长度与高分辨率视觉导致显存/时延压力；采用TokenLearner/Q-Former压缩、流匹配加速采样、并行非自回归解码；建议端到端微调+动作头适配的折中训练策略。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境与平台：</li></ul>\n<p> - 真实机器人：机械臂、移动操作、人形与足式机器人等多样化本体与场景。</p>\n<p> - 仿真到真实：模拟环境用于训练与数据扩增，结合真实部署验证。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 任务层：成功率、平均回报、EpLen、鲁棒性（噪声/遮挡/跨域）。</p>\n<p> - 控制层：时延与频率（如50Hz）、动作平滑性与稳定性。</p>\n<p> - 泛化：跨任务/对象/环境/本体的转移能力。</p>\n<p> - 安全与失败检测：碰撞率、回退策略、恢复成功率。</p>\n<p> - 效率：参数规模、推理时延、部署资源占用。</p>"
  },
  {
    "date": "2025-10-08",
    "title": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models",
    "link": "http://arxiv.org/abs/2510.07067",
    "summary_markdown": "# 论文研究单位\n作者单位未在提供的HTML中明确给出。\n# 论文概述\n论文研究了嵌入在真实人机交互中的自然语言指令对视觉-语言-动作（VLA）模型稳健性的影响，重点考察两类指令噪声：人自然转述与插入与任务无关的上下文。论文在LIBERO和Habitat 2.0两个仿真环境中，对OpenVLA、UniAct、MoDE、π0和LLARP等SOTA VLA模型进行系统评估。发现随上下文长度增长性能显著下降；与训练集语义/词法相似的无关上下文可导致约50%质量损失；人类转述可导致约20%下降。为此提出基于大语言模型（LLM）的无关上下文过滤框架，能将性能恢复至原始的98.5%。\n# 论文核心贡献点\n- 发现并量化了VLA模型在两类语言扰动下的脆弱性：人类转述与无关上下文插入，且对语义/词法接近训练命令的上下文最敏感。\n- 表明随无关上下文长度增加，性能呈持续下降；在上下文长度接近目标命令时，质量损失可达到与语义接近型上下文相当的水平。\n- 提出并验证了LLM过滤框架作为VLA前置预处理，能显著提升鲁棒性并恢复性能。\n# 论文方法描述\n- 设计系统化的指令扰动类型：\n - 按长度变化：单引导词、短句（3–5词）、长句（7–10词）。\n - 按语义/词法相似度：描述型（Description）、不可执行指令（Infeasible）、位置型（Location，包含场景对象与位置引用但与后续命令无关）。\n - 上下文插入于目标命令前后，并使最终噪声指令与训练模板在标点与大小写上保持一致以排除非相关变量。\n- 人研究众包转述：众包工作者对各任务指令进行转述，要求语义不变；每位参与者的五条指令由五名不同工作者独立转述，耗时中位数296秒，收集后由专家审核保留语义保真样本。\n- 过滤框架：\n - 使用多尺寸LLM（Flan‑T5 Base、Qwen2.5‑0.5B、1.5B、3B、Llama3.2‑1B、3B、Llama‑3‑8B）以少样本提示进行过滤，去除无关上下文并恢复核心指令。\n - 为LIBERO与LLARP分别适配过滤指令；小模型对随机上下文过滤较好，语义接近型需要≥3B；开源8B模型在绝大多数场景几乎完全恢复模板指令。\n - 对极端场景（不可执行型前置）仍有少量残留影响；少数情况下会误删有用细节（如LLARP的初始位置描述），但在测试数据中出现概率极低。\n# 论文使用数据集和训练资源\n- 仿真环境：\n - LIBERO：四套任务（Goal、Object、Spatial、Long各10个任务；前三种为短时域，Long为长时域）。\n - Habitat 2.0：生成100条导航+操作指令用于评估。\n- VLA模型：OpenVLA、UniAct、MoDE、π0（用于LIBERO）；LLARP（用于Habitat 2.0）。\n- 试验资源：\n - LIBERO：每任务套50次试验，结果为三随机种子平均（共150次/统计量）。\n - Habitat 2.0：LLARP并行运行在32个环境中，每任务30次试验，三随机种子平均。\n - 众包：在英语熟练度筛选后进行转述，数据匿名化并按研究许可发布。\n# 论文使用的评估环境和评估指标\n- 评估环境：LIBERO与Habitat 2.0仿真平台。\n- 评估指标：成功率（SR）。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>作者单位未在提供的HTML中明确给出。</p>\n<h1>论文概述</h1>\n<p>论文研究了嵌入在真实人机交互中的自然语言指令对视觉-语言-动作（VLA）模型稳健性的影响，重点考察两类指令噪声：人自然转述与插入与任务无关的上下文。论文在LIBERO和Habitat 2.0两个仿真环境中，对OpenVLA、UniAct、MoDE、π0和LLARP等SOTA VLA模型进行系统评估。发现随上下文长度增长性能显著下降；与训练集语义/词法相似的无关上下文可导致约50%质量损失；人类转述可导致约20%下降。为此提出基于大语言模型（LLM）的无关上下文过滤框架，能将性能恢复至原始的98.5%。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>发现并量化了VLA模型在两类语言扰动下的脆弱性：人类转述与无关上下文插入，且对语义/词法接近训练命令的上下文最敏感。</li><li>表明随无关上下文长度增加，性能呈持续下降；在上下文长度接近目标命令时，质量损失可达到与语义接近型上下文相当的水平。</li><li>提出并验证了LLM过滤框架作为VLA前置预处理，能显著提升鲁棒性并恢复性能。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>设计系统化的指令扰动类型：</li></ul>\n<p> - 按长度变化：单引导词、短句（3–5词）、长句（7–10词）。</p>\n<p> - 按语义/词法相似度：描述型（Description）、不可执行指令（Infeasible）、位置型（Location，包含场景对象与位置引用但与后续命令无关）。</p>\n<p> - 上下文插入于目标命令前后，并使最终噪声指令与训练模板在标点与大小写上保持一致以排除非相关变量。</p>\n<ul><li>人研究众包转述：众包工作者对各任务指令进行转述，要求语义不变；每位参与者的五条指令由五名不同工作者独立转述，耗时中位数296秒，收集后由专家审核保留语义保真样本。</li><li>过滤框架：</li></ul>\n<p> - 使用多尺寸LLM（Flan‑T5 Base、Qwen2.5‑0.5B、1.5B、3B、Llama3.2‑1B、3B、Llama‑3‑8B）以少样本提示进行过滤，去除无关上下文并恢复核心指令。</p>\n<p> - 为LIBERO与LLARP分别适配过滤指令；小模型对随机上下文过滤较好，语义接近型需要≥3B；开源8B模型在绝大多数场景几乎完全恢复模板指令。</p>\n<p> - 对极端场景（不可执行型前置）仍有少量残留影响；少数情况下会误删有用细节（如LLARP的初始位置描述），但在测试数据中出现概率极低。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>仿真环境：</li></ul>\n<p> - LIBERO：四套任务（Goal、Object、Spatial、Long各10个任务；前三种为短时域，Long为长时域）。</p>\n<p> - Habitat 2.0：生成100条导航+操作指令用于评估。</p>\n<ul><li>VLA模型：OpenVLA、UniAct、MoDE、π0（用于LIBERO）；LLARP（用于Habitat 2.0）。</li><li>试验资源：</li></ul>\n<p> - LIBERO：每任务套50次试验，结果为三随机种子平均（共150次/统计量）。</p>\n<p> - Habitat 2.0：LLARP并行运行在32个环境中，每任务30次试验，三随机种子平均。</p>\n<p> - 众包：在英语熟练度筛选后进行转述，数据匿名化并按研究许可发布。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：LIBERO与Habitat 2.0仿真平台。</li><li>评估指标：成功率（SR）。</li></ul>"
  },
  {
    "date": "2025-10-08",
    "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training",
    "link": "http://arxiv.org/abs/2510.06710",
    "summary_markdown": "# RLinf-VLA: A unified and efficient framework for VLA+RL training\n## 论文研究单位\n- 清华大学、Zhongguancun Academy、Infinigence AI、北京大学、加州大学伯克利分校、哈尔滨工业大学、中国科学院自动化研究所\n## 论文概述\n- 背景与动机：VLA（视觉-语言-动作）模型大多采用SFT（监督微调），易受分布偏移影响，泛化与稳健性受限；RL（强化学习）通过交互直接优化任务表现，但现有VLA+RL工作碎片化，缺乏统一平台，难以公平对比与规模化扩展。\n- 贡献概览：提出RLinf-VLA，一个统一且高效的VLA强化学习训练框架。通过三大GPU分配模式、统一的模型/算法/仿真器接口、细致的算法设计（优势与日志概率粒度、部分重置、动作掩码、长度归一化等），在系统与算法两端同时提效。\n- 关键数字：在仿真中，单一统一模型在LIBERO-130（130项任务）达98.11%成功率，在ManiSkill 25项Pick-and-Place任务达97.66%；系统层面相对基线吞吐2.27×，在GPU并行仿真中采用混合细粒度流水实现1.61×–1.88×加速。\n## 论文核心贡献点\n- 统一设计与接口：支持多仿真器（ManiSkill、LIBERO）、多VLA架构（OpenVLA、OpenVLA-OFT）、多RL算法（PPO、GRPO）；统一训练/生成/仿真器接口，简化跨配置迁移与公平对比。\n- GPU分配策略：提出colocated、disaggregated、hybrid三种模式；针对GPU并行仿真，提出hybrid + 细粒度流水，有效削减“GPU气泡”与频繁卸载开销。\n- 算法与工程细节：优势与日志概率支持chunk/action/token三级粒度并可组合；PPO支持部分重置（提升样本效率）、轻量值网络（共享参数）；GRPO支持分组设计、有效动作掩码、轨迹长度归一化与成功率筛选。\n- 强实证与实践准则：给出PPO/GRPO与VLA结合的最佳实践；开源并维护生态，持续更新。\n## 论文方法描述\n- GPU分配策略\n - Colocated（共享）：训练/生成/仿真共用所有GPU，支持CPU卸载（offload），但在频繁交互场景开销大。\n - Disaggregated（分离）：各组件分配独占GPU分区，避免内存争抢，但因依赖关系产生“GPU气泡”。\n - Hybrid + 细粒度流水：将一个GPU上的仿真实例拆分为多个子仿真器S^(1)…S^(k)，仿真与生成并发流水，消除空闲；通过配置可切换模式，无需改动代码。\n- 模型兼容\n - LoRA支持：低秩适配，冻结原权重，仅训练少量参数，降低显存与成本。\n - 模型类型：OpenVLA（~7B，连续/离散动作）与OpenVLA-OFT（连续动作空间+L1回归损失，支持并行解码与动作分块，提高推理吞吐）。\n - 统一接口：屏蔽模型差异，一套API适配多模型。\n- 多仿真器支持\n - 统一接口：Gym风格reset/step、auto_reset、ignore_terminations、chunk_step（处理分块动作与边界）、可视化与固定重置状态等工具。\n - 任务集合：ManiSkill 25项PutOnPlateInScene（OOD设置复现）；LIBERO六类任务组合为LIBERO-130（130项）。\n- 多算法支持与设计\n - PPO：GAE估计优势；轻量值头（共享LM参数）；chunk/action级价值估计；支持“固定episode长度”与“部分重置”两种优化目标。\n - GRPO：以组为单位进行相对优势估计（去值函数），通过分组（相同任务+相同初始状态）、有效动作掩码、轨迹长度归一化与成功率筛选提升稳定性与效率。\n- 优势与日志概率粒度\n - 支持优势（chunk/action）、日志概率（chunk/action/token）组合；不兼容时采用广播（优势广播至更细粒度）。\n## 论文使用数据集和训练资源\n- 仿真环境\n - ManiSkill：PutOnPlateInScene25Main-v3（25个抓取-放置任务）。\n - LIBERO：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-10、LIBERO-90；统一为LIBERO-130。\n- 训练设置\n - 算法与模式：PPO（固定episode/部分重置），GRPO（固定episode/有效动作掩码）。\n - 分块与解码：OpenVLA-OFT支持动作分块与并行解码，提升高频控制能力。\n - 超参与资源：支持LoRA与不同rollout批量；建议更大rollout批量；LoRA对性能影响不显著但常需超参再调；部分重置与长度归一化提升样本效率与稳定性。\n- GPU模式与流水\n - Colocated/Disaggregated/Hybrid三模式切换；细粒度流水stage数量可配置；支持各组件独立offload开关。\n## 论文使用的评估环境和评估指标\n- 评估环境\n - 仿真：LIBERO-130（跨任务类别、长视野）、ManiSkill（25项OOD抓取-放置）。\n - 实机：Franka机器人在6个未见物体上零样本抓取-放置。\n- 指标\n - 成功率：LIBERO-130上98.11%；ManiSkill上97.66%。\n - 系统效率：相比基线2.27×吞吐；在GPU并行仿真中混合细粒度流水加速1.61×–1.88×。\n - 消融与实践：PPO中动作级价值估计优于分块级；部分重置显著提升样本效率；GRPO中长度归一化与有效动作掩码关键；成功率筛选改善稳定性；更大rollout批量有益；LoRA需调参但本身不降性能。\n - 实机对比：RL策略零样本完成8/30次；SFT策略0/30次，体现更强泛化。",
    "summary_html": "<h1>RLinf-VLA: A unified and efficient framework for VLA+RL training</h1>\n<h2>论文研究单位</h2>\n<ul><li>清华大学、Zhongguancun Academy、Infinigence AI、北京大学、加州大学伯克利分校、哈尔滨工业大学、中国科学院自动化研究所</li></ul>\n<h2>论文概述</h2>\n<ul><li>背景与动机：VLA（视觉-语言-动作）模型大多采用SFT（监督微调），易受分布偏移影响，泛化与稳健性受限；RL（强化学习）通过交互直接优化任务表现，但现有VLA+RL工作碎片化，缺乏统一平台，难以公平对比与规模化扩展。</li><li>贡献概览：提出RLinf-VLA，一个统一且高效的VLA强化学习训练框架。通过三大GPU分配模式、统一的模型/算法/仿真器接口、细致的算法设计（优势与日志概率粒度、部分重置、动作掩码、长度归一化等），在系统与算法两端同时提效。</li><li>关键数字：在仿真中，单一统一模型在LIBERO-130（130项任务）达98.11%成功率，在ManiSkill 25项Pick-and-Place任务达97.66%；系统层面相对基线吞吐2.27×，在GPU并行仿真中采用混合细粒度流水实现1.61×–1.88×加速。</li></ul>\n<h2>论文核心贡献点</h2>\n<ul><li>统一设计与接口：支持多仿真器（ManiSkill、LIBERO）、多VLA架构（OpenVLA、OpenVLA-OFT）、多RL算法（PPO、GRPO）；统一训练/生成/仿真器接口，简化跨配置迁移与公平对比。</li><li>GPU分配策略：提出colocated、disaggregated、hybrid三种模式；针对GPU并行仿真，提出hybrid + 细粒度流水，有效削减“GPU气泡”与频繁卸载开销。</li><li>算法与工程细节：优势与日志概率支持chunk/action/token三级粒度并可组合；PPO支持部分重置（提升样本效率）、轻量值网络（共享参数）；GRPO支持分组设计、有效动作掩码、轨迹长度归一化与成功率筛选。</li><li>强实证与实践准则：给出PPO/GRPO与VLA结合的最佳实践；开源并维护生态，持续更新。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>GPU分配策略</li></ul>\n<p> - Colocated（共享）：训练/生成/仿真共用所有GPU，支持CPU卸载（offload），但在频繁交互场景开销大。</p>\n<p> - Disaggregated（分离）：各组件分配独占GPU分区，避免内存争抢，但因依赖关系产生“GPU气泡”。</p>\n<p> - Hybrid + 细粒度流水：将一个GPU上的仿真实例拆分为多个子仿真器S^(1)…S^(k)，仿真与生成并发流水，消除空闲；通过配置可切换模式，无需改动代码。</p>\n<ul><li>模型兼容</li></ul>\n<p> - LoRA支持：低秩适配，冻结原权重，仅训练少量参数，降低显存与成本。</p>\n<p> - 模型类型：OpenVLA（~7B，连续/离散动作）与OpenVLA-OFT（连续动作空间+L1回归损失，支持并行解码与动作分块，提高推理吞吐）。</p>\n<p> - 统一接口：屏蔽模型差异，一套API适配多模型。</p>\n<ul><li>多仿真器支持</li></ul>\n<p> - 统一接口：Gym风格reset/step、auto_reset、ignore_terminations、chunk_step（处理分块动作与边界）、可视化与固定重置状态等工具。</p>\n<p> - 任务集合：ManiSkill 25项PutOnPlateInScene（OOD设置复现）；LIBERO六类任务组合为LIBERO-130（130项）。</p>\n<ul><li>多算法支持与设计</li></ul>\n<p> - PPO：GAE估计优势；轻量值头（共享LM参数）；chunk/action级价值估计；支持“固定episode长度”与“部分重置”两种优化目标。</p>\n<p> - GRPO：以组为单位进行相对优势估计（去值函数），通过分组（相同任务+相同初始状态）、有效动作掩码、轨迹长度归一化与成功率筛选提升稳定性与效率。</p>\n<ul><li>优势与日志概率粒度</li></ul>\n<p> - 支持优势（chunk/action）、日志概率（chunk/action/token）组合；不兼容时采用广播（优势广播至更细粒度）。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>仿真环境</li></ul>\n<p> - ManiSkill：PutOnPlateInScene25Main-v3（25个抓取-放置任务）。</p>\n<p> - LIBERO：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-10、LIBERO-90；统一为LIBERO-130。</p>\n<ul><li>训练设置</li></ul>\n<p> - 算法与模式：PPO（固定episode/部分重置），GRPO（固定episode/有效动作掩码）。</p>\n<p> - 分块与解码：OpenVLA-OFT支持动作分块与并行解码，提升高频控制能力。</p>\n<p> - 超参与资源：支持LoRA与不同rollout批量；建议更大rollout批量；LoRA对性能影响不显著但常需超参再调；部分重置与长度归一化提升样本效率与稳定性。</p>\n<ul><li>GPU模式与流水</li></ul>\n<p> - Colocated/Disaggregated/Hybrid三模式切换；细粒度流水stage数量可配置；支持各组件独立offload开关。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境</li></ul>\n<p> - 仿真：LIBERO-130（跨任务类别、长视野）、ManiSkill（25项OOD抓取-放置）。</p>\n<p> - 实机：Franka机器人在6个未见物体上零样本抓取-放置。</p>\n<ul><li>指标</li></ul>\n<p> - 成功率：LIBERO-130上98.11%；ManiSkill上97.66%。</p>\n<p> - 系统效率：相比基线2.27×吞吐；在GPU并行仿真中混合细粒度流水加速1.61×–1.88×。</p>\n<p> - 消融与实践：PPO中动作级价值估计优于分块级；部分重置显著提升样本效率；GRPO中长度归一化与有效动作掩码关键；成功率筛选改善稳定性；更大rollout批量有益；LoRA需调参但本身不降性能。</p>\n<p> - 实机对比：RL策略零样本完成8/30次；SFT策略0/30次，体现更强泛化。</p>"
  },
  {
    "date": "2025-10-07",
    "title": "Verifier-free Test-Time Sampling for Vision Language Action Models",
    "link": "http://arxiv.org/abs/2510.05681",
    "summary_markdown": "根据提供的Arxiv论文HTML原文，我为您总结这篇关于Vision-Language-Action模型的无验证器测试时采样论文：\n## 论文研究单位\nKAIST（韩国科学技术院）、首尔国立大学（SNU）、RLWRLD\n## 论文概述\nVision-Language-Action (VLA) 模型在机器人控制中展现出卓越性能，但在需要高精度的任务中仍存在根本性局限。当前方法主要依赖单次推理范式，限制了精细操作任务的表现。虽然已有测试时扩展方法使用外部验证器提升性能，但这些方法需要额外训练且泛化能力有限。本研究提出Masking Distribution Guided Selection (MG-Select)，一种新颖的VLA测试时扩展框架，仅利用模型内部属性，无需额外训练或外部模块。\n## 论文核心贡献点\n1. **提出MG-Select框架**：利用KL散度作为置信度指标，从多个候选动作中选择最优动作\n2. **条件掩蔽分布设计**：通过随机掩蔽状态和语言条件生成参考分布，确保最大不确定性同时保持任务分布对齐\n3. **联合训练策略**：通过dropout技术使模型学习条件和无条件分布，进一步提升参考分布质量\n4. **显著性能提升**：在真实世界任务中实现28%的内分布和35%的外分布性能提升，在RoboCasa pick-and-place任务中实现168%相对增益\n## 论文方法描述\n### 测试时扩展框架\n- **阶段1**：并行随机采样生成N个候选动作\n- **阶段2**：使用特定标准进行Best-of-N选择\n### 条件掩蔽分布置信度\n- 使用KL散度测量预测分布与参考分布间的距离作为置信度指标\n- 参考分布通过掩蔽文本、状态或两者创建，分别对应：文本掩蔽、状态掩蔽、文本&状态掩蔽\n- 针对不同任务环境选择最优置信度变体\n### 联合训练策略\n- 训练时引入四种掩蔽变体：(qₜ,I)、(qₜ,∅)、(∅,I)、(∅,∅)\n- 通过dropout技术增强模型对条件掩蔽分布的感知能力\n## 论文使用数据集和训练资源\n### 训练资源\n- **硬件**：NVIDIA A100 GPU（2块）\n- **模型**：π₀-FAST (Paligemma-3B VLM)、OpenVLA (Prismatic-7B VLM)\n- **优化器**：AdamW，学习率2.5e-5到2.5e-6的余弦衰减\n- **训练配置**：warmup_steps=1,000，global batch size根据数据集变化\n### 使用数据集\n- **RoboCasa**：24个家庭厨房环境中的原子任务，专注于8个pick-and-place任务\n- **SIMPLER-WidowX**：4个pick-and-place任务，基于BridgeData V2训练\n- **LIBERO**：多轴泛化评估，包括布局、物体和目标变化，以及长期任务\n- **真实世界数据**：DROID数据集，基于Franka Research 3机器人\n## 论文使用的评估环境和评估指标\n### 评估环境\n- **仿真环境**：RoboCasa、SIMPLER-WidowX、LIBERO\n- **真实世界环境**：Franka Research 3机器人，7-DoF机械臂\n### 评估指标\n- **成功率**：以百分比表示的成功率，基于多次试验（通常为50次仿真、16-24次真实世界）\n- **样本效率**：在30、100、300个演示样本下的性能表现\n- **泛化能力**：内分布（ID）和外分布（OOD）任务的区分评估\n- **效率分析**：推理延迟分析，通过单预填充策略实现45%的延迟减少\n\n实验结果表明MG-Select在不同演示规模下均能持续改进基础模型性能，特别是在低数据场景下效果显著。",
    "summary_html": "<p>根据提供的Arxiv论文HTML原文，我为您总结这篇关于Vision-Language-Action模型的无验证器测试时采样论文：</p>\n<h2>论文研究单位</h2>\n<p>KAIST（韩国科学技术院）、首尔国立大学（SNU）、RLWRLD</p>\n<h2>论文概述</h2>\n<p>Vision-Language-Action (VLA) 模型在机器人控制中展现出卓越性能，但在需要高精度的任务中仍存在根本性局限。当前方法主要依赖单次推理范式，限制了精细操作任务的表现。虽然已有测试时扩展方法使用外部验证器提升性能，但这些方法需要额外训练且泛化能力有限。本研究提出Masking Distribution Guided Selection (MG-Select)，一种新颖的VLA测试时扩展框架，仅利用模型内部属性，无需额外训练或外部模块。</p>\n<h2>论文核心贡献点</h2>\n<ol><li><strong>提出MG-Select框架</strong>：利用KL散度作为置信度指标，从多个候选动作中选择最优动作</li><li><strong>条件掩蔽分布设计</strong>：通过随机掩蔽状态和语言条件生成参考分布，确保最大不确定性同时保持任务分布对齐</li><li><strong>联合训练策略</strong>：通过dropout技术使模型学习条件和无条件分布，进一步提升参考分布质量</li><li><strong>显著性能提升</strong>：在真实世界任务中实现28%的内分布和35%的外分布性能提升，在RoboCasa pick-and-place任务中实现168%相对增益</li></ol>\n<h2>论文方法描述</h2>\n<h3>测试时扩展框架</h3>\n<ul><li><strong>阶段1</strong>：并行随机采样生成N个候选动作</li><li><strong>阶段2</strong>：使用特定标准进行Best-of-N选择</li></ul>\n<h3>条件掩蔽分布置信度</h3>\n<ul><li>使用KL散度测量预测分布与参考分布间的距离作为置信度指标</li><li>参考分布通过掩蔽文本、状态或两者创建，分别对应：文本掩蔽、状态掩蔽、文本&状态掩蔽</li><li>针对不同任务环境选择最优置信度变体</li></ul>\n<h3>联合训练策略</h3>\n<ul><li>训练时引入四种掩蔽变体：(qₜ,I)、(qₜ,∅)、(∅,I)、(∅,∅)</li><li>通过dropout技术增强模型对条件掩蔽分布的感知能力</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<h3>训练资源</h3>\n<ul><li><strong>硬件</strong>：NVIDIA A100 GPU（2块）</li><li><strong>模型</strong>：π₀-FAST (Paligemma-3B VLM)、OpenVLA (Prismatic-7B VLM)</li><li><strong>优化器</strong>：AdamW，学习率2.5e-5到2.5e-6的余弦衰减</li><li><strong>训练配置</strong>：warmup_steps=1,000，global batch size根据数据集变化</li></ul>\n<h3>使用数据集</h3>\n<ul><li><strong>RoboCasa</strong>：24个家庭厨房环境中的原子任务，专注于8个pick-and-place任务</li><li><strong>SIMPLER-WidowX</strong>：4个pick-and-place任务，基于BridgeData V2训练</li><li><strong>LIBERO</strong>：多轴泛化评估，包括布局、物体和目标变化，以及长期任务</li><li><strong>真实世界数据</strong>：DROID数据集，基于Franka Research 3机器人</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<h3>评估环境</h3>\n<ul><li><strong>仿真环境</strong>：RoboCasa、SIMPLER-WidowX、LIBERO</li><li><strong>真实世界环境</strong>：Franka Research 3机器人，7-DoF机械臂</li></ul>\n<h3>评估指标</h3>\n<ul><li><strong>成功率</strong>：以百分比表示的成功率，基于多次试验（通常为50次仿真、16-24次真实世界）</li><li><strong>样本效率</strong>：在30、100、300个演示样本下的性能表现</li><li><strong>泛化能力</strong>：内分布（ID）和外分布（OOD）任务的区分评估</li><li><strong>效率分析</strong>：推理延迟分析，通过单预填充策略实现45%的延迟减少</li></ul>\n\n<p>实验结果表明MG-Select在不同演示规模下均能持续改进基础模型性能，特别是在低数据场景下效果显著。</p>"
  },
  {
    "date": "2025-10-07",
    "title": "MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption",
    "link": "http://arxiv.org/abs/2510.05580",
    "summary_markdown": "# 论文研究单位\nCarnegie Mellon University, Meta Reality Labs, USA\n# 论文概述\n该论文提出MetaVLA，一个统一的、与骨干网络无关的视觉-语言-动作模型训练后框架，旨在实现高效和可扩展的对齐。现有VLA模型通常需要针对特定任务的微调，计算成本高，且对未见任务的泛化能力差。MetaVLA通过引入“上下文感知元协同训练”来解决这个问题，该方法将多个目标任务整合到单个微调阶段，同时利用结构多样的辅助任务来提高领域内的泛化能力。该方法集成了一个源自注意力神经过程的轻量级元学习机制，使模型能够从多样化的上下文中快速适应，而无需大的架构更改或推理开销。在LIBERO基准测试上，MetaVLA在减少训练步骤和GPU时间的同时，性能优于OpenVLA。\n# 论文核心贡献点\n- 探索了一个新的方向：通过引入多样化的辅助任务，以可忽略的优化开销来提升训练后效率和泛化能力。\n- 提出了MetaVLA，一套即插即用的模块和训练方案，能够实现快速、可扩展的适应并具备强大的泛化能力。该方法工程友好，且与骨干架构和底层训练流程无关。\n- 进行了全面的实验，证明MetaVLA在显著提高效率（减少模型数量和GPU训练时间）的同时，取得了更优越的性能，同时保持了快速的推理速度。\n# 论文方法描述\nMetaVLA的核心是“上下文感知元协同训练”框架。它主要包含一个名为Meta-Action-Reasoner (MAR)的轻量级模块，该模块基于注意力神经过程，集成在Llama2动作解码器中。MAR通过自注意力从上下文示例中提取全局先验，再通过交叉注意力将其与目标查询融合，形成任务感知的混合表示。数据分为两个部分：上下文库和目标库。上下文库包含来自目标任务（LIBERO）和辅助任务（GR00T）的数据，作为外部记忆；目标库仅包含目标任务的数据，用于训练单一模型以适应所有任务。训练期间，上下文集每200步刷新一次，上下文批次大小设为32。辅助任务选自GR00T数据集，因其与LIBERO任务部分相关但结构多样（如视角和自由度不同），能在提供多样性的同时平衡优化稳定性。\n# 论文使用数据集和训练资源\n- 数据集：目标任务为LIBERO基准，包含LIBERO-Goal, LIBERO-Spatial, LIBERO-Object和LIBERO-Long四个套件。辅助任务来自GR00T数据集，具体包括单臂和双臂操作任务。\n- 训练资源：使用8块A100 80GB GPU进行训练。基于OpenVLA-7B模型，采用LoRA进行微调，总训练步数为75K，总GPU时间约为24小时。\n# 论文使用的评估环境和评估指标\n- 评估环境：在LIBERO模拟环境中进行评估，使用一块24GB RTX-4090 GPU进行推理。\n- 评估指标：主要评估指标为任务成功率。此外，在训练分析中也使用了准确率、模仿损失和L1损失等辅助指标来衡量模型收敛情况。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Carnegie Mellon University, Meta Reality Labs, USA</p>\n<h1>论文概述</h1>\n<p>该论文提出MetaVLA，一个统一的、与骨干网络无关的视觉-语言-动作模型训练后框架，旨在实现高效和可扩展的对齐。现有VLA模型通常需要针对特定任务的微调，计算成本高，且对未见任务的泛化能力差。MetaVLA通过引入“上下文感知元协同训练”来解决这个问题，该方法将多个目标任务整合到单个微调阶段，同时利用结构多样的辅助任务来提高领域内的泛化能力。该方法集成了一个源自注意力神经过程的轻量级元学习机制，使模型能够从多样化的上下文中快速适应，而无需大的架构更改或推理开销。在LIBERO基准测试上，MetaVLA在减少训练步骤和GPU时间的同时，性能优于OpenVLA。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>探索了一个新的方向：通过引入多样化的辅助任务，以可忽略的优化开销来提升训练后效率和泛化能力。</li><li>提出了MetaVLA，一套即插即用的模块和训练方案，能够实现快速、可扩展的适应并具备强大的泛化能力。该方法工程友好，且与骨干架构和底层训练流程无关。</li><li>进行了全面的实验，证明MetaVLA在显著提高效率（减少模型数量和GPU训练时间）的同时，取得了更优越的性能，同时保持了快速的推理速度。</li></ul>\n<h1>论文方法描述</h1>\n<p>MetaVLA的核心是“上下文感知元协同训练”框架。它主要包含一个名为Meta-Action-Reasoner (MAR)的轻量级模块，该模块基于注意力神经过程，集成在Llama2动作解码器中。MAR通过自注意力从上下文示例中提取全局先验，再通过交叉注意力将其与目标查询融合，形成任务感知的混合表示。数据分为两个部分：上下文库和目标库。上下文库包含来自目标任务（LIBERO）和辅助任务（GR00T）的数据，作为外部记忆；目标库仅包含目标任务的数据，用于训练单一模型以适应所有任务。训练期间，上下文集每200步刷新一次，上下文批次大小设为32。辅助任务选自GR00T数据集，因其与LIBERO任务部分相关但结构多样（如视角和自由度不同），能在提供多样性的同时平衡优化稳定性。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：目标任务为LIBERO基准，包含LIBERO-Goal, LIBERO-Spatial, LIBERO-Object和LIBERO-Long四个套件。辅助任务来自GR00T数据集，具体包括单臂和双臂操作任务。</li><li>训练资源：使用8块A100 80GB GPU进行训练。基于OpenVLA-7B模型，采用LoRA进行微调，总训练步数为75K，总GPU时间约为24小时。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：在LIBERO模拟环境中进行评估，使用一块24GB RTX-4090 GPU进行推理。</li><li>评估指标：主要评估指标为任务成功率。此外，在训练分析中也使用了准确率、模仿损失和L1损失等辅助指标来衡量模型收敛情况。</li></ul>"
  },
  {
    "date": "2025-10-06",
    "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation",
    "link": "http://arxiv.org/abs/2510.05057",
    "summary_markdown": "# 论文研究单位\n浙江大学、南京大学、香港科技大学\n# 论文概述\n论文提出 StaMo 方法，旨在从静态图像中学习可泛化的机器人运动。核心思想是将视觉观测压缩为高度紧凑的状态表示（仅需 2 个 1024 维 token），并利用预训练的 Diffusion Transformer（DiT）解码器进行重建。更重要的是，在该紧凑状态空间中，通过简单的线性插值或差分即可自然得到潜在动作（latent action），可解码为可执行的控制信号，避免了对视频时序建模的依赖。该表示可无缝集成到现有 VLA 框架中进行世界建模与策略共训，在模拟与真实环境均取得显著提升。\n# 论文核心贡献点\n- 提出紧凑状态表示学习框架，用 2×1024 token 编码视觉观测，并通过 DiT 解码器实现高质量重建。\n- 发现并利用“状态差分即动作”的性质：在紧凑潜在空间中，状态之间的线性差分或插值可直接作为潜在动作，无需视频监督。\n- 将该表示作为未来状态预测目标，集成到 OpenVLA/OFT 等 VLA 框架，提升任务成功率且推理开销极小。\n- 引入策略共训（co-training）方案：将无标签视频的相邻帧编码为潜在动作，作为伪标签与少量带标签机器人数据联合训练，显著提升泛化与跨域迁移能力。\n- 验证方法在 LIBERO 基准提升 +14.3%，真实任务成功率提升 +30%，并呈现良好的可扩展性与跨域（sim-to-real）迁移特性。\n# 论文方法描述\n- 图像压缩与状态表示学习：采用 Diffusion Autoencoder 结构；编码器为冻结的 DINOv2 特征提取器 + Transformer 压缩器，解码器为预训练的 DiT（基于 Stable Diffusion 3）；使用 Flow Matching 目标优化，仅训练压缩器与解码器。\n- 状态与运动的统一：运动定义为状态 token 的差分 a_t = s_{t+1} − s_t 或在潜空间中的线性插值；该潜在动作可直接解码为机器人控制信号，并用于世界模型与策略学习。\n- 世界建模：在 VLA（如 OpenVLA/OFT）的自回归主干上添加轻量 MLP 头，预测下一状态表示；总损失为动作交叉熵与未来状态回归（MSE + L1）之和，权衡即时控制与未来预测。\n- 潜在动作的策略共训：将相邻视频帧编码为潜在动作 m_t = E(o_{t+1}) − E(o_t)，作为伪标签，与少量真实机器人动作数据共同训练下游策略。\n# 论文使用数据集和训练资源\n- 数据集：LIBERO（10/90/goal/object/spatial）、DROID、Maniskill（OOD）、Open X-Embodiment；并使用人类 egocentric 视频。\n- 硬件与训练：8×NVIDIA H100 GPU；AdamW 优化器，学习率 3e-5（余弦退火），批大小 512/GPU，权重衰减 1e-3；训练约 10 天，PyTorch 2.1、Ubuntu 22.04；随机种子 33。\n# 论文使用的评估环境和评估指标\n- 环境与基准：LIBERO 模拟环境（不同子任务 Spatial/Object/Goal/Long）；真实世界机器人实验（Franka Research 3 + UMI 夹爪 + RealSense D435，20 Hz，SE(3) 绝对末端位姿控制）。\n- 评估指标：图像重建（PSNR、SSIM）、任务成功率（%）、线性探针 MSE（预测动作序列与真实动作的均方误差）、推理频率（Hz）、策略共训与跨域迁移表现。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>浙江大学、南京大学、香港科技大学</p>\n<h1>论文概述</h1>\n<p>论文提出 StaMo 方法，旨在从静态图像中学习可泛化的机器人运动。核心思想是将视觉观测压缩为高度紧凑的状态表示（仅需 2 个 1024 维 token），并利用预训练的 Diffusion Transformer（DiT）解码器进行重建。更重要的是，在该紧凑状态空间中，通过简单的线性插值或差分即可自然得到潜在动作（latent action），可解码为可执行的控制信号，避免了对视频时序建模的依赖。该表示可无缝集成到现有 VLA 框架中进行世界建模与策略共训，在模拟与真实环境均取得显著提升。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出紧凑状态表示学习框架，用 2×1024 token 编码视觉观测，并通过 DiT 解码器实现高质量重建。</li><li>发现并利用“状态差分即动作”的性质：在紧凑潜在空间中，状态之间的线性差分或插值可直接作为潜在动作，无需视频监督。</li><li>将该表示作为未来状态预测目标，集成到 OpenVLA/OFT 等 VLA 框架，提升任务成功率且推理开销极小。</li><li>引入策略共训（co-training）方案：将无标签视频的相邻帧编码为潜在动作，作为伪标签与少量带标签机器人数据联合训练，显著提升泛化与跨域迁移能力。</li><li>验证方法在 LIBERO 基准提升 +14.3%，真实任务成功率提升 +30%，并呈现良好的可扩展性与跨域（sim-to-real）迁移特性。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>图像压缩与状态表示学习：采用 Diffusion Autoencoder 结构；编码器为冻结的 DINOv2 特征提取器 + Transformer 压缩器，解码器为预训练的 DiT（基于 Stable Diffusion 3）；使用 Flow Matching 目标优化，仅训练压缩器与解码器。</li><li>状态与运动的统一：运动定义为状态 token 的差分 a_t = s_{t+1} − s_t 或在潜空间中的线性插值；该潜在动作可直接解码为机器人控制信号，并用于世界模型与策略学习。</li><li>世界建模：在 VLA（如 OpenVLA/OFT）的自回归主干上添加轻量 MLP 头，预测下一状态表示；总损失为动作交叉熵与未来状态回归（MSE + L1）之和，权衡即时控制与未来预测。</li><li>潜在动作的策略共训：将相邻视频帧编码为潜在动作 m_t = E(o_{t+1}) − E(o_t)，作为伪标签，与少量真实机器人动作数据共同训练下游策略。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：LIBERO（10/90/goal/object/spatial）、DROID、Maniskill（OOD）、Open X-Embodiment；并使用人类 egocentric 视频。</li><li>硬件与训练：8×NVIDIA H100 GPU；AdamW 优化器，学习率 3e-5（余弦退火），批大小 512/GPU，权重衰减 1e-3；训练约 10 天，PyTorch 2.1、Ubuntu 22.04；随机种子 33。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>环境与基准：LIBERO 模拟环境（不同子任务 Spatial/Object/Goal/Long）；真实世界机器人实验（Franka Research 3 + UMI 夹爪 + RealSense D435，20 Hz，SE(3) 绝对末端位姿控制）。</li><li>评估指标：图像重建（PSNR、SSIM）、任务成功率（%）、线性探针 MSE（预测动作序列与真实动作的均方误差）、推理频率（Hz）、策略共训与跨域迁移表现。</li></ul>"
  },
  {
    "date": "2025-10-06",
    "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
    "link": "http://arxiv.org/abs/2510.04898",
    "summary_markdown": "论文研究单位\nUniversity of Oxford\n\n论文概述\n该论文指出，现有的视觉-语言-动作（VLA）模型虽然具有强大的泛化能力，但推理成本极高。为了解决这个问题，论文提出了HyperVLA，一种基于超网络的新型VLA架构。HyperVLA在训练时利用一个大型超网络来获取处理多样化多任务行为所需的高模型容量，但在推理时仅激活一个由超网络生成的、针对特定任务的紧凑策略。该方法通过引入几项关键的算法设计，成功训练了HN-based VLA。实验表明，与单体VLA相比，HyperVLA在实现相似甚至更高的零样本泛化和少样本适应成功率的同时，显著降低了推理成本。\n\n论文核心贡献点\n1. 提出了HyperVLA，一个基于超网络的VLA架构，通过在训练时使用高容量模型、推理时仅激活小型任务特定策略，实现了推理效率的提升。\n2. 识别并解决了训练大型HN-based VLA的挑战，提出了三项关键的算法设计：利用预训练视觉主干网络、超网络嵌入归一化和简化的线性动作生成策略。\n3. 在零样本泛化（SIMPLER基准）和少样本适应（LIBERO基准）实验中，HyperVLA的性能匹配或超越了现有的单体VLA基线（如OpenVLA）。\n4. 实现了显著的推理效率提升：与OpenVLA相比，HyperVLA将测试时激活的参数数量减少了90倍，推理速度提升了120倍。\n\n论文方法描述\nHyperVLA的核心思想是利用超网络的分层结构来解耦不同任务所需的技能。\n1. 架构：HyperVLA由一个基础策略和一个超网络组成。\n - 基础策略：一个视觉Transformer（ViT），它仅将当前图像观察作为输入来预测动作。它包括一个DINOv2图像编码器、一个线性投影层、一个小型Transformer策略头和一个线性动作头。\n - 超网络（HN）：一个基于Transformer的上下文编码器，它接收语言指令（通过冻结的T5编码器）、初始图像的类别标记（通过冻结的DINOv2编码器）和一个可学习的任务上下文标记作为输入，然后生成基础策略的参数。\n2. 算法设计：\n - 视觉主干网络：使用预训练的DINOv2作为基础策略的图像编码器并进行微调，以利用视觉基础模型的先验知识，避免在相对较小的机器人数据集上从头训练导致的过拟合。\n - 上下文嵌入归一化：为了稳定超网络的训练，在将上下文嵌入输入到输出头之前进行归一化。这使得基础策略参数的更新动态与直接训练该策略时相似。\n - 动作生成策略：采用简单的线性动作头和均方误差（MSE）损失进行训练，而非现有VLAs常用的自回归或扩散模型，这在HN-based VLA设置中更简单、更高效。\n\n论文使用数据集和训练资源\n1. 数据集：模型在Open X-Embodiment (OXE)数据集上进行训练。评估在SIMPLER和LIBERO两个模拟基准上进行，SIMPLER用于评估零样本泛化能力，LIBERO用于评估少样本适应能力。\n2. 训练资源：HyperVLA在4块NVIDIA A5000 GPU上训练了一天。作为对比，基线模型OpenVLA在64块A100 GPU上训练了14天。\n\n论文使用的评估环境和评估指标\n1. 评估环境：性能评估在两个模拟环境中进行：SIMPLER和LIBERO。推理速度（时间/步）在NVIDIA L4 GPU上进行测量。\n2. 评估指标：\n - 任务成功率：在SIMPLER和LIBERO基准上，评估模型完成任务的百分比。\n - 推理效率：通过激活的参数数量、每推理步的耗时（毫秒）和浮点运算数（FLOPs）来衡量。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Oxford</p>\n\n<p>论文概述</p>\n<p>该论文指出，现有的视觉-语言-动作（VLA）模型虽然具有强大的泛化能力，但推理成本极高。为了解决这个问题，论文提出了HyperVLA，一种基于超网络的新型VLA架构。HyperVLA在训练时利用一个大型超网络来获取处理多样化多任务行为所需的高模型容量，但在推理时仅激活一个由超网络生成的、针对特定任务的紧凑策略。该方法通过引入几项关键的算法设计，成功训练了HN-based VLA。实验表明，与单体VLA相比，HyperVLA在实现相似甚至更高的零样本泛化和少样本适应成功率的同时，显著降低了推理成本。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了HyperVLA，一个基于超网络的VLA架构，通过在训练时使用高容量模型、推理时仅激活小型任务特定策略，实现了推理效率的提升。</li><li>识别并解决了训练大型HN-based VLA的挑战，提出了三项关键的算法设计：利用预训练视觉主干网络、超网络嵌入归一化和简化的线性动作生成策略。</li><li>在零样本泛化（SIMPLER基准）和少样本适应（LIBERO基准）实验中，HyperVLA的性能匹配或超越了现有的单体VLA基线（如OpenVLA）。</li><li>实现了显著的推理效率提升：与OpenVLA相比，HyperVLA将测试时激活的参数数量减少了90倍，推理速度提升了120倍。</li></ol>\n\n<p>论文方法描述</p>\n<p>HyperVLA的核心思想是利用超网络的分层结构来解耦不同任务所需的技能。</p>\n<p>1. 架构：HyperVLA由一个基础策略和一个超网络组成。</p>\n<p> - 基础策略：一个视觉Transformer（ViT），它仅将当前图像观察作为输入来预测动作。它包括一个DINOv2图像编码器、一个线性投影层、一个小型Transformer策略头和一个线性动作头。</p>\n<p> - 超网络（HN）：一个基于Transformer的上下文编码器，它接收语言指令（通过冻结的T5编码器）、初始图像的类别标记（通过冻结的DINOv2编码器）和一个可学习的任务上下文标记作为输入，然后生成基础策略的参数。</p>\n<p>2. 算法设计：</p>\n<p> - 视觉主干网络：使用预训练的DINOv2作为基础策略的图像编码器并进行微调，以利用视觉基础模型的先验知识，避免在相对较小的机器人数据集上从头训练导致的过拟合。</p>\n<p> - 上下文嵌入归一化：为了稳定超网络的训练，在将上下文嵌入输入到输出头之前进行归一化。这使得基础策略参数的更新动态与直接训练该策略时相似。</p>\n<p> - 动作生成策略：采用简单的线性动作头和均方误差（MSE）损失进行训练，而非现有VLAs常用的自回归或扩散模型，这在HN-based VLA设置中更简单、更高效。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：模型在Open X-Embodiment (OXE)数据集上进行训练。评估在SIMPLER和LIBERO两个模拟基准上进行，SIMPLER用于评估零样本泛化能力，LIBERO用于评估少样本适应能力。</li><li>训练资源：HyperVLA在4块NVIDIA A5000 GPU上训练了一天。作为对比，基线模型OpenVLA在64块A100 GPU上训练了14天。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：性能评估在两个模拟环境中进行：SIMPLER和LIBERO。推理速度（时间/步）在NVIDIA L4 GPU上进行测量。</li><li>评估指标：</li></ol>\n<p> - 任务成功率：在SIMPLER和LIBERO基准上，评估模型完成任务的百分比。</p>\n<p> - 推理效率：通过激活的参数数量、每推理步的耗时（毫秒）和浮点运算数（FLOPs）来衡量。</p>"
  },
  {
    "date": "2025-10-05",
    "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context",
    "link": "http://arxiv.org/abs/2510.04246",
    "summary_markdown": "## 论文总结\n\n**论文研究单位：**\nKAIST（韩国科学技术院）、RLWRLD、加州大学伯克利分校\n\n**论文概述：**\n针对部分可观察机器人任务中时序上下文缺失导致的策略性能不稳定问题，提出ContextVLA框架。该框架基于预训练视觉语言模型（VLM）的视觉语言动作模型（VLA），通过将历史多帧观测压缩为单一上下文令牌来高效利用时序信息，同时避免直接处理高维视频序列的计算开销。与传统多帧训练策略不同，该方法利用VLM的时序理解能力，在模拟和真实机器人任务中均显著提升策略性能。\n\n**论文核心贡献点：**\n1. **揭示关键瓶颈**：系统性分析发现VLA架构在利用多帧观测方面优于传统策略，原因在于预训练VLM具备的时序理解能力；\n2. **创新压缩机制**：提出分两阶段处理策略——前半段VLM层保留多帧输入以提取时序特征，后半段将历史帧聚合为单令牌（平均池化），实现高效时序上下文融合；\n3. **通用适配性**：支持自回归和扩散式动作解码器，可直接增强现有VLA模型（如π₀、GR00T N1.5）；\n4. **显著性能提升**：\n - 模拟任务：Libero基准平均提升1.9%，Simpler-WidowX提升14.4%（41.8%→56.2%）\n - 真实任务：长时序\"Pick-and-Place Twice\"任务从25%提升至65%\n\n**论文方法描述：**\n- **观测压缩流程**：\n 1. 用视觉编码器处理8帧历史观测得到视觉特征\n 2. 在VLM第2层对历史帧隐藏状态执行平均池化生成上下文令牌m\n 3. 用m替代后续VLM层中的历史帧令牌，仅保留当前帧令牌\n- **动作生成**：上下文令牌与VLM当前层特征共同输入动作解码器，支持自回归（令牌化动作）或扩散式生成\n- **高效推理**：结合因果注意力掩码与KV缓存机制，在t-1步预计算历史令牌和KV缓存，t步仅处理当前观测和上下文令牌\n\n**论文使用数据集和训练资源：**\n- **模拟数据集**：Libero（40个任务）、Simpler-WidowX（4任务）、Robocasa（24任务）\n- **真实数据集**：Bridge v2、3类长时序操作任务（抓取-放置-两次、覆盖-堆叠）\n- **训练配置**：\n - 60K迭代，批量32，优化器AdamW\n - 8帧历史观测，在第2层VLM块执行压缩（n=2）\n - 硬件：NVIDIA A100 80GB GPU（4卡并行训练，单卡推理）\n\n**论文使用的评估环境和评估指标：**\n- **环境**：\n - 模拟任务：Libero（4子基准）、Simpler-WidowX（4任务）、Robocasa（24任务）\n - 真实机器人任务：3类时序敏感操作（抓取-放置序列、手部开合等）\n- **指标**：\n - **性能**：任务成功率（%）\n - **效率**：训练墙钟时间（基于π₀在Libero的60K迭代训练）、推理延迟（ms/8帧2视角输入）\n- **关键结果**：\n - 在Simpler-WidowX实现14.4%平均提升\n - 推理时间从227.2ms（未压缩）降至96.3ms（压缩+KV缓存）\n - 长时序真实任务成功率最高达80%\n---\n注：该论文在ICLR/NeurIPS等多帧机器人策略研究基础上，通过VLM时序建模和计算优化，首次系统解决了多帧观测在VLA中的高效利用难题。",
    "summary_html": "<h2>论文总结</h2>\n\n<p><strong>论文研究单位：</strong></p>\n<p>KAIST（韩国科学技术院）、RLWRLD、加州大学伯克利分校</p>\n\n<p><strong>论文概述：</strong></p>\n<p>针对部分可观察机器人任务中时序上下文缺失导致的策略性能不稳定问题，提出ContextVLA框架。该框架基于预训练视觉语言模型（VLM）的视觉语言动作模型（VLA），通过将历史多帧观测压缩为单一上下文令牌来高效利用时序信息，同时避免直接处理高维视频序列的计算开销。与传统多帧训练策略不同，该方法利用VLM的时序理解能力，在模拟和真实机器人任务中均显著提升策略性能。</p>\n\n<p><strong>论文核心贡献点：</strong></p>\n<ol><li><strong>揭示关键瓶颈</strong>：系统性分析发现VLA架构在利用多帧观测方面优于传统策略，原因在于预训练VLM具备的时序理解能力；</li><li><strong>创新压缩机制</strong>：提出分两阶段处理策略——前半段VLM层保留多帧输入以提取时序特征，后半段将历史帧聚合为单令牌（平均池化），实现高效时序上下文融合；</li><li><strong>通用适配性</strong>：支持自回归和扩散式动作解码器，可直接增强现有VLA模型（如π₀、GR00T N1.5）；</li><li><strong>显著性能提升</strong>：</li></ol>\n<p> - 模拟任务：Libero基准平均提升1.9%，Simpler-WidowX提升14.4%（41.8%→56.2%）</p>\n<p> - 真实任务：长时序\"Pick-and-Place Twice\"任务从25%提升至65%</p>\n\n<p><strong>论文方法描述：</strong></p>\n<ul><li><strong>观测压缩流程</strong>：</li></ul>\n<p> 1. 用视觉编码器处理8帧历史观测得到视觉特征</p>\n<p> 2. 在VLM第2层对历史帧隐藏状态执行平均池化生成上下文令牌m</p>\n<p> 3. 用m替代后续VLM层中的历史帧令牌，仅保留当前帧令牌</p>\n<ul><li><strong>动作生成</strong>：上下文令牌与VLM当前层特征共同输入动作解码器，支持自回归（令牌化动作）或扩散式生成</li><li><strong>高效推理</strong>：结合因果注意力掩码与KV缓存机制，在t-1步预计算历史令牌和KV缓存，t步仅处理当前观测和上下文令牌</li></ul>\n\n<p><strong>论文使用数据集和训练资源：</strong></p>\n<ul><li><strong>模拟数据集</strong>：Libero（40个任务）、Simpler-WidowX（4任务）、Robocasa（24任务）</li><li><strong>真实数据集</strong>：Bridge v2、3类长时序操作任务（抓取-放置-两次、覆盖-堆叠）</li><li><strong>训练配置</strong>：</li></ul>\n<p> - 60K迭代，批量32，优化器AdamW</p>\n<p> - 8帧历史观测，在第2层VLM块执行压缩（n=2）</p>\n<p> - 硬件：NVIDIA A100 80GB GPU（4卡并行训练，单卡推理）</p>\n\n<p><strong>论文使用的评估环境和评估指标：</strong></p>\n<ul><li><strong>环境</strong>：</li></ul>\n<p> - 模拟任务：Libero（4子基准）、Simpler-WidowX（4任务）、Robocasa（24任务）</p>\n<p> - 真实机器人任务：3类时序敏感操作（抓取-放置序列、手部开合等）</p>\n<ul><li><strong>指标</strong>：</li></ul>\n<p> - <strong>性能</strong>：任务成功率（%）</p>\n<p> - <strong>效率</strong>：训练墙钟时间（基于π₀在Libero的60K迭代训练）、推理延迟（ms/8帧2视角输入）</p>\n<ul><li><strong>关键结果</strong>：</li></ul>\n<p> - 在Simpler-WidowX实现14.4%平均提升</p>\n<p> - 推理时间从227.2ms（未压缩）降至96.3ms（压缩+KV缓存）</p>\n<p> - 长时序真实任务成功率最高达80%</p>\n<hr/>\n<p>注：该论文在ICLR/NeurIPS等多帧机器人策略研究基础上，通过VLM时序建模和计算优化，首次系统解决了多帧观测在VLA中的高效利用难题。</p>"
  },
  {
    "date": "2025-10-05",
    "title": "SITCOM: Scaling Inference-Time COMpute for VLAs",
    "link": "http://arxiv.org/abs/2510.04041",
    "summary_markdown": "### 论文研究单位\nCarnegie Mellon University\n### 论文概述\n论文提出了一种名为SITCOM（Scaling Inference-Time COMpute for VLAs）的框架，旨在解决视觉-语言-动作模型在长时程任务中缺乏前瞻规划和误差累积的问题。该框架通过结合预训练的VLA、一个学习的动力学模型和一个奖励模型，将VLA从单步动作执行器转变为能够进行多步规划的鲁棒长期规划器。SITCOM利用模型预测控制（MPC）的思想，在推理时生成多个动作序列的展开，并通过奖励机制选择最优序列执行。在SIMPLER环境中的实验表明，该方法能将任务完成率从48%显著提升至72%。\n### 论文核心贡献点\n1. 提出了一个通用的推理时规划框架SITCOM，可以增强任何预训练的VLA模型，通过模拟多步动作展开并基于奖励选择最优动作序列。\n2. 开发了一个高效的基于Transformer的动力学模型，该模型在BridgeV2数据上预训练，并在SIMPLER环境中微调以弥合真实到模拟的差距。同时，引入了一种受DAgger启发的自适应策略来减少长时程展开中的误差累积。\n3. 提供了对通过增加候选动作序列数量和未来预测深度来扩展推理时计算以获得性能提升的深入分析。\n### 论文方法描述\nSITCOM的核心是一个迭代决策过程。在每个决策点：\n1. VLA模型根据当前图像观察和任务指令，通过高温度采样生成n个候选动作。\n2. 每个候选动作初始化一个轨迹。利用一个独立的动力学模型来模拟未来状态，即对于每个候选动作，动力学模型预测下一帧图像。\n3. 这个过程会迭代l步（rollout length），为每个候选动作生成一个完整的多步动作序列和相应的未来状态轨迹。\n4. 使用一个奖励模型对每条轨迹的最终状态进行评分，该奖励函数综合考虑了夹爪与物体的间隙、物体与目标的距离以及抓取成功与否等指标。\n5. 选择奖励最高的轨迹，并将其第一个动作在真实世界中执行。\n6. 更新环境观察，并根据预设的重规划频率重复此过程，直到任务完成或终止。\n该方法包含两个变体：SITCOM (EnvSim) 使用真实的模拟器进行展开，而 SITCOM (World Model) 则使用训练好的动力学模型进行展开。\n### 论文使用数据集和训练资源\n* **数据集**:\n * 动力学模型预训练：使用了BridgeV2数据集，包含约25,000条轨迹，覆盖了13种操作技能和24个场景。\n * 动力学模型微调与VLA微调：使用了在SIMPLER环境中收集的100条多任务专家轨迹。\n* **训练资源**:\n * 论文中未明确指定训练所使用的具体硬件（如GPU类型和数量）或训练时长。\n### 论文使用的评估环境和评估指标\n* **评估环境**:\n * SIMPLER，一个开源的模拟环境套件，用于评估通用机器人操作策略。\n * 使用了7自由度的WidowX机械臂进行四项任务的评估。\n* **评估指标**:\n * **Average Success Rate (平均成功率)**: 主要性能指标，计算为成功完成的任务数与总试验数的比值。\n * **Partial Success Rate (部分成功率)**: 衡量机器人实现部分目标的场景，例如成功抓取物体但未能正确放置。\n * **Time (时间)**: 与计算资源成正比，衡量为不同数量的候选轨迹生成计划所需的时间，评估了推理时的计算开销。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Carnegie Mellon University</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为SITCOM（Scaling Inference-Time COMpute for VLAs）的框架，旨在解决视觉-语言-动作模型在长时程任务中缺乏前瞻规划和误差累积的问题。该框架通过结合预训练的VLA、一个学习的动力学模型和一个奖励模型，将VLA从单步动作执行器转变为能够进行多步规划的鲁棒长期规划器。SITCOM利用模型预测控制（MPC）的思想，在推理时生成多个动作序列的展开，并通过奖励机制选择最优序列执行。在SIMPLER环境中的实验表明，该方法能将任务完成率从48%显著提升至72%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个通用的推理时规划框架SITCOM，可以增强任何预训练的VLA模型，通过模拟多步动作展开并基于奖励选择最优动作序列。</li><li>开发了一个高效的基于Transformer的动力学模型，该模型在BridgeV2数据上预训练，并在SIMPLER环境中微调以弥合真实到模拟的差距。同时，引入了一种受DAgger启发的自适应策略来减少长时程展开中的误差累积。</li><li>提供了对通过增加候选动作序列数量和未来预测深度来扩展推理时计算以获得性能提升的深入分析。</li></ol>\n<h3>论文方法描述</h3>\n<p>SITCOM的核心是一个迭代决策过程。在每个决策点：</p>\n<ol><li>VLA模型根据当前图像观察和任务指令，通过高温度采样生成n个候选动作。</li><li>每个候选动作初始化一个轨迹。利用一个独立的动力学模型来模拟未来状态，即对于每个候选动作，动力学模型预测下一帧图像。</li><li>这个过程会迭代l步（rollout length），为每个候选动作生成一个完整的多步动作序列和相应的未来状态轨迹。</li><li>使用一个奖励模型对每条轨迹的最终状态进行评分，该奖励函数综合考虑了夹爪与物体的间隙、物体与目标的距离以及抓取成功与否等指标。</li><li>选择奖励最高的轨迹，并将其第一个动作在真实世界中执行。</li><li>更新环境观察，并根据预设的重规划频率重复此过程，直到任务完成或终止。</li></ol>\n<p>该方法包含两个变体：SITCOM (EnvSim) 使用真实的模拟器进行展开，而 SITCOM (World Model) 则使用训练好的动力学模型进行展开。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>* <strong>数据集</strong>:</p>\n<p> * 动力学模型预训练：使用了BridgeV2数据集，包含约25,000条轨迹，覆盖了13种操作技能和24个场景。</p>\n<p> * 动力学模型微调与VLA微调：使用了在SIMPLER环境中收集的100条多任务专家轨迹。</p>\n<p>* <strong>训练资源</strong>:</p>\n<p> * 论文中未明确指定训练所使用的具体硬件（如GPU类型和数量）或训练时长。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>* <strong>评估环境</strong>:</p>\n<p> * SIMPLER，一个开源的模拟环境套件，用于评估通用机器人操作策略。</p>\n<p> * 使用了7自由度的WidowX机械臂进行四项任务的评估。</p>\n<p>* <strong>评估指标</strong>:</p>\n<p> * <strong>Average Success Rate (平均成功率)</strong>: 主要性能指标，计算为成功完成的任务数与总试验数的比值。</p>\n<p> * <strong>Partial Success Rate (部分成功率)</strong>: 衡量机器人实现部分目标的场景，例如成功抓取物体但未能正确放置。</p>\n<p> * <strong>Time (时间)</strong>: 与计算资源成正比，衡量为不同数量的候选轨迹生成计划所需的时间，评估了推理时的计算开销。</p>"
  },
  {
    "date": "2025-10-04",
    "title": "Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert",
    "link": "http://arxiv.org/abs/2510.03896",
    "summary_markdown": "论文研究单位\n浙江大学和上海人工智能实验室。\n\n论文概述\n该论文提出了一种新框架，旨在解决将视觉-语言模型（VLM）的规划与推理能力转化为物理世界动作时面临的挑战。传统的视觉-语言-动作（VLA）模型因将推理与行动耦合在单一架构中，且依赖稀缺的领域数据，导致泛化能力差。本文引入了以“通用动作专家”为核心的框架，通过使用稀疏的3D轨迹作为明确的中间表示，将VLM的高级规划与低级的物理动作执行完全解耦。VLM仅需生成粗略的3D路径点，再由动作专家根据实时点云观测将其细化为密集、可执行的动作序列。此外，论文还提出了“动作预训练，点云微调”的训练范式，以提升训练效率和泛化鲁棒性。该方法结合了VLM在视觉理解与规划上的泛化能力，以及动作专家在细粒度动作层面的泛化能力，在仿真和真实世界实验中均展现了强大的零样本部署能力。\n\n论文核心贡献点\n- 提出了一个以通用动作专家为中心、以稀疏3D轨迹为清晰接口的框架，实现了高级VLM规划与低级电机控制的完全解耦。据作者所知，这是首次训练出无需任务特定微调即可部署的通用专家。\n- 提出了“动作预训练，点云微调”策略，使动作专家专注于几何轨迹的精炼而非语义解释，从而实现泛化。\n- 系统在多样化的实验中展现了卓越的泛化能力，通过成功的零样本部署（无需任何领域内微调）验证了其可行性。\n\n论文方法描述\n方法流程分为两部分：\n1. **VLM规划模块**：VLM在少量经过精心标注的数据上进行监督微调，学习根据2D关键点和深度信息，在相机坐标系中直接预测稀疏的3D路径点和目标末端执行器姿态。这些路径点被转换到机器人基座坐标系后，通过B样条插值生成一条连续平滑的姿态轨迹，为后续的动作专家提供密集的指导信号。这种方式更符合VLM以视觉为中心的特性，避免了学习复杂的相机到机器人坐标转换。\n2. **通用动作专家训练与执行**：动作专家被建模为一个条件扩散策略。\n - **模型架构**：该模型接收多模态输入，包括机器人本体状态S_t、从轨迹中采样的指导姿态P_g以及裁剪后的环境点云O_pcd。这些输入经由各自的编码器处理后，拼接成最终的条件特征向量。\n - **训练范式**：采用“动作预训练，点云微调”的两阶段策略。预训练阶段，模型在仅包含轨迹数据的大批次（可达32768）上训练，学习基本的轨迹跟随能力，此阶段点云特征被屏蔽。微调阶段，模型在包含点云的数据上进行训练，学习根据环境观测对轨迹进行精细化调整。这种解耦训练策略加快了收敛速度并提高了数据利用效率。\n\n论文使用数据集和训练资源\n- **数据集**：结合了仿真和真实世界的数据。\n - 仿真数据：RoboTwin 2.0, CALVIN, LIBERO, RLBench。\n - 真实世界数据：DROID, AgiBot World, BridgeV2。\n - 为解决真实世界数据深度图稀疏的问题，论文使用FoundationStereo处理DROID数据，使用PromptDepthAnything处理AgiBot数据，并使用MoGe对部分数据进行深度补全，以增强模型在低质量深度信息下的鲁棒性。\n- **训练资源**：\n - VLM微调：在8块80GB NVIDIA A100 GPU上训练1000步，批次大小为32。\n - 动作专家训练：在相同硬件上进行。预训练阶段持续2天，批次大小为32768；点云微调阶段持续3天，批次大小为256。\n\n论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真环境：RoboTwin和ManiSkill基准。\n - 真实世界环境：配备UMI夹持器的Franka Research 3机械臂，以及一个固定的RealSense D435相机（640x480分辨率）。控制频率为20Hz。动作空间定义为SE(3)，即表示末端执行器的7维绝对目标位姿（3D位置+4D四元数）。\n - 真实世界任务：设计了6个任务，分为短、中、长时程三类，每类两个任务，以评估不同复杂度下的性能。\n- **评估指标**：\n - 主要评估指标为任务成功率。\n - 泛化能力评估：通过在未见过的相机视角、新物体/颜色/语义指令下的零样本性能，以及在ManiSkill（训练中未包含的环境）上的跨环境迁移能力来衡量。\n - 消融研究：通过分析不同训练步数、噪声尺度、训练策略对最终成功率的影响来验证模型设计的有效性。同时，使用MMLU分数来衡量VLM在微调后语言能力的保留情况。",
    "summary_html": "<p>论文研究单位</p>\n<p>浙江大学和上海人工智能实验室。</p>\n\n<p>论文概述</p>\n<p>该论文提出了一种新框架，旨在解决将视觉-语言模型（VLM）的规划与推理能力转化为物理世界动作时面临的挑战。传统的视觉-语言-动作（VLA）模型因将推理与行动耦合在单一架构中，且依赖稀缺的领域数据，导致泛化能力差。本文引入了以“通用动作专家”为核心的框架，通过使用稀疏的3D轨迹作为明确的中间表示，将VLM的高级规划与低级的物理动作执行完全解耦。VLM仅需生成粗略的3D路径点，再由动作专家根据实时点云观测将其细化为密集、可执行的动作序列。此外，论文还提出了“动作预训练，点云微调”的训练范式，以提升训练效率和泛化鲁棒性。该方法结合了VLM在视觉理解与规划上的泛化能力，以及动作专家在细粒度动作层面的泛化能力，在仿真和真实世界实验中均展现了强大的零样本部署能力。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了一个以通用动作专家为中心、以稀疏3D轨迹为清晰接口的框架，实现了高级VLM规划与低级电机控制的完全解耦。据作者所知，这是首次训练出无需任务特定微调即可部署的通用专家。</li><li>提出了“动作预训练，点云微调”策略，使动作专家专注于几何轨迹的精炼而非语义解释，从而实现泛化。</li><li>系统在多样化的实验中展现了卓越的泛化能力，通过成功的零样本部署（无需任何领域内微调）验证了其可行性。</li></ul>\n\n<p>论文方法描述</p>\n<p>方法流程分为两部分：</p>\n<ol><li><strong>VLM规划模块</strong>：VLM在少量经过精心标注的数据上进行监督微调，学习根据2D关键点和深度信息，在相机坐标系中直接预测稀疏的3D路径点和目标末端执行器姿态。这些路径点被转换到机器人基座坐标系后，通过B样条插值生成一条连续平滑的姿态轨迹，为后续的动作专家提供密集的指导信号。这种方式更符合VLM以视觉为中心的特性，避免了学习复杂的相机到机器人坐标转换。</li><li><strong>通用动作专家训练与执行</strong>：动作专家被建模为一个条件扩散策略。</li></ol>\n<p> - <strong>模型架构</strong>：该模型接收多模态输入，包括机器人本体状态S_t、从轨迹中采样的指导姿态P_g以及裁剪后的环境点云O_pcd。这些输入经由各自的编码器处理后，拼接成最终的条件特征向量。</p>\n<p> - <strong>训练范式</strong>：采用“动作预训练，点云微调”的两阶段策略。预训练阶段，模型在仅包含轨迹数据的大批次（可达32768）上训练，学习基本的轨迹跟随能力，此阶段点云特征被屏蔽。微调阶段，模型在包含点云的数据上进行训练，学习根据环境观测对轨迹进行精细化调整。这种解耦训练策略加快了收敛速度并提高了数据利用效率。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>数据集</strong>：结合了仿真和真实世界的数据。</li></ul>\n<p> - 仿真数据：RoboTwin 2.0, CALVIN, LIBERO, RLBench。</p>\n<p> - 真实世界数据：DROID, AgiBot World, BridgeV2。</p>\n<p> - 为解决真实世界数据深度图稀疏的问题，论文使用FoundationStereo处理DROID数据，使用PromptDepthAnything处理AgiBot数据，并使用MoGe对部分数据进行深度补全，以增强模型在低质量深度信息下的鲁棒性。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - VLM微调：在8块80GB NVIDIA A100 GPU上训练1000步，批次大小为32。</p>\n<p> - 动作专家训练：在相同硬件上进行。预训练阶段持续2天，批次大小为32768；点云微调阶段持续3天，批次大小为256。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真环境：RoboTwin和ManiSkill基准。</p>\n<p> - 真实世界环境：配备UMI夹持器的Franka Research 3机械臂，以及一个固定的RealSense D435相机（640x480分辨率）。控制频率为20Hz。动作空间定义为SE(3)，即表示末端执行器的7维绝对目标位姿（3D位置+4D四元数）。</p>\n<p> - 真实世界任务：设计了6个任务，分为短、中、长时程三类，每类两个任务，以评估不同复杂度下的性能。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要评估指标为任务成功率。</p>\n<p> - 泛化能力评估：通过在未见过的相机视角、新物体/颜色/语义指令下的零样本性能，以及在ManiSkill（训练中未包含的环境）上的跨环境迁移能力来衡量。</p>\n<p> - 消融研究：通过分析不同训练步数、噪声尺度、训练策略对最终成功率的影响来验证模型设计的有效性。同时，使用MMLU分数来衡量VLM在微调后语言能力的保留情况。</p>"
  },
  {
    "date": "2025-10-04",
    "title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation",
    "link": "http://arxiv.org/abs/2510.03895",
    "summary_markdown": "# 论文研究单位\n浙江大学\n# 论文概述\nNoTVLA（Narrowing of Trajectory Vision-Language-Action）是一个针对通用机器人操作的框架，旨在解决视觉-语言-动作（VLA）模型中的灾难性遗忘问题。该框架通过将密集的动作轨迹压缩为稀疏的轨迹表示，避免了密集轨迹微调带来的知识遗忘，同时显著降低了计算成本。方法采用三阶段流程：首先通过锚点预测进行深度推理，然后基于运动学原理选择关键帧，最后使用样条基动作去令牌化器生成平滑的高频轨迹。\n# 论文核心贡献点\n- 解耦高层VLM与低层动作专家，在减少微调计算的同时提高具身任务成功率\n- 使用稀疏且语义修剪的轨迹监督，增强跨平台和跨任务的泛化能力，缓解灾难性遗忘\n- 保留模型内在的视觉语言推理能力，支持复杂指令跟随和多轮交互，无需额外任务特定微调\n- 在训练效率上显著优于现有方法，使用计算资源比π₀少一个数量级以上\n# 论文方法描述\n**锚点预测与令牌生成**：通过锚点预测模块（APP）输出2D锚点，结合外部深度源获取深度信息，在锚点条件化令牌生成（ACTG）中输出包含深度、图像坐标、夹爪状态和姿态的多模态令牌序列。\n\n**运动学基关键帧选择**：基于末端执行器的加速度阈值和夹爪状态变化来识别关键帧，将原始演示分割成逻辑阶段，并通过子关键帧和均匀下采样来保持时间相干性。\n\n**样条基动作去令牌化器**：将离散令牌序列转换为平滑的高频机器人轨迹。使用三次样条插值处理XYZ位置，球面线性插值（SLERP）处理四元数姿态，确保位置和方向的高频平滑轨迹。\n# 论文使用数据集和训练资源\n- **数据来源**：ManiSkill（3000条轨迹）、RoboTwin 2.0（约2000条轨迹）、AgiBot World（500条轨迹）\n- **训练平台**：统一模拟混合环境，包含40个任务的多平台机器人数据\n- **硬件要求**：在32 GPU小时下完成训练，即使在约8 GPU预算下也能展现执行能力\n- **模型规模**：基于Qwen VL 2.5（7B）参数模型\n- **多平台支持**：Aloha-AgileX、ARX-X5、Franka Panda、UR5、Piper、AgiBot-G1\n# 论文使用的评估环境和评估指标\n- **基准测试**：RoboTwin 2.0官方基准、AGIBOT挑战赛官方评估\n- **主要指标**：任务成功率（0-1之间），与专家模型（ACT、DP、DP3）和通用模型（π₀、RDT）对比\n- **泛化测试**：零样本泛化评估，包括未见指令、背景变换、颜色提示反转等场景\n- **轨迹质量评估**：几何和时间指标，包括F1分数、DTW距离、Fréchet距离、Hausdorff距离\n- **跨视角评估**：训练域内（ID）与训练域外（OOD）视角性能对比\n- **多任务综合性能**：在AGIBOT挑战的10个官方任务中进行测试，总分从2.795提升至3.697",
    "summary_html": "<h1>论文研究单位</h1>\n<p>浙江大学</p>\n<h1>论文概述</h1>\n<p>NoTVLA（Narrowing of Trajectory Vision-Language-Action）是一个针对通用机器人操作的框架，旨在解决视觉-语言-动作（VLA）模型中的灾难性遗忘问题。该框架通过将密集的动作轨迹压缩为稀疏的轨迹表示，避免了密集轨迹微调带来的知识遗忘，同时显著降低了计算成本。方法采用三阶段流程：首先通过锚点预测进行深度推理，然后基于运动学原理选择关键帧，最后使用样条基动作去令牌化器生成平滑的高频轨迹。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>解耦高层VLM与低层动作专家，在减少微调计算的同时提高具身任务成功率</li><li>使用稀疏且语义修剪的轨迹监督，增强跨平台和跨任务的泛化能力，缓解灾难性遗忘</li><li>保留模型内在的视觉语言推理能力，支持复杂指令跟随和多轮交互，无需额外任务特定微调</li><li>在训练效率上显著优于现有方法，使用计算资源比π₀少一个数量级以上</li></ul>\n<h1>论文方法描述</h1>\n<p><strong>锚点预测与令牌生成</strong>：通过锚点预测模块（APP）输出2D锚点，结合外部深度源获取深度信息，在锚点条件化令牌生成（ACTG）中输出包含深度、图像坐标、夹爪状态和姿态的多模态令牌序列。</p>\n\n<p><strong>运动学基关键帧选择</strong>：基于末端执行器的加速度阈值和夹爪状态变化来识别关键帧，将原始演示分割成逻辑阶段，并通过子关键帧和均匀下采样来保持时间相干性。</p>\n\n<p><strong>样条基动作去令牌化器</strong>：将离散令牌序列转换为平滑的高频机器人轨迹。使用三次样条插值处理XYZ位置，球面线性插值（SLERP）处理四元数姿态，确保位置和方向的高频平滑轨迹。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>数据来源</strong>：ManiSkill（3000条轨迹）、RoboTwin 2.0（约2000条轨迹）、AgiBot World（500条轨迹）</li><li><strong>训练平台</strong>：统一模拟混合环境，包含40个任务的多平台机器人数据</li><li><strong>硬件要求</strong>：在32 GPU小时下完成训练，即使在约8 GPU预算下也能展现执行能力</li><li><strong>模型规模</strong>：基于Qwen VL 2.5（7B）参数模型</li><li><strong>多平台支持</strong>：Aloha-AgileX、ARX-X5、Franka Panda、UR5、Piper、AgiBot-G1</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>基准测试</strong>：RoboTwin 2.0官方基准、AGIBOT挑战赛官方评估</li><li><strong>主要指标</strong>：任务成功率（0-1之间），与专家模型（ACT、DP、DP3）和通用模型（π₀、RDT）对比</li><li><strong>泛化测试</strong>：零样本泛化评估，包括未见指令、背景变换、颜色提示反转等场景</li><li><strong>轨迹质量评估</strong>：几何和时间指标，包括F1分数、DTW距离、Fréchet距离、Hausdorff距离</li><li><strong>跨视角评估</strong>：训练域内（ID）与训练域外（OOD）视角性能对比</li><li><strong>多任务综合性能</strong>：在AGIBOT挑战的10个官方任务中进行测试，总分从2.795提升至3.697</li></ul>"
  },
  {
    "date": "2025-10-04",
    "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization",
    "link": "http://arxiv.org/abs/2510.03827",
    "summary_markdown": "```markdown\n## 论文研究单位\n华中科技大学、哈佛大学、麻省理工学院、武汉理工大学、理海大学\n## 论文概述\n论文指出LIBERO基准作为视觉-语言-动作（VLA）模型主流评估平台存在训练与评估同质化问题，导致模型通过记忆训练数据即可获得超90%的虚高性能。为此，提出LIBERO-PRO扩展基准，通过四个维度（操作对象、初始状态、任务指令、环境）引入扰动评估模型鲁棒性。实验表明，现有模型在扰动条件下性能骤降至0%，暴露其缺乏真实任务理解和泛化能力，仅依赖动作序列记忆。\n## 论文核心贡献点\n1. 通过实验证明LIBERO当前评估协议存在根本缺陷：高分数主要反映训练数据记忆而非真实任务理解。\n2. 提出LIBERO-PRO基准，支持对象、位置、指令、环境四维度扰动及随机组合，实现更可靠评估。\n3. 在LIBERO-PRO上评测OpenVLA、pi0等模型，揭示其在扰动下性能崩溃，呼吁采用新评估标准。\n## 论文方法描述\n定义扰动框架$\\tau^{(k)}=\\phi_k(\\tau)$，$k \\in \\{O,S,L,E\\}$：\n- **对象属性扰动**：修改非关键属性（如颜色、大小），任务语义不变。\n- **初始位置扰动**：调整物体位置，保持物理合理性。\n- **指令扰动**：分语义级（改写指令）和任务级（更换目标对象/动作），确保组件来自训练集。\n- **环境扰动**：替换背景场景，不影响任务可行性。\n施加约束：扰动幅度$\\delta_k$可控，任务间总变距离$d_{TV} > \\epsilon$，确保任务有效且差异显著。\n## 论文使用数据集和训练资源\n- **数据集**：基于LIBERO原始数据扩展，新增对象资产、空间区域、指令改写版本及环境变体。\n- **训练资源**：直接使用预训练模型，未新增训练。评估模型包括OpenVLA（多任务检查点）、pi0/pi0.5（单任务检查点），均采用官方发布权重。\n## 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO-PRO仿真环境，支持四维度扰动随机组合。\n- **评估指标**：任务成功率（Success Rate），每任务50次试验。对比标准LIBERO与扰动场景下的表现，包括对象（Obj）、位置（Pos）、语义（Sem）、任务（Task）、环境（Env）扰动。\n```",
    "summary_html": "<p>```markdown</p>\n<h2>论文研究单位</h2>\n<p>华中科技大学、哈佛大学、麻省理工学院、武汉理工大学、理海大学</p>\n<h2>论文概述</h2>\n<p>论文指出LIBERO基准作为视觉-语言-动作（VLA）模型主流评估平台存在训练与评估同质化问题，导致模型通过记忆训练数据即可获得超90%的虚高性能。为此，提出LIBERO-PRO扩展基准，通过四个维度（操作对象、初始状态、任务指令、环境）引入扰动评估模型鲁棒性。实验表明，现有模型在扰动条件下性能骤降至0%，暴露其缺乏真实任务理解和泛化能力，仅依赖动作序列记忆。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>通过实验证明LIBERO当前评估协议存在根本缺陷：高分数主要反映训练数据记忆而非真实任务理解。</li><li>提出LIBERO-PRO基准，支持对象、位置、指令、环境四维度扰动及随机组合，实现更可靠评估。</li><li>在LIBERO-PRO上评测OpenVLA、pi0等模型，揭示其在扰动下性能崩溃，呼吁采用新评估标准。</li></ol>\n<h2>论文方法描述</h2>\n<p>定义扰动框架$\\tau^{(k)}=\\phi_k(\\tau)$，$k \\in \\{O,S,L,E\\}$：</p>\n<ul><li><strong>对象属性扰动</strong>：修改非关键属性（如颜色、大小），任务语义不变。</li><li><strong>初始位置扰动</strong>：调整物体位置，保持物理合理性。</li><li><strong>指令扰动</strong>：分语义级（改写指令）和任务级（更换目标对象/动作），确保组件来自训练集。</li><li><strong>环境扰动</strong>：替换背景场景，不影响任务可行性。</li></ul>\n<p>施加约束：扰动幅度$\\delta_k$可控，任务间总变距离$d_{TV} > \\epsilon$，确保任务有效且差异显著。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：基于LIBERO原始数据扩展，新增对象资产、空间区域、指令改写版本及环境变体。</li><li><strong>训练资源</strong>：直接使用预训练模型，未新增训练。评估模型包括OpenVLA（多任务检查点）、pi0/pi0.5（单任务检查点），均采用官方发布权重。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：LIBERO-PRO仿真环境，支持四维度扰动随机组合。</li><li><strong>评估指标</strong>：任务成功率（Success Rate），每任务50次试验。对比标准LIBERO与扰动场景下的表现，包括对象（Obj）、位置（Pos）、语义（Sem）、任务（Task）、环境（Env）扰动。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-10-02",
    "title": "Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer",
    "link": "http://arxiv.org/abs/2510.03342",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-03",
    "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
    "link": "http://arxiv.org/abs/2510.03142",
    "summary_markdown": "### 论文研究单位\n- 北京大学（Peking University）\n- Galbot\n- 上海交通大学（Shanghai Jiao Tong University）\n- 清华大学（Tsinghua University）\n- BAAI（可能指北京人工智能研究院）\n### 论文概述\n论文提出MM-Nav，一个多视图视觉语言行动（VLA）模型，旨在通过多专家学习实现鲁棒的视觉导航。核心思想是结合合成环境中的多样导航数据与VLA模型的泛化能力，解决视觉导航中观察数据建模困难的问题。MM-Nav使用360度视觉观察（四个水平分布的相机视图），通过两阶段训练学习：先从强化学习（RL）专家收集数据预训练，再进行在线教师-学生迭代训练。方法最终在模拟和真实环境中展示了强泛化能力，并优于单一能力RL专家。\n### 论文核心贡献点\n1. 提出MM-Nav多视图VLA模型，支持360度视觉输入，直接输出连续速度命令。\n2. 设计三种RL专家（reaching、squeezing、avoiding），分别学习导航能力。\n3. 两阶段训练：离线数据预训练和在线迭代训练（DAgger方式），引入能力平衡数据聚合策略（基于性能差距动态调整数据比例）。\n4. 在模拟和真实环境中评估，模型性能超越RL教师，验证多能力学习的协同效应。\n5. 提供开源实现和项目页面，促进研究社区应用。\n### 论文方法描述\n- **方法概述**：采用教师-学生范式。学生模型是VLA（基于SigLIP视觉编码器和Qwen2语言模型），处理多视图RGB输入（四个相机），并预测速度命令。教师模型为三个RL专家（训练于不同合成环境，使用特权深度信息）。训练分两阶段：离线预训练（用专家数据初始化VLA）和在线迭代（VLA部署后收集专家数据，动态平衡训练比例）。\n- **RL专家**：三个专家（reaching：到达目标；squeezing：穿行狭窄通道；avoiding：避开动态障碍）。使用PPO算法、深度图像输入（四个相机）、奖励函数针对能力调整。\n- **VLA学生模型**：视觉编码（SigLIP输出视觉令牌；历史滑窗处理），语言提示（点目标转化为文本），动作预测（输出速度[v_x, v_y, v_yaw]）。\n- **在线训练**：能力平衡数据聚合（计算VLA与专家的加权旅行时间（WTT）差距，调整数据比例），迭代优化直至收敛。\n### 论文使用数据集和训练资源\n- **数据集**：合成环境数据（IsaacLab生成）：用于训练RL专家的三个能力特定场景（reaching、squeezing、avoiding），收集500k步专家数据；真实环境测试数据（四个场景：Narrow Zigzag Corridor、Thin Obstacle Avoidance、Dynamic Environment、Cluttered Static Environment）。\n- **训练资源**：\n - RL专家训练：IsaacLab模拟，NVIDIA RTX 4090 GPU，单次训练8-12小时。\n - VLA预训练：8个NVIDIA H100 GPU，约5小时（40 GPU小时）。\n - 在线迭代：每次迭代2小时（200k专家数据）。\n - 部署：服务器配备NVIDIA RTX 5090 GPU，机器人端（Unitree GO2）实时推理。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：IsaacLab中三个能力特定场景（reaching、squeezing、avoiding）和一个混合场景（包含所有能力）。\n - 真实环境：四个场景测试sim-to-real泛化。\n- **评估指标**：\n - 成功率（SR）：成功到达目标的百分比。\n - 碰撞率（CR）：发生碰撞的百分比。\n - 加权旅行时间（WTT）：成功到达目标的平均时间除以成功率。\n- 对比基线：iPlanner、ViPlanner、NavDP等，结果显示MM-Nav在SR和CR上优于基线，WTT更低。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>北京大学（Peking University）</li><li>Galbot</li><li>上海交通大学（Shanghai Jiao Tong University）</li><li>清华大学（Tsinghua University）</li><li>BAAI（可能指北京人工智能研究院）</li></ul>\n<h3>论文概述</h3>\n<p>论文提出MM-Nav，一个多视图视觉语言行动（VLA）模型，旨在通过多专家学习实现鲁棒的视觉导航。核心思想是结合合成环境中的多样导航数据与VLA模型的泛化能力，解决视觉导航中观察数据建模困难的问题。MM-Nav使用360度视觉观察（四个水平分布的相机视图），通过两阶段训练学习：先从强化学习（RL）专家收集数据预训练，再进行在线教师-学生迭代训练。方法最终在模拟和真实环境中展示了强泛化能力，并优于单一能力RL专家。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出MM-Nav多视图VLA模型，支持360度视觉输入，直接输出连续速度命令。</li><li>设计三种RL专家（reaching、squeezing、avoiding），分别学习导航能力。</li><li>两阶段训练：离线数据预训练和在线迭代训练（DAgger方式），引入能力平衡数据聚合策略（基于性能差距动态调整数据比例）。</li><li>在模拟和真实环境中评估，模型性能超越RL教师，验证多能力学习的协同效应。</li><li>提供开源实现和项目页面，促进研究社区应用。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>方法概述</strong>：采用教师-学生范式。学生模型是VLA（基于SigLIP视觉编码器和Qwen2语言模型），处理多视图RGB输入（四个相机），并预测速度命令。教师模型为三个RL专家（训练于不同合成环境，使用特权深度信息）。训练分两阶段：离线预训练（用专家数据初始化VLA）和在线迭代（VLA部署后收集专家数据，动态平衡训练比例）。</li><li><strong>RL专家</strong>：三个专家（reaching：到达目标；squeezing：穿行狭窄通道；avoiding：避开动态障碍）。使用PPO算法、深度图像输入（四个相机）、奖励函数针对能力调整。</li><li><strong>VLA学生模型</strong>：视觉编码（SigLIP输出视觉令牌；历史滑窗处理），语言提示（点目标转化为文本），动作预测（输出速度[v_x, v_y, v_yaw]）。</li><li><strong>在线训练</strong>：能力平衡数据聚合（计算VLA与专家的加权旅行时间（WTT）差距，调整数据比例），迭代优化直至收敛。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：合成环境数据（IsaacLab生成）：用于训练RL专家的三个能力特定场景（reaching、squeezing、avoiding），收集500k步专家数据；真实环境测试数据（四个场景：Narrow Zigzag Corridor、Thin Obstacle Avoidance、Dynamic Environment、Cluttered Static Environment）。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - RL专家训练：IsaacLab模拟，NVIDIA RTX 4090 GPU，单次训练8-12小时。</p>\n<p> - VLA预训练：8个NVIDIA H100 GPU，约5小时（40 GPU小时）。</p>\n<p> - 在线迭代：每次迭代2小时（200k专家数据）。</p>\n<p> - 部署：服务器配备NVIDIA RTX 5090 GPU，机器人端（Unitree GO2）实时推理。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：IsaacLab中三个能力特定场景（reaching、squeezing、avoiding）和一个混合场景（包含所有能力）。</p>\n<p> - 真实环境：四个场景测试sim-to-real泛化。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（SR）：成功到达目标的百分比。</p>\n<p> - 碰撞率（CR）：发生碰撞的百分比。</p>\n<p> - 加权旅行时间（WTT）：成功到达目标的平均时间除以成功率。</p>\n<ul><li>对比基线：iPlanner、ViPlanner、NavDP等，结果显示MM-Nav在SR和CR上优于基线，WTT更低。</li></ul>"
  },
  {
    "date": "2025-10-03",
    "title": "Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation - Technical Report for IROS 2025 RoboSense Challenge Track 4",
    "link": "http://arxiv.org/abs/2510.02728",
    "summary_markdown": "# 论文研究单位\n\nTsinghua University, Xiaomi EV, HKUSTGZ, Georgia Institute of Technology, National University of Singapore, The Chinese University of Hong Kong, Renmin University of China\n# 论文概述\n\n本文针对跨模态无人机导航任务，提出了一种名为图像描述引导检索系统的两阶段检索优化方法。该方法旨在解决现有基线模型在自然语言查询与视觉内容之间进行细粒度语义匹配时遇到的挑战。首先，利用基线模型进行粗粒度检索，筛选出Top 20的候选图像。然后，使用视觉语言模型为这些候选图像生成详细的描述，并基于原始文本查询与生成描述之间的语义相似度进行重排序，从而构建连接视觉内容和自然语言描述的语义桥梁。该方法在RoboSense 2025挑战赛的Track 4中取得了所有关键指标上5%的稳定提升，并获得TOP-3的成绩。\n# 论文核心贡献点\n\n- 提出了一个图像描述引导检索系统，这是一个两阶段的由粗到细的检索框架，利用视觉语言模型来弥合自然语言查询与航空图像之间的语义鸿沟。\n- 引入了一种新颖的图像描述引导重排序机制，将跨模态匹配转化为文本到文本的相似度计算，从而在复杂的航空场景中实现更细粒度的语义对齐。\n- 与基线方法相比，该方法在所有关键指标上取得了5%的稳定提升，并在IROS 2025 RoboSense挑战赛的跨模态无人机导航赛道中获得了TOP-3的成绩。\n# 论文方法描述\n\n该方法是一个两阶段的由粗到细的检索流程。第一阶段，利用GeoText-1652基线模型（包含Swin Transformer图像编码器、BERT文本编码器和跨模态融合模块）对图像库进行初步检索，为每个文本查询筛选出最相关的20张候选图像。第二阶段，核心创新点在于重排序：首先，使用GPT-4o等视觉语言模型为20张候选图像生成包含丰富语义和空间信息的详细描述；然后，使用BERT等句子嵌入模型计算原始查询与生成描述之间的文本到文本语义相似度；最后，将基线模型的视觉相似度分数与文本语义相似度分数以加权方式（alpha=0.3）融合，根据融合后的最终得分对候选图像进行重排序，得到最终结果。\n# 论文使用数据集和训练资源\n\n- 数据集：RoboSense 2025挑战赛提供的GeoText-1652数据集。训练集包含50,218张图像，测试集包含54,227张图像，每个图像都配有全局描述和包含空间关系（如“左”、“上右”）的区域级描述。\n- 训练资源：基线模型在8块NVIDIA A800 GPU上训练了5个epoch，耗时约30 GPU小时。优化器使用AdamW，学习率为3e-5。图像描述生成阶段使用GPT-4o，为所有候选图像离线生成描述耗时约12小时。\n# 论文使用的评估环境和评估指标\n\n- 评估环境：IROS 2025 RoboSense Challenge Cross-Modal Drone Navigation Track (Track 4)。\n- 评估指标：使用Recall@K (R@K)作为主要评估指标，具体包括Recall@1 (R@1), Recall@5 (R@5), 和 Recall@10 (R@10)，用于衡量在检索结果的前K位中出现至少一个正确图像的查询百分比。",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>Tsinghua University, Xiaomi EV, HKUSTGZ, Georgia Institute of Technology, National University of Singapore, The Chinese University of Hong Kong, Renmin University of China</p>\n<h1>论文概述</h1>\n\n<p>本文针对跨模态无人机导航任务，提出了一种名为图像描述引导检索系统的两阶段检索优化方法。该方法旨在解决现有基线模型在自然语言查询与视觉内容之间进行细粒度语义匹配时遇到的挑战。首先，利用基线模型进行粗粒度检索，筛选出Top 20的候选图像。然后，使用视觉语言模型为这些候选图像生成详细的描述，并基于原始文本查询与生成描述之间的语义相似度进行重排序，从而构建连接视觉内容和自然语言描述的语义桥梁。该方法在RoboSense 2025挑战赛的Track 4中取得了所有关键指标上5%的稳定提升，并获得TOP-3的成绩。</p>\n<h1>论文核心贡献点</h1>\n\n<ul><li>提出了一个图像描述引导检索系统，这是一个两阶段的由粗到细的检索框架，利用视觉语言模型来弥合自然语言查询与航空图像之间的语义鸿沟。</li><li>引入了一种新颖的图像描述引导重排序机制，将跨模态匹配转化为文本到文本的相似度计算，从而在复杂的航空场景中实现更细粒度的语义对齐。</li><li>与基线方法相比，该方法在所有关键指标上取得了5%的稳定提升，并在IROS 2025 RoboSense挑战赛的跨模态无人机导航赛道中获得了TOP-3的成绩。</li></ul>\n<h1>论文方法描述</h1>\n\n<p>该方法是一个两阶段的由粗到细的检索流程。第一阶段，利用GeoText-1652基线模型（包含Swin Transformer图像编码器、BERT文本编码器和跨模态融合模块）对图像库进行初步检索，为每个文本查询筛选出最相关的20张候选图像。第二阶段，核心创新点在于重排序：首先，使用GPT-4o等视觉语言模型为20张候选图像生成包含丰富语义和空间信息的详细描述；然后，使用BERT等句子嵌入模型计算原始查询与生成描述之间的文本到文本语义相似度；最后，将基线模型的视觉相似度分数与文本语义相似度分数以加权方式（alpha=0.3）融合，根据融合后的最终得分对候选图像进行重排序，得到最终结果。</p>\n<h1>论文使用数据集和训练资源</h1>\n\n<ul><li>数据集：RoboSense 2025挑战赛提供的GeoText-1652数据集。训练集包含50,218张图像，测试集包含54,227张图像，每个图像都配有全局描述和包含空间关系（如“左”、“上右”）的区域级描述。</li><li>训练资源：基线模型在8块NVIDIA A800 GPU上训练了5个epoch，耗时约30 GPU小时。优化器使用AdamW，学习率为3e-5。图像描述生成阶段使用GPT-4o，为所有候选图像离线生成描述耗时约12小时。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<ul><li>评估环境：IROS 2025 RoboSense Challenge Cross-Modal Drone Navigation Track (Track 4)。</li><li>评估指标：使用Recall@K (R@K)作为主要评估指标，具体包括Recall@1 (R@1), Recall@5 (R@5), 和 Recall@10 (R@10)，用于衡量在检索结果的前K位中出现至少一个正确图像的查询百分比。</li></ul>"
  },
  {
    "date": "2025-10-02",
    "title": "Contrastive Representation Regularization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.01711",
    "summary_markdown": "论文研究单位\nKAIST, UC Berkeley, RLWRLD\n\n论文概述\n论文提出了一种名为机器人状态感知对比损失（RS-CL）的正则化方法，用于提升视觉-语言-动作（VLA）模型的机器人操作性能。该方法通过对比学习将视觉-语言模型（VLM）的表示与机器人本体的感觉状态对齐，从而增强模型对控制相关信号的学习能力，同时保持轻量级和与现有训练流程的兼容性。\n\n论文核心贡献点\n1. 提出了一种新的VLA模型目标函数RS-CL，通过对比损失明确对齐VLM表示与本体感觉状态。\n2. 设计了表示层面的增强方法“视角截断”，通过随机遮蔽观察视图的特征来构建对比样本，减少计算开销。\n3. 在多个模拟基准和真实机器人实验中验证了RS-CL的有效性，显著提升了操作性能，特别是在需要精确定位的抓取和放置任务上。\n\n论文方法描述\n1. VLA模型框架：使用预训练的VLM处理多视角图像和指令，通过适配器模块生成隐藏表示，动作解码器基于该表示和机器人状态预测动作序列。\n2. RS-CL损失：引入可学习的汇总token生成紧凑表示，利用本体感觉状态的距离作为软权重构建加权InfoNCE损失，拉近相似状态的表示。\n3. 视角截断增强：在表示层面随机遮蔽一个视角的特征，生成多样化的对比样本，避免额外的VLM前向计算。\n4. 训练目标：结合流匹配损失和RS-CL损失，联合优化适配器、投影头和动作解码器。\n\n论文使用数据集和训练资源\n数据集：\nRoboCasa-Kitchen（模拟厨房环境，24个任务）\nLIBERO（4个任务套件：spatial, object, goal, long）\n真实世界数据集：60个专家演示，涵盖4个抓取放置任务和1个合盖任务\n训练资源：\n使用GR00T N1.5作为基础VLA框架，投影头为2层MLP。\n训练在模拟环境进行，真实世界实验使用Franka Research 3机械臂和双摄像头设置。\n\n论文使用的评估环境和评估指标\n评估环境：\n模拟环境：RoboCasa-Kitchen和LIBERO基准\n真实环境：Franka Research 3机械臂，配备双摄像头（外部和腕部视角）\n评估指标：\n成功率（Success Rate）：任务完成的百分比\nRoboCasa-Kitchen中细分了抓取放置任务和其他任务的平均成功率\n真实世界任务中区分部分成功（如成功抓取）和完全成功（如精确放置）",
    "summary_html": "<p>论文研究单位</p>\n<p>KAIST, UC Berkeley, RLWRLD</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为机器人状态感知对比损失（RS-CL）的正则化方法，用于提升视觉-语言-动作（VLA）模型的机器人操作性能。该方法通过对比学习将视觉-语言模型（VLM）的表示与机器人本体的感觉状态对齐，从而增强模型对控制相关信号的学习能力，同时保持轻量级和与现有训练流程的兼容性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种新的VLA模型目标函数RS-CL，通过对比损失明确对齐VLM表示与本体感觉状态。</li><li>设计了表示层面的增强方法“视角截断”，通过随机遮蔽观察视图的特征来构建对比样本，减少计算开销。</li><li>在多个模拟基准和真实机器人实验中验证了RS-CL的有效性，显著提升了操作性能，特别是在需要精确定位的抓取和放置任务上。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>VLA模型框架：使用预训练的VLM处理多视角图像和指令，通过适配器模块生成隐藏表示，动作解码器基于该表示和机器人状态预测动作序列。</li><li>RS-CL损失：引入可学习的汇总token生成紧凑表示，利用本体感觉状态的距离作为软权重构建加权InfoNCE损失，拉近相似状态的表示。</li><li>视角截断增强：在表示层面随机遮蔽一个视角的特征，生成多样化的对比样本，避免额外的VLM前向计算。</li><li>训练目标：结合流匹配损失和RS-CL损失，联合优化适配器、投影头和动作解码器。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<p>RoboCasa-Kitchen（模拟厨房环境，24个任务）</p>\n<p>LIBERO（4个任务套件：spatial, object, goal, long）</p>\n<p>真实世界数据集：60个专家演示，涵盖4个抓取放置任务和1个合盖任务</p>\n<p>训练资源：</p>\n<p>使用GR00T N1.5作为基础VLA框架，投影头为2层MLP。</p>\n<p>训练在模拟环境进行，真实世界实验使用Franka Research 3机械臂和双摄像头设置。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<p>模拟环境：RoboCasa-Kitchen和LIBERO基准</p>\n<p>真实环境：Franka Research 3机械臂，配备双摄像头（外部和腕部视角）</p>\n<p>评估指标：</p>\n<p>成功率（Success Rate）：任务完成的百分比</p>\n<p>RoboCasa-Kitchen中细分了抓取放置任务和其他任务的平均成功率</p>\n<p>真实世界任务中区分部分成功（如成功抓取）和完全成功（如精确放置）</p>"
  },
  {
    "date": "2025-10-02",
    "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.01642",
    "summary_markdown": "### 论文研究单位\n南洋理工大学（Nanyang Technological University）、A*STAR前沿人工智能研究中心（Centre for Frontier AI Research, A*STAR）、艾伦人工智能研究所（Allen Institute for AI）、华盛顿大学（University of Washington）。\n### 论文概述\n论文提出FailSafe框架，用于视觉-语言-动作（VLA）模型的故障推理和自动恢复。VLA模型在机器人操作任务中表现良好，但现有数据集缺乏故障和恢复数据，导致模型无法应对执行中的失败。FailSafe通过在仿真中自动生成故障场景和可执行恢复动作，构建大规模数据集，并将LLaVa-OneVision-7B微调为FailSafe-VLM。实验表明，FailSafe-VLM能检测故障和生成恢复动作，并显著提升多个VLA模型在ManiSkill任务中的性能，同时具备跨视角、对象和机器人本体的泛化能力。\n### 论文核心贡献点\n- 首次提出FailSafe框架，自动生成故障推理和可执行恢复动作，支持任意仿真任务。\n- 构建FailSafe数据集，使VLM和VLA模型具备故障推理能力，并提升性能，泛化到不同视角、空间配置、对象和机器人。\n- 开源FailSafe代码，促进社区开发更稳健的具身智能系统。\n### 论文方法描述\nFailSafe流水线包括四个阶段：\n1. **故障生成**：定义三种故障模式（translation、rotation、no-ops），通过YAML配置在仿真中随机注入扰动，将任务轨迹扰动为失败场景。\n2. **动作收集**：从失败和正确轨迹中映射偏差姿态Pd到正确姿态Pc，生成多个候选恢复动作ΔA（7-DoF差值），避免碰撞。\n3. **系统验证**：回放轨迹验证ΔA能否使失败恢复为成功，仅通过验证的ΔA被纳入数据集。\n4. **指令微调**：使用FailSafe数据集微调LLaVa-OneVision-7B，得到FailSafe-VLM。训练设置：32个H100 GPU，DeepSpeed ZeRO 3，学习率1e-5（视觉塔2e-6），余弦退火warmup 3%，bfloat16/TF32。\n### 论文使用数据集和训练资源\n- **数据集**：FailSafe数据集（131k失败-动作对 + 56k真值轨迹），来自ManiSkill仿真中三个任务（pick cube、push cube、stack cube），含多视角图像（front、side、hand）。\n- **训练资源**：32个H100 GPU（DeepSpeed ZeRO 3），初始化自LLaVa-OV-7B单图像检查点，语言主干Qwen2-7B-Instruct，视觉塔SigLIP，两层GELU MLP投影器。\n### 论文使用的评估环境和评估指标\n- **评估环境**：ManiSkill仿真平台，使用Franka Emika Panda机器人臂；测试种子生成未见空间配置；评估不同对象（sphere、charger）和机器人（xArm 6）的泛化能力。\n- **评估指标**：\n - **VLM比较**：二元成功率（区分失败/成功）、准确性（识别故障类型）、余弦相似度（预测ΔA与真值ΔA相似度）。\n - **VLA模型**：成功率提升（有无FailSafe-VLM的对比），在三个ManiSkill任务（pick cube、push cube、stack cube）上报告平均改进。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南洋理工大学（Nanyang Technological University）、A*STAR前沿人工智能研究中心（Centre for Frontier AI Research, A*STAR）、艾伦人工智能研究所（Allen Institute for AI）、华盛顿大学（University of Washington）。</p>\n<h3>论文概述</h3>\n<p>论文提出FailSafe框架，用于视觉-语言-动作（VLA）模型的故障推理和自动恢复。VLA模型在机器人操作任务中表现良好，但现有数据集缺乏故障和恢复数据，导致模型无法应对执行中的失败。FailSafe通过在仿真中自动生成故障场景和可执行恢复动作，构建大规模数据集，并将LLaVa-OneVision-7B微调为FailSafe-VLM。实验表明，FailSafe-VLM能检测故障和生成恢复动作，并显著提升多个VLA模型在ManiSkill任务中的性能，同时具备跨视角、对象和机器人本体的泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首次提出FailSafe框架，自动生成故障推理和可执行恢复动作，支持任意仿真任务。</li><li>构建FailSafe数据集，使VLM和VLA模型具备故障推理能力，并提升性能，泛化到不同视角、空间配置、对象和机器人。</li><li>开源FailSafe代码，促进社区开发更稳健的具身智能系统。</li></ul>\n<h3>论文方法描述</h3>\n<p>FailSafe流水线包括四个阶段：</p>\n<ol><li><strong>故障生成</strong>：定义三种故障模式（translation、rotation、no-ops），通过YAML配置在仿真中随机注入扰动，将任务轨迹扰动为失败场景。</li><li><strong>动作收集</strong>：从失败和正确轨迹中映射偏差姿态Pd到正确姿态Pc，生成多个候选恢复动作ΔA（7-DoF差值），避免碰撞。</li><li><strong>系统验证</strong>：回放轨迹验证ΔA能否使失败恢复为成功，仅通过验证的ΔA被纳入数据集。</li><li><strong>指令微调</strong>：使用FailSafe数据集微调LLaVa-OneVision-7B，得到FailSafe-VLM。训练设置：32个H100 GPU，DeepSpeed ZeRO 3，学习率1e-5（视觉塔2e-6），余弦退火warmup 3%，bfloat16/TF32。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：FailSafe数据集（131k失败-动作对 + 56k真值轨迹），来自ManiSkill仿真中三个任务（pick cube、push cube、stack cube），含多视角图像（front、side、hand）。</li><li><strong>训练资源</strong>：32个H100 GPU（DeepSpeed ZeRO 3），初始化自LLaVa-OV-7B单图像检查点，语言主干Qwen2-7B-Instruct，视觉塔SigLIP，两层GELU MLP投影器。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：ManiSkill仿真平台，使用Franka Emika Panda机器人臂；测试种子生成未见空间配置；评估不同对象（sphere、charger）和机器人（xArm 6）的泛化能力。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>VLM比较</strong>：二元成功率（区分失败/成功）、准确性（识别故障类型）、余弦相似度（预测ΔA与真值ΔA相似度）。</p>\n<p> - <strong>VLA模型</strong>：成功率提升（有无FailSafe-VLM的对比），在三个ManiSkill任务（pick cube、push cube、stack cube）上报告平均改进。</p>"
  },
  {
    "date": "2025-10-02",
    "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.01623",
    "summary_markdown": "### 论文研究单位\nGigaAI, CASIA, Tsinghua University\n### 论文概述\n该论文提出了VLA-R1，一个旨在增强视觉-语言-动作（VLA）模型推理能力的模型。现有VLA模型常缺乏显式的分步推理能力，其后训练流程也很少强化推理质量。为解决这些问题，VLA-R1集成了基于可验证奖励的强化学习（RLVR）和组相对策略优化（GRPO），通过精心设计的奖励函数来系统性地优化模型的推理和执行鲁棒性。此外，论文还构建了VLA-CoT-13K数据集，为模型提供与可供性和轨迹标签对齐的思维链监督。在多个基准测试、仿真环境和真实机器人平台上的评估表明，VLA-R1具有优越的性能和泛化能力。\n### 论文核心贡献点\n1. 提出了VLA-R1模型，引入了基于RLVR的优化方案，通过GRPO和精心设计的奖励（区域对齐、轨迹一致性、输出格式），系统性地增强了模型的推理和执行鲁棒性。\n2. 开发了VLA-CoT数据引擎，生成了与可供性和轨迹标签对齐的高质量数据集VLA-CoT-13K，弥补了现有VLA模型缺乏逐步推理监督的不足。\n3. 在域内、域外、仿真和真实机器人平台上对VLA-R1进行了全面评估，验证了其有效性和跨域泛化能力。\n### 论文方法描述\nVLA-R1的训练方法包含两个主要阶段：\n1. **监督微调（SFT）**：首先，使用Qwen2.5-VL-72B模型构建VLA-CoT-13K数据集，该数据集包含与可供性和轨迹注释对齐的逐步思维链。然后，在此数据集上对基于Qwen2.5-VL-3B的模型进行微调，同时监督模型输出的思维片段和最终动作片段。\n2. **强化学习（RL）**：在SFT之后，使用GRPO算法进行进一步优化。该过程设计了三种可验证的奖励函数：\n - **Fréchet轨迹奖励**：采用角度长度增强的Fréchet距离（ALAF）来衡量预测轨迹与真实轨迹的对齐程度，考虑了位置、角度和分段长度。\n - **GIoU可供性奖励**：使用广义交并比（GIoU）作为奖励，以评估预测边界框与真实边界框的空间对齐质量，特别是在非重叠情况下也能提供有效梯度。\n - **格式奖励**：一个二元奖励，用于强制模型的输出必须遵循指定的结构（思维片段后跟动作片段）。\n### 论文使用数据集和训练资源\n- **使用数据集**:\n - **训练数据**: ShareRobot数据集，源自Open X-Embodiment，包含102个操作场景和12种机器人形态的供能力与轨迹注释。\n - **合成数据**: VLA-CoT-13K，一个包含13K条思维链注释的高质量数据集，通过VLA-CoT数据引擎生成，与可供性和轨迹标签对齐。\n - **域外评估数据**:\n - 可供性任务：UMD Part Affordance数据集的子集。\n - 轨迹任务：VAIT数据集（LLARVA的验证集）。\n- **训练资源**: 论文中未明确说明所使用的具体计算资源（如GPU类型、数量、训练时长等）。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - **基准测试**: 在ShareRobot（域内）以及UMD和VAIT（域外）数据集上进行离线评估。\n - **仿真环境**: 使用RoboTwin模拟器，在带有随机杂物的桌面场景中进行测试，评估了Piper和UR5两种机器人平台。\n - **真实世界**: 在一个真实的桌面平台上，设计了四个经典场景（碗抓取、水果抓取、厨房场景、混合场景）进行评估。\n- **评估指标**:\n - **可供性任务**: 使用交并比（IoU）作为主要度量指标。\n - **轨迹任务**: 使用离散Fréchet距离（DFD）、豪斯多夫距离（HD）和均方根误差（RMSE）来评估轨迹相似性。\n - **仿真与真实世界任务**: 使用成功率（SR）作为任务级指标。对于可供性任务，成功定义为正确定位并抓取物体；对于轨迹任务，成功定义为将物体成功运送到指定目标区域。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>GigaAI, CASIA, Tsinghua University</p>\n<h3>论文概述</h3>\n<p>该论文提出了VLA-R1，一个旨在增强视觉-语言-动作（VLA）模型推理能力的模型。现有VLA模型常缺乏显式的分步推理能力，其后训练流程也很少强化推理质量。为解决这些问题，VLA-R1集成了基于可验证奖励的强化学习（RLVR）和组相对策略优化（GRPO），通过精心设计的奖励函数来系统性地优化模型的推理和执行鲁棒性。此外，论文还构建了VLA-CoT-13K数据集，为模型提供与可供性和轨迹标签对齐的思维链监督。在多个基准测试、仿真环境和真实机器人平台上的评估表明，VLA-R1具有优越的性能和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了VLA-R1模型，引入了基于RLVR的优化方案，通过GRPO和精心设计的奖励（区域对齐、轨迹一致性、输出格式），系统性地增强了模型的推理和执行鲁棒性。</li><li>开发了VLA-CoT数据引擎，生成了与可供性和轨迹标签对齐的高质量数据集VLA-CoT-13K，弥补了现有VLA模型缺乏逐步推理监督的不足。</li><li>在域内、域外、仿真和真实机器人平台上对VLA-R1进行了全面评估，验证了其有效性和跨域泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<p>VLA-R1的训练方法包含两个主要阶段：</p>\n<ol><li><strong>监督微调（SFT）</strong>：首先，使用Qwen2.5-VL-72B模型构建VLA-CoT-13K数据集，该数据集包含与可供性和轨迹注释对齐的逐步思维链。然后，在此数据集上对基于Qwen2.5-VL-3B的模型进行微调，同时监督模型输出的思维片段和最终动作片段。</li><li><strong>强化学习（RL）</strong>：在SFT之后，使用GRPO算法进行进一步优化。该过程设计了三种可验证的奖励函数：</li></ol>\n<p> - <strong>Fréchet轨迹奖励</strong>：采用角度长度增强的Fréchet距离（ALAF）来衡量预测轨迹与真实轨迹的对齐程度，考虑了位置、角度和分段长度。</p>\n<p> - <strong>GIoU可供性奖励</strong>：使用广义交并比（GIoU）作为奖励，以评估预测边界框与真实边界框的空间对齐质量，特别是在非重叠情况下也能提供有效梯度。</p>\n<p> - <strong>格式奖励</strong>：一个二元奖励，用于强制模型的输出必须遵循指定的结构（思维片段后跟动作片段）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>使用数据集</strong>:</li></ul>\n<p> - <strong>训练数据</strong>: ShareRobot数据集，源自Open X-Embodiment，包含102个操作场景和12种机器人形态的供能力与轨迹注释。</p>\n<p> - <strong>合成数据</strong>: VLA-CoT-13K，一个包含13K条思维链注释的高质量数据集，通过VLA-CoT数据引擎生成，与可供性和轨迹标签对齐。</p>\n<p> - <strong>域外评估数据</strong>:</p>\n<p> - 可供性任务：UMD Part Affordance数据集的子集。</p>\n<p> - 轨迹任务：VAIT数据集（LLARVA的验证集）。</p>\n<ul><li><strong>训练资源</strong>: 论文中未明确说明所使用的具体计算资源（如GPU类型、数量、训练时长等）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - <strong>基准测试</strong>: 在ShareRobot（域内）以及UMD和VAIT（域外）数据集上进行离线评估。</p>\n<p> - <strong>仿真环境</strong>: 使用RoboTwin模拟器，在带有随机杂物的桌面场景中进行测试，评估了Piper和UR5两种机器人平台。</p>\n<p> - <strong>真实世界</strong>: 在一个真实的桌面平台上，设计了四个经典场景（碗抓取、水果抓取、厨房场景、混合场景）进行评估。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>可供性任务</strong>: 使用交并比（IoU）作为主要度量指标。</p>\n<p> - <strong>轨迹任务</strong>: 使用离散Fréchet距离（DFD）、豪斯多夫距离（HD）和均方根误差（RMSE）来评估轨迹相似性。</p>\n<p> - <strong>仿真与真实世界任务</strong>: 使用成功率（SR）作为任务级指标。对于可供性任务，成功定义为正确定位并抓取物体；对于轨迹任务，成功定义为将物体成功运送到指定目标区域。</p>"
  },
  {
    "date": "2025-10-01",
    "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.01389",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-10-01",
    "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition",
    "link": "http://arxiv.org/abs/2510.01068",
    "summary_markdown": "### 论文研究单位\nThe University of Hong Kong, Beijing Innovation Center of Humanoid Robotics, Shanghai AI Lab, Shanghai Jiaotong University, The Hong Kong University of Science and Technology\n### 论文概述\n本文提出了一种替代范式，通过组合现有的预训练模型来创建更强的机器人策略，无需额外的模型训练。具体而言，介绍了一种称为通用策略组合（GPC）的训练免费方法，在推理时通过凸组合和测试时搜索组合多个预训练策略的分布分数。研究表明，组合后的策略性能可以超过任何单个父策略的性能。\n### 论文核心贡献点\n1. 建立了机器人策略组合的理论基础，证明了分布分数的凸组合可以产生改进的功能目标，并且这种优势会传播到系统层面。\n2. 提出了通用策略组合（GPC），一个灵活的、训练免费的框架，可以将不同模态和架构的预训练策略组合成一个更具表现力的策略。\n3. 在模拟和现实世界进行了广泛评估，展示了GPC的持续性能提升，并分析了关键设计选择。\n### 论文方法描述\nGPC方法通过凸组合多个预训练策略的分布分数来工作，使用测试时搜索来找到最优的组合权重。该方法可以处理不同类型的策略，包括视觉-动作（VA）和视觉-语言-动作（VLA）模型，以及基于扩散或流匹配的策略，无论其输入视觉模态如何。算法流程包括：初始化噪声轨迹，对不同的权重值进行测试时搜索，在每个去噪步骤中计算组合分数，最终选择最优权重。\n### 论文使用数据集和训练资源\n- 数据集：Robomimic（包括Can、Lift、Square任务）、PushT、RoboTwin（包含多个双臂操作任务）\n- 真实世界实验：Place Bottles、Hang Mug、Close Table、Punch Holes\n- 由于GPC是训练免费的，直接使用预训练策略，不需要额外训练资源\n### 论文使用的评估环境和评估指标\n- 评估环境：模拟环境（Robomimic、PushT、RoboTwin）和真实世界机器人环境\n- 评估指标：成功率（Success Rate, SR），平均成功率（Average SR）\n- 每个设置评估200次（RoboTwin为100次）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>The University of Hong Kong, Beijing Innovation Center of Humanoid Robotics, Shanghai AI Lab, Shanghai Jiaotong University, The Hong Kong University of Science and Technology</p>\n<h3>论文概述</h3>\n<p>本文提出了一种替代范式，通过组合现有的预训练模型来创建更强的机器人策略，无需额外的模型训练。具体而言，介绍了一种称为通用策略组合（GPC）的训练免费方法，在推理时通过凸组合和测试时搜索组合多个预训练策略的分布分数。研究表明，组合后的策略性能可以超过任何单个父策略的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>建立了机器人策略组合的理论基础，证明了分布分数的凸组合可以产生改进的功能目标，并且这种优势会传播到系统层面。</li><li>提出了通用策略组合（GPC），一个灵活的、训练免费的框架，可以将不同模态和架构的预训练策略组合成一个更具表现力的策略。</li><li>在模拟和现实世界进行了广泛评估，展示了GPC的持续性能提升，并分析了关键设计选择。</li></ol>\n<h3>论文方法描述</h3>\n<p>GPC方法通过凸组合多个预训练策略的分布分数来工作，使用测试时搜索来找到最优的组合权重。该方法可以处理不同类型的策略，包括视觉-动作（VA）和视觉-语言-动作（VLA）模型，以及基于扩散或流匹配的策略，无论其输入视觉模态如何。算法流程包括：初始化噪声轨迹，对不同的权重值进行测试时搜索，在每个去噪步骤中计算组合分数，最终选择最优权重。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：Robomimic（包括Can、Lift、Square任务）、PushT、RoboTwin（包含多个双臂操作任务）</li><li>真实世界实验：Place Bottles、Hang Mug、Close Table、Punch Holes</li><li>由于GPC是训练免费的，直接使用预训练策略，不需要额外训练资源</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：模拟环境（Robomimic、PushT、RoboTwin）和真实世界机器人环境</li><li>评估指标：成功率（Success Rate, SR），平均成功率（Average SR）</li><li>每个设置评估200次（RoboTwin为100次）</li></ul>"
  },
  {
    "date": "2025-10-01",
    "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy",
    "link": "http://arxiv.org/abs/2510.00695",
    "summary_markdown": "### 论文研究单位\nKAIST、UC Berkeley、RLWRLD\n### 论文概述\n现有视觉-语言-行动（Vision-Language-Action, VLA）模型依赖当前观察进行行动预测，忽略历史上下文，导致在需要长期推理的任务中性能受限。本论文提出HAMLET框架，通过引入记忆机制使VLA能够利用历史信息。具体方法包括：\n- 使用可学习的\"moment tokens\"压缩每个时间步的感知信息，并通过时间对比学习初始化以突出时序特征。\n- 集成轻量级\"memory module\"聚合历史moment tokens，生成行动预测的条件特征。\n实验表明，HAMLET在真实世界长时序任务和仿真基准上显著提升性能，且无需修改原始VLA架构。\n### 论文核心贡献点\n- 动机分析：现有VLA假设当前观察独立，限制了在历史依赖任务（如遮挡或长期操作）中的表现。\n- 方法创新：提出HAMLET框架，通过moment tokens和memory module增强历史感知。\n- 实验验证：跨多VLA backbone（GR00T N1.5、CogACT）验证有效性，在真实世界任务平均提升47.2%，仿真基准（RoboCasa、LIBERO）也获益。\n- 通用性：backbone-agnostic设计，plug-and-play适配，无需额外预训练。\n- 效率优化：避免多帧输入带来的计算开销，内存占用仅为多帧基线的约2倍。\n### 论文方法描述\nHAMLET包含两个核心组件：\n- **Context compression via moment tokens**：在每个时间步t，附加可学习向量$\\mathbf{m}_t$到VLM输入，通过冻结VLM进行时间对比学习初始化（使用同一时间步的正样本和不同时间步的负样本），使moment tokens捕捉时序判别特征，过滤静态背景。\n- **Memory consolidation via memory module**：使用2层Transformer处理堆叠的近T个moment tokens（如$\\mathbf{M}' = [\\mathbf{m}'_{t-k(T-1)}, ..., \\mathbf{m}'_t]$），通过因果自注意力生成历史增强特征$\\tilde{\\mathbf{m}}'$，与VLM表示$\\mathbf{h}_t$拼接后输入行动专家，预测k步行动。\n训练流程：冻结VLM进行moment tokens初始化（30k步），随后联合训练行动预测（60k步）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 真实世界任务：3个桌面任务（Pick-and-Place Twice、Cover-and-Stack、Swap Cubes），每任务50个演示，平均约268帧/轨迹。\n - 仿真基准：\n - RoboCasa Kitchen：24个任务，训练演示30/100/300每任务。\n - LIBERO：40个任务，分4套件（Spatial、Object、Goal、Long）。\n - SimplerEnv-Bridge：基于WidowX机器人，使用BridgeV2数据集。\n- **训练资源**：使用NVIDIA A100 GPU进行训练和测量（延迟和内存）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 真实世界：Franka Research 3机器人+Robotiq 2F-85 gripper，双视角摄像头。\n - 仿真：RoboCasa Kitchen（Franka机器人）、LIBERO（Franka）、SimplerEnv-Bridge（WidowX）。\n- **评估指标**：\n - 成功率和部分成功率（如Pick-and-Place Once、Cover Cube），跨任务和基准报告。\n - 效率指标：延迟（毫秒）和峰值内存使用（MB），在RoboCasa数据集上测量。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>KAIST、UC Berkeley、RLWRLD</p>\n<h3>论文概述</h3>\n<p>现有视觉-语言-行动（Vision-Language-Action, VLA）模型依赖当前观察进行行动预测，忽略历史上下文，导致在需要长期推理的任务中性能受限。本论文提出HAMLET框架，通过引入记忆机制使VLA能够利用历史信息。具体方法包括：</p>\n<ul><li>使用可学习的\"moment tokens\"压缩每个时间步的感知信息，并通过时间对比学习初始化以突出时序特征。</li><li>集成轻量级\"memory module\"聚合历史moment tokens，生成行动预测的条件特征。</li></ul>\n<p>实验表明，HAMLET在真实世界长时序任务和仿真基准上显著提升性能，且无需修改原始VLA架构。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>动机分析：现有VLA假设当前观察独立，限制了在历史依赖任务（如遮挡或长期操作）中的表现。</li><li>方法创新：提出HAMLET框架，通过moment tokens和memory module增强历史感知。</li><li>实验验证：跨多VLA backbone（GR00T N1.5、CogACT）验证有效性，在真实世界任务平均提升47.2%，仿真基准（RoboCasa、LIBERO）也获益。</li><li>通用性：backbone-agnostic设计，plug-and-play适配，无需额外预训练。</li><li>效率优化：避免多帧输入带来的计算开销，内存占用仅为多帧基线的约2倍。</li></ul>\n<h3>论文方法描述</h3>\n<p>HAMLET包含两个核心组件：</p>\n<ul><li><strong>Context compression via moment tokens</strong>：在每个时间步t，附加可学习向量$\\mathbf{m}_t$到VLM输入，通过冻结VLM进行时间对比学习初始化（使用同一时间步的正样本和不同时间步的负样本），使moment tokens捕捉时序判别特征，过滤静态背景。</li><li><strong>Memory consolidation via memory module</strong>：使用2层Transformer处理堆叠的近T个moment tokens（如$\\mathbf{M}' = [\\mathbf{m}'_{t-k(T-1)}, ..., \\mathbf{m}'_t]$），通过因果自注意力生成历史增强特征$\\tilde{\\mathbf{m}}'$，与VLM表示$\\mathbf{h}_t$拼接后输入行动专家，预测k步行动。</li></ul>\n<p>训练流程：冻结VLM进行moment tokens初始化（30k步），随后联合训练行动预测（60k步）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 真实世界任务：3个桌面任务（Pick-and-Place Twice、Cover-and-Stack、Swap Cubes），每任务50个演示，平均约268帧/轨迹。</p>\n<p> - 仿真基准：</p>\n<p> - RoboCasa Kitchen：24个任务，训练演示30/100/300每任务。</p>\n<p> - LIBERO：40个任务，分4套件（Spatial、Object、Goal、Long）。</p>\n<p> - SimplerEnv-Bridge：基于WidowX机器人，使用BridgeV2数据集。</p>\n<ul><li><strong>训练资源</strong>：使用NVIDIA A100 GPU进行训练和测量（延迟和内存）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 真实世界：Franka Research 3机器人+Robotiq 2F-85 gripper，双视角摄像头。</p>\n<p> - 仿真：RoboCasa Kitchen（Franka机器人）、LIBERO（Franka）、SimplerEnv-Bridge（WidowX）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率和部分成功率（如Pick-and-Place Once、Cover Cube），跨任务和基准报告。</p>\n<p> - 效率指标：延迟（毫秒）和峰值内存使用（MB），在RoboCasa数据集上测量。</p>"
  },
  {
    "date": "2025-10-01",
    "title": "Hybrid Training for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2510.00600",
    "summary_markdown": "### 论文研究单位\nQualcomm AI Research, University of Tübingen, Max Planck Institute\n### 论文概述\n本文提出了一种名为混合训练（Hybrid Training, HyT）的框架，用于训练视觉-语言-动作模型（VLAs）。该方法使模型能够从思维链（CoT）推理中学习，同时保持与标准VLA相同的快速推理速度。HyT通过引入模态变量，支持模型在推理时直接预测动作、生成思维或遵循指令等多种模式，从而在模拟基准测试和真实世界实验中提升了性能。\n### 论文核心贡献点\n- 提出混合训练（HyT）框架，使VLAs能够从CoT中学习，同时在推理时跳过CoT生成，实现快速动作预测。\n- 通过模态变量控制模型输出模式，支持直接动作预测（act）、思维生成（think）和指令跟随（follow）三种模式。\n- 验证了CoT技术的性能提升主要源于训练时对思维的内部化，而非推理时的显式生成。\n### 论文方法描述\n1. **混合训练公式**：定义动作条件分布为思维和模态变量的边缘化分布，公式为：\n \\[\n p(a_t\\|x_t,l) = \\sum_i \\sum_j p_{\\theta}(a_t, \\tau^i, m^j\\|x_t,l)p(m^j)\n \\]\n 其中，\\(\\tau\\) 为思维，\\(m\\) 为模态变量。\n2. **训练实现**：使用蒙特卡洛估计，以不同概率采样条件输入和输出（如act、think、follow模式），训练模型预测多样化条件分布。\n3. **推理模式**：通过设置模态变量（如`<act>`），模型可直接输出动作，无需生成思维；或使用`<think>`生成思维，增强可解释性。\n### 论文使用数据集和训练资源\n- **数据集**：\n - ClevrSkills：包含3000个演示轨迹，涵盖空间放置、堆叠等9个任务。\n - LIBERO：4个任务套件（Spatial、Object、Goal、Long），使用LLM生成思维链标注。\n - 真实世界数据：320条轨迹，来自UFactory xArm 6机械臂，涵盖分布内和分布外任务。\n- **训练资源**：\n - ClevrSkills：使用4块A100 GPU，批量大小32，学习率2e-5，从PaliGemma-2（3B参数）微调。\n - LIBERO：结合OFT策略，从Prismatic VLM微调。\n - 真实世界实验：使用OpenVLA（7B参数），LoRA微调（rank 32），学习率5e-4。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：ClevrSkills（基于ManiSkill2）、LIBERO。\n - 真实环境：UFactory xArm 6机械臂，配备RealSense D435相机。\n- **评估指标**：\n - 成功率（Success Rate）：任务完成率，每个模型在100个评估回合中测试。\n - 推理时间：动作生成频率，HyT与标准VLA相当（约3Hz），显著快于ECoT（慢3倍）和分层VLA（慢4倍）。\n - 平均性能：多任务场景下的综合成功率，如LIBERO的4套件平均得分。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Qualcomm AI Research, University of Tübingen, Max Planck Institute</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为混合训练（Hybrid Training, HyT）的框架，用于训练视觉-语言-动作模型（VLAs）。该方法使模型能够从思维链（CoT）推理中学习，同时保持与标准VLA相同的快速推理速度。HyT通过引入模态变量，支持模型在推理时直接预测动作、生成思维或遵循指令等多种模式，从而在模拟基准测试和真实世界实验中提升了性能。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出混合训练（HyT）框架，使VLAs能够从CoT中学习，同时在推理时跳过CoT生成，实现快速动作预测。</li><li>通过模态变量控制模型输出模式，支持直接动作预测（act）、思维生成（think）和指令跟随（follow）三种模式。</li><li>验证了CoT技术的性能提升主要源于训练时对思维的内部化，而非推理时的显式生成。</li></ul>\n<h3>论文方法描述</h3>\n<p>1. <strong>混合训练公式</strong>：定义动作条件分布为思维和模态变量的边缘化分布，公式为：</p>\n<p> \\[</p>\n<p> p(a_t\\|x_t,l) = \\sum_i \\sum_j p_{\\theta}(a_t, \\tau^i, m^j\\|x_t,l)p(m^j)</p>\n<p> \\]</p>\n<p> 其中，\\(\\tau\\) 为思维，\\(m\\) 为模态变量。</p>\n<ol><li><strong>训练实现</strong>：使用蒙特卡洛估计，以不同概率采样条件输入和输出（如act、think、follow模式），训练模型预测多样化条件分布。</li><li><strong>推理模式</strong>：通过设置模态变量（如<code><act></code>），模型可直接输出动作，无需生成思维；或使用<code><think></code>生成思维，增强可解释性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - ClevrSkills：包含3000个演示轨迹，涵盖空间放置、堆叠等9个任务。</p>\n<p> - LIBERO：4个任务套件（Spatial、Object、Goal、Long），使用LLM生成思维链标注。</p>\n<p> - 真实世界数据：320条轨迹，来自UFactory xArm 6机械臂，涵盖分布内和分布外任务。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - ClevrSkills：使用4块A100 GPU，批量大小32，学习率2e-5，从PaliGemma-2（3B参数）微调。</p>\n<p> - LIBERO：结合OFT策略，从Prismatic VLM微调。</p>\n<p> - 真实世界实验：使用OpenVLA（7B参数），LoRA微调（rank 32），学习率5e-4。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：ClevrSkills（基于ManiSkill2）、LIBERO。</p>\n<p> - 真实环境：UFactory xArm 6机械臂，配备RealSense D435相机。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（Success Rate）：任务完成率，每个模型在100个评估回合中测试。</p>\n<p> - 推理时间：动作生成频率，HyT与标准VLA相当（约3Hz），显著快于ECoT（慢3倍）和分层VLA（慢4倍）。</p>\n<p> - 平均性能：多任务场景下的综合成功率，如LIBERO的4套件平均得分。</p>"
  },
  {
    "date": "2025-10-01",
    "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators",
    "link": "http://arxiv.org/abs/2510.00406",
    "summary_markdown": "### 论文研究单位\n西湖大学、浙江大学、复旦大学、OpenHelix团队、北京邮电大学、郑州大学、河北工业大学等\n### 论文概述\n论文提出 VLA-RFT（Vision-Language-Action 强化微调），用学习到的世界模型作为可控模拟器为 VLA 提供验证奖励，在模拟器中对策略进行轨迹级强化优化，以更低样本复杂度提升 VLA 的泛化与鲁棒性。\n### 论文核心贡献点\n- 世界模型作为数据驱动的可交互模拟器，基于动作预测未来视觉观察，支持策略多 rollout 生成与反馈\n- 轨迹级“验证奖励”，通过将生成的视觉轨迹与目标达成参考轨迹对比获得密集且动作对齐的学习信号\n- 将流匹配扩展为 SDE 策略（SDE-Policy），引入 Sigma Net 表征随机性；结合 GRPO 进行高效稳定微调\n- 只需约 400 步强化微调即可显著优于强监督基线，并在扰动场景下展现更强鲁棒性\n### 论文方法描述\n两阶段训练：Stage I 预训练世界模型与 VLA；Stage II 在世界模型中交互 rollout、计算验证奖励并用 GRPO 优化 VLA。\n- 世界模型：基于 LLaMA 架构的轻量自回归视频预测模型（138M 参数），以最大似然训练\n- VLA 预训练：VLM 编码器 + 流匹配动作头，MSE 目标稳定输出动作片段\n- SDE-Policy：在流匹配基础上加入 Sigma Net（输出方差），将 ODE 扩展为 SDE；在 K=10 步扩散积分中计算平均对数似然并形成策略比 r\n- 验证奖励：将世界模型生成的轨迹与离线专家轨迹对齐，定义为 L1 与 LPIPS 的负加权和；同起点 rollout 的奖励做组内平均以减方差\n- 目标函数：GRPO 损失 + 流匹配 MSE 辅助项 + 熵正则，确保高效与稳定\n### 论文使用数据集和训练资源\n- 数据集与基准：LIBERO（Spatial、Object、Goal、Long 四套任务）\n- 基础策略：轻量 VLA-Adapter；先进行监督微调，再进行强化微调\n- 世界模型：138M 参数的自回归模型，基于 LLaMA 架构，在 LIBERO 上预训练\n- 训练资源：4× A800 GPU；采用 VERL 分布式强化框架与 FSDP 切分训练\n### 论文使用的评估环境和评估指标\n- 世界模型评估：像素误差（MSE）、峰值信噪比（PSNR）、结构相似性（SSIM）、感知距离（LPIPS）\n- 策略评估：成功率（SR）在标准套件与扰动套件；扰动包含物体/目标位置、机器人状态及组合扰动\n- 关键结果：约 400 步 RFT 将平均 SR 从 86.6% 提升至 91.1%，且在各类扰动下保持更高稳定性；世界模型具备高保真视觉预测能力，显著低于监督扩展 SFT 所需迭代量",
    "summary_html": "<h3>论文研究单位</h3>\n<p>西湖大学、浙江大学、复旦大学、OpenHelix团队、北京邮电大学、郑州大学、河北工业大学等</p>\n<h3>论文概述</h3>\n<p>论文提出 VLA-RFT（Vision-Language-Action 强化微调），用学习到的世界模型作为可控模拟器为 VLA 提供验证奖励，在模拟器中对策略进行轨迹级强化优化，以更低样本复杂度提升 VLA 的泛化与鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>世界模型作为数据驱动的可交互模拟器，基于动作预测未来视觉观察，支持策略多 rollout 生成与反馈</li><li>轨迹级“验证奖励”，通过将生成的视觉轨迹与目标达成参考轨迹对比获得密集且动作对齐的学习信号</li><li>将流匹配扩展为 SDE 策略（SDE-Policy），引入 Sigma Net 表征随机性；结合 GRPO 进行高效稳定微调</li><li>只需约 400 步强化微调即可显著优于强监督基线，并在扰动场景下展现更强鲁棒性</li></ul>\n<h3>论文方法描述</h3>\n<p>两阶段训练：Stage I 预训练世界模型与 VLA；Stage II 在世界模型中交互 rollout、计算验证奖励并用 GRPO 优化 VLA。</p>\n<ul><li>世界模型：基于 LLaMA 架构的轻量自回归视频预测模型（138M 参数），以最大似然训练</li><li>VLA 预训练：VLM 编码器 + 流匹配动作头，MSE 目标稳定输出动作片段</li><li>SDE-Policy：在流匹配基础上加入 Sigma Net（输出方差），将 ODE 扩展为 SDE；在 K=10 步扩散积分中计算平均对数似然并形成策略比 r</li><li>验证奖励：将世界模型生成的轨迹与离线专家轨迹对齐，定义为 L1 与 LPIPS 的负加权和；同起点 rollout 的奖励做组内平均以减方差</li><li>目标函数：GRPO 损失 + 流匹配 MSE 辅助项 + 熵正则，确保高效与稳定</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集与基准：LIBERO（Spatial、Object、Goal、Long 四套任务）</li><li>基础策略：轻量 VLA-Adapter；先进行监督微调，再进行强化微调</li><li>世界模型：138M 参数的自回归模型，基于 LLaMA 架构，在 LIBERO 上预训练</li><li>训练资源：4× A800 GPU；采用 VERL 分布式强化框架与 FSDP 切分训练</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>世界模型评估：像素误差（MSE）、峰值信噪比（PSNR）、结构相似性（SSIM）、感知距离（LPIPS）</li><li>策略评估：成功率（SR）在标准套件与扰动套件；扰动包含物体/目标位置、机器人状态及组合扰动</li><li>关键结果：约 400 步 RFT 将平均 SR 从 86.6% 提升至 91.1%，且在各类扰动下保持更高稳定性；世界模型具备高保真视觉预测能力，显著低于监督扩展 SFT 所需迭代量</li></ul>"
  },
  {
    "date": "2025-09-30",
    "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2509.26642",
    "summary_markdown": "# 论文研究单位\n- 北京大学多媒体信息处理国家重点实验室\n- 北京人形机器人创新中心\n- 香港中文大学（CUHK）\n# 论文概述\n本论文提出了一种多感觉语言-动作模型（MLA），用于机器人的多模态理解和预测。MLA通过无编码器的多模态对齐机制，直接整合2D图像、3D点云和触觉信号，并利用未来多感觉生成后训练策略增强物理动态理解。在复杂、接触丰富的真实世界任务中，MLA在成功率上分别超越之前的SOTA 2D和3D VLA方法12%和24%，同时展示了在未见配置下的强泛化能力。模型基于LLM主干（如LLaMA-2 7B），通过渐进式训练管道（预训练、监督微调、后训练）实现感知、理解和动作生成的统一。\n# 论文核心贡献点\n1. **无编码器的多模态对齐机制**：MLA将LLM自身重新用作感知模块，通过标记级对比学习对齐图像、点云和触觉标记，利用位置引导的一致性约束。\n2. **未来多感觉生成后训练策略**：模型联合预测未来图像、点云和触觉状态，增强物理动态建模和动作生成条件。\n3. **渐进式训练配方**：通过预训练（570K+轨迹）、监督微调（SFT）和后训练，实现SOTA性能和泛化，覆盖真实世界单臂和双臂任务。\n4. **真实世界和仿真验证**：在RLBench仿真器上评估并实现竞争性能，验证方法的稳健性。\n# 论文方法描述\nMLA方法包括以下组件：\n- **MLA架构**：\n - **多模态标记器**：图像标记器（14×14 patches，256 tokens）、3D点云标记器（FPS、KNN，256 tokens）、触觉标记器（MLP编码器，单token）。\n - **LLM主干**：基于LLaMA-2 7B，处理统一标记序列，支持扩散动作生成。\n - **未来预测解码器**：变换器解码器，分别预测未来图像（MSE损失）、点云（Chamfer距离）和触觉信号（MSE损失）。\n- **无编码器多模态对齐**：通过位置映射构建跨模态正对（图像-点云-触觉），应用标记级InfoNCE损失（温度τ），使用8层变换器输出优化。\n- **未来多感觉生成**：预测关键帧未来状态（由机器人关节速度触发），针对图像、点云和触觉联合训练，增强语义、几何和交互理解。\n- **训练配方**：预训练（SFT阶段前）：仅图像-动作数据，10轮训练；SFT：引入多模态输入，对比损失；后训练：添加未来生成监督。总损失为扩散损失（DDPM）、对比损失和未来生成损失之和。\n# 论文使用数据集和训练资源\n- **预训练数据集**：570K轨迹，结合28个开放源数据集（如Open-X-Embodiment、RoboMIND、DROID等），采样率详见附录Table III，总计36M帧。\n- **自收集真实世界数据**：6个任务（单臂4个：盖章、擦白板、放盘子、放鸡蛋；双臂2个：舀玉米粒、开锅盖），每任务200个高质量演示，使用Gello平台采集。\n- **仿真数据**：RLBench任务（10个），每任务100演示轨迹。\n- **训练资源**：AdamW优化器；SFT：300轮，后训练：100轮；基础模型初始化自Prismatic VLM；推理使用DDIM（4步）。\n# 论文使用的评估环境和评估指标\n- **真实世界评估**：Frank a Research 3单臂和双臂设置，配备RealSense D455相机（第三人和手腕视角）和Tactile传感器。评估接触丰富任务，每个任务15次 rollout，成功率由人工判定。指标：成功率（S.R.）和方差。\n- **仿真评估**：基于RLBench（CoppeliaSim），图像和点云数据，20次 rollout per task，成功率由内置评估模块计算。指标：平均成功率（%）和方差。\n- **泛化测试**：未见物体（如生菜代替鸡蛋）和未见背景（杂散场景），对比π_0基线，量化性能下降（如-15%）。\n- **消融研究**：无编码器对齐（标记级vs图像级）、对比损失层位置、生成模态贡献等，指标为任务成功率变化。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>北京大学多媒体信息处理国家重点实验室</li><li>北京人形机器人创新中心</li><li>香港中文大学（CUHK）</li></ul>\n<h1>论文概述</h1>\n<p>本论文提出了一种多感觉语言-动作模型（MLA），用于机器人的多模态理解和预测。MLA通过无编码器的多模态对齐机制，直接整合2D图像、3D点云和触觉信号，并利用未来多感觉生成后训练策略增强物理动态理解。在复杂、接触丰富的真实世界任务中，MLA在成功率上分别超越之前的SOTA 2D和3D VLA方法12%和24%，同时展示了在未见配置下的强泛化能力。模型基于LLM主干（如LLaMA-2 7B），通过渐进式训练管道（预训练、监督微调、后训练）实现感知、理解和动作生成的统一。</p>\n<h1>论文核心贡献点</h1>\n<ol><li><strong>无编码器的多模态对齐机制</strong>：MLA将LLM自身重新用作感知模块，通过标记级对比学习对齐图像、点云和触觉标记，利用位置引导的一致性约束。</li><li><strong>未来多感觉生成后训练策略</strong>：模型联合预测未来图像、点云和触觉状态，增强物理动态建模和动作生成条件。</li><li><strong>渐进式训练配方</strong>：通过预训练（570K+轨迹）、监督微调（SFT）和后训练，实现SOTA性能和泛化，覆盖真实世界单臂和双臂任务。</li><li><strong>真实世界和仿真验证</strong>：在RLBench仿真器上评估并实现竞争性能，验证方法的稳健性。</li></ol>\n<h1>论文方法描述</h1>\n<p>MLA方法包括以下组件：</p>\n<ul><li><strong>MLA架构</strong>：</li></ul>\n<p> - <strong>多模态标记器</strong>：图像标记器（14×14 patches，256 tokens）、3D点云标记器（FPS、KNN，256 tokens）、触觉标记器（MLP编码器，单token）。</p>\n<p> - <strong>LLM主干</strong>：基于LLaMA-2 7B，处理统一标记序列，支持扩散动作生成。</p>\n<p> - <strong>未来预测解码器</strong>：变换器解码器，分别预测未来图像（MSE损失）、点云（Chamfer距离）和触觉信号（MSE损失）。</p>\n<ul><li><strong>无编码器多模态对齐</strong>：通过位置映射构建跨模态正对（图像-点云-触觉），应用标记级InfoNCE损失（温度τ），使用8层变换器输出优化。</li><li><strong>未来多感觉生成</strong>：预测关键帧未来状态（由机器人关节速度触发），针对图像、点云和触觉联合训练，增强语义、几何和交互理解。</li><li><strong>训练配方</strong>：预训练（SFT阶段前）：仅图像-动作数据，10轮训练；SFT：引入多模态输入，对比损失；后训练：添加未来生成监督。总损失为扩散损失（DDPM）、对比损失和未来生成损失之和。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>预训练数据集</strong>：570K轨迹，结合28个开放源数据集（如Open-X-Embodiment、RoboMIND、DROID等），采样率详见附录Table III，总计36M帧。</li><li><strong>自收集真实世界数据</strong>：6个任务（单臂4个：盖章、擦白板、放盘子、放鸡蛋；双臂2个：舀玉米粒、开锅盖），每任务200个高质量演示，使用Gello平台采集。</li><li><strong>仿真数据</strong>：RLBench任务（10个），每任务100演示轨迹。</li><li><strong>训练资源</strong>：AdamW优化器；SFT：300轮，后训练：100轮；基础模型初始化自Prismatic VLM；推理使用DDIM（4步）。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>真实世界评估</strong>：Frank a Research 3单臂和双臂设置，配备RealSense D455相机（第三人和手腕视角）和Tactile传感器。评估接触丰富任务，每个任务15次 rollout，成功率由人工判定。指标：成功率（S.R.）和方差。</li><li><strong>仿真评估</strong>：基于RLBench（CoppeliaSim），图像和点云数据，20次 rollout per task，成功率由内置评估模块计算。指标：平均成功率（%）和方差。</li><li><strong>泛化测试</strong>：未见物体（如生菜代替鸡蛋）和未见背景（杂散场景），对比π_0基线，量化性能下降（如-15%）。</li><li><strong>消融研究</strong>：无编码器对齐（标记级vs图像级）、对比损失层位置、生成模态贡献等，指标为任务成功率变化。</li></ul>"
  },
  {
    "date": "2025-09-30",
    "title": "Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA",
    "link": "http://arxiv.org/abs/2509.26251",
    "summary_markdown": "### 论文研究单位\n- 主要单位：清华大学深圳国际研究生院（Tsinghua Shenzhen International Graduate School, Tsinghua University）、阿里巴巴集团高德地图（Amap, Alibaba Group）\n- 合作单位：西安交通大学软件学院（School of Software Engineering, Xi’an Jiaotong University）\n### 论文概述\n论文针对潜在行动模型（LAMs）在视觉-语言-行动（VLA）系统中的两个核心瓶颈：空间理解不足（仅依赖RGB编码，忽视几何结构）和时序感知有限（依赖稀疏双帧输入，忽略长时动态），提出了一种名为SSM-VLA（Seeing Space and Motion - Vision-Language-Action）的框架。SSM-VLA通过融合几何感知空间编码、多尺度时序建模和视觉思维链推理，增强了VLA系统的鲁棒性和可解释性。\n### 论文核心贡献点\n1. **Farsighted-LAM模型**：通过DINOv2特征的几何感知空间编码和多尺度时序建模，提升潜在行动的空间-动态表示能力。\n2. **SSM-VLA架构**：集成Farsighted-LAM与视觉思维链（VisualCoT）推理模块，实现结构化感知与显式推理的结合，增强决策一致性和可解释性。\n3. **实验验证**：在仿真（CALVIN ABC-D基准）和真实环境中取得最先进性能，证明几何建模、时序一致性和显式推理的有效性。\n### 论文方法描述\n- **Farsighted-LAM**：\n - 编码器：处理当前RGB帧和多个未来关键帧（DINOv2特征），通过潜在行动查询生成离散潜在行动序列。\n - 解码器：重构未来观测（RGB和深度），采用多模态重建损失（L2和LPIPS损失、梯度感知深度损失）监督。\n- **VLA策略**：\n - 三阶段推理：视觉思维链预测未来观测 → 远视潜在行动推理 → 条件流匹配生成行动。\n - 多模态协同注意：统一变换器内实现，结构化注意机制管理信息流。\n- **关键组件**：几何先验（深度监督）、视觉思维链推理、潜在行动量化。\n### 论文使用数据集和训练资源\n- **数据集**：\n - CALVIN基准：仿真环境（Franka Panda机械臂），训练集为A/B/C环境，测试集为未见环境D。\n - 真实世界数据：基于Open-X-Embodiment数据集预训练，在50个人类演示上微调（AgileX Piper机器人）。\n- **训练资源**：\n - 潜在行动模型：AdamW优化器，学习率10^-4，权重衰减10^-5，批大小256，训练步100，代码本大小32。\n - VLA模型：AdamW优化器，学习率10^-3，权重衰减10^-4，批大小64，训练步30，损失权重λ_vision=0.1，λ_latent=0.01。\n - 流程匹配：10步去噪。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - CALVIN仿真基准：34个操控任务，语言指令，1000个指令链（每链5个连续任务）。\n - 真实世界实验：AgileX Piper机器人执行放置任务（粉色球入盒），环境包括杂乱背景。\n- **评估指标**：\n - CALVIN基准：连续任务完成率（1-5步）和平均成功链长（与基线对比）。\n - 消融研究：验证Farsighted-LAM结构、多模态注意和几何先验的贡献。\n - 真实世界：定性展示泛化能力（视觉重建对齐）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>主要单位：清华大学深圳国际研究生院（Tsinghua Shenzhen International Graduate School, Tsinghua University）、阿里巴巴集团高德地图（Amap, Alibaba Group）</li><li>合作单位：西安交通大学软件学院（School of Software Engineering, Xi’an Jiaotong University）</li></ul>\n<h3>论文概述</h3>\n<p>论文针对潜在行动模型（LAMs）在视觉-语言-行动（VLA）系统中的两个核心瓶颈：空间理解不足（仅依赖RGB编码，忽视几何结构）和时序感知有限（依赖稀疏双帧输入，忽略长时动态），提出了一种名为SSM-VLA（Seeing Space and Motion - Vision-Language-Action）的框架。SSM-VLA通过融合几何感知空间编码、多尺度时序建模和视觉思维链推理，增强了VLA系统的鲁棒性和可解释性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>Farsighted-LAM模型</strong>：通过DINOv2特征的几何感知空间编码和多尺度时序建模，提升潜在行动的空间-动态表示能力。</li><li><strong>SSM-VLA架构</strong>：集成Farsighted-LAM与视觉思维链（VisualCoT）推理模块，实现结构化感知与显式推理的结合，增强决策一致性和可解释性。</li><li><strong>实验验证</strong>：在仿真（CALVIN ABC-D基准）和真实环境中取得最先进性能，证明几何建模、时序一致性和显式推理的有效性。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>Farsighted-LAM</strong>：</li></ul>\n<p> - 编码器：处理当前RGB帧和多个未来关键帧（DINOv2特征），通过潜在行动查询生成离散潜在行动序列。</p>\n<p> - 解码器：重构未来观测（RGB和深度），采用多模态重建损失（L2和LPIPS损失、梯度感知深度损失）监督。</p>\n<ul><li><strong>VLA策略</strong>：</li></ul>\n<p> - 三阶段推理：视觉思维链预测未来观测 → 远视潜在行动推理 → 条件流匹配生成行动。</p>\n<p> - 多模态协同注意：统一变换器内实现，结构化注意机制管理信息流。</p>\n<ul><li><strong>关键组件</strong>：几何先验（深度监督）、视觉思维链推理、潜在行动量化。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - CALVIN基准：仿真环境（Franka Panda机械臂），训练集为A/B/C环境，测试集为未见环境D。</p>\n<p> - 真实世界数据：基于Open-X-Embodiment数据集预训练，在50个人类演示上微调（AgileX Piper机器人）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 潜在行动模型：AdamW优化器，学习率10^-4，权重衰减10^-5，批大小256，训练步100，代码本大小32。</p>\n<p> - VLA模型：AdamW优化器，学习率10^-3，权重衰减10^-4，批大小64，训练步30，损失权重λ_vision=0.1，λ_latent=0.01。</p>\n<p> - 流程匹配：10步去噪。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - CALVIN仿真基准：34个操控任务，语言指令，1000个指令链（每链5个连续任务）。</p>\n<p> - 真实世界实验：AgileX Piper机器人执行放置任务（粉色球入盒），环境包括杂乱背景。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - CALVIN基准：连续任务完成率（1-5步）和平均成功链长（与基线对比）。</p>\n<p> - 消融研究：验证Farsighted-LAM结构、多模态注意和几何先验的贡献。</p>\n<p> - 真实世界：定性展示泛化能力（视觉重建对齐）。</p>"
  },
  {
    "date": "2025-09-30",
    "title": "TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses",
    "link": "http://arxiv.org/abs/2509.25746",
    "summary_markdown": "### 论文研究单位\n小米机器人（Xiaomi Robotics）\n### 论文概述\n针对机器人灵巧手抓取执行阶段的“最后一公里”精度问题，论文提出TacRefineNet——首个仅依靠多指指尖触觉反馈实现已知物体任意姿态间精确调整的框架。该方法通过迭代调整末端执行器姿态，利用高分辨率压阻式触觉传感器捕获指尖接触力，实现毫米级抓取精度。为跨越仿真到现实的鸿沟，研究团队构建了MuJoCo仿真环境与真实机器人结合的数据集，并设计了多分支融合策略网络。\n### 论文核心贡献点\n1. **首个纯触觉驱动框架**：提出TacRefineNet，实现六自由度手部姿态的毫米级精确调整，完全依赖指尖触觉反馈\n2. **多分支融合策略网络**：通过融合多指触觉信号与本体感受，设计支持任意姿态间转换的策略网络，无需逐姿态重训练\n3. **仿真-现实高效结合**：采用仿真预训练+现实微调的混合训练策略，仅用少量真实数据即显著提升性能\n### 论文方法描述\n- **迭代调整策略**：在多指建立接触后，根据触觉图像与目标图像的差异，预测末端执行器的六自由度位姿增量（Δx），通过重复抓取-调整过程收敛至目标姿态\n- **触觉传感系统**：指尖集成11×9压阻式触觉阵列（像素间距1.1mm），实时输出触觉图像用于特征提取；在MuJoCo中建立物理基触觉模拟，弹性接触点模拟真实传感器弹性行为\n- **TacRefineNet网络架构**：\n 1. 多分支视觉编码器处理每根手指的当前/目标触觉图像\n 2. 融合多指触觉特征与关节位置信息\n 3. 三层MLP回归六自由度位姿增量\n- **跨组合训练方案**：随机配对当前与目标触觉图像构建N×N组合，增强策略对任意目标姿态的适应性\n- **数据增强**：对触觉图像实施缩放噪声，对关节位置添加高斯噪声提升鲁棒性\n### 论文使用数据集和训练资源\n- **仿真数据集（Ds_img）**：基于MuJoCo物理引擎生成，在限制维度范围内系统采样手部姿态（pitch/roll/y/z轴），记录对应触觉图像、关节位置及目标姿态\n- **真实数据集（Dr_img）**：复现实实验证中的可行姿态，增大采样步长降低数据密度，剔除实际碰撞姿态\n- **混合训练策略**：对比两种政策\n - 政策A：仅仿真数据训练\n - 政策B：仿真预训练 + 真实数据微调\n- **硬件资源**：\n - 仿真：MuJoCo物理引擎\n - 实物：11自由度灵巧手 + 11×9指尖触觉传感器阵列（0-255离散力值）\n### 论文使用的评估环境和评估指标\n- **实验场景**：\n - 多维度姿态调整：16种初始-目标姿态组合（pitch/roll/y/z轴对称设置）\n - 动态目标跟踪：连续扰动下的长期姿态维持任务\n - 未见物体泛化：相似几何但不同形态的物体测试\n- **评估指标**：\n 1. **精度指标（Metric 1）**：10步调整后的6DOF误差\n - 位置误差：δ_pos = \\|\\|p - p_g\\|\\|₂\n - 角度误差：δ_rot = 2arccos(\\|⟨Q, Q_g⟩\\|)\n 2. **效率指标（Metric 2）**：达到精度阈值的步数\n - 位置阈值：ε_pos = 0.005m\n - 角度阈值：ε_rot = 0.05rad\n 3. **成功率（Metric 3）**：R=5次试验的成功率\n - 成功判定：同时满足δ_pos ≤ ε_pos、δ_rot ≤ ε_rot，且调整步数≤S_max\n- **关键结论**：政策B在所有指标上显著优于政策A，最佳组合实现1.1mm位置精度/0.016rad角度精度，100%成功率。16种组合均达毫米级位置精度，最优姿态误差0.009rad。动态场景验证稳定性，未见物体在滚转维度表现出较好泛化性。\n\n> 注：论文方法在当前对象范围内有效，未来需融合视觉信息突破单对象限制。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>小米机器人（Xiaomi Robotics）</p>\n<h3>论文概述</h3>\n<p>针对机器人灵巧手抓取执行阶段的“最后一公里”精度问题，论文提出TacRefineNet——首个仅依靠多指指尖触觉反馈实现已知物体任意姿态间精确调整的框架。该方法通过迭代调整末端执行器姿态，利用高分辨率压阻式触觉传感器捕获指尖接触力，实现毫米级抓取精度。为跨越仿真到现实的鸿沟，研究团队构建了MuJoCo仿真环境与真实机器人结合的数据集，并设计了多分支融合策略网络。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首个纯触觉驱动框架</strong>：提出TacRefineNet，实现六自由度手部姿态的毫米级精确调整，完全依赖指尖触觉反馈</li><li><strong>多分支融合策略网络</strong>：通过融合多指触觉信号与本体感受，设计支持任意姿态间转换的策略网络，无需逐姿态重训练</li><li><strong>仿真-现实高效结合</strong>：采用仿真预训练+现实微调的混合训练策略，仅用少量真实数据即显著提升性能</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>迭代调整策略</strong>：在多指建立接触后，根据触觉图像与目标图像的差异，预测末端执行器的六自由度位姿增量（Δx），通过重复抓取-调整过程收敛至目标姿态</li><li><strong>触觉传感系统</strong>：指尖集成11×9压阻式触觉阵列（像素间距1.1mm），实时输出触觉图像用于特征提取；在MuJoCo中建立物理基触觉模拟，弹性接触点模拟真实传感器弹性行为</li><li><strong>TacRefineNet网络架构</strong>：</li></ul>\n<p> 1. 多分支视觉编码器处理每根手指的当前/目标触觉图像</p>\n<p> 2. 融合多指触觉特征与关节位置信息</p>\n<p> 3. 三层MLP回归六自由度位姿增量</p>\n<ul><li><strong>跨组合训练方案</strong>：随机配对当前与目标触觉图像构建N×N组合，增强策略对任意目标姿态的适应性</li><li><strong>数据增强</strong>：对触觉图像实施缩放噪声，对关节位置添加高斯噪声提升鲁棒性</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>仿真数据集（Ds_img）</strong>：基于MuJoCo物理引擎生成，在限制维度范围内系统采样手部姿态（pitch/roll/y/z轴），记录对应触觉图像、关节位置及目标姿态</li><li><strong>真实数据集（Dr_img）</strong>：复现实实验证中的可行姿态，增大采样步长降低数据密度，剔除实际碰撞姿态</li><li><strong>混合训练策略</strong>：对比两种政策</li></ul>\n<p> - 政策A：仅仿真数据训练</p>\n<p> - 政策B：仿真预训练 + 真实数据微调</p>\n<ul><li><strong>硬件资源</strong>：</li></ul>\n<p> - 仿真：MuJoCo物理引擎</p>\n<p> - 实物：11自由度灵巧手 + 11×9指尖触觉传感器阵列（0-255离散力值）</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>实验场景</strong>：</li></ul>\n<p> - 多维度姿态调整：16种初始-目标姿态组合（pitch/roll/y/z轴对称设置）</p>\n<p> - 动态目标跟踪：连续扰动下的长期姿态维持任务</p>\n<p> - 未见物体泛化：相似几何但不同形态的物体测试</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> 1. <strong>精度指标（Metric 1）</strong>：10步调整后的6DOF误差</p>\n<p> - 位置误差：δ_pos = \\|\\|p - p_g\\|\\|₂</p>\n<p> - 角度误差：δ_rot = 2arccos(\\|⟨Q, Q_g⟩\\|)</p>\n<p> 2. <strong>效率指标（Metric 2）</strong>：达到精度阈值的步数</p>\n<p> - 位置阈值：ε_pos = 0.005m</p>\n<p> - 角度阈值：ε_rot = 0.05rad</p>\n<p> 3. <strong>成功率（Metric 3）</strong>：R=5次试验的成功率</p>\n<p> - 成功判定：同时满足δ_pos ≤ ε_pos、δ_rot ≤ ε_rot，且调整步数≤S_max</p>\n<ul><li><strong>关键结论</strong>：政策B在所有指标上显著优于政策A，最佳组合实现1.1mm位置精度/0.016rad角度精度，100%成功率。16种组合均达毫米级位置精度，最优姿态误差0.009rad。动态场景验证稳定性，未见物体在滚转维度表现出较好泛化性。</li></ul>\n\n<p>> 注：论文方法在当前对象范围内有效，未来需融合视觉信息突破单对象限制。</p>"
  },
  {
    "date": "2025-09-30",
    "title": "VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning",
    "link": "http://arxiv.org/abs/2509.25718",
    "summary_markdown": "# VLA模型通过动作分块PPO和自监督行为克隆的后训练\n## 论文研究单位\n- 中国科学院自动化研究所多模态人工智能系统国家重点实验室\n- 中国科学院大学人工智能学院\n## 论文概述\n本文提出了一种基于动作分块近端策略优化（PPO）和自监督行为克隆的VLA（视觉-语言-动作）模型后训练方法。该方法针对强化学习在VLA模型后训练中面临的稀疏奖励和不稳定训练挑战，通过将连续动作聚合为动作分块来提高策略的时间一致性和反馈密度，同时结合动态更新的演示缓冲区进行自监督行为克隆辅助训练。\n## 论文核心贡献点\n1. 开发了用于VLA模型后训练的动作分块PPO算法，增加了有效的信息反馈密度\n2. 构建了基于动态演示缓冲区的自监督行为克隆辅助损失，在训练过程中持续收集高质量任务轨迹\n3. 实验表明，该方法仅用10个演示就超过了使用100个演示的监督微调性能，成功率达0.93，步数减少到42.17\n## 论文方法描述\n### 动作分块PPO\n- 将连续动作聚合为长度为h≥1的动作分块，提高奖励反馈频率\n- 在actor-critic架构上实现PPO框架，使用截断代理目标限制策略差异\n- 通过广义优势估计（GAE）计算优势函数\n### 自监督行为克隆\n- 初始化时使用专家轨迹构建动态演示缓冲区\n- 训练过程中将高质量成功轨迹加入缓冲区\n- 使用轨迹长度作为质量代理指标，较短轨迹表示更高质量\n- 构建辅助监督损失L_BC进行策略优化\n### 在线后训练\n- 总损失函数为加权组合：L_online = β_t * L_PPO + L_BC\n- β_t采用渐进调度：β_t = tanh(t/T_warmup)，实现从监督学习到强化学习的平滑过渡\n- 早期训练完全由行为克隆驱动，随训练进展逐步强调PPO目标\n## 论文使用数据集和训练资源\n- **数据集**：MetaWorld环境中的MT10基准，包含10个单任务环境\n- **演示数据**：每个任务使用规则策略收集演示轨迹，每条轨迹限制200步\n- **模型基础**：Octo-small作为VLA模型骨干网络\n- **训练配置**：\n - 优化器：AdamW，学习率10^-5\n - GAE λ=0.99，折扣因子γ=0.99\n - PPO截断ε=0.2，值损失权重=0.5，熵损失权重=0.0\n - 动作分块长度h=4，预热步数T_warmup=40k\n - 批次大小16，总训练步数500k\n## 论文使用的评估环境和评估指标\n- **评估环境**：MetaWorld MT10基准测试\n- **评估配置**：每任务评估128个回合，使用统一随机种子\n- **评估指标**：\n - 平均成功率（Acc.）：128个回合的成功率\n - 轨迹长度分布的第10百分位数（Len.）\n - 最短10%轨迹的平均长度（Avg(l).）\n- **对比基线**：\n - SFT：10个演示的监督微调\n - SFT_100：100个演示的监督微调\n - 标准PPO：无额外演示的标准PPO\n- **评估结果**：提出的方法在平均性能和大多数单任务指标上均排名第一，证明了RL后训练的可行性和有效性",
    "summary_html": "<h1>VLA模型通过动作分块PPO和自监督行为克隆的后训练</h1>\n<h2>论文研究单位</h2>\n<ul><li>中国科学院自动化研究所多模态人工智能系统国家重点实验室</li><li>中国科学院大学人工智能学院</li></ul>\n<h2>论文概述</h2>\n<p>本文提出了一种基于动作分块近端策略优化（PPO）和自监督行为克隆的VLA（视觉-语言-动作）模型后训练方法。该方法针对强化学习在VLA模型后训练中面临的稀疏奖励和不稳定训练挑战，通过将连续动作聚合为动作分块来提高策略的时间一致性和反馈密度，同时结合动态更新的演示缓冲区进行自监督行为克隆辅助训练。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>开发了用于VLA模型后训练的动作分块PPO算法，增加了有效的信息反馈密度</li><li>构建了基于动态演示缓冲区的自监督行为克隆辅助损失，在训练过程中持续收集高质量任务轨迹</li><li>实验表明，该方法仅用10个演示就超过了使用100个演示的监督微调性能，成功率达0.93，步数减少到42.17</li></ol>\n<h2>论文方法描述</h2>\n<h3>动作分块PPO</h3>\n<ul><li>将连续动作聚合为长度为h≥1的动作分块，提高奖励反馈频率</li><li>在actor-critic架构上实现PPO框架，使用截断代理目标限制策略差异</li><li>通过广义优势估计（GAE）计算优势函数</li></ul>\n<h3>自监督行为克隆</h3>\n<ul><li>初始化时使用专家轨迹构建动态演示缓冲区</li><li>训练过程中将高质量成功轨迹加入缓冲区</li><li>使用轨迹长度作为质量代理指标，较短轨迹表示更高质量</li><li>构建辅助监督损失L_BC进行策略优化</li></ul>\n<h3>在线后训练</h3>\n<ul><li>总损失函数为加权组合：L_online = β_t * L_PPO + L_BC</li><li>β_t采用渐进调度：β_t = tanh(t/T_warmup)，实现从监督学习到强化学习的平滑过渡</li><li>早期训练完全由行为克隆驱动，随训练进展逐步强调PPO目标</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：MetaWorld环境中的MT10基准，包含10个单任务环境</li><li><strong>演示数据</strong>：每个任务使用规则策略收集演示轨迹，每条轨迹限制200步</li><li><strong>模型基础</strong>：Octo-small作为VLA模型骨干网络</li><li><strong>训练配置</strong>：</li></ul>\n<p> - 优化器：AdamW，学习率10^-5</p>\n<p> - GAE λ=0.99，折扣因子γ=0.99</p>\n<p> - PPO截断ε=0.2，值损失权重=0.5，熵损失权重=0.0</p>\n<p> - 动作分块长度h=4，预热步数T_warmup=40k</p>\n<p> - 批次大小16，总训练步数500k</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：MetaWorld MT10基准测试</li><li><strong>评估配置</strong>：每任务评估128个回合，使用统一随机种子</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 平均成功率（Acc.）：128个回合的成功率</p>\n<p> - 轨迹长度分布的第10百分位数（Len.）</p>\n<p> - 最短10%轨迹的平均长度（Avg(l).）</p>\n<ul><li><strong>对比基线</strong>：</li></ul>\n<p> - SFT：10个演示的监督微调</p>\n<p> - SFT_100：100个演示的监督微调</p>\n<p> - 标准PPO：无额外演示的标准PPO</p>\n<ul><li><strong>评估结果</strong>：提出的方法在平均性能和大多数单任务指标上均排名第一，证明了RL后训练的可行性和有效性</li></ul>"
  },
  {
    "date": "2025-09-30",
    "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought",
    "link": "http://arxiv.org/abs/2509.25681",
    "summary_markdown": "### 论文研究单位\n美的集团、北京大学、上海交通大学\n### 论文概述\n论文提出 dVLA（diffusion Vision-Language-Action），首个以离散扩散语言模型（DLM）为核心的视觉-语言-动作一体化框架。该模型在统一的扩散目标下，联合优化视觉推理、文本推理与动作生成，并通过多模态思维链（CoT）将高级指令分解为可执行的子目标和动作。在模拟和真实环境中验证有效性：LIBERO 基准平均成功率达 96.4%，真实 Franka 机器人完成多项任务，包括具有挑战性的多步分拣。\n### 论文核心贡献点\n- 首个基于离散扩散语言模型的 VLA 框架，统一视觉、语言与动作的概率建模与生成。\n- 提出多模态 CoT 训练范式，模型需同时生成子目标图像（视觉 CoT）、文本推理与离散动作，并在训练中对三者均执行随机掩码与重建，强化跨模态一致性。\n- 在 LIBERO 基准取得 96.4% 平均成功率，优于离散与连续动作策略；在真实机器人任务上同样领先。\n- 引入推理加速策略：块级因果前缀注意力（训练时）与 KV 缓存（推理时），获得约 2× 推理加速，性能损失微小。\n- 模型可预测不安全动作对应的失败视觉 CoT，体现对物理规律和执行后果的内在理解。\n### 论文方法描述\n- 统一离散化与扩散训练：将视觉（MAGVIT-v2）、文本（LLaDA tokenizer）与动作（FAST：DCT + BPE）统一为离散 token，在相同扩散目标下随机掩码并重建，仅对掩码 token 计算损失。\n- 架构与初始化：基于 MMaDA 扩展，使用不同分词器并将词汇表从 126,464 扩展至 136,704；所有模态共享一个离散扩散建模目标。\n- 多模态 CoT 数据与序列结构：输入为 [多视角图像、语言指令、机器人状态]，输出为 [视觉子目标图像、文本推理、离散动作块]，通过起止标记组织序列。\n- 推理生成：并行输出视觉 CoT（未来状态图像）与文本 CoT（子任务步骤），再据此生成可执行的离散动作。\n- 加速策略：训练时采用分块因果前缀注意力（块内双向，块间单向），推理时引入 KV 缓存（dLLM-Cache 思想），减少重复计算，实现约 2× 加速。\n### 论文使用数据集和训练资源\n- 模拟：LIBERO 四个套件（Spatial、Object、Goal、Long），共 40 个任务，每任务 50 条演示；分辨率提升至 256×256。\n- 真实：Franka 7-DoF 机械臂，1100 条轨迹，涵盖四项任务：\n - Bin Picking：600 条\n - Open Box：100 条\n - Hang Cups：200 条\n - Pick & place Object：200 条\n- 训练细节：\n - 输入图像统一 resize 至 256×256。\n - 子目标图像预测时域在 [0.9C, 1.1C]，其中 C=5（LIBERO）、C=50（真实任务），仅预测俯视相机图像，使用 classifier-free guidance（scale=3.5）。\n - 文本推理使用 SEED-1.5VL 的视频分割注释，长任务每 3 秒一段，简单任务可省略以加速推理。\n - 训练流程与 MMaDA 一致，未明确给出硬件资源。\n### 论文使用的评估环境和评估指标\n- 模拟评估：LIBERO 基准，每任务 50 次试验（合计 500 次），以任务成功率（SR）为主要指标；与多种连续与离散动作基线对比。\n- 真实评估：Franka 机器人（2 个 ZED 外置相机 + 1 个 Realsense 435i 手腕相机），每任务 10 次试验（合计 40 次），报告成功率。\n- 推理效率：比较全注意力与前缀注意力 + KV 缓存两方案下的动作频率（Hz）与成功率，验证约 2× 推理加速与性能保持。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>美的集团、北京大学、上海交通大学</p>\n<h3>论文概述</h3>\n<p>论文提出 dVLA（diffusion Vision-Language-Action），首个以离散扩散语言模型（DLM）为核心的视觉-语言-动作一体化框架。该模型在统一的扩散目标下，联合优化视觉推理、文本推理与动作生成，并通过多模态思维链（CoT）将高级指令分解为可执行的子目标和动作。在模拟和真实环境中验证有效性：LIBERO 基准平均成功率达 96.4%，真实 Franka 机器人完成多项任务，包括具有挑战性的多步分拣。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首个基于离散扩散语言模型的 VLA 框架，统一视觉、语言与动作的概率建模与生成。</li><li>提出多模态 CoT 训练范式，模型需同时生成子目标图像（视觉 CoT）、文本推理与离散动作，并在训练中对三者均执行随机掩码与重建，强化跨模态一致性。</li><li>在 LIBERO 基准取得 96.4% 平均成功率，优于离散与连续动作策略；在真实机器人任务上同样领先。</li><li>引入推理加速策略：块级因果前缀注意力（训练时）与 KV 缓存（推理时），获得约 2× 推理加速，性能损失微小。</li><li>模型可预测不安全动作对应的失败视觉 CoT，体现对物理规律和执行后果的内在理解。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>统一离散化与扩散训练：将视觉（MAGVIT-v2）、文本（LLaDA tokenizer）与动作（FAST：DCT + BPE）统一为离散 token，在相同扩散目标下随机掩码并重建，仅对掩码 token 计算损失。</li><li>架构与初始化：基于 MMaDA 扩展，使用不同分词器并将词汇表从 126,464 扩展至 136,704；所有模态共享一个离散扩散建模目标。</li><li>多模态 CoT 数据与序列结构：输入为 [多视角图像、语言指令、机器人状态]，输出为 [视觉子目标图像、文本推理、离散动作块]，通过起止标记组织序列。</li><li>推理生成：并行输出视觉 CoT（未来状态图像）与文本 CoT（子任务步骤），再据此生成可执行的离散动作。</li><li>加速策略：训练时采用分块因果前缀注意力（块内双向，块间单向），推理时引入 KV 缓存（dLLM-Cache 思想），减少重复计算，实现约 2× 加速。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>模拟：LIBERO 四个套件（Spatial、Object、Goal、Long），共 40 个任务，每任务 50 条演示；分辨率提升至 256×256。</li><li>真实：Franka 7-DoF 机械臂，1100 条轨迹，涵盖四项任务：</li></ul>\n<p> - Bin Picking：600 条</p>\n<p> - Open Box：100 条</p>\n<p> - Hang Cups：200 条</p>\n<p> - Pick & place Object：200 条</p>\n<ul><li>训练细节：</li></ul>\n<p> - 输入图像统一 resize 至 256×256。</p>\n<p> - 子目标图像预测时域在 [0.9C, 1.1C]，其中 C=5（LIBERO）、C=50（真实任务），仅预测俯视相机图像，使用 classifier-free guidance（scale=3.5）。</p>\n<p> - 文本推理使用 SEED-1.5VL 的视频分割注释，长任务每 3 秒一段，简单任务可省略以加速推理。</p>\n<p> - 训练流程与 MMaDA 一致，未明确给出硬件资源。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>模拟评估：LIBERO 基准，每任务 50 次试验（合计 500 次），以任务成功率（SR）为主要指标；与多种连续与离散动作基线对比。</li><li>真实评估：Franka 机器人（2 个 ZED 外置相机 + 1 个 Realsense 435i 手腕相机），每任务 10 次试验（合计 40 次），报告成功率。</li><li>推理效率：比较全注意力与前缀注意力 + KV 缓存两方案下的动作频率（Hz）与成功率，验证约 2× 推理加速与性能保持。</li></ul>"
  },
  {
    "date": "2025-09-29",
    "title": "World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training",
    "link": "http://arxiv.org/abs/2509.24948",
    "summary_markdown": "## 论文研究单位\nSun Yat-sen University, China; Amap, Alibaba Group\n## 论文概述\nVision-Language-Action (VLA) 模型在少样本与高风险场景下受限：模仿学习对大量演示数据依赖强，强化学习（RL）后训练需真实交互但环境状态不可重置、反馈稀疏且难以判定任务完成。作者提出 World-Env，将世界模型作为低成本、可安全探索的虚拟环境用于 VLA 的 RL 后训练：包括一个视频世界模型用于生成时序一致的未来视觉观测，和一个 VLM 引导的即时反射器用于连续奖励与实时终止信号。该方法在 5 条专家演示下即可显著提升复杂操控任务的成功率，兼顾数据效率与安全性。\n## 论文核心贡献点\n- 提出 World-Env 框架：以世界模型替代真实交互，支持 VLA 的低成本、风险可控的 RL 后训练。\n- 集成视频世界模拟器和 VLM 引导的即时反射器，提供语义对齐的连续奖励与语言对齐的任务完成判定。\n- 引入动态终止机制（基于 R(o1:t, g) > η），避免成功后冗余动作，提升执行效率与成功率。\n## 论文方法描述\n- 视频世界模拟器（EVAC 思路）：以动作 a_t 和下时刻本体感觉状态 s_{t+1}（6D末端位姿与 1D夹爪）作为输入，投影构建动作图（前景标记+黑背景），以像素级条件通过扩散图像生成预测下一帧视觉观测 o_{t+1}。训练数据混合人类演示与自探索轨迹；自探索通过在模拟器内执行 SFT 策略并用 Laplace 分布引入扰动（scale head 预测 β_t），以覆盖失败与次优行为分布。\n- VLM 引导的即时反射器：基于 LLaVA，使用冻结的视觉编码器与 LLM 以视频-文本为条件，奖励头输出 R(o1:t, g) ∈ [0,1]，采用阈值 η=0.5 触发终止；训练采用二进制交叉熵（BCE）监督，数据源自 LIBERO 成功判定与模拟器内 oracle 标记。\n- VLA 策略与 RL：基于 OpenVLA-OFT 的 VLA 策略添加 scale head 使动作分布为 Laplace(μ_t, β_t)，用于不确定度驱动的探索。RL 优化采用 LOOP（RLOO 优势 + PPO 截断目标）：对同一初始状态生成 N 条轨迹（N=8），用 RLOO 计算轨迹优势 A_n，基于 PPO 截断目标更新策略与 scale head。\n- 终止与奖励使用：在终止步 t_end 给出单步轨迹奖励 R_n = R(o1:t_end, g)，优势广播到轨迹各步；终止时提前结束 rollout。\n- 训练实现：LoRA 微调视觉-语言骨干（rank=32），LoRA LR=1e-4，动作/标度头全参 LR=1e-5，批量 4，8×H20 GPU（96GB）。\n## 论文使用数据集和训练资源\n- 数据集：LIBERO（含 LIBERO-Goal、LIBERO-Object、LIBERO-Spatial、LIBERO-Long 四套），每任务仅用 5 条专家演示进行 SFT 预训练；世界模型额外加入自探索过渡数据（含成功与失败）。\n- 资源：8×NVIDIA H20 GPU（96GB），LoRA（rank=32）进行参数高效微调。\n## 论文使用的评估环境和评估指标\n- 评估环境：LIBERO 仿真平台，面向机器人视觉-语言操控。\n- 指标：成功率（对全测试集）。在“真实反馈约束”设定下比较终止策略（无法获得真实终止信号），验证动态终止与连续奖励的稳定性。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>Sun Yat-sen University, China; Amap, Alibaba Group</p>\n<h2>论文概述</h2>\n<p>Vision-Language-Action (VLA) 模型在少样本与高风险场景下受限：模仿学习对大量演示数据依赖强，强化学习（RL）后训练需真实交互但环境状态不可重置、反馈稀疏且难以判定任务完成。作者提出 World-Env，将世界模型作为低成本、可安全探索的虚拟环境用于 VLA 的 RL 后训练：包括一个视频世界模型用于生成时序一致的未来视觉观测，和一个 VLM 引导的即时反射器用于连续奖励与实时终止信号。该方法在 5 条专家演示下即可显著提升复杂操控任务的成功率，兼顾数据效率与安全性。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出 World-Env 框架：以世界模型替代真实交互，支持 VLA 的低成本、风险可控的 RL 后训练。</li><li>集成视频世界模拟器和 VLM 引导的即时反射器，提供语义对齐的连续奖励与语言对齐的任务完成判定。</li><li>引入动态终止机制（基于 R(o1:t, g) > η），避免成功后冗余动作，提升执行效率与成功率。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>视频世界模拟器（EVAC 思路）：以动作 a_t 和下时刻本体感觉状态 s_{t+1}（6D末端位姿与 1D夹爪）作为输入，投影构建动作图（前景标记+黑背景），以像素级条件通过扩散图像生成预测下一帧视觉观测 o_{t+1}。训练数据混合人类演示与自探索轨迹；自探索通过在模拟器内执行 SFT 策略并用 Laplace 分布引入扰动（scale head 预测 β_t），以覆盖失败与次优行为分布。</li><li>VLM 引导的即时反射器：基于 LLaVA，使用冻结的视觉编码器与 LLM 以视频-文本为条件，奖励头输出 R(o1:t, g) ∈ [0,1]，采用阈值 η=0.5 触发终止；训练采用二进制交叉熵（BCE）监督，数据源自 LIBERO 成功判定与模拟器内 oracle 标记。</li><li>VLA 策略与 RL：基于 OpenVLA-OFT 的 VLA 策略添加 scale head 使动作分布为 Laplace(μ_t, β_t)，用于不确定度驱动的探索。RL 优化采用 LOOP（RLOO 优势 + PPO 截断目标）：对同一初始状态生成 N 条轨迹（N=8），用 RLOO 计算轨迹优势 A_n，基于 PPO 截断目标更新策略与 scale head。</li><li>终止与奖励使用：在终止步 t_end 给出单步轨迹奖励 R_n = R(o1:t_end, g)，优势广播到轨迹各步；终止时提前结束 rollout。</li><li>训练实现：LoRA 微调视觉-语言骨干（rank=32），LoRA LR=1e-4，动作/标度头全参 LR=1e-5，批量 4，8×H20 GPU（96GB）。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据集：LIBERO（含 LIBERO-Goal、LIBERO-Object、LIBERO-Spatial、LIBERO-Long 四套），每任务仅用 5 条专家演示进行 SFT 预训练；世界模型额外加入自探索过渡数据（含成功与失败）。</li><li>资源：8×NVIDIA H20 GPU（96GB），LoRA（rank=32）进行参数高效微调。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：LIBERO 仿真平台，面向机器人视觉-语言操控。</li><li>指标：成功率（对全测试集）。在“真实反馈约束”设定下比较终止策略（无法获得真实终止信号），验证动态终止与连续奖励的稳定性。</li></ul>"
  },
  {
    "date": "2025-09-29",
    "title": "IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks",
    "link": "http://arxiv.org/abs/2509.24768",
    "summary_markdown": "# 论文研究单位\n\n芬兰阿尔托大学智能机器人组、芬兰奥卢大学生物仿生与智能系统组、丹麦技术大学机械技术系\n# 论文概述\n\n论文提出了IA-VLA框架，用于解决视觉语言行动模型(VLAs)在处理语义复杂任务时的局限性，特别是在涉及视觉重复对象场景下的机器人操控任务。当前VLA模型受限于推理延迟，无法使用足够大的语言模型来处理复杂空间关系和语义指令。\n# 论文核心贡献点\n\n1. 提出了IA-VLA框架，通过大型视觉语言模型作为预处理阶段来增强VLA的输入\n2. 形式化了视觉重复对象这一复杂的任务类别，构建了相关数据集\n3. 在涉及重复对象的设置中进行了全面的实验评估，总计1290次评估运行\n4. 证明了该框架能够显著提高VLA在语义复杂指令下的性能表现\n# 论文方法描述\n\n**IA-VLA框架流程：**\n- 使用Semantic-SAM对输入图像进行分割，添加数字标签\n- 调用大型VLM（如GPT-4.1）根据任务描述选择相关对象的数字标签\n- 对相关对象应用语义分割掩码，对其他区域使用半透明灰色掩码（alpha值0.8）\n- 通过SAM2实现掩码在时间序列中的传播\n- 支持两种变体：原始指令版本和重标记版本（将VLA指令简化为\"lift the highlighted object\"等）\n\n**掩码过滤算法：**\n- 掩码块过滤器：将非连通块分离为独立掩码\n- 掩码重叠过滤器：根据重叠程度合并、丢弃或相减掩码\n- 面积阈值过滤：移除过小的掩码\n# 论文使用数据集和训练资源\n\n**数据集构成：**\n- 积木块提升任务：120个演示，12种语言指令\n- 蔬菜装锅任务：120个演示，12种语言指令\n- 抽屉开启任务：600个演示，12种语言指令\n- 训练在桌面任务（积木块和厨房）使用联合模型，抽屉任务单独训练\n\n**训练配置：**\n- 基于OpenVLA模型进行微调\n- 批次大小：16，学习率：0.0005，LoRA秩：32，dropout：0.0\n- 桌面任务训练25000步，抽屉任务训练50000步\n- 使用OpenVLA默认图像增强策略\n\n**计算资源：**\n- Aalto Science-IT项目和CSC - 芬兰IT科学中心提供的计算资源\n- Aalto电气工程学院MIDAS研究基础设施\n# 论文使用的评估环境和评估指标\n\n**评估任务设置：**\n三类任务指令复杂度：\n- Category 1：训练数据中见过的语言指令\n- Category 2：见过概念的未见过组合\n- Category 3：需要从训练概念外推的指令\n\n**具体任务：**\n1. **积木块提升**：要求识别特定颜色和位置的积木块进行抓取\n2. **蔬菜装锅**：将蔬菜放入指定位置的锅中\n3. **抽屉开启**：打开指定位置和行的抽屉\n\n**评估指标：**\n- 成功完成任务得1分\n- 抓取正确对象但任务未完成得0.5分\n- 30秒时间限制（不包括预处理时间）\n- 每个配置进行5次评估运行\n- 结果以最大可能积分的百分比形式报告\n\n**评估规模：**\n- 积木块任务：450次评估运行\n- 蔬菜装锅任务：480次评估运行\n- 抽屉开启任务：360次评估运行\n- 总计1290次评估运行",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>芬兰阿尔托大学智能机器人组、芬兰奥卢大学生物仿生与智能系统组、丹麦技术大学机械技术系</p>\n<h1>论文概述</h1>\n\n<p>论文提出了IA-VLA框架，用于解决视觉语言行动模型(VLAs)在处理语义复杂任务时的局限性，特别是在涉及视觉重复对象场景下的机器人操控任务。当前VLA模型受限于推理延迟，无法使用足够大的语言模型来处理复杂空间关系和语义指令。</p>\n<h1>论文核心贡献点</h1>\n\n<ol><li>提出了IA-VLA框架，通过大型视觉语言模型作为预处理阶段来增强VLA的输入</li><li>形式化了视觉重复对象这一复杂的任务类别，构建了相关数据集</li><li>在涉及重复对象的设置中进行了全面的实验评估，总计1290次评估运行</li><li>证明了该框架能够显著提高VLA在语义复杂指令下的性能表现</li></ol>\n<h1>论文方法描述</h1>\n\n<p><strong>IA-VLA框架流程：</strong></p>\n<ul><li>使用Semantic-SAM对输入图像进行分割，添加数字标签</li><li>调用大型VLM（如GPT-4.1）根据任务描述选择相关对象的数字标签</li><li>对相关对象应用语义分割掩码，对其他区域使用半透明灰色掩码（alpha值0.8）</li><li>通过SAM2实现掩码在时间序列中的传播</li><li>支持两种变体：原始指令版本和重标记版本（将VLA指令简化为\"lift the highlighted object\"等）</li></ul>\n\n<p><strong>掩码过滤算法：</strong></p>\n<ul><li>掩码块过滤器：将非连通块分离为独立掩码</li><li>掩码重叠过滤器：根据重叠程度合并、丢弃或相减掩码</li><li>面积阈值过滤：移除过小的掩码</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n\n<p><strong>数据集构成：</strong></p>\n<ul><li>积木块提升任务：120个演示，12种语言指令</li><li>蔬菜装锅任务：120个演示，12种语言指令</li><li>抽屉开启任务：600个演示，12种语言指令</li><li>训练在桌面任务（积木块和厨房）使用联合模型，抽屉任务单独训练</li></ul>\n\n<p><strong>训练配置：</strong></p>\n<ul><li>基于OpenVLA模型进行微调</li><li>批次大小：16，学习率：0.0005，LoRA秩：32，dropout：0.0</li><li>桌面任务训练25000步，抽屉任务训练50000步</li><li>使用OpenVLA默认图像增强策略</li></ul>\n\n<p><strong>计算资源：</strong></p>\n<ul><li>Aalto Science-IT项目和CSC - 芬兰IT科学中心提供的计算资源</li><li>Aalto电气工程学院MIDAS研究基础设施</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<p><strong>评估任务设置：</strong></p>\n<p>三类任务指令复杂度：</p>\n<ul><li>Category 1：训练数据中见过的语言指令</li><li>Category 2：见过概念的未见过组合</li><li>Category 3：需要从训练概念外推的指令</li></ul>\n\n<p><strong>具体任务：</strong></p>\n<ol><li><strong>积木块提升</strong>：要求识别特定颜色和位置的积木块进行抓取</li><li><strong>蔬菜装锅</strong>：将蔬菜放入指定位置的锅中</li><li><strong>抽屉开启</strong>：打开指定位置和行的抽屉</li></ol>\n\n<p><strong>评估指标：</strong></p>\n<ul><li>成功完成任务得1分</li><li>抓取正确对象但任务未完成得0.5分</li><li>30秒时间限制（不包括预处理时间）</li><li>每个配置进行5次评估运行</li><li>结果以最大可能积分的百分比形式报告</li></ul>\n\n<p><strong>评估规模：</strong></p>\n<ul><li>积木块任务：450次评估运行</li><li>蔬菜装锅任务：480次评估运行</li><li>抽屉开启任务：360次评估运行</li><li>总计1290次评估运行</li></ul>"
  },
  {
    "date": "2025-09-29",
    "title": "Emergent World Representations in OpenVLA",
    "link": "http://arxiv.org/abs/2509.24559",
    "summary_markdown": "### 论文研究单位\n- LSE.AI, London School of Economics\n- ETH Zurich\n- Princeton University - Department of Computer Science\n- Mila - Quebec AI Institute\n### 论文概述\nVision-Language-Action models (VLAs) 如OpenVLA，训练于策略基础强化学习（RL），但是否隐式学习世界模型（状态转移函数）未知。论文通过embedding arithmetic在状态表示上实验，探测OpenVLA是否编码潜在世界模型。使用线性/非线性探针预测状态转移向量，发现激活探针优于embeddings基线，表明世界模型存在。调查训练进展，发现世界模型随计算扩展而出现，并提出SAEs的可解释规划管道。\n### 论文核心贡献点\n- 利用embedding arithmetic证明OpenVLA编码潜在世界模型。\n- 展示训练计算扩展增强世界模型发展，并定位其于中间层。\n- 提出SAEs应用于可解释规划：预测状态转移向量后分解为可解释特征。\n- 线性探针优于MLP探针，支持线性表示假设（LHR）。\n### 论文方法描述\n- **理论框架**：基于Koopman算子近似世界模型，定义K步状态转移算子。\n- **状态转移向量**：学习函数 \\( f: \\mathbf{a}_t \\mapsto \\Delta\\mathbf{e}_{t\\rightarrow t+K} \\)，其中 \\(\\Delta\\mathbf{e} = \\mathbf{e}_{t+K} - \\mathbf{e}_t\\)。\n- **探针**：\n - 线性探针（Lasso回归）预测 \\(\\Delta\\mathbf{e}\\) 从激活 \\(\\mathbf{a}_t\\)。\n - MLP探针测试非线性表示。\n- **基线对比**：训练探针于embeddings作为基线，隔离因果效应。\n- **评估**：R²分数和置换检验（p < 0.01）量化预测性能。\n### 论文使用数据集和训练资源\n- **数据集**：LIBERO数据集（goal, spatial, long, object子集），共400 episodes, 66,931步。\n- **模型**：OpenVLA (7B参数)，预训练于Open X-Embodiment数据集。\n- **训练资源**：探针训练使用网格搜索调优超参数；计算资源未详述，代码可用。\n### 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO各子集测试集。\n- **评估指标**：\n - 回归R²分数评估预测能力。\n - 置换检验（100次）检验统计显著性（p值）。\n - Allan方差分析长时状态转移信噪比。\n - 时间一致性（cosine similarity）测量嵌入稳定性。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>LSE.AI, London School of Economics</li><li>ETH Zurich</li><li>Princeton University - Department of Computer Science</li><li>Mila - Quebec AI Institute</li></ul>\n<h3>论文概述</h3>\n<p>Vision-Language-Action models (VLAs) 如OpenVLA，训练于策略基础强化学习（RL），但是否隐式学习世界模型（状态转移函数）未知。论文通过embedding arithmetic在状态表示上实验，探测OpenVLA是否编码潜在世界模型。使用线性/非线性探针预测状态转移向量，发现激活探针优于embeddings基线，表明世界模型存在。调查训练进展，发现世界模型随计算扩展而出现，并提出SAEs的可解释规划管道。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>利用embedding arithmetic证明OpenVLA编码潜在世界模型。</li><li>展示训练计算扩展增强世界模型发展，并定位其于中间层。</li><li>提出SAEs应用于可解释规划：预测状态转移向量后分解为可解释特征。</li><li>线性探针优于MLP探针，支持线性表示假设（LHR）。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>理论框架</strong>：基于Koopman算子近似世界模型，定义K步状态转移算子。</li><li><strong>状态转移向量</strong>：学习函数 \\( f: \\mathbf{a}_t \\mapsto \\Delta\\mathbf{e}_{t\\rightarrow t+K} \\)，其中 \\(\\Delta\\mathbf{e} = \\mathbf{e}_{t+K} - \\mathbf{e}_t\\)。</li><li><strong>探针</strong>：</li></ul>\n<p> - 线性探针（Lasso回归）预测 \\(\\Delta\\mathbf{e}\\) 从激活 \\(\\mathbf{a}_t\\)。</p>\n<p> - MLP探针测试非线性表示。</p>\n<ul><li><strong>基线对比</strong>：训练探针于embeddings作为基线，隔离因果效应。</li><li><strong>评估</strong>：R²分数和置换检验（p < 0.01）量化预测性能。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：LIBERO数据集（goal, spatial, long, object子集），共400 episodes, 66,931步。</li><li><strong>模型</strong>：OpenVLA (7B参数)，预训练于Open X-Embodiment数据集。</li><li><strong>训练资源</strong>：探针训练使用网格搜索调优超参数；计算资源未详述，代码可用。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：LIBERO各子集测试集。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 回归R²分数评估预测能力。</p>\n<p> - 置换检验（100次）检验统计显著性（p值）。</p>\n<p> - Allan方差分析长时状态转移信噪比。</p>\n<p> - 时间一致性（cosine similarity）测量嵌入稳定性。</p>"
  },
  {
    "date": "2025-09-29",
    "title": "PhysiAgent: An Embodied Agent Framework in Physical World",
    "link": "http://arxiv.org/abs/2509.24524",
    "summary_markdown": "# 论文研究单位\n\n根据论文致谢部分，该研究由无锡应用技术研究院、清华大学等机构支持，具体包括Wuxi Research Institute of Applied Technologies, Tsinghua University under Grant 20242001120。\n# 论文概述\n\n本文提出了PhysiAgent框架，一个针对物理世界优化的具身智能体框架，旨在解决视觉-语言-动作（VLA）模型在有限泛化能力方面的问题。该框架通过整合监控、记忆、自反思机制以及轻量级工具箱，使VLMs能够根据VLAs的实时性能反馈动态组织不同组件，从而最大化VLAs的能力。实验在真实世界机器人操控任务中验证了显著的任务解决性能提升，展现了有效的自我调节能力和自适应演化特性。\n# 论文核心贡献点\n\n1. **提出PhysiAgent框架**：一个训练无关的模块化具身智能体框架，能够灵活集成VLMs和VLAs用于真实世界部署\n\n2. **将智能体范式引入物理世界**：将传统在语言或模拟领域探索的智能体范式引入物理世界，为VLMs配备真实世界感知和工具使用能力\n\n3. **真实世界验证**：在真实世界机器人操控任务中验证框架，展现涌现的自我反思能力和显著的任务性能提升\n# 论文方法描述\n\nPhysiAgent采用统一的自主脚手架机制，包含五个关键组件：\n\n- **规划器（Planner）**：将原始语言指令分解为适合VLAs的可执行中间指令\n- **监控器（Monitor）**：持续跟踪VLA执行进度，评估任务进展状态（阻碍、进行中、失败、完成）\n- **反思器（Reflector）**：作为验证层改善监控精度，生成视觉约束纠正误判\n- **记忆（Memory）**：包括短期记忆（存储步骤级数据）和长期记忆（存储高级摘要），支持持续适应\n- **工具箱（Toolbox）**：提供感知、控制和推理工具，增强系统鲁棒性和适应性\n\n框架实现双向信息流：从VLMs到VLAs的前向流和从VLAs行为到VLMs的后向反馈，使VLMs能够根据实时反馈调整输出。\n# 论文使用数据集和训练资源\n\n**数据集**：\n- 真实世界桌面操控数据集，包含5个任务：put broccoli on plate、put mushroom on plate、put sausage on plate、put shrimp on plate、put chips on plate\n- 每个任务包含150个人类远程操控演示数据\n\n**训练资源**：\n- Diffusion Policy：使用4个NVIDIA A800 GPU训练120万步，27小时，批大小64，学习率0.0003\n- RDT-1B：使用8个A800 GPU训练5万步，20小时，批大小64，学习率0.0001\n# 论文使用的评估环境和评估指标\n\n**评估环境**：\n- 硬件：AIRBOT 6-DOF机械臂配抓取器\n- 相机设置：三个RGB相机分别位于顶部、前方和手腕安装位置\n- 测试任务包括三个不同复杂度的桌面操控任务：获取含膳食纤维食物、获取含蛋白质和脂肪食物、烹饪一餐（需要5个步骤）\n\n**评估指标**：\n- 累积任务进度（Y轴）vs VLA步骤（X轴）\n- 任务成功率\n- 执行效率（完成任务所需的最少步骤）\n- 性能对比：与vanilla VLA策略、传统层级框架、人机交互层级方法进行对比\n- 平均5次独立试验的结果展示",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>根据论文致谢部分，该研究由无锡应用技术研究院、清华大学等机构支持，具体包括Wuxi Research Institute of Applied Technologies, Tsinghua University under Grant 20242001120。</p>\n<h1>论文概述</h1>\n\n<p>本文提出了PhysiAgent框架，一个针对物理世界优化的具身智能体框架，旨在解决视觉-语言-动作（VLA）模型在有限泛化能力方面的问题。该框架通过整合监控、记忆、自反思机制以及轻量级工具箱，使VLMs能够根据VLAs的实时性能反馈动态组织不同组件，从而最大化VLAs的能力。实验在真实世界机器人操控任务中验证了显著的任务解决性能提升，展现了有效的自我调节能力和自适应演化特性。</p>\n<h1>论文核心贡献点</h1>\n\n<p>1. <strong>提出PhysiAgent框架</strong>：一个训练无关的模块化具身智能体框架，能够灵活集成VLMs和VLAs用于真实世界部署</p>\n\n<p>2. <strong>将智能体范式引入物理世界</strong>：将传统在语言或模拟领域探索的智能体范式引入物理世界，为VLMs配备真实世界感知和工具使用能力</p>\n\n<p>3. <strong>真实世界验证</strong>：在真实世界机器人操控任务中验证框架，展现涌现的自我反思能力和显著的任务性能提升</p>\n<h1>论文方法描述</h1>\n\n<p>PhysiAgent采用统一的自主脚手架机制，包含五个关键组件：</p>\n\n<ul><li><strong>规划器（Planner）</strong>：将原始语言指令分解为适合VLAs的可执行中间指令</li><li><strong>监控器（Monitor）</strong>：持续跟踪VLA执行进度，评估任务进展状态（阻碍、进行中、失败、完成）</li><li><strong>反思器（Reflector）</strong>：作为验证层改善监控精度，生成视觉约束纠正误判</li><li><strong>记忆（Memory）</strong>：包括短期记忆（存储步骤级数据）和长期记忆（存储高级摘要），支持持续适应</li><li><strong>工具箱（Toolbox）</strong>：提供感知、控制和推理工具，增强系统鲁棒性和适应性</li></ul>\n\n<p>框架实现双向信息流：从VLMs到VLAs的前向流和从VLAs行为到VLMs的后向反馈，使VLMs能够根据实时反馈调整输出。</p>\n<h1>论文使用数据集和训练资源</h1>\n\n<p><strong>数据集</strong>：</p>\n<ul><li>真实世界桌面操控数据集，包含5个任务：put broccoli on plate、put mushroom on plate、put sausage on plate、put shrimp on plate、put chips on plate</li><li>每个任务包含150个人类远程操控演示数据</li></ul>\n\n<p><strong>训练资源</strong>：</p>\n<ul><li>Diffusion Policy：使用4个NVIDIA A800 GPU训练120万步，27小时，批大小64，学习率0.0003</li><li>RDT-1B：使用8个A800 GPU训练5万步，20小时，批大小64，学习率0.0001</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<p><strong>评估环境</strong>：</p>\n<ul><li>硬件：AIRBOT 6-DOF机械臂配抓取器</li><li>相机设置：三个RGB相机分别位于顶部、前方和手腕安装位置</li><li>测试任务包括三个不同复杂度的桌面操控任务：获取含膳食纤维食物、获取含蛋白质和脂肪食物、烹饪一餐（需要5个步骤）</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li>累积任务进度（Y轴）vs VLA步骤（X轴）</li><li>任务成功率</li><li>执行效率（完成任务所需的最少步骤）</li><li>性能对比：与vanilla VLA策略、传统层级框架、人机交互层级方法进行对比</li><li>平均5次独立试验的结果展示</li></ul>"
  },
  {
    "date": "2025-09-28",
    "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models",
    "link": "http://arxiv.org/abs/2509.23655",
    "summary_markdown": "# 论文研究单位\nCentre for Artificial Intelligence, UCL 和 Qualcomm AI Research\n# 论文概述\n针对视觉-语言-动作模型（VLA）训练计算成本高的问题，论文提出了Oat-VLA（Object-Agent-centric Tokenization for VLAs）方法。该方法通过对象中心和智能体中心的标记化策略，将视觉输入的标记数量从传统的256个大幅减少到16个，同时保持或提升模型性能。实验表明，Oat-VLA在LIBERO基准上的收敛速度比OpenVLA快2倍以上，并在真实世界的抓取放置任务中表现更优。\n# 论文核心贡献点\n- 设计了对象中心标记化方法，将场景中对象信息压缩为少量语义标记\n- 提出智能体中心标记化策略，获取机械臂末端执行器的精确视觉信息\n- Oat-VLA具有模块化和可扩展性，可重用现有VLA检查点，便于适应\n- 在LIBERO基准上实现超过2倍的训练收敛加速\n- 在真实世界抓取放置任务中展现出比OpenVLA更高的成功率\n- 视觉标记数量减少93.75%，显著降低GPU内存需求和计算成本\n# 论文方法描述\n**对象中心标记化**：使用FT-Dinosaur无监督对象中心模型提取图像分割掩码，通过平均池化将同一对象区域的视觉标记压缩为单一标记，主实验使用7个对象标记。\n\n**智能体中心标记化**：训练轻量级ResNet Faster R-CNN检测机械臂末端执行器位置，获取该位置周围3×3区域的9个视觉标记，确保精确操作。\n\n**Oat-VLA架构**：结合7个对象标记和9个智能体标记（共16个视觉标记），通过MLP投影器输入到Llama 2 LLM主干网络中，保持与OpenVLA架构兼容性。\n# 论文使用数据集和训练资源\n**数据集**：LIBERO基准测试套件（包含Spatial、Object、Goal、10四个任务套件）、Open X-Embodiment数据集子集（Bridge+FMB+Fractal）、自建真实世界数据集（320条轨迹）\n\n**训练资源**：8×H100节点，全参数微调使用批量大小8×64=512，LoRA微调使用批量大小8×48=384，从OpenVLA预训练检查点开始微调\n# 论文使用的评估环境和评估指标\n**评估环境**：LIBERO仿真环境（四个任务套件）、真实世界环境（UFACTORY xArm 6机械臂）\n\n**评估指标**：\n- LIBERO基准：成功率及标准差（每套件100次评估），与OpenVLA、Octo、Diffusion Policy对比\n- 真实世界评估：分布内/外任务成功率及详细分解\n- 训练效率：动作标记准确率vs训练时间，训练吞吐量（样本/秒）\n- 消融实验：不同标记化策略的性能对比",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Centre for Artificial Intelligence, UCL 和 Qualcomm AI Research</p>\n<h1>论文概述</h1>\n<p>针对视觉-语言-动作模型（VLA）训练计算成本高的问题，论文提出了Oat-VLA（Object-Agent-centric Tokenization for VLAs）方法。该方法通过对象中心和智能体中心的标记化策略，将视觉输入的标记数量从传统的256个大幅减少到16个，同时保持或提升模型性能。实验表明，Oat-VLA在LIBERO基准上的收敛速度比OpenVLA快2倍以上，并在真实世界的抓取放置任务中表现更优。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>设计了对象中心标记化方法，将场景中对象信息压缩为少量语义标记</li><li>提出智能体中心标记化策略，获取机械臂末端执行器的精确视觉信息</li><li>Oat-VLA具有模块化和可扩展性，可重用现有VLA检查点，便于适应</li><li>在LIBERO基准上实现超过2倍的训练收敛加速</li><li>在真实世界抓取放置任务中展现出比OpenVLA更高的成功率</li><li>视觉标记数量减少93.75%，显著降低GPU内存需求和计算成本</li></ul>\n<h1>论文方法描述</h1>\n<p><strong>对象中心标记化</strong>：使用FT-Dinosaur无监督对象中心模型提取图像分割掩码，通过平均池化将同一对象区域的视觉标记压缩为单一标记，主实验使用7个对象标记。</p>\n\n<p><strong>智能体中心标记化</strong>：训练轻量级ResNet Faster R-CNN检测机械臂末端执行器位置，获取该位置周围3×3区域的9个视觉标记，确保精确操作。</p>\n\n<p><strong>Oat-VLA架构</strong>：结合7个对象标记和9个智能体标记（共16个视觉标记），通过MLP投影器输入到Llama 2 LLM主干网络中，保持与OpenVLA架构兼容性。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p><strong>数据集</strong>：LIBERO基准测试套件（包含Spatial、Object、Goal、10四个任务套件）、Open X-Embodiment数据集子集（Bridge+FMB+Fractal）、自建真实世界数据集（320条轨迹）</p>\n\n<p><strong>训练资源</strong>：8×H100节点，全参数微调使用批量大小8×64=512，LoRA微调使用批量大小8×48=384，从OpenVLA预训练检查点开始微调</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p><strong>评估环境</strong>：LIBERO仿真环境（四个任务套件）、真实世界环境（UFACTORY xArm 6机械臂）</p>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li>LIBERO基准：成功率及标准差（每套件100次评估），与OpenVLA、Octo、Diffusion Policy对比</li><li>真实世界评估：分布内/外任务成功率及详细分解</li><li>训练效率：动作标记准确率vs训练时间，训练吞吐量（样本/秒）</li><li>消融实验：不同标记化策略的性能对比</li></ul>"
  },
  {
    "date": "2025-09-27",
    "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
    "link": "http://arxiv.org/abs/2509.23224",
    "summary_markdown": "# 论文研究单位\n\n东京大学（The University of Tokyo）\n# 论文概述\n\n大型视觉-语言-动作（VLA）模型在机器人控制中面临推理延迟的挑战。尽管这些模型能够生成连贯的行为，但延迟导致机器人在动态环境中响应迟缓。为解决这一问题，研究团队提出了异步动作块纠正（A2C2）方法，通过轻量级纠正头在每个控制步骤实时修正动作。该方法能在不重新训练基础策略的情况下，结合最新观察信息和时空位置特征，对预测的动作块进行逐步调整，从而保持闭环响应性并提升在延迟和长视野条件下的成功率。\n# 论文核心贡献点\n\n1. 首次正式定义了VLA模型生成动作块时的延迟问题，并分析了其对机器人实时控制的影响\n2. 提出了A2C2（异步动作块纠正）方法，这是一个轻量级的附加动作纠正策略，可应用于任何VLA模型\n3. 建立了基于时间位置特征的动作纠正机制，能有效处理异步执行中的观察滞后问题\n4. 在Kinetix动态任务和LIBERO Spatial基准测试中，方法在存在延迟和长执行视野条件下均实现了显著的成功率提升\n# 论文方法描述\n\nA2C2框架通过在基础策略之上添加轻量级纠正头πa2c2来扩展基于动作块的策略π。纠正头接收时间t+k时刻的观测ot+k、基础策略预测的动作a_t+k（来自之前t时刻的观察）、时间特征τ_k（位置在动作块内的相对位置）、基础策略的潜在表示zt+k，以及语言指令l。方法采用正弦嵌入表示位置特征，为动作块长度提供周期性结构。纠正头预测残差动作Δat+k，并与基础动作相加产生执行动作aexec_t+k=k。基础策略π每e步执行一次推理，而纠正头运行在每个控制步骤。训练过程中，使用均方误差（MSE）损失函数优化纠正头，通过对比专家演示的目标动作与基础策略输出之间的差异来学习残差预测。方法兼容现有的演示数据集，无需强化学习微调，并且可作为独立插件集成到任何VLA模型中。\n# 论文使用数据集和训练资源\n\n1. **Kinetix动态任务套件**：包含12个高度动态的操控和运动任务，使用1百万步专家演示数据，由RPO训练的专家策略生成\n2. **LIBERO Spatial基准**：提供432个演示和52,970帧数据，涵盖10个空间操控任务，包含顶部和手腕RGB图像、状态信息和语言指令\n3. **训练设置**：Kinetix环境使用flow-matching策略作为基础，纠正头为3层MLP（0.31M参数）；LIBERO使用SmolVLA作为基础（450M参数），纠正头结合6层transformer编码器和MLP（32M参数）\n# 论文使用的评估环境和评估指标\n\n1. **评估环境**：\n - Kinetix仿真环境（12个动态任务）：包括汽车发射、倒立推力投石、捕捉游戏、链式着陆器、抓取、半人马独轮车、硬着陆器、半豹式游泳者、豹式行走和蹦床等场景\n - LIBERO Spatial真实机器人基准：包含10个需要精细空间推理的操控任务\n\n2. **评估指标**：\n - **成功率**：通过多次试验计算任务成功完成的百分比，在不同延迟d和执行视野e的组合下进行评估\n - **延迟敏感性**：分析推理延迟从0到4步变化时的性能下降\n - **视野稳定性**：研究执行视野从1到50步对性能的影响\n - **性能对比**：与Naive async和RTC（Real Time Chunking）基线方法进行对比，评估A2C2的改进效果",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>东京大学（The University of Tokyo）</p>\n<h1>论文概述</h1>\n\n<p>大型视觉-语言-动作（VLA）模型在机器人控制中面临推理延迟的挑战。尽管这些模型能够生成连贯的行为，但延迟导致机器人在动态环境中响应迟缓。为解决这一问题，研究团队提出了异步动作块纠正（A2C2）方法，通过轻量级纠正头在每个控制步骤实时修正动作。该方法能在不重新训练基础策略的情况下，结合最新观察信息和时空位置特征，对预测的动作块进行逐步调整，从而保持闭环响应性并提升在延迟和长视野条件下的成功率。</p>\n<h1>论文核心贡献点</h1>\n\n<ol><li>首次正式定义了VLA模型生成动作块时的延迟问题，并分析了其对机器人实时控制的影响</li><li>提出了A2C2（异步动作块纠正）方法，这是一个轻量级的附加动作纠正策略，可应用于任何VLA模型</li><li>建立了基于时间位置特征的动作纠正机制，能有效处理异步执行中的观察滞后问题</li><li>在Kinetix动态任务和LIBERO Spatial基准测试中，方法在存在延迟和长执行视野条件下均实现了显著的成功率提升</li></ol>\n<h1>论文方法描述</h1>\n\n<p>A2C2框架通过在基础策略之上添加轻量级纠正头πa2c2来扩展基于动作块的策略π。纠正头接收时间t+k时刻的观测ot+k、基础策略预测的动作a_t+k（来自之前t时刻的观察）、时间特征τ_k（位置在动作块内的相对位置）、基础策略的潜在表示zt+k，以及语言指令l。方法采用正弦嵌入表示位置特征，为动作块长度提供周期性结构。纠正头预测残差动作Δat+k，并与基础动作相加产生执行动作aexec_t+k=k。基础策略π每e步执行一次推理，而纠正头运行在每个控制步骤。训练过程中，使用均方误差（MSE）损失函数优化纠正头，通过对比专家演示的目标动作与基础策略输出之间的差异来学习残差预测。方法兼容现有的演示数据集，无需强化学习微调，并且可作为独立插件集成到任何VLA模型中。</p>\n<h1>论文使用数据集和训练资源</h1>\n\n<ol><li><strong>Kinetix动态任务套件</strong>：包含12个高度动态的操控和运动任务，使用1百万步专家演示数据，由RPO训练的专家策略生成</li><li><strong>LIBERO Spatial基准</strong>：提供432个演示和52,970帧数据，涵盖10个空间操控任务，包含顶部和手腕RGB图像、状态信息和语言指令</li><li><strong>训练设置</strong>：Kinetix环境使用flow-matching策略作为基础，纠正头为3层MLP（0.31M参数）；LIBERO使用SmolVLA作为基础（450M参数），纠正头结合6层transformer编码器和MLP（32M参数）</li></ol>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<p>1. <strong>评估环境</strong>：</p>\n<p> - Kinetix仿真环境（12个动态任务）：包括汽车发射、倒立推力投石、捕捉游戏、链式着陆器、抓取、半人马独轮车、硬着陆器、半豹式游泳者、豹式行走和蹦床等场景</p>\n<p> - LIBERO Spatial真实机器人基准：包含10个需要精细空间推理的操控任务</p>\n\n<p>2. <strong>评估指标</strong>：</p>\n<p> - <strong>成功率</strong>：通过多次试验计算任务成功完成的百分比，在不同延迟d和执行视野e的组合下进行评估</p>\n<p> - <strong>延迟敏感性</strong>：分析推理延迟从0到4步变化时的性能下降</p>\n<p> - <strong>视野稳定性</strong>：研究执行视野从1到50步对性能的影响</p>\n<p> - <strong>性能对比</strong>：与Naive async和RTC（Real Time Chunking）基线方法进行对比，评估A2C2的改进效果</p>"
  },
  {
    "date": "2025-09-27",
    "title": "Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges",
    "link": "http://arxiv.org/abs/2509.23121",
    "summary_markdown": "## 论文研究单位\n\n辽宁辽河实验室（项目编号：LLL24ZZ-02-01 和 LLL24ZZ-02-02）\n## 论文概述\n\n该论文评估了视觉语言动作（VLA）模型在工业环境中的适用性。研究从工业部署的角度比较了现有SOTA VLA模型在工业场景中的性能，并从数据收集和模型架构两个角度分析了VLA模型在现实工业部署中的局限性。研究发现，当前VLA模型在复杂工业环境、多样化物体类别和高精度放置任务方面仍有很大改进空间。\n## 论文核心贡献点\n\n- 评估了最先进的VLA模型在工业场景中的拾取和放置任务性能\n- 从数据集和模型架构两个角度分析VLA模型对非结构化环境的适应性\n- 讨论了提高VLA模型鲁棒性和任务泛化能力的潜在方向\n- 提供了VLA模型在工业应用中的实用见解\n## 论文方法描述\n\n提出了VLA的通用技术框架，包含以下关键技术模块：\n- 归一化和反归一化：消除不同任务或机器人平台间的维度不一致性\n- 数据增强：包括图像增强（随机裁剪、颜色抖动、图像损坏等）和指令增强（提示变化、释义等）\n- 投影器：连接视觉编码器和LLM的桥梁，进行维度对齐和语义映射\n- 预训练VLM：包括视觉编码器（如DINOv2、SigLIP）和语言编码器（如LLaMA、Gemma、Qwen2-VL）\n- 政策头：三种主要架构类型包括自回归、扩散和混合模型\n## 论文使用数据集和训练资源\n\n- 使用来自真实工业场景采集的回合数据进行模型微调，每个任务使用100个回合\n- 在NVIDIA H20 96GB GPU上训练10小时\n- 使用Ubuntu 20.04和ROS 1 noetic系统\n- 在Mobile ALOHA双机械臂机器人上进行所有测试\n## 论文使用的评估环境和评估指标\n\n- 评估环境：三个工业场景，涵盖视觉遮挡、相机抖动、目标姿态随机化和目标多样性等扰动\n- 主要指标：成功率（%），计算方法为成功试验次数除以总试验次数\n- 精度指标：位置误差（2.2 cm）和方向误差（12.4°）\n- 实验设置：每个试验包含10次测试，涵盖目标姿态随机化和多样化物体类别\n- 评估结果：Pi0模型经过微调后，在简单抓取任务上达到约60%的成功率，但在高精度放置任务中精度仍有不足",
    "summary_html": "<h2>论文研究单位</h2>\n\n<p>辽宁辽河实验室（项目编号：LLL24ZZ-02-01 和 LLL24ZZ-02-02）</p>\n<h2>论文概述</h2>\n\n<p>该论文评估了视觉语言动作（VLA）模型在工业环境中的适用性。研究从工业部署的角度比较了现有SOTA VLA模型在工业场景中的性能，并从数据收集和模型架构两个角度分析了VLA模型在现实工业部署中的局限性。研究发现，当前VLA模型在复杂工业环境、多样化物体类别和高精度放置任务方面仍有很大改进空间。</p>\n<h2>论文核心贡献点</h2>\n\n<ul><li>评估了最先进的VLA模型在工业场景中的拾取和放置任务性能</li><li>从数据集和模型架构两个角度分析VLA模型对非结构化环境的适应性</li><li>讨论了提高VLA模型鲁棒性和任务泛化能力的潜在方向</li><li>提供了VLA模型在工业应用中的实用见解</li></ul>\n<h2>论文方法描述</h2>\n\n<p>提出了VLA的通用技术框架，包含以下关键技术模块：</p>\n<ul><li>归一化和反归一化：消除不同任务或机器人平台间的维度不一致性</li><li>数据增强：包括图像增强（随机裁剪、颜色抖动、图像损坏等）和指令增强（提示变化、释义等）</li><li>投影器：连接视觉编码器和LLM的桥梁，进行维度对齐和语义映射</li><li>预训练VLM：包括视觉编码器（如DINOv2、SigLIP）和语言编码器（如LLaMA、Gemma、Qwen2-VL）</li><li>政策头：三种主要架构类型包括自回归、扩散和混合模型</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n\n<ul><li>使用来自真实工业场景采集的回合数据进行模型微调，每个任务使用100个回合</li><li>在NVIDIA H20 96GB GPU上训练10小时</li><li>使用Ubuntu 20.04和ROS 1 noetic系统</li><li>在Mobile ALOHA双机械臂机器人上进行所有测试</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n\n<ul><li>评估环境：三个工业场景，涵盖视觉遮挡、相机抖动、目标姿态随机化和目标多样性等扰动</li><li>主要指标：成功率（%），计算方法为成功试验次数除以总试验次数</li><li>精度指标：位置误差（2.2 cm）和方向误差（12.4°）</li><li>实验设置：每个试验包含10次测试，涵盖目标姿态随机化和多样化物体类别</li><li>评估结果：Pi0模型经过微调后，在简单抓取任务上达到约60%的成功率，但在高精度放置任务中精度仍有不足</li></ul>"
  },
  {
    "date": "2025-09-26",
    "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search",
    "link": "http://arxiv.org/abs/2509.22643",
    "summary_markdown": "# 论文研究单位\n南洋理工大学（NTU）、清华大学深圳国际研究生院（Tsinghua SIGS）、清华大学、北京邮电大学（BUP）\n# 论文概述\n现有视觉-语言-动作模型（VLA）在部署中依赖短期状态到动作的映射，易产生逐步偏差，难以完成长时序任务。本文提出VLA-Reasoner，一个可在测试时扩展的即插即用框架，通过在线蒙特卡洛树搜索（MCTS）在世界模型中前瞻未来状态并搜索最优动作，从而缓解VLA的短视问题并显著提升成功率和鲁棒性。\n# 论文核心贡献点\n提出可插拔的测试时推理框架，向任意VLA注入树搜索与未来前瞻能力，无需重新训练底层策略；提出KDE先验的高效候选采样与基于图像的离线奖励塑形，用以在MCTS中实现高效扩展与密集反馈；通过线性融合（式1）将MCTS输出的推理动作与VLA原动作结合，完成即时动作优化与长时指导。\n# 论文方法描述\n问题形式化：给定观测o_t与语言指令l，VLA生成a_t^VLA；VLA-Reasoner在世界模型W中模拟未来轨迹，并用MCTS在动作空间搜索最优a_t^Reasoner，最终执行a_t=α·a_t^VLA+(1−α)·a_t^Reasoner，α为注入强度。\n\n在线MCTS：围绕VLA预测作为根节点展开。四步循环为\n- 扩展（Expansion）：在世界模型中进行动作采样与扩展；扩展后的状态通过世界模型W(a_i,o_i)→o_{i+1}得到下一状态。\n- 模拟（Simulation）：利用动作感知的世界模型生成未来状态以评估当前动作的影响。\n- 回传（Backpropagation）：自叶向根更新节点值Q与访问次数N，用到KDE概率密度近似N(a)以降低开销。\n- 选择（Selection）：采用UCB（Upper Confidence Bound）在价值与访问次数间平衡选点。\n\nKDE高效采样：用离线机器人演示训练KDE核密度估计得到动作分布π^KDE，从该分布采样候选并在Top-k近邻内扩展，避免重复调用VLA并维持探索多样性；支持对动作块（chunk）进行整体采样。\n\n视觉奖励塑形：为获得密集且稳健的中间反馈，用ResNet-34图像编码器+两层MLP（MSE）在降采样后的离线轨迹上学习从状态到奖励的映射r=MLP(o)，以奖励为导向修正MCTS中的轨迹选择。\n\n算法伪代码与实现：完整给出从根节点初始化、循环扩展/模拟/回传/选择、选出最优子节点并计算a_t^Reasoner、以及最终动作注入的流程。除世界模型外各组件基于与VLA微调相同的数据进行训练；额外收集少量失败演示用于世界模型微调以提升失败情形下的预测能力。\n# 论文使用数据集和训练资源\n- 仿真基准：LIBERO（Spatial/Goal/Object/Long四套任务，共约500条演示/套）和SimplerEnv（Block/Spoon/Carrot/Eggplant四任务）；包含公共机器人演示数据集与仿真器生成轨迹。\n- 基础策略：OpenVLA-7B、Octo-Small（27M）、SpatialVLA（4B）等。\n- 世界模型：iVideoGPT架构的动作感知世界模型（600M参数）。\n- 训练资源：服务器6×NVIDIA RTX 6000 GPU进行训练；真实世界推理使用NVIDIA RTX 4090。\n- KDE与奖励网络：在与VLA微调相同的离线数据集上训练；KDE用于动作先验采样，奖励网络以ResNet-34+MSE学习图像至奖励的映射；为覆盖失败情形另行收集少量失败演示用于世界模型微调。\n# 论文使用的评估环境和评估指标\n- 仿真评估：在LIBERO与SimplerEnv上进行定量评估，指标为平均成功率（每套任务各500/100回合）。\n- 真实世界评估：在Galaxea-A1机械臂上完成5项任务（Block、Fruit、1 Cup、2 Cups、Circle），侧视与腕部摄像头提供视觉输入（OpenVLA不使用腕部图像），每项任务进行20回合评估。\n- 消融实验：考察注入强度α（0.6最优）以及关键组件（KDE采样vs高斯噪声；图像奖励塑形vs token式奖励头）对成功率的影响。\n- 核心结论：VLA-Reasoner在仿真与真实世界均显著提升成功率与鲁棒性，优于多种强基线VLA，表明测试时树搜索与未来预测能有效缓解短期偏差并增强长时任务执行能力。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>南洋理工大学（NTU）、清华大学深圳国际研究生院（Tsinghua SIGS）、清华大学、北京邮电大学（BUP）</p>\n<h1>论文概述</h1>\n<p>现有视觉-语言-动作模型（VLA）在部署中依赖短期状态到动作的映射，易产生逐步偏差，难以完成长时序任务。本文提出VLA-Reasoner，一个可在测试时扩展的即插即用框架，通过在线蒙特卡洛树搜索（MCTS）在世界模型中前瞻未来状态并搜索最优动作，从而缓解VLA的短视问题并显著提升成功率和鲁棒性。</p>\n<h1>论文核心贡献点</h1>\n<p>提出可插拔的测试时推理框架，向任意VLA注入树搜索与未来前瞻能力，无需重新训练底层策略；提出KDE先验的高效候选采样与基于图像的离线奖励塑形，用以在MCTS中实现高效扩展与密集反馈；通过线性融合（式1）将MCTS输出的推理动作与VLA原动作结合，完成即时动作优化与长时指导。</p>\n<h1>论文方法描述</h1>\n<p>问题形式化：给定观测o_t与语言指令l，VLA生成a_t^VLA；VLA-Reasoner在世界模型W中模拟未来轨迹，并用MCTS在动作空间搜索最优a_t^Reasoner，最终执行a_t=α·a_t^VLA+(1−α)·a_t^Reasoner，α为注入强度。</p>\n\n<p>在线MCTS：围绕VLA预测作为根节点展开。四步循环为</p>\n<ul><li>扩展（Expansion）：在世界模型中进行动作采样与扩展；扩展后的状态通过世界模型W(a_i,o_i)→o_{i+1}得到下一状态。</li><li>模拟（Simulation）：利用动作感知的世界模型生成未来状态以评估当前动作的影响。</li><li>回传（Backpropagation）：自叶向根更新节点值Q与访问次数N，用到KDE概率密度近似N(a)以降低开销。</li><li>选择（Selection）：采用UCB（Upper Confidence Bound）在价值与访问次数间平衡选点。</li></ul>\n\n<p>KDE高效采样：用离线机器人演示训练KDE核密度估计得到动作分布π^KDE，从该分布采样候选并在Top-k近邻内扩展，避免重复调用VLA并维持探索多样性；支持对动作块（chunk）进行整体采样。</p>\n\n<p>视觉奖励塑形：为获得密集且稳健的中间反馈，用ResNet-34图像编码器+两层MLP（MSE）在降采样后的离线轨迹上学习从状态到奖励的映射r=MLP(o)，以奖励为导向修正MCTS中的轨迹选择。</p>\n\n<p>算法伪代码与实现：完整给出从根节点初始化、循环扩展/模拟/回传/选择、选出最优子节点并计算a_t^Reasoner、以及最终动作注入的流程。除世界模型外各组件基于与VLA微调相同的数据进行训练；额外收集少量失败演示用于世界模型微调以提升失败情形下的预测能力。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>仿真基准：LIBERO（Spatial/Goal/Object/Long四套任务，共约500条演示/套）和SimplerEnv（Block/Spoon/Carrot/Eggplant四任务）；包含公共机器人演示数据集与仿真器生成轨迹。</li><li>基础策略：OpenVLA-7B、Octo-Small（27M）、SpatialVLA（4B）等。</li><li>世界模型：iVideoGPT架构的动作感知世界模型（600M参数）。</li><li>训练资源：服务器6×NVIDIA RTX 6000 GPU进行训练；真实世界推理使用NVIDIA RTX 4090。</li><li>KDE与奖励网络：在与VLA微调相同的离线数据集上训练；KDE用于动作先验采样，奖励网络以ResNet-34+MSE学习图像至奖励的映射；为覆盖失败情形另行收集少量失败演示用于世界模型微调。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>仿真评估：在LIBERO与SimplerEnv上进行定量评估，指标为平均成功率（每套任务各500/100回合）。</li><li>真实世界评估：在Galaxea-A1机械臂上完成5项任务（Block、Fruit、1 Cup、2 Cups、Circle），侧视与腕部摄像头提供视觉输入（OpenVLA不使用腕部图像），每项任务进行20回合评估。</li><li>消融实验：考察注入强度α（0.6最优）以及关键组件（KDE采样vs高斯噪声；图像奖励塑形vs token式奖励头）对成功率的影响。</li><li>核心结论：VLA-Reasoner在仿真与真实世界均显著提升成功率与鲁棒性，优于多种强基线VLA，表明测试时树搜索与未来预测能有效缓解短期偏差并增强长时任务执行能力。</li></ul>"
  },
  {
    "date": "2025-09-26",
    "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation",
    "link": "http://arxiv.org/abs/2509.22441",
    "summary_markdown": "### 论文研究单位\n西湖大学工程学院（主单位），浙江大学信息与电子工程学院、环境与资源学院，澳大利亚国立大学。\n### 论文概述\n提出首个针对自主水下航行器（AUV）的视觉-语言-动作（VLA）框架 UnderwaterVLA，旨在解决海洋环境的极端挑战（水动力学干扰、感知退化、通信受限）。该框架首次将VLA模型应用于水下机器人，通过双脑架构解耦高层推理与低层控制，实现零数据训练和水动力实时补偿，显著提升导航精度与任务成功率。\n### 论文核心贡献点\n1. **首个水下VLA架构**：系统性解决VLA在水下应用的层级、数据需求和动力学补偿三大核心挑战。\n2. **零数据训练策略**：通过预训练多模态模型的迁移学习与物理先验融合，规避昂贵的水下演示数据。\n3. **双脑解耦控制**：云脑负责长时域任务规划，小脑实现实时感知-控制闭环，在通信/算力受限下保障鲁棒性。\n4. **水动力实时补偿**：在MPC中嵌入流体动力学模型，通过在线参数估计自适应复杂水流环境。\n### 论文方法描述\n1. **双脑架构**：\n - **云脑**：Surfacing时运行QVQmax模型，将高层指令分解为序列化子任务（如“右转躲避障碍→直行→低速接近”）。\n - **小脑**：本地Qwen-VL模型循环执行JSON格式控制指令，动态调整决策（如检测安全距离后自主终止任务）。\n - 结构示例：`云脑计划 → 子任务序列 → 本地执行（感知-动作循环）`。\n\n2. **提示工程与可解释性**：\n - 强制云脑/小脑输出思维链（CoT）推理过程，记录决策逻辑。\n - 小脑输出标准化JSON结构：\n ```json\n {\n \"reasoning\": \"检测到左侧障碍，右转\",\n \"decision\": \"right\",\n \"velocity\": \"medium\",\n \"sub_task_done\": false\n }\n ```\n\n3. **零数据MPC水动力控制**：\n - **运动剖面**：平移/旋转动作严格分为1秒周期的加速-恒速-减速阶段（速度：0.2/0.5/0.8 m/s，角速度：0.5/1.0/1.5 rad/s）。\n - **MPC优化**：最小化代价函数同时追踪参考信号、控制力和流体阻力：\n ```math\n J = \\int_0^1 \\left[ \\beta\\\\|v(t)-v_{\\text{ref}}\\\\|^2 + \\gamma\\\\|\\tau\\\\|^2 + \\delta\\\\|F_{\\text{drag}}\\\\|^2 \\right] dt\n ```\n - **流体参数在线估计**：基于IMU数据实时估算拖拽系数：\n ```math\n \\hat{D}_v = \\frac{\\tau_v - M\\dot{v}}{v\\|v\\|}\n ```\n### 论文使用数据集和训练资源\n- **训练数据需求**：**零**（完全依赖预训练模型 + 物理先验，无需水下演示数据）\n- **基线数据对比**：QUAR-VLA等基线需262K演示数据（来源自其他机器人任务）。\n- **模型资源**：\n - 云脑：QVQmax（大语言模型）\n - 小脑：Qwen 2.5-VL-7B（多模态视觉-语言模型）\n### 论文使用的评估环境和评估指标\n1. **实验环境**：\n - **实验室测试**：控制变量水池，3垂直柱状障碍，用于导航与避障任务。\n - **真实环境模拟**：降低光照强度至海平面5%，注入硅藻土提升浊度至18 NTU（模拟近岸浑浊水域）。\n\n2. **评估指标**：\n - **量化性能**：\n - 任务成功率：对比QUAR-VLA基线，感知/导航/隧道穿越/避障任务提升19%-27%。\n - 零数据效率：基线需262K数据，UnderwaterVLA无需任何水下演示数据。\n - **鲁棒性评估**：\n - 双脑架构（DBM）vs 单脑模型（SBM）：在浑浊环境中，DBM能自主终止任务避免过冲，SBM因目标丢失持续前进导致任务失败。\n - 可视化证据：轨迹对比图显示DBM在低能见度下精确接近目标，SBM偏离路径。\n\n**结论**：通过解耦控制层级、融合物理先验和可解释推理链，UnderwaterVLA在零数据条件下实现水下机器人任务性能突破，为复杂海洋环境中的自主作业提供了可行路径。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>西湖大学工程学院（主单位），浙江大学信息与电子工程学院、环境与资源学院，澳大利亚国立大学。</p>\n<h3>论文概述</h3>\n<p>提出首个针对自主水下航行器（AUV）的视觉-语言-动作（VLA）框架 UnderwaterVLA，旨在解决海洋环境的极端挑战（水动力学干扰、感知退化、通信受限）。该框架首次将VLA模型应用于水下机器人，通过双脑架构解耦高层推理与低层控制，实现零数据训练和水动力实时补偿，显著提升导航精度与任务成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首个水下VLA架构</strong>：系统性解决VLA在水下应用的层级、数据需求和动力学补偿三大核心挑战。</li><li><strong>零数据训练策略</strong>：通过预训练多模态模型的迁移学习与物理先验融合，规避昂贵的水下演示数据。</li><li><strong>双脑解耦控制</strong>：云脑负责长时域任务规划，小脑实现实时感知-控制闭环，在通信/算力受限下保障鲁棒性。</li><li><strong>水动力实时补偿</strong>：在MPC中嵌入流体动力学模型，通过在线参数估计自适应复杂水流环境。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>双脑架构</strong>：</p>\n<p> - <strong>云脑</strong>：Surfacing时运行QVQmax模型，将高层指令分解为序列化子任务（如“右转躲避障碍→直行→低速接近”）。</p>\n<p> - <strong>小脑</strong>：本地Qwen-VL模型循环执行JSON格式控制指令，动态调整决策（如检测安全距离后自主终止任务）。</p>\n<p> - 结构示例：<code>云脑计划 → 子任务序列 → 本地执行（感知-动作循环）</code>。</p>\n\n<p>2. <strong>提示工程与可解释性</strong>：</p>\n<p> - 强制云脑/小脑输出思维链（CoT）推理过程，记录决策逻辑。</p>\n<p> - 小脑输出标准化JSON结构：</p>\n<p> ```json</p>\n<p> {</p>\n<p> \"reasoning\": \"检测到左侧障碍，右转\",</p>\n<p> \"decision\": \"right\",</p>\n<p> \"velocity\": \"medium\",</p>\n<p> \"sub_task_done\": false</p>\n<p> }</p>\n<p> ```</p>\n\n<p>3. <strong>零数据MPC水动力控制</strong>：</p>\n<p> - <strong>运动剖面</strong>：平移/旋转动作严格分为1秒周期的加速-恒速-减速阶段（速度：0.2/0.5/0.8 m/s，角速度：0.5/1.0/1.5 rad/s）。</p>\n<p> - <strong>MPC优化</strong>：最小化代价函数同时追踪参考信号、控制力和流体阻力：</p>\n<p> ```math</p>\n<p> J = \\int_0^1 \\left[ \\beta\\\\|v(t)-v_{\\text{ref}}\\\\|^2 + \\gamma\\\\|\\tau\\\\|^2 + \\delta\\\\|F_{\\text{drag}}\\\\|^2 \\right] dt</p>\n<p> ```</p>\n<p> - <strong>流体参数在线估计</strong>：基于IMU数据实时估算拖拽系数：</p>\n<p> ```math</p>\n<p> \\hat{D}_v = \\frac{\\tau_v - M\\dot{v}}{v\\|v\\|}</p>\n<p> ```</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>训练数据需求</strong>：<strong>零</strong>（完全依赖预训练模型 + 物理先验，无需水下演示数据）</li><li><strong>基线数据对比</strong>：QUAR-VLA等基线需262K演示数据（来源自其他机器人任务）。</li><li><strong>模型资源</strong>：</li></ul>\n<p> - 云脑：QVQmax（大语言模型）</p>\n<p> - 小脑：Qwen 2.5-VL-7B（多模态视觉-语言模型）</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>实验环境</strong>：</p>\n<p> - <strong>实验室测试</strong>：控制变量水池，3垂直柱状障碍，用于导航与避障任务。</p>\n<p> - <strong>真实环境模拟</strong>：降低光照强度至海平面5%，注入硅藻土提升浊度至18 NTU（模拟近岸浑浊水域）。</p>\n\n<p>2. <strong>评估指标</strong>：</p>\n<p> - <strong>量化性能</strong>：</p>\n<p> - 任务成功率：对比QUAR-VLA基线，感知/导航/隧道穿越/避障任务提升19%-27%。</p>\n<p> - 零数据效率：基线需262K数据，UnderwaterVLA无需任何水下演示数据。</p>\n<p> - <strong>鲁棒性评估</strong>：</p>\n<p> - 双脑架构（DBM）vs 单脑模型（SBM）：在浑浊环境中，DBM能自主终止任务避免过冲，SBM因目标丢失持续前进导致任务失败。</p>\n<p> - 可视化证据：轨迹对比图显示DBM在低能见度下精确接近目标，SBM偏离路径。</p>\n\n<p><strong>结论</strong>：通过解耦控制层级、融合物理先验和可解释推理链，UnderwaterVLA在零数据条件下实现水下机器人任务性能突破，为复杂海洋环境中的自主作业提供了可行路径。</p>"
  },
  {
    "date": "2025-09-26",
    "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer",
    "link": "http://arxiv.org/abs/2509.22407",
    "summary_markdown": "### 论文研究单位\n- 所属机构：GigaAI、Peking University、Tsinghua University、CASIA\n### 论文概述\n- 目标：解决VLA（视觉–语言–动作）模型因真实机器人演示数据昂贵、难以扩展而导致的泛化能力不足问题\n- 核心方案：提出EMMA框架，结合生成式数据引擎与自适应训练策略，通过文本控制的视频生成扩充数据，提升策略在新外观与新环境的零样本泛化\n### 论文核心贡献点\n- 提出EMMA框架，集成DreamTransfer与AdaMix，显著提升真实世界机器人操作的泛化\n- DreamTransfer：DiT（扩散Transformer）双分支架构，融合深度与文本，实现多视角一致、几何可控的机器人操作视频生成\n- AdaMix：基于轨迹性能的自适应重采样，动态提高“困难样本”的训练权重，增强策略稳健性与泛化\n- 与SOTA相比，DreamTransfer在多视角一致性提升42%、深度一致性提升24%；真实世界零样本视觉任务相较仅用真实数据提升超过200%，AdaMix额外提升13%\n### 论文方法描述\n- DreamTransfer总体结构：主分支用于去噪潜空间视频令牌，ControlNet分支注入深度结构约束；多视角视频深度沿宽度拼接作为统一潜表示；T5文本编码器提供语义嵌入，并通过交叉注意力融合至主分支\n- 文本控制与几何一致性：用户可通过自然语言编辑前景、背景与光照，同时保持3D结构与几何合理性；多视角与深度约束共同维持跨视角与时间的一致性\n- AdaMix训练策略：初期使用高质量视频（低质量样本权重为零），稳定训练；在收敛后根据三类指标动态调整采样权重，聚焦“困难样本”\n - 指标：动作预测误差MSE（负号）、轨迹平滑度（角加速度二阶差分负号）、关节限位（越界为0，否则为1）\n - 组合得分：归一化后取平均\n - 更新权重：p(i) ∝ γ + λ·(1 − s_i)，保证最小支持并强调困难样本\n- 数据混合与筛选：真实与生成数据按比例混合训练；生成视频以多视角一致性、深度一致性与文本–视频相似度进行过滤与评分\n- 训练细节：两阶段微调（低分辨率稳定多视角一致性 → 高分辨率细节恢复）；优化器AdamW；分任务（折叠布料、清洁桌面、投掷瓶子）不同步数配置\n### 论文使用数据集和训练资源\n- 真实演示：折叠布料（50条）、清洁桌面（20条）、投掷瓶子（20条）\n- 生成数据：Agibot World与NVIDIA Isaac Sim采集；基于Agibot World构建5万条多视角视频（含对齐RGB、时间一致深度与模板化文本描述）\n- 生成流程：多视角深度一致性由Video Depth Anything估计；前景/背景/光照描述由Qwen2.5-VL-7B生成\n- 仿真演示：NVIDIA Isaac Sim采集，DreamTransfer执行视间一致的风格与外观转移\n- 训练硬件与平台：AgileX CobotMagic，双PiPER臂，三台Intel RealSense D435i摄像头；Isaac Sim用于仿真\n- 两阶段微调配置：阶段一576×128分辨率、批量32、3500步；阶段二1920×480分辨率、批量4、4500步\n### 论文使用的评估环境和评估指标\n- 任务：折叠布料（真实到真实）、清洁桌面/投掷瓶子（仿真到真实）\n- 视频生成质量评估\n - 多视角一致性：匹配像素数（Pix.Mat.）\n - 深度一致性：RMSE、绝对相对误差（Abs.Rel.）、平方相对误差（Sq.Rel.）\n - 文本–视频对齐：CLIP相似度（CLIPSim.）\n- 真实世界机器人评估\n - 行为得分：最大5分，按失败场景扣分（任务不同规则不同）\n - 成功率：每任务在5次试验与4种外观变化下评估，共20次运行\n - 执行质量：执行时间、轨迹平滑度（角加速度）、关节越界帧数\n- 数据混合实验：固定总量与步数，考察真实/生成比例对成功率的影响（50%为峰值）\n- 消融实验：固定混合比例（FixMix）对比AdaMix，显示AdaMix在行为得分、成功率与执行质量上的综合提升",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>所属机构：GigaAI、Peking University、Tsinghua University、CASIA</li></ul>\n<h3>论文概述</h3>\n<ul><li>目标：解决VLA（视觉–语言–动作）模型因真实机器人演示数据昂贵、难以扩展而导致的泛化能力不足问题</li><li>核心方案：提出EMMA框架，结合生成式数据引擎与自适应训练策略，通过文本控制的视频生成扩充数据，提升策略在新外观与新环境的零样本泛化</li></ul>\n<h3>论文核心贡献点</h3>\n<ul><li>提出EMMA框架，集成DreamTransfer与AdaMix，显著提升真实世界机器人操作的泛化</li><li>DreamTransfer：DiT（扩散Transformer）双分支架构，融合深度与文本，实现多视角一致、几何可控的机器人操作视频生成</li><li>AdaMix：基于轨迹性能的自适应重采样，动态提高“困难样本”的训练权重，增强策略稳健性与泛化</li><li>与SOTA相比，DreamTransfer在多视角一致性提升42%、深度一致性提升24%；真实世界零样本视觉任务相较仅用真实数据提升超过200%，AdaMix额外提升13%</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>DreamTransfer总体结构：主分支用于去噪潜空间视频令牌，ControlNet分支注入深度结构约束；多视角视频深度沿宽度拼接作为统一潜表示；T5文本编码器提供语义嵌入，并通过交叉注意力融合至主分支</li><li>文本控制与几何一致性：用户可通过自然语言编辑前景、背景与光照，同时保持3D结构与几何合理性；多视角与深度约束共同维持跨视角与时间的一致性</li><li>AdaMix训练策略：初期使用高质量视频（低质量样本权重为零），稳定训练；在收敛后根据三类指标动态调整采样权重，聚焦“困难样本”</li></ul>\n<p> - 指标：动作预测误差MSE（负号）、轨迹平滑度（角加速度二阶差分负号）、关节限位（越界为0，否则为1）</p>\n<p> - 组合得分：归一化后取平均</p>\n<p> - 更新权重：p(i) ∝ γ + λ·(1 − s_i)，保证最小支持并强调困难样本</p>\n<ul><li>数据混合与筛选：真实与生成数据按比例混合训练；生成视频以多视角一致性、深度一致性与文本–视频相似度进行过滤与评分</li><li>训练细节：两阶段微调（低分辨率稳定多视角一致性 → 高分辨率细节恢复）；优化器AdamW；分任务（折叠布料、清洁桌面、投掷瓶子）不同步数配置</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>真实演示：折叠布料（50条）、清洁桌面（20条）、投掷瓶子（20条）</li><li>生成数据：Agibot World与NVIDIA Isaac Sim采集；基于Agibot World构建5万条多视角视频（含对齐RGB、时间一致深度与模板化文本描述）</li><li>生成流程：多视角深度一致性由Video Depth Anything估计；前景/背景/光照描述由Qwen2.5-VL-7B生成</li><li>仿真演示：NVIDIA Isaac Sim采集，DreamTransfer执行视间一致的风格与外观转移</li><li>训练硬件与平台：AgileX CobotMagic，双PiPER臂，三台Intel RealSense D435i摄像头；Isaac Sim用于仿真</li><li>两阶段微调配置：阶段一576×128分辨率、批量32、3500步；阶段二1920×480分辨率、批量4、4500步</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>任务：折叠布料（真实到真实）、清洁桌面/投掷瓶子（仿真到真实）</li><li>视频生成质量评估</li></ul>\n<p> - 多视角一致性：匹配像素数（Pix.Mat.）</p>\n<p> - 深度一致性：RMSE、绝对相对误差（Abs.Rel.）、平方相对误差（Sq.Rel.）</p>\n<p> - 文本–视频对齐：CLIP相似度（CLIPSim.）</p>\n<ul><li>真实世界机器人评估</li></ul>\n<p> - 行为得分：最大5分，按失败场景扣分（任务不同规则不同）</p>\n<p> - 成功率：每任务在5次试验与4种外观变化下评估，共20次运行</p>\n<p> - 执行质量：执行时间、轨迹平滑度（角加速度）、关节越界帧数</p>\n<ul><li>数据混合实验：固定总量与步数，考察真实/生成比例对成功率的影响（50%为峰值）</li><li>消融实验：固定混合比例（FixMix）对比AdaMix，显示AdaMix在行为得分、成功率与执行质量上的综合提升</li></ul>"
  },
  {
    "date": "2025-09-26",
    "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training",
    "link": "http://arxiv.org/abs/2509.22199",
    "summary_markdown": "# 论文研究单位\nGigaAI; 中科院自动化研究所 (CASIA); 南京理工大学 (NJUST); 清华大学\n# 论文概述\nMimicDreamer提出一套将低成本的人类演示视频转换为可用的机器人监督数据的统一框架，针对视觉、外参视点和动作三个维度的域差异进行联合对齐，使视觉语言动作（VLA）模型能在仅使用人类演示合成数据的情况下完成少样本机器人执行，并随人类数据规模扩大而显著提升成功率。核心包括H2R Aligner（视频扩散模型的视觉对齐）、EgoStabilizer（单目视点稳定与修复）和统一的H2R动作空间与约束逆动力学求解器。\n# 论文核心贡献点\n- 提出统一的人机演示对齐框架，同时在视觉、视点与动作三个维度缩小差距，将人类视频转化为可执行的机器人监督用于可扩展VLA训练。\n- 视觉：基于视频扩散和几何先验的H2R Aligner，将人类演示动作迁移为高保真机器人臂视频；视点：EgoStabilizer通过单应变换与背景修补稳定外参视点；动作：将人手轨迹映射到机器人坐标，并以约束逆动力学生成低抖动、可行的关节指令。\n- 在六项代表性操控任务上，仅用合成数据即可实现少样本执行；随人类数据扩大，显著提升平均成功率（14.7%），优于仅用真实机器人数据的基线。\n# 论文方法描述\n- Viewpoint Stabilization（EgoStabilizer）\n - Warp Perspective：相邻帧或参考帧估计单应Ht，用RANSAC剔除异常；时间平滑得到Ht~并构造补偿Wt，对每帧进行补偿与对齐；计算跨帧最大公共可见区域并裁剪以消除黑边与视场抖动。\n - Video Inpainting：对补偿后产生的空洞与遮挡区域构造掩码Mt，采用视频修补模型进行时空传播与跨帧一致性修复，得到稳定序列。\n- Actions Alignment（H2R动作空间）\n - Human-side Normalization：将人手三维关键点变换至身体坐标系FB，并通过刚体配准到机器人基座坐标系FR，得到目标位置p*与姿态R*。\n - Orientation Treatment：因手腕接近球副而末端执行器常以工具轴滚动，只对齐俯仰/偏航（roll软遮蔽）：使用对数映射与加权矩阵WR降低roll权重。\n - IK Resolver：对每条臂a∈{L,R}，以末端位置误差、姿态加权误差与时间平滑项的二次目标，结合关节限位进行约束优化，用阻尼最小二乘（DLS）逐步求解；夹爪命令由人手开合经轻量VGG分类器与中值滤波推断。\n- Visual Alignment（H2R Aligner）\n - 训练：以真实机器人视频Vgt、环境背景Vscene与仿真前景Vsim为三路条件，经共享的冻结VAE编码为潜变量；在潜空间对目标做噪声扰动，通道拼接后输入H2R DiT进行去噪与条件融合，优化CogVideoXLoss。\n - 推理：Vsimik由IK动作在仿真重放得到；Vsceneik通过对稳定后的人类视频进行人手分割与轻微膨胀得到；目标从噪声起始，经H2R DiT去噪与VAE解码，生成合成机器人域视频；将合成视频与对应IK动作对齐形成可训练数据。\n- VLA训练\n - 初始化自π₀预训练模型，复用VLM与动作词表；在合成数据上进行后训练，并混入少量真实演示以增强真实可执行性。采用条件流匹配（CFM）目标训练动作token，输出意图级控制并投影为关节指令。训练细节与超参数见附录。\n# 论文使用数据集和训练资源\n- 数据集：EgoDex（829小时、1080p单目人演示，涵盖194任务，配有3D上身姿态）。H2R Aligner训练了24类操控，共3735样本（每样本64帧@30fps），随机裁剪至640×360并缩放至672×384，按9:1划分训练/验证。\n- 训练资源：冻结的视频VAE与文本编码器；H2R DiT可训练。π₀预训练模型作为初始化；采用AdamW优化。\n- 合成数据规模：实验逐步添加人转机数据（5→30个轨迹），在20真人+20合成的配比下达到最佳综合效果。\n# 论文使用的评估环境和评估指标\n- 评估环境：六个操控任务——Pick Bag、Clean Surface、Stack Bowls、Dry Hands、Insert Tennis、Stack Cups。任务由真实机器人执行，模仿人类演示情境。\n- 评估指标：成功率（SR）与进度成功率（PSR）。前者衡量整体任务完成度，后者衡量各子任务完成的平均比例。\n- 结果要点：\n - Few-shot：仅20真人 vs 20真人+20合成 vs 20真人+3合成三种设置，后者在平均SR/PSR上分别达85.0%/91.0%，较仅真人基线提升显著，各任务均有+10%~25% SR、+10%~32% PSR提升，并在Clean Surface与Dry Hands上达100%。\n - Scaling：随人类数据增加，所有任务的SR、PSR单调上升，呈现快速上升后趋稳的规模化趋势；50–50配比下相对基线提升约10%量级，难度高的任务增幅更大。\n - H2R Aligner：定性结果显示合成视频具备逼真的机械臂外观、一致的接触几何与背景保持。\n - EgoStabilizer：稳定性（Jitter RMS、Stability）与几何一致性（H-RMSE）均显著提升；平均稳定性下降21.9%，Jitter RMS下降13.1%，几何误差仅小幅下降3.3%。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>GigaAI; 中科院自动化研究所 (CASIA); 南京理工大学 (NJUST); 清华大学</p>\n<h1>论文概述</h1>\n<p>MimicDreamer提出一套将低成本的人类演示视频转换为可用的机器人监督数据的统一框架，针对视觉、外参视点和动作三个维度的域差异进行联合对齐，使视觉语言动作（VLA）模型能在仅使用人类演示合成数据的情况下完成少样本机器人执行，并随人类数据规模扩大而显著提升成功率。核心包括H2R Aligner（视频扩散模型的视觉对齐）、EgoStabilizer（单目视点稳定与修复）和统一的H2R动作空间与约束逆动力学求解器。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出统一的人机演示对齐框架，同时在视觉、视点与动作三个维度缩小差距，将人类视频转化为可执行的机器人监督用于可扩展VLA训练。</li><li>视觉：基于视频扩散和几何先验的H2R Aligner，将人类演示动作迁移为高保真机器人臂视频；视点：EgoStabilizer通过单应变换与背景修补稳定外参视点；动作：将人手轨迹映射到机器人坐标，并以约束逆动力学生成低抖动、可行的关节指令。</li><li>在六项代表性操控任务上，仅用合成数据即可实现少样本执行；随人类数据扩大，显著提升平均成功率（14.7%），优于仅用真实机器人数据的基线。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>Viewpoint Stabilization（EgoStabilizer）</li></ul>\n<p> - Warp Perspective：相邻帧或参考帧估计单应Ht，用RANSAC剔除异常；时间平滑得到Ht~并构造补偿Wt，对每帧进行补偿与对齐；计算跨帧最大公共可见区域并裁剪以消除黑边与视场抖动。</p>\n<p> - Video Inpainting：对补偿后产生的空洞与遮挡区域构造掩码Mt，采用视频修补模型进行时空传播与跨帧一致性修复，得到稳定序列。</p>\n<ul><li>Actions Alignment（H2R动作空间）</li></ul>\n<p> - Human-side Normalization：将人手三维关键点变换至身体坐标系FB，并通过刚体配准到机器人基座坐标系FR，得到目标位置p*与姿态R*。</p>\n<p> - Orientation Treatment：因手腕接近球副而末端执行器常以工具轴滚动，只对齐俯仰/偏航（roll软遮蔽）：使用对数映射与加权矩阵WR降低roll权重。</p>\n<p> - IK Resolver：对每条臂a∈{L,R}，以末端位置误差、姿态加权误差与时间平滑项的二次目标，结合关节限位进行约束优化，用阻尼最小二乘（DLS）逐步求解；夹爪命令由人手开合经轻量VGG分类器与中值滤波推断。</p>\n<ul><li>Visual Alignment（H2R Aligner）</li></ul>\n<p> - 训练：以真实机器人视频Vgt、环境背景Vscene与仿真前景Vsim为三路条件，经共享的冻结VAE编码为潜变量；在潜空间对目标做噪声扰动，通道拼接后输入H2R DiT进行去噪与条件融合，优化CogVideoXLoss。</p>\n<p> - 推理：Vsimik由IK动作在仿真重放得到；Vsceneik通过对稳定后的人类视频进行人手分割与轻微膨胀得到；目标从噪声起始，经H2R DiT去噪与VAE解码，生成合成机器人域视频；将合成视频与对应IK动作对齐形成可训练数据。</p>\n<ul><li>VLA训练</li></ul>\n<p> - 初始化自π₀预训练模型，复用VLM与动作词表；在合成数据上进行后训练，并混入少量真实演示以增强真实可执行性。采用条件流匹配（CFM）目标训练动作token，输出意图级控制并投影为关节指令。训练细节与超参数见附录。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：EgoDex（829小时、1080p单目人演示，涵盖194任务，配有3D上身姿态）。H2R Aligner训练了24类操控，共3735样本（每样本64帧@30fps），随机裁剪至640×360并缩放至672×384，按9:1划分训练/验证。</li><li>训练资源：冻结的视频VAE与文本编码器；H2R DiT可训练。π₀预训练模型作为初始化；采用AdamW优化。</li><li>合成数据规模：实验逐步添加人转机数据（5→30个轨迹），在20真人+20合成的配比下达到最佳综合效果。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：六个操控任务——Pick Bag、Clean Surface、Stack Bowls、Dry Hands、Insert Tennis、Stack Cups。任务由真实机器人执行，模仿人类演示情境。</li><li>评估指标：成功率（SR）与进度成功率（PSR）。前者衡量整体任务完成度，后者衡量各子任务完成的平均比例。</li><li>结果要点：</li></ul>\n<p> - Few-shot：仅20真人 vs 20真人+20合成 vs 20真人+3合成三种设置，后者在平均SR/PSR上分别达85.0%/91.0%，较仅真人基线提升显著，各任务均有+10%~25% SR、+10%~32% PSR提升，并在Clean Surface与Dry Hands上达100%。</p>\n<p> - Scaling：随人类数据增加，所有任务的SR、PSR单调上升，呈现快速上升后趋稳的规模化趋势；50–50配比下相对基线提升约10%量级，难度高的任务增幅更大。</p>\n<p> - H2R Aligner：定性结果显示合成视频具备逼真的机械臂外观、一致的接触几何与背景保持。</p>\n<p> - EgoStabilizer：稳定性（Jitter RMS、Stability）与几何一致性（H-RMSE）均显著提升；平均稳定性下降21.9%，Jitter RMS下降13.1%，几何误差仅小幅下降3.3%。</p>"
  },
  {
    "date": "2025-09-26",
    "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting",
    "link": "http://arxiv.org/abs/2509.22195",
    "summary_markdown": "## 论文总结\n### 论文研究单位\n普林斯顿大学（Department of Mechanical and Aerospace Engineering, Department of Computer Science）\n### 论文概述\n本文针对将视觉语言模型（VLM）微调为视觉语言行动模型（VLA）时存在的核心挑战——灾难性遗忘进行了深入研究。传统方法在获得机器人操作能力的同时会严重削弱VLM的基础推理和多模态理解能力。本文提出了一种数据驱动的解决方案——**VLM2VLA**，其核心思想是将机器人动作转化为自然语言描述（Actions as Language），从而最小化微调数据与预训练数据之间的分布差异。该方法使得仅通过低秩适配（LoRA）即可实现有效微调，无需架构改造或昂贵的共同训练，并显著保持了原VLM的核心能力。\n### 论文核心贡献点\n1. **动作语言化表示**：提出将低级机器人动作转换为自然语言描述，直接利用VLM预训练的知识空间，避免分布偏移。\n2. **数据标注与训练管道**：提供了一套可扩展的机器人演示数据重标注流程，将轨迹分解为子任务、运动规划和动作块三层结构。\n3. **LoRA微调验证**：证明在动作语言化的前提下，LoRA可以有效进行VLA训练并维持VLM知识。\n4. **性能验证**：通过大量VQA实验和800+真实机器人实验，证明了方法在保持推理能力和零样本泛化方面的显著优势。\n### 论文方法描述\n1. **三层行动表示 (Actions as Language)**：\n - **高层子任务预测**：根据视觉观察和指令预测下一步子任务。\n - **中层运动规划**：基于当前子任务和观察，生成动作的空间方向描述（例如“向左并稍向下”）。\n - **低层动作生成**：生成最终执行的文本化动作指令序列（例如“向前移动4.2厘米”），包含各个自由度的具体指令。\n2. **数据重构**：\n - 使用Gemini模型自动标注机器人轨迹（BridgeData v2），将原始状态-动作序列转换为带有语言描述的序列：`\\{(图像, 子任务, 运动计划, 文本化动作块)\\}`。\n3. **训练与推理**：\n - **训练**：在重构后的语言化数据上，使用LoRA对Gemma-3-12B-IT等VLM进行微调。\n - **推理**：采用闭环方式，模型首先生成完整的子任务序列；在执行每个动作块后，调用一个验证器（使用Gemini 2.5 Pro）判断任务完成度，决定继续下一子任务或重试当前任务。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - **机器人数据**：BridgeData v2 数据集的一个子集（包含主要任务指令）。\n - **多模态数据**：用于评估VQA能力（未明确说明具体数据集，可能基于其预训练知识）。\n2. **训练资源**：\n - **基础模型**：Gemma-3-12B-IT (主要), Gemma-3-4B-IT (用于对比)。\n - **标注工具**：Gemini 2.5 Pro 和 Gemini 2.5 Flash（用于成本控制）用于自动生成语言化轨迹标签。\n - **微调方法**：LoRA（应用于所有线性模块）。\n - **标注成本**：约$900。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - **机器人平台**：WidowX 250S 6自由度机械臂（用于真实世界操作）。\n - **物理场景**：标准玩具厨房环境。\n2. **评估指标**：\n - **多模态理解能力**：\n - **指标**：多个VQA基准测试的准确率（如MMMU, MMStar, MME, OCRBench, MMB-en, MMB-cn, TextVQA, DocVQA, InfoVQA, AI2D, ChartQA, RealWorldQA）。\n - **机器人操作能力**：\n - **指标**：任务成功率（每任务30次试验，多语言任务为90次）。\n - **任务类型**：\n - **域内任务 (ID)**：拾取、拾取放置。\n - **组合任务**：多步骤操作。\n - **域外任务 (OOD)**：多语言指令翻译（西班牙语、普通话、印地语）；识别流行文化概念（识别“Ash Ketchum”图像上方的物体）。",
    "summary_html": "<h2>论文总结</h2>\n<h3>论文研究单位</h3>\n<p>普林斯顿大学（Department of Mechanical and Aerospace Engineering, Department of Computer Science）</p>\n<h3>论文概述</h3>\n<p>本文针对将视觉语言模型（VLM）微调为视觉语言行动模型（VLA）时存在的核心挑战——灾难性遗忘进行了深入研究。传统方法在获得机器人操作能力的同时会严重削弱VLM的基础推理和多模态理解能力。本文提出了一种数据驱动的解决方案——<strong>VLM2VLA</strong>，其核心思想是将机器人动作转化为自然语言描述（Actions as Language），从而最小化微调数据与预训练数据之间的分布差异。该方法使得仅通过低秩适配（LoRA）即可实现有效微调，无需架构改造或昂贵的共同训练，并显著保持了原VLM的核心能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>动作语言化表示</strong>：提出将低级机器人动作转换为自然语言描述，直接利用VLM预训练的知识空间，避免分布偏移。</li><li><strong>数据标注与训练管道</strong>：提供了一套可扩展的机器人演示数据重标注流程，将轨迹分解为子任务、运动规划和动作块三层结构。</li><li><strong>LoRA微调验证</strong>：证明在动作语言化的前提下，LoRA可以有效进行VLA训练并维持VLM知识。</li><li><strong>性能验证</strong>：通过大量VQA实验和800+真实机器人实验，证明了方法在保持推理能力和零样本泛化方面的显著优势。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>三层行动表示 (Actions as Language)</strong>：</p>\n<p> - <strong>高层子任务预测</strong>：根据视觉观察和指令预测下一步子任务。</p>\n<p> - <strong>中层运动规划</strong>：基于当前子任务和观察，生成动作的空间方向描述（例如“向左并稍向下”）。</p>\n<p> - <strong>低层动作生成</strong>：生成最终执行的文本化动作指令序列（例如“向前移动4.2厘米”），包含各个自由度的具体指令。</p>\n<p>2. <strong>数据重构</strong>：</p>\n<p> - 使用Gemini模型自动标注机器人轨迹（BridgeData v2），将原始状态-动作序列转换为带有语言描述的序列：<code>\\{(图像, 子任务, 运动计划, 文本化动作块)\\}</code>。</p>\n<p>3. <strong>训练与推理</strong>：</p>\n<p> - <strong>训练</strong>：在重构后的语言化数据上，使用LoRA对Gemma-3-12B-IT等VLM进行微调。</p>\n<p> - <strong>推理</strong>：采用闭环方式，模型首先生成完整的子任务序列；在执行每个动作块后，调用一个验证器（使用Gemini 2.5 Pro）判断任务完成度，决定继续下一子任务或重试当前任务。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：</p>\n<p> - <strong>机器人数据</strong>：BridgeData v2 数据集的一个子集（包含主要任务指令）。</p>\n<p> - <strong>多模态数据</strong>：用于评估VQA能力（未明确说明具体数据集，可能基于其预训练知识）。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - <strong>基础模型</strong>：Gemma-3-12B-IT (主要), Gemma-3-4B-IT (用于对比)。</p>\n<p> - <strong>标注工具</strong>：Gemini 2.5 Pro 和 Gemini 2.5 Flash（用于成本控制）用于自动生成语言化轨迹标签。</p>\n<p> - <strong>微调方法</strong>：LoRA（应用于所有线性模块）。</p>\n<p> - <strong>标注成本</strong>：约$900。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - <strong>机器人平台</strong>：WidowX 250S 6自由度机械臂（用于真实世界操作）。</p>\n<p> - <strong>物理场景</strong>：标准玩具厨房环境。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - <strong>多模态理解能力</strong>：</p>\n<p> - <strong>指标</strong>：多个VQA基准测试的准确率（如MMMU, MMStar, MME, OCRBench, MMB-en, MMB-cn, TextVQA, DocVQA, InfoVQA, AI2D, ChartQA, RealWorldQA）。</p>\n<p> - <strong>机器人操作能力</strong>：</p>\n<p> - <strong>指标</strong>：任务成功率（每任务30次试验，多语言任务为90次）。</p>\n<p> - <strong>任务类型</strong>：</p>\n<p> - <strong>域内任务 (ID)</strong>：拾取、拾取放置。</p>\n<p> - <strong>组合任务</strong>：多步骤操作。</p>\n<p> - <strong>域外任务 (OOD)</strong>：多语言指令翻译（西班牙语、普通话、印地语）；识别流行文化概念（识别“Ash Ketchum”图像上方的物体）。</p>"
  },
  {
    "date": "2025-09-26",
    "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation",
    "link": "http://arxiv.org/abs/2509.22093",
    "summary_markdown": "# 论文研究单位\n悉尼大学计算机学院\n# 论文概述\n本文针对视觉-语言-动作（VLA）模型在机器人操作中的计算效率问题进行研究。VLA模型在处理长视野多模态上下文时，对密集视觉标记的注意力计算消耗了主要计算资源。现有方法通过减少VLA模型内的视觉冗余来优化推理速度，但忽略了机器人操作不同阶段的冗余变化。研究发现视觉标记冗余度在粗糙操作阶段高于精细操作阶段，且与动作动态密切相关。基于此观察，本文提出Action-aware Dynamic Pruning (ADP)方法，这是一个结合文本驱动标记选择与动作感知轨迹门控的多模态修剪框架。\n# 论文核心贡献点\n1. 发现VLA模型中视觉标记重要性在机器人操作不同阶段存在变化的规律，这启发设计了针对操作阶段的动态修剪方法\n2. 提出文本驱动的动作感知修剪方法，结合任务指令相关性和基于末端执行器运动的门控规则，实现修剪和全视觉状态之间的自适应切换\n3. 进行了原理性复杂性分析和广泛的仿真与真实世界实验，证明该方法在减少FLOPs和延迟的同时保持成功操作所需的精细视觉细节\n# 论文方法描述\nADP方法包含两个核心组件：\n1. 文本驱动预期修剪：通过跨模态相似性评估视觉补丁相关性，在进入深层融合前仅选择最相关标记。使用文本和视觉标记的查询-键表示计算相似度矩阵，通过多头部平均获得全局重要性分数，保留排名前k的视觉标记\n2. 动作感知动态策略：使用基于末端执行器轨迹的轻量级决策信号调制修剪决策。将每个解码的动作块视为时间窗口，计算窗口化轨迹距离（欧几里得位移），通过比较当前运动幅度与历史运动统计来动态决定是否启用修剪。当运动幅度相对较高时（粗糙阶段）启用修剪；运动幅度相对较低时（精细阶段）禁用修剪以保留完整视觉\n# 论文使用数据集和训练资源\n数据集：\n- LIBERO仿真套件（包含Spatial、Object、Goal、Long四个任务集）\n- 真实世界任务：4个涵盖抓取、放置和擦拭动作的任务\n\n训练资源：\n- 使用NVIDIA RTX 4090 GPU\n- 基于OpenVLA-OFT（7B参数）模型进行实验\n- Linux工作站环境\n# 论文使用的评估环境和评估指标\n评估环境：\n- 仿真环境：LIBERO仿真套件\n- 真实环境：Jaco2物理机器人平台\n- 硬件：配备NVIDIA RTX 4090的Linux工作站\n\n评估指标：\n- 成功率（SR, Success Rate）：在各类任务上的平均成功率\n- 计算量（FLOPs）：浮点运算次数\n- 延迟（Latency）：动作推理时间\n- 加速比（Speedup）：相对于基线方法的推理速度提升倍数",
    "summary_html": "<h1>论文研究单位</h1>\n<p>悉尼大学计算机学院</p>\n<h1>论文概述</h1>\n<p>本文针对视觉-语言-动作（VLA）模型在机器人操作中的计算效率问题进行研究。VLA模型在处理长视野多模态上下文时，对密集视觉标记的注意力计算消耗了主要计算资源。现有方法通过减少VLA模型内的视觉冗余来优化推理速度，但忽略了机器人操作不同阶段的冗余变化。研究发现视觉标记冗余度在粗糙操作阶段高于精细操作阶段，且与动作动态密切相关。基于此观察，本文提出Action-aware Dynamic Pruning (ADP)方法，这是一个结合文本驱动标记选择与动作感知轨迹门控的多模态修剪框架。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>发现VLA模型中视觉标记重要性在机器人操作不同阶段存在变化的规律，这启发设计了针对操作阶段的动态修剪方法</li><li>提出文本驱动的动作感知修剪方法，结合任务指令相关性和基于末端执行器运动的门控规则，实现修剪和全视觉状态之间的自适应切换</li><li>进行了原理性复杂性分析和广泛的仿真与真实世界实验，证明该方法在减少FLOPs和延迟的同时保持成功操作所需的精细视觉细节</li></ol>\n<h1>论文方法描述</h1>\n<p>ADP方法包含两个核心组件：</p>\n<ol><li>文本驱动预期修剪：通过跨模态相似性评估视觉补丁相关性，在进入深层融合前仅选择最相关标记。使用文本和视觉标记的查询-键表示计算相似度矩阵，通过多头部平均获得全局重要性分数，保留排名前k的视觉标记</li><li>动作感知动态策略：使用基于末端执行器轨迹的轻量级决策信号调制修剪决策。将每个解码的动作块视为时间窗口，计算窗口化轨迹距离（欧几里得位移），通过比较当前运动幅度与历史运动统计来动态决定是否启用修剪。当运动幅度相对较高时（粗糙阶段）启用修剪；运动幅度相对较低时（精细阶段）禁用修剪以保留完整视觉</li></ol>\n<h1>论文使用数据集和训练资源</h1>\n<p>数据集：</p>\n<ul><li>LIBERO仿真套件（包含Spatial、Object、Goal、Long四个任务集）</li><li>真实世界任务：4个涵盖抓取、放置和擦拭动作的任务</li></ul>\n\n<p>训练资源：</p>\n<ul><li>使用NVIDIA RTX 4090 GPU</li><li>基于OpenVLA-OFT（7B参数）模型进行实验</li><li>Linux工作站环境</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>评估环境：</p>\n<ul><li>仿真环境：LIBERO仿真套件</li><li>真实环境：Jaco2物理机器人平台</li><li>硬件：配备NVIDIA RTX 4090的Linux工作站</li></ul>\n\n<p>评估指标：</p>\n<ul><li>成功率（SR, Success Rate）：在各类任务上的平均成功率</li><li>计算量（FLOPs）：浮点运算次数</li><li>延迟（Latency）：动作推理时间</li><li>加速比（Speedup）：相对于基线方法的推理速度提升倍数</li></ul>"
  },
  {
    "date": "2025-09-26",
    "title": "Developing Vision-Language-Action Model from Egocentric Videos",
    "link": "http://arxiv.org/abs/2509.21986",
    "summary_markdown": "## 论文研究单位\n京都大学、国立情报学研究所（NII）、东京科学研究所、NII LLMC（东京）、索尼互动娱乐（东京）\n作者：Tomoya Yoshida（京都大学）、Shuhei Kurita（NII、东京科学研究所）、Taichi Nishimura（索尼互动娱乐）、Shinsuke Mori（京都大学）\n## 论文概述\n论文探讨了利用自我中心视频（egocentric videos）训练视觉-语言-动作模型（Vision-Language-Action models, VLAs）的可行性。传统方法依赖昂贵的人类远程操作数据，导致数据稀缺。论文提出通过EgoScaler框架从自我中心视频中自动提取6DoF对象操作轨迹，构建大规模预训练数据集，并在π0架构上验证其有效性。实验在仿真和真实环境中进行，结果显示该方法可显著提升任务成功率。\n## 论文核心贡献点\n1. 成功从自我中心视频训练π0模型，无需辅助标签（如手部姿态）。\n2. 构建的预训练数据集在性能上与真实机器人数据集竞争。\n3. 结合自我中心数据和真实机器人数据可进一步提升性能。\n## 论文方法描述\n- **EgoScaler框架**：用于从自我中心视频中提取6DoF对象操作轨迹。\n - 步骤：识别操作起止时间及对象；开词汇分割和3D点跟踪提取位置序列；点云配准投影到相机坐标系；奇异值分解计算旋转序列。\n- **数据处理**：应用于Ego4D、Ego-Exo4D、HD-EPIC和Nymeria四个数据集。初始提取124,559 episodes，经规则过滤（移动距离阈值、背景轨迹相似性阈值）和平滑处理后，得到45,157 episodes。\n- **训练策略**：使用π0架构预训练和后训练。动作表示为6DoF位移（平移+旋转），预训练优化MSE损失。后训练时合并真实机器人数据集并归一化动作维度。\n## 论文使用数据集和训练资源\n- **数据集**：\n - 自我中心视频数据集：Ego4D、Ego-Exo4D、HD-EPIC、Nymeria（用于预训练）。\n - 真实机器人数据集：BC-Z、BridgeData V2、Fractal（用于对比）。\n- **训练资源**：\n - 硬件：8×H200 GPUs。\n - 优化器：AdamW（学习率5×10^-5）。\n - 预训练步数：20,000步；后训练步数：40,000步。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真环境：SIMPLER（基于BridgeData V2的pick-and-place任务）。\n - 真实环境：ALOHA（语言引导的pick-and-place任务）。\n- **评估指标**：\n - 成功率：成功次数/总次数。仿真环境200次rollout；真实环境10次rollout。\n - 真实环境评分：抓取正确对象得0.5分，正确放置得0.5分。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>京都大学、国立情报学研究所（NII）、东京科学研究所、NII LLMC（东京）、索尼互动娱乐（东京）</p>\n<p>作者：Tomoya Yoshida（京都大学）、Shuhei Kurita（NII、东京科学研究所）、Taichi Nishimura（索尼互动娱乐）、Shinsuke Mori（京都大学）</p>\n<h2>论文概述</h2>\n<p>论文探讨了利用自我中心视频（egocentric videos）训练视觉-语言-动作模型（Vision-Language-Action models, VLAs）的可行性。传统方法依赖昂贵的人类远程操作数据，导致数据稀缺。论文提出通过EgoScaler框架从自我中心视频中自动提取6DoF对象操作轨迹，构建大规模预训练数据集，并在π0架构上验证其有效性。实验在仿真和真实环境中进行，结果显示该方法可显著提升任务成功率。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>成功从自我中心视频训练π0模型，无需辅助标签（如手部姿态）。</li><li>构建的预训练数据集在性能上与真实机器人数据集竞争。</li><li>结合自我中心数据和真实机器人数据可进一步提升性能。</li></ol>\n<h2>论文方法描述</h2>\n<ul><li><strong>EgoScaler框架</strong>：用于从自我中心视频中提取6DoF对象操作轨迹。</li></ul>\n<p> - 步骤：识别操作起止时间及对象；开词汇分割和3D点跟踪提取位置序列；点云配准投影到相机坐标系；奇异值分解计算旋转序列。</p>\n<ul><li><strong>数据处理</strong>：应用于Ego4D、Ego-Exo4D、HD-EPIC和Nymeria四个数据集。初始提取124,559 episodes，经规则过滤（移动距离阈值、背景轨迹相似性阈值）和平滑处理后，得到45,157 episodes。</li><li><strong>训练策略</strong>：使用π0架构预训练和后训练。动作表示为6DoF位移（平移+旋转），预训练优化MSE损失。后训练时合并真实机器人数据集并归一化动作维度。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 自我中心视频数据集：Ego4D、Ego-Exo4D、HD-EPIC、Nymeria（用于预训练）。</p>\n<p> - 真实机器人数据集：BC-Z、BridgeData V2、Fractal（用于对比）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 硬件：8×H200 GPUs。</p>\n<p> - 优化器：AdamW（学习率5×10^-5）。</p>\n<p> - 预训练步数：20,000步；后训练步数：40,000步。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真环境：SIMPLER（基于BridgeData V2的pick-and-place任务）。</p>\n<p> - 真实环境：ALOHA（语言引导的pick-and-place任务）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率：成功次数/总次数。仿真环境200次rollout；真实环境10次rollout。</p>\n<p> - 真实环境评分：抓取正确对象得0.5分，正确放置得0.5分。</p>"
  },
  {
    "date": "2025-09-25",
    "title": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2509.21243",
    "summary_markdown": "## 论文研究单位\n- 加川大学（School of Computing, Gachon University）\n## 论文概述\n- 针对VLA模型效率与性能的权衡难题，RetoVLA提出“复用Register Tokens”这一被ViT用于去伪影的中间信息，作为“空间上下文”直接注入Action Expert，在维持轻量结构的同时增强空间推理能力。\n- 基于LIBERO基准、自建仿真环境以及自建7-DOF机械臂进行评估，实机与仿真均显示在长时序与三维空间理解任务上显著提升。\n## 论文核心贡献点\n- 重新定义Register Tokens：从“去伪影的净化器”转为“空间上下文提供者”，设计可学习门控将其KV注入Action Expert的最后一层交叉注意力，实现语义与全局空间双流融合。\n- 证明复用信息可弥补轻量模型（如SmolVLA）深度压缩导致的全局理解能力损失，是替代“信息缩减”的新范式。\n- 三类实验验证：标准化仿真（LIBERO）、自建物理仿真（Unity+MuJoCo）、真实7-DOF机械臂上取得大幅提升（尤其长时序与3D空间任务）。\n## 论文方法描述\n- 架构\n - 采用浅层VLM主干（SmolVLM2-500M的前16层）以保证效率；维持单流语义特征与双流信息融合路径。\n- 空间上下文注入（核心）\n - Register Token生成：用可学习初始向量与图像patch做注意力，生成场景依赖的Register Tokens。\n - 注入与融合：将Register Tokens投影为KV并与VLM的KV在Action Expert最后一层交叉注意力处拼接。\n - 门控：引入可学习标量经sigmoid调制注入强度，避免对极精细局部控制造成干扰。\n- 训练目标\n - 条件流匹配（CFM）：学习将噪声动作序列朝真实动作序列演化的向量场，MSE损失优化。\n## 论文使用数据集和训练资源\n- 数据与任务\n - LIBERO四类基准：Spatial、Object、Goal、10(Long)。\n - 真实环境：自建7-DOF机械臂，7项操控任务，收集1,804条episode进行微调。\n - 自建仿真：Unity + MuJoCo插件，对应部分真实任务。\n- 训练资源（报告为主干与步骤细节）\n - 主干：SmolVLM2-500M（使用前16层）。\n - 训练步数：100k；批量：64。\n - 注入寄存器：2个Register Tokens（消融最优）。\n## 论文使用的评估环境和评估指标\n- 评估环境\n - 仿真：LIBERO基准与自定义Unity+MuJoCo仿真。\n - 真实：7-DOF机械臂，多视角（顶视/侧视/腕部）。\n- 评估指标\n - 主要指标：成功率（Success Rate, SR）、平均成功率（MSR）。\n - 性能对比：SmolVLA vs RetoVLA。\n- 关键结果摘要\n - 真实机器人MSR：50.28% → 67.42%，绝对提升17.14%p。\n - 仿真MSR：62.8% → 74.8%，提升12.0%p。\n - 长时序与复杂空间任务提升显著（如Build Domino Line、Close Drawer、Jenga等）；对极精细局部操作有轻微权衡。\n - LIBERO各类总体提升有限，但针对工作记忆与三维空间理解的任务明显受益。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>加川大学（School of Computing, Gachon University）</li></ul>\n<h2>论文概述</h2>\n<ul><li>针对VLA模型效率与性能的权衡难题，RetoVLA提出“复用Register Tokens”这一被ViT用于去伪影的中间信息，作为“空间上下文”直接注入Action Expert，在维持轻量结构的同时增强空间推理能力。</li><li>基于LIBERO基准、自建仿真环境以及自建7-DOF机械臂进行评估，实机与仿真均显示在长时序与三维空间理解任务上显著提升。</li></ul>\n<h2>论文核心贡献点</h2>\n<ul><li>重新定义Register Tokens：从“去伪影的净化器”转为“空间上下文提供者”，设计可学习门控将其KV注入Action Expert的最后一层交叉注意力，实现语义与全局空间双流融合。</li><li>证明复用信息可弥补轻量模型（如SmolVLA）深度压缩导致的全局理解能力损失，是替代“信息缩减”的新范式。</li><li>三类实验验证：标准化仿真（LIBERO）、自建物理仿真（Unity+MuJoCo）、真实7-DOF机械臂上取得大幅提升（尤其长时序与3D空间任务）。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>架构</li></ul>\n<p> - 采用浅层VLM主干（SmolVLM2-500M的前16层）以保证效率；维持单流语义特征与双流信息融合路径。</p>\n<ul><li>空间上下文注入（核心）</li></ul>\n<p> - Register Token生成：用可学习初始向量与图像patch做注意力，生成场景依赖的Register Tokens。</p>\n<p> - 注入与融合：将Register Tokens投影为KV并与VLM的KV在Action Expert最后一层交叉注意力处拼接。</p>\n<p> - 门控：引入可学习标量经sigmoid调制注入强度，避免对极精细局部控制造成干扰。</p>\n<ul><li>训练目标</li></ul>\n<p> - 条件流匹配（CFM）：学习将噪声动作序列朝真实动作序列演化的向量场，MSE损失优化。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据与任务</li></ul>\n<p> - LIBERO四类基准：Spatial、Object、Goal、10(Long)。</p>\n<p> - 真实环境：自建7-DOF机械臂，7项操控任务，收集1,804条episode进行微调。</p>\n<p> - 自建仿真：Unity + MuJoCo插件，对应部分真实任务。</p>\n<ul><li>训练资源（报告为主干与步骤细节）</li></ul>\n<p> - 主干：SmolVLM2-500M（使用前16层）。</p>\n<p> - 训练步数：100k；批量：64。</p>\n<p> - 注入寄存器：2个Register Tokens（消融最优）。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境</li></ul>\n<p> - 仿真：LIBERO基准与自定义Unity+MuJoCo仿真。</p>\n<p> - 真实：7-DOF机械臂，多视角（顶视/侧视/腕部）。</p>\n<ul><li>评估指标</li></ul>\n<p> - 主要指标：成功率（Success Rate, SR）、平均成功率（MSR）。</p>\n<p> - 性能对比：SmolVLA vs RetoVLA。</p>\n<ul><li>关键结果摘要</li></ul>\n<p> - 真实机器人MSR：50.28% → 67.42%，绝对提升17.14%p。</p>\n<p> - 仿真MSR：62.8% → 74.8%，提升12.0%p。</p>\n<p> - 长时序与复杂空间任务提升显著（如Build Domino Line、Close Drawer、Jenga等）；对极精细局部操作有轻微权衡。</p>\n<p> - LIBERO各类总体提升有限，但针对工作记忆与三维空间理解的任务明显受益。</p>"
  },
  {
    "date": "2025-09-25",
    "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning",
    "link": "http://arxiv.org/abs/2509.21126",
    "summary_markdown": "## 论文研究单位\n武汉大学，中国。\n## 论文概述\n本文提出VARL框架，利用视觉语言模型(VLM)作为动作顾问，在在线强化学习(RL)中提供动作建议，旨在改进样本效率，特别是在稀疏奖励任务中。方法通过集成VLM建议的动作，而非修改奖励函数，丰富样本多样性并确保策略最优性和收敛性。论文在多种环境和代理设置中验证了框架的有效性。\n## 论文核心贡献点\n- 提出VLM作为动作顾问的框架，而非奖励设计师，保证策略最优性。\n- 设计门控机制（离散和连续动作空间）防止策略过度拟合VLM动作。\n- 通过策略塑形在早期训练阶段提供指导，提高样本效率。\n- 低计算开销：相比奖励塑形方法，显著减少VLM查询次数（如VARL仅3次查询对比奖励塑形方法的5000次）。\n- 适用于状态和视觉基础任务、离散和连续动作空间。\n## 论文方法描述\nVARL框架包含两个组件：\n- VLM动作生成器：定期采样最新状态-动作对，查询VLM生成启发式动作并存储至引导缓冲区。\n- 策略塑形模块：将VLM动作集成至策略训练，通过行为克隆损失和门控函数塑形策略，在指定步数后移除启发式动作。\n具体算法基于软演员-评论家(SAC)，使用GPT-5作为VLM，损失函数包括基线策略损失、行为克隆损失和门控函数。\n## 论文使用数据集和训练资源\n- 环境：Meta-World（操控）、AI2-THOR（导航）、真实世界（Realman Robotics RM-65B臂）。\n- 任务：10个任务，包括状态基础（Drawer Open、Sweep Into、Soccer）、视觉基础（Pick Up Plate、Open Phone、Toggle Off Lamp、Drawer Open、Push Cube）、真实世界视觉基础（Target Reach、Push Cube）。\n- 训练资源：最大步数500,000，参数λ=10、移除步数N_s=30,000，使用SAC求解器和GPT-5 VLM。\n## 论文使用的评估环境和评估指标\n- 评估环境：10个任务，分为三类（状态基础操控、视觉基础操控和导航、真实世界视觉基础操控）。\n- 评估指标：成功率、学习曲线（平均回报或成功率），与基线方法（SAC、SAC+专家数据、RL-VLM-F、ERL-VLM）比较，包括样本效率分析和消融研究。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>武汉大学，中国。</p>\n<h2>论文概述</h2>\n<p>本文提出VARL框架，利用视觉语言模型(VLM)作为动作顾问，在在线强化学习(RL)中提供动作建议，旨在改进样本效率，特别是在稀疏奖励任务中。方法通过集成VLM建议的动作，而非修改奖励函数，丰富样本多样性并确保策略最优性和收敛性。论文在多种环境和代理设置中验证了框架的有效性。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出VLM作为动作顾问的框架，而非奖励设计师，保证策略最优性。</li><li>设计门控机制（离散和连续动作空间）防止策略过度拟合VLM动作。</li><li>通过策略塑形在早期训练阶段提供指导，提高样本效率。</li><li>低计算开销：相比奖励塑形方法，显著减少VLM查询次数（如VARL仅3次查询对比奖励塑形方法的5000次）。</li><li>适用于状态和视觉基础任务、离散和连续动作空间。</li></ul>\n<h2>论文方法描述</h2>\n<p>VARL框架包含两个组件：</p>\n<ul><li>VLM动作生成器：定期采样最新状态-动作对，查询VLM生成启发式动作并存储至引导缓冲区。</li><li>策略塑形模块：将VLM动作集成至策略训练，通过行为克隆损失和门控函数塑形策略，在指定步数后移除启发式动作。</li></ul>\n<p>具体算法基于软演员-评论家(SAC)，使用GPT-5作为VLM，损失函数包括基线策略损失、行为克隆损失和门控函数。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>环境：Meta-World（操控）、AI2-THOR（导航）、真实世界（Realman Robotics RM-65B臂）。</li><li>任务：10个任务，包括状态基础（Drawer Open、Sweep Into、Soccer）、视觉基础（Pick Up Plate、Open Phone、Toggle Off Lamp、Drawer Open、Push Cube）、真实世界视觉基础（Target Reach、Push Cube）。</li><li>训练资源：最大步数500,000，参数λ=10、移除步数N_s=30,000，使用SAC求解器和GPT-5 VLM。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：10个任务，分为三类（状态基础操控、视觉基础操控和导航、真实世界视觉基础操控）。</li><li>评估指标：成功率、学习曲线（平均回报或成功率），与基线方法（SAC、SAC+专家数据、RL-VLM-F、ERL-VLM）比较，包括样本效率分析和消融研究。</li></ul>"
  },
  {
    "date": "2025-09-25",
    "title": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
    "link": "http://arxiv.org/abs/2509.21006",
    "summary_markdown": "### 论文研究单位\nSkolkovo Institute of Science and Technology（莫斯科斯科尔科沃理工学院）智能空间机器人实验室。\n### 论文概述\nAnywhereVLA是一个模块化框架，用于在未见过的大型室内环境中实现自然语言驱动的抓取-放置任务。系统将语言指令转换为结构化任务图，结合LiDAR-SLAM、度量语义映射和主动探索策略，并在接近模块引导下由细调的SmolVLA模型执行操作。在消费级硬件（Jetson Orin NX和Intel NUC）上实时运行（≥10Hz），在实验室环境（静态场景和人类活动）中实现46%整体成功率，结合经典导航与VLA操作确保鲁棒性与灵活性。\n### 论文核心贡献点\n- **统一模块化框架**：单一语言指令同时驱动环境探索和VLA操作。\n- **实时性能**：嵌入式设备上实现≥10Hz的全模块运行。\n- **环境适应能力**：支持未见过的动态场景，探索并定位目标对象。\n- **方法融合**：结合经典SLAM导航与轻量级VLA（SmolVLA 450M）模型，提升操作可靠性与泛化能力。\n### 论文方法描述\n- **架构模块**：\n - **3D语义映射（SM）**：LiDAR-惯性-视觉SLAM构建语义点云，通过LiDAR密集化（插值环间点）、对象聚合（DBSCAN聚类）和置信度估计生成目标地图。\n - **主动环境探索（AEE）**：前沿驱动的探索策略，条件化目标类，优化视野覆盖并抑制虚假目标。\n - **接近模块（Approach）**：基于语义地图计算安全接近姿势，验证无碰撞导航。\n - **VLA操作**：细调SmolVLA模型生成操作动作，集成多视角相机输入。\n- **工作流**：解析语言指令→触发SM和AEE→SM构建地图→AEE定位目标→Approach导航→VLA执行操作。\n- **硬件部署**：Jetson Orin NX处理感知和VLA，Intel NUC处理SLAM和导航。\n### 论文使用数据集和训练资源\n- **数据集**：SO-101操作器抓取-放置轨迹50条（遥操作采集）。\n- **训练资源**：RTX 4090 GPU（16GB VRAM）细调SmolVLA（批量16，学习率0.0001，余弦退火，AdamW优化，梯度裁剪10.0）。\n- **部署硬件**：Jetson Orin NX（16GB）和Intel NUC（Core i7, 32GB）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：未见过的大型室内多房间实验室（动态场景，正常人类活动）。\n- **任务**：50次抓取-放置试验（目标对象随机分布在半径内）。\n- **指令模板**：自然语言指令，如“Pick up the <object> and place it in the <area>. And bring the <object> to <location>.”\n- **评估指标**：\n - 成功率（SR）：整体SR 46%，VLA操作模块SR 85%（细调后）。\n - 模块性能：SLAM SR 100%，AEE SR 75%，导航SR 90%，对象检测SR 85%，VLA操作SR 80%。\n - 实时性能：总任务时间（探索半径5m时平均133秒，10m时<10分钟）。\n - 计算效率：模块频率≥10Hz（如表I所示）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Skolkovo Institute of Science and Technology（莫斯科斯科尔科沃理工学院）智能空间机器人实验室。</p>\n<h3>论文概述</h3>\n<p>AnywhereVLA是一个模块化框架，用于在未见过的大型室内环境中实现自然语言驱动的抓取-放置任务。系统将语言指令转换为结构化任务图，结合LiDAR-SLAM、度量语义映射和主动探索策略，并在接近模块引导下由细调的SmolVLA模型执行操作。在消费级硬件（Jetson Orin NX和Intel NUC）上实时运行（≥10Hz），在实验室环境（静态场景和人类活动）中实现46%整体成功率，结合经典导航与VLA操作确保鲁棒性与灵活性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li><strong>统一模块化框架</strong>：单一语言指令同时驱动环境探索和VLA操作。</li><li><strong>实时性能</strong>：嵌入式设备上实现≥10Hz的全模块运行。</li><li><strong>环境适应能力</strong>：支持未见过的动态场景，探索并定位目标对象。</li><li><strong>方法融合</strong>：结合经典SLAM导航与轻量级VLA（SmolVLA 450M）模型，提升操作可靠性与泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>架构模块</strong>：</li></ul>\n<p> - <strong>3D语义映射（SM）</strong>：LiDAR-惯性-视觉SLAM构建语义点云，通过LiDAR密集化（插值环间点）、对象聚合（DBSCAN聚类）和置信度估计生成目标地图。</p>\n<p> - <strong>主动环境探索（AEE）</strong>：前沿驱动的探索策略，条件化目标类，优化视野覆盖并抑制虚假目标。</p>\n<p> - <strong>接近模块（Approach）</strong>：基于语义地图计算安全接近姿势，验证无碰撞导航。</p>\n<p> - <strong>VLA操作</strong>：细调SmolVLA模型生成操作动作，集成多视角相机输入。</p>\n<ul><li><strong>工作流</strong>：解析语言指令→触发SM和AEE→SM构建地图→AEE定位目标→Approach导航→VLA执行操作。</li><li><strong>硬件部署</strong>：Jetson Orin NX处理感知和VLA，Intel NUC处理SLAM和导航。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：SO-101操作器抓取-放置轨迹50条（遥操作采集）。</li><li><strong>训练资源</strong>：RTX 4090 GPU（16GB VRAM）细调SmolVLA（批量16，学习率0.0001，余弦退火，AdamW优化，梯度裁剪10.0）。</li><li><strong>部署硬件</strong>：Jetson Orin NX（16GB）和Intel NUC（Core i7, 32GB）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：未见过的大型室内多房间实验室（动态场景，正常人类活动）。</li><li><strong>任务</strong>：50次抓取-放置试验（目标对象随机分布在半径内）。</li><li><strong>指令模板</strong>：自然语言指令，如“Pick up the <object> and place it in the <area>. And bring the <object> to <location>.”</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（SR）：整体SR 46%，VLA操作模块SR 85%（细调后）。</p>\n<p> - 模块性能：SLAM SR 100%，AEE SR 75%，导航SR 90%，对象检测SR 85%，VLA操作SR 80%。</p>\n<p> - 实时性能：总任务时间（探索半径5m时平均133秒，10m时<10分钟）。</p>\n<p> - 计算效率：模块频率≥10Hz（如表I所示）。</p>"
  },
  {
    "date": "2025-09-25",
    "title": "ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2509.20841",
    "summary_markdown": "### 论文研究单位\n德坤卢、高伟和贾奎（Dekun Lu, Wei Gao, and Kui Jia）- 作者信息未明确标注所属机构\n### 论文概述\n论文提出了一种名为ImaginationPolicy的端到端机器人操作策略，旨在解决传统模块化管道信息丢失和特征对齐问题。该方法通过**可操作性导向的关键点**实现对多样化操作任务的统一表示，显著提升机器人在抓取、倒水等任务中的泛化能力和精度。研究采用Chain of Moving Oriented Keypoints (CoMOK)作为动作表示，将操作行为转化为关键点序列，使网络能在不同形状/尺寸物体间实现**亚厘米级精度**。\n### 论文核心贡献点\n1. **创新动作表示**：提出CoMOK（移动方向关键点链）作为统一操作表示，突破末端执行器姿态表示限制\n2. **任务泛化能力**：基于可操作性关键点自然适配不同物体形状/尺寸，包括可变形物体\n3. **多任务统一框架**：单神经网络同时处理抓取、倒水、稳定放置等任务\n4. **端到端训练**：融合Groma视觉语言模型进行任务规划，Score-Matching网络生成动作分布\n5. **复杂场景适配**：支持多阶段操作、多模态行为及轨迹动作生成\n### 论文方法描述\n**CoMOK核心公式**：\n```\n网络输出 (o_manipulated, T_affordance, T_action)\n```\n- **o_manipulated**: 机器人控制对象（工具/物体/局部区域）\n- **T_affordance**: 操作方向关键点（如杯子把手、缆绳抓取点）\n- **T_action**: 目标对齐姿态（若T_affordance与T_action对齐则任务完成）\n\n**扩展机制**：\n- **多阶段操作**：全局任务描述自动分解子任务（如倒水任务拆解为\"抓杯子→倒水→放置\"）\n- **多模态候选**：扩散模型生成动作分布（如多个抓取位姿可选）\n- **轨迹序列**：输出SE(3)位姿序列（如切割水果的上下两帧）\n\n**神经网络架构**：\n1. **任务规划层**：基于Groma VLM，从RGBD图像和全局指令生成子任务列表\n2. **动作预测层**：点云输入+阶段任务特征，通过Score-Matching生成关键点序列\n3. **轨迹生成层**：\n - 仿真：Motion Policy Networks学习生成关节空间轨迹\n - 实机：传统任务与运动规划算法筛选可行轨迹\n### 论文使用数据集和训练资源\n- **抓取检测**：复用现有点云数据集（SE(3)-DiffusionFields来源）\n- **仿真实验**：三类物体稳定放置任务\n - 20个瓶子、10个盒子、5个三脚架\n - 随机放置5个障碍物\n- **实机实验**：6种缆绳+3种挂钩的多样化操作\n- **硬件配置**：Rokae SR5六自由度机械臂 + 末端RGBD传感器\n### 论文使用的评估环境和评估指标\n**评估任务**：\n1. **抓取检测**：双帧输出（预抓取+抓取）\n2. **稳定放置**（仿真）：\n - 成功率：距离桌面≤1cm，Z轴偏差≤15°\n - 碰撞检测：放置后与障碍物碰撞\n3. **缆绳插入**（实机）：缆绳是否插入夹具\n4. **挂钩挂杯**（实机）：20种杯子×3种挂钩\n\n**评估指标**：\n- **精度**：位置误差（厘米级）、姿态误差（角度）\n- **泛化性**：跨物体形状/尺寸成功率\n- **可靠性**：任务级成功率（不含中间路径错误）\n- **动态适应**：可变形物体（缆绳）操作成功率\n\n**结论**：方法实现跨任务统一操作，成功率从稳定放置的仿真数据到实机缆绳插入均有验证，平均执行精度达厘米级且对物体变化鲁棒。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>德坤卢、高伟和贾奎（Dekun Lu, Wei Gao, and Kui Jia）- 作者信息未明确标注所属机构</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为ImaginationPolicy的端到端机器人操作策略，旨在解决传统模块化管道信息丢失和特征对齐问题。该方法通过<strong>可操作性导向的关键点</strong>实现对多样化操作任务的统一表示，显著提升机器人在抓取、倒水等任务中的泛化能力和精度。研究采用Chain of Moving Oriented Keypoints (CoMOK)作为动作表示，将操作行为转化为关键点序列，使网络能在不同形状/尺寸物体间实现<strong>亚厘米级精度</strong>。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>创新动作表示</strong>：提出CoMOK（移动方向关键点链）作为统一操作表示，突破末端执行器姿态表示限制</li><li><strong>任务泛化能力</strong>：基于可操作性关键点自然适配不同物体形状/尺寸，包括可变形物体</li><li><strong>多任务统一框架</strong>：单神经网络同时处理抓取、倒水、稳定放置等任务</li><li><strong>端到端训练</strong>：融合Groma视觉语言模型进行任务规划，Score-Matching网络生成动作分布</li><li><strong>复杂场景适配</strong>：支持多阶段操作、多模态行为及轨迹动作生成</li></ol>\n<h3>论文方法描述</h3>\n<p><strong>CoMOK核心公式</strong>：</p>\n<p>```</p>\n<p>网络输出 (o_manipulated, T_affordance, T_action)</p>\n<p>```</p>\n<ul><li><strong>o_manipulated</strong>: 机器人控制对象（工具/物体/局部区域）</li><li><strong>T_affordance</strong>: 操作方向关键点（如杯子把手、缆绳抓取点）</li><li><strong>T_action</strong>: 目标对齐姿态（若T_affordance与T_action对齐则任务完成）</li></ul>\n\n<p><strong>扩展机制</strong>：</p>\n<ul><li><strong>多阶段操作</strong>：全局任务描述自动分解子任务（如倒水任务拆解为\"抓杯子→倒水→放置\"）</li><li><strong>多模态候选</strong>：扩散模型生成动作分布（如多个抓取位姿可选）</li><li><strong>轨迹序列</strong>：输出SE(3)位姿序列（如切割水果的上下两帧）</li></ul>\n\n<p><strong>神经网络架构</strong>：</p>\n<ol><li><strong>任务规划层</strong>：基于Groma VLM，从RGBD图像和全局指令生成子任务列表</li><li><strong>动作预测层</strong>：点云输入+阶段任务特征，通过Score-Matching生成关键点序列</li><li><strong>轨迹生成层</strong>：</li></ol>\n<p> - 仿真：Motion Policy Networks学习生成关节空间轨迹</p>\n<p> - 实机：传统任务与运动规划算法筛选可行轨迹</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>抓取检测</strong>：复用现有点云数据集（SE(3)-DiffusionFields来源）</li><li><strong>仿真实验</strong>：三类物体稳定放置任务</li></ul>\n<p> - 20个瓶子、10个盒子、5个三脚架</p>\n<p> - 随机放置5个障碍物</p>\n<ul><li><strong>实机实验</strong>：6种缆绳+3种挂钩的多样化操作</li><li><strong>硬件配置</strong>：Rokae SR5六自由度机械臂 + 末端RGBD传感器</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估任务</strong>：</p>\n<ol><li><strong>抓取检测</strong>：双帧输出（预抓取+抓取）</li><li><strong>稳定放置</strong>（仿真）：</li></ol>\n<p> - 成功率：距离桌面≤1cm，Z轴偏差≤15°</p>\n<p> - 碰撞检测：放置后与障碍物碰撞</p>\n<ol><li><strong>缆绳插入</strong>（实机）：缆绳是否插入夹具</li><li><strong>挂钩挂杯</strong>（实机）：20种杯子×3种挂钩</li></ol>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li><strong>精度</strong>：位置误差（厘米级）、姿态误差（角度）</li><li><strong>泛化性</strong>：跨物体形状/尺寸成功率</li><li><strong>可靠性</strong>：任务级成功率（不含中间路径错误）</li><li><strong>动态适应</strong>：可变形物体（缆绳）操作成功率</li></ul>\n\n<p><strong>结论</strong>：方法实现跨任务统一操作，成功率从稳定放置的仿真数据到实机缆绳插入均有验证，平均执行精度达厘米级且对物体变化鲁棒。</p>"
  },
  {
    "date": "2025-09-24",
    "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
    "link": "http://arxiv.org/abs/2509.20109",
    "summary_markdown": "## 论文研究单位\n- **LiAuto**（理想汽车）\n- **清华大学**（Tsinghua University）\n## 论文概述\n本文针对端到端自动驾驶系统中模仿学习无法内在编码物理规则（如碰撞避免、遵守可行驶区域）的核心挑战，提出了一种名为 **ReflectDrive** 的创新框架。该框架将离散扩散模型应用于轨迹规划，并引入**安全反射机制**（Safety-Guided Regeneration），在推理阶段通过迭代局部搜索和安全锚点重绘，无需梯度计算即可实现轨迹的安全自校正，从而提升规划的可控性和可靠性。\n## 论文核心贡献点\n1. **首次将离散扩散应用于端到端自动驾驶规划**：将连续轨迹空间离散化为动作码本，利用预训练的扩散语言模型（DLM）进行微调，实现并行解码和双向特征融合。\n2. **提出安全反射机制**：在推理阶段结合目标条件生成（Goal-Conditioned Generation）和安全导向再生，通过局部搜索识别不安全点并替换为安全锚点，再利用扩散模型的修复能力（inpainting）重新生成轨迹。\n3. **验证安全约束的有效性**：在真实场景基准测试中，该框架在确保硬安全约束的同时，显著提升了轨迹质量，且性能接近人类驾驶水平。\n## 论文方法描述\n1. **轨迹离散化（Trajectory Discretization）**：\n - 将二维驾驶空间中的连续坐标 `(x, y)` 分别映射到预定义的1D码本 \\(\\mathcal{A}\\) 中，生成轨迹的离散化序列 \\(\\mathbf{y}\\)。\n - 使用均匀网格量化坐标范围 \\([-M, M]\\)，分辨率 \\(\\Delta_g\\) 可控，确保离散化后的轨迹可通过逆量化恢复为连续坐标。\n\n2. **离散扩散模型（Discrete Diffusion Model）**：\n - 基于离散扩散框架（掩码-去噪），采用预训练的VLA模型（如LLaDA-V）作为主干网络。\n - 通过监督微调训练模型，使其具备根据场景上下文生成离散轨迹序列的能力。\n\n3. **安全反射推理（Reflective Inference）**：\n - **目标条件生成**：对终点位置进行概率采样，应用非极大值抑制（NMS）获取空间多样化的目标点，生成多模态轨迹候选。\n - **安全导向再生**：\n - **全局评分器**：评估完整轨迹的安全性和质量。\n - **安全评分器**：定位不安全路标点。\n - **局部搜索**：在路标点附近（如曼哈顿距离 \\(\\delta \\leq 10\\)）搜索可行替代点作为“安全锚点”。\n - **轨迹修复**：固定安全锚点，通过扩散模型重绘周围轨迹段。\n## 论文使用数据集和训练资源\n- **数据集**：基于大规模真实世界自动驾驶基准 **NAVSIM** 的数据。\n- **训练资源**：\n - 输入：前、左前、右前摄像头图像及语言指令（如“左转”）。\n - 模型初始化：预训练的扩散语言模型（如LLaDA-V）。\n - 训练方法：监督微调（SFT），批量大小为16，学习率 \\(1 \\times 10^{-5}\\)，训练3轮。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - **NAVSIM 基准**：使用官方闭环仿真器进行评估。\n- **评估指标**：\n - **PDMS 分数**（主要指标）：聚合5项指标（分数越高越好）：\n - **NC**（No-Collision）：无碰撞率（防止事故）。\n - **DAC**（Drivable Area Compliance）：可行驶区域合规性（遵守道路边界）。\n - **TTC**（Time-to-Collision）：碰撞时间安全性（避让反应时间）。\n - **Comfort**：舒适性（加速度/急动度受控）。\n - **EP**（Ego Progress）：ego进展（路径完成度）。\n\n**结果摘要**（以相机输入为例）：\n\\|方法 \\|DAC ↑ \\|TTC ↑ \\|NC ↑ \\|EP ↑ \\|PDMS ↑ \\|\n\\|\n---\n--\n---\n--\n---\n--\n---\n-\\|\n---\n----\\|\n---\n----\\|\n---\n---\\|\n---\n---\\|\n---\n--\n---\n\\|\n\\|ReflectDrive \\|**99.3%** \\|93.5% \\|97.7% \\|**86.9%** \\|**91.1** \\|\n\\|ReflectDrive（无反射） \\|95.4% \\|92.2% \\|96.9% \\|79.0% \\|84.8 \\|\n\\|**人类表现** \\|100.0% \\|100.0% \\|100.0% \\|87.5% \\|94.8 \\|\n\n> **结论**：ReflectDrive 的安全反射机制显著提升轨迹安全性（如 DAC 提升 +3.9）同时改善路径完成度（EP 提升 +7.9），接近人类驾驶水平。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li><strong>LiAuto</strong>（理想汽车）</li><li><strong>清华大学</strong>（Tsinghua University）</li></ul>\n<h2>论文概述</h2>\n<p>本文针对端到端自动驾驶系统中模仿学习无法内在编码物理规则（如碰撞避免、遵守可行驶区域）的核心挑战，提出了一种名为 <strong>ReflectDrive</strong> 的创新框架。该框架将离散扩散模型应用于轨迹规划，并引入<strong>安全反射机制</strong>（Safety-Guided Regeneration），在推理阶段通过迭代局部搜索和安全锚点重绘，无需梯度计算即可实现轨迹的安全自校正，从而提升规划的可控性和可靠性。</p>\n<h2>论文核心贡献点</h2>\n<ol><li><strong>首次将离散扩散应用于端到端自动驾驶规划</strong>：将连续轨迹空间离散化为动作码本，利用预训练的扩散语言模型（DLM）进行微调，实现并行解码和双向特征融合。</li><li><strong>提出安全反射机制</strong>：在推理阶段结合目标条件生成（Goal-Conditioned Generation）和安全导向再生，通过局部搜索识别不安全点并替换为安全锚点，再利用扩散模型的修复能力（inpainting）重新生成轨迹。</li><li><strong>验证安全约束的有效性</strong>：在真实场景基准测试中，该框架在确保硬安全约束的同时，显著提升了轨迹质量，且性能接近人类驾驶水平。</li></ol>\n<h2>论文方法描述</h2>\n<p>1. <strong>轨迹离散化（Trajectory Discretization）</strong>：</p>\n<p> - 将二维驾驶空间中的连续坐标 <code>(x, y)</code> 分别映射到预定义的1D码本 \\(\\mathcal{A}\\) 中，生成轨迹的离散化序列 \\(\\mathbf{y}\\)。</p>\n<p> - 使用均匀网格量化坐标范围 \\([-M, M]\\)，分辨率 \\(\\Delta_g\\) 可控，确保离散化后的轨迹可通过逆量化恢复为连续坐标。</p>\n\n<p>2. <strong>离散扩散模型（Discrete Diffusion Model）</strong>：</p>\n<p> - 基于离散扩散框架（掩码-去噪），采用预训练的VLA模型（如LLaDA-V）作为主干网络。</p>\n<p> - 通过监督微调训练模型，使其具备根据场景上下文生成离散轨迹序列的能力。</p>\n\n<p>3. <strong>安全反射推理（Reflective Inference）</strong>：</p>\n<p> - <strong>目标条件生成</strong>：对终点位置进行概率采样，应用非极大值抑制（NMS）获取空间多样化的目标点，生成多模态轨迹候选。</p>\n<p> - <strong>安全导向再生</strong>：</p>\n<p> - <strong>全局评分器</strong>：评估完整轨迹的安全性和质量。</p>\n<p> - <strong>安全评分器</strong>：定位不安全路标点。</p>\n<p> - <strong>局部搜索</strong>：在路标点附近（如曼哈顿距离 \\(\\delta \\leq 10\\)）搜索可行替代点作为“安全锚点”。</p>\n<p> - <strong>轨迹修复</strong>：固定安全锚点，通过扩散模型重绘周围轨迹段。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：基于大规模真实世界自动驾驶基准 <strong>NAVSIM</strong> 的数据。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 输入：前、左前、右前摄像头图像及语言指令（如“左转”）。</p>\n<p> - 模型初始化：预训练的扩散语言模型（如LLaDA-V）。</p>\n<p> - 训练方法：监督微调（SFT），批量大小为16，学习率 \\(1 \\times 10^{-5}\\)，训练3轮。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>NAVSIM 基准</strong>：使用官方闭环仿真器进行评估。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>PDMS 分数</strong>（主要指标）：聚合5项指标（分数越高越好）：</p>\n<p> - <strong>NC</strong>（No-Collision）：无碰撞率（防止事故）。</p>\n<p> - <strong>DAC</strong>（Drivable Area Compliance）：可行驶区域合规性（遵守道路边界）。</p>\n<p> - <strong>TTC</strong>（Time-to-Collision）：碰撞时间安全性（避让反应时间）。</p>\n<p> - <strong>Comfort</strong>：舒适性（加速度/急动度受控）。</p>\n<p> - <strong>EP</strong>（Ego Progress）：ego进展（路径完成度）。</p>\n\n<p><strong>结果摘要</strong>（以相机输入为例）：</p>\n<p>\\|方法 \\|DAC ↑ \\|TTC ↑ \\|NC ↑ \\|EP ↑ \\|PDMS ↑ \\|</p>\n<p>\\|</p>\n<hr/>\n<p>--</p>\n<hr/>\n<p>--</p>\n<hr/>\n<p>--</p>\n<hr/>\n<p>-\\|</p>\n<hr/>\n<p>----\\|</p>\n<hr/>\n<p>----\\|</p>\n<hr/>\n<p>---\\|</p>\n<hr/>\n<p>---\\|</p>\n<hr/>\n<p>--</p>\n<hr/>\n<p>\\|</p>\n<p>\\|ReflectDrive \\|<strong>99.3%</strong> \\|93.5% \\|97.7% \\|<strong>86.9%</strong> \\|<strong>91.1</strong> \\|</p>\n<p>\\|ReflectDrive（无反射） \\|95.4% \\|92.2% \\|96.9% \\|79.0% \\|84.8 \\|</p>\n<p>\\|<strong>人类表现</strong> \\|100.0% \\|100.0% \\|100.0% \\|87.5% \\|94.8 \\|</p>\n\n<p>> <strong>结论</strong>：ReflectDrive 的安全反射机制显著提升轨迹安全性（如 DAC 提升 +3.9）同时改善路径完成度（EP 提升 +7.9），接近人类驾驶水平。</p>"
  },
  {
    "date": "2025-09-24",
    "title": "FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2509.19870",
    "summary_markdown": "论文研究单位\n- 复旦大学\n- 上海人工智能实验室\n- Sea AI Lab\n\n论文概述\n- 针对视觉-语言-动作（VLA）模型提出并形式化“行动冻结”（action-freezing）攻击：对抗图像使机器人忽略后续指令，进入持续无响应状态，可能在关键时刻导致不作为，引发实际安全风险。\n- 提出FreezeVLA框架，通过最小-最大双层优化生成跨提示可迁移的对抗图像，在三个SOTA VLA模型与四个机器人基准上实现高攻击成功率，显著优于现有方法。\n\n论文核心贡献点\n- 识别并形式化VLA模型的“行动冻结”漏洞，揭示与错误动作同等严重的“不作为”威胁。\n- 提出FreezeVLA：使用内部最大化（对抗提示优化）与外部最小化（对抗图像优化）的双层优化方法，扩展提示嵌入空间覆盖，显著提升跨提示迁移能力。\n- 跨模型验证：SpatialVLA、OpenVLA、π0 上分别达到平均攻击成功率73.3%、95.4%、59.8%，全面超越随机噪声、PGD、多提示等基线。\n\n论文方法描述\n- 威胁模型与目标：白盒访问VLA模型、黑盒用户指令，通过图像扰动使模型在任意指令下稳定输出冻结令牌（如<freeze>、<eos>或“do-nothing”token），导致行动终止。\n- 内部最大化（对抗提示优化）：从初始参考提示集出发，通过梯度分析定位高影响词并进行贪婪同义词替换，迭代生成更难被冻结的“硬提示”集，扩展提示语义空间。\n- 外部最小化（对抗图像优化）：基于“硬提示”集合，对图像进行梯度聚合与更新，使VLA模型在多种提示下均以高概率输出冻结令牌，实现持久冻结。\n- 形式化目标：min−max 双层优化，外层在 L∞ 扰动预算 ε 内最小化冻结损失，内层最大化难提示下的损失，使对抗图像在跨提示下保持有效性。\n- 提示多样性增强：对比“随机采样”与“GPT生成”提示，显示GPT生成的语义多样性进一步提升攻击迁移性。\n\n论文使用数据集和训练资源\n- 数据集与基准：LIBERO-10、LIBERO-Goal、LIBERO-Object、LIBERO-Spatial四个机器人操控基准。\n- 目标模型：SpatialVLA、OpenVLA、π0 三种SOTA VLA模型（具备动作分块/离散化/连续动作头等不同架构与冻结令牌策略）。\n- 训练与实验资源：HPC集群，32×NVIDIA A800-SXM4-80GB GPUs。\n\n论文使用的评估环境和评估指标\n- 评估指标：攻击成功率（ASR），定义为使VLA模型进入一致冻结状态的对抗图像比例。\n- 评估设置：\n - 扰动预算：L∞ 约束下的 ε=4/255（主实验），对比 ε∈{1/255,2/255,4/255,8/255,16/255} 的敏感性。\n - 优化步数与提示数：图像迭代K=100、步长α=1/255；参考提示规模\\|P\\|=20；内部提示迭代M=10。\n - 基线对比：随机噪声、Single-Prompt PGD、Multi-Prompt（随机/随机+GPT）、FreezeVLA（随机）与FreezeVLA+GPT。\n - 冻结策略差异：SpatialVLA/π0 使用 <eos>；OpenVLA 使用“do-nothing”token。\n- 主要结果：FreezeVLA+GPT 在 ε=4/255 下实现平均ASR 73.3%（SpatialVLA）、95.4%（OpenVLA）、59.8%（π0），并具有强跨提示可迁移性与随预算提升的稳定增益。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>复旦大学</li><li>上海人工智能实验室</li><li>Sea AI Lab</li></ul>\n\n<p>论文概述</p>\n<ul><li>针对视觉-语言-动作（VLA）模型提出并形式化“行动冻结”（action-freezing）攻击：对抗图像使机器人忽略后续指令，进入持续无响应状态，可能在关键时刻导致不作为，引发实际安全风险。</li><li>提出FreezeVLA框架，通过最小-最大双层优化生成跨提示可迁移的对抗图像，在三个SOTA VLA模型与四个机器人基准上实现高攻击成功率，显著优于现有方法。</li></ul>\n\n<p>论文核心贡献点</p>\n<ul><li>识别并形式化VLA模型的“行动冻结”漏洞，揭示与错误动作同等严重的“不作为”威胁。</li><li>提出FreezeVLA：使用内部最大化（对抗提示优化）与外部最小化（对抗图像优化）的双层优化方法，扩展提示嵌入空间覆盖，显著提升跨提示迁移能力。</li><li>跨模型验证：SpatialVLA、OpenVLA、π0 上分别达到平均攻击成功率73.3%、95.4%、59.8%，全面超越随机噪声、PGD、多提示等基线。</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>威胁模型与目标：白盒访问VLA模型、黑盒用户指令，通过图像扰动使模型在任意指令下稳定输出冻结令牌（如<freeze>、<eos>或“do-nothing”token），导致行动终止。</li><li>内部最大化（对抗提示优化）：从初始参考提示集出发，通过梯度分析定位高影响词并进行贪婪同义词替换，迭代生成更难被冻结的“硬提示”集，扩展提示语义空间。</li><li>外部最小化（对抗图像优化）：基于“硬提示”集合，对图像进行梯度聚合与更新，使VLA模型在多种提示下均以高概率输出冻结令牌，实现持久冻结。</li><li>形式化目标：min−max 双层优化，外层在 L∞ 扰动预算 ε 内最小化冻结损失，内层最大化难提示下的损失，使对抗图像在跨提示下保持有效性。</li><li>提示多样性增强：对比“随机采样”与“GPT生成”提示，显示GPT生成的语义多样性进一步提升攻击迁移性。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集与基准：LIBERO-10、LIBERO-Goal、LIBERO-Object、LIBERO-Spatial四个机器人操控基准。</li><li>目标模型：SpatialVLA、OpenVLA、π0 三种SOTA VLA模型（具备动作分块/离散化/连续动作头等不同架构与冻结令牌策略）。</li><li>训练与实验资源：HPC集群，32×NVIDIA A800-SXM4-80GB GPUs。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估指标：攻击成功率（ASR），定义为使VLA模型进入一致冻结状态的对抗图像比例。</li><li>评估设置：</li></ul>\n<p> - 扰动预算：L∞ 约束下的 ε=4/255（主实验），对比 ε∈{1/255,2/255,4/255,8/255,16/255} 的敏感性。</p>\n<p> - 优化步数与提示数：图像迭代K=100、步长α=1/255；参考提示规模\\|P\\|=20；内部提示迭代M=10。</p>\n<p> - 基线对比：随机噪声、Single-Prompt PGD、Multi-Prompt（随机/随机+GPT）、FreezeVLA（随机）与FreezeVLA+GPT。</p>\n<p> - 冻结策略差异：SpatialVLA/π0 使用 <eos>；OpenVLA 使用“do-nothing”token。</p>\n<ul><li>主要结果：FreezeVLA+GPT 在 ε=4/255 下实现平均ASR 73.3%（SpatialVLA）、95.4%（OpenVLA）、59.8%（π0），并具有强跨提示可迁移性与随预算提升的稳定增益。</li></ul>"
  },
  {
    "date": "2025-09-24",
    "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training",
    "link": "http://arxiv.org/abs/2509.19752",
    "summary_markdown": "# 论文研究单位\n- 香港科技大学\n- 微软亚洲研究院\n- 武汉大学\n- 中国科学院大学\n- 清华大学\n- 中南大学大数据研究院\n# 论文概述\n当前视觉-语言-行动（VLA）模型高度依赖大规模人工演示数据，但人工数据采集成本高、可扩展性差。强化学习（RL）能自动生成策略，但对长时程、奖励稀疏的操控任务效果不稳且轨迹方差大。文章提出以扩散模型为核心的RL数据生成方法，利用扩散策略的强表达能力和迭代去噪的隐式正则，获得高质、低方差的轨迹，构建“扩散RL → 数据生成 → VLA训练”的完整管线。在LIBERO基准（130个长时操控任务）上验证，扩散RL生成数据训练出的VLA模型平均成功率81.94%，相比人类数据+5.3%、相比高斯RL+12.6%，并揭示了轨迹平滑、低方差特性是提升VLA性能的关键。\n# 论文核心贡献点\n- 面向VLA训练的扩散RL数据生成框架：两阶段训练（BC暖启动 + 在线PPO），在架构、采样与训练流程上提出稳定化改进。\n- 实证证明扩散RL数据的优越性：在LIBERO的130个任务上显著提高VLA的域内成功率与部分泛化表现。\n- 轨迹质量量化分析：任务效率、轨迹平滑（均方 jerk）、动作一致性（低方差）三个维度解释数据质量提升如何转化为VLA性能增益。\n- 消融验证稳定性：ResNet+U‑Net、DDIM 5步采样、余弦退火学习率、并行环境增强数据多样性等设计的必要性。\n# 论文方法描述\n- 策略与采样\n - 扩散策略：将动作视为由K步扩散过程生成，训练噪声预测网络 εθ(a_k, s_t, k) 反演去噪得到最终动作 a_0。使用DDIM（5步）加快采样、降低方差，保证稳定RL梯度。\n - 两阶段训练\n - 阶段一：行为克隆（BC）暖启动。用少量人类多模态演示训练扩散策略，拟合复杂演示分布（公式1）。\n - 阶段二：在线PPO强化学习微调。以价值网络Vϕ估计状态价值，GAE计算优势，PPO clip目标在每个去噪步上优化策略（公式2–5）。\n- 稳定化策略\n - 架构：ResNet主干+U‑Net解码用于多模态与样本效率；FiLM融合本体感觉（proprioception）稳定条件信号。\n - 学习率：余弦退火，初期鼓励探索，后期稳定收敛。\n - 数据多样性：并行环境扩充经验缓冲，防止高容量扩散策略在相关小批次上过拟合、模式坍缩。\n- VLA训练\n - 专家扩散RL策略收敛后收集数据 D_RL。使用标准行为克隆目标最大化对数似然（公式6）训练VLA，兼容不同策略架构（如π0/OpenVLA）。\n# 论文使用数据集和训练资源\n- 数据集\n - LIBERO基准：包含多套任务共计130个长时操控任务；常用于知识迁移评测。数据源包括人类演示与由不同方法（高斯RL、扩散RL）生成的轨迹。\n- 训练流程资源（基于文中描述）\n - 并行环境用于在线RL采集数据与多样经验回放。\n - 用于对比的生成方法：人类数据（每任务50条演示）、高斯RL数据、扩散RL数据（每任务50条最优轨迹）、人类+扩散RL混合数据。\n - VLA训练遵循统一预处理（如OpenVLA流程），超参数保持一致以保证公平对比。\n# 论文使用的评估环境和评估指标\n- 评估环境\n - LIBERO任务套件，涵盖空间/目标/对象变化与长时任务（LIBERO‑Spatial/Goal/Object/Long），以及由LIBERO‑90与LIBERO‑Long组成的大规模套件（文中称为LIBERO‑100）。\n- 评估指标\n - 成功率（SR）：每套任务50次评估的平均成功率；OOD设定为在LIBERO‑90上训练、零样本评估其他未见任务套件。\n - 轨迹质量分析\n - 任务效率：平均轨迹长度与“无操作”比例（速度≈0且抓持状态不变）。\n - 轨迹平滑度：末端执行器轨迹的均方 jerk，越低越平滑。\n - 动作一致性：同一任务上多条成功轨迹的方差，越低越一致。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>香港科技大学</li><li>微软亚洲研究院</li><li>武汉大学</li><li>中国科学院大学</li><li>清华大学</li><li>中南大学大数据研究院</li></ul>\n<h1>论文概述</h1>\n<p>当前视觉-语言-行动（VLA）模型高度依赖大规模人工演示数据，但人工数据采集成本高、可扩展性差。强化学习（RL）能自动生成策略，但对长时程、奖励稀疏的操控任务效果不稳且轨迹方差大。文章提出以扩散模型为核心的RL数据生成方法，利用扩散策略的强表达能力和迭代去噪的隐式正则，获得高质、低方差的轨迹，构建“扩散RL → 数据生成 → VLA训练”的完整管线。在LIBERO基准（130个长时操控任务）上验证，扩散RL生成数据训练出的VLA模型平均成功率81.94%，相比人类数据+5.3%、相比高斯RL+12.6%，并揭示了轨迹平滑、低方差特性是提升VLA性能的关键。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>面向VLA训练的扩散RL数据生成框架：两阶段训练（BC暖启动 + 在线PPO），在架构、采样与训练流程上提出稳定化改进。</li><li>实证证明扩散RL数据的优越性：在LIBERO的130个任务上显著提高VLA的域内成功率与部分泛化表现。</li><li>轨迹质量量化分析：任务效率、轨迹平滑（均方 jerk）、动作一致性（低方差）三个维度解释数据质量提升如何转化为VLA性能增益。</li><li>消融验证稳定性：ResNet+U‑Net、DDIM 5步采样、余弦退火学习率、并行环境增强数据多样性等设计的必要性。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>策略与采样</li></ul>\n<p> - 扩散策略：将动作视为由K步扩散过程生成，训练噪声预测网络 εθ(a_k, s_t, k) 反演去噪得到最终动作 a_0。使用DDIM（5步）加快采样、降低方差，保证稳定RL梯度。</p>\n<p> - 两阶段训练</p>\n<p> - 阶段一：行为克隆（BC）暖启动。用少量人类多模态演示训练扩散策略，拟合复杂演示分布（公式1）。</p>\n<p> - 阶段二：在线PPO强化学习微调。以价值网络Vϕ估计状态价值，GAE计算优势，PPO clip目标在每个去噪步上优化策略（公式2–5）。</p>\n<ul><li>稳定化策略</li></ul>\n<p> - 架构：ResNet主干+U‑Net解码用于多模态与样本效率；FiLM融合本体感觉（proprioception）稳定条件信号。</p>\n<p> - 学习率：余弦退火，初期鼓励探索，后期稳定收敛。</p>\n<p> - 数据多样性：并行环境扩充经验缓冲，防止高容量扩散策略在相关小批次上过拟合、模式坍缩。</p>\n<ul><li>VLA训练</li></ul>\n<p> - 专家扩散RL策略收敛后收集数据 D_RL。使用标准行为克隆目标最大化对数似然（公式6）训练VLA，兼容不同策略架构（如π0/OpenVLA）。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集</li></ul>\n<p> - LIBERO基准：包含多套任务共计130个长时操控任务；常用于知识迁移评测。数据源包括人类演示与由不同方法（高斯RL、扩散RL）生成的轨迹。</p>\n<ul><li>训练流程资源（基于文中描述）</li></ul>\n<p> - 并行环境用于在线RL采集数据与多样经验回放。</p>\n<p> - 用于对比的生成方法：人类数据（每任务50条演示）、高斯RL数据、扩散RL数据（每任务50条最优轨迹）、人类+扩散RL混合数据。</p>\n<p> - VLA训练遵循统一预处理（如OpenVLA流程），超参数保持一致以保证公平对比。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境</li></ul>\n<p> - LIBERO任务套件，涵盖空间/目标/对象变化与长时任务（LIBERO‑Spatial/Goal/Object/Long），以及由LIBERO‑90与LIBERO‑Long组成的大规模套件（文中称为LIBERO‑100）。</p>\n<ul><li>评估指标</li></ul>\n<p> - 成功率（SR）：每套任务50次评估的平均成功率；OOD设定为在LIBERO‑90上训练、零样本评估其他未见任务套件。</p>\n<p> - 轨迹质量分析</p>\n<p> - 任务效率：平均轨迹长度与“无操作”比例（速度≈0且抓持状态不变）。</p>\n<p> - 轨迹平滑度：末端执行器轨迹的均方 jerk，越低越平滑。</p>\n<p> - 动作一致性：同一任务上多条成功轨迹的方差，越低越一致。</p>"
  },
  {
    "date": "2025-09-23",
    "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action",
    "link": "http://arxiv.org/abs/2509.19571",
    "summary_markdown": "论文研究单位\nUniversité de Montréal；Mila - Quebec AI Institute；Sapienza University of Rome\n\n论文概述\n针对开放词汇自然语言指令的机器人执行难题，论文提出“Agentic Scene Policies”(ASP)。其核心思想是将语言指令分解为三类可查询操作：对象定位、空间推理与部件级交互，并基于显式场景表示(开放词汇对象地图、CLIP语义特征、3D点云与可 affordance 区域)为大型语言模型(LLM)代理提供可调用工具(检索、空间度量与交互技能)。系统支持桌面与移动操控，并通过 afforfance 驱动的技能库(如 tip_push、pinch_pull、hook_pull)实现零样本操作。\n\n论文核心贡献点\n- 统一空间、语义与 affordance 的场景查询式机器人策略(ASP)\n- 在15项桌面操控任务上与主流 VLA(π0-FAST、π0.5)进行系统性比较，报告成功率与进展率\n- 提出移动版 ASP，实现基于 affordance 引导的导航、跨关键帧地图聚合与到目标后的局部重检测\n\n论文方法描述\n- 对象地图(ObjectMap)：包含 Object(几何点云、CLIP语义特征、RGB/Depth裁剪、多视图)与 Affordance(point_cloud、part描述、对应技能)\n- LLM 代理：通过工具链调用与符号状态(当前抓取对象与清单)执行查询计划，不直接访问传感器或完整地图\n- 工具集：\n - object_retrieval：基于文本与CLIP特征的开放词汇对象检索，返回对象键加入清单\n - spatial：距离、左右关系、尺寸等点云几何运算\n - interact：将自然语言动作与目标对象传给内部 affordance 检测与技能调度\n - 技能(基于 AnyGrasp 与运动规划)：grasp、place、drop、grasp_part、tip_push、pinch_pull、hook_pull\n - 反馈与重试：ToolOutput 结构返回成功/反馈信息，支持条件检查与策略重试\n- remapping：技能失败或状态不明时触发地图重算\n- 移动 ASP：新增 go_to 工具，利用目标 affordance 方向计算期望视角并进行代价优化；跨多帧 RGB-D 聚合对象地图；到达后在本地构建 ObjectMap 进行对象重检测\n\n论文使用数据集和训练资源\n- 无专用新数据集；方法在开放词汇场景表示与零样本 VLM 能力之上工作\n- 移动版构建房间级对象地图：通过遥控采集1–5个关键帧，地图构建与合并约每查询<1分钟\n- 计算资源：工作站配备 NVIDIA Titan RTX；RGB-D 通过 Wi-Fi 传输；推理组件包括 SAM/MobileSAM、CLIP、AnyGrasp、SLAM(RTABMap)与导航(Nav2)\n\n论文使用的评估环境和评估指标\n- 桌面操控：UFactory XArm6 + Intel Realsense D435i(腕部)；与 VLA 对比时采用 Franka 臂+DROID 设定以降低分布偏移\n- 移动操控：Agilex Ranger Mini 2.0 移动基座 + XArm6 + 追踪相机 T265\n- 指标：\n - 成功率(10次/任务平均)\n - 进展率(接近、尝试、部分完成等分级评分)\n - 失败模式分析(感知、affordance 检测错误、把柄使用不当等)\n- 实验规模：15项桌面任务，共540次尝试；移动端进行双物品放置、指尖/拉拔、空间区分等多类查询",
    "summary_html": "<p>论文研究单位</p>\n<p>Université de Montréal；Mila - Quebec AI Institute；Sapienza University of Rome</p>\n\n<p>论文概述</p>\n<p>针对开放词汇自然语言指令的机器人执行难题，论文提出“Agentic Scene Policies”(ASP)。其核心思想是将语言指令分解为三类可查询操作：对象定位、空间推理与部件级交互，并基于显式场景表示(开放词汇对象地图、CLIP语义特征、3D点云与可 affordance 区域)为大型语言模型(LLM)代理提供可调用工具(检索、空间度量与交互技能)。系统支持桌面与移动操控，并通过 afforfance 驱动的技能库(如 tip_push、pinch_pull、hook_pull)实现零样本操作。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>统一空间、语义与 affordance 的场景查询式机器人策略(ASP)</li><li>在15项桌面操控任务上与主流 VLA(π0-FAST、π0.5)进行系统性比较，报告成功率与进展率</li><li>提出移动版 ASP，实现基于 affordance 引导的导航、跨关键帧地图聚合与到目标后的局部重检测</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>对象地图(ObjectMap)：包含 Object(几何点云、CLIP语义特征、RGB/Depth裁剪、多视图)与 Affordance(point_cloud、part描述、对应技能)</li><li>LLM 代理：通过工具链调用与符号状态(当前抓取对象与清单)执行查询计划，不直接访问传感器或完整地图</li><li>工具集：</li></ul>\n<p> - object_retrieval：基于文本与CLIP特征的开放词汇对象检索，返回对象键加入清单</p>\n<p> - spatial：距离、左右关系、尺寸等点云几何运算</p>\n<p> - interact：将自然语言动作与目标对象传给内部 affordance 检测与技能调度</p>\n<p> - 技能(基于 AnyGrasp 与运动规划)：grasp、place、drop、grasp_part、tip_push、pinch_pull、hook_pull</p>\n<p> - 反馈与重试：ToolOutput 结构返回成功/反馈信息，支持条件检查与策略重试</p>\n<ul><li>remapping：技能失败或状态不明时触发地图重算</li><li>移动 ASP：新增 go_to 工具，利用目标 affordance 方向计算期望视角并进行代价优化；跨多帧 RGB-D 聚合对象地图；到达后在本地构建 ObjectMap 进行对象重检测</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>无专用新数据集；方法在开放词汇场景表示与零样本 VLM 能力之上工作</li><li>移动版构建房间级对象地图：通过遥控采集1–5个关键帧，地图构建与合并约每查询<1分钟</li><li>计算资源：工作站配备 NVIDIA Titan RTX；RGB-D 通过 Wi-Fi 传输；推理组件包括 SAM/MobileSAM、CLIP、AnyGrasp、SLAM(RTABMap)与导航(Nav2)</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>桌面操控：UFactory XArm6 + Intel Realsense D435i(腕部)；与 VLA 对比时采用 Franka 臂+DROID 设定以降低分布偏移</li><li>移动操控：Agilex Ranger Mini 2.0 移动基座 + XArm6 + 追踪相机 T265</li><li>指标：</li></ul>\n<p> - 成功率(10次/任务平均)</p>\n<p> - 进展率(接近、尝试、部分完成等分级评分)</p>\n<p> - 失败模式分析(感知、affordance 检测错误、把柄使用不当等)</p>\n<ul><li>实验规模：15项桌面任务，共540次尝试；移动端进行双物品放置、指尖/拉拔、空间区分等多类查询</li></ul>"
  },
  {
    "date": "2025-09-23",
    "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation",
    "link": "http://arxiv.org/abs/2509.19480",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-23",
    "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
    "link": "http://arxiv.org/abs/2509.19012",
    "summary_markdown": "# Pure Vision-Language-Action (VLA) Models: A Comprehensive Survey\n## 论文研究单位\n\n- 兰州大学信息科学与工程学院（中国）\n- 新加坡国立大学NExT++研究中心（新加坡）\n- 中国科学院计算技术研究所（中国）\n## 论文概述\n\n本论文是关于纯 Vision-Language-Action (VLA) 模型的综合调研，系统地回顾了该领域的研究进展。论文从传统基于策略的控制方法出发，聚焦于 VLA 如何将 Vision-Language Models (VLMs) 从被动序列生成器转变为主动的机器人和决策制定代理。调研覆盖了超过300项近期研究，提供了清晰的方法分类体系，分析了 VLA 在不同应用场景中的表现，并提出了未来研究的关键挑战和发展方向。\n## 论文核心贡献点\n\n- **系统化分类体系**：提出了基于动作生成策略的 VLA 方法分类，将现有方法分为自回归、扩散、强化学习和混合/专用方法四大类别\n- **方法创新总结**：深入分析了每个类别的方法动机、核心策略和实现机制，强调了定义性特征和技术创新\n- **资源全面概述**：提供了 VLA 模型训练和评估所需的关联资源（数据集、基准测试、仿真平台）的详细概览\n- **挑战与机遇分析**：识别了现有技术的关键局限性，并提出了潜在的研究探索方向\n## 论文方法描述\n\n论文将 VLA 方法分为以下四个主要类别：\n### 1. 自回归模型\n- 将动作序列视为时间依赖过程，步进式生成动作\n- 代表性方法包括 Gato、RT-1/RT-2、PaLM-E 等\n- 进一步细分为：通用 VLA 方法、LLM 驱动的语义规划、轨迹生成与视觉对齐、结构优化与高效推理机制\n### 2. 扩散模型\n- 将动作生成建模为条件扩散过程\n- 代表性方法包括 SE(3)-DiffusionFields、Diffusion Policy、3D Diffuser Actor 等\n- 涵盖：扩散式通用 VLA 方法、多模态架构融合、应用优化与部署\n### 3. 强化学习模型\n- 基于强化学习的 VLA 微调策略\n- 通过奖励机制优化策略性能\n### 4. 混合与专用方法\n- 结合多种范式的混合架构\n- 针对特定领域和应用的专门设计\n## 论文使用数据集和训练资源\n### 真实世界数据集\n- **Open X-Embodiment (OXE)**：整合了22个机器人数据集，包含527种技能和160,266个任务\n- **BridgeData**：涵盖10个环境中的71个任务\n- 自主驾驶领域的相关数据集\n### 仿真数据集\n- **Open X-Embodiment (OXE)** 仿真版本\n- **BridgeData** 仿真版本\n- 自主驾驶仿真数据集\n### 仿真平台\n- **THOR、Habitat**：室内环境仿真\n- **MuJoCo、Isaac Gym**：物理仿真\n- **CARLA**：自动驾驶仿真\n- 提供可扩展的虚拟环境，支持多模态标注生成\n## 论文使用的评估环境和评估指标\n\n论文主要围绕以下应用领域进行评估：\n### 机器人操作\n- 机械臂控制\n- 四足机器人\n- 人形机器人\n- 移动机器人\n### 自主驾驶\n- 2D/3D 感知融合\n- 轨迹预测\n- 闭环控制\n### 评估指标\n- 任务成功率\n- 轨迹精度\n- 跨平台泛化能力\n- 零样本/少样本性能\n- 实时推理效率\n- 多模态对齐质量\n\n论文强调需要建立更全面的评估框架，特别关注长时序任务稳定性、语义对齐鲁棒性和实际部署效率等关键指标。",
    "summary_html": "<h1>Pure Vision-Language-Action (VLA) Models: A Comprehensive Survey</h1>\n<h2>论文研究单位</h2>\n\n<ul><li>兰州大学信息科学与工程学院（中国）</li><li>新加坡国立大学NExT++研究中心（新加坡）</li><li>中国科学院计算技术研究所（中国）</li></ul>\n<h2>论文概述</h2>\n\n<p>本论文是关于纯 Vision-Language-Action (VLA) 模型的综合调研，系统地回顾了该领域的研究进展。论文从传统基于策略的控制方法出发，聚焦于 VLA 如何将 Vision-Language Models (VLMs) 从被动序列生成器转变为主动的机器人和决策制定代理。调研覆盖了超过300项近期研究，提供了清晰的方法分类体系，分析了 VLA 在不同应用场景中的表现，并提出了未来研究的关键挑战和发展方向。</p>\n<h2>论文核心贡献点</h2>\n\n<ul><li><strong>系统化分类体系</strong>：提出了基于动作生成策略的 VLA 方法分类，将现有方法分为自回归、扩散、强化学习和混合/专用方法四大类别</li><li><strong>方法创新总结</strong>：深入分析了每个类别的方法动机、核心策略和实现机制，强调了定义性特征和技术创新</li><li><strong>资源全面概述</strong>：提供了 VLA 模型训练和评估所需的关联资源（数据集、基准测试、仿真平台）的详细概览</li><li><strong>挑战与机遇分析</strong>：识别了现有技术的关键局限性，并提出了潜在的研究探索方向</li></ul>\n<h2>论文方法描述</h2>\n\n<p>论文将 VLA 方法分为以下四个主要类别：</p>\n<h3>1. 自回归模型</h3>\n<ul><li>将动作序列视为时间依赖过程，步进式生成动作</li><li>代表性方法包括 Gato、RT-1/RT-2、PaLM-E 等</li><li>进一步细分为：通用 VLA 方法、LLM 驱动的语义规划、轨迹生成与视觉对齐、结构优化与高效推理机制</li></ul>\n<h3>2. 扩散模型</h3>\n<ul><li>将动作生成建模为条件扩散过程</li><li>代表性方法包括 SE(3)-DiffusionFields、Diffusion Policy、3D Diffuser Actor 等</li><li>涵盖：扩散式通用 VLA 方法、多模态架构融合、应用优化与部署</li></ul>\n<h3>3. 强化学习模型</h3>\n<ul><li>基于强化学习的 VLA 微调策略</li><li>通过奖励机制优化策略性能</li></ul>\n<h3>4. 混合与专用方法</h3>\n<ul><li>结合多种范式的混合架构</li><li>针对特定领域和应用的专门设计</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<h3>真实世界数据集</h3>\n<ul><li><strong>Open X-Embodiment (OXE)</strong>：整合了22个机器人数据集，包含527种技能和160,266个任务</li><li><strong>BridgeData</strong>：涵盖10个环境中的71个任务</li><li>自主驾驶领域的相关数据集</li></ul>\n<h3>仿真数据集</h3>\n<ul><li><strong>Open X-Embodiment (OXE)</strong> 仿真版本</li><li><strong>BridgeData</strong> 仿真版本</li><li>自主驾驶仿真数据集</li></ul>\n<h3>仿真平台</h3>\n<ul><li><strong>THOR、Habitat</strong>：室内环境仿真</li><li><strong>MuJoCo、Isaac Gym</strong>：物理仿真</li><li><strong>CARLA</strong>：自动驾驶仿真</li><li>提供可扩展的虚拟环境，支持多模态标注生成</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n\n<p>论文主要围绕以下应用领域进行评估：</p>\n<h3>机器人操作</h3>\n<ul><li>机械臂控制</li><li>四足机器人</li><li>人形机器人</li><li>移动机器人</li></ul>\n<h3>自主驾驶</h3>\n<ul><li>2D/3D 感知融合</li><li>轨迹预测</li><li>闭环控制</li></ul>\n<h3>评估指标</h3>\n<ul><li>任务成功率</li><li>轨迹精度</li><li>跨平台泛化能力</li><li>零样本/少样本性能</li><li>实时推理效率</li><li>多模态对齐质量</li></ul>\n\n<p>论文强调需要建立更全面的评估框架，特别关注长时序任务稳定性、语义对齐鲁棒性和实际部署效率等关键指标。</p>"
  },
  {
    "date": "2025-09-23",
    "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
    "link": "http://arxiv.org/abs/2509.18953",
    "summary_markdown": "### 论文研究单位\n上海交通大学（人工智能学院重点实验室）、中国军事科学院国防创新研究院、智能博弈决策实验室\n### 论文概述\n当前视觉-语言-动作（VLA）模型在真实部署中易受物理变化影响（如光照、物体位置干扰），但缺乏系统性评估工具。本文提出Eva-VLA框架，将离散物理变化转化为连续参数优化问题，通过黑箱优化算法探索最恶劣场景，暴露了当前VLA模型在多种物理干扰下的严重脆弱性。\n### 论文核心贡献点\n1. **首次系统性分解物理变化**：将真实世界变化分为物体3D变换（旋转）、光照变化、对抗补丁三类，突破传统梯度攻击限制\n2. **统一参数化评估框架**：将各类变化转化为连续参数分布，通过仿真环境可复现地评估鲁棒性\n3. **揭示VLA系统脆弱性**：在OpenVLA等先进模型中验证，物理干扰使失败率从23.5%激增至82.6%，长时序任务达97.8%\n### 论文方法描述\n#### 三类物理变化参数化\n- **物体3D变换**：用Tait-Bryan角α,β,γ∈[-90°,90°]约束旋转参数\n- **光照变化**：通过高斯衰减函数建模点光源L(z)=I·exp(-\\|\\|z-(x,y)\\|\\|²/2σ²)，参数λ={x,y,σ,I}控制位置/强度\n- **对抗补丁**：用自然图像在桌面纹理上优化位置φ={x,y}∈[(W/3,2W/3)×(H/3,2H/3)]\n#### 黑箱优化算法\n使用CMA-ES（协方差矩阵自适应进化策略）优化参数分布：\n1. 将变化参数建模为多元高斯分布N(μ,Σ²C)\n2. 迭代采样配置，计算对抗损失函数L_adv=-∑cos(A_clean, A_adv)\n3. 基于损失值更新分布参数μ, C, Σ\n4. 引入学习率自适应和早停机制加速收敛\n### 论文使用数据集和训练资源\n- **仿真数据集**：LIBERO基准套件（包含空间/物体/目标/长时序四类任务，每类10任务×50轮试验）\n- **真实数据集**：BridgeData v2（用于部分真实场景验证）\n- **硬件资源**：NVIDIA A800 GPU (80GB内存) + AgileX Piper机械臂（7自由度） + RealSense D435i相机\n### 论文使用的评估环境和指标\n- **仿真评估**：基于LIBERO套件，按任务最大步长设超时标准，统计失败率FR=1-SR（成功率）\n- **真实评估**：在三任务（抓取/定位/放置）上执行50次试验，攻击成功率44.6%\n- **鲁棒性指标**：\n - 失败率变化率：清洁环境(23.5%) vs 物理干扰(82.6%)\n - 不同干扰类型影响：物体变换>对抗补丁>光照变化\n - 时序影响：长时序任务在干扰下失败率超97%",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学（人工智能学院重点实验室）、中国军事科学院国防创新研究院、智能博弈决策实验室</p>\n<h3>论文概述</h3>\n<p>当前视觉-语言-动作（VLA）模型在真实部署中易受物理变化影响（如光照、物体位置干扰），但缺乏系统性评估工具。本文提出Eva-VLA框架，将离散物理变化转化为连续参数优化问题，通过黑箱优化算法探索最恶劣场景，暴露了当前VLA模型在多种物理干扰下的严重脆弱性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首次系统性分解物理变化</strong>：将真实世界变化分为物体3D变换（旋转）、光照变化、对抗补丁三类，突破传统梯度攻击限制</li><li><strong>统一参数化评估框架</strong>：将各类变化转化为连续参数分布，通过仿真环境可复现地评估鲁棒性</li><li><strong>揭示VLA系统脆弱性</strong>：在OpenVLA等先进模型中验证，物理干扰使失败率从23.5%激增至82.6%，长时序任务达97.8%</li></ol>\n<h3>论文方法描述</h3>\n<h4>三类物理变化参数化</h4>\n<ul><li><strong>物体3D变换</strong>：用Tait-Bryan角α,β,γ∈[-90°,90°]约束旋转参数</li><li><strong>光照变化</strong>：通过高斯衰减函数建模点光源L(z)=I·exp(-\\|\\|z-(x,y)\\|\\|²/2σ²)，参数λ={x,y,σ,I}控制位置/强度</li><li><strong>对抗补丁</strong>：用自然图像在桌面纹理上优化位置φ={x,y}∈[(W/3,2W/3)×(H/3,2H/3)]</li></ul>\n<h4>黑箱优化算法</h4>\n<p>使用CMA-ES（协方差矩阵自适应进化策略）优化参数分布：</p>\n<ol><li>将变化参数建模为多元高斯分布N(μ,Σ²C)</li><li>迭代采样配置，计算对抗损失函数L_adv=-∑cos(A_clean, A_adv)</li><li>基于损失值更新分布参数μ, C, Σ</li><li>引入学习率自适应和早停机制加速收敛</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>仿真数据集</strong>：LIBERO基准套件（包含空间/物体/目标/长时序四类任务，每类10任务×50轮试验）</li><li><strong>真实数据集</strong>：BridgeData v2（用于部分真实场景验证）</li><li><strong>硬件资源</strong>：NVIDIA A800 GPU (80GB内存) + AgileX Piper机械臂（7自由度） + RealSense D435i相机</li></ul>\n<h3>论文使用的评估环境和指标</h3>\n<ul><li><strong>仿真评估</strong>：基于LIBERO套件，按任务最大步长设超时标准，统计失败率FR=1-SR（成功率）</li><li><strong>真实评估</strong>：在三任务（抓取/定位/放置）上执行50次试验，攻击成功率44.6%</li><li><strong>鲁棒性指标</strong>：</li></ul>\n<p> - 失败率变化率：清洁环境(23.5%) vs 物理干扰(82.6%)</p>\n<p> - 不同干扰类型影响：物体变换>对抗补丁>光照变化</p>\n<p> - 时序影响：长时序任务在干扰下失败率超97%</p>"
  },
  {
    "date": "2025-09-23",
    "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
    "link": "http://arxiv.org/abs/2509.18865",
    "summary_markdown": "# 论文研究单位\n- D3 Center, The University of Osaka\n- Graduate School of Information Science and Technology, The University of Osaka\n- Graduate School of Maritime Sciences, Kobe University\n# 论文概述\n提出 Bi-VLA（Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation），将双侧控制（bilateral control）的位置与力信息、视觉观测与自然语言指令融合，构建单模型多任务策略，在真实机器人上完成抓取-搬运-放置任务，相比传统双侧控制式模仿学习在任务成功率与适应性上显著提升。\n# 论文核心贡献点\n- 融合视觉与语言：在双侧控制式模仿学习框架中引入 SigLIP 语言编码与 FiLM 调制，将视觉与语言对齐后统一嵌入决策。\n- 单模型多任务：打破既往双侧控制方法仅能单任务建模的局限，实现一个策略在语言可区分与视觉可区分两类任务间灵活切换。\n- 真实机器人验证：在两类任务与干扰环境中取得更高成功率，证明视觉-语言融合在接触丰富与动态环境下的有效性。\n# 论文方法描述\n- 数据采集：采用四通道双侧控制，领导者与跟随者实时交换位置与力矩；关节角度、速度与外力矩由编码器、DOB 与 RFOB 估计；同步采集 RGB 图像与自然语言指令，形成多模态演示。\n- 学习模型：文本用 SigLIP 编码，视觉用 EfficientNet 提取特征，经 FiLM 进行跨模态融合；融合特征与机器人关节状态共同输入 Transformer 驱动的 CVAE，输出动作块（领导者关节角度、速度、力矩）。\n- 推理流程：给定跟随者当前关节状态、同步图像与语言指令，模型预测下一步动作块，经双侧控制转换为电流指令执行，实现闭环控制与灵活任务切换。\n# 论文使用数据集和训练资源\n- 硬件：OpenManipulator-X 机械臂两台（领导/跟随），两个 RGB 摄像头（俯视与夹爪侧）。\n- 数据频率：双侧控制 1000 Hz，图像 100 Hz；每臂 15 维状态（5 关节 × 角度/速度/力矩），合计 30 维联合状态。\n- 任务演示：每任务收集 6 条演示（上/下各 3）；为评估跨任务低数据泛化，同时收集 4 条混合演示（上/下目标与上/下源各 1）。\n- 数据增强：采用 DABI 将演示从 1000 Hz 下采样至 100 Hz 与图像对齐，6 条演示扩展至 60 条，4 条混合演示扩展至 40 条。\n- 模型配置：Transformer 编码器 4 层、解码器 7 层，CVAE 结构，动作块预测。\n# 论文使用的评估环境和评估指标\n- 评估环境：两类任务——语言可区分任务（目标位置由指令指定）、视觉可区分任务（拾取位置由场景指定），以及未学习的三球干扰环境。\n- 评估指标：分阶段成功率（Pick/Move/Place）与总体成功率（各 10 次独立试验，仅当所有阶段均完成视为成功）。\n- 对比模型：Bi-ACT（无语言）、Bi-VLA（DistilBERT）、Bi-VLA（SigLIP）、Bi-VLA（SigLIP-Mix）。\n- 关键结果：\n - 语言可区分任务：Bi-ACT 仅 50%（仅 Up 成功），Bi-VLA（DistilBERT）60%，Bi-VLA（SigLIP）90%，Bi-VLA（SigLIP-Mix）70%。\n - 视觉可区分任务：Bi-ACT 95%，Bi-VLA（SigLIP）90%，Bi-VLA（SigLIP-Mix）90%。\n - 未学习三球环境：Bi-ACT 50%，Bi-VLA（SigLIP）75%，Bi-VLA（SigLIP-Mix）75%。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>D3 Center, The University of Osaka</li><li>Graduate School of Information Science and Technology, The University of Osaka</li><li>Graduate School of Maritime Sciences, Kobe University</li></ul>\n<h1>论文概述</h1>\n<p>提出 Bi-VLA（Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation），将双侧控制（bilateral control）的位置与力信息、视觉观测与自然语言指令融合，构建单模型多任务策略，在真实机器人上完成抓取-搬运-放置任务，相比传统双侧控制式模仿学习在任务成功率与适应性上显著提升。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>融合视觉与语言：在双侧控制式模仿学习框架中引入 SigLIP 语言编码与 FiLM 调制，将视觉与语言对齐后统一嵌入决策。</li><li>单模型多任务：打破既往双侧控制方法仅能单任务建模的局限，实现一个策略在语言可区分与视觉可区分两类任务间灵活切换。</li><li>真实机器人验证：在两类任务与干扰环境中取得更高成功率，证明视觉-语言融合在接触丰富与动态环境下的有效性。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>数据采集：采用四通道双侧控制，领导者与跟随者实时交换位置与力矩；关节角度、速度与外力矩由编码器、DOB 与 RFOB 估计；同步采集 RGB 图像与自然语言指令，形成多模态演示。</li><li>学习模型：文本用 SigLIP 编码，视觉用 EfficientNet 提取特征，经 FiLM 进行跨模态融合；融合特征与机器人关节状态共同输入 Transformer 驱动的 CVAE，输出动作块（领导者关节角度、速度、力矩）。</li><li>推理流程：给定跟随者当前关节状态、同步图像与语言指令，模型预测下一步动作块，经双侧控制转换为电流指令执行，实现闭环控制与灵活任务切换。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>硬件：OpenManipulator-X 机械臂两台（领导/跟随），两个 RGB 摄像头（俯视与夹爪侧）。</li><li>数据频率：双侧控制 1000 Hz，图像 100 Hz；每臂 15 维状态（5 关节 × 角度/速度/力矩），合计 30 维联合状态。</li><li>任务演示：每任务收集 6 条演示（上/下各 3）；为评估跨任务低数据泛化，同时收集 4 条混合演示（上/下目标与上/下源各 1）。</li><li>数据增强：采用 DABI 将演示从 1000 Hz 下采样至 100 Hz 与图像对齐，6 条演示扩展至 60 条，4 条混合演示扩展至 40 条。</li><li>模型配置：Transformer 编码器 4 层、解码器 7 层，CVAE 结构，动作块预测。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：两类任务——语言可区分任务（目标位置由指令指定）、视觉可区分任务（拾取位置由场景指定），以及未学习的三球干扰环境。</li><li>评估指标：分阶段成功率（Pick/Move/Place）与总体成功率（各 10 次独立试验，仅当所有阶段均完成视为成功）。</li><li>对比模型：Bi-ACT（无语言）、Bi-VLA（DistilBERT）、Bi-VLA（SigLIP）、Bi-VLA（SigLIP-Mix）。</li><li>关键结果：</li></ul>\n<p> - 语言可区分任务：Bi-ACT 仅 50%（仅 Up 成功），Bi-VLA（DistilBERT）60%，Bi-VLA（SigLIP）90%，Bi-VLA（SigLIP-Mix）70%。</p>\n<p> - 视觉可区分任务：Bi-ACT 95%，Bi-VLA（SigLIP）90%，Bi-VLA（SigLIP-Mix）90%。</p>\n<p> - 未学习三球环境：Bi-ACT 50%，Bi-VLA（SigLIP）75%，Bi-VLA（SigLIP-Mix）75%。</p>"
  },
  {
    "date": "2025-09-22",
    "title": "Latent Action Pretraining Through World Modeling",
    "link": "http://arxiv.org/abs/2509.18428",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n\nMohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE\nAlexandria University, Alexandria, Egypt\n## 论文概述\n\n本文提出了LAWM框架，通过世界建模从无标签视频数据中学习潜在动作表示，用于预训练模仿学习模型。该框架能够从机器人录制视频或人类操作日常物体的视频中进行自监督学习，无需依赖人工标注的动作数据。框架设计为模型无关，可以跨任务、环境和具身形态进行迁移，在LIBERO基准测试和真实世界设置中的表现优于使用真实机器人动作训练的模型和类似的预训练方法，同时更高效且适用于实际场景。\n## 论文核心贡献点\n\n1. 提出LAWM框架，这是一个模型无关的框架，可以从机器人和人类视频中学习动作块表示，无需动作标签\n2. 实验表明该框架可以从人类演示和机器人操作视频中学习优于监督预训练的动作先验，且无需使用真实动作标签\n3. 使用小型模型（BAKU和Dreamerv3）的框架在LIBERO基准测试上优于使用大型模型的类似方法（villa-X）\n## 论文方法描述\n\nLAWM包含两个阶段：\n\n潜在动作预训练阶段：\n- 输入包括图像帧和自然语言指令\n- 模仿学习模型处理输入产生n个潜在动作表示\n- 这些潜在动作与当前帧和后续n-1帧配对，输入世界模型\n- 世界模型基于RSSM架构，包含编码器、动态模型和解码器\n- 通过预测未来图像帧进行端到端训练，损失函数包括MSE重建损失和KL散度正则化项\n- 学习信号来自预测视频序列中的下一帧图像\n\n动作微调阶段：\n- 预训练的模仿学习模型适配到下游机器人任务\n- 不再使用世界模型\n- 使用标注演示数据将观察（图像、语言指令和机器人状态）直接映射到真实动作\n- 对BAKU采用负对数似然损失，对Diffusion Policy采用去噪扩散损失\n## 论文使用数据集和训练资源\n\n数据集：\n- BridgeData v2：60,096条轨迹，24个环境，包含拾取放置、推动、折叠等任务\n- Something-Something v2：220,847个人类操作日常物体的视频片段（使用其中10%用于预训练）\n- LIBERO基准测试：包含LIBERO-90（90个任务）和四个任务套件（Spatial、Object、Goal、Long），每个套件10个任务，每个任务50个演示样本\n- 真实世界自定义数据集：5个任务（3个拾取放置、1个堆叠、1个移动），每个任务50个演示，使用VR控制器收集\n\n训练资源：\n- 单个A100 GPU\n- DreamerV3世界模型使用50M参数配置\n- 潜在动作空间维度为7（与真实动作相同）\n- BAKU实验的动作块大小为10，Diffusion Policy实验为16\n- 在BridgeData v2或Something-Something v2上预训练约30小时\n- 在LIBERO-90上微调约24小时\n- 在LIBERO任务套件上微调约2小时\n- 真实世界数据集微调约20分钟\n## 论文使用的评估环境和评估指标\n\n评估环境：\n- LIBERO-90基准测试：90个多样化任务\n- LIBERO任务套件：\n - LIBERO-Spatial：新布局下的相同任务和对象类型\n - LIBERO-Object：新对象类型下的相同任务和布局\n - LIBERO-Goal：新任务下的相同对象类型和布局\n - LIBERO-Long：长时域任务，包含多样化的对象、布局和背景\n- 真实世界设置：6自由度Realman机器人臂，配备1自由度夹爪，双视角相机观测\n\n评估指标：\n- 成功率（Success Rate, SR）：成功试验次数占总尝试次数的百分比\n- 每个任务进行10次评估试验\n- 使用典型相关分析（CCA）的第一规范分量的Pearson相关系数来量化潜在动作与真实动作之间的对齐程度",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n\n<p>Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE</p>\n<p>Alexandria University, Alexandria, Egypt</p>\n<h2>论文概述</h2>\n\n<p>本文提出了LAWM框架，通过世界建模从无标签视频数据中学习潜在动作表示，用于预训练模仿学习模型。该框架能够从机器人录制视频或人类操作日常物体的视频中进行自监督学习，无需依赖人工标注的动作数据。框架设计为模型无关，可以跨任务、环境和具身形态进行迁移，在LIBERO基准测试和真实世界设置中的表现优于使用真实机器人动作训练的模型和类似的预训练方法，同时更高效且适用于实际场景。</p>\n<h2>论文核心贡献点</h2>\n\n<ol><li>提出LAWM框架，这是一个模型无关的框架，可以从机器人和人类视频中学习动作块表示，无需动作标签</li><li>实验表明该框架可以从人类演示和机器人操作视频中学习优于监督预训练的动作先验，且无需使用真实动作标签</li><li>使用小型模型（BAKU和Dreamerv3）的框架在LIBERO基准测试上优于使用大型模型的类似方法（villa-X）</li></ol>\n<h2>论文方法描述</h2>\n\n<p>LAWM包含两个阶段：</p>\n\n<p>潜在动作预训练阶段：</p>\n<ul><li>输入包括图像帧和自然语言指令</li><li>模仿学习模型处理输入产生n个潜在动作表示</li><li>这些潜在动作与当前帧和后续n-1帧配对，输入世界模型</li><li>世界模型基于RSSM架构，包含编码器、动态模型和解码器</li><li>通过预测未来图像帧进行端到端训练，损失函数包括MSE重建损失和KL散度正则化项</li><li>学习信号来自预测视频序列中的下一帧图像</li></ul>\n\n<p>动作微调阶段：</p>\n<ul><li>预训练的模仿学习模型适配到下游机器人任务</li><li>不再使用世界模型</li><li>使用标注演示数据将观察（图像、语言指令和机器人状态）直接映射到真实动作</li><li>对BAKU采用负对数似然损失，对Diffusion Policy采用去噪扩散损失</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n\n<p>数据集：</p>\n<ul><li>BridgeData v2：60,096条轨迹，24个环境，包含拾取放置、推动、折叠等任务</li><li>Something-Something v2：220,847个人类操作日常物体的视频片段（使用其中10%用于预训练）</li><li>LIBERO基准测试：包含LIBERO-90（90个任务）和四个任务套件（Spatial、Object、Goal、Long），每个套件10个任务，每个任务50个演示样本</li><li>真实世界自定义数据集：5个任务（3个拾取放置、1个堆叠、1个移动），每个任务50个演示，使用VR控制器收集</li></ul>\n\n<p>训练资源：</p>\n<ul><li>单个A100 GPU</li><li>DreamerV3世界模型使用50M参数配置</li><li>潜在动作空间维度为7（与真实动作相同）</li><li>BAKU实验的动作块大小为10，Diffusion Policy实验为16</li><li>在BridgeData v2或Something-Something v2上预训练约30小时</li><li>在LIBERO-90上微调约24小时</li><li>在LIBERO任务套件上微调约2小时</li><li>真实世界数据集微调约20分钟</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n\n<p>评估环境：</p>\n<ul><li>LIBERO-90基准测试：90个多样化任务</li><li>LIBERO任务套件：</li></ul>\n<p> - LIBERO-Spatial：新布局下的相同任务和对象类型</p>\n<p> - LIBERO-Object：新对象类型下的相同任务和布局</p>\n<p> - LIBERO-Goal：新任务下的相同对象类型和布局</p>\n<p> - LIBERO-Long：长时域任务，包含多样化的对象、布局和背景</p>\n<ul><li>真实世界设置：6自由度Realman机器人臂，配备1自由度夹爪，双视角相机观测</li></ul>\n\n<p>评估指标：</p>\n<ul><li>成功率（Success Rate, SR）：成功试验次数占总尝试次数的百分比</li><li>每个任务进行10次评估试验</li><li>使用典型相关分析（CCA）的第一规范分量的Pearson相关系数来量化潜在动作与真实动作之间的对齐程度</li></ul>"
  },
  {
    "date": "2025-09-22",
    "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
    "link": "http://arxiv.org/abs/2509.18282",
    "summary_markdown": "## 论文研究单位\nUniversity of Washington, NVIDIA, University of Southern California, Allen Institute for AI。\n## 论文概述\n当前机器人操控策略往往同时学习“关注哪里（where）”“做什么（what）”“如何执行（how）”，导致泛化能力受限。该论文提出将“where/what”的高层语义推理交由视觉语言模型（VLM）承担，使低层策略专注“how”。为此提出了PEEK（Policy-agnostic Extraction of Essential Keypoints），一个策略无关的最小中间表示框架：由VLM预测2D末端执行器路径（what）与任务相关掩蔽点（where），并将标注直接叠加到观测图像上，再由任意RGB/RGB-D策略进行训练与推理。在535次真实世界评估中，PEEK显著提升零样本泛化，包括模拟训练的3D策略在真实场景中成功率提升41.4倍，以及对大型VLA与小型操控策略分别带来2–3.5倍增益。\n## 论文核心贡献点\n- 统一的点式中间表示：将“路径+掩蔽点”作为策略输入，实现策略无关、可迁移的表示。\n- 可扩展标注流水线：自动从机器人视频中提取任务相关点与末端执行器路径，支持不同视角与小目标。\n- 多策略与多具身验证：在2D/3D、不同规模策略与两种真实机械臂系统上统一验证。\n- 零样本泛化实证：面对视觉干扰与语义新任务大幅提升成功率与鲁棒性。\n## 论文方法描述\n- 目标表示：联合预测两个点集——末端执行器的2D轨迹点 p_t 与任务相关掩蔽点 m_t，两者作为自然语言响应由VLM输出。\n- VLM微调：以VILA-1.5-3B为基座，整合机器人与通用点预测/VQA数据；联合优化路径与掩蔽点预测，训练约20小时（A100×8）。\n- 数据标注流水线：先用点跟踪（CoTracker3）识别场景中显著移动的点集作为“任务相关点”，再以检测器+掩蔽机制构建末端执行器路径；通过“停止点数”K-Means将长轨迹切分为更短的“子轨迹”，以提高表示的最小性与可预测性。\n- 策略接口：推理时每隔H步查询VLM，将路径与掩蔽点以可视化方式叠加到观测图像上，供任意RGB或RGB-D策略训练与执行。掩蔽通过对预测点为中心的8%边长区域开窗；路径以随时间颜色渐变的线段表示。\n## 论文使用数据集和训练资源\n- 数据来源：Open X-Embodiment（OXE）20+子数据集、DROID、LIBERO-90、BRIDGE-v2、RoboPoint；总计2M+问答/点预测对、148k轨迹、9种具身。\n- 训练资源：VLM微调在8×NVIDIA A100上约20小时；推理在RTX 3090上单次查询约4–6秒。\n- 工具/模型：CoTracker3点跟踪、Detectron2末端执行器检测、FoundationStereo深度估计。\n## 论文使用的评估环境和评估指标\n- 评估环境：\n - Franka（Sim-to-Real）：仿真采集2.5k条“堆叠彩色方块”轨迹；真实世界Zed 2立体相机+FoundationStereoDepth；评估Basic/Clutter/Semantic三类任务。\n - WidowX（BRIDGE）：单目RGB环境，改换桌面与背景；在Basic/Clutter/Semantic三类任务上评估。\n- 评估指标：任务完成率与成功率（平均与分项），包含抓取/到达的Partial Credit；零样本跨场景与跨语义泛化能力；消融实验使用路径/掩蔽的组合成功率；VLM质量使用DTW、起点/终点L2、IoU。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>University of Washington, NVIDIA, University of Southern California, Allen Institute for AI。</p>\n<h2>论文概述</h2>\n<p>当前机器人操控策略往往同时学习“关注哪里（where）”“做什么（what）”“如何执行（how）”，导致泛化能力受限。该论文提出将“where/what”的高层语义推理交由视觉语言模型（VLM）承担，使低层策略专注“how”。为此提出了PEEK（Policy-agnostic Extraction of Essential Keypoints），一个策略无关的最小中间表示框架：由VLM预测2D末端执行器路径（what）与任务相关掩蔽点（where），并将标注直接叠加到观测图像上，再由任意RGB/RGB-D策略进行训练与推理。在535次真实世界评估中，PEEK显著提升零样本泛化，包括模拟训练的3D策略在真实场景中成功率提升41.4倍，以及对大型VLA与小型操控策略分别带来2–3.5倍增益。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>统一的点式中间表示：将“路径+掩蔽点”作为策略输入，实现策略无关、可迁移的表示。</li><li>可扩展标注流水线：自动从机器人视频中提取任务相关点与末端执行器路径，支持不同视角与小目标。</li><li>多策略与多具身验证：在2D/3D、不同规模策略与两种真实机械臂系统上统一验证。</li><li>零样本泛化实证：面对视觉干扰与语义新任务大幅提升成功率与鲁棒性。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>目标表示：联合预测两个点集——末端执行器的2D轨迹点 p_t 与任务相关掩蔽点 m_t，两者作为自然语言响应由VLM输出。</li><li>VLM微调：以VILA-1.5-3B为基座，整合机器人与通用点预测/VQA数据；联合优化路径与掩蔽点预测，训练约20小时（A100×8）。</li><li>数据标注流水线：先用点跟踪（CoTracker3）识别场景中显著移动的点集作为“任务相关点”，再以检测器+掩蔽机制构建末端执行器路径；通过“停止点数”K-Means将长轨迹切分为更短的“子轨迹”，以提高表示的最小性与可预测性。</li><li>策略接口：推理时每隔H步查询VLM，将路径与掩蔽点以可视化方式叠加到观测图像上，供任意RGB或RGB-D策略训练与执行。掩蔽通过对预测点为中心的8%边长区域开窗；路径以随时间颜色渐变的线段表示。</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>数据来源：Open X-Embodiment（OXE）20+子数据集、DROID、LIBERO-90、BRIDGE-v2、RoboPoint；总计2M+问答/点预测对、148k轨迹、9种具身。</li><li>训练资源：VLM微调在8×NVIDIA A100上约20小时；推理在RTX 3090上单次查询约4–6秒。</li><li>工具/模型：CoTracker3点跟踪、Detectron2末端执行器检测、FoundationStereo深度估计。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>评估环境：</li></ul>\n<p> - Franka（Sim-to-Real）：仿真采集2.5k条“堆叠彩色方块”轨迹；真实世界Zed 2立体相机+FoundationStereoDepth；评估Basic/Clutter/Semantic三类任务。</p>\n<p> - WidowX（BRIDGE）：单目RGB环境，改换桌面与背景；在Basic/Clutter/Semantic三类任务上评估。</p>\n<ul><li>评估指标：任务完成率与成功率（平均与分项），包含抓取/到达的Partial Credit；零样本跨场景与跨语义泛化能力；消融实验使用路径/掩蔽的组合成功率；VLM质量使用DTW、起点/终点L2、IoU。</li></ul>"
  },
  {
    "date": "2025-09-18",
    "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation",
    "link": "http://arxiv.org/abs/2509.18183",
    "summary_markdown": "### 论文研究单位\nLi Auto Inc.，中国北京\n### 论文概述\nVLA模型在视觉视角变化时（如不同位置、背景或高度）泛化性能下降，影响任务执行。论文提出轻量级视角适应融合框架VLA-LPAF，通过潜在空间融合多视角2D图像，仅使用单视图数据进行微调，弥补视角差异。与RoboFlamingo结合构建RoboFlamingo-LPAF，在CALVIN、LIBERO和自定义CabinEnv数据集上提升成功率8%-30%，并在真实任务中验证视角适应能力。\n### 论文核心贡献点\n- 首次实现基于2D图像的轻量级潜在视角特征融合框架VLA-LPAF，降低视角一致性约束。\n- 实例化VLA-LPAF为RoboFlamingo-LPAF，通过多数据集和真实任务验证有效性。\n- 三阶段训练策略（单视图仅动作、多视图仅融合、多视图联合训练）优化泛化能力。\n### 论文方法描述\n- **问题定义**：VLA模型映射多模态输入（图像、文本）到动作，但视角差异导致特征偏移。\n- **架构**：添加MLP基融合模块，利用单视图参考数据集（D_R）和多视图辅助数据集（D_M)。\n- **对齐融合**：融合模块在ViT编码的潜在空间中，对齐参考视图（R）和辅助视图（M）的特征。\n- **训练策略**：\n 1. 单视图阶段：冻结ViT，微调LLM参数（θ），使用动作损失（公式1）。\n 2. 多视图阶段：仅训练融合模块参数（θ'），使用对齐损失（公式4）。\n 3. 联合阶段：同时微调θ和θ'，结合动作损失（公式3）和对齐损失（公式5）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 模拟：CALVIN、LIBERO、CabinEnv（自定义舱内环境，包含按钮按压和杠杆翻转任务）。\n - 数据集构建：参考视图（0°）+ 辅助视图（±45°范围，v=4个视角）。\n- **训练资源**：8个NVIDIA A800 80GB GPU，图像统一尺寸224×224。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：使用多视角测试数据（如CALVIN中参考视图±90°范围，间隔10°）。\n - 真实环境：Realman RML机器人臂（单臂），Intel RealSense D415（全局参考和腕部摄像头），Azure Kinect（辅助全局摄像头），执行按钮按压和杠杆翻转任务。\n- **评估指标**：任务成功率（success rate），基线比较显示CALVIN平均提升8%，LIBERO提升15%，CabinEnv提升30%，并在真实任务中验证RoboFlamingo-LPAF完成未包含视角（如30°）的任务。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Li Auto Inc.，中国北京</p>\n<h3>论文概述</h3>\n<p>VLA模型在视觉视角变化时（如不同位置、背景或高度）泛化性能下降，影响任务执行。论文提出轻量级视角适应融合框架VLA-LPAF，通过潜在空间融合多视角2D图像，仅使用单视图数据进行微调，弥补视角差异。与RoboFlamingo结合构建RoboFlamingo-LPAF，在CALVIN、LIBERO和自定义CabinEnv数据集上提升成功率8%-30%，并在真实任务中验证视角适应能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首次实现基于2D图像的轻量级潜在视角特征融合框架VLA-LPAF，降低视角一致性约束。</li><li>实例化VLA-LPAF为RoboFlamingo-LPAF，通过多数据集和真实任务验证有效性。</li><li>三阶段训练策略（单视图仅动作、多视图仅融合、多视图联合训练）优化泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>问题定义</strong>：VLA模型映射多模态输入（图像、文本）到动作，但视角差异导致特征偏移。</li><li><strong>架构</strong>：添加MLP基融合模块，利用单视图参考数据集（D_R）和多视图辅助数据集（D_M)。</li><li><strong>对齐融合</strong>：融合模块在ViT编码的潜在空间中，对齐参考视图（R）和辅助视图（M）的特征。</li><li><strong>训练策略</strong>：</li></ul>\n<p> 1. 单视图阶段：冻结ViT，微调LLM参数（θ），使用动作损失（公式1）。</p>\n<p> 2. 多视图阶段：仅训练融合模块参数（θ'），使用对齐损失（公式4）。</p>\n<p> 3. 联合阶段：同时微调θ和θ'，结合动作损失（公式3）和对齐损失（公式5）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟：CALVIN、LIBERO、CabinEnv（自定义舱内环境，包含按钮按压和杠杆翻转任务）。</p>\n<p> - 数据集构建：参考视图（0°）+ 辅助视图（±45°范围，v=4个视角）。</p>\n<ul><li><strong>训练资源</strong>：8个NVIDIA A800 80GB GPU，图像统一尺寸224×224。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：使用多视角测试数据（如CALVIN中参考视图±90°范围，间隔10°）。</p>\n<p> - 真实环境：Realman RML机器人臂（单臂），Intel RealSense D415（全局参考和腕部摄像头），Azure Kinect（辅助全局摄像头），执行按钮按压和杠杆翻转任务。</p>\n<ul><li><strong>评估指标</strong>：任务成功率（success rate），基线比较显示CALVIN平均提升8%，LIBERO提升15%，CabinEnv提升30%，并在真实任务中验证RoboFlamingo-LPAF完成未包含视角（如30°）的任务。</li></ul>"
  },
  {
    "date": "2025-09-22",
    "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States",
    "link": "http://arxiv.org/abs/2509.18043",
    "summary_markdown": "论文研究单位\n弗吉尼亚理工大学，协作机器人实验室，机械工程系。\n\n论文概述\n论文旨在解决模仿学习（IL）策略在处理训练分布之外的初始状态时表现不佳的问题。受人类在执行任务前会先重新整理环境的启发，论文提出了一种名为ReSET的算法。该算法通过学习人类的整理方式，将不熟悉的初始状态自主地改造为更简单、更接近训练数据的“锚状态”，然后再执行预训练的任务策略。论文从理论上证明了这种两阶段方法（先重排环境，再执行策略）能降低泛化误差上限，并在实验中验证了其相较于扩散策略、视觉-语言-动作模型（VLA）等基线方法，能在相同数据量下实现更鲁棒的执行效果和更高的数据效率。\n\n论文核心贡献点\n1. 泛化性与数据效率的理论分析：从理论上证明，在任务策略之上学习一个缩减策略，能获得更低的泛化误差上界，并减少总训练数据的需求。\n2. 基于流的方法：提出了一种结合无关动作的人类视频和无关任务的机器人遥操作数据的方法，用于学习一个缩减策略。该方法包含三个部分：决定何时修改场景、预测人类会如何简化场景（物体点流）、以及将这些预测映射到机器人的动作原语。\n3. 实验验证：在多个少样本任务设置中，通过实验表明ReSET在处理分布外状态时，性能超越了多种最先进的基线方法。\n\n论文方法描述\nReSET是一个两阶段框架，包含一个缩减策略和一个基础任务策略。核心是学习一个与任务无关的缩减策略，它由三个网络组件构成：\n1. 评分网络：输入当前场景观察，输出一个场景分数，用于判断是否应该继续调整环境或直接执行基础任务策略。该网络使用带有时间先验标签的人类视频进行训练，认为视频中越靠后的帧，环境越利于任务执行。\n2. 流生成网络：输入当前场景观察，预测物体的点流，即人类会如何移动物体以简化场景。该网络使用CoTracker3等工具从人类视频中提取物体点流作为训练目标，并采用时空Transformer架构。\n3. 缩减策略：一个任务无关的策略，将流生成网络预测的点流和机器人当前观察作为输入，输出机器人的具体动作原语（如抓取-放置、推拉、旋转）及其参数。该策略使用任务无关的机器人遥操作（玩耍）数据进行训练。\n在部署时，ReSET首先使用评分网络评估当前状态，若分数高于阈值，则依次调用流生成网络和缩减策略来改造环境，循环此过程直到状态分数低于阈值，最后执行原始任务策略。\n\n论文使用数据集和训练资源\n使用的数据包括：\n1. 无关动作的人类视频：用于训练评分网络和流生成网络，视频中展示了人类如何整理环境。\n2. 专家机器人演示数据：用于训练基础任务策略，例如每个任务20条轨迹。\n3. 任务无关的机器人遥操作数据：用于训练缩减策略，例如20分钟的机器人玩耍数据。\n训练和实验在实体机器人上完成，使用的是Franka Emika机械臂，配备GELLO遥操作控制器以及两个摄像头（一个侧视固定摄像头，一个夹爪上摄像头）。\n\n论文使用的评估环境和评估指标\n评估环境为实体桌面操作场景，机器人平台为Franka Emika机械臂。\n评估任务包括四个真实世界任务：抓取-放置、揭示-抓取、旋转-放置和多任务。\n评估指标为任务成功率。每个任务在15个测试场景中进行评估，其中约80%的场景包含训练分布外的状态，以测试方法的泛化能力。对比的基线方法包括ReSET Naive、Diffusion Policy、Dynamics-DP和Pi_0。",
    "summary_html": "<p>论文研究单位</p>\n<p>弗吉尼亚理工大学，协作机器人实验室，机械工程系。</p>\n\n<p>论文概述</p>\n<p>论文旨在解决模仿学习（IL）策略在处理训练分布之外的初始状态时表现不佳的问题。受人类在执行任务前会先重新整理环境的启发，论文提出了一种名为ReSET的算法。该算法通过学习人类的整理方式，将不熟悉的初始状态自主地改造为更简单、更接近训练数据的“锚状态”，然后再执行预训练的任务策略。论文从理论上证明了这种两阶段方法（先重排环境，再执行策略）能降低泛化误差上限，并在实验中验证了其相较于扩散策略、视觉-语言-动作模型（VLA）等基线方法，能在相同数据量下实现更鲁棒的执行效果和更高的数据效率。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>泛化性与数据效率的理论分析：从理论上证明，在任务策略之上学习一个缩减策略，能获得更低的泛化误差上界，并减少总训练数据的需求。</li><li>基于流的方法：提出了一种结合无关动作的人类视频和无关任务的机器人遥操作数据的方法，用于学习一个缩减策略。该方法包含三个部分：决定何时修改场景、预测人类会如何简化场景（物体点流）、以及将这些预测映射到机器人的动作原语。</li><li>实验验证：在多个少样本任务设置中，通过实验表明ReSET在处理分布外状态时，性能超越了多种最先进的基线方法。</li></ol>\n\n<p>论文方法描述</p>\n<p>ReSET是一个两阶段框架，包含一个缩减策略和一个基础任务策略。核心是学习一个与任务无关的缩减策略，它由三个网络组件构成：</p>\n<ol><li>评分网络：输入当前场景观察，输出一个场景分数，用于判断是否应该继续调整环境或直接执行基础任务策略。该网络使用带有时间先验标签的人类视频进行训练，认为视频中越靠后的帧，环境越利于任务执行。</li><li>流生成网络：输入当前场景观察，预测物体的点流，即人类会如何移动物体以简化场景。该网络使用CoTracker3等工具从人类视频中提取物体点流作为训练目标，并采用时空Transformer架构。</li><li>缩减策略：一个任务无关的策略，将流生成网络预测的点流和机器人当前观察作为输入，输出机器人的具体动作原语（如抓取-放置、推拉、旋转）及其参数。该策略使用任务无关的机器人遥操作（玩耍）数据进行训练。</li></ol>\n<p>在部署时，ReSET首先使用评分网络评估当前状态，若分数高于阈值，则依次调用流生成网络和缩减策略来改造环境，循环此过程直到状态分数低于阈值，最后执行原始任务策略。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用的数据包括：</p>\n<ol><li>无关动作的人类视频：用于训练评分网络和流生成网络，视频中展示了人类如何整理环境。</li><li>专家机器人演示数据：用于训练基础任务策略，例如每个任务20条轨迹。</li><li>任务无关的机器人遥操作数据：用于训练缩减策略，例如20分钟的机器人玩耍数据。</li></ol>\n<p>训练和实验在实体机器人上完成，使用的是Franka Emika机械臂，配备GELLO遥操作控制器以及两个摄像头（一个侧视固定摄像头，一个夹爪上摄像头）。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境为实体桌面操作场景，机器人平台为Franka Emika机械臂。</p>\n<p>评估任务包括四个真实世界任务：抓取-放置、揭示-抓取、旋转-放置和多任务。</p>\n<p>评估指标为任务成功率。每个任务在15个测试场景中进行评估，其中约80%的场景包含训练分布外的状态，以测试方法的泛化能力。对比的基线方法包括ReSET Naive、Diffusion Policy、Dynamics-DP和Pi_0。</p>"
  },
  {
    "date": "2025-09-20",
    "title": "ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering",
    "link": "http://arxiv.org/abs/2509.16680",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n- Dartmouth College（达特茅斯学院）\n- Shandong University（山东大学）\n- Harvard University（哈佛大学）\n## 论文概述\n论文提出了ProtoVQA，这是一个用于可解释细粒度视觉问答的统一原型框架。该框架通过学习问题感知的原型作为推理锚点，将答案与判别性图像区域连接起来，并应用空间约束匹配来确保选择的证据具有连贯性和语义相关性。通过共享原型主干网络，该框架支持视觉问答和定位任务，并提出了视觉-语言对齐评分（VLAS）来评估解释质量。\n## 论文核心贡献点\n1. 引入适应性原型框架，能够无缝处理不同的视觉-语言下游任务，包括视觉问答和定位\n2. 采用空间约束的贪婪匹配策略建模动态视觉问题关系和几何变化\n3. 通过明确的视觉证据和系统化的视觉-语言对齐验证实现全面的可解释性\n## 论文方法描述\n**框架组成：**\n- **特征提取模块**：使用DeiT作为视觉特征提取器，DeBERTa作为文本编码器，将图像和文本特征投影到共享的视觉-语言空间\n- **可解释原型部分选择模块**：引入子补丁原型（m×k结构）和贪婪匹配算法，通过空间约束选择与原型最匹配的图像区域\n- **答案处理**：支持两种类型——Type 1（视觉定位）和Type 2（描述性问答），分别处理坐标输入和文本答案\n- **视觉-语言对齐评估**：提出VLAS指标，测量模型关注区域与真实证据的对齐程度\n\n**核心技术：**\n- 子补丁原型：每个原型由k个子补丁组成，形成语义锚点\n- 空间约束贪婪匹配：通过迭代选择相似度最高的补丁-子补丁对，并使用邻接掩码确保空间连续性\n- 权重共享机制：在问题编码和答案处理之间共享特征投影器参数\n## 论文使用数据集和训练资源\n**数据集：**\n- Visual7W：包含327,939个问题-答案对，覆盖47,300张COCO图像，每个问题配有4个人工选择选项和561,459个对象级定位标注\n\n**训练资源：**\n- 硬件：NVIDIA A800 GPU（80GB）\n- 训练配置：200个epoch，Adam优化器，学习率1×10^-4，批大小64\n- 图像处理：224×224像素，16×16补丁\n- 原型参数：m=10个原型，k=3个子补丁，空间约束半径r=3\n## 论文使用的评估环境和评估指标\n**评估环境：**\n- 主要在Visual7W测试集上进行评估\n- 对比基线包括SUPER、QOI_Attention、SDF of VLT、STL、CFR、BriVL、CTI、Bi-CMA等代表性VQA模型\n\n**评估指标：**\n- **准确性指标**：分类准确率（Accuracy）\n- **解释质量指标**：视觉-语言对齐评分（VLAS），通过IoU阈值（θ=0.5）计算模型关注区域与真实标注的对齐程度\n- **可视化分析**：定性展示模型选择的图像区域与真实标注的对应关系\n\n**实验结果：**\n- ProtoVQA在Visual7W上达到70.23%准确率，与强基线模型相当\n- 在VLAS指标上显著优于基线方法，VLAS@1达到0.4103（比Bi-CMA提升66.4%），VLAS@3达到0.2466（比Bi-CMA提升119.6%）\n- 定性分析显示模型能够准确关注与问题相关的语义区域",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<ul><li>Dartmouth College（达特茅斯学院）</li><li>Shandong University（山东大学）</li><li>Harvard University（哈佛大学）</li></ul>\n<h2>论文概述</h2>\n<p>论文提出了ProtoVQA，这是一个用于可解释细粒度视觉问答的统一原型框架。该框架通过学习问题感知的原型作为推理锚点，将答案与判别性图像区域连接起来，并应用空间约束匹配来确保选择的证据具有连贯性和语义相关性。通过共享原型主干网络，该框架支持视觉问答和定位任务，并提出了视觉-语言对齐评分（VLAS）来评估解释质量。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>引入适应性原型框架，能够无缝处理不同的视觉-语言下游任务，包括视觉问答和定位</li><li>采用空间约束的贪婪匹配策略建模动态视觉问题关系和几何变化</li><li>通过明确的视觉证据和系统化的视觉-语言对齐验证实现全面的可解释性</li></ol>\n<h2>论文方法描述</h2>\n<p><strong>框架组成：</strong></p>\n<ul><li><strong>特征提取模块</strong>：使用DeiT作为视觉特征提取器，DeBERTa作为文本编码器，将图像和文本特征投影到共享的视觉-语言空间</li><li><strong>可解释原型部分选择模块</strong>：引入子补丁原型（m×k结构）和贪婪匹配算法，通过空间约束选择与原型最匹配的图像区域</li><li><strong>答案处理</strong>：支持两种类型——Type 1（视觉定位）和Type 2（描述性问答），分别处理坐标输入和文本答案</li><li><strong>视觉-语言对齐评估</strong>：提出VLAS指标，测量模型关注区域与真实证据的对齐程度</li></ul>\n\n<p><strong>核心技术：</strong></p>\n<ul><li>子补丁原型：每个原型由k个子补丁组成，形成语义锚点</li><li>空间约束贪婪匹配：通过迭代选择相似度最高的补丁-子补丁对，并使用邻接掩码确保空间连续性</li><li>权重共享机制：在问题编码和答案处理之间共享特征投影器参数</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<p><strong>数据集：</strong></p>\n<ul><li>Visual7W：包含327,939个问题-答案对，覆盖47,300张COCO图像，每个问题配有4个人工选择选项和561,459个对象级定位标注</li></ul>\n\n<p><strong>训练资源：</strong></p>\n<ul><li>硬件：NVIDIA A800 GPU（80GB）</li><li>训练配置：200个epoch，Adam优化器，学习率1×10^-4，批大小64</li><li>图像处理：224×224像素，16×16补丁</li><li>原型参数：m=10个原型，k=3个子补丁，空间约束半径r=3</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<p><strong>评估环境：</strong></p>\n<ul><li>主要在Visual7W测试集上进行评估</li><li>对比基线包括SUPER、QOI_Attention、SDF of VLT、STL、CFR、BriVL、CTI、Bi-CMA等代表性VQA模型</li></ul>\n\n<p><strong>评估指标：</strong></p>\n<ul><li><strong>准确性指标</strong>：分类准确率（Accuracy）</li><li><strong>解释质量指标</strong>：视觉-语言对齐评分（VLAS），通过IoU阈值（θ=0.5）计算模型关注区域与真实标注的对齐程度</li><li><strong>可视化分析</strong>：定性展示模型选择的图像区域与真实标注的对应关系</li></ul>\n\n<p><strong>实验结果：</strong></p>\n<ul><li>ProtoVQA在Visual7W上达到70.23%准确率，与强基线模型相当</li><li>在VLAS指标上显著优于基线方法，VLAS@1达到0.4103（比Bi-CMA提升66.4%），VLAS@3达到0.2466（比Bi-CMA提升119.6%）</li><li>定性分析显示模型能够准确关注与问题相关的语义区域</li></ul>"
  },
  {
    "date": "2025-09-19",
    "title": "Randomized Smoothing Meets Vision-Language Models",
    "link": "http://arxiv.org/abs/2509.16088",
    "summary_markdown": "## 论文研究单位\n- National Technical University of Athens, Athens, Greece\n- Université Grenoble Alpes, Grenoble, France\n- CSX‑AI, Grenoble, France\n- Carl von Ossietzky University of Oldenburg, Oldenburg, Germany\n- Chalmers University of Technology, Gothenburg, Sweden\n## 论文概述\n随机化平滑（Randomized Smoothing, RS）是目前在大规模分类模型上唯一可行的鲁棒性认证技术，但其原本只能处理离散标签，难以直接用于生成式的视觉‑语言模型（Vision‑Language Models, VLMs）。本文提出一种将生成式模型的输出映射为分类问题的通用方法，从而把 RS 迁移到 VLMs 场景。核心思路是引入一个“oracle”分类层，对模型生成的文本进行二分类（如有害/无害）或离散动作映射，或将语义等价的答案归为同一等价类。通过对图像添加高斯噪声、保持提示文本不变、进行多次采样并使用 oracle 进行投票，得到多数类的置信下界，进而推导出可验证的鲁棒半径。论文还从理论上分析了样本数对半径和认证精度的影响，给出改进的样本复杂度标度律，使得在实际应用中只需要 10²‑10³ 级别的样本即可得到与经典方法（10⁴‑10⁵ 样本）相匹配的证书。实验在最新的 VLM 上对最近的越狱式攻击进行了验证，展示了该方法的可扩展性和实用性。\n## 论文核心贡献点\n- 将随机化平滑从分类扩展到生成式 VLM，提出基于 oracle 的抽象层，使得生成结果可以被视作离散类。\n- 给出在 oracle 错误率 ε<0.5 下的概率下界修正公式\n \\[\n \\bar p_y = \\frac{\\bar q_y - \\epsilon}{1-2\\epsilon}\n \\]\n 并证明在 \\(\\bar q_y>0.5\\) 时该下界仍然是有效的。\n- 推导出样本数 \\(n\\) 与可验证半径的关系（利用中心极限定理和 Shore 对 \\(\\Phi^{-1}\\) 的近似），得到标度律\n \\[\n r_\\sigma(\\alpha,n) \\approx 1 - 1.64\\frac{z_\\alpha}{\\sqrt n}\n \\]\n 表明样本量只需降低 2‑3 个数量级即可保持接近最优的半径。\n- 在理论上放宽了前期工作对均匀分布的要求，仅需多数类概率分布主要集中在 \\([\\beta,1)\\) 且 \\(\\beta\\ge 0.7\\)，从而提升了方法的适用性。\n- 给出适用于 VLM 的完整认证算法（Algorithm 2），包括语义等价聚类的投票机制。\n- 通过在最新 VLMs 上进行实验，验证了对 jailbreak‑style 攻击的防御能力，并报告了认证半径、认证精度以及样本效率的定量结果。\n## 论文方法描述\n1. **模型与噪声注入**\n - 输入为图像 \\(\\mathbf x\\) 与文本提示 \\(\\mathbf t\\)。\n - 对图像加高斯噪声 \\(\\mathbf z\\sim\\mathcal N(\\mathbf 0,\\sigma^2\\mathbf I)\\) 生成 \\(\\mathbf x'\\)。\n - 文本保持不变，调用 VLM \\(f_\\theta(\\mathbf x',\\mathbf t)\\) 获得生成的回答 \\(\\mathbf y\\)。\n\n2. **Oracle 分类层**\n - **内容安全分类**：oracle 将 \\(\\mathbf y\\) 判为 “有害” 或 “无害”。\n - **离散动作映射**：若 VLM 充当 VLA，oracle 将答案映射为有限的动作集合（如 `base‑forward`、`gripper‑open`）。\n - **语义等价聚类**：oracle 判断新回答是否与已出现的回答语义相同，若相同则累计计数，否则创建新类。\n\n3. **投票与计数**（Algorithm 2）\n - 用字典 `ans` 存储每个等价类（或每个离散动作）的出现次数。\n - 对每个噪声样本执行 oracle 判断并更新相应计数。\n - 最终返回计数最高的回答 \\(y\\) 与其计数 \\(c\\)。\n\n4. **概率下界与证书半径**\n - 将计数 \\(c\\) 与样本数 \\(n\\) 带入 Clopper‑Pearson 方法得到 \\(\\bar q_y\\)（在置信度 \\(1-\\alpha\\) 下的下界）。\n - 考虑 oracle 误差 \\(\\epsilon\\)（假设 \\(\\epsilon<0.5\\)），修正得到真实概率下界\n \\[\n \\bar p_y = \\frac{\\bar q_y - \\epsilon}{1-2\\epsilon}\n \\]\n - 若 \\(\\bar p_y>0.5\\)，则可验证半径为\n \\[\n R = \\sigma \\Phi^{-1}(\\bar p_y)\n \\]\n - 对于二分类情形，若对 \\(\\epsilon\\) 没有任何已知信息，只要 \\(\\bar q_y>0.5\\) 仍可直接使用 \\(R = \\sigma\\Phi^{-1}(\\bar q_y)\\) 作为下界（Theorem 4.2）。\n\n5. **样本效率分析**\n - 利用 CLT 对 \\(\\bar p_y\\) 进行近似，得到期望半径\n \\[\n R_\\sigma^{\\alpha,n}(p_A) \\approx \\sigma \\Phi^{-1}\\!\\bigl(p_A - t_{\\alpha,n}\\bigr),\\quad\n t_{\\alpha,n}=z_\\alpha\\sqrt{p_A(1-p_A)/n}\n \\]\n - 通过 Shore 对 \\(\\Phi^{-1}\\) 的幂级数近似，进一步得到平均半径的下降比例\n \\[\n r_\\sigma(\\alpha,n) = \\frac{\\bar R_\\sigma(\\alpha,n)}{\\bar R_\\sigma(0,\\infty)} \\approx 1 - 1.64\\frac{z_\\alpha}{\\sqrt n}\n \\]\n - 该公式说明把样本数从 \\(10^5\\) 降到 \\(10^3\\) 仍能保持约 90% 以上的理想半径。\n\n6. **实验实现**\n - 采样若干噪声图像（如 \\(n=500\\)‑\\(2000\\)），对每个图像调用 VLM 并由强 LLM（如 GPT‑4、Llama‑70B）充当 oracle。\n - 计算多数类的计数、Clopper‑Pearson 下界与对应半径。\n - 与原始分类 RS 基线以及仅使用 Clopper‑Pearson 不进行 oracle 修正的方案进行对比。\n## 论文使用数据集和训练资源\n- **模型**：直接使用公开的预训练 VLM（如 LLaVA、InstructBLIP、GPT‑4V 等），不进行额外微调。\n- **Oracle**：使用更强的语言模型（例如 GPT‑4 或 Llama‑70B）作为分类器或语义等价判断器。\n- **实验数据**：在论文正文中未提供详细数据集描述，附录 B 中列有用于内容安全、VLA 动作与语义聚类的图像‑提示对（来源于公开的 VLM 评测集），并对每个样本进行多次噪声采样。\n- **计算资源**：实验基于多卡 GPU 集群执行 VLM 推理，oracle 的调用使用相同的 GPU 或 CPU 后端。由于模型已预训练，计算开销主要是多次前向推理（每张图像 \\(n\\) 次），具体硬件规格未在正文中给出。\n## 论文使用的评估环境和评估指标\n- **评估环境**：在标准机器学习服务器上运行（Python + PyTorch），所有 VLM 与 oracle 均通过相同的推理框架进行调用，实验代码已开源。\n- **鲁棒性指标**\n - **可验证半径 \\(R\\)**：在给定置信度 \\(\\alpha\\) 下，对每个输入返回的最大半径。\n - **认证精度**：在特定半径阈值（如 \\(R=0.5\\)）下，能够给出非“ABSTAIN”结果的样本比例。\n - **半径下降比例**：通过标度律或实际测量比较不同样本数 \\(n\\) 下的平均半径 \\(\\bar R_\\sigma(\\alpha,n)\\)。\n - **Oracle 误差敏感性**：评估在不同假设的 \\(\\epsilon\\)（oracle 错误率）下半径下界的变化。\n- **对抗评估**\n - 对最新的 jailbreak‑style 攻击（Qi et al., 2024）进行防御测试，统计攻击成功率的降低程度。\n - 与未加噪声的原始 VLM 对比，展示 RS 增强后的安全性提升。\n- **样本效率**\n - 记录在不同样本规模（如 \\(n=100, 500, 1000, 5000\\)）下的认证半径、认证精度以及所需的计算时间。\n - 通过经验曲线验证理论标度律 \\(1-1.64z_\\alpha/\\sqrt n\\) 的准确性。\n\n以上内容概括了论文的研究单位、整体概述、主要贡献、方法细节、实验使用的数据与资源以及评估环境和指标。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>National Technical University of Athens, Athens, Greece</li><li>Université Grenoble Alpes, Grenoble, France</li><li>CSX‑AI, Grenoble, France</li><li>Carl von Ossietzky University of Oldenburg, Oldenburg, Germany</li><li>Chalmers University of Technology, Gothenburg, Sweden</li></ul>\n<h2>论文概述</h2>\n<p>随机化平滑（Randomized Smoothing, RS）是目前在大规模分类模型上唯一可行的鲁棒性认证技术，但其原本只能处理离散标签，难以直接用于生成式的视觉‑语言模型（Vision‑Language Models, VLMs）。本文提出一种将生成式模型的输出映射为分类问题的通用方法，从而把 RS 迁移到 VLMs 场景。核心思路是引入一个“oracle”分类层，对模型生成的文本进行二分类（如有害/无害）或离散动作映射，或将语义等价的答案归为同一等价类。通过对图像添加高斯噪声、保持提示文本不变、进行多次采样并使用 oracle 进行投票，得到多数类的置信下界，进而推导出可验证的鲁棒半径。论文还从理论上分析了样本数对半径和认证精度的影响，给出改进的样本复杂度标度律，使得在实际应用中只需要 10²‑10³ 级别的样本即可得到与经典方法（10⁴‑10⁵ 样本）相匹配的证书。实验在最新的 VLM 上对最近的越狱式攻击进行了验证，展示了该方法的可扩展性和实用性。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>将随机化平滑从分类扩展到生成式 VLM，提出基于 oracle 的抽象层，使得生成结果可以被视作离散类。</li><li>给出在 oracle 错误率 ε<0.5 下的概率下界修正公式</li></ul>\n<p> \\[</p>\n<p> \\bar p_y = \\frac{\\bar q_y - \\epsilon}{1-2\\epsilon}</p>\n<p> \\]</p>\n<p> 并证明在 \\(\\bar q_y>0.5\\) 时该下界仍然是有效的。</p>\n<ul><li>推导出样本数 \\(n\\) 与可验证半径的关系（利用中心极限定理和 Shore 对 \\(\\Phi^{-1}\\) 的近似），得到标度律</li></ul>\n<p> \\[</p>\n<p> r_\\sigma(\\alpha,n) \\approx 1 - 1.64\\frac{z_\\alpha}{\\sqrt n}</p>\n<p> \\]</p>\n<p> 表明样本量只需降低 2‑3 个数量级即可保持接近最优的半径。</p>\n<ul><li>在理论上放宽了前期工作对均匀分布的要求，仅需多数类概率分布主要集中在 \\([\\beta,1)\\) 且 \\(\\beta\\ge 0.7\\)，从而提升了方法的适用性。</li><li>给出适用于 VLM 的完整认证算法（Algorithm 2），包括语义等价聚类的投票机制。</li><li>通过在最新 VLMs 上进行实验，验证了对 jailbreak‑style 攻击的防御能力，并报告了认证半径、认证精度以及样本效率的定量结果。</li></ul>\n<h2>论文方法描述</h2>\n<p>1. <strong>模型与噪声注入</strong></p>\n<p> - 输入为图像 \\(\\mathbf x\\) 与文本提示 \\(\\mathbf t\\)。</p>\n<p> - 对图像加高斯噪声 \\(\\mathbf z\\sim\\mathcal N(\\mathbf 0,\\sigma^2\\mathbf I)\\) 生成 \\(\\mathbf x'\\)。</p>\n<p> - 文本保持不变，调用 VLM \\(f_\\theta(\\mathbf x',\\mathbf t)\\) 获得生成的回答 \\(\\mathbf y\\)。</p>\n\n<p>2. <strong>Oracle 分类层</strong></p>\n<p> - <strong>内容安全分类</strong>：oracle 将 \\(\\mathbf y\\) 判为 “有害” 或 “无害”。</p>\n<p> - <strong>离散动作映射</strong>：若 VLM 充当 VLA，oracle 将答案映射为有限的动作集合（如 <code>base‑forward</code>、<code>gripper‑open</code>）。</p>\n<p> - <strong>语义等价聚类</strong>：oracle 判断新回答是否与已出现的回答语义相同，若相同则累计计数，否则创建新类。</p>\n\n<p>3. <strong>投票与计数</strong>（Algorithm 2）</p>\n<p> - 用字典 <code>ans</code> 存储每个等价类（或每个离散动作）的出现次数。</p>\n<p> - 对每个噪声样本执行 oracle 判断并更新相应计数。</p>\n<p> - 最终返回计数最高的回答 \\(y\\) 与其计数 \\(c\\)。</p>\n\n<p>4. <strong>概率下界与证书半径</strong></p>\n<p> - 将计数 \\(c\\) 与样本数 \\(n\\) 带入 Clopper‑Pearson 方法得到 \\(\\bar q_y\\)（在置信度 \\(1-\\alpha\\) 下的下界）。</p>\n<p> - 考虑 oracle 误差 \\(\\epsilon\\)（假设 \\(\\epsilon<0.5\\)），修正得到真实概率下界</p>\n<p> \\[</p>\n<p> \\bar p_y = \\frac{\\bar q_y - \\epsilon}{1-2\\epsilon}</p>\n<p> \\]</p>\n<p> - 若 \\(\\bar p_y>0.5\\)，则可验证半径为</p>\n<p> \\[</p>\n<p> R = \\sigma \\Phi^{-1}(\\bar p_y)</p>\n<p> \\]</p>\n<p> - 对于二分类情形，若对 \\(\\epsilon\\) 没有任何已知信息，只要 \\(\\bar q_y>0.5\\) 仍可直接使用 \\(R = \\sigma\\Phi^{-1}(\\bar q_y)\\) 作为下界（Theorem 4.2）。</p>\n\n<p>5. <strong>样本效率分析</strong></p>\n<p> - 利用 CLT 对 \\(\\bar p_y\\) 进行近似，得到期望半径</p>\n<p> \\[</p>\n<p> R_\\sigma^{\\alpha,n}(p_A) \\approx \\sigma \\Phi^{-1}\\!\\bigl(p_A - t_{\\alpha,n}\\bigr),\\quad</p>\n<p> t_{\\alpha,n}=z_\\alpha\\sqrt{p_A(1-p_A)/n}</p>\n<p> \\]</p>\n<p> - 通过 Shore 对 \\(\\Phi^{-1}\\) 的幂级数近似，进一步得到平均半径的下降比例</p>\n<p> \\[</p>\n<p> r_\\sigma(\\alpha,n) = \\frac{\\bar R_\\sigma(\\alpha,n)}{\\bar R_\\sigma(0,\\infty)} \\approx 1 - 1.64\\frac{z_\\alpha}{\\sqrt n}</p>\n<p> \\]</p>\n<p> - 该公式说明把样本数从 \\(10^5\\) 降到 \\(10^3\\) 仍能保持约 90% 以上的理想半径。</p>\n\n<p>6. <strong>实验实现</strong></p>\n<p> - 采样若干噪声图像（如 \\(n=500\\)‑\\(2000\\)），对每个图像调用 VLM 并由强 LLM（如 GPT‑4、Llama‑70B）充当 oracle。</p>\n<p> - 计算多数类的计数、Clopper‑Pearson 下界与对应半径。</p>\n<p> - 与原始分类 RS 基线以及仅使用 Clopper‑Pearson 不进行 oracle 修正的方案进行对比。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>模型</strong>：直接使用公开的预训练 VLM（如 LLaVA、InstructBLIP、GPT‑4V 等），不进行额外微调。</li><li><strong>Oracle</strong>：使用更强的语言模型（例如 GPT‑4 或 Llama‑70B）作为分类器或语义等价判断器。</li><li><strong>实验数据</strong>：在论文正文中未提供详细数据集描述，附录 B 中列有用于内容安全、VLA 动作与语义聚类的图像‑提示对（来源于公开的 VLM 评测集），并对每个样本进行多次噪声采样。</li><li><strong>计算资源</strong>：实验基于多卡 GPU 集群执行 VLM 推理，oracle 的调用使用相同的 GPU 或 CPU 后端。由于模型已预训练，计算开销主要是多次前向推理（每张图像 \\(n\\) 次），具体硬件规格未在正文中给出。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：在标准机器学习服务器上运行（Python + PyTorch），所有 VLM 与 oracle 均通过相同的推理框架进行调用，实验代码已开源。</li><li><strong>鲁棒性指标</strong></li></ul>\n<p> - <strong>可验证半径 \\(R\\)</strong>：在给定置信度 \\(\\alpha\\) 下，对每个输入返回的最大半径。</p>\n<p> - <strong>认证精度</strong>：在特定半径阈值（如 \\(R=0.5\\)）下，能够给出非“ABSTAIN”结果的样本比例。</p>\n<p> - <strong>半径下降比例</strong>：通过标度律或实际测量比较不同样本数 \\(n\\) 下的平均半径 \\(\\bar R_\\sigma(\\alpha,n)\\)。</p>\n<p> - <strong>Oracle 误差敏感性</strong>：评估在不同假设的 \\(\\epsilon\\)（oracle 错误率）下半径下界的变化。</p>\n<ul><li><strong>对抗评估</strong></li></ul>\n<p> - 对最新的 jailbreak‑style 攻击（Qi et al., 2024）进行防御测试，统计攻击成功率的降低程度。</p>\n<p> - 与未加噪声的原始 VLM 对比，展示 RS 增强后的安全性提升。</p>\n<ul><li><strong>样本效率</strong></li></ul>\n<p> - 记录在不同样本规模（如 \\(n=100, 500, 1000, 5000\\)）下的认证半径、认证精度以及所需的计算时间。</p>\n<p> - 通过经验曲线验证理论标度律 \\(1-1.64z_\\alpha/\\sqrt n\\) 的准确性。</p>\n\n<p>以上内容概括了论文的研究单位、整体概述、主要贡献、方法细节、实验使用的数据与资源以及评估环境和指标。</p>"
  },
  {
    "date": "2025-09-19",
    "title": "CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine",
    "link": "http://arxiv.org/abs/2509.15968",
    "summary_markdown": "# 论文研究单位\nCollege of Transportation, Tongji University\n# 论文概述\n提出 CoReVLA，一个面向长尾与安全关键场景的持续学习端到端自动驾驶框架，采用“收集-精炼(Collect-and-Refine)”双阶段流程：在基础阶段利用开源驾驶 QA 数据对 Qwen2.5-VL-7B 进行有监督微调(SFT)，获得基本的场景理解与决策能力；随后在 CAVE 沉浸式仿真平台中进行人机在环(HITL)测试，采集接管数据(包含历史图像、人类注意力、人类接管动作与模型错误行为)，并将其转化为偏好对；最后用直接偏好优化(DPO)进行行为精炼，使模型直接对齐人类偏好，避免手工奖励设计与奖励 hacking。\n# 论文核心贡献点\n- 构建基于 CAVE 平台的人机在环接管数据采集流程，系统化获取长尾失败样本、人类注意力与接管行为，转化为高质量训练数据。\n- 首次将 DPO 应用于自动驾驶长尾场景的行为精炼，利用稀疏接管数据进行偏好对齐，提升模型在高风险场景中的安全性与鲁棒性，避免奖励工程。\n- 在开放问答与闭环驾驶双重任务上验证方法有效性；在 Bench2Drive 基准上实现 DS 72.18、SR 50%，较最佳基线提升 7.96 DS 与 15% SR，并展示跨平台泛化与持续学习能力。\n# 论文方法描述\n- 预阶段 SFT：整合 LingoQA、BDD、HAD 为 70GB 驾驶 QA 数据；以 Qwen2.5-VL-7B 为基座，在视觉投影器与 LLM 主干上引入 LoRA 微调，训练目标为自回归交叉熵(公式 1)。\n- Stage 1 接管数据采集：将模型部署于 CAVE 闭环仿真平台，实时与背景交通交互；当模型出现死锁或碰撞风险时切换回放模式，由安全驾驶员佩戴 VR 进行人工接管。记录历史图像序列、人类视觉注意、接管动作与模型错误行为，自动转化为 DPO 格式的三元组(x, y+, y-)。\n- Stage 2 DPO 行为精炼：以人类偏好对训练，将策略分布建模为动作上的 softmax(公式 2)，最大化人类偏好概率(公式 3)，优化负对数似然损失(公式 4)；可选 KL 正则以约束策略漂移(公式 5)。相较 PPO 等 RLHF，DPO 无需显式奖励、可直接用离线人类接管数据，数据效率高，适合稀疏长尾事件。\n# 论文使用数据集和训练资源\n- 数据集：70GB 融合数据集(LingoQA、BDD、HAD)，涵盖场景认知与安全驾驶策略；每条样本为五帧连续图像与链式思维(CoT)QA 对。\n- 接管数据：CAVE 平台采集的人类接管三元组，用于 DPO 精炼。\n- 训练资源与实现：基于 Qwen2.5-VL-7B，在视觉投影器与 LLM 主干上使用 LoRA 微调；所有代码、预处理数据与场景配置开源于 GitHub。\n# 论文使用的评估环境和评估指标\n- 开放问答(open-loop)评估：在 LingoQA、BDD、HAD 上报告 BLEU 与 ROUGE(1/L)，对比 Qwen2.5-VL-7B、Llava-7B、LlavaNext-7B、Impromptu 等基线。\n- 闭环驾驶(closed-loop)评估：在 CAVE 平台进行人机在环测试与接管采集；将精炼模型在 Bench2Drive 基准上与多种小规模任务特定模型与大规模预训练模型对比，指标包括：\n - Driving Score(DS)\n - Success Rate(SR)\n - Efficiency\n - Comfortness\n- 性能结果：CoReVLA 在 Bench2Drive 取得 DS 72.18、SR 50%，较次优方法提升 7.96 DS 与 14.99% SR；案例研究显示 DPO 前后行为改进与跨平台(CAVE↔Bench2Drive)泛化。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>College of Transportation, Tongji University</p>\n<h1>论文概述</h1>\n<p>提出 CoReVLA，一个面向长尾与安全关键场景的持续学习端到端自动驾驶框架，采用“收集-精炼(Collect-and-Refine)”双阶段流程：在基础阶段利用开源驾驶 QA 数据对 Qwen2.5-VL-7B 进行有监督微调(SFT)，获得基本的场景理解与决策能力；随后在 CAVE 沉浸式仿真平台中进行人机在环(HITL)测试，采集接管数据(包含历史图像、人类注意力、人类接管动作与模型错误行为)，并将其转化为偏好对；最后用直接偏好优化(DPO)进行行为精炼，使模型直接对齐人类偏好，避免手工奖励设计与奖励 hacking。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>构建基于 CAVE 平台的人机在环接管数据采集流程，系统化获取长尾失败样本、人类注意力与接管行为，转化为高质量训练数据。</li><li>首次将 DPO 应用于自动驾驶长尾场景的行为精炼，利用稀疏接管数据进行偏好对齐，提升模型在高风险场景中的安全性与鲁棒性，避免奖励工程。</li><li>在开放问答与闭环驾驶双重任务上验证方法有效性；在 Bench2Drive 基准上实现 DS 72.18、SR 50%，较最佳基线提升 7.96 DS 与 15% SR，并展示跨平台泛化与持续学习能力。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>预阶段 SFT：整合 LingoQA、BDD、HAD 为 70GB 驾驶 QA 数据；以 Qwen2.5-VL-7B 为基座，在视觉投影器与 LLM 主干上引入 LoRA 微调，训练目标为自回归交叉熵(公式 1)。</li><li>Stage 1 接管数据采集：将模型部署于 CAVE 闭环仿真平台，实时与背景交通交互；当模型出现死锁或碰撞风险时切换回放模式，由安全驾驶员佩戴 VR 进行人工接管。记录历史图像序列、人类视觉注意、接管动作与模型错误行为，自动转化为 DPO 格式的三元组(x, y+, y-)。</li><li>Stage 2 DPO 行为精炼：以人类偏好对训练，将策略分布建模为动作上的 softmax(公式 2)，最大化人类偏好概率(公式 3)，优化负对数似然损失(公式 4)；可选 KL 正则以约束策略漂移(公式 5)。相较 PPO 等 RLHF，DPO 无需显式奖励、可直接用离线人类接管数据，数据效率高，适合稀疏长尾事件。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：70GB 融合数据集(LingoQA、BDD、HAD)，涵盖场景认知与安全驾驶策略；每条样本为五帧连续图像与链式思维(CoT)QA 对。</li><li>接管数据：CAVE 平台采集的人类接管三元组，用于 DPO 精炼。</li><li>训练资源与实现：基于 Qwen2.5-VL-7B，在视觉投影器与 LLM 主干上使用 LoRA 微调；所有代码、预处理数据与场景配置开源于 GitHub。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>开放问答(open-loop)评估：在 LingoQA、BDD、HAD 上报告 BLEU 与 ROUGE(1/L)，对比 Qwen2.5-VL-7B、Llava-7B、LlavaNext-7B、Impromptu 等基线。</li><li>闭环驾驶(closed-loop)评估：在 CAVE 平台进行人机在环测试与接管采集；将精炼模型在 Bench2Drive 基准上与多种小规模任务特定模型与大规模预训练模型对比，指标包括：</li></ul>\n<p> - Driving Score(DS)</p>\n<p> - Success Rate(SR)</p>\n<p> - Efficiency</p>\n<p> - Comfortness</p>\n<ul><li>性能结果：CoReVLA 在 Bench2Drive 取得 DS 72.18、SR 50%，较次优方法提升 7.96 DS 与 14.99% SR；案例研究显示 DPO 前后行为改进与跨平台(CAVE↔Bench2Drive)泛化。</li></ul>"
  },
  {
    "date": "2025-09-19",
    "title": "A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning",
    "link": "http://arxiv.org/abs/2509.15937",
    "summary_markdown": "# 论文研究单位\n- 上海人工智能实验室（Shanghai AI Lab）\n# 论文概述\n- 现实世界机器人强化学习（RL）通常依赖人工设计的稀疏奖励、探索低效、数据昂贵，导致泛化与稳定性受限。\n- 提出VLAC（Vision-Language-Action-Critic）一体化模型，作为通用过程奖励模型（产生密集进度增量与完成信号），同时作为策略生成动作，统一“批评家（奖励/完成/价值）”与“演员（动作）”角色。\n- 基于InternVL预训练于超4000小时多源数据（人类演示、机器人轨迹、VQA等），支持零样本与一次性上下文迁移、跨场景/跨任务泛化，并通过人类在环分级机制稳定早期学习与样本效率。\n- 在四类真实操控任务上，200次真实交互内成功率由约30%提升至约90%；结合人类在环可进一步提升约50%样本效率，最高达100%最终成功率。\n# 论文核心贡献点\n- 构建统一的VLAC模型：同一自回归架构交替生成奖励/完成信号与动作token，简化接口、降低奖励工程成本。\n- 设计pairwise任务进度理解：输入两帧图像与语言目标，输出进度增量，天然作为TD奖励，并可判别退步/停滞；结合任务描述估计与完成判定辅助任务理解。\n- 引入一次性上下文学习与样本构造：联合正负样本、跨采样描述、像素差过滤、任务描述与图像序列错配，显著提升跨场景/跨任务泛化与对失败轨迹的判别。\n- 现实世界RL异步执行架构与动态推理调度：0.1秒内响应，修正时间戳匹配动作执行，确保连续性与低延迟；PPO基于结构化动作模板与价值头训练。\n- 人类在环分级策略（离线演示回放、Return and Explore、Human Guided Explore）：稳定训练、加速探索、减少早期崩溃风险。\n- 跨实体/跨视角/跨场景的大规模泛化评估与真实任务提升：四个现实操控任务成功率与样本效率显著改善，并支持多机器人扩展。\n# 论文方法描述\n- 模型架构（基于InternVL的多模态大模型）\n - 批评家（奖励/完成/价值）：pairwise进度学习、任务描述生成、任务完成判定；产生密集进度与done信号；价值头连接以供GAE。\n - 演员（动作生成）：结构化动作模板自回归生成delta端位姿（x/y/z/roll/pitch/yaw/open）；记录数值token logits用于PPO；支持多样性采样。\n - 上下文学习：参考轨迹与起始帧作为可选输入，提升迁移能力与绝对进度估计。\n- 训练数据与构造\n - 数据规模：超3000小时人类演示+1200小时公共机器人操控+15小时自收集；多VQA数据集（对话、机器人理解、空间推理、图像差异），总计约4000万数据点。\n - 样本构造：像素差过滤（阈值1%）、正反向/细粒度与全局联合采样、任务完成正负配对、错配描述负样本，提升稳健性与判别能力。\n- 现实世界RL框架\n - 异步推理与动态分配：多机器人通过ZeroMQ与Ray连接；推理请求调度至空闲VLA副本；观察时间戳与动作时间戳对齐。\n - PPO优化：结构化动作token化与概率记录；价值头预热；GAE估计优势；熵正则化促进探索；针对vllm与torch的概率漂移需重算训练阶段概率。\n - 人类在环：专家演示回放（NLL损失）、针对性Reset的Return and Explore、人类指导演示补全回放，三级干预稳定学习与提升探索效率。\n# 论文使用数据集和训练资源\n- 数据集与领域\n - 人类/机器人操控：Ego4D HOD、AGIBOT、Bridge、Droid、FMB、RoboSet、自收集数据\n - 多模态VQA：Llava、SpatialQA、RobotVQA、Spot the diff、InstructPix2Pix\n - 跨域评测集：RT1、RoboNet、Dobb-E、RH20T、EgoDex、RoboFAC（成功/失败）\n- 训练资源与实现\n - 预训练：batch=3200，最大学习率8e-4\n - 现实RL：2B VLAC为演员（策略），8B VLAC为批评家（奖励/完成/价值）；AGILE PiPER 7-DOF末端执行器（delta pose）；单帧观察含语言指令、前视相机图像、末端位姿\n - 推理与训练：基于Ray与ZeroMQ；动态推理调度，推理端使用vllm或torch；训练端对vllm生成动作概率进行torch重算以稳定PPO；价值头在演示/早期探索样本上预热\n - 机器人与系统：单控制器架构；动作时间戳相对观察滞后以匹配执行时间；推理响应目标<0.1秒\n# 论文使用的评估环境和评估指标\n- 任务环境\n - 真实机器人：AGILE PiPER；四类操控任务（开盖、取放碗、桌面清扫、大米转移等）\n - 多机器人扩展与跨场景/跨实体评估（RT1、RoboNet、Dobb-E、EgoDex、RoboFAC等）\n- 评估指标\n - 批评家进度理解：VOC（值序相关）、VROC（反转序列一致性）、VOC-F1综合指标、NR（负进度比例）\n - 演员动作与策略：任务进度（人工标注）、成功率（10次trial）、学习曲线（样本效率）\n - 现实RL表现：成功率曲线、收敛速度、人类在环干预对样本效率的提升幅度\n- 主要结果（摘要）\n - 批评家：在Bridge/Droid等分布内与RT1/RoboNet/EgoDex/RoboFAC等分布外均取得高VOC-F1，并能清晰区分成功/失败轨迹；一次性上下文显著提升跨域性能\n - 演员与RL：四任务平均成功率约75%，单任务最高达90%；200episode内由约30%提升至约90%；人类在环可再提升约50%样本效率，最终可达100%",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>上海人工智能实验室（Shanghai AI Lab）</li></ul>\n<h1>论文概述</h1>\n<ul><li>现实世界机器人强化学习（RL）通常依赖人工设计的稀疏奖励、探索低效、数据昂贵，导致泛化与稳定性受限。</li><li>提出VLAC（Vision-Language-Action-Critic）一体化模型，作为通用过程奖励模型（产生密集进度增量与完成信号），同时作为策略生成动作，统一“批评家（奖励/完成/价值）”与“演员（动作）”角色。</li><li>基于InternVL预训练于超4000小时多源数据（人类演示、机器人轨迹、VQA等），支持零样本与一次性上下文迁移、跨场景/跨任务泛化，并通过人类在环分级机制稳定早期学习与样本效率。</li><li>在四类真实操控任务上，200次真实交互内成功率由约30%提升至约90%；结合人类在环可进一步提升约50%样本效率，最高达100%最终成功率。</li></ul>\n<h1>论文核心贡献点</h1>\n<ul><li>构建统一的VLAC模型：同一自回归架构交替生成奖励/完成信号与动作token，简化接口、降低奖励工程成本。</li><li>设计pairwise任务进度理解：输入两帧图像与语言目标，输出进度增量，天然作为TD奖励，并可判别退步/停滞；结合任务描述估计与完成判定辅助任务理解。</li><li>引入一次性上下文学习与样本构造：联合正负样本、跨采样描述、像素差过滤、任务描述与图像序列错配，显著提升跨场景/跨任务泛化与对失败轨迹的判别。</li><li>现实世界RL异步执行架构与动态推理调度：0.1秒内响应，修正时间戳匹配动作执行，确保连续性与低延迟；PPO基于结构化动作模板与价值头训练。</li><li>人类在环分级策略（离线演示回放、Return and Explore、Human Guided Explore）：稳定训练、加速探索、减少早期崩溃风险。</li><li>跨实体/跨视角/跨场景的大规模泛化评估与真实任务提升：四个现实操控任务成功率与样本效率显著改善，并支持多机器人扩展。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>模型架构（基于InternVL的多模态大模型）</li></ul>\n<p> - 批评家（奖励/完成/价值）：pairwise进度学习、任务描述生成、任务完成判定；产生密集进度与done信号；价值头连接以供GAE。</p>\n<p> - 演员（动作生成）：结构化动作模板自回归生成delta端位姿（x/y/z/roll/pitch/yaw/open）；记录数值token logits用于PPO；支持多样性采样。</p>\n<p> - 上下文学习：参考轨迹与起始帧作为可选输入，提升迁移能力与绝对进度估计。</p>\n<ul><li>训练数据与构造</li></ul>\n<p> - 数据规模：超3000小时人类演示+1200小时公共机器人操控+15小时自收集；多VQA数据集（对话、机器人理解、空间推理、图像差异），总计约4000万数据点。</p>\n<p> - 样本构造：像素差过滤（阈值1%）、正反向/细粒度与全局联合采样、任务完成正负配对、错配描述负样本，提升稳健性与判别能力。</p>\n<ul><li>现实世界RL框架</li></ul>\n<p> - 异步推理与动态分配：多机器人通过ZeroMQ与Ray连接；推理请求调度至空闲VLA副本；观察时间戳与动作时间戳对齐。</p>\n<p> - PPO优化：结构化动作token化与概率记录；价值头预热；GAE估计优势；熵正则化促进探索；针对vllm与torch的概率漂移需重算训练阶段概率。</p>\n<p> - 人类在环：专家演示回放（NLL损失）、针对性Reset的Return and Explore、人类指导演示补全回放，三级干预稳定学习与提升探索效率。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集与领域</li></ul>\n<p> - 人类/机器人操控：Ego4D HOD、AGIBOT、Bridge、Droid、FMB、RoboSet、自收集数据</p>\n<p> - 多模态VQA：Llava、SpatialQA、RobotVQA、Spot the diff、InstructPix2Pix</p>\n<p> - 跨域评测集：RT1、RoboNet、Dobb-E、RH20T、EgoDex、RoboFAC（成功/失败）</p>\n<ul><li>训练资源与实现</li></ul>\n<p> - 预训练：batch=3200，最大学习率8e-4</p>\n<p> - 现实RL：2B VLAC为演员（策略），8B VLAC为批评家（奖励/完成/价值）；AGILE PiPER 7-DOF末端执行器（delta pose）；单帧观察含语言指令、前视相机图像、末端位姿</p>\n<p> - 推理与训练：基于Ray与ZeroMQ；动态推理调度，推理端使用vllm或torch；训练端对vllm生成动作概率进行torch重算以稳定PPO；价值头在演示/早期探索样本上预热</p>\n<p> - 机器人与系统：单控制器架构；动作时间戳相对观察滞后以匹配执行时间；推理响应目标<0.1秒</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>任务环境</li></ul>\n<p> - 真实机器人：AGILE PiPER；四类操控任务（开盖、取放碗、桌面清扫、大米转移等）</p>\n<p> - 多机器人扩展与跨场景/跨实体评估（RT1、RoboNet、Dobb-E、EgoDex、RoboFAC等）</p>\n<ul><li>评估指标</li></ul>\n<p> - 批评家进度理解：VOC（值序相关）、VROC（反转序列一致性）、VOC-F1综合指标、NR（负进度比例）</p>\n<p> - 演员动作与策略：任务进度（人工标注）、成功率（10次trial）、学习曲线（样本效率）</p>\n<p> - 现实RL表现：成功率曲线、收敛速度、人类在环干预对样本效率的提升幅度</p>\n<ul><li>主要结果（摘要）</li></ul>\n<p> - 批评家：在Bridge/Droid等分布内与RT1/RoboNet/EgoDex/RoboFAC等分布外均取得高VOC-F1，并能清晰区分成功/失败轨迹；一次性上下文显著提升跨域性能</p>\n<p> - 演员与RL：四任务平均成功率约75%，单任务最高达90%；200episode内由约30%提升至约90%；人类在环可再提升约50%样本效率，最终可达100%</p>"
  },
  {
    "date": "2025-09-18",
    "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
    "link": "http://arxiv.org/abs/2509.15212",
    "summary_markdown": "论文研究单位：阿里巴巴集团DAMO Academy（1]DAMO Academy, Alibaba Group）和Hupan Lab（2]Hupan Lab）。\n\n论文概述：论文提出RynnVLA-001模型，旨在通过人类演示增强机器人操作能力。该模型采用两阶段预训练方法：首先利用1200万个自我中心人类操作视频训练图像到视频生成模型，预测未来帧；随后在人类关键点轨迹数据上联合学习帧和轨迹预测，桥接视觉与行动预测。同时引入ActionVAE编码行动序列为紧凑嵌入，降低VLA输出空间复杂度。最终在机器人数据集微调后，RynnVLA-001在拾取放置等任务上优于先进基线（如GR00T N1.5和Pi0），验证了预训练策略对VLA模型的有效性。\n\n论文核心贡献点：\n- 提出基于人类演示的视频生成预训练方法，缓解机器人操作数据稀缺问题。\n- 设计两阶段预训练管道：自我中心视频生成预训练和人类中心轨迹感知视频建模。\n- 引入ActionVAE优化行动表示，通过压缩行动序列提升预测平滑性和效率。\n- 实验证明预训练权重能显著提升VLA模型在多任务场景下的成功率。\n\n论文方法描述：\n模型采用三阶段训练流程：\n1. **Ego-Centric Video Generative Pretraining**：基于Chameleon架构的自回归变换器，以图像和语言指令为输入，交替排列视觉令牌和语言令牌，预测未来帧。训练数据包含1200万人类操作视频和244K机器人视频。\n2. **Human-Centric Trajectory-Aware Video Modeling**：在EgoDex数据集上微调，联合预测未来帧和手腕关键点轨迹。使用ActionVAE压缩轨迹为连续嵌入，并引入状态嵌入表示当前手腕位置。\n3. **Robot-Centric Vision-Language Action Modeling**：在SO100机器人数据上微调，输入双视角RGB和机器人状态，预测行动嵌入（由ActionVAE解码为行动序列）。推理时仅输出行动嵌入，跳过未来帧生成以提升实时性。\n\n论文使用数据集和训练资源：\n- **数据集**：\n - 预训练：12M自我中心人类操作视频（网络源过滤）、244K机器人操作视频（BridgeData V2等）、EgoDex人类关键点数据。\n - 微调：SO100机器人自收集数据集（3任务：拾取放置绿草莓249演示、拾取草莓248演示、插入笔架301演示；场景含单/多目标、干扰物）。\n- **训练资源**：基于Chameleon代码扩展，使用大规模计算资源进行视频预训练和机器人微调。\n\n论文使用的评估环境和评估指标：\n- **评估环境**：LeRobot SO100机械臂，任务包括拾取放置绿草莓、拾取草莓、插入笔架；场景分为单目标、多目标和指令跟随（带干扰物）；测试覆盖多机械臂和不同环境。\n- **评估指标**：\n - 任务成功率（SR）：目标完成比例。\n - Success Rate@1（SR@1）：单次试验成功率。\n - 平均成功率：跨任务平均SR。\n - 失败条件：超时、抓取失败超过5次、触碰干扰物。",
    "summary_html": "<p>论文研究单位：阿里巴巴集团DAMO Academy（1]DAMO Academy, Alibaba Group）和Hupan Lab（2]Hupan Lab）。</p>\n\n<p>论文概述：论文提出RynnVLA-001模型，旨在通过人类演示增强机器人操作能力。该模型采用两阶段预训练方法：首先利用1200万个自我中心人类操作视频训练图像到视频生成模型，预测未来帧；随后在人类关键点轨迹数据上联合学习帧和轨迹预测，桥接视觉与行动预测。同时引入ActionVAE编码行动序列为紧凑嵌入，降低VLA输出空间复杂度。最终在机器人数据集微调后，RynnVLA-001在拾取放置等任务上优于先进基线（如GR00T N1.5和Pi0），验证了预训练策略对VLA模型的有效性。</p>\n\n<p>论文核心贡献点：</p>\n<ul><li>提出基于人类演示的视频生成预训练方法，缓解机器人操作数据稀缺问题。</li><li>设计两阶段预训练管道：自我中心视频生成预训练和人类中心轨迹感知视频建模。</li><li>引入ActionVAE优化行动表示，通过压缩行动序列提升预测平滑性和效率。</li><li>实验证明预训练权重能显著提升VLA模型在多任务场景下的成功率。</li></ul>\n\n<p>论文方法描述：</p>\n<p>模型采用三阶段训练流程：</p>\n<ol><li><strong>Ego-Centric Video Generative Pretraining</strong>：基于Chameleon架构的自回归变换器，以图像和语言指令为输入，交替排列视觉令牌和语言令牌，预测未来帧。训练数据包含1200万人类操作视频和244K机器人视频。</li><li><strong>Human-Centric Trajectory-Aware Video Modeling</strong>：在EgoDex数据集上微调，联合预测未来帧和手腕关键点轨迹。使用ActionVAE压缩轨迹为连续嵌入，并引入状态嵌入表示当前手腕位置。</li><li><strong>Robot-Centric Vision-Language Action Modeling</strong>：在SO100机器人数据上微调，输入双视角RGB和机器人状态，预测行动嵌入（由ActionVAE解码为行动序列）。推理时仅输出行动嵌入，跳过未来帧生成以提升实时性。</li></ol>\n\n<p>论文使用数据集和训练资源：</p>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：12M自我中心人类操作视频（网络源过滤）、244K机器人操作视频（BridgeData V2等）、EgoDex人类关键点数据。</p>\n<p> - 微调：SO100机器人自收集数据集（3任务：拾取放置绿草莓249演示、拾取草莓248演示、插入笔架301演示；场景含单/多目标、干扰物）。</p>\n<ul><li><strong>训练资源</strong>：基于Chameleon代码扩展，使用大规模计算资源进行视频预训练和机器人微调。</li></ul>\n\n<p>论文使用的评估环境和评估指标：</p>\n<ul><li><strong>评估环境</strong>：LeRobot SO100机械臂，任务包括拾取放置绿草莓、拾取草莓、插入笔架；场景分为单目标、多目标和指令跟随（带干扰物）；测试覆盖多机械臂和不同环境。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（SR）：目标完成比例。</p>\n<p> - Success Rate@1（SR@1）：单次试验成功率。</p>\n<p> - 平均成功率：跨任务平均SR。</p>\n<p> - 失败条件：超时、抓取失败超过5次、触碰干扰物。</p>"
  },
  {
    "date": "2025-09-18",
    "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue",
    "link": "http://arxiv.org/abs/2509.15061",
    "summary_markdown": "## 论文研究单位\n复旦大学计算机科学与人工智能学院\n上海创新研究院\n加州大学伯克利分校机械系统控制实验室（UC Berkeley Mechanical Systems Control Lab）\n## 论文概述\n提出 Ask-to-Clarify 框架以解决真实环境中指令歧义性。框架首先通过多轮对话主动向用户提问澄清歧义，然后端到端生成低层动作执行具体任务。框架由协作组件（VLM）与动作组件（扩散专家）构成，并通过连接模块与两阶段“知识隔离”训练策略实现协作与动作生成能力的融合。在推理时通过信号检测器实现“提问/动作”模式的无缝切换。\n## 论文核心贡献点\n- 提出任务与框架：要求具身智能体先通过多轮问答澄清歧义，再执行指令；采用 VLM 协作与扩散动作专家的组合，并设计连接模块以在两者间建立可靠条件。\n- 两阶段知识隔离训练策略：先学习歧义澄清与交互，再学习端到端低层动作；通过冻结 VLM 防止灾难性遗忘，同时用连接模块补偿 VLM 与扩散模型的联动。\n- 真实世界实验与验证：在 8 项真实任务上对比多种 SOTA VLA，表明该框架显著提升在歧义指令与复杂场景下的成功率与鲁棒性。\n## 论文方法描述\n- 任务定义：对给定视觉观察与歧义指令，Agent 依序生成澄清问题并接收回答；若干轮后推断正确指令，然后生成低层动作序列完成任务。\n- 框架结构\n - 协作组件（VLM，Qwen2-VL-2B）：负责提问、判断歧义与推导正确指令。\n - 连接模块（FiLM）：以语言指令为条件对视觉观察进行特征调制，为动作专家提供更具区分性的条件。\n - 动作组件（扩散专家，ScaleDP-Huge）：端到端生成低层动作（动作块长度 50）。\n - 信号检测器：解析 VLM 输出末端的信号token（<AMBG>、<NOT_AMBG>、<ACT>、<REJ>），以无训练方式路由“提问/动作”状态。\n- 两阶段训练\n - Stage 1：使用歧义交互对话数据训练协作组件；冻结视觉编码器，微调 LLM，新增信号token用于区分歧义、行动/拒绝等状态。\n - Stage 2：冻结协作组件与视觉编码器，训练连接模块与扩散动作专家；采用专家演示的具身数据进行端到端动作生成。\n- 推理流程\n - 若指令被判为歧义则输出 <AMBG> 并生成问题；根据用户回答更新对话历史。\n - 多轮后由 VLM 推断正确指令并标记 <NOT_AMBG>；随后由检测器触发 <ACT>（目标可见则执行）或 <REJ>（目标不可见则拒绝）。\n## 论文使用数据集和训练资源\n- 机器人与设备：xArm 7（7 DoF + 1 DoF 夹爪）、RealSense D435 相机（手腕与第三视角），xArm Python SDK；Stage 2 采用 Meta Quest 3 进行遥操作采集演示。\n- 数据\n - Stage 1（对话数据）：收集多对象图像并由 Qwen3-235B-A22B 生成歧义指令、问答对与正确指令，构成交互对话集。\n - Stage 2（具身演示）：8 项任务，每项约 10 条演示。\n- 实现细节\n - 协作组件：Qwen2-VL-2B-Instruct\n - 动作组件：ScaleDP-Huge（扩散专家）\n - 训练超参数（示例）\n - Stage 1：学习率 1e-5，批量 128，50 轮，训练参数约 1.5B\n - Stage 2：学习率 2e-5，批量 64，40 轮，训练参数约 978M\n - 动作块长度：50 步\n## 论文使用的评估环境和评估指标\n- 环境与任务：xArm 7 真实场景下 8 项任务，含三类通用任务：\n - Put the Object on the plate（Apple/Peach/Orange）\n - Pour the water from the Color cup onto the plate（Red/Green/White）\n - Stack the Color1 block on top of the Color2 block（(Blue, Yellow)、(Yellow, Blue)）\n- 指标：成功率（每任务 20 次试验）；与基线对比（π0、π0-FAST、OpenVLA-OFT）；消融（知识隔离策略、连接模块、必要性）。\n- 附加评估\n - 协作能力：在“目标存在/不存在”场景下判断并执行的正确率。\n - 鲁棒性：低光照（灯光减半）、视觉干扰（相似干扰物）条件下的成功率。\n\n结果表明：Ask-to-Clarify 在全部 8 项任务显著优于基线；在低光照与干扰条件下仍保持较高成功率；两阶段训练与连接模块二者缺一不可，共同保证歧义澄清与端到端动作生成的综合能力。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>复旦大学计算机科学与人工智能学院</p>\n<p>上海创新研究院</p>\n<p>加州大学伯克利分校机械系统控制实验室（UC Berkeley Mechanical Systems Control Lab）</p>\n<h2>论文概述</h2>\n<p>提出 Ask-to-Clarify 框架以解决真实环境中指令歧义性。框架首先通过多轮对话主动向用户提问澄清歧义，然后端到端生成低层动作执行具体任务。框架由协作组件（VLM）与动作组件（扩散专家）构成，并通过连接模块与两阶段“知识隔离”训练策略实现协作与动作生成能力的融合。在推理时通过信号检测器实现“提问/动作”模式的无缝切换。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出任务与框架：要求具身智能体先通过多轮问答澄清歧义，再执行指令；采用 VLM 协作与扩散动作专家的组合，并设计连接模块以在两者间建立可靠条件。</li><li>两阶段知识隔离训练策略：先学习歧义澄清与交互，再学习端到端低层动作；通过冻结 VLM 防止灾难性遗忘，同时用连接模块补偿 VLM 与扩散模型的联动。</li><li>真实世界实验与验证：在 8 项真实任务上对比多种 SOTA VLA，表明该框架显著提升在歧义指令与复杂场景下的成功率与鲁棒性。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li>任务定义：对给定视觉观察与歧义指令，Agent 依序生成澄清问题并接收回答；若干轮后推断正确指令，然后生成低层动作序列完成任务。</li><li>框架结构</li></ul>\n<p> - 协作组件（VLM，Qwen2-VL-2B）：负责提问、判断歧义与推导正确指令。</p>\n<p> - 连接模块（FiLM）：以语言指令为条件对视觉观察进行特征调制，为动作专家提供更具区分性的条件。</p>\n<p> - 动作组件（扩散专家，ScaleDP-Huge）：端到端生成低层动作（动作块长度 50）。</p>\n<p> - 信号检测器：解析 VLM 输出末端的信号token（<AMBG>、<NOT_AMBG>、<ACT>、<REJ>），以无训练方式路由“提问/动作”状态。</p>\n<ul><li>两阶段训练</li></ul>\n<p> - Stage 1：使用歧义交互对话数据训练协作组件；冻结视觉编码器，微调 LLM，新增信号token用于区分歧义、行动/拒绝等状态。</p>\n<p> - Stage 2：冻结协作组件与视觉编码器，训练连接模块与扩散动作专家；采用专家演示的具身数据进行端到端动作生成。</p>\n<ul><li>推理流程</li></ul>\n<p> - 若指令被判为歧义则输出 <AMBG> 并生成问题；根据用户回答更新对话历史。</p>\n<p> - 多轮后由 VLM 推断正确指令并标记 <NOT_AMBG>；随后由检测器触发 <ACT>（目标可见则执行）或 <REJ>（目标不可见则拒绝）。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li>机器人与设备：xArm 7（7 DoF + 1 DoF 夹爪）、RealSense D435 相机（手腕与第三视角），xArm Python SDK；Stage 2 采用 Meta Quest 3 进行遥操作采集演示。</li><li>数据</li></ul>\n<p> - Stage 1（对话数据）：收集多对象图像并由 Qwen3-235B-A22B 生成歧义指令、问答对与正确指令，构成交互对话集。</p>\n<p> - Stage 2（具身演示）：8 项任务，每项约 10 条演示。</p>\n<ul><li>实现细节</li></ul>\n<p> - 协作组件：Qwen2-VL-2B-Instruct</p>\n<p> - 动作组件：ScaleDP-Huge（扩散专家）</p>\n<p> - 训练超参数（示例）</p>\n<p> - Stage 1：学习率 1e-5，批量 128，50 轮，训练参数约 1.5B</p>\n<p> - Stage 2：学习率 2e-5，批量 64，40 轮，训练参数约 978M</p>\n<p> - 动作块长度：50 步</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li>环境与任务：xArm 7 真实场景下 8 项任务，含三类通用任务：</li></ul>\n<p> - Put the Object on the plate（Apple/Peach/Orange）</p>\n<p> - Pour the water from the Color cup onto the plate（Red/Green/White）</p>\n<p> - Stack the Color1 block on top of the Color2 block（(Blue, Yellow)、(Yellow, Blue)）</p>\n<ul><li>指标：成功率（每任务 20 次试验）；与基线对比（π0、π0-FAST、OpenVLA-OFT）；消融（知识隔离策略、连接模块、必要性）。</li><li>附加评估</li></ul>\n<p> - 协作能力：在“目标存在/不存在”场景下判断并执行的正确率。</p>\n<p> - 鲁棒性：低光照（灯光减半）、视觉干扰（相似干扰物）条件下的成功率。</p>\n\n<p>结果表明：Ask-to-Clarify 在全部 8 项任务显著优于基线；在低光照与干扰条件下仍保持较高成功率；两阶段训练与连接模块二者缺一不可，共同保证歧义澄清与端到端动作生成的综合能力。</p>"
  },
  {
    "date": "2025-09-18",
    "title": "Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale",
    "link": "http://arxiv.org/abs/2509.14932",
    "summary_markdown": "# Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale\n## 论文研究单位\n- Department of Computer Science & Artificial Intelligence, University of Technology Nuremberg, Germany\n- Learning, Adaptive Systems and Robotics (LASR) Lab, Faculty of Computer Science, TU Dresden, Germany\n- Siemens Foundational Technologies, Siemens AG, Germany\n- Chair for Robotics, Artificial Intelligence and Real-Time Systems, TUM School of Computation, Information and Technology, Technical University of Munich, Germany\n## 论文概述\n论文提出了Robot Control Stack (RCS)，一个为大规模机器人学习设计的轻量级生态系统。随着Vision-Language-Action models (VLAs)的发展，传统机器人软件框架成为瓶颈，RCS旨在弥合这一差距。RCS采用模块化分层架构，为模拟和物理机器人提供统一接口，促进sim-to-real转换，在保持最小依赖和轻量级设计的同时，提供完整功能集，支持真实世界实验和大规模模拟训练。\n## 论文核心贡献点\n1. 提出基于环境包装器的RCS架构，支持在不同抽象级别轻松添加新功能，同时支持Python和C++\n2. 在常见用例上评估RCS，包括跨实体支持、模拟和真实环境中的训练数据收集、VLA和RL智能体的训练和评估\n3. 在可重现的取物任务上对Octo、OpenVLA和π₀进行广泛实验，涵盖多种不同机器人\n4. 展示将合成数据与真实数据混合可以显著提升π₀在真实世界中的性能\n## 论文方法描述\nRCS基于环境包装器概念设计，通过包装器元组W=⟨f:S→S′,g:A′→A,P′,R′⟩将状态和动作从马尔可夫决策过程(MDP)进行转换。架构包含：\n- C++底层接口定义抽象机器人控制函数，支持Python绑定\n- 场景包装器序列，可变异或观察环境动作和观察空间\n- 硬件抽象：标准化传感器和执行器接口，支持同步/异步操作\n- 仿真集成：基于MuJoCo，提供面向对象的场景视图和回调机制\n- 机器人工具包：集成Pinocchio进行运动学计算，OMPL用于运动规划\n- 数字孪生：实时运行仿真作为安全检查器\n- Agents应用层：通过RPC通信解决VLA策略依赖冲突\n## 论文使用数据集和训练资源\n- **真实数据集**：FR3 (143个演示)、xArm7 (100个演示)、UR5e (167个演示)、SO101 (120个演示)，均为30Hz频率收集\n- **仿真数据集**：3000个脚本化演示，成功率73%，生成2193个成功演示\n- **任务设置**：Pick-Cuboid任务，要求抓取绿色3D打印立方体\n- **训练资源**：消费者级GPU笔记本电脑用于脚本化仿真数据生成，Nvidia RTX 4080 + 12核CPU用于RL训练\n## 论文使用的评估环境和评估指标\n**评估环境**：\n- 四个真实机器人设置：FR3、xArm7、UR5e、SO101\n- 匹配MuJoCo仿真环境，复制FR3设置\n- 支持多种传感器：RealSense摄像头、DIGIT触觉传感器、Tacto触觉传感器\n\n**评估指标**：\n- **成功率**：50次真实世界 rollout 的抓取成功百分比\n- **跨VLA模型比较**：在30Hz和5Hz操作下比较Octo、OpenVLA、π₀\n- **Sim-to-Real评估**：使用SIMPLER方法评估真实到仿真的域转移\n- **数据混合实验**：混合真实和仿真数据对性能的影响\n- **RL评估**：训练3小时内(8.5M环境步骤)达到100%成功率，吞吐量>2000步/秒(24个并行环境)",
    "summary_html": "<h1>Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale</h1>\n<h2>论文研究单位</h2>\n<ul><li>Department of Computer Science & Artificial Intelligence, University of Technology Nuremberg, Germany</li><li>Learning, Adaptive Systems and Robotics (LASR) Lab, Faculty of Computer Science, TU Dresden, Germany</li><li>Siemens Foundational Technologies, Siemens AG, Germany</li><li>Chair for Robotics, Artificial Intelligence and Real-Time Systems, TUM School of Computation, Information and Technology, Technical University of Munich, Germany</li></ul>\n<h2>论文概述</h2>\n<p>论文提出了Robot Control Stack (RCS)，一个为大规模机器人学习设计的轻量级生态系统。随着Vision-Language-Action models (VLAs)的发展，传统机器人软件框架成为瓶颈，RCS旨在弥合这一差距。RCS采用模块化分层架构，为模拟和物理机器人提供统一接口，促进sim-to-real转换，在保持最小依赖和轻量级设计的同时，提供完整功能集，支持真实世界实验和大规模模拟训练。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出基于环境包装器的RCS架构，支持在不同抽象级别轻松添加新功能，同时支持Python和C++</li><li>在常见用例上评估RCS，包括跨实体支持、模拟和真实环境中的训练数据收集、VLA和RL智能体的训练和评估</li><li>在可重现的取物任务上对Octo、OpenVLA和π₀进行广泛实验，涵盖多种不同机器人</li><li>展示将合成数据与真实数据混合可以显著提升π₀在真实世界中的性能</li></ol>\n<h2>论文方法描述</h2>\n<p>RCS基于环境包装器概念设计，通过包装器元组W=⟨f:S→S′,g:A′→A,P′,R′⟩将状态和动作从马尔可夫决策过程(MDP)进行转换。架构包含：</p>\n<ul><li>C++底层接口定义抽象机器人控制函数，支持Python绑定</li><li>场景包装器序列，可变异或观察环境动作和观察空间</li><li>硬件抽象：标准化传感器和执行器接口，支持同步/异步操作</li><li>仿真集成：基于MuJoCo，提供面向对象的场景视图和回调机制</li><li>机器人工具包：集成Pinocchio进行运动学计算，OMPL用于运动规划</li><li>数字孪生：实时运行仿真作为安全检查器</li><li>Agents应用层：通过RPC通信解决VLA策略依赖冲突</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>真实数据集</strong>：FR3 (143个演示)、xArm7 (100个演示)、UR5e (167个演示)、SO101 (120个演示)，均为30Hz频率收集</li><li><strong>仿真数据集</strong>：3000个脚本化演示，成功率73%，生成2193个成功演示</li><li><strong>任务设置</strong>：Pick-Cuboid任务，要求抓取绿色3D打印立方体</li><li><strong>训练资源</strong>：消费者级GPU笔记本电脑用于脚本化仿真数据生成，Nvidia RTX 4080 + 12核CPU用于RL训练</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<p><strong>评估环境</strong>：</p>\n<ul><li>四个真实机器人设置：FR3、xArm7、UR5e、SO101</li><li>匹配MuJoCo仿真环境，复制FR3设置</li><li>支持多种传感器：RealSense摄像头、DIGIT触觉传感器、Tacto触觉传感器</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li><strong>成功率</strong>：50次真实世界 rollout 的抓取成功百分比</li><li><strong>跨VLA模型比较</strong>：在30Hz和5Hz操作下比较Octo、OpenVLA、π₀</li><li><strong>Sim-to-Real评估</strong>：使用SIMPLER方法评估真实到仿真的域转移</li><li><strong>数据混合实验</strong>：混合真实和仿真数据对性能的影响</li><li><strong>RL评估</strong>：训练3小时内(8.5M环境步骤)达到100%成功率，吞吐量>2000步/秒(24个并行环境)</li></ul>"
  },
  {
    "date": "2025-09-18",
    "title": "CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human",
    "link": "http://arxiv.org/abs/2509.14889",
    "summary_markdown": "# 论文研究单位\n- 清华大学计算机科学与技术系（作者归属与资金支持单位）\n# 论文概述\n- 提出 CollabVLA，一个自反思的视觉–语言–动作（VLA）框架，将标准视觉运动策略扩展为可与人“共同想象”与协作的代理\n- 核心动机：改善现有 VLA 的领域过拟合、推理不可解释，以及依赖重生成模型导致的延迟问题\n- 方案：通过在 MoE 适配下结合 VLM 的自反思语言推理与扩散式动作生成；在两阶段训练（动作基础化 + 反思调优）下，统一场景理解与动作生成，支持显式自反思并在不确定或失败时主动寻求人类指导\n- 效果：相比依赖生成的智能体，CollabVLA 归一化“时间”约降低 2×，“梦境”次数约降低 4×，同时成功率更高、可解释性更好、延迟更低\n# 论文核心贡献点\n- 系统化分析了直接自回归 VLA、潜在动作与世界模型三类路线的权衡，指出在执行期轻量级人类在环的缺失是一个被忽视的改进机会\n- 引入 CollabVLA 框架：在单一骨架视觉运动策略上原生支持反思推理、动作生成与人类交互，通过 MoE 适配实现“反思/控制”的自适应切换\n- 证明该方法能提升成功率与保持低延迟，并将自反思扩展为可触发的实时人类指导，实现更具稳健性的长尾任务表现\n# 论文方法描述\n- 问题形式化：给定当前观察、过往帧、本体感受与多模态目标，策略输出动作块、反思文本与二值“是否询问人类”的标志\n- 数据构造：两类互补语料\n - 多模态目标预训练：融合 Interleave-VLA 的交错图文提示与 MDT 的目标图像增强，并附加 Diffusion-VLA 风格的简洁语言理由\n - 反思强化调优：扩展 InstructVLA 流程，加入“上下文反思”任务，合成时序不一致、目标多选、动作/目标扰动等失败场景，以自然语言反思形式监督何时反思、如何诊断、如何修订\n- 模型架构\n - VLM 主干（InternVL2.5）：输入交错图文（含 [NOW]/[PAST] 图像、目标、人类提示标签、关节信息与可学习 [ACT] 查询）；输出反思字符串、二值询问指示与潜动作嵌入\n - MoE 适配：在每层 Transformer 的 MHA/FFN 线性投影中插入 LoRA“控制专家”与“反思专家”，通过轻量门控基于局部 token 隐状态自适应选择专家\n - 扩散式动作模型（DiT）：以潜动作 token 作为交叉注意键值记忆、以反思嵌入通过 FiLM 调制全层，迭代去噪生成低延迟、可行、推理一致的动作轨迹\n - 推理流程：两阶段“反思–询问/执行”循环；二值头预测是否提问；人类回复作为新增条件再次前向；为降低延迟，支持首次出现 [ACT] 即停止自回归、并行解码剩余查询、缓存跨步记忆，并在新指导到来时用相似度加权融合平滑动作\n- 训练管道（两阶段）\n - 动作基础化（Stage 1）：冻结主干，仅激活控制 LoRA；联合优化语言规划损失与扩散去噪损失，隐式学习潜动作 token 作为动作模型的 conditioning\n - 反思调优（Stage 2）：冻结主干与动作模型；联合训练控制/反思 LoRA、门控网络与询问头；目标函数为反思文本的交叉熵与询问标志的二分类交叉熵，确保不损失动作性能的同时获得可触发的人类交互能力\n# 论文使用数据集和训练资源\n- 数据来源与合成\n - 机器人/仿真操纵数据与真实世界演示：AgibotWorld、Simpler-3D 等\n - 多模态预训练：交错图文提示（Interleave-VLA 风格）、目标帧采样（MDT/GR-MG 风格）、扩散式轨迹理由注入（Diffusion-VLA 风格）\n - 反思强化调优：基于环境回放并由大语言模型生成反思答案，结合生成视觉状态与人工校验；插入无关/打乱帧诱导时序断裂、添加干扰对象诱发多义、扰动动作或目标模拟失败\n- 预训练与模型规模\n - 基于 InternVL2.5（约 4B 参数）的视觉语言主干\n - MoE + LoRA 适配（控制与反思专家）\n - DiT 动作生成器\n- 训练与调优\n - Stage 1：学习基础动作与轻量规划语言\n - Stage 2：学习反思生成与询问决策，同时冻结主干与动作模型以保性能\n# 论文使用的评估环境和评估指标\n- 评估维度\n - 多模态理解：MMMU、MMStar、OCRBench、HallBench，以及 TextVQA、DocVQA、InfoVQA、RealWorldQA；额外 500 样本的 ContextReflection 集（从 AgibotWorld 与 GenieSim 保留）\n - 仿真任务：在 Simpler 基础上扩展为“Simpler-Collab”评测，200 任务、8 类操纵任务，涵盖长时程控制与歧义消解，支持自动化的在环人类模拟\n - 现实世界任务：DOBOT CR5（ROBOTIQ 夹爪）与 UR5（AG95 夹爪），五个类别，每类四个实例\n- 指标\n - 成功率（SR）与平均完成长度（LEN）\n - 归一化时间（Time）：对每个任务进行分位裁剪并线性缩放后平均\n - Dream 计数：生成代理的平均显式推理步数；协作变体记为人类询问次数\n- 主要结果\n - CollabVLA 在多数仿真子任务上获得最高的 SR 与 LEN，并保持最低的 Time 与 Dream（Time≈36/1.9Dream）\n - 多模态理解对比显示 CollabVLA 相比常见 2B/4B VLA 在多个理解与 VQA 指标上保持或提升（如 MMMU、TextVQA、DocVQA、InfoVQA、RealWorldQA）\n - 消融实验验证：去反思（No-Ref）、无 FiLM（No-FiLM）、无询问（No-Ask）、无 MoE（No-MoE）、仅 Stage1（No-Tuning）等设置均弱于完整模型\n- 实现细节\n - 仿真环境基于 ManiSkill3（SAPIEN）实现\n - 对比基线：OpenVLA、ChatVLA、InstructVLA、ECoT、CoT-VLA、DiVLA、RoboDreamer、π0、UniVLA、MDT 等；为部分方法构建协作变体以公平对比\n - 人类在环评估：人类问题与任务脚本共同输入 LLM 自动模拟人类回复以形成自动化 HITL 评测",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>清华大学计算机科学与技术系（作者归属与资金支持单位）</li></ul>\n<h1>论文概述</h1>\n<ul><li>提出 CollabVLA，一个自反思的视觉–语言–动作（VLA）框架，将标准视觉运动策略扩展为可与人“共同想象”与协作的代理</li><li>核心动机：改善现有 VLA 的领域过拟合、推理不可解释，以及依赖重生成模型导致的延迟问题</li><li>方案：通过在 MoE 适配下结合 VLM 的自反思语言推理与扩散式动作生成；在两阶段训练（动作基础化 + 反思调优）下，统一场景理解与动作生成，支持显式自反思并在不确定或失败时主动寻求人类指导</li><li>效果：相比依赖生成的智能体，CollabVLA 归一化“时间”约降低 2×，“梦境”次数约降低 4×，同时成功率更高、可解释性更好、延迟更低</li></ul>\n<h1>论文核心贡献点</h1>\n<ul><li>系统化分析了直接自回归 VLA、潜在动作与世界模型三类路线的权衡，指出在执行期轻量级人类在环的缺失是一个被忽视的改进机会</li><li>引入 CollabVLA 框架：在单一骨架视觉运动策略上原生支持反思推理、动作生成与人类交互，通过 MoE 适配实现“反思/控制”的自适应切换</li><li>证明该方法能提升成功率与保持低延迟，并将自反思扩展为可触发的实时人类指导，实现更具稳健性的长尾任务表现</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>问题形式化：给定当前观察、过往帧、本体感受与多模态目标，策略输出动作块、反思文本与二值“是否询问人类”的标志</li><li>数据构造：两类互补语料</li></ul>\n<p> - 多模态目标预训练：融合 Interleave-VLA 的交错图文提示与 MDT 的目标图像增强，并附加 Diffusion-VLA 风格的简洁语言理由</p>\n<p> - 反思强化调优：扩展 InstructVLA 流程，加入“上下文反思”任务，合成时序不一致、目标多选、动作/目标扰动等失败场景，以自然语言反思形式监督何时反思、如何诊断、如何修订</p>\n<ul><li>模型架构</li></ul>\n<p> - VLM 主干（InternVL2.5）：输入交错图文（含 [NOW]/[PAST] 图像、目标、人类提示标签、关节信息与可学习 [ACT] 查询）；输出反思字符串、二值询问指示与潜动作嵌入</p>\n<p> - MoE 适配：在每层 Transformer 的 MHA/FFN 线性投影中插入 LoRA“控制专家”与“反思专家”，通过轻量门控基于局部 token 隐状态自适应选择专家</p>\n<p> - 扩散式动作模型（DiT）：以潜动作 token 作为交叉注意键值记忆、以反思嵌入通过 FiLM 调制全层，迭代去噪生成低延迟、可行、推理一致的动作轨迹</p>\n<p> - 推理流程：两阶段“反思–询问/执行”循环；二值头预测是否提问；人类回复作为新增条件再次前向；为降低延迟，支持首次出现 [ACT] 即停止自回归、并行解码剩余查询、缓存跨步记忆，并在新指导到来时用相似度加权融合平滑动作</p>\n<ul><li>训练管道（两阶段）</li></ul>\n<p> - 动作基础化（Stage 1）：冻结主干，仅激活控制 LoRA；联合优化语言规划损失与扩散去噪损失，隐式学习潜动作 token 作为动作模型的 conditioning</p>\n<p> - 反思调优（Stage 2）：冻结主干与动作模型；联合训练控制/反思 LoRA、门控网络与询问头；目标函数为反思文本的交叉熵与询问标志的二分类交叉熵，确保不损失动作性能的同时获得可触发的人类交互能力</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据来源与合成</li></ul>\n<p> - 机器人/仿真操纵数据与真实世界演示：AgibotWorld、Simpler-3D 等</p>\n<p> - 多模态预训练：交错图文提示（Interleave-VLA 风格）、目标帧采样（MDT/GR-MG 风格）、扩散式轨迹理由注入（Diffusion-VLA 风格）</p>\n<p> - 反思强化调优：基于环境回放并由大语言模型生成反思答案，结合生成视觉状态与人工校验；插入无关/打乱帧诱导时序断裂、添加干扰对象诱发多义、扰动动作或目标模拟失败</p>\n<ul><li>预训练与模型规模</li></ul>\n<p> - 基于 InternVL2.5（约 4B 参数）的视觉语言主干</p>\n<p> - MoE + LoRA 适配（控制与反思专家）</p>\n<p> - DiT 动作生成器</p>\n<ul><li>训练与调优</li></ul>\n<p> - Stage 1：学习基础动作与轻量规划语言</p>\n<p> - Stage 2：学习反思生成与询问决策，同时冻结主干与动作模型以保性能</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估维度</li></ul>\n<p> - 多模态理解：MMMU、MMStar、OCRBench、HallBench，以及 TextVQA、DocVQA、InfoVQA、RealWorldQA；额外 500 样本的 ContextReflection 集（从 AgibotWorld 与 GenieSim 保留）</p>\n<p> - 仿真任务：在 Simpler 基础上扩展为“Simpler-Collab”评测，200 任务、8 类操纵任务，涵盖长时程控制与歧义消解，支持自动化的在环人类模拟</p>\n<p> - 现实世界任务：DOBOT CR5（ROBOTIQ 夹爪）与 UR5（AG95 夹爪），五个类别，每类四个实例</p>\n<ul><li>指标</li></ul>\n<p> - 成功率（SR）与平均完成长度（LEN）</p>\n<p> - 归一化时间（Time）：对每个任务进行分位裁剪并线性缩放后平均</p>\n<p> - Dream 计数：生成代理的平均显式推理步数；协作变体记为人类询问次数</p>\n<ul><li>主要结果</li></ul>\n<p> - CollabVLA 在多数仿真子任务上获得最高的 SR 与 LEN，并保持最低的 Time 与 Dream（Time≈36/1.9Dream）</p>\n<p> - 多模态理解对比显示 CollabVLA 相比常见 2B/4B VLA 在多个理解与 VQA 指标上保持或提升（如 MMMU、TextVQA、DocVQA、InfoVQA、RealWorldQA）</p>\n<p> - 消融实验验证：去反思（No-Ref）、无 FiLM（No-FiLM）、无询问（No-Ask）、无 MoE（No-MoE）、仅 Stage1（No-Tuning）等设置均弱于完整模型</p>\n<ul><li>实现细节</li></ul>\n<p> - 仿真环境基于 ManiSkill3（SAPIEN）实现</p>\n<p> - 对比基线：OpenVLA、ChatVLA、InstructVLA、ECoT、CoT-VLA、DiVLA、RoboDreamer、π0、UniVLA、MDT 等；为部分方法构建协作变体以公平对比</p>\n<p> - 人类在环评估：人类问题与任务脚本共同输入 LLM 自动模拟人类回复以形成自动化 HITL 评测</p>"
  },
  {
    "date": "2025-09-18",
    "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI",
    "link": "http://arxiv.org/abs/2509.14687",
    "summary_markdown": "# 论文研究单位\n\n中兴通讯股份有限公司，中国；香港中文大学（深圳），中国\n# 论文概述\n\nRealMirror是一个综合性、开源的具身AI视觉-语言-动作（VLA）平台，专门针对人形机器人VLA研究设计。该平台旨在解决当前具身AI研究中的关键瓶颈：数据采集成本高、缺乏标准化基准测试、以及仿真与现实世界之间的显著差距。RealMirror提供了一个端到端的解决方案，涵盖数据采集、模型训练、模型推理和性能评估的完整流程。\n# 论文核心贡献点\n\n1. 构建了一个高效、低成本的数据采集、模型训练和模型推理系统，实现了端到端VLA研究，无需真实机器人参与\n\n2. 提出专门针对人形机器人的VLA基准测试，通过多场景和多种VLA模型的广泛实验促进模型演进和公平比较\n\n3. 通过集成生成模型和3D高斯溅射技术，展示了零样本Sim2Real的可行性，使仅在仿真数据上训练的模型能够在真实机器人上无缝执行任务，无需任何微调\n# 论文方法描述\n## 物理仿真场景构建\n基于NVIDIA Isaac Sim平台构建多样化的室内仿真环境，整合CAD模型和来自各种资产库的资产，分配适当的物理属性（质量、摩擦、碰撞参数等），确保仿真中的合理性和人形机器人实体的兼容性。\n## 数据采集系统\n开发了基于远程操作的数据采集系统，包含两个主要组件：\n- 运动控制管线：实现多级滤波机制，包括IK关节控制跳跃滤波、末端执行器位姿通信和漂移补偿、IK求解器阈值滤波、跨帧末端执行器位姿阈值滤波\n- 轻量级WebXR通信系统：实现90 Hz传输频率，与通用通信框架相比减少了114毫秒的端到端延迟\n## 统一训练和推理框架\n支持多种代表性VLA模型（ACT、Diffusion Policy、SmolVLA），集成时间集成机制以增强动作预测稳健性，基于LeRobot库扩展以支持人形机器人实体，与Isaac Sim深度集成实现交互式评估。\n## Sim2Real转换框架\n采用多管齐下的策略：\n- 静态环境渲染：使用3D高斯溅射从多个视角捕获目标真实世界工作空间，重建整个静态场景\n- 高保真关节机器人模型：使用3D高斯溅射重建物理人形机器人并分割为单独连杆，通过S、R、T变换与Isaac Sim中的USD模型对齐\n- 交互对象差异化处理：高精度对象采用数字孪生方法，低精度对象使用少样本3D生成模型\n- 坐标系对齐和相机校准：使用ICP算法对齐CAD资产与3DGS重建环境，通过SfM解决相机位姿\n# 论文使用数据集和训练资源\n## 数据集构成\n构建了高质量的人形机器人VLA数据集，包含五个任务场景，每个场景240条轨迹，总计超过1200条仿真轨迹：\n- Kitchen Cleanup（厨房清理）：Pick and Place、双臂协作\n- Air Fryer Manipulation（空气炸锅操作）：Pick and Place、推拉、双臂协作\n- Assembly Line Sorting（装配线分拣）：Pick and Place、双臂协作、动态抓取\n- Cup-to-Cup Transfer（杯间转移）：双臂协作、精密控制\n- Can Stacking（易拉罐叠放）：Pick and Place、精密控制\n## 训练资源\n- 硬件：PICO Neo3 Pro头戴设备和Ada5880工作站\n- 训练参数：每个模型训练100,000步，批次大小为16\n- 统一动作空间：26维（每条机械臂13维：7维机械臂+6维手部）\n# 论文使用的评估环境和评估指标\n## 评估环境\n- 仿真环境：基于Isaac Sim的交互式评估环境\n- 真实环境：ZHIYUAN A2机器人进行Sim2Real实验验证\n## 评估指标\n- 主要指标：任务成功率（Task Success Rate）\n- 评估规模：Kitchen Cleanup (400次试验)、Air Fryer Manipulation (400次试验)、Can Stacking (400次试验)、Cup-to-Cup Transfer (200次试验)、Assembly Line Sorting (100次试验)\n- 技能评估：从任务中抽象出五个核心机器人技能进行细粒度分析\n- Sim2Real评估：基本任务（拾取和放置）达到92.86%准确率，复杂任务（球体转移）达到71.43%准确率，无需微调即可实现真实世界部署\n## 实验结果\n三个代表性VLA模型在基准测试中的表现：\n- ACT：平均成功率73.55%，在Kitchen Cleanup和Assembly Line Sorting表现突出\n- Diffusion Policy：平均成功率75.15%，在Air Fryer Manipulation表现最佳\n- SmolVLA：平均成功率79.75%，整体表现最为稳健，特别是在精密控制任务中",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>中兴通讯股份有限公司，中国；香港中文大学（深圳），中国</p>\n<h1>论文概述</h1>\n\n<p>RealMirror是一个综合性、开源的具身AI视觉-语言-动作（VLA）平台，专门针对人形机器人VLA研究设计。该平台旨在解决当前具身AI研究中的关键瓶颈：数据采集成本高、缺乏标准化基准测试、以及仿真与现实世界之间的显著差距。RealMirror提供了一个端到端的解决方案，涵盖数据采集、模型训练、模型推理和性能评估的完整流程。</p>\n<h1>论文核心贡献点</h1>\n\n<p>1. 构建了一个高效、低成本的数据采集、模型训练和模型推理系统，实现了端到端VLA研究，无需真实机器人参与</p>\n\n<p>2. 提出专门针对人形机器人的VLA基准测试，通过多场景和多种VLA模型的广泛实验促进模型演进和公平比较</p>\n\n<p>3. 通过集成生成模型和3D高斯溅射技术，展示了零样本Sim2Real的可行性，使仅在仿真数据上训练的模型能够在真实机器人上无缝执行任务，无需任何微调</p>\n<h1>论文方法描述</h1>\n<h2>物理仿真场景构建</h2>\n<p>基于NVIDIA Isaac Sim平台构建多样化的室内仿真环境，整合CAD模型和来自各种资产库的资产，分配适当的物理属性（质量、摩擦、碰撞参数等），确保仿真中的合理性和人形机器人实体的兼容性。</p>\n<h2>数据采集系统</h2>\n<p>开发了基于远程操作的数据采集系统，包含两个主要组件：</p>\n<ul><li>运动控制管线：实现多级滤波机制，包括IK关节控制跳跃滤波、末端执行器位姿通信和漂移补偿、IK求解器阈值滤波、跨帧末端执行器位姿阈值滤波</li><li>轻量级WebXR通信系统：实现90 Hz传输频率，与通用通信框架相比减少了114毫秒的端到端延迟</li></ul>\n<h2>统一训练和推理框架</h2>\n<p>支持多种代表性VLA模型（ACT、Diffusion Policy、SmolVLA），集成时间集成机制以增强动作预测稳健性，基于LeRobot库扩展以支持人形机器人实体，与Isaac Sim深度集成实现交互式评估。</p>\n<h2>Sim2Real转换框架</h2>\n<p>采用多管齐下的策略：</p>\n<ul><li>静态环境渲染：使用3D高斯溅射从多个视角捕获目标真实世界工作空间，重建整个静态场景</li><li>高保真关节机器人模型：使用3D高斯溅射重建物理人形机器人并分割为单独连杆，通过S、R、T变换与Isaac Sim中的USD模型对齐</li><li>交互对象差异化处理：高精度对象采用数字孪生方法，低精度对象使用少样本3D生成模型</li><li>坐标系对齐和相机校准：使用ICP算法对齐CAD资产与3DGS重建环境，通过SfM解决相机位姿</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<h2>数据集构成</h2>\n<p>构建了高质量的人形机器人VLA数据集，包含五个任务场景，每个场景240条轨迹，总计超过1200条仿真轨迹：</p>\n<ul><li>Kitchen Cleanup（厨房清理）：Pick and Place、双臂协作</li><li>Air Fryer Manipulation（空气炸锅操作）：Pick and Place、推拉、双臂协作</li><li>Assembly Line Sorting（装配线分拣）：Pick and Place、双臂协作、动态抓取</li><li>Cup-to-Cup Transfer（杯间转移）：双臂协作、精密控制</li><li>Can Stacking（易拉罐叠放）：Pick and Place、精密控制</li></ul>\n<h2>训练资源</h2>\n<ul><li>硬件：PICO Neo3 Pro头戴设备和Ada5880工作站</li><li>训练参数：每个模型训练100,000步，批次大小为16</li><li>统一动作空间：26维（每条机械臂13维：7维机械臂+6维手部）</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<h2>评估环境</h2>\n<ul><li>仿真环境：基于Isaac Sim的交互式评估环境</li><li>真实环境：ZHIYUAN A2机器人进行Sim2Real实验验证</li></ul>\n<h2>评估指标</h2>\n<ul><li>主要指标：任务成功率（Task Success Rate）</li><li>评估规模：Kitchen Cleanup (400次试验)、Air Fryer Manipulation (400次试验)、Can Stacking (400次试验)、Cup-to-Cup Transfer (200次试验)、Assembly Line Sorting (100次试验)</li><li>技能评估：从任务中抽象出五个核心机器人技能进行细粒度分析</li><li>Sim2Real评估：基本任务（拾取和放置）达到92.86%准确率，复杂任务（球体转移）达到71.43%准确率，无需微调即可实现真实世界部署</li></ul>\n<h2>实验结果</h2>\n<p>三个代表性VLA模型在基准测试中的表现：</p>\n<ul><li>ACT：平均成功率73.55%，在Kitchen Cleanup和Assembly Line Sorting表现突出</li><li>Diffusion Policy：平均成功率75.15%，在Air Fryer Manipulation表现最佳</li><li>SmolVLA：平均成功率79.75%，整体表现最为稳健，特别是在精密控制任务中</li></ul>"
  },
  {
    "date": "2025-09-17",
    "title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping",
    "link": "http://arxiv.org/abs/2509.14143",
    "summary_markdown": "# 论文研究单位\n德雷塞尔大学电气与计算机工程系；弗吉尼亚理工大学海产农业研究与推广中心与生物系统工程系\n# 论文概述\n提出CLAW（CLIP-Language-Action for Weight）框架，用于“重量感知”的机器人抓取。核心思想是将条件评估从动作生成中解耦：由轻量的微调CLIP作为提示生成器，实时监测电子秤读数并产生离散语言提示（continue/stop）；π₀ 作为流匹配VLA策略接收多视角视觉与语言提示，生成连续控制信号。验证表明CLAW在单物体与混合物体场景中均可可靠执行重量约束抓取，并优于原始与仅微调的π₀。\n# 论文核心贡献点\n- 引入CLAW：在标准VLA之上增加任务专用VLM用于显式条件监控，实现重量感知操控。\n- 设计CLIP微调：将电子秤数字显示转换为可被VLA理解的离散提示。\n- 使用提示监督微调π₀：使其能融合CLIP提示与多视角观测，产生精确动作。\n- 跨单物体与混合任务（双臂）场景评估，显示鲁棒性与一致尊重重量阈值；对比基线（原始π₀与仅微调π₀）显著提升。\n# 论文方法描述\n- 整体架构\n - 输入：人类指令（指定对象与目标重量）、电子秤图像、场景多视角图像。\n - 模块1（CLIP）：对电子秤图像与指令生成二元提示 m_t ∈ {continue, stop}，参数化为 p_φ(m_t \\|o_t^scale, l)。\n - 模块2（π₀）：对场景图像与提示生成连续动作 a_t，参数化为 p_θ(a_t \\|o_t^scene, m_t)，采用流匹配生成30Hz控制（50步动作块）。\n - 频率：CLIP以20Hz更新，π₀以30Hz控制；当某步CLIP未更新时复用最近提示。\n- 训练流程\n - CLIP微调：采集2000张秤显示裁剪图；每张与N个“load k g target”指令配对，标签由真实读数 w* 与阈值k比较得到（y=continue当k<w*，否则y=stop），共2000N样本，最小化分类损失。\n - π₀微调：每任务收集50条演示（抓取与撤碗两阶段）；在演示中人工标注clip_prompt（抓取阶段为“continue…”，撤碗阶段为“stop…”），以流匹配损失最小化 \\|\\|v_θ(x(t),t,o_t^scene,m_t)−(a_t−x(t))\\|\\|^2，在H200上训练60,000步。\n- 推理运行\n - CLIP实时监测秤并输出continue/stop提示；π₀据提示与场景多视角生成动作块；重量达标后提示切换为“stop”，π₀执行撤碗。\n# 论文使用数据集和训练资源\n- 数据集\n - CLIP微调：2000张电子秤显示裁剪图，每图与N个阈值指令配对，生成2000N条continue/stop样本。\n - π₀微调：每任务50条演示，覆盖抓取与撤碗阶段；演示中帧级标注“clip_prompt”。\n- 训练资源\n - π₀微调：H200 GPU；60,000步；控制30Hz；CLIP提示更新20Hz。\n - CLIP微调：硬件未详述。\n# 论文使用的评估环境和评估指标\n- 评估环境\n - 单物体：桌面设含装目标物的篮子、电子秤与空碗；任务为“抓取至目标重量并撤碗”，覆盖糖果（20/30/40g）与大蒜（20/30/40g）。\n - 混合物体：左右各置一盒（糖果/大蒜），左右臂分工（抓取与撤碗不同对象），评估跨对象与双臂协调。\n - 鲁棒性测试：在抓取中途投加过量物体，使秤瞬时超过阈值，检验系统是否能即时切换为撤碗并中断未完成动作。\n- 评估指标\n - 成功率（20次试验）：分别统计“动作完成”（抓取与撤碗）与“停止点准确”（达到指定重量阈值即停止），对比原始π₀、仅微调π₀与CLAW。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>德雷塞尔大学电气与计算机工程系；弗吉尼亚理工大学海产农业研究与推广中心与生物系统工程系</p>\n<h1>论文概述</h1>\n<p>提出CLAW（CLIP-Language-Action for Weight）框架，用于“重量感知”的机器人抓取。核心思想是将条件评估从动作生成中解耦：由轻量的微调CLIP作为提示生成器，实时监测电子秤读数并产生离散语言提示（continue/stop）；π₀ 作为流匹配VLA策略接收多视角视觉与语言提示，生成连续控制信号。验证表明CLAW在单物体与混合物体场景中均可可靠执行重量约束抓取，并优于原始与仅微调的π₀。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>引入CLAW：在标准VLA之上增加任务专用VLM用于显式条件监控，实现重量感知操控。</li><li>设计CLIP微调：将电子秤数字显示转换为可被VLA理解的离散提示。</li><li>使用提示监督微调π₀：使其能融合CLIP提示与多视角观测，产生精确动作。</li><li>跨单物体与混合任务（双臂）场景评估，显示鲁棒性与一致尊重重量阈值；对比基线（原始π₀与仅微调π₀）显著提升。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>整体架构</li></ul>\n<p> - 输入：人类指令（指定对象与目标重量）、电子秤图像、场景多视角图像。</p>\n<p> - 模块1（CLIP）：对电子秤图像与指令生成二元提示 m_t ∈ {continue, stop}，参数化为 p_φ(m_t \\|o_t^scale, l)。</p>\n<p> - 模块2（π₀）：对场景图像与提示生成连续动作 a_t，参数化为 p_θ(a_t \\|o_t^scene, m_t)，采用流匹配生成30Hz控制（50步动作块）。</p>\n<p> - 频率：CLIP以20Hz更新，π₀以30Hz控制；当某步CLIP未更新时复用最近提示。</p>\n<ul><li>训练流程</li></ul>\n<p> - CLIP微调：采集2000张秤显示裁剪图；每张与N个“load k g target”指令配对，标签由真实读数 w* 与阈值k比较得到（y=continue当k<w*，否则y=stop），共2000N样本，最小化分类损失。</p>\n<p> - π₀微调：每任务收集50条演示（抓取与撤碗两阶段）；在演示中人工标注clip_prompt（抓取阶段为“continue…”，撤碗阶段为“stop…”），以流匹配损失最小化 \\|\\|v_θ(x(t),t,o_t^scene,m_t)−(a_t−x(t))\\|\\|^2，在H200上训练60,000步。</p>\n<ul><li>推理运行</li></ul>\n<p> - CLIP实时监测秤并输出continue/stop提示；π₀据提示与场景多视角生成动作块；重量达标后提示切换为“stop”，π₀执行撤碗。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集</li></ul>\n<p> - CLIP微调：2000张电子秤显示裁剪图，每图与N个阈值指令配对，生成2000N条continue/stop样本。</p>\n<p> - π₀微调：每任务50条演示，覆盖抓取与撤碗阶段；演示中帧级标注“clip_prompt”。</p>\n<ul><li>训练资源</li></ul>\n<p> - π₀微调：H200 GPU；60,000步；控制30Hz；CLIP提示更新20Hz。</p>\n<p> - CLIP微调：硬件未详述。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境</li></ul>\n<p> - 单物体：桌面设含装目标物的篮子、电子秤与空碗；任务为“抓取至目标重量并撤碗”，覆盖糖果（20/30/40g）与大蒜（20/30/40g）。</p>\n<p> - 混合物体：左右各置一盒（糖果/大蒜），左右臂分工（抓取与撤碗不同对象），评估跨对象与双臂协调。</p>\n<p> - 鲁棒性测试：在抓取中途投加过量物体，使秤瞬时超过阈值，检验系统是否能即时切换为撤碗并中断未完成动作。</p>\n<ul><li>评估指标</li></ul>\n<p> - 成功率（20次试验）：分别统计“动作完成”（抓取与撤碗）与“停止点准确”（达到指定重量阈值即停止），对比原始π₀、仅微调π₀与CLAW。</p>"
  },
  {
    "date": "2025-09-17",
    "title": "SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2509.14138",
    "summary_markdown": "### 论文研究单位\n- **Virginia Seafood Agricultural Research and Extension Center, and Department of Biological Systems Engineering, Virginia Tech, USA**\n- **Department of Electrical and Computer Engineering, Drexel University, USA**\n### 论文概述\n长时序机器人操作任务要求执行多个相互依赖的子任务，错误检测机制可能引发级联失效。现有视觉-语言-动作（VLA）模型（如π₀）在连续低级控制方面表现出色，但缺乏识别子任务完成的内部信号，在顺序设置中表现脆弱。本研究提出SeqVLA，它是π₀的完成感知扩展，通过添加轻量级检测头感知当前子任务是否完成。该双头设计使模型不仅生成操作动作，还能自主触发子任务转换。研究调研了四种微调策略（联合或顺序微调，冻结或不冻结预训练主干网）。实验在沙拉包装（七个连续子任务）和糖果包装（四个子任务）上进行。结果显示SeqVLA显著提升整体成功率，其中联合微调并冻结主干网策略表现最佳，消除了顺序相关失效。\n### 论文核心贡献点\n- 集成学习任务完成检测头到π₀模型，实现从多模态上下文中推断子任务完成。\n- 识别最有效的微调策略：联合微调并冻结主干网，确保可靠顺序执行。\n- 在两个实际长时序场景中评估，框架在任务级性能上显著优于强基线。\n### 论文方法描述\n- **问题建模**：长时序任务表示为顺序子任务序列$\\mathcal{T}=\\{\\mathcal{T}_{1}, \\mathcal{T}_{2}, ..., \\mathcal{T}_{n}\\}$，子任务$\\mathcal{T}_{i}$的完成是启动$\\mathcal{T}_{i+1}$的先决条件。\n- **架构**：SeqVLA扩展π₀架构（基于SigLIP视觉编码器、Gemma-2B语言主干网和Gemma-300M动作专家），添加共享动作专家特征的轻量级完成检测头（线性分类器），输出子任务完成概率$p$：\n - $p = \\sigma(\\textbf{W} \\cdot \\textbf{F} + b)$，其中$\\textbf{W} \\in \\mathbb{R}^{1024}$, $b \\in \\mathbb{R}$为参数，$\\sigma$为sigmoid函数。\n - 总损失函数：$L_{\\text{total}} = L_{\\text{action}} + \\lambda \\cdot L_{\\text{completion}}$，$L_{\\text{completion}}$为二分类交叉熵损失，权重$\\lambda=0.1$。\n- **微调策略**：\n - **联合微调**：动作和分类头同时优化。\n - **顺序微调**：先训练动作头和主干网，后训练分类头。\n - **冻结策略**：冻结预训练VLM主干网以保留原始知识，或全微调适应域。\n - 组合为四种配置：SeqVLA-J（联合微调，不冻结）、SeqVLA-JF（联合微调，冻结）、SeqVLA-S（顺序微调，不冻结）、SeqVLA-SF（顺序微调，冻结）。\n- **子任务执行**：每个推理步骤输出动作块和执行概率$p$。当$p < \\tau = 0.2$时触发转换：停止当前动作、回程家姿态、切换到下一子任务提示。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **沙拉包装**：七个连续子任务（菠菜、卷心菜、肉丸、鸡、西红柿、酱料杯、容器关闭），收集350集子任务演示数据。\n - **糖果包装**：四个子任务（软糖、两次Kinder巧克力、两次士力架、棒棒糖），收集200集子任务演示数据。\n - 附加长时序演示数据：完整沙拉和糖果任务轨迹用于π₀基线微调。\n- **数据收集**：使用Aloha双机械臂（14自由度）进行示教，三摄像头（顶视、左抓手、右抓手）记录RGB图像和机器人状态。\n- **训练资源**：\n - 基于π₀预训练模型（物理智能版），硬件为Aloha机器人。\n - 训练环境涉及实时数据采集和微调流程，具体计算资源未详述。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 机器人：Aloha双机械臂（14自由度），三摄像头视角。\n - 任务：沙拉包装（七子任务）和糖果包装（四子任务）的长时序执行。\n - 基线：π₀模型在完整长时序演示上微调，直接执行无子任务监控。\n- **评估指标**：\n - **成功率**：整体任务和子任务级成功率（如图表显示）。\n - **分类置信度**：使用熵值衡量完成检测的不确定性（SeqVLA-J熵0.76 vs SeqVLA-S熵1.35）。\n - **统计可靠性**：Kolmogorov–Smirnov（KS）统计量评估执行和完成阶段分布差异（KS值0.75–0.85，p < 0.001）。\n - **行为比较**：通过执行记录图（图8、9）定性分析π₀顺序失效vs SeqVLA可靠性。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li><strong>Virginia Seafood Agricultural Research and Extension Center, and Department of Biological Systems Engineering, Virginia Tech, USA</strong></li><li><strong>Department of Electrical and Computer Engineering, Drexel University, USA</strong></li></ul>\n<h3>论文概述</h3>\n<p>长时序机器人操作任务要求执行多个相互依赖的子任务，错误检测机制可能引发级联失效。现有视觉-语言-动作（VLA）模型（如π₀）在连续低级控制方面表现出色，但缺乏识别子任务完成的内部信号，在顺序设置中表现脆弱。本研究提出SeqVLA，它是π₀的完成感知扩展，通过添加轻量级检测头感知当前子任务是否完成。该双头设计使模型不仅生成操作动作，还能自主触发子任务转换。研究调研了四种微调策略（联合或顺序微调，冻结或不冻结预训练主干网）。实验在沙拉包装（七个连续子任务）和糖果包装（四个子任务）上进行。结果显示SeqVLA显著提升整体成功率，其中联合微调并冻结主干网策略表现最佳，消除了顺序相关失效。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>集成学习任务完成检测头到π₀模型，实现从多模态上下文中推断子任务完成。</li><li>识别最有效的微调策略：联合微调并冻结主干网，确保可靠顺序执行。</li><li>在两个实际长时序场景中评估，框架在任务级性能上显著优于强基线。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>问题建模</strong>：长时序任务表示为顺序子任务序列$\\mathcal{T}=\\{\\mathcal{T}_{1}, \\mathcal{T}_{2}, ..., \\mathcal{T}_{n}\\}$，子任务$\\mathcal{T}_{i}$的完成是启动$\\mathcal{T}_{i+1}$的先决条件。</li><li><strong>架构</strong>：SeqVLA扩展π₀架构（基于SigLIP视觉编码器、Gemma-2B语言主干网和Gemma-300M动作专家），添加共享动作专家特征的轻量级完成检测头（线性分类器），输出子任务完成概率$p$：</li></ul>\n<p> - $p = \\sigma(\\textbf{W} \\cdot \\textbf{F} + b)$，其中$\\textbf{W} \\in \\mathbb{R}^{1024}$, $b \\in \\mathbb{R}$为参数，$\\sigma$为sigmoid函数。</p>\n<p> - 总损失函数：$L_{\\text{total}} = L_{\\text{action}} + \\lambda \\cdot L_{\\text{completion}}$，$L_{\\text{completion}}$为二分类交叉熵损失，权重$\\lambda=0.1$。</p>\n<ul><li><strong>微调策略</strong>：</li></ul>\n<p> - <strong>联合微调</strong>：动作和分类头同时优化。</p>\n<p> - <strong>顺序微调</strong>：先训练动作头和主干网，后训练分类头。</p>\n<p> - <strong>冻结策略</strong>：冻结预训练VLM主干网以保留原始知识，或全微调适应域。</p>\n<p> - 组合为四种配置：SeqVLA-J（联合微调，不冻结）、SeqVLA-JF（联合微调，冻结）、SeqVLA-S（顺序微调，不冻结）、SeqVLA-SF（顺序微调，冻结）。</p>\n<ul><li><strong>子任务执行</strong>：每个推理步骤输出动作块和执行概率$p$。当$p < \\tau = 0.2$时触发转换：停止当前动作、回程家姿态、切换到下一子任务提示。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>沙拉包装</strong>：七个连续子任务（菠菜、卷心菜、肉丸、鸡、西红柿、酱料杯、容器关闭），收集350集子任务演示数据。</p>\n<p> - <strong>糖果包装</strong>：四个子任务（软糖、两次Kinder巧克力、两次士力架、棒棒糖），收集200集子任务演示数据。</p>\n<p> - 附加长时序演示数据：完整沙拉和糖果任务轨迹用于π₀基线微调。</p>\n<ul><li><strong>数据收集</strong>：使用Aloha双机械臂（14自由度）进行示教，三摄像头（顶视、左抓手、右抓手）记录RGB图像和机器人状态。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 基于π₀预训练模型（物理智能版），硬件为Aloha机器人。</p>\n<p> - 训练环境涉及实时数据采集和微调流程，具体计算资源未详述。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 机器人：Aloha双机械臂（14自由度），三摄像头视角。</p>\n<p> - 任务：沙拉包装（七子任务）和糖果包装（四子任务）的长时序执行。</p>\n<p> - 基线：π₀模型在完整长时序演示上微调，直接执行无子任务监控。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：整体任务和子任务级成功率（如图表显示）。</p>\n<p> - <strong>分类置信度</strong>：使用熵值衡量完成检测的不确定性（SeqVLA-J熵0.76 vs SeqVLA-S熵1.35）。</p>\n<p> - <strong>统计可靠性</strong>：Kolmogorov–Smirnov（KS）统计量评估执行和完成阶段分布差异（KS值0.75–0.85，p < 0.001）。</p>\n<p> - <strong>行为比较</strong>：通过执行记录图（图8、9）定性分析π₀顺序失效vs SeqVLA可靠性。</p>"
  },
  {
    "date": "2025-09-17",
    "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2509.14117",
    "summary_markdown": "# 论文研究单位\n\nMohamed bin Zayed University of Artificial Intelligence, Department of Robotics, Masdar City, Abu Dhabi, UAE\n# 论文概述\n\n论文针对Vision-Language-Action (VLA)模型在面对新颖相机视角时泛化能力不足的问题，提出了一种名为GeoAware-VLA的方法。该方法通过将标准VLA架构中的视觉编码器替换为预训练的视觉几何基础模型VGGT（Visual Geometry Grounded Transformer），显著提升了模型在新视角下的零样本泛化能力。\n# 论文核心贡献点\n\n1. 提出GeoAware-VLA方法，有效集成视觉几何基础模型到VLA架构中\n2. 在新视角下实现成功率翻倍提升，在仿真环境中表现优异\n3. 在真实机器人平台上验证了方法的实用性，特别是在新颖相机角度下的性能提升\n4. 确认了该方法在不同策略解码器上的一致改进效果\n# 论文方法描述\n\nGeoAware-VLA的核心在于使用冻结的预训练几何感知视觉编码器替代标准的可训练视觉编码器。具体包括：\n\n1. **几何感知视觉编码器**：采用VGGT作为冻结的特征提取器，利用其多层中间层特征捕获视觉和几何信息\n2. **视觉投影层**：通过1D卷积网络、池化操作和MLP将VGGT特征转换为统一表示空间\n3. **多模态编码**：语言和本体感受编码器使用简单的MLP投影器\n4. **策略解码器**：基于GPT风格的解码器transformer处理多模态输入序列\n5. **动作生成头**：提供两种变体 - MLP头用于连续动作回归，VQ-BeT头用于建模多模态行为\n# 论文使用数据集和训练资源\n\n1. **数据集**：使用LIBERO基准测试的四个任务套件进行评估\n - LIBERO-Spatial：测试空间布局泛化\n - LIBERO-Goal：测试新任务泛化\n - LIBERO-Object：测试新对象类型泛化\n - LIBERO-Long：测试更广泛的对象、布局和背景变化\n\n2. **训练资源**：使用PyTorch框架，在单个NVIDIA A100 GPU（40GB VRAM）上训练，每个模型训练150,000步（仿真）或50,000步（真实世界）\n# 论文使用的评估环境和评估指标\n\n1. **仿真评估环境**：\n - LIBERO基准测试，包含多样化的操控任务\n - 在训练视角和新颖视角下进行测试\n - 包含小、中、大三种视角偏移程度的评估\n\n2. **真实世界评估环境**：\n - Realman 65B机械臂\n - 桌面操控环境，配备两个固定视角相机\n - 5项不同的操控任务测试\n\n3. **评估指标**：\n - 主要指标：成功率（基于10次试验的平均值）\n - 视角鲁棒性：跨不同视角偏移程度的性能变化\n - 泛化能力：新视角下的零样本性能表现",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>Mohamed bin Zayed University of Artificial Intelligence, Department of Robotics, Masdar City, Abu Dhabi, UAE</p>\n<h1>论文概述</h1>\n\n<p>论文针对Vision-Language-Action (VLA)模型在面对新颖相机视角时泛化能力不足的问题，提出了一种名为GeoAware-VLA的方法。该方法通过将标准VLA架构中的视觉编码器替换为预训练的视觉几何基础模型VGGT（Visual Geometry Grounded Transformer），显著提升了模型在新视角下的零样本泛化能力。</p>\n<h1>论文核心贡献点</h1>\n\n<ol><li>提出GeoAware-VLA方法，有效集成视觉几何基础模型到VLA架构中</li><li>在新视角下实现成功率翻倍提升，在仿真环境中表现优异</li><li>在真实机器人平台上验证了方法的实用性，特别是在新颖相机角度下的性能提升</li><li>确认了该方法在不同策略解码器上的一致改进效果</li></ol>\n<h1>论文方法描述</h1>\n\n<p>GeoAware-VLA的核心在于使用冻结的预训练几何感知视觉编码器替代标准的可训练视觉编码器。具体包括：</p>\n\n<ol><li><strong>几何感知视觉编码器</strong>：采用VGGT作为冻结的特征提取器，利用其多层中间层特征捕获视觉和几何信息</li><li><strong>视觉投影层</strong>：通过1D卷积网络、池化操作和MLP将VGGT特征转换为统一表示空间</li><li><strong>多模态编码</strong>：语言和本体感受编码器使用简单的MLP投影器</li><li><strong>策略解码器</strong>：基于GPT风格的解码器transformer处理多模态输入序列</li><li><strong>动作生成头</strong>：提供两种变体 - MLP头用于连续动作回归，VQ-BeT头用于建模多模态行为</li></ol>\n<h1>论文使用数据集和训练资源</h1>\n\n<p>1. <strong>数据集</strong>：使用LIBERO基准测试的四个任务套件进行评估</p>\n<p> - LIBERO-Spatial：测试空间布局泛化</p>\n<p> - LIBERO-Goal：测试新任务泛化</p>\n<p> - LIBERO-Object：测试新对象类型泛化</p>\n<p> - LIBERO-Long：测试更广泛的对象、布局和背景变化</p>\n\n<p>2. <strong>训练资源</strong>：使用PyTorch框架，在单个NVIDIA A100 GPU（40GB VRAM）上训练，每个模型训练150,000步（仿真）或50,000步（真实世界）</p>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<p>1. <strong>仿真评估环境</strong>：</p>\n<p> - LIBERO基准测试，包含多样化的操控任务</p>\n<p> - 在训练视角和新颖视角下进行测试</p>\n<p> - 包含小、中、大三种视角偏移程度的评估</p>\n\n<p>2. <strong>真实世界评估环境</strong>：</p>\n<p> - Realman 65B机械臂</p>\n<p> - 桌面操控环境，配备两个固定视角相机</p>\n<p> - 5项不同的操控任务测试</p>\n\n<p>3. <strong>评估指标</strong>：</p>\n<p> - 主要指标：成功率（基于10次试验的平均值）</p>\n<p> - 视角鲁棒性：跨不同视角偏移程度的性能变化</p>\n<p> - 泛化能力：新视角下的零样本性能表现</p>"
  },
  {
    "date": "2025-09-17",
    "title": "Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach",
    "link": "http://arxiv.org/abs/2509.13774",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n\n北京小米机器人技术有限公司（Beijing Xiaomi Robot Technology Co., Ltd.）和香港城市大学（City University of Hong Kong）\n## 论文概述\n\n本文提出了一种基于人机交互的双执行器（dual-actor）VLA模型微调框架。该框架集成了一个主执行器用于鲁棒的多任务性能，以及一个精调执行器在潜在空间中进行自适应调整。除了标准的物理干预外，论文引入了一种轻量级的talk-and-tweak方案，将人类修正转换为语义化的语言命令，从而生成新的策略学习数据集。在真实世界多任务实验中，该方法在101分钟的在线微调内在三个任务上实现了100%的成功率。对于长视距任务，在超过12个连续操作中保持了50%的成功率。该框架还能有效扩展到多机器人训练，使用双机器人时效率提升可达2倍。\n## 论文核心贡献点\n\n1. 提出了一种新颖的双执行器VLA微调框架，集成了用于鲁棒多任务策略生成的主执行器和在潜在噪声空间中操作以实现细粒度可控动作调整的精调执行器\n\n2. 开发了一种轻量级人机交互方案，将实时物理修正（tweak）转换为语义化的语言命令（talk），形成talk-and-tweak数据集。精调执行器利用这些指令进行可解释的调整，而主执行器通过直接干预改进其基线策略\n\n3. 在真实机器人上验证了所提方法，展示了快速的多任务微调能力（101分钟内达到100%成功率）。对于长视距序列，在12个连续操作中保持50%成功率。此外，框架可无缝扩展到多机器人训练，使用双机器人训练时效率提升达2倍\n## 论文方法描述\n\n方法分为三个主要部分：\n\n1. 双执行器强化学习系统：包含预训练的VLA模型进行任务和状态编码，主执行器采用一致性策略（consistency policy）生成动作，精调执行器在潜在噪声空间中进行调整。训练分为离线预热阶段和在线交互阶段，主执行器通过混合目标（行为克隆+Q函数最大化）优化，精调执行器通过行为克隆、Q函数最大化和正则化损失联合训练\n\n2. Talk-and-Tweak人类干预设计：将人类物理修正（通过SpaceMouse）自动转换为自然语言精调命令。通过计算时间窗口内的累积位移，当超过阈值时生成相应的方向命令（如\"向右移动\"），形成包含（状态、动作、精调命令）的三元组数据集\n\n3. 高效多任务学习：采用共享的多任务执行器和任务特定的critic架构。为每个任务维护三个独立缓冲区（专家演示、策略rollouts、人类干预）。在在线阶段，主执行器从所有任务和缓冲区类型中均匀采样更新，精调执行器使用聚合的干预数据集训练，每个任务特定的critic仅使用其对应任务缓冲区的数据更新。引入任务权重机制平衡多任务学习进度\n## 论文使用数据集和训练资源\n\n数据集：使用Octo作为骨干VLA模型。预热阶段每个任务使用20条轨迹初始化策略（约3000个状态-动作对）。在线微调阶段在三个任务中收集约15000个交互对（每个任务约100条轨迹）。人类干预数据约占15%，用于训练精调策略\n\n硬件和训练资源：使用自主开发的7自由度机械臂。观察包括两个RGB图像（手腕相机128×128分辨率，头部相机256×256分辨率）和机械臂本体感知状态。动作空间为7维末端执行器增量位姿。数据采集和策略执行频率为10Hz。执行器进程运行在机器人上的NVIDIA Jetson Orin，学习器在配备NVIDIA RTX 3090 GPU的工作站上执行\n## 论文使用的评估环境和评估指标\n\n评估环境：真实物理环境中的螺栓操作任务，包括三个子任务：(1)将螺栓竖直放置，(2)拾取螺栓，(3)组装螺栓。机器人和物体位置在x-y轴上均匀随机化±5厘米以增强泛化性\n\n评估指标：\n1. 成功率：每个子任务如果在50个时间步内完成则视为成功，超过此限制计为失败。每个子任务独立评估成功率\n2. 片段长度：完成任务所需的时间步数\n3. 长视距任务评估：完全成功需要机器人顺序完成所有三个子任务\n4. 所有结果均为25次试验的平均值\n5. 多机器人扩展性：评估在不同机器人和训练配置下的训练效率和任务性能",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n\n<p>北京小米机器人技术有限公司（Beijing Xiaomi Robot Technology Co., Ltd.）和香港城市大学（City University of Hong Kong）</p>\n<h2>论文概述</h2>\n\n<p>本文提出了一种基于人机交互的双执行器（dual-actor）VLA模型微调框架。该框架集成了一个主执行器用于鲁棒的多任务性能，以及一个精调执行器在潜在空间中进行自适应调整。除了标准的物理干预外，论文引入了一种轻量级的talk-and-tweak方案，将人类修正转换为语义化的语言命令，从而生成新的策略学习数据集。在真实世界多任务实验中，该方法在101分钟的在线微调内在三个任务上实现了100%的成功率。对于长视距任务，在超过12个连续操作中保持了50%的成功率。该框架还能有效扩展到多机器人训练，使用双机器人时效率提升可达2倍。</p>\n<h2>论文核心贡献点</h2>\n\n<p>1. 提出了一种新颖的双执行器VLA微调框架，集成了用于鲁棒多任务策略生成的主执行器和在潜在噪声空间中操作以实现细粒度可控动作调整的精调执行器</p>\n\n<p>2. 开发了一种轻量级人机交互方案，将实时物理修正（tweak）转换为语义化的语言命令（talk），形成talk-and-tweak数据集。精调执行器利用这些指令进行可解释的调整，而主执行器通过直接干预改进其基线策略</p>\n\n<p>3. 在真实机器人上验证了所提方法，展示了快速的多任务微调能力（101分钟内达到100%成功率）。对于长视距序列，在12个连续操作中保持50%成功率。此外，框架可无缝扩展到多机器人训练，使用双机器人训练时效率提升达2倍</p>\n<h2>论文方法描述</h2>\n\n<p>方法分为三个主要部分：</p>\n\n<p>1. 双执行器强化学习系统：包含预训练的VLA模型进行任务和状态编码，主执行器采用一致性策略（consistency policy）生成动作，精调执行器在潜在噪声空间中进行调整。训练分为离线预热阶段和在线交互阶段，主执行器通过混合目标（行为克隆+Q函数最大化）优化，精调执行器通过行为克隆、Q函数最大化和正则化损失联合训练</p>\n\n<p>2. Talk-and-Tweak人类干预设计：将人类物理修正（通过SpaceMouse）自动转换为自然语言精调命令。通过计算时间窗口内的累积位移，当超过阈值时生成相应的方向命令（如\"向右移动\"），形成包含（状态、动作、精调命令）的三元组数据集</p>\n\n<p>3. 高效多任务学习：采用共享的多任务执行器和任务特定的critic架构。为每个任务维护三个独立缓冲区（专家演示、策略rollouts、人类干预）。在在线阶段，主执行器从所有任务和缓冲区类型中均匀采样更新，精调执行器使用聚合的干预数据集训练，每个任务特定的critic仅使用其对应任务缓冲区的数据更新。引入任务权重机制平衡多任务学习进度</p>\n<h2>论文使用数据集和训练资源</h2>\n\n<p>数据集：使用Octo作为骨干VLA模型。预热阶段每个任务使用20条轨迹初始化策略（约3000个状态-动作对）。在线微调阶段在三个任务中收集约15000个交互对（每个任务约100条轨迹）。人类干预数据约占15%，用于训练精调策略</p>\n\n<p>硬件和训练资源：使用自主开发的7自由度机械臂。观察包括两个RGB图像（手腕相机128×128分辨率，头部相机256×256分辨率）和机械臂本体感知状态。动作空间为7维末端执行器增量位姿。数据采集和策略执行频率为10Hz。执行器进程运行在机器人上的NVIDIA Jetson Orin，学习器在配备NVIDIA RTX 3090 GPU的工作站上执行</p>\n<h2>论文使用的评估环境和评估指标</h2>\n\n<p>评估环境：真实物理环境中的螺栓操作任务，包括三个子任务：(1)将螺栓竖直放置，(2)拾取螺栓，(3)组装螺栓。机器人和物体位置在x-y轴上均匀随机化±5厘米以增强泛化性</p>\n\n<p>评估指标：</p>\n<ol><li>成功率：每个子任务如果在50个时间步内完成则视为成功，超过此限制计为失败。每个子任务独立评估成功率</li><li>片段长度：完成任务所需的时间步数</li><li>长视距任务评估：完全成功需要机器人顺序完成所有三个子任务</li><li>所有结果均为25次试验的平均值</li><li>多机器人扩展性：评估在不同机器人和训练配置下的训练效率和任务性能</li></ol>"
  },
  {
    "date": "2025-09-17",
    "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving",
    "link": "http://arxiv.org/abs/2509.13769",
    "summary_markdown": "# 论文研究单位\n清华大学（Tsinghua University）、小米EV（Xiaomi EV）、澳门大学（University of Macau）、南洋理工大学（Nanyang Technological University）、北京大学（Peking University）\n# 论文概述\nAdaThinkDrive提出一种“快速回答/慢速思考”的双模推理机制，解决在端到端自动驾驶中思维链（CoT）在简单场景下过度推理、复杂场景下推理收益不一致的问题。论文在Navsim基准上进行实验与评估。\n# 论文核心贡献点\n- 通过在Navsim上系统性对比发现：简单场景下使用CoT收益有限甚至有害，复杂场景下CoT收益显著。由此提出“按场景复杂度自适应推理”的必要性。\n- 设计AdaThinkDrive框架：三阶段训练范式——预训练获得驾驶常识与知识、两阶段SFT学习双模输出能力、基于GRPO的强化学习通过奖励信号学习“何时思考与何时直接作答”。\n- 提出“Adaptive Think Reward”策略：通过多 rollout 采样，比较同一场景下Thinking与Non-thinking轨迹质量（PDMS），动态奖惩并更新场景复杂度标签，避免对固定人工标签的依赖。\n- 在Navsim达成SOTA：PDMS 90.3（仅视觉输入），比最佳纯视觉基线提升1.7；同时在复杂场景中优先使用思考模式（96%），在简单场景中优先直接输出（84%）；相对Always-Think推理时间降低14%。\n# 论文方法描述\n- 问题建模：以联合分布P(m, o \\|q)建模对查询q选择模式m（Thinking/Non-thinking）与输出轨迹o的决策；通过最大化期望任务效用U(q, o)选择最优模式。\n- 数据准备：\n - 预训练数据：DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive等开源驾驶QA数据，用于获得基础驾驶常识。\n - 混合SFT数据：为同一查询生成Thinking（含完整推理轨迹）与Non-thinking（省略推理仅含轨迹）两种输出；场景注释通过Qwen2.5-VL-72B自动生成，包含交通灯状态、天气与道路边界等；动态交互对象分为CIPO-1（本车道障碍物）、CIPO-2（可能并线）、Motion Interaction（未来轨迹与自车相交）。\n - 场景分类：将Navsim训练与验证集按“是否靠近道路边界/是否有关键物体”划分为Level 1（简单，无条件）、Level 2（有单一条件）、Level 3（复杂，同时满足两个条件），并映射为D+（Level 2&3）与D-（Level 1）作为强化学习初始辅助标签。\n- 两阶段SFT：\n - 阶段一：在大规模驾驶QA数据上微调，提升场景理解与驾驶语义认知能力。\n - 阶段二：在Navsim规划数据（含Thinking/Non-thinking双模输出）上微调，使模型具备统一接口生成两种输出风格且不偏向其一。\n- 自适应思考强化学习（基于GRPO）：\n - 奖励设计：\n - PDMS Reward：基于轨迹质量评价PDMS。\n - Format Reward：强制输出遵循<think>、<answer>标签与轨迹格式规范。\n - Endpoint Reward：对轨迹终点误差进行分段奖励（<2m=1.0；<4m=0.8；<6m=0.6；<10m=0.4；<15m=0.2；否则=0）。\n - Adaptive Think Reward：依据多rollout比较Thinking与Non-thinking的平均PDMS与样本数，在D=0/D=1的当前标签下动态奖惩；当满足阈值T与条件时允许修正场景复杂度标签为“challenging/simple”。\n - 优化目标：采用GRPO优化，引入截断重要性权重与KL正则防止策略偏移。\n# 论文使用数据集和训练资源\n- 数据集：Navsim（主评估）、DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive（用于预训练与知识注入）\n- 模型：InternVL3-8B作为基础视觉-语言模型\n- 训练资源：\n - 阶段一：学习率1e-5，批量1，训练2个epoch\n - 阶段二：学习率4e-5，批量2，训练2个epoch\n - 阶段三（RL）：学习率2e-6，批量4，使用64×NVIDIA H20 GPU进行2个epoch\n# 论文使用的评估环境和评估指标\n- 评估环境：Navsim非反应式仿真，OpenScene平台支持\n- 评估指标：\n - PDMS（Predictive Driver Model Score）：综合评分，融合NC、DAC、TTC、CF、EP\n - 推理时间：预测4秒轨迹的平均推理时间（Navsim测试集）\n - 场景分层：按Level 1/2/3的Think vs Non-think选择比例与性能表现\n- 定量结果（主要指标）：\n - AdaThinkDrive（仅视觉）：PDMS 90.3，NC 98.4、DAC 97.8、TTC 95.2、CF 100、EP 84.4\n - Best-of-N：PDMS 93.0\n - 相对Non-Think RL：PDMS +2.0；推理时间 Non-Think RL 0.68s、Think RL 0.86s、AdaThinkDrive 0.74s\n - 相对Always-Think：推理时间减少14%",
    "summary_html": "<h1>论文研究单位</h1>\n<p>清华大学（Tsinghua University）、小米EV（Xiaomi EV）、澳门大学（University of Macau）、南洋理工大学（Nanyang Technological University）、北京大学（Peking University）</p>\n<h1>论文概述</h1>\n<p>AdaThinkDrive提出一种“快速回答/慢速思考”的双模推理机制，解决在端到端自动驾驶中思维链（CoT）在简单场景下过度推理、复杂场景下推理收益不一致的问题。论文在Navsim基准上进行实验与评估。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>通过在Navsim上系统性对比发现：简单场景下使用CoT收益有限甚至有害，复杂场景下CoT收益显著。由此提出“按场景复杂度自适应推理”的必要性。</li><li>设计AdaThinkDrive框架：三阶段训练范式——预训练获得驾驶常识与知识、两阶段SFT学习双模输出能力、基于GRPO的强化学习通过奖励信号学习“何时思考与何时直接作答”。</li><li>提出“Adaptive Think Reward”策略：通过多 rollout 采样，比较同一场景下Thinking与Non-thinking轨迹质量（PDMS），动态奖惩并更新场景复杂度标签，避免对固定人工标签的依赖。</li><li>在Navsim达成SOTA：PDMS 90.3（仅视觉输入），比最佳纯视觉基线提升1.7；同时在复杂场景中优先使用思考模式（96%），在简单场景中优先直接输出（84%）；相对Always-Think推理时间降低14%。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>问题建模：以联合分布P(m, o \\|q)建模对查询q选择模式m（Thinking/Non-thinking）与输出轨迹o的决策；通过最大化期望任务效用U(q, o)选择最优模式。</li><li>数据准备：</li></ul>\n<p> - 预训练数据：DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive等开源驾驶QA数据，用于获得基础驾驶常识。</p>\n<p> - 混合SFT数据：为同一查询生成Thinking（含完整推理轨迹）与Non-thinking（省略推理仅含轨迹）两种输出；场景注释通过Qwen2.5-VL-72B自动生成，包含交通灯状态、天气与道路边界等；动态交互对象分为CIPO-1（本车道障碍物）、CIPO-2（可能并线）、Motion Interaction（未来轨迹与自车相交）。</p>\n<p> - 场景分类：将Navsim训练与验证集按“是否靠近道路边界/是否有关键物体”划分为Level 1（简单，无条件）、Level 2（有单一条件）、Level 3（复杂，同时满足两个条件），并映射为D+（Level 2&3）与D-（Level 1）作为强化学习初始辅助标签。</p>\n<ul><li>两阶段SFT：</li></ul>\n<p> - 阶段一：在大规模驾驶QA数据上微调，提升场景理解与驾驶语义认知能力。</p>\n<p> - 阶段二：在Navsim规划数据（含Thinking/Non-thinking双模输出）上微调，使模型具备统一接口生成两种输出风格且不偏向其一。</p>\n<ul><li>自适应思考强化学习（基于GRPO）：</li></ul>\n<p> - 奖励设计：</p>\n<p> - PDMS Reward：基于轨迹质量评价PDMS。</p>\n<p> - Format Reward：强制输出遵循<think>、<answer>标签与轨迹格式规范。</p>\n<p> - Endpoint Reward：对轨迹终点误差进行分段奖励（<2m=1.0；<4m=0.8；<6m=0.6；<10m=0.4；<15m=0.2；否则=0）。</p>\n<p> - Adaptive Think Reward：依据多rollout比较Thinking与Non-thinking的平均PDMS与样本数，在D=0/D=1的当前标签下动态奖惩；当满足阈值T与条件时允许修正场景复杂度标签为“challenging/simple”。</p>\n<p> - 优化目标：采用GRPO优化，引入截断重要性权重与KL正则防止策略偏移。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：Navsim（主评估）、DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive（用于预训练与知识注入）</li><li>模型：InternVL3-8B作为基础视觉-语言模型</li><li>训练资源：</li></ul>\n<p> - 阶段一：学习率1e-5，批量1，训练2个epoch</p>\n<p> - 阶段二：学习率4e-5，批量2，训练2个epoch</p>\n<p> - 阶段三（RL）：学习率2e-6，批量4，使用64×NVIDIA H20 GPU进行2个epoch</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：Navsim非反应式仿真，OpenScene平台支持</li><li>评估指标：</li></ul>\n<p> - PDMS（Predictive Driver Model Score）：综合评分，融合NC、DAC、TTC、CF、EP</p>\n<p> - 推理时间：预测4秒轨迹的平均推理时间（Navsim测试集）</p>\n<p> - 场景分层：按Level 1/2/3的Think vs Non-think选择比例与性能表现</p>\n<ul><li>定量结果（主要指标）：</li></ul>\n<p> - AdaThinkDrive（仅视觉）：PDMS 90.3，NC 98.4、DAC 97.8、TTC 95.2、CF 100、EP 84.4</p>\n<p> - Best-of-N：PDMS 93.0</p>\n<p> - 相对Non-Think RL：PDMS +2.0；推理时间 Non-Think RL 0.68s、Think RL 0.86s、AdaThinkDrive 0.74s</p>\n<p> - 相对Always-Think：推理时间减少14%</p>"
  },
  {
    "date": "2025-09-13",
    "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft",
    "link": "http://arxiv.org/abs/2509.13347",
    "summary_markdown": "### 论文研究单位\n北京大学和新加坡国立大学的研究人员。\n### 论文概述\n该论文探讨了在开发具有端到端训练能力的智能体时，动作表示选择的关键问题。研究首先在Minecraft环境中对多种动作抽象方法进行了大规模、系统性的比较，发现没有单一的动作空间是普遍最优的，其有效性高度依赖于具体任务。为解决这一难题，论文提出了“动作链”（Chain of Action, CoA）框架，该框架将高层规划与低层控制统一在单个视觉-语言-动作（VLA）模型中。此外，研究还展示了一个“All-in-One”智能体，通过在混合动作空间数据集上使用CoA范式进行训练，学习到更鲁棒和泛化的策略，并在整体任务成功率上达到了新的最先进水平。为了促进可复现研究，论文发布了OpenHA套件，包括全面的基准测试、精选数据集、源代码和所有预训练模型检查点。\n### 论文核心贡献点\n1. 通过大规模系统性分析，证明了最优的抽象动作空间具有任务依赖性，不同的动作表示在特定任务领域各有优势。\n2. 提出了“动作链”（CoA）框架，通过使用抽象动作作为中间计划，协同了高层推理和低层控制，在性能上优于传统的双系统架构。\n3. 证明了在多样化动作空间的混合数据集上训练单个智能体，可以增强其在不同任务间的泛化能力，提升整体决策水平。\n4. 发布了一套全面的资源，包括各种分层智能体、分层VLA模型和OpenHA模型的训练检查点、代码和数据集，以支持动作表示和泛化方面的进一步研究。\n### 论文方法描述\n论文的核心方法是“动作链”（CoA）框架。该方法将动作生成构建为一个两步的自回归过程：首先，模型生成一个高层抽象动作，作为中间的“思考”或“推理”步骤；然后，以这个“思考”为条件，模型生成最终的底层可执行动作。例如，为了执行“砍树”指令，CoA智能体首先预测一个接地的动作，如`Approach(object=<tree>, coordinate=[136, 287])`，接着是到达该位置并执行动作所需的底层动作序列。这种方法有效地协同了高层推理与低层控制。该框架支持两种推理模式：解耦推理模式（快速）和统一自回归模式（慢速）。此外，论文提出了“All-in-One”训练策略，通过在包含多种动作表示的复合数据集上训练单个智能体，使其能够掌握多样化的动作，从而超越在单一类型动作上训练的专家模型。\n### 论文使用数据集和训练资源\n训练数据源自OpenAI Video Pre-Training (VPT) 数据集，该数据集包含Minecraft中的大量专家轨迹。由于原始VPT数据集仅提供视觉观察和底层环境动作对，研究开发了一个“程序化标注管道”，通过一套基于规则的启发式方法，将原始底层动作序列转换为对应的抽象动作表示（如运动、接地和语言技能动作），生成了三种类型的轨迹数据集：高层动作数据、底层动作数据和动作链数据。所有模型均基于Qwen2-VL-7B基础模型进行初始化，该模型已在Minecraft特定的VQA和字幕数据语料库上进行了后训练。训练使用了TRL和VeOmni库，并使用vLLM库支持高效的智能体推理。\n### 论文使用的评估环境和评估指标\n评估环境为Minecraft（版本1.16.5）。智能体的观察空间仅包含第一人称RGB视觉帧，分辨率为360x640x3。动作空间包括离散化的、类人鼠标和键盘控制。研究构建了一个包含近1000个不同任务的大规模基准，并将其分为三类：体现任务（需要导航和物理交互）、GUI任务（涉及图形用户界面交互）和战斗任务（需要与敌对生物交战）。为确保泛化测试，所有评估环境都与训练数据分布外，通过从Minecraft的程序化生成空间中采样新的初始世界种子和出生位置来实现。\n主要评估指标为：1. 成功率：智能体成功完成给定任务的运行百分比。2. 完成步数：完成任务所需的环境步数。最终分析报告了每个基准类别中所有任务的平均成功率和平均步数。每个智能体在每个任务上至少进行三次独立运行（使用不同的世界种子），并从三个类别中各选取10个不同难度的任务进行了超过10次运行的密集评估。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学和新加坡国立大学的研究人员。</p>\n<h3>论文概述</h3>\n<p>该论文探讨了在开发具有端到端训练能力的智能体时，动作表示选择的关键问题。研究首先在Minecraft环境中对多种动作抽象方法进行了大规模、系统性的比较，发现没有单一的动作空间是普遍最优的，其有效性高度依赖于具体任务。为解决这一难题，论文提出了“动作链”（Chain of Action, CoA）框架，该框架将高层规划与低层控制统一在单个视觉-语言-动作（VLA）模型中。此外，研究还展示了一个“All-in-One”智能体，通过在混合动作空间数据集上使用CoA范式进行训练，学习到更鲁棒和泛化的策略，并在整体任务成功率上达到了新的最先进水平。为了促进可复现研究，论文发布了OpenHA套件，包括全面的基准测试、精选数据集、源代码和所有预训练模型检查点。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>通过大规模系统性分析，证明了最优的抽象动作空间具有任务依赖性，不同的动作表示在特定任务领域各有优势。</li><li>提出了“动作链”（CoA）框架，通过使用抽象动作作为中间计划，协同了高层推理和低层控制，在性能上优于传统的双系统架构。</li><li>证明了在多样化动作空间的混合数据集上训练单个智能体，可以增强其在不同任务间的泛化能力，提升整体决策水平。</li><li>发布了一套全面的资源，包括各种分层智能体、分层VLA模型和OpenHA模型的训练检查点、代码和数据集，以支持动作表示和泛化方面的进一步研究。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文的核心方法是“动作链”（CoA）框架。该方法将动作生成构建为一个两步的自回归过程：首先，模型生成一个高层抽象动作，作为中间的“思考”或“推理”步骤；然后，以这个“思考”为条件，模型生成最终的底层可执行动作。例如，为了执行“砍树”指令，CoA智能体首先预测一个接地的动作，如<code>Approach(object=<tree>, coordinate=[136, 287])</code>，接着是到达该位置并执行动作所需的底层动作序列。这种方法有效地协同了高层推理与低层控制。该框架支持两种推理模式：解耦推理模式（快速）和统一自回归模式（慢速）。此外，论文提出了“All-in-One”训练策略，通过在包含多种动作表示的复合数据集上训练单个智能体，使其能够掌握多样化的动作，从而超越在单一类型动作上训练的专家模型。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>训练数据源自OpenAI Video Pre-Training (VPT) 数据集，该数据集包含Minecraft中的大量专家轨迹。由于原始VPT数据集仅提供视觉观察和底层环境动作对，研究开发了一个“程序化标注管道”，通过一套基于规则的启发式方法，将原始底层动作序列转换为对应的抽象动作表示（如运动、接地和语言技能动作），生成了三种类型的轨迹数据集：高层动作数据、底层动作数据和动作链数据。所有模型均基于Qwen2-VL-7B基础模型进行初始化，该模型已在Minecraft特定的VQA和字幕数据语料库上进行了后训练。训练使用了TRL和VeOmni库，并使用vLLM库支持高效的智能体推理。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境为Minecraft（版本1.16.5）。智能体的观察空间仅包含第一人称RGB视觉帧，分辨率为360x640x3。动作空间包括离散化的、类人鼠标和键盘控制。研究构建了一个包含近1000个不同任务的大规模基准，并将其分为三类：体现任务（需要导航和物理交互）、GUI任务（涉及图形用户界面交互）和战斗任务（需要与敌对生物交战）。为确保泛化测试，所有评估环境都与训练数据分布外，通过从Minecraft的程序化生成空间中采样新的初始世界种子和出生位置来实现。</p>\n<p>主要评估指标为：1. 成功率：智能体成功完成给定任务的运行百分比。2. 完成步数：完成任务所需的环境步数。最终分析报告了每个基准类别中所有任务的平均成功率和平均步数。每个智能体在每个任务上至少进行三次独立运行（使用不同的世界种子），并从三个类别中各选取10个不同难度的任务进行了超过10次运行的密集评估。</p>"
  },
  {
    "date": "2025-09-16",
    "title": "The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning",
    "link": "http://arxiv.org/abs/2509.12594",
    "summary_markdown": "# 论文研究单位\n- LiAuto Inc.（理想汽车股份有限公司）\n- 清华大学车辆与运载学院\n- 中国科学院计算技术研究所\n# 论文概述\n论文针对视觉语言动作（VLA）模型在资源受限平台（如边缘设备）上的部署瓶颈问题，提出了LightVLA框架，通过自适应剪枝视觉token来提高计算效率同时保持或提升性能。VLA模型在机器人任务中表现出色，但受限于大量视觉token的注意力计算成本。LightVLA通过动态生成查询评估token重要性，并使用Gumbel-softmax实现可微token选择，从而在优化过程中自动保留关键token。实验在LIBERO基准上验证其效果。\n# 论文核心贡献点\n- 提出LightVLA，一个性能驱动的可微视觉token剪枝框架，无需额外超参数或可训练参数。\n- 证明VLA模型的效率和性能可协同优化，剪枝同时减少计算开销并提升成功率。\n- 在LIBERO基准上实现最先进性能：成功率提升2.6%，FLOPs减少59.1%，延迟降低38.2%。\n- 提出LightVLA*变体，使用可学习查询（额外参数）探索token剪枝的可能性。\n- 首次将自适应视觉token剪枝应用于VLA任务，突破效率-性能权衡。\n# 论文方法描述\nLightVLA方法包括三个核心步骤：\n1. **查询生成**：通过视觉和语言token的交叉注意力生成动态查询（参数无关），以评估token对任务的重要性。\n2. **Token评分**：计算每个查询对所有视觉token的评分矩阵（S），评分反映token的贡献度。\n3. **Token选择**：使用Gumbel-softmax采样技术使argmax操作可微，将评分矩阵转换为指示矩阵（I），选择高分token组成剪枝集（H'_v）。\n训练中，Gumbel噪声强度随时间衰减，促进多样性选择和稳定性；推理时直接使用argmax。保留CLS token和位置信息以维持全局信息。LightVLA*在视觉编码器或LLM早期层引入可学习查询（Q*）和层归一化参数，适用于更复杂场景。\n# 论文使用数据集和训练资源\n- **数据集**：LIBERO基准测试，包含四个任务集（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个任务集提供500个专家演示（总计2000演示）。测试每任务集50次试验（500试验）。\n- **训练资源**：\n - 基础模型：OpenVLA-OFT（双分支视觉编码器DINOv2和SigLIP、LLaMA-2-7B LLM、LoRA rank 32微调）。\n - 微调设置：40000梯度步，初始学习率5e-4（30000步后降至5e-5），批大小每设备8（全局64）。\n - 硬件：8个Nvidia H20 GPUs。\n# 论文使用的评估环境和评估指标\n- **评估环境**：LIBERO仿真环境，使用Franka Emika Panda机械臂，集成相机图像、机器人状态和任务指令。\n- **评估指标**：\n - 主要指标：任务成功率（SR, %），分任务集报告。\n - 效率指标：FLOPs（总计算量）、端到端延迟（毫秒）、平均保留视觉token数。\n - 对比基线：包括其他VLA模型（如OpenVLA、π_0）和token剪枝方法（如FastV、SparseVLM）。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>LiAuto Inc.（理想汽车股份有限公司）</li><li>清华大学车辆与运载学院</li><li>中国科学院计算技术研究所</li></ul>\n<h1>论文概述</h1>\n<p>论文针对视觉语言动作（VLA）模型在资源受限平台（如边缘设备）上的部署瓶颈问题，提出了LightVLA框架，通过自适应剪枝视觉token来提高计算效率同时保持或提升性能。VLA模型在机器人任务中表现出色，但受限于大量视觉token的注意力计算成本。LightVLA通过动态生成查询评估token重要性，并使用Gumbel-softmax实现可微token选择，从而在优化过程中自动保留关键token。实验在LIBERO基准上验证其效果。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出LightVLA，一个性能驱动的可微视觉token剪枝框架，无需额外超参数或可训练参数。</li><li>证明VLA模型的效率和性能可协同优化，剪枝同时减少计算开销并提升成功率。</li><li>在LIBERO基准上实现最先进性能：成功率提升2.6%，FLOPs减少59.1%，延迟降低38.2%。</li><li>提出LightVLA*变体，使用可学习查询（额外参数）探索token剪枝的可能性。</li><li>首次将自适应视觉token剪枝应用于VLA任务，突破效率-性能权衡。</li></ul>\n<h1>论文方法描述</h1>\n<p>LightVLA方法包括三个核心步骤：</p>\n<ol><li><strong>查询生成</strong>：通过视觉和语言token的交叉注意力生成动态查询（参数无关），以评估token对任务的重要性。</li><li><strong>Token评分</strong>：计算每个查询对所有视觉token的评分矩阵（S），评分反映token的贡献度。</li><li><strong>Token选择</strong>：使用Gumbel-softmax采样技术使argmax操作可微，将评分矩阵转换为指示矩阵（I），选择高分token组成剪枝集（H'_v）。</li></ol>\n<p>训练中，Gumbel噪声强度随时间衰减，促进多样性选择和稳定性；推理时直接使用argmax。保留CLS token和位置信息以维持全局信息。LightVLA*在视觉编码器或LLM早期层引入可学习查询（Q*）和层归一化参数，适用于更复杂场景。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>数据集</strong>：LIBERO基准测试，包含四个任务集（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个任务集提供500个专家演示（总计2000演示）。测试每任务集50次试验（500试验）。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 基础模型：OpenVLA-OFT（双分支视觉编码器DINOv2和SigLIP、LLaMA-2-7B LLM、LoRA rank 32微调）。</p>\n<p> - 微调设置：40000梯度步，初始学习率5e-4（30000步后降至5e-5），批大小每设备8（全局64）。</p>\n<p> - 硬件：8个Nvidia H20 GPUs。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>评估环境</strong>：LIBERO仿真环境，使用Franka Emika Panda机械臂，集成相机图像、机器人状态和任务指令。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：任务成功率（SR, %），分任务集报告。</p>\n<p> - 效率指标：FLOPs（总计算量）、端到端延迟（毫秒）、平均保留视觉token数。</p>\n<p> - 对比基线：包括其他VLA模型（如OpenVLA、π_0）和token剪枝方法（如FastV、SparseVLM）。</p>"
  },
  {
    "date": "2025-09-15",
    "title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning",
    "link": "http://arxiv.org/abs/2509.11839",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n浙江大学、西湖大学、上海交通大学、上海创新研究院\n## 论文概述\n当前视觉-语言-动作（VLA）模型在跨实体泛化方面显示出潜力，但在高质量演示稀缺时，特别是对于双足人形机器人，难以快速对齐新的机器人动作空间。本文提出了TrajBooster，一个跨实体框架，利用丰富的轮式人形机器人数据来提升双足VLA性能。\n## 论文核心贡献点\n1. 首次利用重定向动作数据微调VLA模型并实现双足人形机器人全身操作的真实世界应用\n2. 提出了TrajBooster真实到仿真到真实的跨实体框架，使用末端执行器轨迹作为形态不可知的信号\n3. 在Unitree G1上仅需10分钟遥操作数据收集，就能实现超越桌面的家庭任务，包括蹲下、跨高度操作和协调的全身运动\n## 论文方法描述\n### 1. 真实轨迹提取\n从Agibot-World beta数据集中提取6D双臂末端执行器轨迹，并将其从Agibot数据映射到Unitree官方G1操作数据集，解决工作空间差异问题。\n### 2. 仿真中的重定向\n设计复合层次化模型用于全身操作重定向，包括：\n- **手臂策略**：使用闭环逆运动学（CLIK）计算目标关节角度\n- **工作者策略**：基于目标的条件强化学习策略，输出12-DoF下肢目标关节位置\n- **管理者策略**：从手腕姿态生成下肢命令（基座速度命令和躯干高度）\n\n采用启发式增强的调和在线DAgger算法进行训练。\n### 3. 双步后训练\n- **后预训练**：使用重定向的动作-视觉-语言三元组数据对预训练GR00T N1.5模型进行后预训练\n- **后训练**：使用收集的遥操作数据进行微调\n## 论文使用数据集和训练资源\n- **源数据**：Agibot-World beta数据集，包含超过一百万条真实机器人轨迹\n- **目标数据**：28个真实世界全身操作数据集片段，涵盖四种不同高度配置\n- **训练资源**：\n - 重定向模型训练：512个并行环境，200次训练迭代，单个RTX 4090 GPU\n - 后预训练：双A100 80GB GPU，批次大小=128，60K步数\n - 后训练：单个A100 GPU，批次大小=16，3K步数\n## 论文使用的评估环境和评估指标\n### 评估环境\n- **仿真评估**：在Isaac Gym中评估重定向模型性能\n- **真实世界评估**：在Unitree G1人形机器人上进行四个遥操作任务评估\n### 评估指标\n- **轨迹跟踪精度**：位置误差E_p（厘米）和旋转误差E_r（度）\n- **任务成功率**：在十个试验中完成任务的百分比\n- **轨迹泛化能力**：使用FastDTW算法计算生成轨迹与真实遥操作数据的相似性\n- **零样本技能泛化**：评估未见任务的执行能力",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>浙江大学、西湖大学、上海交通大学、上海创新研究院</p>\n<h2>论文概述</h2>\n<p>当前视觉-语言-动作（VLA）模型在跨实体泛化方面显示出潜力，但在高质量演示稀缺时，特别是对于双足人形机器人，难以快速对齐新的机器人动作空间。本文提出了TrajBooster，一个跨实体框架，利用丰富的轮式人形机器人数据来提升双足VLA性能。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>首次利用重定向动作数据微调VLA模型并实现双足人形机器人全身操作的真实世界应用</li><li>提出了TrajBooster真实到仿真到真实的跨实体框架，使用末端执行器轨迹作为形态不可知的信号</li><li>在Unitree G1上仅需10分钟遥操作数据收集，就能实现超越桌面的家庭任务，包括蹲下、跨高度操作和协调的全身运动</li></ol>\n<h2>论文方法描述</h2>\n<h3>1. 真实轨迹提取</h3>\n<p>从Agibot-World beta数据集中提取6D双臂末端执行器轨迹，并将其从Agibot数据映射到Unitree官方G1操作数据集，解决工作空间差异问题。</p>\n<h3>2. 仿真中的重定向</h3>\n<p>设计复合层次化模型用于全身操作重定向，包括：</p>\n<ul><li><strong>手臂策略</strong>：使用闭环逆运动学（CLIK）计算目标关节角度</li><li><strong>工作者策略</strong>：基于目标的条件强化学习策略，输出12-DoF下肢目标关节位置</li><li><strong>管理者策略</strong>：从手腕姿态生成下肢命令（基座速度命令和躯干高度）</li></ul>\n\n<p>采用启发式增强的调和在线DAgger算法进行训练。</p>\n<h3>3. 双步后训练</h3>\n<ul><li><strong>后预训练</strong>：使用重定向的动作-视觉-语言三元组数据对预训练GR00T N1.5模型进行后预训练</li><li><strong>后训练</strong>：使用收集的遥操作数据进行微调</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>源数据</strong>：Agibot-World beta数据集，包含超过一百万条真实机器人轨迹</li><li><strong>目标数据</strong>：28个真实世界全身操作数据集片段，涵盖四种不同高度配置</li><li><strong>训练资源</strong>：</li></ul>\n<p> - 重定向模型训练：512个并行环境，200次训练迭代，单个RTX 4090 GPU</p>\n<p> - 后预训练：双A100 80GB GPU，批次大小=128，60K步数</p>\n<p> - 后训练：单个A100 GPU，批次大小=16，3K步数</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<h3>评估环境</h3>\n<ul><li><strong>仿真评估</strong>：在Isaac Gym中评估重定向模型性能</li><li><strong>真实世界评估</strong>：在Unitree G1人形机器人上进行四个遥操作任务评估</li></ul>\n<h3>评估指标</h3>\n<ul><li><strong>轨迹跟踪精度</strong>：位置误差E_p（厘米）和旋转误差E_r（度）</li><li><strong>任务成功率</strong>：在十个试验中完成任务的百分比</li><li><strong>轨迹泛化能力</strong>：使用FastDTW算法计算生成轨迹与真实遥操作数据的相似性</li><li><strong>零样本技能泛化</strong>：评估未见任务的执行能力</li></ul>"
  },
  {
    "date": "2025-09-15",
    "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
    "link": "http://arxiv.org/abs/2509.11480",
    "summary_markdown": "### 论文研究单位\n* 东北大学电气与计算机工程系\n* EmbodyX Inc.\n### 论文概述\n该论文系统地评估了五种视觉-语言-动作模型在不同硬件平台（从边缘设备到数据中心GPU）以及不同功率预算下的性能缩放情况。通过在LIBERO基准上测试，作者测量了模型的准确率、延迟、吞吐量和峰值内存使用等指标，旨在揭示模型架构、硬件类别和功耗限制之间的相互作用，并为不同部署场景下的模型选择和优化提供实践指导。\n### 论文核心贡献点\n* 对VLA模型在边缘和数据中心GPU上的跨平台性能进行了首次系统性评估。\n* 识别了关键的缩放趋势：架构选择（如动作分块大小、模型骨干尺寸）显著影响吞吐量和内存占用；功率受限的边缘设备表现出非线性的性能退化；高吞吐量模型可以在不显著牺牲准确率的情况下实现。\n* 提出了一个挑战性观点：现代高端边缘设备（如Jetson AGX Orin）在特定配置下的性能可以超越旧款的数据中心GPU（如V100）。\n* 提供了可操作的见解，用于在不同部署约束下选择和优化VLA模型。\n### 论文方法描述\n论文评估了五个VLA模型，包括三个基线模型和两个新提出的架构。这些模型在语言骨干、视觉编码器和动作头设计上有所不同。\n* **评估模型**：OpenVLA (7B), SpatialVLA (4B), OpenVLA-OFT (7B), VOTE (7B, 包含1T, 2T, MLP4三种配置) 和 QwenVLA (2.6B)。\n* **新架构**：VOTE模型通过使用特殊动作令牌（ST）来减少输出令牌数量，从而降低延迟。QwenVLA模型采用更小的语言骨干，以探索模型尺寸与效率的权衡。\n* **实验方法**：在所有硬件平台上，对每个模型运行基准测试，测量生成一个动作块的平均延迟和每秒生成的动作数。在测量前进行预热，并记录100次连续推理的时间以确保稳定性。\n### 论文使用数据集和训练资源\n* **数据集**：LIBERO基准，该基准包含四个任务套件，分别测试模型在空间关系、物体类型、任务导向行为和长时任务上的泛化能力。\n* **训练资源**：新提出的VOTE和QwenVLA模型在LIBERO基准上进行微调。优化器为AdamW，VOTE和QwenVLA的学习率分别为1e-4和1e-3。使用了低秩自适应进行参数高效微调，LoRA的秩为32，alpha为16。VOTE和QwenVLA的全局批处理大小分别为40和64。\n### 论文使用的评估环境和评估指标\n* **评估环境**：\n * **边缘平台**：NVIDIA Jetson AGX Orin，在四种功率模式下运行（15W, 30W, 50W, MAX），以模拟不同的功耗约束。\n * **数据中心平台**：四款NVIDIA数据中心GPU，包括H100 (Hopper架构)、A100 (Ampere架构)、A6000 (Ampere架构) 和 V100 (Volta架构)。\n* **评估指标**：\n * **准确率**：在LIBERO基准的四个任务套件上的任务成功率（Success Rate, SR）。\n * **延迟**：生成一个动作块所需的平均时间（毫秒）。\n * **吞吐量**：每秒生成的动作数量。\n * **峰值内存使用**：模型推理时占用的最大显存（GB）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>* 东北大学电气与计算机工程系</p>\n<p>* EmbodyX Inc.</p>\n<h3>论文概述</h3>\n<p>该论文系统地评估了五种视觉-语言-动作模型在不同硬件平台（从边缘设备到数据中心GPU）以及不同功率预算下的性能缩放情况。通过在LIBERO基准上测试，作者测量了模型的准确率、延迟、吞吐量和峰值内存使用等指标，旨在揭示模型架构、硬件类别和功耗限制之间的相互作用，并为不同部署场景下的模型选择和优化提供实践指导。</p>\n<h3>论文核心贡献点</h3>\n<p>* 对VLA模型在边缘和数据中心GPU上的跨平台性能进行了首次系统性评估。</p>\n<p>* 识别了关键的缩放趋势：架构选择（如动作分块大小、模型骨干尺寸）显著影响吞吐量和内存占用；功率受限的边缘设备表现出非线性的性能退化；高吞吐量模型可以在不显著牺牲准确率的情况下实现。</p>\n<p>* 提出了一个挑战性观点：现代高端边缘设备（如Jetson AGX Orin）在特定配置下的性能可以超越旧款的数据中心GPU（如V100）。</p>\n<p>* 提供了可操作的见解，用于在不同部署约束下选择和优化VLA模型。</p>\n<h3>论文方法描述</h3>\n<p>论文评估了五个VLA模型，包括三个基线模型和两个新提出的架构。这些模型在语言骨干、视觉编码器和动作头设计上有所不同。</p>\n<p>* <strong>评估模型</strong>：OpenVLA (7B), SpatialVLA (4B), OpenVLA-OFT (7B), VOTE (7B, 包含1T, 2T, MLP4三种配置) 和 QwenVLA (2.6B)。</p>\n<p>* <strong>新架构</strong>：VOTE模型通过使用特殊动作令牌（ST）来减少输出令牌数量，从而降低延迟。QwenVLA模型采用更小的语言骨干，以探索模型尺寸与效率的权衡。</p>\n<p>* <strong>实验方法</strong>：在所有硬件平台上，对每个模型运行基准测试，测量生成一个动作块的平均延迟和每秒生成的动作数。在测量前进行预热，并记录100次连续推理的时间以确保稳定性。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>* <strong>数据集</strong>：LIBERO基准，该基准包含四个任务套件，分别测试模型在空间关系、物体类型、任务导向行为和长时任务上的泛化能力。</p>\n<p>* <strong>训练资源</strong>：新提出的VOTE和QwenVLA模型在LIBERO基准上进行微调。优化器为AdamW，VOTE和QwenVLA的学习率分别为1e-4和1e-3。使用了低秩自适应进行参数高效微调，LoRA的秩为32，alpha为16。VOTE和QwenVLA的全局批处理大小分别为40和64。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>* <strong>评估环境</strong>：</p>\n<p> * <strong>边缘平台</strong>：NVIDIA Jetson AGX Orin，在四种功率模式下运行（15W, 30W, 50W, MAX），以模拟不同的功耗约束。</p>\n<p> * <strong>数据中心平台</strong>：四款NVIDIA数据中心GPU，包括H100 (Hopper架构)、A100 (Ampere架构)、A6000 (Ampere架构) 和 V100 (Volta架构)。</p>\n<p>* <strong>评估指标</strong>：</p>\n<p> * <strong>准确率</strong>：在LIBERO基准的四个任务套件上的任务成功率（Success Rate, SR）。</p>\n<p> * <strong>延迟</strong>：生成一个动作块所需的平均时间（毫秒）。</p>\n<p> * <strong>吞吐量</strong>：每秒生成的动作数量。</p>\n<p> * <strong>峰值内存使用</strong>：模型推理时占用的最大显存（GB）。</p>"
  },
  {
    "date": "2025-09-14",
    "title": "Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations",
    "link": "http://arxiv.org/abs/2509.11417",
    "summary_markdown": "# 论文研究单位\n- UC San Diego（加州大学圣地亚哥分校）\n- Hillbot\n# 论文概述\nVLA模型往往通过直接微调预训练的VLM获取机器人控制能力，但会导致预训练表征严重退化，在视觉背景变化与指令同义改写下性能显著下降。本文提出保留预训练表征的VLA训练框架，在仿真与真实机器人上均显著提升稳健性与泛化。\n# 论文核心贡献点\n- 部分冻结的双视觉编码器：冻结一个编码器作为“锚点”保留预训练表征，另一个可训练以适配机器人任务，拼接后输入后续模块。\n- 字符串动作分词器：将连续动作转换为字符序列，使用与语言一致的词表和自回归目标，使动作预测与VLM预训练目标对齐，支持细粒度逐步生成。\n- 特征正则化与联合训练：利用统一的字符串输出空间，在机器人数据与强调空间推理/可供性的视觉-语言数据上进行等量（50%/批次）联合训练，防止灾难性遗忘并增强泛化。\n- 架构与配方通用性：可无缝适配OpenVLA与π0等多种VLA体系。\n# 论文方法描述\n- 部分冻结双编码器架构：使用两个视觉编码器（一个冻结、一个训练），对同一观测生成两组特征并拼接 z_t = [φ_frozen(o_t) \\|\\|φ_train(o_t)]，再与语言指令 c 一起送入动作分词器 ψ 生成动作。\n- 字符串动作分词器：将动作各维度（如Δx=0.0312）序列化为字符序列（如“0 . 0 3 1 2”），利用语言词表与自回归生成目标进行预测，使动作空间与语言空间对齐，支持在语言模型内统一优化机器人与视觉-语言任务。\n- 联合训练策略：每批训练从机器人数据与视觉-语言数据各取50%，数据涵盖空间推理、视觉问答、目标定位等，确保与任务相关且一致的梯度来源。\n# 论文使用数据集和训练资源\n- 数据集\n - 机器人：Open X-Embodiment (OXE)\n - 视觉-语言：LLaVA Visual Instruct CC3M、VQASynth-Spatial、LLaVA OneVision、RoboPoint\n- 仿真评估：SimplerEnv（Visual Matching与Visual Variant Aggregation），包含背景、光照、桌面纹理、干扰物与视角扰动\n- 训练与评估硬件：双GTX 1080 Ti；真实机器人使用Logitech C920 Webcam作为视觉输入\n# 论文使用的评估环境和评估指标\n- 仿真：SimplerEnv的Visual Matching与Visual Variant Aggregation\n- 视觉稳健性：背景遮挡与视觉多样性扰动\n- 语言稳健性：同义指令改写（GPT-4生成），如“grasp the can”→“get the can”\n- 泛化与推理：在Text-VQA、POPE、GQA、VizWiz、VSR等VLM基准上评估保留的推理能力\n- 指标\n - 任务成功率（%）\n - VQA准确率（%）\n - CIFAR-10线性探测准确率（%）与t-SNE可视化\n - 真实机器人每任务25次试验的成功次数（表V）",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>UC San Diego（加州大学圣地亚哥分校）</li><li>Hillbot</li></ul>\n<h1>论文概述</h1>\n<p>VLA模型往往通过直接微调预训练的VLM获取机器人控制能力，但会导致预训练表征严重退化，在视觉背景变化与指令同义改写下性能显著下降。本文提出保留预训练表征的VLA训练框架，在仿真与真实机器人上均显著提升稳健性与泛化。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>部分冻结的双视觉编码器：冻结一个编码器作为“锚点”保留预训练表征，另一个可训练以适配机器人任务，拼接后输入后续模块。</li><li>字符串动作分词器：将连续动作转换为字符序列，使用与语言一致的词表和自回归目标，使动作预测与VLM预训练目标对齐，支持细粒度逐步生成。</li><li>特征正则化与联合训练：利用统一的字符串输出空间，在机器人数据与强调空间推理/可供性的视觉-语言数据上进行等量（50%/批次）联合训练，防止灾难性遗忘并增强泛化。</li><li>架构与配方通用性：可无缝适配OpenVLA与π0等多种VLA体系。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>部分冻结双编码器架构：使用两个视觉编码器（一个冻结、一个训练），对同一观测生成两组特征并拼接 z_t = [φ_frozen(o_t) \\|\\|φ_train(o_t)]，再与语言指令 c 一起送入动作分词器 ψ 生成动作。</li><li>字符串动作分词器：将动作各维度（如Δx=0.0312）序列化为字符序列（如“0 . 0 3 1 2”），利用语言词表与自回归生成目标进行预测，使动作空间与语言空间对齐，支持在语言模型内统一优化机器人与视觉-语言任务。</li><li>联合训练策略：每批训练从机器人数据与视觉-语言数据各取50%，数据涵盖空间推理、视觉问答、目标定位等，确保与任务相关且一致的梯度来源。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集</li></ul>\n<p> - 机器人：Open X-Embodiment (OXE)</p>\n<p> - 视觉-语言：LLaVA Visual Instruct CC3M、VQASynth-Spatial、LLaVA OneVision、RoboPoint</p>\n<ul><li>仿真评估：SimplerEnv（Visual Matching与Visual Variant Aggregation），包含背景、光照、桌面纹理、干扰物与视角扰动</li><li>训练与评估硬件：双GTX 1080 Ti；真实机器人使用Logitech C920 Webcam作为视觉输入</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>仿真：SimplerEnv的Visual Matching与Visual Variant Aggregation</li><li>视觉稳健性：背景遮挡与视觉多样性扰动</li><li>语言稳健性：同义指令改写（GPT-4生成），如“grasp the can”→“get the can”</li><li>泛化与推理：在Text-VQA、POPE、GQA、VizWiz、VSR等VLM基准上评估保留的推理能力</li><li>指标</li></ul>\n<p> - 任务成功率（%）</p>\n<p> - VQA准确率（%）</p>\n<p> - CIFAR-10线性探测准确率（%）与t-SNE可视化</p>\n<p> - 真实机器人每任务25次试验的成功次数（表V）</p>"
  },
  {
    "date": "2025-09-11",
    "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "link": "http://arxiv.org/abs/2509.09674",
    "summary_markdown": "### 论文研究单位\n上海交通大学, 北京大学, 香港大学, 上海AI实验室\n### 论文概述\n论文针对视觉-语言-动作（VLA）模型在监督微调（SFT）阶段面临的两个核心挑战：大规模人类操作轨迹数据的稀缺与高昂成本，以及在分布偏移任务上的泛化能力有限。受大型推理模型通过强化学习（RL）取得成功的启发，论文提出了SimpleVLA-RL，一个专为VLA模型设计的高效在线强化学习框架。该方法基于veRL框架，实现了交互式轨迹采样、可扩展的并行化和优化的损失计算。通过采用简单的二元结果奖励和一系列探索增强策略，该方法在LIBERO和RoboTwin等基准上取得了最先进的性能，显著提升了数据效率和泛化能力，并能有效迁移至真实世界任务。此外，研究还发现了一种名为“pushcut”的新颖行为模式，即策略在RL训练中学会了训练数据之外的新策略。\n### 论文核心贡献点\n- 提出了一个高效、端到端的VLA在线RL框架，该框架基于veRL，为VLA实现了稳定的、样本高效的训练，并优化了渲染并行化和分布式训练。\n- 通过引入探索增强策略，实现了10-15%的一致性性能提升。SimpleVLA-RL在LIBERO和RoboTwin 1.0 & 2.0上均超越了多个SOTA基线模型。\n- 展现了卓越的数据效率和泛化能力。仅使用每个任务一个演示，RL就将LIBERO-Long任务的成功率从17.1%提升至91.7%，并在空间、物体和任务泛化上显著优于SFT。\n- 具备真实世界部署能力。在模拟器上训练的策略可以有效迁移到真实世界，在无需任何真实机器人数据的情况下实现了强大的模拟到现实的性能增益。\n- 发现了“pushcut”现象，即策略在RL训练期间发现了先前在监督训练数据中未见过的全新行为模式。\n### 论文方法描述\nSimpleVLA-RL是一个基于veRL框架的在线RL训练方法，专为VLA模型设计。其核心方法包括：\n1. **交互式VLA轨迹生成**：与LLM的纯文本生成不同，VLA需要与模拟环境进行多轮交互，根据实时感官反馈动态更新状态和动作。模型通过在动作token上进行随机采样来生成多样化的轨迹。\n2. **结果奖励建模**：采用简单且可扩展的二元奖励函数。一条轨迹如果任务成功则获得奖励1，否则为0。该奖励会均匀分配给轨迹中的所有动作token。\n3. **探索增强策略**：为提升RL的探索效率，引入了三种策略：\n - **动态采样**：在rollout时，只保留包含混合成功和失败轨迹的批次，确保优势估计非零，避免梯度消失。\n - **调整裁剪范围**：将GRPO目标中的重要性采样比率裁剪范围从[0.8, 1.2]调整为[0.8, 1.28]，以鼓励对低概率动作的探索。\n - **提高采样温度**：将rollout阶段的采样温度从1.0提高到1.6，以生成更多样的探索轨迹。\n4. **训练目标**：采用改进的Group Relative Policy Optimization (GRPO)算法。关键修改是移除了KL散度正则化项，消除了对参考模型的需求，降低了内存消耗，并鼓励策略探索新行为。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **LIBERO**：一个包含五个任务套件的语言引导操作终身学习基准，用于评估模型的长期规划能力。\n - **RoboTwin 1.0**：一个包含17个双臂任务的模拟基准。\n - **RoboTwin 2.0**：RoboTwin 1.0的扩展版，包含50个任务、多种机器人形态、731个物体实例和全面的域随机化，任务按步长分为短、中、长、超长四种水平。\n- **训练资源**：使用8块NVIDIA A800 80GB GPU进行全参数训练。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：LIBERO、RoboTwin 1.0和RoboTwin 2.0的模拟器环境。\n - **真实世界环境**：使用Agilex Piper机械臂进行真实世界实验，以验证模拟到现实的迁移能力。\n- **评估指标**：\n - **成功率**：主要评估指标，指任务成功完成的百分比。对于LIBERO，是每个任务在50个保留测试场景上的平均成功率；对于RoboTwin，是每个任务在100个保留测试场景上的平均成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学, 北京大学, 香港大学, 上海AI实验室</p>\n<h3>论文概述</h3>\n<p>论文针对视觉-语言-动作（VLA）模型在监督微调（SFT）阶段面临的两个核心挑战：大规模人类操作轨迹数据的稀缺与高昂成本，以及在分布偏移任务上的泛化能力有限。受大型推理模型通过强化学习（RL）取得成功的启发，论文提出了SimpleVLA-RL，一个专为VLA模型设计的高效在线强化学习框架。该方法基于veRL框架，实现了交互式轨迹采样、可扩展的并行化和优化的损失计算。通过采用简单的二元结果奖励和一系列探索增强策略，该方法在LIBERO和RoboTwin等基准上取得了最先进的性能，显著提升了数据效率和泛化能力，并能有效迁移至真实世界任务。此外，研究还发现了一种名为“pushcut”的新颖行为模式，即策略在RL训练中学会了训练数据之外的新策略。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个高效、端到端的VLA在线RL框架，该框架基于veRL，为VLA实现了稳定的、样本高效的训练，并优化了渲染并行化和分布式训练。</li><li>通过引入探索增强策略，实现了10-15%的一致性性能提升。SimpleVLA-RL在LIBERO和RoboTwin 1.0 & 2.0上均超越了多个SOTA基线模型。</li><li>展现了卓越的数据效率和泛化能力。仅使用每个任务一个演示，RL就将LIBERO-Long任务的成功率从17.1%提升至91.7%，并在空间、物体和任务泛化上显著优于SFT。</li><li>具备真实世界部署能力。在模拟器上训练的策略可以有效迁移到真实世界，在无需任何真实机器人数据的情况下实现了强大的模拟到现实的性能增益。</li><li>发现了“pushcut”现象，即策略在RL训练期间发现了先前在监督训练数据中未见过的全新行为模式。</li></ul>\n<h3>论文方法描述</h3>\n<p>SimpleVLA-RL是一个基于veRL框架的在线RL训练方法，专为VLA模型设计。其核心方法包括：</p>\n<ol><li><strong>交互式VLA轨迹生成</strong>：与LLM的纯文本生成不同，VLA需要与模拟环境进行多轮交互，根据实时感官反馈动态更新状态和动作。模型通过在动作token上进行随机采样来生成多样化的轨迹。</li><li><strong>结果奖励建模</strong>：采用简单且可扩展的二元奖励函数。一条轨迹如果任务成功则获得奖励1，否则为0。该奖励会均匀分配给轨迹中的所有动作token。</li><li><strong>探索增强策略</strong>：为提升RL的探索效率，引入了三种策略：</li></ol>\n<p> - <strong>动态采样</strong>：在rollout时，只保留包含混合成功和失败轨迹的批次，确保优势估计非零，避免梯度消失。</p>\n<p> - <strong>调整裁剪范围</strong>：将GRPO目标中的重要性采样比率裁剪范围从[0.8, 1.2]调整为[0.8, 1.28]，以鼓励对低概率动作的探索。</p>\n<p> - <strong>提高采样温度</strong>：将rollout阶段的采样温度从1.0提高到1.6，以生成更多样的探索轨迹。</p>\n<p>4. <strong>训练目标</strong>：采用改进的Group Relative Policy Optimization (GRPO)算法。关键修改是移除了KL散度正则化项，消除了对参考模型的需求，降低了内存消耗，并鼓励策略探索新行为。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>LIBERO</strong>：一个包含五个任务套件的语言引导操作终身学习基准，用于评估模型的长期规划能力。</p>\n<p> - <strong>RoboTwin 1.0</strong>：一个包含17个双臂任务的模拟基准。</p>\n<p> - <strong>RoboTwin 2.0</strong>：RoboTwin 1.0的扩展版，包含50个任务、多种机器人形态、731个物体实例和全面的域随机化，任务按步长分为短、中、长、超长四种水平。</p>\n<ul><li><strong>训练资源</strong>：使用8块NVIDIA A800 80GB GPU进行全参数训练。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：LIBERO、RoboTwin 1.0和RoboTwin 2.0的模拟器环境。</p>\n<p> - <strong>真实世界环境</strong>：使用Agilex Piper机械臂进行真实世界实验，以验证模拟到现实的迁移能力。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：主要评估指标，指任务成功完成的百分比。对于LIBERO，是每个任务在50个保留测试场景上的平均成功率；对于RoboTwin，是每个任务在100个保留测试场景上的平均成功率。</p>"
  },
  {
    "date": "2025-09-11",
    "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2509.09372",
    "summary_markdown": "论文研究单位\n北京邮电大学、西湖大学、浙江大学、OpenHelix团队、网络与交换技术国家重点实验室、香港科技大学（广州）。\n\n论文概述\n该论文提出了VLA-Adapter，一种新型的视觉-语言-动作（VLA）模型范式，旨在通过一个轻量级的策略模块和桥接注意力机制，有效桥接视觉-语言（VL）表示与动作（A）空间。该方法减少了对大规模视觉-语言模型（VLM）和大规模预训练的依赖，使用仅0.5B参数的骨干网络，无需机器人数据预训练，即可在模拟和真实世界机器人任务中实现最先进的性能和快速的推理速度。\n\n论文核心贡献点\n1. 首次系统分析了不同VL条件对动作生成的影响，并提出了关于VLA模型设计的关键发现。\n2. 提出的VLA-Adapter通过将多模态信息有效转移到策略网络进行动作生成，成功桥接了VL到A的模态差距。\n3. 在多种模拟和真实世界机器人任务中，VLA-Adapter展示了更高的成功率、更小的模型规模、更低的调优成本和更快的推理速度。\n\n论文方法描述\nVLA-Adapter方法包括：\n1. 骨干网络：采用Qwen2.5-0.5B作为默认VLM骨干网络，支持不同规模骨干网络。\n2. 条件探索：系统分析了VLM中间层原始特征（Raw Features）和动作查询（ActionQuery）作为桥接条件的效果，发现中间层原始特征和深层动作查询特征更有效，且多层特征优于单层。\n3. 策略网络：设计了L1-based策略网络，包含桥接注意力模块。该模块通过两个交叉注意力（分别与原始特征和动作查询特征交互）和一个自注意力，将VL条件注入动作空间。引入可学习的比率参数（Ratio）调制原始特征的影响。\n4. 训练：端到端训练策略网络，使用L1损失函数优化动作序列预测。\n\n论文使用数据集和训练资源\n数据集：LIBERO（Spatial, Object, Goal, Long）、CALVIN（ABC→D零样本泛化任务）和真实世界数据。\n训练资源：在4块NVIDIA H100 GPU上进行训练；使用单个消费级GPU可在8小时内完成训练。\n\n论文使用的评估环境和评估指标\n评估环境：模拟环境（LIBERO和CALVIN）和真实世界机器人平台。\n评估指标：\n1. 成功率（Success Rate）：任务成功的百分比，越高越好。\n2. 平均长度（Avg. len）：CALVIN中完成任务的平均子任务数，越高越好。\n3. 推理效率：吞吐量（Hz，越高越好）和延迟（秒，越低越好）。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京邮电大学、西湖大学、浙江大学、OpenHelix团队、网络与交换技术国家重点实验室、香港科技大学（广州）。</p>\n\n<p>论文概述</p>\n<p>该论文提出了VLA-Adapter，一种新型的视觉-语言-动作（VLA）模型范式，旨在通过一个轻量级的策略模块和桥接注意力机制，有效桥接视觉-语言（VL）表示与动作（A）空间。该方法减少了对大规模视觉-语言模型（VLM）和大规模预训练的依赖，使用仅0.5B参数的骨干网络，无需机器人数据预训练，即可在模拟和真实世界机器人任务中实现最先进的性能和快速的推理速度。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>首次系统分析了不同VL条件对动作生成的影响，并提出了关于VLA模型设计的关键发现。</li><li>提出的VLA-Adapter通过将多模态信息有效转移到策略网络进行动作生成，成功桥接了VL到A的模态差距。</li><li>在多种模拟和真实世界机器人任务中，VLA-Adapter展示了更高的成功率、更小的模型规模、更低的调优成本和更快的推理速度。</li></ol>\n\n<p>论文方法描述</p>\n<p>VLA-Adapter方法包括：</p>\n<ol><li>骨干网络：采用Qwen2.5-0.5B作为默认VLM骨干网络，支持不同规模骨干网络。</li><li>条件探索：系统分析了VLM中间层原始特征（Raw Features）和动作查询（ActionQuery）作为桥接条件的效果，发现中间层原始特征和深层动作查询特征更有效，且多层特征优于单层。</li><li>策略网络：设计了L1-based策略网络，包含桥接注意力模块。该模块通过两个交叉注意力（分别与原始特征和动作查询特征交互）和一个自注意力，将VL条件注入动作空间。引入可学习的比率参数（Ratio）调制原始特征的影响。</li><li>训练：端到端训练策略网络，使用L1损失函数优化动作序列预测。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：LIBERO（Spatial, Object, Goal, Long）、CALVIN（ABC→D零样本泛化任务）和真实世界数据。</p>\n<p>训练资源：在4块NVIDIA H100 GPU上进行训练；使用单个消费级GPU可在8小时内完成训练。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：模拟环境（LIBERO和CALVIN）和真实世界机器人平台。</p>\n<p>评估指标：</p>\n<ol><li>成功率（Success Rate）：任务成功的百分比，越高越好。</li><li>平均长度（Avg. len）：CALVIN中完成任务的平均子任务数，越高越好。</li><li>推理效率：吞吐量（Hz，越高越好）和延迟（秒，越低越好）。</li></ol>"
  },
  {
    "date": "2025-09-11",
    "title": "SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2509.09090",
    "summary_markdown": "论文研究单位\n南京大学电子科学与工程学院，亚利桑那大学\n\n论文概述\n本文提出了SQAP-VLA，一个协同的量化感知剪枝框架，用于高性能的视觉-语言-动作（VLA）模型。针对现有VLA模型计算与内存成本高、且量化与token剪枝两种压缩技术存在内在不兼容性的问题，该框架首次在免训练的前提下，同时实现了最先进的量化和token剪枝。SQAP-VLA通过协同设计量化和剪枝流程，提出了针对量化模型的token剪枝标准，并改进了量化器设计以增强剪枝的有效性，从而在保持模型核心性能的同时，显著提升了计算效率和推理速度。\n\n论文核心贡献点\n- 识别了VLA模型中量化与token剪枝之间的内在不兼容性，并提出一个量化感知的token剪枝协同设计框架，使剪枝策略适应量化引起的特征分布变化，同时通过量化技术提升剪枝的有效性。\n- 在剪枝方面，引入了量化不敏感保留、机器人感知保护和空间感知采样策略，使token剪枝能适应量化表示；在量化方面，提出了逐张量Hadamard变换来优化注意力分布，从而提供更可靠的剪枝标准。\n- 通过全面的实验验证，该方法不仅解决了传统剪枝在极端压缩下的失效问题，还实现了最高的性能水平，为高性能VLA模型在资源受限设备上的高效、低延迟部署开辟了可行的路径。\n\n论文方法描述\n该方法首先分析了量化对token剪枝的挑战：量化会扭曲注意力分数的分布，使其变得分散和偏移，导致基于注意力分数的剪枝策略失效。为解决此问题，方法提出了三个协同的量化感知剪枝策略：1) 量化不敏感token保留：选择注意力分数最高的top-k个token，因为其相对排序对量化噪声具有鲁棒性。2) 机器人感知token保护：利用已知的机器人手臂三维坐标，将其投影到图像平面，强制保留机械臂周围的token环。3) 空间感知token采样：对剩余token使用最远点采样（FPS）以确保空间覆盖的多样性。此外，方法还通过在查询和键层引入Hadamard变换，来平滑激活分布并抑制离群值，从而增强注意力图的可靠性，使剪枝标准更有效。\n\n论文使用数据集和训练资源\n- 数据集：Open X-Embodiment (OXE) 数据集。\n- 训练资源：该框架是免训练的。实验直接使用在OXE数据集上预训练好的CogAct模型，无需任何额外的微调或重新训练。\n\n论文使用的评估环境和评估指标\n- 评估环境：标准的机器人仿真基准。\n- 评估任务：Pick Coke Can, Move Near, Open/Close Drawer, Place Apple in Top Drawer。\n- 评估指标：\n - 成功率（SR）：任务成功的百分比。\n - 加速比：相对于FP16 CogAct基线的理论加速倍数。",
    "summary_html": "<p>论文研究单位</p>\n<p>南京大学电子科学与工程学院，亚利桑那大学</p>\n\n<p>论文概述</p>\n<p>本文提出了SQAP-VLA，一个协同的量化感知剪枝框架，用于高性能的视觉-语言-动作（VLA）模型。针对现有VLA模型计算与内存成本高、且量化与token剪枝两种压缩技术存在内在不兼容性的问题，该框架首次在免训练的前提下，同时实现了最先进的量化和token剪枝。SQAP-VLA通过协同设计量化和剪枝流程，提出了针对量化模型的token剪枝标准，并改进了量化器设计以增强剪枝的有效性，从而在保持模型核心性能的同时，显著提升了计算效率和推理速度。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>识别了VLA模型中量化与token剪枝之间的内在不兼容性，并提出一个量化感知的token剪枝协同设计框架，使剪枝策略适应量化引起的特征分布变化，同时通过量化技术提升剪枝的有效性。</li><li>在剪枝方面，引入了量化不敏感保留、机器人感知保护和空间感知采样策略，使token剪枝能适应量化表示；在量化方面，提出了逐张量Hadamard变换来优化注意力分布，从而提供更可靠的剪枝标准。</li><li>通过全面的实验验证，该方法不仅解决了传统剪枝在极端压缩下的失效问题，还实现了最高的性能水平，为高性能VLA模型在资源受限设备上的高效、低延迟部署开辟了可行的路径。</li></ul>\n\n<p>论文方法描述</p>\n<p>该方法首先分析了量化对token剪枝的挑战：量化会扭曲注意力分数的分布，使其变得分散和偏移，导致基于注意力分数的剪枝策略失效。为解决此问题，方法提出了三个协同的量化感知剪枝策略：1) 量化不敏感token保留：选择注意力分数最高的top-k个token，因为其相对排序对量化噪声具有鲁棒性。2) 机器人感知token保护：利用已知的机器人手臂三维坐标，将其投影到图像平面，强制保留机械臂周围的token环。3) 空间感知token采样：对剩余token使用最远点采样（FPS）以确保空间覆盖的多样性。此外，方法还通过在查询和键层引入Hadamard变换，来平滑激活分布并抑制离群值，从而增强注意力图的可靠性，使剪枝标准更有效。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：Open X-Embodiment (OXE) 数据集。</li><li>训练资源：该框架是免训练的。实验直接使用在OXE数据集上预训练好的CogAct模型，无需任何额外的微调或重新训练。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：标准的机器人仿真基准。</li><li>评估任务：Pick Coke Can, Move Near, Open/Close Drawer, Place Apple in Top Drawer。</li><li>评估指标：</li></ul>\n<p> - 成功率（SR）：任务成功的百分比。</p>\n<p> - 加速比：相对于FP16 CogAct基线的理论加速倍数。</p>"
  },
  {
    "date": "2025-09-10",
    "title": "RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation",
    "link": "http://arxiv.org/abs/2509.08820",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-09-09",
    "title": "TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2509.07962",
    "summary_markdown": "# 论文研究单位\n\n北京人工智能研究院 (BAAI)\n清华大学产业智能研究院 (AIR)\n南洋理工大学 (NTU)\n# 论文概述\n\n论文研究如何将关节扭矩信号有效融入视觉-语言-动作（VLA）模型，以提升机器人在接触丰富任务中的感知与控制能力。论文系统地探索了扭矩作为本体感受信号的设计空间，并在真实机器人（Cobot Magic ALOHA双臂平台）上验证了所提出方法对成功率和泛化性能的提升。\n# 论文核心贡献点\n\n- 确定了扭矩信号在VLA中的最佳集成位置与方法：解码器侧、单历史token优于编码器与多token方案。\n- 提出将扭矩历史聚合为单一token插入解码器的观测融入方式，保持解码器输入模式完整性并显著提升接触丰富任务表现。\n- 引入“动作–扭矩联合预测”的训练目标，将扭矩预测作为辅助输出，使模型学习物理上合理的交互动力学表示。\n- 在跨模型（RDT、π0）与跨本体（Cobot Magic ALOHA、ROKAE SR）上验证有效性。\n# 论文方法描述\n\n- 扭矩作为观测（Sec 4）\n - 架构对比：编码器插入、decoder预连接、decoder后连接；结论为decoder后连接最优。\n - 历史信息：比较逐帧token与单历史token；单历史token在解码器侧最优，避免破坏解码器学习到的输入模式。\n- 扭矩作为目标（Sec 5）\n - 联合扩散损失：将动作块与扭矩块拼接为统一token，使用共享扩散权重与分头MSE损失进行联合预测；通过权重β平衡动作与扭矩目标。\n# 论文使用数据集和训练资源\n\n- 数据：作者自采数据集，基于Cobot Magic ALOHA双臂平台的多种任务演示，用于VLA模型微调；涉及10项任务（5项接触丰富、5项常规）。\n- 训练资源：基于公开预训练权重的VLA模型（ACT、RDT、π0）；硬件平台包括3个D435深度相机与7-DOF机械臂；关节扭矩由电机电流与转矩常数实时估计，无需外置力/扭矩传感器。\n# 论文使用的评估环境和评估指标\n\n- 评估环境：Cobot Magic ALOHA与ROKAE SR机械臂；任务包括按钮按压、充电器插拔、USB插拔、拔插座、门把手转动、瓶子拾取与放置、倒液体、叠 cubes、推到定位、开抽屉等。\n- 评估指标：20轮试验成功率（成功次数/20）；对比基线与不同扭矩集成策略的定量结果；展示接触阶段的扭矩曲线与任务重试的定性可视化。",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>北京人工智能研究院 (BAAI)</p>\n<p>清华大学产业智能研究院 (AIR)</p>\n<p>南洋理工大学 (NTU)</p>\n<h1>论文概述</h1>\n\n<p>论文研究如何将关节扭矩信号有效融入视觉-语言-动作（VLA）模型，以提升机器人在接触丰富任务中的感知与控制能力。论文系统地探索了扭矩作为本体感受信号的设计空间，并在真实机器人（Cobot Magic ALOHA双臂平台）上验证了所提出方法对成功率和泛化性能的提升。</p>\n<h1>论文核心贡献点</h1>\n\n<ul><li>确定了扭矩信号在VLA中的最佳集成位置与方法：解码器侧、单历史token优于编码器与多token方案。</li><li>提出将扭矩历史聚合为单一token插入解码器的观测融入方式，保持解码器输入模式完整性并显著提升接触丰富任务表现。</li><li>引入“动作–扭矩联合预测”的训练目标，将扭矩预测作为辅助输出，使模型学习物理上合理的交互动力学表示。</li><li>在跨模型（RDT、π0）与跨本体（Cobot Magic ALOHA、ROKAE SR）上验证有效性。</li></ul>\n<h1>论文方法描述</h1>\n\n<ul><li>扭矩作为观测（Sec 4）</li></ul>\n<p> - 架构对比：编码器插入、decoder预连接、decoder后连接；结论为decoder后连接最优。</p>\n<p> - 历史信息：比较逐帧token与单历史token；单历史token在解码器侧最优，避免破坏解码器学习到的输入模式。</p>\n<ul><li>扭矩作为目标（Sec 5）</li></ul>\n<p> - 联合扩散损失：将动作块与扭矩块拼接为统一token，使用共享扩散权重与分头MSE损失进行联合预测；通过权重β平衡动作与扭矩目标。</p>\n<h1>论文使用数据集和训练资源</h1>\n\n<ul><li>数据：作者自采数据集，基于Cobot Magic ALOHA双臂平台的多种任务演示，用于VLA模型微调；涉及10项任务（5项接触丰富、5项常规）。</li><li>训练资源：基于公开预训练权重的VLA模型（ACT、RDT、π0）；硬件平台包括3个D435深度相机与7-DOF机械臂；关节扭矩由电机电流与转矩常数实时估计，无需外置力/扭矩传感器。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<ul><li>评估环境：Cobot Magic ALOHA与ROKAE SR机械臂；任务包括按钮按压、充电器插拔、USB插拔、拔插座、门把手转动、瓶子拾取与放置、倒液体、叠 cubes、推到定位、开抽屉等。</li><li>评估指标：20轮试验成功率（成功次数/20）；对比基线与不同扭矩集成策略的定量结果；展示接触阶段的扭矩曲线与任务重试的定性可视化。</li></ul>"
  },
  {
    "date": "2025-09-09",
    "title": "Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation",
    "link": "http://arxiv.org/abs/2509.07957",
    "summary_markdown": "# 论文研究单位\n杭州电子科技大学人工智能学院机器学习与健康国际合作基地（Shunlei Li、Jiuwen Cao）；新墨西哥大学电气与计算机工程系（Longsen Gao）；慕尼黑工业大学计算、信息与技术学院（Yingbai Hu）。\n# 论文概述\n论文提出图融合的视觉-语言-动作模型 GF-VLA，用于从单次 RGB-D 人类演示中为双臂机器人学习可泛化的层级策略与动作。与传统轨迹复制不同，GF-VLA 通过信息论从演示中抽取关键手-物与物-物交互，构造成时间排序的场景图，并将其与语言条件下的Transformer结合，生成可解释的行为树与笛卡尔运动基元。为提升双任务执行效率，论文设计了跨臂分配策略，自主确定抓握任务分配，无需显式几何建模。实验在四类双臂方块组装基准上验证策略迁移与空间泛化能力。\n# 论文核心贡献点\n- 提出基于信息论的交互抽取方法：从多模态演示中以熵与互信息选择任务相关的手-物与物-物交互，构建交互感知的场景图。\n- 提出 GF-VLA 框架：首次将结构化的交互建模与视觉-语言-动作（VLA）推理统一，支持鲁棒、可泛化的双臂操作。\n- 增强可解释性：在VLA中嵌入链式思维（CoT）提示与自验证，实现显式的子目标分解与执行验证。\n# 论文方法描述\n- 信息论驱动的场景图：使用滑动时间窗对位置信号进行熵分析，识别动态活跃区间；用互信息检测手-物耦合（分为“耦合运动”与“停靠”）与物-物交互（分为“高效OO”与“瞬态OO”）。图节点包含实体类别与六维位姿，边表示交互类型与强度。\n- 图融合的视觉-语言-动作：统一双头架构包含LLM头（高层语义规划与CoT推理、自验证）与动作头（低层笛卡尔末端与夹爪控制，5 Hz指令流）。视觉侧采用RGB与RGB-D分割、特征投影与token对齐；语言侧基于7B参数的LLaMA-2骨干适配。\n- 链式思维语义策略：LLM头输出结构化的推理轨迹与行为树节点（动作类型、参数、验证条件），确保时序一致性与可审计性。\n- 参数高效微调：使用LoRA适配LLM与动作双头，仅更新主干与适配器。策略头监督来自250段人类演示（125用于训练、125用于评估）；动作头在240段双臂试验上微调（UR5e+Robotiq 2F-85与UR10e+Barrett BH282），任务覆盖形状泛化、空间关系、绝对与相对6D位姿执行。\n# 论文使用数据集和训练资源\n- 人类演示数据：10名参与者提供的250个RGB视频，包含字母搭建与塔式构造任务；125个用于策略头训练、125个用于规划评估。\n- 机器人试验数据：240次双臂试验，涵盖4类任务×3种形状×20次重复（各120/120划分为微调/评估）；Intel RealSense D435i顶置RGB-D感知。\n- 计算资源与时延：单张RTX 4090进行约40小时微调，指令-推理平均时延约7.56秒（bfloat16）。\n# 论文使用的评估环境和评估指标\n- 评估环境：双UR机械臂与不同夹爪协同（UR5e+Robotiq 2F-85、UR10e+Barrett BH282），顶部RGB-D相机（Intel RealSense D435i）；任务含形状泛化、空间关系、6D位姿执行与字母/塔式结构构建。\n- 指标：\n - 图表示准确度（GRA）与任务切分准确度（TSA）：单手任务达GRA 98.5%、TSA 95.6%；字母任务GRA 97.2%、TSA 93.9%；塔式任务GRA 96.8%、TSA 93.1%。\n - LLM规划评估：覆盖度、排序准确度、验证正确性与CoT可解释性评分。\n - 抓取成功率（GSR）、放置成功率（PSR）、6D位姿误差（6DPE）、指令一致性评分（ICS）。\n - 任务成功率（TSR）、计划迁移率（PTR）、双臂协调评分（BCS）。\n- 性能表现（示例）：\n - 形状泛化：GSR 0.98±0.02、PSR 0.95±0.03、6DPE 1.2cm/2.5°、ICS 4.8±0.3。\n - 相对位姿：GSR 0.91±0.04、PSR 0.85±0.06、6DPE 3.1cm/6.8°。\n - 任务迁移：六个新任务平均TSR 0.90、PTR≥0.83、BCS≥4.4；总体抓取/放置/任务成功率约为94%/89%/90%，且跨形状、姿态与视角变化保持鲁棒性。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>杭州电子科技大学人工智能学院机器学习与健康国际合作基地（Shunlei Li、Jiuwen Cao）；新墨西哥大学电气与计算机工程系（Longsen Gao）；慕尼黑工业大学计算、信息与技术学院（Yingbai Hu）。</p>\n<h1>论文概述</h1>\n<p>论文提出图融合的视觉-语言-动作模型 GF-VLA，用于从单次 RGB-D 人类演示中为双臂机器人学习可泛化的层级策略与动作。与传统轨迹复制不同，GF-VLA 通过信息论从演示中抽取关键手-物与物-物交互，构造成时间排序的场景图，并将其与语言条件下的Transformer结合，生成可解释的行为树与笛卡尔运动基元。为提升双任务执行效率，论文设计了跨臂分配策略，自主确定抓握任务分配，无需显式几何建模。实验在四类双臂方块组装基准上验证策略迁移与空间泛化能力。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出基于信息论的交互抽取方法：从多模态演示中以熵与互信息选择任务相关的手-物与物-物交互，构建交互感知的场景图。</li><li>提出 GF-VLA 框架：首次将结构化的交互建模与视觉-语言-动作（VLA）推理统一，支持鲁棒、可泛化的双臂操作。</li><li>增强可解释性：在VLA中嵌入链式思维（CoT）提示与自验证，实现显式的子目标分解与执行验证。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>信息论驱动的场景图：使用滑动时间窗对位置信号进行熵分析，识别动态活跃区间；用互信息检测手-物耦合（分为“耦合运动”与“停靠”）与物-物交互（分为“高效OO”与“瞬态OO”）。图节点包含实体类别与六维位姿，边表示交互类型与强度。</li><li>图融合的视觉-语言-动作：统一双头架构包含LLM头（高层语义规划与CoT推理、自验证）与动作头（低层笛卡尔末端与夹爪控制，5 Hz指令流）。视觉侧采用RGB与RGB-D分割、特征投影与token对齐；语言侧基于7B参数的LLaMA-2骨干适配。</li><li>链式思维语义策略：LLM头输出结构化的推理轨迹与行为树节点（动作类型、参数、验证条件），确保时序一致性与可审计性。</li><li>参数高效微调：使用LoRA适配LLM与动作双头，仅更新主干与适配器。策略头监督来自250段人类演示（125用于训练、125用于评估）；动作头在240段双臂试验上微调（UR5e+Robotiq 2F-85与UR10e+Barrett BH282），任务覆盖形状泛化、空间关系、绝对与相对6D位姿执行。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>人类演示数据：10名参与者提供的250个RGB视频，包含字母搭建与塔式构造任务；125个用于策略头训练、125个用于规划评估。</li><li>机器人试验数据：240次双臂试验，涵盖4类任务×3种形状×20次重复（各120/120划分为微调/评估）；Intel RealSense D435i顶置RGB-D感知。</li><li>计算资源与时延：单张RTX 4090进行约40小时微调，指令-推理平均时延约7.56秒（bfloat16）。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：双UR机械臂与不同夹爪协同（UR5e+Robotiq 2F-85、UR10e+Barrett BH282），顶部RGB-D相机（Intel RealSense D435i）；任务含形状泛化、空间关系、6D位姿执行与字母/塔式结构构建。</li><li>指标：</li></ul>\n<p> - 图表示准确度（GRA）与任务切分准确度（TSA）：单手任务达GRA 98.5%、TSA 95.6%；字母任务GRA 97.2%、TSA 93.9%；塔式任务GRA 96.8%、TSA 93.1%。</p>\n<p> - LLM规划评估：覆盖度、排序准确度、验证正确性与CoT可解释性评分。</p>\n<p> - 抓取成功率（GSR）、放置成功率（PSR）、6D位姿误差（6DPE）、指令一致性评分（ICS）。</p>\n<p> - 任务成功率（TSR）、计划迁移率（PTR）、双臂协调评分（BCS）。</p>\n<ul><li>性能表现（示例）：</li></ul>\n<p> - 形状泛化：GSR 0.98±0.02、PSR 0.95±0.03、6DPE 1.2cm/2.5°、ICS 4.8±0.3。</p>\n<p> - 相对位姿：GSR 0.91±0.04、PSR 0.85±0.06、6DPE 3.1cm/6.8°。</p>\n<p> - 任务迁移：六个新任务平均TSR 0.90、PTR≥0.83、BCS≥4.4；总体抓取/放置/任务成功率约为94%/89%/90%，且跨形状、姿态与视角变化保持鲁棒性。</p>"
  },
  {
    "date": "2025-09-08",
    "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
    "link": "http://arxiv.org/abs/2509.06951",
    "summary_markdown": "# 论文研究单位\n上海人工智能实验室；哈尔滨工业大学（深圳）\n# 论文概述\n论文提出 F1-VLA，一个将视觉、语言与动作统一建模的具身智能框架。该模型通过显式的“视觉前瞻生成”模块，在感知与行动之间引入目标条件化的未来视觉状态预测，并采用 Mixture-of-Transformer 架构将理解、前瞻与控制三类专家进行层次化整合。基于“预测式逆动力学”思想，F1 将动作生成重塑为“以前瞻为目标的逆问题求解”，在动态与长时序任务中显著提升鲁棒性与成功率。模型在 330k+ 轨迹、136 项任务、5 种机器人形态的大规模数据上进行三阶段训练，并在真实场景与模拟基准上均取得领先表现。\n# 论文核心贡献点\n- 引入视觉前瞻作为显式规划信号，将 VLA 从“反应式状态到动作”的映射转变为“前瞻引导的逆动力学”推理。\n- 提出 UGA（理解–生成–行动）渐进式注意力与跨专家因果信息流，确保前瞻与行动模块的稳定性与可解释性。\n- 设计 next-scale（多尺度）预测的高效机制，生成多分辨率的视觉前瞻 tokens，兼顾实时性与预测质量。\n- 给出三阶段训练配方：先对齐前瞻与理解，再在大规模机器人数据上进行预训练，最后进行任务特定后训练，实现跨任务、跨形态的可迁移能力。\n- 在 LIBERO、SimplerEnv Bridge 等模拟基准与真实机器人平台（Genie、Franka、ARX LIFT II）上系统验证优越性，特别是在动态环境与长时序任务中的稳定增益。\n# 论文方法描述\n- 架构：Mixture-of-Transformer（MoT），包含理解专家、生成专家、行动专家。理解专家采用 PaliGemma（基于 SigLIP 视觉编码与 Gemma 解码器）以实现语言–视觉对齐；生成专家基于 VAR 的残差 VQ-VAE 进行图像离散化，采用 next-scale 预测与时间卷积聚合历史运动特征以高效合成前瞻；行动专家以 flow matching 在连续动作空间学习从噪声到目标动作的向量场，配合 chunked action 预测生成短期动作序列。\n- 信息流：理解 → 生成 → 行动的单向因果层次，生成模块不接收来自行动的逆向信息，防止信息泄漏与训练不稳定。\n- 训练目标：阶段 I 以教师强制最小化前瞻 tokens 的负对数似然；阶段 II/III 以自回归 next-scale 预测和 flow matching 的行动损失联合优化，总体损失为两者加权。\n- 推理优化：实际推理时仅使用 4 个尺度的前瞻预测以平衡效率与效果。\n# 论文使用数据集和训练资源\n- 训练数据：330k+ 轨迹，覆盖 136 项任务与 5 种机器人形态；包含 LIBERO、Open-X-Embodiment、AgiBotWorld 等公开机器人数据集，涵盖从基础抓取、放置到复杂协作与推送的广泛技能，任务时长 10 秒至 2 分钟以上。\n- 三阶段训练：Stage I 冻结理解专家，仅训练生成专家进行前瞻对齐；Stage II 在大规模机器人数据上联合预训练；Stage III 进行任务特定后训练以适配新形态与细粒度技能。\n- 实现细节：理解与行动专家由 π0 初始化，生成专家随机初始化并配备预训练的 VAR 残差 VQ-VAE；主干采用 Swish 激活、RMSNorm 与 RoPE；训练超参数详见附录。\n# 论文使用的评估环境和评估指标\n- 模拟基准：\n - LIBERO（空间、物体、目标、长时序四套件）：评价指标为成功率（SR，越高越好）和排名（Rank，越低越好）。\n - SimplerEnv Bridge（多步精细操控）：评价指标为抓取成功率与整体任务成功率。\n- 真实实验：\n - 平台：Genie（双臂）、Franka、ARX LIFT II；任务包括基础抓取与放置、精细操控、双臂协作与人机交互、动态传送带抓取、长时序序列操作等。\n - 指标：每项任务 15 次试验的抓取与任务成功率；报告各任务均值与对比基线（π0、gr00t-N1/N1.5、OpenVLA、SpatialVLA、CoT-VLA、RT-1-X、RoboVLM 等）。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>上海人工智能实验室；哈尔滨工业大学（深圳）</p>\n<h1>论文概述</h1>\n<p>论文提出 F1-VLA，一个将视觉、语言与动作统一建模的具身智能框架。该模型通过显式的“视觉前瞻生成”模块，在感知与行动之间引入目标条件化的未来视觉状态预测，并采用 Mixture-of-Transformer 架构将理解、前瞻与控制三类专家进行层次化整合。基于“预测式逆动力学”思想，F1 将动作生成重塑为“以前瞻为目标的逆问题求解”，在动态与长时序任务中显著提升鲁棒性与成功率。模型在 330k+ 轨迹、136 项任务、5 种机器人形态的大规模数据上进行三阶段训练，并在真实场景与模拟基准上均取得领先表现。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>引入视觉前瞻作为显式规划信号，将 VLA 从“反应式状态到动作”的映射转变为“前瞻引导的逆动力学”推理。</li><li>提出 UGA（理解–生成–行动）渐进式注意力与跨专家因果信息流，确保前瞻与行动模块的稳定性与可解释性。</li><li>设计 next-scale（多尺度）预测的高效机制，生成多分辨率的视觉前瞻 tokens，兼顾实时性与预测质量。</li><li>给出三阶段训练配方：先对齐前瞻与理解，再在大规模机器人数据上进行预训练，最后进行任务特定后训练，实现跨任务、跨形态的可迁移能力。</li><li>在 LIBERO、SimplerEnv Bridge 等模拟基准与真实机器人平台（Genie、Franka、ARX LIFT II）上系统验证优越性，特别是在动态环境与长时序任务中的稳定增益。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>架构：Mixture-of-Transformer（MoT），包含理解专家、生成专家、行动专家。理解专家采用 PaliGemma（基于 SigLIP 视觉编码与 Gemma 解码器）以实现语言–视觉对齐；生成专家基于 VAR 的残差 VQ-VAE 进行图像离散化，采用 next-scale 预测与时间卷积聚合历史运动特征以高效合成前瞻；行动专家以 flow matching 在连续动作空间学习从噪声到目标动作的向量场，配合 chunked action 预测生成短期动作序列。</li><li>信息流：理解 → 生成 → 行动的单向因果层次，生成模块不接收来自行动的逆向信息，防止信息泄漏与训练不稳定。</li><li>训练目标：阶段 I 以教师强制最小化前瞻 tokens 的负对数似然；阶段 II/III 以自回归 next-scale 预测和 flow matching 的行动损失联合优化，总体损失为两者加权。</li><li>推理优化：实际推理时仅使用 4 个尺度的前瞻预测以平衡效率与效果。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>训练数据：330k+ 轨迹，覆盖 136 项任务与 5 种机器人形态；包含 LIBERO、Open-X-Embodiment、AgiBotWorld 等公开机器人数据集，涵盖从基础抓取、放置到复杂协作与推送的广泛技能，任务时长 10 秒至 2 分钟以上。</li><li>三阶段训练：Stage I 冻结理解专家，仅训练生成专家进行前瞻对齐；Stage II 在大规模机器人数据上联合预训练；Stage III 进行任务特定后训练以适配新形态与细粒度技能。</li><li>实现细节：理解与行动专家由 π0 初始化，生成专家随机初始化并配备预训练的 VAR 残差 VQ-VAE；主干采用 Swish 激活、RMSNorm 与 RoPE；训练超参数详见附录。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>模拟基准：</li></ul>\n<p> - LIBERO（空间、物体、目标、长时序四套件）：评价指标为成功率（SR，越高越好）和排名（Rank，越低越好）。</p>\n<p> - SimplerEnv Bridge（多步精细操控）：评价指标为抓取成功率与整体任务成功率。</p>\n<ul><li>真实实验：</li></ul>\n<p> - 平台：Genie（双臂）、Franka、ARX LIFT II；任务包括基础抓取与放置、精细操控、双臂协作与人机交互、动态传送带抓取、长时序序列操作等。</p>\n<p> - 指标：每项任务 15 次试验的抓取与任务成功率；报告各任务均值与对比基线（π0、gr00t-N1/N1.5、OpenVLA、SpatialVLA、CoT-VLA、RT-1-X、RoboVLM 等）。</p>"
  },
  {
    "date": "2025-09-08",
    "title": "LLaDA-VLA: Vision Language Diffusion Action Models",
    "link": "http://arxiv.org/abs/2509.06932",
    "summary_markdown": "### 论文研究单位\n\n中国科学技术大学、南京大学、Dexmal\n### 论文概述\n\n该论文提出了LLaDA-VLA，这是首个基于预训练扩散视觉语言模型构建的视觉-语言-扩散-动作模型，用于机器人操作任务。为了解决预训练d-VLMs与机器人动作生成之间的领域差距以及掩码扩散范式在生成结构化动作序列方面的不足，论文引入了两个核心设计：一是局部化特殊令牌分类策略，通过将分类空间限制在特殊动作令牌上，降低了模型适应的难度；二是分层动作结构化解码策略，该策略在解码过程中显式地考虑动作内部和动作之间的依赖关系，生成了更连贯、精确的动作序列。大量的实验表明，LLaDA-VLA在模拟环境和真实机器人上的性能均显著优于现有的最先进视觉-语言-动作模型。\n### 论文核心贡献点\n\n1. 提出了首个基于预训练d-VLMs构建的视觉-语言-扩散-动作模型LLaDA-VLA，为机器人策略学习建立了一个新范式。\n2. 设计了两种关键技术使掩码扩散模型适用于动作生成：用于简化领域适应的局部化特殊令牌分类策略，以及用于无缝集成到动作生成中的分层动作结构化解码策略。\n3. 在SimplerEnv和CALVIN模拟基准以及WidowX真实机器人上的广泛实验，验证了LLaDA-VLA的卓越性能，凸显了d-VLMs在机器人操作中的潜力。\n### 论文方法描述\n\n1. **模型架构**:\n * 模型基于LLaDA-V预训练d-VLM，采用SigLIP-2作为视觉编码器，MLP作为投影器。\n * 输入为语言指令和RGB图像。图像通过视觉编码器提取特征，经投影后与文本令牌拼接，一同输入到大语言扩散模型中。\n * **动作分词化**: 将连续的动作值离散化为32个区间，并引入对应数量的特殊动作令牌。一个包含7维（3个位置、3个旋转、1个夹爪状态）的动作由7个特殊令牌表示。模型一次性预测一个包含K个时间步的动作块（K*D个令牌）。\n2. **局部化特殊令牌分类**:\n * 在训练和推理时，模型不进行全词汇表分类，而是仅在新增的特殊动作令牌集合上进行分类。\n * 损失函数也相应修改，只计算在掩码位置上的动作令牌的交叉熵损失，忽略其他令牌。此举缩小了学习目标，使模型更容易适应机器人领域。\n3. **分层动作结构化解码**:\n * 在标准的掩码扩散“预测-重掩码”流程基础上，引入了针对动作块的分层解码。\n * **动作级解码**: 每个迭代步骤，计算每个动作（包含D个令牌）的置信度分数（将内部令牌的置信度求和）。保留置信度最高的动作，将其他所有动作重新掩码。\n * **令牌级解码**: 在保留的动作内部，根据令牌级置信度进行排序，保留高置信度令牌，重掩码其余令牌。\n * 通过迭代此过程，模型能以动作为单位逐步完善输出，从而生成结构合理、前后一致的动作序列。\n### 论文使用数据集和训练资源\n\n* **数据集**:\n * **SimplerEnv**: 使用WidowX机器人在Visual Matching设置下进行评估，包含4项操作任务。\n * **CALVIN**: 一个长视野、语言条件操作的基准。采用ABC-D协议，在A、B、C环境上训练，在D环境上评估泛化性能。\n * **真实世界WidowX机器人**: 使用WidowX 250S机器人，评估了4个域内任务和4个分布外（OOD）泛化任务。\n* **训练资源和细节**:\n * 基于开源的LLaDA-V预训练权重进行微调。\n * 训练3个周期，学习率为2e-5，批次大小为128。\n * 动作块大小K设置为5，预测的是增量动作。\n * 推理时使用10次迭代扩散步骤，并采用dllm-cache方法加速解码。\n### 论文使用的评估环境和评估指标\n\n* **评估环境**:\n * **模拟环境**: SimplerEnv和CALVIN。\n * **真实机器人**: WidowX 250S机械臂。\n* **评估指标**:\n * **SimplerEnv**: 任务成功率（%）。\n * **CALVIN**: （1）连续完成1到5个任务的成功率（%）；（2）连续完成5个任务的平均长度。\n * **真实机器人**: 域内任务和分布外（OOD）任务的任务成功率（%）。",
    "summary_html": "<h3>论文研究单位</h3>\n\n<p>中国科学技术大学、南京大学、Dexmal</p>\n<h3>论文概述</h3>\n\n<p>该论文提出了LLaDA-VLA，这是首个基于预训练扩散视觉语言模型构建的视觉-语言-扩散-动作模型，用于机器人操作任务。为了解决预训练d-VLMs与机器人动作生成之间的领域差距以及掩码扩散范式在生成结构化动作序列方面的不足，论文引入了两个核心设计：一是局部化特殊令牌分类策略，通过将分类空间限制在特殊动作令牌上，降低了模型适应的难度；二是分层动作结构化解码策略，该策略在解码过程中显式地考虑动作内部和动作之间的依赖关系，生成了更连贯、精确的动作序列。大量的实验表明，LLaDA-VLA在模拟环境和真实机器人上的性能均显著优于现有的最先进视觉-语言-动作模型。</p>\n<h3>论文核心贡献点</h3>\n\n<ol><li>提出了首个基于预训练d-VLMs构建的视觉-语言-扩散-动作模型LLaDA-VLA，为机器人策略学习建立了一个新范式。</li><li>设计了两种关键技术使掩码扩散模型适用于动作生成：用于简化领域适应的局部化特殊令牌分类策略，以及用于无缝集成到动作生成中的分层动作结构化解码策略。</li><li>在SimplerEnv和CALVIN模拟基准以及WidowX真实机器人上的广泛实验，验证了LLaDA-VLA的卓越性能，凸显了d-VLMs在机器人操作中的潜力。</li></ol>\n<h3>论文方法描述</h3>\n\n<p>1. <strong>模型架构</strong>:</p>\n<p> * 模型基于LLaDA-V预训练d-VLM，采用SigLIP-2作为视觉编码器，MLP作为投影器。</p>\n<p> * 输入为语言指令和RGB图像。图像通过视觉编码器提取特征，经投影后与文本令牌拼接，一同输入到大语言扩散模型中。</p>\n<p> * <strong>动作分词化</strong>: 将连续的动作值离散化为32个区间，并引入对应数量的特殊动作令牌。一个包含7维（3个位置、3个旋转、1个夹爪状态）的动作由7个特殊令牌表示。模型一次性预测一个包含K个时间步的动作块（K*D个令牌）。</p>\n<p>2. <strong>局部化特殊令牌分类</strong>:</p>\n<p> * 在训练和推理时，模型不进行全词汇表分类，而是仅在新增的特殊动作令牌集合上进行分类。</p>\n<p> * 损失函数也相应修改，只计算在掩码位置上的动作令牌的交叉熵损失，忽略其他令牌。此举缩小了学习目标，使模型更容易适应机器人领域。</p>\n<p>3. <strong>分层动作结构化解码</strong>:</p>\n<p> * 在标准的掩码扩散“预测-重掩码”流程基础上，引入了针对动作块的分层解码。</p>\n<p> * <strong>动作级解码</strong>: 每个迭代步骤，计算每个动作（包含D个令牌）的置信度分数（将内部令牌的置信度求和）。保留置信度最高的动作，将其他所有动作重新掩码。</p>\n<p> * <strong>令牌级解码</strong>: 在保留的动作内部，根据令牌级置信度进行排序，保留高置信度令牌，重掩码其余令牌。</p>\n<p> * 通过迭代此过程，模型能以动作为单位逐步完善输出，从而生成结构合理、前后一致的动作序列。</p>\n<h3>论文使用数据集和训练资源</h3>\n\n<p>* <strong>数据集</strong>:</p>\n<p> * <strong>SimplerEnv</strong>: 使用WidowX机器人在Visual Matching设置下进行评估，包含4项操作任务。</p>\n<p> * <strong>CALVIN</strong>: 一个长视野、语言条件操作的基准。采用ABC-D协议，在A、B、C环境上训练，在D环境上评估泛化性能。</p>\n<p> * <strong>真实世界WidowX机器人</strong>: 使用WidowX 250S机器人，评估了4个域内任务和4个分布外（OOD）泛化任务。</p>\n<p>* <strong>训练资源和细节</strong>:</p>\n<p> * 基于开源的LLaDA-V预训练权重进行微调。</p>\n<p> * 训练3个周期，学习率为2e-5，批次大小为128。</p>\n<p> * 动作块大小K设置为5，预测的是增量动作。</p>\n<p> * 推理时使用10次迭代扩散步骤，并采用dllm-cache方法加速解码。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n\n<p>* <strong>评估环境</strong>:</p>\n<p> * <strong>模拟环境</strong>: SimplerEnv和CALVIN。</p>\n<p> * <strong>真实机器人</strong>: WidowX 250S机械臂。</p>\n<p>* <strong>评估指标</strong>:</p>\n<p> * <strong>SimplerEnv</strong>: 任务成功率（%）。</p>\n<p> * <strong>CALVIN</strong>: （1）连续完成1到5个任务的成功率（%）；（2）连续完成5个任务的平均长度。</p>\n<p> * <strong>真实机器人</strong>: 域内任务和分布外（OOD）任务的任务成功率（%）。</p>"
  },
  {
    "date": "2025-09-06",
    "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning",
    "link": "http://arxiv.org/abs/2509.05614",
    "summary_markdown": "### 论文研究单位\n未在提供的文本中明确提及。\n### 论文概述\n本文提出了一种名为SpecPrune-VLA的免训练剪枝方法，用于加速视觉-语言-动作（VLA）模型的推理。该方法通过结合当前动作生成的局部信息和先前动作的全局信息来选择token，实现两级剪枝和启发式控制。实验表明，在LIBERO模拟基准测试中，相比高性能模型OpenVLA-OFT，该方法在NVIDIA A800和RTX 3090 GPU上分别实现了1.46倍和1.57倍的平均加速，且任务成功率损失可忽略不计。\n### 论文核心贡献点\n1. 提出了一种新见解：连续动作之间的信息高度相似，应结合当前动作的局部信息和先前动作的全局信息进行token选择。\n2. 设计了SpecPrune-VLA框架，包含三个技术：动作级静态token剪枝、层级动态token剪枝和轻量级动作感知控制器。\n3. 实现了在不同硬件平台上的高效推理加速，且保持任务成功率。\n### 论文方法描述\n1. **动作级静态剪枝**：利用先前动作的全局注意力信息选择top-K全局重要token，通过基于速度的帧比较和自推测token选择补充动态token和任务相关token，融合局部和全局选择后剪枝50-70%的视觉token。\n2. **层级动态剪枝**：通过动态更新token的重要性分数并在不同层重新评估token重要性，引入基于排名的权重和层级置信度分数计算最终分数。\n3. **轻量级动作感知控制器**：根据末端执行器速度将动作分为粗粒度（如大位移）和细粒度（如抓取），动态调整剪枝策略，以平衡速度和精度。\n### 论文使用数据集和训练资源\n- **数据集**：LIBERO模拟基准，包含四个任务套件（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个套件10个任务。\n- **训练资源**：未提及训练细节，方法为免训练，基于OpenVLA-OFT实现，实验在NVIDIA A800-80GB和RTX 3090 GPU进行。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Linux工作站，使用NVIDIA A800-80GB和RTX 3090 GPU，模拟环境为Franka Emika Panda机械臂。\n- **评估指标**：\n - **任务成功率（Success Rate, SR）**：衡量任务完成的百分比。\n - **延迟（Latency）**：从接收输入到生成动作的时间，单位毫秒。\n - **加速比（Speedup）**：相对于基线模型的延迟减少倍数。\n - **FLOPs**：计算量，用于量化剪枝效果。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>未在提供的文本中明确提及。</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为SpecPrune-VLA的免训练剪枝方法，用于加速视觉-语言-动作（VLA）模型的推理。该方法通过结合当前动作生成的局部信息和先前动作的全局信息来选择token，实现两级剪枝和启发式控制。实验表明，在LIBERO模拟基准测试中，相比高性能模型OpenVLA-OFT，该方法在NVIDIA A800和RTX 3090 GPU上分别实现了1.46倍和1.57倍的平均加速，且任务成功率损失可忽略不计。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新见解：连续动作之间的信息高度相似，应结合当前动作的局部信息和先前动作的全局信息进行token选择。</li><li>设计了SpecPrune-VLA框架，包含三个技术：动作级静态token剪枝、层级动态token剪枝和轻量级动作感知控制器。</li><li>实现了在不同硬件平台上的高效推理加速，且保持任务成功率。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>动作级静态剪枝</strong>：利用先前动作的全局注意力信息选择top-K全局重要token，通过基于速度的帧比较和自推测token选择补充动态token和任务相关token，融合局部和全局选择后剪枝50-70%的视觉token。</li><li><strong>层级动态剪枝</strong>：通过动态更新token的重要性分数并在不同层重新评估token重要性，引入基于排名的权重和层级置信度分数计算最终分数。</li><li><strong>轻量级动作感知控制器</strong>：根据末端执行器速度将动作分为粗粒度（如大位移）和细粒度（如抓取），动态调整剪枝策略，以平衡速度和精度。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：LIBERO模拟基准，包含四个任务套件（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个套件10个任务。</li><li><strong>训练资源</strong>：未提及训练细节，方法为免训练，基于OpenVLA-OFT实现，实验在NVIDIA A800-80GB和RTX 3090 GPU进行。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Linux工作站，使用NVIDIA A800-80GB和RTX 3090 GPU，模拟环境为Franka Emika Panda机械臂。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率（Success Rate, SR）</strong>：衡量任务完成的百分比。</p>\n<p> - <strong>延迟（Latency）</strong>：从接收输入到生成动作的时间，单位毫秒。</p>\n<p> - <strong>加速比（Speedup）</strong>：相对于基线模型的延迟减少倍数。</p>\n<p> - <strong>FLOPs</strong>：计算量，用于量化剪枝效果。</p>"
  },
  {
    "date": "2025-09-05",
    "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies",
    "link": "http://arxiv.org/abs/2509.04996",
    "summary_markdown": "论文研究单位\n卡尔斯鲁厄理工学院直观机器人实验室 (Intuitive Robots Lab, Karlsruhe Institute of Technology)，与微软研究院 (Microsoft Research) 合作。\n\n论文概述\n当前视觉-语言-动作（VLA）策略模型面临计算成本高和资源需求大的问题，现有扩散型VLA需数十亿参数和大规模数据集。本文提出两项创新：中间模态融合（通过剪枝30-50%的LLM层重新分配容量至扩散头）和动作特定Global-AdaLN条件（通过模块化适配减少20%参数）。基于此构建了参数量为950M的VLA模型FLOWER，仅需200 H100 GPU小时预训练，在10个模拟与真实基准的190个任务上达到与更大VLA模型相当的性能，并在CALVIN ABC基准上创4.53的新SOTA。模型在多样化机器人具身上表现出鲁棒性。\n\n论文核心贡献点\n1. **中间模态融合**：从VLM中间层提取特征（剪枝30-50%层），保留语义信息同时为流变换器释放参数空间，提升训练效率和推理速度。\n2. **动作空间全局自适应层归一化（Global-AdaLN）**：共享调制权重并生成动作类型特定信号，减少20%参数而不损失表达能力。\n3. **高效架构设计**：集成上述技术构建FLOWER模型，参数仅947M，预训练成本降低99%。\n4. **广泛验证**：在10个基准（如CALVIN、LIBERO）上190个任务实现SOTA或相当性能，真实场景泛化能力突出。\n\n论文方法描述\n模型包含剪枝后的Florence-2-L VLM编码器（移除解码器或后30%层）和18层流变换器（潜在维度1024）。VLM中间特征通过交叉注意力注入流变换器，使用动作类型特定编码器/解码器处理异构动作空间（如Delta-EEF和关节角度）。采用Rectified Flow生成动作，优化目标为最小化噪声与数据间的速度场误差，推理仅需4-8步去噪。训练使用动作块长度20和单静态图像输入，预训练数据混合包含74% Delta-EEF和26%单臂关节状态数据。\n\n论文使用数据集和训练资源\n预训练数据集为8个公开机器人数据集的混合（约250k轨迹），包括Droid、Google Robot和BridgeV2（占75%），注重场景和具身多样性。训练资源为4×H100 GPU，48小时（约200 GPU小时），批量大小1024（梯度累积步数4），使用BF16精度优化内存。\n\n论文使用的评估环境和评估指标\n**评估环境**：\n- 模拟基准：CALVIN（任务链指令跟随）、LIBERO（长视野任务）、Aloha（双臂高频控制）、SIMPLER（真实到仿真泛化）。\n- 真实场景：Franka Panda机器人厨房环境（20个任务），测试泛化（新物体、光照、背景干扰、任务组合）。\n**评估指标**：\n- 任务成功率（CALVIN需顺序完成5个指令链，LIBERO/LIBERO-90为单任务成功率）。\n- 平均序列长度（CALVIN）。\n- 推理效率：吞吐量（Hz）、延迟（秒）、VRAM占用（MB），在RTX 4090上测试。",
    "summary_html": "<p>论文研究单位</p>\n<p>卡尔斯鲁厄理工学院直观机器人实验室 (Intuitive Robots Lab, Karlsruhe Institute of Technology)，与微软研究院 (Microsoft Research) 合作。</p>\n\n<p>论文概述</p>\n<p>当前视觉-语言-动作（VLA）策略模型面临计算成本高和资源需求大的问题，现有扩散型VLA需数十亿参数和大规模数据集。本文提出两项创新：中间模态融合（通过剪枝30-50%的LLM层重新分配容量至扩散头）和动作特定Global-AdaLN条件（通过模块化适配减少20%参数）。基于此构建了参数量为950M的VLA模型FLOWER，仅需200 H100 GPU小时预训练，在10个模拟与真实基准的190个任务上达到与更大VLA模型相当的性能，并在CALVIN ABC基准上创4.53的新SOTA。模型在多样化机器人具身上表现出鲁棒性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li><strong>中间模态融合</strong>：从VLM中间层提取特征（剪枝30-50%层），保留语义信息同时为流变换器释放参数空间，提升训练效率和推理速度。</li><li><strong>动作空间全局自适应层归一化（Global-AdaLN）</strong>：共享调制权重并生成动作类型特定信号，减少20%参数而不损失表达能力。</li><li><strong>高效架构设计</strong>：集成上述技术构建FLOWER模型，参数仅947M，预训练成本降低99%。</li><li><strong>广泛验证</strong>：在10个基准（如CALVIN、LIBERO）上190个任务实现SOTA或相当性能，真实场景泛化能力突出。</li></ol>\n\n<p>论文方法描述</p>\n<p>模型包含剪枝后的Florence-2-L VLM编码器（移除解码器或后30%层）和18层流变换器（潜在维度1024）。VLM中间特征通过交叉注意力注入流变换器，使用动作类型特定编码器/解码器处理异构动作空间（如Delta-EEF和关节角度）。采用Rectified Flow生成动作，优化目标为最小化噪声与数据间的速度场误差，推理仅需4-8步去噪。训练使用动作块长度20和单静态图像输入，预训练数据混合包含74% Delta-EEF和26%单臂关节状态数据。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>预训练数据集为8个公开机器人数据集的混合（约250k轨迹），包括Droid、Google Robot和BridgeV2（占75%），注重场景和具身多样性。训练资源为4×H100 GPU，48小时（约200 GPU小时），批量大小1024（梯度累积步数4），使用BF16精度优化内存。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p><strong>评估环境</strong>：</p>\n<ul><li>模拟基准：CALVIN（任务链指令跟随）、LIBERO（长视野任务）、Aloha（双臂高频控制）、SIMPLER（真实到仿真泛化）。</li><li>真实场景：Franka Panda机器人厨房环境（20个任务），测试泛化（新物体、光照、背景干扰、任务组合）。</li></ul>\n<p><strong>评估指标</strong>：</p>\n<ul><li>任务成功率（CALVIN需顺序完成5个指令链，LIBERO/LIBERO-90为单任务成功率）。</li><li>平均序列长度（CALVIN）。</li><li>推理效率：吞吐量（Hz）、延迟（秒）、VRAM占用（MB），在RTX 4090上测试。</li></ul>"
  },
  {
    "date": "2025-09-04",
    "title": "Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models",
    "link": "http://arxiv.org/abs/2509.04063",
    "summary_markdown": "### 论文研究单位\n论文作者包括 Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang。单位信息部分缺失，但通讯作者 Donglin Wang 来自西湖大学。\n### 论文概述\n针对基于流匹配的视觉-语言-行动（VLA）模型在复杂任务中行动准确性不足的问题，论文提出了一种离线强化学习后训练方法 ARFM（自适应强化流匹配）。该方法通过在 VLA 流模型损失中引入自适应缩放因子，理论构建了一个权衡偏差-方差的优化目标，以平衡强化学习信号和梯度方差影响，从而实现更稳定和高效的微调过程。实验表明 ARFM 在泛化、鲁棒性、少样本学习和持续学习方面表现优异。\n### 论文核心贡献点\n1. 提出 ARFM 方法，一种用于 VLA 流模型的新型离线强化学习后训练方法，能够自适应调整数据质量分布。\n2. 理论建立缩放因子 α 的优化目标，并诱导二分迭代算法进行实时更新，实现高效 VLA 流模型微调。\n3. 通过仿真和真实机器人操作任务实验，验证 ARFM 在泛化能力、对动态扰动的鲁棒性、少样本学习和持续学习场景中的卓越性能。\n### 论文方法描述\n方法基于能量加权流匹配（EWFM）框架：\n- **能量加权 VLA 流模型**：构建能量引导分布 π(A_t\\|o_t) ∝ p(A_t\\|o_t) exp(α R*(A_t, o_t))，其中 R* 为标准化强化学习优势，α 为缩放因子。使用条件能量加权流匹配（CEFM）损失优化向量场，学习策略分布。\n- **自适应缩放因子 α 调整**：通过最小化目标函数 J(α) = Var(ĝ(α)) - λ S(α) 实现信号与方差的权衡，其中 ĝ(α) 是损失梯度，S(α) 是强化学习优势评分函数。基于高斯假设，推导出 J(α) 的具体形式，并通过二分迭代算法（Algorithm 1）实时求解最优 α*。最终算法（Algorithm 2）集成到 VLA 流模型微调中。\n### 论文使用数据集和训练资源\n- **数据集**：使用 LIBERO 仿真基准（包括 Object、Long、Spatial 和 Goal 四个任务套件）和真实世界 UR5 机器人平台（三个抓取和放置任务）。\n- **训练资源**：论文未明确指定硬件资源，但假设采用标准计算环境（如 GPU）进行仿真和实验。模型基于流匹配实现，训练过程涉及数据采样和损失优化。\n### 论文使用的评估环境和评估指标\n- **评估环境**： LIBERO 仿真环境和 UR5 真实机器人实验平台。\n- **评估指标**：主要使用成功率（Success Rate, SR）衡量任务完成度；在动作扰动实验中，添加高斯噪声（0.1 至 0.3 级别）评估鲁棒性，通过扰动后成功率评估性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>论文作者包括 Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang。单位信息部分缺失，但通讯作者 Donglin Wang 来自西湖大学。</p>\n<h3>论文概述</h3>\n<p>针对基于流匹配的视觉-语言-行动（VLA）模型在复杂任务中行动准确性不足的问题，论文提出了一种离线强化学习后训练方法 ARFM（自适应强化流匹配）。该方法通过在 VLA 流模型损失中引入自适应缩放因子，理论构建了一个权衡偏差-方差的优化目标，以平衡强化学习信号和梯度方差影响，从而实现更稳定和高效的微调过程。实验表明 ARFM 在泛化、鲁棒性、少样本学习和持续学习方面表现优异。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出 ARFM 方法，一种用于 VLA 流模型的新型离线强化学习后训练方法，能够自适应调整数据质量分布。</li><li>理论建立缩放因子 α 的优化目标，并诱导二分迭代算法进行实时更新，实现高效 VLA 流模型微调。</li><li>通过仿真和真实机器人操作任务实验，验证 ARFM 在泛化能力、对动态扰动的鲁棒性、少样本学习和持续学习场景中的卓越性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>方法基于能量加权流匹配（EWFM）框架：</p>\n<ul><li><strong>能量加权 VLA 流模型</strong>：构建能量引导分布 π(A_t\\|o_t) ∝ p(A_t\\|o_t) exp(α R*(A_t, o_t))，其中 R* 为标准化强化学习优势，α 为缩放因子。使用条件能量加权流匹配（CEFM）损失优化向量场，学习策略分布。</li><li><strong>自适应缩放因子 α 调整</strong>：通过最小化目标函数 J(α) = Var(ĝ(α)) - λ S(α) 实现信号与方差的权衡，其中 ĝ(α) 是损失梯度，S(α) 是强化学习优势评分函数。基于高斯假设，推导出 J(α) 的具体形式，并通过二分迭代算法（Algorithm 1）实时求解最优 α*。最终算法（Algorithm 2）集成到 VLA 流模型微调中。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：使用 LIBERO 仿真基准（包括 Object、Long、Spatial 和 Goal 四个任务套件）和真实世界 UR5 机器人平台（三个抓取和放置任务）。</li><li><strong>训练资源</strong>：论文未明确指定硬件资源，但假设采用标准计算环境（如 GPU）进行仿真和实验。模型基于流匹配实现，训练过程涉及数据采样和损失优化。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>： LIBERO 仿真环境和 UR5 真实机器人实验平台。</li><li><strong>评估指标</strong>：主要使用成功率（Success Rate, SR）衡量任务完成度；在动作扰动实验中，添加高斯噪声（0.1 至 0.3 级别）评估鲁棒性，通过扰动后成功率评估性能。</li></ul>"
  },
  {
    "date": "2025-09-04",
    "title": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction",
    "link": "http://arxiv.org/abs/2509.04018",
    "summary_markdown": "### 论文研究单位\n南开大学机器人与自动化信息系统研究所、小米汽车、东北大学机器人科学与工程学院、澳门大学\n### 论文概述\n论文提出了FPC-VLA框架，这是一个结合视觉-语言-动作（VLA）模型与监督器的双模型系统，旨在预测和修正机器人操作中的潜在失败。传统VLA模型缺乏失败恢复机制，而该框架通过VLM监督器评估动作可行性并生成修正策略，同时引入相似性引导的动作融合模块提升动作平滑性，在仿真和真实环境中均表现出优越性能。\n### 论文核心贡献点\n1. 提出基于VLM的监督器，通过结构化提示和图像输入评估夹爪动作，预测潜在失败并生成修正指令。\n2. 设计相似性引导的动作融合模块，聚合历史动作预测以生成更平滑、可靠的机器人运动。\n3. 在SIMPLER和LIBERO基准测试中，使用WidowX、Google Robot和Franka机器人进行评估，FPC-VLA在零样本和微调设置下均超越最先进方法。\n4. 在真实机器人上成功部署多样化长视野任务，验证了框架的泛化能力和实用价值。\n### 论文方法描述\n1. **架构概述**：输入为RGB图像和自然语言指令，输出为精细化动作。包含三个模块：数据集生成、VLM监督器和动作融合。\n2. **数据集生成**：从RLDS格式数据中自动生成故障预测修正数据集，通过检测夹爪状态变化识别抓取事件，将姿态差转换为结构化语言描述。\n3. **VLM监督器**：当夹爪状态变化超过阈值时触发，输入当前图像和提示，输出“是”或“否”及修正方向/幅度（如“向上移动. 大. 顺时针旋转. 小”）。\n4. **动作融合模块**：收集历史动作预测，计算与最新预测的余弦相似性，结合时间衰减权重生成融合动作，提高鲁棒性。\n### 论文使用数据集和训练资源\n1. **数据集**：基于BridgeV2（408,771条目）、RT-1（728,760条目）和RT-X（15,604条目）生成故障预测修正数据集。\n2. **训练资源**：\n - VLA模型：8块NVIDIA H100 GPU，批量大小256，学习率2×10⁻⁵，预测15步动作。\n - 监督器模型：Qwen2.5-vl 7B，bfloat16精度，批量大小64，学习率10⁻⁴，LoRA秩8。\n - 动作融合参数：α=0.1, λ=β=0.01，动作窗口大小N根据任务调整（2-11）。\n### 论文使用的评估环境和评估指标\n1. **仿真环境**：\n - SIMPLER：测试WidowX（抓取、堆叠任务）和Google Robot（抓取、抽屉操作任务）。\n - LIBERO：测试Franka机器人，包括目标导向、空间配置、对象多样性和长视野任务。\n2. **真实环境**：小米机器人和ALOHA平台，执行抓取、堆叠等长视野任务。\n3. **评估指标**：\n - 抓取成功率（Grasp Success Rate）。\n - 任务成功率（Task Success Rate）。\n - 平均抓取成功率（Avg. Grasp）和平均任务成功率（Avg. Success）。\n - 执行时间对比（关键帧1.766s vs 非关键帧0.176s）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南开大学机器人与自动化信息系统研究所、小米汽车、东北大学机器人科学与工程学院、澳门大学</p>\n<h3>论文概述</h3>\n<p>论文提出了FPC-VLA框架，这是一个结合视觉-语言-动作（VLA）模型与监督器的双模型系统，旨在预测和修正机器人操作中的潜在失败。传统VLA模型缺乏失败恢复机制，而该框架通过VLM监督器评估动作可行性并生成修正策略，同时引入相似性引导的动作融合模块提升动作平滑性，在仿真和真实环境中均表现出优越性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出基于VLM的监督器，通过结构化提示和图像输入评估夹爪动作，预测潜在失败并生成修正指令。</li><li>设计相似性引导的动作融合模块，聚合历史动作预测以生成更平滑、可靠的机器人运动。</li><li>在SIMPLER和LIBERO基准测试中，使用WidowX、Google Robot和Franka机器人进行评估，FPC-VLA在零样本和微调设置下均超越最先进方法。</li><li>在真实机器人上成功部署多样化长视野任务，验证了框架的泛化能力和实用价值。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>架构概述</strong>：输入为RGB图像和自然语言指令，输出为精细化动作。包含三个模块：数据集生成、VLM监督器和动作融合。</li><li><strong>数据集生成</strong>：从RLDS格式数据中自动生成故障预测修正数据集，通过检测夹爪状态变化识别抓取事件，将姿态差转换为结构化语言描述。</li><li><strong>VLM监督器</strong>：当夹爪状态变化超过阈值时触发，输入当前图像和提示，输出“是”或“否”及修正方向/幅度（如“向上移动. 大. 顺时针旋转. 小”）。</li><li><strong>动作融合模块</strong>：收集历史动作预测，计算与最新预测的余弦相似性，结合时间衰减权重生成融合动作，提高鲁棒性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：基于BridgeV2（408,771条目）、RT-1（728,760条目）和RT-X（15,604条目）生成故障预测修正数据集。</li><li><strong>训练资源</strong>：</li></ol>\n<p> - VLA模型：8块NVIDIA H100 GPU，批量大小256，学习率2×10⁻⁵，预测15步动作。</p>\n<p> - 监督器模型：Qwen2.5-vl 7B，bfloat16精度，批量大小64，学习率10⁻⁴，LoRA秩8。</p>\n<p> - 动作融合参数：α=0.1, λ=β=0.01，动作窗口大小N根据任务调整（2-11）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>仿真环境</strong>：</p>\n<p> - SIMPLER：测试WidowX（抓取、堆叠任务）和Google Robot（抓取、抽屉操作任务）。</p>\n<p> - LIBERO：测试Franka机器人，包括目标导向、空间配置、对象多样性和长视野任务。</p>\n<ol><li><strong>真实环境</strong>：小米机器人和ALOHA平台，执行抓取、堆叠等长视野任务。</li><li><strong>评估指标</strong>：</li></ol>\n<p> - 抓取成功率（Grasp Success Rate）。</p>\n<p> - 任务成功率（Task Success Rate）。</p>\n<p> - 平均抓取成功率（Avg. Grasp）和平均任务成功率（Avg. Success）。</p>\n<p> - 执行时间对比（关键帧1.766s vs 非关键帧0.176s）。</p>"
  },
  {
    "date": "2025-09-03",
    "title": "ANNIE: Be Careful of Your Robots",
    "link": "http://arxiv.org/abs/2509.03383",
    "summary_markdown": "## 论文研究单位\n\n中国科学院计算技术研究所、中国科学院自动化研究所、佐治亚理工学院、德克萨斯大学达拉斯分校\n## 论文概述\n\n论文研究了具身AI（EAI）系统中的对抗性安全攻击问题。针对EAI系统将视觉-语言-动作（VLA）模型整合到机器人中带来的新安全风险，提出了第一个系统性的对抗安全攻击研究框架。论文基于ISO/TS 15066标准，建立了从传统机器学习安全向物理AI安全转变的范式转移，并通过仿真和真实机器人实验验证了攻击效果。\n## 论文核心贡献点\n\n- **安全定义与分类**：提出了基于ISO/TS 15066标准的EAI系统安全定义，将安全违规分为critical、dangerous、risky三个等级\n- **基准测试数据集**：构建了Annie-Bench基准，包含9个安全关键场景和2400个视频动作序列，专注于评估EAI安全性能\n- **攻击框架**：开发了Annie-Attack任务感知对抗框架，通过攻击领导者模型将长期目标分解为帧级扰动\n- **实证验证**：在ACT和Baku两个代表性VLA模型上验证了攻击效果，各安全类别攻击成功率均超过50%\n## 论文方法描述\n\n- **安全标准定义**：基于人机协作ISO标准，定义了距离约束、速度约束和碰撞约束三个安全标准\n- **攻击框架设计**：Annie-Attack包含攻击领导者模型，将高层攻击目标转换为帧级动作扰动，使用PGD方法生成对抗样本\n- **行动空间设置**：使用末端执行器的4维动作表示（3维笛卡尔空间移动+1维夹爪状态）\n- **稀疏攻击策略**：提出自适应稀疏攻击，根据动作序列阶段动态调整攻击频率，在早期阶段使用高频率攻击，在后期阶段降低频率\n## 论文使用数据集和训练资源\n\n- **仿真环境**：基于ManiSkill仿真平台构建数据集\n- **硬件配置**：使用Franka Emika Panda 7自由度机械臂，配备2自由度夹爪和双摄像头系统\n- **数据集规模**：生成2400个视频动作序列，包含9个不同安全级别场景\n- **训练数据**：每个场景约240个演示序列用于训练专门的攻击领导者模型\n- **评估数据**：每个场景20个测试序列用于评估\n## 论文使用的评估环境和评估指标\n\n- **评估环境**：仿真环境中的9个测试场景，覆盖所有三个安全违规等级\n- **评估指标**：\n - 攻击成功率（ASR）：衡量安全约束规则被违反的百分比\n - 动作一致性（AC）：评估攻击是否导致动作序列中的突然变化\n - 动作偏差（AD）：测量对抗动作与原始动作分布的偏离程度\n - 任务成功率变化（TSRC）：攻击前后任务成功率的变化百分比\n- **基线模型**：在ACT和Baku两个VLA模型上进行评估\n- **真实实验**：在真实Franka Panda机器人上验证了攻击的物理影响",
    "summary_html": "<h2>论文研究单位</h2>\n\n<p>中国科学院计算技术研究所、中国科学院自动化研究所、佐治亚理工学院、德克萨斯大学达拉斯分校</p>\n<h2>论文概述</h2>\n\n<p>论文研究了具身AI（EAI）系统中的对抗性安全攻击问题。针对EAI系统将视觉-语言-动作（VLA）模型整合到机器人中带来的新安全风险，提出了第一个系统性的对抗安全攻击研究框架。论文基于ISO/TS 15066标准，建立了从传统机器学习安全向物理AI安全转变的范式转移，并通过仿真和真实机器人实验验证了攻击效果。</p>\n<h2>论文核心贡献点</h2>\n\n<ul><li><strong>安全定义与分类</strong>：提出了基于ISO/TS 15066标准的EAI系统安全定义，将安全违规分为critical、dangerous、risky三个等级</li><li><strong>基准测试数据集</strong>：构建了Annie-Bench基准，包含9个安全关键场景和2400个视频动作序列，专注于评估EAI安全性能</li><li><strong>攻击框架</strong>：开发了Annie-Attack任务感知对抗框架，通过攻击领导者模型将长期目标分解为帧级扰动</li><li><strong>实证验证</strong>：在ACT和Baku两个代表性VLA模型上验证了攻击效果，各安全类别攻击成功率均超过50%</li></ul>\n<h2>论文方法描述</h2>\n\n<ul><li><strong>安全标准定义</strong>：基于人机协作ISO标准，定义了距离约束、速度约束和碰撞约束三个安全标准</li><li><strong>攻击框架设计</strong>：Annie-Attack包含攻击领导者模型，将高层攻击目标转换为帧级动作扰动，使用PGD方法生成对抗样本</li><li><strong>行动空间设置</strong>：使用末端执行器的4维动作表示（3维笛卡尔空间移动+1维夹爪状态）</li><li><strong>稀疏攻击策略</strong>：提出自适应稀疏攻击，根据动作序列阶段动态调整攻击频率，在早期阶段使用高频率攻击，在后期阶段降低频率</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n\n<ul><li><strong>仿真环境</strong>：基于ManiSkill仿真平台构建数据集</li><li><strong>硬件配置</strong>：使用Franka Emika Panda 7自由度机械臂，配备2自由度夹爪和双摄像头系统</li><li><strong>数据集规模</strong>：生成2400个视频动作序列，包含9个不同安全级别场景</li><li><strong>训练数据</strong>：每个场景约240个演示序列用于训练专门的攻击领导者模型</li><li><strong>评估数据</strong>：每个场景20个测试序列用于评估</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n\n<ul><li><strong>评估环境</strong>：仿真环境中的9个测试场景，覆盖所有三个安全违规等级</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 攻击成功率（ASR）：衡量安全约束规则被违反的百分比</p>\n<p> - 动作一致性（AC）：评估攻击是否导致动作序列中的突然变化</p>\n<p> - 动作偏差（AD）：测量对抗动作与原始动作分布的偏离程度</p>\n<p> - 任务成功率变化（TSRC）：攻击前后任务成功率的变化百分比</p>\n<ul><li><strong>基线模型</strong>：在ACT和Baku两个VLA模型上进行评估</li><li><strong>真实实验</strong>：在真实Franka Panda机器人上验证了攻击的物理影响</li></ul>"
  },
  {
    "date": "2025-09-02",
    "title": "Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance",
    "link": "http://arxiv.org/abs/2509.02055",
    "summary_markdown": "### 论文研究单位\n- Institute of Artificial Intelligence, China Telecom\n- Tsinghua University\n- The Chinese University of Hong Kong, Shenzhen\n- Northwestern Polytechnical University\n### 论文概述\nVision-Language-Action（VLA）模型在预训练后能处理多任务机器人操作，但跨 embodiment（如单臂到双臂）和跨任务的适应常因动作分布不匹配而效率低下。本文提出Align-Then-stEer（ATE）框架，通过两阶段策略实现高效适应：先构建统一动作潜在空间桥接预训练和适应数据分布，再以分类器引导机制在潜在空间中引导VLA生成过程向目标域偏移。实验在仿真（RoboTwin 1.0、ManiSkill3）和真实双臂机器人上验证，ATE显著提升成功率，仿真平均增益9.8%，真实跨 embodiment场景增益达32%。\n### 论文核心贡献点\n- 提出动作分布对齐策略，利用反向KL散度的模式寻求特性构建统一潜在空间，将适应动作嵌入预训练潜在分布模式中。\n- 设计基于统一潜在空间的分类器引导机制，无缝集成至扩散/流匹配VLA训练目标，实现精确快速适应。\n- ATE为即插即用设计，模型无关且计算开销小，仅需训练两个轻量级VAEs。\n- 跨 embodiment、任务和架构（扩散/流匹配）验证有效性，突出适应效率提升。\n### 论文方法描述\nATE分两阶段：\n1. **统一动作潜在空间对齐（Stage 1）**：\n - 预训练阶段：在预训练动作数据（如DROID、Open X-Embodiment）上训练InfoVAE（V_pretrain），学习潜在分布。\n - 适应阶段：在适应数据（目标 embodiment）上训练另一个InfoVAE（V_adaptation），通过最小化反向KL散度 D_KL(q_ψ(z\\|ã) \\|\\|q_φ(z))，将适应潜在分布嵌入预训练潜在空间的特定模式，生成统一潜在空间Z。\n2. **分类器引导适应（Stage 2）**：\n - 定义引导函数g = -∇\\|\\|E_ψ(â_t:t+h^k) - E_ψ(a_t:t+h^0)\\|\\|²，度量中间动作与目标动作在潜在空间中的距离。\n - 对扩散模型，修改噪声预测为 ε̂ = ε_θ - √(1-ā_k)g，并嵌入训练目标 L(θ) = E[\\|\\|ε - ε_θ(...) + √(1-ā_k)·λ·g\\|\\|²]。\n - 对流匹配模型，修改速度场为 v̂_θ = v_θ + ((1-τ)/τ)·λ·g，并嵌入训练目标。\n 引导机制确保VLA输出保持于统一潜在空间内，在适配中保留预训练知识。\n### 论文使用数据集和训练资源\n- **数据集来源**：\n - 预训练：大规模机器人数据（如DROID、Open X-Embodiment子集、Kuka、ALOHA）。\n - 适应：仿真（RoboTwin 1.0含17任务，ManiSkill3含2任务）、真实（双RealMan 7-DoF机器人长期任务）。\n- **训练资源**：\n - InfoVAE训练分两步：Step 1用通用数据（3000 episodes）训练潜在结构（约12小时）；Step 2用域特定数据微调（RoboTwin/ManiSkill每任务50-100轨迹，真实每任务50轨迹，约0.5小时）。\n - 潜在维度512，训练含互信息项以增强表示。\n### 论文使用的评估环境和评估指标\n- **仿真环境**：\n - RoboTwin 1.0基准：17项单/双臂任务（工具调整、双瓶拾取）。\n - ManiSkill3基准：2项接触丰富单臂操作（推立方体、拾取立方体）。\n- **真实环境**：\n - 双RealMan 7-DoF双臂机器人：4项长期任务（插入、协调操作）和工具使用任务。\n- **评估指标**：\n - 成功率（Success Rate）：以任务完成百分比衡量。\n - 样本效率：比较达到性能阈值所需训练步数（如RDT基线90k步 vs ATE 70k步）。\n - 泛化能力：在光照、物体位置、视觉干扰下测试鲁棒性。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>Institute of Artificial Intelligence, China Telecom</li><li>Tsinghua University</li><li>The Chinese University of Hong Kong, Shenzhen</li><li>Northwestern Polytechnical University</li></ul>\n<h3>论文概述</h3>\n<p>Vision-Language-Action（VLA）模型在预训练后能处理多任务机器人操作，但跨 embodiment（如单臂到双臂）和跨任务的适应常因动作分布不匹配而效率低下。本文提出Align-Then-stEer（ATE）框架，通过两阶段策略实现高效适应：先构建统一动作潜在空间桥接预训练和适应数据分布，再以分类器引导机制在潜在空间中引导VLA生成过程向目标域偏移。实验在仿真（RoboTwin 1.0、ManiSkill3）和真实双臂机器人上验证，ATE显著提升成功率，仿真平均增益9.8%，真实跨 embodiment场景增益达32%。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出动作分布对齐策略，利用反向KL散度的模式寻求特性构建统一潜在空间，将适应动作嵌入预训练潜在分布模式中。</li><li>设计基于统一潜在空间的分类器引导机制，无缝集成至扩散/流匹配VLA训练目标，实现精确快速适应。</li><li>ATE为即插即用设计，模型无关且计算开销小，仅需训练两个轻量级VAEs。</li><li>跨 embodiment、任务和架构（扩散/流匹配）验证有效性，突出适应效率提升。</li></ul>\n<h3>论文方法描述</h3>\n<p>ATE分两阶段：</p>\n<p>1. <strong>统一动作潜在空间对齐（Stage 1）</strong>：</p>\n<p> - 预训练阶段：在预训练动作数据（如DROID、Open X-Embodiment）上训练InfoVAE（V_pretrain），学习潜在分布。</p>\n<p> - 适应阶段：在适应数据（目标 embodiment）上训练另一个InfoVAE（V_adaptation），通过最小化反向KL散度 D_KL(q_ψ(z\\|ã) \\|\\|q_φ(z))，将适应潜在分布嵌入预训练潜在空间的特定模式，生成统一潜在空间Z。</p>\n<p>2. <strong>分类器引导适应（Stage 2）</strong>：</p>\n<p> - 定义引导函数g = -∇\\|\\|E_ψ(â_t:t+h^k) - E_ψ(a_t:t+h^0)\\|\\|²，度量中间动作与目标动作在潜在空间中的距离。</p>\n<p> - 对扩散模型，修改噪声预测为 ε̂ = ε_θ - √(1-ā_k)g，并嵌入训练目标 L(θ) = E[\\|\\|ε - ε_θ(...) + √(1-ā_k)·λ·g\\|\\|²]。</p>\n<p> - 对流匹配模型，修改速度场为 v̂_θ = v_θ + ((1-τ)/τ)·λ·g，并嵌入训练目标。</p>\n<p> 引导机制确保VLA输出保持于统一潜在空间内，在适配中保留预训练知识。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集来源</strong>：</li></ul>\n<p> - 预训练：大规模机器人数据（如DROID、Open X-Embodiment子集、Kuka、ALOHA）。</p>\n<p> - 适应：仿真（RoboTwin 1.0含17任务，ManiSkill3含2任务）、真实（双RealMan 7-DoF机器人长期任务）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - InfoVAE训练分两步：Step 1用通用数据（3000 episodes）训练潜在结构（约12小时）；Step 2用域特定数据微调（RoboTwin/ManiSkill每任务50-100轨迹，真实每任务50轨迹，约0.5小时）。</p>\n<p> - 潜在维度512，训练含互信息项以增强表示。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>仿真环境</strong>：</li></ul>\n<p> - RoboTwin 1.0基准：17项单/双臂任务（工具调整、双瓶拾取）。</p>\n<p> - ManiSkill3基准：2项接触丰富单臂操作（推立方体、拾取立方体）。</p>\n<ul><li><strong>真实环境</strong>：</li></ul>\n<p> - 双RealMan 7-DoF双臂机器人：4项长期任务（插入、协调操作）和工具使用任务。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（Success Rate）：以任务完成百分比衡量。</p>\n<p> - 样本效率：比较达到性能阈值所需训练步数（如RDT基线90k步 vs ATE 70k步）。</p>\n<p> - 泛化能力：在光照、物体位置、视觉干扰下测试鲁棒性。</p>"
  },
  {
    "date": "2025-09-02",
    "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving",
    "link": "http://arxiv.org/abs/2509.01944",
    "summary_markdown": "# 论文研究单位\n\n阿里巴巴集团高德（AMAP）主导研究，昆士兰大学、兰州大学、凯斯西储大学合作参与\n# 论文概述\n\n论文提出AutoDrive-R²，这是一个用于自动驾驶的视觉-语言-动作(VLA)框架，通过思维链(CoT)处理和强化学习(RL)来增强自动驾驶系统的推理和自我反思能力。方法采用两阶段训练：首先构建包含四步逻辑推理和自我反思机制的nuScenesR²-6K数据集进行监督微调，然后基于Group Relative Policy Optimization (GRPO)算法结合物理约束奖励进行强化学习训练。\n# 论文核心贡献点\n\n- 提出AutoDrive-R²框架，实现从视觉信息和语言指令到轨迹规划的语义推理与自我反思\n- 构建nuScenesR²-6K数据集，采用四步逻辑链(可视化→计算→逻辑→反思)与自我反思机制\n- 基于GRPO强化学习的后训练方法，结合基于物理的奖励框架，包含空间对齐、车辆动力学和时间平滑性约束\n# 论文方法描述\n\n**阶段一监督微调(SFT)**：使用Qwen2.5-VL-7B模型在nuScenesR²-6K数据集上微调，数据集包含6k图像-轨迹对，每对包含前视图像、历史车辆状态和相应的思维链推理序列。\n\n**阶段二强化学习(RL)**：采用GRPO算法，设置最大完成长度4096 tokens，每输入采样6个候选响应。设计物理约束奖励函数：空间对齐奖励r_pos(位置误差)、车辆动力学奖励r_ste(转向角)和r_vel(速度)、时间平滑性奖励r_tem(控制信号平滑性)，各奖励权重相等。\n# 论文使用数据集和训练资源\n\n**训练数据**：nuScenesR²-6K数据集(6k样本对)，每个样本包含前视图像和3秒轨迹规划(0.5秒间隔)\n\n**评估数据**：nuScenes数据集(1000个城市驾驶场景，六个同步摄像头)和Waymo数据集(4021个驾驶片段，八个摄像头视图)\n\n**模型规模**：Qwen2.5-VL-3B和Qwen2.5-VL-7B两种规格\n**训练配置**：学习率5e-7，累积批大小1，GRPO最大长度4096，生成候选数6\n# 论文使用的评估环境和评估指标\n\n**评估环境**：在nuScenes和Waymo数据集上进行轨迹规划评估\n\n**评估指标**：L2误差(米)，计算预测轨迹与真实轨迹在1s、2s、3s未来时间点的距离，以及平均L2误差\n\n**性能表现**：AutoDrive-R²在nuScenes上达到0.19m平均L2误差，Waymo上达到0.20m，显著超越EMMA+、DriveVLM等现有方法，展现出强大的零样本泛化能力",
    "summary_html": "<h1>论文研究单位</h1>\n\n<p>阿里巴巴集团高德（AMAP）主导研究，昆士兰大学、兰州大学、凯斯西储大学合作参与</p>\n<h1>论文概述</h1>\n\n<p>论文提出AutoDrive-R²，这是一个用于自动驾驶的视觉-语言-动作(VLA)框架，通过思维链(CoT)处理和强化学习(RL)来增强自动驾驶系统的推理和自我反思能力。方法采用两阶段训练：首先构建包含四步逻辑推理和自我反思机制的nuScenesR²-6K数据集进行监督微调，然后基于Group Relative Policy Optimization (GRPO)算法结合物理约束奖励进行强化学习训练。</p>\n<h1>论文核心贡献点</h1>\n\n<ul><li>提出AutoDrive-R²框架，实现从视觉信息和语言指令到轨迹规划的语义推理与自我反思</li><li>构建nuScenesR²-6K数据集，采用四步逻辑链(可视化→计算→逻辑→反思)与自我反思机制</li><li>基于GRPO强化学习的后训练方法，结合基于物理的奖励框架，包含空间对齐、车辆动力学和时间平滑性约束</li></ul>\n<h1>论文方法描述</h1>\n\n<p><strong>阶段一监督微调(SFT)</strong>：使用Qwen2.5-VL-7B模型在nuScenesR²-6K数据集上微调，数据集包含6k图像-轨迹对，每对包含前视图像、历史车辆状态和相应的思维链推理序列。</p>\n\n<p><strong>阶段二强化学习(RL)</strong>：采用GRPO算法，设置最大完成长度4096 tokens，每输入采样6个候选响应。设计物理约束奖励函数：空间对齐奖励r_pos(位置误差)、车辆动力学奖励r_ste(转向角)和r_vel(速度)、时间平滑性奖励r_tem(控制信号平滑性)，各奖励权重相等。</p>\n<h1>论文使用数据集和训练资源</h1>\n\n<p><strong>训练数据</strong>：nuScenesR²-6K数据集(6k样本对)，每个样本包含前视图像和3秒轨迹规划(0.5秒间隔)</p>\n\n<p><strong>评估数据</strong>：nuScenes数据集(1000个城市驾驶场景，六个同步摄像头)和Waymo数据集(4021个驾驶片段，八个摄像头视图)</p>\n\n<p><strong>模型规模</strong>：Qwen2.5-VL-3B和Qwen2.5-VL-7B两种规格</p>\n<p><strong>训练配置</strong>：学习率5e-7，累积批大小1，GRPO最大长度4096，生成候选数6</p>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<p><strong>评估环境</strong>：在nuScenes和Waymo数据集上进行轨迹规划评估</p>\n\n<p><strong>评估指标</strong>：L2误差(米)，计算预测轨迹与真实轨迹在1s、2s、3s未来时间点的距离，以及平均L2误差</p>\n\n<p><strong>性能表现</strong>：AutoDrive-R²在nuScenes上达到0.19m平均L2误差，Waymo上达到0.20m，显著超越EMMA+、DriveVLM等现有方法，展现出强大的零样本泛化能力</p>"
  },
  {
    "date": "2025-08-31",
    "title": "OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving",
    "link": "http://arxiv.org/abs/2509.00789",
    "summary_markdown": "### 论文研究单位\n未在提供的HTML原文中明确提及。\n### 论文概述\nOmniReason是一个用于自动驾驶的时空引导视觉-语言-动作框架，旨在解决现有方法在动态驾驶场景中忽视时间维度的问题。该框架通过联合建模动态3D环境和决策过程，建立了鲁棒的时空推理能力。论文提出了OmniReason-Data数据集和OmniReason-Agent模型架构，数据集包含密集时空标注和自然语言解释，模型通过时空知识蒸馏实现可解释的决策。\n### 论文核心贡献点\n1. 提出OmniReason-nuScenes和OmniReason-Bench2Drive两个大规模视觉-语言-动作（VLA）数据集，包含密集时空标注和自然语言解释，通过减少幻觉的自动标注流水线生成。\n2. 设计OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器，通过时空知识蒸馏捕获因果推理模式，生成人类可解释的决策依据。\n3. 在开环规划和视觉问答（VQA）任务上实现最先进性能，显著提升安全性、舒适性和解释质量。\n### 论文方法描述\n1. **OmniReason-Data构建**：\n - 基于nuScenes和Bench2Drive，通过规则和原则模板整合人类先验知识，引导Qwen2.5VL 72B模型生成场景感知描述和因果推理链。\n - 流水线包括场景空间标注、人类先验知识引导和MLLM时序推理，确保物理合理性和时间连贯性。\n2. **OmniReason-Agent架构**：\n - 视觉主干网络通过分层编码将多视图输入转换为时空令牌，稀疏时间记忆模块使用记忆压缩查询聚合长期上下文。\n - VLM推理核心处理历史驾驶状态和语言指令，通过轻量适配器增强冻结VLM。\n - 运动感知归一化模块动态适应对象状态，混合注意力机制传播对象查询，跨模态聚合融合图像特征。\n3. **训练目标**：\n - 联合优化3D检测和场景理解，检测损失包括分类（Focal Loss）和回归（L1），车道分析损失类似。\n - LLM使用自回归交叉熵损失，总损失为感知和语言损失的加权和。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - OmniReason-nuScenes和OmniReason-Bench2Drive，基于nuScenes和Bench2Drive构建，包含多视图视频、对象标注和轨迹数据。\n - 涵盖环境描述、动态/静态对象、因果推理和动作标注，支持VQA和开环规划任务。\n2. **训练资源**：\n - 128块NVIDIA H20 GPU（96GB内存）。\n - 视觉编码器使用EVA-02-L（CLIP知识蒸馏预训练），基础模型为LLaVA v1.5。\n - 微调阶段使用AdamW优化器，批大小16，学习率分层：投影模块4e-4，视觉编码器和LLM为2e-5。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 开环规划任务在nuScenes基准测试，VQA任务在自定义OmniReason数据集评估。\n - 实施细节中提及使用标准训练配置和余弦退火调度。\n2. **评估指标**：\n - **开环规划**：L2位移误差（1/2/3秒）、平均碰撞率（CR）、违规率（IR）。\n - **VQA**：CIDEr、BLEU-1/4、METEOR、ROUGE-L、Precision、Recall，衡量语言理解和多模态对齐。\n - 消融实验中分析语言组件和记忆模块对BL-1、L2、CR、IR的影响。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>未在提供的HTML原文中明确提及。</p>\n<h3>论文概述</h3>\n<p>OmniReason是一个用于自动驾驶的时空引导视觉-语言-动作框架，旨在解决现有方法在动态驾驶场景中忽视时间维度的问题。该框架通过联合建模动态3D环境和决策过程，建立了鲁棒的时空推理能力。论文提出了OmniReason-Data数据集和OmniReason-Agent模型架构，数据集包含密集时空标注和自然语言解释，模型通过时空知识蒸馏实现可解释的决策。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出OmniReason-nuScenes和OmniReason-Bench2Drive两个大规模视觉-语言-动作（VLA）数据集，包含密集时空标注和自然语言解释，通过减少幻觉的自动标注流水线生成。</li><li>设计OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器，通过时空知识蒸馏捕获因果推理模式，生成人类可解释的决策依据。</li><li>在开环规划和视觉问答（VQA）任务上实现最先进性能，显著提升安全性、舒适性和解释质量。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>OmniReason-Data构建</strong>：</p>\n<p> - 基于nuScenes和Bench2Drive，通过规则和原则模板整合人类先验知识，引导Qwen2.5VL 72B模型生成场景感知描述和因果推理链。</p>\n<p> - 流水线包括场景空间标注、人类先验知识引导和MLLM时序推理，确保物理合理性和时间连贯性。</p>\n<p>2. <strong>OmniReason-Agent架构</strong>：</p>\n<p> - 视觉主干网络通过分层编码将多视图输入转换为时空令牌，稀疏时间记忆模块使用记忆压缩查询聚合长期上下文。</p>\n<p> - VLM推理核心处理历史驾驶状态和语言指令，通过轻量适配器增强冻结VLM。</p>\n<p> - 运动感知归一化模块动态适应对象状态，混合注意力机制传播对象查询，跨模态聚合融合图像特征。</p>\n<p>3. <strong>训练目标</strong>：</p>\n<p> - 联合优化3D检测和场景理解，检测损失包括分类（Focal Loss）和回归（L1），车道分析损失类似。</p>\n<p> - LLM使用自回归交叉熵损失，总损失为感知和语言损失的加权和。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：</p>\n<p> - OmniReason-nuScenes和OmniReason-Bench2Drive，基于nuScenes和Bench2Drive构建，包含多视图视频、对象标注和轨迹数据。</p>\n<p> - 涵盖环境描述、动态/静态对象、因果推理和动作标注，支持VQA和开环规划任务。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - 128块NVIDIA H20 GPU（96GB内存）。</p>\n<p> - 视觉编码器使用EVA-02-L（CLIP知识蒸馏预训练），基础模型为LLaVA v1.5。</p>\n<p> - 微调阶段使用AdamW优化器，批大小16，学习率分层：投影模块4e-4，视觉编码器和LLM为2e-5。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - 开环规划任务在nuScenes基准测试，VQA任务在自定义OmniReason数据集评估。</p>\n<p> - 实施细节中提及使用标准训练配置和余弦退火调度。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - <strong>开环规划</strong>：L2位移误差（1/2/3秒）、平均碰撞率（CR）、违规率（IR）。</p>\n<p> - <strong>VQA</strong>：CIDEr、BLEU-1/4、METEOR、ROUGE-L、Precision、Recall，衡量语言理解和多模态对齐。</p>\n<p> - 消融实验中分析语言组件和记忆模块对BL-1、L2、CR、IR的影响。</p>"
  },
  {
    "date": "2025-08-30",
    "title": "Galaxea Open-World Dataset and G0 Dual-System VLA Model",
    "link": "http://arxiv.org/abs/2509.00576",
    "summary_markdown": "# 论文研究单位\nGalaxea Team（https://opengalaxea.github.io/G0/）\n# 论文概述\n论文提出了Galaxea Open-World Dataset，一个在真实人类生活和工作环境中收集的大规模、多样化机器人行为数据集。基于该数据集，论文介绍了G0双系统框架，结合了用于多模态规划的视觉语言模型（VLM）和用于精细执行的视觉语言动作（VLA）模型。G0采用三阶段训练课程：跨具身预训练、单具身预训练和任务特定后训练。论文通过涵盖桌面操作、小样本学习和长期移动操作的综合基准测试证明了方法的有效性。\n# 论文核心贡献点\n1. 构建了Galaxea Open-World Dataset，包含100K条演示轨迹，涵盖150个任务类别、50个真实世界场景、1600+独特对象和58种操作技能\n2. 提出了G0双系统框架，结合VLM（系统2）进行规划和VLA（系统1）执行动作\n3. 设计了有效的三阶段训练策略，验证了单具身预训练在模型性能提升中的关键作用\n4. 建立了涵盖多种任务类型的综合基准测试\n# 论文方法描述\n**G0双系统架构**\n- 系统1（G0-VLA）：端到端视觉语言动作模型，负责感知环境、解释子任务指令并执行动作\n- 系统2（G0-VLM）：处理高层自然语言任务指令、理解场景并为系统1规划子任务指令\n\n**G0-VLA三阶段训练**\n- 阶段1预训练：仅训练VLM组件，使用FAST tokenizer将连续动作块转换为离散索引序列，采用标准交叉熵损失进行自回归训练\n- 阶段2预训练：在标记的Galaxea数据集上训练VLA，包含预训练VLM和新初始化的动作专家，采用flow-matching损失训练连续动作生成\n- 后训练：使用最多100条轨迹对下游任务进行微调，验证预训练模型的泛化能力\n\n**G0-VLM训练**\n- 基于Qwen2.5-VL进行指令调优，使用数据集中的人工注释子任务和合成的人类风格高层指令\n- 采样关键帧并输入历史图像观察和机器人动作，支持长期上下文任务规划\n# 论文使用数据集和训练资源\n**Galaxea Open-World Dataset**\n- 规模：100K演示轨迹，500小时高质量数据\n- 任务覆盖：150个任务类别，50个不同真实世界场景\n- 对象多样性：1600+独特对象，58种操作技能\n- 具身一致性：使用统一的Galaxea R1 Lite平台（23-DoF，包括双臂、3-DoF躯干、6-DoF全向基座）\n\n**训练数据组成**\n- 阶段1预训练：约1000小时OXE轨迹 + 500小时Galaxea数据集（仅高层任务描述）+ 200小时内部数据\n- 阶段2预训练：Galaxea Open-World Dataset的完整标注数据\n- 支持数据：关键帧采样，k帧历史观察，机器人状态信息\n# 论文使用的评估环境和评估指标\n**评估基准任务**\n- Table bussing：整理杂乱桌面，测试精确抓取放置和双臂协调操作\n- Microwave operation：操作微波炉，评估与家电交互和多步操作序列\n- Bed making：整理床铺，测试全身协调控制能力\n- Blocks stacking：搭建积木形成特定词汇，验证语言跟随和精确操作\n\n**评估指标**\n- Progress score：根据任务完成步骤计算分数（Table bussing: 6分，Microwave operation: 5分，Bed making: 4分，Blocks stacking: 6分）\n- 每个测试运行10次取平均成绩\n- 小样本设置：每个任务仅使用20条轨迹进行微调，训练10轮\n\n**评估环境**\n- 真实世界环境：住宅、餐饮、零售和办公空间\n- 物理硬件：Galaxea R1 Lite移动双臂机器人\n- 评估设置：标准化提示，包含任务特定指令、原子动作选项和输出示例",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Galaxea Team（https://opengalaxea.github.io/G0/）</p>\n<h1>论文概述</h1>\n<p>论文提出了Galaxea Open-World Dataset，一个在真实人类生活和工作环境中收集的大规模、多样化机器人行为数据集。基于该数据集，论文介绍了G0双系统框架，结合了用于多模态规划的视觉语言模型（VLM）和用于精细执行的视觉语言动作（VLA）模型。G0采用三阶段训练课程：跨具身预训练、单具身预训练和任务特定后训练。论文通过涵盖桌面操作、小样本学习和长期移动操作的综合基准测试证明了方法的有效性。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>构建了Galaxea Open-World Dataset，包含100K条演示轨迹，涵盖150个任务类别、50个真实世界场景、1600+独特对象和58种操作技能</li><li>提出了G0双系统框架，结合VLM（系统2）进行规划和VLA（系统1）执行动作</li><li>设计了有效的三阶段训练策略，验证了单具身预训练在模型性能提升中的关键作用</li><li>建立了涵盖多种任务类型的综合基准测试</li></ol>\n<h1>论文方法描述</h1>\n<p><strong>G0双系统架构</strong></p>\n<ul><li>系统1（G0-VLA）：端到端视觉语言动作模型，负责感知环境、解释子任务指令并执行动作</li><li>系统2（G0-VLM）：处理高层自然语言任务指令、理解场景并为系统1规划子任务指令</li></ul>\n\n<p><strong>G0-VLA三阶段训练</strong></p>\n<ul><li>阶段1预训练：仅训练VLM组件，使用FAST tokenizer将连续动作块转换为离散索引序列，采用标准交叉熵损失进行自回归训练</li><li>阶段2预训练：在标记的Galaxea数据集上训练VLA，包含预训练VLM和新初始化的动作专家，采用flow-matching损失训练连续动作生成</li><li>后训练：使用最多100条轨迹对下游任务进行微调，验证预训练模型的泛化能力</li></ul>\n\n<p><strong>G0-VLM训练</strong></p>\n<ul><li>基于Qwen2.5-VL进行指令调优，使用数据集中的人工注释子任务和合成的人类风格高层指令</li><li>采样关键帧并输入历史图像观察和机器人动作，支持长期上下文任务规划</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<p><strong>Galaxea Open-World Dataset</strong></p>\n<ul><li>规模：100K演示轨迹，500小时高质量数据</li><li>任务覆盖：150个任务类别，50个不同真实世界场景</li><li>对象多样性：1600+独特对象，58种操作技能</li><li>具身一致性：使用统一的Galaxea R1 Lite平台（23-DoF，包括双臂、3-DoF躯干、6-DoF全向基座）</li></ul>\n\n<p><strong>训练数据组成</strong></p>\n<ul><li>阶段1预训练：约1000小时OXE轨迹 + 500小时Galaxea数据集（仅高层任务描述）+ 200小时内部数据</li><li>阶段2预训练：Galaxea Open-World Dataset的完整标注数据</li><li>支持数据：关键帧采样，k帧历史观察，机器人状态信息</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<p><strong>评估基准任务</strong></p>\n<ul><li>Table bussing：整理杂乱桌面，测试精确抓取放置和双臂协调操作</li><li>Microwave operation：操作微波炉，评估与家电交互和多步操作序列</li><li>Bed making：整理床铺，测试全身协调控制能力</li><li>Blocks stacking：搭建积木形成特定词汇，验证语言跟随和精确操作</li></ul>\n\n<p><strong>评估指标</strong></p>\n<ul><li>Progress score：根据任务完成步骤计算分数（Table bussing: 6分，Microwave operation: 5分，Bed making: 4分，Blocks stacking: 6分）</li><li>每个测试运行10次取平均成绩</li><li>小样本设置：每个任务仅使用20条轨迹进行微调，训练10轮</li></ul>\n\n<p><strong>评估环境</strong></p>\n<ul><li>真实世界环境：住宅、餐饮、零售和办公空间</li><li>物理硬件：Galaxea R1 Lite移动双臂机器人</li><li>评估设置：标准化提示，包含任务特定指令、原子动作选项和输出示例</li></ul>"
  },
  {
    "date": "2025-08-30",
    "title": "Mechanistic interpretability for steering vision-language-action models",
    "link": "http://arxiv.org/abs/2509.00328",
    "summary_markdown": "### 论文研究单位\n加州大学伯克利分校电气工程与计算机科学系（University of California, Berkeley, Department of Electrical Engineering and Computer Sciences）\n### 论文概述\nVLA模型结合视觉和语言信息实现机器人动作控制，但缺乏类似传统机器人管道的可解释性机制。本研究将机械可解释性技术应用于VLA模型，通过分析模型内部激活单元的语义含义，实现无需微调的实时行为引导。核心创新在于提取FFN层的语义价值向量（value vectors），识别与动作选择因果相关的控制方向（如速度、方向），并将其作为实时控制接口激活。\n### 论文核心贡献点\n1. **语义保留发现**：VLA模型在训练中保留大量预训练语义概念（<25% FFN神经元用于动作预测，其余维持语义结构）\n2. **因果关联验证**：验证内部概念与动作的因果关系（如\"慢\"概念直接导致末端执行器缓慢移动）\n3. **实时控制接口**：首次实现基于内部表征的零样本行为控制方法，突破微调/环境交互依赖\n### 论文方法描述\n1. **价值向量提取**\n - 分析FFN输出层权重矩阵`Wθ`，提取独立于输入的固定值向量`wθ(i)`（式1-2）\n - 将值向量投影至标记空间，通过标记概率分布赋予语义含义\n\n2. **概念激活引导**\n - 通过kNN聚类或人工选择识别语义对齐的神经元簇`𝒮`（如\"up\"、\"slow\"）\n - 在推理时覆盖该簇的激活值为固定标量`α`（式3-4）\n - 剩余神经元保持原始激活，生成控制残差`Δx`影响最终动作标记分布\n\n3. **跨框架实现**\n - PyTorch框架：在OpenVLA的FFN下投影层应用前向钩子\n - JAX框架：修改π₀的FFN计算图插入引导算子`I𝒮^α`\n### 论文使用数据集和训练资源\n- **基础预训练数据**：Open X-Embodiment跨平台机器人数据集（OpenVLA/π₀预训练）\n- **模拟评估**：LIBERO-Long长程操作基准（10项任务，OpenVLA 7B模型）\n- **实物实验**：DROID平台数据用于π₀-FAST微调（LoRA方法，5000步）\n- **硬件资源**：\n - 模拟：NVIDIA H100 GPU\n - 实物：NVIDIA A4500 GPU + UR5机械臂（配Robotiq 2F-140夹爪）\n### 论文使用的评估环境和评估指标\n#### 模拟实验（OpenVLA/LIBERO）\n- **环境**：10个长程操作任务（抓取、放置、器具操作等）\n- **干预参数**：对比\"快/慢\"与\"上\"概念簇，测试不同层深度注入效果\n- **指标**：\n - 末端执行器位移变化率（平均提升27.73%，最大148.54%）\n - 统计显著性（配对t检验，p<0.001）\n - 效应大小（Cohen's d范围：-0.091至-1.419）\n#### 实物实验（π₀-FAST/UR5）\n- **任务场景**：\n - 低/高运输：玩具企鹅提升高度控制（75演示轨迹）\n - 慢/快运输：玩具海豹速度控制（120演示轨迹）\n- **基线对比**：无干预/提示词修改/随机向量干预\n- **指标**：\n - 低高组：末端最大高度分布（箱线图统计）\n - 慢快组：平均位移/累积位移时间序列\n- **关键发现**：\n - 低/慢干预显著降低轨迹幅度（p<0.05）\n - 高/快干预效果接近基线（模型已内建高速行为）\n - 语义引导优于随机干预与提示词修改\n\n> **注**：语义方向在不同任务/模型间存在迁移性差异，概念簇（如\"小心\"vs\"卡顿\"）可能引发混淆行为。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>加州大学伯克利分校电气工程与计算机科学系（University of California, Berkeley, Department of Electrical Engineering and Computer Sciences）</p>\n<h3>论文概述</h3>\n<p>VLA模型结合视觉和语言信息实现机器人动作控制，但缺乏类似传统机器人管道的可解释性机制。本研究将机械可解释性技术应用于VLA模型，通过分析模型内部激活单元的语义含义，实现无需微调的实时行为引导。核心创新在于提取FFN层的语义价值向量（value vectors），识别与动作选择因果相关的控制方向（如速度、方向），并将其作为实时控制接口激活。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>语义保留发现</strong>：VLA模型在训练中保留大量预训练语义概念（<25% FFN神经元用于动作预测，其余维持语义结构）</li><li><strong>因果关联验证</strong>：验证内部概念与动作的因果关系（如\"慢\"概念直接导致末端执行器缓慢移动）</li><li><strong>实时控制接口</strong>：首次实现基于内部表征的零样本行为控制方法，突破微调/环境交互依赖</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>价值向量提取</strong></p>\n<p> - 分析FFN输出层权重矩阵<code>Wθ</code>，提取独立于输入的固定值向量<code>wθ(i)</code>（式1-2）</p>\n<p> - 将值向量投影至标记空间，通过标记概率分布赋予语义含义</p>\n\n<p>2. <strong>概念激活引导</strong></p>\n<p> - 通过kNN聚类或人工选择识别语义对齐的神经元簇<code>𝒮</code>（如\"up\"、\"slow\"）</p>\n<p> - 在推理时覆盖该簇的激活值为固定标量<code>α</code>（式3-4）</p>\n<p> - 剩余神经元保持原始激活，生成控制残差<code>Δx</code>影响最终动作标记分布</p>\n\n<p>3. <strong>跨框架实现</strong></p>\n<p> - PyTorch框架：在OpenVLA的FFN下投影层应用前向钩子</p>\n<p> - JAX框架：修改π₀的FFN计算图插入引导算子<code>I𝒮^α</code></p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>基础预训练数据</strong>：Open X-Embodiment跨平台机器人数据集（OpenVLA/π₀预训练）</li><li><strong>模拟评估</strong>：LIBERO-Long长程操作基准（10项任务，OpenVLA 7B模型）</li><li><strong>实物实验</strong>：DROID平台数据用于π₀-FAST微调（LoRA方法，5000步）</li><li><strong>硬件资源</strong>：</li></ul>\n<p> - 模拟：NVIDIA H100 GPU</p>\n<p> - 实物：NVIDIA A4500 GPU + UR5机械臂（配Robotiq 2F-140夹爪）</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<h4>模拟实验（OpenVLA/LIBERO）</h4>\n<ul><li><strong>环境</strong>：10个长程操作任务（抓取、放置、器具操作等）</li><li><strong>干预参数</strong>：对比\"快/慢\"与\"上\"概念簇，测试不同层深度注入效果</li><li><strong>指标</strong>：</li></ul>\n<p> - 末端执行器位移变化率（平均提升27.73%，最大148.54%）</p>\n<p> - 统计显著性（配对t检验，p<0.001）</p>\n<p> - 效应大小（Cohen's d范围：-0.091至-1.419）</p>\n<h4>实物实验（π₀-FAST/UR5）</h4>\n<ul><li><strong>任务场景</strong>：</li></ul>\n<p> - 低/高运输：玩具企鹅提升高度控制（75演示轨迹）</p>\n<p> - 慢/快运输：玩具海豹速度控制（120演示轨迹）</p>\n<ul><li><strong>基线对比</strong>：无干预/提示词修改/随机向量干预</li><li><strong>指标</strong>：</li></ul>\n<p> - 低高组：末端最大高度分布（箱线图统计）</p>\n<p> - 慢快组：平均位移/累积位移时间序列</p>\n<ul><li><strong>关键发现</strong>：</li></ul>\n<p> - 低/慢干预显著降低轨迹幅度（p<0.05）</p>\n<p> - 高/快干预效果接近基线（模型已内建高速行为）</p>\n<p> - 语义引导优于随机干预与提示词修改</p>\n\n<p>> <strong>注</strong>：语义方向在不同任务/模型间存在迁移性差异，概念簇（如\"小心\"vs\"卡顿\"）可能引发混淆行为。</p>"
  },
  {
    "date": "2025-08-28",
    "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control",
    "link": "http://arxiv.org/abs/2508.21112",
    "summary_markdown": "# EO-1: 通用机器人控制中的交错视觉-文本-动作预训练\n## 论文研究单位\nShanghai AI Laboratory, Fudan University, AgiBot, Northwestern Polytechnical University\n## 论文概述\nEO-1是上海人工智能实验室等机构提出的统一具身基础模型，通过交错视觉-文本-动作预训练实现通用机器人控制。该研究针对当前视觉-语言-动作(VLA)模型在开放世界泛化和交错推理能力方面的不足，提出了统一的模型架构和大规模多模态数据集，在具身推理和机器人控制任务中展现出显著优势。\n## 论文核心贡献点\n1. **统一架构**：EO-1采用单一统一解码器转换器，整合离散自回归解码与连续流匹配去噪，无需额外动作特定参数即可实现跨模态知识传递\n2. **交错式具身数据集**：构建包含150万样本的EO-Data1.5M数据集，专门针对交错视觉-文本-动作理解和学习\n3. **真实世界泛化能力**：在ERQA、LIBERO、SimplerEnv等多个开源基准测试中超越现有模型，展现出强大的开放世界理解和控制能力\n## 论文方法描述\nEO-1基于预训练VLM构建统一解码器转换器架构，处理交错多模态输入序列：\n- **输入处理**：文本标记器、视觉编码器、机器人状态投影器和动作去噪投影器将不同模态统一映射到R^d嵌入空间\n- **共享骨干**：初始化自Qwen2.5-VL的转换器骨干，通过因果注意力处理整个交错序列\n- **输出机制**：语言头用于文本解码，流头用于连续动作去噪生成\n- **交错修正采样**：针对混合模态生成中的因果关系破坏问题，提出采样策略确保动作生成段的正确训练\n- **训练目标**：结合自回归的下一个token预测和流匹配的向量场预测损失\n## 论文使用数据集和训练资源\n**数据规模**：\n- 网络多模态数据：570万样本，71亿tokens\n- 机器人控制数据：120万 эпизод，1273亿tokens\n- 交错具身数据：EO-Data1.5M，10亿tokens\n\n**训练资源**：\n- 五个epoch训练，使用Flash-Attention变长打包\n- 批量大小为1，平均序列长度16384\n- 主干学习率5×10^-5，视觉编码器1×10^-5\n- DeepSpeed ZeRO-1优化器\n- 推理时仅需6GB GPU内存\n## 论文使用的评估环境和评估指标\n**评估环境**：\n- **具身推理基准**：RoboVQA、ERQA、EO-Bench\n- **机器人控制基准**：LIBERO、SimplerEnv\n- **真实世界评估**：Franka Panda、WidowX 250S、Agibot G-1等多种机器人平台\n\n**评估指标**：\n- **具身推理**：BLEU-4分数(RoboVQA)，准确率(ERQA)，多选VQA准确率(EO-Bench)\n- **机器人控制**：成功率(SR)，在Google Robot基准中采用匹配和聚合两种评估方式\n- **多维度评估**：空间理解、物理常识、任务推理、状态估计四个维度共648个QA对\n\nEO-1在所有基准测试中均展现出优异性能，平均成功率达98.2%，显著超越OpenVLA、π₀等现有开源模型。",
    "summary_html": "<h1>EO-1: 通用机器人控制中的交错视觉-文本-动作预训练</h1>\n<h2>论文研究单位</h2>\n<p>Shanghai AI Laboratory, Fudan University, AgiBot, Northwestern Polytechnical University</p>\n<h2>论文概述</h2>\n<p>EO-1是上海人工智能实验室等机构提出的统一具身基础模型，通过交错视觉-文本-动作预训练实现通用机器人控制。该研究针对当前视觉-语言-动作(VLA)模型在开放世界泛化和交错推理能力方面的不足，提出了统一的模型架构和大规模多模态数据集，在具身推理和机器人控制任务中展现出显著优势。</p>\n<h2>论文核心贡献点</h2>\n<ol><li><strong>统一架构</strong>：EO-1采用单一统一解码器转换器，整合离散自回归解码与连续流匹配去噪，无需额外动作特定参数即可实现跨模态知识传递</li><li><strong>交错式具身数据集</strong>：构建包含150万样本的EO-Data1.5M数据集，专门针对交错视觉-文本-动作理解和学习</li><li><strong>真实世界泛化能力</strong>：在ERQA、LIBERO、SimplerEnv等多个开源基准测试中超越现有模型，展现出强大的开放世界理解和控制能力</li></ol>\n<h2>论文方法描述</h2>\n<p>EO-1基于预训练VLM构建统一解码器转换器架构，处理交错多模态输入序列：</p>\n<ul><li><strong>输入处理</strong>：文本标记器、视觉编码器、机器人状态投影器和动作去噪投影器将不同模态统一映射到R^d嵌入空间</li><li><strong>共享骨干</strong>：初始化自Qwen2.5-VL的转换器骨干，通过因果注意力处理整个交错序列</li><li><strong>输出机制</strong>：语言头用于文本解码，流头用于连续动作去噪生成</li><li><strong>交错修正采样</strong>：针对混合模态生成中的因果关系破坏问题，提出采样策略确保动作生成段的正确训练</li><li><strong>训练目标</strong>：结合自回归的下一个token预测和流匹配的向量场预测损失</li></ul>\n<h2>论文使用数据集和训练资源</h2>\n<p><strong>数据规模</strong>：</p>\n<ul><li>网络多模态数据：570万样本，71亿tokens</li><li>机器人控制数据：120万 эпизод，1273亿tokens</li><li>交错具身数据：EO-Data1.5M，10亿tokens</li></ul>\n\n<p><strong>训练资源</strong>：</p>\n<ul><li>五个epoch训练，使用Flash-Attention变长打包</li><li>批量大小为1，平均序列长度16384</li><li>主干学习率5×10^-5，视觉编码器1×10^-5</li><li>DeepSpeed ZeRO-1优化器</li><li>推理时仅需6GB GPU内存</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<p><strong>评估环境</strong>：</p>\n<ul><li><strong>具身推理基准</strong>：RoboVQA、ERQA、EO-Bench</li><li><strong>机器人控制基准</strong>：LIBERO、SimplerEnv</li><li><strong>真实世界评估</strong>：Franka Panda、WidowX 250S、Agibot G-1等多种机器人平台</li></ul>\n\n<p><strong>评估指标</strong>：</p>\n<ul><li><strong>具身推理</strong>：BLEU-4分数(RoboVQA)，准确率(ERQA)，多选VQA准确率(EO-Bench)</li><li><strong>机器人控制</strong>：成功率(SR)，在Google Robot基准中采用匹配和聚合两种评估方式</li><li><strong>多维度评估</strong>：空间理解、物理常识、任务推理、状态估计四个维度共648个QA对</li></ul>\n\n<p>EO-1在所有基准测试中均展现出优异性能，平均成功率达98.2%，显著超越OpenVLA、π₀等现有开源模型。</p>"
  },
  {
    "date": "2025-08-28",
    "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
    "link": "http://arxiv.org/abs/2508.21046",
    "summary_markdown": "## 论文研究单位\nSchool of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\n## 论文概述\nCogVLA是一个受人类多模态协调启发的视觉-语言-动作模型，通过指令驱动的路由和稀疏化技术，在保持高性能的同时显著降低计算成本。该模型采用三阶段渐进架构，模拟人类的视觉注意力系统（VAS）、补充运动区（SMA）和前运动皮质（PMC），分别用于感知聚焦、语义意图过滤和动作规划。在LIBERO仿真基准和真实世界机器人任务上，CogVLA实现了97.4%和70.0%的任务成功率，相比OpenVLA训练成本降低2.5倍、推理延迟减少2.8倍，同时在多个维度上优于现有高效VLA方法。\n## 论文核心贡献点\n- 提出认知对齐的CogVLA框架，通过EFA-Routing、LFP-Routing和CAtten模拟人类多模态协调机制，实现从感知到控制的端到端优化。\n- 开发EFA-Routing和LFP-Routing，实现基于指令的视觉稀疏化，有效减少视觉令牌数量（Stage 1压缩至25%，Stage 2修剪50%）。\n- 设计V-L-A Coupled Attention（CAtten），融合因果视觉-语言注意和双向动作并行解码，确保语义一致性和时间连贯性。\n- 在LIBERO和ALOHA基准上验证了优越性能和效率，为可扩展机器人控制提供了解决方案。\n## 论文方法描述\nCogVLA采用三阶段渐进设计：\n1. **Encoder-FiLM based Aggregation Routing (EFA-Routing)**：在视觉编码阶段，通过指令调制的FiLM模块动态聚合视觉令牌到聚合令牌（压缩至原始输入25%），并使用门控机制融合多个编码器分支（如SigLIP和DINOv2）。\n2. **LLM-FiLM based Pruning Routing (LFP-Routing)**：在语言模型阶段，通过任务引导的剪枝路由器基于指令相关性筛选视觉令牌（减少50%令牌），保留语义关键信息以降低计算开销。\n3. **V-L-A Coupled Attention (CAtten)**：采用混合注意机制，结合因果视觉-语言注意和双向动作注意，支持动作块的并行解码，确保跨模态逻辑一致性和时间连贯性。\n整个流程支持指令驱动的端到端优化，通过稀疏化视觉输入和并行解码提高效率。\n## 论文使用数据集和训练资源\n- **数据集**：\n - 仿真环境：LIBERO基准，包含四个任务套件（空间、物体、目标、长时序），每个套件10个任务50个演示。\n - 真实世界：Cobot Agilex ALOHA平台的三个长时序任务（物体放置、抽屉操作、T恤折叠），分别45、45、30个演示。\n- **训练资源**：\n - 使用4×A800 GPUs（80GB显存）进行训练和评估，得益于指令驱动的稀疏化策略。\n## 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真环境：LIBERO基准测试，模拟各种指令遵循任务。\n - 真实世界：Cobot Agilex ALOHA平台，进行真实机器人操作评估。\n- **评估指标**：\n - 任务成功率（Success Rate）：以百分比衡量任务完成能力。\n - 效率指标：推理时间（秒）、吞吐量（Hz）、FLOPs（计算量）、训练成本（小时/10k步）。\n - 性能指标：在LIBERO套件上分空间、物体、目标、长时序子任务报告成功率；真实世界任务中统计子任务成功率（如抽屉操作的三步骤）。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen</p>\n<h2>论文概述</h2>\n<p>CogVLA是一个受人类多模态协调启发的视觉-语言-动作模型，通过指令驱动的路由和稀疏化技术，在保持高性能的同时显著降低计算成本。该模型采用三阶段渐进架构，模拟人类的视觉注意力系统（VAS）、补充运动区（SMA）和前运动皮质（PMC），分别用于感知聚焦、语义意图过滤和动作规划。在LIBERO仿真基准和真实世界机器人任务上，CogVLA实现了97.4%和70.0%的任务成功率，相比OpenVLA训练成本降低2.5倍、推理延迟减少2.8倍，同时在多个维度上优于现有高效VLA方法。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出认知对齐的CogVLA框架，通过EFA-Routing、LFP-Routing和CAtten模拟人类多模态协调机制，实现从感知到控制的端到端优化。</li><li>开发EFA-Routing和LFP-Routing，实现基于指令的视觉稀疏化，有效减少视觉令牌数量（Stage 1压缩至25%，Stage 2修剪50%）。</li><li>设计V-L-A Coupled Attention（CAtten），融合因果视觉-语言注意和双向动作并行解码，确保语义一致性和时间连贯性。</li><li>在LIBERO和ALOHA基准上验证了优越性能和效率，为可扩展机器人控制提供了解决方案。</li></ul>\n<h2>论文方法描述</h2>\n<p>CogVLA采用三阶段渐进设计：</p>\n<ol><li><strong>Encoder-FiLM based Aggregation Routing (EFA-Routing)</strong>：在视觉编码阶段，通过指令调制的FiLM模块动态聚合视觉令牌到聚合令牌（压缩至原始输入25%），并使用门控机制融合多个编码器分支（如SigLIP和DINOv2）。</li><li><strong>LLM-FiLM based Pruning Routing (LFP-Routing)</strong>：在语言模型阶段，通过任务引导的剪枝路由器基于指令相关性筛选视觉令牌（减少50%令牌），保留语义关键信息以降低计算开销。</li><li><strong>V-L-A Coupled Attention (CAtten)</strong>：采用混合注意机制，结合因果视觉-语言注意和双向动作注意，支持动作块的并行解码，确保跨模态逻辑一致性和时间连贯性。</li></ol>\n<p>整个流程支持指令驱动的端到端优化，通过稀疏化视觉输入和并行解码提高效率。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 仿真环境：LIBERO基准，包含四个任务套件（空间、物体、目标、长时序），每个套件10个任务50个演示。</p>\n<p> - 真实世界：Cobot Agilex ALOHA平台的三个长时序任务（物体放置、抽屉操作、T恤折叠），分别45、45、30个演示。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 使用4×A800 GPUs（80GB显存）进行训练和评估，得益于指令驱动的稀疏化策略。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真环境：LIBERO基准测试，模拟各种指令遵循任务。</p>\n<p> - 真实世界：Cobot Agilex ALOHA平台，进行真实机器人操作评估。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（Success Rate）：以百分比衡量任务完成能力。</p>\n<p> - 效率指标：推理时间（秒）、吞吐量（Hz）、FLOPs（计算量）、训练成本（小时/10k步）。</p>\n<p> - 性能指标：在LIBERO套件上分空间、物体、目标、长时序子任务报告成功率；真实世界任务中统计子任务成功率（如抽屉操作的三步骤）。</p>"
  },
  {
    "date": "2025-08-27",
    "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
    "link": "http://arxiv.org/abs/2508.20072",
    "summary_markdown": "# 论文研究单位\n- The University of Hong Kong\n- Shanghai AI Laboratory\n- Shanghai Jiao Tong University\n- Huawei Cloud Computing Technologies Co., Ltd.\n# 论文概述\nVision-Language-Action (VLA)模型通过将大视觉-语言模型适配来映射图像和指令到机器人动作。现有的VLAs主要采用两种范式：一是自回归(AR)方法，按固定顺序预测离散化的动作token；二是使用单独的MLP或扩散头将VLM输出映射到可执行控制。Discrete Diffusion VLA提出了首个将离散扩散应用于VLA的框架，通过单一transformer统一视觉、语言和动作，采用自适应解码顺序和二次重新掩码机制，实现了精确动作建模和一致的训练效果。\n# 论文核心贡献点\n1. 首个离散扩散VLA框架，在单一transformer中统一动作生成与视觉-语言，保持强大性能\n2. 开发了自适应解码策略和迭代重新掩码机制，支持并行动作token解码和错误纠正\n3. 在Franka Panda、Google Robot和WidowX上验证，实现LIBERO上96.3%平均成功率，SimplerEnv–Fractal上64.1%和SimplerEnv–Bridge上54.2%的整体性能\n4. 在统一架构中打破了AR模型的左到右瓶颈，减少了函数评估数量\n# 论文方法描述\nDiscrete Diffusion VLA将动作解码建模为离散扩散，通过掩码token去噪在相同的transformer内进行。具体方法包括：\n\n**动作token化和分块**：每个连续控制维度被量化为256个bin的离散token，单步动作包含7个token（3个平移、3个旋转、1个夹爪）。将H个未来时间步的tokens排列成固定长度的动作块，总长度为L=H×D_act。\n\n**统一架构**：基于OpenVLA架构，将因果注意力修改为双向transformer，所有token（视觉、语言和动作）都通过统一transformer处理，动作位置使用双向注意力。\n\n**训练过程**：从掩码调度中采样掩码比例γ，用特殊token [MASK]替换γL个动作位置，对掩码位置最小化掩码交叉熵损失。\n\n**推理过程**：从全掩码状态开始，执行T轮并行细化。每轮对当前掩码位置预测token分布，按置信度对掩码位置排序，保留top (1-γ_t)位置并重新掩码剩余部分。包含阈值检查和残差下降检查的二次重新掩码机制。\n# 论文使用数据集和训练资源\n在三个机器人设置上评估：\n1. LIBERO：Franka Panda机械臂，包含四个套件（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个套件有10个任务和500个专家演示\n2. SimplerEnv-Fractal：Google Robot，在Fractal数据集上训练，评估视觉匹配和变体聚合性能\n3. SimplerEnv-Bridge：WidowX机器人，在BridgeData-V2上训练和评估\n\n使用与OpenVLA相同的VLM主干（Prismatic-7B）进行微调，图像调整为224×224像素，批量训练采用标准协议。\n# 论文使用的评估环境和评估指标\n**LIBERO**：报告四个套件上的成功率(SR)，每套500次rollouts（10个任务×50集）\n**SimplerEnv-Fractal**：报告视觉匹配和变体聚合的成功率，包含Pick Coke、Mv Near、Drawer三个任务的评估\n**SimplerEnv-Bridge**：报告四个任务上的部分和整体成功率，包括Put Spoon on Towel、Put Carrot on Plate、Stack Green on Yellow、Put Eggplant in Basket\n\n所有基准测试仅使用RGB图像输入（第三人称视角和手腕视角），语言指令和可选末端执行器位置，不使用深度信息或辅助信息。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>The University of Hong Kong</li><li>Shanghai AI Laboratory</li><li>Shanghai Jiao Tong University</li><li>Huawei Cloud Computing Technologies Co., Ltd.</li></ul>\n<h1>论文概述</h1>\n<p>Vision-Language-Action (VLA)模型通过将大视觉-语言模型适配来映射图像和指令到机器人动作。现有的VLAs主要采用两种范式：一是自回归(AR)方法，按固定顺序预测离散化的动作token；二是使用单独的MLP或扩散头将VLM输出映射到可执行控制。Discrete Diffusion VLA提出了首个将离散扩散应用于VLA的框架，通过单一transformer统一视觉、语言和动作，采用自适应解码顺序和二次重新掩码机制，实现了精确动作建模和一致的训练效果。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>首个离散扩散VLA框架，在单一transformer中统一动作生成与视觉-语言，保持强大性能</li><li>开发了自适应解码策略和迭代重新掩码机制，支持并行动作token解码和错误纠正</li><li>在Franka Panda、Google Robot和WidowX上验证，实现LIBERO上96.3%平均成功率，SimplerEnv–Fractal上64.1%和SimplerEnv–Bridge上54.2%的整体性能</li><li>在统一架构中打破了AR模型的左到右瓶颈，减少了函数评估数量</li></ol>\n<h1>论文方法描述</h1>\n<p>Discrete Diffusion VLA将动作解码建模为离散扩散，通过掩码token去噪在相同的transformer内进行。具体方法包括：</p>\n\n<p><strong>动作token化和分块</strong>：每个连续控制维度被量化为256个bin的离散token，单步动作包含7个token（3个平移、3个旋转、1个夹爪）。将H个未来时间步的tokens排列成固定长度的动作块，总长度为L=H×D_act。</p>\n\n<p><strong>统一架构</strong>：基于OpenVLA架构，将因果注意力修改为双向transformer，所有token（视觉、语言和动作）都通过统一transformer处理，动作位置使用双向注意力。</p>\n\n<p><strong>训练过程</strong>：从掩码调度中采样掩码比例γ，用特殊token [MASK]替换γL个动作位置，对掩码位置最小化掩码交叉熵损失。</p>\n\n<p><strong>推理过程</strong>：从全掩码状态开始，执行T轮并行细化。每轮对当前掩码位置预测token分布，按置信度对掩码位置排序，保留top (1-γ_t)位置并重新掩码剩余部分。包含阈值检查和残差下降检查的二次重新掩码机制。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>在三个机器人设置上评估：</p>\n<ol><li>LIBERO：Franka Panda机械臂，包含四个套件（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个套件有10个任务和500个专家演示</li><li>SimplerEnv-Fractal：Google Robot，在Fractal数据集上训练，评估视觉匹配和变体聚合性能</li><li>SimplerEnv-Bridge：WidowX机器人，在BridgeData-V2上训练和评估</li></ol>\n\n<p>使用与OpenVLA相同的VLM主干（Prismatic-7B）进行微调，图像调整为224×224像素，批量训练采用标准协议。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p><strong>LIBERO</strong>：报告四个套件上的成功率(SR)，每套500次rollouts（10个任务×50集）</p>\n<p><strong>SimplerEnv-Fractal</strong>：报告视觉匹配和变体聚合的成功率，包含Pick Coke、Mv Near、Drawer三个任务的评估</p>\n<p><strong>SimplerEnv-Bridge</strong>：报告四个任务上的部分和整体成功率，包括Put Spoon on Towel、Put Carrot on Plate、Stack Green on Yellow、Put Eggplant in Basket</p>\n\n<p>所有基准测试仅使用RGB图像输入（第三人称视角和手腕视角），语言指令和可选末端执行器位置，不使用深度信息或辅助信息。</p>"
  },
  {
    "date": "2025-08-27",
    "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation",
    "link": "http://arxiv.org/abs/2508.19958",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-27",
    "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
    "link": "http://arxiv.org/abs/2508.19852",
    "summary_markdown": "### 论文研究单位\n未明确提供\n### 论文概述\n本文提出Ego-PM（Ego-centric Predictive Model），一种基于手部轨迹条件的自我中心预测模型，旨在同时预测未来动作（手部轨迹）和视觉结果。该模型采用两阶段统一框架：第一阶段通过连续状态建模显式预测手部轨迹，第二阶段引入因果交叉注意力融合多模态线索，引导潜扩散模型逐帧生成视频。Ego-PM是首个同时处理人类自我中心活动和机器人操作任务的模型，无需额外标注即可自主预测动作及其视觉后果。\n### 论文核心贡献点\n1. **首个统一预测模型**：首次实现同时预测未来动作和视觉帧，解决以往方法分离处理动作与视觉预测的局限。\n2. **连续状态建模（CoSMo）与注意力机制**：提出CoSMo策略利用历史状态预测手部轨迹，并设计因果交叉注意力增强动作条件融合，显著提升预测连贯性。\n3. **跨场景通用性**：首个统一模型适用于人类自我中心视角（Ego4D）和机器人操作（BridgeData、RLBench），在动作预测与视频生成任务上均超越基线。\n### 论文方法描述\n模型分为两个训练阶段：\n1. **阶段一：显式动作建模**\n - **视觉编码器**：使用CLIP提取帧特征并投影至文本嵌入空间。\n - **动作编码器/解码器**：设计轻量MLP处理动作嵌入，通过特殊令牌<ACT>标识动作。\n - **自回归模型**：基于LLaVA处理多模态序列（视觉、文本、动作）。\n - **连续状态建模（CoSMo）**：采用相邻两个状态（t和t-1）作为输入，增强时序依赖性。\n2. **阶段二：动作增强帧预测**\n - **多模态条件融合**：通过因果交叉注意力将视觉、文本与动作嵌入对齐，约束未来帧生成。\n - **帧预测**：基于潜扩散模型（LDM），以融合条件为指导，从最后一帧开始迭代生成未来帧。\n训练目标包括阶段一的语言与动作损失（L1+GIoU），以及阶段二的扩散损失。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 人类活动：Ego4D（PRE-15和PNR时刻，文本叙述与手部轨迹）。\n - 机器人操作：BridgeData V2（人类演示视频与指令）和RLBench（9项多任务评估）。\n- **训练资源**：\n - 输入分辨率：256×256。\n - 初始化权重：LLaVA（Ego4D）或OpenVLA（BridgeData），LDM使用Stable Diffusion。\n - 训练配置：微调3个epoch，损失权重λ1=0.1、λ2=0.01。\n - 计算资源：未明确说明（原文未提及硬件细节）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：未明确提供具体环境（如硬件或软件框架），依赖学术评估设置。\n- **评估指标**：\n - **帧预测**：\n - 对齐分数：EgoVLP、EgoVLP+（视频-文本）。\n - 图像质量：CLIP相似度（越高越好）、FID（越低越好）、PSNR（越高越好）、SSIM（越高越好）、LPIPS（越低越好）。\n - **动作预测**：\n - Ego4D：手部掩码IoU（预测与真实区域重叠，越高越好）。\n - BridgeData/RLBench：任务成功率（百分比，越高越好）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>未明确提供</p>\n<h3>论文概述</h3>\n<p>本文提出Ego-PM（Ego-centric Predictive Model），一种基于手部轨迹条件的自我中心预测模型，旨在同时预测未来动作（手部轨迹）和视觉结果。该模型采用两阶段统一框架：第一阶段通过连续状态建模显式预测手部轨迹，第二阶段引入因果交叉注意力融合多模态线索，引导潜扩散模型逐帧生成视频。Ego-PM是首个同时处理人类自我中心活动和机器人操作任务的模型，无需额外标注即可自主预测动作及其视觉后果。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首个统一预测模型</strong>：首次实现同时预测未来动作和视觉帧，解决以往方法分离处理动作与视觉预测的局限。</li><li><strong>连续状态建模（CoSMo）与注意力机制</strong>：提出CoSMo策略利用历史状态预测手部轨迹，并设计因果交叉注意力增强动作条件融合，显著提升预测连贯性。</li><li><strong>跨场景通用性</strong>：首个统一模型适用于人类自我中心视角（Ego4D）和机器人操作（BridgeData、RLBench），在动作预测与视频生成任务上均超越基线。</li></ol>\n<h3>论文方法描述</h3>\n<p>模型分为两个训练阶段：</p>\n<p>1. <strong>阶段一：显式动作建模</strong></p>\n<p> - <strong>视觉编码器</strong>：使用CLIP提取帧特征并投影至文本嵌入空间。</p>\n<p> - <strong>动作编码器/解码器</strong>：设计轻量MLP处理动作嵌入，通过特殊令牌<ACT>标识动作。</p>\n<p> - <strong>自回归模型</strong>：基于LLaVA处理多模态序列（视觉、文本、动作）。</p>\n<p> - <strong>连续状态建模（CoSMo）</strong>：采用相邻两个状态（t和t-1）作为输入，增强时序依赖性。</p>\n<p>2. <strong>阶段二：动作增强帧预测</strong></p>\n<p> - <strong>多模态条件融合</strong>：通过因果交叉注意力将视觉、文本与动作嵌入对齐，约束未来帧生成。</p>\n<p> - <strong>帧预测</strong>：基于潜扩散模型（LDM），以融合条件为指导，从最后一帧开始迭代生成未来帧。</p>\n<p>训练目标包括阶段一的语言与动作损失（L1+GIoU），以及阶段二的扩散损失。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 人类活动：Ego4D（PRE-15和PNR时刻，文本叙述与手部轨迹）。</p>\n<p> - 机器人操作：BridgeData V2（人类演示视频与指令）和RLBench（9项多任务评估）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 输入分辨率：256×256。</p>\n<p> - 初始化权重：LLaVA（Ego4D）或OpenVLA（BridgeData），LDM使用Stable Diffusion。</p>\n<p> - 训练配置：微调3个epoch，损失权重λ1=0.1、λ2=0.01。</p>\n<p> - 计算资源：未明确说明（原文未提及硬件细节）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：未明确提供具体环境（如硬件或软件框架），依赖学术评估设置。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>帧预测</strong>：</p>\n<p> - 对齐分数：EgoVLP、EgoVLP+（视频-文本）。</p>\n<p> - 图像质量：CLIP相似度（越高越好）、FID（越低越好）、PSNR（越高越好）、SSIM（越高越好）、LPIPS（越低越好）。</p>\n<p> - <strong>动作预测</strong>：</p>\n<p> - Ego4D：手部掩码IoU（预测与真实区域重叠，越高越好）。</p>\n<p> - BridgeData/RLBench：任务成功率（百分比，越高越好）。</p>"
  },
  {
    "date": "2025-08-15",
    "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2508.19257",
    "summary_markdown": "### 论文研究单位\n北京大学 (Peking University)\n### 论文概述\n当前视觉-语言-动作（VLA）模型在处理连续帧时缺乏时序信息整合，逐帧独立处理导致视觉噪声敏感性和时序关联丢失。论文提出TTF-VLA（时序令牌融合），通过融合历史与当前视觉特征增强VLA推理质量，在不训练的前提下改善机器人操作任务的鲁棒性和效率。\n### 论文核心贡献点\n1. **双维度检测机制**：提出结合灰度像素差异分析与注意力语义相关性的检测方法，用于识别视觉变化区域和任务关键区域\n2. **硬融合策略与关键帧**：采用二值令牌选择决策，并通过周期性关键帧重置防止误差累积\n3. **跨模型适用性**：验证在OpenVLA和VLA-Cache架构上的通用性，无需任务特定调参\n4. **Query矩阵复用发现**：揭示选择性Query矩阵复用可提升性能，提出KQV矩阵直接复用的潜在加速方向\n### 论文方法描述\n1. **时序令牌融合框架**\n - 输入：当前帧I_t、历史帧I_{t-1}、历史令牌T_{t-1}、任务指令L_t\n - 输出：融合令牌T̃_t\n - 公式：T̃_t = F(T_t, T_{t-1}, I_t, I_{t-1}, L_t)\n\n2. **硬融合决策**\n - 对每个令牌i选择当前或历史令牌：\n t̃_t^(i) = t_t^(i) 若 m_i^fusion=1，否则 t_{t-1}^(i)\n\n3. **关键帧机制**\n - 定期重置所有令牌：IsKeyframe(t) = (t mod K=0) ∨ (T_{t-1}=∅)\n - 参数K=3平衡时序稳定性和响应速度\n\n4. **双维度检测融合**\n - 像素维度：计算灰度图绝对差异d_i^pixel = (1/196) ∑\\|G_t(u,v)-G_{t-1}(u,v)\\|\n - 注意力维度：提取文本-视觉和动作-视觉注意力S_text^((l))和S_action^((l))\n - 最终融合：m_i^fusion = m_i^pixel ∨ m_i^attention\n### 论文使用数据集和训练资源\n1. **仿真数据集**\n - LIBERO：4套任务（Object/Spatial/Goal/Long），每套10任务×20 episodes\n - SimplerEnv：3任务（Move Near/Pick Coke/Drawer），总756 episodes\n\n2. **真实机器人**\n - 设备：Franka Research 3机械臂\n - 数据：3任务×80 demos，5Hz操作频率\n\n3. **训练资源**\n - 微调配置：OpenVLA-7B，20,000步，batch size=8，8×A100 GPUs\n - 推理部署：单A100 GPU，5Hz控制频率\n### 论文使用的评估环境和评估指标\n1. **评估环境**\n - 仿真：LIBERO基准套件 + SimplerEnv跨环境平台\n - 真实：Franka机械臂实验室场景\n\n2. **评估指标**\n - **任务成功率**：成功执行次数/总次数\n - **时序融合率**：复用历史令牌比例\n - **计算效率**：令牌复用带来的加速效果\n\n3. **主要结果**\n - LIBERO平均提升：OpenVLA +4.0pp (72.4% vs 68.4%)，VLA-Cache +2.7pp\n - SimplerEnv跨环境：+4.8%相对提升\n - 真实机器人：+8.7%相对提升，Pick-and-Place任务改善显著\n - 消融研究：双维度融合优于单一维度，关键帧K=3性能最优\n\n论文证明TTF-VLA能有效提升VLA模型在多环境下的操作能力，特别是在长序列任务和噪声场景中表现突出，为时序信息利用和计算加速提供了新方向。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学 (Peking University)</p>\n<h3>论文概述</h3>\n<p>当前视觉-语言-动作（VLA）模型在处理连续帧时缺乏时序信息整合，逐帧独立处理导致视觉噪声敏感性和时序关联丢失。论文提出TTF-VLA（时序令牌融合），通过融合历史与当前视觉特征增强VLA推理质量，在不训练的前提下改善机器人操作任务的鲁棒性和效率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>双维度检测机制</strong>：提出结合灰度像素差异分析与注意力语义相关性的检测方法，用于识别视觉变化区域和任务关键区域</li><li><strong>硬融合策略与关键帧</strong>：采用二值令牌选择决策，并通过周期性关键帧重置防止误差累积</li><li><strong>跨模型适用性</strong>：验证在OpenVLA和VLA-Cache架构上的通用性，无需任务特定调参</li><li><strong>Query矩阵复用发现</strong>：揭示选择性Query矩阵复用可提升性能，提出KQV矩阵直接复用的潜在加速方向</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>时序令牌融合框架</strong></p>\n<p> - 输入：当前帧I_t、历史帧I_{t-1}、历史令牌T_{t-1}、任务指令L_t</p>\n<p> - 输出：融合令牌T̃_t</p>\n<p> - 公式：T̃_t = F(T_t, T_{t-1}, I_t, I_{t-1}, L_t)</p>\n\n<p>2. <strong>硬融合决策</strong></p>\n<p> - 对每个令牌i选择当前或历史令牌：</p>\n<p> t̃_t^(i) = t_t^(i) 若 m_i^fusion=1，否则 t_{t-1}^(i)</p>\n\n<p>3. <strong>关键帧机制</strong></p>\n<p> - 定期重置所有令牌：IsKeyframe(t) = (t mod K=0) ∨ (T_{t-1}=∅)</p>\n<p> - 参数K=3平衡时序稳定性和响应速度</p>\n\n<p>4. <strong>双维度检测融合</strong></p>\n<p> - 像素维度：计算灰度图绝对差异d_i^pixel = (1/196) ∑\\|G_t(u,v)-G_{t-1}(u,v)\\|</p>\n<p> - 注意力维度：提取文本-视觉和动作-视觉注意力S_text^((l))和S_action^((l))</p>\n<p> - 最终融合：m_i^fusion = m_i^pixel ∨ m_i^attention</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>仿真数据集</strong></p>\n<p> - LIBERO：4套任务（Object/Spatial/Goal/Long），每套10任务×20 episodes</p>\n<p> - SimplerEnv：3任务（Move Near/Pick Coke/Drawer），总756 episodes</p>\n\n<p>2. <strong>真实机器人</strong></p>\n<p> - 设备：Franka Research 3机械臂</p>\n<p> - 数据：3任务×80 demos，5Hz操作频率</p>\n\n<p>3. <strong>训练资源</strong></p>\n<p> - 微调配置：OpenVLA-7B，20,000步，batch size=8，8×A100 GPUs</p>\n<p> - 推理部署：单A100 GPU，5Hz控制频率</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong></p>\n<p> - 仿真：LIBERO基准套件 + SimplerEnv跨环境平台</p>\n<p> - 真实：Franka机械臂实验室场景</p>\n\n<p>2. <strong>评估指标</strong></p>\n<p> - <strong>任务成功率</strong>：成功执行次数/总次数</p>\n<p> - <strong>时序融合率</strong>：复用历史令牌比例</p>\n<p> - <strong>计算效率</strong>：令牌复用带来的加速效果</p>\n\n<p>3. <strong>主要结果</strong></p>\n<p> - LIBERO平均提升：OpenVLA +4.0pp (72.4% vs 68.4%)，VLA-Cache +2.7pp</p>\n<p> - SimplerEnv跨环境：+4.8%相对提升</p>\n<p> - 真实机器人：+8.7%相对提升，Pick-and-Place任务改善显著</p>\n<p> - 消融研究：双维度融合优于单一维度，关键帧K=3性能最优</p>\n\n<p>论文证明TTF-VLA能有效提升VLA模型在多环境下的操作能力，特别是在长序列任务和噪声场景中表现突出，为时序信息利用和计算加速提供了新方向。</p>"
  },
  {
    "date": "2025-08-26",
    "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2508.19236",
    "summary_markdown": "### 论文研究单位\n清华大学自动化系BNRist、Dexmal、MEGVII Technology、天津大学、哈尔滨工业大学、StepFun\n### 论文概述\nMemoryVLA是一个受认知科学启发的视觉-语言-动作模型框架，通过引入感知-认知记忆机制解决机器人操作中的长期时间依赖问题。该方法结合了工作记忆（短期神经网络活动）和长时记忆（海马体系统），用于处理非马尔可夫性的操作任务，如\"推按钮\"等视觉变化不明显的场景。\n### 论文核心贡献点\n- 提出认知-记忆-行动框架，利用VLM常识先验和记忆机制建模长期时间依赖\n- 设计感知-认知记忆库（PCMB），支持高低层特征的记忆检索、门控融合和记忆整合\n- 实现记忆条件化扩散动作专家，生成时间感知的动作序列\n- 在150+仿真和真实任务上达到SOTA性能，长期任务提升显著\n### 论文方法描述\n1. **视觉-语言认知模块**：\n - 使用7B Prismatic VLM处理RGB图像和语言指令\n - DINOv2和SigLIP视觉编码器提取感知token（256维）\n - LLaMA-7B生成认知token（1维），形成工作记忆\n\n2. **感知-认知记忆模块**：\n - **记忆检索**：通过带时间位置编码的交叉注意力从PCMB获取历史上下文\n - **门控融合**：用学习门控自适应融合当前token和检索内容\n - **记忆整合**：当容量满时合并时间相邻且语义相似的条目\n\n3. **记忆条件化动作专家**：\n - 基于扩散的Transformer（DiT）生成16步7-DoF动作序列\n - 认知token提供高层语义指导，感知token补充细节\n - DDIM采样10步，使用分类器自由引导（CFG）\n### 论文使用数据集和训练资源\n- **数据集**：Bridge v2、RT-1、LIBERO（5套件）、真实世界数据\n- **训练资源**：8块NVIDIA A100 GPU，PyTorch FSDP，全局批大小256，学习率2e-5\n- **模型参数**：VLM 7B，扩散专家约300M\n### 论文使用的评估环境和评估指标\n- **仿真环境**：\n - SimplerEnv-Bridge（WidowX机器人）\n - SimplerEnv-Fractal（Google机器人，含VM/VA设置）\n - LIBERO（Franka机器人，5套件：Spatial/Object/Goal/Long/LIBERO-90）\n- **真实环境**：\n - Franka和WidowX机器人\n - Intel RealSense D435 RGB摄像头（640×480→224×224）\n - ROS集成系统\n- **评估指标**：\n - 成功率（%），每个任务15-50次试验\n - 长期任务采用子目标逐步评分",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学自动化系BNRist、Dexmal、MEGVII Technology、天津大学、哈尔滨工业大学、StepFun</p>\n<h3>论文概述</h3>\n<p>MemoryVLA是一个受认知科学启发的视觉-语言-动作模型框架，通过引入感知-认知记忆机制解决机器人操作中的长期时间依赖问题。该方法结合了工作记忆（短期神经网络活动）和长时记忆（海马体系统），用于处理非马尔可夫性的操作任务，如\"推按钮\"等视觉变化不明显的场景。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出认知-记忆-行动框架，利用VLM常识先验和记忆机制建模长期时间依赖</li><li>设计感知-认知记忆库（PCMB），支持高低层特征的记忆检索、门控融合和记忆整合</li><li>实现记忆条件化扩散动作专家，生成时间感知的动作序列</li><li>在150+仿真和真实任务上达到SOTA性能，长期任务提升显著</li></ul>\n<h3>论文方法描述</h3>\n<p>1. <strong>视觉-语言认知模块</strong>：</p>\n<p> - 使用7B Prismatic VLM处理RGB图像和语言指令</p>\n<p> - DINOv2和SigLIP视觉编码器提取感知token（256维）</p>\n<p> - LLaMA-7B生成认知token（1维），形成工作记忆</p>\n\n<p>2. <strong>感知-认知记忆模块</strong>：</p>\n<p> - <strong>记忆检索</strong>：通过带时间位置编码的交叉注意力从PCMB获取历史上下文</p>\n<p> - <strong>门控融合</strong>：用学习门控自适应融合当前token和检索内容</p>\n<p> - <strong>记忆整合</strong>：当容量满时合并时间相邻且语义相似的条目</p>\n\n<p>3. <strong>记忆条件化动作专家</strong>：</p>\n<p> - 基于扩散的Transformer（DiT）生成16步7-DoF动作序列</p>\n<p> - 认知token提供高层语义指导，感知token补充细节</p>\n<p> - DDIM采样10步，使用分类器自由引导（CFG）</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Bridge v2、RT-1、LIBERO（5套件）、真实世界数据</li><li><strong>训练资源</strong>：8块NVIDIA A100 GPU，PyTorch FSDP，全局批大小256，学习率2e-5</li><li><strong>模型参数</strong>：VLM 7B，扩散专家约300M</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>仿真环境</strong>：</li></ul>\n<p> - SimplerEnv-Bridge（WidowX机器人）</p>\n<p> - SimplerEnv-Fractal（Google机器人，含VM/VA设置）</p>\n<p> - LIBERO（Franka机器人，5套件：Spatial/Object/Goal/Long/LIBERO-90）</p>\n<ul><li><strong>真实环境</strong>：</li></ul>\n<p> - Franka和WidowX机器人</p>\n<p> - Intel RealSense D435 RGB摄像头（640×480→224×224）</p>\n<p> - ROS集成系统</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（%），每个任务15-50次试验</p>\n<p> - 长期任务采用子目标逐步评分</p>"
  },
  {
    "date": "2025-08-25",
    "title": "FlowVLA: Thinking in Motion with a Visual Chain of Thought",
    "link": "http://arxiv.org/abs/2508.18269",
    "summary_markdown": "# 论文研究单位\n未明确标注单位，仅在作者列表中出现两个上标标记；推断主要来自多机构合作，具体单位信息未在页面中明确给出。\n# 论文概述\n当前视觉-语言-动作（Vision-Language-Action, VLA）模型常以“下一帧预测”作为世界模型的预训练范式，直接从当前帧预测未来帧。这种跳过显式物理推理的方式容易导致“像素复制陷阱”，使预测在物理上不可信，并在长时序中不稳定。为解决该问题，论文提出“视觉思维链”（Visual Chain of Thought, Visual CoT），将预测分解为显式的“先推理运动，再生成外观”的结构化过程，具体采用v_t→f_t→v_{t+1}的因果链条，并以光流f_t作为中间运动表示。论文据此构建FlowVLA：一个两阶段的统一自回归Transformer，第一阶段用Visual CoT进行世界模型预训练，第二阶段进行策略微调以生成动作块。FlowVLA在LIBERO、SimplerEnv等仿真基准以及AgileX Cobot真实机器人平台上实现了更物理合理、更高效的策略学习，并提升样本效率和收敛速度。\n# 论文核心贡献点\n- 指出下一帧预测范式的根本缺陷（像素复制陷阱、缺乏物理因果性），提出“视觉思维链”（Visual CoT）作为世界模型学习新范式。\n- 通过v_t→f_t→v_{t+1}显式将运动推理与外观生成解耦，并在预训练中引入强归纳偏置以促进动态理解。\n- 设计FlowVLA：单一统一自回归Transformer，使用共享的矢量量化（VQ）分词器将RGB帧与光流映射为同一词表，实现端到端的跨模态推理。\n- 两阶段训练：阶段一以Visual CoT进行大规模视频的世界模型预训练；阶段二微调为策略模型以生成动作块，显著缩小预训练-下游任务的领域鸿沟。\n- 在仿真和真实平台取得最优或大幅领先的性能，并验证样本效率、收敛速度与长时序规划能力提升。\n# 论文方法描述\n- 视觉思维链（Visual CoT）\n - 将建模目标从P(v_{t+1}\\|v_t, L)转化为联合建模P(v_{t+1}, f_t\\|v_t, L)，并因式分解为两步：\n 1) 运动推理：P(f_t\\|v_t, L)，要求显式预测光流f_t；\n 2) 外观生成：P(v_{t+1}\\|f_t, v_t, L)，在给定光流条件下生成下一帧。\n - 将学习从像素回归重构为结构化物理推理，为策略学习提供与“如何动”对齐的中间表示。\n- 两阶段训练框架\n - 阶段一：世界模型预训练（Visual CoT）\n - 输入序列构造为交错的出现（帧）与运动（光流）token：S_wm = {L_instr, v_0, f_0, v_1, f_1, …, v_T, f_T}。\n - 统一分词：用同一VQ-GAN分词器将RGB帧与光流映射为离散token；光流由RAFT计算，并通过VideoJAM方式将二维位移编码为3通道RGB图，再进行非线性归一化以保留细微运动。\n - 训练目标：L_WM为对flow token与next-frame token的交叉熵损失之和（通常权重λ=1），以标准的“下一token预测”优化，交错预测f_t与v_{t+1}。\n - 阶段二：策略微调（动作预测）\n - 权重初始化自预训练世界模型。\n - 输入序列为S_policy = {L_instr, v_0, a_0, v_1, a_1, …}；动作按FAST方法离散化为token。\n - 训练目标：L_policy仅在动作token上计算交叉熵，使模型将已学到的视觉-动力知识聚焦于行动生成。\n- 统一与简洁性\n - 无需引入运动特定的网络分支：光流与帧共享同一分词器，单一自回归Transformer即可学习跨模态交错序列，兼具参数与结构效率。\n# 论文使用数据集和训练资源\n- 数据与基准\n - 仿真基准：LIBERO（空间/目标/长期组合四套Suite）、SimplerEnv-WidowX（评估域移鲁棒性：光照、纹理、视角变化）。\n - 真实机器人：AgileX Cobot双机械臂平台，包含腕部与前向摄像头；设计四项单臂与双臂任务，采集每任务50–200条人类遥操作演示用于微调。\n- 预训练与微调设置（典型）\n - 基于Emu3（约8.5B参数）架构；光流由RAFT预计算；分词采用VQ-GAN。\n - LIBERO：世界模型预训练约5k步（batch=16）；策略微调约5k步（batch=96）。\n - SimplerEnv：预训练约12k步（batch=32）；微调约20k步（batch=128）。\n- 实现细节\n - 光流转换：2通道(u, v)映射为3通道RGB，方向→色相，速度→饱和度与明度；非线性归一化保留细微运动并避免饱和。\n - 损失平衡：L_WM中λ=1，交错预测flow与帧；动作token化遵循FAST。\n# 论文使用的评估环境和评估指标\n- 评估环境\n - LIBERO四套Suite（空间泛化、目标泛化、目标改变、长期组合）以及SimplerEnv-WidowX域移场景。\n - 真实AgileX Cobot平台的双臂操作任务。\n- 评估指标\n - 任务成功率（%）：在LIBERO与SimplerEnv的仿真结果以及真实机器人任务上进行报告，通常每任务多次试验（真实实验每任务25次）以保证统计可靠性。\n - 收敛与样本效率：通过成功率和训练步数曲线，对比FlowVLA与基线（如UniVLA）在全数据与低数据（50%）条件下的收敛速度与最终性能。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>未明确标注单位，仅在作者列表中出现两个上标标记；推断主要来自多机构合作，具体单位信息未在页面中明确给出。</p>\n<h1>论文概述</h1>\n<p>当前视觉-语言-动作（Vision-Language-Action, VLA）模型常以“下一帧预测”作为世界模型的预训练范式，直接从当前帧预测未来帧。这种跳过显式物理推理的方式容易导致“像素复制陷阱”，使预测在物理上不可信，并在长时序中不稳定。为解决该问题，论文提出“视觉思维链”（Visual Chain of Thought, Visual CoT），将预测分解为显式的“先推理运动，再生成外观”的结构化过程，具体采用v_t→f_t→v_{t+1}的因果链条，并以光流f_t作为中间运动表示。论文据此构建FlowVLA：一个两阶段的统一自回归Transformer，第一阶段用Visual CoT进行世界模型预训练，第二阶段进行策略微调以生成动作块。FlowVLA在LIBERO、SimplerEnv等仿真基准以及AgileX Cobot真实机器人平台上实现了更物理合理、更高效的策略学习，并提升样本效率和收敛速度。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>指出下一帧预测范式的根本缺陷（像素复制陷阱、缺乏物理因果性），提出“视觉思维链”（Visual CoT）作为世界模型学习新范式。</li><li>通过v_t→f_t→v_{t+1}显式将运动推理与外观生成解耦，并在预训练中引入强归纳偏置以促进动态理解。</li><li>设计FlowVLA：单一统一自回归Transformer，使用共享的矢量量化（VQ）分词器将RGB帧与光流映射为同一词表，实现端到端的跨模态推理。</li><li>两阶段训练：阶段一以Visual CoT进行大规模视频的世界模型预训练；阶段二微调为策略模型以生成动作块，显著缩小预训练-下游任务的领域鸿沟。</li><li>在仿真和真实平台取得最优或大幅领先的性能，并验证样本效率、收敛速度与长时序规划能力提升。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>视觉思维链（Visual CoT）</li></ul>\n<p> - 将建模目标从P(v_{t+1}\\|v_t, L)转化为联合建模P(v_{t+1}, f_t\\|v_t, L)，并因式分解为两步：</p>\n<p> 1) 运动推理：P(f_t\\|v_t, L)，要求显式预测光流f_t；</p>\n<p> 2) 外观生成：P(v_{t+1}\\|f_t, v_t, L)，在给定光流条件下生成下一帧。</p>\n<p> - 将学习从像素回归重构为结构化物理推理，为策略学习提供与“如何动”对齐的中间表示。</p>\n<ul><li>两阶段训练框架</li></ul>\n<p> - 阶段一：世界模型预训练（Visual CoT）</p>\n<p> - 输入序列构造为交错的出现（帧）与运动（光流）token：S_wm = {L_instr, v_0, f_0, v_1, f_1, …, v_T, f_T}。</p>\n<p> - 统一分词：用同一VQ-GAN分词器将RGB帧与光流映射为离散token；光流由RAFT计算，并通过VideoJAM方式将二维位移编码为3通道RGB图，再进行非线性归一化以保留细微运动。</p>\n<p> - 训练目标：L_WM为对flow token与next-frame token的交叉熵损失之和（通常权重λ=1），以标准的“下一token预测”优化，交错预测f_t与v_{t+1}。</p>\n<p> - 阶段二：策略微调（动作预测）</p>\n<p> - 权重初始化自预训练世界模型。</p>\n<p> - 输入序列为S_policy = {L_instr, v_0, a_0, v_1, a_1, …}；动作按FAST方法离散化为token。</p>\n<p> - 训练目标：L_policy仅在动作token上计算交叉熵，使模型将已学到的视觉-动力知识聚焦于行动生成。</p>\n<ul><li>统一与简洁性</li></ul>\n<p> - 无需引入运动特定的网络分支：光流与帧共享同一分词器，单一自回归Transformer即可学习跨模态交错序列，兼具参数与结构效率。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据与基准</li></ul>\n<p> - 仿真基准：LIBERO（空间/目标/长期组合四套Suite）、SimplerEnv-WidowX（评估域移鲁棒性：光照、纹理、视角变化）。</p>\n<p> - 真实机器人：AgileX Cobot双机械臂平台，包含腕部与前向摄像头；设计四项单臂与双臂任务，采集每任务50–200条人类遥操作演示用于微调。</p>\n<ul><li>预训练与微调设置（典型）</li></ul>\n<p> - 基于Emu3（约8.5B参数）架构；光流由RAFT预计算；分词采用VQ-GAN。</p>\n<p> - LIBERO：世界模型预训练约5k步（batch=16）；策略微调约5k步（batch=96）。</p>\n<p> - SimplerEnv：预训练约12k步（batch=32）；微调约20k步（batch=128）。</p>\n<ul><li>实现细节</li></ul>\n<p> - 光流转换：2通道(u, v)映射为3通道RGB，方向→色相，速度→饱和度与明度；非线性归一化保留细微运动并避免饱和。</p>\n<p> - 损失平衡：L_WM中λ=1，交错预测flow与帧；动作token化遵循FAST。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境</li></ul>\n<p> - LIBERO四套Suite（空间泛化、目标泛化、目标改变、长期组合）以及SimplerEnv-WidowX域移场景。</p>\n<p> - 真实AgileX Cobot平台的双臂操作任务。</p>\n<ul><li>评估指标</li></ul>\n<p> - 任务成功率（%）：在LIBERO与SimplerEnv的仿真结果以及真实机器人任务上进行报告，通常每任务多次试验（真实实验每任务25次）以保证统计可靠性。</p>\n<p> - 收敛与样本效率：通过成功率和训练步数曲线，对比FlowVLA与基线（如UniVLA）在全数据与低数据（50%）条件下的收敛速度与最终性能。</p>"
  },
  {
    "date": "2025-08-23",
    "title": "NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows",
    "link": "http://arxiv.org/abs/2508.16845",
    "summary_markdown": "论文研究单位\nAIRI, ETH Zürich, MIPT, Skoltech, Innopolis University, HSE\n\n论文概述\n该论文探讨了视觉-语言-动作模型中动作解码器的效率问题。当前的VLA模型通常采用扩散模型作为动作解码器，但其推理过程中的多步去噪特性导致高延迟，不适用于需要高频控制的实时机器人场景。为此，论文提出了NinA（Normalizing Flows in Action），一种使用标准化流作为动作解码器的替代方案。NinA通过可逆变换实现单步采样，显著降低了推理时间。研究将NinA集成到FLOWER VLA架构中，并在LIBERO基准上进行了实验，结果表明NinA在匹配扩散模型性能的同时，实现了更快的推理速度和更少的参数量。\n\n论文核心贡献点\n1. 提出了NinA，一种将标准化流作为VLA模型中动作解码器的替代方案，以取代传统的基于扩散模型的解码器。\n2. 展示了标准化流能够通过单次前向传递生成动作，从而将推理速度提升了一个数量级，解决了扩散模型在实时控制中的延迟瓶颈。\n3. 在LIBERO基准上的实验证明，NinA在性能上与最先进的扩散模型相当，同时模型参数更少，推理速度显著更快。\n4. 对NinA的关键设计选择进行了研究，包括比较了MLP和Transformer两种backbone，并验证了在训练中向动作数据添加高斯噪声的有效性。\n\n论文方法描述\nNinA基于FLOWER VLA架构，保留了其预训练的视觉-语言模型（VLM）用于编码图像观测和任务指令。其核心创新在于用标准化流（NF）替换了原始的扩散动作解码器。\n标准化流的具体实现有两种变体：\n- MLP变体：将动作块视为单个向量，通过一个耦合层网络，该网络将条件向量（VLM的输出）和动作的一部分拼接起来，生成用于变换另一部分的缩放和偏置参数。\n- Transformer变体：将动作块视为序列，通过自注意力和交叉注意力机制来实现条件变换，其中交叉注意力层接收VLM的输出作为条件。\n训练时，首先从数据中采样状态-目标-动作块，然后通过VLM获得条件嵌入。接着，向动作块中注入高斯噪声以进行正则化，再将带噪的动作数据通过一系列可逆的流层，最终映射到一个简单的潜在分布（如标准正态分布）。损失函数通过最大化数据在真实分布下的对数似然来计算。推理时，从潜在分布中采样一个点，然后逆向通过流层，单次前向传播即可生成一个动作块。此外，方法中还集成了一种可训练的线性层（PLU）以增强模型表达能力。\n\n论文使用数据集和训练资源\n数据集: LIBERO基准，包含LIBERO Spatial, LIBERO Object, LIBERO Goal, LIBERO 10, LIBERO 90等多个模拟任务子集。\n训练资源:\n- 硬件: 使用NVIDIA H100 GPU进行训练。\n- 软件: 基于FLOWER代码库，VLM部分使用Florence-2 Large模型。\n- 训练配置: 训练了100个epoch，批次大小为80。\n\n论文使用的评估环境和评估指标\n评估环境: 在LIBERO基准的模拟环境中进行评估。\n评估指标:\n- 主要指标: 任务成功率，即在各个LIBERO任务子集上完成任务的百分比，并计算平均成功率。\n- 次要指标: 推理时间（每次动作生成所需的秒数）和模型参数量（以百万为单位），用于衡量模型的效率。",
    "summary_html": "<p>论文研究单位</p>\n<p>AIRI, ETH Zürich, MIPT, Skoltech, Innopolis University, HSE</p>\n\n<p>论文概述</p>\n<p>该论文探讨了视觉-语言-动作模型中动作解码器的效率问题。当前的VLA模型通常采用扩散模型作为动作解码器，但其推理过程中的多步去噪特性导致高延迟，不适用于需要高频控制的实时机器人场景。为此，论文提出了NinA（Normalizing Flows in Action），一种使用标准化流作为动作解码器的替代方案。NinA通过可逆变换实现单步采样，显著降低了推理时间。研究将NinA集成到FLOWER VLA架构中，并在LIBERO基准上进行了实验，结果表明NinA在匹配扩散模型性能的同时，实现了更快的推理速度和更少的参数量。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了NinA，一种将标准化流作为VLA模型中动作解码器的替代方案，以取代传统的基于扩散模型的解码器。</li><li>展示了标准化流能够通过单次前向传递生成动作，从而将推理速度提升了一个数量级，解决了扩散模型在实时控制中的延迟瓶颈。</li><li>在LIBERO基准上的实验证明，NinA在性能上与最先进的扩散模型相当，同时模型参数更少，推理速度显著更快。</li><li>对NinA的关键设计选择进行了研究，包括比较了MLP和Transformer两种backbone，并验证了在训练中向动作数据添加高斯噪声的有效性。</li></ol>\n\n<p>论文方法描述</p>\n<p>NinA基于FLOWER VLA架构，保留了其预训练的视觉-语言模型（VLM）用于编码图像观测和任务指令。其核心创新在于用标准化流（NF）替换了原始的扩散动作解码器。</p>\n<p>标准化流的具体实现有两种变体：</p>\n<ul><li>MLP变体：将动作块视为单个向量，通过一个耦合层网络，该网络将条件向量（VLM的输出）和动作的一部分拼接起来，生成用于变换另一部分的缩放和偏置参数。</li><li>Transformer变体：将动作块视为序列，通过自注意力和交叉注意力机制来实现条件变换，其中交叉注意力层接收VLM的输出作为条件。</li></ul>\n<p>训练时，首先从数据中采样状态-目标-动作块，然后通过VLM获得条件嵌入。接着，向动作块中注入高斯噪声以进行正则化，再将带噪的动作数据通过一系列可逆的流层，最终映射到一个简单的潜在分布（如标准正态分布）。损失函数通过最大化数据在真实分布下的对数似然来计算。推理时，从潜在分布中采样一个点，然后逆向通过流层，单次前向传播即可生成一个动作块。此外，方法中还集成了一种可训练的线性层（PLU）以增强模型表达能力。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集: LIBERO基准，包含LIBERO Spatial, LIBERO Object, LIBERO Goal, LIBERO 10, LIBERO 90等多个模拟任务子集。</p>\n<p>训练资源:</p>\n<ul><li>硬件: 使用NVIDIA H100 GPU进行训练。</li><li>软件: 基于FLOWER代码库，VLM部分使用Florence-2 Large模型。</li><li>训练配置: 训练了100个epoch，批次大小为80。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境: 在LIBERO基准的模拟环境中进行评估。</p>\n<p>评估指标:</p>\n<ul><li>主要指标: 任务成功率，即在各个LIBERO任务子集上完成任务的百分比，并计算平均成功率。</li><li>次要指标: 推理时间（每次动作生成所需的秒数）和模型参数量（以百万为单位），用于衡量模型的效率。</li></ul>"
  },
  {
    "date": "2025-08-22",
    "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
    "link": "http://arxiv.org/abs/2508.16292",
    "summary_markdown": "# 论文研究单位\nUniversity of California, Berkeley\n# 论文概述\n该论文探讨了视觉-语言-动作（VLA）模型在处理基于错误前提的指令时面临的挑战，即那些引用环境中不存在的对象或条件的指令。作者指出，当前大多数VLA模型缺乏识别和回应此类不可能指令的机制。为了解决这个问题，论文提出了一个名为“Instruct-Verify-and-Act (IVA)”的统一框架。该框架旨在使VLA模型能够（一）检测因错误前提而无法执行的指令，（二）通过语言进行澄清或纠正，以及（三）将感知和动作与合理的替代方案相结合。论文通过构建一个包含成对正确指令和错误前提指令的大规模半合成数据集，对VLA模型进行了指令微调，实现了对错误前提指令的鲁棒检测和自然语言纠正。\n# 论文核心贡献点\n1. 提出了Instruct-Verify-and-Act (IVA)框架，这是首个致力于让VLA模型能够识别、解释并纠正错误前提指令的统一框架。\n2. 构建了一个大规模、上下文增强的半合成指令数据集，该数据集包含成对的正样本和错误前提指令，覆盖了“域内”和“域外”两种错误类型，专门用于训练模型处理此类情况。\n3. 实验结果表明，与基线模型相比，IVA在错误前提检测准确率上提升了97.56%，在错误前提场景下的成功响应率提升了50.78%，同时保持了处理正常指令的基本能力。\n# 论文方法描述\n论文的方法基于LLARVA模型，一个为机器人指令跟踪设计的VLA架构。该方法主要包括模型构建、数据集生成和训练过程三个部分。\n模型上，IVA由三部分组成：一个冻结的CLIP ViT-L/14视觉编码器，用于将RGB图像观测编码为视觉token；一个语言编码器，用于对包含机器人类型、控制模式、任务描述和先前本体感觉状态的结构化自然语言指令进行编码；一个多模态自回归transformer解码器，用于融合视觉和语言token，并预测未来的机器人动作序列和2D视觉轨迹。\n数据集上，作者在RLBench数据集的轨迹基础上生成了一个错误前提指令数据集。该数据集包含两类错误指令：“域内错误指令”，指引用场景中存在但当前任务不相关的对象，期望模型进行纠正；“域外错误指令”，指引用完全不可能的对象（如大象），期望模型拒绝。训练数据按特定比例混合了正样本和错误指令。\n训练上，作者采用端到端的指令微调方法。在保持视觉和语言编码器冻结的同时，使用LoRA适配器对多模态解码器进行微调。训练目标是让模型在给定图像和指令后，能够自回归地生成正确的动作序列或适当的语言纠正/拒绝响应。\n# 论文使用数据集和训练资源\n1. **数据集**:\n * **预训练数据**: Open X-Embodiment (OXE) 数据集。\n * **微调数据**: 基于RLBench轨迹构建的自定义“错误前提指令数据集”。\n * **数据构成**: 每个任务使用800个回合进行训练。其中约20%的回合包含域外错误指令，约65%的回合包含域内错误指令（这些指令被随机注入到10%的步骤中）。\n2. **训练资源**:\n * 论文未明确说明所使用的具体硬件资源（如GPU类型和数量），但其方法涉及对大型多模态模型（基于LLARVA）进行微调，使用了LoRA适配器，这通常需要相当大的GPU计算资源。\n# 论文使用的评估环境和评估指标\n1. **评估环境**:\n * **模拟器**: RLBench。\n * **任务**: 9个RLBench任务。\n * **设置**: 每个任务生成25个评估回合，物体位置随机化。模型输入为前摄像头视角图像和前5个时间步的机器人关节位置。\n * **基线模型**: LLARVA模型。\n2. **评估指标**:\n * **错误前提 (FP) 检测率**: 模型正确识别出指令为错误前提的百分比。该指标分别针对“域内”和“域外”错误指令进行报告。\n * **错误前提成功率**: 模型在接收到错误前提指令后，能够做出成功响应（如：对于域内指令提出正确替代方案，或对于域外指令明确拒绝）的百分比。\n * **真实前提 (TP) 成功率**: 模型在接收到正确、可执行的指令后，成功完成任务的百分比。\n * **总体成功率**: 错误前提成功率和真实前提成功率的平均值，作为综合性能的衡量标准。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>University of California, Berkeley</p>\n<h1>论文概述</h1>\n<p>该论文探讨了视觉-语言-动作（VLA）模型在处理基于错误前提的指令时面临的挑战，即那些引用环境中不存在的对象或条件的指令。作者指出，当前大多数VLA模型缺乏识别和回应此类不可能指令的机制。为了解决这个问题，论文提出了一个名为“Instruct-Verify-and-Act (IVA)”的统一框架。该框架旨在使VLA模型能够（一）检测因错误前提而无法执行的指令，（二）通过语言进行澄清或纠正，以及（三）将感知和动作与合理的替代方案相结合。论文通过构建一个包含成对正确指令和错误前提指令的大规模半合成数据集，对VLA模型进行了指令微调，实现了对错误前提指令的鲁棒检测和自然语言纠正。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出了Instruct-Verify-and-Act (IVA)框架，这是首个致力于让VLA模型能够识别、解释并纠正错误前提指令的统一框架。</li><li>构建了一个大规模、上下文增强的半合成指令数据集，该数据集包含成对的正样本和错误前提指令，覆盖了“域内”和“域外”两种错误类型，专门用于训练模型处理此类情况。</li><li>实验结果表明，与基线模型相比，IVA在错误前提检测准确率上提升了97.56%，在错误前提场景下的成功响应率提升了50.78%，同时保持了处理正常指令的基本能力。</li></ol>\n<h1>论文方法描述</h1>\n<p>论文的方法基于LLARVA模型，一个为机器人指令跟踪设计的VLA架构。该方法主要包括模型构建、数据集生成和训练过程三个部分。</p>\n<p>模型上，IVA由三部分组成：一个冻结的CLIP ViT-L/14视觉编码器，用于将RGB图像观测编码为视觉token；一个语言编码器，用于对包含机器人类型、控制模式、任务描述和先前本体感觉状态的结构化自然语言指令进行编码；一个多模态自回归transformer解码器，用于融合视觉和语言token，并预测未来的机器人动作序列和2D视觉轨迹。</p>\n<p>数据集上，作者在RLBench数据集的轨迹基础上生成了一个错误前提指令数据集。该数据集包含两类错误指令：“域内错误指令”，指引用场景中存在但当前任务不相关的对象，期望模型进行纠正；“域外错误指令”，指引用完全不可能的对象（如大象），期望模型拒绝。训练数据按特定比例混合了正样本和错误指令。</p>\n<p>训练上，作者采用端到端的指令微调方法。在保持视觉和语言编码器冻结的同时，使用LoRA适配器对多模态解码器进行微调。训练目标是让模型在给定图像和指令后，能够自回归地生成正确的动作序列或适当的语言纠正/拒绝响应。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>1. <strong>数据集</strong>:</p>\n<p> * <strong>预训练数据</strong>: Open X-Embodiment (OXE) 数据集。</p>\n<p> * <strong>微调数据</strong>: 基于RLBench轨迹构建的自定义“错误前提指令数据集”。</p>\n<p> * <strong>数据构成</strong>: 每个任务使用800个回合进行训练。其中约20%的回合包含域外错误指令，约65%的回合包含域内错误指令（这些指令被随机注入到10%的步骤中）。</p>\n<p>2. <strong>训练资源</strong>:</p>\n<p> * 论文未明确说明所使用的具体硬件资源（如GPU类型和数量），但其方法涉及对大型多模态模型（基于LLARVA）进行微调，使用了LoRA适配器，这通常需要相当大的GPU计算资源。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>1. <strong>评估环境</strong>:</p>\n<p> * <strong>模拟器</strong>: RLBench。</p>\n<p> * <strong>任务</strong>: 9个RLBench任务。</p>\n<p> * <strong>设置</strong>: 每个任务生成25个评估回合，物体位置随机化。模型输入为前摄像头视角图像和前5个时间步的机器人关节位置。</p>\n<p> * <strong>基线模型</strong>: LLARVA模型。</p>\n<p>2. <strong>评估指标</strong>:</p>\n<p> * <strong>错误前提 (FP) 检测率</strong>: 模型正确识别出指令为错误前提的百分比。该指标分别针对“域内”和“域外”错误指令进行报告。</p>\n<p> * <strong>错误前提成功率</strong>: 模型在接收到错误前提指令后，能够做出成功响应（如：对于域内指令提出正确替代方案，或对于域外指令明确拒绝）的百分比。</p>\n<p> * <strong>真实前提 (TP) 成功率</strong>: 模型在接收到正确、可执行的指令后，成功完成任务的百分比。</p>\n<p> * <strong>总体成功率</strong>: 错误前提成功率和真实前提成功率的平均值，作为综合性能的衡量标准。</p>"
  },
  {
    "date": "2025-08-21",
    "title": "Survey of Vision-Language-Action Models for Embodied Manipulation",
    "link": "http://arxiv.org/abs/2508.15201",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-19",
    "title": "CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2508.13446",
    "summary_markdown": "## 论文研究单位\nUniversity of California Berkeley, Princeton University\n## 论文概述\n本文提出了一种名为CAST（Counterfactual Augmentation with Synthetic Trajectories）的新方法，用于改进视觉-语言-动作（VLA）模型遵循指令的能力。当前VLA模型在遵循细粒度命令方面存在困难，原因在于现有机器人数据集缺乏语义多样性和语言基础。CAST利用视觉-语言模型（VLM）为现有机器人数据集生成反事实标签，通过增加语言基础的多样性和粒度来提升VLA的性能。该方法无需额外收集数据，通过合成反事实语言和动作，显著提高了策略遵循指令的能力。在三个真实环境中的视觉语言导航实验表明，使用CAST的策略在导航任务上的成功率比基线提高了27%，达到了与最先进方法相当的水平。\n## 论文核心贡献点\n- 提出了CAST方法：一种新颖的数据增强方案，通过利用VLM为现有数据集生成反事实的语言和动作标签，以解决语言条件策略中的后验崩溃问题。\n- 创建了CAST数据集：通过上述方法生成的增强数据集，包含原始轨迹和分支出去的反事实轨迹。\n- 训练了CounterfactualVLA模型：一个基于PaliGemma的3B参数VLA模型，在CAST数据集上进行微调。\n- 实验验证：在27个语言指令和3个真实环境中的实验表明，该方法在指令遵循任务上比仅使用后验标签的基线提升了27%的成功率，并优于多个SOTA基线。\n- 开源贡献：公开了CAST增强代码、CounterfactualVLA策略训练代码、CAST数据集以及训练好的模型检查点。\n## 论文方法描述\nCAST方法旨在为同一观察生成多个不同的语言-动作对，迫使策略关注语言指令以选择正确动作，从而避免后验崩溃。该方法包含以下步骤：\n1. **原子标签生成**：将原始轨迹中的动作离散化为简单的原子命令，如“turn left”、“turn right”、“go forward”、“stop”等，形成一个带原子标签的数据集。\n2. **原子策略训练**：使用上述数据集训练一个原子策略，该策略专门用于执行这些短视距的原子命令。该策略基于EfficientNet-b2和扩散模型。\n3. **反事实指令生成**：在原始轨迹的决策点，使用VLM根据当前观察和原始指令生成多种可行的反事实指令及其对应的原子命令。\n4. **反事实轨迹合成**：利用训练好的原子策略，为每个反事实的原子命令采样生成相应的动作序列，从而构建出从原始轨迹分支出但未实际执行的反事实轨迹。\n5. **VLA模型训练**：将原始数据和合成的反事实数据（观察、指令、动作）结合，用于训练一个高容量的VLA模型（CounterfactualVLA），该模型基于PaliGemma，能够将复杂的语言指令映射到机器人动作。\n## 论文使用数据集和训练资源\n- **数据集**：使用GNM数据集，该数据集包含多种机器人形态（轮式机器人、四足机器人等）在室内和室外环境中的导航轨迹。通过CAST方法对该数据集进行增强，生成用于训练的CAST数据集。\n- **训练资源**：CounterfactualVLA模型在v4-8 TPU虚拟机上训练了100,000步，耗时约50小时。批次大小为192，学习率为1e-4。若使用v4-32 TPO pod，可将训练时间缩短至约20小时。\n## 论文使用的评估环境和评估指标\n- **评估环境**：在3个真实的物理环境中进行评估，包括拥挤的办公室走廊、厨房/公共区域和一个室外公园。\n- **任务**：共27个语言指令，分为三类：\n - Object Navigation：导航到特定物体（如“移动到橙色椅子”）。\n - Referential Navigation：相对于物体或结构进行导航，需要空间理解（如“移动到椅子的右边”）。\n - Continuous Navigation：执行相对于环境的连续行为（如“沿着白墙移动”）。\n- **评估指标**：主要指标是**成功率**。根据任务类型定义不同的成功标准：物体导航中，成功定义为到达目标物体50厘米范围内；关系导航中，成功定义为正确执行相对位置移动且无碰撞；连续导航中，成功定义为在参考结构1米内移动并持续行为2米以上。每个指令在每种环境中进行5次试验，结果以成功次数/总试验数（如3/5）和平均成功率报告，并计算标准误差。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>University of California Berkeley, Princeton University</p>\n<h2>论文概述</h2>\n<p>本文提出了一种名为CAST（Counterfactual Augmentation with Synthetic Trajectories）的新方法，用于改进视觉-语言-动作（VLA）模型遵循指令的能力。当前VLA模型在遵循细粒度命令方面存在困难，原因在于现有机器人数据集缺乏语义多样性和语言基础。CAST利用视觉-语言模型（VLM）为现有机器人数据集生成反事实标签，通过增加语言基础的多样性和粒度来提升VLA的性能。该方法无需额外收集数据，通过合成反事实语言和动作，显著提高了策略遵循指令的能力。在三个真实环境中的视觉语言导航实验表明，使用CAST的策略在导航任务上的成功率比基线提高了27%，达到了与最先进方法相当的水平。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了CAST方法：一种新颖的数据增强方案，通过利用VLM为现有数据集生成反事实的语言和动作标签，以解决语言条件策略中的后验崩溃问题。</li><li>创建了CAST数据集：通过上述方法生成的增强数据集，包含原始轨迹和分支出去的反事实轨迹。</li><li>训练了CounterfactualVLA模型：一个基于PaliGemma的3B参数VLA模型，在CAST数据集上进行微调。</li><li>实验验证：在27个语言指令和3个真实环境中的实验表明，该方法在指令遵循任务上比仅使用后验标签的基线提升了27%的成功率，并优于多个SOTA基线。</li><li>开源贡献：公开了CAST增强代码、CounterfactualVLA策略训练代码、CAST数据集以及训练好的模型检查点。</li></ul>\n<h2>论文方法描述</h2>\n<p>CAST方法旨在为同一观察生成多个不同的语言-动作对，迫使策略关注语言指令以选择正确动作，从而避免后验崩溃。该方法包含以下步骤：</p>\n<ol><li><strong>原子标签生成</strong>：将原始轨迹中的动作离散化为简单的原子命令，如“turn left”、“turn right”、“go forward”、“stop”等，形成一个带原子标签的数据集。</li><li><strong>原子策略训练</strong>：使用上述数据集训练一个原子策略，该策略专门用于执行这些短视距的原子命令。该策略基于EfficientNet-b2和扩散模型。</li><li><strong>反事实指令生成</strong>：在原始轨迹的决策点，使用VLM根据当前观察和原始指令生成多种可行的反事实指令及其对应的原子命令。</li><li><strong>反事实轨迹合成</strong>：利用训练好的原子策略，为每个反事实的原子命令采样生成相应的动作序列，从而构建出从原始轨迹分支出但未实际执行的反事实轨迹。</li><li><strong>VLA模型训练</strong>：将原始数据和合成的反事实数据（观察、指令、动作）结合，用于训练一个高容量的VLA模型（CounterfactualVLA），该模型基于PaliGemma，能够将复杂的语言指令映射到机器人动作。</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：使用GNM数据集，该数据集包含多种机器人形态（轮式机器人、四足机器人等）在室内和室外环境中的导航轨迹。通过CAST方法对该数据集进行增强，生成用于训练的CAST数据集。</li><li><strong>训练资源</strong>：CounterfactualVLA模型在v4-8 TPU虚拟机上训练了100,000步，耗时约50小时。批次大小为192，学习率为1e-4。若使用v4-32 TPO pod，可将训练时间缩短至约20小时。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：在3个真实的物理环境中进行评估，包括拥挤的办公室走廊、厨房/公共区域和一个室外公园。</li><li><strong>任务</strong>：共27个语言指令，分为三类：</li></ul>\n<p> - Object Navigation：导航到特定物体（如“移动到橙色椅子”）。</p>\n<p> - Referential Navigation：相对于物体或结构进行导航，需要空间理解（如“移动到椅子的右边”）。</p>\n<p> - Continuous Navigation：执行相对于环境的连续行为（如“沿着白墙移动”）。</p>\n<ul><li><strong>评估指标</strong>：主要指标是<strong>成功率</strong>。根据任务类型定义不同的成功标准：物体导航中，成功定义为到达目标物体50厘米范围内；关系导航中，成功定义为正确执行相对位置移动且无碰撞；连续导航中，成功定义为在参考结构1米内移动并持续行为2米以上。每个指令在每种环境中进行5次试验，结果以成功次数/总试验数（如3/5）和平均成功率报告，并计算标准误差。</li></ul>"
  },
  {
    "date": "2025-08-18",
    "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy",
    "link": "http://arxiv.org/abs/2508.13103",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n\n浙江大学计算机科学与技术学院、上海人工智能实验室、商汤科技研究院、南京大学、清华大学\n## 论文概述\n\n论文提出了Observation-Centric VLA (OC-VLA)框架，旨在解决视觉-语言-动作（VLA）模型在真实环境中的泛化问题。现有VLA模型存在观察空间与动作空间不一致的问题：训练数据来自不同相机视角，但模型通常在机器人基坐标系中预测末端执行器姿态，导致空间不一致。OC-VLA通过利用相机外参标定矩阵，将动作预测从机器人基坐标系转换到相机坐标系，统一了不同视角下的预测目标，从而提高模型对相机视角变化的鲁棒性和泛化能力。\n## 论文核心贡献点\n\n提出观察中心的VLA框架，将动作预测直接锚定在相机观察空间，通过相机外参矩阵实现坐标系转换\n\n轻量级、即插即用的策略，与现有VLA架构完全兼容，无需架构修改\n\n从优化角度分析了相机坐标系预测相比机器人坐标系预测的优势，相机坐标与图像坐标的转换仅需内参矩阵，而机器人坐标转换需要外参矩阵\n\n在仿真和真实机器人实验中验证了方法的有效性，显著提升任务成功率、加速收敛并增强跨视角泛化能力\n## 论文方法描述\n\n模型架构采用轻量级VLA模型（约334M参数），使用CLIP文本编码器处理语言指令，DINOv2处理RGB图像，通过Q-Former（4层）进行特征融合和压缩，最后使用LLaMA2风格的Transformer（12层）预测动作\n\n支持连续动作空间（使用Diffusion Transformer，DDPM 100步训练，DDIM 10步推理）和离散动作空间（非自回归预测）两种模式\n\n坐标系转换：利用相机外参矩阵T，将机器人基坐标系中的末端执行器姿态转换到相机坐标系，公式为A_cam = T × A_world × T^(-1)\n\n训练时使用相机坐标系下的动作作为监督目标，推理时将预测的相机坐标系动作转换回机器人坐标系用于机器人执行\n## 论文使用数据集和训练资源\n\n预训练数据集：Droid数据集，包含来自1417个不同第三人称相机视角的机器人操作轨迹\n\n仿真评估数据：ManiSkill2数据集，选择5个任务（PickCube, StackCube, PickSingleYCB, PickClutterYCB, PickSingleEGAD），生成约40,000条轨迹，每条轨迹从300,000个随机相机视角池中采样20个相机进行渲染\n\n真实机器人数据：使用Franka Emika Panda机械臂收集两组数据，Camera 1收集15个任务（固定相机），Camera 2收集8个任务（相机位置有轻微扰动），每个任务10条演示轨迹（10-shot设置）\n\n训练资源：8张NVIDIA A100 GPU，总batch size 2048（每GPU 256样本），使用AdamW优化器训练30,000步，学习率为Transformer和Q-Former 1e-4，DINOv2为1e-5\n## 论文使用的评估环境和评估指标\n\n仿真评估环境：ManiSkill2 benchmark，5个任务类型，每个任务从验证集随机采样100条轨迹进行评估（共500条轨迹）\n\n真实机器人平台：Franka Emika Panda 7自由度机械臂，配备Robotiq 2F-85夹爪和多个RealSense D435i RGB-D相机\n\n评估指标：任务成功率（Success Rate）\n\n评估设置包括三种场景：固定相机视角（与训练视角一致）、轻微相机扰动（训练时引入相机位置变化）、新相机视角（零样本评估，使用训练时未见过的相机）\n\n基线方法：OpenVLA-OFT、π0、以及使用机器人基坐标系预测的相同架构模型\n\n真实机器人任务涵盖pick & place、pouring、stacking、pick & rotation、pull & push等多种类型，共15个任务，每个任务进行10次试验",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n\n<p>浙江大学计算机科学与技术学院、上海人工智能实验室、商汤科技研究院、南京大学、清华大学</p>\n<h2>论文概述</h2>\n\n<p>论文提出了Observation-Centric VLA (OC-VLA)框架，旨在解决视觉-语言-动作（VLA）模型在真实环境中的泛化问题。现有VLA模型存在观察空间与动作空间不一致的问题：训练数据来自不同相机视角，但模型通常在机器人基坐标系中预测末端执行器姿态，导致空间不一致。OC-VLA通过利用相机外参标定矩阵，将动作预测从机器人基坐标系转换到相机坐标系，统一了不同视角下的预测目标，从而提高模型对相机视角变化的鲁棒性和泛化能力。</p>\n<h2>论文核心贡献点</h2>\n\n<p>提出观察中心的VLA框架，将动作预测直接锚定在相机观察空间，通过相机外参矩阵实现坐标系转换</p>\n\n<p>轻量级、即插即用的策略，与现有VLA架构完全兼容，无需架构修改</p>\n\n<p>从优化角度分析了相机坐标系预测相比机器人坐标系预测的优势，相机坐标与图像坐标的转换仅需内参矩阵，而机器人坐标转换需要外参矩阵</p>\n\n<p>在仿真和真实机器人实验中验证了方法的有效性，显著提升任务成功率、加速收敛并增强跨视角泛化能力</p>\n<h2>论文方法描述</h2>\n\n<p>模型架构采用轻量级VLA模型（约334M参数），使用CLIP文本编码器处理语言指令，DINOv2处理RGB图像，通过Q-Former（4层）进行特征融合和压缩，最后使用LLaMA2风格的Transformer（12层）预测动作</p>\n\n<p>支持连续动作空间（使用Diffusion Transformer，DDPM 100步训练，DDIM 10步推理）和离散动作空间（非自回归预测）两种模式</p>\n\n<p>坐标系转换：利用相机外参矩阵T，将机器人基坐标系中的末端执行器姿态转换到相机坐标系，公式为A_cam = T × A_world × T^(-1)</p>\n\n<p>训练时使用相机坐标系下的动作作为监督目标，推理时将预测的相机坐标系动作转换回机器人坐标系用于机器人执行</p>\n<h2>论文使用数据集和训练资源</h2>\n\n<p>预训练数据集：Droid数据集，包含来自1417个不同第三人称相机视角的机器人操作轨迹</p>\n\n<p>仿真评估数据：ManiSkill2数据集，选择5个任务（PickCube, StackCube, PickSingleYCB, PickClutterYCB, PickSingleEGAD），生成约40,000条轨迹，每条轨迹从300,000个随机相机视角池中采样20个相机进行渲染</p>\n\n<p>真实机器人数据：使用Franka Emika Panda机械臂收集两组数据，Camera 1收集15个任务（固定相机），Camera 2收集8个任务（相机位置有轻微扰动），每个任务10条演示轨迹（10-shot设置）</p>\n\n<p>训练资源：8张NVIDIA A100 GPU，总batch size 2048（每GPU 256样本），使用AdamW优化器训练30,000步，学习率为Transformer和Q-Former 1e-4，DINOv2为1e-5</p>\n<h2>论文使用的评估环境和评估指标</h2>\n\n<p>仿真评估环境：ManiSkill2 benchmark，5个任务类型，每个任务从验证集随机采样100条轨迹进行评估（共500条轨迹）</p>\n\n<p>真实机器人平台：Franka Emika Panda 7自由度机械臂，配备Robotiq 2F-85夹爪和多个RealSense D435i RGB-D相机</p>\n\n<p>评估指标：任务成功率（Success Rate）</p>\n\n<p>评估设置包括三种场景：固定相机视角（与训练视角一致）、轻微相机扰动（训练时引入相机位置变化）、新相机视角（零样本评估，使用训练时未见过的相机）</p>\n\n<p>基线方法：OpenVLA-OFT、π0、以及使用机器人基坐标系预测的相同架构模型</p>\n\n<p>真实机器人任务涵盖pick & place、pouring、stacking、pick & rotation、pull & push等多种类型，共15个任务，每个任务进行10次试验</p>"
  },
  {
    "date": "2025-08-18",
    "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey",
    "link": "http://arxiv.org/abs/2508.13073",
    "summary_markdown": "# 论文研究单位\n哈尔滨工业大学（深圳）计算机科学与技术学院\n# 论文概述\n论文首次系统性地综述了基于大型视觉-语言模型（VLM）的视觉-语言-行动（VLA）模型在机器人操作中的应用，涵盖单片式与分层式架构、与其他学习范式的结合、关键特性、数据集与基准，以及未来方向。图1概述了VLA模型的核心优势：开放世界泛化、分层任务规划、知识增强推理与多模态融合。图2给出了综述组织与时间线；图3展示了两类主范式。\n# 论文核心贡献点\n- 提出清晰的VLA定义与两级分类：单片式（包括单系统与双系统）与分层式（规划器+执行器，带可解释中间表示）。\n- 纵向梳理VLM与操作学习演进路线，呈现大型VLM驱动VLA的历史脉络与代表工作。\n- 横向比较两类架构的优缺点：单片式强调端到端统一与推理效率，分层式强调可解释的中间表示与模块化训练。\n- 汇总高级前沿方向：强化学习、无训练优化、从人类视频学习、世界模型融合等。\n- 系统总结VLA特征：多模态融合、指令跟随、多维泛化。\n- 分类整理数据集与基准：现实机器人、仿真、人类行为、具身任务。\n- 识别未来方向：记忆机制、4D感知、高效适配、多智能体协同等。\n# 论文方法描述\n- 架构范式\n - 单片式（Monolithic）\n - 单系统（Single-system）：视觉、语言与机器人状态统一输入，LLM自回归生成离散动作令牌（RT-1/2、OpenVLA等）。\n - 双系统（Dual-system）：系统2（VLM背骨）负责慢速但泛化的推理；系统1（动作专家）负责快速执行（DP-VLA、RoboDual、LCB、GR00T N1、CogACT、ChatVLA系列、Fast-in-Slow等）。\n - 分层式（Hierarchical）：显式解耦规划与执行，通过可解释中间表示（子任务、关键点、程序、轨迹、可用性地图等）连接Planner与Policy；支持子任务式、关键点式、程序式等子方法。\n- 性能增强\n - 感知增强：引入3D与4D时序、触觉、音频等多模态（Leo Agent、SpatialVLA、TraceVLA、4D-VLA、ST-VLA、VTLA、VLAS、FuSe、OE-VLA）。\n - 推理增强：链式思维与视觉链式思维（ECoT、CoT-VLA）、层级闭环控制（LoHoVLA）、选择性迁移微调（ReFineVLA）。\n - 泛化增强：统一行动码本（UniAct）、可逆训练（ReVLA）、扩散+自回归协同生成（HybridVLA）、集成投票（VOTE）、世界模型/动力学学习（WorldVLA、UnifiedVLA、UP-VLA）。\n- 推理效率优化\n - 架构：层路由/早退（MoLe-VLA、DeeR-VLA、CogVLA），引入Mamba结构（RoboMamba）。\n - 参数：小参数模型（NORA）、低位宽/三值权重（BitVLA）。\n - 解码：并行解码（RoboFlamingo、PD-VLA）、投机解码（Spec-VLA）、触发式跳推理（FlashVLA）。\n- 其他高级方向\n - 强化学习（RL）集成、免训练优化、从人类视频学习、世界模型驱动VLA等。\n# 论文使用数据集和训练资源\n- 现实机器人数据集与基准：Open X-Embodiment（跨实体）、BridgeData、DROID、RT-1/RT-2演示集等。\n- 仿真数据集与基准：常用模拟操控数据集与评测基准（用于高效训练与泛化评估）。\n- 人类行为数据集：人类演示视频与交互数据（支持模仿/从视频学习）。\n- 具身数据集与基准：多任务/多场景具身评测集合。\n# 论文使用的评估环境和评估指标\n- 评估环境：真实机器人平台与仿真环境；跨实体、跨任务与多模态组合的具身场景。\n- 评估指标：典型操控任务指标，如成功率、路径误差、任务完成度、泛化到新对象/指令、推理质量与效率指标（延迟/吞吐）；论文以综述性评估与特性分析为主，未提供统一的数值对比。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>哈尔滨工业大学（深圳）计算机科学与技术学院</p>\n<h1>论文概述</h1>\n<p>论文首次系统性地综述了基于大型视觉-语言模型（VLM）的视觉-语言-行动（VLA）模型在机器人操作中的应用，涵盖单片式与分层式架构、与其他学习范式的结合、关键特性、数据集与基准，以及未来方向。图1概述了VLA模型的核心优势：开放世界泛化、分层任务规划、知识增强推理与多模态融合。图2给出了综述组织与时间线；图3展示了两类主范式。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出清晰的VLA定义与两级分类：单片式（包括单系统与双系统）与分层式（规划器+执行器，带可解释中间表示）。</li><li>纵向梳理VLM与操作学习演进路线，呈现大型VLM驱动VLA的历史脉络与代表工作。</li><li>横向比较两类架构的优缺点：单片式强调端到端统一与推理效率，分层式强调可解释的中间表示与模块化训练。</li><li>汇总高级前沿方向：强化学习、无训练优化、从人类视频学习、世界模型融合等。</li><li>系统总结VLA特征：多模态融合、指令跟随、多维泛化。</li><li>分类整理数据集与基准：现实机器人、仿真、人类行为、具身任务。</li><li>识别未来方向：记忆机制、4D感知、高效适配、多智能体协同等。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>架构范式</li></ul>\n<p> - 单片式（Monolithic）</p>\n<p> - 单系统（Single-system）：视觉、语言与机器人状态统一输入，LLM自回归生成离散动作令牌（RT-1/2、OpenVLA等）。</p>\n<p> - 双系统（Dual-system）：系统2（VLM背骨）负责慢速但泛化的推理；系统1（动作专家）负责快速执行（DP-VLA、RoboDual、LCB、GR00T N1、CogACT、ChatVLA系列、Fast-in-Slow等）。</p>\n<p> - 分层式（Hierarchical）：显式解耦规划与执行，通过可解释中间表示（子任务、关键点、程序、轨迹、可用性地图等）连接Planner与Policy；支持子任务式、关键点式、程序式等子方法。</p>\n<ul><li>性能增强</li></ul>\n<p> - 感知增强：引入3D与4D时序、触觉、音频等多模态（Leo Agent、SpatialVLA、TraceVLA、4D-VLA、ST-VLA、VTLA、VLAS、FuSe、OE-VLA）。</p>\n<p> - 推理增强：链式思维与视觉链式思维（ECoT、CoT-VLA）、层级闭环控制（LoHoVLA）、选择性迁移微调（ReFineVLA）。</p>\n<p> - 泛化增强：统一行动码本（UniAct）、可逆训练（ReVLA）、扩散+自回归协同生成（HybridVLA）、集成投票（VOTE）、世界模型/动力学学习（WorldVLA、UnifiedVLA、UP-VLA）。</p>\n<ul><li>推理效率优化</li></ul>\n<p> - 架构：层路由/早退（MoLe-VLA、DeeR-VLA、CogVLA），引入Mamba结构（RoboMamba）。</p>\n<p> - 参数：小参数模型（NORA）、低位宽/三值权重（BitVLA）。</p>\n<p> - 解码：并行解码（RoboFlamingo、PD-VLA）、投机解码（Spec-VLA）、触发式跳推理（FlashVLA）。</p>\n<ul><li>其他高级方向</li></ul>\n<p> - 强化学习（RL）集成、免训练优化、从人类视频学习、世界模型驱动VLA等。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>现实机器人数据集与基准：Open X-Embodiment（跨实体）、BridgeData、DROID、RT-1/RT-2演示集等。</li><li>仿真数据集与基准：常用模拟操控数据集与评测基准（用于高效训练与泛化评估）。</li><li>人类行为数据集：人类演示视频与交互数据（支持模仿/从视频学习）。</li><li>具身数据集与基准：多任务/多场景具身评测集合。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：真实机器人平台与仿真环境；跨实体、跨任务与多模态组合的具身场景。</li><li>评估指标：典型操控任务指标，如成功率、路径误差、任务完成度、泛化到新对象/指令、推理质量与效率指标（延迟/吞吐）；论文以综述性评估与特性分析为主，未提供统一的数值对比。</li></ul>"
  },
  {
    "date": "2025-08-17",
    "title": "Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search",
    "link": "http://arxiv.org/abs/2508.12211",
    "summary_markdown": "# 论文总结\n\n**论文研究单位**\n- Mila — Quebec AI Institute, Canada\n- Université de Montréal, Canada\n- The University of British Columbia, Canada\n\n**论文概述**\n提出视觉-语言-动作规划与搜索（VLAPS）框架，通过将基于模型的搜索（MCTS）集成到预训练VLA策略的推理过程中，解决VLA模型在复杂环境中的脆弱性问题。该方法利用VLA模型指导搜索过程，使其能够高效探索大空间中的语言条件机器人任务，显著提升成功率（最高67个百分点）。\n\n**论文核心贡献点**\n1. 集成VLA策略与MCTS的VLAPS框架\n2. 自动定义任务导向搜索空间的算法\n3. 基于VLA的节点选择引导机制\n4. 实验验证在所有测试场景中均优于VLA基线\n5. 自适应计算分配机制（失败任务分配更多搜索时间）\n\n**论文方法描述**\n1. **VLA驱动的行动块采样**\n - 定义有限行动块库Φ（2000个K-medoids聚类原型）\n - 使用VLA策略构建上下文相关采样分布βΦ\n - 每个节点采样k=10个候选行动块进行搜索\n\n2. **VLA引导的树遍历**\n - 采用PUCT选择策略（Q≡0）\n - 使用VLA先验ψΦv分配节点访问次数\n - 仿真VLA策略至任务完成或达最大步长\n\n3. **搜索终止条件**\n - 发现目标状态即返回轨迹\n - 或在600秒计算预算内返回最高频动作序列\n\n**论文使用数据集和训练资源**\n- **数据集**：Libero模拟机器人数据集（五个任务套件：Spatial、Goal、Object、10、90）\n- **基础VLA模型**：Octo（97M参数）\n- **训练资源**：\n - NVIDIA A100 GPU\n - 演示数据过滤no-op动作\n - K-medoids聚类生成2000个行动块原型\n\n**论文使用的评估环境和评估指标**\n- **评估环境**：\n - Libero模拟套件\n - 256×256固定相机图像\n - 128×128腕部相机图像\n- **评估指标**：\n - **任务成功率**（基于1000次测试）\n - **算法运行时间**（仅统计成功案例）\n - **不同微调阶段的性能对比**（10k-200k步）\n- **实验设置**：\n - 每次搜索300蒙特卡洛样本\n - 行动块时域H=4\n - 最大搜索深度100\n - 600秒超时限制\n\n实验显示VLAPS在所有任务套件中始终优于VLA基线，尤其在基础成功率较低时提升最大（如Libero-Object从6%提升至73%）。当基础VLA改进时，VLAPS搜索时间显著减少。自适应计算分配机制使失败任务获得更多搜索资源。",
    "summary_html": "<h1>论文总结</h1>\n\n<p><strong>论文研究单位</strong></p>\n<ul><li>Mila — Quebec AI Institute, Canada</li><li>Université de Montréal, Canada</li><li>The University of British Columbia, Canada</li></ul>\n\n<p><strong>论文概述</strong></p>\n<p>提出视觉-语言-动作规划与搜索（VLAPS）框架，通过将基于模型的搜索（MCTS）集成到预训练VLA策略的推理过程中，解决VLA模型在复杂环境中的脆弱性问题。该方法利用VLA模型指导搜索过程，使其能够高效探索大空间中的语言条件机器人任务，显著提升成功率（最高67个百分点）。</p>\n\n<p><strong>论文核心贡献点</strong></p>\n<ol><li>集成VLA策略与MCTS的VLAPS框架</li><li>自动定义任务导向搜索空间的算法</li><li>基于VLA的节点选择引导机制</li><li>实验验证在所有测试场景中均优于VLA基线</li><li>自适应计算分配机制（失败任务分配更多搜索时间）</li></ol>\n\n<p><strong>论文方法描述</strong></p>\n<p>1. <strong>VLA驱动的行动块采样</strong></p>\n<p> - 定义有限行动块库Φ（2000个K-medoids聚类原型）</p>\n<p> - 使用VLA策略构建上下文相关采样分布βΦ</p>\n<p> - 每个节点采样k=10个候选行动块进行搜索</p>\n\n<p>2. <strong>VLA引导的树遍历</strong></p>\n<p> - 采用PUCT选择策略（Q≡0）</p>\n<p> - 使用VLA先验ψΦv分配节点访问次数</p>\n<p> - 仿真VLA策略至任务完成或达最大步长</p>\n\n<p>3. <strong>搜索终止条件</strong></p>\n<p> - 发现目标状态即返回轨迹</p>\n<p> - 或在600秒计算预算内返回最高频动作序列</p>\n\n<p><strong>论文使用数据集和训练资源</strong></p>\n<ul><li><strong>数据集</strong>：Libero模拟机器人数据集（五个任务套件：Spatial、Goal、Object、10、90）</li><li><strong>基础VLA模型</strong>：Octo（97M参数）</li><li><strong>训练资源</strong>：</li></ul>\n<p> - NVIDIA A100 GPU</p>\n<p> - 演示数据过滤no-op动作</p>\n<p> - K-medoids聚类生成2000个行动块原型</p>\n\n<p><strong>论文使用的评估环境和评估指标</strong></p>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - Libero模拟套件</p>\n<p> - 256×256固定相机图像</p>\n<p> - 128×128腕部相机图像</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率</strong>（基于1000次测试）</p>\n<p> - <strong>算法运行时间</strong>（仅统计成功案例）</p>\n<p> - <strong>不同微调阶段的性能对比</strong>（10k-200k步）</p>\n<ul><li><strong>实验设置</strong>：</li></ul>\n<p> - 每次搜索300蒙特卡洛样本</p>\n<p> - 行动块时域H=4</p>\n<p> - 最大搜索深度100</p>\n<p> - 600秒超时限制</p>\n\n<p>实验显示VLAPS在所有任务套件中始终优于VLA基线，尤其在基础成功率较低时提升最大（如Libero-Object从6%提升至73%）。当基础VLA改进时，VLAPS搜索时间显著减少。自适应计算分配机制使失败任务获得更多搜索资源。</p>"
  },
  {
    "date": "2025-08-16",
    "title": "Toward General Physical Intelligence for Resilient Agile Manufacturing Automation",
    "link": "http://arxiv.org/abs/2508.11960",
    "summary_markdown": "```markdown\n# 论文研究单位\nNortheastern University (美国马萨诸塞州波士顿)\nConsiglio Nazionale delle Ricerche (意大利热那亚)\nColumbia Sportswear (美国俄勒冈州波特兰)\nKINETICAI INC (美国德克萨斯州)\nSunbelt Rentals Inc (美国南卡罗来纳州)\nBmade Robotics (英国伦敦)\n# 论文概述\n本文探讨了通用物理智能(GPI)在弹性敏捷制造自动化中的应用。随着工业5.0的发展，市场需求从大规模生产转向大规模定制，中小型企业需要能够进行上下文推理和在非结构化环境中安全交互的弹性机器人解决方案。论文系统性调查了视觉-语言-行动(VLA)模型在GPI背景下的最新进展，进行了全面的比较分析，并通过结构化消融研究评估了其工业部署准备情况。最后，论文阐述了开放研究挑战并提出了将GPI更好地集成到下一代工业生态系统的方向。\n# 论文核心贡献点\n1. 对GPI在敏捷制造中的作用提供了全面视角，强调了其在实现人与机器人协作、适应复杂环境和执行多样化车间任务方面的潜力\n2. 将最先进方法组织为五个主题支柱：多感官表示学习、数据生成与仿真到现实迁移、规划与控制框架、不确定性估计与安全保障、基准测试与评估协议\n3. 进行了结构化消融研究，量化了每个组件对系统性能的影响，特别是在精确螺母螺栓装配任务上的表现\n4. 提出了将GPI集成到符合工业5.0的下一代工业生态系统的方向\n# 论文方法描述\n论文提出了GPI框架的数学形式化定义，通过策略πθ将多模态观察映射到物理基础行动。学习目标结合了动作预测损失、前向模型预测损失和任务成功损失。对主流VLA模型(Gato、RT-2、PaLM-E、OpenVLA、VIMA)进行了工业适配改造，增加了本体感受、触觉反馈和体现描述符。设计了双系统架构：系统1(规划导向过程)和系统2(快速反射控制器)，将高级推理与精确电机控制相结合。\n# 论文使用数据集和训练资源\n实验在NVIDIA Isaac Sim-5.0环境中进行，使用7自由度Franka Emika FR3机器人平台配备平行夹爪。所有试验在高性能系统上执行(Alienware Dell配备13th Gen Intel Core i9、32 GB DDR5 RAM和NVIDIA RTX 4060)。实验任务为精确螺母螺栓装配，具有0.5毫米的紧密螺纹匹配公差。\n# 论文使用的评估环境和评估指标\n评估在NVIDIA Isaac Sim-5.0的高保真物理环境中进行。评估指标包括：\n1. 成功率(%)：成功完成的试验百分比，报告为均值和标准差\n2. 时间(s)：成功试验的平均持续时间\n3. 位姿误差(mm)：末端执行器与目标位姿的最终欧几里得距离\n4. 角度误差(°)：旋转差异测量\n5. 泛化分数：量化随机试验中性能一致性的归一化指标(0-1)\n6. 控制频率(Hz)：模型向机器人控制器发送命令的速率\n7. 故障模式：任务失败主要原因的分类统计\n```",
    "summary_html": "<p>```markdown</p>\n<h1>论文研究单位</h1>\n<p>Northeastern University (美国马萨诸塞州波士顿)</p>\n<p>Consiglio Nazionale delle Ricerche (意大利热那亚)</p>\n<p>Columbia Sportswear (美国俄勒冈州波特兰)</p>\n<p>KINETICAI INC (美国德克萨斯州)</p>\n<p>Sunbelt Rentals Inc (美国南卡罗来纳州)</p>\n<p>Bmade Robotics (英国伦敦)</p>\n<h1>论文概述</h1>\n<p>本文探讨了通用物理智能(GPI)在弹性敏捷制造自动化中的应用。随着工业5.0的发展，市场需求从大规模生产转向大规模定制，中小型企业需要能够进行上下文推理和在非结构化环境中安全交互的弹性机器人解决方案。论文系统性调查了视觉-语言-行动(VLA)模型在GPI背景下的最新进展，进行了全面的比较分析，并通过结构化消融研究评估了其工业部署准备情况。最后，论文阐述了开放研究挑战并提出了将GPI更好地集成到下一代工业生态系统的方向。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>对GPI在敏捷制造中的作用提供了全面视角，强调了其在实现人与机器人协作、适应复杂环境和执行多样化车间任务方面的潜力</li><li>将最先进方法组织为五个主题支柱：多感官表示学习、数据生成与仿真到现实迁移、规划与控制框架、不确定性估计与安全保障、基准测试与评估协议</li><li>进行了结构化消融研究，量化了每个组件对系统性能的影响，特别是在精确螺母螺栓装配任务上的表现</li><li>提出了将GPI集成到符合工业5.0的下一代工业生态系统的方向</li></ol>\n<h1>论文方法描述</h1>\n<p>论文提出了GPI框架的数学形式化定义，通过策略πθ将多模态观察映射到物理基础行动。学习目标结合了动作预测损失、前向模型预测损失和任务成功损失。对主流VLA模型(Gato、RT-2、PaLM-E、OpenVLA、VIMA)进行了工业适配改造，增加了本体感受、触觉反馈和体现描述符。设计了双系统架构：系统1(规划导向过程)和系统2(快速反射控制器)，将高级推理与精确电机控制相结合。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>实验在NVIDIA Isaac Sim-5.0环境中进行，使用7自由度Franka Emika FR3机器人平台配备平行夹爪。所有试验在高性能系统上执行(Alienware Dell配备13th Gen Intel Core i9、32 GB DDR5 RAM和NVIDIA RTX 4060)。实验任务为精确螺母螺栓装配，具有0.5毫米的紧密螺纹匹配公差。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>评估在NVIDIA Isaac Sim-5.0的高保真物理环境中进行。评估指标包括：</p>\n<ol><li>成功率(%)：成功完成的试验百分比，报告为均值和标准差</li><li>时间(s)：成功试验的平均持续时间</li><li>位姿误差(mm)：末端执行器与目标位姿的最终欧几里得距离</li><li>角度误差(°)：旋转差异测量</li><li>泛化分数：量化随机试验中性能一致性的归一化指标(0-1)</li><li>控制频率(Hz)：模型向机器人控制器发送命令的速率</li><li>故障模式：任务失败主要原因的分类统计</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-08-14",
    "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model",
    "link": "http://arxiv.org/abs/2508.10416",
    "summary_markdown": "论文研究单位\n- 作者信息：Zhuoyuan Yu, Yuxing Long, Zihan Yang, Chengyan Zeng, Hongwei Fan, Jiyao Zhang, Hao Dong\n\n论文概述\n- 本文提出了一种名为\"Self-correction Flywheel\"的后训练范式，用于增强视觉-语言-动作(VLA)导航模型。现有视觉-语言导航模型在执行指令时常偏离正确轨迹，且缺乏有效的错误纠正能力。该方法将模型在训练集上的错误轨迹视为有价值的数据源，通过检测这些错误轨迹的偏差并自动生成感知和动作的自我纠正数据来驱动模型的持续训练。经过多轮飞轮迭代，开发的单目RGB导航模型CorrectNav在R2R-CE和RxR-CE基准测试中分别达到65.1%和69.3%的新SOTA成功率。\n\n论文核心贡献点\n- 提出了Self-correction Flywheel后训练范式，将模型错误轨迹转化为训练数据\n- 开发了自动轨迹偏差检测方法，精确定位错误位置\n- 设计了创新的自我纠正数据生成技术，从动作和感知角度创建纠错数据\n- 实现了多轮飞轮迭代训练机制，持续提升导航模型性能\n- 开发的CorrectNav模型在模拟和真实机器人环境中均表现出优异的纠错和导航能力\n\n论文方法描述\n- 模型结构：CorrectNav由SigLIP视觉编码器、2层MLP投影器和Qwen2大语言模型组成，初始化自LLaVA-Video 7B\n- 导航微调策略：包括导航动作预测（使用210万+步级数据）、轨迹指令生成和通用多模态数据召回，并实施相机高度、视野等随机化增强视觉多样性\n- Self-correction Flywheel流程：(1)在训练集评估模型收集错误轨迹；(2)检测偏差并定位；(3)创建纠错轨迹数据（使用轨迹规划器生成恢复路径）和关键帧感知数据（使用多模态LLM生成描述和QA）；(4)继续训练模型；(5)多轮迭代\n\n论文使用数据集和训练资源\n- 数据集：VLN-CE的R2R和RxR训练集，包括52.7K样本（R2R）和158万样本（RxR）\n- 训练资源：8块NVIDIA A100 GPU，导航微调需80小时，单次飞轮迭代需20小时\n- 多模态数据：从LLaVA-Video数据集中随机采样24万个ActivityQA和NextQA训练实例\n\n论文使用的评估环境和评估指标\n- 模拟环境：使用Habitat 3.0模拟器在VLN-CE基准的R2R-CE和RxR-CE的Val-Unseen分割上评估\n- 真实环境：在AgiBot Lingxi D1四足机器人平台上测试，部署在远程A100 GPU服务器\n- 评估指标：导航错误(NE)、成功率(SR)、加权路径长度(SPL)、标准化动态时间规整(nDTW)、路径长度误差(NE<3m)",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>作者信息：Zhuoyuan Yu, Yuxing Long, Zihan Yang, Chengyan Zeng, Hongwei Fan, Jiyao Zhang, Hao Dong</li></ul>\n\n<p>论文概述</p>\n<ul><li>本文提出了一种名为\"Self-correction Flywheel\"的后训练范式，用于增强视觉-语言-动作(VLA)导航模型。现有视觉-语言导航模型在执行指令时常偏离正确轨迹，且缺乏有效的错误纠正能力。该方法将模型在训练集上的错误轨迹视为有价值的数据源，通过检测这些错误轨迹的偏差并自动生成感知和动作的自我纠正数据来驱动模型的持续训练。经过多轮飞轮迭代，开发的单目RGB导航模型CorrectNav在R2R-CE和RxR-CE基准测试中分别达到65.1%和69.3%的新SOTA成功率。</li></ul>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了Self-correction Flywheel后训练范式，将模型错误轨迹转化为训练数据</li><li>开发了自动轨迹偏差检测方法，精确定位错误位置</li><li>设计了创新的自我纠正数据生成技术，从动作和感知角度创建纠错数据</li><li>实现了多轮飞轮迭代训练机制，持续提升导航模型性能</li><li>开发的CorrectNav模型在模拟和真实机器人环境中均表现出优异的纠错和导航能力</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>模型结构：CorrectNav由SigLIP视觉编码器、2层MLP投影器和Qwen2大语言模型组成，初始化自LLaVA-Video 7B</li><li>导航微调策略：包括导航动作预测（使用210万+步级数据）、轨迹指令生成和通用多模态数据召回，并实施相机高度、视野等随机化增强视觉多样性</li><li>Self-correction Flywheel流程：(1)在训练集评估模型收集错误轨迹；(2)检测偏差并定位；(3)创建纠错轨迹数据（使用轨迹规划器生成恢复路径）和关键帧感知数据（使用多模态LLM生成描述和QA）；(4)继续训练模型；(5)多轮迭代</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：VLN-CE的R2R和RxR训练集，包括52.7K样本（R2R）和158万样本（RxR）</li><li>训练资源：8块NVIDIA A100 GPU，导航微调需80小时，单次飞轮迭代需20小时</li><li>多模态数据：从LLaVA-Video数据集中随机采样24万个ActivityQA和NextQA训练实例</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>模拟环境：使用Habitat 3.0模拟器在VLN-CE基准的R2R-CE和RxR-CE的Val-Unseen分割上评估</li><li>真实环境：在AgiBot Lingxi D1四足机器人平台上测试，部署在远程A100 GPU服务器</li><li>评估指标：导航错误(NE)、成功率(SR)、加权路径长度(SPL)、标准化动态时间规整(nDTW)、路径长度误差(NE<3m)</li></ul>"
  },
  {
    "date": "2025-08-14",
    "title": "Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning",
    "link": "http://arxiv.org/abs/2508.10399",
    "summary_markdown": "# 论文研究单位\nUniversity of Electronic Science and Technology of China, Chengdu, China\n# 论文概述\n这是一篇关于大型模型赋能具身AI的综述论文，重点关注决策制定和具身学习。论文提供了全面的框架，分析了分层和端到端决策制定范式，详细阐述了大型模型如何增强高层规划、底层执行和反馈机制。对于具身学习，论文介绍了主流学习方法，包括模仿学习、强化学习、迁移学习和元学习，并详细探讨了大型模型如何增强这些学习方法。论文还首次将世界模型整合到具身AI综述中，展示了它们的设计方法以及在增强决策和学习中的关键作用。最后，论文讨论了该领域面临的挑战和未来研究方向。\n# 论文核心贡献点\n- 专注于从具身AI视角看大型模型的赋能作用，根据分层决策制定的高层规划、底层执行和反馈增强进行分类，根据端到端决策制定的VLA模型进行分类\n- 全面综述大型模型赋能的决策制定和学习，回顾了分层和端到端两种范式，以及模仿学习和强化学习等主要学习方法，还涵盖了世界模型及其在决策和学习中的促进作用\n- 采用双重分析方法：横向分析比较不同方法，纵向分析追踪核心模型的演进，提供宏观概述和深入见解\n# 论文方法描述\n分层自主决策方法包括：高层规划（结构化语言规划、自然语言规划、编程语言规划）、底层执行（传统控制算法、学习驱动控制、模块化控制）和反馈增强（大模型自反思、人类反馈、环境反馈）。端到端自主决策方法通过视觉-语言-行动模型实现，包含标记化和表示、多模态信息融合和行动去标记化三个组件。论文还介绍了VLA模型的增强方法，包括感知能力增强、轨迹行动优化和训练成本降低。\n# 论文使用数据集和训练资源\n作为综述论文，未涉及具体实验数据集或训练资源。\n# 论文使用的评估环境和评估指标\n作为综述论文，未涉及具体实验评估环境和指标。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>University of Electronic Science and Technology of China, Chengdu, China</p>\n<h1>论文概述</h1>\n<p>这是一篇关于大型模型赋能具身AI的综述论文，重点关注决策制定和具身学习。论文提供了全面的框架，分析了分层和端到端决策制定范式，详细阐述了大型模型如何增强高层规划、底层执行和反馈机制。对于具身学习，论文介绍了主流学习方法，包括模仿学习、强化学习、迁移学习和元学习，并详细探讨了大型模型如何增强这些学习方法。论文还首次将世界模型整合到具身AI综述中，展示了它们的设计方法以及在增强决策和学习中的关键作用。最后，论文讨论了该领域面临的挑战和未来研究方向。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>专注于从具身AI视角看大型模型的赋能作用，根据分层决策制定的高层规划、底层执行和反馈增强进行分类，根据端到端决策制定的VLA模型进行分类</li><li>全面综述大型模型赋能的决策制定和学习，回顾了分层和端到端两种范式，以及模仿学习和强化学习等主要学习方法，还涵盖了世界模型及其在决策和学习中的促进作用</li><li>采用双重分析方法：横向分析比较不同方法，纵向分析追踪核心模型的演进，提供宏观概述和深入见解</li></ul>\n<h1>论文方法描述</h1>\n<p>分层自主决策方法包括：高层规划（结构化语言规划、自然语言规划、编程语言规划）、底层执行（传统控制算法、学习驱动控制、模块化控制）和反馈增强（大模型自反思、人类反馈、环境反馈）。端到端自主决策方法通过视觉-语言-行动模型实现，包含标记化和表示、多模态信息融合和行动去标记化三个组件。论文还介绍了VLA模型的增强方法，包括感知能力增强、轨迹行动优化和训练成本降低。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>作为综述论文，未涉及具体实验数据集或训练资源。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>作为综述论文，未涉及具体实验评估环境和指标。</p>"
  },
  {
    "date": "2025-08-14",
    "title": "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver",
    "link": "http://arxiv.org/abs/2508.10333",
    "summary_markdown": "论文研究单位\n信息未在提供的HTML原文中明确提及。\n\n论文概述\n论文指出现有的视觉-语言-动作模型在执行任务时，视觉注意力会分散，难以聚焦到正确的目标区域。为了解决这个问题，论文提出了ReconVLA，一个基于重构的视觉-语言-动作模型。该模型通过一个辅助的视觉重建模块，在VLA模型的视觉输出引导下，重构出图像中对应目标操作物体的“注视区域”，从而促使模型学习细粒度的视觉表示，准确地分配视觉注意力，实现精确的机器人操作。此外，论文还构建了一个包含超过10万条轨迹和200万个样本的大规模预训练数据集，以增强模型的泛化能力。\n\n论文核心贡献点\n1. 提出了ReconVLA，一个采用隐式基础范式的重构性VLA模型。通过重构目标操作区域，引导模型进行精确的视觉注意力分配和细粒度的表示学习，提升了视觉基础能力和精确操作能力。\n2. 构建了一个大规模的机器人预训练数据集，包含超过10万条轨迹和200万个数据样本，通过在该数据集上的预训练，显著增强了模型在视觉重建任务上的泛化能力。\n3. 在模拟和真实世界进行了广泛的实验，证明了所提出的隐式基础方法的优越性，以及模型在精确操作和对未见目标泛化方面的能力。\n\n论文方法描述\nReconVLA模型在LLaVA-7b（基于Qwen2-7b和SigLIP-so400m-patch14-384）的基础上构建。模型的核心是一个重构性视觉模块，该模块由一个扩散Transformer实现。具体方法如下：\n1. 重构目标：模型的目标是重构图像中的“注视区域”，即机器人需要操作的目标区域，这模仿了人眼的注视行为。\n2. 损失函数：总损失函数由动作预测的交叉熵损失和视觉重构损失组成，即 `L_ReconVLA = L_VLA^action + L_VLA^visual`。\n3. 潜在视觉重构：\n - 使用一个冻结的VAE视觉分词器将注视区域图像转换为场景标记 `z_0`。\n - VLA模型生成“重构标记”。\n - 一个扩散Transformer去噪器以重构标记为条件，从噪声 `z_t` 中恢复场景标记 `z_0`。\n - 视觉重构损失是基于扩散过程的均方误差损失。\n4. 视觉预训练：\n - 数据集：结合BridgeData V2, LIBERO和CALVIN等开源数据集，使用精调后的Grounding DINO自动标注出注视区域，构建了包含10万+轨迹和200万+样本的大规模预训练数据集。\n - 训练：预训练同时对重构损失和动作损失进行梯度反向传播，之后在特定任务上进行微调。\n\n论文使用数据集和训练资源\n- 数据集：\n - 预训练数据集：自建的大规模数据集，源于BridgeData V2, LIBERO, CALVIN，包含超过10万条轨迹和200万个（完整图像，注视区域图像）样本对。\n - 微调与评估数据集：CALVIN基准数据集（用于模拟评估）和真实世界采集的任务数据集（用于真实世界评估）。\n- 训练资源：HTML原文中未明确提及训练资源（如GPU数量、训练时间等）。\n\n论文使用的评估环境和评估指标\n- 评估环境：\n - 模拟环境：CALVIN基准，基于PyBullet模拟器，使用Franka Panda机器人臂，包含34个任务和4种不同的环境（A, B, C, D）。\n - 真实世界：使用一台6-DoF的AgileX PiPer机械臂，配有1-DoF的平行夹爪，以及RealSense D515和ORBBEC Dabai深度相机作为视觉输入。\n- 评估指标：\n - CALVIN基准：长时序任务中每个子任务的成功率，以及连续完成5个子任务的平均长度。在500次滚动测试上评估。\n - 真实世界任务：每个任务在20次试验中的成功率，包括对未见目标任务的泛化能力评估。",
    "summary_html": "<p>论文研究单位</p>\n<p>信息未在提供的HTML原文中明确提及。</p>\n\n<p>论文概述</p>\n<p>论文指出现有的视觉-语言-动作模型在执行任务时，视觉注意力会分散，难以聚焦到正确的目标区域。为了解决这个问题，论文提出了ReconVLA，一个基于重构的视觉-语言-动作模型。该模型通过一个辅助的视觉重建模块，在VLA模型的视觉输出引导下，重构出图像中对应目标操作物体的“注视区域”，从而促使模型学习细粒度的视觉表示，准确地分配视觉注意力，实现精确的机器人操作。此外，论文还构建了一个包含超过10万条轨迹和200万个样本的大规模预训练数据集，以增强模型的泛化能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了ReconVLA，一个采用隐式基础范式的重构性VLA模型。通过重构目标操作区域，引导模型进行精确的视觉注意力分配和细粒度的表示学习，提升了视觉基础能力和精确操作能力。</li><li>构建了一个大规模的机器人预训练数据集，包含超过10万条轨迹和200万个数据样本，通过在该数据集上的预训练，显著增强了模型在视觉重建任务上的泛化能力。</li><li>在模拟和真实世界进行了广泛的实验，证明了所提出的隐式基础方法的优越性，以及模型在精确操作和对未见目标泛化方面的能力。</li></ol>\n\n<p>论文方法描述</p>\n<p>ReconVLA模型在LLaVA-7b（基于Qwen2-7b和SigLIP-so400m-patch14-384）的基础上构建。模型的核心是一个重构性视觉模块，该模块由一个扩散Transformer实现。具体方法如下：</p>\n<ol><li>重构目标：模型的目标是重构图像中的“注视区域”，即机器人需要操作的目标区域，这模仿了人眼的注视行为。</li><li>损失函数：总损失函数由动作预测的交叉熵损失和视觉重构损失组成，即 <code>L_ReconVLA = L_VLA^action + L_VLA^visual</code>。</li><li>潜在视觉重构：</li></ol>\n<p> - 使用一个冻结的VAE视觉分词器将注视区域图像转换为场景标记 <code>z_0</code>。</p>\n<p> - VLA模型生成“重构标记”。</p>\n<p> - 一个扩散Transformer去噪器以重构标记为条件，从噪声 <code>z_t</code> 中恢复场景标记 <code>z_0</code>。</p>\n<p> - 视觉重构损失是基于扩散过程的均方误差损失。</p>\n<p>4. 视觉预训练：</p>\n<p> - 数据集：结合BridgeData V2, LIBERO和CALVIN等开源数据集，使用精调后的Grounding DINO自动标注出注视区域，构建了包含10万+轨迹和200万+样本的大规模预训练数据集。</p>\n<p> - 训练：预训练同时对重构损失和动作损失进行梯度反向传播，之后在特定任务上进行微调。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：</li></ul>\n<p> - 预训练数据集：自建的大规模数据集，源于BridgeData V2, LIBERO, CALVIN，包含超过10万条轨迹和200万个（完整图像，注视区域图像）样本对。</p>\n<p> - 微调与评估数据集：CALVIN基准数据集（用于模拟评估）和真实世界采集的任务数据集（用于真实世界评估）。</p>\n<ul><li>训练资源：HTML原文中未明确提及训练资源（如GPU数量、训练时间等）。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：</li></ul>\n<p> - 模拟环境：CALVIN基准，基于PyBullet模拟器，使用Franka Panda机器人臂，包含34个任务和4种不同的环境（A, B, C, D）。</p>\n<p> - 真实世界：使用一台6-DoF的AgileX PiPer机械臂，配有1-DoF的平行夹爪，以及RealSense D515和ORBBEC Dabai深度相机作为视觉输入。</p>\n<ul><li>评估指标：</li></ul>\n<p> - CALVIN基准：长时序任务中每个子任务的成功率，以及连续完成5个子任务的平均长度。在500次滚动测试上评估。</p>\n<p> - 真实世界任务：每个任务在20次试验中的成功率，包括对未见目标任务的泛化能力评估。</p>"
  },
  {
    "date": "2025-08-12",
    "title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2508.09071",
    "summary_markdown": "### 论文研究单位\n- 天津大学\n- Dexmal\n- 清华大学\n### 论文概述\n该论文提出了GeoVLA，一种新的视觉-语言-动作（VLA）模型框架，通过有效整合3D信息来推进机器人操作。当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界中的丰富几何信息，这限制了它们的空间感知能力和适应性。GeoVLA使用视觉-语言模型（VLM）处理图像和语言指令，提取融合的视觉-语言嵌入；同时，将深度图转换为点云，并采用定制的点嵌入网络（PEN）独立生成3D几何嵌入。这些嵌入随后被拼接，并由提出的空间感知动作专家（3DAE）处理，以生成精确的动作序列。通过在模拟和真实世界环境中的大量实验，GeoVLA展示了优越的性能和鲁棒性。\n### 论文核心贡献点\n1. 提出GeoVLA框架，首次在VLA模型中并行处理视觉和点云模态，增强空间理解和几何感知能力。\n2. 引入点嵌入网络（PEN）和3D增强动作专家（3DAE）两个新模块：PEN提取判别性几何特征，3DAE通过模态特定专家有效整合视觉和几何线索。\n3. 在LIBERO和ManiSkill2模拟基准上达到SOTA性能，并在真实世界任务中表现出对高度适应性、尺度感知和视角不变性的显著鲁棒性。\n### 论文方法描述\nGeoVLA采用三阶段端到端架构：\n1. **视觉-语言处理**：使用预训练VLM（如Prismatic-7B）处理RGB图像和语言指令，生成通用理解特征F_VL。\n2. **3D点云编码**：将深度图转换为以末端执行器为中心的点云，通过PEN进行双路径处理：\n - 几何特征路径：使用大卷积核和局部池化提取patch级特征。\n - 位置编码路径：通过旋转位置编码（RoPE）保留3D空间信息。\n 最终选择末端执行器对应的锚点token作为输出特征F_P。\n3. **动作生成**：3DAE基于扩散变换器（DiT）架构，采用静态路由的混合专家（MoE）设计：\n - 训练时随机丢弃一种模态（纯视觉语言、纯语言+几何、全模态），确保专家平衡。\n - 推理时基于DDIM采样，由多模态条件逐步去噪生成动作块（T=16）。\n### 论文使用数据集和训练资源\n- **数据集**：\n - LIBERO：包含5个任务套件（LIBERO-Spatial/Object/Goal/Long/90），评估空间、物体多样性和长时序操作。\n - ManiSkill2：使用5个抓取任务（如PickCube、PickSingleYCB），测试基本操作和泛化能力。\n - 真实世界数据：8项任务（如放置胡萝卜、叠积木、套杯等），包含高度、尺度和视角变体。\n- **训练资源**：\n - 8块NVIDIA A100 GPU（FSDP策略）。\n - 总批次大小256，学习率2e-5（AdamW优化器）。\n - 混合精度训练，LIBERO训练约6轮（20小时），ManiSkill2约2轮。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：LIBERO和ManiSkill2标准基准。\n - 真实环境：WidowX-250s机械臂搭配RealSense-435i深度相机（0.8米距离）。\n- **评估指标**：\n - 成功率（SR）：LIBERO各任务50次尝试、ManiSkill2各20次尝试的平均成功率。\n - 泛化测试：真实世界任务中调整目标高度（篮筐层数）、物体尺度（套娃大小）、相机视角（0°-45°）和背景（海绵垫移除）下的成功率变化。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>天津大学</li><li>Dexmal</li><li>清华大学</li></ul>\n<h3>论文概述</h3>\n<p>该论文提出了GeoVLA，一种新的视觉-语言-动作（VLA）模型框架，通过有效整合3D信息来推进机器人操作。当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界中的丰富几何信息，这限制了它们的空间感知能力和适应性。GeoVLA使用视觉-语言模型（VLM）处理图像和语言指令，提取融合的视觉-语言嵌入；同时，将深度图转换为点云，并采用定制的点嵌入网络（PEN）独立生成3D几何嵌入。这些嵌入随后被拼接，并由提出的空间感知动作专家（3DAE）处理，以生成精确的动作序列。通过在模拟和真实世界环境中的大量实验，GeoVLA展示了优越的性能和鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出GeoVLA框架，首次在VLA模型中并行处理视觉和点云模态，增强空间理解和几何感知能力。</li><li>引入点嵌入网络（PEN）和3D增强动作专家（3DAE）两个新模块：PEN提取判别性几何特征，3DAE通过模态特定专家有效整合视觉和几何线索。</li><li>在LIBERO和ManiSkill2模拟基准上达到SOTA性能，并在真实世界任务中表现出对高度适应性、尺度感知和视角不变性的显著鲁棒性。</li></ol>\n<h3>论文方法描述</h3>\n<p>GeoVLA采用三阶段端到端架构：</p>\n<ol><li><strong>视觉-语言处理</strong>：使用预训练VLM（如Prismatic-7B）处理RGB图像和语言指令，生成通用理解特征F_VL。</li><li><strong>3D点云编码</strong>：将深度图转换为以末端执行器为中心的点云，通过PEN进行双路径处理：</li></ol>\n<p> - 几何特征路径：使用大卷积核和局部池化提取patch级特征。</p>\n<p> - 位置编码路径：通过旋转位置编码（RoPE）保留3D空间信息。</p>\n<p> 最终选择末端执行器对应的锚点token作为输出特征F_P。</p>\n<p>3. <strong>动作生成</strong>：3DAE基于扩散变换器（DiT）架构，采用静态路由的混合专家（MoE）设计：</p>\n<p> - 训练时随机丢弃一种模态（纯视觉语言、纯语言+几何、全模态），确保专家平衡。</p>\n<p> - 推理时基于DDIM采样，由多模态条件逐步去噪生成动作块（T=16）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - LIBERO：包含5个任务套件（LIBERO-Spatial/Object/Goal/Long/90），评估空间、物体多样性和长时序操作。</p>\n<p> - ManiSkill2：使用5个抓取任务（如PickCube、PickSingleYCB），测试基本操作和泛化能力。</p>\n<p> - 真实世界数据：8项任务（如放置胡萝卜、叠积木、套杯等），包含高度、尺度和视角变体。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 8块NVIDIA A100 GPU（FSDP策略）。</p>\n<p> - 总批次大小256，学习率2e-5（AdamW优化器）。</p>\n<p> - 混合精度训练，LIBERO训练约6轮（20小时），ManiSkill2约2轮。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：LIBERO和ManiSkill2标准基准。</p>\n<p> - 真实环境：WidowX-250s机械臂搭配RealSense-435i深度相机（0.8米距离）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 成功率（SR）：LIBERO各任务50次尝试、ManiSkill2各20次尝试的平均成功率。</p>\n<p> - 泛化测试：真实世界任务中调整目标高度（篮筐层数）、物体尺度（套娃大小）、相机视角（0°-45°）和背景（海绵垫移除）下的成功率变化。</p>"
  },
  {
    "date": "2025-08-12",
    "title": "Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding",
    "link": "http://arxiv.org/abs/2508.09032",
    "summary_markdown": "### 论文研究单位\n莫斯科物理理工学院 (MIPT), 俄罗斯多尔戈普鲁德内, 141701\n人工智能研究所 (AIRI), 俄罗斯莫斯科, 121170\n### 论文概述\n该论文提出了一种名为“Spatial Traces”的新方法，旨在增强视觉-语言-动作（VLA）模型。该方法通过将历史关键点的视觉轨迹投影到深度图上，将空间信息和时间信息集成到一个统一的视觉提示中。这种设计使得模型能够同时捕捉空间和时间上下文，从而提高在复杂操作任务中的性能。在SimplerEnv环境中的实验表明，该方法仅需最少的训练数据即可实现性能提升，其平均任务完成成功率比SpatialVLA高出4%，比TraceVLA高出19%。\n### 论文核心贡献点\n1. 提出了一种名为“Spatial Traces”的方法，该方法能够让VLA模型通过单张图像同时利用环境中的空间和时间信息。\n2. 在SimplerEnv中的实验表明，仅用52条训练轨迹进行微调，该方法相较于SpatialVLA提升了4%的任务成功率，相较于TraceVLA提升了19%，证明了在数据收集困难的真实世界应用中，融合空间和时间信息的价值。\n### 论文方法描述\n该方法的核心是构建一个融合了空间和时间信息的视觉表示。具体来说，它包含两个主要组成部分：深度信息和视觉轨迹。首先，使用一个深度估计模型（如ZoeDepth）从当前观察中预测深度图。接着，使用一个轨迹预测模型（如Co-Tracker）从过去一系列观察中追踪并生成关键点的视觉轨迹。然后，将这些二维轨迹投影到当前预测的深度图上，形成一个带有轨迹标记的深度图。最后，原始的RGB观察图像和这个带有轨迹的深度图分别通过各自的图像处理器（如Siglip和Ego3D Positional Encoder）转换成嵌入向量，并将这两个嵌入向量相加，生成最终的视觉嵌入。这个融合的视觉嵌入与文本指令的嵌入一起被输入到VLA模型（如PaliGemma2）中，以预测下一步的动作。\n### 论文使用数据集和训练资源\n数据集：Bridge数据集的一个子集，包含52条来自真实机器人操作的轨迹，总计1969个步骤。\n训练资源：模型在单个NVIDIA TITAN RTX GPU上进行训练。训练采用了LoRA适配器，应用于模型的所有线性层。训练参数设置如下：学习率为5e-5，批次大小为1，训练周期为2，LoRA秩为32，优化器为AdamW。完成一个模型的训练大约需要2小时。\n### 论文使用的评估环境和评估指标\n评估环境：SimplerEnv，一个基于Robosuite构建的虚拟环境，专门用于机器人操作任务。评估选取了四个代表性任务：“Put Spoon”（放勺子）、“Put Carrot”（放胡萝卜）、“Stack Blocks”（堆叠积木）和“Put Eggplant”（放茄子）。\n评估指标：主要使用两个指标：\n1. 目标条件成功率（GCS）：衡量智能体是否成功达到基本的任务完成条件（例如，将目标物体从桌面上拿起）。\n2. 成功率（SR）：衡量整个任务是否完全成功完成，达到最终的目标状态。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>莫斯科物理理工学院 (MIPT), 俄罗斯多尔戈普鲁德内, 141701</p>\n<p>人工智能研究所 (AIRI), 俄罗斯莫斯科, 121170</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种名为“Spatial Traces”的新方法，旨在增强视觉-语言-动作（VLA）模型。该方法通过将历史关键点的视觉轨迹投影到深度图上，将空间信息和时间信息集成到一个统一的视觉提示中。这种设计使得模型能够同时捕捉空间和时间上下文，从而提高在复杂操作任务中的性能。在SimplerEnv环境中的实验表明，该方法仅需最少的训练数据即可实现性能提升，其平均任务完成成功率比SpatialVLA高出4%，比TraceVLA高出19%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种名为“Spatial Traces”的方法，该方法能够让VLA模型通过单张图像同时利用环境中的空间和时间信息。</li><li>在SimplerEnv中的实验表明，仅用52条训练轨迹进行微调，该方法相较于SpatialVLA提升了4%的任务成功率，相较于TraceVLA提升了19%，证明了在数据收集困难的真实世界应用中，融合空间和时间信息的价值。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法的核心是构建一个融合了空间和时间信息的视觉表示。具体来说，它包含两个主要组成部分：深度信息和视觉轨迹。首先，使用一个深度估计模型（如ZoeDepth）从当前观察中预测深度图。接着，使用一个轨迹预测模型（如Co-Tracker）从过去一系列观察中追踪并生成关键点的视觉轨迹。然后，将这些二维轨迹投影到当前预测的深度图上，形成一个带有轨迹标记的深度图。最后，原始的RGB观察图像和这个带有轨迹的深度图分别通过各自的图像处理器（如Siglip和Ego3D Positional Encoder）转换成嵌入向量，并将这两个嵌入向量相加，生成最终的视觉嵌入。这个融合的视觉嵌入与文本指令的嵌入一起被输入到VLA模型（如PaliGemma2）中，以预测下一步的动作。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：Bridge数据集的一个子集，包含52条来自真实机器人操作的轨迹，总计1969个步骤。</p>\n<p>训练资源：模型在单个NVIDIA TITAN RTX GPU上进行训练。训练采用了LoRA适配器，应用于模型的所有线性层。训练参数设置如下：学习率为5e-5，批次大小为1，训练周期为2，LoRA秩为32，优化器为AdamW。完成一个模型的训练大约需要2小时。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：SimplerEnv，一个基于Robosuite构建的虚拟环境，专门用于机器人操作任务。评估选取了四个代表性任务：“Put Spoon”（放勺子）、“Put Carrot”（放胡萝卜）、“Stack Blocks”（堆叠积木）和“Put Eggplant”（放茄子）。</p>\n<p>评估指标：主要使用两个指标：</p>\n<ol><li>目标条件成功率（GCS）：衡量智能体是否成功达到基本的任务完成条件（例如，将目标物体从桌面上拿起）。</li><li>成功率（SR）：衡量整个任务是否完全成功完成，达到最终的目标状态。</li></ol>"
  },
  {
    "date": "2025-08-12",
    "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing",
    "link": "http://arxiv.org/abs/2508.08706",
    "summary_markdown": "论文研究单位\n- Paxini Tech.\n- Shanghai Jiao Tong University\n\n论文概述\n该论文提出了OmniVTLA，一个视觉-触觉-语言-动作模型，旨在解决现有视觉-语言-动作（VLA）模型在接触密集型操作任务中由于忽视触觉感知而表现不佳的问题。OmniVTLA通过引入一个双路径触觉编码器框架来整合触觉信息，并构建了一个名为ObjTac的大规模力-触觉数据集来训练语义对齐的触觉编码器。实验表明，该方法在真实世界的拾取与放置任务中，显著优于现有的VLA模型，提高了成功率，减少了任务完成时间，并生成了更平滑的机器人运动轨迹。\n\n论文核心贡献点\n1. 提出OmniVTLA框架，一个用于接触密集型操作任务的端到端视觉-触觉-语言-动作模型。该模型利用双编码器路径来克服不同触觉传感器之间的异构性问题。\n2. 引入ObjTac数据集，一个包含135K个三模态样本的综合触觉数据集，涵盖了10个类别的56个物体，为模型训练提供了丰富的触觉、视觉和文本数据。\n3. 通过真实世界实验验证了OmniVTLA的优越性能，相较于典型的VLA模型，在夹爪任务上成功率提高了21.9%，在灵巧手任务上提高了6.2%，同时缩短了任务完成时间并优化了轨迹平滑度。\n\n论文方法描述\n该模型在VLA框架的基础上扩展，将触觉数据作为输入。其核心是一个双路径触觉编码器：\n1. 第一条路径使用一个预训练的视觉Transformer（ViT），继承了大规模视觉数据中的丰富语义表示，对应于VTLA-Pre方案。\n2. 第二条路径使用一个语义对齐的触觉ViT（SA-ViT），通过跨模态对比学习在ObjTac等数据集上进行训练，以实现触觉、视觉和文本模态之间的语义对齐，对应于VTLA-SA方案。\n3. OmniVTLA将这两条路径的编码token拼接后，与文本token和视觉token一同输入到Gemma-2B骨干网络中，最后由动作头解码生成机器人动作序列。\n为了训练SA-ViT，论文提出了一个多模态对齐损失函数，同时优化视觉-语言、视觉-触觉和触觉-语言之间的对齐。\n\n论文使用数据集和训练资源\n1. 数据集\n - ObjTac：一个新收集的力-触觉数据集，包含56个物体（10个类别，如塑料、玻璃、木材等），总计135K个同步的文本、视觉和触觉三元组样本。\n - 任务特定数据集：通过远程操作收集的拾取与放置任务数据，用于训练和评估模型。使用UR5机械臂、带触觉传感器的夹爪和灵巧手在真实环境中进行采集，每类物体包含40个演示片段。\n2. 训练资源\n - GPU: NVIDIA A100 (80GB VRAM)\n - 训练方法：在π0基线上进行微调。\n - 训练步数: 30K\n - 学习率: 峰值2.5e-5，带预热和余弦衰减。\n - 批量大小: 32\n\n论文使用的评估环境和评估指标\n1. 评估环境\n - 硬件：UR5机械臂，末端执行器为配备两个触觉传感器的双指夹爪或配备11个触觉传感器的四指灵巧手，同时配备腕部摄像头和基座摄像头。\n - 任务：拾取与放置任务，使用夹爪操作4种物体，使用灵巧手操作2种物体，并在未见过的物体上进行泛化测试。\n2. 评估指标\n - 离线验证：均方误差（MSE），计算模型预测的机器人状态与真实远程操作数据之间的差异。\n - 真实世界评估：\n - 成功率（SR）：成功将物体放置到目标位置的试验百分比。\n - 完成时间（CT）：从任务开始到成功放置所需的步数。\n - 运动平滑度：末端执行器轨迹的方差，值越小表示轨迹越平滑。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>Paxini Tech.</li><li>Shanghai Jiao Tong University</li></ul>\n\n<p>论文概述</p>\n<p>该论文提出了OmniVTLA，一个视觉-触觉-语言-动作模型，旨在解决现有视觉-语言-动作（VLA）模型在接触密集型操作任务中由于忽视触觉感知而表现不佳的问题。OmniVTLA通过引入一个双路径触觉编码器框架来整合触觉信息，并构建了一个名为ObjTac的大规模力-触觉数据集来训练语义对齐的触觉编码器。实验表明，该方法在真实世界的拾取与放置任务中，显著优于现有的VLA模型，提高了成功率，减少了任务完成时间，并生成了更平滑的机器人运动轨迹。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出OmniVTLA框架，一个用于接触密集型操作任务的端到端视觉-触觉-语言-动作模型。该模型利用双编码器路径来克服不同触觉传感器之间的异构性问题。</li><li>引入ObjTac数据集，一个包含135K个三模态样本的综合触觉数据集，涵盖了10个类别的56个物体，为模型训练提供了丰富的触觉、视觉和文本数据。</li><li>通过真实世界实验验证了OmniVTLA的优越性能，相较于典型的VLA模型，在夹爪任务上成功率提高了21.9%，在灵巧手任务上提高了6.2%，同时缩短了任务完成时间并优化了轨迹平滑度。</li></ol>\n\n<p>论文方法描述</p>\n<p>该模型在VLA框架的基础上扩展，将触觉数据作为输入。其核心是一个双路径触觉编码器：</p>\n<ol><li>第一条路径使用一个预训练的视觉Transformer（ViT），继承了大规模视觉数据中的丰富语义表示，对应于VTLA-Pre方案。</li><li>第二条路径使用一个语义对齐的触觉ViT（SA-ViT），通过跨模态对比学习在ObjTac等数据集上进行训练，以实现触觉、视觉和文本模态之间的语义对齐，对应于VTLA-SA方案。</li><li>OmniVTLA将这两条路径的编码token拼接后，与文本token和视觉token一同输入到Gemma-2B骨干网络中，最后由动作头解码生成机器人动作序列。</li></ol>\n<p>为了训练SA-ViT，论文提出了一个多模态对齐损失函数，同时优化视觉-语言、视觉-触觉和触觉-语言之间的对齐。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. 数据集</p>\n<p> - ObjTac：一个新收集的力-触觉数据集，包含56个物体（10个类别，如塑料、玻璃、木材等），总计135K个同步的文本、视觉和触觉三元组样本。</p>\n<p> - 任务特定数据集：通过远程操作收集的拾取与放置任务数据，用于训练和评估模型。使用UR5机械臂、带触觉传感器的夹爪和灵巧手在真实环境中进行采集，每类物体包含40个演示片段。</p>\n<p>2. 训练资源</p>\n<p> - GPU: NVIDIA A100 (80GB VRAM)</p>\n<p> - 训练方法：在π0基线上进行微调。</p>\n<p> - 训练步数: 30K</p>\n<p> - 学习率: 峰值2.5e-5，带预热和余弦衰减。</p>\n<p> - 批量大小: 32</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境</p>\n<p> - 硬件：UR5机械臂，末端执行器为配备两个触觉传感器的双指夹爪或配备11个触觉传感器的四指灵巧手，同时配备腕部摄像头和基座摄像头。</p>\n<p> - 任务：拾取与放置任务，使用夹爪操作4种物体，使用灵巧手操作2种物体，并在未见过的物体上进行泛化测试。</p>\n<p>2. 评估指标</p>\n<p> - 离线验证：均方误差（MSE），计算模型预测的机器人状态与真实远程操作数据之间的差异。</p>\n<p> - 真实世界评估：</p>\n<p> - 成功率（SR）：成功将物体放置到目标位置的试验百分比。</p>\n<p> - 完成时间（CT）：从任务开始到成功放置所需的步数。</p>\n<p> - 运动平滑度：末端执行器轨迹的方差，值越小表示轨迹越平滑。</p>"
  },
  {
    "date": "2025-08-11",
    "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions",
    "link": "http://arxiv.org/abs/2508.07650",
    "summary_markdown": "论文研究单位\n论文未明确提供作者单位信息。\n\n论文概述\n针对现有视觉-语言-动作模型在处理模糊指令和未知环境状态时存在局限，且其感知大多局限于静态二维观察，缺乏对机器人与环境三维交互建模能力的问题，本文提出了GraphCoT-VLA，一个高效的端到端模型。该模型通过设计一个结构化的思维链推理模块来增强对模糊指令的解析和任务规划能力，该模块集成了高级任务理解与规划、任务失败反馈以及对未来物体位置和机器人动作的低层次想象推理。此外，模型构建了一个实时更新的3D姿态-物体图，以捕获机器人关节和物体在3D空间中的空间配置与拓扑关系。实验表明，GraphCoT-VLA在任务成功率和响应速度上显著优于现有方法，在开放环境和不确定指令下表现出强大的泛化和鲁棒性。\n\n论文核心贡献点\n- 提出了一种名为GraphCoT-VLA的新型端到端模型，用于处理模糊指令和开放世界条件下的机器人操作。\n- 设计了一个结构化的思维链模块，支持场景理解、反馈和未来想象。\n- 设计了一个实时的姿态-物体图，用于建模机器人与周围物体的3D交互。\n- 提出了一种基于dropout训练的混合思维链推理策略，可实现快速推理和迭代优化，用于实时控制。\n- GraphCoT-VLA在真实机器人上得到验证，在成功率、动作流畅性、时间建模和任务泛化方面表现出优越性能。\n\n论文方法描述\n该方法主要包括四个部分：\n1. **姿态-物体图**：在每个时间步，利用机器人运动学和深度数据实时构建一个图。该图通过YOLO-World检测物体，将RGB图像中的2D中心点结合深度信息投影到3D空间，并转换到机器人基坐标系。机器人的末端执行器位置通过正向运动学计算得到。所有物体和末端执行器节点全连接形成图，随后使用一个两层图神经网络进行编码。\n2. **思维链推理**：该模块集成了理解、反馈和想象。首先，使用Qwen2.5-VL对头部相机图像进行场景理解。然后，进行可行性分析并分解模糊指令，如果场景不满足指令，则生成反馈和建议。通过一个通用的帧采样策略，对未来的物体状态和机器人状态进行想象和预测。\n3. **整体框架**：模型基于π0构建。输入包括多视角RGB-D图像、本体感觉信息、模糊指令和姿态-物体图。视觉输入由Vision Transformer编码，图由图编码器编码。这些嵌入与指令的token拼接后，送入一个基于PaliGemma的VLM。VLM的输出分为两部分：第一部分自回归生成思维链解释，第二部分送入基于流匹配的动作专家模块，结合机器人状态和动作噪声来预测未来的动作序列。\n4. **使用Dropout进行联合训练**：训练包含两个损失函数：用于监督思维链生成的交叉熵损失，以及用于学习动作的条件流匹配损失。为了支持实时推理，采用了一种带dropout的联合训练策略，在训练时以一定概率随机丢弃思维链监督，使模型同时学习推理引导和直接动作预测两种模式。在推理时，仅在第一帧生成思维链以提供反馈和指导，后续帧则直接预测动作以实现快速控制。\n\n论文使用数据集和训练资源\n- **数据集**：研究团队使用一个每只手臂有7个自由度的双手机器人收集了遥操作数据。数据包括来自头部、颈部和手部摄像头的RGB-D图像，以及机器人的关节角度信息。数据采集时，物体位置被随机平移10厘米并旋转最多30度，衣物排列也按预定义排列或均匀采样变化。共收集了600个演示样本用于训练。\n- **训练资源**：论文未明确指出具体硬件，但提及在使用相同硬件上进行推理速度比较，表明了训练和测试的硬件环境一致性。\n\n论文使用的评估环境和评估指标\n- **评估环境**：评估在真实世界的双手机器人上进行，具体为一个每只手臂拥有7个自由度的机器人平台。\n- **评估指标**：主要评估指标是任务成功率，遵循ACT的评估协议，每个任务测试20次，只有在整个任务被正确完成时才计为成功。此外，还评估了模型在处理模糊指令时的任务混淆情况，并以推理频率来衡量模型的响应速度。",
    "summary_html": "<p>论文研究单位</p>\n<p>论文未明确提供作者单位信息。</p>\n\n<p>论文概述</p>\n<p>针对现有视觉-语言-动作模型在处理模糊指令和未知环境状态时存在局限，且其感知大多局限于静态二维观察，缺乏对机器人与环境三维交互建模能力的问题，本文提出了GraphCoT-VLA，一个高效的端到端模型。该模型通过设计一个结构化的思维链推理模块来增强对模糊指令的解析和任务规划能力，该模块集成了高级任务理解与规划、任务失败反馈以及对未来物体位置和机器人动作的低层次想象推理。此外，模型构建了一个实时更新的3D姿态-物体图，以捕获机器人关节和物体在3D空间中的空间配置与拓扑关系。实验表明，GraphCoT-VLA在任务成功率和响应速度上显著优于现有方法，在开放环境和不确定指令下表现出强大的泛化和鲁棒性。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了一种名为GraphCoT-VLA的新型端到端模型，用于处理模糊指令和开放世界条件下的机器人操作。</li><li>设计了一个结构化的思维链模块，支持场景理解、反馈和未来想象。</li><li>设计了一个实时的姿态-物体图，用于建模机器人与周围物体的3D交互。</li><li>提出了一种基于dropout训练的混合思维链推理策略，可实现快速推理和迭代优化，用于实时控制。</li><li>GraphCoT-VLA在真实机器人上得到验证，在成功率、动作流畅性、时间建模和任务泛化方面表现出优越性能。</li></ul>\n\n<p>论文方法描述</p>\n<p>该方法主要包括四个部分：</p>\n<ol><li><strong>姿态-物体图</strong>：在每个时间步，利用机器人运动学和深度数据实时构建一个图。该图通过YOLO-World检测物体，将RGB图像中的2D中心点结合深度信息投影到3D空间，并转换到机器人基坐标系。机器人的末端执行器位置通过正向运动学计算得到。所有物体和末端执行器节点全连接形成图，随后使用一个两层图神经网络进行编码。</li><li><strong>思维链推理</strong>：该模块集成了理解、反馈和想象。首先，使用Qwen2.5-VL对头部相机图像进行场景理解。然后，进行可行性分析并分解模糊指令，如果场景不满足指令，则生成反馈和建议。通过一个通用的帧采样策略，对未来的物体状态和机器人状态进行想象和预测。</li><li><strong>整体框架</strong>：模型基于π0构建。输入包括多视角RGB-D图像、本体感觉信息、模糊指令和姿态-物体图。视觉输入由Vision Transformer编码，图由图编码器编码。这些嵌入与指令的token拼接后，送入一个基于PaliGemma的VLM。VLM的输出分为两部分：第一部分自回归生成思维链解释，第二部分送入基于流匹配的动作专家模块，结合机器人状态和动作噪声来预测未来的动作序列。</li><li><strong>使用Dropout进行联合训练</strong>：训练包含两个损失函数：用于监督思维链生成的交叉熵损失，以及用于学习动作的条件流匹配损失。为了支持实时推理，采用了一种带dropout的联合训练策略，在训练时以一定概率随机丢弃思维链监督，使模型同时学习推理引导和直接动作预测两种模式。在推理时，仅在第一帧生成思维链以提供反馈和指导，后续帧则直接预测动作以实现快速控制。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>数据集</strong>：研究团队使用一个每只手臂有7个自由度的双手机器人收集了遥操作数据。数据包括来自头部、颈部和手部摄像头的RGB-D图像，以及机器人的关节角度信息。数据采集时，物体位置被随机平移10厘米并旋转最多30度，衣物排列也按预定义排列或均匀采样变化。共收集了600个演示样本用于训练。</li><li><strong>训练资源</strong>：论文未明确指出具体硬件，但提及在使用相同硬件上进行推理速度比较，表明了训练和测试的硬件环境一致性。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：评估在真实世界的双手机器人上进行，具体为一个每只手臂拥有7个自由度的机器人平台。</li><li><strong>评估指标</strong>：主要评估指标是任务成功率，遵循ACT的评估协议，每个任务测试20次，只有在整个任务被正确完成时才计为成功。此外，还评估了模型在处理模糊指令时的任务混淆情况，并以推理频率来衡量模型的响应速度。</li></ul>"
  },
  {
    "date": "2025-08-07",
    "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model",
    "link": "http://arxiv.org/abs/2508.06571",
    "summary_markdown": "论文研究单位\nBosch Corporate Research, Shanghai, China\nSchool of Communication and Information Engineering, Shanghai University\nSchool of Mechanical Engineering, Shanghai Jiao Tong University\nBosch Mobility Solutions, Robert Bosch GmbH, Suzhou\nAIR, Tsinghua University, Beijing\n\n论文概述\n本文提出了一种名为IRL-VLA的闭环强化学习框架，用于训练端到端自动驾驶的视觉-语言-动作模型。该方法旨在解决现有VLA模型依赖模仿学习导致性能受限，以及传统闭环训练依赖高保真模拟器所带来的计算开销和领域鸿沟问题。IRL-VLA采用三阶段范式：首先通过模仿学习预训练一个VLA策略；然后通过逆向强化学习构建一个轻量级的奖励世界模型来代替昂贵的模拟器进行奖励计算；最后使用PPO算法在奖励世界模型的引导下对VLA策略进行强化学习微调，以平衡安全、舒适和交通效率。\n\n论文核心贡献点\n1. 提出了IRL-VLA，一个开创性的无需依赖模拟器的闭环VLA训练框架。它引入了一个基于逆向强化学习的高效奖励世界模型，实现了可扩展且有效的闭环训练。\n2. 提出了一个新颖的VLA模型，该模型在模仿学习和强化学习设置下均能实现优越性能，包含语义推理、3D推理和统一扩散规划器三个模块。\n3. 在NAVSIM v2端到端驾驶基准测试中取得了最先进的性能，在CVPR2025自动驾驶挑战赛中获得第一名。\n\n论文方法描述\n1. 模仿策略学习：设计了一个包含语义推理（基于Senna-VLM）、3D推理（BEV编码器和自适应器）和统一扩散规划器的VLA模型。使用模仿学习损失（L1重建损失和二元交叉熵分类损失）进行预训练。\n2. 逆向环境学习：为了构建奖励世界模型，首先收集多样化的轨迹数据，包括记录扩散过程中的轨迹、使用K-means聚类采样以及改变自车初始位置。然后，奖励世界模型以多视角相机信息和预测轨迹为输入，通过多个独立的MLP头预测EPDMS的各项子指标，并加权求和得到最终奖励。模型通过最小化预测分数与真实分数之间的误差来进行训练。\n3. 使用奖励世界模型的强化学习：采用PPO算法对预训练的VLA策略进行微调。策略生成一组轨迹，由奖励世界模型评估并计算优势值（使用GAE）。通过最大化累计奖励来更新策略参数，并结合KL散度和模仿学习损失以保证训练的稳定性和防止灾难性遗忘。\n\n论文使用数据集和训练资源\n数据集: NAVSIM v2 (基于nuPlan和OpenScene)，包含1,192个训练场景和136个评估场景。\n训练资源: 8个NVIDIA A100 GPU。\n\n论文使用的评估环境和评估指标\n评估环境: NAVSIM v2 基准测试，一个非反应式模拟环境。\n评估指标: 扩展预测性驾驶员模型分数（EPDMS）及其八个子指标，包括无责任碰撞、可行驶区域合规性、驾驶方向合规性、交通灯合规性、自车进度、碰撞时间、车道保持、历史舒适性和扩展舒适性。最终EPDMS分数由惩罚项和加权平均项共同计算得出。",
    "summary_html": "<p>论文研究单位</p>\n<p>Bosch Corporate Research, Shanghai, China</p>\n<p>School of Communication and Information Engineering, Shanghai University</p>\n<p>School of Mechanical Engineering, Shanghai Jiao Tong University</p>\n<p>Bosch Mobility Solutions, Robert Bosch GmbH, Suzhou</p>\n<p>AIR, Tsinghua University, Beijing</p>\n\n<p>论文概述</p>\n<p>本文提出了一种名为IRL-VLA的闭环强化学习框架，用于训练端到端自动驾驶的视觉-语言-动作模型。该方法旨在解决现有VLA模型依赖模仿学习导致性能受限，以及传统闭环训练依赖高保真模拟器所带来的计算开销和领域鸿沟问题。IRL-VLA采用三阶段范式：首先通过模仿学习预训练一个VLA策略；然后通过逆向强化学习构建一个轻量级的奖励世界模型来代替昂贵的模拟器进行奖励计算；最后使用PPO算法在奖励世界模型的引导下对VLA策略进行强化学习微调，以平衡安全、舒适和交通效率。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了IRL-VLA，一个开创性的无需依赖模拟器的闭环VLA训练框架。它引入了一个基于逆向强化学习的高效奖励世界模型，实现了可扩展且有效的闭环训练。</li><li>提出了一个新颖的VLA模型，该模型在模仿学习和强化学习设置下均能实现优越性能，包含语义推理、3D推理和统一扩散规划器三个模块。</li><li>在NAVSIM v2端到端驾驶基准测试中取得了最先进的性能，在CVPR2025自动驾驶挑战赛中获得第一名。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>模仿策略学习：设计了一个包含语义推理（基于Senna-VLM）、3D推理（BEV编码器和自适应器）和统一扩散规划器的VLA模型。使用模仿学习损失（L1重建损失和二元交叉熵分类损失）进行预训练。</li><li>逆向环境学习：为了构建奖励世界模型，首先收集多样化的轨迹数据，包括记录扩散过程中的轨迹、使用K-means聚类采样以及改变自车初始位置。然后，奖励世界模型以多视角相机信息和预测轨迹为输入，通过多个独立的MLP头预测EPDMS的各项子指标，并加权求和得到最终奖励。模型通过最小化预测分数与真实分数之间的误差来进行训练。</li><li>使用奖励世界模型的强化学习：采用PPO算法对预训练的VLA策略进行微调。策略生成一组轨迹，由奖励世界模型评估并计算优势值（使用GAE）。通过最大化累计奖励来更新策略参数，并结合KL散度和模仿学习损失以保证训练的稳定性和防止灾难性遗忘。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集: NAVSIM v2 (基于nuPlan和OpenScene)，包含1,192个训练场景和136个评估场景。</p>\n<p>训练资源: 8个NVIDIA A100 GPU。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境: NAVSIM v2 基准测试，一个非反应式模拟环境。</p>\n<p>评估指标: 扩展预测性驾驶员模型分数（EPDMS）及其八个子指标，包括无责任碰撞、可行驶区域合规性、驾驶方向合规性、交通灯合规性、自车进度、碰撞时间、车道保持、历史舒适性和扩展舒适性。最终EPDMS分数由惩罚项和加权平均项共同计算得出。</p>"
  },
  {
    "date": "2025-08-06",
    "title": "Static and Plugged: Make Embodied Evaluation Simple",
    "link": "http://arxiv.org/abs/2508.06553",
    "summary_markdown": "### 论文研究单位\n上海人工智能实验室 (Shanghai AI Laboratory)\n### 论文概述\n论文指出了当前具身智能评估基准的两个主要问题：一是严重依赖高成本、高配置的模拟环境，二是缺乏统一的评估框架来同时评估高级认知和低级执行能力。为解决这些问题，论文提出了StaticEmbodiedBench，一个静态、统一、即插即用的基准。它通过仅使用静态关键帧和文本元数据来评估模型，无需复杂的模拟器，从而显著降低了评估的门槛。该基准包含42个场景和8个核心维度，能够分别评估视觉语言模型（VLM）的认知与决策能力以及视觉-语言-动作模型（VLA）的执行能力。\n### 论文核心贡献点\n- 提出了一种新颖的基于静态关键帧的评估方法和大脑-小脑协同评估框架，用于解耦和评估VLM与VLA在具身场景下的能力。\n- 构建了StaticEmbodiedBench，一个静态、统一、即插即用的基准，包含42个场景和8个评估维度，并发布了包含200个样本的子集。\n- 对19个VLM和11个VLA进行了全面评估，建立了首个具身智能的统一静态排行榜，并进行了深入分析。\n- 通过在真实机器人上实验，量化了静态评估与动态执行之间的性能差距（Static-to-Dynamic Gap），验证了静态评估方法的有效性。\n### 论文方法描述\n- **静态关键帧评估思路：** 核心洞察是，大多数具身任务的成功取决于轨迹中少数几个关键点（如识别目标物体、执行抓取等）。通过仅保留这些关键帧进行评估，可以极大减少计算和工程开销，同时保留对核心能力的评估。\n- **大脑-小脑协同评估设计：**\n - **StaticEmbodiedBench-VLM（大脑）：** 评估VLM作为认知核心的能力，涵盖三个维度：宏观规划（将高级指令分解为子任务）、微观感知（识别细粒度视觉线索）和分阶段推理（判断当前步骤并确定下一步动作）。评估在第一人称和第三人称两种视角下进行。\n - **StaticEmbodiedBench-VLA（小脑）：** 评估VLA的执行能力。VLA模型需预测一个7自由度（7-DoF）的动作向量（包含位置、姿态和夹爪状态）。通过计算预测动作与专家轨迹之间的L2距离来评分，并进一步将误差分解为位置误差、姿态误差和末端执行器误差，以提供更细粒度的分析。\n### 论文使用数据集和训练资源\n- **StaticEmbodiedBench-VLM：** 从Droid、VLABench等5个高质量数据集中收集了超过30万张高分辨率图像，经过GPT自动过滤、GPT-4o生成问答对和人工校验后，最终构建了一个包含1000个高质量任务样本的数据集。\n- **StaticEmbodiedBench-VLA：** 在真实实验室环境中，使用UR5机械臂构建了100个桌面操作任务数据集。每个任务组合了6种背景、10种物体和5种交互动词，记录了50个均匀采样的动作步骤及其对应的多视角图像和专家轨迹。\n- **训练资源：** 评估过程本身不涉及模型训练。VLM评估使用了两块NVIDIA A800 GPU，VLA评估使用了一块A800 GPU。\n### 论文使用的评估环境和评估指标\n- **评估环境：** 评估在静态数据上进行，不需要任何模拟器或真实机器人。为了验证静态评估的有效性，部分实验（S2D Gap验证）在真实的UR5机器人上部署执行。\n- **评估指标：**\n - **VLM指标：** 采用基于问答的准确率作为评估指标，分别计算宏观规划、微观感知、分阶段推理三个维度的得分，以及第一人称和第三人称视角下的得分，最后汇总为总分。\n - **VLA指标：** 采用预测的7-DoF动作向量与专家轨迹之间的L2距离作为核心指标。评分标准为：距离小于等于1毫米得100分，大于等于1米得0分，中间距离通过对数映射到0-100分。同时报告位置、姿态、末端执行器三个分解维度的得分。\n - **静态到动态差距（S2D）：** 使用皮尔逊线性相关系数（PLCC）来衡量静态评估分数与真实世界执行成功率之间的相关性，以此验证静态评估方法的可靠性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海人工智能实验室 (Shanghai AI Laboratory)</p>\n<h3>论文概述</h3>\n<p>论文指出了当前具身智能评估基准的两个主要问题：一是严重依赖高成本、高配置的模拟环境，二是缺乏统一的评估框架来同时评估高级认知和低级执行能力。为解决这些问题，论文提出了StaticEmbodiedBench，一个静态、统一、即插即用的基准。它通过仅使用静态关键帧和文本元数据来评估模型，无需复杂的模拟器，从而显著降低了评估的门槛。该基准包含42个场景和8个核心维度，能够分别评估视觉语言模型（VLM）的认知与决策能力以及视觉-语言-动作模型（VLA）的执行能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一种新颖的基于静态关键帧的评估方法和大脑-小脑协同评估框架，用于解耦和评估VLM与VLA在具身场景下的能力。</li><li>构建了StaticEmbodiedBench，一个静态、统一、即插即用的基准，包含42个场景和8个评估维度，并发布了包含200个样本的子集。</li><li>对19个VLM和11个VLA进行了全面评估，建立了首个具身智能的统一静态排行榜，并进行了深入分析。</li><li>通过在真实机器人上实验，量化了静态评估与动态执行之间的性能差距（Static-to-Dynamic Gap），验证了静态评估方法的有效性。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>静态关键帧评估思路：</strong> 核心洞察是，大多数具身任务的成功取决于轨迹中少数几个关键点（如识别目标物体、执行抓取等）。通过仅保留这些关键帧进行评估，可以极大减少计算和工程开销，同时保留对核心能力的评估。</li><li><strong>大脑-小脑协同评估设计：</strong></li></ul>\n<p> - <strong>StaticEmbodiedBench-VLM（大脑）：</strong> 评估VLM作为认知核心的能力，涵盖三个维度：宏观规划（将高级指令分解为子任务）、微观感知（识别细粒度视觉线索）和分阶段推理（判断当前步骤并确定下一步动作）。评估在第一人称和第三人称两种视角下进行。</p>\n<p> - <strong>StaticEmbodiedBench-VLA（小脑）：</strong> 评估VLA的执行能力。VLA模型需预测一个7自由度（7-DoF）的动作向量（包含位置、姿态和夹爪状态）。通过计算预测动作与专家轨迹之间的L2距离来评分，并进一步将误差分解为位置误差、姿态误差和末端执行器误差，以提供更细粒度的分析。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>StaticEmbodiedBench-VLM：</strong> 从Droid、VLABench等5个高质量数据集中收集了超过30万张高分辨率图像，经过GPT自动过滤、GPT-4o生成问答对和人工校验后，最终构建了一个包含1000个高质量任务样本的数据集。</li><li><strong>StaticEmbodiedBench-VLA：</strong> 在真实实验室环境中，使用UR5机械臂构建了100个桌面操作任务数据集。每个任务组合了6种背景、10种物体和5种交互动词，记录了50个均匀采样的动作步骤及其对应的多视角图像和专家轨迹。</li><li><strong>训练资源：</strong> 评估过程本身不涉及模型训练。VLM评估使用了两块NVIDIA A800 GPU，VLA评估使用了一块A800 GPU。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境：</strong> 评估在静态数据上进行，不需要任何模拟器或真实机器人。为了验证静态评估的有效性，部分实验（S2D Gap验证）在真实的UR5机器人上部署执行。</li><li><strong>评估指标：</strong></li></ul>\n<p> - <strong>VLM指标：</strong> 采用基于问答的准确率作为评估指标，分别计算宏观规划、微观感知、分阶段推理三个维度的得分，以及第一人称和第三人称视角下的得分，最后汇总为总分。</p>\n<p> - <strong>VLA指标：</strong> 采用预测的7-DoF动作向量与专家轨迹之间的L2距离作为核心指标。评分标准为：距离小于等于1毫米得100分，大于等于1米得0分，中间距离通过对数映射到0-100分。同时报告位置、姿态、末端执行器三个分解维度的得分。</p>\n<p> - <strong>静态到动态差距（S2D）：</strong> 使用皮尔逊线性相关系数（PLCC）来衡量静态评估分数与真实世界执行成功率之间的相关性，以此验证静态评估方法的可靠性。</p>"
  },
  {
    "date": "2025-08-06",
    "title": "A tutorial note on collecting simulated data for vision-language-action models",
    "link": "http://arxiv.org/abs/2508.06547",
    "summary_markdown": "论文研究单位\nThe University of Auckland, New Zealand\n\n论文概述\n本文是一篇关于如何为视觉-语言-动作模型收集仿真数据的教程。VLA模型通过单一神经网络统一处理视觉观察、语言理解和动作生成，但其性能高度依赖于高质量、大规模的视觉-语言-动作三元组训练数据。本文回顾并实践了三种具有代表性的数据收集系统：用于灵活定制的PyBullet仿真框架、用于标准化任务定义和评估的LIBERO基准测试套件，以及用于大规模多机器人数据采集的RT-X数据集。文章旨在为研究人员提供构建VLA数据集的具体指导。\n\n论文核心贡献点\n1. 提供了一份关于VLA模型数据收集的系统性教程，综述了PyBullet、LIBERO和RT-X三种不同方法的特点和适用场景。\n2. 详细演示了如何在PyBullet仿真环境中，结合Ravens框架，生成特定任务的高质量自定义数据集，包括任务选择、数据采集流程和存储格式的具体实现。\n3. 展示了如何对LIBERO基准进行定制化修改（如添加干扰物），并通过人类遥操作收集新的演示数据，详细介绍了从BDDL任务定义到HDF5数据格式生成的完整流程。\n4. 概述了RT-X大规模跨平台数据集的构成、特点及其在实现跨机器人学习中的作用，强调了其标准化动作表示形式的重要性。\n\n论文方法描述\n1. PyBullet数据收集方法：\n - 使用PyBullet物理引擎和Ravens框架进行仿真和数据收集。\n - 通过编写脚本化的专家策略来生成演示轨迹，避免了人为操作的不确定性。\n - 核心流程为：初始化环境和任务 -> 重置场景并随机化 -> 循环执行：获取RGB图像、专家策略生成动作、执行动作、保存数据。\n - 选取了`block-insertion`、`place-red-in-green`和`towers-of-hanoi`三个代表性任务进行数据采集。\n - 数据按`episode_id-step_id.pkl`的命名规则，分布式存储在`color`、`depth`、`action`、`reward`、`info`等目录中。\n2. LIBERO数据收集方法：\n - 利用LIBERO现有的基准数据集，或通过人类遥操作收集新数据。\n - 通过修改BDDL（Behavioral Domain Definition Language）文件来定制任务场景，例如添加内置的干扰物对象以增加任务难度。\n - 遥操作数据收集流程包括：解析BDDL任务 -> 初始化环境 -> 操作员通过SpaceMouse等设备控制机器人 -> 实时记录状态、动作和图像 -> 通过10步连续成功验证机制确保任务完成 -> 将`.npz`临时文件聚合成HDF5格式的结构化数据集。\n3. RT-X数据集应用方法：\n - RT-X数据集本身是一种资源，其核心方法在于将来自22种不同机器人的超过100万条真实轨迹数据统一到标准化的数据格式中。\n - 关键方法是其标准化的7维动作表示法 `[x, y, z, roll, pitch, yaw, gripper]`，它抽象了不同机器人的硬件差异，使得单个模型可以学习和控制多种机器人。\n\n论文使用数据集和训练资源\n1. 数据集：\n - 在PyBullet中生成的自定义数据集，包含`block-insertion`、`place-red-in-green`、`towers-of-hanoi`等任务数据。\n - LIBERO基准数据集（如`libero_10_no_noops`）。\n - 在LIBERO中通过修改场景和遥操作收集的新数据集。\n - RT-X数据集，包含来自22种机器人的超过100万条真实世界轨迹。\n2. 训练/仿真资源与工具：\n - PyBullet物理仿真引擎。\n - Ravens框架（基于PyBullet的任务套件）。\n - LIBERO基准测试套件（基于robosuite和MuJoCo）。\n - 用于遥操作的SpaceMouse等设备。\n - 代码资源已在GitHub上公开。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n - PyBullet仿真环境。\n - LIBERO/robosuite/MuJoCo仿真环境。\n - RT-X数据集中的真实世界多机器人环境。\n2. 评估指标：\n - 在PyBullet数据收集中，使用任务成功率作为评估指标，其脚本化的专家策略在所选任务上达到了95%的成功率。\n - 在LIBERO中，使用由BDDL文件定义的`env._check_success()`函数作为任务成功的判断标准。为了确保评估的鲁棒性，引入了10步连续验证机制，即任务成功状态必须连续保持10个时间步（0.5秒）才被最终确认，以防止误报。",
    "summary_html": "<p>论文研究单位</p>\n<p>The University of Auckland, New Zealand</p>\n\n<p>论文概述</p>\n<p>本文是一篇关于如何为视觉-语言-动作模型收集仿真数据的教程。VLA模型通过单一神经网络统一处理视觉观察、语言理解和动作生成，但其性能高度依赖于高质量、大规模的视觉-语言-动作三元组训练数据。本文回顾并实践了三种具有代表性的数据收集系统：用于灵活定制的PyBullet仿真框架、用于标准化任务定义和评估的LIBERO基准测试套件，以及用于大规模多机器人数据采集的RT-X数据集。文章旨在为研究人员提供构建VLA数据集的具体指导。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提供了一份关于VLA模型数据收集的系统性教程，综述了PyBullet、LIBERO和RT-X三种不同方法的特点和适用场景。</li><li>详细演示了如何在PyBullet仿真环境中，结合Ravens框架，生成特定任务的高质量自定义数据集，包括任务选择、数据采集流程和存储格式的具体实现。</li><li>展示了如何对LIBERO基准进行定制化修改（如添加干扰物），并通过人类遥操作收集新的演示数据，详细介绍了从BDDL任务定义到HDF5数据格式生成的完整流程。</li><li>概述了RT-X大规模跨平台数据集的构成、特点及其在实现跨机器人学习中的作用，强调了其标准化动作表示形式的重要性。</li></ol>\n\n<p>论文方法描述</p>\n<p>1. PyBullet数据收集方法：</p>\n<p> - 使用PyBullet物理引擎和Ravens框架进行仿真和数据收集。</p>\n<p> - 通过编写脚本化的专家策略来生成演示轨迹，避免了人为操作的不确定性。</p>\n<p> - 核心流程为：初始化环境和任务 -> 重置场景并随机化 -> 循环执行：获取RGB图像、专家策略生成动作、执行动作、保存数据。</p>\n<p> - 选取了<code>block-insertion</code>、<code>place-red-in-green</code>和<code>towers-of-hanoi</code>三个代表性任务进行数据采集。</p>\n<p> - 数据按<code>episode_id-step_id.pkl</code>的命名规则，分布式存储在<code>color</code>、<code>depth</code>、<code>action</code>、<code>reward</code>、<code>info</code>等目录中。</p>\n<p>2. LIBERO数据收集方法：</p>\n<p> - 利用LIBERO现有的基准数据集，或通过人类遥操作收集新数据。</p>\n<p> - 通过修改BDDL（Behavioral Domain Definition Language）文件来定制任务场景，例如添加内置的干扰物对象以增加任务难度。</p>\n<p> - 遥操作数据收集流程包括：解析BDDL任务 -> 初始化环境 -> 操作员通过SpaceMouse等设备控制机器人 -> 实时记录状态、动作和图像 -> 通过10步连续成功验证机制确保任务完成 -> 将<code>.npz</code>临时文件聚合成HDF5格式的结构化数据集。</p>\n<p>3. RT-X数据集应用方法：</p>\n<p> - RT-X数据集本身是一种资源，其核心方法在于将来自22种不同机器人的超过100万条真实轨迹数据统一到标准化的数据格式中。</p>\n<p> - 关键方法是其标准化的7维动作表示法 <code>[x, y, z, roll, pitch, yaw, gripper]</code>，它抽象了不同机器人的硬件差异，使得单个模型可以学习和控制多种机器人。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. 数据集：</p>\n<p> - 在PyBullet中生成的自定义数据集，包含<code>block-insertion</code>、<code>place-red-in-green</code>、<code>towers-of-hanoi</code>等任务数据。</p>\n<p> - LIBERO基准数据集（如<code>libero_10_no_noops</code>）。</p>\n<p> - 在LIBERO中通过修改场景和遥操作收集的新数据集。</p>\n<p> - RT-X数据集，包含来自22种机器人的超过100万条真实世界轨迹。</p>\n<p>2. 训练/仿真资源与工具：</p>\n<p> - PyBullet物理仿真引擎。</p>\n<p> - Ravens框架（基于PyBullet的任务套件）。</p>\n<p> - LIBERO基准测试套件（基于robosuite和MuJoCo）。</p>\n<p> - 用于遥操作的SpaceMouse等设备。</p>\n<p> - 代码资源已在GitHub上公开。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> - PyBullet仿真环境。</p>\n<p> - LIBERO/robosuite/MuJoCo仿真环境。</p>\n<p> - RT-X数据集中的真实世界多机器人环境。</p>\n<p>2. 评估指标：</p>\n<p> - 在PyBullet数据收集中，使用任务成功率作为评估指标，其脚本化的专家策略在所选任务上达到了95%的成功率。</p>\n<p> - 在LIBERO中，使用由BDDL文件定义的<code>env._check_success()</code>函数作为任务成功的判断标准。为了确保评估的鲁棒性，引入了10步连续验证机制，即任务成功状态必须连续保持10个时间步（0.5秒）才被最终确认，以防止误报。</p>"
  },
  {
    "date": "2025-08-07",
    "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control",
    "link": "http://arxiv.org/abs/2508.05342",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-07",
    "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction",
    "link": "http://arxiv.org/abs/2508.05294",
    "summary_markdown": "### 论文研究单位\n- University of Turku\n- Zurich University of Applied Sciences\n- Centre for Artificial Ingelligence, Zurich University of Applied Sciences\n- Agentic Systems Lab, Department of Management, Technology and Economics, ETH Zürich\n- Faculty of Mathematics and Information Science, Warsaw University of Technology\n- Robotec.ai\n- Binabik.ai\n### 论文概述\n本文是一篇综述论文，回顾和分类了基于大语言模型（LLM）、视觉语言模型（VLM）和视觉-语言-动作模型（VLA）的机器人自主性与交互方法。文章聚焦于将基础模型作为高级代理的模块化系统，这些代理能够理解用户意图、生成计划、调用机器人API或与中间件（如ROS）交互，而不是取代底层的机器人软件栈。论文旨在填补现有综述的空白，特别关注AI代理与现有控制软件、库或中间件交互的新兴设计模式，并包括社区驱动项目、ROS包和工业框架。\n### 论文核心贡献点\n- 提出了一个双维度的分类法：一个维度是模型与机器人系统的集成方法（协议集成、接口集成、编排导向集成、嵌入式/直接集成）；另一个维度是代理在系统中扮演的角色（规划代理、编排代理、任务特定代理、以模型为中心的代理、通用代理、通用系统代理）。\n- 对现有学术工作和社区项目进行了全面的回顾和比较分析，涵盖了从早期的“代码即策略”到最新的通用VLA模型和代理框架。\n- 提供了一个实践设计工具包部分，指导开发者和研究人员的实现与原型设计。\n- 在讨论部分深入探讨了关键开放性问题，并概述了代理式具身AI的未来研究方向，如多智能体系统、伦理考虑、边缘-云计算连续体、记忆机制和世界模型等。\n### 论文方法描述\n论文的核心方法是提出并使用其分类法来分析和组织现有工作。该方法包括四个主要集成途径：\n1. **协议集成**：将LLM用作不同协议间的“翻译器”，例如将自然语言指令转换为单个ROS命令行调用（如 `ros2ai`）或通过模型上下文协议（MCP）服务器进行工具调用。\n2. **接口集成**：在协议集成的基础上增强交互性，代理通过多轮工具调用与环境和用户进行交互。代理可以分解高级指令，并根据工具调用的结果进行适应性调整，例如ROSA、RAI、BUMBLE等框架。\n3. **编排导向集成**：LLM/VLM作为资源管理器、规划器或协调器，负责管理多个子系统、工具或下属代理。其重点在于资源分配和任务调度，而非直接的用户交互。\n4. **嵌入式或直接集成**：模型直接映射感知输入到机器人动作（端到端），或作为特定子系统（如感知模块）。这通常指VLA或大型行为模型（LBM），例如RT-2、π0。\n### 论文使用数据集和训练资源\n作为一篇综述论文，本文本身没有提出需要特定数据集或训练资源的新模型。其分析和分类基于已发表的学术论文和公开的社区项目。论文回顾的这些基础模型（如RT-2、π0等）是在各种公开的机器人轨迹数据和互联网规模的视觉-语言数据集上进行训练的，但本文未详细列出这些具体的数据集和计算资源。\n### 论文使用的评估环境和评估指标\n本文不进行新的实验评估，而是回顾和比较了其所引用工作中的评估方法。评估环境多样化，包括物理机器人平台（如JPL的NeBula-Spot四足机器人、Husarion ROSBot XL）和仿真环境（如NVIDIA Isaac Sim）。评估指标因任务而异，通常包括操作成功率、导航效率、任务完成时间以及人机交互的自然性和安全性等。论文指出，目前该领域缺乏统一的评估基准和标准化的指标体系，这是一个重要的开放性问题。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>University of Turku</li><li>Zurich University of Applied Sciences</li><li>Centre for Artificial Ingelligence, Zurich University of Applied Sciences</li><li>Agentic Systems Lab, Department of Management, Technology and Economics, ETH Zürich</li><li>Faculty of Mathematics and Information Science, Warsaw University of Technology</li><li>Robotec.ai</li><li>Binabik.ai</li></ul>\n<h3>论文概述</h3>\n<p>本文是一篇综述论文，回顾和分类了基于大语言模型（LLM）、视觉语言模型（VLM）和视觉-语言-动作模型（VLA）的机器人自主性与交互方法。文章聚焦于将基础模型作为高级代理的模块化系统，这些代理能够理解用户意图、生成计划、调用机器人API或与中间件（如ROS）交互，而不是取代底层的机器人软件栈。论文旨在填补现有综述的空白，特别关注AI代理与现有控制软件、库或中间件交互的新兴设计模式，并包括社区驱动项目、ROS包和工业框架。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个双维度的分类法：一个维度是模型与机器人系统的集成方法（协议集成、接口集成、编排导向集成、嵌入式/直接集成）；另一个维度是代理在系统中扮演的角色（规划代理、编排代理、任务特定代理、以模型为中心的代理、通用代理、通用系统代理）。</li><li>对现有学术工作和社区项目进行了全面的回顾和比较分析，涵盖了从早期的“代码即策略”到最新的通用VLA模型和代理框架。</li><li>提供了一个实践设计工具包部分，指导开发者和研究人员的实现与原型设计。</li><li>在讨论部分深入探讨了关键开放性问题，并概述了代理式具身AI的未来研究方向，如多智能体系统、伦理考虑、边缘-云计算连续体、记忆机制和世界模型等。</li></ul>\n<h3>论文方法描述</h3>\n<p>论文的核心方法是提出并使用其分类法来分析和组织现有工作。该方法包括四个主要集成途径：</p>\n<ol><li><strong>协议集成</strong>：将LLM用作不同协议间的“翻译器”，例如将自然语言指令转换为单个ROS命令行调用（如 <code>ros2ai</code>）或通过模型上下文协议（MCP）服务器进行工具调用。</li><li><strong>接口集成</strong>：在协议集成的基础上增强交互性，代理通过多轮工具调用与环境和用户进行交互。代理可以分解高级指令，并根据工具调用的结果进行适应性调整，例如ROSA、RAI、BUMBLE等框架。</li><li><strong>编排导向集成</strong>：LLM/VLM作为资源管理器、规划器或协调器，负责管理多个子系统、工具或下属代理。其重点在于资源分配和任务调度，而非直接的用户交互。</li><li><strong>嵌入式或直接集成</strong>：模型直接映射感知输入到机器人动作（端到端），或作为特定子系统（如感知模块）。这通常指VLA或大型行为模型（LBM），例如RT-2、π0。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>作为一篇综述论文，本文本身没有提出需要特定数据集或训练资源的新模型。其分析和分类基于已发表的学术论文和公开的社区项目。论文回顾的这些基础模型（如RT-2、π0等）是在各种公开的机器人轨迹数据和互联网规模的视觉-语言数据集上进行训练的，但本文未详细列出这些具体的数据集和计算资源。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>本文不进行新的实验评估，而是回顾和比较了其所引用工作中的评估方法。评估环境多样化，包括物理机器人平台（如JPL的NeBula-Spot四足机器人、Husarion ROSBot XL）和仿真环境（如NVIDIA Isaac Sim）。评估指标因任务而异，通常包括操作成功率、导航效率、任务完成时间以及人机交互的自然性和安全性等。论文指出，目前该领域缺乏统一的评估基准和标准化的指标体系，这是一个重要的开放性问题。</p>"
  },
  {
    "date": "2025-08-07",
    "title": "Learning to See and Act: Task-Aware View Planning for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2508.05186",
    "summary_markdown": "### 论文研究单位\n中山大学、鹏城实验室、南洋理工大学、中国科学院深圳先进技术研究院、X-Era AI Lab\n### 论文概述\n论文提出Task-Aware View Planning (TAVP)框架，通过整合主动视角规划与任务感知表征学习，解决多任务机器人操作中固定视角导致的3D感知受限和任务干扰问题。该方法利用高效的探索策略获取信息丰富的视角，并结合任务感知的视觉编码器提升多任务泛化能力，在RLBench任务上实现了优于固定视角基线模型的性能。\n### 论文核心贡献点\n1. Multi-Viewpoint Exploration Policy (MVEP)：通过动态多视角重渲染解决遮挡和视角不足问题，增强3D感知能力。\n2. Task-aware Mixture-of-Experts (TaskMoE)：根据任务相关的指令和场景视觉信息动态选择感知与动作生成专家，提升多任务处理能力和鲁棒性。\n3. 在18个RLBench仿真任务上实现显著优于现有基线的多任务操作性能，在准确性和鲁棒性方面表现突出。\n### 论文方法描述\nTAVP框架包含三个阶段：\n1. **固定视角训练**：使用三个默认视角训练TaskMoE集成的RVT-2变体，损失函数包括热力图交叉熵损失、旋转损失、夹爪状态损失和碰撞损失。\n2. **视角策略优化**：通过伪环境交互机制和PPO算法训练MVEP，奖励函数包括任务损失奖励、置信度奖励和视角多样性奖励。\n3. **联合微调**：使用相同损失函数微调整个TAVP模型（除MVEP外）。\nTaskMoE通过跨模态模块融合指令和视觉信息，使用解耦门控策略实现参数共享，MVEP基于点云预测K个相机姿态并使用look-at模型参数化。\n### 论文使用数据集和训练资源\n- **数据集**：RLBench仿真环境18个操作任务，每个任务包含多个变体。\n- **训练资源**：4×NVIDIA RTX A800 GPU（80GB内存）。\n- **关键参数**：视角数K=3，TaskMoE门数N_G=8，专家数N_E=16，每个任务选择Top-2专家，相机径向约束0.75-1.3米。\n### 论文使用的评估环境和评估指标\n- **评估环境**：RLBench仿真环境（CoppeliaSim）和真实世界设置（6-DoF Dobot Nova 2机械臂+3个Intel RealSense深度相机）。\n- **评估指标**：任务平均成功率（%）及其标准差，按任务类型细分为单独成功率。\n- **基线对比**：与固定视角模型（RVT2, ARP, ARP+）和3D基线模型（PerAct, HiveFormer等）在18个任务上对比成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中山大学、鹏城实验室、南洋理工大学、中国科学院深圳先进技术研究院、X-Era AI Lab</p>\n<h3>论文概述</h3>\n<p>论文提出Task-Aware View Planning (TAVP)框架，通过整合主动视角规划与任务感知表征学习，解决多任务机器人操作中固定视角导致的3D感知受限和任务干扰问题。该方法利用高效的探索策略获取信息丰富的视角，并结合任务感知的视觉编码器提升多任务泛化能力，在RLBench任务上实现了优于固定视角基线模型的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>Multi-Viewpoint Exploration Policy (MVEP)：通过动态多视角重渲染解决遮挡和视角不足问题，增强3D感知能力。</li><li>Task-aware Mixture-of-Experts (TaskMoE)：根据任务相关的指令和场景视觉信息动态选择感知与动作生成专家，提升多任务处理能力和鲁棒性。</li><li>在18个RLBench仿真任务上实现显著优于现有基线的多任务操作性能，在准确性和鲁棒性方面表现突出。</li></ol>\n<h3>论文方法描述</h3>\n<p>TAVP框架包含三个阶段：</p>\n<ol><li><strong>固定视角训练</strong>：使用三个默认视角训练TaskMoE集成的RVT-2变体，损失函数包括热力图交叉熵损失、旋转损失、夹爪状态损失和碰撞损失。</li><li><strong>视角策略优化</strong>：通过伪环境交互机制和PPO算法训练MVEP，奖励函数包括任务损失奖励、置信度奖励和视角多样性奖励。</li><li><strong>联合微调</strong>：使用相同损失函数微调整个TAVP模型（除MVEP外）。</li></ol>\n<p>TaskMoE通过跨模态模块融合指令和视觉信息，使用解耦门控策略实现参数共享，MVEP基于点云预测K个相机姿态并使用look-at模型参数化。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：RLBench仿真环境18个操作任务，每个任务包含多个变体。</li><li><strong>训练资源</strong>：4×NVIDIA RTX A800 GPU（80GB内存）。</li><li><strong>关键参数</strong>：视角数K=3，TaskMoE门数N_G=8，专家数N_E=16，每个任务选择Top-2专家，相机径向约束0.75-1.3米。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：RLBench仿真环境（CoppeliaSim）和真实世界设置（6-DoF Dobot Nova 2机械臂+3个Intel RealSense深度相机）。</li><li><strong>评估指标</strong>：任务平均成功率（%）及其标准差，按任务类型细分为单独成功率。</li><li><strong>基线对比</strong>：与固定视角模型（RVT2, ARP, ARP+）和3D基线模型（PerAct, HiveFormer等）在18个任务上对比成功率。</li></ul>"
  },
  {
    "date": "2025-08-04",
    "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
    "link": "http://arxiv.org/abs/2508.02549",
    "summary_markdown": "### 论文研究单位\nShuo Wang<sup>1,3</sup>, Yongcai Wang<sup>1</sup>, Zhaoxin Fan<sup>2</sup>, Yucheng Wang<sup>3</sup>, Maiyue Chen<sup>3</sup>, Kaihui Wang<sup>3</sup>, Zhizhong Su<sup>3</sup>, Wanting Li<sup>1</sup>, Xudong Cai<sup>1</sup>, Yeying Jin<sup>4</sup>, Deying Li<sup>1</sup>。注：原文HTML未提供具体机构名称。\n### 论文概述\n本文针对单目视觉语言导航任务中，由于视角受限导致性能显著低于使用全景RGB-D输入方法的问题，提出了一个名为MonoDream的轻量级视觉语言行动框架。该框架使单目智能体能够学习一个统一的导航表示，通过潜在全景梦境任务进行监督，从而仅凭单目输入就能推断出隐式的全局、几何和时序上下文。实验表明，MonoDream在多个VLN基准测试中持续提升单目导航性能，并显著缩小了与基于全景智能体之间的性能差距。\n### 论文核心贡献点\n1. 提出了MonoDream，一种新颖的单目VLN框架，通过增强智能体的内部全局感知能力，使其能够从单目图像中推断隐式的全局、几何和时序上下文。\n2. 引入了两个核心组件：统一导航表示，用于共同编码导航动作和潜在的全局场景信息；以及潜在全景梦境任务，通过监督当前和未来全景RGB-D的潜在特征来学习UNR。\n3. 在R2R-CE和RxR-CE等单目VLN-CE基准上取得了最先进的性能，并且在没有使用外部训练数据的情况下展现了强大的泛化能力。\n### 论文方法描述\nMonoDream构建于一个视觉语言模型之上，核心是统一导航表示。该表示是一个共享的潜在空间，共同对齐了导航相关的信息，包括动作意图、全局场景布局、深度感知和未来动态。为了训练UNR，方法设计了潜在全景梦境任务作为辅助监督信号，仅在训练中使用。LPD任务引导模型从单目输入中预测当前和未来步骤的全景RGB和深度观察的潜在特征。整个框架通过多任务协同训练进行优化，联合训练动作预测、指令推理和LPD任务，使模型学习将动作决策与想象中的全局和未来上下文对齐。\n### 论文使用数据集和训练资源\n数据集：使用了模拟环境中的数据，包括R2R-CE和RxR-CE的训练集，并通过DAgger策略额外收集了50万个样本，总计约142万个步级样本。\n训练资源：基础模型为NVILA-lite-2B，使用8块NVIDIA H20 GPU进行训练，训练了5个周期，学习率为1e-5，批大小为80。\n### 论文使用的评估环境和评估指标\n评估环境：在VLN-CE基准测试中进行评估，具体包括R2R-CE和RxR-CE数据集的Val-Unseen拆分。\n评估指标：遵循标准VLN评估协议，主要指标包括导航误差、成功率、成功率加权路径长度和导航成功率，其中SR和SPL被认为是主要评估指标。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Shuo Wang<sup>1,3</sup>, Yongcai Wang<sup>1</sup>, Zhaoxin Fan<sup>2</sup>, Yucheng Wang<sup>3</sup>, Maiyue Chen<sup>3</sup>, Kaihui Wang<sup>3</sup>, Zhizhong Su<sup>3</sup>, Wanting Li<sup>1</sup>, Xudong Cai<sup>1</sup>, Yeying Jin<sup>4</sup>, Deying Li<sup>1</sup>。注：原文HTML未提供具体机构名称。</p>\n<h3>论文概述</h3>\n<p>本文针对单目视觉语言导航任务中，由于视角受限导致性能显著低于使用全景RGB-D输入方法的问题，提出了一个名为MonoDream的轻量级视觉语言行动框架。该框架使单目智能体能够学习一个统一的导航表示，通过潜在全景梦境任务进行监督，从而仅凭单目输入就能推断出隐式的全局、几何和时序上下文。实验表明，MonoDream在多个VLN基准测试中持续提升单目导航性能，并显著缩小了与基于全景智能体之间的性能差距。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了MonoDream，一种新颖的单目VLN框架，通过增强智能体的内部全局感知能力，使其能够从单目图像中推断隐式的全局、几何和时序上下文。</li><li>引入了两个核心组件：统一导航表示，用于共同编码导航动作和潜在的全局场景信息；以及潜在全景梦境任务，通过监督当前和未来全景RGB-D的潜在特征来学习UNR。</li><li>在R2R-CE和RxR-CE等单目VLN-CE基准上取得了最先进的性能，并且在没有使用外部训练数据的情况下展现了强大的泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<p>MonoDream构建于一个视觉语言模型之上，核心是统一导航表示。该表示是一个共享的潜在空间，共同对齐了导航相关的信息，包括动作意图、全局场景布局、深度感知和未来动态。为了训练UNR，方法设计了潜在全景梦境任务作为辅助监督信号，仅在训练中使用。LPD任务引导模型从单目输入中预测当前和未来步骤的全景RGB和深度观察的潜在特征。整个框架通过多任务协同训练进行优化，联合训练动作预测、指令推理和LPD任务，使模型学习将动作决策与想象中的全局和未来上下文对齐。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：使用了模拟环境中的数据，包括R2R-CE和RxR-CE的训练集，并通过DAgger策略额外收集了50万个样本，总计约142万个步级样本。</p>\n<p>训练资源：基础模型为NVILA-lite-2B，使用8块NVIDIA H20 GPU进行训练，训练了5个周期，学习率为1e-5，批大小为80。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：在VLN-CE基准测试中进行评估，具体包括R2R-CE和RxR-CE数据集的Val-Unseen拆分。</p>\n<p>评估指标：遵循标准VLN评估协议，主要指标包括导航误差、成功率、成功率加权路径长度和导航成功率，其中SR和SPL被认为是主要评估指标。</p>"
  },
  {
    "date": "2025-08-04",
    "title": "CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning",
    "link": "http://arxiv.org/abs/2508.02219",
    "summary_markdown": "论文研究单位\nDongchi Huang, Zhirui Fang, Tianle Zhang, Yihang Li, Lin Zhao, Chunhe Xia (通讯作者: xch@buaa.edu.cn)。具体单位信息在提供的HTML原文中未明确列出。\n\n论文概述\n该论文探讨了如何通过离线强化学习来高效微调视觉-语言-动作（VLA）模型。现有的微调方法主要依赖行为克隆，存在对数据质量和数量依赖性强、难以处理分布外（OOD）场景等问题。尽管强化学习（RL）有潜力解决这些问题，但在样本效率、与VLA模型常用的动作分块技术的兼容性以及训练稳定性方面仍面临挑战。为此，本文提出了一种名为CO-RFT（Chunked Offline Reinforced Fine-tuning）的算法，该算法结合了动作分块技术，旨在仅使用少量（30到60个）人类演示数据的情况下，高效地微调VLA模型，并在真实世界的机器人任务中取得更好的性能和泛化能力。\n\n论文核心贡献点\n1. 提出了Chunked RL框架：这是一个专门为VLA模型设计的强化学习新框架，通过扩展时序差分（TD）学习来兼容VLA模型中的动作分块特性。该框架利用一个基于Transformer的Critic网络来评估整个动作序列的价值，从而提高样本效率和训练稳定性。\n2. 提出了CO-RFT算法：这是一个两阶段的微调算法。第一阶段通过行为克隆（BC）和全参数微调，将预训练的VLA模型适应到新的机器人实体和工作空间。第二阶段则采用Chunked RL框架进行离线强化学习，以优化策略并超越演示数据的表现。\n3. 实现了优越的性能：在真实世界的机器人操作任务中，CO-RFT相比传统的监督微调（SFT）方法，成功率平均提升了57%，平均循环时间减少了22.3%。此外，该方法在从未见过的物体位置上表现出强大的泛化能力，成功率达到44.3%。\n\n论文方法描述\n该方法包含两个核心部分：Chunked RL框架和CO-RFT两阶段微调流程。\n1. **Chunked RL框架**：\n * **分块时序差分学习**：与传统的单步TD学习不同，该方法训练Critic和Actor来预测和优化一个包含`h`个连续动作的“动作块”。TD目标计算了执行整个动作块后`h`步的回报，从而加速价值传播。\n * **分块Critic网络**：Critic网络采用Transformer架构，输入当前状态和动作块，通过自注意力机制和因果掩码，为动作块中的每个动作输出一个Q值。这样仅需一个网络即可学习所有Q值，降低了计算成本。\n * **训练目标**：Critic的损失函数是预测的Q值与多步返回目标之间的均方误差。Actor则通过最大化Q值序列的平均值来更新。\n2. **CO-RFT两阶段微调流程**：\n * **阶段一：行为克隆（BC）**：收集少量（约30条）人类演示数据，通过全参数微调对VLA模型进行初始化，使其适应目标机器人的本体感觉和动作空间。\n * **阶段二：分块离线RL**：基于BC阶段初始化的模型，应用Chunked RL框架进行离线RL训练。论文采用了Cal-QL算法，通过添加保守正则化项来处理离线数据分布外（OOD）动作的问题，从而稳定训练过程。\n3. **实用技术**：\n * **奖励上采样**：为了处理现实任务中常见的稀疏奖励问题，提出一种数据收集策略，即在遥操作演示时记录额外的成功步骤，以增加包含奖励信号的样本数量。\n * **模型架构**：方法在RoboVLMs上实现，该模型使用Kosmos-2作为视觉语言骨干网络，并结合了TD3算法，该算法擅长生成确定性动作。\n\n论文使用数据集和训练资源\n1. **数据集**：论文设计了6个具有挑战性的灵巧手操作任务（抓取杯子、捏方块、抓取消毒液、手持条形码扫描器、抓取圆环、取回马克杯）。对于每个任务，通过遥操作收集了30个人工演示轨迹。数据包含语言指令、视觉图像和本体感觉信息。为了研究数据多样性，作者还收集了“随机初始化”和“固定初始化”两种不同类型的演示数据。\n2. **训练资源**：\n * **硬件**：实验平台是Realman单臂机器人，配备一个6自由度（DoF）的机械臂和Inspire公司的6自由度灵巧手作为末端执行器。头部安装了一台ZED2立体摄像头用于捕捉RGB图像。\n * **计算资源**：HTML原文中未明确指出所使用的GPU型号或数量。\n\n论文使用的评估环境和评估指标\n1. **评估环境**：所有实验均在真实世界环境中进行。任务在统一的照明条件下于白色桌面上执行。评估分为两种场景：\n * **分布内（IND）评估**：在与训练数据相似的位置上测试模型性能。\n * **分布外（OOD）评估**：将目标物体放置在训练时从未见过的位置上，测试模型的位置泛化能力。\n2. **评估指标**：\n * **成功率**：任务成功的标准是灵巧手成功将目标物体举到空中。该指标通过多次试验（例如40次）的成功次数来计算。\n * **平均循环时间**：记录机器人成功完成一次任务所花费的平均步数，用于衡量策略的执行效率。",
    "summary_html": "<p>论文研究单位</p>\n<p>Dongchi Huang, Zhirui Fang, Tianle Zhang, Yihang Li, Lin Zhao, Chunhe Xia (通讯作者: xch@buaa.edu.cn)。具体单位信息在提供的HTML原文中未明确列出。</p>\n\n<p>论文概述</p>\n<p>该论文探讨了如何通过离线强化学习来高效微调视觉-语言-动作（VLA）模型。现有的微调方法主要依赖行为克隆，存在对数据质量和数量依赖性强、难以处理分布外（OOD）场景等问题。尽管强化学习（RL）有潜力解决这些问题，但在样本效率、与VLA模型常用的动作分块技术的兼容性以及训练稳定性方面仍面临挑战。为此，本文提出了一种名为CO-RFT（Chunked Offline Reinforced Fine-tuning）的算法，该算法结合了动作分块技术，旨在仅使用少量（30到60个）人类演示数据的情况下，高效地微调VLA模型，并在真实世界的机器人任务中取得更好的性能和泛化能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了Chunked RL框架：这是一个专门为VLA模型设计的强化学习新框架，通过扩展时序差分（TD）学习来兼容VLA模型中的动作分块特性。该框架利用一个基于Transformer的Critic网络来评估整个动作序列的价值，从而提高样本效率和训练稳定性。</li><li>提出了CO-RFT算法：这是一个两阶段的微调算法。第一阶段通过行为克隆（BC）和全参数微调，将预训练的VLA模型适应到新的机器人实体和工作空间。第二阶段则采用Chunked RL框架进行离线强化学习，以优化策略并超越演示数据的表现。</li><li>实现了优越的性能：在真实世界的机器人操作任务中，CO-RFT相比传统的监督微调（SFT）方法，成功率平均提升了57%，平均循环时间减少了22.3%。此外，该方法在从未见过的物体位置上表现出强大的泛化能力，成功率达到44.3%。</li></ol>\n\n<p>论文方法描述</p>\n<p>该方法包含两个核心部分：Chunked RL框架和CO-RFT两阶段微调流程。</p>\n<p>1. <strong>Chunked RL框架</strong>：</p>\n<p> * <strong>分块时序差分学习</strong>：与传统的单步TD学习不同，该方法训练Critic和Actor来预测和优化一个包含<code>h</code>个连续动作的“动作块”。TD目标计算了执行整个动作块后<code>h</code>步的回报，从而加速价值传播。</p>\n<p> * <strong>分块Critic网络</strong>：Critic网络采用Transformer架构，输入当前状态和动作块，通过自注意力机制和因果掩码，为动作块中的每个动作输出一个Q值。这样仅需一个网络即可学习所有Q值，降低了计算成本。</p>\n<p> * <strong>训练目标</strong>：Critic的损失函数是预测的Q值与多步返回目标之间的均方误差。Actor则通过最大化Q值序列的平均值来更新。</p>\n<p>2. <strong>CO-RFT两阶段微调流程</strong>：</p>\n<p> * <strong>阶段一：行为克隆（BC）</strong>：收集少量（约30条）人类演示数据，通过全参数微调对VLA模型进行初始化，使其适应目标机器人的本体感觉和动作空间。</p>\n<p> * <strong>阶段二：分块离线RL</strong>：基于BC阶段初始化的模型，应用Chunked RL框架进行离线RL训练。论文采用了Cal-QL算法，通过添加保守正则化项来处理离线数据分布外（OOD）动作的问题，从而稳定训练过程。</p>\n<p>3. <strong>实用技术</strong>：</p>\n<p> * <strong>奖励上采样</strong>：为了处理现实任务中常见的稀疏奖励问题，提出一种数据收集策略，即在遥操作演示时记录额外的成功步骤，以增加包含奖励信号的样本数量。</p>\n<p> * <strong>模型架构</strong>：方法在RoboVLMs上实现，该模型使用Kosmos-2作为视觉语言骨干网络，并结合了TD3算法，该算法擅长生成确定性动作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li><strong>数据集</strong>：论文设计了6个具有挑战性的灵巧手操作任务（抓取杯子、捏方块、抓取消毒液、手持条形码扫描器、抓取圆环、取回马克杯）。对于每个任务，通过遥操作收集了30个人工演示轨迹。数据包含语言指令、视觉图像和本体感觉信息。为了研究数据多样性，作者还收集了“随机初始化”和“固定初始化”两种不同类型的演示数据。</li><li><strong>训练资源</strong>：</li></ol>\n<p> * <strong>硬件</strong>：实验平台是Realman单臂机器人，配备一个6自由度（DoF）的机械臂和Inspire公司的6自由度灵巧手作为末端执行器。头部安装了一台ZED2立体摄像头用于捕捉RGB图像。</p>\n<p> * <strong>计算资源</strong>：HTML原文中未明确指出所使用的GPU型号或数量。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. <strong>评估环境</strong>：所有实验均在真实世界环境中进行。任务在统一的照明条件下于白色桌面上执行。评估分为两种场景：</p>\n<p> * <strong>分布内（IND）评估</strong>：在与训练数据相似的位置上测试模型性能。</p>\n<p> * <strong>分布外（OOD）评估</strong>：将目标物体放置在训练时从未见过的位置上，测试模型的位置泛化能力。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> * <strong>成功率</strong>：任务成功的标准是灵巧手成功将目标物体举到空中。该指标通过多次试验（例如40次）的成功次数来计算。</p>\n<p> * <strong>平均循环时间</strong>：记录机器人成功完成一次任务所花费的平均步数，用于衡量策略的执行效率。</p>"
  },
  {
    "date": "2025-08-04",
    "title": "FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2508.02190",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-08-04",
    "title": "RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2508.02062",
    "summary_markdown": "### 论文研究单位\n宾夕法尼亚大学（University of Pennsylvania）和不列颠哥伦比亚大学（University of British Columbia）\n### 论文概述\n本文提出了一种名为RICL（Retraining for In-Context Learning）的方法，旨在向预训练的视觉-语言-动作（VLA）模型中注入上下文学习能力。传统VLA模型缺乏上下文学习（ICL）能力，而RICL通过微调少量机器人演示数据，使模型能够利用检索增强生成（RAG）和ICL适应新任务。用户只需提供10-20个目标任务演示，RICL即可检索相关上下文信息并提升任务性能，无需参数更新。实验应用于π₀-FAST VLA，在涉及未见对象、新颖动作和新场景的操作任务中验证了有效性。\n### 论文核心贡献点\n- 提出RICL方法，首次实现向预训练VLA中注入ICL能力，无需从头训练。\n- 允许用户通过少量演示（10-20个）教导VLA执行新任务，支持零参数更新适应。\n- 结合RAG和ICL，提升模型在未见对象、新颖动作和新场景中的泛化能力。\n- 实验证明RICL-π₀在8个任务上平均完整任务成功率达31.25%，远高于基线的2.5%。\n- 支持进一步微调：在目标任务演示上微调RICL-VLA，可将平均成功率提升至61.67%。\n- 开源代码和模型权重（RICL-π₀），提供首个机器人操作任务的简单ICL接口。\n### 论文方法描述\n- **RICL训练流程**：后训练预训练VLA，输入序列包含查询图像/状态和检索自演示的多个图像、状态、动作。使用DINO-v2图像编码器和ℓ₂距离检索最相关的4组邻居数据，按距离排序（最近邻居置于上下文左侧）。\n- **动作插值层**：预测动作通过加权插值结合检索动作a'和LLM输出，公式为：\n ```\n πᵗʰᵉᵗᵃ_RICL-VLA = e^{-λd} * one-hot(a') + (1 - e^{-λd}) * σ(π_θ(retrieved, query))\n ```\n 其中d为查询图像与最近邻居的ℓ₂距离，λ=10，σ为Softmax函数。\n- **训练细节**：仅微调LLM部分，冻结图像编码器；最小化查询动作块的交叉熵损失；使用CosineDecaySchedule学习率调度。\n- **部署与微调**：部署时检索任务演示数据并执行ICL；微调阶段在相同演示上进行检索增强训练，进一步提升性能。\n### 论文使用数据集和训练资源\n- **训练数据**：使用Franka DROID平台收集400个演示（20个拾取放置任务，每任务20个演示），任务如\"移动物体至左侧/右侧\"或\"拾取物体放入碗中\"。\n- **基础模型**：π₀-FAST-DROID（基于π₀-FAST在DROID数据集微调）。\n- **检索数据**：评估时每个目标任务收集20个演示（如pokeball、idliplate等），用于RAG和ICL。\n- **训练资源**：两个NVIDIA A100 GPU，批量大小16，训练3个epoch，峰值学习率2.5e-5，动作块长度15。\n- **硬件平台**：Franka机械臂配移动底座，搭载顶部、右侧和手腕摄像头（图3）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Franka DROID平台，测试场景包括桌面（tabletop）和厨房水槽（sink），涉及新摄像头位置、光照和干扰物。\n- **评估任务**：8个任务涵盖未见对象（如pokeball、squeegee）、新颖动作（如推动lever、打开door）和新场景（如sink-idliplate）。任务细节如：\n - pokeball：拾取pokeball放入托盘。\n - squeegee：拖动刮刀清洁台面。\n - door：打开底部柜门。\n- **评估指标**：每个任务10次测试轨迹，随机化初始位置/方向。报告：\n - 完整任务成功率。\n - 中间检查点成功率（如抓取成功、移动成功）。\n - 结果以堆叠条形图展示（图4），包括不同方法对比。\n- **对比方法**：原始π₀-FAST-DROID、Retrieve and Play基线、Diffusion Policy基线，以及RICL-π₀的微调版本。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>宾夕法尼亚大学（University of Pennsylvania）和不列颠哥伦比亚大学（University of British Columbia）</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为RICL（Retraining for In-Context Learning）的方法，旨在向预训练的视觉-语言-动作（VLA）模型中注入上下文学习能力。传统VLA模型缺乏上下文学习（ICL）能力，而RICL通过微调少量机器人演示数据，使模型能够利用检索增强生成（RAG）和ICL适应新任务。用户只需提供10-20个目标任务演示，RICL即可检索相关上下文信息并提升任务性能，无需参数更新。实验应用于π₀-FAST VLA，在涉及未见对象、新颖动作和新场景的操作任务中验证了有效性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出RICL方法，首次实现向预训练VLA中注入ICL能力，无需从头训练。</li><li>允许用户通过少量演示（10-20个）教导VLA执行新任务，支持零参数更新适应。</li><li>结合RAG和ICL，提升模型在未见对象、新颖动作和新场景中的泛化能力。</li><li>实验证明RICL-π₀在8个任务上平均完整任务成功率达31.25%，远高于基线的2.5%。</li><li>支持进一步微调：在目标任务演示上微调RICL-VLA，可将平均成功率提升至61.67%。</li><li>开源代码和模型权重（RICL-π₀），提供首个机器人操作任务的简单ICL接口。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>RICL训练流程</strong>：后训练预训练VLA，输入序列包含查询图像/状态和检索自演示的多个图像、状态、动作。使用DINO-v2图像编码器和ℓ₂距离检索最相关的4组邻居数据，按距离排序（最近邻居置于上下文左侧）。</li><li><strong>动作插值层</strong>：预测动作通过加权插值结合检索动作a'和LLM输出，公式为：</li></ul>\n<p> ```</p>\n<p> πᵗʰᵉᵗᵃ_RICL-VLA = e^{-λd} * one-hot(a') + (1 - e^{-λd}) * σ(π_θ(retrieved, query))</p>\n<p> ```</p>\n<p> 其中d为查询图像与最近邻居的ℓ₂距离，λ=10，σ为Softmax函数。</p>\n<ul><li><strong>训练细节</strong>：仅微调LLM部分，冻结图像编码器；最小化查询动作块的交叉熵损失；使用CosineDecaySchedule学习率调度。</li><li><strong>部署与微调</strong>：部署时检索任务演示数据并执行ICL；微调阶段在相同演示上进行检索增强训练，进一步提升性能。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>训练数据</strong>：使用Franka DROID平台收集400个演示（20个拾取放置任务，每任务20个演示），任务如\"移动物体至左侧/右侧\"或\"拾取物体放入碗中\"。</li><li><strong>基础模型</strong>：π₀-FAST-DROID（基于π₀-FAST在DROID数据集微调）。</li><li><strong>检索数据</strong>：评估时每个目标任务收集20个演示（如pokeball、idliplate等），用于RAG和ICL。</li><li><strong>训练资源</strong>：两个NVIDIA A100 GPU，批量大小16，训练3个epoch，峰值学习率2.5e-5，动作块长度15。</li><li><strong>硬件平台</strong>：Franka机械臂配移动底座，搭载顶部、右侧和手腕摄像头（图3）。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Franka DROID平台，测试场景包括桌面（tabletop）和厨房水槽（sink），涉及新摄像头位置、光照和干扰物。</li><li><strong>评估任务</strong>：8个任务涵盖未见对象（如pokeball、squeegee）、新颖动作（如推动lever、打开door）和新场景（如sink-idliplate）。任务细节如：</li></ul>\n<p> - pokeball：拾取pokeball放入托盘。</p>\n<p> - squeegee：拖动刮刀清洁台面。</p>\n<p> - door：打开底部柜门。</p>\n<ul><li><strong>评估指标</strong>：每个任务10次测试轨迹，随机化初始位置/方向。报告：</li></ul>\n<p> - 完整任务成功率。</p>\n<p> - 中间检查点成功率（如抓取成功、移动成功）。</p>\n<p> - 结果以堆叠条形图展示（图4），包括不同方法对比。</p>\n<ul><li><strong>对比方法</strong>：原始π₀-FAST-DROID、Retrieve and Play基线、Diffusion Policy基线，以及RICL-π₀的微调版本。</li></ul>"
  },
  {
    "date": "2025-07-31",
    "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation",
    "link": "http://arxiv.org/abs/2508.00097",
    "summary_markdown": "论文研究单位\nByteDance, PICO, San Jose, CA, USA\n佐治亚理工学院, Institute for Robotics and Intelligent Machines (IRIM), Atlanta, GA, USA\n乔治梅森大学, Computer Science, Fairfax, Virginia, USA\n\n论文概述\n该论文介绍了XRoboToolkit，一个基于OpenXR标准的跨平台框架，用于通过扩展现实（XR）设备进行机器人遥操作。该系统旨在解决当前遥操作方法在可扩展性、设置复杂性和数据质量方面的局限性，以满足大规模、高质量机器人演示数据集的需求，特别是用于训练视觉-语言-动作（VLA）模型。XRoboToolkit提供了低延迟的立体视觉反馈、基于优化的逆运动学求解器，并支持多种跟踪模态，如头戴设备、控制器、手部和辅助运动追踪器。\n\n论文核心贡献点\n1. 提出了一个基于OpenXR标准的跨平台遥操作框架XRoboToolkit，解决了XR设备与机器人控制器之间缺乏标准化数据格式的问题，实现了跨设备的无缝集成。\n2. 开发了一个低延迟的立体视觉反馈系统，集成了高效的通信协议和视频流管道，以最小化延迟并减少晕动症。\n3. 实现了一个基于二次规划（QP）的优化逆运动学（IK）求解器，能生成平滑可靠的机器人运动，特别是在运动学奇异点附近，并集成了灵巧手重定向功能。\n4. 设计了模块化架构，使框架能够轻松集成到不同的机器人平台（如精密机械臂、移动机器人、灵巧手）和仿真环境（如MuJoCo）中。\n5. 通过实际应用和训练VLA模型验证了框架的有效性，证明了其收集的数据质量足以用于微调出具有鲁棒自主性能的策略。\n\n论文方法描述\nXRoboToolkit系统主要由部署在XR头显上的Unity客户端和运行在PC上的C++服务组成。Unity客户端负责捕获头、手、控制器和全身等姿态跟踪数据，并呈现立体视觉界面；PC服务则负责接收数据并将其传递给机器人控制模块。数据流采用异步、回调驱动的架构，所有跟踪数据以90Hz的频率在单一JSON对象中传输。机器人控制模块包括：1) 基于PlaCo和Pinocchio库的QP求解器进行机械臂逆运动学求解，支持通过辅助运动追踪器添加约束；2) 通过优化方法将26个OpenXR手部关节点重定向到机器人灵巧手；3) 通过XR控制器摇杆控制移动基座的线速度和角速度。立体视觉支持PICO 4 Ultra头显和ZED Mini相机，并通过自定义着色器调整瞳距和焦点以增强深度感知。\n\n论文使用数据集和训练资源\n数据集：使用ARX R5双臂系统收集了100个双臂地毯折叠任务的演示数据。\n数据细节：数据以50Hz记录，每帧包含14维机器人关节状态、14维位置控制指令以及来自三个RealSense相机（D405i腕部相机和D435i顶置相机）的424x240 RGB图像。\n训练资源：在预训练的VLA模型pi_0上进行LoRA（Low-Rank Adaptation）微调。训练参数为：80,000步，批次大小为16，动作范围（action horizon）为50帧。\n\n论文使用的评估环境和评估指标\n评估环境1：视频流延迟对比。使用Kandao QooCam 3D相机和一个以100Hz频率刷新的LED面板进行测量。对比了XRoboToolkit（使用ZED Mini/PICO 4 Ultra与Quest 3/PICO 4 Ultra的组合）和Open-TeleVision系统的性能。所有设备连接在同一局域网，视频参数设置为1280x720分辨率、60FPS、1Mbps码率。\n评估指标1：视频流延迟的平均值和标准差。\n评估环境2：VLA模型性能验证。将收集的地毯折叠数据集用于微调pi_0模型，并在ARX R5双臂系统上部署训练好的策略进行30分钟的连续运行测试。\n评估指标2：任务成功率、平均任务完成时间，以及策略是否展现出自主重新抓取和重新定位等自适应行为。",
    "summary_html": "<p>论文研究单位</p>\n<p>ByteDance, PICO, San Jose, CA, USA</p>\n<p>佐治亚理工学院, Institute for Robotics and Intelligent Machines (IRIM), Atlanta, GA, USA</p>\n<p>乔治梅森大学, Computer Science, Fairfax, Virginia, USA</p>\n\n<p>论文概述</p>\n<p>该论文介绍了XRoboToolkit，一个基于OpenXR标准的跨平台框架，用于通过扩展现实（XR）设备进行机器人遥操作。该系统旨在解决当前遥操作方法在可扩展性、设置复杂性和数据质量方面的局限性，以满足大规模、高质量机器人演示数据集的需求，特别是用于训练视觉-语言-动作（VLA）模型。XRoboToolkit提供了低延迟的立体视觉反馈、基于优化的逆运动学求解器，并支持多种跟踪模态，如头戴设备、控制器、手部和辅助运动追踪器。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一个基于OpenXR标准的跨平台遥操作框架XRoboToolkit，解决了XR设备与机器人控制器之间缺乏标准化数据格式的问题，实现了跨设备的无缝集成。</li><li>开发了一个低延迟的立体视觉反馈系统，集成了高效的通信协议和视频流管道，以最小化延迟并减少晕动症。</li><li>实现了一个基于二次规划（QP）的优化逆运动学（IK）求解器，能生成平滑可靠的机器人运动，特别是在运动学奇异点附近，并集成了灵巧手重定向功能。</li><li>设计了模块化架构，使框架能够轻松集成到不同的机器人平台（如精密机械臂、移动机器人、灵巧手）和仿真环境（如MuJoCo）中。</li><li>通过实际应用和训练VLA模型验证了框架的有效性，证明了其收集的数据质量足以用于微调出具有鲁棒自主性能的策略。</li></ol>\n\n<p>论文方法描述</p>\n<p>XRoboToolkit系统主要由部署在XR头显上的Unity客户端和运行在PC上的C++服务组成。Unity客户端负责捕获头、手、控制器和全身等姿态跟踪数据，并呈现立体视觉界面；PC服务则负责接收数据并将其传递给机器人控制模块。数据流采用异步、回调驱动的架构，所有跟踪数据以90Hz的频率在单一JSON对象中传输。机器人控制模块包括：1) 基于PlaCo和Pinocchio库的QP求解器进行机械臂逆运动学求解，支持通过辅助运动追踪器添加约束；2) 通过优化方法将26个OpenXR手部关节点重定向到机器人灵巧手；3) 通过XR控制器摇杆控制移动基座的线速度和角速度。立体视觉支持PICO 4 Ultra头显和ZED Mini相机，并通过自定义着色器调整瞳距和焦点以增强深度感知。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：使用ARX R5双臂系统收集了100个双臂地毯折叠任务的演示数据。</p>\n<p>数据细节：数据以50Hz记录，每帧包含14维机器人关节状态、14维位置控制指令以及来自三个RealSense相机（D405i腕部相机和D435i顶置相机）的424x240 RGB图像。</p>\n<p>训练资源：在预训练的VLA模型pi_0上进行LoRA（Low-Rank Adaptation）微调。训练参数为：80,000步，批次大小为16，动作范围（action horizon）为50帧。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境1：视频流延迟对比。使用Kandao QooCam 3D相机和一个以100Hz频率刷新的LED面板进行测量。对比了XRoboToolkit（使用ZED Mini/PICO 4 Ultra与Quest 3/PICO 4 Ultra的组合）和Open-TeleVision系统的性能。所有设备连接在同一局域网，视频参数设置为1280x720分辨率、60FPS、1Mbps码率。</p>\n<p>评估指标1：视频流延迟的平均值和标准差。</p>\n<p>评估环境2：VLA模型性能验证。将收集的地毯折叠数据集用于微调pi_0模型，并在ARX R5双臂系统上部署训练好的策略进行30分钟的连续运行测试。</p>\n<p>评估指标2：任务成功率、平均任务完成时间，以及策略是否展现出自主重新抓取和重新定位等自适应行为。</p>"
  },
  {
    "date": "2025-07-31",
    "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2507.23682",
    "summary_markdown": "### 论文研究单位\n清华大学\n微软研究院\n### 论文概述\n本文提出了villa-X框架，一种视觉-语言-潜在动作模型，旨在提升机器人操作策略的泛化能力。该方法通过改进潜在动作的学习和融入视觉-语言-动作模型的预训练，实现了在仿真环境和真实机器人任务中的优越性能。\n### 论文核心贡献点\n1. 改进的潜在动作学习：引入本体感知前向动力学模型(proprioceptive FDM)，将潜在动作与物理动力学对齐，提升动作质量。\n2. 联合扩散建模：提出ACT模块，包含潜在动作专家(ACT-latent)和机器人动作专家(ACT-robot)，通过条件生成有效利用潜在动作。\n3. 零样本泛化能力：通过大规模预训练，模型能够泛化到未见过的机器人形态和开放词汇符号理解。\n### 论文方法描述\n1. 潜在动作模型(LAM)：\n - 逆向动力学模型(IDM)：从帧对预测潜在动作。\n - 视觉前向动力学模型(FDM)：重建未来观测。\n - 本体感知前向动力学模型(proprioceptive FDM)：预测未来机器人状态和动作，引入形态上下文来区分异构数据。\n - 联合优化图像重建损失、本体感知预测损失和向量量化承诺。\n\n2. 演员模块(ACT)：\n - 架构：包含VLM编码器、潜在动作专家(ACT-latent)和机器人动作专家(ACT-robot)。\n - 注意力掩码策略：训练中随机掩码机器人动作到潜在动作的注意力，增强鲁棒性。\n - 联合扩散建模：使用条件流匹配框架联合建模潜在动作和机器人动作序列。\n### 论文使用数据集和训练资源\n- 数据集：Bridge V2、Fractal、Something-Something V2、LIBERO、Xhand Dataset等混合数据集。\n- 训练资源：大规模混合数据预训练，特定机器人形态微调。具体硬件未明确提及，但涉及视觉-语言模型预训练和扩散模型训练。\n### 论文使用的评估环境和评估指标\n- 仿真环境：SIMPLER benchmark（包含Google机器人和WidowX机器人任务）。\n- 真实机器人：Realman RM 75机械臂（夹爪）和XArm + XHand灵巧手平台。\n- 评估指标：任务成功率（%），在视觉匹配设置下评估，包括平均成功率、零样本泛化测试等。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学</p>\n<p>微软研究院</p>\n<h3>论文概述</h3>\n<p>本文提出了villa-X框架，一种视觉-语言-潜在动作模型，旨在提升机器人操作策略的泛化能力。该方法通过改进潜在动作的学习和融入视觉-语言-动作模型的预训练，实现了在仿真环境和真实机器人任务中的优越性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>改进的潜在动作学习：引入本体感知前向动力学模型(proprioceptive FDM)，将潜在动作与物理动力学对齐，提升动作质量。</li><li>联合扩散建模：提出ACT模块，包含潜在动作专家(ACT-latent)和机器人动作专家(ACT-robot)，通过条件生成有效利用潜在动作。</li><li>零样本泛化能力：通过大规模预训练，模型能够泛化到未见过的机器人形态和开放词汇符号理解。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. 潜在动作模型(LAM)：</p>\n<p> - 逆向动力学模型(IDM)：从帧对预测潜在动作。</p>\n<p> - 视觉前向动力学模型(FDM)：重建未来观测。</p>\n<p> - 本体感知前向动力学模型(proprioceptive FDM)：预测未来机器人状态和动作，引入形态上下文来区分异构数据。</p>\n<p> - 联合优化图像重建损失、本体感知预测损失和向量量化承诺。</p>\n\n<p>2. 演员模块(ACT)：</p>\n<p> - 架构：包含VLM编码器、潜在动作专家(ACT-latent)和机器人动作专家(ACT-robot)。</p>\n<p> - 注意力掩码策略：训练中随机掩码机器人动作到潜在动作的注意力，增强鲁棒性。</p>\n<p> - 联合扩散建模：使用条件流匹配框架联合建模潜在动作和机器人动作序列。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：Bridge V2、Fractal、Something-Something V2、LIBERO、Xhand Dataset等混合数据集。</li><li>训练资源：大规模混合数据预训练，特定机器人形态微调。具体硬件未明确提及，但涉及视觉-语言模型预训练和扩散模型训练。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>仿真环境：SIMPLER benchmark（包含Google机器人和WidowX机器人任务）。</li><li>真实机器人：Realman RM 75机械臂（夹爪）和XArm + XHand灵巧手平台。</li><li>评估指标：任务成功率（%），在视觉匹配设置下评估，包括平均成功率、零样本泛化测试等。</li></ul>"
  },
  {
    "date": "2025-07-31",
    "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving",
    "link": "http://arxiv.org/abs/2507.23540",
    "summary_markdown": "## 论文研究单位\nTechnical University of Munich (TUM)\n## 论文概述\n论文提出一个统一的感知-语言-行动框架，以解决自动驾驶系统在复杂、开放世界环境中实现类似人类的适应性、鲁棒性和可解释性所面临的挑战。该框架集成了多传感器融合（相机、激光雷达、雷达）与由大型语言模型（LLM）增强的视觉-语言-行动（VLA）架构，将低级感官处理与高级上下文推理相结合，实现了上下文感知、可解释和有安全边界的自动驾驶。通过对一个包含施工区的城市交叉路口场景的评估，证明了该框架在轨迹跟踪、速度预测和自适应规划方面的优越性能。\n## 论文核心贡献点\n- 提出了一个统一的认知框架，将多模态感知与基于LLM的推理和运动规划紧密耦合。\n- 开发了一个鲁棒的多传感器语义融合模块，将激光雷达、雷达和相机数据融合成结构化的场景描述。\n- 通过融入LLM驱动的推理，提高了对施工区或不可预测行人行为等未知场景的泛化能力。\n- 通过在nuScenes数据集上对具有施工区的城市交叉路口案例研究，进行了实证验证，展示了框架的有效性和实时适应性。\n## 论文方法描述\n该框架包含三个主要层次：\n1. **感知层**：处理来自相机、激光雷达和雷达的原始数据。相机图像由GPT-4.1进行解释，激光雷达点云通过CNN进行3D目标检测，雷达数据通过欧几里得聚类进行目标划分。融合机制将激光雷达和雷达的输出集成为包含精确位置和速度信息的结构化文本文件。\n2. **语言层**：处理结构化文本文件和相机图像，利用增强的VLA推理核心（GPT-4.1）进行全面的场景风险分析和理解，生成精确的驾驶指令和轨迹可视化。\n3. **行动层**：接收来自语言层的驾驶指令和轨迹可视化，进行详细的轨迹规划，并在高保真数字孪生仿真中进行验证，最终输出对车辆运动的直接控制。\n## 论文使用数据集和训练资源\n- **数据集**：nuScenes数据集。\n- **关键模型/组件**：\n - GPT-4.1作为VLA推理核心。\n - PointPillars架构用于激光雷达点云处理。\n - 基于CNN的模型用于相机特征提取和目标检测。\n## 论文使用的评估环境和评估指标\n- **评估环境**：在一个具有施工区的城市交叉路口的跟车任务中进行案例研究。使用高保真数字孪生仿真进行轨迹的安全性和效率验证。\n- **评估指标**：\n - **速度和转向角预测**：平均绝对误差（MAE）和决定系数（R² Score）。\n - **轨迹准确性**：平均位移误差（ADE）和最终位移误差（FDE）。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>Technical University of Munich (TUM)</p>\n<h2>论文概述</h2>\n<p>论文提出一个统一的感知-语言-行动框架，以解决自动驾驶系统在复杂、开放世界环境中实现类似人类的适应性、鲁棒性和可解释性所面临的挑战。该框架集成了多传感器融合（相机、激光雷达、雷达）与由大型语言模型（LLM）增强的视觉-语言-行动（VLA）架构，将低级感官处理与高级上下文推理相结合，实现了上下文感知、可解释和有安全边界的自动驾驶。通过对一个包含施工区的城市交叉路口场景的评估，证明了该框架在轨迹跟踪、速度预测和自适应规划方面的优越性能。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了一个统一的认知框架，将多模态感知与基于LLM的推理和运动规划紧密耦合。</li><li>开发了一个鲁棒的多传感器语义融合模块，将激光雷达、雷达和相机数据融合成结构化的场景描述。</li><li>通过融入LLM驱动的推理，提高了对施工区或不可预测行人行为等未知场景的泛化能力。</li><li>通过在nuScenes数据集上对具有施工区的城市交叉路口案例研究，进行了实证验证，展示了框架的有效性和实时适应性。</li></ul>\n<h2>论文方法描述</h2>\n<p>该框架包含三个主要层次：</p>\n<ol><li><strong>感知层</strong>：处理来自相机、激光雷达和雷达的原始数据。相机图像由GPT-4.1进行解释，激光雷达点云通过CNN进行3D目标检测，雷达数据通过欧几里得聚类进行目标划分。融合机制将激光雷达和雷达的输出集成为包含精确位置和速度信息的结构化文本文件。</li><li><strong>语言层</strong>：处理结构化文本文件和相机图像，利用增强的VLA推理核心（GPT-4.1）进行全面的场景风险分析和理解，生成精确的驾驶指令和轨迹可视化。</li><li><strong>行动层</strong>：接收来自语言层的驾驶指令和轨迹可视化，进行详细的轨迹规划，并在高保真数字孪生仿真中进行验证，最终输出对车辆运动的直接控制。</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：nuScenes数据集。</li><li><strong>关键模型/组件</strong>：</li></ul>\n<p> - GPT-4.1作为VLA推理核心。</p>\n<p> - PointPillars架构用于激光雷达点云处理。</p>\n<p> - 基于CNN的模型用于相机特征提取和目标检测。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：在一个具有施工区的城市交叉路口的跟车任务中进行案例研究。使用高保真数字孪生仿真进行轨迹的安全性和效率验证。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>速度和转向角预测</strong>：平均绝对误差（MAE）和决定系数（R² Score）。</p>\n<p> - <strong>轨迹准确性</strong>：平均位移误差（ADE）和最终位移误差（FDE）。</p>"
  },
  {
    "date": "2025-07-31",
    "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning",
    "link": "http://arxiv.org/abs/2507.23318",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n中国的研究团队（由国家自然科学基金62476011资助）\n## 论文概述\nFastDriveVLA是一种针对端到端自动驾驶中视觉-语言-动作（VLA）模型的重建式视觉token剪枝框架。现有VLA模型由于大量视觉token导致计算成本高，而传统的基于注意力或相似度的剪枝方法在自动驾驶场景中表现不佳。受人类驾驶员主要关注前景区域的启发，本文提出了一种通过MAE风格像素重建来优先处理前景信息的方法，从而有效减少视觉token数量。\n## 论文核心贡献点\n1. 提出FastDriveVLA框架，一种新颖的基于重建的token剪枝方法，区别于现有的基于注意力和相似度的剪枝方法\n2. 设计ReconPruner，一个即插即用的剪枝器，通过MAE风格的像素重建进行训练\n3. 引入对抗性前景-背景重建策略，增强识别有价值token的能力\n4. 构建nuScenes-FG数据集，包含241K个带有前景分割标注的图像-掩码对\n5. 在nuScenes开环规划基准上，在不同剪枝比例下均达到SOTA性能\n## 论文方法描述\n1. **ReconPruner架构**：包含一个PrunerLayer（Qwen2.5-VL-3B的单个解码器层）和一个Scorer（单层前馈网络），总参数仅0.07B\n2. **训练策略**：\n - 使用可学习查询token捕获视觉token的前景显著性\n - 通过MAE风格像素重建训练，鼓励模型关注前景区域\n - 采用对抗性前景-背景重建策略，避免模型对所有token分配高显著性分数\n3. **推理过程**：根据目标剪枝比例，选择显著性分数最高的前K个token，保留其空间语义信息后输入大语言模型\n## 论文使用数据集和训练资源\n1. **数据集**：\n - nuScenes-FG：基于nuScenes构建的大规模数据集，包含241K个图像-掩码对，覆盖6个摄像头视角\n - 前景定义包括人、道路、车辆、交通标志（包括交通灯）和交通障碍物\n - 使用Grounded-SAM生成精细的前景分割标注\n2. **训练资源**：\n - 学习率：2e-5，使用余弦调度器\n - 训练周期：10个epoch\n - 硬件：两台H800 GPU\n - 训练时间：3小时\n## 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 数据集：nuScenes，包含1000个驾驶场景，每个约20秒\n - 测试样本：6019个\n - 基础模型：Impromptu-VLA（当前SOTA的端到端VLA模型）\n2. **评估指标**：\n - L2误差：轨迹预测的L2距离误差（1s、2s、3s和平均值）\n - 碰撞率：预测轨迹与物体的碰撞百分比\n - 交叉率：预测轨迹与道路边界的交叉百分比\n3. **性能表现**：\n - 在25%剪枝率下，甚至在L2和交叉指标上超过原始未剪枝模型（分别提升0.1%和1.0%）\n - 在50%剪枝率下达到最平衡的性能（保持99.1%的原始模型性能）\n - 实现7.5倍FLOPs减少，预填充和解码时间分别减少3.7倍和1.3倍",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>中国的研究团队（由国家自然科学基金62476011资助）</p>\n<h2>论文概述</h2>\n<p>FastDriveVLA是一种针对端到端自动驾驶中视觉-语言-动作（VLA）模型的重建式视觉token剪枝框架。现有VLA模型由于大量视觉token导致计算成本高，而传统的基于注意力或相似度的剪枝方法在自动驾驶场景中表现不佳。受人类驾驶员主要关注前景区域的启发，本文提出了一种通过MAE风格像素重建来优先处理前景信息的方法，从而有效减少视觉token数量。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出FastDriveVLA框架，一种新颖的基于重建的token剪枝方法，区别于现有的基于注意力和相似度的剪枝方法</li><li>设计ReconPruner，一个即插即用的剪枝器，通过MAE风格的像素重建进行训练</li><li>引入对抗性前景-背景重建策略，增强识别有价值token的能力</li><li>构建nuScenes-FG数据集，包含241K个带有前景分割标注的图像-掩码对</li><li>在nuScenes开环规划基准上，在不同剪枝比例下均达到SOTA性能</li></ol>\n<h2>论文方法描述</h2>\n<ol><li><strong>ReconPruner架构</strong>：包含一个PrunerLayer（Qwen2.5-VL-3B的单个解码器层）和一个Scorer（单层前馈网络），总参数仅0.07B</li><li><strong>训练策略</strong>：</li></ol>\n<p> - 使用可学习查询token捕获视觉token的前景显著性</p>\n<p> - 通过MAE风格像素重建训练，鼓励模型关注前景区域</p>\n<p> - 采用对抗性前景-背景重建策略，避免模型对所有token分配高显著性分数</p>\n<p>3. <strong>推理过程</strong>：根据目标剪枝比例，选择显著性分数最高的前K个token，保留其空间语义信息后输入大语言模型</p>\n<h2>论文使用数据集和训练资源</h2>\n<p>1. <strong>数据集</strong>：</p>\n<p> - nuScenes-FG：基于nuScenes构建的大规模数据集，包含241K个图像-掩码对，覆盖6个摄像头视角</p>\n<p> - 前景定义包括人、道路、车辆、交通标志（包括交通灯）和交通障碍物</p>\n<p> - 使用Grounded-SAM生成精细的前景分割标注</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - 学习率：2e-5，使用余弦调度器</p>\n<p> - 训练周期：10个epoch</p>\n<p> - 硬件：两台H800 GPU</p>\n<p> - 训练时间：3小时</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - 数据集：nuScenes，包含1000个驾驶场景，每个约20秒</p>\n<p> - 测试样本：6019个</p>\n<p> - 基础模型：Impromptu-VLA（当前SOTA的端到端VLA模型）</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - L2误差：轨迹预测的L2距离误差（1s、2s、3s和平均值）</p>\n<p> - 碰撞率：预测轨迹与物体的碰撞百分比</p>\n<p> - 交叉率：预测轨迹与道路边界的交叉百分比</p>\n<p>3. <strong>性能表现</strong>：</p>\n<p> - 在25%剪枝率下，甚至在L2和交叉指标上超过原始未剪枝模型（分别提升0.1%和1.0%）</p>\n<p> - 在50%剪枝率下达到最平衡的性能（保持99.1%的原始模型性能）</p>\n<p> - 实现7.5倍FLOPs减少，预填充和解码时间分别减少3.7倍和1.3倍</p>"
  },
  {
    "date": "2025-07-30",
    "title": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance",
    "link": "http://arxiv.org/abs/2507.22424",
    "summary_markdown": "论文研究单位\n- 清华大学\n- Infinigence AI\n- 澳门大学\n\n论文概述\n本文提出了Spec-VLA，这是首个专为视觉-语言-动作（VLA）模型设计的推测解码（SD）框架。由于VLA模型庞大的参数量和自回归解码特性，其推理速度受到严重限制。传统的推测解码虽然能加速大语言模型，但直接应用于VLA任务时效果甚微。为解决此问题，Spec-VLA引入了一种宽松接受机制，该机制利用VLA模型中动作标记的离散化特性（如分箱ID），通过计算草稿标记与验证模型标记之间的距离来放宽接受标准，从而在不牺牲任务成功率的前提下，显著提升推理速度。\n\n论文核心贡献点\n- 提出了首个应用于VLA模型的推测解码框架Spec-VLA，通过引入高效的草稿模型和并行验证机制来加速动作序列生成。\n- 指出直接将现有推测解码框架应用于VLA任务效果不佳的原因，即动作预测任务的复杂性以及VLA模型采用的贪婪解码策略。\n- 设计了一种新颖且几乎零计算开销的宽松接受机制，通过比较动作标记分箱ID的距离，允许接受与验证标记相近的草稿标记，有效提升了接受长度和解码速度。\n\n论文方法描述\nSpec-VLA框架包含两个核心部分：\n1. **推测解码架构**：采用一个轻量级的Llama解码层作为草稿模型，利用验证模型的隐藏状态以及视觉和文本嵌入来自回归地生成多个候选动作标记。随后，原始的VLA模型作为验证模型，对草稿序列进行并行验证。\n2. **宽松接受机制**：VLA模型（如OpenVLA）将连续的动作维度离散化为256个分箱。该方法将宽松接受定义为：如果草稿标记对应的分箱ID与验证模型预测的分箱ID之间的绝对距离小于或等于一个预设的阈值r，则接受该草稿标记。这一机制通过简单的ID比较实现，避免了复杂的相似度计算，从而在不显著增加计算成本的情况下，大幅提高了草稿标记的接受率。\n\n论文使用数据集和训练资源\n- **数据集**：LIBERO仿真基准，包括LIBERO-Goal, LIBERO-Object, LIBERO-Spatial和LIBERO-Long四个任务套件。\n- **训练资源**：草稿模型的训练在4块Tesla A100 (80G) GPU上完成，耗时约6小时，批次大小为16。\n\n论文使用的评估环境和评估指标\n- **评估环境**：在LIBERO仿真环境中进行评估。每个任务执行50次试验来测试策略性能。硬件环境为Tesla A100 (80G) GPU。\n- **评估指标**：\n - **成功率**：任务成功完成的百分比。\n - **加速比**：与标准自回归解码相比，生成速度的提升倍数。\n - **接受长度**：单次前向传递中平均预测的token数量，反映了推测解码的效率。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>清华大学</li><li>Infinigence AI</li><li>澳门大学</li></ul>\n\n<p>论文概述</p>\n<p>本文提出了Spec-VLA，这是首个专为视觉-语言-动作（VLA）模型设计的推测解码（SD）框架。由于VLA模型庞大的参数量和自回归解码特性，其推理速度受到严重限制。传统的推测解码虽然能加速大语言模型，但直接应用于VLA任务时效果甚微。为解决此问题，Spec-VLA引入了一种宽松接受机制，该机制利用VLA模型中动作标记的离散化特性（如分箱ID），通过计算草稿标记与验证模型标记之间的距离来放宽接受标准，从而在不牺牲任务成功率的前提下，显著提升推理速度。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了首个应用于VLA模型的推测解码框架Spec-VLA，通过引入高效的草稿模型和并行验证机制来加速动作序列生成。</li><li>指出直接将现有推测解码框架应用于VLA任务效果不佳的原因，即动作预测任务的复杂性以及VLA模型采用的贪婪解码策略。</li><li>设计了一种新颖且几乎零计算开销的宽松接受机制，通过比较动作标记分箱ID的距离，允许接受与验证标记相近的草稿标记，有效提升了接受长度和解码速度。</li></ul>\n\n<p>论文方法描述</p>\n<p>Spec-VLA框架包含两个核心部分：</p>\n<ol><li><strong>推测解码架构</strong>：采用一个轻量级的Llama解码层作为草稿模型，利用验证模型的隐藏状态以及视觉和文本嵌入来自回归地生成多个候选动作标记。随后，原始的VLA模型作为验证模型，对草稿序列进行并行验证。</li><li><strong>宽松接受机制</strong>：VLA模型（如OpenVLA）将连续的动作维度离散化为256个分箱。该方法将宽松接受定义为：如果草稿标记对应的分箱ID与验证模型预测的分箱ID之间的绝对距离小于或等于一个预设的阈值r，则接受该草稿标记。这一机制通过简单的ID比较实现，避免了复杂的相似度计算，从而在不显著增加计算成本的情况下，大幅提高了草稿标记的接受率。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>数据集</strong>：LIBERO仿真基准，包括LIBERO-Goal, LIBERO-Object, LIBERO-Spatial和LIBERO-Long四个任务套件。</li><li><strong>训练资源</strong>：草稿模型的训练在4块Tesla A100 (80G) GPU上完成，耗时约6小时，批次大小为16。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：在LIBERO仿真环境中进行评估。每个任务执行50次试验来测试策略性能。硬件环境为Tesla A100 (80G) GPU。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：任务成功完成的百分比。</p>\n<p> - <strong>加速比</strong>：与标准自回归解码相比，生成速度的提升倍数。</p>\n<p> - <strong>接受长度</strong>：单次前向传递中平均预测的token数量，反映了推测解码的效率。</p>"
  },
  {
    "date": "2025-07-23",
    "title": "InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation",
    "link": "http://arxiv.org/abs/2507.17520",
    "summary_markdown": "论文研究单位\nUniversity of Science and Technology of China, Zhejiang University, Shanghai Artificial Intelligence Laboratory\n\n论文概述\n本文提出了InstructVLA，一个端到端的视觉-语言-动作(VLA)模型，旨在解决现有VLA模型在保持多模态推理能力的同时进行精确动作生成的挑战。该方法通过视觉-语言-动作指令调优(VLA-IT)训练范式，结合混合专家适配框架，在650K样本的数据集上训练，实现了在机器人操纵任务中30.5%的性能提升，并在新提出的SimplerEnv-Instruct基准上超越基线模型92%。模型同时保持了强大的多模态理解能力，在多个标准基准上表现优异，并支持真实世界部署。\n\n论文核心贡献点\n- 提出InstructVLA模型和训练管道，强调语言能力在VLA中的重要性，有效保留预训练视觉-语言知识的同时将操纵作为指令跟随的组成部分\n- 设计实用的数据与评估管道，支持650K定制VLA-IT注释和手动策划的基准套件，能够评估VLA的指令泛化能力\n- 在机器人操纵任务、多模态基准和真实世界部署中取得领先性能，实现直观且可控的操纵\n\n论文方法描述\n- 模型架构：基于Eagle2-2B VLM，引入可学习的动作查询Q∈R^(N×D)提取任务相关潜在动作C，采用混合专家(MoE)设计，使用LoRA模块作为专家，通过缩放头预测门控系数λi，实现自适应融合文本推理与动作生成\n- 两阶段训练：第一阶段动作预训练使用异构操纵数据，训练模型预测动作和基于规则的语言动作描述，损失函数L=LM+FM；第二阶段视觉-语言-动作指令调优冻结动作专家，添加新语言LoRA适配器和MoE缩放头，在多模态数据集和VLA-IT语料库上联合训练\n- 推理优化：采用贪婪搜索直至第一个动作查询标记出现，其余动作查询在VLM单次前向传递中并行解码，通过缓存语言响应和潜在动作减少计算延迟\n\n论文使用数据集和训练资源\n- 数据集：650K样本的VLA-IT数据集，包含来自RT-1和Bridge数据集的人类-机器人交互数据，带有多样化指令、场景描述和问答对；SimplerEnv-Instruct基准包含80个零样本操纵任务，分为指令聚合(50个任务)和情境推理(30个任务)两个层次\n- 训练资源：VLM训练分辨率448×448，动作专家分辨率224×224；动作专家采用12层transformer主干，隐藏大小768；第一阶段训练650M参数，第二阶段训练220M参数；使用A100 GPU进行训练\n\n论文使用的评估环境和评估指标\n- 评估环境：多模态基准(MMMU Val、MMStar、MME、OCRBench、HallB Avg、MMB Dev En V1.1、TextVQA、DocVQA、InfoVQA、AI2D、ChartQA、RWQA)；SimplerEnv平台进行真实到模拟评估；SimplerEnv-Instruct基准；真实世界实验使用WidowX-250机械臂和Franka Research 3机器人\n- 评估指标：多模态理解准确率；机器人操纵任务成功率；指令跟随准确率；情境推理任务性能；零样本泛化能力；真实世界任务成功率；推理时性能提升比例",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Science and Technology of China, Zhejiang University, Shanghai Artificial Intelligence Laboratory</p>\n\n<p>论文概述</p>\n<p>本文提出了InstructVLA，一个端到端的视觉-语言-动作(VLA)模型，旨在解决现有VLA模型在保持多模态推理能力的同时进行精确动作生成的挑战。该方法通过视觉-语言-动作指令调优(VLA-IT)训练范式，结合混合专家适配框架，在650K样本的数据集上训练，实现了在机器人操纵任务中30.5%的性能提升，并在新提出的SimplerEnv-Instruct基准上超越基线模型92%。模型同时保持了强大的多模态理解能力，在多个标准基准上表现优异，并支持真实世界部署。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出InstructVLA模型和训练管道，强调语言能力在VLA中的重要性，有效保留预训练视觉-语言知识的同时将操纵作为指令跟随的组成部分</li><li>设计实用的数据与评估管道，支持650K定制VLA-IT注释和手动策划的基准套件，能够评估VLA的指令泛化能力</li><li>在机器人操纵任务、多模态基准和真实世界部署中取得领先性能，实现直观且可控的操纵</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>模型架构：基于Eagle2-2B VLM，引入可学习的动作查询Q∈R^(N×D)提取任务相关潜在动作C，采用混合专家(MoE)设计，使用LoRA模块作为专家，通过缩放头预测门控系数λi，实现自适应融合文本推理与动作生成</li><li>两阶段训练：第一阶段动作预训练使用异构操纵数据，训练模型预测动作和基于规则的语言动作描述，损失函数L=LM+FM；第二阶段视觉-语言-动作指令调优冻结动作专家，添加新语言LoRA适配器和MoE缩放头，在多模态数据集和VLA-IT语料库上联合训练</li><li>推理优化：采用贪婪搜索直至第一个动作查询标记出现，其余动作查询在VLM单次前向传递中并行解码，通过缓存语言响应和潜在动作减少计算延迟</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：650K样本的VLA-IT数据集，包含来自RT-1和Bridge数据集的人类-机器人交互数据，带有多样化指令、场景描述和问答对；SimplerEnv-Instruct基准包含80个零样本操纵任务，分为指令聚合(50个任务)和情境推理(30个任务)两个层次</li><li>训练资源：VLM训练分辨率448×448，动作专家分辨率224×224；动作专家采用12层transformer主干，隐藏大小768；第一阶段训练650M参数，第二阶段训练220M参数；使用A100 GPU进行训练</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：多模态基准(MMMU Val、MMStar、MME、OCRBench、HallB Avg、MMB Dev En V1.1、TextVQA、DocVQA、InfoVQA、AI2D、ChartQA、RWQA)；SimplerEnv平台进行真实到模拟评估；SimplerEnv-Instruct基准；真实世界实验使用WidowX-250机械臂和Franka Research 3机器人</li><li>评估指标：多模态理解准确率；机器人操纵任务成功率；指令跟随准确率；情境推理任务性能；零样本泛化能力；真实世界任务成功率；推理时性能提升比例</li></ul>"
  },
  {
    "date": "2025-07-23",
    "title": "ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents",
    "link": "http://arxiv.org/abs/2507.17462",
    "summary_markdown": "论文研究单位\n- 上海交通大学自动化与智能感知学院及系统控制与信息处理教育部重点实验室\n- 剑桥大学工程系\n\n论文概述\n论文提出ERMV（Editing 4D Robotic Multi-view images）框架，用于编辑4D机器人多视角序列图像，以缓解具身智能体（如VLA模型）训练中的数据稀缺问题。现有方法面临三大挑战：动态多视角下的时空一致性维护、计算成本限制导致的工作窗口狭小、以及关键物体（如机械臂）的语义完整性维护。ERMV通过视觉引导、状态注入、稀疏时空建模和反馈干预机制解决这些问题，显著提升下游策略的鲁棒性和泛化性。\n\n论文核心贡献点\n- 提出首个面向4D机器人多视角序列编辑的框架ERMV，有效增强VLA模型训练数据\n- 设计极线运动感知注意力（EMA-Attn）处理运动模糊，结合稀疏时空模块（SST）实现大窗口低计算编辑\n- 引入多模态大语言模型（MLLM）的反馈干预机制，最小化人工参与下保障核心物体一致性\n- 在仿真、真实数据集和物理机器人平台验证效果，并证明其缩小sim-to-real差距的能力\n\n论文方法描述\n- 基于潜在扩散模型（LDM），学习条件分布 p(X'\\|X,C_guide,C_state,C_history)\n- 视觉引导：使用单帧编辑图像通过CLIP编码作为精确语义目标，避免文本歧义\n- 状态注入：融合相机位姿P_t^(v)、机器人动作q_t及其运动差分ΔP_t^(v)、Δq_t，经MLP编码为时空条件\n- 稀疏时空模块：随机采样K张图像（K<<L×N），解耦时空维度，将4D编辑重构为单帧多视角问题\n- 极线运动感知注意力：在应用几何约束前预测运动引起的像素偏移，保持运动模糊下的跨视图一致性\n- 反馈干预：MLLM检测编辑不一致性，仅在必要时请求专家提供分割掩码修正\n\n论文使用数据集和训练资源\n- 仿真数据集：RoboTwin多视角操作基准\n- 真实数据集：RDT真实机器人数据集及自建双臂机器人平台数据\n- 训练资源：单个消费级GPU（通过稀疏采样降低内存需求）\n\n论文使用的评估环境和评估指标\n- 仿真环境：RoboTwin基准测试\n- 真实环境：RDT数据集评估及物理双臂机器人部署\n- 评估指标：\n - VLA模型任务成功率\n - 未知环境泛化性能\n - 编辑结果的时空一致性指标\n - sim-to-real差距缩小程度",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>上海交通大学自动化与智能感知学院及系统控制与信息处理教育部重点实验室</li><li>剑桥大学工程系</li></ul>\n\n<p>论文概述</p>\n<p>论文提出ERMV（Editing 4D Robotic Multi-view images）框架，用于编辑4D机器人多视角序列图像，以缓解具身智能体（如VLA模型）训练中的数据稀缺问题。现有方法面临三大挑战：动态多视角下的时空一致性维护、计算成本限制导致的工作窗口狭小、以及关键物体（如机械臂）的语义完整性维护。ERMV通过视觉引导、状态注入、稀疏时空建模和反馈干预机制解决这些问题，显著提升下游策略的鲁棒性和泛化性。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出首个面向4D机器人多视角序列编辑的框架ERMV，有效增强VLA模型训练数据</li><li>设计极线运动感知注意力（EMA-Attn）处理运动模糊，结合稀疏时空模块（SST）实现大窗口低计算编辑</li><li>引入多模态大语言模型（MLLM）的反馈干预机制，最小化人工参与下保障核心物体一致性</li><li>在仿真、真实数据集和物理机器人平台验证效果，并证明其缩小sim-to-real差距的能力</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>基于潜在扩散模型（LDM），学习条件分布 p(X'\\|X,C_guide,C_state,C_history)</li><li>视觉引导：使用单帧编辑图像通过CLIP编码作为精确语义目标，避免文本歧义</li><li>状态注入：融合相机位姿P_t^(v)、机器人动作q_t及其运动差分ΔP_t^(v)、Δq_t，经MLP编码为时空条件</li><li>稀疏时空模块：随机采样K张图像（K<<L×N），解耦时空维度，将4D编辑重构为单帧多视角问题</li><li>极线运动感知注意力：在应用几何约束前预测运动引起的像素偏移，保持运动模糊下的跨视图一致性</li><li>反馈干预：MLLM检测编辑不一致性，仅在必要时请求专家提供分割掩码修正</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>仿真数据集：RoboTwin多视角操作基准</li><li>真实数据集：RDT真实机器人数据集及自建双臂机器人平台数据</li><li>训练资源：单个消费级GPU（通过稀疏采样降低内存需求）</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>仿真环境：RoboTwin基准测试</li><li>真实环境：RDT数据集评估及物理双臂机器人部署</li><li>评估指标：</li></ul>\n<p> - VLA模型任务成功率</p>\n<p> - 未知环境泛化性能</p>\n<p> - 编辑结果的时空一致性指标</p>\n<p> - sim-to-real差距缩小程度</p>"
  },
  {
    "date": "2025-07-23",
    "title": "Confidence Calibration in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2507.17383",
    "summary_markdown": "- 论文研究单位\nColumbia University\n\n- 论文概述\n本文首次对视觉-语言-动作模型的置信度校准进行了系统性研究。通过广泛基准测试发现任务性能与校准误差呈负相关，即任务性能越好，校准误差越小。提出了提示集成方法，通过平均多个语义等价指令的置信度来改善校准效果。分析了任务时间范围内的置信度变化，发现任务进行一段时间后置信度最为可靠，这为风险感知干预提供了自然介入点。揭示了不同动作维度上存在系统性的过/欠置信问题，并提出动作级Platt缩放方法，通过独立重新校准每个动作维度来产生更可靠的置信度估计。\n\n- 论文核心贡献点\n任务成功率与校准的基准测试：在多个数据集和VLA变体上全面评估任务成功率与校准误差的关系，发现更好的任务性能与更好的校准相关联。\n提示集成：提出了一种轻量级、贝叶斯风格的方法，通过平均多个语义等效指令改述的VLA置信度来持续改善校准，平均将期望校准误差降低超过20%。\n任务时间内的校准分析：分析了任务时间范围内的校准，显示在取得一些进展后置信度通常最可靠，为风险感知干预提供了自然介入点。\n动作维度缩放：揭示了不同动作维度上存在差异化的误校准，并提出动作级Platt缩放方法，通过独立重新校准每个动作维度来产生更可靠的置信度估计。\n\n- 论文方法描述\n基线置信度估计：通过平均每个动作维度最大token概率来计算置信度，防止机器人自由度过多导致置信度不公平降低。\n提示集成：使用多个语义等价的指令改述，对每个指令的置信度进行平均，以提高置信度估计的可靠性。\n动作级缩放：针对每个动作维度独立应用Platt缩放，校正系统性的过/欠置信问题。\n\n- 论文使用数据集和训练资源\n使用了多个数据集进行评估，但未具体说明数据集名称。实验基于不同的VLA变体进行。\n\n- 论文使用的评估环境和评估指标\n评估指标包括：\n期望校准误差：测量置信度与准确度之间的期望差异，使用线性加权(ECE1)和平方加权(ECE2)两种形式。\nBrier分数：衡量概率预测质量，计算为预测概率与真实标签差值平方的平均值。\n负对数似然：评估概率预测的对数似然，值越小表示预测越准确。",
    "summary_html": "<ul><li>论文研究单位</li></ul>\n<p>Columbia University</p>\n\n<ul><li>论文概述</li></ul>\n<p>本文首次对视觉-语言-动作模型的置信度校准进行了系统性研究。通过广泛基准测试发现任务性能与校准误差呈负相关，即任务性能越好，校准误差越小。提出了提示集成方法，通过平均多个语义等价指令的置信度来改善校准效果。分析了任务时间范围内的置信度变化，发现任务进行一段时间后置信度最为可靠，这为风险感知干预提供了自然介入点。揭示了不同动作维度上存在系统性的过/欠置信问题，并提出动作级Platt缩放方法，通过独立重新校准每个动作维度来产生更可靠的置信度估计。</p>\n\n<ul><li>论文核心贡献点</li></ul>\n<p>任务成功率与校准的基准测试：在多个数据集和VLA变体上全面评估任务成功率与校准误差的关系，发现更好的任务性能与更好的校准相关联。</p>\n<p>提示集成：提出了一种轻量级、贝叶斯风格的方法，通过平均多个语义等效指令改述的VLA置信度来持续改善校准，平均将期望校准误差降低超过20%。</p>\n<p>任务时间内的校准分析：分析了任务时间范围内的校准，显示在取得一些进展后置信度通常最可靠，为风险感知干预提供了自然介入点。</p>\n<p>动作维度缩放：揭示了不同动作维度上存在差异化的误校准，并提出动作级Platt缩放方法，通过独立重新校准每个动作维度来产生更可靠的置信度估计。</p>\n\n<ul><li>论文方法描述</li></ul>\n<p>基线置信度估计：通过平均每个动作维度最大token概率来计算置信度，防止机器人自由度过多导致置信度不公平降低。</p>\n<p>提示集成：使用多个语义等价的指令改述，对每个指令的置信度进行平均，以提高置信度估计的可靠性。</p>\n<p>动作级缩放：针对每个动作维度独立应用Platt缩放，校正系统性的过/欠置信问题。</p>\n\n<ul><li>论文使用数据集和训练资源</li></ul>\n<p>使用了多个数据集进行评估，但未具体说明数据集名称。实验基于不同的VLA变体进行。</p>\n\n<ul><li>论文使用的评估环境和评估指标</li></ul>\n<p>评估指标包括：</p>\n<p>期望校准误差：测量置信度与准确度之间的期望差异，使用线性加权(ECE1)和平方加权(ECE2)两种形式。</p>\n<p>Brier分数：衡量概率预测质量，计算为预测概率与真实标签差值平方的平均值。</p>\n<p>负对数似然：评估概率预测的对数似然，值越小表示预测越准确。</p>"
  },
  {
    "date": "2025-07-23",
    "title": "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback",
    "link": "http://arxiv.org/abs/2507.17294",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-07-22",
    "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning",
    "link": "http://arxiv.org/abs/2507.16815",
    "summary_markdown": "论文研究单位\nNVIDIA 和 National Taiwan University\n\n论文概述\n该论文提出了ThinkAct，一个双系统框架，旨在通过强化视觉潜在规划来实现视觉-语言-行动推理。ThinkAct通过一个行动对齐的视觉奖励来引导多模态大语言模型生成具身推理计划，并将这些推理计划压缩成一个视觉计划潜在表示，用于指导下游行动模型在目标环境中执行鲁棒的动作。该框架在具身推理和机器人操作基准上实现了少样本适应、长视野规划和自我纠错能力。\n\n论文核心贡献点\n1. 提出了ThinkAct，一个将行动执行与视觉基础的具身推理通过视觉潜在规划相互连接的双系统框架。\n2. 利用目标完成和轨迹对齐的视觉反馈作为行动对齐的奖励，使长视野推理基于具身场景。\n3. 推进视觉潜在规划，通过提供跨多样化环境的推理增强轨迹指导来引导下游行动执行。\n4. 证明学习到的推理VLA能够在多样化的具身操作任务中实现少样本适应、长视野规划和自我纠错能力。\n\n论文方法描述\nThinkAct框架包含两个主要模块：一个多模态大语言模型（MLLM）用于高级推理，和一个行动模型用于底层动作执行。MLLM通过强化学习进行训练，使用一个基于视觉目标完成和轨迹分布匹配的行动对齐奖励函数。具体来说，它使用一个目标奖励，通过比较预测的起点和终点位置与检测到的轨迹点来鼓励目标完成；以及一个轨迹奖励，使用动态时间规整（DTW）距离来正则化预测轨迹与演示轨迹的分布对齐。奖励函数还结合了格式正确性分数。强化过程采用GRPO算法。推理步骤被压缩成一个紧凑的潜在轨迹，该轨迹编码了高级意图和规划上下文。行动模型是一个基于Transformer的扩散策略，它根据当前状态（视觉观察和语言指令）以及来自MLLM的潜在计划来预测动作。推理和执行可以异步操作，每个潜在计划对应N次与环境的交互。训练分为多阶段：MLLM先通过监督数据进行冷启动，然后通过强化学习进行微调；行动模型在Open X-Embodiment数据集上预训练，然后在目标环境中通过模仿学习进行微调，同时冻结MLLM。\n\n论文使用数据集和训练资源\n训练数据集：\n- 监督微调冷启动：Open X-Embodiment (OXE)的子集、RoboVQA、EgoPlan-IT、Video-R1-CoT。\n- 强化微调：OXE子集、Something-Something V2人类视频、以及具身QA数据集如RoboVQA、EgoPlan-IT/Val、Reflect (RoboFail)、LLaVA-Video-178K。\n- 行动模型训练：Open X-Embodiment (OXE)数据集。\n评估基准：\n- 机器人操作：SimplerEnv（包含Google-VM、Google-VA、Bridge-VM设置）和LIBERO（包含Spatial、Object、Goal、Long子任务）。\n- 具身推理：EgoPlan-Bench2（多选题，准确率）、RoboVQA（自由形式QA，BLEU分数）、OpenEQA（自由形式QA，LLM基础评分和BLEU分数）。\n训练资源：\n- 使用16张NVIDIA A100 GPU（80GB内存）进行所有实验。\n\n论文使用的评估环境和评估指标\n评估环境：\n- SimplerEnv：一个仿真基准，包含视觉匹配和变体聚合两种评估设置，提供在不同光照条件、桌面纹理、背景、物体干扰项和机器人相机姿态下的多样化操作场景。\n- LIBERO：一个机器人操作仿真基准，包含四个结构化任务套件，针对空间布局变化、物体多样性、目标变化和长视野规划。\n- EgoPlan-Bench2：评估在复杂真实世界场景中以自我为中心的规划能力，是一个包含1,321个高质量多选题QA对的非重叠评估集。\n- RoboVQA：专注于机器人操作中的视觉问答，其验证集包含1,893个视频-文本对。\n- OpenEQA：一个具身问答基准，用于评估代理通过自然语言理解和推理真实世界环境的能力。\n评估指标：\n- 机器人操作：任务成功率（Success Rate）。\n- 具身推理：\n - EgoPlan-Bench2：多选题准确率（Accuracy）。\n - RoboVQA：BLEU分数（BLEU-1/2/3/4）。\n - OpenEQA：LLM基础评分（LLM-based scoring）和BLEU分数。",
    "summary_html": "<p>论文研究单位</p>\n<p>NVIDIA 和 National Taiwan University</p>\n\n<p>论文概述</p>\n<p>该论文提出了ThinkAct，一个双系统框架，旨在通过强化视觉潜在规划来实现视觉-语言-行动推理。ThinkAct通过一个行动对齐的视觉奖励来引导多模态大语言模型生成具身推理计划，并将这些推理计划压缩成一个视觉计划潜在表示，用于指导下游行动模型在目标环境中执行鲁棒的动作。该框架在具身推理和机器人操作基准上实现了少样本适应、长视野规划和自我纠错能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了ThinkAct，一个将行动执行与视觉基础的具身推理通过视觉潜在规划相互连接的双系统框架。</li><li>利用目标完成和轨迹对齐的视觉反馈作为行动对齐的奖励，使长视野推理基于具身场景。</li><li>推进视觉潜在规划，通过提供跨多样化环境的推理增强轨迹指导来引导下游行动执行。</li><li>证明学习到的推理VLA能够在多样化的具身操作任务中实现少样本适应、长视野规划和自我纠错能力。</li></ol>\n\n<p>论文方法描述</p>\n<p>ThinkAct框架包含两个主要模块：一个多模态大语言模型（MLLM）用于高级推理，和一个行动模型用于底层动作执行。MLLM通过强化学习进行训练，使用一个基于视觉目标完成和轨迹分布匹配的行动对齐奖励函数。具体来说，它使用一个目标奖励，通过比较预测的起点和终点位置与检测到的轨迹点来鼓励目标完成；以及一个轨迹奖励，使用动态时间规整（DTW）距离来正则化预测轨迹与演示轨迹的分布对齐。奖励函数还结合了格式正确性分数。强化过程采用GRPO算法。推理步骤被压缩成一个紧凑的潜在轨迹，该轨迹编码了高级意图和规划上下文。行动模型是一个基于Transformer的扩散策略，它根据当前状态（视觉观察和语言指令）以及来自MLLM的潜在计划来预测动作。推理和执行可以异步操作，每个潜在计划对应N次与环境的交互。训练分为多阶段：MLLM先通过监督数据进行冷启动，然后通过强化学习进行微调；行动模型在Open X-Embodiment数据集上预训练，然后在目标环境中通过模仿学习进行微调，同时冻结MLLM。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>训练数据集：</p>\n<ul><li>监督微调冷启动：Open X-Embodiment (OXE)的子集、RoboVQA、EgoPlan-IT、Video-R1-CoT。</li><li>强化微调：OXE子集、Something-Something V2人类视频、以及具身QA数据集如RoboVQA、EgoPlan-IT/Val、Reflect (RoboFail)、LLaVA-Video-178K。</li><li>行动模型训练：Open X-Embodiment (OXE)数据集。</li></ul>\n<p>评估基准：</p>\n<ul><li>机器人操作：SimplerEnv（包含Google-VM、Google-VA、Bridge-VM设置）和LIBERO（包含Spatial、Object、Goal、Long子任务）。</li><li>具身推理：EgoPlan-Bench2（多选题，准确率）、RoboVQA（自由形式QA，BLEU分数）、OpenEQA（自由形式QA，LLM基础评分和BLEU分数）。</li></ul>\n<p>训练资源：</p>\n<ul><li>使用16张NVIDIA A100 GPU（80GB内存）进行所有实验。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ul><li>SimplerEnv：一个仿真基准，包含视觉匹配和变体聚合两种评估设置，提供在不同光照条件、桌面纹理、背景、物体干扰项和机器人相机姿态下的多样化操作场景。</li><li>LIBERO：一个机器人操作仿真基准，包含四个结构化任务套件，针对空间布局变化、物体多样性、目标变化和长视野规划。</li><li>EgoPlan-Bench2：评估在复杂真实世界场景中以自我为中心的规划能力，是一个包含1,321个高质量多选题QA对的非重叠评估集。</li><li>RoboVQA：专注于机器人操作中的视觉问答，其验证集包含1,893个视频-文本对。</li><li>OpenEQA：一个具身问答基准，用于评估代理通过自然语言理解和推理真实世界环境的能力。</li></ul>\n<p>评估指标：</p>\n<ul><li>机器人操作：任务成功率（Success Rate）。</li><li>具身推理：</li></ul>\n<p> - EgoPlan-Bench2：多选题准确率（Accuracy）。</p>\n<p> - RoboVQA：BLEU分数（BLEU-1/2/3/4）。</p>\n<p> - OpenEQA：LLM基础评分（LLM-based scoring）和BLEU分数。</p>"
  },
  {
    "date": "2025-07-21",
    "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos",
    "link": "http://arxiv.org/abs/2507.15597",
    "summary_markdown": "```markdown\n### 论文研究单位\n北京大学、中国人民大学、BeingBeyond\n### 论文概述\n本文提出了Being-H0，一个基于大规模人类视频训练的灵巧视觉-语言-动作模型（VLA）。现有VLA在复杂操作任务中表现不佳，主要依赖于存在仿真到现实差距的合成数据或缺乏规模和多样性的远程操作演示。为解决这一数据瓶颈，论文提出利用人类手部作为“基础操作器”，利用网络数据中丰富的灵巧性和可扩展性。方法核心是“物理指令微调”，一种新的训练范式，结合了大规模VLA预训练、物理空间对齐和用于机器人任务的后训练自适应。此外，论文提出了一种部分级别的运动标记化方法，实现毫米级重建精度，以建模用于动作学习的精确手部轨迹。为了支持所提出的范式，论文进一步开发了一个全面的数据整理管道，将异构来源（包括动作捕捉、VR和仅RGB视频）集成到包含数百万个基于运动指令实例的大规模数据集中。\n### 论文核心贡献点\n1. **物理指令微调**：一种新的范式，将人类手部确立为机器人手部转移的基础操作器，弥合了人类视频与具身动作之间的差距。\n2. **部分级别运动标记化**：一种量化方法，在连续手部运动中保持毫米级精度，同时与自回归语言模型的离散架构兼容。\n3. **UniHand**：一个包含超过1.5亿个指令跟随样本的大规模数据集，跨越多样化操作场景，通过可扩展的数据管道统一动作捕捉、VR和仅RGB视频收集。\n4. **Being-H0**：通过整合上述创新，提出了第一个基于大规模人类视频中显式运动建模训练的灵巧VLA。该模型在视觉、语言和细粒度手部运动之间实现了强大的跨模态推理，并为下游机器人操作任务提供了定制的自适应策略。\n### 论文方法描述\n论文方法称为“物理指令微调”，包含三个关键组成部分：\n1. **预训练**：在人类视频上对基础VLA进行预训练，将人类手部视为理想操作器，机器人等价物为其灵巧性的简化版本。模型训练目标是根据视觉和语言预测手部运动。\n2. **物理空间对齐**：通过弱透视投影对齐和视图不变运动分布平衡来统一来自不同相机系统和记录条件的异构数据源，同时嵌入3D空间推理能力。\n3. **后训练自适应**：在预训练和物理空间对齐后，将基础VLA适应下游操作任务。本文采用简单的基于MLP的投影策略，并计划在未来探索更复杂的方法。\n\n模型采用统一的架构，通过共享注意力机制在视觉、语言和运动之间实现无缝跨模态推理。对于精确的运动标记化，引入了基于分组残差量化（GRQ）的有效部分级别运动标记化，实现毫米级重建精度。\n### 论文使用数据集和训练资源\n论文构建了名为UniHand的大规模数据集，包含超过1.5亿个样本，整合了动作捕捉、VR记录和仅RGB视频，跨越1000多个小时和150多个任务。数据整理管道包括手部姿势标准化、任务描述标注和指令数据生成。训练资源方面，论文使用InternVL3架构作为基础，包括预训练的InternViT-300M视觉编码器和2层MLP投影器。运动标记化使用1D-CNN编码器将连续MANO参数序列离散化为标记。\n### 论文使用的评估环境和评估指标\n评估在多个环境中进行：\n1. **手部运动生成评估**：包括手部运动生成和翻译任务，评估模型根据指令生成精确手部运动的能力。\n2. **长时程运动生成评估**：评估模型在长时间序列上生成连贯运动的能力。\n3. **灵巧操作评估**：在真实机器人操作任务上评估模型通过后训练自适应学习到的策略。\n评估指标包括：\n- 运动重建精度（毫米级误差）\n- 指令跟随准确性\n- 任务完成成功率\n- 跨模态推理能力\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>北京大学、中国人民大学、BeingBeyond</p>\n<h3>论文概述</h3>\n<p>本文提出了Being-H0，一个基于大规模人类视频训练的灵巧视觉-语言-动作模型（VLA）。现有VLA在复杂操作任务中表现不佳，主要依赖于存在仿真到现实差距的合成数据或缺乏规模和多样性的远程操作演示。为解决这一数据瓶颈，论文提出利用人类手部作为“基础操作器”，利用网络数据中丰富的灵巧性和可扩展性。方法核心是“物理指令微调”，一种新的训练范式，结合了大规模VLA预训练、物理空间对齐和用于机器人任务的后训练自适应。此外，论文提出了一种部分级别的运动标记化方法，实现毫米级重建精度，以建模用于动作学习的精确手部轨迹。为了支持所提出的范式，论文进一步开发了一个全面的数据整理管道，将异构来源（包括动作捕捉、VR和仅RGB视频）集成到包含数百万个基于运动指令实例的大规模数据集中。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>物理指令微调</strong>：一种新的范式，将人类手部确立为机器人手部转移的基础操作器，弥合了人类视频与具身动作之间的差距。</li><li><strong>部分级别运动标记化</strong>：一种量化方法，在连续手部运动中保持毫米级精度，同时与自回归语言模型的离散架构兼容。</li><li><strong>UniHand</strong>：一个包含超过1.5亿个指令跟随样本的大规模数据集，跨越多样化操作场景，通过可扩展的数据管道统一动作捕捉、VR和仅RGB视频收集。</li><li><strong>Being-H0</strong>：通过整合上述创新，提出了第一个基于大规模人类视频中显式运动建模训练的灵巧VLA。该模型在视觉、语言和细粒度手部运动之间实现了强大的跨模态推理，并为下游机器人操作任务提供了定制的自适应策略。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文方法称为“物理指令微调”，包含三个关键组成部分：</p>\n<ol><li><strong>预训练</strong>：在人类视频上对基础VLA进行预训练，将人类手部视为理想操作器，机器人等价物为其灵巧性的简化版本。模型训练目标是根据视觉和语言预测手部运动。</li><li><strong>物理空间对齐</strong>：通过弱透视投影对齐和视图不变运动分布平衡来统一来自不同相机系统和记录条件的异构数据源，同时嵌入3D空间推理能力。</li><li><strong>后训练自适应</strong>：在预训练和物理空间对齐后，将基础VLA适应下游操作任务。本文采用简单的基于MLP的投影策略，并计划在未来探索更复杂的方法。</li></ol>\n\n<p>模型采用统一的架构，通过共享注意力机制在视觉、语言和运动之间实现无缝跨模态推理。对于精确的运动标记化，引入了基于分组残差量化（GRQ）的有效部分级别运动标记化，实现毫米级重建精度。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>论文构建了名为UniHand的大规模数据集，包含超过1.5亿个样本，整合了动作捕捉、VR记录和仅RGB视频，跨越1000多个小时和150多个任务。数据整理管道包括手部姿势标准化、任务描述标注和指令数据生成。训练资源方面，论文使用InternVL3架构作为基础，包括预训练的InternViT-300M视觉编码器和2层MLP投影器。运动标记化使用1D-CNN编码器将连续MANO参数序列离散化为标记。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估在多个环境中进行：</p>\n<ol><li><strong>手部运动生成评估</strong>：包括手部运动生成和翻译任务，评估模型根据指令生成精确手部运动的能力。</li><li><strong>长时程运动生成评估</strong>：评估模型在长时间序列上生成连贯运动的能力。</li><li><strong>灵巧操作评估</strong>：在真实机器人操作任务上评估模型通过后训练自适应学习到的策略。</li></ol>\n<p>评估指标包括：</p>\n<ul><li>运动重建精度（毫米级误差）</li><li>指令跟随准确性</li><li>任务完成成功率</li><li>跨模态推理能力</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-07-21",
    "title": "GR-3 Technical Report",
    "link": "http://arxiv.org/abs/2507.15493",
    "summary_markdown": "```markdown\n# 论文研究单位\nByteDance Seed\n# 论文概述\n本论文介绍了GR-3，一个大规模的视觉-语言-动作模型。GR-3在泛化到新物体、环境和包含抽象概念的指令方面展现出卓越能力。此外，它可以通过最少的人类轨迹数据进行高效微调，从而实现快速且经济有效地适应新设置。GR-3还擅长处理长视野和需要双手操作与移动的灵巧任务，表现出强大而可靠的性能。这些能力通过多方面的训练方法实现，包括与网络规模的视觉-语言数据进行协同训练，通过VR设备收集的人类轨迹数据进行高效微调，以及使用机器人轨迹数据进行有效的模仿学习。论文还介绍了ByteMini，一个设计精良的双手移动机器人，具有出色的灵活性与可靠性。通过广泛的现实世界实验，研究表明GR-3在多种具有挑战性的任务上超越了最先进的基线方法π₀。\n# 论文核心贡献点\nGR-3的核心贡献体现在其卓越的能力上：\n1. 严格遵守语言指令，并能泛化到新物体、环境和涉及抽象概念的指令。\n2. 能够通过少量的人类轨迹数据高效学习，实现对新设置的快速、低成本适应。\n3. 能够以高鲁棒性和可靠性执行长视野和灵巧任务。\n# 论文方法描述\nGR-3模型采用混合Transformer架构，基于预训练的视觉-语言模型Qwen2.5-VL-3B-Instruct构建，并使用一个动作扩散Transformer来预测动作。具体而言，模型通过流程匹配进行动作预测。训练方法包含三个关键组成部分：\n1. 模仿学习：使用机器人轨迹数据，通过流程匹配损失监督动作预测。\n2. 协同训练：将机器人轨迹数据与大规模的视觉-语言数据混合训练，对VLM主干网络使用下一个词预测目标，对动作DiT使用流程匹配目标。\n3. 少样本泛化：通过VR设备收集的人类轨迹数据对模型进行高效微调，以适应新设置。\n为提升训练稳定性与语言遵循能力，在动作DiT的注意力及前馈网络的线性层后添加了额外的RMSNorm。\n# 论文使用数据集和训练资源\n论文使用了混合数据源进行训练：\n1. 机器人轨迹数据：为三个主要任务收集，包括泛化拾取与放置（35k轨迹，覆盖101个物体，总计69小时）、长视野桌面清理（约101小时）和灵巧布料操作（116小时）。\n2. 视觉-语言数据：一个策划的大规模数据集，融合自多个来源，涵盖图像描述、视觉问答、图像定位和交错定位图像描述等多种任务。\n3. 人类轨迹数据：使用PICO 4 Ultra Enterprise VR设备收集，用于少样本学习场景（例如，每个新物体10条轨迹）。\n模型包含40亿参数。具体计算资源未在文中提及。\n# 论文使用的评估环境和评估指标\n评估在现实世界中通过定制的ByteMini机器人进行，涉及三个具有挑战性的任务：\n1. 泛化拾取与放置：在四种设置下评估，包括基础、未见环境、未见指令和未见物体。评估指标为指令遵循率（IF Rate）和成功率。\n2. 长视野桌面清理：在平铺设置和指令跟随（IF）设置下评估，后者包含多种子任务（如基本、多物体、多目的地、新目的地、无效任务）。评估指标为平均任务进度和子任务成功率。\n3. 灵巧布料操作：在基础、位置和未见实例设置下评估。评估指标为平均任务进度，该进度基于完成关键里程碑（如拾取衣架、放置右肩、放置左肩、悬挂衬衫）的比率。\n```",
    "summary_html": "<p>```markdown</p>\n<h1>论文研究单位</h1>\n<p>ByteDance Seed</p>\n<h1>论文概述</h1>\n<p>本论文介绍了GR-3，一个大规模的视觉-语言-动作模型。GR-3在泛化到新物体、环境和包含抽象概念的指令方面展现出卓越能力。此外，它可以通过最少的人类轨迹数据进行高效微调，从而实现快速且经济有效地适应新设置。GR-3还擅长处理长视野和需要双手操作与移动的灵巧任务，表现出强大而可靠的性能。这些能力通过多方面的训练方法实现，包括与网络规模的视觉-语言数据进行协同训练，通过VR设备收集的人类轨迹数据进行高效微调，以及使用机器人轨迹数据进行有效的模仿学习。论文还介绍了ByteMini，一个设计精良的双手移动机器人，具有出色的灵活性与可靠性。通过广泛的现实世界实验，研究表明GR-3在多种具有挑战性的任务上超越了最先进的基线方法π₀。</p>\n<h1>论文核心贡献点</h1>\n<p>GR-3的核心贡献体现在其卓越的能力上：</p>\n<ol><li>严格遵守语言指令，并能泛化到新物体、环境和涉及抽象概念的指令。</li><li>能够通过少量的人类轨迹数据高效学习，实现对新设置的快速、低成本适应。</li><li>能够以高鲁棒性和可靠性执行长视野和灵巧任务。</li></ol>\n<h1>论文方法描述</h1>\n<p>GR-3模型采用混合Transformer架构，基于预训练的视觉-语言模型Qwen2.5-VL-3B-Instruct构建，并使用一个动作扩散Transformer来预测动作。具体而言，模型通过流程匹配进行动作预测。训练方法包含三个关键组成部分：</p>\n<ol><li>模仿学习：使用机器人轨迹数据，通过流程匹配损失监督动作预测。</li><li>协同训练：将机器人轨迹数据与大规模的视觉-语言数据混合训练，对VLM主干网络使用下一个词预测目标，对动作DiT使用流程匹配目标。</li><li>少样本泛化：通过VR设备收集的人类轨迹数据对模型进行高效微调，以适应新设置。</li></ol>\n<p>为提升训练稳定性与语言遵循能力，在动作DiT的注意力及前馈网络的线性层后添加了额外的RMSNorm。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>论文使用了混合数据源进行训练：</p>\n<ol><li>机器人轨迹数据：为三个主要任务收集，包括泛化拾取与放置（35k轨迹，覆盖101个物体，总计69小时）、长视野桌面清理（约101小时）和灵巧布料操作（116小时）。</li><li>视觉-语言数据：一个策划的大规模数据集，融合自多个来源，涵盖图像描述、视觉问答、图像定位和交错定位图像描述等多种任务。</li><li>人类轨迹数据：使用PICO 4 Ultra Enterprise VR设备收集，用于少样本学习场景（例如，每个新物体10条轨迹）。</li></ol>\n<p>模型包含40亿参数。具体计算资源未在文中提及。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>评估在现实世界中通过定制的ByteMini机器人进行，涉及三个具有挑战性的任务：</p>\n<ol><li>泛化拾取与放置：在四种设置下评估，包括基础、未见环境、未见指令和未见物体。评估指标为指令遵循率（IF Rate）和成功率。</li><li>长视野桌面清理：在平铺设置和指令跟随（IF）设置下评估，后者包含多种子任务（如基本、多物体、多目的地、新目的地、无效任务）。评估指标为平均任务进度和子任务成功率。</li><li>灵巧布料操作：在基础、位置和未见实例设置下评估。评估指标为平均任务进度，该进度基于完成关键里程碑（如拾取衣架、放置右肩、放置左肩、悬挂衬衫）的比率。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-07-18",
    "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
    "link": "http://arxiv.org/abs/2507.14067",
    "summary_markdown": "好的，我正在分析您提供的Arxiv论文HTML原文，并按照您的要求总结要点。\n\n论文研究单位\nThe Hong Kong University of Science and Technology (Guangzhou)\nThe Hong Kong University of Science and Technology\nUniversity of Toronto\nAnt Group, Alibaba\nNew York University Shanghai\n\n论文概述\n本文提出了一种名为VLA-Mark的跨模态水印框架，用于保护大型视觉-语言对齐模型（VLA）生成的知识产权。现有文本水印方法通过有偏见的词元选择和静态策略破坏视觉-文本对齐，使语义关键概念易受攻击。VLA-Mark通过跨模态协调嵌入可检测水印，同时保持语义保真度，集成多尺度视觉-文本对齐指标和熵敏感机制，优先在低不确定性生成阶段保护视觉基础内容。实验表明，相比传统方法，该方法在文本质量和检测性能上均有显著提升。\n\n论文核心贡献点\n1. 提出首个面向视觉-语言模型的文本水印方法，通过VLA架构的原生对齐机制实现跨模态语义引导，无需训练开销，文本质量（PPL↓和BLEU↑）平均提升7.4%和26.6%。\n2. 开发不确定性感知协调机制，根据logits熵自适应调整水印强度，在保持生成质量的同时达到SOTA检测性能，打破了保真度-检测的权衡。\n3. 通过专门的语义关键令牌（SCT）保护，建立分层防御体系抵御改写、同义词替换和翻译等攻击，确保扰动下的文本-视觉一致性。\n\n论文方法描述\nVLA-Mark框架利用跨模态嵌入对齐和多尺度语义显著性指标来指导水印注入。首先，提取视觉编码器的输出并通过投影层映射到文本嵌入空间；然后，计算语言词汇表中每个词元与视觉嵌入的局部区块亲和性（LPA）、全局语义一致性（GSC）和跨模态情境显著性（CCS）三种指标，经归一化和融合后形成层次化语义评估，生成优先级排序的词汇表。接着，根据模型输出的logits熵动态调整语义关键令牌（SCT）和绿色列表的比例：低熵时优先保留SCT以保语义保真度，高熵时强化水印强度。最后，通过提升SCT和绿色列表词元的logit值调整token分布，生成带有水印的文本，同时保护视觉相关概念免受文本空间攻击。\n\n论文使用数据集和训练资源\n实验使用AMBER数据集评估图像描述任务，并在MS COCO数据集上进行额外验证。模型包括LLaVA-v1.5、LLaVA-Next、Qwen2-VL和DeepSeek-VL四种SOTA多模态模型。所有方法在统一超参数下进行公平比较，响应长度限制为200个token，训练资源未指定，因方法无需模型重训练。\n\n论文使用的评估环境和评估指标\n评估环境基于Python实现，利用MarkLLM仓库进行基线方法集成。评估指标包括：检测性能（AUC和准确率）、文本质量（困惑度PPL和BLEU）、语义对齐（语义相似度STS和BertScore），以及对抗鲁棒性（A1攻击：词增删改；A2攻击：翻译和改写）。实验还包括对水印延迟、超参数敏感性和消融研究的分析。</think>\n论文研究单位\nThe Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, University of Toronto, Ant Group, Alibaba, New York University Shanghai\n\n论文概述\n本文提出VLA-Mark，一种针对大型视觉-语言对齐模型的跨模态水印框架。传统文本水印方法通过有偏见的词元选择和静态策略破坏视觉-文本对齐，使语义关键概念易受攻击。VLA-Mark通过跨模态协调嵌入可检测水印，同时保持语义保真度，集成多尺度视觉-文本对齐指标和熵敏感机制，优先在低不确定性生成阶段保护视觉基础内容，实验证明其在文本质量和检测性能上均优于传统方法。\n\n论文核心贡献点\n1. 提出首个面向视觉-语言模型的文本水印方法，利用VLA架构的原生对齐机制实现跨模态语义引导，无需训练开销，文本质量（PPL↓和BLEU↑）平均提升7.4%和26.6%。\n2. 开发不确定性感知协调机制，根据logits熵自适应调整水印强度，在保持SOTA检测性能的同时增强生成质量，打破保真度-检测的权衡。\n3. 通过专门的语义关键令牌（SCT）保护，建立分层防御体系抵御改写、同义词替换和翻译等攻击，确保扰动下的文本-视觉一致性。\n\n论文方法描述\nVLA-Mark框架利用跨模态嵌入对齐和多尺度语义显著性指标指导水印注入。首先，提取视觉编码器输出并通过投影层映射到文本嵌入空间；然后，计算语言词汇表各词元与视觉嵌入的局部区块亲和性（LPA）、全局语义一致性（GSC）和跨模态情境显著性（CCS）三种指标，经归一化和融合后形成层次化语义评估，生成优先级排序的词汇表。接着，根据模型输出的logits熵动态调整语义关键令牌（SCT）和绿色列表的比例：低熵时优先保留SCT以保语义保真度，高熵时强化水印强度。最后，通过提升SCT和绿色列表词元的logit值调整token分布，生成带水印文本，同时保护视觉相关概念免受文本空间攻击。\n\n论文使用数据集和训练资源\n实验使用AMBER数据集评估图像描述任务，并在MS COCO数据集上进行补充验证。测试模型包括LLaVA-v1.5、LLaVA-Next、Qwen2-VL和DeepSeek-VL四种多模态模型。所有方法在统一超参数下进行公平比较，响应长度限制为200个token，方法无需模型重训练。\n\n论文使用的评估环境和评估指标\n评估基于Python实现，集成MarkLLM仓库的基线方法。评估指标包括：检测性能（AUC和准确率）、文本质量（困惑度PPL和BLEU）、语义对齐（语义相似度STS和BertScore）及对抗鲁棒性（A1攻击：词增删改；A2攻击：翻译和改写）。实验还涵盖水印延迟、超参数敏感性和消融研究。",
    "summary_html": "<p>好的，我正在分析您提供的Arxiv论文HTML原文，并按照您的要求总结要点。</p>\n\n<p>论文研究单位</p>\n<p>The Hong Kong University of Science and Technology (Guangzhou)</p>\n<p>The Hong Kong University of Science and Technology</p>\n<p>University of Toronto</p>\n<p>Ant Group, Alibaba</p>\n<p>New York University Shanghai</p>\n\n<p>论文概述</p>\n<p>本文提出了一种名为VLA-Mark的跨模态水印框架，用于保护大型视觉-语言对齐模型（VLA）生成的知识产权。现有文本水印方法通过有偏见的词元选择和静态策略破坏视觉-文本对齐，使语义关键概念易受攻击。VLA-Mark通过跨模态协调嵌入可检测水印，同时保持语义保真度，集成多尺度视觉-文本对齐指标和熵敏感机制，优先在低不确定性生成阶段保护视觉基础内容。实验表明，相比传统方法，该方法在文本质量和检测性能上均有显著提升。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出首个面向视觉-语言模型的文本水印方法，通过VLA架构的原生对齐机制实现跨模态语义引导，无需训练开销，文本质量（PPL↓和BLEU↑）平均提升7.4%和26.6%。</li><li>开发不确定性感知协调机制，根据logits熵自适应调整水印强度，在保持生成质量的同时达到SOTA检测性能，打破了保真度-检测的权衡。</li><li>通过专门的语义关键令牌（SCT）保护，建立分层防御体系抵御改写、同义词替换和翻译等攻击，确保扰动下的文本-视觉一致性。</li></ol>\n\n<p>论文方法描述</p>\n<p>VLA-Mark框架利用跨模态嵌入对齐和多尺度语义显著性指标来指导水印注入。首先，提取视觉编码器的输出并通过投影层映射到文本嵌入空间；然后，计算语言词汇表中每个词元与视觉嵌入的局部区块亲和性（LPA）、全局语义一致性（GSC）和跨模态情境显著性（CCS）三种指标，经归一化和融合后形成层次化语义评估，生成优先级排序的词汇表。接着，根据模型输出的logits熵动态调整语义关键令牌（SCT）和绿色列表的比例：低熵时优先保留SCT以保语义保真度，高熵时强化水印强度。最后，通过提升SCT和绿色列表词元的logit值调整token分布，生成带有水印的文本，同时保护视觉相关概念免受文本空间攻击。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>实验使用AMBER数据集评估图像描述任务，并在MS COCO数据集上进行额外验证。模型包括LLaVA-v1.5、LLaVA-Next、Qwen2-VL和DeepSeek-VL四种SOTA多模态模型。所有方法在统一超参数下进行公平比较，响应长度限制为200个token，训练资源未指定，因方法无需模型重训练。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境基于Python实现，利用MarkLLM仓库进行基线方法集成。评估指标包括：检测性能（AUC和准确率）、文本质量（困惑度PPL和BLEU）、语义对齐（语义相似度STS和BertScore），以及对抗鲁棒性（A1攻击：词增删改；A2攻击：翻译和改写）。实验还包括对水印延迟、超参数敏感性和消融研究的分析。</think></p>\n<p>论文研究单位</p>\n<p>The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, University of Toronto, Ant Group, Alibaba, New York University Shanghai</p>\n\n<p>论文概述</p>\n<p>本文提出VLA-Mark，一种针对大型视觉-语言对齐模型的跨模态水印框架。传统文本水印方法通过有偏见的词元选择和静态策略破坏视觉-文本对齐，使语义关键概念易受攻击。VLA-Mark通过跨模态协调嵌入可检测水印，同时保持语义保真度，集成多尺度视觉-文本对齐指标和熵敏感机制，优先在低不确定性生成阶段保护视觉基础内容，实验证明其在文本质量和检测性能上均优于传统方法。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出首个面向视觉-语言模型的文本水印方法，利用VLA架构的原生对齐机制实现跨模态语义引导，无需训练开销，文本质量（PPL↓和BLEU↑）平均提升7.4%和26.6%。</li><li>开发不确定性感知协调机制，根据logits熵自适应调整水印强度，在保持SOTA检测性能的同时增强生成质量，打破保真度-检测的权衡。</li><li>通过专门的语义关键令牌（SCT）保护，建立分层防御体系抵御改写、同义词替换和翻译等攻击，确保扰动下的文本-视觉一致性。</li></ol>\n\n<p>论文方法描述</p>\n<p>VLA-Mark框架利用跨模态嵌入对齐和多尺度语义显著性指标指导水印注入。首先，提取视觉编码器输出并通过投影层映射到文本嵌入空间；然后，计算语言词汇表各词元与视觉嵌入的局部区块亲和性（LPA）、全局语义一致性（GSC）和跨模态情境显著性（CCS）三种指标，经归一化和融合后形成层次化语义评估，生成优先级排序的词汇表。接着，根据模型输出的logits熵动态调整语义关键令牌（SCT）和绿色列表的比例：低熵时优先保留SCT以保语义保真度，高熵时强化水印强度。最后，通过提升SCT和绿色列表词元的logit值调整token分布，生成带水印文本，同时保护视觉相关概念免受文本空间攻击。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>实验使用AMBER数据集评估图像描述任务，并在MS COCO数据集上进行补充验证。测试模型包括LLaVA-v1.5、LLaVA-Next、Qwen2-VL和DeepSeek-VL四种多模态模型。所有方法在统一超参数下进行公平比较，响应长度限制为200个token，方法无需模型重训练。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估基于Python实现，集成MarkLLM仓库的基线方法。评估指标包括：检测性能（AUC和准确率）、文本质量（困惑度PPL和BLEU）、语义对齐（语义相似度STS和BertScore）及对抗鲁棒性（A1攻击：词增删改；A2攻击：翻译和改写）。实验还涵盖水印延迟、超参数敏感性和消融研究。</p>"
  },
  {
    "date": "2025-07-18",
    "title": "EdgeVLA: Efficient Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2507.14049",
    "summary_markdown": "### 论文研究单位\nK-Scale Labs, McGill University\n### 论文概述\n该论文介绍了EdgeVLA (EVLA)，一种新颖的视觉-语言-动作（VLA）模型，旨在显著提高VLA模型的推理速度和效率，使其能在资源受限的边缘设备上实现实时性能。传统的大规模VLA模型如OpenVLA虽然性能强大，但在移动操控系统等计算资源有限的平台上部署困难。EVLA通过两个核心创新解决了这个问题：1) 取消了末端执行器位置预测的自回归要求，从而将推理速度提升了7倍；2) 利用小型语言模型（SLMs）的效率，以显著降低的计算需求实现了与更大模型可比的训练性能。论文的早期结果表明，EVLA在保持与OpenVLA相当训练特性的同时，在推理速度和内存效率方面获得了巨大提升。\n### 论文核心贡献点\n1. 通过消除末端执行器位置预测的自回归特性，实现了7倍的推理速度提升。\n2. 利用小型语言模型（SLMs）的效率，在模型大小显著减小（1B参数）的情况下，保持了与更大规模模型（如7.5B参数的OpenVLA）可比的训练性能。\n### 论文方法描述\nEdgeVLA的训练方法分为两个阶段：\n1. **阶段1：VLM预训练**: 模型基于一个视觉-语言模型（VLM）进行预训练，使用了来自多样化字幕数据集的120万个图文对，遵循了PrismaticVLM家族的模型配方。语言模型部分采用0.5B参数的Qwen2。视觉编码器采用两部分结构，结合了预训练的SigLIP和DinoV2模型。模型学习一个投影层，将视觉特征映射到语言模型的词元空间。\n2. **阶段2：用于末端执行器预测的联合控制**: 在此阶段，使用来自OpenX数据集的大约100万个机械臂操作示例对模型进行微调。该阶段的关键创新是摒弃了传统的自回归方式来预测末端执行器位置，转而采用“联合控制”的方式，即一次性预测整个末端执行器的位置。这是通过移除语言模型中的因果掩码并训练模型一次性输出所有位置坐标来实现的。\n### 论文使用数据集和训练资源\n- **数据集**:\n - 预训练数据：包含120万个图文对的多样化字幕数据集。\n - 机器人操作训练数据：OpenX数据集（约100万个样本）。\n - 评估数据集：BridgeData V2 和 OpenX。\n- **训练资源**:\n - BridgeData V2训练：使用单个节点的8块A100-80GB GPU。\n - OpenX训练：使用80块A100-40GB GPU，训练时间约为5天。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - 推理性能评估在A100-40GB GPU上进行。\n - 论文的目标是使模型能够在Jetson Nano等边缘设备上部署。\n- **评估指标**:\n - **效率指标**: 推理时间（毫秒）、内存使用量（GB）。\n - **训练性能指标**: 训练损失、动作词元准确率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>K-Scale Labs, McGill University</p>\n<h3>论文概述</h3>\n<p>该论文介绍了EdgeVLA (EVLA)，一种新颖的视觉-语言-动作（VLA）模型，旨在显著提高VLA模型的推理速度和效率，使其能在资源受限的边缘设备上实现实时性能。传统的大规模VLA模型如OpenVLA虽然性能强大，但在移动操控系统等计算资源有限的平台上部署困难。EVLA通过两个核心创新解决了这个问题：1) 取消了末端执行器位置预测的自回归要求，从而将推理速度提升了7倍；2) 利用小型语言模型（SLMs）的效率，以显著降低的计算需求实现了与更大模型可比的训练性能。论文的早期结果表明，EVLA在保持与OpenVLA相当训练特性的同时，在推理速度和内存效率方面获得了巨大提升。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>通过消除末端执行器位置预测的自回归特性，实现了7倍的推理速度提升。</li><li>利用小型语言模型（SLMs）的效率，在模型大小显著减小（1B参数）的情况下，保持了与更大规模模型（如7.5B参数的OpenVLA）可比的训练性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>EdgeVLA的训练方法分为两个阶段：</p>\n<ol><li><strong>阶段1：VLM预训练</strong>: 模型基于一个视觉-语言模型（VLM）进行预训练，使用了来自多样化字幕数据集的120万个图文对，遵循了PrismaticVLM家族的模型配方。语言模型部分采用0.5B参数的Qwen2。视觉编码器采用两部分结构，结合了预训练的SigLIP和DinoV2模型。模型学习一个投影层，将视觉特征映射到语言模型的词元空间。</li><li><strong>阶段2：用于末端执行器预测的联合控制</strong>: 在此阶段，使用来自OpenX数据集的大约100万个机械臂操作示例对模型进行微调。该阶段的关键创新是摒弃了传统的自回归方式来预测末端执行器位置，转而采用“联合控制”的方式，即一次性预测整个末端执行器的位置。这是通过移除语言模型中的因果掩码并训练模型一次性输出所有位置坐标来实现的。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> - 预训练数据：包含120万个图文对的多样化字幕数据集。</p>\n<p> - 机器人操作训练数据：OpenX数据集（约100万个样本）。</p>\n<p> - 评估数据集：BridgeData V2 和 OpenX。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - BridgeData V2训练：使用单个节点的8块A100-80GB GPU。</p>\n<p> - OpenX训练：使用80块A100-40GB GPU，训练时间约为5天。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - 推理性能评估在A100-40GB GPU上进行。</p>\n<p> - 论文的目标是使模型能够在Jetson Nano等边缘设备上部署。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>效率指标</strong>: 推理时间（毫秒）、内存使用量（GB）。</p>\n<p> - <strong>训练性能指标</strong>: 训练损失、动作词元准确率。</p>"
  },
  {
    "date": "2025-07-17",
    "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
    "link": "http://arxiv.org/abs/2507.12768",
    "summary_markdown": "### 论文研究单位\n清华大学计算机科学与技术系、人工智能研究院、清华-博世联合机器学习中心\n### 论文概述\n本文提出一种任务无关动作（task-agnostic action）范式，将动作执行与任务条件解耦，以提升可扩展性和效率。为此，设计了ATARA数据收集框架和AnyPos模型：ATARA通过脚本策略自动生成任务无关数据，收集速度比人工操作快30倍；AnyPos作为逆动力学模型，通过手臂解耦估计和方向感知解码器从任务无关数据学习动作预测。实验表明，该方法在动作预测准确率上提升51%，真实世界任务成功率提升30-40%。\n### 论文核心贡献点\n- 任务无关动作范式：解耦动作与特定任务，降低数据采集成本。\n- ATARA框架：自动化、可扩展的任务无关数据收集，无人工干预。\n- AnyPos模型：引入手臂解耦估计和方向感知解码器（DAD），提升学习效率和精度。\n- 两阶段框架：结合视频生成模型（语义先验）和逆动力学模型（物理控制），实现跨任务泛化。\n### 论文方法描述\n1. **任务无关动作数据**：\n - 定义为不依赖任务指令或奖励的轨迹，直接建模 \\(p(\\text{action} \\mid \\text{image})\\)。\n - 理论推导：任务特定动作 \\(p(a \\mid x, l)\\) 可分解为任务特定图像生成 \\(p(x \\mid x_0, l)\\) 与任务无关动作 \\(p(a \\mid x)\\) 的乘积（公式4）。\n2. **ATARA数据收集**：\n - 使用脚本策略 uniformly 探索双臂机器人的立方工作空间，生成610k图像-动作对。\n3. **AnyPos模型**：\n - **手臂解耦估计**：通过空间分割（flood-fill 和对称线）隔离单臂区域，减少假设空间，提升精度约20%。\n - **方向感知解码器（DAD）**：对齐视觉特征（DINOv2）与物理运动方向（关节角度、连杆方向），增强鲁棒性，再提升20%性能。\n### 论文使用数据集和训练资源\n- **训练数据**：ATARA生成的610k图像-动作对（双臂14维动作空间）。\n- **测试数据**：真实世界收集的2.5k图像-动作对（包含未见技能和物体）。\n- **训练资源**：未明确细节，但涉及大规模数据集和预训练视觉骨干（如DINOv2），需高性能计算资源。\n### 论文使用的评估环境和评估指标\n1. **动作预测评估**：\n - 环境：真实世界测试集（未见任务）。\n - 指标：动作预测准确率（AnyPos: 57.13%，比基线高51%）。\n2. **真实世界重放评估**：\n - 环境：物理机器人执行提升、抓取、点击等任务。\n - 指标：任务成功率（AnyPos-ATARA: 92.59%，比人工数据高33%）。\n3. **视频生成模型部署**：\n - 环境：结合扩散视频模型进行动作验证。\n - 指标：下游任务成功率（30-40%提升）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学计算机科学与技术系、人工智能研究院、清华-博世联合机器学习中心</p>\n<h3>论文概述</h3>\n<p>本文提出一种任务无关动作（task-agnostic action）范式，将动作执行与任务条件解耦，以提升可扩展性和效率。为此，设计了ATARA数据收集框架和AnyPos模型：ATARA通过脚本策略自动生成任务无关数据，收集速度比人工操作快30倍；AnyPos作为逆动力学模型，通过手臂解耦估计和方向感知解码器从任务无关数据学习动作预测。实验表明，该方法在动作预测准确率上提升51%，真实世界任务成功率提升30-40%。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>任务无关动作范式：解耦动作与特定任务，降低数据采集成本。</li><li>ATARA框架：自动化、可扩展的任务无关数据收集，无人工干预。</li><li>AnyPos模型：引入手臂解耦估计和方向感知解码器（DAD），提升学习效率和精度。</li><li>两阶段框架：结合视频生成模型（语义先验）和逆动力学模型（物理控制），实现跨任务泛化。</li></ul>\n<h3>论文方法描述</h3>\n<p>1. <strong>任务无关动作数据</strong>：</p>\n<p> - 定义为不依赖任务指令或奖励的轨迹，直接建模 \\(p(\\text{action} \\mid \\text{image})\\)。</p>\n<p> - 理论推导：任务特定动作 \\(p(a \\mid x, l)\\) 可分解为任务特定图像生成 \\(p(x \\mid x_0, l)\\) 与任务无关动作 \\(p(a \\mid x)\\) 的乘积（公式4）。</p>\n<p>2. <strong>ATARA数据收集</strong>：</p>\n<p> - 使用脚本策略 uniformly 探索双臂机器人的立方工作空间，生成610k图像-动作对。</p>\n<p>3. <strong>AnyPos模型</strong>：</p>\n<p> - <strong>手臂解耦估计</strong>：通过空间分割（flood-fill 和对称线）隔离单臂区域，减少假设空间，提升精度约20%。</p>\n<p> - <strong>方向感知解码器（DAD）</strong>：对齐视觉特征（DINOv2）与物理运动方向（关节角度、连杆方向），增强鲁棒性，再提升20%性能。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>训练数据</strong>：ATARA生成的610k图像-动作对（双臂14维动作空间）。</li><li><strong>测试数据</strong>：真实世界收集的2.5k图像-动作对（包含未见技能和物体）。</li><li><strong>训练资源</strong>：未明确细节，但涉及大规模数据集和预训练视觉骨干（如DINOv2），需高性能计算资源。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>动作预测评估</strong>：</p>\n<p> - 环境：真实世界测试集（未见任务）。</p>\n<p> - 指标：动作预测准确率（AnyPos: 57.13%，比基线高51%）。</p>\n<p>2. <strong>真实世界重放评估</strong>：</p>\n<p> - 环境：物理机器人执行提升、抓取、点击等任务。</p>\n<p> - 指标：任务成功率（AnyPos-ATARA: 92.59%，比人工数据高33%）。</p>\n<p>3. <strong>视频生成模型部署</strong>：</p>\n<p> - 环境：结合扩散视频模型进行动作验证。</p>\n<p> - 指标：下游任务成功率（30-40%提升）。</p>"
  },
  {
    "date": "2025-07-16",
    "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos",
    "link": "http://arxiv.org/abs/2507.12440",
    "summary_markdown": "### 论文研究单位\nUC San Diego, UIUC, MIT, NVIDIA\n### 论文概述\n论文提出了EgoVLA，一种从第一人称人类视频中学习视觉-语言-动作（VLA）模型的方法。该方法旨在通过大规模人类视频数据预训练模型，然后使用少量机器人演示进行微调，以解决机器人数据收集的规模和多样性瓶颈。论文还引入了一个名为Ego Humanoid Manipulation Benchmark的仿真基准测试，用于评估人形机器人操作策略。\n### 论文核心贡献点\n1. 提出了一种利用第一人称人类视频预训练VLA模型（EgoVLA）的新范式，减少了大规模机器人数据的依赖。\n2. 设计了统一的人类与机器人动作空间，通过逆运动学和动作重定向将人类动作转换为机器人动作，实现了跨具身迁移。\n3. 构建了大规模的第一人称人类操作数据集，整合了HOI4D、HOT3D、HoloAssist和TACO等多个数据源。\n4. 提出了Ego Humanoid Manipulation Benchmark，包含12个双手机器人操作任务，用于在仿真中评估策略的泛化能力。\n### 论文方法描述\n1. **数据集构建**：整合了多个第一人称人类视频数据集，包含RGB观测、手腕姿态、手部姿态和相机姿态，数据总量约50万图像-动作对。\n2. **模型架构**：基于NVILA-2B视觉语言模型构建EgoVLA，输入包括历史视觉观测、语言指令、动作查询令牌和人类本体感受状态，输出未来人类或机器人动作序列。\n3. **动作表示**：动作空间包括手腕姿态（相机坐标系下的3D平移和旋转）和手部关节角度（使用MANO手部模型的前15个PCA分量）。\n4. **训练过程**：先在人类数据集上预训练20个周期，然后在机器人演示数据上微调115个周期。\n5. **重定向方法**：通过优化MANO参数使预测的指尖位置与机器人手部指尖位置对齐，再通过轻量级MLP将预测的指尖位置映射为机器人关节命令。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - 人类数据：HOI4D（4000个视频）、HOT3D（833分钟视频）、HoloAssist（166小时视频，采样1/10）、TACO（2317个运动序列）。\n - 机器人数据：在Ego Humanoid Manipulation Benchmark中通过遥操作收集的100个任务演示。\n2. **训练资源**：未明确提及，但模型基于NVILA-2B（2B参数），使用AdamW优化器，预训练20个周期，微调115个周期。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 仿真环境：NVIDIA Isaac Lab中的Ego Humanoid Manipulation Benchmark，使用Unitree H1人形机器人和Inspire灵巧手。\n - 视觉配置：包括5种房间纹理和5种桌子纹理，共25种视觉背景组合，分为已见和未见两种设置。\n2. **评估指标**：\n - 成功率（SR）：任务整体成功的比例。\n - 进度率（PSR）：长时程任务中完成的子任务平均比例。\n - 人类动作预测误差：手腕平移误差（约8厘米）和2D图像平面归一化误差（约0.13）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>UC San Diego, UIUC, MIT, NVIDIA</p>\n<h3>论文概述</h3>\n<p>论文提出了EgoVLA，一种从第一人称人类视频中学习视觉-语言-动作（VLA）模型的方法。该方法旨在通过大规模人类视频数据预训练模型，然后使用少量机器人演示进行微调，以解决机器人数据收集的规模和多样性瓶颈。论文还引入了一个名为Ego Humanoid Manipulation Benchmark的仿真基准测试，用于评估人形机器人操作策略。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种利用第一人称人类视频预训练VLA模型（EgoVLA）的新范式，减少了大规模机器人数据的依赖。</li><li>设计了统一的人类与机器人动作空间，通过逆运动学和动作重定向将人类动作转换为机器人动作，实现了跨具身迁移。</li><li>构建了大规模的第一人称人类操作数据集，整合了HOI4D、HOT3D、HoloAssist和TACO等多个数据源。</li><li>提出了Ego Humanoid Manipulation Benchmark，包含12个双手机器人操作任务，用于在仿真中评估策略的泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据集构建</strong>：整合了多个第一人称人类视频数据集，包含RGB观测、手腕姿态、手部姿态和相机姿态，数据总量约50万图像-动作对。</li><li><strong>模型架构</strong>：基于NVILA-2B视觉语言模型构建EgoVLA，输入包括历史视觉观测、语言指令、动作查询令牌和人类本体感受状态，输出未来人类或机器人动作序列。</li><li><strong>动作表示</strong>：动作空间包括手腕姿态（相机坐标系下的3D平移和旋转）和手部关节角度（使用MANO手部模型的前15个PCA分量）。</li><li><strong>训练过程</strong>：先在人类数据集上预训练20个周期，然后在机器人演示数据上微调115个周期。</li><li><strong>重定向方法</strong>：通过优化MANO参数使预测的指尖位置与机器人手部指尖位置对齐，再通过轻量级MLP将预测的指尖位置映射为机器人关节命令。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：</p>\n<p> - 人类数据：HOI4D（4000个视频）、HOT3D（833分钟视频）、HoloAssist（166小时视频，采样1/10）、TACO（2317个运动序列）。</p>\n<p> - 机器人数据：在Ego Humanoid Manipulation Benchmark中通过遥操作收集的100个任务演示。</p>\n<p>2. <strong>训练资源</strong>：未明确提及，但模型基于NVILA-2B（2B参数），使用AdamW优化器，预训练20个周期，微调115个周期。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - 仿真环境：NVIDIA Isaac Lab中的Ego Humanoid Manipulation Benchmark，使用Unitree H1人形机器人和Inspire灵巧手。</p>\n<p> - 视觉配置：包括5种房间纹理和5种桌子纹理，共25种视觉背景组合，分为已见和未见两种设置。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - 成功率（SR）：任务整体成功的比例。</p>\n<p> - 进度率（PSR）：长时程任务中完成的子任务平均比例。</p>\n<p> - 人类动作预测误差：手腕平移误差（约8厘米）和2D图像平面归一化误差（约0.13）。</p>"
  },
  {
    "date": "2025-07-14",
    "title": "Vision Language Action Models in Robotic Manipulation: A Systematic Review",
    "link": "http://arxiv.org/abs/2507.10672",
    "summary_markdown": "### 论文研究单位\n- Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, United Arab Emirates.\n- Institute of Industrial and Control Engineering (IOC), Universitat Politecnica de Catalunya, Spain.\n### 论文概述\n本文是一篇关于机器人操作领域中视觉语言动作模型的系统性综述。它旨在全面整合视觉感知、自然语言理解和具身控制于一个统一的学习框架中。该综述分析了102个VLA模型、26个基础数据集和12个仿真平台，并提出了一个新颖的VLA数据集基准测试框架，该框架基于任务复杂性和模态丰富度。文章还探讨了VLA模型的应用、评估方法、当前面临的挑战以及未来的研究方向，为推进具身智能和通用机器人控制提供了技术参考和概念路线图。\n### 论文核心贡献点\n- 提出了一个结构化的VLA模型架构分类法，根据其整合感知、语言理解和控制的不同方法进行归类。\n- 引入了一个新颖的定量VLA数据集基准框架，使用任务复杂度和模态丰富度等指标，通过二维图表可视化当前数据格局，识别出研究空白。\n- 对关键的仿真平台进行了深入回顾，评估了它们在生成大规模数据、促进从仿真到现实的迁移以及支持任务多样性方面的作用。\n- 识别了VLA模型开发中存在的持续性挑战，并为未来研究提供了清晰的路线图，强调了模块化架构设计、可扩展数据生成策略和统一的语言基础API等方向。\n### 论文方法描述\n本文采用系统性文献综述的方法。首先，在IEEE Xplore、Elsevier、Springer Nature、MDPI、Wiley和arXiv等多个学术数据库中，针对VLA模型、数据集和仿真工具使用特定关键词组合进行全面检索。其次，通过对话式查询大型语言模型（如GPT）来补充文献库。然后，根据明确的纳入标准（如提出或评估了新颖的VLA模型、数据集或仿真器）对检索到的文献进行筛选和验证。最终，对入选的文献进行深入分析、分类和总结，以构建一个全面的VLA领域知识图谱。\n### 论文使用数据集和训练资源\n- 论文分析了多个关键数据集，包括：\n - Open X-Embodiment: 统一了来自22种机器人形态和超过500个任务的数据。\n - DROID: 结合了人类标注语言与机器人视频演示的大规模数据集。\n - 其他如RT-1-Kitchen, VIMA, RLBench等。\n- 论文回顾了多个仿真工具，包括：\n - Habitat, Isaac Gym, RoboSuite, iGibson, AI2-THOR。\n- 作者创建了一个公共代码库，用于总结VLA模型、数据集和仿真器，链接为: https://github.com/Muhayyuddin/VLAs。\n### 论文使用的评估环境和评估指标\n- 本文作为综述论文，未在单一物理或仿真环境中进行具体实验。其核心是提出和分析了现有的评估协议。\n- **数据集评估指标**: 提出了一个基于任务复杂性和模态丰富度的二维定量评估框架，用于系统地比较和基准化VLA训练数据集，以揭示当前数据集中任务多样性和多模态对齐的不足。\n- **模型评估分析**: 讨论了评估VLA模型的标准和方法，包括在真实世界的成功率、零样本泛化能力、以及在不同应用领域（如家居、工业、导航）的性能表现，并指出了当前评估协议中的局限性。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>Khalifa University Center for Autonomous Robotic Systems (KUCARS), Khalifa University, United Arab Emirates.</li><li>Institute of Industrial and Control Engineering (IOC), Universitat Politecnica de Catalunya, Spain.</li></ul>\n<h3>论文概述</h3>\n<p>本文是一篇关于机器人操作领域中视觉语言动作模型的系统性综述。它旨在全面整合视觉感知、自然语言理解和具身控制于一个统一的学习框架中。该综述分析了102个VLA模型、26个基础数据集和12个仿真平台，并提出了一个新颖的VLA数据集基准测试框架，该框架基于任务复杂性和模态丰富度。文章还探讨了VLA模型的应用、评估方法、当前面临的挑战以及未来的研究方向，为推进具身智能和通用机器人控制提供了技术参考和概念路线图。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个结构化的VLA模型架构分类法，根据其整合感知、语言理解和控制的不同方法进行归类。</li><li>引入了一个新颖的定量VLA数据集基准框架，使用任务复杂度和模态丰富度等指标，通过二维图表可视化当前数据格局，识别出研究空白。</li><li>对关键的仿真平台进行了深入回顾，评估了它们在生成大规模数据、促进从仿真到现实的迁移以及支持任务多样性方面的作用。</li><li>识别了VLA模型开发中存在的持续性挑战，并为未来研究提供了清晰的路线图，强调了模块化架构设计、可扩展数据生成策略和统一的语言基础API等方向。</li></ul>\n<h3>论文方法描述</h3>\n<p>本文采用系统性文献综述的方法。首先，在IEEE Xplore、Elsevier、Springer Nature、MDPI、Wiley和arXiv等多个学术数据库中，针对VLA模型、数据集和仿真工具使用特定关键词组合进行全面检索。其次，通过对话式查询大型语言模型（如GPT）来补充文献库。然后，根据明确的纳入标准（如提出或评估了新颖的VLA模型、数据集或仿真器）对检索到的文献进行筛选和验证。最终，对入选的文献进行深入分析、分类和总结，以构建一个全面的VLA领域知识图谱。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>论文分析了多个关键数据集，包括：</li></ul>\n<p> - Open X-Embodiment: 统一了来自22种机器人形态和超过500个任务的数据。</p>\n<p> - DROID: 结合了人类标注语言与机器人视频演示的大规模数据集。</p>\n<p> - 其他如RT-1-Kitchen, VIMA, RLBench等。</p>\n<ul><li>论文回顾了多个仿真工具，包括：</li></ul>\n<p> - Habitat, Isaac Gym, RoboSuite, iGibson, AI2-THOR。</p>\n<ul><li>作者创建了一个公共代码库，用于总结VLA模型、数据集和仿真器，链接为: https://github.com/Muhayyuddin/VLAs。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>本文作为综述论文，未在单一物理或仿真环境中进行具体实验。其核心是提出和分析了现有的评估协议。</li><li><strong>数据集评估指标</strong>: 提出了一个基于任务复杂性和模态丰富度的二维定量评估框架，用于系统地比较和基准化VLA训练数据集，以揭示当前数据集中任务多样性和多模态对齐的不足。</li><li><strong>模型评估分析</strong>: 讨论了评估VLA模型的标准和方法，包括在真实世界的成功率、零样本泛化能力、以及在不同应用领域（如家居、工业、导航）的性能表现，并指出了当前评估协议中的局限性。</li></ul>"
  },
  {
    "date": "2025-07-12",
    "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization",
    "link": "http://arxiv.org/abs/2507.09160",
    "summary_markdown": "根据您提供的HTML原文，该内容仅包含LaTeX宏包定义（如 \\usepackage 和 \\DeclareMathOperator）以及LaTeXML生成器的页脚信息，并未包含论文的任何实质内容，如摘要、引言、方法、实验结果等。\n\n因此，无法根据此片段提取所需信息并生成论文总结。请提供包含论文正文内容的HTML原文。",
    "summary_html": "<p>根据您提供的HTML原文，该内容仅包含LaTeX宏包定义（如 \\usepackage 和 \\DeclareMathOperator）以及LaTeXML生成器的页脚信息，并未包含论文的任何实质内容，如摘要、引言、方法、实验结果等。</p>\n\n<p>因此，无法根据此片段提取所需信息并生成论文总结。请提供包含论文正文内容的HTML原文。</p>"
  },
  {
    "date": "2025-07-07",
    "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting",
    "link": "http://arxiv.org/abs/2507.05116",
    "summary_markdown": "### 论文研究单位\n东北大学和EmbodyX公司。\n### 论文概述\n论文提出了VOTE（Vision-Language-Action Optimization with Trajectory Ensemble Voting）框架，旨在优化视觉-语言-动作（VLA）模型的训练和推理过程。该框架通过引入特殊令牌和集成投票策略，解决了现有VLA模型计算开销大和动作利用不足的问题，实现了更快的推理速度和更高的任务成功率，同时降低了训练成本。\n### 论文核心贡献点\n1. 提出轻量级VLA训练框架，通过单个特殊令牌替代多个动作令牌，显著减少生成令牌数量，降低训练和推理成本。\n2. 设计集成投票策略，结合当前和历史动作预测，通过投票机制选择最终动作，提高动作利用率和任务成功率。\n3. 实验表明，该方法在多个基准上达到最优性能，推理速度比OpenVLA快39倍，边缘设备吞吐量达46 Hz，具有实际部署潜力。\n### 论文方法描述\n训练框架：在指令序列末尾插入特殊令牌`<ACT>`，VLA模型仅生成该令牌而非多个动作令牌。其隐藏状态通过瓶颈式MLP动作头直接预测连续动作块，避免顺序解码。训练损失结合令牌级交叉熵和动作级L1损失，权重分别为0.01和0.99。\n集成投票：推理时，收集当前及过去K步的动作预测。计算各历史动作与当前动作的余弦相似度，按阈值0.5分为相似集`M`和不相似集`N`。选择票数更多的集合，取平均作为最终动作。若当前预测获票少则被忽略，优先历史一致预测。\n### 论文使用数据集和训练资源\n数据集：SimplerEnv（使用BridgeData V2和Fractal数据微调）和LIBERO基准。\n训练资源：SimplerEnv训练在4块H100 GPU上，全局批量大小80，微调60K步；LIBERO训练在2块H100 GPU上，批量大小40。优化器为AdamW，学习率1e-4，LoRA秩32，α=16。\n### 论文使用的评估环境和评估指标\n评估环境：NVIDIA RTX A6000和H100 GPU进行模拟评估；边缘设备NVIDIA Jetson Orin测试部署性能。\n评估指标：任务成功率（SR）、推理延迟、吞吐量及GPU内存占用。SimplerEnv中每个任务重复24次试验，LIBERO报告各任务套件平均SR。</think>\n### 论文研究单位\n东北大学和EmbodyX公司。\n### 论文概述\n论文提出了VOTE（Vision-Language-Action Optimization with Trajectory Ensemble Voting）框架，旨在优化视觉-语言-动作（VLA）模型的训练和推理过程。该框架通过引入特殊令牌和集成投票策略，解决了现有VLA模型计算开销大和动作利用不足的问题，实现了更快的推理速度和更高的任务成功率，同时降低了训练成本。\n### 论文核心贡献点\n1. 提出轻量级VLA训练框架，通过单个特殊令牌替代多个动作令牌，显著减少生成令牌数量，降低训练和推理成本。\n2. 设计集成投票策略，结合当前和历史动作预测，通过投票机制选择最终动作，提高动作利用率和任务成功率。\n3. 实验表明，该方法在多个基准上达到最优性能，推理速度比OpenVLA快39倍，边缘设备吞吐量达46 Hz，具有实际部署潜力。\n### 论文方法描述\n训练框架：在指令序列末尾插入特殊令牌`<ACT>`，VLA模型仅生成该令牌而非多个动作令牌。其隐藏状态通过瓶颈式MLP动作头直接预测连续动作块，避免顺序解码。训练损失结合令牌级交叉熵和动作级L1损失，权重分别为0.01和0.99。\n集成投票：推理时，收集当前及过去K步的动作预测。计算各历史动作与当前动作的余弦相似度，按阈值0.5分为相似集`M`和不相似集`N`。选择票数更多的集合，取平均作为最终动作。若当前预测获票少则被忽略，优先历史一致预测。\n### 论文使用数据集和训练资源\n数据集：SimplerEnv（使用BridgeData V2和Fractal数据微调）和LIBERO基准。\n训练资源：SimplerEnv训练在4块H100 GPU上，全局批量大小80，微调60K步；LIBERO训练在2块H100 GPU上，批量大小40。优化器为AdamW，学习率1e-4，LoRA秩32，α=16。\n### 论文使用的评估环境和评估指标\n评估环境：NVIDIA RTX A6000和H100 GPU进行模拟评估；边缘设备NVIDIA Jetson Orin测试部署性能。\n评估指标：任务成功率（SR）、推理延迟、吞吐量及GPU内存占用。SimplerEnv中每个任务重复24次试验，LIBERO报告各任务套件平均SR。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>东北大学和EmbodyX公司。</p>\n<h3>论文概述</h3>\n<p>论文提出了VOTE（Vision-Language-Action Optimization with Trajectory Ensemble Voting）框架，旨在优化视觉-语言-动作（VLA）模型的训练和推理过程。该框架通过引入特殊令牌和集成投票策略，解决了现有VLA模型计算开销大和动作利用不足的问题，实现了更快的推理速度和更高的任务成功率，同时降低了训练成本。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出轻量级VLA训练框架，通过单个特殊令牌替代多个动作令牌，显著减少生成令牌数量，降低训练和推理成本。</li><li>设计集成投票策略，结合当前和历史动作预测，通过投票机制选择最终动作，提高动作利用率和任务成功率。</li><li>实验表明，该方法在多个基准上达到最优性能，推理速度比OpenVLA快39倍，边缘设备吞吐量达46 Hz，具有实际部署潜力。</li></ol>\n<h3>论文方法描述</h3>\n<p>训练框架：在指令序列末尾插入特殊令牌<code><ACT></code>，VLA模型仅生成该令牌而非多个动作令牌。其隐藏状态通过瓶颈式MLP动作头直接预测连续动作块，避免顺序解码。训练损失结合令牌级交叉熵和动作级L1损失，权重分别为0.01和0.99。</p>\n<p>集成投票：推理时，收集当前及过去K步的动作预测。计算各历史动作与当前动作的余弦相似度，按阈值0.5分为相似集<code>M</code>和不相似集<code>N</code>。选择票数更多的集合，取平均作为最终动作。若当前预测获票少则被忽略，优先历史一致预测。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：SimplerEnv（使用BridgeData V2和Fractal数据微调）和LIBERO基准。</p>\n<p>训练资源：SimplerEnv训练在4块H100 GPU上，全局批量大小80，微调60K步；LIBERO训练在2块H100 GPU上，批量大小40。优化器为AdamW，学习率1e-4，LoRA秩32，α=16。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：NVIDIA RTX A6000和H100 GPU进行模拟评估；边缘设备NVIDIA Jetson Orin测试部署性能。</p>\n<p>评估指标：任务成功率（SR）、推理延迟、吞吐量及GPU内存占用。SimplerEnv中每个任务重复24次试验，LIBERO报告各任务套件平均SR。</think></p>\n<h3>论文研究单位</h3>\n<p>东北大学和EmbodyX公司。</p>\n<h3>论文概述</h3>\n<p>论文提出了VOTE（Vision-Language-Action Optimization with Trajectory Ensemble Voting）框架，旨在优化视觉-语言-动作（VLA）模型的训练和推理过程。该框架通过引入特殊令牌和集成投票策略，解决了现有VLA模型计算开销大和动作利用不足的问题，实现了更快的推理速度和更高的任务成功率，同时降低了训练成本。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出轻量级VLA训练框架，通过单个特殊令牌替代多个动作令牌，显著减少生成令牌数量，降低训练和推理成本。</li><li>设计集成投票策略，结合当前和历史动作预测，通过投票机制选择最终动作，提高动作利用率和任务成功率。</li><li>实验表明，该方法在多个基准上达到最优性能，推理速度比OpenVLA快39倍，边缘设备吞吐量达46 Hz，具有实际部署潜力。</li></ol>\n<h3>论文方法描述</h3>\n<p>训练框架：在指令序列末尾插入特殊令牌<code><ACT></code>，VLA模型仅生成该令牌而非多个动作令牌。其隐藏状态通过瓶颈式MLP动作头直接预测连续动作块，避免顺序解码。训练损失结合令牌级交叉熵和动作级L1损失，权重分别为0.01和0.99。</p>\n<p>集成投票：推理时，收集当前及过去K步的动作预测。计算各历史动作与当前动作的余弦相似度，按阈值0.5分为相似集<code>M</code>和不相似集<code>N</code>。选择票数更多的集合，取平均作为最终动作。若当前预测获票少则被忽略，优先历史一致预测。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：SimplerEnv（使用BridgeData V2和Fractal数据微调）和LIBERO基准。</p>\n<p>训练资源：SimplerEnv训练在4块H100 GPU上，全局批量大小80，微调60K步；LIBERO训练在2块H100 GPU上，批量大小40。优化器为AdamW，学习率1e-4，LoRA秩32，α=16。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：NVIDIA RTX A6000和H100 GPU进行模拟评估；边缘设备NVIDIA Jetson Orin测试部署性能。</p>\n<p>评估指标：任务成功率（SR）、推理延迟、吞吐量及GPU内存占用。SimplerEnv中每个任务重复24次试验，LIBERO报告各任务套件平均SR。</p>"
  },
  {
    "date": "2025-07-06",
    "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge",
    "link": "http://arxiv.org/abs/2507.04447",
    "summary_markdown": "论文研究单位\n上海交通大学(SJTU), EIT, 清华大学(THU), Galbot, 北京大学(PKU), 伊利诺伊大学厄巴纳-香槟分校(UIUC), 中国科学技术大学(USTC)\n\n论文概述\n论文提出了DreamVLA，一个新颖的视觉-语言-行动模型框架，旨在通过集成全面的世界知识预测来改善机器人操作。现有VLA模型受限于基于图像的预测，存在信息冗余和缺乏关键世界知识（动态、空间和语义信息）的问题。为解决此问题，DreamVLA将VLA模型重塑为一个感知-预测-行动模型，显式预测未来动态区域、深度和高级语义信息，为动作规划提供紧凑而全面的中间表示。为防止不同类型知识之间的信息干扰，模型采用了分块结构化注意力机制，并结合一个基于扩散的transformer进行动作序列的生成。在模拟和真实世界的实验表明，DreamVLA在CALVIN ABC-D基准上取得了4.44的平均任务长度，在真实机器人任务上实现了76.7%的成功率，显著优于先前的方法。\n\n论文核心贡献点\n1. 将视觉-语言-行动模型重塑为感知-预测-行动模型，使模型能够显式预测一组紧凑的动态、空间和高级语义信息，为动作规划提供简洁而全面的前瞻性线索。\n2. 引入了分块结构化注意力机制，配合一个扩散transformer解码器，以抑制来自跨类型知识泄漏的表示噪声，从而实现连贯的多步动作推理。\n3. DreamVLA在CALVIN ABC-D基准上创造了新的最先进纪录（4.44平均任务长度），在模拟平台上性能比先前方法提升高达3.5%，并将真实世界任务成功率提升至76.7%。消融研究证实了模型各组件的有效性。\n\n论文方法描述\nDreamVLA框架包含三个核心模块：多模态编码器、世界知识预测器和基于扩散的动作生成器。\n1. **多模态输入编码**：语言指令通过CLIP文本编码器，视觉帧通过Masked Autoencoder (MAE)编码为时空patch表示，机器人本体感觉信号通过卷积和全连接层处理。\n2. **世界知识预测**：模型使用一组可学习的`<dream>`查询，通过一个基于GPT-2的大型语言模型生成`world embedding`。该嵌入被预测为三个互补的未来知识：\n - **动态区域**：利用CoTracker识别并仅重建场景中运动的区域，避免整个帧的冗余重建。\n - **深度预测**：预测未来的单目深度图，提供空间三维结构信息。\n - **语义预测**：通过InfoNCE损失预测与DINOv2和SAM对齐的未来高级语义特征。\n3. **结构化注意力**：为保持不同类型知识的解耦，`<dream>`查询的三个子查询（动态、深度、语义）之间相互屏蔽注意力，仅关注共享的视觉、语言和状态token。\n4. **动作生成**：模型使用一个`<action>`查询从`world embedding`中聚合相关信息，形成一个动作嵌入。然后，一个去噪扩散transformer (DiT) 以该动作为条件，逐步将高斯噪声精炼为n步的动作序列，实现逆动力学建模。\n\n论文使用数据集和训练资源\n- **数据集**：\n - **CALVIN**：用于模拟环境评估的长时程、语言条件机器人操作基准。\n - **LIBERO**：包含四个子套件的模拟基准，用于评估空间推理、对象操作和目标导向任务。\n - **DROID**：大规模真实机器人轨迹数据集，用于真实世界任务的预训练。\n - **自采集数据集**：在真实世界中，为每个任务（抓取、放置、开关抽屉）采集了100个演示轨迹用于微调。\n- **训练资源**：\n - 使用PyTorch框架实现。\n - 在8块NVIDIA A800 GPU上进行训练。\n - 采用AdamW优化器，初始学习率为1e-3，权重衰减为1e-4，使用带5%线性预热的余弦学习率调度。\n - 批量大小设置为64。\n\n论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：\n - **CALVIN**：一个包含四个不同操作环境的模拟基准，提供多传感器数据（RGB-D相机、触觉图像、本体感觉读数）。\n - **LIBERO**：一个模拟基准，包含LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long四个任务套件。\n - **真实环境**：\n - **设置**：使用Franka Panda机械臂，配备一个第三人称视角和一个手腕端的RealSense D415 RGB相机。\n - **任务**：包括对不同物体（瓶子、玩偶等）的抓取和放置，以及抽屉的打开和关闭。\n- **评估指标**：\n - **CALVIN**：\n - **成功率**：每个任务在1000次测试回合中的成功率。\n - **平均长度**：连续成功完成5个指令的平均任务数。\n - **LIBERO**：\n - **成功率**：在每个任务套件上的任务成功百分比。\n - **真实世界**：\n - **成功率**：在限定尝试次数内（最多20次）成功完成任务的百分比。例如，抓取任务中成功抓取目标物体的比例，放置任务中成功抓取并放置物体的比例，以及抽屉任务中成功使其位移超过10厘米的比例。结果报告了各子任务及总任务的平均成功率。",
    "summary_html": "<p>论文研究单位</p>\n<p>上海交通大学(SJTU), EIT, 清华大学(THU), Galbot, 北京大学(PKU), 伊利诺伊大学厄巴纳-香槟分校(UIUC), 中国科学技术大学(USTC)</p>\n\n<p>论文概述</p>\n<p>论文提出了DreamVLA，一个新颖的视觉-语言-行动模型框架，旨在通过集成全面的世界知识预测来改善机器人操作。现有VLA模型受限于基于图像的预测，存在信息冗余和缺乏关键世界知识（动态、空间和语义信息）的问题。为解决此问题，DreamVLA将VLA模型重塑为一个感知-预测-行动模型，显式预测未来动态区域、深度和高级语义信息，为动作规划提供紧凑而全面的中间表示。为防止不同类型知识之间的信息干扰，模型采用了分块结构化注意力机制，并结合一个基于扩散的transformer进行动作序列的生成。在模拟和真实世界的实验表明，DreamVLA在CALVIN ABC-D基准上取得了4.44的平均任务长度，在真实机器人任务上实现了76.7%的成功率，显著优于先前的方法。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>将视觉-语言-行动模型重塑为感知-预测-行动模型，使模型能够显式预测一组紧凑的动态、空间和高级语义信息，为动作规划提供简洁而全面的前瞻性线索。</li><li>引入了分块结构化注意力机制，配合一个扩散transformer解码器，以抑制来自跨类型知识泄漏的表示噪声，从而实现连贯的多步动作推理。</li><li>DreamVLA在CALVIN ABC-D基准上创造了新的最先进纪录（4.44平均任务长度），在模拟平台上性能比先前方法提升高达3.5%，并将真实世界任务成功率提升至76.7%。消融研究证实了模型各组件的有效性。</li></ol>\n\n<p>论文方法描述</p>\n<p>DreamVLA框架包含三个核心模块：多模态编码器、世界知识预测器和基于扩散的动作生成器。</p>\n<ol><li><strong>多模态输入编码</strong>：语言指令通过CLIP文本编码器，视觉帧通过Masked Autoencoder (MAE)编码为时空patch表示，机器人本体感觉信号通过卷积和全连接层处理。</li><li><strong>世界知识预测</strong>：模型使用一组可学习的<code><dream></code>查询，通过一个基于GPT-2的大型语言模型生成<code>world embedding</code>。该嵌入被预测为三个互补的未来知识：</li></ol>\n<p> - <strong>动态区域</strong>：利用CoTracker识别并仅重建场景中运动的区域，避免整个帧的冗余重建。</p>\n<p> - <strong>深度预测</strong>：预测未来的单目深度图，提供空间三维结构信息。</p>\n<p> - <strong>语义预测</strong>：通过InfoNCE损失预测与DINOv2和SAM对齐的未来高级语义特征。</p>\n<ol><li><strong>结构化注意力</strong>：为保持不同类型知识的解耦，<code><dream></code>查询的三个子查询（动态、深度、语义）之间相互屏蔽注意力，仅关注共享的视觉、语言和状态token。</li><li><strong>动作生成</strong>：模型使用一个<code><action></code>查询从<code>world embedding</code>中聚合相关信息，形成一个动作嵌入。然后，一个去噪扩散transformer (DiT) 以该动作为条件，逐步将高斯噪声精炼为n步的动作序列，实现逆动力学建模。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>CALVIN</strong>：用于模拟环境评估的长时程、语言条件机器人操作基准。</p>\n<p> - <strong>LIBERO</strong>：包含四个子套件的模拟基准，用于评估空间推理、对象操作和目标导向任务。</p>\n<p> - <strong>DROID</strong>：大规模真实机器人轨迹数据集，用于真实世界任务的预训练。</p>\n<p> - <strong>自采集数据集</strong>：在真实世界中，为每个任务（抓取、放置、开关抽屉）采集了100个演示轨迹用于微调。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 使用PyTorch框架实现。</p>\n<p> - 在8块NVIDIA A800 GPU上进行训练。</p>\n<p> - 采用AdamW优化器，初始学习率为1e-3，权重衰减为1e-4，使用带5%线性预热的余弦学习率调度。</p>\n<p> - 批量大小设置为64。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：</p>\n<p> - <strong>CALVIN</strong>：一个包含四个不同操作环境的模拟基准，提供多传感器数据（RGB-D相机、触觉图像、本体感觉读数）。</p>\n<p> - <strong>LIBERO</strong>：一个模拟基准，包含LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long四个任务套件。</p>\n<p> - <strong>真实环境</strong>：</p>\n<p> - <strong>设置</strong>：使用Franka Panda机械臂，配备一个第三人称视角和一个手腕端的RealSense D415 RGB相机。</p>\n<p> - <strong>任务</strong>：包括对不同物体（瓶子、玩偶等）的抓取和放置，以及抽屉的打开和关闭。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>CALVIN</strong>：</p>\n<p> - <strong>成功率</strong>：每个任务在1000次测试回合中的成功率。</p>\n<p> - <strong>平均长度</strong>：连续成功完成5个指令的平均任务数。</p>\n<p> - <strong>LIBERO</strong>：</p>\n<p> - <strong>成功率</strong>：在每个任务套件上的任务成功百分比。</p>\n<p> - <strong>真实世界</strong>：</p>\n<p> - <strong>成功率</strong>：在限定尝试次数内（最多20次）成功完成任务的百分比。例如，抓取任务中成功抓取目标物体的比例，放置任务中成功抓取并放置物体的比例，以及抽屉任务中成功使其位移超过10厘米的比例。结果报告了各子任务及总任务的平均成功率。</p>"
  },
  {
    "date": "2025-07-03",
    "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale",
    "link": "http://arxiv.org/abs/2507.02747",
    "summary_markdown": "# 论文研究单位\nInstitution1, Institution2\n# 论文概述\n该论文介绍了一个大规模的灵巧视觉-语言-抓握模型和数据集，称为DexVLG。其核心目标是根据语言指令生成稳定且语义对齐的灵巧抓握姿态。该研究利用基于能量的优化方法来合成高质量的抓握，并创建了一个名为DexGraspNet 3.0的大规模、经过部分标注的数据集，用于训练和评估视觉-语言-抓握模型。\n# 论文核心贡献点\n- 提出了一种基于线性规划的、可微分的力封闭（LP-based DFC）方法，用于改进基于能量的抓握合成，使其能够考虑接触力的大小，从而产生更自然的抓握姿态。\n- 创建了DexGraspNet 3.0，这是一个包含174k个物体和170M个抓握姿态的大规模数据集。该数据集使用GPT-4o从Objaverse中筛选和注释，并对物体进行了语义分割，分为Ours-Wrap和Ours-Pinch两种抓握风格。\n- 开发了一种新颖的、与部分对齐的手部姿态初始化策略，该策略根据物体部分的几何形状（盖状、盘状、L形、杆状）对物体进行分类，并应用特定的启发式方法来对齐初始手部姿态，从而为优化过程注入了强大的、符合人类直觉的先验知识。\n- 构建了一个完整的、可扩展的视觉-语言-抓握（VLG）框架，研究表明，流匹配去噪方法优于传统的DDPM和DDIM范式，用于抓握姿态的生成。\n# 论文方法描述\n论文的方法包括三个主要阶段：数据集创建、手部姿态初始化和基于能量的抓握优化。\n1. **数据集创建**：从庞大的Objaverse数据集中，使用GPT-4o和一组五个标准（例如，是否为场景、颜色为单色、有地面平面）来过滤物体。然后使用SAMesh对物体进行语义分割，并通过结合来自六个正交视图的多视角图像，用GPT-4o为每个部分生成语义描述。最后，查询GPT-4o以获得合理的物体尺寸，并将物体的对角线长度重新缩放到20-50厘米的范围内。\n2. **手部姿态初始化**：根据方向、长度和部分间关系，将分割后的物体部分分为四种几何类型（盖状、盘状、L形、杆状）。对于每种类型，都设计了一个特定的策略来对齐手掌的初始位置和方向。例如，对于盖状部分，手掌会后退且与主轴方向垂直。这产生了两组初始化抓握：Ours-Wrap（用于环绕抓握）和Ours-Pinch（用于捏取抓握），每个部分各生成5000个姿态。\n3. **基于能量的抓握优化**：给定一个初始手部姿态，使用一个梯度下降过程来优化一个全面的能量函数。该函数由几个项组成：\n - **E_FC（LP-based DFC能量）**：一项用于确保力封闭稳定性的关键项。它首先求解一个线性规划问题，以找到在给定姿态下的最优接触力，然后如果姿态稳定，则使用这些力的大小来重新加权能量，以鼓励更自然的接触。\n - **E_pen（穿透能量）和E_spen（自穿透能量）**：使用CuRobo库来防止手与物体之间以及手自身之间的穿透。\n - **E_limit（关节限制能量）**：将手部关节角度约束在物理允许的范围内。\n - **E_dir（方向对齐能量）**：一个余弦相似性项，鼓励手-物接触点与手部网格的正面朝向对齐，以避免扭曲的抓握。\n 最终的总能量为这些项的加权和：E = w_fc * E_FC + w_pen * E_pen + w_spen * E_spen + w_limit * E_limit + w_dir * E_dir。\n# 论文使用数据集和训练资源\n- **数据集**：DexGraspNet 3.0，一个在Objaverse数据集基础上构建的新数据集。它包含174k个物体和170M个抓握姿态。这些数据被分成两个子集：Ours-Wrap（169k个物体，103M个抓握姿态）和Ours-Pinch（139k个物体，67M个抓握姿态）。\n- **训练资源**：论文中未具体说明硬件和训练时间。所使用的方法（GPT-4o过滤、SAMesh分割、流匹配去噪）暗示其依赖于标准的大规模深度学习基础设施（例如，GPU集群）。\n# 论文使用的评估环境和评估指标\n- **评估环境**：评估在一个仿真桌面环境中进行。物体被放置在桌子表面上，相机的位置被安排在距离桌子中心80厘米的圆周上，并以45度的俯角向下朝向原点看。\n- **评估指标**：评估使用以下指标：\n - **Q1**：一个用于量化力封闭质量的度量，与现有数据集相当。\n - **Pen（穿透深度）和SPen（自穿透深度）**：以毫米为单位测量的穿透深度，用于评估合成抓握的物理质量。\n - **Suc（成功率）**：一个任务导向的指标，用于衡量在语言指令下成功抓握目标物体部分的百分比。\n - **PGA（部分抓握能力）**：一个指标，用于评估生成的抓握在语义上与目标物体部分的对齐程度。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Institution1, Institution2</p>\n<h1>论文概述</h1>\n<p>该论文介绍了一个大规模的灵巧视觉-语言-抓握模型和数据集，称为DexVLG。其核心目标是根据语言指令生成稳定且语义对齐的灵巧抓握姿态。该研究利用基于能量的优化方法来合成高质量的抓握，并创建了一个名为DexGraspNet 3.0的大规模、经过部分标注的数据集，用于训练和评估视觉-语言-抓握模型。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出了一种基于线性规划的、可微分的力封闭（LP-based DFC）方法，用于改进基于能量的抓握合成，使其能够考虑接触力的大小，从而产生更自然的抓握姿态。</li><li>创建了DexGraspNet 3.0，这是一个包含174k个物体和170M个抓握姿态的大规模数据集。该数据集使用GPT-4o从Objaverse中筛选和注释，并对物体进行了语义分割，分为Ours-Wrap和Ours-Pinch两种抓握风格。</li><li>开发了一种新颖的、与部分对齐的手部姿态初始化策略，该策略根据物体部分的几何形状（盖状、盘状、L形、杆状）对物体进行分类，并应用特定的启发式方法来对齐初始手部姿态，从而为优化过程注入了强大的、符合人类直觉的先验知识。</li><li>构建了一个完整的、可扩展的视觉-语言-抓握（VLG）框架，研究表明，流匹配去噪方法优于传统的DDPM和DDIM范式，用于抓握姿态的生成。</li></ul>\n<h1>论文方法描述</h1>\n<p>论文的方法包括三个主要阶段：数据集创建、手部姿态初始化和基于能量的抓握优化。</p>\n<ol><li><strong>数据集创建</strong>：从庞大的Objaverse数据集中，使用GPT-4o和一组五个标准（例如，是否为场景、颜色为单色、有地面平面）来过滤物体。然后使用SAMesh对物体进行语义分割，并通过结合来自六个正交视图的多视角图像，用GPT-4o为每个部分生成语义描述。最后，查询GPT-4o以获得合理的物体尺寸，并将物体的对角线长度重新缩放到20-50厘米的范围内。</li><li><strong>手部姿态初始化</strong>：根据方向、长度和部分间关系，将分割后的物体部分分为四种几何类型（盖状、盘状、L形、杆状）。对于每种类型，都设计了一个特定的策略来对齐手掌的初始位置和方向。例如，对于盖状部分，手掌会后退且与主轴方向垂直。这产生了两组初始化抓握：Ours-Wrap（用于环绕抓握）和Ours-Pinch（用于捏取抓握），每个部分各生成5000个姿态。</li><li><strong>基于能量的抓握优化</strong>：给定一个初始手部姿态，使用一个梯度下降过程来优化一个全面的能量函数。该函数由几个项组成：</li></ol>\n<p> - <strong>E_FC（LP-based DFC能量）</strong>：一项用于确保力封闭稳定性的关键项。它首先求解一个线性规划问题，以找到在给定姿态下的最优接触力，然后如果姿态稳定，则使用这些力的大小来重新加权能量，以鼓励更自然的接触。</p>\n<p> - <strong>E_pen（穿透能量）和E_spen（自穿透能量）</strong>：使用CuRobo库来防止手与物体之间以及手自身之间的穿透。</p>\n<p> - <strong>E_limit（关节限制能量）</strong>：将手部关节角度约束在物理允许的范围内。</p>\n<p> - <strong>E_dir（方向对齐能量）</strong>：一个余弦相似性项，鼓励手-物接触点与手部网格的正面朝向对齐，以避免扭曲的抓握。</p>\n<p> 最终的总能量为这些项的加权和：E = w_fc * E_FC + w_pen * E_pen + w_spen * E_spen + w_limit * E_limit + w_dir * E_dir。</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>数据集</strong>：DexGraspNet 3.0，一个在Objaverse数据集基础上构建的新数据集。它包含174k个物体和170M个抓握姿态。这些数据被分成两个子集：Ours-Wrap（169k个物体，103M个抓握姿态）和Ours-Pinch（139k个物体，67M个抓握姿态）。</li><li><strong>训练资源</strong>：论文中未具体说明硬件和训练时间。所使用的方法（GPT-4o过滤、SAMesh分割、流匹配去噪）暗示其依赖于标准的大规模深度学习基础设施（例如，GPU集群）。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>评估环境</strong>：评估在一个仿真桌面环境中进行。物体被放置在桌子表面上，相机的位置被安排在距离桌子中心80厘米的圆周上，并以45度的俯角向下朝向原点看。</li><li><strong>评估指标</strong>：评估使用以下指标：</li></ul>\n<p> - <strong>Q1</strong>：一个用于量化力封闭质量的度量，与现有数据集相当。</p>\n<p> - <strong>Pen（穿透深度）和SPen（自穿透深度）</strong>：以毫米为单位测量的穿透深度，用于评估合成抓握的物理质量。</p>\n<p> - <strong>Suc（成功率）</strong>：一个任务导向的指标，用于衡量在语言指令下成功抓握目标物体部分的百分比。</p>\n<p> - <strong>PGA（部分抓握能力）</strong>：一个指标，用于评估生成的抓握在语义上与目标物体部分的对齐程度。</p>"
  },
  {
    "date": "2025-07-02",
    "title": "cVLA: Towards Efficient Camera-Space VLAs",
    "link": "http://arxiv.org/abs/2507.02190",
    "summary_markdown": "论文研究单位\nUniversity of Freiburg, Germany\n\n论文概述\n该论文提出了cVLA，一种高效的视觉-语言-动作（VLA）模型。与预测低级控制的现有VLA不同，cVLA直接在图像帧坐标中预测机器人末端执行器的轨迹关键位姿。这种方法使训练更高效且与机器人具体形态无关。模型基于PaliGemma架构，使用模拟数据训练，并展示了强大的模拟到现实迁移能力。论文还探索了深度图像输入、推理时策略（如新的beam-search-NMS解码算法）以及基于演示的单样本模仿学习的潜力。\n\n论文核心贡献点\n1. 提出了一个用于高效训练和评估VLA的设置，包含一个轻量级的单样本模仿学习系统。\n2. 对VLA的推理时预测策略进行了调研，并评估了其影响，包括提出一种新的解码算法beam-search-NMS。\n3. 计划公开发布代码、数据集和模型。\n\n论文方法描述\n方法基于对预训练的PaliGemma2模型进行微调，主要训练注意力层参数。模型将任务表述为单步预测两个末端执行器关键位姿（一个抓取位姿，一个释放位姿）。动作可以用机器人基座坐标或图像帧坐标（归一化的宽度、高度和相机距离）表示。模型通过将深度图转换为彩色图（如viridis色图）来处理深度信息。此外，方法还扩展到支持单样本模仿学习，此时模型根据提供的演示图像-轨迹对来推断任务，并将其应用于新的查询图像。\n\n论文使用数据集和训练资源\n训练数据集使用ManiSkill模拟器生成，包含多种3D资产，分为CLEVR（简单几何形状）和Objaverse（真实世界对象）两类，并有easy和hard两种难度级别以控制场景和相机随机化程度。评估使用了DROID真实世界数据集的两个子集（DROID-easy和DROID-hard）。模型微调自预训练的PaliGemma2，训练了4687次迭代（约15万个样本），学习率为3e-5，使用Adafactor优化器。\n\n论文使用的评估环境和评估指标\n评估在三种环境中进行：1) ManiSkill模拟环境，2) DROID数据集的离线评估，3) 配备Franka Panda机械臂和ZED2i相机的真实机器人系统。评估指标包括模拟成功率（SR）、结合位置与旋转误差的轨迹L1误差（以1厘米=1度归一化），以及用于评估轨迹预测分布的平均精度均值。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Freiburg, Germany</p>\n\n<p>论文概述</p>\n<p>该论文提出了cVLA，一种高效的视觉-语言-动作（VLA）模型。与预测低级控制的现有VLA不同，cVLA直接在图像帧坐标中预测机器人末端执行器的轨迹关键位姿。这种方法使训练更高效且与机器人具体形态无关。模型基于PaliGemma架构，使用模拟数据训练，并展示了强大的模拟到现实迁移能力。论文还探索了深度图像输入、推理时策略（如新的beam-search-NMS解码算法）以及基于演示的单样本模仿学习的潜力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一个用于高效训练和评估VLA的设置，包含一个轻量级的单样本模仿学习系统。</li><li>对VLA的推理时预测策略进行了调研，并评估了其影响，包括提出一种新的解码算法beam-search-NMS。</li><li>计划公开发布代码、数据集和模型。</li></ol>\n\n<p>论文方法描述</p>\n<p>方法基于对预训练的PaliGemma2模型进行微调，主要训练注意力层参数。模型将任务表述为单步预测两个末端执行器关键位姿（一个抓取位姿，一个释放位姿）。动作可以用机器人基座坐标或图像帧坐标（归一化的宽度、高度和相机距离）表示。模型通过将深度图转换为彩色图（如viridis色图）来处理深度信息。此外，方法还扩展到支持单样本模仿学习，此时模型根据提供的演示图像-轨迹对来推断任务，并将其应用于新的查询图像。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>训练数据集使用ManiSkill模拟器生成，包含多种3D资产，分为CLEVR（简单几何形状）和Objaverse（真实世界对象）两类，并有easy和hard两种难度级别以控制场景和相机随机化程度。评估使用了DROID真实世界数据集的两个子集（DROID-easy和DROID-hard）。模型微调自预训练的PaliGemma2，训练了4687次迭代（约15万个样本），学习率为3e-5，使用Adafactor优化器。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估在三种环境中进行：1) ManiSkill模拟环境，2) DROID数据集的离线评估，3) 配备Franka Panda机械臂和ZED2i相机的真实机器人系统。评估指标包括模拟成功率（SR）、结合位置与旋转误差的轨迹L1误差（以1厘米=1度归一化），以及用于评估轨迹预测分布的平均精度均值。</p>"
  },
  {
    "date": "2025-07-02",
    "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective",
    "link": "http://arxiv.org/abs/2507.01925",
    "summary_markdown": "论文研究单位\n北京大学人工智能研究院、PKU-PsiBot联合实验室\n\n论文概述\n这篇综述论文从动作标记化（action tokenization）的角度，系统性地回顾和分析了视觉-语言-动作（VLA）模型的研究进展。论文提出了一个统一的框架：视觉和语言输入通过一系列VLA模块处理，产生逐步编码更具体和可操作信息的动作标记链，最终生成可执行动作。作者将动作标记分为八类：语言描述、代码、可供性、轨迹、目标状态、潜在表示、原始动作和推理，并分析了各类标记的优势、局限性和未来方向。\n\n论文核心贡献点\n1. 提出了VLA模型的统一框架，将视觉和语言输入的处理抽象为通过VLA模块生成动作标记链的过程。\n2. 系统性地将动作标记分类为八种类型，建立了动作标记化的分类体系。\n3. 分析了各类动作标记的优缺点、适用场景和代表性工作，如表1所示。\n4. 探讨了VLA模型的演进趋势，包括从模仿学习到强化学习、从模型到智能体、从能力中心到安全意识的转变。\n5. 强调了模型、数据和硬件协同发展的重要性，指出当前研究主要局限于简化的实验室环境。\n\n论文方法描述\n作为综述论文，主要采用文献分析和分类研究的方法。通过对现有VLA模型的系统梳理，识别出它们共同的设计模式，并将其抽象为统一的动作标记化框架。通过对比不同类型动作标记的特性，分析它们在表达能力、数据需求、训练效率等方面的差异。\n\n论文使用数据集和训练资源\n本文为综述论文，不涉及具体数据集的使用和训练资源的消耗。论文在讨论各类VLA模型时引用了相关工作中使用的数据集，如RT-H使用的数据集、Instruct2Act使用的数据集等，但未提供统一的数据集列表或训练资源统计。\n\n论文使用的评估环境和评估指标\n本文为综述论文，未设计新的评估环境或指标。在分析各类VLA模型时，引用了原始论文中的评估指标，如任务成功率、泛化能力等，但未提供统一的评估基准或指标对比。论文强调了当前评估主要局限于简化实验室环境（如夹爪操作），距离真实世界应用需求尚有较大差距。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京大学人工智能研究院、PKU-PsiBot联合实验室</p>\n\n<p>论文概述</p>\n<p>这篇综述论文从动作标记化（action tokenization）的角度，系统性地回顾和分析了视觉-语言-动作（VLA）模型的研究进展。论文提出了一个统一的框架：视觉和语言输入通过一系列VLA模块处理，产生逐步编码更具体和可操作信息的动作标记链，最终生成可执行动作。作者将动作标记分为八类：语言描述、代码、可供性、轨迹、目标状态、潜在表示、原始动作和推理，并分析了各类标记的优势、局限性和未来方向。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了VLA模型的统一框架，将视觉和语言输入的处理抽象为通过VLA模块生成动作标记链的过程。</li><li>系统性地将动作标记分类为八种类型，建立了动作标记化的分类体系。</li><li>分析了各类动作标记的优缺点、适用场景和代表性工作，如表1所示。</li><li>探讨了VLA模型的演进趋势，包括从模仿学习到强化学习、从模型到智能体、从能力中心到安全意识的转变。</li><li>强调了模型、数据和硬件协同发展的重要性，指出当前研究主要局限于简化的实验室环境。</li></ol>\n\n<p>论文方法描述</p>\n<p>作为综述论文，主要采用文献分析和分类研究的方法。通过对现有VLA模型的系统梳理，识别出它们共同的设计模式，并将其抽象为统一的动作标记化框架。通过对比不同类型动作标记的特性，分析它们在表达能力、数据需求、训练效率等方面的差异。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>本文为综述论文，不涉及具体数据集的使用和训练资源的消耗。论文在讨论各类VLA模型时引用了相关工作中使用的数据集，如RT-H使用的数据集、Instruct2Act使用的数据集等，但未提供统一的数据集列表或训练资源统计。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>本文为综述论文，未设计新的评估环境或指标。在分析各类VLA模型时，引用了原始论文中的评估指标，如任务成功率、泛化能力等，但未提供统一的评估基准或指标对比。论文强调了当前评估主要局限于简化实验室环境（如夹爪操作），距离真实世界应用需求尚有较大差距。</p>"
  },
  {
    "date": "2025-07-02",
    "title": "TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control",
    "link": "http://arxiv.org/abs/2507.01424",
    "summary_markdown": "### 1. 论文研究单位\nFudan University and Shanghai Innovation Institute\n### 2. 论文概述\nTriVLA 是一个受认知神经科学中的情景记忆理论启发的视觉-语言-动作（VLA）统一框架，通过三系统架构实现了情景世界模型。该模型使机器人能够积累、回忆和预测顺序的多模态体验，从而在动态环境中实现鲁棒的长期任务执行。当前基于VLM的VLA系统主要依赖静态表示和有限的时间上下文，而TriVLA通过集成预训练的视觉语言模型（VLM）和视频扩散模型（VDM），提供了高层次的推理和动态预测能力，实验证明其在模拟和真实世界中均优于现有基线。\n### 3. 论文核心贡献点\n1. **受认知神经科学启发的情景世界模型**：提出了用于具身智能体的情景世界模型，使机器人能够积累、回忆和预测顺序的多模态体验。\n2. **统一的三系统组合架构**：提出了TriVLA，一个实现情景世界模型的三系统架构，包括情景多模态感知（System 2）、情景动态感知（System 3）和策略学习模块（System 1）。\n3. **最先进的性能**：在Calvin ABC→D、LIBERO和MetaWorld基准测试中，TriVLA相比之前的最先进方法分别提升了0.21、0.11和0.13，并在真实世界的长视野任务中表现出色。\n### 4. 论文方法描述\nTriVLA采用三系统架构：\n- **System 2（情景多模态感知）**：使用预训练的Eagle-2 VLM处理视觉和语言输入，提取视觉语言嵌入。\n- **System 3（情景动态感知）**：使用在人类和机器人操控数据集上微调的1.5B参数Stable Video Diffusion (SVD)模型，编码过去状态序列并预测未来场景轨迹。为避免计算密集的完整视频去噪，采用单次前向传播提取特征，并自动聚合多层上采样特征。\n- **System 1（策略学习）**：集成Systems 2和3的输出，使用扩散策略和扩散变换器（DiT）生成动作。通过流匹配和跨模态注意力机制，结合视觉语言标记、动态预测标记、机器人状态和动作历史，预测动作序列。训练时采用扩散损失函数。\n### 5. 论文使用数据集和训练资源\n- **数据集**：\n - 模拟基准：Calvin（ABC→D设置）、LIBERO（Spatial、Object、Goal、Long四个子集）、MetaWorld（50个Sawyer机器人任务）。\n - 真实世界：使用KINOVA GEN2机器人和RealSense D455相机，在室内环境中操作各种物体，并设计了长视野高动态任务。\n- **训练资源**：\n - System 2：使用预训练的Eagle-2 VLM。\n - System 3：微调SVD模型，使用193,690个人类轨迹和179,074个机器人轨迹，补充Calvin ABC、MetaWorld和真实任务视频。在8块NVIDIA H100 GPU上训练2-3天。\n - System 1：在任务数据集上训练，使用4块H100 GPU，耗时5-9小时。\n### 6. 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟环境：Calvin、LIBERO、MetaWorld基准测试环境。\n - 真实环境：KINOVA GEN2机器人配备RealSense D455深度相机，采用眼在手配置，在室内场景进行操作。\n- **评估指标**：\n - Calvin：任务序列的平均成功长度（Avg. Len）。\n - LIBERO和MetaWorld：平均任务成功率（%）。\n - 真实世界：任务执行的定性分析，包括长视野任务的成功率。\n - 额外指标：推理延迟（ms）和模型参数量（B）。</think>\n### 论文研究单位\nFudan University and Shanghai Innovation Institute\n### 论文概述\nTriVLA 是一个受认知神经科学中的情景记忆理论启发的视觉-语言-动作（VLA）统一框架，通过三系统架构实现了情景世界模型。该模型使机器人能够积累、回忆和预测顺序的多模态体验，从而在动态环境中实现鲁棒的长期任务执行。当前基于VLM的VLA系统主要依赖静态表示和有限的时间上下文，而TriVLA通过集成预训练的视觉语言模型（VLM）和视频扩散模型（VDM），提供了高层次的推理和动态预测能力，实验证明其在模拟和真实世界中均优于现有基线。\n### 论文核心贡献点\n1. 受认知神经科学启发的情景世界模型：提出了用于具身智能体的情景世界模型，使机器人能够积累、回忆和预测顺序的多模态体验。\n2. 统一的三系统组合架构：提出了TriVLA，一个实现情景世界模型的三系统架构，包括情景多模态感知（System 2）、情景动态感知（System 3）和策略学习模块（System 1）。\n3. 最先进的性能：在Calvin ABC→D、LIBERO和MetaWorld基准测试中，TriVLA相比之前的最先进方法分别提升了0.21、0.11和0.13，并在真实世界的长视野任务中表现出色。\n### 论文方法描述\nTriVLA采用三系统架构：\n- System 2（情景多模态感知）：使用预训练的Eagle-2 VLM处理视觉和语言输入，提取视觉语言嵌入。\n- System 3（情景动态感知）：使用在人类和机器人操控数据集上微调的1.5B参数Stable Video Diffusion (SVD)模型，编码过去状态序列并预测未来场景轨迹。为避免计算密集的完整视频去噪，采用单次前向传播提取特征，并自动聚合多层上采样特征。\n- System 1（策略学习）：集成Systems 2和3的输出，使用扩散策略和扩散变换器（DiT）生成动作。通过流匹配和跨模态注意力机制，结合视觉语言标记、动态预测标记、机器人状态和动作历史，预测动作序列。训练时采用扩散损失函数。\n### 论文使用数据集和训练资源\n- 数据集：\n - 模拟基准：Calvin（ABC→D设置）、LIBERO（Spatial、Object、Goal、Long四个子集）、MetaWorld（50个Sawyer机器人任务）。\n - 真实世界：使用KINOVA GEN2机器人和RealSense D455相机，在室内环境中操作各种物体，并设计了长视野高动态任务。\n- 训练资源：\n - System 2：使用预训练的Eagle-2 VLM。\n - System 3：微调SVD模型，使用193,690个人类轨迹和179,074个机器人轨迹，补充Calvin ABC、MetaWorld和真实任务视频。在8块NVIDIA H100 GPU上训练2-3天。\n - System 1：在任务数据集上训练，使用4块H100 GPU，耗时5-9小时。\n### 论文使用的评估环境和评估指标\n- 评估环境：\n - 模拟环境：Calvin、LIBERO、MetaWorld基准测试环境。\n - 真实环境：KINOVA GEN2机器人配备RealSense D455深度相机，采用眼在手配置，在室内场景进行操作。\n- 评估指标：\n - Calvin：任务序列的平均成功长度（Avg. Len）。\n - LIBERO和MetaWorld：平均任务成功率（%）。\n - 真实世界：任务执行的定性分析，包括长视野任务的成功率。\n - 额外指标：推理延迟（ms）和模型参数量（B）。",
    "summary_html": "<h3>1. 论文研究单位</h3>\n<p>Fudan University and Shanghai Innovation Institute</p>\n<h3>2. 论文概述</h3>\n<p>TriVLA 是一个受认知神经科学中的情景记忆理论启发的视觉-语言-动作（VLA）统一框架，通过三系统架构实现了情景世界模型。该模型使机器人能够积累、回忆和预测顺序的多模态体验，从而在动态环境中实现鲁棒的长期任务执行。当前基于VLM的VLA系统主要依赖静态表示和有限的时间上下文，而TriVLA通过集成预训练的视觉语言模型（VLM）和视频扩散模型（VDM），提供了高层次的推理和动态预测能力，实验证明其在模拟和真实世界中均优于现有基线。</p>\n<h3>3. 论文核心贡献点</h3>\n<ol><li><strong>受认知神经科学启发的情景世界模型</strong>：提出了用于具身智能体的情景世界模型，使机器人能够积累、回忆和预测顺序的多模态体验。</li><li><strong>统一的三系统组合架构</strong>：提出了TriVLA，一个实现情景世界模型的三系统架构，包括情景多模态感知（System 2）、情景动态感知（System 3）和策略学习模块（System 1）。</li><li><strong>最先进的性能</strong>：在Calvin ABC→D、LIBERO和MetaWorld基准测试中，TriVLA相比之前的最先进方法分别提升了0.21、0.11和0.13，并在真实世界的长视野任务中表现出色。</li></ol>\n<h3>4. 论文方法描述</h3>\n<p>TriVLA采用三系统架构：</p>\n<ul><li><strong>System 2（情景多模态感知）</strong>：使用预训练的Eagle-2 VLM处理视觉和语言输入，提取视觉语言嵌入。</li><li><strong>System 3（情景动态感知）</strong>：使用在人类和机器人操控数据集上微调的1.5B参数Stable Video Diffusion (SVD)模型，编码过去状态序列并预测未来场景轨迹。为避免计算密集的完整视频去噪，采用单次前向传播提取特征，并自动聚合多层上采样特征。</li><li><strong>System 1（策略学习）</strong>：集成Systems 2和3的输出，使用扩散策略和扩散变换器（DiT）生成动作。通过流匹配和跨模态注意力机制，结合视觉语言标记、动态预测标记、机器人状态和动作历史，预测动作序列。训练时采用扩散损失函数。</li></ul>\n<h3>5. 论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 模拟基准：Calvin（ABC→D设置）、LIBERO（Spatial、Object、Goal、Long四个子集）、MetaWorld（50个Sawyer机器人任务）。</p>\n<p> - 真实世界：使用KINOVA GEN2机器人和RealSense D455相机，在室内环境中操作各种物体，并设计了长视野高动态任务。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - System 2：使用预训练的Eagle-2 VLM。</p>\n<p> - System 3：微调SVD模型，使用193,690个人类轨迹和179,074个机器人轨迹，补充Calvin ABC、MetaWorld和真实任务视频。在8块NVIDIA H100 GPU上训练2-3天。</p>\n<p> - System 1：在任务数据集上训练，使用4块H100 GPU，耗时5-9小时。</p>\n<h3>6. 论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟环境：Calvin、LIBERO、MetaWorld基准测试环境。</p>\n<p> - 真实环境：KINOVA GEN2机器人配备RealSense D455深度相机，采用眼在手配置，在室内场景进行操作。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - Calvin：任务序列的平均成功长度（Avg. Len）。</p>\n<p> - LIBERO和MetaWorld：平均任务成功率（%）。</p>\n<p> - 真实世界：任务执行的定性分析，包括长视野任务的成功率。</p>\n<p> - 额外指标：推理延迟（ms）和模型参数量（B）。</think></p>\n<h3>论文研究单位</h3>\n<p>Fudan University and Shanghai Innovation Institute</p>\n<h3>论文概述</h3>\n<p>TriVLA 是一个受认知神经科学中的情景记忆理论启发的视觉-语言-动作（VLA）统一框架，通过三系统架构实现了情景世界模型。该模型使机器人能够积累、回忆和预测顺序的多模态体验，从而在动态环境中实现鲁棒的长期任务执行。当前基于VLM的VLA系统主要依赖静态表示和有限的时间上下文，而TriVLA通过集成预训练的视觉语言模型（VLM）和视频扩散模型（VDM），提供了高层次的推理和动态预测能力，实验证明其在模拟和真实世界中均优于现有基线。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>受认知神经科学启发的情景世界模型：提出了用于具身智能体的情景世界模型，使机器人能够积累、回忆和预测顺序的多模态体验。</li><li>统一的三系统组合架构：提出了TriVLA，一个实现情景世界模型的三系统架构，包括情景多模态感知（System 2）、情景动态感知（System 3）和策略学习模块（System 1）。</li><li>最先进的性能：在Calvin ABC→D、LIBERO和MetaWorld基准测试中，TriVLA相比之前的最先进方法分别提升了0.21、0.11和0.13，并在真实世界的长视野任务中表现出色。</li></ol>\n<h3>论文方法描述</h3>\n<p>TriVLA采用三系统架构：</p>\n<ul><li>System 2（情景多模态感知）：使用预训练的Eagle-2 VLM处理视觉和语言输入，提取视觉语言嵌入。</li><li>System 3（情景动态感知）：使用在人类和机器人操控数据集上微调的1.5B参数Stable Video Diffusion (SVD)模型，编码过去状态序列并预测未来场景轨迹。为避免计算密集的完整视频去噪，采用单次前向传播提取特征，并自动聚合多层上采样特征。</li><li>System 1（策略学习）：集成Systems 2和3的输出，使用扩散策略和扩散变换器（DiT）生成动作。通过流匹配和跨模态注意力机制，结合视觉语言标记、动态预测标记、机器人状态和动作历史，预测动作序列。训练时采用扩散损失函数。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：</li></ul>\n<p> - 模拟基准：Calvin（ABC→D设置）、LIBERO（Spatial、Object、Goal、Long四个子集）、MetaWorld（50个Sawyer机器人任务）。</p>\n<p> - 真实世界：使用KINOVA GEN2机器人和RealSense D455相机，在室内环境中操作各种物体，并设计了长视野高动态任务。</p>\n<ul><li>训练资源：</li></ul>\n<p> - System 2：使用预训练的Eagle-2 VLM。</p>\n<p> - System 3：微调SVD模型，使用193,690个人类轨迹和179,074个机器人轨迹，补充Calvin ABC、MetaWorld和真实任务视频。在8块NVIDIA H100 GPU上训练2-3天。</p>\n<p> - System 1：在任务数据集上训练，使用4块H100 GPU，耗时5-9小时。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：</li></ul>\n<p> - 模拟环境：Calvin、LIBERO、MetaWorld基准测试环境。</p>\n<p> - 真实环境：KINOVA GEN2机器人配备RealSense D455深度相机，采用眼在手配置，在室内场景进行操作。</p>\n<ul><li>评估指标：</li></ul>\n<p> - Calvin：任务序列的平均成功长度（Avg. Len）。</p>\n<p> - LIBERO和MetaWorld：平均任务成功率（%）。</p>\n<p> - 真实世界：任务执行的定性分析，包括长视野任务的成功率。</p>\n<p> - 额外指标：推理延迟（ms）和模型参数量（B）。</p>"
  },
  {
    "date": "2025-07-01",
    "title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding",
    "link": "http://arxiv.org/abs/2507.00416",
    "summary_markdown": "### 论文研究单位\n上海交通大学人工智能学院、EvoMind Tech、上海市人工智能研究院、剑桥大学。\n### 论文概述\n该论文提出了Evo-0，一种具有隐式空间理解的视觉-语言-动作（VLA）模型。现有的VLA模型通常依赖于在2D图像-文本对上预训练的视觉-语言模型（VLM），因此缺乏精确的3D空间理解能力。为解决此问题，一些方法引入了显式的3D输入（如点云或深度图），但这需要额外的深度传感器或预训练的深度估计模型。Evo-0通过一个即插即用的模块，利用一个现成的视觉几何基础模型（VGGT）隐式地将3D几何特征融入VLA模型，仅从RGB图像中就能为模型提供具有深度感知的视觉表示，从而增强其对场景几何结构和物体空间关系的理解。论文在模拟和真实世界的多种空间挑战性任务上进行了评估，结果表明Evo-0显著优于最先进的VLA模型。\n### 论文核心贡献点\n1. 提出了一个即插即用的模块，通过隐式注入3D几何先验来增强VLA模型的空间理解能力，而无需使用深度传感器或显式的深度估计。\n2. 在模拟和真实世界的一系列具有空间挑战性的任务上评估了该方法，并证明了其相对于强基线模型的持续改进。\n3. 设计了一个在多种扰动条件下的鲁棒性评估设置，以验证该方法在真实世界扰动中的有效性。\n### 论文方法描述\nEvo-0基于开源VLA模型π₀构建。该方法的架构包含两个编码器：一个2D图像编码器（来自ViT）和一个VGGT空间编码器。输入是多视角RGB图像。VGGT编码器输出包含3D几何信息的特征，这些特征被提取为3D tokens。然后，一个轻量级的融合模块通过单层交叉注意力机制将2D视觉 tokens（作为查询）与VGGT的3D tokens（作为键和值）进行融合。融合后的特征被输入到PaliGemma视觉语言模型中，该模型结合几何增强的视觉输入和语言 tokens 来预测机器人动作。为了保持计算效率，核心VLM参数被冻结，仅在融合模块、LoRA层和流匹配动作专家模块上进行微调。\n### 论文使用数据集和训练资源\n- **模拟数据集**: RLBench基准测试中的五个任务：PlayJenga, PutKnifeOnChoppingBoard, TakeUmbrellaOutOfUmbrellaStand, PlaceHangerOnRack, MoveHanger。每个任务生成100条演示轨迹进行多任务训练。\n- **真实世界数据集**: 针对五个真实世界任务，通过遥操作收集了100条专家演示数据。这些任务包括：在目标上居中圆柱体、轴孔插入、抓取中间的瓶子、罐子抓取放置、透明物体抓取放置。\n- **训练资源**: 使用单块NVIDIA A800 GPU（80GB）进行训练，采用bfloat16混合精度，批次大小为32。优化器为AdamW，权重衰减为10^{-10}，采用余弦学习率调度，初始学习率为2.5 × 10^{-5}，在1000步内预热后衰减至2.5 × 10^{-6}。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - **模拟**: 在CoppeliaSim模拟器中的RLBench环境，使用配备前置、腕部和俯视摄像头的Franka Panda机器人。\n - **真实世界**: 使用真实机器人臂执行五个空间操作任务。\n - **鲁棒性测试**: 在真实世界环境中，对一个简化任务逐步引入五种扰动：未见过的干扰物、背景颜色变化、目标位置偏移、目标高度变化和相机视角变化。\n- **评估指标**:\n - **主要指标**: 任务成功率。在模拟任务中，成功标准依据官方RLBench规范。\n - **真实世界任务**: 除“在目标上居中圆柱体”任务外，均采用二元成功指标。该任务采用0-5分的评分系统，根据圆柱体中心落在目标靶的哪个环来评分，得分越高代表精度越高。\n - **鲁棒性测试**: 同样使用任务成功率，并在特定条件下（如存在干扰物时）评估子任务的成功率（如选择正确物体的成功率）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学人工智能学院、EvoMind Tech、上海市人工智能研究院、剑桥大学。</p>\n<h3>论文概述</h3>\n<p>该论文提出了Evo-0，一种具有隐式空间理解的视觉-语言-动作（VLA）模型。现有的VLA模型通常依赖于在2D图像-文本对上预训练的视觉-语言模型（VLM），因此缺乏精确的3D空间理解能力。为解决此问题，一些方法引入了显式的3D输入（如点云或深度图），但这需要额外的深度传感器或预训练的深度估计模型。Evo-0通过一个即插即用的模块，利用一个现成的视觉几何基础模型（VGGT）隐式地将3D几何特征融入VLA模型，仅从RGB图像中就能为模型提供具有深度感知的视觉表示，从而增强其对场景几何结构和物体空间关系的理解。论文在模拟和真实世界的多种空间挑战性任务上进行了评估，结果表明Evo-0显著优于最先进的VLA模型。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个即插即用的模块，通过隐式注入3D几何先验来增强VLA模型的空间理解能力，而无需使用深度传感器或显式的深度估计。</li><li>在模拟和真实世界的一系列具有空间挑战性的任务上评估了该方法，并证明了其相对于强基线模型的持续改进。</li><li>设计了一个在多种扰动条件下的鲁棒性评估设置，以验证该方法在真实世界扰动中的有效性。</li></ol>\n<h3>论文方法描述</h3>\n<p>Evo-0基于开源VLA模型π₀构建。该方法的架构包含两个编码器：一个2D图像编码器（来自ViT）和一个VGGT空间编码器。输入是多视角RGB图像。VGGT编码器输出包含3D几何信息的特征，这些特征被提取为3D tokens。然后，一个轻量级的融合模块通过单层交叉注意力机制将2D视觉 tokens（作为查询）与VGGT的3D tokens（作为键和值）进行融合。融合后的特征被输入到PaliGemma视觉语言模型中，该模型结合几何增强的视觉输入和语言 tokens 来预测机器人动作。为了保持计算效率，核心VLM参数被冻结，仅在融合模块、LoRA层和流匹配动作专家模块上进行微调。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>模拟数据集</strong>: RLBench基准测试中的五个任务：PlayJenga, PutKnifeOnChoppingBoard, TakeUmbrellaOutOfUmbrellaStand, PlaceHangerOnRack, MoveHanger。每个任务生成100条演示轨迹进行多任务训练。</li><li><strong>真实世界数据集</strong>: 针对五个真实世界任务，通过遥操作收集了100条专家演示数据。这些任务包括：在目标上居中圆柱体、轴孔插入、抓取中间的瓶子、罐子抓取放置、透明物体抓取放置。</li><li><strong>训练资源</strong>: 使用单块NVIDIA A800 GPU（80GB）进行训练，采用bfloat16混合精度，批次大小为32。优化器为AdamW，权重衰减为10^{-10}，采用余弦学习率调度，初始学习率为2.5 × 10^{-5}，在1000步内预热后衰减至2.5 × 10^{-6}。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - <strong>模拟</strong>: 在CoppeliaSim模拟器中的RLBench环境，使用配备前置、腕部和俯视摄像头的Franka Panda机器人。</p>\n<p> - <strong>真实世界</strong>: 使用真实机器人臂执行五个空间操作任务。</p>\n<p> - <strong>鲁棒性测试</strong>: 在真实世界环境中，对一个简化任务逐步引入五种扰动：未见过的干扰物、背景颜色变化、目标位置偏移、目标高度变化和相机视角变化。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>主要指标</strong>: 任务成功率。在模拟任务中，成功标准依据官方RLBench规范。</p>\n<p> - <strong>真实世界任务</strong>: 除“在目标上居中圆柱体”任务外，均采用二元成功指标。该任务采用0-5分的评分系统，根据圆柱体中心落在目标靶的哪个环来评分，得分越高代表精度越高。</p>\n<p> - <strong>鲁棒性测试</strong>: 同样使用任务成功率，并在特定条件下（如存在干扰物时）评估子任务的成功率（如选择正确物体的成功率）。</p>"
  },
  {
    "date": "2025-06-30",
    "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
    "link": "http://arxiv.org/abs/2506.24044",
    "summary_markdown": "### 论文研究单位\nMcGill University (加拿大), Tsinghua University (中国), Xiaomi Corporation, University of Wisconsin–Madison (美国), University of Minnesota–Twin Cities (美国), State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University (中国)\n### 论文概述\n这是第一篇关于自动驾驶视觉-语言-动作（VLA）模型的全面综述，系统梳理了VLA4AD领域的发展脉络。论文从自动驾驶技术演进切入，对比了经典模块化流水线、端到端学习、视觉语言模型（VLM）和最新的VLA范式，分析了超过20个代表性模型，整合了现有数据集和基准测试，并探讨了开放挑战与未来方向。\n### 论文核心贡献点\n- 首次系统性地梳理自动驾驶领域的VLA模型研究进展\n- 形式化了VLA4AD的通用架构构建模块\n- 追溯了从早期解释器到推理中心VLA模型的四个演进阶段\n- 对比分析了20多个代表性VLA模型的核心特性与技术差异\n- 整合了现有数据集和评估基准，提出联合评估驾驶安全性、准确性和解释质量的协议\n- 详述了鲁棒性、实时效率、形式验证等开放挑战\n- 提出了基础规模驾驶模型、神经符号安全内核等未来研究方向\n### 论文方法描述\nVLA4AD的核心架构包含三个主要部分：\n1. 多模态输入处理：视觉数据（单目/多目相机、BEV表示）、其他传感器数据（LiDAR、雷达、IMU、GPS）和语言输入（导航指令、环境查询、任务级规范）\n2. 核心架构模块：\n - 视觉编码器：使用DINOv2、ConvNeXt-V2或CLIP等自监督主干网络\n - 语言处理器：基于LLaMA2或GPT风格的Transformer，支持指令微调和检索增强\n - 动作解码器：包括自回归分词器、扩散头或流匹配/策略梯度专家\n3. 驾驶输出：低级动作（转向角、油门、制动）和轨迹规划（BEV坐标下的路径点）\n### 论文使用数据集和训练资源\n- BDD100K/BDD-X：100k美国驾驶视频，7k带有人类解释标注的片段\n- nuScenes：1k个20秒真实场景（波士顿/新加坡），含6摄像头+LiDAR+雷达\n- Bench2Drive：CARLA闭环基准，220条路线覆盖44种场景类型\n- Reason2Drive：600k视频-文本对，包含CoT风格的问答标注\n- DriveLM-Data：18k场景图结构QA，支持条件推理\n- Impromptu VLA：80k边缘案例片段（30秒），密集人群/救护车/恶劣天气\n- NuInteract：1k多视图场景，密集标注和3D多轮问答\n- DriveAction：2.6k真实场景和16.2k视觉语言QA对\n训练资源包括大规模多传感器日志（如nuScenes、Waymo车队数据）和互联网规模视觉-语言预训练模型\n### 论文使用的评估环境和评估指标\n评估环境：\n- 闭环驾驶测试（CARLA模拟器、Navsim-v2）\n- 开环预测评估\n- 多基准测试套件（Bench2Drive、Reason2Drive、DriveLM）\n\n评估指标：\n- 驾驶安全性指标（碰撞率、违规率）\n- 控制准确性指标（轨迹偏差、成功率）\n- 语言能力评估（BLEU、图一致性、逻辑一致性）\n- 鲁棒性测试（压力场景表现）\n- 人类偏好对齐评估（通过DriveAction等数据集）\n- 实时性指标（推理延迟、控制频率）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>McGill University (加拿大), Tsinghua University (中国), Xiaomi Corporation, University of Wisconsin–Madison (美国), University of Minnesota–Twin Cities (美国), State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University (中国)</p>\n<h3>论文概述</h3>\n<p>这是第一篇关于自动驾驶视觉-语言-动作（VLA）模型的全面综述，系统梳理了VLA4AD领域的发展脉络。论文从自动驾驶技术演进切入，对比了经典模块化流水线、端到端学习、视觉语言模型（VLM）和最新的VLA范式，分析了超过20个代表性模型，整合了现有数据集和基准测试，并探讨了开放挑战与未来方向。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>首次系统性地梳理自动驾驶领域的VLA模型研究进展</li><li>形式化了VLA4AD的通用架构构建模块</li><li>追溯了从早期解释器到推理中心VLA模型的四个演进阶段</li><li>对比分析了20多个代表性VLA模型的核心特性与技术差异</li><li>整合了现有数据集和评估基准，提出联合评估驾驶安全性、准确性和解释质量的协议</li><li>详述了鲁棒性、实时效率、形式验证等开放挑战</li><li>提出了基础规模驾驶模型、神经符号安全内核等未来研究方向</li></ul>\n<h3>论文方法描述</h3>\n<p>VLA4AD的核心架构包含三个主要部分：</p>\n<ol><li>多模态输入处理：视觉数据（单目/多目相机、BEV表示）、其他传感器数据（LiDAR、雷达、IMU、GPS）和语言输入（导航指令、环境查询、任务级规范）</li><li>核心架构模块：</li></ol>\n<p> - 视觉编码器：使用DINOv2、ConvNeXt-V2或CLIP等自监督主干网络</p>\n<p> - 语言处理器：基于LLaMA2或GPT风格的Transformer，支持指令微调和检索增强</p>\n<p> - 动作解码器：包括自回归分词器、扩散头或流匹配/策略梯度专家</p>\n<p>3. 驾驶输出：低级动作（转向角、油门、制动）和轨迹规划（BEV坐标下的路径点）</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>BDD100K/BDD-X：100k美国驾驶视频，7k带有人类解释标注的片段</li><li>nuScenes：1k个20秒真实场景（波士顿/新加坡），含6摄像头+LiDAR+雷达</li><li>Bench2Drive：CARLA闭环基准，220条路线覆盖44种场景类型</li><li>Reason2Drive：600k视频-文本对，包含CoT风格的问答标注</li><li>DriveLM-Data：18k场景图结构QA，支持条件推理</li><li>Impromptu VLA：80k边缘案例片段（30秒），密集人群/救护车/恶劣天气</li><li>NuInteract：1k多视图场景，密集标注和3D多轮问答</li><li>DriveAction：2.6k真实场景和16.2k视觉语言QA对</li></ul>\n<p>训练资源包括大规模多传感器日志（如nuScenes、Waymo车队数据）和互联网规模视觉-语言预训练模型</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：</p>\n<ul><li>闭环驾驶测试（CARLA模拟器、Navsim-v2）</li><li>开环预测评估</li><li>多基准测试套件（Bench2Drive、Reason2Drive、DriveLM）</li></ul>\n\n<p>评估指标：</p>\n<ul><li>驾驶安全性指标（碰撞率、违规率）</li><li>控制准确性指标（轨迹偏差、成功率）</li><li>语言能力评估（BLEU、图一致性、逻辑一致性）</li><li>鲁棒性测试（压力场景表现）</li><li>人类偏好对齐评估（通过DriveAction等数据集）</li><li>实时性指标（推理延迟、控制频率）</li></ul>"
  },
  {
    "date": "2025-06-29",
    "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering",
    "link": "http://arxiv.org/abs/2506.23329",
    "summary_markdown": "论文研究单位\nCUHK, TJU, EPFL, HKUST, XMU, MIT\n\n论文概述\n该论文提出了IR3D-Bench，一个用于评估视觉语言模型（VLMs）通过智能体逆向渲染进行场景理解能力的基准测试。基准的核心思想是“通过创造来理解”，要求VLMs通过使用编程和渲染工具主动创建底层3D场景来展示其理解能力，而不仅仅是进行被动的识别或描述。这种方法将传统的逆向渲染任务转变为一个多模态编程任务，旨在探究VLMs的工具使用生成能力，并系统性地评估当前模型在视觉精度上的局限性。\n\n论文核心贡献点\n提出了IR3D-Bench基准，将场景理解任务从被动的描述性任务（如图像字幕生成、VQA）转变为主动的生成性任务，即通过工具使用进行智能体逆向渲染。\n构建了一个包含三个维度的全面评估指标体系：定位（几何精度、空间关系）、视觉外观（形状、材料）和语言对齐语义（布局合理性）。\n基于CLEVR数据集构建了受控的测试环境，提供了精确的3D场景图和对象级元数据，以专注于评估模型有效使用工具的能力。\n通过初步实验揭示了当前VLMs的瓶颈主要在于视觉感知的精确度而非基本工具使用，并发布基准数据与评估协议以促进未来研究。\n\n论文方法描述\n任务定义：将智能体逆向渲染形式化为，给定一张单张图像I，视觉语言智能体（VLA）需生成一个可执行的程序P（如Blender Python脚本），该程序在确定的渲染环境R中运行后，应生成与原始图像尽可能匹配的渲染图像R(P)。\n数据与工具：使用CLEVR数据集的验证集，该数据集包含15,000张合成图像及精确的3D坐标、形状、颜色、大小、材料和空间关系注释。智能体通过模型上下文协议（MCP）与Blender交互，执行生成的脚本。\n评估流程：首先，智能体预测结构化的JSON格式场景表示。然后，在固定相机内参和外参下，将预测的3D对象投影到2D图像平面。接着，通过基于CLIP文本编码器计算的对象属性相似度矩阵，使用匈牙利算法进行最优二分匹配，以建立预测对象与真实对象之间的对应关系。最后，利用SAM模型获取对象级分割掩码，进行多维度评估。\n\n论文使用数据集和训练资源\n数据集：采用CLEVR数据集的验证分割部分，包含15,000张由3D场景图渲染的合成图像。每张图像分辨率为480x320像素，包含3到10个对象，并提供了精确的3D坐标、像素投影、形状、颜色、大小、材料和对象间空间关系的真值。\n训练资源：论文未提及模型的训练过程或使用的计算资源，重点在于评估现有预训练VLMs的零样本生成能力。\n\n论文使用的评估环境和评估指标\n评估环境：使用Blender作为3D渲染环境，通过模型上下文协议（MCP）与智能体交互。分割任务采用Segment Anything Model (SAM)。\n评估指标：分为三类。\n定位：包括像素距离（预测与真实2D投影中心的平均L2距离）、计数准确性（对象数量一致性）、边界框边缘分数（中心到四边距离的归一化差异）和空间关系（如“左/右”、“前/后”关系的一致性）。\n视觉外观：通过掩码级和属性级分数评估形状和材料的准确性。\n语言对齐语义：使用GPT-4o评估布局保真度和对象合理性，以及整体的视觉语义一致性。",
    "summary_html": "<p>论文研究单位</p>\n<p>CUHK, TJU, EPFL, HKUST, XMU, MIT</p>\n\n<p>论文概述</p>\n<p>该论文提出了IR3D-Bench，一个用于评估视觉语言模型（VLMs）通过智能体逆向渲染进行场景理解能力的基准测试。基准的核心思想是“通过创造来理解”，要求VLMs通过使用编程和渲染工具主动创建底层3D场景来展示其理解能力，而不仅仅是进行被动的识别或描述。这种方法将传统的逆向渲染任务转变为一个多模态编程任务，旨在探究VLMs的工具使用生成能力，并系统性地评估当前模型在视觉精度上的局限性。</p>\n\n<p>论文核心贡献点</p>\n<p>提出了IR3D-Bench基准，将场景理解任务从被动的描述性任务（如图像字幕生成、VQA）转变为主动的生成性任务，即通过工具使用进行智能体逆向渲染。</p>\n<p>构建了一个包含三个维度的全面评估指标体系：定位（几何精度、空间关系）、视觉外观（形状、材料）和语言对齐语义（布局合理性）。</p>\n<p>基于CLEVR数据集构建了受控的测试环境，提供了精确的3D场景图和对象级元数据，以专注于评估模型有效使用工具的能力。</p>\n<p>通过初步实验揭示了当前VLMs的瓶颈主要在于视觉感知的精确度而非基本工具使用，并发布基准数据与评估协议以促进未来研究。</p>\n\n<p>论文方法描述</p>\n<p>任务定义：将智能体逆向渲染形式化为，给定一张单张图像I，视觉语言智能体（VLA）需生成一个可执行的程序P（如Blender Python脚本），该程序在确定的渲染环境R中运行后，应生成与原始图像尽可能匹配的渲染图像R(P)。</p>\n<p>数据与工具：使用CLEVR数据集的验证集，该数据集包含15,000张合成图像及精确的3D坐标、形状、颜色、大小、材料和空间关系注释。智能体通过模型上下文协议（MCP）与Blender交互，执行生成的脚本。</p>\n<p>评估流程：首先，智能体预测结构化的JSON格式场景表示。然后，在固定相机内参和外参下，将预测的3D对象投影到2D图像平面。接着，通过基于CLIP文本编码器计算的对象属性相似度矩阵，使用匈牙利算法进行最优二分匹配，以建立预测对象与真实对象之间的对应关系。最后，利用SAM模型获取对象级分割掩码，进行多维度评估。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：采用CLEVR数据集的验证分割部分，包含15,000张由3D场景图渲染的合成图像。每张图像分辨率为480x320像素，包含3到10个对象，并提供了精确的3D坐标、像素投影、形状、颜色、大小、材料和对象间空间关系的真值。</p>\n<p>训练资源：论文未提及模型的训练过程或使用的计算资源，重点在于评估现有预训练VLMs的零样本生成能力。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：使用Blender作为3D渲染环境，通过模型上下文协议（MCP）与智能体交互。分割任务采用Segment Anything Model (SAM)。</p>\n<p>评估指标：分为三类。</p>\n<p>定位：包括像素距离（预测与真实2D投影中心的平均L2距离）、计数准确性（对象数量一致性）、边界框边缘分数（中心到四边距离的归一化差异）和空间关系（如“左/右”、“前/后”关系的一致性）。</p>\n<p>视觉外观：通过掩码级和属性级分数评估形状和材料的准确性。</p>\n<p>语言对齐语义：使用GPT-4o评估布局保真度和对象合理性，以及整体的视觉语义一致性。</p>"
  },
  {
    "date": "2025-06-27",
    "title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration",
    "link": "http://arxiv.org/abs/2506.22242",
    "summary_markdown": "### 论文研究单位\n- 复旦大学数据科学学院\n- 华为诺亚方舟实验室\n### 论文概述\n- 论文提出4D-VLA模型，旨在解决现有机器人预训练方法中因输入信息不完整导致的“坐标系混乱”和“状态混乱”问题。\n- 通过引入RGB-D序列输入，融合深度和时空信息，对齐机器人与场景的坐标系，增强时空推理能力。\n- 提出“记忆库采样”策略，从历史帧中提取关键信息，提升效率和效果。\n- 引入多视图仿真基准MV-Bench评估空间感知和新视角泛化能力。\n### 论文核心贡献点\n- 提出4D-VLA模型，整合空间模块生成3D感知的视觉token，缓解坐标系和状态混乱。\n- 设计记忆库采样方法，高效利用历史信息。\n- 在LIBERO和真实世界任务中显著超越基线，如在LIBERO-LONG任务上成功率比OpenVLA高25.4%。\n- 构建MV-Bench多视角数据集，验证模型的空间理解和新视角泛化能力。\n### 论文方法描述\n- 输入处理：使用序列RGB-D图像，通过3D坐标嵌入生成空间感知视觉token。\n- 时空编码：将历史帧与当前帧融合，加入可学习时间位置编码，形成4D表示。\n- 记忆库采样：自适应选择信息量大的历史帧，避免均匀采样的冗余。\n- 模型架构：基于InternVL-4B，冻结视觉编码器，训练其他参数。\n- 损失函数：结合平移、旋转、夹爪和方向损失，其中方向损失强调动作方向准确性。\n### 论文使用数据集和训练资源\n- 数据集：DROID（真实世界，76k轨迹）、LIBERO（仿真，多任务集）、自建MV-Bench（多视角）。\n- 训练资源：8块NVIDIA A6000 GPU，训练约96小时，预训练 batch size 512，微调batch size 128。\n### 论文使用的评估环境和评估指标\n- 评估环境：\n - 仿真：LIBERO任务集（LIBERO-SPATIAL、OBJECT、GOAL、LONG）、MV-Bench多视角基准。\n - 真实：Franka机械臂执行4项任务（空间泛化、抗干扰、精确放置、指令跟随）和2项多视角任务。\n- 评估指标：\n - 任务成功率（仿真/真实任务）。\n - 平均成功率（多视角任务分视角和跨视角评估）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>复旦大学数据科学学院</li><li>华为诺亚方舟实验室</li></ul>\n<h3>论文概述</h3>\n<ul><li>论文提出4D-VLA模型，旨在解决现有机器人预训练方法中因输入信息不完整导致的“坐标系混乱”和“状态混乱”问题。</li><li>通过引入RGB-D序列输入，融合深度和时空信息，对齐机器人与场景的坐标系，增强时空推理能力。</li><li>提出“记忆库采样”策略，从历史帧中提取关键信息，提升效率和效果。</li><li>引入多视图仿真基准MV-Bench评估空间感知和新视角泛化能力。</li></ul>\n<h3>论文核心贡献点</h3>\n<ul><li>提出4D-VLA模型，整合空间模块生成3D感知的视觉token，缓解坐标系和状态混乱。</li><li>设计记忆库采样方法，高效利用历史信息。</li><li>在LIBERO和真实世界任务中显著超越基线，如在LIBERO-LONG任务上成功率比OpenVLA高25.4%。</li><li>构建MV-Bench多视角数据集，验证模型的空间理解和新视角泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>输入处理：使用序列RGB-D图像，通过3D坐标嵌入生成空间感知视觉token。</li><li>时空编码：将历史帧与当前帧融合，加入可学习时间位置编码，形成4D表示。</li><li>记忆库采样：自适应选择信息量大的历史帧，避免均匀采样的冗余。</li><li>模型架构：基于InternVL-4B，冻结视觉编码器，训练其他参数。</li><li>损失函数：结合平移、旋转、夹爪和方向损失，其中方向损失强调动作方向准确性。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：DROID（真实世界，76k轨迹）、LIBERO（仿真，多任务集）、自建MV-Bench（多视角）。</li><li>训练资源：8块NVIDIA A6000 GPU，训练约96小时，预训练 batch size 512，微调batch size 128。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：</li></ul>\n<p> - 仿真：LIBERO任务集（LIBERO-SPATIAL、OBJECT、GOAL、LONG）、MV-Bench多视角基准。</p>\n<p> - 真实：Franka机械臂执行4项任务（空间泛化、抗干扰、精确放置、指令跟随）和2项多视角任务。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 任务成功率（仿真/真实任务）。</p>\n<p> - 平均成功率（多视角任务分视角和跨视角评估）。</p>"
  },
  {
    "date": "2025-06-26",
    "title": "WorldVLA: Towards Autoregressive Action World Model",
    "link": "http://arxiv.org/abs/2506.21539",
    "summary_markdown": "论文研究单位\nDAMO Academy, Alibaba Group, Hupan Lab, Zhejiang University\n\n论文概述\n该论文提出了WorldVLA，一个自回归的动作世界模型，它统一了动作和图像的理解与生成。WorldVLA集成了视觉-语言-动作（VLA）模型和世界模型于单一框架中。世界模型通过利用动作和图像理解来预测未来图像，其目的是学习环境的潜在物理知识以改进动作生成。同时，动作模型基于图像观察生成后续动作，这有助于视觉理解，进而帮助世界模型的视觉生成。论文证明了WorldVLA优于独立的动作模型和世界模型，突显了世界模型和动作模型之间的相互增强。此外，论文发现当以自回归方式生成动作序列时，动作模型的性能会下降。这种现象归因于模型在动作预测上的泛化能力有限，导致早期动作的错误传播到后续动作中。为解决此问题，论文提出了一种注意力掩码策略，在生成当前动作时有选择性地掩蔽先前的动作，这在动作块生成任务中显示出显著的性能提升。\n\n论文核心贡献点\n1. 提出了WorldVLA，一个统一了动作和图像理解与生成的自回归动作世界模型。\n2. 引入了一种用于自回归模型中动作块生成任务的动作注意力掩码策略，解决了在按顺序生成多个动作时动作误差累积的挑战。\n3. 实验证明WorldVLA优于独立的动作和世界模型，突显了世界模型和动作模型之间的相互增强。此外，动作注意力掩码策略解决了生成动作块时的性能下降问题，并显著提高了抓取性能。\n\n论文方法描述\n1. 问题定义：将动作模型和世界模型统一到一个单一的模型中，该模型能够同时预测动作和预测未来世界状态。\n2. 架构：\n - 模型从Chameleon初始化，它是一个统一的图像理解和生成模型。\n - 使用三种标记化器：图像标记化器（VQ-GAN，压缩比为16，码本大小为8192）、文本标记化器（BPE，词汇表大小为65536，包含8192个图像标记和256个动作标记）和动作标记化器（将连续机器人动作的每个维度离散化为256个区间之一）。\n - 所有文本、动作和图像都被离散化为标记，并以自回归方式进行训练。\n3. 训练策略：\n - 混合动作模型数据和世界模型数据来训练WorldVLA。\n - 动作模型数据：输入为文本指令和图像观察，输出为动作序列。\n - 世界模型数据：输入为当前图像观察和动作，输出为下一帧图像。\n - 注意力掩码：提出一种用于动作生成的替代注意力掩码，确保当前动作仅依赖于文本和视觉输入，而禁止访问先前的动作。\n - 训练目标：总损失函数为动作损失和世界损失的加权和，由于图像标记远多于动作标记，使用alpha系数来平衡损失贡献。\n\n论文使用数据集和训练资源\n数据集：\nLIBERO benchmark (LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long, LIBERO-90)。过滤掉不成功的轨迹和无操作动作，90%的轨迹作为训练集，10%的轨迹作为验证集。\n\n训练资源：\n未明确提及训练资源，但描述了模型配置和训练设置。\n\n论文使用的评估环境和评估指标\n评估环境：\nLIBERO benchmark。使用标准的训练集和验证集划分进行公平比较。\n\n评估指标：\n动作模型：抓取成功率。\n世界模型：Fréchet Video Distance (FVD)。",
    "summary_html": "<p>论文研究单位</p>\n<p>DAMO Academy, Alibaba Group, Hupan Lab, Zhejiang University</p>\n\n<p>论文概述</p>\n<p>该论文提出了WorldVLA，一个自回归的动作世界模型，它统一了动作和图像的理解与生成。WorldVLA集成了视觉-语言-动作（VLA）模型和世界模型于单一框架中。世界模型通过利用动作和图像理解来预测未来图像，其目的是学习环境的潜在物理知识以改进动作生成。同时，动作模型基于图像观察生成后续动作，这有助于视觉理解，进而帮助世界模型的视觉生成。论文证明了WorldVLA优于独立的动作模型和世界模型，突显了世界模型和动作模型之间的相互增强。此外，论文发现当以自回归方式生成动作序列时，动作模型的性能会下降。这种现象归因于模型在动作预测上的泛化能力有限，导致早期动作的错误传播到后续动作中。为解决此问题，论文提出了一种注意力掩码策略，在生成当前动作时有选择性地掩蔽先前的动作，这在动作块生成任务中显示出显著的性能提升。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了WorldVLA，一个统一了动作和图像理解与生成的自回归动作世界模型。</li><li>引入了一种用于自回归模型中动作块生成任务的动作注意力掩码策略，解决了在按顺序生成多个动作时动作误差累积的挑战。</li><li>实验证明WorldVLA优于独立的动作和世界模型，突显了世界模型和动作模型之间的相互增强。此外，动作注意力掩码策略解决了生成动作块时的性能下降问题，并显著提高了抓取性能。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>问题定义：将动作模型和世界模型统一到一个单一的模型中，该模型能够同时预测动作和预测未来世界状态。</li><li>架构：</li></ol>\n<p> - 模型从Chameleon初始化，它是一个统一的图像理解和生成模型。</p>\n<p> - 使用三种标记化器：图像标记化器（VQ-GAN，压缩比为16，码本大小为8192）、文本标记化器（BPE，词汇表大小为65536，包含8192个图像标记和256个动作标记）和动作标记化器（将连续机器人动作的每个维度离散化为256个区间之一）。</p>\n<p> - 所有文本、动作和图像都被离散化为标记，并以自回归方式进行训练。</p>\n<p>3. 训练策略：</p>\n<p> - 混合动作模型数据和世界模型数据来训练WorldVLA。</p>\n<p> - 动作模型数据：输入为文本指令和图像观察，输出为动作序列。</p>\n<p> - 世界模型数据：输入为当前图像观察和动作，输出为下一帧图像。</p>\n<p> - 注意力掩码：提出一种用于动作生成的替代注意力掩码，确保当前动作仅依赖于文本和视觉输入，而禁止访问先前的动作。</p>\n<p> - 训练目标：总损失函数为动作损失和世界损失的加权和，由于图像标记远多于动作标记，使用alpha系数来平衡损失贡献。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<p>LIBERO benchmark (LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long, LIBERO-90)。过滤掉不成功的轨迹和无操作动作，90%的轨迹作为训练集，10%的轨迹作为验证集。</p>\n\n<p>训练资源：</p>\n<p>未明确提及训练资源，但描述了模型配置和训练设置。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<p>LIBERO benchmark。使用标准的训练集和验证集划分进行公平比较。</p>\n\n<p>评估指标：</p>\n<p>动作模型：抓取成功率。</p>\n<p>世界模型：Fréchet Video Distance (FVD)。</p>"
  },
  {
    "date": "2025-06-26",
    "title": "Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends",
    "link": "http://arxiv.org/abs/2506.20966",
    "summary_markdown": "### 论文研究单位\n中国科学院自动化研究所多模态人工智能系统重点实验室，中国科学院大学人工智能学院，美国伊利诺伊大学厄巴纳-香槟分校格兰杰工程学院。\n### 论文概述\n本文是一篇综述性论文，探讨了视觉-语言-行动模型的后训练与人类运动学习之间的相似性。论文指出，VLA模型在经过大规模预训练后，仍需通过后训练来适应特定的下游应用，以提高在特定环境、任务和机器人实体上的性能。文章借鉴人类运动学习的视角，围绕环境、实体和任务三个维度，系统性地回顾了VLA模型的后训练策略，并提出了一个与之对齐的分类法，最后讨论了该领域面临的挑战和未来趋势。\n### 论文核心贡献点\n1. 首次从人类运动学习的视角，对VLA模型的后训练方法进行了系统性综述。\n2. 提出了一个围绕环境、实体和任务的结构化分类法，该分类法与人类运动学习机制相呼应，用于组织和分析现有的VLA后训练方法。\n3. 识别并阐述了当前VLA模型后训练面临的关键挑战和未来的研究方向，为该领域的后续研究提供了概念框架和实践见解。\n### 论文方法描述\n论文提出一个与人类运动学习机制对齐的分类法来综述VLA模型的后训练方法，该分类法包含四个主要方面：\n1. 增强环境感知：旨在提升模型对操作环境的理解能力。方法包括可供性引导学习、为操作任务增强编码器和操作任务增强表征。\n2. 改进实体意识：旨在让模型更好地理解和适应特定机器人实体的动态特性。方法包括学习正向/逆向运动学、设计更好的动作输出头。\n3. 深化任务理解：旨在让模型更深刻地掌握操作任务的逻辑和层次。方法包括人机交互学习、分层任务操作。\n4. 多组件集成：旨在将环境、实体和任务的多个方面进行协同优化。方法包括使用强化学习、视觉交互预测和主动数据处理。\n### 论文使用数据集和训练资源\n论文主要提到了Open X-Embodiment数据集，该数据集聚合了58个现有的机器人操作数据集。此外，还提及了多种高保真度模拟器和任务特定的模拟环境被用于生成操作数据以降低数据采集成本。关于具体的训练资源（如计算硬件、训练时长等），原文中未提及。\n### 论文使用的评估环境和评估指标\n所提供的论文HTML原文中未包含此部分内容。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中国科学院自动化研究所多模态人工智能系统重点实验室，中国科学院大学人工智能学院，美国伊利诺伊大学厄巴纳-香槟分校格兰杰工程学院。</p>\n<h3>论文概述</h3>\n<p>本文是一篇综述性论文，探讨了视觉-语言-行动模型的后训练与人类运动学习之间的相似性。论文指出，VLA模型在经过大规模预训练后，仍需通过后训练来适应特定的下游应用，以提高在特定环境、任务和机器人实体上的性能。文章借鉴人类运动学习的视角，围绕环境、实体和任务三个维度，系统性地回顾了VLA模型的后训练策略，并提出了一个与之对齐的分类法，最后讨论了该领域面临的挑战和未来趋势。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>首次从人类运动学习的视角，对VLA模型的后训练方法进行了系统性综述。</li><li>提出了一个围绕环境、实体和任务的结构化分类法，该分类法与人类运动学习机制相呼应，用于组织和分析现有的VLA后训练方法。</li><li>识别并阐述了当前VLA模型后训练面临的关键挑战和未来的研究方向，为该领域的后续研究提供了概念框架和实践见解。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文提出一个与人类运动学习机制对齐的分类法来综述VLA模型的后训练方法，该分类法包含四个主要方面：</p>\n<ol><li>增强环境感知：旨在提升模型对操作环境的理解能力。方法包括可供性引导学习、为操作任务增强编码器和操作任务增强表征。</li><li>改进实体意识：旨在让模型更好地理解和适应特定机器人实体的动态特性。方法包括学习正向/逆向运动学、设计更好的动作输出头。</li><li>深化任务理解：旨在让模型更深刻地掌握操作任务的逻辑和层次。方法包括人机交互学习、分层任务操作。</li><li>多组件集成：旨在将环境、实体和任务的多个方面进行协同优化。方法包括使用强化学习、视觉交互预测和主动数据处理。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>论文主要提到了Open X-Embodiment数据集，该数据集聚合了58个现有的机器人操作数据集。此外，还提及了多种高保真度模拟器和任务特定的模拟环境被用于生成操作数据以降低数据采集成本。关于具体的训练资源（如计算硬件、训练时长等），原文中未提及。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>所提供的论文HTML原文中未包含此部分内容。</p>"
  },
  {
    "date": "2025-06-24",
    "title": "Unified Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2506.19850",
    "summary_markdown": "# 论文总结\n## 论文研究单位\nCASIA, BAAI, THU, HKISI\n## 论文概述\n该论文提出了UniVLA，一个统一的视觉-语言-动作（VLA）模型。与以往依赖外部视觉编码器且仅输出动作的VLA方法不同，UniVLA将视觉、语言和动作信号表示为离散令牌序列，并在统一的自回归框架中进行建模。这种统一的多模态范式支持灵活的多模态任务学习，特别是从大规模视频数据中学习。通过在后训练阶段结合世界模型，UniVLA能够从视频中捕捉因果动态，从而有效迁移到下游的策略学习，尤其是在长视野任务中。\n## 论文核心贡献点\n1. 提出了UniVLA，这是第一个将视觉、语言和动作编码为共享词汇表中的离散令牌，并通过自回归序列学习联合建模的统一VLA模型。\n2. 统一的序列建模框架支持广泛的多模态任务。通过研究不同的后训练策略，证明了世界模型可以有效地从视频数据中学习时间动态，显著提升下游策略学习的性能和效率，尤其是在长视野和分布外场景中。\n3. 模型在多个模拟基准测试中取得了最先进的性能，并支持大规模视频训练的开源VLA方法。进一步探索了其在空间推理和视频预测等不同模态的能力，并展示了其在自动驾驶场景中的有效迁移，突显了其在通用具身智能方面的潜力。\n## 论文方法描述\n1. **统一多模态模型**：视觉、语言和动作信号被转换为离散令牌，并使用共享词汇表进行自回归建模。图像通过VQ分词器进行离散化，动作通过FAST分词器使用离散余弦变换（DCT）进行编码。训练目标是标准的下一个令牌预测任务，使用交叉熵损失。\n2. **统一多模态序列建模**：采用基于马尔可夫链的自回归序列建模方法，其中观察和动作交错排列。这自然地包含了因果依赖关系，使模型能够对时间动态进行推理。\n - **世界模型（后训练）**：通过仅对视觉令牌进行监督来学习环境的动态，使模型能够生成给定指令和观察状态条件的视觉预测。\n - **策略学习（微调）**：在交错格式中，采用两阶段训练策略。模型首先使用视觉语言（VL）对齐的检查点进行初始化，后训练阶段利用大规模机器人中心视频数据集来研究不同后训练策略对下游策略学习的影响，微调阶段则专注于动作学习以细化任务特定行为。\n## 论文使用数据集和训练资源\n1. **数据集**：\n - CALVIN：用于评估长视野、语言条件机器人操作的模拟基准。\n - LIBERO：终身机器人操作的综合套件，包含四个任务套件。\n - SimplerEnv：用于评估在真实世界视频数据上训练的模型的迁移性和泛化能力的模拟基准。\n2. **训练资源**：\n - 模型采用8.5B参数的纯自回归Transformer架构。\n - 后训练阶段使用622K视频，训练30K步，批量大小为64。\n - 微调阶段：CALVIN基准使用A100 GPU，批量大小192，训练8k步；LIBERO基准使用A100 GPU，批量大小192，训练8k步；SimplerEnv基准使用A100 GPU，批量大小128，训练20k步。\n## 论文使用的评估环境和评估指标\n1. **评估环境**：\n - CALVIN模拟环境：评估长视野任务，包括ABC→D和ABCD→D设置。\n - LIBERO模拟环境：评估空间推理、对象级泛化、目标条件行为和长视野组合任务。\n - SimplerEnv模拟环境：评估在WidowX和Google Robot平台上的迁移性和泛化能力。\n2. **评估指标**：\n - CALVIN：平均连续完成的子任务数（Avg. Len ↑）。\n - LIBERO：每个任务套件的平均成功率（%），在500个剧集上进行评估。\n - SimplerEnv：各种操作任务的平均成功率（%），包括抓取成功率（Grasp）和整体成功率。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>CASIA, BAAI, THU, HKISI</p>\n<h2>论文概述</h2>\n<p>该论文提出了UniVLA，一个统一的视觉-语言-动作（VLA）模型。与以往依赖外部视觉编码器且仅输出动作的VLA方法不同，UniVLA将视觉、语言和动作信号表示为离散令牌序列，并在统一的自回归框架中进行建模。这种统一的多模态范式支持灵活的多模态任务学习，特别是从大规模视频数据中学习。通过在后训练阶段结合世界模型，UniVLA能够从视频中捕捉因果动态，从而有效迁移到下游的策略学习，尤其是在长视野任务中。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出了UniVLA，这是第一个将视觉、语言和动作编码为共享词汇表中的离散令牌，并通过自回归序列学习联合建模的统一VLA模型。</li><li>统一的序列建模框架支持广泛的多模态任务。通过研究不同的后训练策略，证明了世界模型可以有效地从视频数据中学习时间动态，显著提升下游策略学习的性能和效率，尤其是在长视野和分布外场景中。</li><li>模型在多个模拟基准测试中取得了最先进的性能，并支持大规模视频训练的开源VLA方法。进一步探索了其在空间推理和视频预测等不同模态的能力，并展示了其在自动驾驶场景中的有效迁移，突显了其在通用具身智能方面的潜力。</li></ol>\n<h2>论文方法描述</h2>\n<ol><li><strong>统一多模态模型</strong>：视觉、语言和动作信号被转换为离散令牌，并使用共享词汇表进行自回归建模。图像通过VQ分词器进行离散化，动作通过FAST分词器使用离散余弦变换（DCT）进行编码。训练目标是标准的下一个令牌预测任务，使用交叉熵损失。</li><li><strong>统一多模态序列建模</strong>：采用基于马尔可夫链的自回归序列建模方法，其中观察和动作交错排列。这自然地包含了因果依赖关系，使模型能够对时间动态进行推理。</li></ol>\n<p> - <strong>世界模型（后训练）</strong>：通过仅对视觉令牌进行监督来学习环境的动态，使模型能够生成给定指令和观察状态条件的视觉预测。</p>\n<p> - <strong>策略学习（微调）</strong>：在交错格式中，采用两阶段训练策略。模型首先使用视觉语言（VL）对齐的检查点进行初始化，后训练阶段利用大规模机器人中心视频数据集来研究不同后训练策略对下游策略学习的影响，微调阶段则专注于动作学习以细化任务特定行为。</p>\n<h2>论文使用数据集和训练资源</h2>\n<p>1. <strong>数据集</strong>：</p>\n<p> - CALVIN：用于评估长视野、语言条件机器人操作的模拟基准。</p>\n<p> - LIBERO：终身机器人操作的综合套件，包含四个任务套件。</p>\n<p> - SimplerEnv：用于评估在真实世界视频数据上训练的模型的迁移性和泛化能力的模拟基准。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - 模型采用8.5B参数的纯自回归Transformer架构。</p>\n<p> - 后训练阶段使用622K视频，训练30K步，批量大小为64。</p>\n<p> - 微调阶段：CALVIN基准使用A100 GPU，批量大小192，训练8k步；LIBERO基准使用A100 GPU，批量大小192，训练8k步；SimplerEnv基准使用A100 GPU，批量大小128，训练20k步。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - CALVIN模拟环境：评估长视野任务，包括ABC→D和ABCD→D设置。</p>\n<p> - LIBERO模拟环境：评估空间推理、对象级泛化、目标条件行为和长视野组合任务。</p>\n<p> - SimplerEnv模拟环境：评估在WidowX和Google Robot平台上的迁移性和泛化能力。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - CALVIN：平均连续完成的子任务数（Avg. Len ↑）。</p>\n<p> - LIBERO：每个任务套件的平均成功率（%），在500个剧集上进行评估。</p>\n<p> - SimplerEnv：各种操作任务的平均成功率（%），包括抓取成功率（Grasp）和整体成功率。</p>"
  },
  {
    "date": "2025-06-24",
    "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation",
    "link": "http://arxiv.org/abs/2506.19816",
    "summary_markdown": "论文研究单位\n未在提供的文本中找到。\n\n论文概述\n该论文提出CronusVLA，一个用于高效且鲁棒机器人操作的统一框架。现有的视觉-语言-动作模型受限于单帧图像范式，无法充分利用多帧历史信息，并且直接处理多帧会带来巨大的计算开销和推理延迟。CronusVLA通过一个两阶段过程将单帧VLA模型扩展到多帧范式：(1) 单帧预训练，在大规模具身数据集上建立有效的视觉-语言基础；(2) 多帧后训练，通过特征分块聚合历史信息，将视觉-语言骨干网络的预测从离散标记调整为可学习特征。此外，论文还引入了SimplerEnv-OR基准，用于定量评估模型在时间和空间干扰下的观测鲁棒性。实验表明，CronusVLA在模拟和真实世界中实现了领先的性能和优越的鲁棒性。\n\n论文核心贡献点\n- 提出了一个名为CronusVLA的通用框架，通过单帧预训练和多帧后训练将VLA模型扩展到多帧范式。\n- 提出了SimplerEnv-OR，一个新基准，用于在观测干扰下对模型鲁棒性进行定量评估。\n- 通过广泛的模拟和真实世界实验，证明了CronusVLA的领先性能和强大的观测鲁棒性。\n\n论文方法描述\nCronusVLA采用一个两阶段训练方法。首先是单帧预训练，在大规模异构数据集（如OXE）上训练一个基本的单帧VLA模型，通过自回归预测离散的动作标记。然后是多帧后训练，引入可学习特征来代替离散动作标记，并对高质量跨具身数据集（如Bridge-v2和Fractal）进行训练。该方法核心是特征分块，将来自多个历史帧的可学习特征聚合成一个块。一个基于DiT的跨帧解码器通过特征调节器和交叉注意力机制来解码这个特征块，以预测动作序列。为了保护预训练的单帧感知能力，论文采用了多帧正则化，将历史特征作为辅助输入，其梯度不回传到视觉-语言骨干网络。\n\n论文使用数据集和训练资源\n- 数据集：\n - 预训练：OXE等大规模异构具身数据集。\n - 后训练：Bridge-v2和Fractal数据集，包含约148k个片段和5M个多帧片段。\n - 评估：SimplerEnv、LIBERO、SimplerEnv-OR以及真实世界收集的数据。\n- 训练资源：所有实验均基于A100 GPU。\n\n论文使用的评估环境和评估指标\n- 评估环境：\n - 模拟环境：SimplerEnv（包含Google Robot和WidowX Robot两种设置）、LIBERO（包含Spatial, Object, Goal, Long四种任务套件）、SimplerEnv-OR（用于鲁棒性测试）。\n - 真实世界环境：Franka Research 3机器人平台。\n- 评估指标：\n - 成功率：任务成功的试验百分比，在SimplerEnv、LIBERO和真实世界实验中使用。\n - 鲁棒性得分：在SimplerEnv-OR中使用，定义为 R-Score = 100 * (SR_i / SR)，其中SR是原始成功率，SR_i是干扰下的成功率。\n - 推理速度：以Hz为单位，用于评估模型效率。",
    "summary_html": "<p>论文研究单位</p>\n<p>未在提供的文本中找到。</p>\n\n<p>论文概述</p>\n<p>该论文提出CronusVLA，一个用于高效且鲁棒机器人操作的统一框架。现有的视觉-语言-动作模型受限于单帧图像范式，无法充分利用多帧历史信息，并且直接处理多帧会带来巨大的计算开销和推理延迟。CronusVLA通过一个两阶段过程将单帧VLA模型扩展到多帧范式：(1) 单帧预训练，在大规模具身数据集上建立有效的视觉-语言基础；(2) 多帧后训练，通过特征分块聚合历史信息，将视觉-语言骨干网络的预测从离散标记调整为可学习特征。此外，论文还引入了SimplerEnv-OR基准，用于定量评估模型在时间和空间干扰下的观测鲁棒性。实验表明，CronusVLA在模拟和真实世界中实现了领先的性能和优越的鲁棒性。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了一个名为CronusVLA的通用框架，通过单帧预训练和多帧后训练将VLA模型扩展到多帧范式。</li><li>提出了SimplerEnv-OR，一个新基准，用于在观测干扰下对模型鲁棒性进行定量评估。</li><li>通过广泛的模拟和真实世界实验，证明了CronusVLA的领先性能和强大的观测鲁棒性。</li></ul>\n\n<p>论文方法描述</p>\n<p>CronusVLA采用一个两阶段训练方法。首先是单帧预训练，在大规模异构数据集（如OXE）上训练一个基本的单帧VLA模型，通过自回归预测离散的动作标记。然后是多帧后训练，引入可学习特征来代替离散动作标记，并对高质量跨具身数据集（如Bridge-v2和Fractal）进行训练。该方法核心是特征分块，将来自多个历史帧的可学习特征聚合成一个块。一个基于DiT的跨帧解码器通过特征调节器和交叉注意力机制来解码这个特征块，以预测动作序列。为了保护预训练的单帧感知能力，论文采用了多帧正则化，将历史特征作为辅助输入，其梯度不回传到视觉-语言骨干网络。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：</li></ul>\n<p> - 预训练：OXE等大规模异构具身数据集。</p>\n<p> - 后训练：Bridge-v2和Fractal数据集，包含约148k个片段和5M个多帧片段。</p>\n<p> - 评估：SimplerEnv、LIBERO、SimplerEnv-OR以及真实世界收集的数据。</p>\n<ul><li>训练资源：所有实验均基于A100 GPU。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：</li></ul>\n<p> - 模拟环境：SimplerEnv（包含Google Robot和WidowX Robot两种设置）、LIBERO（包含Spatial, Object, Goal, Long四种任务套件）、SimplerEnv-OR（用于鲁棒性测试）。</p>\n<p> - 真实世界环境：Franka Research 3机器人平台。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 成功率：任务成功的试验百分比，在SimplerEnv、LIBERO和真实世界实验中使用。</p>\n<p> - 鲁棒性得分：在SimplerEnv-OR中使用，定义为 R-Score = 100 * (SR_i / SR)，其中SR是原始成功率，SR_i是干扰下的成功率。</p>\n<p> - 推理速度：以Hz为单位，用于评估模型效率。</p>"
  },
  {
    "date": "2025-06-23",
    "title": "MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis",
    "link": "http://arxiv.org/abs/2506.18897",
    "summary_markdown": "```markdown\n### 论文研究单位\n论文作者来自多个机构，包括编号1、2、3所属的学术研究机构和企业研究部门（具体机构名称未在HTML中明确列出）。\n### 论文概述\n论文提出MinD（Manipulate in Dream）框架，一个双系统世界模型，用于解决视频生成模型在机器人实时控制中的应用挑战。现有方法将VGMs仅用作特征提取器，未能充分利用其预测未来状态的能力。MinD通过结合低频视频生成和高频动作策略，实现实时规划和隐式风险分析，在RL-Bench上达到63%成功率，真实Franka机器人上达到60%成功率，以11.3 FPS运行。\n### 论文核心贡献点\n1. 提出MinD双系统扩散世界模型，统一低频视频想象过程与高频动作策略。\n2. 引入DiffMatcher模块，采用扩散强制（diffusion-forcing）协同训练策略，对齐异步视频和动作扩散过程。\n3. 实现最先进性能（RL-Bench 63%、真实任务60%），并能提前识别74%的潜在任务失败，为安全监控提供新范式。\n### 论文方法描述\n- 分层扩散框架：包括低频视频生成器（LoDiff-Visual）生成未来场景，高频动作策略（HiDiff-Policy）输出实时动作，通过DiffMatcher模块连接两者。\n- 异步协同训练策略：联合优化三个损失函数（视频损失、动作损失、对齐损失），其中对齐损失强制DiffMatcher学习噪声不变表示。\n- 单步预测推理：LoDiff-Visual仅执行单步去噪生成潜在特征，经DiffMatcher映射后 conditioning HiDiff-Policy，实现高效动作生成。\n- 隐式风险评估：通过分析生成视频的潜在特征分布和视觉失败案例，预测任务执行风险。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：RT-1、Robomind、OXE的混合数据集。\n - 微调：RL-Bench（1000条轨迹，7任务）和真实世界Franka机器人任务（每任务100条演示）。\n- **训练资源**：\n - 硬件：4×A100 GPU。\n - 软件：PyTorch 2.5.1，CUDA 12.1。\n - 参数：AdamW优化器，学习率2e-5，批次大小16，预训练50k步，微调10k步。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 模拟：RL-Bench（CoppeliaSim环境，Franka Panda机器人）。\n - 真实世界：Franka Research 3机器人，配备前视和腕视相机。\n- **评估指标**：\n - 任务成功率（Success Rate）：主要指标，衡量任务完成百分比。\n - Fréchet Video Distance（FVD）：评估生成视频质量。\n - 推理速度（FPS）：衡量实时控制能力。\n - 风险识别率：通过生成视频预测失败的能力。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>论文作者来自多个机构，包括编号1、2、3所属的学术研究机构和企业研究部门（具体机构名称未在HTML中明确列出）。</p>\n<h3>论文概述</h3>\n<p>论文提出MinD（Manipulate in Dream）框架，一个双系统世界模型，用于解决视频生成模型在机器人实时控制中的应用挑战。现有方法将VGMs仅用作特征提取器，未能充分利用其预测未来状态的能力。MinD通过结合低频视频生成和高频动作策略，实现实时规划和隐式风险分析，在RL-Bench上达到63%成功率，真实Franka机器人上达到60%成功率，以11.3 FPS运行。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出MinD双系统扩散世界模型，统一低频视频想象过程与高频动作策略。</li><li>引入DiffMatcher模块，采用扩散强制（diffusion-forcing）协同训练策略，对齐异步视频和动作扩散过程。</li><li>实现最先进性能（RL-Bench 63%、真实任务60%），并能提前识别74%的潜在任务失败，为安全监控提供新范式。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li>分层扩散框架：包括低频视频生成器（LoDiff-Visual）生成未来场景，高频动作策略（HiDiff-Policy）输出实时动作，通过DiffMatcher模块连接两者。</li><li>异步协同训练策略：联合优化三个损失函数（视频损失、动作损失、对齐损失），其中对齐损失强制DiffMatcher学习噪声不变表示。</li><li>单步预测推理：LoDiff-Visual仅执行单步去噪生成潜在特征，经DiffMatcher映射后 conditioning HiDiff-Policy，实现高效动作生成。</li><li>隐式风险评估：通过分析生成视频的潜在特征分布和视觉失败案例，预测任务执行风险。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：RT-1、Robomind、OXE的混合数据集。</p>\n<p> - 微调：RL-Bench（1000条轨迹，7任务）和真实世界Franka机器人任务（每任务100条演示）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 硬件：4×A100 GPU。</p>\n<p> - 软件：PyTorch 2.5.1，CUDA 12.1。</p>\n<p> - 参数：AdamW优化器，学习率2e-5，批次大小16，预训练50k步，微调10k步。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 模拟：RL-Bench（CoppeliaSim环境，Franka Panda机器人）。</p>\n<p> - 真实世界：Franka Research 3机器人，配备前视和腕视相机。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 任务成功率（Success Rate）：主要指标，衡量任务完成百分比。</p>\n<p> - Fréchet Video Distance（FVD）：评估生成视频质量。</p>\n<p> - 推理速度（FPS）：衡量实时控制能力。</p>\n<p> - 风险识别率：通过生成视频预测失败的能力。</p>\n<p>```</p>"
  },
  {
    "date": "2025-06-22",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation",
    "link": "http://arxiv.org/abs/2506.18088",
    "summary_markdown": "论文研究单位\nMoE key Lab of Artificial Intelligence, AI Institute, SJTU, HKU MMLab, Shanghai AI Lab, D-Robotics, SZU, THU, TeleAI, FDU, USTC, SUSTech, SYSU, CSU, NEU, HKU-SH ICRC, NJU, Lumina EAI\n\n论文概述\nRoboTwin 2.0是一个可扩展的仿真框架，旨在自动化、大规模生成用于鲁棒双臂机器人操作的高质量、多样化且现实的数据，并提供统一的评估协议。该框架解决了现有合成数据集在新型任务数据生成效率和仿真环境真实性方面的不足，通过多模态大语言模型与仿真反馈循环相结合的方法，生成专家级轨迹数据，并采用系统性域随机化策略增强策略鲁棒性。\n\n论文核心贡献点\n1. 开发了自动化专家数据生成框架，结合多模态大语言模型与仿真反馈循环确保高质量专家轨迹\n2. 提出系统性域随机化策略，通过增加数据多样性增强策略鲁棒性和仿真到现实的泛化能力\n3. 引入具身感知抓取调整机制，基于物体功能属性生成特定机器人操作候选方案\n4. 发布RoboTwin-OD资产库、大规模预收集多具身域随机轨迹数据集、可扩展双臂数据生成器和标准化评估基准\n\n论文方法描述\n1. 专家代码生成：使用多模态大语言模型生成初始Python程序，通过模拟执行和视觉语言模型观察进行迭代优化，包含代码生成代理和VLM观察者的双代理架构\n2. 域随机化：沿五个维度实施系统性随机化 - 场景杂乱、背景纹理、光照条件、桌面高度和多样化语言指令\n3. 具身感知抓取适应：为每个物体标注候选操作姿态，覆盖多个抓取轴和接近方向，并应用偏向高臂可达性方向的角扰动\n\n论文使用数据集和训练资源\n1. 数据集：RoboTwin-OD包含731个物体实例（147个类别），其中534个实例通过RGB到3D重建生成，153个来自Objaverse，44个来自SAPIEN PartNet-Mobility\n2. 预收集数据：超过100,000个双臂操作轨迹，跨越50个任务和5个机器人平台\n3. 训练资源：使用多模态大语言模型进行代码生成，视觉语言模型进行执行监控，Stable Diffusion v2生成11,000个高质量纹理\n\n论文使用的评估环境和评估指标\n1. 评估环境：仿真环境使用Aloha-AgileX、Piper、Franka、UR5、ARX-X5五种双臂机器人平台；真实环境使用COBOT-Magic双臂平台\n2. 评估指标：\n - ASR（Average Success Rate）：平均成功率\n - Top5-ASR：前5候选程序成功率\n - CR-Iter：平均精炼迭代次数\n - Token：生成策略代码的平均token数\n - 任务成功率：在清洁和域随机化条件下的单任务成功率",
    "summary_html": "<p>论文研究单位</p>\n<p>MoE key Lab of Artificial Intelligence, AI Institute, SJTU, HKU MMLab, Shanghai AI Lab, D-Robotics, SZU, THU, TeleAI, FDU, USTC, SUSTech, SYSU, CSU, NEU, HKU-SH ICRC, NJU, Lumina EAI</p>\n\n<p>论文概述</p>\n<p>RoboTwin 2.0是一个可扩展的仿真框架，旨在自动化、大规模生成用于鲁棒双臂机器人操作的高质量、多样化且现实的数据，并提供统一的评估协议。该框架解决了现有合成数据集在新型任务数据生成效率和仿真环境真实性方面的不足，通过多模态大语言模型与仿真反馈循环相结合的方法，生成专家级轨迹数据，并采用系统性域随机化策略增强策略鲁棒性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>开发了自动化专家数据生成框架，结合多模态大语言模型与仿真反馈循环确保高质量专家轨迹</li><li>提出系统性域随机化策略，通过增加数据多样性增强策略鲁棒性和仿真到现实的泛化能力</li><li>引入具身感知抓取调整机制，基于物体功能属性生成特定机器人操作候选方案</li><li>发布RoboTwin-OD资产库、大规模预收集多具身域随机轨迹数据集、可扩展双臂数据生成器和标准化评估基准</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>专家代码生成：使用多模态大语言模型生成初始Python程序，通过模拟执行和视觉语言模型观察进行迭代优化，包含代码生成代理和VLM观察者的双代理架构</li><li>域随机化：沿五个维度实施系统性随机化 - 场景杂乱、背景纹理、光照条件、桌面高度和多样化语言指令</li><li>具身感知抓取适应：为每个物体标注候选操作姿态，覆盖多个抓取轴和接近方向，并应用偏向高臂可达性方向的角扰动</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：RoboTwin-OD包含731个物体实例（147个类别），其中534个实例通过RGB到3D重建生成，153个来自Objaverse，44个来自SAPIEN PartNet-Mobility</li><li>预收集数据：超过100,000个双臂操作轨迹，跨越50个任务和5个机器人平台</li><li>训练资源：使用多模态大语言模型进行代码生成，视觉语言模型进行执行监控，Stable Diffusion v2生成11,000个高质量纹理</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：仿真环境使用Aloha-AgileX、Piper、Franka、UR5、ARX-X5五种双臂机器人平台；真实环境使用COBOT-Magic双臂平台</li><li>评估指标：</li></ol>\n<p> - ASR（Average Success Rate）：平均成功率</p>\n<p> - Top5-ASR：前5候选程序成功率</p>\n<p> - CR-Iter：平均精炼迭代次数</p>\n<p> - Token：生成策略代码的平均token数</p>\n<p> - 任务成功率：在清洁和域随机化条件下的单任务成功率</p>"
  },
  {
    "date": "2025-06-21",
    "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.17811",
    "summary_markdown": "论文研究单位\n斯坦福大学、加州大学伯克利分校、NVIDIA研究院\n论文概述\n本文探讨了如何通过测试时计算扩展来提升视觉-语言-动作模型在非结构化环境中的鲁棒性和泛化能力。作者首先通过实验证明了动作误差与生成样本数量之间存在指数化幂律关系，揭示了推理时缩放定律的存在。基于此发现，提出了RoboMonkey框架，该框架在部署时从VLA采样一组动作，应用高斯扰动和多数投票来构建动作提议分布，然后使用一个基于VLM的验证器来选择最优动作。此外，本文还提出了一个用于训练此类VLM动作验证器的可扩展合成数据生成流程。通过广泛的模拟和硬件实验，证明了RoboMonkey能显著提升现有VLA的性能。\n论文核心贡献点\n1. 提出了高效的采样方法，并证明了动作误差与样本数量之间的近似幂律关系。\n2. 提出了一个可扩展的流程，用于自动生成合成动作偏好并训练基于VLM的动作验证器。\n3. 展示了所提出的测试时缩放框架能显著提升VLA性能，在真实世界的分布外任务上实现了25%的绝对提升，在分布内SIMPLER环境中提升了9%。\n4. 证明了在适应新机器人设置时，同时微调VLA和动作验证器比仅微调VLA在LIBERO-Long基准上性能高出7%。\n论文方法描述\nRoboMonkey框架包含两个阶段。第一阶段是训练动作验证器：从一个模仿学习数据集中，使用通用机器人策略为每个状态采样N个候选动作，通过聚类将其减少到K个代表性动作，构建K选2的合成动作比较，并根据每个采样动作与真实动作之间的均方根误差（RMSE）分配偏好，然后用这个合成偏好数据集微调一个基于VLM的动作验证器。第二阶段是测试时计算扩展：在部署时，从通用机器人策略采样N个初始动作，对这些动作的平移和旋转分量拟合一个高斯分布，使用多数投票确定夹爪状态，这创建了一个动作提议分布，可以从中高效地采样候选动作。最后，使用微调的基于VLM的验证器评估这些K个候选动作并选择最优动作。\n论文使用数据集和训练资源\n数据集：Bridge V2数据集。\n训练资源：论文未明确提及，但提到了使用7B的VLM作为动作验证器的基础模型。\n论文使用的评估环境和评估指标\n评估环境：SIMPLER环境（分布内任务）、真实世界机器人实验（分布外任务）、LIBERO-Long基准（用于新机器人设置的适应）。\n评估指标：归一化均方根误差（RMSE）、任务成功率（绝对百分比提升）。",
    "summary_html": "<p>论文研究单位</p>\n<p>斯坦福大学、加州大学伯克利分校、NVIDIA研究院</p>\n<p>论文概述</p>\n<p>本文探讨了如何通过测试时计算扩展来提升视觉-语言-动作模型在非结构化环境中的鲁棒性和泛化能力。作者首先通过实验证明了动作误差与生成样本数量之间存在指数化幂律关系，揭示了推理时缩放定律的存在。基于此发现，提出了RoboMonkey框架，该框架在部署时从VLA采样一组动作，应用高斯扰动和多数投票来构建动作提议分布，然后使用一个基于VLM的验证器来选择最优动作。此外，本文还提出了一个用于训练此类VLM动作验证器的可扩展合成数据生成流程。通过广泛的模拟和硬件实验，证明了RoboMonkey能显著提升现有VLA的性能。</p>\n<p>论文核心贡献点</p>\n<ol><li>提出了高效的采样方法，并证明了动作误差与样本数量之间的近似幂律关系。</li><li>提出了一个可扩展的流程，用于自动生成合成动作偏好并训练基于VLM的动作验证器。</li><li>展示了所提出的测试时缩放框架能显著提升VLA性能，在真实世界的分布外任务上实现了25%的绝对提升，在分布内SIMPLER环境中提升了9%。</li><li>证明了在适应新机器人设置时，同时微调VLA和动作验证器比仅微调VLA在LIBERO-Long基准上性能高出7%。</li></ol>\n<p>论文方法描述</p>\n<p>RoboMonkey框架包含两个阶段。第一阶段是训练动作验证器：从一个模仿学习数据集中，使用通用机器人策略为每个状态采样N个候选动作，通过聚类将其减少到K个代表性动作，构建K选2的合成动作比较，并根据每个采样动作与真实动作之间的均方根误差（RMSE）分配偏好，然后用这个合成偏好数据集微调一个基于VLM的动作验证器。第二阶段是测试时计算扩展：在部署时，从通用机器人策略采样N个初始动作，对这些动作的平移和旋转分量拟合一个高斯分布，使用多数投票确定夹爪状态，这创建了一个动作提议分布，可以从中高效地采样候选动作。最后，使用微调的基于VLM的验证器评估这些K个候选动作并选择最优动作。</p>\n<p>论文使用数据集和训练资源</p>\n<p>数据集：Bridge V2数据集。</p>\n<p>训练资源：论文未明确提及，但提到了使用7B的VLM作为动作验证器的基础模型。</p>\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：SIMPLER环境（分布内任务）、真实世界机器人实验（分布外任务）、LIBERO-Long基准（用于新机器人设置的适应）。</p>\n<p>评估指标：归一化均方根误差（RMSE）、任务成功率（绝对百分比提升）。</p>"
  },
  {
    "date": "2025-06-21",
    "title": "RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.17639",
    "summary_markdown": "论文研究单位\n上海交通大学机械工程学院\n\n论文概述\n论文提出RLRC，一种用于压缩视觉-语言-动作（VLA）模型的三阶段恢复方法。VLA模型因参数量大和推理延迟高，难以部署到资源受限的机器人平台。RLRC通过结构化剪枝、基于监督微调（SFT）和强化学习（RL）的性能恢复，以及可选的4位量化，实现高达8倍的内存减少和2.3倍的推理吞吐量提升，同时保持或超过原始VLA的任务成功率。\n\n论文核心贡献点\n- 探索了通用模型压缩技术（量化、剪枝、知识蒸馏）在VLA中的应用，提供了性能权衡的实证见解。\n- 提出了一种结合结构化剪枝、监督微调、强化学习和训练后量化的新压缩框架，实现高压缩率的同时保持甚至提高任务精度。\n- 通过广泛实验验证方法有效性，展示了在资源受限条件下部署VLA的实用价值。\n\n论文方法描述\nRLRC包含三阶段：\n1. 结构化剪枝：针对VLA的LLM组件，采用LLM-Pruner框架，以90%剪枝率移除冗余结构，使用Taylor重要性准则，保留首尾层以维持稳定性。\n2. 性能恢复：首先用SFT在任务特定数据上微调剪枝后模型，恢复大部分性能；随后采用PPO算法进行RL微调，通过稀疏奖励函数（成功放置1.0、抓取0.1、其他0）优化长期回报，使用共享Transformer骨干的actor-critic设计。\n3. 进一步量化：可选应用4位量化，进一步压缩模型内存占用，适配资源受限设备部署。\n\n论文使用数据集和训练资源\n数据集：ManiSkill3的PutOnPlateInScene25Main任务套件，包含16个IND任务（训练时见过）和9个OOD任务（未见过）。\n训练资源：硬件测试使用单块NVIDIA RTX 5880 Ada；训练细节包括SFT约10k步，RL约0.6M步。\n\n论文使用的评估环境和评估指标\n评估环境：ManiSkill3模拟环境，基于8-DoF WidowX-250S机械臂。\n评估指标：任务成功率（主指标）、内存消耗、推理时间每步、动作吞吐量。",
    "summary_html": "<p>论文研究单位</p>\n<p>上海交通大学机械工程学院</p>\n\n<p>论文概述</p>\n<p>论文提出RLRC，一种用于压缩视觉-语言-动作（VLA）模型的三阶段恢复方法。VLA模型因参数量大和推理延迟高，难以部署到资源受限的机器人平台。RLRC通过结构化剪枝、基于监督微调（SFT）和强化学习（RL）的性能恢复，以及可选的4位量化，实现高达8倍的内存减少和2.3倍的推理吞吐量提升，同时保持或超过原始VLA的任务成功率。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>探索了通用模型压缩技术（量化、剪枝、知识蒸馏）在VLA中的应用，提供了性能权衡的实证见解。</li><li>提出了一种结合结构化剪枝、监督微调、强化学习和训练后量化的新压缩框架，实现高压缩率的同时保持甚至提高任务精度。</li><li>通过广泛实验验证方法有效性，展示了在资源受限条件下部署VLA的实用价值。</li></ul>\n\n<p>论文方法描述</p>\n<p>RLRC包含三阶段：</p>\n<ol><li>结构化剪枝：针对VLA的LLM组件，采用LLM-Pruner框架，以90%剪枝率移除冗余结构，使用Taylor重要性准则，保留首尾层以维持稳定性。</li><li>性能恢复：首先用SFT在任务特定数据上微调剪枝后模型，恢复大部分性能；随后采用PPO算法进行RL微调，通过稀疏奖励函数（成功放置1.0、抓取0.1、其他0）优化长期回报，使用共享Transformer骨干的actor-critic设计。</li><li>进一步量化：可选应用4位量化，进一步压缩模型内存占用，适配资源受限设备部署。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：ManiSkill3的PutOnPlateInScene25Main任务套件，包含16个IND任务（训练时见过）和9个OOD任务（未见过）。</p>\n<p>训练资源：硬件测试使用单块NVIDIA RTX 5880 Ada；训练细节包括SFT约10k步，RL约0.6M步。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：ManiSkill3模拟环境，基于8-DoF WidowX-250S机械臂。</p>\n<p>评估指标：任务成功率（主指标）、内存消耗、推理时间每步、动作吞吐量。</p>"
  },
  {
    "date": "2025-06-21",
    "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.17561",
    "summary_markdown": "论文研究单位\n新加坡国立大学、复旦大学、清华大学、南洋理工大学\n\n论文概述\n该论文提出了VLA-OS模型系列，用于系统研究视觉-语言-动作模型中任务规划的不同表示和范式。论文通过统一的架构设计，隔离了网络架构和训练数据的影响，对语言、视觉和图像预测等规划表示进行了全面比较，并在多种机器人操作任务上评估了不同VLA范式的性能。\n\n论文核心贡献点\n- 提出了统一的VLA-OS模型家族，包含三种主流VLA范式：ActionOnly-VLA、Integrated-VLA和Hierarchical-VLA\n- 设计了三种规划表示：语言推理、视觉推理和图像预测，并构建了相应的标注数据集\n- 在六个基准数据集上进行了系统性实验，包括刚体操作、3D泛化、真实世界可变形操作、灵巧操作和双臂操作任务\n- 得出了视觉基础的规划表示优于语言表示、分层范式性能最佳但成本更高的关键结论\n\n论文方法描述\nVLA-OS采用Qwen2.5作为LLM骨干，使用集成视觉编码器(DINOV2+SigLIP)。ActionOnly范式(VLA-OS-A)直接生成动作；Integrated范式(VLA-OS-I)在单一模型中同时进行规划和动作生成，支持隐式和显式规划；Hierarchical范式(VLA-OS-H)使用两个独立模型分别处理任务规划和策略学习。模型支持2D/3D输入，使用块级因果注意力机制，并设计了可插拔的规划头。\n\n论文使用数据集和训练资源\n数据集包括LIBERO系列(2D刚体操作)、COLOSSEUM(3D泛化)、真实世界可变形操作数据集、DexArt(灵巧操作)、FurnitureBench(长时序任务)和PerAct2(双臂操作)。训练资源为8块NVIDIA A100 80G GPU，模型规模从0.5B到7B参数不等。\n\n论文使用的评估环境和评估指标\n评估环境涵盖仿真环境(LIBERO、COLOSSEUM)和真实世界环境。评估指标包括任务成功率、规划分数(用于评估任务规划部分性能)、泛化能力(在扰动环境下的表现)、数据/模型可扩展性测试、持续学习能力和训练/推理效率。",
    "summary_html": "<p>论文研究单位</p>\n<p>新加坡国立大学、复旦大学、清华大学、南洋理工大学</p>\n\n<p>论文概述</p>\n<p>该论文提出了VLA-OS模型系列，用于系统研究视觉-语言-动作模型中任务规划的不同表示和范式。论文通过统一的架构设计，隔离了网络架构和训练数据的影响，对语言、视觉和图像预测等规划表示进行了全面比较，并在多种机器人操作任务上评估了不同VLA范式的性能。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了统一的VLA-OS模型家族，包含三种主流VLA范式：ActionOnly-VLA、Integrated-VLA和Hierarchical-VLA</li><li>设计了三种规划表示：语言推理、视觉推理和图像预测，并构建了相应的标注数据集</li><li>在六个基准数据集上进行了系统性实验，包括刚体操作、3D泛化、真实世界可变形操作、灵巧操作和双臂操作任务</li><li>得出了视觉基础的规划表示优于语言表示、分层范式性能最佳但成本更高的关键结论</li></ul>\n\n<p>论文方法描述</p>\n<p>VLA-OS采用Qwen2.5作为LLM骨干，使用集成视觉编码器(DINOV2+SigLIP)。ActionOnly范式(VLA-OS-A)直接生成动作；Integrated范式(VLA-OS-I)在单一模型中同时进行规划和动作生成，支持隐式和显式规划；Hierarchical范式(VLA-OS-H)使用两个独立模型分别处理任务规划和策略学习。模型支持2D/3D输入，使用块级因果注意力机制，并设计了可插拔的规划头。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集包括LIBERO系列(2D刚体操作)、COLOSSEUM(3D泛化)、真实世界可变形操作数据集、DexArt(灵巧操作)、FurnitureBench(长时序任务)和PerAct2(双臂操作)。训练资源为8块NVIDIA A100 80G GPU，模型规模从0.5B到7B参数不等。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境涵盖仿真环境(LIBERO、COLOSSEUM)和真实世界环境。评估指标包括任务成功率、规划分数(用于评估任务规划部分性能)、泛化能力(在扰动环境下的表现)、数据/模型可扩展性测试、持续学习能力和训练/推理效率。</p>"
  },
  {
    "date": "2025-06-19",
    "title": "CapsDT: Diffusion-Transformer for Capsule Robot Manipulation",
    "link": "http://arxiv.org/abs/2506.16263",
    "summary_markdown": "论文研究单位\n香港中文大学电子工程系，香港中文大学深圳研究院\n\n论文概述\n本文提出了CapsDT，一种用于胃内胶囊机器人操纵的扩散Transformer模型。该模型能够处理交错的视觉输入和文本指令，推断相应的机器人控制信号以促进内窥镜任务。此外，作者开发了一个胶囊内窥镜机器人系统，该系统由一个7自由度机器人手臂持磁控制的胶囊机器人组成，解决了不同难度的四个内窥镜任务，并在胃模拟器中创建了相应的胶囊机器人数据集。综合评估表明，CapsDT在各种内窥镜任务中实现了最先进的性能，在真实世界模拟操纵中达到26.25%的成功率。\n\n论文核心贡献点\n- 开发了一个内窥镜胶囊机器人系统，包括胶囊机器人、7自由度Kuka机器人手臂和胃模拟器，结合四个不同的内窥镜任务和任务特定的数据集，数据集包含超过1k条轨迹，是迄今为止最广泛的内窥镜胶囊机器人数据集之一\n- 提出了一种新颖的视觉-语言-动作扩散Transformer模型CapsDT，整合了机器人传感器、视觉数据和自然语言指令，有效地桥接了语言表达与感知线索，使内窥镜机器人能够生成最优动作\n\n论文方法描述\nCapsDT基于扩散模型，使用Transformer架构处理多模态输入。模型将不同的机器人动作空间集成到单一动作环境中。去噪输入包括本体感觉、噪声动作块、控制频率和扩散时间步；条件输入包括图片输入（来自夹爪相机和外部视角的图像集合）和语言输入；输出去噪后的动作。模型采用多模态输入编码器：机器人物理特征使用带有傅里叶特征的MLP编码；图像输入使用SigLIP视觉编码器提取紧凑表示；文本输入使用预训练的BART-large语言模型编码。在训练期间，随机掩码各种多模态输入以防止模型过度依赖任何单一输入。\n\n论文使用数据集和训练资源\n使用自收集的多任务胶囊机器人数据集，包含超过1000条轨迹。数据集涵盖四个复杂任务：基本导航、旋转操作、液体内导航、液体内带旋转的导航。每个数据点包含带有LLM生成解释的文本指令，以及夹爪和外部视角的双重视觉。数据收集使用KUKA LBR iiwa 7 R800机器人手臂，永磁体边长50mm，磁偶极矩为119.6A·m²，固定在机器人末端执行器上，距离工具中心点37.0mm。胶囊机器人尺寸为26mm×15mm，质量4.56g，磁偶极矩为0.126A·m²。\n\n论文使用的评估环境和评估指标\n在四种不同难度的内窥镜任务上评估：导航（低级）、旋转（低级）、液体内视角调整（中级）、液体内带旋转的视角调整（高级）。评估环境包括硅胶胃模型，模拟人类胃环境。使用成功率作为主要评估指标，衡量模型完成任务的能力。实验设置包括每个任务不同数量的演示：导航12个演示，旋转20个演示，视角调整每个水位10个演示，带旋转的视角调整每个水位5个演示。在真实世界模拟环境中，CapsDT达到了26.25%的整体成功率。",
    "summary_html": "<p>论文研究单位</p>\n<p>香港中文大学电子工程系，香港中文大学深圳研究院</p>\n\n<p>论文概述</p>\n<p>本文提出了CapsDT，一种用于胃内胶囊机器人操纵的扩散Transformer模型。该模型能够处理交错的视觉输入和文本指令，推断相应的机器人控制信号以促进内窥镜任务。此外，作者开发了一个胶囊内窥镜机器人系统，该系统由一个7自由度机器人手臂持磁控制的胶囊机器人组成，解决了不同难度的四个内窥镜任务，并在胃模拟器中创建了相应的胶囊机器人数据集。综合评估表明，CapsDT在各种内窥镜任务中实现了最先进的性能，在真实世界模拟操纵中达到26.25%的成功率。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>开发了一个内窥镜胶囊机器人系统，包括胶囊机器人、7自由度Kuka机器人手臂和胃模拟器，结合四个不同的内窥镜任务和任务特定的数据集，数据集包含超过1k条轨迹，是迄今为止最广泛的内窥镜胶囊机器人数据集之一</li><li>提出了一种新颖的视觉-语言-动作扩散Transformer模型CapsDT，整合了机器人传感器、视觉数据和自然语言指令，有效地桥接了语言表达与感知线索，使内窥镜机器人能够生成最优动作</li></ul>\n\n<p>论文方法描述</p>\n<p>CapsDT基于扩散模型，使用Transformer架构处理多模态输入。模型将不同的机器人动作空间集成到单一动作环境中。去噪输入包括本体感觉、噪声动作块、控制频率和扩散时间步；条件输入包括图片输入（来自夹爪相机和外部视角的图像集合）和语言输入；输出去噪后的动作。模型采用多模态输入编码器：机器人物理特征使用带有傅里叶特征的MLP编码；图像输入使用SigLIP视觉编码器提取紧凑表示；文本输入使用预训练的BART-large语言模型编码。在训练期间，随机掩码各种多模态输入以防止模型过度依赖任何单一输入。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用自收集的多任务胶囊机器人数据集，包含超过1000条轨迹。数据集涵盖四个复杂任务：基本导航、旋转操作、液体内导航、液体内带旋转的导航。每个数据点包含带有LLM生成解释的文本指令，以及夹爪和外部视角的双重视觉。数据收集使用KUKA LBR iiwa 7 R800机器人手臂，永磁体边长50mm，磁偶极矩为119.6A·m²，固定在机器人末端执行器上，距离工具中心点37.0mm。胶囊机器人尺寸为26mm×15mm，质量4.56g，磁偶极矩为0.126A·m²。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>在四种不同难度的内窥镜任务上评估：导航（低级）、旋转（低级）、液体内视角调整（中级）、液体内带旋转的视角调整（高级）。评估环境包括硅胶胃模型，模拟人类胃环境。使用成功率作为主要评估指标，衡量模型完成任务的能力。实验设置包括每个任务不同数量的演示：导航12个演示，旋转20个演示，视角调整每个水位10个演示，带旋转的视角调整每个水位5个演示。在真实世界模拟环境中，CapsDT达到了26.25%的整体成功率。</p>"
  },
  {
    "date": "2025-06-19",
    "title": "ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.16211",
    "summary_markdown": "论文研究单位\n清华大学, 北京通用人工智能国家重点实验室 (BIGAI), 北京大学, Astribot Inc.\n\n论文概述\nControlVLA是一个新颖的框架，旨在通过ControlNet风格的架构将预训练的视觉-语言-动作(VLA)模型与对象中心表示连接起来，实现高效的小样本微调。该方法在不覆盖先验知识的情况下引入对象中心条件，通过零初始化一组投影层，使它们能够逐步适应预训练的操纵策略。在真实世界的6个不同任务实验中，包括倒方块和折叠衣服，仅需要10-20个演示就实现了76.7%的成功率，显著优于传统方法（需要100多个演示才能达到可比成功率）。\n\n论文核心贡献点\n1. 提出了ControlVLA框架，首次将预训练VLA模型与对象中心表示通过ControlNet风格架构结合\n2. 设计了零初始化投影层机制，在不破坏预训练先验知识的前提下引入对象中心条件\n3. 实现了高效的小样本学习，仅需10-20个演示达到76.7%成功率，比传统方法减少5倍以上数据需求\n4. 验证了方法对长视野任务的扩展性，以及对未见对象和背景的鲁棒性\n\n论文方法描述\n1. VLA模型预训练：使用大型多任务操纵数据集预训练通用策略π_g，采用扩散变换器架构建模条件动作分布\n2. 对象中心表示构建：使用GroundingDINO和SAM2分割和跟踪任务相关对象，通过位置特征（正弦位置编码）和几何特征（CNN特征）编码对象表示\n3. ControlNet风格微调：引入双注意力结构，为对象中心观测添加额外的键值投影层K_z、V_z，初始化为零矩阵，实现逐步融入对象中心表示\n\n论文使用数据集和训练资源\n1. 预训练数据：公开大型多任务操纵数据集\n2. 评估数据：8个真实世界任务，包括刚性物体操作、软体操作、精确操作、铰链物体操作、流体操作和长视野任务\n3. 演示数量：每个任务使用10-20个演示进行微调\n4. 训练资源：使用扩散策略进行动作采样，采用DDIM加速实时控制\n\n论文使用的评估环境和评估指标\n1. 评估环境：真实世界机器人操作环境\n2. 任务类型：8种不同操纵任务，包括短视野任务（如RearrangeCup、OrganizeToy）和长视野任务（如OrganizeMultiObjs、ReplaceObjInDrawer）\n3. 评估指标：成功率（success rate），定义为任务完成的二元奖励\n4. 评估维度：包括基本任务性能、数据缩放效率、长视野任务表现、未见对象泛化能力和背景变化鲁棒性",
    "summary_html": "<p>论文研究单位</p>\n<p>清华大学, 北京通用人工智能国家重点实验室 (BIGAI), 北京大学, Astribot Inc.</p>\n\n<p>论文概述</p>\n<p>ControlVLA是一个新颖的框架，旨在通过ControlNet风格的架构将预训练的视觉-语言-动作(VLA)模型与对象中心表示连接起来，实现高效的小样本微调。该方法在不覆盖先验知识的情况下引入对象中心条件，通过零初始化一组投影层，使它们能够逐步适应预训练的操纵策略。在真实世界的6个不同任务实验中，包括倒方块和折叠衣服，仅需要10-20个演示就实现了76.7%的成功率，显著优于传统方法（需要100多个演示才能达到可比成功率）。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了ControlVLA框架，首次将预训练VLA模型与对象中心表示通过ControlNet风格架构结合</li><li>设计了零初始化投影层机制，在不破坏预训练先验知识的前提下引入对象中心条件</li><li>实现了高效的小样本学习，仅需10-20个演示达到76.7%成功率，比传统方法减少5倍以上数据需求</li><li>验证了方法对长视野任务的扩展性，以及对未见对象和背景的鲁棒性</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>VLA模型预训练：使用大型多任务操纵数据集预训练通用策略π_g，采用扩散变换器架构建模条件动作分布</li><li>对象中心表示构建：使用GroundingDINO和SAM2分割和跟踪任务相关对象，通过位置特征（正弦位置编码）和几何特征（CNN特征）编码对象表示</li><li>ControlNet风格微调：引入双注意力结构，为对象中心观测添加额外的键值投影层K_z、V_z，初始化为零矩阵，实现逐步融入对象中心表示</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>预训练数据：公开大型多任务操纵数据集</li><li>评估数据：8个真实世界任务，包括刚性物体操作、软体操作、精确操作、铰链物体操作、流体操作和长视野任务</li><li>演示数量：每个任务使用10-20个演示进行微调</li><li>训练资源：使用扩散策略进行动作采样，采用DDIM加速实时控制</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：真实世界机器人操作环境</li><li>任务类型：8种不同操纵任务，包括短视野任务（如RearrangeCup、OrganizeToy）和长视野任务（如OrganizeMultiObjs、ReplaceObjInDrawer）</li><li>评估指标：成功率（success rate），定义为任务完成的二元奖励</li><li>评估维度：包括基本任务性能、数据缩放效率、长视野任务表现、未见对象泛化能力和背景变化鲁棒性</li></ol>"
  },
  {
    "date": "2025-06-17",
    "title": "FormGym: Doing Paperwork with Agents",
    "link": "http://arxiv.org/abs/2506.14079",
    "summary_markdown": "论文研究单位\nColumbia University, Georgia Institute of Technology, Arklex.ai\n\n论文概述\n该论文探讨了使用智能体处理纯图像领域表格填写任务的挑战。由于缺乏OCR、排版PDF文本或DOM访问，该任务要求智能体具备多模态理解、信息检索和工具使用等多种能力。论文提出了一个包含432个字段、分布在55份文档和3个任务中的新基准测试FormGym，用于评估智能体的性能。研究发现，基线视觉语言智能体（VLA）的准确率大多低于1%，主要由于其定位能力差；而GUI智能体尽管成本高、延迟大，得分也仅在10.6%到68.0%之间。为解决定位瓶颈，论文还贡献了一个名为FieldFinder的工具，以辅助大语言模型识别表格上的文本放置位置。通过FieldFinder，所有模型在六种研究条件下的性能均有提升，最高增幅从2%提高到56%。\n\n论文核心贡献点\n1. 一个名为FormGym的基准测试，用于评估智能体在现实表格完成场景中的表现，揭示了当前VLAs在精确定位字段位置方面的困难。\n2. 一个名为FieldFinder的开放词汇表字段检测模型，证明了该模型能帮助VLAs克服空间推理的局限性。\n3. 论文计划将基准测试和FieldFinder模型在GitHub上公开发布。\n\n论文方法描述\n1. FormGym基准测试：\n * 文档：包含四类任务：汽车贷款任务（文本和文档传输两种）、数据库任务和FUNSD任务。任务要求智能体根据自然语言用户资料、SQL数据库或另一份已填写的源文档来填写目标表格。\n * 操作：为智能体提供了一套API操作，包括PlaceText(x, y, value)、DeleteText(x, y)、SignOrInitial(x, y, value)、QuerySql(query)和Terminate()。\n * 流程：评估在两种工作流程下进行：一次性完成和迭代式（最多10轮，允许纠错）。\n2. FieldFinder工具：\n * 架构：一个模块化工具，将语义理解与空间定位分离。VLA首先确定要填写的字段名称（如\"Date of Birth\"），然后调用FieldFinder来定位该字段在图像中的输入区域（如空行、单元格、复选框等）的边界框。\n * 模型与训练：FieldFinder是一个在Florence 2 Large模型（0.77B参数）上进行微调的视觉语言模型。使用FUNSD和XFUND数据集进行训练，通过水平内容感知填充技术自动移除已填写文本以生成训练数据。训练在1块NVIDIA A100 GPU上进行，耗时约20小时。\n\n论文使用数据集和训练资源\n1. 数据集：\n * FormGym基准数据集：包含55份文档（4份汽车贷款、2份商业银行、50份FUNSD），总计432个字段。涉及4个用户资料，共236个用户特征。\n * FieldFinder训练数据集：基于FUNSD和多语言XFUND文档理解数据集构建。\n2. 训练资源：FieldFinder模型的训练使用了1块NVIDIA A100 GPU，训练时间约为20小时。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n * 视觉语言模型（VLA）：通过API调用进行评估，提供API文档和示例。\n * GUI智能体：在浏览器内的在线照片编辑应用Photopea中进行评估，该应用界面与Photoshop类似。为GUI智能体设定了每页5分钟的时间限制，并提供了详细的界面使用说明。\n2. 评估指标：\n * 主要指标：字段准确率，计算方式为“正确填写的字段数 / 所有应填写的字段数”。忽略应为空白的字段。\n * 字段放置正确性判断：如果放置文本的中心点位于预定义的字段边界框内，则视为放置正确。\n * FieldFinder性能：使用交并比作为评估指标，其在FUNSD测试集上的IoU为20.9%。",
    "summary_html": "<p>论文研究单位</p>\n<p>Columbia University, Georgia Institute of Technology, Arklex.ai</p>\n\n<p>论文概述</p>\n<p>该论文探讨了使用智能体处理纯图像领域表格填写任务的挑战。由于缺乏OCR、排版PDF文本或DOM访问，该任务要求智能体具备多模态理解、信息检索和工具使用等多种能力。论文提出了一个包含432个字段、分布在55份文档和3个任务中的新基准测试FormGym，用于评估智能体的性能。研究发现，基线视觉语言智能体（VLA）的准确率大多低于1%，主要由于其定位能力差；而GUI智能体尽管成本高、延迟大，得分也仅在10.6%到68.0%之间。为解决定位瓶颈，论文还贡献了一个名为FieldFinder的工具，以辅助大语言模型识别表格上的文本放置位置。通过FieldFinder，所有模型在六种研究条件下的性能均有提升，最高增幅从2%提高到56%。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>一个名为FormGym的基准测试，用于评估智能体在现实表格完成场景中的表现，揭示了当前VLAs在精确定位字段位置方面的困难。</li><li>一个名为FieldFinder的开放词汇表字段检测模型，证明了该模型能帮助VLAs克服空间推理的局限性。</li><li>论文计划将基准测试和FieldFinder模型在GitHub上公开发布。</li></ol>\n\n<p>论文方法描述</p>\n<p>1. FormGym基准测试：</p>\n<p> * 文档：包含四类任务：汽车贷款任务（文本和文档传输两种）、数据库任务和FUNSD任务。任务要求智能体根据自然语言用户资料、SQL数据库或另一份已填写的源文档来填写目标表格。</p>\n<p> * 操作：为智能体提供了一套API操作，包括PlaceText(x, y, value)、DeleteText(x, y)、SignOrInitial(x, y, value)、QuerySql(query)和Terminate()。</p>\n<p> * 流程：评估在两种工作流程下进行：一次性完成和迭代式（最多10轮，允许纠错）。</p>\n<p>2. FieldFinder工具：</p>\n<p> * 架构：一个模块化工具，将语义理解与空间定位分离。VLA首先确定要填写的字段名称（如\"Date of Birth\"），然后调用FieldFinder来定位该字段在图像中的输入区域（如空行、单元格、复选框等）的边界框。</p>\n<p> * 模型与训练：FieldFinder是一个在Florence 2 Large模型（0.77B参数）上进行微调的视觉语言模型。使用FUNSD和XFUND数据集进行训练，通过水平内容感知填充技术自动移除已填写文本以生成训练数据。训练在1块NVIDIA A100 GPU上进行，耗时约20小时。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. 数据集：</p>\n<p> * FormGym基准数据集：包含55份文档（4份汽车贷款、2份商业银行、50份FUNSD），总计432个字段。涉及4个用户资料，共236个用户特征。</p>\n<p> * FieldFinder训练数据集：基于FUNSD和多语言XFUND文档理解数据集构建。</p>\n<p>2. 训练资源：FieldFinder模型的训练使用了1块NVIDIA A100 GPU，训练时间约为20小时。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> * 视觉语言模型（VLA）：通过API调用进行评估，提供API文档和示例。</p>\n<p> * GUI智能体：在浏览器内的在线照片编辑应用Photopea中进行评估，该应用界面与Photoshop类似。为GUI智能体设定了每页5分钟的时间限制，并提供了详细的界面使用说明。</p>\n<p>2. 评估指标：</p>\n<p> * 主要指标：字段准确率，计算方式为“正确填写的字段数 / 所有应填写的字段数”。忽略应为空白的字段。</p>\n<p> * 字段放置正确性判断：如果放置文本的中心点位于预定义的字段边界框内，则视为放置正确。</p>\n<p> * FieldFinder性能：使用交并比作为评估指标，其在FUNSD测试集上的IoU为20.9%。</p>"
  },
  {
    "date": "2025-06-16",
    "title": "GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics",
    "link": "http://arxiv.org/abs/2506.14009",
    "summary_markdown": "### 论文研究单位\n斯坦福大学机械工程系和航空航天系\n### 论文概述\n本文提出了GRaD-Nav++，一个轻量级的视觉-语言-行动(VLA)框架，能够在无人机机载计算硬件上完全运行，实时遵循自然语言指令。该策略通过可微强化学习(DiffRL)在照片般逼真的3D高斯分布(3DGS)模拟器中训练，能够从视觉和语言输入中高效学习低级控制。\n### 论文核心贡献点\n1. 提出了一种新颖的轻量级无人机飞行VLA框架，完全在无人机机载计算硬件上运行\n2. VLA策略使无人机能够基于高级自然语言指令完成复杂任务，展示了对未见任务的泛化能力和对不同环境条件的适应性\n3. 开发了新颖的多专家混合(MoE)动作模块，使用3DGS和DiffRL训练，在样本效率和任务成功率方面实现了最先进的性能\n### 论文方法描述\n- 使用预训练的CLIP模型进行高级场景理解和视觉指令匹配，冻结CLIP模型参数，微调一个线性层来融合视觉和文本嵌入\n- 策略网络采用MoE架构，包含两个专家子网络，每个时间步路由器激活top-k=2个专家\n- 每个专家是多层感知机，处理VLM特征向量和观测值\n- 使用3DGS进行场景表示，支持高保真视觉渲染\n- 使用可微动力学模型进行无人机仿真，包括角加速度、方向更新和线性加速度计算\n- 引入上下文估计网络来缩小模拟到现实的差距，提高策略鲁棒性\n### 论文使用数据集和训练资源\n- 在12个两阶段任务上训练，包括8个训练任务和4个零样本评估任务\n- 每个任务涉及选择正确的方向(通过、左、右、上)通过门，并识别飞向目标对象\n- 使用4个关键航点定义参考轨迹，通过A*规划连接\n- 训练在3DGS构建的仿真环境中进行\n- 使用DiffRL进行策略训练，结合3DGS渲染和可微动力学\n### 论文使用的评估环境和评估指标\n评估环境：\n- 多任务泛化实验：在训练过的任务和未见任务上评估\n- 多环境适应实验：在两个不同的3DGS环境中评估\n- 任务切换实验：评估环境变化时的适应能力\n- 在仿真环境和真实世界硬件部署中都进行测试\n\n评估指标：\n- 任务成功率作为主要评估指标\n- 在仿真环境中：训练任务成功率83%，未见任务成功率75%\n- 在真实硬件中：训练任务成功率67%，未见任务成功率50%\n- 多环境平均成功率：仿真81%，真实世界67%",
    "summary_html": "<h3>论文研究单位</h3>\n<p>斯坦福大学机械工程系和航空航天系</p>\n<h3>论文概述</h3>\n<p>本文提出了GRaD-Nav++，一个轻量级的视觉-语言-行动(VLA)框架，能够在无人机机载计算硬件上完全运行，实时遵循自然语言指令。该策略通过可微强化学习(DiffRL)在照片般逼真的3D高斯分布(3DGS)模拟器中训练，能够从视觉和语言输入中高效学习低级控制。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新颖的轻量级无人机飞行VLA框架，完全在无人机机载计算硬件上运行</li><li>VLA策略使无人机能够基于高级自然语言指令完成复杂任务，展示了对未见任务的泛化能力和对不同环境条件的适应性</li><li>开发了新颖的多专家混合(MoE)动作模块，使用3DGS和DiffRL训练，在样本效率和任务成功率方面实现了最先进的性能</li></ol>\n<h3>论文方法描述</h3>\n<ul><li>使用预训练的CLIP模型进行高级场景理解和视觉指令匹配，冻结CLIP模型参数，微调一个线性层来融合视觉和文本嵌入</li><li>策略网络采用MoE架构，包含两个专家子网络，每个时间步路由器激活top-k=2个专家</li><li>每个专家是多层感知机，处理VLM特征向量和观测值</li><li>使用3DGS进行场景表示，支持高保真视觉渲染</li><li>使用可微动力学模型进行无人机仿真，包括角加速度、方向更新和线性加速度计算</li><li>引入上下文估计网络来缩小模拟到现实的差距，提高策略鲁棒性</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>在12个两阶段任务上训练，包括8个训练任务和4个零样本评估任务</li><li>每个任务涉及选择正确的方向(通过、左、右、上)通过门，并识别飞向目标对象</li><li>使用4个关键航点定义参考轨迹，通过A*规划连接</li><li>训练在3DGS构建的仿真环境中进行</li><li>使用DiffRL进行策略训练，结合3DGS渲染和可微动力学</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：</p>\n<ul><li>多任务泛化实验：在训练过的任务和未见任务上评估</li><li>多环境适应实验：在两个不同的3DGS环境中评估</li><li>任务切换实验：评估环境变化时的适应能力</li><li>在仿真环境和真实世界硬件部署中都进行测试</li></ul>\n\n<p>评估指标：</p>\n<ul><li>任务成功率作为主要评估指标</li><li>在仿真环境中：训练任务成功率83%，未见任务成功率75%</li><li>在真实硬件中：训练任务成功率67%，未见任务成功率50%</li><li>多环境平均成功率：仿真81%，真实世界67%</li></ul>"
  },
  {
    "date": "2025-06-16",
    "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
    "link": "http://arxiv.org/abs/2506.13757",
    "summary_markdown": "论文研究单位\nUniversity of California, Los Angeles\n\n论文概述\n提出了AutoVLA，一种用于端到端自动驾驶的视觉-语言-动作（VLA）模型。该模型通过一个单一的回归生成框架统一了场景推理和动作生成，将连续轨迹标记为离散的、物理可行的动作。AutoVLA通过监督微调学习双思维模式：快速思维（直接生成轨迹）和慢速思维（结合思维链推理）。此外，论文还引入了基于分组相对策略优化（GRPO）的强化微调方法，以实现自适应推理，进一步提升规划性能和效率。实验表明，该方法在nuPlan、nuScenes、Waymo和CARLA等多个基准上均具有竞争力。\n\n论文核心贡献点\n- 提出了AutoVLA框架，将预训练的VLM与物理动作标记相结合，实现从原始视觉和语言输入到驾驶策略的直接学习。\n- 提出了基于GRPO的强化微调方法，使模型能够进行自适应推理，在简单场景中跳过不必要的推理，提升性能和效率。\n- 在包括开环和闭环测试的多个自动驾驶基准上验证了AutoVLA的优越性能。\n\n论文方法描述\n- 模型框架：基于Qwen2.5-VL-3B作为视觉语言骨干。输入为多视角相机图像、导航指令和车辆状态。模型输出推理文本和物理动作标记。\n- 动作标记化：将连续的车辆轨迹（每0.5秒一个动作）离散化为2048个离散的动作标记，每个标记代表短期的空间位置和航向变化。这使规划任务转变为语言模型的下一个标记预测问题。\n- 统一推理与动作：模型在单一的自回归Transformer框架内，根据场景复杂性自适应地在快速思维（直接输出动作）和慢速思维（先生成CoT推理再输出动作）之间切换。\n- 训练过程：分为两阶段。监督微调（SFT）阶段，使用轨迹-only数据和由大型VLM生成的CoT推理数据共同训练模型。强化微调（RFT）阶段，采用GRPO算法，结合驾驶任务奖励函数（如PDMS）和CoT长度惩罚，优化模型以提升规划准确性和运行效率。\n\n论文使用数据集和训练资源\n- 数据集：训练数据集包括nuPlan、Waymo端到端驾驶数据集、nuScenes和CARLA-Garage。推理数据通过自动化管道使用Qwen2.5-VL-72B生成，约45.6k条nuPlan数据和7.2k条Waymo数据，并整合了DriveLM数据集。\n- 训练资源：监督微调使用8块NVIDIA L40S GPU，训练5个epoch，有效批量大小为32。强化微调使用LoRA适配器，训练6000步。\n\n论文使用的评估环境和评估指标\n- 评估环境：在NAVSIM和nuScenes上进行开环评估，在CARLA的Bench2Drive基准上进行闭环评估，并在Waymo端到端驾驶数据集上报告结果。\n- 评估指标：NAVSIM使用预测驾驶模型评分（PDMS）；nuScenes使用L2距离和碰撞率；Waymo使用评分员反馈评分（RFS）和平均位移误差（ADE）；Bench2Drive使用驾驶分数、成功率、效率和舒适性。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of California, Los Angeles</p>\n\n<p>论文概述</p>\n<p>提出了AutoVLA，一种用于端到端自动驾驶的视觉-语言-动作（VLA）模型。该模型通过一个单一的回归生成框架统一了场景推理和动作生成，将连续轨迹标记为离散的、物理可行的动作。AutoVLA通过监督微调学习双思维模式：快速思维（直接生成轨迹）和慢速思维（结合思维链推理）。此外，论文还引入了基于分组相对策略优化（GRPO）的强化微调方法，以实现自适应推理，进一步提升规划性能和效率。实验表明，该方法在nuPlan、nuScenes、Waymo和CARLA等多个基准上均具有竞争力。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了AutoVLA框架，将预训练的VLM与物理动作标记相结合，实现从原始视觉和语言输入到驾驶策略的直接学习。</li><li>提出了基于GRPO的强化微调方法，使模型能够进行自适应推理，在简单场景中跳过不必要的推理，提升性能和效率。</li><li>在包括开环和闭环测试的多个自动驾驶基准上验证了AutoVLA的优越性能。</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>模型框架：基于Qwen2.5-VL-3B作为视觉语言骨干。输入为多视角相机图像、导航指令和车辆状态。模型输出推理文本和物理动作标记。</li><li>动作标记化：将连续的车辆轨迹（每0.5秒一个动作）离散化为2048个离散的动作标记，每个标记代表短期的空间位置和航向变化。这使规划任务转变为语言模型的下一个标记预测问题。</li><li>统一推理与动作：模型在单一的自回归Transformer框架内，根据场景复杂性自适应地在快速思维（直接输出动作）和慢速思维（先生成CoT推理再输出动作）之间切换。</li><li>训练过程：分为两阶段。监督微调（SFT）阶段，使用轨迹-only数据和由大型VLM生成的CoT推理数据共同训练模型。强化微调（RFT）阶段，采用GRPO算法，结合驾驶任务奖励函数（如PDMS）和CoT长度惩罚，优化模型以提升规划准确性和运行效率。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：训练数据集包括nuPlan、Waymo端到端驾驶数据集、nuScenes和CARLA-Garage。推理数据通过自动化管道使用Qwen2.5-VL-72B生成，约45.6k条nuPlan数据和7.2k条Waymo数据，并整合了DriveLM数据集。</li><li>训练资源：监督微调使用8块NVIDIA L40S GPU，训练5个epoch，有效批量大小为32。强化微调使用LoRA适配器，训练6000步。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：在NAVSIM和nuScenes上进行开环评估，在CARLA的Bench2Drive基准上进行闭环评估，并在Waymo端到端驾驶数据集上报告结果。</li><li>评估指标：NAVSIM使用预测驾驶模型评分（PDMS）；nuScenes使用L2距离和碰撞率；Waymo使用评分员反馈评分（RFS）和平均位移误差（ADE）；Bench2Drive使用驾驶分数、成功率、效率和舒适性。</li></ul>"
  },
  {
    "date": "2025-06-16",
    "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction",
    "link": "http://arxiv.org/abs/2506.13751",
    "summary_markdown": "论文研究单位\nUniversity of California Berkeley, Norwegian University of Science and Technology, Simon Fraser University, Carnegie Mellon University\n\n论文概述\n本文提出了LeVERB（Latent Vision-Language Encoded Robot Behavior），这是首个用于人形机器人全身控制的视觉-语言-动作模型。该研究还创建了第一个支持sim-to-real的视觉语言闭环基准测试LeVERB-Bench，包含超过150个任务，分为10个类别。LeVERB采用分层架构，将高级视觉语言策略与低级反应控制器解耦，实现了零样本的sim-to-real部署，在基准测试中取得了58.5%的整体成功率。\n\n论文核心贡献点\n1. 提出了LeVERB，首个用于人形机器人全身控制的视觉语言潜在动作模型\n2. 创建了LeVERB-Bench，第一个支持sim-to-real的视觉语言闭环基准测试\n3. 设计了基于CVAE的分层视觉语言策略，学习结构化潜在空间\n4. 实现了零样本sim-to-real部署，在真实人形机器人上验证了方法有效性\n5. 在基准测试中取得58.5%整体成功率，比朴素分层VLA实现高7.8倍\n\n论文方法描述\nLeVERB采用双过程架构：\n1. LeVERB-VL（System 2）：高级视觉语言策略，使用基于CVAE的架构，将视觉和语言输入映射到结构化潜在动作词汇空间\n2. LeVERB-A（System 1）：低级反应控制器，接收潜在指令并生成动力学级命令\n\n训练过程分为两个阶段：\n- 首先使用运动学重建训练视觉语言组件\n- 然后冻结其潜在空间，训练单独的动作模块\n\n论文使用数据集和训练资源\n- LeVERB-Bench数据集：154个轨迹，每个随机化100次，生成17.1小时光运动回放数据\n- 使用IsaacSim进行光线追踪渲染，实现逼真场景模拟\n- 训练使用2个NVIDIA Ada 6000 GPU，全局批大小512\n- System 2总参数量为102.56百万\n\n论文使用的评估环境和评估指标\n评估环境：\n1. LeVERB-Bench仿真环境：包含目标、干扰物和杂乱三种场景设置\n2. Unitree G1人形机器人：用于真实世界部署验证\n\n评估指标：\n- 任务成功率：在20个未见过的材料和物体组合场景中测试，报告成功百分比\n- 针对视觉导航（前/后）、语言任务（坐/站/运动）等子类别分别评估",
    "summary_html": "<p>论文研究单位</p>\n<p>University of California Berkeley, Norwegian University of Science and Technology, Simon Fraser University, Carnegie Mellon University</p>\n\n<p>论文概述</p>\n<p>本文提出了LeVERB（Latent Vision-Language Encoded Robot Behavior），这是首个用于人形机器人全身控制的视觉-语言-动作模型。该研究还创建了第一个支持sim-to-real的视觉语言闭环基准测试LeVERB-Bench，包含超过150个任务，分为10个类别。LeVERB采用分层架构，将高级视觉语言策略与低级反应控制器解耦，实现了零样本的sim-to-real部署，在基准测试中取得了58.5%的整体成功率。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了LeVERB，首个用于人形机器人全身控制的视觉语言潜在动作模型</li><li>创建了LeVERB-Bench，第一个支持sim-to-real的视觉语言闭环基准测试</li><li>设计了基于CVAE的分层视觉语言策略，学习结构化潜在空间</li><li>实现了零样本sim-to-real部署，在真实人形机器人上验证了方法有效性</li><li>在基准测试中取得58.5%整体成功率，比朴素分层VLA实现高7.8倍</li></ol>\n\n<p>论文方法描述</p>\n<p>LeVERB采用双过程架构：</p>\n<ol><li>LeVERB-VL（System 2）：高级视觉语言策略，使用基于CVAE的架构，将视觉和语言输入映射到结构化潜在动作词汇空间</li><li>LeVERB-A（System 1）：低级反应控制器，接收潜在指令并生成动力学级命令</li></ol>\n\n<p>训练过程分为两个阶段：</p>\n<ul><li>首先使用运动学重建训练视觉语言组件</li><li>然后冻结其潜在空间，训练单独的动作模块</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>LeVERB-Bench数据集：154个轨迹，每个随机化100次，生成17.1小时光运动回放数据</li><li>使用IsaacSim进行光线追踪渲染，实现逼真场景模拟</li><li>训练使用2个NVIDIA Ada 6000 GPU，全局批大小512</li><li>System 2总参数量为102.56百万</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ol><li>LeVERB-Bench仿真环境：包含目标、干扰物和杂乱三种场景设置</li><li>Unitree G1人形机器人：用于真实世界部署验证</li></ol>\n\n<p>评估指标：</p>\n<ul><li>任务成功率：在20个未见过的材料和物体组合场景中测试，报告成功百分比</li><li>针对视觉导航（前/后）、语言任务（坐/站/运动）等子类别分别评估</li></ul>"
  },
  {
    "date": "2025-06-16",
    "title": "CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding",
    "link": "http://arxiv.org/abs/2506.13725",
    "summary_markdown": "论文研究单位\nHKUST(GZ), Westlake University, Zhejiang University\n\n论文概述\n视觉-语言-动作（VLA）模型因其多模态理解和泛化能力而成为机器人学的重要研究方向，但其推理速度瓶颈严重制约了实际部署，尤其是在高频和灵巧操作任务中。虽然Jacobi解码作为一种替代方案被提出，但其加速效果有限。为解决此问题，论文提出CEED-VLA模型，通过一致性蒸馏训练使模型能够在单次迭代中预测多个正确的动作标记，并引入混合标签监督来减轻蒸馏过程中的误差累积。此外，论文识别出Jacobi解码中的低效迭代问题，并提出早期退出解码策略来放宽收敛条件，从而进一步提升平均推理效率。实验结果表明，该方法在不同基线上实现了超过4倍的推理加速，同时保持了较高的任务成功率。\n\n论文核心贡献点\n- 提出CEED-VLA，一种用于显著提升推理效率同时保持操作性能的通用加速方法。\n- 通过一致性蒸馏过程解锁模型的快速推理能力，并提出在自回归损失中采用混合标签监督来保持模型的操作性能。\n- 识别出Jacobi解码中的低效迭代是加速的瓶颈，并提出早期退出解码策略解决该问题，最终实现了4.1倍的加速和超过4.3倍的频率提升。\n\n论文方法描述\n方法主要包含教师模型和学生模型。教师模型是标准的预训练VLA（如OpenVLA和LLaVA-VLA），用于通过Jacobi解码生成动作序列并构建Jacobi轨迹数据集。学生模型（CEED-VLA）通过一致性蒸馏进行训练，其目标是将Jacobi轨迹中的任意中间状态直接映射到收敛的固定点。为实现此目标，训练过程包含两个损失函数：1）一致性损失，使用KL散度使学生模型的预测分布与教师模型在固定点的分布对齐；2）混合标签自回归损失，它自适应地在教师模型输出和真实标签之间选择监督信号，以防止模型性能偏离原始分布。推理时，采用早期退出解码策略，即不完全满足严格收敛条件便提前退出迭代，从而绕过低效迭代，提升整体速度。\n\n论文使用数据集和训练资源\n- 数据集：CALVIN和LIBERO模拟环境数据集，以及一个真实的机器人臂部署环境用于真实世界实验。训练所需的Jacobi轨迹数据集是通过在原始机器人数据集上运行教师模型生成的。\n- 训练资源：论文在LLaVA-VLA和OpenVLA这两个预训练VLA模型的基础上进行微调，但未明确提及所使用的具体硬件资源（如GPU型号）。\n\n论文使用的评估环境和评估指标\n- 评估环境：包括模拟环境（CALVIN和LIBERO基准）和真实世界环境（搭载真实机器人臂）。\n- 评估指标：主要指标是任务成功率，用以衡量操作性能；另一个核心指标是推理加速倍数，用于量化模型推理速度的提升程度。在真实世界实验中还评估了控制频率。",
    "summary_html": "<p>论文研究单位</p>\n<p>HKUST(GZ), Westlake University, Zhejiang University</p>\n\n<p>论文概述</p>\n<p>视觉-语言-动作（VLA）模型因其多模态理解和泛化能力而成为机器人学的重要研究方向，但其推理速度瓶颈严重制约了实际部署，尤其是在高频和灵巧操作任务中。虽然Jacobi解码作为一种替代方案被提出，但其加速效果有限。为解决此问题，论文提出CEED-VLA模型，通过一致性蒸馏训练使模型能够在单次迭代中预测多个正确的动作标记，并引入混合标签监督来减轻蒸馏过程中的误差累积。此外，论文识别出Jacobi解码中的低效迭代问题，并提出早期退出解码策略来放宽收敛条件，从而进一步提升平均推理效率。实验结果表明，该方法在不同基线上实现了超过4倍的推理加速，同时保持了较高的任务成功率。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出CEED-VLA，一种用于显著提升推理效率同时保持操作性能的通用加速方法。</li><li>通过一致性蒸馏过程解锁模型的快速推理能力，并提出在自回归损失中采用混合标签监督来保持模型的操作性能。</li><li>识别出Jacobi解码中的低效迭代是加速的瓶颈，并提出早期退出解码策略解决该问题，最终实现了4.1倍的加速和超过4.3倍的频率提升。</li></ul>\n\n<p>论文方法描述</p>\n<p>方法主要包含教师模型和学生模型。教师模型是标准的预训练VLA（如OpenVLA和LLaVA-VLA），用于通过Jacobi解码生成动作序列并构建Jacobi轨迹数据集。学生模型（CEED-VLA）通过一致性蒸馏进行训练，其目标是将Jacobi轨迹中的任意中间状态直接映射到收敛的固定点。为实现此目标，训练过程包含两个损失函数：1）一致性损失，使用KL散度使学生模型的预测分布与教师模型在固定点的分布对齐；2）混合标签自回归损失，它自适应地在教师模型输出和真实标签之间选择监督信号，以防止模型性能偏离原始分布。推理时，采用早期退出解码策略，即不完全满足严格收敛条件便提前退出迭代，从而绕过低效迭代，提升整体速度。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：CALVIN和LIBERO模拟环境数据集，以及一个真实的机器人臂部署环境用于真实世界实验。训练所需的Jacobi轨迹数据集是通过在原始机器人数据集上运行教师模型生成的。</li><li>训练资源：论文在LLaVA-VLA和OpenVLA这两个预训练VLA模型的基础上进行微调，但未明确提及所使用的具体硬件资源（如GPU型号）。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：包括模拟环境（CALVIN和LIBERO基准）和真实世界环境（搭载真实机器人臂）。</li><li>评估指标：主要指标是任务成功率，用以衡量操作性能；另一个核心指标是推理加速倍数，用于量化模型推理速度的提升程度。在真实世界实验中还评估了控制频率。</li></ul>"
  },
  {
    "date": "2025-06-16",
    "title": "ROSA: Harnessing Robot States for Vision-Language and Action Alignment",
    "link": "http://arxiv.org/abs/2506.13679",
    "summary_markdown": "### 论文研究单位\n中国科学技术大学, 南京大学, Dexmal\n### 论文概述\n该论文提出了ROSA，一种新的训练范式，旨在解决视觉-语言-动作模型在将高级视觉-语言理解与低级机器人物理动作对齐时存在的时空鸿沟问题。现有方法直接微调视觉-语言模型，但面临数据效率低下和对人工标注依赖性强的问题。ROSA通过引入一种自动收集的机器人状态估计数据作为辅助监督，增强模型对3D空间的理解和自我感知能力，从而提升模型在数据有限情况下的性能和泛化能力。实验在RLBench模拟环境和真实的WidowX机器人平台上验证了该方法的有效性。\n### 论文核心贡献点\n1. 提出了一种名为ROSA的新颖训练范式，它利用机器人状态估计数据来实现视觉-语言和动作空间之间更好的对齐。\n2. 提出了一种简单而有效的解决方案来创建机器人状态估计数据，在不需要额外人类收集工作的情况下，显著增强了VLA的数据效率。\n3. 在RLBench模拟和一个真实世界的WidowX平台上进行了广泛的实验，证明ROSA有效增强了当前的VLA模型，并实现了优于先前方法的性能。\n### 论文方法描述\nROSA方法的核心是将VLA模型的训练分解为两个互补部分：预测未来动作和估计当前机器人状态。\n1. **训练数据**：\n * **专家动作数据**：由人类操作员收集的轨迹数据，包含视觉观测、语言指令和对应的7自由度机器人动作（3D位置、3D欧拉角、1个夹爪开合状态）。\n * **机器人状态估计数据**：通过自动化脚本控制机器人在预定义环境中随机移动并记录观测与状态。这些数据同样包含7自由度的状态信息，并配以统一的语言指令，如“What is the current state of the robot?”，以实现与专家数据的格式统一和联合训练。\n2. **模型架构**：\n * 基于LLaVA架构，包含一个视觉编码器、一个投影器和一个大语言模型。\n * 视觉编码器使用CLIP ViT-L/14，投影器为两层MLP，大语言模型骨干为Qwen-2.5-7B。\n * 为了让LLM预测连续的动作和状态，采用线性量化的方法将连续值离散化为token，推理时再通过逆映射恢复为连续值。\n3. **训练目标**：\n * 对专家动作数据和机器人状态数据使用统一的训练目标，即下一个token预测的交叉熵损失。\n * 模型以1:4的比例混合两种数据进行训练，并完全微调所有层。\n### 论文使用数据集和训练资源\n1. **数据集**：\n * **RLBench**：用于模拟实验，包含12个任务，每个任务包含多个训练变体。\n * **真实机器人数据**：在WidowX 250S机器人上采集，包括4个已见任务和4个用于泛化评估的未见任务。\n2. **训练资源**：\n * 使用8块NVIDIA A100 GPU进行训练。\n * 基于Qwen-2.5-7B、CLIP ViT-L/14模型构建。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n * **RLBench模拟环境**：使用Franka Panda机器人，配备一个固定的前置RGB摄像头（336x336分辨率）。\n * **真实世界环境**：使用WidowX 250S机器人和一个Intel RealSense D435摄像头提供第三人称视角。\n2. **评估指标**：\n * 主要评估指标是**成功率**，即任务成功的episode或试验次数占总数的百分比。\n * 在RLBench上，每个任务评估25个episode，并重复三次，报告平均成功率。\n * 在真实机器人上，每个任务评估10次试验。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中国科学技术大学, 南京大学, Dexmal</p>\n<h3>论文概述</h3>\n<p>该论文提出了ROSA，一种新的训练范式，旨在解决视觉-语言-动作模型在将高级视觉-语言理解与低级机器人物理动作对齐时存在的时空鸿沟问题。现有方法直接微调视觉-语言模型，但面临数据效率低下和对人工标注依赖性强的问题。ROSA通过引入一种自动收集的机器人状态估计数据作为辅助监督，增强模型对3D空间的理解和自我感知能力，从而提升模型在数据有限情况下的性能和泛化能力。实验在RLBench模拟环境和真实的WidowX机器人平台上验证了该方法的有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种名为ROSA的新颖训练范式，它利用机器人状态估计数据来实现视觉-语言和动作空间之间更好的对齐。</li><li>提出了一种简单而有效的解决方案来创建机器人状态估计数据，在不需要额外人类收集工作的情况下，显著增强了VLA的数据效率。</li><li>在RLBench模拟和一个真实世界的WidowX平台上进行了广泛的实验，证明ROSA有效增强了当前的VLA模型，并实现了优于先前方法的性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>ROSA方法的核心是将VLA模型的训练分解为两个互补部分：预测未来动作和估计当前机器人状态。</p>\n<p>1. <strong>训练数据</strong>：</p>\n<p> * <strong>专家动作数据</strong>：由人类操作员收集的轨迹数据，包含视觉观测、语言指令和对应的7自由度机器人动作（3D位置、3D欧拉角、1个夹爪开合状态）。</p>\n<p> * <strong>机器人状态估计数据</strong>：通过自动化脚本控制机器人在预定义环境中随机移动并记录观测与状态。这些数据同样包含7自由度的状态信息，并配以统一的语言指令，如“What is the current state of the robot?”，以实现与专家数据的格式统一和联合训练。</p>\n<p>2. <strong>模型架构</strong>：</p>\n<p> * 基于LLaVA架构，包含一个视觉编码器、一个投影器和一个大语言模型。</p>\n<p> * 视觉编码器使用CLIP ViT-L/14，投影器为两层MLP，大语言模型骨干为Qwen-2.5-7B。</p>\n<p> * 为了让LLM预测连续的动作和状态，采用线性量化的方法将连续值离散化为token，推理时再通过逆映射恢复为连续值。</p>\n<p>3. <strong>训练目标</strong>：</p>\n<p> * 对专家动作数据和机器人状态数据使用统一的训练目标，即下一个token预测的交叉熵损失。</p>\n<p> * 模型以1:4的比例混合两种数据进行训练，并完全微调所有层。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：</p>\n<p> * <strong>RLBench</strong>：用于模拟实验，包含12个任务，每个任务包含多个训练变体。</p>\n<p> * <strong>真实机器人数据</strong>：在WidowX 250S机器人上采集，包括4个已见任务和4个用于泛化评估的未见任务。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> * 使用8块NVIDIA A100 GPU进行训练。</p>\n<p> * 基于Qwen-2.5-7B、CLIP ViT-L/14模型构建。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> * <strong>RLBench模拟环境</strong>：使用Franka Panda机器人，配备一个固定的前置RGB摄像头（336x336分辨率）。</p>\n<p> * <strong>真实世界环境</strong>：使用WidowX 250S机器人和一个Intel RealSense D435摄像头提供第三人称视角。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> * 主要评估指标是<strong>成功率</strong>，即任务成功的episode或试验次数占总数的百分比。</p>\n<p> * 在RLBench上，每个任务评估25个episode，并重复三次，报告平均成功率。</p>\n<p> * 在真实机器人上，每个任务评估10次试验。</p>"
  },
  {
    "date": "2025-06-15",
    "title": "SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration",
    "link": "http://arxiv.org/abs/2506.12723",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-11",
    "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.10100",
    "summary_markdown": "### 论文研究单位\n上海交通大学人工智能学院，哈尔滨工业大学，西安交通大学，电子科技大学\n### 论文概述\n论文提出EfficientVLA，一个无需训练的结构化推理加速框架，用于解决视觉-语言-动作（VLA）模型在推理阶段的计算和内存瓶颈。该方法通过系统性地消除多层冗余：基于层间相似性分析修剪语言模型中的冗余层；采用任务感知策略选择紧凑且多样化的视觉token；在扩散式动作头中缓存中间特征以减少时间冗余。实验表明，该方法在CogACT模型上实现1.93倍推理加速，FLOPs降至28.9%，成功率仅下降0.6%。\n### 论文核心贡献点\n1. 系统性分析扩散式VLA架构的计算瓶颈与多层冗余机制。\n2. 提出无需训练的EfficientVLA框架，集成三种冗余消除策略：语言层修剪、任务感知视觉token选择、动作头特征缓存。\n3. 设计时间相关性缓存机制，在扩散去噪过程中重用中间注意力与MLP计算。\n4. 在SIMPLER基准上验证有效性：实现1.93×加速，FLOPs降至28.9%，精度损失0.6%。\n### 论文方法描述\n1. **语言模型修剪**：通过计算层输入/输出隐藏状态的余弦相似度，定义重要性分数（公式1），修剪低分数的非连续层。\n2. **视觉token修剪**：量化任务相关性（基于视觉-语言交叉注意力分数），选择关键token后通过多样性增强（任务驱动+多样性驱动）平衡相关性与信息覆盖。\n3. **特征缓存**：利用DiT块中特征的时间连贯性，在扩散动作头中静态缓存N步中间特征，减少相邻步骤的重复计算。\n### 论文使用数据集和训练资源\n- **基础模型**：标准VLA模型CogACT\n- **评估数据集**：SIMPLER环境（仿真环境）的任务数据集\n- **训练资源**：方法无需额外训练，直接应用于预训练模型\n### 论文使用的评估环境和评估指标\n- **评估环境**：SIMPLER仿真环境\n- **评估指标**：推理速度（加速倍数）、FLOPs（计算量）、成功率（任务完成率）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>上海交通大学人工智能学院，哈尔滨工业大学，西安交通大学，电子科技大学</p>\n<h3>论文概述</h3>\n<p>论文提出EfficientVLA，一个无需训练的结构化推理加速框架，用于解决视觉-语言-动作（VLA）模型在推理阶段的计算和内存瓶颈。该方法通过系统性地消除多层冗余：基于层间相似性分析修剪语言模型中的冗余层；采用任务感知策略选择紧凑且多样化的视觉token；在扩散式动作头中缓存中间特征以减少时间冗余。实验表明，该方法在CogACT模型上实现1.93倍推理加速，FLOPs降至28.9%，成功率仅下降0.6%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>系统性分析扩散式VLA架构的计算瓶颈与多层冗余机制。</li><li>提出无需训练的EfficientVLA框架，集成三种冗余消除策略：语言层修剪、任务感知视觉token选择、动作头特征缓存。</li><li>设计时间相关性缓存机制，在扩散去噪过程中重用中间注意力与MLP计算。</li><li>在SIMPLER基准上验证有效性：实现1.93×加速，FLOPs降至28.9%，精度损失0.6%。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>语言模型修剪</strong>：通过计算层输入/输出隐藏状态的余弦相似度，定义重要性分数（公式1），修剪低分数的非连续层。</li><li><strong>视觉token修剪</strong>：量化任务相关性（基于视觉-语言交叉注意力分数），选择关键token后通过多样性增强（任务驱动+多样性驱动）平衡相关性与信息覆盖。</li><li><strong>特征缓存</strong>：利用DiT块中特征的时间连贯性，在扩散动作头中静态缓存N步中间特征，减少相邻步骤的重复计算。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>基础模型</strong>：标准VLA模型CogACT</li><li><strong>评估数据集</strong>：SIMPLER环境（仿真环境）的任务数据集</li><li><strong>训练资源</strong>：方法无需额外训练，直接应用于预训练模型</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：SIMPLER仿真环境</li><li><strong>评估指标</strong>：推理速度（加速倍数）、FLOPs（计算量）、成功率（任务完成率）</li></ul>"
  },
  {
    "date": "2025-06-11",
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.09937",
    "summary_markdown": "## 论文研究单位\n多伦多大学、多伦多大学机器人研究所、Vector研究所、丰田研究所。\n## 论文概述\n论文针对视觉-语言-动作（VLA）模型在部署于新任务时成功率有限的问题，提出了一个名为SAFE的多任务失败检测框架。SAFE旨在作为VLA等通用机器人策略的即插即用型失败检测器，通过分析VLA的内部特征来预测任务失败的可能性，从而允许机器人及时停止、回溯或寻求帮助。\n## 论文核心贡献点\n1. 通过分析VLA的潜在空间，发现不同任务中成功与失败轨迹的内部特征在特征空间中存在明显且一致的分离现象。\n2. 提出了SAFE框架，一个可扩展的、基于VLA内部特征的多任务失败检测方法。SAFE通过训练学习任务通用的失败表示，并使用保形预测（CP）校准阈值。\n3. 在多个VLA模型（OpenVLA, π₀, π₀-FAST）上进行了广泛的模拟和真实世界实验，并与多种基线方法比较，验证了SAFE的先进性能。\n## 论文方法描述\n1. **视觉分析**：使用t-SNE可视化VLA（如π₀-FAST）在LIBERO-10数据集上的特征，发现失败轨迹的特征会聚集在一个“失败区”，且该现象在不同任务间具有普遍性。\n2. **特征探测失败检测**：SAFE从VLA最后一层提取特征，并使用简单的MLP或LSTM网络将特征回归为一个标量失败分数。MLP模型对每个时间步的特征独立处理并累加分数，使用L1损失；LSTM模型则序列处理特征，使用二元交叉熵损失。\n3. **保形预测阈值选择**：采用功能型保形预测（CP），利用校准集上成功轨迹的分数分布，构建一个随时间变化的预测带。在测试时，若预测分数超出该带，则判定为失败，从而在保持低误报率的同时平衡检测的准确性与及时性。\n## 论文使用数据集和训练资源\n1. **数据集**：\n - 模拟：LIBERO-10（10个长时序任务）、SimplerEnv（基于RT系列和BridgeData V2的高保真环境）。\n - 真实世界：在Franka Emika Panda机器人上部署π₀-FAST-DROID模型，设计了13个任务，收集了30个成功和30个失败轨迹；在WidowX机器人上部署OpenVLA，收集了8个任务的532个轨迹。\n2. **训练资源**：论文未明确提及具体的硬件资源，但SAFE本身是一个轻量级模型（如LSTM含230万参数），引入的计算开销极小（<1毫秒）。\n## 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 在多个模拟环境（LIBERO-10, SimplerEnv）和真实世界机器人（Franka, WidowX）上对三种VLA模型（OpenVLA, π₀, π₀-FAST）进行评估。\n - 任务被划分为训练集（Seen任务）、验证集和测试集（Unseen任务）以评估跨任务的零样本泛化能力。\n2. **评估指标**：\n - ROC曲线下面积（ROC-AUC）：衡量所有可能阈值下区分成功与失败的平均性能。\n - 平衡准确率、真阳性率、假阳性率：用于使用保形预测阈值时的二分类性能评估。\n - 平均检测时间（T-det）：检测到失败的相对时间步，衡量检测的及时性。\n - 此外，通过可视化失败模式与人类判断的匹配程度进行定性评估。",
    "summary_html": "<h2>论文研究单位</h2>\n<p>多伦多大学、多伦多大学机器人研究所、Vector研究所、丰田研究所。</p>\n<h2>论文概述</h2>\n<p>论文针对视觉-语言-动作（VLA）模型在部署于新任务时成功率有限的问题，提出了一个名为SAFE的多任务失败检测框架。SAFE旨在作为VLA等通用机器人策略的即插即用型失败检测器，通过分析VLA的内部特征来预测任务失败的可能性，从而允许机器人及时停止、回溯或寻求帮助。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>通过分析VLA的潜在空间，发现不同任务中成功与失败轨迹的内部特征在特征空间中存在明显且一致的分离现象。</li><li>提出了SAFE框架，一个可扩展的、基于VLA内部特征的多任务失败检测方法。SAFE通过训练学习任务通用的失败表示，并使用保形预测（CP）校准阈值。</li><li>在多个VLA模型（OpenVLA, π₀, π₀-FAST）上进行了广泛的模拟和真实世界实验，并与多种基线方法比较，验证了SAFE的先进性能。</li></ol>\n<h2>论文方法描述</h2>\n<ol><li><strong>视觉分析</strong>：使用t-SNE可视化VLA（如π₀-FAST）在LIBERO-10数据集上的特征，发现失败轨迹的特征会聚集在一个“失败区”，且该现象在不同任务间具有普遍性。</li><li><strong>特征探测失败检测</strong>：SAFE从VLA最后一层提取特征，并使用简单的MLP或LSTM网络将特征回归为一个标量失败分数。MLP模型对每个时间步的特征独立处理并累加分数，使用L1损失；LSTM模型则序列处理特征，使用二元交叉熵损失。</li><li><strong>保形预测阈值选择</strong>：采用功能型保形预测（CP），利用校准集上成功轨迹的分数分布，构建一个随时间变化的预测带。在测试时，若预测分数超出该带，则判定为失败，从而在保持低误报率的同时平衡检测的准确性与及时性。</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<p>1. <strong>数据集</strong>：</p>\n<p> - 模拟：LIBERO-10（10个长时序任务）、SimplerEnv（基于RT系列和BridgeData V2的高保真环境）。</p>\n<p> - 真实世界：在Franka Emika Panda机器人上部署π₀-FAST-DROID模型，设计了13个任务，收集了30个成功和30个失败轨迹；在WidowX机器人上部署OpenVLA，收集了8个任务的532个轨迹。</p>\n<p>2. <strong>训练资源</strong>：论文未明确提及具体的硬件资源，但SAFE本身是一个轻量级模型（如LSTM含230万参数），引入的计算开销极小（<1毫秒）。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - 在多个模拟环境（LIBERO-10, SimplerEnv）和真实世界机器人（Franka, WidowX）上对三种VLA模型（OpenVLA, π₀, π₀-FAST）进行评估。</p>\n<p> - 任务被划分为训练集（Seen任务）、验证集和测试集（Unseen任务）以评估跨任务的零样本泛化能力。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - ROC曲线下面积（ROC-AUC）：衡量所有可能阈值下区分成功与失败的平均性能。</p>\n<p> - 平衡准确率、真阳性率、假阳性率：用于使用保形预测阈值时的二分类性能评估。</p>\n<p> - 平均检测时间（T-det）：检测到失败的相对时间步，衡量检测的及时性。</p>\n<p> - 此外，通过可视化失败模式与人类判断的匹配程度进行定性评估。</p>"
  },
  {
    "date": "2025-06-11",
    "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.09930",
    "summary_markdown": "### 论文研究单位\nNew York University\n### 论文概述\n本文提出了一个名为INT-ACT的视觉-语言-动作（VLA）模型泛化能力探测套件，包含50个模拟任务，覆盖10个子类别，旨在系统性地评估当前最先进VLA模型的泛化边界。研究发现，尽管VLA模型在语义理解（意图）方面表现出色，但在执行动作时存在显著差距，特别是在分布外（OOD）观察下，策略往往表现出连贯的意图，却在动作执行上失败。此外，在动作数据上的微调可能会削弱原始VLM的通用推理能力。论文发布了任务套件和评估代码，作为未来VLA模型的标准化基准。\n### 论文核心贡献点\n1. 提出并开源了INT-ACT，一个全面的VLA泛化探测套件，包含跨越3大类和10个子类别的50个任务，显著扩展了现有VLA基准的范围。\n2. 通过广泛的基准测试，揭示了当前最先进VLA模型的两个关键失败模式：\n - 持续且显著的意图-行动差距（Intention-Action Gap），即在分布偏移下强大的语义理解未能转化为可靠的执行。\n - 脆弱的多模态泛化能力，尤其是在语言变化和复合视觉-语言分布偏移下表现不佳。\n### 论文方法描述\n1. **测试平台选择**：基于SimplerEnv基准（构建于ManiSkill2模拟器之上），因其设计能紧密匹配模型在真实世界中的性能。\n2. **设计原则**：探测任务分为三个主要类别：\n - **对象多样性（Object Diversity）**：引入分布外对象，包括家居物品和工业工具，测试模型对新对象、外观和功能性的泛化能力。\n - **语言复杂性（Language Complexity）**：从模板化命令（如“把A放在B上”）扩展到组合性、知识和推理密集的指令，包括动作动词改写、语义否定和参考性外观描述。\n - **视觉-语言思维（Vision-Language Thinking）**：通过添加视觉干扰物和需要常识推理的干扰物（如橙子与橙汁盒），测试模型在复杂环境中的鲁棒性。\n3. **评估指标**：\n - **抓取成功率**：机器人夹爪是否成功抓取正确的源对象。\n - **意图正确率**：夹爪是否在任何一帧内移动到正确源对象的小半径内，捕获策略意图。\n - **任务成功率**：任务是否成功完成。\n### 论文使用数据集和训练资源\n1. **数据集**：主要基于BridgeV2数据集进行微调和评估，该数据集与SimplerEnv一致。\n2. **模型选择与训练协议**：评估了四种代表性模型（π₀、SpatialVLA、Magma、Octo）及其变体，所有微调实验均在BridgeV2上进行，遵循各自论文的训练和微调协议以确保公平比较。\n3. **训练资源**：使用了NYU IT高性能计算资源、服务和专业知识，并得到NSF资助（拨款号2238968, 2322242, 2026479）。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：完全在模拟环境中进行，基于ManiSkill2模拟器构建的SimplerEnv基准，确保部署的低门槛和可重复性。\n2. **评估指标**：\n - **抓取成功率（Grasp Success Rate）**：衡量夹爪成功抓取正确源对象的能力。\n - **意图正确率（Intention Correct Rate）**：新增指标，评估夹爪是否移动到正确源对象附近，即使后续抓取失败也记录意图。\n - **任务成功率（Task Success Rate）**：衡量任务整体完成情况。\n - **错误对象尝试率（Wrong Object Attempt Rate）**：在干扰物场景中监控是否移动了非源对象。\n3. **评估协议**：每个任务评估24个情节，对应所有预定义的场景和对象配置，每个配置重复3个随机种子，所有指标平均跨情节和种子计算。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>New York University</p>\n<h3>论文概述</h3>\n<p>本文提出了一个名为INT-ACT的视觉-语言-动作（VLA）模型泛化能力探测套件，包含50个模拟任务，覆盖10个子类别，旨在系统性地评估当前最先进VLA模型的泛化边界。研究发现，尽管VLA模型在语义理解（意图）方面表现出色，但在执行动作时存在显著差距，特别是在分布外（OOD）观察下，策略往往表现出连贯的意图，却在动作执行上失败。此外，在动作数据上的微调可能会削弱原始VLM的通用推理能力。论文发布了任务套件和评估代码，作为未来VLA模型的标准化基准。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出并开源了INT-ACT，一个全面的VLA泛化探测套件，包含跨越3大类和10个子类别的50个任务，显著扩展了现有VLA基准的范围。</li><li>通过广泛的基准测试，揭示了当前最先进VLA模型的两个关键失败模式：</li></ol>\n<p> - 持续且显著的意图-行动差距（Intention-Action Gap），即在分布偏移下强大的语义理解未能转化为可靠的执行。</p>\n<p> - 脆弱的多模态泛化能力，尤其是在语言变化和复合视觉-语言分布偏移下表现不佳。</p>\n<h3>论文方法描述</h3>\n<ol><li><strong>测试平台选择</strong>：基于SimplerEnv基准（构建于ManiSkill2模拟器之上），因其设计能紧密匹配模型在真实世界中的性能。</li><li><strong>设计原则</strong>：探测任务分为三个主要类别：</li></ol>\n<p> - <strong>对象多样性（Object Diversity）</strong>：引入分布外对象，包括家居物品和工业工具，测试模型对新对象、外观和功能性的泛化能力。</p>\n<p> - <strong>语言复杂性（Language Complexity）</strong>：从模板化命令（如“把A放在B上”）扩展到组合性、知识和推理密集的指令，包括动作动词改写、语义否定和参考性外观描述。</p>\n<p> - <strong>视觉-语言思维（Vision-Language Thinking）</strong>：通过添加视觉干扰物和需要常识推理的干扰物（如橙子与橙汁盒），测试模型在复杂环境中的鲁棒性。</p>\n<p>3. <strong>评估指标</strong>：</p>\n<p> - <strong>抓取成功率</strong>：机器人夹爪是否成功抓取正确的源对象。</p>\n<p> - <strong>意图正确率</strong>：夹爪是否在任何一帧内移动到正确源对象的小半径内，捕获策略意图。</p>\n<p> - <strong>任务成功率</strong>：任务是否成功完成。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：主要基于BridgeV2数据集进行微调和评估，该数据集与SimplerEnv一致。</li><li><strong>模型选择与训练协议</strong>：评估了四种代表性模型（π₀、SpatialVLA、Magma、Octo）及其变体，所有微调实验均在BridgeV2上进行，遵循各自论文的训练和微调协议以确保公平比较。</li><li><strong>训练资源</strong>：使用了NYU IT高性能计算资源、服务和专业知识，并得到NSF资助（拨款号2238968, 2322242, 2026479）。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：完全在模拟环境中进行，基于ManiSkill2模拟器构建的SimplerEnv基准，确保部署的低门槛和可重复性。</li><li><strong>评估指标</strong>：</li></ol>\n<p> - <strong>抓取成功率（Grasp Success Rate）</strong>：衡量夹爪成功抓取正确源对象的能力。</p>\n<p> - <strong>意图正确率（Intention Correct Rate）</strong>：新增指标，评估夹爪是否移动到正确源对象附近，即使后续抓取失败也记录意图。</p>\n<p> - <strong>任务成功率（Task Success Rate）</strong>：衡量任务整体完成情况。</p>\n<p> - <strong>错误对象尝试率（Wrong Object Attempt Rate）</strong>：在干扰物场景中监控是否移动了非源对象。</p>\n<p>3. <strong>评估协议</strong>：每个任务评估24个情节，对应所有预定义的场景和对象配置，每个配置重复3个随机种子，所有指标平均跨情节和种子计算。</p>"
  },
  {
    "date": "2025-06-11",
    "title": "OctoNav: Towards Generalist Embodied Navigation",
    "link": "http://arxiv.org/abs/2506.09839",
    "summary_markdown": "```markdown\n论文研究单位\n北京航空航天大学, 新加坡国立大学, 北京大学, 中关村学院\n\n论文概述\n该论文提出了OctoNav-Bench和OctoNav-R1，旨在构建通用型具身导航智能体。OctoNav-Bench是一个大规模基准，包含400多个场景和45k+的指令-轨迹对，支持自由形式、多模态和多能力指令，并提供了Think-Before-Action (TBA-CoT)数据集。OctoNav-R1是一个基于VLA的模型，通过混合训练范式（HTP）进行训练，包括Action-/TBA-SFT、Nav-GRPO和在线强化学习阶段，以增强模型的推理能力。\n\n论文核心贡献点\n1. 提出了OctoNav-Bench，一个大规模、统一且支持连续环境的通用导航基准，包含多模态、多能力的自由形式指令和TBA-CoT数据集。\n2. 提出了OctoNav-R1，一个基于VLA的模型，能够遵循自由形式的多模态指令并生成低层动作。\n3. 设计了混合训练范式（HTP），结合了监督微调（Action-SFT和TBA-SFT）、Nav-GRPO和在线强化学习，特别引入了“行动前思考”机制以提升推理能力。\n4. 展示了OctoNav-R1在多个导航任务上的优越性能，并初步实现了在真实世界机器人上的部署。\n\n论文方法描述\n1. OctoNav-Bench构建：通过自动化流程构建，包括模板生成（使用GPT生成多能力指令模板）、轨迹生成与指令实例化（在MP3D等场景中采样轨迹并提取多模态元素实例化指令）、指令扩展与质量检查。\n2. TBA-CoT数据集构建：使用Qwen-VL和DeepSeek-R1为轨迹中的每个动作步骤生成多模态思维链数据，将视觉信息转换为文本描述，并结合当前和历史观察生成推理过程。\n3. OctoNav-R1模型架构：基于LLaMA-VID，能够处理多模态输入（历史观察、当前观察、多模态指令）并输出结构化动作。\n4. 混合训练范式（HTP）：\n - 阶段一（Action-/TBA-SFT）：使用指令-轨迹对进行行为监督微调（Action-SFT），使用TBA-CoT数据集进行思维前行动监督微调（TBA-SFT）。\n - 阶段二（Nav-GRPO）：采用群体相对策略优化，通过定制的奖励函数进一步增强模型思维能力。\n - 阶段三（在线强化学习）：在连续环境中进行试错和主动学习。\n\n论文使用数据集和训练资源\n1. 数据集：OctoNav-Bench，包含来自HM3D、Gibson、MP3D和ProcTHOR的400+个3D室内场景，45k+的指令-轨迹对，以及多模态TBA-CoT数据集。\n2. 训练资源：未在提供的文本中明确说明具体计算资源，但提及了基于LLaMA-VID的模型架构和三阶段的混合训练流程。\n\n论文使用的评估环境和评估指标\n1. 评估环境：OctoNav-Bench提供的连续环境。\n2. 评估指标：未在提供的文本中明确列出具体评估指标，但提及了在多个导航能力（ObjNav、PointNav、ImgNav、Ins-ImgNav、VLN）上的性能比较。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位</p>\n<p>北京航空航天大学, 新加坡国立大学, 北京大学, 中关村学院</p>\n\n<p>论文概述</p>\n<p>该论文提出了OctoNav-Bench和OctoNav-R1，旨在构建通用型具身导航智能体。OctoNav-Bench是一个大规模基准，包含400多个场景和45k+的指令-轨迹对，支持自由形式、多模态和多能力指令，并提供了Think-Before-Action (TBA-CoT)数据集。OctoNav-R1是一个基于VLA的模型，通过混合训练范式（HTP）进行训练，包括Action-/TBA-SFT、Nav-GRPO和在线强化学习阶段，以增强模型的推理能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了OctoNav-Bench，一个大规模、统一且支持连续环境的通用导航基准，包含多模态、多能力的自由形式指令和TBA-CoT数据集。</li><li>提出了OctoNav-R1，一个基于VLA的模型，能够遵循自由形式的多模态指令并生成低层动作。</li><li>设计了混合训练范式（HTP），结合了监督微调（Action-SFT和TBA-SFT）、Nav-GRPO和在线强化学习，特别引入了“行动前思考”机制以提升推理能力。</li><li>展示了OctoNav-R1在多个导航任务上的优越性能，并初步实现了在真实世界机器人上的部署。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>OctoNav-Bench构建：通过自动化流程构建，包括模板生成（使用GPT生成多能力指令模板）、轨迹生成与指令实例化（在MP3D等场景中采样轨迹并提取多模态元素实例化指令）、指令扩展与质量检查。</li><li>TBA-CoT数据集构建：使用Qwen-VL和DeepSeek-R1为轨迹中的每个动作步骤生成多模态思维链数据，将视觉信息转换为文本描述，并结合当前和历史观察生成推理过程。</li><li>OctoNav-R1模型架构：基于LLaMA-VID，能够处理多模态输入（历史观察、当前观察、多模态指令）并输出结构化动作。</li><li>混合训练范式（HTP）：</li></ol>\n<p> - 阶段一（Action-/TBA-SFT）：使用指令-轨迹对进行行为监督微调（Action-SFT），使用TBA-CoT数据集进行思维前行动监督微调（TBA-SFT）。</p>\n<p> - 阶段二（Nav-GRPO）：采用群体相对策略优化，通过定制的奖励函数进一步增强模型思维能力。</p>\n<p> - 阶段三（在线强化学习）：在连续环境中进行试错和主动学习。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：OctoNav-Bench，包含来自HM3D、Gibson、MP3D和ProcTHOR的400+个3D室内场景，45k+的指令-轨迹对，以及多模态TBA-CoT数据集。</li><li>训练资源：未在提供的文本中明确说明具体计算资源，但提及了基于LLaMA-VID的模型架构和三阶段的混合训练流程。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：OctoNav-Bench提供的连续环境。</li><li>评估指标：未在提供的文本中明确列出具体评估指标，但提及了在多个导航能力（ObjNav、PointNav、ImgNav、Ins-ImgNav、VLN）上的性能比较。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-06-10",
    "title": "An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models",
    "link": "http://arxiv.org/abs/2506.09172",
    "summary_markdown": "### 论文研究单位\nPranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Jaewoo Song, Harshvardhan Sikka\n### 论文概述\n论文介绍了 MultiNet，一个全新的、完全开源的基准测试和配套的软件生态系统，旨在严格评估和适应视觉、语言和动作领域的模型。该工作建立了标准化的评估协议来评估视觉语言模型（VLMs）和视觉-语言-动作模型（VLAs），并提供开源软件以下载相关数据、模型和进行评估。此外，论文提供了一个包含超过1.3万亿个令牌的复合数据集，涵盖了图像字幕、视觉问答、常识推理、机器人控制、数字游戏、模拟运动/操作等多种任务。MultiNet 的基准、框架、工具包和评估框架已被用于下游研究，探讨VLA泛化的局限性。\n### 论文核心贡献点\n1. **大规模通用数据集**：发布了一个庞大的开源数据集，整合了视觉、语言和动作等多个领域的不同数据源，适用于训练和评估通用模型。\n2. **开源数据整理SDK**：提供了一个开源的软件工具包，方便访问整合后的数据集，并将来自不同来源的控制数据（强化学习和机器人数据）标准化为通用、易于访问的格式。\n3. **系统性评估框架**：引入了一个标准化的、设计合理的评估方法，包括测试集拆分和精心设计的指标，用于评估最先进的VLMs和VLAs在熟悉和全新领域（包括真实世界机器人任务和过程生成游戏环境）中的泛化能力。\n4. **最先进模型的开源适配**：开源了对最先进的VLMs和VLAs的适配，使其能够有效地处理MultiNet中的数据格式和多样化领域，甚至是那些在其原始训练中未见过的领域。\n5. **深入的实验与分析**：利用MultiNet的基准、框架和评估工具，对领先的VLMs、VLAs和新兴通用模型的性能进行了评估和分析。\n### 论文方法描述\n1. **开源数据集SDK**：提供了一个代码库，可以无缝下载集合中的任何或所有数据集，并包含一个用于标准化机器人和强化学习数据的工具包。该工具包将不同格式的控制数据转换为统一的TensorFlow数据集格式。\n2. **评估框架**：为集合中的数据集引入了系统性的测试集拆分，以防止数据污染。同时，设计了一套评估指标套件，用于公平、可量化地捕捉模型的性能。\n3. **通用提示框架**：提出了一个名为GenESIS的开源模块化框架，用于简化将不同VLMs集成到各种任务和数据集中的过程。该框架基于可互换性、抽象性、封装性和提示工程等核心原则，包含系统级指令、任务和环境上下文、多模态输入集成、动作空间定义和输出指令等关键元素。\n4. **模型适配**：对多个最先进的VLA和VLM模型进行了架构和后处理适配，使其能够处理离线机器人数据集和过程生成的离散动作环境等OOD领域。具体适配的模型包括JAT、OpenVLA、Pi0 Base和Pi0 Fast，通过处理模型输入输出、架构更改和推理管道来适应特定领域的结构和统计特性。\n### 论文使用数据集和训练资源\n1. **数据集**：论文整合了一个大规模、多样化的数据集集合，包含超过1.3万亿个令牌。数据集分为三类：\n * **视觉语言**：占比29%，包括OBELICS、DataComp-1B、COYO-700M、MS-COCO Captions、Conceptual Captions、VQA-V2等。\n * **语言**：占比13%，包括Fineweb-edu、HellaSwag、ARC、CommonsenseQA、MMLU等。\n * **控制**：占比58%，主要包括OpenX-Embodiment，以及DM Lab、ALE Atari、BabyAI、MuJoCo、Meta-World、Procgen等。\n2. **训练资源**：论文本身是关于基准测试和工具包，因此未详细说明训练特定模型所需的计算资源（如GPU类型和数量）。但是，研究中评估了如GPT-4o、JAT、OpenVLA、Pi0等现有模型，这些模型的原始训练资源在其各自的论文中有描述。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n * **分布式机器人环境**：使用来自OpenX-Embodiment数据集的离线轨迹进行评估，测试模型在真实机器人操作和运动任务上的泛化能力。\n * **分布式过程生成环境**：使用如Procgen等程序生成的游戏环境进行评估，测试模型在数字世界中面对全新、动态场景时的泛化能力。\n * 评估采用了系统性的测试集拆分，以确保基准测试的可靠性并防止数据污染。\n2. **评估指标**：\n * **强化学习与机器人**：均方误差（MSE）、布里尔平均绝对误差（Brier MAE）、精确率、召回率、F1分数、无效输出百分比。\n * **图像描述与检索**：CIDEr（用于图像描述和基于图像的文本检索）、Recall@K（用于图像理解和基于文本的图像检索）。\n * **视觉问答与文本理解**：VQA准确率、准确率（用于常识推理和文本理解）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Jaewoo Song, Harshvardhan Sikka</p>\n<h3>论文概述</h3>\n<p>论文介绍了 MultiNet，一个全新的、完全开源的基准测试和配套的软件生态系统，旨在严格评估和适应视觉、语言和动作领域的模型。该工作建立了标准化的评估协议来评估视觉语言模型（VLMs）和视觉-语言-动作模型（VLAs），并提供开源软件以下载相关数据、模型和进行评估。此外，论文提供了一个包含超过1.3万亿个令牌的复合数据集，涵盖了图像字幕、视觉问答、常识推理、机器人控制、数字游戏、模拟运动/操作等多种任务。MultiNet 的基准、框架、工具包和评估框架已被用于下游研究，探讨VLA泛化的局限性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>大规模通用数据集</strong>：发布了一个庞大的开源数据集，整合了视觉、语言和动作等多个领域的不同数据源，适用于训练和评估通用模型。</li><li><strong>开源数据整理SDK</strong>：提供了一个开源的软件工具包，方便访问整合后的数据集，并将来自不同来源的控制数据（强化学习和机器人数据）标准化为通用、易于访问的格式。</li><li><strong>系统性评估框架</strong>：引入了一个标准化的、设计合理的评估方法，包括测试集拆分和精心设计的指标，用于评估最先进的VLMs和VLAs在熟悉和全新领域（包括真实世界机器人任务和过程生成游戏环境）中的泛化能力。</li><li><strong>最先进模型的开源适配</strong>：开源了对最先进的VLMs和VLAs的适配，使其能够有效地处理MultiNet中的数据格式和多样化领域，甚至是那些在其原始训练中未见过的领域。</li><li><strong>深入的实验与分析</strong>：利用MultiNet的基准、框架和评估工具，对领先的VLMs、VLAs和新兴通用模型的性能进行了评估和分析。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>开源数据集SDK</strong>：提供了一个代码库，可以无缝下载集合中的任何或所有数据集，并包含一个用于标准化机器人和强化学习数据的工具包。该工具包将不同格式的控制数据转换为统一的TensorFlow数据集格式。</li><li><strong>评估框架</strong>：为集合中的数据集引入了系统性的测试集拆分，以防止数据污染。同时，设计了一套评估指标套件，用于公平、可量化地捕捉模型的性能。</li><li><strong>通用提示框架</strong>：提出了一个名为GenESIS的开源模块化框架，用于简化将不同VLMs集成到各种任务和数据集中的过程。该框架基于可互换性、抽象性、封装性和提示工程等核心原则，包含系统级指令、任务和环境上下文、多模态输入集成、动作空间定义和输出指令等关键元素。</li><li><strong>模型适配</strong>：对多个最先进的VLA和VLM模型进行了架构和后处理适配，使其能够处理离线机器人数据集和过程生成的离散动作环境等OOD领域。具体适配的模型包括JAT、OpenVLA、Pi0 Base和Pi0 Fast，通过处理模型输入输出、架构更改和推理管道来适应特定领域的结构和统计特性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：论文整合了一个大规模、多样化的数据集集合，包含超过1.3万亿个令牌。数据集分为三类：</p>\n<p> * <strong>视觉语言</strong>：占比29%，包括OBELICS、DataComp-1B、COYO-700M、MS-COCO Captions、Conceptual Captions、VQA-V2等。</p>\n<p> * <strong>语言</strong>：占比13%，包括Fineweb-edu、HellaSwag、ARC、CommonsenseQA、MMLU等。</p>\n<p> * <strong>控制</strong>：占比58%，主要包括OpenX-Embodiment，以及DM Lab、ALE Atari、BabyAI、MuJoCo、Meta-World、Procgen等。</p>\n<p>2. <strong>训练资源</strong>：论文本身是关于基准测试和工具包，因此未详细说明训练特定模型所需的计算资源（如GPU类型和数量）。但是，研究中评估了如GPT-4o、JAT、OpenVLA、Pi0等现有模型，这些模型的原始训练资源在其各自的论文中有描述。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> * <strong>分布式机器人环境</strong>：使用来自OpenX-Embodiment数据集的离线轨迹进行评估，测试模型在真实机器人操作和运动任务上的泛化能力。</p>\n<p> * <strong>分布式过程生成环境</strong>：使用如Procgen等程序生成的游戏环境进行评估，测试模型在数字世界中面对全新、动态场景时的泛化能力。</p>\n<p> * 评估采用了系统性的测试集拆分，以确保基准测试的可靠性并防止数据污染。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> * <strong>强化学习与机器人</strong>：均方误差（MSE）、布里尔平均绝对误差（Brier MAE）、精确率、召回率、F1分数、无效输出百分比。</p>\n<p> * <strong>图像描述与检索</strong>：CIDEr（用于图像描述和基于图像的文本检索）、Recall@K（用于图像理解和基于文本的图像检索）。</p>\n<p> * <strong>视觉问答与文本理解</strong>：VQA准确率、准确率（用于常识推理和文本理解）。</p>"
  },
  {
    "date": "2025-06-10",
    "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency",
    "link": "http://arxiv.org/abs/2506.08822",
    "summary_markdown": "### 论文研究单位\n\n北京类人机器人创新中心 (Beijing Innovation Center of Humanoid Robotics)\n中国科学院自动化研究所模式识别国家重点实验室 (NLPR, MAIS, Institute of Automation of Chinese Academy of Sciences)\n### 论文概述\n\n论文提出了FreqPolicy，一种新颖的基于流的视觉运动策略，旨在通过频率一致性约束实现高效的单步动作生成。该方法首次在基于流的视觉运动策略中引入时间知识，以解决机器人操作中动作轨迹的时间依赖性问题。通过在频域中对齐不同时间步的动作特征，并结合自适应频率分量损失，FreqPolicy在保持生成动作质量的同时显著提高了推理速度。\n### 论文核心贡献点\n\n1. 首个利用时间知识进行机器人操作的单步视觉运动策略。\n2. 借鉴时间序列和语音处理领域，提出频率一致性约束目标以增强任意两个动作速度的规律化，并提出自适应频率分量损失以有效捕捉动作序列的结构性时间变化。\n3. 在3个模拟基准的53个任务上进行广泛实验，证明其优于现有单步动作生成器，例如在MetaWorld上达到84.2%的成功率。\n4. 将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在不损失任务性能的情况下显著提升推理速度（例如，5倍加速）。\n### 论文方法描述\n\nFreqPolicy基于流匹配框架，通过学习一个时间依赖的向量场将先验分布（如高斯噪声）转换到目标动作分布。核心创新在于引入频率一致性约束，强制在频域中对齐沿流路径不同时间步的动作速度，从而促进单步动作生成向目标分布收敛。此外，设计了一种自适应频率分量损失，动态强调具有更大差异的频率分量，以捕捉机器人操作任务中固有的结构性时间变化。模型支持2D图像和3D点云输入，输出动作块的速度向量。\n### 论文使用数据集和训练资源\n\n数据集: 在3个模拟基准（MetaWorld、Robomimic、Libero）的53个任务上进行评估。\n训练资源: 未明确指定硬件细节，但提及模型训练和推理在标准GPU环境下进行，其中现实世界实验达到93.5 Hz的推理频率。\n### 论文使用的评估环境和评估指标\n\n评估环境: 模拟环境（MetaWorld、Robomimic、Libero）和真实世界机器人场景。\n评估指标: 任务成功率（如MetaWorld上的平均成功率）、推理速度（Hz）、与基线方法（如FlowPolicy、CP、ManiCM）的性能比较，以及在VLA模型集成后的任务完成率和推理加速比。",
    "summary_html": "<h3>论文研究单位</h3>\n\n<p>北京类人机器人创新中心 (Beijing Innovation Center of Humanoid Robotics)</p>\n<p>中国科学院自动化研究所模式识别国家重点实验室 (NLPR, MAIS, Institute of Automation of Chinese Academy of Sciences)</p>\n<h3>论文概述</h3>\n\n<p>论文提出了FreqPolicy，一种新颖的基于流的视觉运动策略，旨在通过频率一致性约束实现高效的单步动作生成。该方法首次在基于流的视觉运动策略中引入时间知识，以解决机器人操作中动作轨迹的时间依赖性问题。通过在频域中对齐不同时间步的动作特征，并结合自适应频率分量损失，FreqPolicy在保持生成动作质量的同时显著提高了推理速度。</p>\n<h3>论文核心贡献点</h3>\n\n<ol><li>首个利用时间知识进行机器人操作的单步视觉运动策略。</li><li>借鉴时间序列和语音处理领域，提出频率一致性约束目标以增强任意两个动作速度的规律化，并提出自适应频率分量损失以有效捕捉动作序列的结构性时间变化。</li><li>在3个模拟基准的53个任务上进行广泛实验，证明其优于现有单步动作生成器，例如在MetaWorld上达到84.2%的成功率。</li><li>将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在不损失任务性能的情况下显著提升推理速度（例如，5倍加速）。</li></ol>\n<h3>论文方法描述</h3>\n\n<p>FreqPolicy基于流匹配框架，通过学习一个时间依赖的向量场将先验分布（如高斯噪声）转换到目标动作分布。核心创新在于引入频率一致性约束，强制在频域中对齐沿流路径不同时间步的动作速度，从而促进单步动作生成向目标分布收敛。此外，设计了一种自适应频率分量损失，动态强调具有更大差异的频率分量，以捕捉机器人操作任务中固有的结构性时间变化。模型支持2D图像和3D点云输入，输出动作块的速度向量。</p>\n<h3>论文使用数据集和训练资源</h3>\n\n<p>数据集: 在3个模拟基准（MetaWorld、Robomimic、Libero）的53个任务上进行评估。</p>\n<p>训练资源: 未明确指定硬件细节，但提及模型训练和推理在标准GPU环境下进行，其中现实世界实验达到93.5 Hz的推理频率。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n\n<p>评估环境: 模拟环境（MetaWorld、Robomimic、Libero）和真实世界机器人场景。</p>\n<p>评估指标: 任务成功率（如MetaWorld上的平均成功率）、推理速度（Hz）、与基线方法（如FlowPolicy、CP、ManiCM）的性能比较，以及在VLA模型集成后的任务完成率和推理加速比。</p>"
  },
  {
    "date": "2025-06-10",
    "title": "Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing",
    "link": "http://arxiv.org/abs/2506.08462",
    "summary_markdown": "论文研究单位\n剑桥大学工程系\n\n论文概述\n论文提出了CIPHER（Control and Interpretation of Production via Hybrid Expertise and Reasoning）框架，这是一个视觉-语言-动作（VLA）模型框架，旨在通过混合推理实现制造业中的感知、解释和自主行动。该框架在商业级3D打印机上实例化，集成了过程专家、回归模型和检索增强生成（RAG）模块，支持物理信息的链式推理，能够在无需显式注释的情况下处理分布外任务，实现精确控制、上下文推理和透明决策。\n\n论文核心贡献点\n1. 提出混合推理架构，结合视觉-语言模型与专家模块，解决工业环境中数据稀缺和高精度需求问题\n2. 集成过程专家（Process Expert）模块，增强连续参数的定量表征能力，将流程特定信息压缩为单令牌\n3. 实现检索增强生成（RAG），访问外部专家知识，支持物理信息的链式推理\n4. 展示在3D打印中的自主控制能力，包括传统控制、分布外任务响应和无需几何监督的打印\n5. 通过模块化设计保持先验知识，避免灾难性遗忘，同时实现定性-定量对齐\n\n论文方法描述\nCIPHER框架采用三专家架构：\n1. **视觉-语言模块**：基于Llama-3.2-11B-Vision，集成ViT-H/14图像编码器和投影层，通过交叉注意力层连接\n2. **过程专家**：ResNet-152模型（116M参数），接收224×224×3输入，预测流速值并转换为令牌注入语言模块\n3. **几何专家**：处理几何请求，通过复杂度测试路由到基元组合或Shap-E生成器，动态选择处理路径\n训练采用端到端微调，使用低秩适应（LoRA）减少内存消耗52.4%，AdamW优化器和学习率调度器\n\n论文使用数据集和训练资源\n**数据集**：\n- 超过100万张3D打印喷嘴图像，同步记录实时工艺参数\n- 语言标注通过模板转换数值标签（流速值）为自然语言描述，包含通用陈述、定量语句和定性描述符\n- 知识库包含3,930条3D打印领域事实，通过Ada-v2生成嵌入向量\n**训练资源**：\n- 单节点4×NVIDIA A100-SXM-80GB GPU，1TB内存\n- 微调耗时20小时（单轮次），超参数包括LoRA秩为64，学习率初始值1e-4\n\n论文使用的评估环境和评估指标\n**评估环境**：\n- 商业级3D打印机（型号未明确）\n- 比较模型：LLaMA-3.2、GPT-4o-mini、GPT-4o\n- 测试任务：流速预测控制、分布外问题回答、几何打印生成\n**评估指标**：\n1. 平均绝对误差（MAE）：量化预测流速与实际值偏差\n2. 语言召回率（Language Recall）：评估生成文本与参考的令牌覆盖率\n3. CIDEr分数：衡量生成描述与人类参考的共识相似度\n4. 定性-定量对齐：检查流速预测与挤出质量描述的分类一致性\n5. Elo排名系统：通过成对比较评估模型在问答和推理任务中的相对性能",
    "summary_html": "<p>论文研究单位</p>\n<p>剑桥大学工程系</p>\n\n<p>论文概述</p>\n<p>论文提出了CIPHER（Control and Interpretation of Production via Hybrid Expertise and Reasoning）框架，这是一个视觉-语言-动作（VLA）模型框架，旨在通过混合推理实现制造业中的感知、解释和自主行动。该框架在商业级3D打印机上实例化，集成了过程专家、回归模型和检索增强生成（RAG）模块，支持物理信息的链式推理，能够在无需显式注释的情况下处理分布外任务，实现精确控制、上下文推理和透明决策。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出混合推理架构，结合视觉-语言模型与专家模块，解决工业环境中数据稀缺和高精度需求问题</li><li>集成过程专家（Process Expert）模块，增强连续参数的定量表征能力，将流程特定信息压缩为单令牌</li><li>实现检索增强生成（RAG），访问外部专家知识，支持物理信息的链式推理</li><li>展示在3D打印中的自主控制能力，包括传统控制、分布外任务响应和无需几何监督的打印</li><li>通过模块化设计保持先验知识，避免灾难性遗忘，同时实现定性-定量对齐</li></ol>\n\n<p>论文方法描述</p>\n<p>CIPHER框架采用三专家架构：</p>\n<ol><li><strong>视觉-语言模块</strong>：基于Llama-3.2-11B-Vision，集成ViT-H/14图像编码器和投影层，通过交叉注意力层连接</li><li><strong>过程专家</strong>：ResNet-152模型（116M参数），接收224×224×3输入，预测流速值并转换为令牌注入语言模块</li><li><strong>几何专家</strong>：处理几何请求，通过复杂度测试路由到基元组合或Shap-E生成器，动态选择处理路径</li></ol>\n<p>训练采用端到端微调，使用低秩适应（LoRA）减少内存消耗52.4%，AdamW优化器和学习率调度器</p>\n\n<p>论文使用数据集和训练资源</p>\n<p><strong>数据集</strong>：</p>\n<ul><li>超过100万张3D打印喷嘴图像，同步记录实时工艺参数</li><li>语言标注通过模板转换数值标签（流速值）为自然语言描述，包含通用陈述、定量语句和定性描述符</li><li>知识库包含3,930条3D打印领域事实，通过Ada-v2生成嵌入向量</li></ul>\n<p><strong>训练资源</strong>：</p>\n<ul><li>单节点4×NVIDIA A100-SXM-80GB GPU，1TB内存</li><li>微调耗时20小时（单轮次），超参数包括LoRA秩为64，学习率初始值1e-4</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p><strong>评估环境</strong>：</p>\n<ul><li>商业级3D打印机（型号未明确）</li><li>比较模型：LLaMA-3.2、GPT-4o-mini、GPT-4o</li><li>测试任务：流速预测控制、分布外问题回答、几何打印生成</li></ul>\n<p><strong>评估指标</strong>：</p>\n<ol><li>平均绝对误差（MAE）：量化预测流速与实际值偏差</li><li>语言召回率（Language Recall）：评估生成文本与参考的令牌覆盖率</li><li>CIDEr分数：衡量生成描述与人类参考的共识相似度</li><li>定性-定量对齐：检查流速预测与挤出质量描述的分类一致性</li><li>Elo排名系统：通过成对比较评估模型在问答和推理任务中的相对性能</li></ol>"
  },
  {
    "date": "2025-06-10",
    "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization",
    "link": "http://arxiv.org/abs/2506.08440",
    "summary_markdown": "### 论文研究单位\n吉林大学人工智能学院\n### 论文概述\n论文针对视觉-语言-动作模型主要依赖成功演示数据训练，在分布外场景下泛化能力有限的问题，提出了一种基于在线强化学习的VLA模型微调框架TGRPO。该方法利用大语言模型自动构建多阶段密集奖励函数，并结合一种基于分组的策略优化算法，通过相对比较降低训练方差，从而提升模型在复杂长时程任务中的性能。\n### 论文核心贡献点\n1. 提出了一个面向VLA模型的在线强化学习训练框架，使其能够通过与环境的交互从失败中学习，克服了仅依赖成功演示的根本限制。\n2. 设计了TGRPO算法，该算法是一种新颖的基于分组的策略优化方法，融合了轨迹级和步级优势估计，以更好地捕获长时程机器人任务的结构，改善了信誉分配并增强了策略稳定性。\n3. 强调了密集奖励设计与基于分组优化的协同作用，利用LLM解析任务并生成阶段性密集奖励，再结合分组策略，显著提升了强化学习在复杂机器人任务中训练VLA模型的效果。\n### 论文方法描述\n该方法包含两部分。首先，利用大语言模型（如Claude 3.7 Sonnet）将自然语言指令分解为子任务，并结合环境中关键物体位置与预采集的成功演示中的机器人末端姿态，设计一个多阶段的密集奖励函数，为每个步骤提供细粒度的反馈。其次，提出轨迹分组相对策略优化算法（TGRPO），算法并行采样多条轨迹，然后将这些轨迹在步级和轨迹级两个维度进行分组，分别计算步级相对优势和轨迹级相对优势。通过加权融合这两种优势，得到最终的相对优势信号，并用于一个类似于PPO的裁剪目标函数中，以无价值网络的方式更新VLA模型策略。\n### 论文使用数据集和训练资源\n实验使用LIBERO机器人模拟器基准进行评估，该基准包含Spatial、Object、Goal、Long四个任务套件。基础模型采用OpenVLA，并使用LoRA进行微调。所有实验均在单块NVIDIA A100 GPU上进行。\n### 论文使用的评估环境和评估指标\n评估环境为LIBERO模拟器。评估指标为任务成功率，具体为在每个任务上运行50个测试回合，报告每个任务和每个任务套件的平均成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>吉林大学人工智能学院</p>\n<h3>论文概述</h3>\n<p>论文针对视觉-语言-动作模型主要依赖成功演示数据训练，在分布外场景下泛化能力有限的问题，提出了一种基于在线强化学习的VLA模型微调框架TGRPO。该方法利用大语言模型自动构建多阶段密集奖励函数，并结合一种基于分组的策略优化算法，通过相对比较降低训练方差，从而提升模型在复杂长时程任务中的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个面向VLA模型的在线强化学习训练框架，使其能够通过与环境的交互从失败中学习，克服了仅依赖成功演示的根本限制。</li><li>设计了TGRPO算法，该算法是一种新颖的基于分组的策略优化方法，融合了轨迹级和步级优势估计，以更好地捕获长时程机器人任务的结构，改善了信誉分配并增强了策略稳定性。</li><li>强调了密集奖励设计与基于分组优化的协同作用，利用LLM解析任务并生成阶段性密集奖励，再结合分组策略，显著提升了强化学习在复杂机器人任务中训练VLA模型的效果。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法包含两部分。首先，利用大语言模型（如Claude 3.7 Sonnet）将自然语言指令分解为子任务，并结合环境中关键物体位置与预采集的成功演示中的机器人末端姿态，设计一个多阶段的密集奖励函数，为每个步骤提供细粒度的反馈。其次，提出轨迹分组相对策略优化算法（TGRPO），算法并行采样多条轨迹，然后将这些轨迹在步级和轨迹级两个维度进行分组，分别计算步级相对优势和轨迹级相对优势。通过加权融合这两种优势，得到最终的相对优势信号，并用于一个类似于PPO的裁剪目标函数中，以无价值网络的方式更新VLA模型策略。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>实验使用LIBERO机器人模拟器基准进行评估，该基准包含Spatial、Object、Goal、Long四个任务套件。基础模型采用OpenVLA，并使用LoRA进行微调。所有实验均在单块NVIDIA A100 GPU上进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境为LIBERO模拟器。评估指标为任务成功率，具体为在每个任务上运行50个测试回合，报告每个任务和每个任务套件的平均成功率。</p>"
  },
  {
    "date": "2025-06-09",
    "title": "HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation",
    "link": "http://arxiv.org/abs/2506.08296",
    "summary_markdown": "### 论文研究单位\n约翰霍普金斯大学、意大利技术研究院、多伦多大学、哈佛大学麻省总医院\n### 论文概述\n本文提出了HiBerNAC（Hierarchical Brain-emulated Robotic Neural Agent Collective），一个受神经科学启发的多智能体框架，用于解决复杂机器人操作任务。该框架结合了多模态视觉-语言-动作（VLA）模型规划与神经启发的反思及多智能体机制，旨在克服现有方法在长视野任务、动态环境适应和实时性能方面的局限性。通过在7自由度Franka机器人上的实验，HiBerNAC在长视野任务平均完成时间上减少23%，在多路径任务上实现了12-31%的非零成功率。\n### 论文核心贡献点\n1. 提出了HiBerNAC框架，首次将神经科学的分层决策机制与多智能体协作结合，用于机器人操作。\n2. 设计了多频率异步处理架构，包括0.01Hz的高层规划、0.1Hz的海马体记忆和100Hz的实时反应式VLA控制。\n3. 实现了模块化神经智能体结构，包括前额叶规划器（PFP）、感知智能体（PA）、语义智能体（SA）、操作智能体（MA）和海马体模块（HM）。\n4. 开发了基于DAG的异步管道（DBHTN），支持分层任务管理和动态资源分配。\n### 论文方法描述\nHiBerNAC采用三层架构：\n1. 多智能体神经结构：模拟人脑神经分工，PFP（0.01Hz）分解高级任务，SA处理语言指令，MA执行动作，IA提供误差校正，HM（0.1Hz）管理情景记忆。智能体间通过连接矩阵F_ij通信。\n2. 异步管道：实现DBHTN规划器，将任务分解为状态和转换，使用潜在向量l_t、动作历史h_a和语义记忆m_s。状态审查机制以0.1Hz运行。\n3. 反应式VLA（RVLA）：基于开源预训练模型，以100Hz处理低级控制，用于简单反应任务。\n\n系统使用数学公式建模智能体交互：o_t^i = A_i(x_t^i, s_t^i, m_t; Θ_i) + ∑_{j≠i}F_ij · o_{t-1}^j，其中A_i为智能体特定函数，F_ij为连接矩阵。\n### 论文使用数据集和训练资源\n数据集：Open X-Embodiment数据集\n训练资源：未明确指定硬件，但提及使用预训练VLA模型（如OpenVLA）和神经网络模块。实验在7自由度Franka机器人上进行，包括仿真和真实世界测试。\n### 论文使用的评估环境和评估指标\n评估环境：Isaac Gym仿真环境和真实世界的7自由度Franka机器人平台。\n评估指标：1. 任务成功率（多路径任务的非零成功率） 2. 平均任务完成时间（长视野任务） 3. 动态环境适应性（遮挡/动态删除场景） 4. 多模态任务成功率（语言-视觉-动作一致性）",
    "summary_html": "<h3>论文研究单位</h3>\n<p>约翰霍普金斯大学、意大利技术研究院、多伦多大学、哈佛大学麻省总医院</p>\n<h3>论文概述</h3>\n<p>本文提出了HiBerNAC（Hierarchical Brain-emulated Robotic Neural Agent Collective），一个受神经科学启发的多智能体框架，用于解决复杂机器人操作任务。该框架结合了多模态视觉-语言-动作（VLA）模型规划与神经启发的反思及多智能体机制，旨在克服现有方法在长视野任务、动态环境适应和实时性能方面的局限性。通过在7自由度Franka机器人上的实验，HiBerNAC在长视野任务平均完成时间上减少23%，在多路径任务上实现了12-31%的非零成功率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了HiBerNAC框架，首次将神经科学的分层决策机制与多智能体协作结合，用于机器人操作。</li><li>设计了多频率异步处理架构，包括0.01Hz的高层规划、0.1Hz的海马体记忆和100Hz的实时反应式VLA控制。</li><li>实现了模块化神经智能体结构，包括前额叶规划器（PFP）、感知智能体（PA）、语义智能体（SA）、操作智能体（MA）和海马体模块（HM）。</li><li>开发了基于DAG的异步管道（DBHTN），支持分层任务管理和动态资源分配。</li></ol>\n<h3>论文方法描述</h3>\n<p>HiBerNAC采用三层架构：</p>\n<ol><li>多智能体神经结构：模拟人脑神经分工，PFP（0.01Hz）分解高级任务，SA处理语言指令，MA执行动作，IA提供误差校正，HM（0.1Hz）管理情景记忆。智能体间通过连接矩阵F_ij通信。</li><li>异步管道：实现DBHTN规划器，将任务分解为状态和转换，使用潜在向量l_t、动作历史h_a和语义记忆m_s。状态审查机制以0.1Hz运行。</li><li>反应式VLA（RVLA）：基于开源预训练模型，以100Hz处理低级控制，用于简单反应任务。</li></ol>\n\n<p>系统使用数学公式建模智能体交互：o_t^i = A_i(x_t^i, s_t^i, m_t; Θ_i) + ∑_{j≠i}F_ij · o_{t-1}^j，其中A_i为智能体特定函数，F_ij为连接矩阵。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：Open X-Embodiment数据集</p>\n<p>训练资源：未明确指定硬件，但提及使用预训练VLA模型（如OpenVLA）和神经网络模块。实验在7自由度Franka机器人上进行，包括仿真和真实世界测试。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：Isaac Gym仿真环境和真实世界的7自由度Franka机器人平台。</p>\n<p>评估指标：1. 任务成功率（多路径任务的非零成功率） 2. 平均任务完成时间（长视野任务） 3. 动态环境适应性（遮挡/动态删除场景） 4. 多模态任务成功率（语言-视觉-动作一致性）</p>"
  },
  {
    "date": "2025-06-09",
    "title": "Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework",
    "link": "http://arxiv.org/abs/2506.08185",
    "summary_markdown": "论文研究单位\nCedars-Sinai Medical Center, 700 N. San Vicente Blvd., West Hollywood, CA 90069\n\n论文概述\n该论文提出了一种新颖的代理建模方法，用于机器人手术中外科医生特定行为的预测。该方法将离散扩散框架与视觉-语言-动作(VLA)管道相结合，将手势预测构建为结构化序列去噪任务，条件输入包括手术视频、意图语言以及外科医生身份和技能的个性化嵌入。研究通过JIGSAWS数据集评估方法，并使用成员推断攻击量化隐私风险。\n\n论文核心贡献点\n1. 基于扩散的个性化离散手势预测建模方法，用于机器人手术\n2. 基于语言模型的嵌入方案，通过第三方大语言模型对自然语言提示编码外科医生身份和技能线索\n3. 使用成员推断攻击进行定量隐私分析，评估身份泄漏风险\n4. 在JIGSAWS数据集上进行的全面实验，涵盖手势预测准确性和风格一致性\n\n论文方法描述\n1. 前向过程：通过多项式噪声过程逐步破坏手势序列，使用时间索引转移矩阵控制噪声调度\n2. 反向过程：训练基于Transformer的模型来预测原始手势序列，条件输入包括视觉特征、语言特征、外科医生嵌入和时间步嵌入\n3. 外科医生嵌入：通过可学习嵌入层或第三方LLM（如Sentence-BERT、MiniLM）从自然语言提示生成\n4. 训练目标：使用交叉熵损失在所有手势位置计算损失\n5. 推理：从完全噪声序列开始，迭代应用学习到的反向过程生成去噪手势序列\n\n论文使用数据集和训练资源\n数据集：JIGSAWS数据集\n训练配置：20个epoch，Adam优化器，学习率1e-3，批量大小32\n网络架构：所有投影层、嵌入和去噪网络在512维隐藏空间运行\n手势空间：16个token（15个外科手势+1个MASK token）\n序列长度：5个离散手势token\n噪声步数：10个离散时间步\n\n论文使用的评估环境和评估指标\n预测性能评估：Top-1 Accuracy, Top-5 Accuracy, Weighted F1-Score\n隐私风险评估：Membership Inference Attack（AUC, Accuracy, Precision, Recall, F1 Score）\n可视化分析：t-SNE降维可视化外科医生嵌入空间，热图展示手势分布",
    "summary_html": "<p>论文研究单位</p>\n<p>Cedars-Sinai Medical Center, 700 N. San Vicente Blvd., West Hollywood, CA 90069</p>\n\n<p>论文概述</p>\n<p>该论文提出了一种新颖的代理建模方法，用于机器人手术中外科医生特定行为的预测。该方法将离散扩散框架与视觉-语言-动作(VLA)管道相结合，将手势预测构建为结构化序列去噪任务，条件输入包括手术视频、意图语言以及外科医生身份和技能的个性化嵌入。研究通过JIGSAWS数据集评估方法，并使用成员推断攻击量化隐私风险。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>基于扩散的个性化离散手势预测建模方法，用于机器人手术</li><li>基于语言模型的嵌入方案，通过第三方大语言模型对自然语言提示编码外科医生身份和技能线索</li><li>使用成员推断攻击进行定量隐私分析，评估身份泄漏风险</li><li>在JIGSAWS数据集上进行的全面实验，涵盖手势预测准确性和风格一致性</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>前向过程：通过多项式噪声过程逐步破坏手势序列，使用时间索引转移矩阵控制噪声调度</li><li>反向过程：训练基于Transformer的模型来预测原始手势序列，条件输入包括视觉特征、语言特征、外科医生嵌入和时间步嵌入</li><li>外科医生嵌入：通过可学习嵌入层或第三方LLM（如Sentence-BERT、MiniLM）从自然语言提示生成</li><li>训练目标：使用交叉熵损失在所有手势位置计算损失</li><li>推理：从完全噪声序列开始，迭代应用学习到的反向过程生成去噪手势序列</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：JIGSAWS数据集</p>\n<p>训练配置：20个epoch，Adam优化器，学习率1e-3，批量大小32</p>\n<p>网络架构：所有投影层、嵌入和去噪网络在512维隐藏空间运行</p>\n<p>手势空间：16个token（15个外科手势+1个MASK token）</p>\n<p>序列长度：5个离散手势token</p>\n<p>噪声步数：10个离散时间步</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>预测性能评估：Top-1 Accuracy, Top-5 Accuracy, Weighted F1-Score</p>\n<p>隐私风险评估：Membership Inference Attack（AUC, Accuracy, Precision, Recall, F1 Score）</p>\n<p>可视化分析：t-SNE降维可视化外科医生嵌入空间，热图展示手势分布</p>"
  },
  {
    "date": "2025-06-09",
    "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models",
    "link": "http://arxiv.org/abs/2506.07961",
    "summary_markdown": "### 论文研究单位\nCASIA, ByteDance Seed, UCAS, FiveAges, NJU.\n### 论文概述\n论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。\n### 论文核心贡献点\n1. 提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。\n2. 提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。\n3. 在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。\n### 论文方法描述\nBridgeVLA采用两阶段训练策略：\n1. 2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。\n2. 3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：RoboPoint的120K对象检测分割。\n - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。\n- **训练资源**：\n - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。\n - 训练采用AdamW优化器，具体超参数见附录。\n - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。\n - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。\n- **评估指标**：\n - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。\n - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。\n - 样本效率：在仅3条轨迹/任务训练下的成功率。</think>\n### 论文研究单位\nCASIA, ByteDance Seed, UCAS, FiveAges, NJU\n### 论文概述\n论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。\n### 论文核心贡献点\n1. 提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。\n2. 提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。\n3. 在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。\n### 论文方法描述\nBridgeVLA采用两阶段训练策略：\n1. 2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。\n2. 3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：RoboPoint的120K对象检测分割。\n - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。\n- **训练资源**：\n - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。\n - 训练采用AdamW优化器，具体超参数见附录。\n - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。\n - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。\n- **评估指标**：\n - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。\n - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。\n - 样本效率：在仅3条轨迹/任务训练下的成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>CASIA, ByteDance Seed, UCAS, FiveAges, NJU.</p>\n<h3>论文概述</h3>\n<p>论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。</li><li>提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。</li><li>在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。</li></ol>\n<h3>论文方法描述</h3>\n<p>BridgeVLA采用两阶段训练策略：</p>\n<ol><li>2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。</li><li>3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：RoboPoint的120K对象检测分割。</p>\n<p> - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。</p>\n<p> - 训练采用AdamW优化器，具体超参数见附录。</p>\n<p> - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。</p>\n<p> - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。</p>\n<p> - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。</p>\n<p> - 样本效率：在仅3条轨迹/任务训练下的成功率。</think></p>\n<h3>论文研究单位</h3>\n<p>CASIA, ByteDance Seed, UCAS, FiveAges, NJU</p>\n<h3>论文概述</h3>\n<p>论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。</li><li>提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。</li><li>在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。</li></ol>\n<h3>论文方法描述</h3>\n<p>BridgeVLA采用两阶段训练策略：</p>\n<ol><li>2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。</li><li>3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：RoboPoint的120K对象检测分割。</p>\n<p> - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。</p>\n<p> - 训练采用AdamW优化器，具体超参数见附录。</p>\n<p> - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。</p>\n<p> - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。</p>\n<p> - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。</p>\n<p> - 样本效率：在仅3条轨迹/任务训练下的成功率。</p>"
  },
  {
    "date": "2025-06-09",
    "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
    "link": "http://arxiv.org/abs/2506.07639",
    "summary_markdown": "论文研究单位\n- 伦敦大学学院计算机科学系\n- 弗赖堡大学计算机科学系\n- 思科研究院\n\n论文概述\n论文提出Fast ECoT，一种用于加速具身思维链（ECoT）推理的推理时方法。ECoT虽然能提升视觉-语言-动作（VLA）模型的性能和可解释性，但其顺序自回归的令牌生成方式导致了高推理延迟。Fast ECoT利用ECoT的结构化与重复性特点，通过缓存和重用跨时间步的高层推理、并行生成模块化推理步骤，以及引入异步调度器来解耦推理与动作解码，从而显著降低延迟。该方法无需更改模型或额外训练，易于集成到现有VLA流程中。\n\n论文核心贡献点\n- 提出一种无需训练的推理时加速方法Fast ECoT，用于解决ECoT推理的延迟瓶颈。\n- 利用ECoT推理的时间局部性，缓存并重用跨时间步的高层推理步骤，减少冗余计算。\n- 将传统的顺序推理过程转化为并行过程，通过连续批处理技术高效生成各推理模块。\n- 设计一个异步调度机制，将推理生成与动作解码解耦，优先保证动作的快速响应。\n\n论文方法描述\nFast ECoT的核心方法基于对ECoT推理特性的分析，即高层推理（如任务规划）在多个时间步中变化缓慢，而低层推理更新频繁。方法包含三个部分：\n1. **推理重用**：识别并缓存那些在不同时间步中重复出现的高层推理内容（如任务目标），在后续步骤中直接重用，避免重新生成。\n2. **并行生成**：将每个推理步骤（如规划、子任务识别、运动命令）视为独立的生成任务。结合当前观察、指令和上一时间步的推理步骤作为输入，使用连续批处理技术（如vLLM）并行处理这些长度不一的生成请求，减少因静态批处理填充带来的计算浪费。\n3. **异步更新**：鉴于动作生成（约7个令牌）远快于完整推理生成（数百个令牌），该方法将两者解耦。控制器利用缓存的高层推理快速解码动作，而完整的推理链则在后台异步更新，从而在不牺牲决策质量的前提下提升系统的响应速度。\n\n论文使用数据集和训练资源\n- **模拟环境数据集**：LIBERO基准，包含LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, 和 LIBERO-Long四个任务套件。\n- **真实世界数据集**：通过Droid远程操作管道收集了50个专家演示数据，用于真实世界任务。\n- **预训练数据**：模型基于在Bridge V2和Open X-Embodiment (OXE)数据集上预训练的ECoT模型。\n- **训练资源**：使用LoRA（rank 32）对模型进行微调，训练20万步，批大小为1，在4张NVIDIA A6000 GPU上完成。\n\n论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：LIBERO模拟环境中的Franka Emika Panda机械臂。\n - **真实世界环境**：配备RealSense D455相机的物理Franka Emika Panda机械臂，执行一系列家庭操作任务。\n - **标准化平台**：AutoEval在线评估平台，用于在BridgeData V2任务上进行可复现的第三方评估。\n- **评估指标**：\n - **成功率 (Success Rate, SR %)**：衡量任务完成情况的百分比。\n - **延迟 (Latency per Step, ms)**：每个控制步骤的推理时间，以毫秒为单位，包括均值和标准差。\n - **动作忠实性 (Action Faithfulness, AF)**：一个衡量CoT推理忠实度的指标，计算在生成不同数量推理步骤后预测的动作与最终动作之间的L1距离，距离越大表示对后续推理步骤的依赖性越高，忠实性越好。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>伦敦大学学院计算机科学系</li><li>弗赖堡大学计算机科学系</li><li>思科研究院</li></ul>\n\n<p>论文概述</p>\n<p>论文提出Fast ECoT，一种用于加速具身思维链（ECoT）推理的推理时方法。ECoT虽然能提升视觉-语言-动作（VLA）模型的性能和可解释性，但其顺序自回归的令牌生成方式导致了高推理延迟。Fast ECoT利用ECoT的结构化与重复性特点，通过缓存和重用跨时间步的高层推理、并行生成模块化推理步骤，以及引入异步调度器来解耦推理与动作解码，从而显著降低延迟。该方法无需更改模型或额外训练，易于集成到现有VLA流程中。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出一种无需训练的推理时加速方法Fast ECoT，用于解决ECoT推理的延迟瓶颈。</li><li>利用ECoT推理的时间局部性，缓存并重用跨时间步的高层推理步骤，减少冗余计算。</li><li>将传统的顺序推理过程转化为并行过程，通过连续批处理技术高效生成各推理模块。</li><li>设计一个异步调度机制，将推理生成与动作解码解耦，优先保证动作的快速响应。</li></ul>\n\n<p>论文方法描述</p>\n<p>Fast ECoT的核心方法基于对ECoT推理特性的分析，即高层推理（如任务规划）在多个时间步中变化缓慢，而低层推理更新频繁。方法包含三个部分：</p>\n<ol><li><strong>推理重用</strong>：识别并缓存那些在不同时间步中重复出现的高层推理内容（如任务目标），在后续步骤中直接重用，避免重新生成。</li><li><strong>并行生成</strong>：将每个推理步骤（如规划、子任务识别、运动命令）视为独立的生成任务。结合当前观察、指令和上一时间步的推理步骤作为输入，使用连续批处理技术（如vLLM）并行处理这些长度不一的生成请求，减少因静态批处理填充带来的计算浪费。</li><li><strong>异步更新</strong>：鉴于动作生成（约7个令牌）远快于完整推理生成（数百个令牌），该方法将两者解耦。控制器利用缓存的高层推理快速解码动作，而完整的推理链则在后台异步更新，从而在不牺牲决策质量的前提下提升系统的响应速度。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>模拟环境数据集</strong>：LIBERO基准，包含LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, 和 LIBERO-Long四个任务套件。</li><li><strong>真实世界数据集</strong>：通过Droid远程操作管道收集了50个专家演示数据，用于真实世界任务。</li><li><strong>预训练数据</strong>：模型基于在Bridge V2和Open X-Embodiment (OXE)数据集上预训练的ECoT模型。</li><li><strong>训练资源</strong>：使用LoRA（rank 32）对模型进行微调，训练20万步，批大小为1，在4张NVIDIA A6000 GPU上完成。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：LIBERO模拟环境中的Franka Emika Panda机械臂。</p>\n<p> - <strong>真实世界环境</strong>：配备RealSense D455相机的物理Franka Emika Panda机械臂，执行一系列家庭操作任务。</p>\n<p> - <strong>标准化平台</strong>：AutoEval在线评估平台，用于在BridgeData V2任务上进行可复现的第三方评估。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率 (Success Rate, SR %)</strong>：衡量任务完成情况的百分比。</p>\n<p> - <strong>延迟 (Latency per Step, ms)</strong>：每个控制步骤的推理时间，以毫秒为单位，包括均值和标准差。</p>\n<p> - <strong>动作忠实性 (Action Faithfulness, AF)</strong>：一个衡量CoT推理忠实度的指标，计算在生成不同数量推理步骤后预测的动作与最终动作之间的L1距离，距离越大表示对后续推理步骤的依赖性越高，忠实性越好。</p>"
  },
  {
    "date": "2025-06-09",
    "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
    "link": "http://arxiv.org/abs/2506.07530",
    "summary_markdown": "```markdown\n# 论文研究单位\nKey Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences\n# 论文概述\n本文提出了BitVLA，这是第一个用于机器人操作的1-bit视觉-语言-动作（VLA）模型，其所有参数均为三元值（-1, 0, 1）。该研究旨在解决现有VLA模型因体积过大而难以部署在资源受限的机器人系统上的问题。尽管缺乏大规模的机器人预训练，BitVLA在LIBERO基准测试上实现了与当前最先进的OpenVLA-OFT（4位后训练量化模型）相当的性能，同时仅消耗其29.8%的内存，这使其在内存受限的边缘设备上具有广阔的应用前景。\n# 论文核心贡献点\n1. 提出了首个1-bit VLA模型BitVLA，其中语言模型和视觉编码器的所有参数均被量化为三元值（-1, 0, 1），显著降低了模型部署的内存和计算成本。\n2. 设计了一种蒸馏感知训练策略，用于将全精度的视觉编码器压缩至1.58-bit。该方法利用全精度编码器作为教师模型，通过最小化中间层表示之间的差异来对齐学生模型（1.58-bit）的潜在表示，有效保留了性能。\n3. 通过在LIBERO基准测试上的实验证明，BitVLA在多个机器人操作任务上的平均成功率与OpenVLA-OFT（4-bit）模型相当，但内存占用仅为后者的29.8%，验证了其在资源受限环境下的高效性和可行性。\n# 论文方法描述\nBitVLA的构建分为三个主要部分：\n1. 模型架构：以1-bit LLM BitNet b1.58 2B4T为语言骨干，使用SigLIP-L作为视觉编码器，并通过一个两层的MLP连接器进行对齐。模型采用量化感知训练，权重使用`absmean`量化器量化为`{-1, 0, 1}`，激活值使用`absmax`量化器量化为8-bit整数（[-128, 127]）。为解决量化操作不可微的问题，训练中采用直通估计器（STE）来传递梯度。\n2. 蒸馏感知训练：为压缩视觉编码器，该方法将全精度的视觉编码器作为教师模型，指导一个1.58-bit权重的学生模型进行训练。总损失函数由两部分组成：标准的语言建模损失（`L_LM`）和表示对齐损失（`L_aux`）。表示对齐损失通过计算教师和学生模型各层输出特征之间的均方误差来确保潜在表示的一致性，其系数`γ`设为0.1。\n3. 机器人微调：在特定机器人任务上，模型采用与OpenVLA-OFT相同的微调策略。具体包括使用双向注意力掩码替代因果掩码以实现并行解码，并采用动作分块技术一次性生成多个时间步的动作序列。同时，添加一个基于MLP的动作头，将模型的输出映射到连续的机器人动作空间，并通过最小化L1损失进行训练。\n# 论文使用数据集和训练资源\n模型训练遵循一个三阶段流程：\n1. 第一阶段（连接器预训练）：使用LLaVA 1.5-558k数据集。\n2. 第二阶段（视觉指令微调）：使用MammoTH-VL数据集的1000万样本子集。\n3. 第三阶段（蒸馏训练）：使用第二阶段数据中的500万样本子集，将视觉编码器从全精度（W16A16）量化至1.58-bit权重和8-bit激活（W1.58A8），训练数据量最高达100亿个token。\n训练资源：模型在8张NVIDIA A100（80GB）显卡上完成训练，共耗时14天。\n# 论文使用的评估环境和评估指标\n评估环境：实验在LIBERO仿真环境中进行，该环境包含四个任务套件，分别评估模型在不同方面的泛化能力：\n1. LIBERO-Spatial：评估空间泛化能力。\n2. LIBERO-Object：评估对未见物体类别的泛化能力。\n3. LIBERO-Goal：评估对多样化语言指令的泛化能力。\n4. LIBERO-Long：评估长时程推理和多任务执行能力。\n评估指标：主要指标是任务成功率（%），即在各个任务套件上成功完成任务的百分比。报告了每个子任务套件的成功率以及四个套件的平均成功率。此外，在视觉问答（VQA）任务上，评估指标为零样本准确率（%）。\n```",
    "summary_html": "<p>```markdown</p>\n<h1>论文研究单位</h1>\n<p>Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences</p>\n<h1>论文概述</h1>\n<p>本文提出了BitVLA，这是第一个用于机器人操作的1-bit视觉-语言-动作（VLA）模型，其所有参数均为三元值（-1, 0, 1）。该研究旨在解决现有VLA模型因体积过大而难以部署在资源受限的机器人系统上的问题。尽管缺乏大规模的机器人预训练，BitVLA在LIBERO基准测试上实现了与当前最先进的OpenVLA-OFT（4位后训练量化模型）相当的性能，同时仅消耗其29.8%的内存，这使其在内存受限的边缘设备上具有广阔的应用前景。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出了首个1-bit VLA模型BitVLA，其中语言模型和视觉编码器的所有参数均被量化为三元值（-1, 0, 1），显著降低了模型部署的内存和计算成本。</li><li>设计了一种蒸馏感知训练策略，用于将全精度的视觉编码器压缩至1.58-bit。该方法利用全精度编码器作为教师模型，通过最小化中间层表示之间的差异来对齐学生模型（1.58-bit）的潜在表示，有效保留了性能。</li><li>通过在LIBERO基准测试上的实验证明，BitVLA在多个机器人操作任务上的平均成功率与OpenVLA-OFT（4-bit）模型相当，但内存占用仅为后者的29.8%，验证了其在资源受限环境下的高效性和可行性。</li></ol>\n<h1>论文方法描述</h1>\n<p>BitVLA的构建分为三个主要部分：</p>\n<ol><li>模型架构：以1-bit LLM BitNet b1.58 2B4T为语言骨干，使用SigLIP-L作为视觉编码器，并通过一个两层的MLP连接器进行对齐。模型采用量化感知训练，权重使用<code>absmean</code>量化器量化为<code>{-1, 0, 1}</code>，激活值使用<code>absmax</code>量化器量化为8-bit整数（[-128, 127]）。为解决量化操作不可微的问题，训练中采用直通估计器（STE）来传递梯度。</li><li>蒸馏感知训练：为压缩视觉编码器，该方法将全精度的视觉编码器作为教师模型，指导一个1.58-bit权重的学生模型进行训练。总损失函数由两部分组成：标准的语言建模损失（<code>L_LM</code>）和表示对齐损失（<code>L_aux</code>）。表示对齐损失通过计算教师和学生模型各层输出特征之间的均方误差来确保潜在表示的一致性，其系数<code>γ</code>设为0.1。</li><li>机器人微调：在特定机器人任务上，模型采用与OpenVLA-OFT相同的微调策略。具体包括使用双向注意力掩码替代因果掩码以实现并行解码，并采用动作分块技术一次性生成多个时间步的动作序列。同时，添加一个基于MLP的动作头，将模型的输出映射到连续的机器人动作空间，并通过最小化L1损失进行训练。</li></ol>\n<h1>论文使用数据集和训练资源</h1>\n<p>模型训练遵循一个三阶段流程：</p>\n<ol><li>第一阶段（连接器预训练）：使用LLaVA 1.5-558k数据集。</li><li>第二阶段（视觉指令微调）：使用MammoTH-VL数据集的1000万样本子集。</li><li>第三阶段（蒸馏训练）：使用第二阶段数据中的500万样本子集，将视觉编码器从全精度（W16A16）量化至1.58-bit权重和8-bit激活（W1.58A8），训练数据量最高达100亿个token。</li></ol>\n<p>训练资源：模型在8张NVIDIA A100（80GB）显卡上完成训练，共耗时14天。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>评估环境：实验在LIBERO仿真环境中进行，该环境包含四个任务套件，分别评估模型在不同方面的泛化能力：</p>\n<ol><li>LIBERO-Spatial：评估空间泛化能力。</li><li>LIBERO-Object：评估对未见物体类别的泛化能力。</li><li>LIBERO-Goal：评估对多样化语言指令的泛化能力。</li><li>LIBERO-Long：评估长时程推理和多任务执行能力。</li></ol>\n<p>评估指标：主要指标是任务成功率（%），即在各个任务套件上成功完成任务的百分比。报告了每个子任务套件的成功率以及四个套件的平均成功率。此外，在视觉问答（VQA）任务上，评估指标为零样本准确率（%）。</p>\n<p>```</p>"
  },
  {
    "date": "2025-06-09",
    "title": "Real-Time Execution of Action Chunking Flow Policies",
    "link": "http://arxiv.org/abs/2506.07339",
    "summary_markdown": "- 论文研究单位: Physical Intelligence 和 UC Berkeley\n- 论文概述: 本文提出了一种名为实时分块（RTC）的新型推理时算法，该算法通过将异步执行动作分块策略问题转化为修复（inpainting）问题，实现了平滑异步执行。该方法适用于任何基于扩散或流的视觉-语言-动作（VLA）模型，无需重新训练，通过在执行当前动作块的同时生成下一个动作块，并“冻结”保证执行的动作，“修复”其余部分。实验表明，RTC在模拟和真实世界的动态任务中表现出对推理延迟的独特鲁棒性，显著提高了任务吞吐量。\n- 论文核心贡献点: 1. 提出实时分块（RTC）算法，将异步动作分块执行问题转化为推理时修复问题。 2. 引入软掩码技术，通过指数衰减的引导权重改善跨块连续性。 3. 提出引导权重裁剪（β）以增强训练自由修复在少量去噪步下的稳定性。 4. 创建了一个包含12个高度动态任务的新基准（基于Kinetix模拟器），并评估了6个具有挑战性的真实世界双手机器人操作任务。 5. 证明RTC在显著推理延迟下（例如>300ms）仍能保持高成功率（如点火柴），并提升任务效率。\n- 论文方法描述: 该方法基于条件流匹配模型。在推理时，它将动作序列的一部分视为待修复区域：前d个动作（由推理延迟d决定）被“冻结”并施加单位引导权重，中间的重叠区域使用指数衰减的软权重（从1到0），后续s个动作（执行范围）权重为零。具体而言，通过向学习到的速度场v_π添加基于伪逆引导（ΠGDM）的梯度项来实现修复。该梯度项鼓励最终生成与目标Y匹配，其中Y是损坏版本（掩码动作序列）的期望结果。引导权重通过min(β, (1-τ)/(τ·r_τ^2))进行裁剪，其中r_τ^2 = (1-τ)^2/(τ^2 + (1-τ)^2)，τ是流匹配时间步。新动作块生成时，使用前一块的H-s个重叠动作作为软掩码输入，确保策略在引入新观测的同时保持连续性。\n- 论文使用数据集和训练资源: 使用基于Kinetix模拟器创建的新基准，包含12个高度动态的操控和运动任务。在真实世界评估中，使用π_0.5 VLA作为基础策略在6个具有挑战性的双手机器人操作任务上进行测试。训练资源方面，未对基础策略（π_0.5）进行微调，仅进行推理时修改。实验使用NVIDIA RTX 4090 GPU进行评估，具体计算资源未在摘要中明确说明，但引用了π_0模型在相似硬件上的前46ms KV缓存预填充延迟（在50Hz控制频率下）。\n- 论文使用的评估环境和评估指标: 评估环境包括两个部分： 1. 模拟基准：使用Kinetix模拟器，评估12个高度动态的任务（如操控和移动）。 2. 真实世界：使用真实的双手机器人系统，执行6个挑战性操作任务（例如点火柴）。评估指标包括任务成功率、任务吞吐量（执行速度）、跨块连续性（通过位置、速度和加速度曲线测量）以及对推理延迟的鲁棒性（在不同延迟下评估成功率）。与基线（如同步推理、时间集成、双向解码BID）比较，RTC在平滑性、速度和延迟容忍度方面表现出优越性。</think>\n论文研究单位\nPhysical Intelligence 和 UC Berkeley\n\n论文概述\n本文提出了一种名为实时分块（RTC）的新型推理时算法，该算法通过将异步执行动作分块策略问题转化为修复（inpainting）问题，实现了平滑异步执行。该方法适用于任何基于扩散或流的视觉-语言-动作（VLA）模型，无需重新训练，通过在执行当前动作块的同时生成下一个动作块，并“冻结”保证执行的动作，“修复”其余部分。实验表明，RTC在模拟和真实世界的动态任务中表现出对推理延迟的独特鲁棒性，显著提高了任务吞吐量。\n\n论文核心贡献点\n1. 提出实时分块（RTC）算法，将异步动作分块执行问题转化为推理时修复问题。\n2. 引入软掩码技术，通过指数衰减的引导权重改善跨块连续性。\n3. 提出引导权重裁剪（β）以增强训练自由修复在少量去噪步下的稳定性。\n4. 创建了一个包含12个高度动态任务的新基准（基于Kinetix模拟器），并评估了6个具有挑战性的真实世界双手机器人操作任务。\n5. 证明RTC在显著推理延迟下（例如>300ms）仍能保持高成功率（如点火柴），并提升任务效率。\n\n论文方法描述\n该方法基于条件流匹配模型。在推理时，它将动作序列的一部分视为待修复区域：前d个动作（由推理延迟d决定）被“冻结”并施加单位引导权重，中间的重叠区域使用指数衰减的软权重（从1到0），后续s个动作（执行范围）权重为零。具体而言，通过向学习到的速度场v_π添加基于伪逆引导（ΠGDM）的梯度项来实现修复。该梯度项鼓励最终生成与目标Y匹配，其中Y是损坏版本（掩码动作序列）的期望结果。引导权重通过min(β, (1-τ)/(τ·r_τ^2))进行裁剪，其中r_τ^2 = (1-τ)^2/(τ^2 + (1-τ)^2)，τ是流匹配时间步。新动作块生成时，使用前一块的H-s个重叠动作作为软掩码输入，确保策略在引入新观测的同时保持连续性。\n\n论文使用数据集和训练资源\n使用基于Kinetix模拟器创建的新基准，包含12个高度动态的操控和运动任务。在真实世界评估中，使用π_0.5 VLA作为基础策略在6个具有挑战性的双手机器人操作任务上进行测试。训练资源方面，未对基础策略（π_0.5）进行微调，仅进行推理时修改。实验使用NVIDIA RTX 4090 GPU进行评估，具体计算资源未在摘要中明确说明，但引用了π_0模型在相似硬件上的前46ms KV缓存预填充延迟（在50Hz控制频率下）。\n\n论文使用的评估环境和评估指标\n评估环境包括两个部分： 1. 模拟基准：使用Kinetix模拟器，评估12个高度动态的任务（如操控和移动）。 2. 真实世界：使用真实的双手机器人系统，执行6个挑战性操作任务（例如点火柴）。评估指标包括任务成功率、任务吞吐量（执行速度）、跨块连续性（通过位置、速度和加速度曲线测量）以及对推理延迟的鲁棒性（在不同延迟下评估成功率）。与基线（如同步推理、时间集成、双向解码BID）比较，RTC在平滑性、速度和延迟容忍度方面表现出优越性。",
    "summary_html": "<ul><li>论文研究单位: Physical Intelligence 和 UC Berkeley</li><li>论文概述: 本文提出了一种名为实时分块（RTC）的新型推理时算法，该算法通过将异步执行动作分块策略问题转化为修复（inpainting）问题，实现了平滑异步执行。该方法适用于任何基于扩散或流的视觉-语言-动作（VLA）模型，无需重新训练，通过在执行当前动作块的同时生成下一个动作块，并“冻结”保证执行的动作，“修复”其余部分。实验表明，RTC在模拟和真实世界的动态任务中表现出对推理延迟的独特鲁棒性，显著提高了任务吞吐量。</li><li>论文核心贡献点: 1. 提出实时分块（RTC）算法，将异步动作分块执行问题转化为推理时修复问题。 2. 引入软掩码技术，通过指数衰减的引导权重改善跨块连续性。 3. 提出引导权重裁剪（β）以增强训练自由修复在少量去噪步下的稳定性。 4. 创建了一个包含12个高度动态任务的新基准（基于Kinetix模拟器），并评估了6个具有挑战性的真实世界双手机器人操作任务。 5. 证明RTC在显著推理延迟下（例如>300ms）仍能保持高成功率（如点火柴），并提升任务效率。</li><li>论文方法描述: 该方法基于条件流匹配模型。在推理时，它将动作序列的一部分视为待修复区域：前d个动作（由推理延迟d决定）被“冻结”并施加单位引导权重，中间的重叠区域使用指数衰减的软权重（从1到0），后续s个动作（执行范围）权重为零。具体而言，通过向学习到的速度场v_π添加基于伪逆引导（ΠGDM）的梯度项来实现修复。该梯度项鼓励最终生成与目标Y匹配，其中Y是损坏版本（掩码动作序列）的期望结果。引导权重通过min(β, (1-τ)/(τ·r_τ^2))进行裁剪，其中r_τ^2 = (1-τ)^2/(τ^2 + (1-τ)^2)，τ是流匹配时间步。新动作块生成时，使用前一块的H-s个重叠动作作为软掩码输入，确保策略在引入新观测的同时保持连续性。</li><li>论文使用数据集和训练资源: 使用基于Kinetix模拟器创建的新基准，包含12个高度动态的操控和运动任务。在真实世界评估中，使用π_0.5 VLA作为基础策略在6个具有挑战性的双手机器人操作任务上进行测试。训练资源方面，未对基础策略（π_0.5）进行微调，仅进行推理时修改。实验使用NVIDIA RTX 4090 GPU进行评估，具体计算资源未在摘要中明确说明，但引用了π_0模型在相似硬件上的前46ms KV缓存预填充延迟（在50Hz控制频率下）。</li><li>论文使用的评估环境和评估指标: 评估环境包括两个部分： 1. 模拟基准：使用Kinetix模拟器，评估12个高度动态的任务（如操控和移动）。 2. 真实世界：使用真实的双手机器人系统，执行6个挑战性操作任务（例如点火柴）。评估指标包括任务成功率、任务吞吐量（执行速度）、跨块连续性（通过位置、速度和加速度曲线测量）以及对推理延迟的鲁棒性（在不同延迟下评估成功率）。与基线（如同步推理、时间集成、双向解码BID）比较，RTC在平滑性、速度和延迟容忍度方面表现出优越性。</think></li></ul>\n<p>论文研究单位</p>\n<p>Physical Intelligence 和 UC Berkeley</p>\n\n<p>论文概述</p>\n<p>本文提出了一种名为实时分块（RTC）的新型推理时算法，该算法通过将异步执行动作分块策略问题转化为修复（inpainting）问题，实现了平滑异步执行。该方法适用于任何基于扩散或流的视觉-语言-动作（VLA）模型，无需重新训练，通过在执行当前动作块的同时生成下一个动作块，并“冻结”保证执行的动作，“修复”其余部分。实验表明，RTC在模拟和真实世界的动态任务中表现出对推理延迟的独特鲁棒性，显著提高了任务吞吐量。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出实时分块（RTC）算法，将异步动作分块执行问题转化为推理时修复问题。</li><li>引入软掩码技术，通过指数衰减的引导权重改善跨块连续性。</li><li>提出引导权重裁剪（β）以增强训练自由修复在少量去噪步下的稳定性。</li><li>创建了一个包含12个高度动态任务的新基准（基于Kinetix模拟器），并评估了6个具有挑战性的真实世界双手机器人操作任务。</li><li>证明RTC在显著推理延迟下（例如>300ms）仍能保持高成功率（如点火柴），并提升任务效率。</li></ol>\n\n<p>论文方法描述</p>\n<p>该方法基于条件流匹配模型。在推理时，它将动作序列的一部分视为待修复区域：前d个动作（由推理延迟d决定）被“冻结”并施加单位引导权重，中间的重叠区域使用指数衰减的软权重（从1到0），后续s个动作（执行范围）权重为零。具体而言，通过向学习到的速度场v_π添加基于伪逆引导（ΠGDM）的梯度项来实现修复。该梯度项鼓励最终生成与目标Y匹配，其中Y是损坏版本（掩码动作序列）的期望结果。引导权重通过min(β, (1-τ)/(τ·r_τ^2))进行裁剪，其中r_τ^2 = (1-τ)^2/(τ^2 + (1-τ)^2)，τ是流匹配时间步。新动作块生成时，使用前一块的H-s个重叠动作作为软掩码输入，确保策略在引入新观测的同时保持连续性。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用基于Kinetix模拟器创建的新基准，包含12个高度动态的操控和运动任务。在真实世界评估中，使用π_0.5 VLA作为基础策略在6个具有挑战性的双手机器人操作任务上进行测试。训练资源方面，未对基础策略（π_0.5）进行微调，仅进行推理时修改。实验使用NVIDIA RTX 4090 GPU进行评估，具体计算资源未在摘要中明确说明，但引用了π_0模型在相似硬件上的前46ms KV缓存预填充延迟（在50Hz控制频率下）。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境包括两个部分： 1. 模拟基准：使用Kinetix模拟器，评估12个高度动态的任务（如操控和移动）。 2. 真实世界：使用真实的双手机器人系统，执行6个挑战性操作任务（例如点火柴）。评估指标包括任务成功率、任务吞吐量（执行速度）、跨块连续性（通过位置、速度和加速度曲线测量）以及对推理延迟的鲁棒性（在不同延迟下评估成功率）。与基线（如同步推理、时间集成、双向解码BID）比较，RTC在平滑性、速度和延迟容忍度方面表现出优越性。</p>"
  },
  {
    "date": "2025-06-08",
    "title": "Robotic Policy Learning via Human-assisted Action Preference Optimization",
    "link": "http://arxiv.org/abs/2506.07127",
    "summary_markdown": "```markdown\n### 论文研究单位\n中国人民大学高瓴人工智能学院、ByteDance Seed、下一代智能搜索与推荐教育部工程研究中心、大型模型与智能治理北京市重点实验室。\n### 论文概述\n该论文提出了一种名为“动作偏好优化（APO）”的新方法，用于通过人类辅助的偏好对齐来优化视觉-语言-动作（VLA）模型。该方法旨在解决VLA模型在部署后难以通过失败经验进行持续优化的局限性。APO包含一个用于可靠部署和交互轨迹收集的人-机器人协作框架，以及一个利用二元可取性信号和自适应重新加权算法的优化过程。该算法能有效抑制失败动作并增强纠错能力，使VLA模型能够从失败中学习，实现迭代改进，从而在动态环境中可靠部署。实验在模拟和真实场景中验证了该方法的有效性。\n### 论文核心贡献点\n1. 提出了一种新的VLA模型优化范式——动作偏好优化（APO），它通过动作级别的偏好对齐，超越了行为克隆和强化学习的限制。\n2. 设计了一个用于可靠部署和交互轨迹收集的人-机器人协作框架，通过实时人工干预保证任务成功并提供修正轨迹。\n3. 针对机器人交互的不可逆性和VLA模型中动作标记与连续动作之间的概率失配问题，提出了一种自适应重新加权算法。该算法利用从交互中派生的二元可取性信号，引导模型有效学习。\n4. APO使VLA模型具备了从失败中学习的关键能力，为机器人在真实世界中的持续迭代优化铺平了道路。\n5. 在多种模拟和真实世界的操作任务上进行了全面实验，证明了APO在泛化性和鲁棒性方面的优越性能。\n### 论文方法描述\n该方法分为两个主要部分：\n1. **人-机器人协作部署**：\n - 首先，通过行为克隆（BC）在专家演示数据集上微调预训练的VLA模型，得到一个基础策略。\n - 部署该策略与环境交互。当策略执行失败时，人工操作员进行实时干预，确保任务完成并收集修正轨迹。\n - 对交互轨迹进行标注：将干预前K步内的动作标记为“不可取”，而人类干预和策略成功执行的动作标记为“可取”。\n - 将连续动作离散化为动作标记。\n2. **动作偏好优化（APO）**：\n - 针对不可逆交互和动作标记概率失配两个挑战，构建了一个基于前景理论的效用函数，该函数从二元可取性信号（而非配对偏好）中学习。\n - 采用一个损失函数，通过KL散度项约束优化过程，使模型在参考模型的基础上学习偏好对齐，避免灾难性遗忘。\n - 提出一种自适应重新加权方法：计算每个样本的连续动作L1损失并进行批次归一化，得到样本权重。然后动态调整可取和不可取样本在损失函数中的权重，使模型更关注预测误差大的样本（即失败相关的动作），从而弥合了离散标记预测和连续动作回归之间的差距。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **模拟**：使用RoboMimic数据集，包含`Coffee_D0`、`StackThree_D0`、`ThreePieceAssembly_D0`和`Square_D0`等4个长时程操作任务。每个任务使用300个专家演示进行初始微调，并收集50条交互轨迹用于APO优化。\n - **真实世界**：设计了“将方块插入杆中”的精细操作任务。通过SpaceMouse设备收集了100条专家演示轨迹，并部署策略后收集了20条人工干预轨迹。此外，还在“挂杯子”和“放柠檬”等任务上进行了验证。\n- **训练资源**：\n - **基础模型**：主要对OpenVLA模型进行微调，并在π0-FAST模型上验证了泛化性。\n - **硬件**：初始微调使用8块NVIDIA A100 GPU（LoRA, rank=32, batch size=16），APO优化阶段使用4块NVIDIA A100 GPU（learning rate=5e-5, batch size=8）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：RoboMimic模拟环境。\n - **真实世界环境**：使用真实机械臂执行操作任务。为了测试泛化能力，设计了多种扰动场景，包括位置扰动（改变物体初始位置）、背景扰动（更换背景颜色）和纹理扰动（更换物体纹理）。\n- **评估指标**：\n - 主要评估指标是**任务平均成功率**。在模拟环境中，每个任务在不同随机种子下进行50次试验并报告平均成功率。在真实世界中，对每种场景进行20次试验并报告平均成功率。\n - 在终身学习实验中，每20次交互后更新模型并进行50次评估，绘制成功率随迭代次数的变化曲线。\n - 通过比较不同方法在原始任务和扰动场景下的性能，评估模型的泛化和抗干扰能力。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>中国人民大学高瓴人工智能学院、ByteDance Seed、下一代智能搜索与推荐教育部工程研究中心、大型模型与智能治理北京市重点实验室。</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种名为“动作偏好优化（APO）”的新方法，用于通过人类辅助的偏好对齐来优化视觉-语言-动作（VLA）模型。该方法旨在解决VLA模型在部署后难以通过失败经验进行持续优化的局限性。APO包含一个用于可靠部署和交互轨迹收集的人-机器人协作框架，以及一个利用二元可取性信号和自适应重新加权算法的优化过程。该算法能有效抑制失败动作并增强纠错能力，使VLA模型能够从失败中学习，实现迭代改进，从而在动态环境中可靠部署。实验在模拟和真实场景中验证了该方法的有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新的VLA模型优化范式——动作偏好优化（APO），它通过动作级别的偏好对齐，超越了行为克隆和强化学习的限制。</li><li>设计了一个用于可靠部署和交互轨迹收集的人-机器人协作框架，通过实时人工干预保证任务成功并提供修正轨迹。</li><li>针对机器人交互的不可逆性和VLA模型中动作标记与连续动作之间的概率失配问题，提出了一种自适应重新加权算法。该算法利用从交互中派生的二元可取性信号，引导模型有效学习。</li><li>APO使VLA模型具备了从失败中学习的关键能力，为机器人在真实世界中的持续迭代优化铺平了道路。</li><li>在多种模拟和真实世界的操作任务上进行了全面实验，证明了APO在泛化性和鲁棒性方面的优越性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法分为两个主要部分：</p>\n<p>1. <strong>人-机器人协作部署</strong>：</p>\n<p> - 首先，通过行为克隆（BC）在专家演示数据集上微调预训练的VLA模型，得到一个基础策略。</p>\n<p> - 部署该策略与环境交互。当策略执行失败时，人工操作员进行实时干预，确保任务完成并收集修正轨迹。</p>\n<p> - 对交互轨迹进行标注：将干预前K步内的动作标记为“不可取”，而人类干预和策略成功执行的动作标记为“可取”。</p>\n<p> - 将连续动作离散化为动作标记。</p>\n<p>2. <strong>动作偏好优化（APO）</strong>：</p>\n<p> - 针对不可逆交互和动作标记概率失配两个挑战，构建了一个基于前景理论的效用函数，该函数从二元可取性信号（而非配对偏好）中学习。</p>\n<p> - 采用一个损失函数，通过KL散度项约束优化过程，使模型在参考模型的基础上学习偏好对齐，避免灾难性遗忘。</p>\n<p> - 提出一种自适应重新加权方法：计算每个样本的连续动作L1损失并进行批次归一化，得到样本权重。然后动态调整可取和不可取样本在损失函数中的权重，使模型更关注预测误差大的样本（即失败相关的动作），从而弥合了离散标记预测和连续动作回归之间的差距。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>模拟</strong>：使用RoboMimic数据集，包含<code>Coffee_D0</code>、<code>StackThree_D0</code>、<code>ThreePieceAssembly_D0</code>和<code>Square_D0</code>等4个长时程操作任务。每个任务使用300个专家演示进行初始微调，并收集50条交互轨迹用于APO优化。</p>\n<p> - <strong>真实世界</strong>：设计了“将方块插入杆中”的精细操作任务。通过SpaceMouse设备收集了100条专家演示轨迹，并部署策略后收集了20条人工干预轨迹。此外，还在“挂杯子”和“放柠檬”等任务上进行了验证。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - <strong>基础模型</strong>：主要对OpenVLA模型进行微调，并在π0-FAST模型上验证了泛化性。</p>\n<p> - <strong>硬件</strong>：初始微调使用8块NVIDIA A100 GPU（LoRA, rank=32, batch size=16），APO优化阶段使用4块NVIDIA A100 GPU（learning rate=5e-5, batch size=8）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：RoboMimic模拟环境。</p>\n<p> - <strong>真实世界环境</strong>：使用真实机械臂执行操作任务。为了测试泛化能力，设计了多种扰动场景，包括位置扰动（改变物体初始位置）、背景扰动（更换背景颜色）和纹理扰动（更换物体纹理）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 主要评估指标是<strong>任务平均成功率</strong>。在模拟环境中，每个任务在不同随机种子下进行50次试验并报告平均成功率。在真实世界中，对每种场景进行20次试验并报告平均成功率。</p>\n<p> - 在终身学习实验中，每20次交互后更新模型并进行50次评估，绘制成功率随迭代次数的变化曲线。</p>\n<p> - 通过比较不同方法在原始任务和扰动场景下的性能，评估模型的泛化和抗干扰能力。</p>\n<p>```</p>"
  },
  {
    "date": "2025-06-07",
    "title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation",
    "link": "http://arxiv.org/abs/2506.06677",
    "summary_markdown": "```markdown\n# 论文研究单位\n\nBeihang University, National University of Singapore, Shanghai Jiao Tong University\n# 论文概述\n\n该论文提出了RoboCerebra，一个用于评估长期机器人操纵中高级推理能力的大规模基准。现有的机器人操纵基准大多关注短期的反应式行为，而缺乏对视觉语言模型（VLMs）在长期规划、反思和记忆等“System 2”能力方面的有效评估。RoboCerebra旨在填补这一空白，它包含一个大规模的仿真数据集、一个分层规划与执行框架以及一个多维度的评估协议，旨在全面评测VLMs在复杂、长时序任务中的表现。\n# 论文核心贡献点\n\n1. 一个大规模仿真数据集：包含1000个人类标注的轨迹，覆盖100个任务变体。与现有基准相比，该数据集的轨迹长度平均长6倍，并且拥有更密集的子任务和时间标注。\n2. 一个分层规划与执行框架（HPE）：该框架结合了一个高级的VLM规划器（负责生成和更新子目标计划）和一个低级的VLA控制器（负责执行精细动作），并通过一个共享的记忆库实现动态协调。\n3. 一个多维度的评估协议：设计了四种评估指标（任务成功率、规划准确率、规划效率、动作完成准确率）来系统性地衡量模型在规划、反思和记忆等关键认知维度上的能力。\n# 论文方法描述\n\n1. **数据集构建流程**：\n * **级联任务生成**：使用GPT根据采样到的物体配置生成高级任务指令，并将其分解为符合逻辑的子任务序列。\n * **场景初始化与验证**：将子任务序列解析为仿真器可执行代码，并通过符号仿真循环和视觉语言验证循环（使用GPT-4o）确保场景的物理和语义合理性。\n * **人类演示与标注**：人类操作员在仿真环境中执行子任务，收集轨迹数据并进行精细的时间段标注，以保证数据质量。\n\n2. **分层规划与执行框架**：\n * **训练阶段**：分两阶段进行。第一阶段，使用单步指令和动作对训练VLA模型（微调OpenVLA），使其掌握底层视觉运动技能。第二阶段，使用带成功/失败标签的视频-指令对训练VLM，使其具备任务进度监控和时序 grounding 能力。\n * **执行阶段**：VLM将高级指令解析为子目标序列并存入记忆库。VLA根据当前子目标和高频视觉输入执行具体动作。VLM定期根据低频观测监控进度，并动态更新计划，形成一个闭环系统。\n# 论文使用数据集和训练资源\n\n1. **数据集**：\n * **名称**：RoboCerebra\n * **规模**：1000条人类演示轨迹，覆盖100个任务变体。\n * **特点**：平均轨迹长度为2972.4个仿真步骤，平均包含9.1个子任务，具有动态物体变化和精细的时间段标注。\n * **平台**：基于Libero仿真平台构建。\n\n2. **训练资源**：\n * **System 1 (VLA)**：在RoboCerebra数据集的100个任务实例上对OpenVLA进行微调。训练配置为8块NVIDIA A100 GPU，全局批量大小为64，初始学习率为5e-5，输入分辨率为256x256，共训练200K步。\n * **System 2 (VLM)**：评估了多个预训练模型，如GPT-4o、Qwen2.5-VL、LLaVA-Next-Video，以及一个在论文视频指令数据集上微调过的VLM。\n# 论文使用的评估环境和评估指标\n\n1. **评估环境**：\n * Libero仿真环境。\n * 评估在60个测试任务上进行，每个任务运行10次试验，总共600次滚动测试。\n\n2. **评估指标**：\n * **任务成功率 (Success Rate, SR)**：衡量智能体是否完成了所有关键物体状态转换。\n * **平均计划匹配准确率 (Average Plan Match Accuracy, Acc_P)**：衡量VLM生成的子任务序列与人工标注的真实序列的精确匹配程度。\n * **计划效率 (Plan Efficiency, η)**：任务成功率除以平均计划长度，用于评估规划的效率。\n * **动作完成准确率 (Action Completion Accuracy, Acc_C)**：通过一个VideoQA问答基准评估模型对任务执行状态的理解（即反思能力）。\n```",
    "summary_html": "<p>```markdown</p>\n<h1>论文研究单位</h1>\n\n<p>Beihang University, National University of Singapore, Shanghai Jiao Tong University</p>\n<h1>论文概述</h1>\n\n<p>该论文提出了RoboCerebra，一个用于评估长期机器人操纵中高级推理能力的大规模基准。现有的机器人操纵基准大多关注短期的反应式行为，而缺乏对视觉语言模型（VLMs）在长期规划、反思和记忆等“System 2”能力方面的有效评估。RoboCerebra旨在填补这一空白，它包含一个大规模的仿真数据集、一个分层规划与执行框架以及一个多维度的评估协议，旨在全面评测VLMs在复杂、长时序任务中的表现。</p>\n<h1>论文核心贡献点</h1>\n\n<ol><li>一个大规模仿真数据集：包含1000个人类标注的轨迹，覆盖100个任务变体。与现有基准相比，该数据集的轨迹长度平均长6倍，并且拥有更密集的子任务和时间标注。</li><li>一个分层规划与执行框架（HPE）：该框架结合了一个高级的VLM规划器（负责生成和更新子目标计划）和一个低级的VLA控制器（负责执行精细动作），并通过一个共享的记忆库实现动态协调。</li><li>一个多维度的评估协议：设计了四种评估指标（任务成功率、规划准确率、规划效率、动作完成准确率）来系统性地衡量模型在规划、反思和记忆等关键认知维度上的能力。</li></ol>\n<h1>论文方法描述</h1>\n\n<p>1. <strong>数据集构建流程</strong>：</p>\n<p> * <strong>级联任务生成</strong>：使用GPT根据采样到的物体配置生成高级任务指令，并将其分解为符合逻辑的子任务序列。</p>\n<p> * <strong>场景初始化与验证</strong>：将子任务序列解析为仿真器可执行代码，并通过符号仿真循环和视觉语言验证循环（使用GPT-4o）确保场景的物理和语义合理性。</p>\n<p> * <strong>人类演示与标注</strong>：人类操作员在仿真环境中执行子任务，收集轨迹数据并进行精细的时间段标注，以保证数据质量。</p>\n\n<p>2. <strong>分层规划与执行框架</strong>：</p>\n<p> * <strong>训练阶段</strong>：分两阶段进行。第一阶段，使用单步指令和动作对训练VLA模型（微调OpenVLA），使其掌握底层视觉运动技能。第二阶段，使用带成功/失败标签的视频-指令对训练VLM，使其具备任务进度监控和时序 grounding 能力。</p>\n<p> * <strong>执行阶段</strong>：VLM将高级指令解析为子目标序列并存入记忆库。VLA根据当前子目标和高频视觉输入执行具体动作。VLM定期根据低频观测监控进度，并动态更新计划，形成一个闭环系统。</p>\n<h1>论文使用数据集和训练资源</h1>\n\n<p>1. <strong>数据集</strong>：</p>\n<p> * <strong>名称</strong>：RoboCerebra</p>\n<p> * <strong>规模</strong>：1000条人类演示轨迹，覆盖100个任务变体。</p>\n<p> * <strong>特点</strong>：平均轨迹长度为2972.4个仿真步骤，平均包含9.1个子任务，具有动态物体变化和精细的时间段标注。</p>\n<p> * <strong>平台</strong>：基于Libero仿真平台构建。</p>\n\n<p>2. <strong>训练资源</strong>：</p>\n<p> * <strong>System 1 (VLA)</strong>：在RoboCerebra数据集的100个任务实例上对OpenVLA进行微调。训练配置为8块NVIDIA A100 GPU，全局批量大小为64，初始学习率为5e-5，输入分辨率为256x256，共训练200K步。</p>\n<p> * <strong>System 2 (VLM)</strong>：评估了多个预训练模型，如GPT-4o、Qwen2.5-VL、LLaVA-Next-Video，以及一个在论文视频指令数据集上微调过的VLM。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n\n<p>1. <strong>评估环境</strong>：</p>\n<p> * Libero仿真环境。</p>\n<p> * 评估在60个测试任务上进行，每个任务运行10次试验，总共600次滚动测试。</p>\n\n<p>2. <strong>评估指标</strong>：</p>\n<p> * <strong>任务成功率 (Success Rate, SR)</strong>：衡量智能体是否完成了所有关键物体状态转换。</p>\n<p> * <strong>平均计划匹配准确率 (Average Plan Match Accuracy, Acc_P)</strong>：衡量VLM生成的子任务序列与人工标注的真实序列的精确匹配程度。</p>\n<p> * <strong>计划效率 (Plan Efficiency, η)</strong>：任务成功率除以平均计划长度，用于评估规划的效率。</p>\n<p> * <strong>动作完成准确率 (Action Completion Accuracy, Acc_C)</strong>：通过一个VideoQA问答基准评估模型对任务执行状态的理解（即反思能力）。</p>\n<p>```</p>"
  },
  {
    "date": "2025-06-06",
    "title": "DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models",
    "link": "http://arxiv.org/abs/2506.05667",
    "summary_markdown": "### 论文研究单位\nLi Auto Inc.\n### 论文概述\n该论文介绍了 DriveAction，一个专为视觉-语言-动作（VLA）模型设计的、以动作为驱动的自动驾驶基准。该基准旨在解决现有基准在场景多样性、可靠的动作级标注以及与人类偏好对齐的评估协议方面的不足。DriveAction 包含 16,185 个 QA 对，源自 2,610 个驾驶场景，它利用自动驾驶车辆驾驶员主动收集的真实世界数据，提供与人类驾驶偏好对齐的高级别离散动作标签，并实施一个以动作为根的树状评估框架，以系统地评估从视觉、语言到动作的完整决策过程。\n### 论文核心贡献点\n1. 驾驶员贡献的广泛覆盖驾驶场景：数据来源于自动驾驶车辆驾驶员主动收集的真实世界数据，覆盖广泛且具有代表性的日常和挑战性驾驶场景，并通过人工筛选保证质量。\n2. 与人类驾驶偏好对齐的真实标签：动作标签直接从驾驶员的实时驾驶操作中收集，忠实地捕捉了决策时刻的人类意图。这些标签被离散化为高级别动作，与端到端大模型的输出粒度相匹配，并经过多轮人工验证。\n3. 以动作为根的树状结构评估：引入了一个以动作为根、树状结构的评估框架。该框架根据目标动作动态确定所需的视觉和语言任务，将 V-L-A 任务系统地整合到一个可扩展的框架中，支持综合评估和任务特定评估，并能分析视觉和语言信息对最终动作决策的影响。\n### 论文方法描述\nDriveAction 的方法包含三个核心部分：\n1. 场景构建：数据来自公司运营的自动驾驶车队，覆盖 148 个城市和多款车型，涵盖了匝道、主辅路切换、导航/效率变道、绕行弱势交通参与者、交叉口和路段等七大关键场景类别。\n2. 标注对齐：动作标签源于真实驾驶操作，而非事后人工标注。为匹配大模型决策频率，标签被离散化为如“变道”、“减速”等高级别动作。所有数据均经过人工审核，排除了错误、不合理或违法的行为。\n3. 评估框架：采用以动作为根的树状任务架构，顶层是动作节点，中间是语言任务（如导航遵循），底层是视觉任务（如车道线检测）。评估时提供连续视觉帧、导航指令和车速等关键场景信息。支持四种综合评估模式（V-L-A, V-A, L-A, A）和针对每个节点的任务特定评估。\n### 论文使用数据集和训练资源\n1. 数据集：使用的数据集是 DriveAction 基准，包含 16,185 个 QA 对，由 2,610 个驾驶场景生成。该数据集由驾驶员贡献的数据构建，已在 Hugging Face 上公开。\n2. 训练资源：为了在驾驶领域进行评估，论文使用专有驾驶数据训练了两个轻量级车载模型：一个非 MOE 架构模型（0.5B 参数）和一个 MOE 架构模型（8×0.4B 参数）。论文未提及训练这些模型所使用的具体硬件资源（如 GPU 类型或数量）。\n### 论文使用的评估环境和评估指标\n1. 评估环境：\n * 模型：评估了 12 个广泛使用的视觉语言模型（VLM），包括 GPT-4o、Claude 3.5 Sonnet、Qwen-Max-Latest 等非推理模型，以及 o1、o3、Claude 3.7 Sonnet Thinking 等推理模型。同时，也评估了上述两个专有的驾驶领域模型。\n * 框架：所有实验使用 VLMEvalKit 工具包实现。\n * 对比基准：与 BDD-X、Next-qa、TextVQA、RealWorldQA、Reason2Drive 等现有基准进行了对比。\n2. 评估指标：\n * 主要评估指标是准确率，用于衡量所有问题类型的模型性能。\n * 在跨基准比较中，使用统一的评估指标，即每个基准所有问题的平均得分（0-100分制）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Li Auto Inc.</p>\n<h3>论文概述</h3>\n<p>该论文介绍了 DriveAction，一个专为视觉-语言-动作（VLA）模型设计的、以动作为驱动的自动驾驶基准。该基准旨在解决现有基准在场景多样性、可靠的动作级标注以及与人类偏好对齐的评估协议方面的不足。DriveAction 包含 16,185 个 QA 对，源自 2,610 个驾驶场景，它利用自动驾驶车辆驾驶员主动收集的真实世界数据，提供与人类驾驶偏好对齐的高级别离散动作标签，并实施一个以动作为根的树状评估框架，以系统地评估从视觉、语言到动作的完整决策过程。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>驾驶员贡献的广泛覆盖驾驶场景：数据来源于自动驾驶车辆驾驶员主动收集的真实世界数据，覆盖广泛且具有代表性的日常和挑战性驾驶场景，并通过人工筛选保证质量。</li><li>与人类驾驶偏好对齐的真实标签：动作标签直接从驾驶员的实时驾驶操作中收集，忠实地捕捉了决策时刻的人类意图。这些标签被离散化为高级别动作，与端到端大模型的输出粒度相匹配，并经过多轮人工验证。</li><li>以动作为根的树状结构评估：引入了一个以动作为根、树状结构的评估框架。该框架根据目标动作动态确定所需的视觉和语言任务，将 V-L-A 任务系统地整合到一个可扩展的框架中，支持综合评估和任务特定评估，并能分析视觉和语言信息对最终动作决策的影响。</li></ol>\n<h3>论文方法描述</h3>\n<p>DriveAction 的方法包含三个核心部分：</p>\n<ol><li>场景构建：数据来自公司运营的自动驾驶车队，覆盖 148 个城市和多款车型，涵盖了匝道、主辅路切换、导航/效率变道、绕行弱势交通参与者、交叉口和路段等七大关键场景类别。</li><li>标注对齐：动作标签源于真实驾驶操作，而非事后人工标注。为匹配大模型决策频率，标签被离散化为如“变道”、“减速”等高级别动作。所有数据均经过人工审核，排除了错误、不合理或违法的行为。</li><li>评估框架：采用以动作为根的树状任务架构，顶层是动作节点，中间是语言任务（如导航遵循），底层是视觉任务（如车道线检测）。评估时提供连续视觉帧、导航指令和车速等关键场景信息。支持四种综合评估模式（V-L-A, V-A, L-A, A）和针对每个节点的任务特定评估。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li>数据集：使用的数据集是 DriveAction 基准，包含 16,185 个 QA 对，由 2,610 个驾驶场景生成。该数据集由驾驶员贡献的数据构建，已在 Hugging Face 上公开。</li><li>训练资源：为了在驾驶领域进行评估，论文使用专有驾驶数据训练了两个轻量级车载模型：一个非 MOE 架构模型（0.5B 参数）和一个 MOE 架构模型（8×0.4B 参数）。论文未提及训练这些模型所使用的具体硬件资源（如 GPU 类型或数量）。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. 评估环境：</p>\n<p> * 模型：评估了 12 个广泛使用的视觉语言模型（VLM），包括 GPT-4o、Claude 3.5 Sonnet、Qwen-Max-Latest 等非推理模型，以及 o1、o3、Claude 3.7 Sonnet Thinking 等推理模型。同时，也评估了上述两个专有的驾驶领域模型。</p>\n<p> * 框架：所有实验使用 VLMEvalKit 工具包实现。</p>\n<p> * 对比基准：与 BDD-X、Next-qa、TextVQA、RealWorldQA、Reason2Drive 等现有基准进行了对比。</p>\n<p>2. 评估指标：</p>\n<p> * 主要评估指标是准确率，用于衡量所有问题类型的模型性能。</p>\n<p> * 在跨基准比较中，使用统一的评估指标，即每个基准所有问题的平均得分（0-100分制）。</p>"
  },
  {
    "date": "2025-06-04",
    "title": "SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2506.03574",
    "summary_markdown": "论文研究单位\nBeijing Innovation Center of Humanoid Robotics\n\n论文概述\n现有的视觉-语言-动作（VLA）模型通常假设任务意图在执行过程中是固定的，因此难以响应用户在任务中途改变意图的场景。为了解决这个问题，论文提出了SwitchVLA，一个统一的、具备执行感知能力的框架，它使VLA模型能够在没有外部规划器或额外针对任务切换的特定数据的情况下，实现平滑、响应式的任务切换。该方法将任务切换视为一个基于执行状态和指令上下文进行条件化调节的行为问题，通过利用专家轨迹中固有的接触阶段信息，让策略能够推断任务进度并相应地调整其行为。\n\n论文核心贡献点\n- 提出了SwitchVLA，一个统一的执行感知框架，通过新颖的训练范式和创新的架构来支持动态任务切换，且不依赖额外的、特定于任务切换的数据。\n- 设计了一个多行为的条件策略，该策略能够在单一策略主干内平滑地执行前进、回滚或推进等动作。\n- 在模拟和真实世界的机器人操作任务中进行了实证验证，结果表明SwitchVLA在任务过渡的平滑性、恢复的有效性和指令的遵循性上，相比现有的VLA基线有显著提升。\n\n论文方法描述\n方法的核心是将任务切换建模为一个条件行为预测问题，模型根据当前的执行反馈（如接触状态）和更新的指令来调整其动作。\n1. 监督信号：引入两个关键的辅助监督信号。接触状态（Contact State, `c_t`），一个二元变量，表示机器人是否与物体发生物理接触，可通过触觉传感器、夹爪开闭信号或视觉-语言模型解析获得。行为模式（Behavior Mode, `b_t`），一个三元变量，包含`forward`（继续执行）、`rollback`（回滚之前操作）和`advance`（推进到新任务）。\n2. 架构：SwitchVLA基于Florence-2构建，包含两个核心组件：\n - 视觉-语言-接触（VLC）嵌入模块：使用DaViT视觉编码器处理多视角RGB图像，使用BART嵌入语言指令，并用MLP编码接触状态。所有token通过Transformer编码器-解码器融合，形成丰富的上下文表示。\n - 条件执行专家：一个结构化的动作解码器，它同时预测接触状态`c_t`、行为模式`b_t`和未来K个时间步的动作块（Action Chunk, `A_t`）。\n3. 训练与推理：\n - 训练：利用专家轨迹，通过行为特定的监督进行训练。对于`forward`行为，预测与当前指令匹配的未来动作。对于`rollback`行为，当检测到指令不匹配且处于接触状态时，预测反向的动作序列。对于`advance`行为，当指令更新但未发生接触时，在当前动作和新任务的起始姿态之间进行插值。动作序列使用流匹配损失进行优化，接触状态和行为模式使用标准分类损失进行监督。\n - 推理：在每个时间步，模型接收当前观察、当前指令和上一个指令、以及上一个接触状态，然后预测新的接触状态、行为模式和动作块。执行完动作块后，系统更新指令和接触状态历史，为下一次决策做准备，形成一个闭环的、条件化的任务切换过程。\n\n论文使用数据集和训练资源\n- 数据集：模拟实验使用了LIBERO-Goal数据集。真实世界实验在两个定制的Franka机器人工作站上进行。论文利用公开可用的机器人操作数据集，并通过组合现有轨迹来自动合成与任务切换相关的数据，而非人工采集新的演示数据。\n- 训练资源：训练在8块NVIDIA A100 GPU上进行。\n\n论文使用的评估环境和评估指标\n- 评估环境：包括模拟环境和真实世界环境。模拟环境基于LIBERO-Goal数据集。真实世界环境使用Franka Emika Panda机械臂在两个不同的工作站设置中执行桌面操作任务。\n- 评估指标：主要评估指标是成功率。在任务切换实验中，一个成功的试次被定义为：初始任务（任务A）在接到新指令前能正常执行，接收到新指令后能按照预定义的行为（前进、回滚或推进）进行响应，并且最终能成功完成新任务（任务B）。此外，论文还定性地评估了交互的自然性。",
    "summary_html": "<p>论文研究单位</p>\n<p>Beijing Innovation Center of Humanoid Robotics</p>\n\n<p>论文概述</p>\n<p>现有的视觉-语言-动作（VLA）模型通常假设任务意图在执行过程中是固定的，因此难以响应用户在任务中途改变意图的场景。为了解决这个问题，论文提出了SwitchVLA，一个统一的、具备执行感知能力的框架，它使VLA模型能够在没有外部规划器或额外针对任务切换的特定数据的情况下，实现平滑、响应式的任务切换。该方法将任务切换视为一个基于执行状态和指令上下文进行条件化调节的行为问题，通过利用专家轨迹中固有的接触阶段信息，让策略能够推断任务进度并相应地调整其行为。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了SwitchVLA，一个统一的执行感知框架，通过新颖的训练范式和创新的架构来支持动态任务切换，且不依赖额外的、特定于任务切换的数据。</li><li>设计了一个多行为的条件策略，该策略能够在单一策略主干内平滑地执行前进、回滚或推进等动作。</li><li>在模拟和真实世界的机器人操作任务中进行了实证验证，结果表明SwitchVLA在任务过渡的平滑性、恢复的有效性和指令的遵循性上，相比现有的VLA基线有显著提升。</li></ul>\n\n<p>论文方法描述</p>\n<p>方法的核心是将任务切换建模为一个条件行为预测问题，模型根据当前的执行反馈（如接触状态）和更新的指令来调整其动作。</p>\n<ol><li>监督信号：引入两个关键的辅助监督信号。接触状态（Contact State, <code>c_t</code>），一个二元变量，表示机器人是否与物体发生物理接触，可通过触觉传感器、夹爪开闭信号或视觉-语言模型解析获得。行为模式（Behavior Mode, <code>b_t</code>），一个三元变量，包含<code>forward</code>（继续执行）、<code>rollback</code>（回滚之前操作）和<code>advance</code>（推进到新任务）。</li><li>架构：SwitchVLA基于Florence-2构建，包含两个核心组件：</li></ol>\n<p> - 视觉-语言-接触（VLC）嵌入模块：使用DaViT视觉编码器处理多视角RGB图像，使用BART嵌入语言指令，并用MLP编码接触状态。所有token通过Transformer编码器-解码器融合，形成丰富的上下文表示。</p>\n<p> - 条件执行专家：一个结构化的动作解码器，它同时预测接触状态<code>c_t</code>、行为模式<code>b_t</code>和未来K个时间步的动作块（Action Chunk, <code>A_t</code>）。</p>\n<p>3. 训练与推理：</p>\n<p> - 训练：利用专家轨迹，通过行为特定的监督进行训练。对于<code>forward</code>行为，预测与当前指令匹配的未来动作。对于<code>rollback</code>行为，当检测到指令不匹配且处于接触状态时，预测反向的动作序列。对于<code>advance</code>行为，当指令更新但未发生接触时，在当前动作和新任务的起始姿态之间进行插值。动作序列使用流匹配损失进行优化，接触状态和行为模式使用标准分类损失进行监督。</p>\n<p> - 推理：在每个时间步，模型接收当前观察、当前指令和上一个指令、以及上一个接触状态，然后预测新的接触状态、行为模式和动作块。执行完动作块后，系统更新指令和接触状态历史，为下一次决策做准备，形成一个闭环的、条件化的任务切换过程。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：模拟实验使用了LIBERO-Goal数据集。真实世界实验在两个定制的Franka机器人工作站上进行。论文利用公开可用的机器人操作数据集，并通过组合现有轨迹来自动合成与任务切换相关的数据，而非人工采集新的演示数据。</li><li>训练资源：训练在8块NVIDIA A100 GPU上进行。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：包括模拟环境和真实世界环境。模拟环境基于LIBERO-Goal数据集。真实世界环境使用Franka Emika Panda机械臂在两个不同的工作站设置中执行桌面操作任务。</li><li>评估指标：主要评估指标是成功率。在任务切换实验中，一个成功的试次被定义为：初始任务（任务A）在接到新指令前能正常执行，接收到新指令后能按照预定义的行为（前进、回滚或推进）进行响应，并且最终能成功完成新任务（任务B）。此外，论文还定性地评估了交互的自然性。</li></ul>"
  },
  {
    "date": "2025-06-03",
    "title": "Adversarial Attacks on Robotic Vision Language Action Models",
    "link": "http://arxiv.org/abs/2506.03350",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-06-02",
    "title": "SAB3R: Semantic-Augmented Backbone in 3D Reconstruction",
    "link": "http://arxiv.org/abs/2506.02112",
    "summary_markdown": "论文研究单位\nUniversity of Virginia, University of Michigan\n\n论文概述\n论文提出了一种名为“Map and Locate”的新任务，该任务统一了开放词汇语义分割和3D重建，旨在从未定位的视频中生成点云并根据开放词汇查询分割物体实例。为解决此任务，论文提出了一个名为SAB3R（Semantic-Augmented Backbone in 3D Reconstruction）的简单而有效的基线方法。该方法基于MASt3R，并通过一种轻量级的知识蒸馏策略，将2D视觉主干（如CLIP和DINOv2）的稠密语义特征迁移到MASt3R中，从而在不引入任何辅助冻结网络的情况下，在单次前向传播中同时生成稠密语义特征和点云地图。\n\n论文核心贡献点\n1. Map and Locate基准测试：引入了一个新颖的多视图3D语义分割基准测试，该任务统一了重建、重组和识别。该基准包含一个大规模数据集、明确的评估协议和标准化指标。\n2. SAB3R模型：提出了一个统一的框架，通过高效的蒸馏策略，从未定位的图像中同时进行开放词汇分割和3D重建。该模型因其性能和计算效率被作为一个基线提出。\n\n论文方法描述\nSAB3R方法基于MASt3R架构，并引入了一个多任务框架来整合2D语义信息。\n1. 基础组件：采用MASt3R的架构，其包含一个ViT-Large编码器和一个ViT-Base解码器。该模型通过编码器-解码器对处理多视图图像，并使用交叉注意力机制交换信息，最终预测点图和置信度图。\n2. 蒸馏2D语义特征：为了在不遗忘3D重建能力的前提下整合2D语义信息，该方法在原有MASt3R模型的基础上增加了新的预测头，用于回归来自CLIP和DINOv2等2D基础模型的稠密特征。训练时，总损失函数由三部分组成：MASt3R原有的置信度损失（L_conf）和匹配损失（L_match），以及新增的2D特征回归损失（L_2D）。总损失为 `L_total = L_conf + β * L_match + γ * L_2D`，其中β和γ是超参数。\n3. 整合附加特征：该蒸馏框架具有灵活性，可以方便地整合多种2D特征。每增加一种新特征，只需添加一个对应的预测头和回归损失项，即可扩展总损失函数。\n\n论文使用数据集和训练资源\n1. 数据集：使用ScanNet数据集进行训练和评估。具体来说，从ScanNet的验证集中挑选了24个多样化的室内场景，并为每个场景创建了包含2、3或4个视图的图像组，确保每组图像间有足够的重叠。语义分类采用NYU40类别体系。\n2. 训练资源：模型基于MASt3R进行微调。教师模型为MaskCLIP（由FeatUp增强）和MASt3R。训练采用多任务学习策略，损失权重通过经验确定。具体的硬件配置和训练时长在提供的文本中未明确提及。\n\n论文使用的评估环境和评估指标\n1. 评估环境：模型主要在提出的“Map and Locate”基准测试上进行评估，该基准基于ScanNet数据集构建。此外，模型还在多个零样本任务上进行了评估，包括单目深度估计、相对相机位姿估计和零样本开放词汇语义分割等，这些任务分别在标准数据集（如NYUv2、ScanNet）上进行测试。\n2. 评估指标：\n - mIoU (mean Intersection over Union)：衡量预测点云与真实点云之间的平均交并比。\n - Acc (Accuracy)：正确分类的3D点占所有真实点的比例。\n - mComp (Mean Completeness)：预测点云到其最近真实点云的平均距离，衡量重建的完整性。\n - mdComp (Median Completeness)：预测点云到其最近真实点云的中值距离，对离群值更具鲁棒性。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Virginia, University of Michigan</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为“Map and Locate”的新任务，该任务统一了开放词汇语义分割和3D重建，旨在从未定位的视频中生成点云并根据开放词汇查询分割物体实例。为解决此任务，论文提出了一个名为SAB3R（Semantic-Augmented Backbone in 3D Reconstruction）的简单而有效的基线方法。该方法基于MASt3R，并通过一种轻量级的知识蒸馏策略，将2D视觉主干（如CLIP和DINOv2）的稠密语义特征迁移到MASt3R中，从而在不引入任何辅助冻结网络的情况下，在单次前向传播中同时生成稠密语义特征和点云地图。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>Map and Locate基准测试：引入了一个新颖的多视图3D语义分割基准测试，该任务统一了重建、重组和识别。该基准包含一个大规模数据集、明确的评估协议和标准化指标。</li><li>SAB3R模型：提出了一个统一的框架，通过高效的蒸馏策略，从未定位的图像中同时进行开放词汇分割和3D重建。该模型因其性能和计算效率被作为一个基线提出。</li></ol>\n\n<p>论文方法描述</p>\n<p>SAB3R方法基于MASt3R架构，并引入了一个多任务框架来整合2D语义信息。</p>\n<ol><li>基础组件：采用MASt3R的架构，其包含一个ViT-Large编码器和一个ViT-Base解码器。该模型通过编码器-解码器对处理多视图图像，并使用交叉注意力机制交换信息，最终预测点图和置信度图。</li><li>蒸馏2D语义特征：为了在不遗忘3D重建能力的前提下整合2D语义信息，该方法在原有MASt3R模型的基础上增加了新的预测头，用于回归来自CLIP和DINOv2等2D基础模型的稠密特征。训练时，总损失函数由三部分组成：MASt3R原有的置信度损失（L_conf）和匹配损失（L_match），以及新增的2D特征回归损失（L_2D）。总损失为 <code>L_total = L_conf + β * L_match + γ * L_2D</code>，其中β和γ是超参数。</li><li>整合附加特征：该蒸馏框架具有灵活性，可以方便地整合多种2D特征。每增加一种新特征，只需添加一个对应的预测头和回归损失项，即可扩展总损失函数。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：使用ScanNet数据集进行训练和评估。具体来说，从ScanNet的验证集中挑选了24个多样化的室内场景，并为每个场景创建了包含2、3或4个视图的图像组，确保每组图像间有足够的重叠。语义分类采用NYU40类别体系。</li><li>训练资源：模型基于MASt3R进行微调。教师模型为MaskCLIP（由FeatUp增强）和MASt3R。训练采用多任务学习策略，损失权重通过经验确定。具体的硬件配置和训练时长在提供的文本中未明确提及。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：模型主要在提出的“Map and Locate”基准测试上进行评估，该基准基于ScanNet数据集构建。此外，模型还在多个零样本任务上进行了评估，包括单目深度估计、相对相机位姿估计和零样本开放词汇语义分割等，这些任务分别在标准数据集（如NYUv2、ScanNet）上进行测试。</li><li>评估指标：</li></ol>\n<p> - mIoU (mean Intersection over Union)：衡量预测点云与真实点云之间的平均交并比。</p>\n<p> - Acc (Accuracy)：正确分类的3D点占所有真实点的比例。</p>\n<p> - mComp (Mean Completeness)：预测点云到其最近真实点云的平均距离，衡量重建的完整性。</p>\n<p> - mdComp (Median Completeness)：预测点云到其最近真实点云的中值距离，对离群值更具鲁棒性。</p>"
  },
  {
    "date": "2025-06-02",
    "title": "Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning",
    "link": "http://arxiv.org/abs/2506.01953",
    "summary_markdown": "论文研究单位\nThe Chinese University of Hong Kong, Peking University, AI2Robotics, Beijing Academy of Artificial Intelligence (BAAI)\n\n论文概述\n本文提出了Fast-in-Slow (FiS)，一个统一的双系统视觉-语言-动作(VLA)基础模型，将快速操作能力嵌入到慢速推理系统内。该模型解决了机器人操作中的泛化策略和执行效率两个关键挑战，通过将VLM的最终transformer块改造成高效的执行模块(System 1)，同时保留完整VLM的推理能力(System 2)，实现了117.7 Hz的控制频率，在模拟和真实世界任务中超越了先前最先进方法8%和11%的平均成功率。\n\n论文核心贡献点\n1. 提出了Fast-in-Slow (FiS)，一个统一的双系统VLA模型，将System 1执行嵌入到预训练的VLM中，同时保留其固有的System 2推理能力，实现了两个系统间的无缝协调\n2. 鉴于System 2和System 1在FiS-VLA中扮演根本不同的角色，为两个系统设计了异构模态输入和异步操作频率，实现了快速和精确的操作\n3. 提出了双感知共训练策略联合优化System 2和System 1，在单臂模拟和双臂真实世界实验中展示了SOTA性能，同时保持高执行频率\n\n论文方法描述\nFiS-VLA架构基于Prismatic VLMs，包含视觉编码器和7B LLaMA2 LLM主干。视觉编码器使用SigLIP和DINOv2联合提取视觉特征，3D点云通过轻量级3D标记器处理，然后通过共享视觉编码器提取空间特征。System 2处理2D图像和语言指令的低频输入，产生中间潜在特征指导System 1。System 1处理3D点云、2D图像和机器人状态的高频输入，进行实时动作生成。训练采用双感知共训练策略，结合快速系统的扩散去噪目标和慢速系统的自回归下一个token预测目标，整体训练目标为两者之和。\n\n论文使用数据集和训练资源\n预训练使用大规模跨具身数据集，包括Open X-Embodiment、DROID、ROBOMIND等，超过860K轨迹样本。然后在RLBench模拟数据和自收集的真实世界数据上进行微调。模型基于7B LLaMA2，在NVIDIA 4090 GPU上进行训练，预训练5个epoch。\n\n论文使用的评估环境和评估指标\n在RLBench基准的10个操作任务上进行模拟评估，包括Close box、Close Laptop等任务，使用Franka Panda机器人。真实世界实验在AgileX和AlphaBot双臂机器人上进行，验证不同机器人配置下的鲁棒性。评估指标为任务成功率，并报告推断速度。与ManipLLM、OpenVLA、π₀和CogACT等基线方法比较，FiS-VLA在平均成功率和推断速度上均取得优势。",
    "summary_html": "<p>论文研究单位</p>\n<p>The Chinese University of Hong Kong, Peking University, AI2Robotics, Beijing Academy of Artificial Intelligence (BAAI)</p>\n\n<p>论文概述</p>\n<p>本文提出了Fast-in-Slow (FiS)，一个统一的双系统视觉-语言-动作(VLA)基础模型，将快速操作能力嵌入到慢速推理系统内。该模型解决了机器人操作中的泛化策略和执行效率两个关键挑战，通过将VLM的最终transformer块改造成高效的执行模块(System 1)，同时保留完整VLM的推理能力(System 2)，实现了117.7 Hz的控制频率，在模拟和真实世界任务中超越了先前最先进方法8%和11%的平均成功率。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了Fast-in-Slow (FiS)，一个统一的双系统VLA模型，将System 1执行嵌入到预训练的VLM中，同时保留其固有的System 2推理能力，实现了两个系统间的无缝协调</li><li>鉴于System 2和System 1在FiS-VLA中扮演根本不同的角色，为两个系统设计了异构模态输入和异步操作频率，实现了快速和精确的操作</li><li>提出了双感知共训练策略联合优化System 2和System 1，在单臂模拟和双臂真实世界实验中展示了SOTA性能，同时保持高执行频率</li></ol>\n\n<p>论文方法描述</p>\n<p>FiS-VLA架构基于Prismatic VLMs，包含视觉编码器和7B LLaMA2 LLM主干。视觉编码器使用SigLIP和DINOv2联合提取视觉特征，3D点云通过轻量级3D标记器处理，然后通过共享视觉编码器提取空间特征。System 2处理2D图像和语言指令的低频输入，产生中间潜在特征指导System 1。System 1处理3D点云、2D图像和机器人状态的高频输入，进行实时动作生成。训练采用双感知共训练策略，结合快速系统的扩散去噪目标和慢速系统的自回归下一个token预测目标，整体训练目标为两者之和。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>预训练使用大规模跨具身数据集，包括Open X-Embodiment、DROID、ROBOMIND等，超过860K轨迹样本。然后在RLBench模拟数据和自收集的真实世界数据上进行微调。模型基于7B LLaMA2，在NVIDIA 4090 GPU上进行训练，预训练5个epoch。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>在RLBench基准的10个操作任务上进行模拟评估，包括Close box、Close Laptop等任务，使用Franka Panda机器人。真实世界实验在AgileX和AlphaBot双臂机器人上进行，验证不同机器人配置下的鲁棒性。评估指标为任务成功率，并报告推断速度。与ManipLLM、OpenVLA、π₀和CogACT等基线方法比较，FiS-VLA在平均成功率和推断速度上均取得优势。</p>"
  },
  {
    "date": "2025-06-02",
    "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics",
    "link": "http://arxiv.org/abs/2506.01844",
    "summary_markdown": "### 论文研究单位\nHugging Face, Sorbonne University, valeo.ai, École Normale Supérieure Paris-Saclay\n### 论文概述\n本文提出了SmolVLA，一个轻量级、高效且由社区驱动的视觉-语言-动作（VLA）模型。SmolVLA旨在显著降低训练和推理成本，同时保持与大型VLA模型相当的性能。它被设计为可在单个GPU上训练，并部署在消费级GPU甚至CPU上。为了进一步提高响应速度，论文引入了一个异步推理栈，将感知和动作预测与动作执行解耦，允许通过分块动作生成实现更高的控制频率。尽管体积紧凑，SmolVLA在模拟和真实世界的机器人基准测试中，实现了与比其大10倍的VLA模型相媲美的性能。\n### 论文核心贡献点\n1. 轻量级架构：提出了SmolVLA，一个为在消费级GPU上训练和部署而优化的紧凑高效视觉-语言代理。\n2. 基于社区驱动数据集的预训练：SmolVLA在完全来自公开社区贡献数据集的少于3万次训练回合上进行端到端训练。\n3. 异步推理：引入了一个优化的异步推理栈，将动作执行与观察处理和动作预测解耦，以降低延迟并实现快速、资源高效的推理。\n### 论文方法描述\nSmolVLA由两个主要组件组成：一个用于感知的预训练视觉-语言模型（VLM）和一个用于行动的动作专家。VLM处理来自多个RGB摄像头的图像、描述任务的语言指令以及机器人的传感器运动状态。VLM输出的特征直接馈送给动作专家，后者输出最终的连续动作。\n模型架构的关键设计包括：\n1. VLM：使用SmolVLM-2作为骨干，通过视觉编码器和像素洗牌技术处理图像序列。\n2. 状态、动作和特征投影器：使用线性投影层将状态投影到VLM维度，将动作投影到动作专家维度，并使VLM特征与动作专家的维度对齐。\n3. 视觉令牌缩减：不使用图像分块，仅使用全局图像，并通过像素洗牌操作将每帧的视觉令牌限制为64个。\n4. 通过层跳过实现更快的推理：跳过VLM中的计算，仅使用到指定层N（通常为总层的一半）的特征，将LLM和动作专家的计算成本减半。\n5. 流匹配动作专家：使用Transformer架构的条件流匹配Transformer作为动作专家，训练目标为预测从VLM特征和带噪声动作中得到的向量场。\n6. 交错交叉注意力和因果自注意力层：在动作专家中交错使用交叉注意力和自注意力层，其中交叉注意力层交叉关注VLM的键和值，自注意力层允许动作令牌相互关注。\n### 论文使用数据集和训练资源\n数据集：使用来自Hugging Face的481个公开社区数据集进行预训练，总计约22.9k次训练回合和1060万帧数据。数据筛选依据是实体类型、回合数、整体数据质量和帧覆盖率。使用VLM（Qwen2.5-VL-3B-Instruct）自动生成简洁的任务描述，并手动将相机映射到标准化视角类型（顶视、腕视、侧视）。\n训练资源：可在单个消费级GPU上进行训练。\n### 论文使用的评估环境和评估指标\n评估环境：在模拟环境和真实世界机器人任务上进行评估。\n评估指标：成功率（Simulation Evaluation 和 Real-World Evaluation），以及在异步推理中控制频率和响应速度的提升。\n基线模型：与π0和ACT等模型进行比较。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Hugging Face, Sorbonne University, valeo.ai, École Normale Supérieure Paris-Saclay</p>\n<h3>论文概述</h3>\n<p>本文提出了SmolVLA，一个轻量级、高效且由社区驱动的视觉-语言-动作（VLA）模型。SmolVLA旨在显著降低训练和推理成本，同时保持与大型VLA模型相当的性能。它被设计为可在单个GPU上训练，并部署在消费级GPU甚至CPU上。为了进一步提高响应速度，论文引入了一个异步推理栈，将感知和动作预测与动作执行解耦，允许通过分块动作生成实现更高的控制频率。尽管体积紧凑，SmolVLA在模拟和真实世界的机器人基准测试中，实现了与比其大10倍的VLA模型相媲美的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>轻量级架构：提出了SmolVLA，一个为在消费级GPU上训练和部署而优化的紧凑高效视觉-语言代理。</li><li>基于社区驱动数据集的预训练：SmolVLA在完全来自公开社区贡献数据集的少于3万次训练回合上进行端到端训练。</li><li>异步推理：引入了一个优化的异步推理栈，将动作执行与观察处理和动作预测解耦，以降低延迟并实现快速、资源高效的推理。</li></ol>\n<h3>论文方法描述</h3>\n<p>SmolVLA由两个主要组件组成：一个用于感知的预训练视觉-语言模型（VLM）和一个用于行动的动作专家。VLM处理来自多个RGB摄像头的图像、描述任务的语言指令以及机器人的传感器运动状态。VLM输出的特征直接馈送给动作专家，后者输出最终的连续动作。</p>\n<p>模型架构的关键设计包括：</p>\n<ol><li>VLM：使用SmolVLM-2作为骨干，通过视觉编码器和像素洗牌技术处理图像序列。</li><li>状态、动作和特征投影器：使用线性投影层将状态投影到VLM维度，将动作投影到动作专家维度，并使VLM特征与动作专家的维度对齐。</li><li>视觉令牌缩减：不使用图像分块，仅使用全局图像，并通过像素洗牌操作将每帧的视觉令牌限制为64个。</li><li>通过层跳过实现更快的推理：跳过VLM中的计算，仅使用到指定层N（通常为总层的一半）的特征，将LLM和动作专家的计算成本减半。</li><li>流匹配动作专家：使用Transformer架构的条件流匹配Transformer作为动作专家，训练目标为预测从VLM特征和带噪声动作中得到的向量场。</li><li>交错交叉注意力和因果自注意力层：在动作专家中交错使用交叉注意力和自注意力层，其中交叉注意力层交叉关注VLM的键和值，自注意力层允许动作令牌相互关注。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：使用来自Hugging Face的481个公开社区数据集进行预训练，总计约22.9k次训练回合和1060万帧数据。数据筛选依据是实体类型、回合数、整体数据质量和帧覆盖率。使用VLM（Qwen2.5-VL-3B-Instruct）自动生成简洁的任务描述，并手动将相机映射到标准化视角类型（顶视、腕视、侧视）。</p>\n<p>训练资源：可在单个消费级GPU上进行训练。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：在模拟环境和真实世界机器人任务上进行评估。</p>\n<p>评估指标：成功率（Simulation Evaluation 和 Real-World Evaluation），以及在异步推理中控制频率和响应速度的提升。</p>\n<p>基线模型：与π0和ACT等模型进行比较。</p>"
  },
  {
    "date": "2025-06-01",
    "title": "OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation",
    "link": "http://arxiv.org/abs/2506.01196",
    "summary_markdown": "- **论文研究单位**\n - 南加州大学 (University of Southern California)\n - NVIDIA\n\n- **论文概述**\n - OG-VLA 是一种新颖的架构和学习框架，结合了视觉语言动作模型 (VLA) 的泛化能力和 3D 感知策略的鲁棒性。\n - 该方法解决了将自然语言指令和一个或多个 RGBD 观测映射到准静态机器人动作的挑战。\n - 通过利用语言和视觉基础模型中的先验知识，提升了 3D 感知关键帧策略的泛化能力。\n - 核心创新是将输入观测反投影为点云，并从正交视图渲染，确保输入视图不变性和输入输出空间一致性。\n - 实验表明，在 Arnold 和 Colosseum 基准测试中，对未见过环境的泛化能力有超过 40% 的相对提升，同时在已见设置中保持鲁棒性能。\n\n- **论文核心贡献点**\n - 提出了一种结合 VLA 泛化能力与 3D 感知策略鲁棒性的新型架构 OG-VLA。\n - 引入正交图像生成机制，通过将 RGBD 输入转换为点云并渲染正交视图，实现输入视图不变性。\n - 使用视觉骨干网络、LLM 和图像扩散模型生成编码末端执行器位置和方向的图像。\n - 在 Arnold 和 Colosseum 基准测试中实现了最先进的泛化性能，相对提升超过 40%。\n - 展示了真实机器人上的快速适应能力，仅需 3-5 个演示即可学习任务。\n\n- **论文方法描述**\n - 系统输入为语言指令和一组带姿态的 RGBD 观测，输出为末端执行器状态（位置和旋转）。\n - 点云渲染：将多视角输入观测反投影为点云，再从固定正交视图（如前、上、左、右）渲染场景。\n - 多模态处理：使用视觉编码器处理正交视图，生成 CLS 和块嵌入。\n - LLM 处理：将嵌入投影到 LLM 空间，与指令提示拼接后输入 LLM，生成动作 token。\n - 图像生成：使用 Stable Diffusion 1.5 将动作 token 解码为正交视图上的热图，编码位置和旋转。\n - 3D 姿态解码：通过优化问题从热图中解码 3D 位置和欧拉角，并处理夹爪状态。\n - 训练细节：冻结视觉编码器，使用 DeepSpeed 训练 LLM、投影层和图像生成器，采用 SE(3) 数据增强。\n\n- **论文使用数据集和训练资源**\n - 数据集：Arnold（8 个任务，约 500 个演示/任务，2 关键帧/演示）和 Colosseum（20 个任务，100 个演示/任务）。\n - 训练资源：8 × A100 GPU，批量大小 64；Arnold 训练 30k 次迭代（1.5 天）或 100k 次（5 天），Colosseum 训练 250k 次；真实世界微调使用 4 × L40 GPU。\n\n- **论文使用的评估环境和评估指标**\n - 评估环境：模拟环境（Arnold 和 Colosseum 基准）及真实世界（Franka Emika Panda 机械臂）。\n - 评估指标：成功率（%），覆盖任务完成统计；Arnold 测试分 Novel Pose、Object、Scene、State；Colosseum 测试集为 all perturbation；真实世界测试 novel object 和 scene 变体。\n - 额外指标：模型延迟（秒/步、步数/回合、总时间），与基线（如 π₀-FAST、PerAct）比较。</think>\n### 论文研究单位\n南加州大学 (University of Southern California)\nNVIDIA\n### 论文概述\nOG-VLA 是一种结合视觉语言动作模型 (VLA) 泛化能力与 3D 感知策略鲁棒性的新型架构和学习框架。该方法旨在解决将自然语言指令和一个或多个 RGBD 观测映射到准静态机器人动作的挑战。通过利用语言和视觉基础模型中的先验知识，OG-VLA 提升了 3D 感知关键帧策略的泛化能力，同时保持精确控制。核心创新是将输入观测反投影为点云并从正交视图渲染，确保输入视图不变性，并通过视觉骨干网络、LLM 和图像扩散模型生成编码末端执行器状态的动作图像。实验在 Arnold 和 Colosseum 基准测试中展示了最先进的泛化性能，相对提升超过 40%，并在真实机器人上实现了 3-5 个演示的快速适应。\n### 论文核心贡献点\n- 提出 OG-VLA 架构，融合 VLA 的泛化优势与 3D 感知策略的鲁棒性。\n- 引入正交图像生成机制：将 RGBD 输入转换为点云，渲染正交视图（如前、上、左、右），实现输入视图不变性和空间一致性。\n- 使用视觉骨干网络、LLM 和图像扩散模型生成动作图像，编码末端执行器的位置、方向和夹爪状态。\n- 在 Arnold 和 Colosseum 基准测试中，对未见过环境的泛化成功率达相对提升超过 40%，同时保持已见环境的鲁棒性。\n- 展示真实机器人适应能力：仅需 3-5 个演示即可学习新任务，并在未见过物体和场景中泛化。\n### 论文方法描述\n系统输入为自然语言指令和多视角 RGBD 观测，输出为末端执行器 6-DOF 姿态（位置和旋转）。流程包括：\n1. **点云渲染**：将所有输入观测反投影为点云，在固定参考系中聚合，并从一组正交相机（前、左、右、上）渲染正交 RGB 图像。\n2. **多模态处理**：使用视觉编码器（如 ImageBind）处理正交视图，生成 CLS token 和块嵌入，通过投影层映射到 LLM 空间。\n3. **LLM 推理**：将嵌入与指令提示拼接后输入 LLM（如 Vicuna-7B），输出动作 token 序列，结合文本响应。\n4. **图像生成**：使用图像扩散模型（如 Stable Diffusion 1.5）将动作 token 解码为热图图像，覆盖在正交视图上，通过颜色编码位置（红）、旋转（黄、蓝、绿）和夹爪状态（左上角颜色）。\n5. **3D 姿态解码**：从热图解码 3D 位置（优化多视图概率积）和旋转（计算相对于水平线的角度），结合夹爪开闭状态。\n6. **训练细节**：冻结视觉编码器，使用 DeepSpeed 训练 LLM、投影层和图像生成器；采用 SE(3) 数据增强（平移 ±0.1m，旋转 ±90°）；Arnold 训练 30k 或 100k 次迭代，Colosseum 训练 250k 次。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **Arnold**：8 个任务（如抓取、开门、倒水），每个任务约 500 个演示，每个演示 2 个关键帧，总计约 7100 关键帧。测试分 Novel Pose、Object、Scene、State。\n - **Colosseum**：20 个桌面任务（如关箱、洗碗机），每个任务 100 个演示，平均 6 个关键帧。测试集为 all perturbation，同时改变物体、光照、相机姿态和干扰物。\n - **真实世界**：4 个任务（抓取、放物、开抽屉、关抽屉），共 22 个演示（3-5 个/任务），使用单相机。\n- **训练资源**：\n - 模拟训练：8 × A100 GPU，批量大小 64；Arnold 训练 30k 次（1.5 天）或 100k 次（5 天），Colosseum 训练 250k 次。\n - 真实世界微调：从 Arnold 预训练检查点微调 10k 次；VLA 基线（如 π₀-FAST）使用 4 × L40 GPU，LoRA 微调 0.5 天，全微调 2 天。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：Arnold 和 Colosseum 基准测试，在 Isaac Gym 中运行，包含多相机观测和运动规划器。\n - **真实世界**：Franka Emika Panda 机械臂，桌面设置，单前置相机，使用运动规划器（如 Viola）执行关键帧。\n- **评估指标**：\n - **成功率 (%)**：任务完成比例，Arnold 每测试集 20 个回合，Colosseum 25 个，真实世界 10 个，报告均值和标准差。\n - **模型延迟**：单步推理时间（秒）、每回合推理步数、总回合时间（含环境执行）。\n - **泛化测试**：包括 Arnold 的 Novel Pose（未见过物体/机器人位姿）、Novel Object（未见过物体）、Novel Scene（未见过场景）、Novel State（未见过目标状态）；Colosseum 的 all perturbation；真实世界的 novel object 和 scene 变体。",
    "summary_html": "<ul><li><strong>论文研究单位</strong></li></ul>\n<p> - 南加州大学 (University of Southern California)</p>\n<p> - NVIDIA</p>\n\n<ul><li><strong>论文概述</strong></li></ul>\n<p> - OG-VLA 是一种新颖的架构和学习框架，结合了视觉语言动作模型 (VLA) 的泛化能力和 3D 感知策略的鲁棒性。</p>\n<p> - 该方法解决了将自然语言指令和一个或多个 RGBD 观测映射到准静态机器人动作的挑战。</p>\n<p> - 通过利用语言和视觉基础模型中的先验知识，提升了 3D 感知关键帧策略的泛化能力。</p>\n<p> - 核心创新是将输入观测反投影为点云，并从正交视图渲染，确保输入视图不变性和输入输出空间一致性。</p>\n<p> - 实验表明，在 Arnold 和 Colosseum 基准测试中，对未见过环境的泛化能力有超过 40% 的相对提升，同时在已见设置中保持鲁棒性能。</p>\n\n<ul><li><strong>论文核心贡献点</strong></li></ul>\n<p> - 提出了一种结合 VLA 泛化能力与 3D 感知策略鲁棒性的新型架构 OG-VLA。</p>\n<p> - 引入正交图像生成机制，通过将 RGBD 输入转换为点云并渲染正交视图，实现输入视图不变性。</p>\n<p> - 使用视觉骨干网络、LLM 和图像扩散模型生成编码末端执行器位置和方向的图像。</p>\n<p> - 在 Arnold 和 Colosseum 基准测试中实现了最先进的泛化性能，相对提升超过 40%。</p>\n<p> - 展示了真实机器人上的快速适应能力，仅需 3-5 个演示即可学习任务。</p>\n\n<ul><li><strong>论文方法描述</strong></li></ul>\n<p> - 系统输入为语言指令和一组带姿态的 RGBD 观测，输出为末端执行器状态（位置和旋转）。</p>\n<p> - 点云渲染：将多视角输入观测反投影为点云，再从固定正交视图（如前、上、左、右）渲染场景。</p>\n<p> - 多模态处理：使用视觉编码器处理正交视图，生成 CLS 和块嵌入。</p>\n<p> - LLM 处理：将嵌入投影到 LLM 空间，与指令提示拼接后输入 LLM，生成动作 token。</p>\n<p> - 图像生成：使用 Stable Diffusion 1.5 将动作 token 解码为正交视图上的热图，编码位置和旋转。</p>\n<p> - 3D 姿态解码：通过优化问题从热图中解码 3D 位置和欧拉角，并处理夹爪状态。</p>\n<p> - 训练细节：冻结视觉编码器，使用 DeepSpeed 训练 LLM、投影层和图像生成器，采用 SE(3) 数据增强。</p>\n\n<ul><li><strong>论文使用数据集和训练资源</strong></li></ul>\n<p> - 数据集：Arnold（8 个任务，约 500 个演示/任务，2 关键帧/演示）和 Colosseum（20 个任务，100 个演示/任务）。</p>\n<p> - 训练资源：8 × A100 GPU，批量大小 64；Arnold 训练 30k 次迭代（1.5 天）或 100k 次（5 天），Colosseum 训练 250k 次；真实世界微调使用 4 × L40 GPU。</p>\n\n<ul><li><strong>论文使用的评估环境和评估指标</strong></li></ul>\n<p> - 评估环境：模拟环境（Arnold 和 Colosseum 基准）及真实世界（Franka Emika Panda 机械臂）。</p>\n<p> - 评估指标：成功率（%），覆盖任务完成统计；Arnold 测试分 Novel Pose、Object、Scene、State；Colosseum 测试集为 all perturbation；真实世界测试 novel object 和 scene 变体。</p>\n<p> - 额外指标：模型延迟（秒/步、步数/回合、总时间），与基线（如 π₀-FAST、PerAct）比较。</think></p>\n<h3>论文研究单位</h3>\n<p>南加州大学 (University of Southern California)</p>\n<p>NVIDIA</p>\n<h3>论文概述</h3>\n<p>OG-VLA 是一种结合视觉语言动作模型 (VLA) 泛化能力与 3D 感知策略鲁棒性的新型架构和学习框架。该方法旨在解决将自然语言指令和一个或多个 RGBD 观测映射到准静态机器人动作的挑战。通过利用语言和视觉基础模型中的先验知识，OG-VLA 提升了 3D 感知关键帧策略的泛化能力，同时保持精确控制。核心创新是将输入观测反投影为点云并从正交视图渲染，确保输入视图不变性，并通过视觉骨干网络、LLM 和图像扩散模型生成编码末端执行器状态的动作图像。实验在 Arnold 和 Colosseum 基准测试中展示了最先进的泛化性能，相对提升超过 40%，并在真实机器人上实现了 3-5 个演示的快速适应。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出 OG-VLA 架构，融合 VLA 的泛化优势与 3D 感知策略的鲁棒性。</li><li>引入正交图像生成机制：将 RGBD 输入转换为点云，渲染正交视图（如前、上、左、右），实现输入视图不变性和空间一致性。</li><li>使用视觉骨干网络、LLM 和图像扩散模型生成动作图像，编码末端执行器的位置、方向和夹爪状态。</li><li>在 Arnold 和 Colosseum 基准测试中，对未见过环境的泛化成功率达相对提升超过 40%，同时保持已见环境的鲁棒性。</li><li>展示真实机器人适应能力：仅需 3-5 个演示即可学习新任务，并在未见过物体和场景中泛化。</li></ul>\n<h3>论文方法描述</h3>\n<p>系统输入为自然语言指令和多视角 RGBD 观测，输出为末端执行器 6-DOF 姿态（位置和旋转）。流程包括：</p>\n<ol><li><strong>点云渲染</strong>：将所有输入观测反投影为点云，在固定参考系中聚合，并从一组正交相机（前、左、右、上）渲染正交 RGB 图像。</li><li><strong>多模态处理</strong>：使用视觉编码器（如 ImageBind）处理正交视图，生成 CLS token 和块嵌入，通过投影层映射到 LLM 空间。</li><li><strong>LLM 推理</strong>：将嵌入与指令提示拼接后输入 LLM（如 Vicuna-7B），输出动作 token 序列，结合文本响应。</li><li><strong>图像生成</strong>：使用图像扩散模型（如 Stable Diffusion 1.5）将动作 token 解码为热图图像，覆盖在正交视图上，通过颜色编码位置（红）、旋转（黄、蓝、绿）和夹爪状态（左上角颜色）。</li><li><strong>3D 姿态解码</strong>：从热图解码 3D 位置（优化多视图概率积）和旋转（计算相对于水平线的角度），结合夹爪开闭状态。</li><li><strong>训练细节</strong>：冻结视觉编码器，使用 DeepSpeed 训练 LLM、投影层和图像生成器；采用 SE(3) 数据增强（平移 ±0.1m，旋转 ±90°）；Arnold 训练 30k 或 100k 次迭代，Colosseum 训练 250k 次。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>Arnold</strong>：8 个任务（如抓取、开门、倒水），每个任务约 500 个演示，每个演示 2 个关键帧，总计约 7100 关键帧。测试分 Novel Pose、Object、Scene、State。</p>\n<p> - <strong>Colosseum</strong>：20 个桌面任务（如关箱、洗碗机），每个任务 100 个演示，平均 6 个关键帧。测试集为 all perturbation，同时改变物体、光照、相机姿态和干扰物。</p>\n<p> - <strong>真实世界</strong>：4 个任务（抓取、放物、开抽屉、关抽屉），共 22 个演示（3-5 个/任务），使用单相机。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 模拟训练：8 × A100 GPU，批量大小 64；Arnold 训练 30k 次（1.5 天）或 100k 次（5 天），Colosseum 训练 250k 次。</p>\n<p> - 真实世界微调：从 Arnold 预训练检查点微调 10k 次；VLA 基线（如 π₀-FAST）使用 4 × L40 GPU，LoRA 微调 0.5 天，全微调 2 天。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：Arnold 和 Colosseum 基准测试，在 Isaac Gym 中运行，包含多相机观测和运动规划器。</p>\n<p> - <strong>真实世界</strong>：Franka Emika Panda 机械臂，桌面设置，单前置相机，使用运动规划器（如 Viola）执行关键帧。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率 (%)</strong>：任务完成比例，Arnold 每测试集 20 个回合，Colosseum 25 个，真实世界 10 个，报告均值和标准差。</p>\n<p> - <strong>模型延迟</strong>：单步推理时间（秒）、每回合推理步数、总回合时间（含环境执行）。</p>\n<p> - <strong>泛化测试</strong>：包括 Arnold 的 Novel Pose（未见过物体/机器人位姿）、Novel Object（未见过物体）、Novel Scene（未见过场景）、Novel State（未见过目标状态）；Colosseum 的 all perturbation；真实世界的 novel object 和 scene 变体。</p>"
  },
  {
    "date": "2025-06-01",
    "title": "GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering",
    "link": "http://arxiv.org/abs/2506.01174",
    "summary_markdown": "论文研究单位\nUniversity of Waterloo, University of California, Los Angeles\n\n论文概述\n本文提出了GraphPad，一种可修改的3D场景图内存系统，用于解决具身智能体中静态场景表示与动态任务需求不匹配的问题。传统的3D场景图在任务执行前构建，常因缺少关键对象或空间关系而导致推理失败。GraphPad允许视觉语言模型（VLM）在推理时通过语言驱动的API调用动态更新其结构化内存，包括检索额外帧、插入新对象和关系以及注释节点。这种在线、按需的内存更新机制，使智能体能够构建与当前任务更对齐的场景表示。在OpenEQA基准测试中，GraphPad在仅使用5个初始帧的情况下达到了55.3%的准确率，比使用25个帧的纯图像基线提升了3个百分点，展示了其高效性和有效性。\n\n论文核心贡献点\n1. 将语言驱动的在线编辑机制应用于3D结构化内存，以解决固定场景图中固有的任务-内存不匹配问题。\n2. 提出了GraphPad系统，在该系统中，单一的视觉语言模型（VLM）能够在推理时识别知识缺口，并通过调用语言函数来更新其3D表示。\n3. 在OpenEQA基准上，证明了GraphPad可将空间问答任务的准确率从52.3%提升至55.3%，同时将处理的帧数从25帧减少到5帧，在无需额外训练的情况下，性能超越了纯图像基线和静态图基线。\n\n论文方法描述\nGraphPad方法包含一个结构化场景内存（SSM）和一套供VLM调用的可修改性API。\n1. 结构化场景内存（SSM）：由四个相互关联的部分组成。\n - Scene Graph：一个有向多重图，节点代表对象轨道，存储点云、视觉嵌入、语言嵌入、标题、房间/楼层ID；边编码四种空间关系。\n - Graphical Scratch-Pad：为每个节点增加一个自由形式的`notes`字段，用于存储任务特定信息。\n - Frame Memory：存储初始的稀疏RGB-D关键帧，并在API调用时追加新帧。\n - Navigation Log：为每个关键帧建立索引，包含房间、视野标签、运动标签和可见节点ID。\n 初始构建时，对每个关键帧运行VLM检测器，使用SAM获取掩码，通过深度反投影生成点云，并结合视觉、语言嵌入和空间重叠进行对象跟踪和关系发现。\n2. 智能体推理循环：在测试时，VLM接收问题和初始SSM。它迭代地分析当前内存，识别与问题相关的知识缺口，然后选择API、目标帧和查询来更新内存，直到信息充足或达到最大调用次数。\n3. 可修改性API：提供三个语言可调用函数。\n - `find_objects`：在指定帧中检测与查询相关的新对象并将其融入场景图。\n - `analyze_objects`：分析指定帧中给定列表的可见节点，并更新其`notes`。\n - `analyze_frame`：一个帧级别的复合操作，同时发现新对象并注释现有对象。\n\n论文使用数据集和训练资源\n- 数据集：OpenEQA基准，具体使用其episodic-memory (EM-EQA) 变体，数据来源于HM3D和ScanNet的3D场景扫描。\n- 训练资源：该方法在推理时动态更新内存，本身无需额外训练。实验中使用的模型均为预训练模型，包括作为推理和检测核心的Gemini 2.0 Flash，以及用于特征提取的CLIP ViT-L/14、BGE和用于分割的SAM。\n\n论文使用的评估环境和评估指标\n- 评估环境：遵循episodic-memory EQA协议。智能体被给予一个预先录制好的3D场景扫描中每隔k-th帧的RGB-D图像和相机位姿，部署后无法进行新的感知，评估完全基于给定的内存进行推理。\n- 评估指标：主要评估指标是准确率，即正确回答问题的百分比。此外，还分析了处理输入帧的数量、API调用的次数和分布，以评估模型的效率和推理行为。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Waterloo, University of California, Los Angeles</p>\n\n<p>论文概述</p>\n<p>本文提出了GraphPad，一种可修改的3D场景图内存系统，用于解决具身智能体中静态场景表示与动态任务需求不匹配的问题。传统的3D场景图在任务执行前构建，常因缺少关键对象或空间关系而导致推理失败。GraphPad允许视觉语言模型（VLM）在推理时通过语言驱动的API调用动态更新其结构化内存，包括检索额外帧、插入新对象和关系以及注释节点。这种在线、按需的内存更新机制，使智能体能够构建与当前任务更对齐的场景表示。在OpenEQA基准测试中，GraphPad在仅使用5个初始帧的情况下达到了55.3%的准确率，比使用25个帧的纯图像基线提升了3个百分点，展示了其高效性和有效性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>将语言驱动的在线编辑机制应用于3D结构化内存，以解决固定场景图中固有的任务-内存不匹配问题。</li><li>提出了GraphPad系统，在该系统中，单一的视觉语言模型（VLM）能够在推理时识别知识缺口，并通过调用语言函数来更新其3D表示。</li><li>在OpenEQA基准上，证明了GraphPad可将空间问答任务的准确率从52.3%提升至55.3%，同时将处理的帧数从25帧减少到5帧，在无需额外训练的情况下，性能超越了纯图像基线和静态图基线。</li></ol>\n\n<p>论文方法描述</p>\n<p>GraphPad方法包含一个结构化场景内存（SSM）和一套供VLM调用的可修改性API。</p>\n<p>1. 结构化场景内存（SSM）：由四个相互关联的部分组成。</p>\n<p> - Scene Graph：一个有向多重图，节点代表对象轨道，存储点云、视觉嵌入、语言嵌入、标题、房间/楼层ID；边编码四种空间关系。</p>\n<p> - Graphical Scratch-Pad：为每个节点增加一个自由形式的<code>notes</code>字段，用于存储任务特定信息。</p>\n<p> - Frame Memory：存储初始的稀疏RGB-D关键帧，并在API调用时追加新帧。</p>\n<p> - Navigation Log：为每个关键帧建立索引，包含房间、视野标签、运动标签和可见节点ID。</p>\n<p> 初始构建时，对每个关键帧运行VLM检测器，使用SAM获取掩码，通过深度反投影生成点云，并结合视觉、语言嵌入和空间重叠进行对象跟踪和关系发现。</p>\n<ol><li>智能体推理循环：在测试时，VLM接收问题和初始SSM。它迭代地分析当前内存，识别与问题相关的知识缺口，然后选择API、目标帧和查询来更新内存，直到信息充足或达到最大调用次数。</li><li>可修改性API：提供三个语言可调用函数。</li></ol>\n<p> - <code>find_objects</code>：在指定帧中检测与查询相关的新对象并将其融入场景图。</p>\n<p> - <code>analyze_objects</code>：分析指定帧中给定列表的可见节点，并更新其<code>notes</code>。</p>\n<p> - <code>analyze_frame</code>：一个帧级别的复合操作，同时发现新对象并注释现有对象。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：OpenEQA基准，具体使用其episodic-memory (EM-EQA) 变体，数据来源于HM3D和ScanNet的3D场景扫描。</li><li>训练资源：该方法在推理时动态更新内存，本身无需额外训练。实验中使用的模型均为预训练模型，包括作为推理和检测核心的Gemini 2.0 Flash，以及用于特征提取的CLIP ViT-L/14、BGE和用于分割的SAM。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：遵循episodic-memory EQA协议。智能体被给予一个预先录制好的3D场景扫描中每隔k-th帧的RGB-D图像和相机位姿，部署后无法进行新的感知，评估完全基于给定的内存进行推理。</li><li>评估指标：主要评估指标是准确率，即正确回答问题的百分比。此外，还分析了处理输入帧的数量、API调用的次数和分布，以评估模型的效率和推理行为。</li></ul>"
  },
  {
    "date": "2025-05-31",
    "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks",
    "link": "http://arxiv.org/abs/2506.00411",
    "summary_markdown": "### 论文研究单位\n复旦大学、上海科技大学、上海交通大学\n### 论文概述\n论文提出LoHoVLA，一个统一的视觉-语言-动作模型，用于处理长视野具身任务。该模型通过将高级任务规划和低级运动控制集成到单一框架中，解决现有VLA模型在规划能力上的不足和分层架构中的协调问题。LoHoVLA利用预训练视觉语言模型作为主干，联合生成语言子任务和机器人动作标记，并通过分层闭环控制机制增强鲁棒性。\n### 论文核心贡献点\n1. 统一架构：将高级任务规划和低级运动控制整合为单一模型，避免分层方法的协调问题。\n2. 共享表示：使用同一VLM主干处理语言和动作生成，促进任务间的泛化。\n3. 分层闭环控制机制：根据失败次数阈值动态调整子任务重规划或动作重预测，减少误差传播。\n4. LoHoSet数据集：构建基于Ravens模拟器的数据集，包含20个长视野任务，每个任务1000个专家演示，涵盖视觉观察、语言目标、子任务和动作。\n### 论文方法描述\nLoHoVLA采用PaliGemma作为主干模型，包括SigLIP图像编码器、Gemma-2B语言模型和线性投影层。动作通过离散化为1024个分箱进行标记化，并在推理时去标记化恢复。控制策略使用Algorithm 1：在子任务执行失败次数超阈值K时触发重规划，否则仅更新动作。训练分两阶段：第一阶段优化文本损失提升规划能力，第二阶段加入原始任务优化动作损失。总损失函数为语言损失和动作损失之和。\n### 论文使用数据集和训练资源\n数据集：LoHoSet，基于Ravens机器人模拟器构建，包含块、碗和区域三种对象，11种颜色，涉及10个长视野任务（如堆叠、放置）和3个拾取-放置原始任务，额外增加10个长视野任务提升泛化。每个任务1000个演示，总数据涵盖视觉观察（RGB和深度图像）、语言目标、子任务描述和机器人动作。\n训练资源：未明确指定硬件，但模型在合成数据集上微调，PaliGemma主干固定图像编码器。\n### 论文使用的评估环境和评估指标\n评估环境：Ravens模拟器，包含UR5e机械臂和吸盘夹具，引入观察噪声和物体掉落概率模拟不确定性。\n评估指标：主要指标包括任务成功率，用于比较LoHoVLA与分层基线和标准VLA方法在已见和未完成任务上的性能。实验还通过闭环策略比较和消融研究验证设计有效性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>复旦大学、上海科技大学、上海交通大学</p>\n<h3>论文概述</h3>\n<p>论文提出LoHoVLA，一个统一的视觉-语言-动作模型，用于处理长视野具身任务。该模型通过将高级任务规划和低级运动控制集成到单一框架中，解决现有VLA模型在规划能力上的不足和分层架构中的协调问题。LoHoVLA利用预训练视觉语言模型作为主干，联合生成语言子任务和机器人动作标记，并通过分层闭环控制机制增强鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>统一架构：将高级任务规划和低级运动控制整合为单一模型，避免分层方法的协调问题。</li><li>共享表示：使用同一VLM主干处理语言和动作生成，促进任务间的泛化。</li><li>分层闭环控制机制：根据失败次数阈值动态调整子任务重规划或动作重预测，减少误差传播。</li><li>LoHoSet数据集：构建基于Ravens模拟器的数据集，包含20个长视野任务，每个任务1000个专家演示，涵盖视觉观察、语言目标、子任务和动作。</li></ol>\n<h3>论文方法描述</h3>\n<p>LoHoVLA采用PaliGemma作为主干模型，包括SigLIP图像编码器、Gemma-2B语言模型和线性投影层。动作通过离散化为1024个分箱进行标记化，并在推理时去标记化恢复。控制策略使用Algorithm 1：在子任务执行失败次数超阈值K时触发重规划，否则仅更新动作。训练分两阶段：第一阶段优化文本损失提升规划能力，第二阶段加入原始任务优化动作损失。总损失函数为语言损失和动作损失之和。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：LoHoSet，基于Ravens机器人模拟器构建，包含块、碗和区域三种对象，11种颜色，涉及10个长视野任务（如堆叠、放置）和3个拾取-放置原始任务，额外增加10个长视野任务提升泛化。每个任务1000个演示，总数据涵盖视觉观察（RGB和深度图像）、语言目标、子任务描述和机器人动作。</p>\n<p>训练资源：未明确指定硬件，但模型在合成数据集上微调，PaliGemma主干固定图像编码器。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：Ravens模拟器，包含UR5e机械臂和吸盘夹具，引入观察噪声和物体掉落概率模拟不确定性。</p>\n<p>评估指标：主要指标包括任务成功率，用于比较LoHoVLA与分层基线和标准VLA方法在已见和未完成任务上的性能。实验还通过闭环策略比较和消融研究验证设计有效性。</p>"
  },
  {
    "date": "2025-05-30",
    "title": "Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction",
    "link": "http://arxiv.org/abs/2505.24156",
    "summary_markdown": "论文研究单位\nInstitute of Artificial intelligence (TeleAI), China Telecom, Northwestern Polytechnical University, Hong Kong University of Science and Technology\n\n论文概述\n该论文提出了一种名为CogRobot的双手机器人基础策略学习方法。核心思想是利用基于光流的视频预测技术，将高级语言指令转化为可执行的机器人动作。该方法分为两个阶段：首先通过文本到光流模型预测运动信息，然后通过光流到视频模型生成未来轨迹视频，最后通过一个轻量级扩散策略从预测视频中推导出底层动作。这种方法减少了语言指令的歧义性，并显著降低了机器人数据的需求，在仿真和真实世界中均展示了有效性。\n\n论文核心贡献点\n1. 提出了一种利用文本到视频（T2V）模型构建双手机器人基础策略的框架CogRobot。\n2. 引入了一个两阶段范式，使用光流作为简洁的视频表示，从而降低了双手机器人操作的数据需求。\n3. 构建了一个双臂机器人平台以收集高质量的双手机器人操作数据，并在仿真和真实机器人上评估了方法的有效性。\n\n论文方法描述\n方法包含三个主要模块：\n1. 文本到光流生成：将光流转换为彩色视频格式，微调预训练的CogVideoX模型以预测未来光流序列，该序列编码了机器人和物体的精细运动细节。\n2. 光流到视频预测：将预测的光流序列作为条件，引导视频生成模型合成未来轨迹视频，通过在通道维度上连接光流和RGB视频的潜在表示来实现。\n3. 视频到动作的扩散策略：训练一个轻量级扩散策略，从预测的视频轨迹中生成可执行的底层机器人动作。\n\n论文使用数据集和训练资源\n- 数据集：使用公开的双臂数据集RDT和RoboMIND进行预训练和评估，并通过VR遥操作在自建的双臂Realman机器人平台上收集了额外的专家演示数据。\n- 训练资源：基于预训练的CogVideoX模型（2B和5B版本）进行微调，冻结了VAE部分，以利用其在大规模网络视频上学到的生成先验。\n\n论文使用的评估环境和评估指标\n- 评估环境：在模拟环境和真实世界的双臂机器人系统（基于Realman机械臂）上进行实验。\n- 评估指标：主要使用任务成功率（二元奖励函数）作为评估指标，通过语言目标是否达成来衡量。",
    "summary_html": "<p>论文研究单位</p>\n<p>Institute of Artificial intelligence (TeleAI), China Telecom, Northwestern Polytechnical University, Hong Kong University of Science and Technology</p>\n\n<p>论文概述</p>\n<p>该论文提出了一种名为CogRobot的双手机器人基础策略学习方法。核心思想是利用基于光流的视频预测技术，将高级语言指令转化为可执行的机器人动作。该方法分为两个阶段：首先通过文本到光流模型预测运动信息，然后通过光流到视频模型生成未来轨迹视频，最后通过一个轻量级扩散策略从预测视频中推导出底层动作。这种方法减少了语言指令的歧义性，并显著降低了机器人数据的需求，在仿真和真实世界中均展示了有效性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种利用文本到视频（T2V）模型构建双手机器人基础策略的框架CogRobot。</li><li>引入了一个两阶段范式，使用光流作为简洁的视频表示，从而降低了双手机器人操作的数据需求。</li><li>构建了一个双臂机器人平台以收集高质量的双手机器人操作数据，并在仿真和真实机器人上评估了方法的有效性。</li></ol>\n\n<p>论文方法描述</p>\n<p>方法包含三个主要模块：</p>\n<ol><li>文本到光流生成：将光流转换为彩色视频格式，微调预训练的CogVideoX模型以预测未来光流序列，该序列编码了机器人和物体的精细运动细节。</li><li>光流到视频预测：将预测的光流序列作为条件，引导视频生成模型合成未来轨迹视频，通过在通道维度上连接光流和RGB视频的潜在表示来实现。</li><li>视频到动作的扩散策略：训练一个轻量级扩散策略，从预测的视频轨迹中生成可执行的底层机器人动作。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：使用公开的双臂数据集RDT和RoboMIND进行预训练和评估，并通过VR遥操作在自建的双臂Realman机器人平台上收集了额外的专家演示数据。</li><li>训练资源：基于预训练的CogVideoX模型（2B和5B版本）进行微调，冻结了VAE部分，以利用其在大规模网络视频上学到的生成先验。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：在模拟环境和真实世界的双臂机器人系统（基于Realman机械臂）上进行实验。</li><li>评估指标：主要使用任务成功率（二元奖励函数）作为评估指标，通过语言目标是否达成来衡量。</li></ul>"
  },
  {
    "date": "2025-05-29",
    "title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2505.23757",
    "summary_markdown": "### 论文研究单位\n清华大学智能产业研究院, 博世研究中心, 清华大学交叉信息研究院\n### 论文概述\n论文提出了Impromptu VLA，一个开放权重和开放数据的驾驶视觉-语言-动作模型。其核心贡献是Impromptu VLA数据集，该数据集专注于非结构化道路的“corner case”场景，旨在解决当前视觉-语言-动作（VLA）模型在这些场景下表现不佳的问题。数据集包含超过80,000个精心筛选的视频片段，源自8个开源的大规模数据集，总计超过200万个源片段。该数据集基于一个包含四种具有挑战性的非结构化类别的新分类法构建，并提供了丰富的、面向规划的问答标注和动作轨迹。实验表明，使用该数据集训练的VLA模型在既定基准上实现了显著的性能提升，包括提高了NeuroNCAP闭环评分和降低碰撞率，并在nuScenes开放环轨迹预测中达到接近最先进的L2准确度。\n### 论文核心贡献点\n1. Impromptu VLA数据集：一个公开可用的大规模、丰富标注的资源，专门专注于多样化和具有挑战性的非结构化驾驶场景，旨在填补现有数据资源的空白。\n2. 一个用于非结构化道路条件的系统分类法：定义了四类非结构化场景（边界不清的道路、临时交通规则变更、非常规动态障碍物、具有挑战性的道路条件），以及一个可扩展的、以VLM为中心的数据整理流程，用于识别、分类和全面的、适合训练高级VLM的多任务问答标注。\n3. 广泛的实验证据：证明了使用Impromptu VLA数据集训练可以显著提升标准驾驶基准上的结果，并可作为评估和提高VLM在非结构化环境中能力的有效诊断工具。\n### 论文方法描述\n1. 数据收集与筛选：从8个公共数据集中聚合了超过200万个片段，并标准化到2Hz的统一时间频率。\n2. 非结构化场景分类法定义：通过使用VLM对数据进行无偏探索和描述，然后进行分类和迭代优化提示，最终自下而上地汇聚并归纳出四个高层类别。\n3. 数据处理与标注流程：包括关键片段选择和稳定性过滤、使用思维链提示的场景分类与结构化信息提取、多任务标注生成（如场景描述、交通信号检测、VRU识别、运动意图预测、元动作规划、规划解释和轨迹预测）以及全面的人工验证。\n4. 模型训练：比较了两种训练流程：直接在nuScenes上微调基础模型，以及先在Impromptu VLA上微调再在nuScenes上微调。\n### 论文使用数据集和训练资源\n* 数据集：源数据集包括Mapillary, ONCE, NAVSIM, nuScenes, Waymo, Argoverse-V2, KITTI, IDD。Impromptu VLA数据集最终包含约80,000个片段，标注数据大小约43.5GB，按80:20比例划分为训练集和验证集。\n* 训练资源：论文中未明确提及使用的具体计算资源。\n### 论文使用的评估环境和评估指标\n* 评估环境：闭环评估使用NeuroNCAP；开放环评估使用nuScenes数据集；诊断评估在Impromptu VLA自身的验证集上进行。\n* 评估指标：闭环指标包括NeuroNCAP评分和碰撞率；开放环指标为不同未来时间点的L2误差；诊断指标包括问答准确率和轨迹预测L2误差。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学智能产业研究院, 博世研究中心, 清华大学交叉信息研究院</p>\n<h3>论文概述</h3>\n<p>论文提出了Impromptu VLA，一个开放权重和开放数据的驾驶视觉-语言-动作模型。其核心贡献是Impromptu VLA数据集，该数据集专注于非结构化道路的“corner case”场景，旨在解决当前视觉-语言-动作（VLA）模型在这些场景下表现不佳的问题。数据集包含超过80,000个精心筛选的视频片段，源自8个开源的大规模数据集，总计超过200万个源片段。该数据集基于一个包含四种具有挑战性的非结构化类别的新分类法构建，并提供了丰富的、面向规划的问答标注和动作轨迹。实验表明，使用该数据集训练的VLA模型在既定基准上实现了显著的性能提升，包括提高了NeuroNCAP闭环评分和降低碰撞率，并在nuScenes开放环轨迹预测中达到接近最先进的L2准确度。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>Impromptu VLA数据集：一个公开可用的大规模、丰富标注的资源，专门专注于多样化和具有挑战性的非结构化驾驶场景，旨在填补现有数据资源的空白。</li><li>一个用于非结构化道路条件的系统分类法：定义了四类非结构化场景（边界不清的道路、临时交通规则变更、非常规动态障碍物、具有挑战性的道路条件），以及一个可扩展的、以VLM为中心的数据整理流程，用于识别、分类和全面的、适合训练高级VLM的多任务问答标注。</li><li>广泛的实验证据：证明了使用Impromptu VLA数据集训练可以显著提升标准驾驶基准上的结果，并可作为评估和提高VLM在非结构化环境中能力的有效诊断工具。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>数据收集与筛选：从8个公共数据集中聚合了超过200万个片段，并标准化到2Hz的统一时间频率。</li><li>非结构化场景分类法定义：通过使用VLM对数据进行无偏探索和描述，然后进行分类和迭代优化提示，最终自下而上地汇聚并归纳出四个高层类别。</li><li>数据处理与标注流程：包括关键片段选择和稳定性过滤、使用思维链提示的场景分类与结构化信息提取、多任务标注生成（如场景描述、交通信号检测、VRU识别、运动意图预测、元动作规划、规划解释和轨迹预测）以及全面的人工验证。</li><li>模型训练：比较了两种训练流程：直接在nuScenes上微调基础模型，以及先在Impromptu VLA上微调再在nuScenes上微调。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>* 数据集：源数据集包括Mapillary, ONCE, NAVSIM, nuScenes, Waymo, Argoverse-V2, KITTI, IDD。Impromptu VLA数据集最终包含约80,000个片段，标注数据大小约43.5GB，按80:20比例划分为训练集和验证集。</p>\n<p>* 训练资源：论文中未明确提及使用的具体计算资源。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>* 评估环境：闭环评估使用NeuroNCAP；开放环评估使用nuScenes数据集；诊断评估在Impromptu VLA自身的验证集上进行。</p>\n<p>* 评估指标：闭环指标包括NeuroNCAP评分和碰撞率；开放环指标为不同未来时间点的L2误差；诊断指标包括问答准确率和轨迹预测L2误差。</p>"
  },
  {
    "date": "2025-05-29",
    "title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better",
    "link": "http://arxiv.org/abs/2505.23705",
    "summary_markdown": "论文研究单位\nPhysical Intelligence\n\n论文概述\n论文研究了视觉-语言-动作模型（VLA）在实时机器人控制中的挑战。传统VLM模型因参数量大和离散token输出限制，难以满足高频连续控制需求。现有方法通过添加连续动作专家（如流匹配或扩散模块）提升控制效率，但会破坏预训练VLM的知识，导致训练速度下降和泛化能力减弱。本文提出知识隔离训练方法，通过隔离梯度流保护预训练知识，同时加速训练并提升泛化性能。\n\n论文核心贡献点\n提出知识隔离训练框架，通过分离VLM主干和动作专家的梯度流，在训练中同时优化离散动作预测和连续动作生成。\n实现更快的训练收敛速度，同时保留预训练VLM的语义知识。\n在模拟和真实机器人任务中验证了方法的泛化优势，支持高频控制（如折叠衣物等动态任务）。\n首次实现VLA在快速训练、实时推理和强泛化能力上的统一。\n\n论文方法描述\n联合离散-连续动作预测：VLM主干使用离散动作token进行next-token预测训练，动作专家同时通过流匹配学习连续动作输出。\n知识隔离机制：动作专家的梯度不反向传播至VLM主干，避免未初始化参数的干扰。\n动作专家架构：基于轻量级网络，通过交叉注意力机制融合VLM特征，支持动作块（action chunk）生成。\n训练策略：混合机器人数据与通用VLM数据，增强跨模态知识迁移。\n\n论文使用数据集和训练资源\n数据集：\n公共基准：DROID（大规模双臂操作数据）、LIBERO（模拟家务任务）。\n真实任务：静态机器人（抽屉整理、衣物折叠、桌面清理）和移动操作机器人（整理床铺、洗碗篮整理、移动抽屉操作、衣物搬运）。\n训练资源：基于PaliGemma初始化VLM，使用大规模GPU集群（具体配置未详述）。\n\n论文使用的评估环境和评估指标\n评估环境：\n模拟环境：基于LIBERO的虚拟场景。\n真实环境：静态机械臂和移动操作机器人（如双臂系统）。\n评估指标：\n任务成功率：在给定语言指令下完成目标的比率。\n动作精度：连续动作与目标轨迹的偏差。\n语言理解能力：指令遵循的语义匹配度。\n训练效率：收敛速度和资源消耗对比。",
    "summary_html": "<p>论文研究单位</p>\n<p>Physical Intelligence</p>\n\n<p>论文概述</p>\n<p>论文研究了视觉-语言-动作模型（VLA）在实时机器人控制中的挑战。传统VLM模型因参数量大和离散token输出限制，难以满足高频连续控制需求。现有方法通过添加连续动作专家（如流匹配或扩散模块）提升控制效率，但会破坏预训练VLM的知识，导致训练速度下降和泛化能力减弱。本文提出知识隔离训练方法，通过隔离梯度流保护预训练知识，同时加速训练并提升泛化性能。</p>\n\n<p>论文核心贡献点</p>\n<p>提出知识隔离训练框架，通过分离VLM主干和动作专家的梯度流，在训练中同时优化离散动作预测和连续动作生成。</p>\n<p>实现更快的训练收敛速度，同时保留预训练VLM的语义知识。</p>\n<p>在模拟和真实机器人任务中验证了方法的泛化优势，支持高频控制（如折叠衣物等动态任务）。</p>\n<p>首次实现VLA在快速训练、实时推理和强泛化能力上的统一。</p>\n\n<p>论文方法描述</p>\n<p>联合离散-连续动作预测：VLM主干使用离散动作token进行next-token预测训练，动作专家同时通过流匹配学习连续动作输出。</p>\n<p>知识隔离机制：动作专家的梯度不反向传播至VLM主干，避免未初始化参数的干扰。</p>\n<p>动作专家架构：基于轻量级网络，通过交叉注意力机制融合VLM特征，支持动作块（action chunk）生成。</p>\n<p>训练策略：混合机器人数据与通用VLM数据，增强跨模态知识迁移。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<p>公共基准：DROID（大规模双臂操作数据）、LIBERO（模拟家务任务）。</p>\n<p>真实任务：静态机器人（抽屉整理、衣物折叠、桌面清理）和移动操作机器人（整理床铺、洗碗篮整理、移动抽屉操作、衣物搬运）。</p>\n<p>训练资源：基于PaliGemma初始化VLM，使用大规模GPU集群（具体配置未详述）。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<p>模拟环境：基于LIBERO的虚拟场景。</p>\n<p>真实环境：静态机械臂和移动操作机器人（如双臂系统）。</p>\n<p>评估指标：</p>\n<p>任务成功率：在给定语言指令下完成目标的比率。</p>\n<p>动作精度：连续动作与目标轨迹的偏差。</p>\n<p>语言理解能力：指令遵循的语义匹配度。</p>\n<p>训练效率：收敛速度和资源消耗对比。</p>"
  },
  {
    "date": "2025-05-29",
    "title": "TrackVLA: Embodied Visual Tracking in the Wild",
    "link": "http://arxiv.org/abs/2505.23189",
    "summary_markdown": "### 论文研究单位\nPeking University, Galbot, Beihang University, Beijing Normal University, BAAI\n### 论文概述\n论文提出TrackVLA，一个Vision-Language-Action (VLA)模型，用于解决具身视觉跟踪任务。该任务要求智能体在动态环境中基于自身视角持续跟踪特定目标。现有方法通常将目标识别和轨迹规划解耦为分离模块，导致错误累积。TrackVLA通过共享LLM主干，联合学习目标识别和轨迹规划的协同作用。\n### 论文核心贡献点\n1. 提出TrackVLA，首个统一VLA框架，同时处理目标识别和轨迹规划\n2. 构建了EVT-Bench (Embodied Visual Tracking Benchmark)基准，包含100个人形化身和804个场景\n3. 收集了170万样本的数据集（85.5万跟踪样本+85.5万识别样本）\n4. 在合成和真实环境中展示了SOTA性能，零样本迁移能力强\n### 论文方法描述\nTrackVLA架构使用Vicuna-7B作为LLM主干。视觉编码采用EVA-CLIP预训练模型，使用网格池化策略。双头设计包括语言建模头用于识别任务，自回归解码文本；基于锚点的扩散模型头用于轨迹规划，从预定义锚点去噪生成轨迹。训练策略联合优化跟踪损失和文本预测损失。推理优化只需2步去噪，实现5倍加速。\n### 论文使用数据集和训练资源\nEVT-Bench数据集包含100个人形化身和804个场景，总计25,986个episode，分为训练集(21,771)和测试集(4,215)，涵盖三个子任务：单目标跟踪(STT)、干扰跟踪(DT)、模糊跟踪(AT)。85.5万跟踪样本来自EVT-Bench训练集。85.5万识别样本包括36.2万人识别样本（基于SYNTH-PEDES）和49.3万开放世界VQA样本。训练资源方面，冻结视觉编码器参数，仅训练1个epoch。\n### 论文使用的评估环境和评估指标\n评估环境包括合成环境（EVT-Bench、Gym-UnrealCV公共基准）和真实环境（配备RGB摄像头的机器人平台）。评估指标包括跟踪性能（成功率，保持1-3米跟随距离且面向目标）、识别性能（文本预测准确率）、推理速度（FPS，报告10 FPS），以及轨迹回归误差(MSE)和分类损失(BCE)。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Peking University, Galbot, Beihang University, Beijing Normal University, BAAI</p>\n<h3>论文概述</h3>\n<p>论文提出TrackVLA，一个Vision-Language-Action (VLA)模型，用于解决具身视觉跟踪任务。该任务要求智能体在动态环境中基于自身视角持续跟踪特定目标。现有方法通常将目标识别和轨迹规划解耦为分离模块，导致错误累积。TrackVLA通过共享LLM主干，联合学习目标识别和轨迹规划的协同作用。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出TrackVLA，首个统一VLA框架，同时处理目标识别和轨迹规划</li><li>构建了EVT-Bench (Embodied Visual Tracking Benchmark)基准，包含100个人形化身和804个场景</li><li>收集了170万样本的数据集（85.5万跟踪样本+85.5万识别样本）</li><li>在合成和真实环境中展示了SOTA性能，零样本迁移能力强</li></ol>\n<h3>论文方法描述</h3>\n<p>TrackVLA架构使用Vicuna-7B作为LLM主干。视觉编码采用EVA-CLIP预训练模型，使用网格池化策略。双头设计包括语言建模头用于识别任务，自回归解码文本；基于锚点的扩散模型头用于轨迹规划，从预定义锚点去噪生成轨迹。训练策略联合优化跟踪损失和文本预测损失。推理优化只需2步去噪，实现5倍加速。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>EVT-Bench数据集包含100个人形化身和804个场景，总计25,986个episode，分为训练集(21,771)和测试集(4,215)，涵盖三个子任务：单目标跟踪(STT)、干扰跟踪(DT)、模糊跟踪(AT)。85.5万跟踪样本来自EVT-Bench训练集。85.5万识别样本包括36.2万人识别样本（基于SYNTH-PEDES）和49.3万开放世界VQA样本。训练资源方面，冻结视觉编码器参数，仅训练1个epoch。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境包括合成环境（EVT-Bench、Gym-UnrealCV公共基准）和真实环境（配备RGB摄像头的机器人平台）。评估指标包括跟踪性能（成功率，保持1-3米跟随距离且面向目标）、识别性能（文本预测准确率）、推理速度（FPS，报告10 FPS），以及轨迹回归误差(MSE)和分类损失(BCE)。</p>"
  },
  {
    "date": "2025-05-28",
    "title": "Zero-Shot 3D Visual Grounding from Vision-Language Models",
    "link": "http://arxiv.org/abs/2505.22429",
    "summary_markdown": "## 论文研究单位\n- 香港科技大学（广州）\n- 新加坡科技研究局信息通信研究院 (I2R, A*STAR)\n- 新加坡国立大学 (NUS)\n- 香港科技大学计算机科学与工程系 (CSE, HKUST)\n## 论文概述\n3D视觉定位（3DVG）旨在根据自然语言描述在3D场景中定位目标对象，对增强现实和机器人等下游应用至关重要。现有方法通常依赖于带标注的3D数据和预定义类别，限制了其在开放世界场景下的可扩展性。本文提出了SeeGround，一个零样本的3DVG框架，它利用2D视觉语言模型（VLMs）来绕过对3D特定训练的需求。为了弥合模态差距，论文引入了一种混合输入格式，该格式将查询对齐的渲染视图与空间丰富的文本描述相结合。该框架包含两个核心组件：一个基于查询动态选择最优视点的视角自适应模块，以及一个融合视觉和空间信号以提高定位精度的融合对齐模块。在ScanRefer和Nr3D数据集上的广泛评估证实，SeeGround显著优于现有的零样本基线，并与完全监督的替代方案相媲美，在挑战性条件下展现了强大的泛化能力。\n## 论文核心贡献点\n- 提出了SeeGround，一个用于零样本3D视觉定位的免训练方法，该方法通过渲染视图和空间文本将3D场景重新格式化为适合2D-VLMs的输入。\n- 设计了一种查询引导的视角选择策略，以捕获对象特定线索和空间上下文。\n- 提出了一种视觉提示机制，以对齐2D图像特征与3D空间描述，减少杂乱场景中的定位模糊性。\n- 该方法在ScanRefer和Nr3D上实现了最先进的零样本结果，证明了无需任何3D特定训练即可实现的强大泛化能力。\n## 论文方法描述\nSeeGround框架主要由三个部分组成：\n1. **多模态3D表示**：首先使用开放词汇3D检测器检测场景中的所有对象，并将其边界框和语义标签存储在一个对象查找表（OLT）中。然后，通过查询驱动的渲染函数生成一个与查询对齐的2D图像和相应的空间文本描述，构成混合表示。\n2. **视角自适应模块**：为了解决传统视角与查询视角不匹配的问题，该模块使用2D-VLM从查询中识别一个锚点对象和一组候选目标。虚拟相机被放置在场景中心，面向锚点对象，并略微后移和上移以获得更好的视野。如果无法明确提取锚点，则默认使用候选对象的质心作为伪锚点。\n3. **融合对齐模块**：该模块采用视觉提示技术。在渲染的图像上，将OLT中可见对象的投影2D边界框高亮显示，引导2D-VLM关注相关区域，解决模糊性问题。被提示的图像、空间文本和原始查询一起被输入到2D-VLM中，以输出目标对象的ID，最后从OLT中检索其对应的3D边界框。\n## 论文使用数据集和训练资源\n- **数据集**：\n - ScanRefer\n - Nr3D\n- **训练资源**：\n - 该方法是免训练和零样本的，不需要进行任何3D特定的训练。\n - 它使用预训练的2D视觉语言模型Qwen2-VL-72B作为核心代理。\n## 论文使用的评估环境和评估指标\n- **评估指标**：\n - 在不同交并比阈值下的定位准确率，具体为Acc@0.25IoU和Acc@0.5IoU。\n- **评估环境**：\n - 实验在ScanRefer和Nr3D的验证集上进行。文中未明确指定具体的硬件环境，但推断过程依赖于大型预训练模型Qwen2-VL-72B。",
    "summary_html": "<h2>论文研究单位</h2>\n<ul><li>香港科技大学（广州）</li><li>新加坡科技研究局信息通信研究院 (I2R, A*STAR)</li><li>新加坡国立大学 (NUS)</li><li>香港科技大学计算机科学与工程系 (CSE, HKUST)</li></ul>\n<h2>论文概述</h2>\n<p>3D视觉定位（3DVG）旨在根据自然语言描述在3D场景中定位目标对象，对增强现实和机器人等下游应用至关重要。现有方法通常依赖于带标注的3D数据和预定义类别，限制了其在开放世界场景下的可扩展性。本文提出了SeeGround，一个零样本的3DVG框架，它利用2D视觉语言模型（VLMs）来绕过对3D特定训练的需求。为了弥合模态差距，论文引入了一种混合输入格式，该格式将查询对齐的渲染视图与空间丰富的文本描述相结合。该框架包含两个核心组件：一个基于查询动态选择最优视点的视角自适应模块，以及一个融合视觉和空间信号以提高定位精度的融合对齐模块。在ScanRefer和Nr3D数据集上的广泛评估证实，SeeGround显著优于现有的零样本基线，并与完全监督的替代方案相媲美，在挑战性条件下展现了强大的泛化能力。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了SeeGround，一个用于零样本3D视觉定位的免训练方法，该方法通过渲染视图和空间文本将3D场景重新格式化为适合2D-VLMs的输入。</li><li>设计了一种查询引导的视角选择策略，以捕获对象特定线索和空间上下文。</li><li>提出了一种视觉提示机制，以对齐2D图像特征与3D空间描述，减少杂乱场景中的定位模糊性。</li><li>该方法在ScanRefer和Nr3D上实现了最先进的零样本结果，证明了无需任何3D特定训练即可实现的强大泛化能力。</li></ul>\n<h2>论文方法描述</h2>\n<p>SeeGround框架主要由三个部分组成：</p>\n<ol><li><strong>多模态3D表示</strong>：首先使用开放词汇3D检测器检测场景中的所有对象，并将其边界框和语义标签存储在一个对象查找表（OLT）中。然后，通过查询驱动的渲染函数生成一个与查询对齐的2D图像和相应的空间文本描述，构成混合表示。</li><li><strong>视角自适应模块</strong>：为了解决传统视角与查询视角不匹配的问题，该模块使用2D-VLM从查询中识别一个锚点对象和一组候选目标。虚拟相机被放置在场景中心，面向锚点对象，并略微后移和上移以获得更好的视野。如果无法明确提取锚点，则默认使用候选对象的质心作为伪锚点。</li><li><strong>融合对齐模块</strong>：该模块采用视觉提示技术。在渲染的图像上，将OLT中可见对象的投影2D边界框高亮显示，引导2D-VLM关注相关区域，解决模糊性问题。被提示的图像、空间文本和原始查询一起被输入到2D-VLM中，以输出目标对象的ID，最后从OLT中检索其对应的3D边界框。</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - ScanRefer</p>\n<p> - Nr3D</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 该方法是免训练和零样本的，不需要进行任何3D特定的训练。</p>\n<p> - 它使用预训练的2D视觉语言模型Qwen2-VL-72B作为核心代理。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 在不同交并比阈值下的定位准确率，具体为Acc@0.25IoU和Acc@0.5IoU。</p>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 实验在ScanRefer和Nr3D的验证集上进行。文中未明确指定具体的硬件环境，但推断过程依赖于大型预训练模型Qwen2-VL-72B。</p>"
  },
  {
    "date": "2025-05-28",
    "title": "ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation",
    "link": "http://arxiv.org/abs/2505.22159",
    "summary_markdown": "### 论文研究单位\n复旦大学、上海交通大学、上海创新研究院、上海AI实验室、新加坡国立大学、上海大学、西安交通大学、Noematrix Intelligence\n### 论文概述\n论文提出ForceVLA，一种增强视觉-语言-动作（VLA）模型的端到端操作框架，通过将外部力感知作为核心模态，解决接触密集型操作任务中精细力控制的需求。该框架引入FVLMoE（力感知混合专家模块），动态融合预训练视觉-语言嵌入与实时6轴力反馈，并在五个接触密集任务中验证有效性，平均成功率提升23.2%，最高达80%。\n### 论文核心贡献点\n- 提出力感知多模态框架：首次将力作为一等公民模态集成到VLA系统，通过FVLMoE实现动态模态感知融合，增强物理交互能力。\n- 构建数据集与工具链：发布ForceVLA-Data数据集（244条轨迹，14万时间步）及完整数据采集系统（含遥操作工具和转换器），开源以支持社区研究。\n- 实验性能突破：在插拔、擦拭等任务中成功率显著超越基线（如π₀模型），泛化至新物体、遮挡和物理扰动场景，验证鲁棒性。\n### 论文方法描述\n- **整体架构**：基于π₀框架，扩展多模态输入（视觉、语言、本体感觉、力）。视觉和语言通过SigLIP编码为嵌入，力信号经线性投影后与VL嵌入融合。\n- **FVLMoE模块**：采用后融合策略，力令牌在VLM处理后输入MoE。包含4个专家网络（MLP）和路由器，通过Top-1路由动态选择专家，输出通过残差连接注入动作流解码器，指导力感知动作生成。\n- **动作生成**：采用条件流匹配模型，融合特征通过逐元素加法调制噪声动作轨迹，迭代去噪输出H步动作块。\n### 论文使用数据集和训练资源\n- **数据集**：ForceVLA-Data，涵盖5个任务（按瓶、插拔、USB插入、擦白板、削黄瓜）。通过Flexiv Rizon 7-DOF机械臂采集，双RGB-D相机（第三人称1280×720@30fps、手腕640×480@30fps）同步视觉、本体感觉和6轴力矩数据。\n- **训练资源**：8×NVIDIA RTX 4090 GPU（24GB VRAM），64核CPU，251GB RAM。使用Adam优化器（β₁=0.9, β₂=0.95），峰值学习率2.5×10⁻⁵，多任务训练30k步（约12小时），单任务训练10k步（约9小时），精度bfloat16。\n### 论文使用的评估环境和评估指标\n- **环境**：真实物理场景，五项接触密集任务：瓶泵按压、插头插入、USB插入、白板擦拭、黄瓜削皮。泛化测试包括物体更换（瓶/插头类型）、高度变化、视觉遮挡、插座不稳等扰动。\n- **指标**：主要任务成功率（按任务特定标准，如完全插入/擦拭）。削黄瓜任务附加指标：平均削皮长度（cm↑）、最少完成行程数（↓）。泛化场景报告各条件下成功率（%）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>复旦大学、上海交通大学、上海创新研究院、上海AI实验室、新加坡国立大学、上海大学、西安交通大学、Noematrix Intelligence</p>\n<h3>论文概述</h3>\n<p>论文提出ForceVLA，一种增强视觉-语言-动作（VLA）模型的端到端操作框架，通过将外部力感知作为核心模态，解决接触密集型操作任务中精细力控制的需求。该框架引入FVLMoE（力感知混合专家模块），动态融合预训练视觉-语言嵌入与实时6轴力反馈，并在五个接触密集任务中验证有效性，平均成功率提升23.2%，最高达80%。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出力感知多模态框架：首次将力作为一等公民模态集成到VLA系统，通过FVLMoE实现动态模态感知融合，增强物理交互能力。</li><li>构建数据集与工具链：发布ForceVLA-Data数据集（244条轨迹，14万时间步）及完整数据采集系统（含遥操作工具和转换器），开源以支持社区研究。</li><li>实验性能突破：在插拔、擦拭等任务中成功率显著超越基线（如π₀模型），泛化至新物体、遮挡和物理扰动场景，验证鲁棒性。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>整体架构</strong>：基于π₀框架，扩展多模态输入（视觉、语言、本体感觉、力）。视觉和语言通过SigLIP编码为嵌入，力信号经线性投影后与VL嵌入融合。</li><li><strong>FVLMoE模块</strong>：采用后融合策略，力令牌在VLM处理后输入MoE。包含4个专家网络（MLP）和路由器，通过Top-1路由动态选择专家，输出通过残差连接注入动作流解码器，指导力感知动作生成。</li><li><strong>动作生成</strong>：采用条件流匹配模型，融合特征通过逐元素加法调制噪声动作轨迹，迭代去噪输出H步动作块。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：ForceVLA-Data，涵盖5个任务（按瓶、插拔、USB插入、擦白板、削黄瓜）。通过Flexiv Rizon 7-DOF机械臂采集，双RGB-D相机（第三人称1280×720@30fps、手腕640×480@30fps）同步视觉、本体感觉和6轴力矩数据。</li><li><strong>训练资源</strong>：8×NVIDIA RTX 4090 GPU（24GB VRAM），64核CPU，251GB RAM。使用Adam优化器（β₁=0.9, β₂=0.95），峰值学习率2.5×10⁻⁵，多任务训练30k步（约12小时），单任务训练10k步（约9小时），精度bfloat16。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>环境</strong>：真实物理场景，五项接触密集任务：瓶泵按压、插头插入、USB插入、白板擦拭、黄瓜削皮。泛化测试包括物体更换（瓶/插头类型）、高度变化、视觉遮挡、插座不稳等扰动。</li><li><strong>指标</strong>：主要任务成功率（按任务特定标准，如完全插入/擦拭）。削黄瓜任务附加指标：平均削皮长度（cm↑）、最少完成行程数（↓）。泛化场景报告各条件下成功率（%）。</li></ul>"
  },
  {
    "date": "2025-05-28",
    "title": "ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge",
    "link": "http://arxiv.org/abs/2505.21906",
    "summary_markdown": "论文研究单位\nMidea Group, East China Normal University\n\n论文概述\n该论文提出ChatVLA-2，一种新型的视觉-语言-动作模型，旨在解决现有VLA模型在针对机器人任务进行微调时，会丢失预训练VLM核心能力的问题。ChatVLA-2通过一个新颖的混合专家模型和一个两阶段训练流程，保留了VLM的开放世界具身推理（如识别任意物体、解决数学问题）和推理遵循（将推理转化为可执行步骤）能力。论文通过设计数学配对游戏和玩具放置实验进行验证，结果表明ChatVLA-2在开放世界场景下展现出卓越的数学推理、OCR和空间推理能力，显著超越了OpenVLA、DexVLA和π₀等现有模仿学习方法。\n\n论文核心贡献点\n- 提出了ChatVLA-2模型，旨在保留并利用预训练VLM的知识，以实现开放世界下的机器人通用化操作。\n- 设计了一个动态混合专家架构，用于解耦多模态理解与机器人控制的特征空间，同时保留它们之间的共享表示。\n- 引入了一个推理遵循增强模块，旨在使机器人的动作能够紧密对齐并遵循其内部生成的推理过程。\n- 提出一个两阶段训练策略：第一阶段通过图像-文本数据与机器人数据的共同训练，赋予模型开放世界推理能力；第二阶段冻结VLM，仅训练动作专家，以增强模型遵循推理的能力。\n- 通过数学匹配和玩具放置任务，验证了模型在分布外的数学推理、OCR和空间推理能力，展示了其在开放环境下的强大泛化性能。\n\n论文方法描述\nChatVLA-2基于DexVLA架构，以Qwen2-VL作为核心VLM。\n- 模型架构：\n - 动态混合专家：采用MoE架构，根据输入动态选择专家模块。部分专家专注于多模态理解或机器人控制等特定任务，另一部分专家则捕获空间推理等跨任务的共享特征。使用8个专家，推理时激活其中2个。\n - 推理遵循增强模块：将模型原始的观测嵌入替换为通过MLP投影的推理token。该推理表征与时间步嵌入结合，用于调节尺度和平移参数的生成，从而将推理上下文注入模型。该模块仅集成在模型的后半层。\n- 训练策略：\n - 第一阶段：赋予开放世界具身推理。在COCO、TextVQA、GQA等图像-文本数据集和自建的600条数学游戏、300条玩具放置机器人轨迹上进行共同训练，图像-文本数据与机器人数据比例为1:3。\n - 第二阶段：增强推理遵循。冻结整个VLM骨干网络，仅训练动作专家，使其学习根据VLM上层生成的推理来生成一致的动作。\n\n论文使用数据集和训练资源\n- 数据集：\n - 图像-文本数据：COCO（约32k样本）、TextVQA（约20k样本）、GQA（约54k样本），以及自建的机器人场景图像-文本对、RoboPoint数据（约2k样本）和真实世界数据（约5k样本）。\n - 机器人数据：自采集的600条数学匹配游戏轨迹和300条玩具放置实验轨迹，所有数据均附有推理短语标注。\n- 训练资源：训练总耗时为340 GPU小时，采用混合精度（FP16）和AdamW优化器。第一阶段训练15k步，学习率2e-5；第二阶段训练50k步，初始学习率2e-5，并使用预热和余弦学习率衰减策略。\n\n论文使用的评估环境和评估指标\n- 评估环境：所有实验均在真实机器人上完成。\n - 数学匹配任务：使用ARX-R5双臂机器人，每只手臂6自由度，顶部配备RealSense L515相机，状态和动作空间为14维，数据采集频率为50Hz。\n - 玩具放置任务：使用7自由度的Franka Emika机器人，配备Robotiq夹爪和ZED 2相机，数据采集频率为15Hz。\n- 评估指标：\n - 数学匹配任务：\n - 操纵成功率：任务完成的平均成功率。\n - OCR：正确识别手写数字（1分）、识别卡牌值与位置（1分）、正确识别符号（2分），总计4分。\n - 数学推理：答案正确（1分）、选择卡牌正确（1分），总计2分。\n - 玩具放置任务：\n - 平均成功率：任务完成的平均成功率。\n - 开放世界对象识别：模型在推理输出中正确识别目标对象并输出其边界框的能力。\n - 空间可供性：模型根据指令在参考对象的相对位置正确放置物体的能力。",
    "summary_html": "<p>论文研究单位</p>\n<p>Midea Group, East China Normal University</p>\n\n<p>论文概述</p>\n<p>该论文提出ChatVLA-2，一种新型的视觉-语言-动作模型，旨在解决现有VLA模型在针对机器人任务进行微调时，会丢失预训练VLM核心能力的问题。ChatVLA-2通过一个新颖的混合专家模型和一个两阶段训练流程，保留了VLM的开放世界具身推理（如识别任意物体、解决数学问题）和推理遵循（将推理转化为可执行步骤）能力。论文通过设计数学配对游戏和玩具放置实验进行验证，结果表明ChatVLA-2在开放世界场景下展现出卓越的数学推理、OCR和空间推理能力，显著超越了OpenVLA、DexVLA和π₀等现有模仿学习方法。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了ChatVLA-2模型，旨在保留并利用预训练VLM的知识，以实现开放世界下的机器人通用化操作。</li><li>设计了一个动态混合专家架构，用于解耦多模态理解与机器人控制的特征空间，同时保留它们之间的共享表示。</li><li>引入了一个推理遵循增强模块，旨在使机器人的动作能够紧密对齐并遵循其内部生成的推理过程。</li><li>提出一个两阶段训练策略：第一阶段通过图像-文本数据与机器人数据的共同训练，赋予模型开放世界推理能力；第二阶段冻结VLM，仅训练动作专家，以增强模型遵循推理的能力。</li><li>通过数学匹配和玩具放置任务，验证了模型在分布外的数学推理、OCR和空间推理能力，展示了其在开放环境下的强大泛化性能。</li></ul>\n\n<p>论文方法描述</p>\n<p>ChatVLA-2基于DexVLA架构，以Qwen2-VL作为核心VLM。</p>\n<ul><li>模型架构：</li></ul>\n<p> - 动态混合专家：采用MoE架构，根据输入动态选择专家模块。部分专家专注于多模态理解或机器人控制等特定任务，另一部分专家则捕获空间推理等跨任务的共享特征。使用8个专家，推理时激活其中2个。</p>\n<p> - 推理遵循增强模块：将模型原始的观测嵌入替换为通过MLP投影的推理token。该推理表征与时间步嵌入结合，用于调节尺度和平移参数的生成，从而将推理上下文注入模型。该模块仅集成在模型的后半层。</p>\n<ul><li>训练策略：</li></ul>\n<p> - 第一阶段：赋予开放世界具身推理。在COCO、TextVQA、GQA等图像-文本数据集和自建的600条数学游戏、300条玩具放置机器人轨迹上进行共同训练，图像-文本数据与机器人数据比例为1:3。</p>\n<p> - 第二阶段：增强推理遵循。冻结整个VLM骨干网络，仅训练动作专家，使其学习根据VLM上层生成的推理来生成一致的动作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：</li></ul>\n<p> - 图像-文本数据：COCO（约32k样本）、TextVQA（约20k样本）、GQA（约54k样本），以及自建的机器人场景图像-文本对、RoboPoint数据（约2k样本）和真实世界数据（约5k样本）。</p>\n<p> - 机器人数据：自采集的600条数学匹配游戏轨迹和300条玩具放置实验轨迹，所有数据均附有推理短语标注。</p>\n<ul><li>训练资源：训练总耗时为340 GPU小时，采用混合精度（FP16）和AdamW优化器。第一阶段训练15k步，学习率2e-5；第二阶段训练50k步，初始学习率2e-5，并使用预热和余弦学习率衰减策略。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：所有实验均在真实机器人上完成。</li></ul>\n<p> - 数学匹配任务：使用ARX-R5双臂机器人，每只手臂6自由度，顶部配备RealSense L515相机，状态和动作空间为14维，数据采集频率为50Hz。</p>\n<p> - 玩具放置任务：使用7自由度的Franka Emika机器人，配备Robotiq夹爪和ZED 2相机，数据采集频率为15Hz。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 数学匹配任务：</p>\n<p> - 操纵成功率：任务完成的平均成功率。</p>\n<p> - OCR：正确识别手写数字（1分）、识别卡牌值与位置（1分）、正确识别符号（2分），总计4分。</p>\n<p> - 数学推理：答案正确（1分）、选择卡牌正确（1分），总计2分。</p>\n<p> - 玩具放置任务：</p>\n<p> - 平均成功率：任务完成的平均成功率。</p>\n<p> - 开放世界对象识别：模型在推理输出中正确识别目标对象并输出其边界框的能力。</p>\n<p> - 空间可供性：模型根据指令在参考对象的相对位置正确放置物体的能力。</p>"
  },
  {
    "date": "2025-05-27",
    "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2505.21567",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-27",
    "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
    "link": "http://arxiv.org/abs/2505.21432",
    "summary_markdown": "论文研究单位\n上海交通大学\n上海人工智能实验室\n复旦大学\nAgiBot\nINSAIT, Sofia University\n浙江大学\n西北工业大学\n\n论文概述\n论文提出了Hume，一个具有价值引导的System-2思维和级联动作去噪的双系统视觉-语言-动作模型，旨在为灵巧的机器人控制引入类似人类的思考能力。模型通过在预训练的视觉-语言模型上增加一个新颖的价值查询头来估计预测动作的状态-动作价值，并通过重复采样多个动作候选并依据状态-动作价值进行选择来实现价值引导的思维。System 1是一个轻量级的反应式视觉运动策略，它接收System 2选择的动作并执行级联动作去噪以生成最终的流畅动作。在部署时，System 2以低频率运行，而System 1以高频率异步运行，从而有效结合了慢速思考与快速行动。Hume在多个模拟基准和真实机器人部署中超越了现有的最先进视觉-语言-动作模型。\n\n论文核心贡献点\n提出了Hume，一个探索System-2慢速思维范式的双系统通用机器人策略。\n引入了新颖的价值引导思维和级联动作去噪方法，无缝结合了低频的System 2和高频的System 1，实现了在各种机器人部署中的有效思考和推理。\nHume在多个基准和真实机器人测试中实现了最先进的性能，在LIBERO基准上比$\\pi_{0}$的成功率提高了4.4%，在Simpler基准上提高了25.9%，在真实世界部署中改进了12.9%。\n\n论文方法描述\nHume包含两个异步工作的系统：System 2和System 1。\nSystem 2是一个基于预训练VLM的模型，包含两个专门的头部：一个流匹配去噪头用于预测机器人动作，和一个价值查询头用于估计预测动作的状态-动作价值。它接收机器人观察、语言指令和状态信息，通过重复采样N个不同噪声水平的候选动作块，并选择具有最高状态-动作价值的一个，从而实现价值引导的慢速思考。\nSystem 1是一个轻量级的反应式视觉运动策略。它接收System 2选定的最佳动作块的一部分，结合当前视觉观察和机器人状态，然后通过一个独立的轻量级扩散策略进行级联动作去噪，以生成用于机器人执行的最终流畅动作。\n部署时，System 2以低频率（4 Hz）执行价值引导思考，而System 1异步接收System 2选定的动作候选并以高频率（90 Hz）预测实时动作。\n\n论文使用数据集和训练资源\n论文在多个模拟基准和真实机器人平台上进行了评估，包括21个真实世界机器人设置和3个模拟环境。\n使用的模拟基准包括LIBERO和Simpler。\n训练采用了多阶段策略，具体训练资源和超参数在附录B.2中有详细说明，但原文未提供具体硬件资源信息。\n\n论文使用的评估环境和评估指标\n评估环境包括模拟基准测试和真实世界机器人控制测试。\n模拟环境包括LIBERO、Simpler等多个基准，用于进行多任务评估。\n真实世界评估在21种不同的机器人设置中进行，涵盖了视角、纹理、光照、布局、未见物体、未见环境的变化，以及最具挑战性的人形机器人控制任务。\n主要评估指标是任务的成功率，用于衡量模型完成指定任务的能力。",
    "summary_html": "<p>论文研究单位</p>\n<p>上海交通大学</p>\n<p>上海人工智能实验室</p>\n<p>复旦大学</p>\n<p>AgiBot</p>\n<p>INSAIT, Sofia University</p>\n<p>浙江大学</p>\n<p>西北工业大学</p>\n\n<p>论文概述</p>\n<p>论文提出了Hume，一个具有价值引导的System-2思维和级联动作去噪的双系统视觉-语言-动作模型，旨在为灵巧的机器人控制引入类似人类的思考能力。模型通过在预训练的视觉-语言模型上增加一个新颖的价值查询头来估计预测动作的状态-动作价值，并通过重复采样多个动作候选并依据状态-动作价值进行选择来实现价值引导的思维。System 1是一个轻量级的反应式视觉运动策略，它接收System 2选择的动作并执行级联动作去噪以生成最终的流畅动作。在部署时，System 2以低频率运行，而System 1以高频率异步运行，从而有效结合了慢速思考与快速行动。Hume在多个模拟基准和真实机器人部署中超越了现有的最先进视觉-语言-动作模型。</p>\n\n<p>论文核心贡献点</p>\n<p>提出了Hume，一个探索System-2慢速思维范式的双系统通用机器人策略。</p>\n<p>引入了新颖的价值引导思维和级联动作去噪方法，无缝结合了低频的System 2和高频的System 1，实现了在各种机器人部署中的有效思考和推理。</p>\n<p>Hume在多个基准和真实机器人测试中实现了最先进的性能，在LIBERO基准上比$\\pi_{0}$的成功率提高了4.4%，在Simpler基准上提高了25.9%，在真实世界部署中改进了12.9%。</p>\n\n<p>论文方法描述</p>\n<p>Hume包含两个异步工作的系统：System 2和System 1。</p>\n<p>System 2是一个基于预训练VLM的模型，包含两个专门的头部：一个流匹配去噪头用于预测机器人动作，和一个价值查询头用于估计预测动作的状态-动作价值。它接收机器人观察、语言指令和状态信息，通过重复采样N个不同噪声水平的候选动作块，并选择具有最高状态-动作价值的一个，从而实现价值引导的慢速思考。</p>\n<p>System 1是一个轻量级的反应式视觉运动策略。它接收System 2选定的最佳动作块的一部分，结合当前视觉观察和机器人状态，然后通过一个独立的轻量级扩散策略进行级联动作去噪，以生成用于机器人执行的最终流畅动作。</p>\n<p>部署时，System 2以低频率（4 Hz）执行价值引导思考，而System 1异步接收System 2选定的动作候选并以高频率（90 Hz）预测实时动作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>论文在多个模拟基准和真实机器人平台上进行了评估，包括21个真实世界机器人设置和3个模拟环境。</p>\n<p>使用的模拟基准包括LIBERO和Simpler。</p>\n<p>训练采用了多阶段策略，具体训练资源和超参数在附录B.2中有详细说明，但原文未提供具体硬件资源信息。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境包括模拟基准测试和真实世界机器人控制测试。</p>\n<p>模拟环境包括LIBERO、Simpler等多个基准，用于进行多任务评估。</p>\n<p>真实世界评估在21种不同的机器人设置中进行，涵盖了视角、纹理、光照、布局、未见物体、未见环境的变化，以及最具挑战性的人形机器人控制任务。</p>\n<p>主要评估指标是任务的成功率，用于衡量模型完成指定任务的能力。</p>"
  },
  {
    "date": "2025-05-27",
    "title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2505.21200",
    "summary_markdown": "论文研究单位\n复旦大学、上海人工智能实验室、香港中文大学、张江实验室。\n\n论文概述\n该论文提出了一种名为FlashVLA的训练无关、即插即用的加速框架，用于提高视觉-语言-动作（VLA）模型的推理效率。它通过识别并利用VLA模型中存在的两种冗余形式：连续动作步骤之间的高相似性以及视觉标记中的显著冗余。FlashVLA采用了一种“三思而后行”的范式，通过token感知的动作重用机制和基于信息贡献分数的视觉标记选择策略，在保持控制精度的同时，显著减少了浮点运算数（FLOPs）和推理延迟。\n\n论文核心贡献点\n1. 识别了VLA推理中一种新颖的动作级和token级冗余：连续的动作步骤产生高度相似的输出，允许在稳定阶段重用动作；同时许多视觉token对整体推理过程贡献很小，揭示了与MLLM中类似的视觉冗余程度。\n2. 引入了FlashVLA，这是第一个用于在VLA模型中实现动作重用的训练无关、即插即用的加速框架。它集成了一个token感知的动作重用机制，以跳过稳定动作步骤中的冗余计算，以及一个基于信息贡献分数的视觉token选择策略，以保留信息丰富的token。值得注意的是，FlashVLA完全兼容基于Flash Attention的VLA主干网络。\n3. 在LIBERO基准测试的四个代表性任务上进行了全面实验。当视觉token减少到原始输入的62.5%时，FlashVLA将推理延迟降低了36.0%，并将视觉token计算的FLOPs减少了55.7%，而成功率仅下降了0.7%。这些结果表明，FlashVLA以有限的性能权衡实现了显著的效率提升。\n\n论文方法描述\nFlashVLA框架包含两个主要组件：\n1. 视觉标记选择策略：基于信息贡献理论（ICS），通过奇异值分解（SVD）量化每个视觉token对全局特征表示的贡献。它计算每个token的ICS分数，该分数衡量了token在主要奇异方向上的投影幅度。通过选择ICS分数最高的top-K个token，该方法能比随机采样保留更多的结构信息，从而在不显著损失性能的情况下减少计算量。\n2. Token感知动作重用策略：该策略通过一个名为FlashTrigger的模块来决定是否重用前一动作或执行新的推理。FlashTrigger包含动作记忆（存储前两个动作向量）、token记忆（存储前两个token状态）和一个触发思考模块。它通过计算前两个动作向量之间的夹角α来衡量动作的一致性。如果动作变化小且视觉token稳定，则跳过当前推理，直接复用上一动作，从而避免冗余计算。为确保稳定性，前两帧不进行动作重用。\n\n论文使用数据集和训练资源\n数据集：LIBERO基准测试，具体包括其四个子任务：LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long。此外，还在VLAbench上进行了验证以评估泛化性。\n训练资源：论文明确指出FlashVLA是一个训练无关（training-free）的框架，因此不需要额外的训练或微调。\n\n论文使用的评估环境和评估指标\n评估环境：模拟环境，具体是LIBERO模拟基准套件。\n评估指标：\n1. 效率指标：浮点运算数，推理延迟。\n2. 性能指标：任务成功率。",
    "summary_html": "<p>论文研究单位</p>\n<p>复旦大学、上海人工智能实验室、香港中文大学、张江实验室。</p>\n\n<p>论文概述</p>\n<p>该论文提出了一种名为FlashVLA的训练无关、即插即用的加速框架，用于提高视觉-语言-动作（VLA）模型的推理效率。它通过识别并利用VLA模型中存在的两种冗余形式：连续动作步骤之间的高相似性以及视觉标记中的显著冗余。FlashVLA采用了一种“三思而后行”的范式，通过token感知的动作重用机制和基于信息贡献分数的视觉标记选择策略，在保持控制精度的同时，显著减少了浮点运算数（FLOPs）和推理延迟。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>识别了VLA推理中一种新颖的动作级和token级冗余：连续的动作步骤产生高度相似的输出，允许在稳定阶段重用动作；同时许多视觉token对整体推理过程贡献很小，揭示了与MLLM中类似的视觉冗余程度。</li><li>引入了FlashVLA，这是第一个用于在VLA模型中实现动作重用的训练无关、即插即用的加速框架。它集成了一个token感知的动作重用机制，以跳过稳定动作步骤中的冗余计算，以及一个基于信息贡献分数的视觉token选择策略，以保留信息丰富的token。值得注意的是，FlashVLA完全兼容基于Flash Attention的VLA主干网络。</li><li>在LIBERO基准测试的四个代表性任务上进行了全面实验。当视觉token减少到原始输入的62.5%时，FlashVLA将推理延迟降低了36.0%，并将视觉token计算的FLOPs减少了55.7%，而成功率仅下降了0.7%。这些结果表明，FlashVLA以有限的性能权衡实现了显著的效率提升。</li></ol>\n\n<p>论文方法描述</p>\n<p>FlashVLA框架包含两个主要组件：</p>\n<ol><li>视觉标记选择策略：基于信息贡献理论（ICS），通过奇异值分解（SVD）量化每个视觉token对全局特征表示的贡献。它计算每个token的ICS分数，该分数衡量了token在主要奇异方向上的投影幅度。通过选择ICS分数最高的top-K个token，该方法能比随机采样保留更多的结构信息，从而在不显著损失性能的情况下减少计算量。</li><li>Token感知动作重用策略：该策略通过一个名为FlashTrigger的模块来决定是否重用前一动作或执行新的推理。FlashTrigger包含动作记忆（存储前两个动作向量）、token记忆（存储前两个token状态）和一个触发思考模块。它通过计算前两个动作向量之间的夹角α来衡量动作的一致性。如果动作变化小且视觉token稳定，则跳过当前推理，直接复用上一动作，从而避免冗余计算。为确保稳定性，前两帧不进行动作重用。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：LIBERO基准测试，具体包括其四个子任务：LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long。此外，还在VLAbench上进行了验证以评估泛化性。</p>\n<p>训练资源：论文明确指出FlashVLA是一个训练无关（training-free）的框架，因此不需要额外的训练或微调。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：模拟环境，具体是LIBERO模拟基准套件。</p>\n<p>评估指标：</p>\n<ol><li>效率指标：浮点运算数，推理延迟。</li><li>性能指标：任务成功率。</li></ol>"
  },
  {
    "date": "2025-05-27",
    "title": "Hierarchical Instruction-aware Embodied Visual Tracking",
    "link": "http://arxiv.org/abs/2505.20710",
    "summary_markdown": "### 论文研究单位\nBeihang University, City University of Macau, Peking University, Mohamed bin Zayed University of Artificial Intelligence, Beijing Normal University\n### 论文概述\n本文提出了一个新的任务：用户中心具身视觉跟踪，并设计了一个名为分层指令感知具身视觉跟踪的代理来解决这个问题。该代理通过引入中间的“空间目标”作为桥梁，将高级用户指令与低级代理行为连接起来。该代理包含两个核心组件：一个基于LLM的语义-空间目标对齐器，用于将用户指令翻译成空间目标；以及一个基于RL的自适应目标对齐策略，使跟踪器能够根据空间目标定位目标。\n### 论文核心贡献点\n1. 引入了用户中心具身视觉跟踪（UC-EVT）任务，为以用户为中心的人机交互奠定了基础。\n2. 提出了一种新颖的分层指令感知EVT（HIEVT）模型，有效解决了现有最先进模型的局限性。\n3. 通过准备1000万条标注轨迹的大规模数据集，并在10个不同的虚拟环境中进行了广泛评估，为UC-EVT任务建立了基准。\n4. 在10个虚拟环境和不同移动速度上的大量实验表明HIEVT显著优于现有基线。此外，在三个不同环境中的真实世界部署验证了该方法的鲁棒性和有效性。\n### 论文方法描述\nHIEVT模型将用户指令与代理状态之间的距离分解为两部分：用户指令与中间目标之间的距离，以及中间目标与代理状态之间的距离。\n- 基于LLM的语义-空间目标对齐器（SSGA）：负责将用户指令翻译成空间目标，该目标由目标类别和边界框格式的空间位置表示。该对齐器包含三个核心组件：语义解析、空间目标生成和检索增强的目标校正。\n - 语义解析：从用户指令中提取关键信息，如目标类别。\n - 空间目标生成：使用基于链式思维（COT）的推理机制，增量地调整当前目标的空间表示，该表示由视觉基础模型从环境观测中提取。\n - 检索增强的目标校正：使用检索增强生成（RAG）机制，确保生成的边界框与存储在记忆库中的轨迹先验保持一致。\n- 基于RL的自适应目标对齐策略：一个通用的离线策略，它结合了空间目标和视觉基础模型处理的观测，输入到策略网络中。该策略网络包括目标状态对齐器和循环策略，以生成适当的动作信号，从而维持与目标的期望空间关系。\n### 论文使用数据集和训练资源\n- 数据集：收集了超过1000万条用于训练的标注轨迹，并在一个已见环境和九个未见过的具有挑战性的环境中进行评估。\n- 训练资源：未在提供的HTML文本中明确说明。\n### 论文使用的评估环境和评估指标\n- 评估环境：在10个多样化的虚拟环境（1个已见环境和9个未见过的环境）和不同的移动速度下进行评估。此外，还在三个不同的真实世界环境中进行了部署和验证。\n- 评估指标：未在提供的HTML文本中明确说明。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Beihang University, City University of Macau, Peking University, Mohamed bin Zayed University of Artificial Intelligence, Beijing Normal University</p>\n<h3>论文概述</h3>\n<p>本文提出了一个新的任务：用户中心具身视觉跟踪，并设计了一个名为分层指令感知具身视觉跟踪的代理来解决这个问题。该代理通过引入中间的“空间目标”作为桥梁，将高级用户指令与低级代理行为连接起来。该代理包含两个核心组件：一个基于LLM的语义-空间目标对齐器，用于将用户指令翻译成空间目标；以及一个基于RL的自适应目标对齐策略，使跟踪器能够根据空间目标定位目标。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>引入了用户中心具身视觉跟踪（UC-EVT）任务，为以用户为中心的人机交互奠定了基础。</li><li>提出了一种新颖的分层指令感知EVT（HIEVT）模型，有效解决了现有最先进模型的局限性。</li><li>通过准备1000万条标注轨迹的大规模数据集，并在10个不同的虚拟环境中进行了广泛评估，为UC-EVT任务建立了基准。</li><li>在10个虚拟环境和不同移动速度上的大量实验表明HIEVT显著优于现有基线。此外，在三个不同环境中的真实世界部署验证了该方法的鲁棒性和有效性。</li></ol>\n<h3>论文方法描述</h3>\n<p>HIEVT模型将用户指令与代理状态之间的距离分解为两部分：用户指令与中间目标之间的距离，以及中间目标与代理状态之间的距离。</p>\n<ul><li>基于LLM的语义-空间目标对齐器（SSGA）：负责将用户指令翻译成空间目标，该目标由目标类别和边界框格式的空间位置表示。该对齐器包含三个核心组件：语义解析、空间目标生成和检索增强的目标校正。</li></ul>\n<p> - 语义解析：从用户指令中提取关键信息，如目标类别。</p>\n<p> - 空间目标生成：使用基于链式思维（COT）的推理机制，增量地调整当前目标的空间表示，该表示由视觉基础模型从环境观测中提取。</p>\n<p> - 检索增强的目标校正：使用检索增强生成（RAG）机制，确保生成的边界框与存储在记忆库中的轨迹先验保持一致。</p>\n<ul><li>基于RL的自适应目标对齐策略：一个通用的离线策略，它结合了空间目标和视觉基础模型处理的观测，输入到策略网络中。该策略网络包括目标状态对齐器和循环策略，以生成适当的动作信号，从而维持与目标的期望空间关系。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：收集了超过1000万条用于训练的标注轨迹，并在一个已见环境和九个未见过的具有挑战性的环境中进行评估。</li><li>训练资源：未在提供的HTML文本中明确说明。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：在10个多样化的虚拟环境（1个已见环境和9个未见过的环境）和不同的移动速度下进行评估。此外，还在三个不同的真实世界环境中进行了部署和验证。</li><li>评估指标：未在提供的HTML文本中明确说明。</li></ul>"
  },
  {
    "date": "2025-05-26",
    "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
    "link": "http://arxiv.org/abs/2505.19789",
    "summary_markdown": "论文研究单位\n清华大学, 上海期智研究院, 中关村学院\n\n论文概述\n本文通过实证研究探讨了强化学习（RL）微调相较于监督微调（SFT）对视觉-语言-动作（VLA）模型泛化能力的独特贡献。研究聚焦于典型抓取放置任务，建立了一个涵盖视觉、语义和执行三个维度的综合评估基准，系统分析了RL在分布外（OOD）场景下的泛化优势。\n\n论文核心贡献点\n- 建立了一个严谨且具有挑战性的基准，用于评估VLA微调方法在视觉、语义和执行维度的泛化能力。\n- 识别出PPO是比GRPO和DPO更有效的RL微调算法，并分析了RL算法从LLM/VLM范式适配到VLA的挑战。\n- 提出了高效的PPO训练配方，包括共享actor-critic主干、VLA模型预热和最小PPO训练轮数。\n- 证明RL微调在语义理解和执行鲁棒性上显著优于SFT，同时在视觉鲁棒性上与SFT相当。\n\n论文方法描述\n基于OpenVLA模型，使用LoRA进行低秩微调（rank=32）。方法核心是优化PPO设计：\n1. 共享actor-critic主干：策略和评论家共享Transformer骨干网络，评论家通过三层MLP头预测状态值，输入使用首个动作token的隐藏向量，提升效率并减少显存占用。\n2. VLA预热：使用运动规划器生成的140条演示轨迹对预训练OpenVLA进行预热，加速收敛。\n3. 最小PPO轮数：设置epoch=1，避免过度更新，缩短训练时间。\nRL微调通过环境交互优化任务奖励，使用广义优势估计（GAE）计算策略梯度。\n\n论文使用数据集和训练资源\n- 数据集：基于模拟器ManiSkill构建的抓取放置任务，包含16个训练桌子、16个物体和位置扰动。OOD测试引入9个新物体、16个新容器和5个新桌面纹理。SFT使用16k条演示轨迹（约1.26M状态转移），由Octo-Small和运动规划器生成。\n- 训练资源：所有实验在单块NVIDIA A100 GPU上进行，PPO微调约需42小时收敛，使用OpenVLA预训练检查点初始化。\n\n论文使用的评估环境和评估指标\n- 评估环境：ManiSkill模拟器，配备8自由度WidowX-250S机械臂。输入为640×480 RGB图像和自然语言指令，输出为末端执行器增量动作和夹爪信号。评估涵盖三个OOD维度：\n - 视觉：未见桌面、动态纹理（前景/图像级）和动态噪声（弱/强强度）。\n - 语义：未见物体、新容器、指令变化、多物体选择和多容器任务。\n - 执行：物体/容器初始位置扰动、机器人初始姿态变化和物体中途重定位。\n- 评估指标：任务成功率（主要）、抓取精度和连续抓取精度。计算相对性能下降率：\\(P = \\frac{\\text{OOD} - \\text{IND}}{\\text{IND}}\\)。所有结果使用多个随机种子平均。",
    "summary_html": "<p>论文研究单位</p>\n<p>清华大学, 上海期智研究院, 中关村学院</p>\n\n<p>论文概述</p>\n<p>本文通过实证研究探讨了强化学习（RL）微调相较于监督微调（SFT）对视觉-语言-动作（VLA）模型泛化能力的独特贡献。研究聚焦于典型抓取放置任务，建立了一个涵盖视觉、语义和执行三个维度的综合评估基准，系统分析了RL在分布外（OOD）场景下的泛化优势。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>建立了一个严谨且具有挑战性的基准，用于评估VLA微调方法在视觉、语义和执行维度的泛化能力。</li><li>识别出PPO是比GRPO和DPO更有效的RL微调算法，并分析了RL算法从LLM/VLM范式适配到VLA的挑战。</li><li>提出了高效的PPO训练配方，包括共享actor-critic主干、VLA模型预热和最小PPO训练轮数。</li><li>证明RL微调在语义理解和执行鲁棒性上显著优于SFT，同时在视觉鲁棒性上与SFT相当。</li></ul>\n\n<p>论文方法描述</p>\n<p>基于OpenVLA模型，使用LoRA进行低秩微调（rank=32）。方法核心是优化PPO设计：</p>\n<ol><li>共享actor-critic主干：策略和评论家共享Transformer骨干网络，评论家通过三层MLP头预测状态值，输入使用首个动作token的隐藏向量，提升效率并减少显存占用。</li><li>VLA预热：使用运动规划器生成的140条演示轨迹对预训练OpenVLA进行预热，加速收敛。</li><li>最小PPO轮数：设置epoch=1，避免过度更新，缩短训练时间。</li></ol>\n<p>RL微调通过环境交互优化任务奖励，使用广义优势估计（GAE）计算策略梯度。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：基于模拟器ManiSkill构建的抓取放置任务，包含16个训练桌子、16个物体和位置扰动。OOD测试引入9个新物体、16个新容器和5个新桌面纹理。SFT使用16k条演示轨迹（约1.26M状态转移），由Octo-Small和运动规划器生成。</li><li>训练资源：所有实验在单块NVIDIA A100 GPU上进行，PPO微调约需42小时收敛，使用OpenVLA预训练检查点初始化。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：ManiSkill模拟器，配备8自由度WidowX-250S机械臂。输入为640×480 RGB图像和自然语言指令，输出为末端执行器增量动作和夹爪信号。评估涵盖三个OOD维度：</li></ul>\n<p> - 视觉：未见桌面、动态纹理（前景/图像级）和动态噪声（弱/强强度）。</p>\n<p> - 语义：未见物体、新容器、指令变化、多物体选择和多容器任务。</p>\n<p> - 执行：物体/容器初始位置扰动、机器人初始姿态变化和物体中途重定位。</p>\n<ul><li>评估指标：任务成功率（主要）、抓取精度和连续抓取精度。计算相对性能下降率：\\(P = \\frac{\\text{OOD} - \\text{IND}}{\\text{IND}}\\)。所有结果使用多个随机种子平均。</li></ul>"
  },
  {
    "date": "2025-05-26",
    "title": "RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback",
    "link": "http://arxiv.org/abs/2505.19767",
    "summary_markdown": "### 论文研究单位\n北京大学王选计算机技术研究所\n### 论文概述\n本文提出了一种名为RFTF的强化微调方法，用于具有时间反馈的具身智能体。该方法旨在解决现有具身智能体在行为克隆训练中依赖昂贵数据和计算资源的问题，以及强化微调中稀疏奖励无法为特定动作提供细粒度反馈的挑战。RFTF通过一个价值模型生成密集奖励，该模型使用时间信息训练，无需昂贵的机器人动作标签。实验结果表明，在CALVIN ABC-D基准测试中，RFTF实现了新的最先进性能，平均成功长度为4.296，并能快速适应新环境。\n### 论文核心贡献点\n1. 提出了一种无需人工机器人动作标签的密集奖励强化微调方法。\n2. 设计了一个利用时间信息训练的价值模型，用于生成密集奖励，并结合奖励塑形和GAE策略以促进RL微调过程。\n3. 在CALVIN基准上的实验验证了方法的有效性，实现了新的最先进性能，并展示了在新环境中的优越适应能力。\n### 论文方法描述\n方法包含两个阶段：\n1. 价值模型训练：使用专家演示轨迹中的时间信息，假设状态价值随时间单调递增，通过对比学习训练模型预测每个状态的价值。模型结构基于VLA，将动作token替换为价值token。\n2. RL微调流程：集成训练好的价值模型到PPO框架中，使用价值模型输出的状态值计算密集奖励，采用GAE计算优势函数，并引入样本平衡系数处理成功与失败样本的不平衡。奖励函数定义为R_t = γV(s_{t+1},l) - V(s_t,l)，优势函数结合任务成功/失败指示器。\n### 论文使用数据集和训练资源\n数据集：CALVIN基准，包含34个任务和四个不同的模拟环境。\n训练资源：价值模型训练使用批大小4×8和学习率1e-5；RL微调在4块NVIDIA A40 GPU上进行，Seer-Large模型约需10小时，GR-MG模型约需14小时，覆盖约1000个回合。\n### 论文使用的评估环境和评估指标\n评估环境：CALVIN基准的四个模拟环境。\n评估指标：\n- 平均成功长度：平均连续完成的任务数，越高越好。\n- 任务完成率：在1000个序列中完成1至5个任务的比例，从L1到L5分别表示完成n个任务的比例。\n- 价值模型准确率：评估价值模型预测状态值单调性的准确率，超过94%后选择第一轮次模型。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学王选计算机技术研究所</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为RFTF的强化微调方法，用于具有时间反馈的具身智能体。该方法旨在解决现有具身智能体在行为克隆训练中依赖昂贵数据和计算资源的问题，以及强化微调中稀疏奖励无法为特定动作提供细粒度反馈的挑战。RFTF通过一个价值模型生成密集奖励，该模型使用时间信息训练，无需昂贵的机器人动作标签。实验结果表明，在CALVIN ABC-D基准测试中，RFTF实现了新的最先进性能，平均成功长度为4.296，并能快速适应新环境。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种无需人工机器人动作标签的密集奖励强化微调方法。</li><li>设计了一个利用时间信息训练的价值模型，用于生成密集奖励，并结合奖励塑形和GAE策略以促进RL微调过程。</li><li>在CALVIN基准上的实验验证了方法的有效性，实现了新的最先进性能，并展示了在新环境中的优越适应能力。</li></ol>\n<h3>论文方法描述</h3>\n<p>方法包含两个阶段：</p>\n<ol><li>价值模型训练：使用专家演示轨迹中的时间信息，假设状态价值随时间单调递增，通过对比学习训练模型预测每个状态的价值。模型结构基于VLA，将动作token替换为价值token。</li><li>RL微调流程：集成训练好的价值模型到PPO框架中，使用价值模型输出的状态值计算密集奖励，采用GAE计算优势函数，并引入样本平衡系数处理成功与失败样本的不平衡。奖励函数定义为R_t = γV(s_{t+1},l) - V(s_t,l)，优势函数结合任务成功/失败指示器。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：CALVIN基准，包含34个任务和四个不同的模拟环境。</p>\n<p>训练资源：价值模型训练使用批大小4×8和学习率1e-5；RL微调在4块NVIDIA A40 GPU上进行，Seer-Large模型约需10小时，GR-MG模型约需14小时，覆盖约1000个回合。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：CALVIN基准的四个模拟环境。</p>\n<p>评估指标：</p>\n<ul><li>平均成功长度：平均连续完成的任务数，越高越好。</li><li>任务完成率：在1000个序列中完成1至5个任务的比例，从L1到L5分别表示完成n个任务的比例。</li><li>价值模型准确率：评估价值模型预测状态值单调性的准确率，超过94%后选择第一轮次模型。</li></ul>"
  },
  {
    "date": "2025-05-26",
    "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving",
    "link": "http://arxiv.org/abs/2505.19381",
    "summary_markdown": "论文研究单位\nRIX, Bosch; AIR, Tsinghua University; Shanghai University; Shanghai Jiao Tong University; Southeast University\n\n论文概述\n该论文提出了一个名为DiffVLA的端到端自动驾驶框架，旨在解决现有方法中存在的BEV计算昂贵、动作多样性不足以及在复杂场景下决策次优等问题。DiffVLA结合了视觉语言模型（VLM）的引导、混合稀疏-密集的感知模块以及一个基于扩散的规划模块。该方法通过探索稀疏扩散表示来高效处理多模态驾驶行为，并利用VLM、智能体和地图实例之间的深度交互来优化轨迹生成。论文在Autonomous Grand Challenge 2025的NAVSIM v2竞赛中取得了优异的性能，其方法在私有测试集上达到了45.0的PDMS分数。\n\n论文核心贡献点\n1. 提出了一个VLM引导模块，该模块处理多视图图像和导航指令，生成高级驾驶命令和初始轨迹，为后续规划提供语义指导。\n2. 设计了一个混合感知模块，包含一个用于生成密集BEV特征表示的密集分支和一个用于提取3D目标、车道线等实例级信息的稀疏分支，从而同时利用场景的隐式和显式特征。\n3. 开发了一个基于扩散的规划模块，该模块使用多模态轨迹锚点作为先验，并采用缩短的扩散策略，通过分层信息编码策略融合来自感知模块和VLM的异构输入，以生成多模态的未来轨迹。\n\n论文方法描述\nDiffVLA框架主要由三个部分组成：\n1. **感知模块**：包含两个并行分支。稀疏感知分支采用采样策略进行3D目标检测和在线地图生成，输出3D边界框和地图矢量。密集感知分支则采用BEV特征投影方法生成密集的BEV特征图。两个分支的输出通过MLP编码后集成到规划模块中。\n2. **VLM模块**：基于Senna-VLM框架，该模块包括一个ViT-L/14视觉编码器、一个驾驶视觉适配器、一个文本编码器和一个Vicuna-v1.5-7B大语言模型。它处理多视图图像和外部导航指令，生成高层次的横向（如变道）和纵向（如加速）控制指令，这些指令被编码后用于引导扩散规划过程。\n3. **规划模块**：采用一个基于扩散的策略。首先，通过k-means聚类从训练数据中构建一组离散的轨迹词汇表，并为其添加高斯噪声生成轨迹锚点。然后，利用来自感知模块的显式（稀疏）和隐式（密集）特征作为条件，一个神经网络`f_θ`逐步对带噪声的轨迹进行去噪，最终预测出多条候选轨迹及其对应的分类得分。该模块使用轨迹分类和回归的联合损失进行训练。\n\n论文使用数据集和训练资源\n* **数据集**：在NAVSIM v2数据集上进行训练和评估。该数据集通过引入反应性的背景交通参与者和真实的合成多视角相机图像，为模型的鲁棒性和泛化能力提供了闭环评估。\n* **训练资源**：模型采用两阶段训练策略。第一阶段分别训练VLM引导模型和稀疏感知模块。第二阶段联合训练密集感知模块和规划头，同时冻结第一阶段模型的权重。训练使用AdamW优化器和余弦学习率衰减策略。VLM模块训练1个epoch，批量大小为192，学习率为2e-5。感知和规划模块训练100个epoch，批量大小为256，学习率为1e-4。所有感知分支均使用VoV-99作为骨干网络。\n\n论文使用的评估环境和评估指标\n* **评估环境**：在Autonomous Grand Challenge 2025竞赛的私有测试集上进行评估，该竞赛使用NAVSIM v2作为基准，提供了一个包含挑战性真实和反应性合成场景的闭环评估环境。\n* **评估指标**：主要评估指标是`extended_pdm_score_combined` (EPDMS)，论文方法在该指标上取得了45.007的成绩。此外，评估还包括一系列分阶段的子指标，主要分为两个阶段（stage_one和stage_two），涵盖安全性、合规性、效率和舒适性等方面。具体指标包括：\n * **安全与合规性**：`no_at_fault_collisions`（无过错碰撞率）、`drivable_area_compliance`（可行驶区域合规率）、`driving_direction_compliance`（行驶方向合规率）、`traffic_light_compliance`（红绿灯合规率）、`time_to_collision_within_bound`（碰撞时间在合理范围内比率）、`lane_keeping`（车道保持率）。\n * **效率**：`ego_progress`（自车进度）。\n * **舒适性**：`history_comfort`（历史舒适性）、`two_frame_extended_comfort`（两帧扩展舒适性）。",
    "summary_html": "<p>论文研究单位</p>\n<p>RIX, Bosch; AIR, Tsinghua University; Shanghai University; Shanghai Jiao Tong University; Southeast University</p>\n\n<p>论文概述</p>\n<p>该论文提出了一个名为DiffVLA的端到端自动驾驶框架，旨在解决现有方法中存在的BEV计算昂贵、动作多样性不足以及在复杂场景下决策次优等问题。DiffVLA结合了视觉语言模型（VLM）的引导、混合稀疏-密集的感知模块以及一个基于扩散的规划模块。该方法通过探索稀疏扩散表示来高效处理多模态驾驶行为，并利用VLM、智能体和地图实例之间的深度交互来优化轨迹生成。论文在Autonomous Grand Challenge 2025的NAVSIM v2竞赛中取得了优异的性能，其方法在私有测试集上达到了45.0的PDMS分数。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一个VLM引导模块，该模块处理多视图图像和导航指令，生成高级驾驶命令和初始轨迹，为后续规划提供语义指导。</li><li>设计了一个混合感知模块，包含一个用于生成密集BEV特征表示的密集分支和一个用于提取3D目标、车道线等实例级信息的稀疏分支，从而同时利用场景的隐式和显式特征。</li><li>开发了一个基于扩散的规划模块，该模块使用多模态轨迹锚点作为先验，并采用缩短的扩散策略，通过分层信息编码策略融合来自感知模块和VLM的异构输入，以生成多模态的未来轨迹。</li></ol>\n\n<p>论文方法描述</p>\n<p>DiffVLA框架主要由三个部分组成：</p>\n<ol><li><strong>感知模块</strong>：包含两个并行分支。稀疏感知分支采用采样策略进行3D目标检测和在线地图生成，输出3D边界框和地图矢量。密集感知分支则采用BEV特征投影方法生成密集的BEV特征图。两个分支的输出通过MLP编码后集成到规划模块中。</li><li><strong>VLM模块</strong>：基于Senna-VLM框架，该模块包括一个ViT-L/14视觉编码器、一个驾驶视觉适配器、一个文本编码器和一个Vicuna-v1.5-7B大语言模型。它处理多视图图像和外部导航指令，生成高层次的横向（如变道）和纵向（如加速）控制指令，这些指令被编码后用于引导扩散规划过程。</li><li><strong>规划模块</strong>：采用一个基于扩散的策略。首先，通过k-means聚类从训练数据中构建一组离散的轨迹词汇表，并为其添加高斯噪声生成轨迹锚点。然后，利用来自感知模块的显式（稀疏）和隐式（密集）特征作为条件，一个神经网络<code>f_θ</code>逐步对带噪声的轨迹进行去噪，最终预测出多条候选轨迹及其对应的分类得分。该模块使用轨迹分类和回归的联合损失进行训练。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>* <strong>数据集</strong>：在NAVSIM v2数据集上进行训练和评估。该数据集通过引入反应性的背景交通参与者和真实的合成多视角相机图像，为模型的鲁棒性和泛化能力提供了闭环评估。</p>\n<p>* <strong>训练资源</strong>：模型采用两阶段训练策略。第一阶段分别训练VLM引导模型和稀疏感知模块。第二阶段联合训练密集感知模块和规划头，同时冻结第一阶段模型的权重。训练使用AdamW优化器和余弦学习率衰减策略。VLM模块训练1个epoch，批量大小为192，学习率为2e-5。感知和规划模块训练100个epoch，批量大小为256，学习率为1e-4。所有感知分支均使用VoV-99作为骨干网络。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>* <strong>评估环境</strong>：在Autonomous Grand Challenge 2025竞赛的私有测试集上进行评估，该竞赛使用NAVSIM v2作为基准，提供了一个包含挑战性真实和反应性合成场景的闭环评估环境。</p>\n<p>* <strong>评估指标</strong>：主要评估指标是<code>extended_pdm_score_combined</code> (EPDMS)，论文方法在该指标上取得了45.007的成绩。此外，评估还包括一系列分阶段的子指标，主要分为两个阶段（stage_one和stage_two），涵盖安全性、合规性、效率和舒适性等方面。具体指标包括：</p>\n<p> * <strong>安全与合规性</strong>：<code>no_at_fault_collisions</code>（无过错碰撞率）、<code>drivable_area_compliance</code>（可行驶区域合规率）、<code>driving_direction_compliance</code>（行驶方向合规率）、<code>traffic_light_compliance</code>（红绿灯合规率）、<code>time_to_collision_within_bound</code>（碰撞时间在合理范围内比率）、<code>lane_keeping</code>（车道保持率）。</p>\n<p> * <strong>效率</strong>：<code>ego_progress</code>（自车进度）。</p>\n<p> * <strong>舒适性</strong>：<code>history_comfort</code>（历史舒适性）、<code>two_frame_extended_comfort</code>（两帧扩展舒适性）。</p>"
  },
  {
    "date": "2025-05-25",
    "title": "ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning",
    "link": "http://arxiv.org/abs/2505.19080",
    "summary_markdown": "```markdown\n### 论文研究单位\nVinRobotics (越南), Max Planck Research School for Intelligent Systems (德国), University of Texas at Arlington (美国), Automation & Control Institute, TU Wien (奥地利), Austrian Institute of Technology (AIT) (奥地利)\n### 论文概述\n本文提出 ReFineVLA，一个通过教师引导的微调框架，为视觉-语言-动作（VLA）模型注入显式多模态推理能力。传统VLA模型学习观察-动作的直接映射，缺乏推理步骤，限制了在复杂任务中的泛化和可解释性。ReFineVLA利用专家教师模型（如Gemini）为机器人轨迹生成结构化的推理标注，通过选择性微调上层参数，联合优化动作预测和推理生成，提升模型对任务逻辑的理解和决策透明度。\n### 论文核心贡献点\n1. 提出首个将显式多模态推理注入VLA模型的微调框架ReFineVLA，通过教师生成的自然语言推理链指导策略学习。\n2. 构建包含125,000个推理增强轨迹的数据集，覆盖观察分析、空间推理、任务规划等结构化推理步骤。\n3. 设计选择性微调策略，仅更新上层参数以保留预训练泛化能力，同时降低计算开销。\n4. 通过注意力图可视化验证，显示模型在推理注入后更关注语义相关物体而非局部动作线索。\n5. 在SimplerEnv基准上超越现有SOTA：WidowX任务平均成功率提升5.0%，变体聚合设置提升8.6%，视觉匹配任务提升1.7%。\n### 论文方法描述\n1. **推理标注生成**：使用教师模型（Gemini）为每个观察-动作对生成多模态推理标注，包含四部分结构：观察（物体识别）、情境分析（上下文理解）、空间推理（物体关系）、任务规划（动作步骤）。\n2. **选择性微调**：冻结VLA模型底层参数（如视觉编码器低层），仅微调上层Transformer块和策略头，避免破坏预训练特征。\n3. **多目标联合训练**：损失函数为动作预测损失（行为克隆）与推理生成损失（语言建模）的加权和，公式为：\n \\[\n \\mathcal{L}_{\\text{ReFineVLA}} = \\mathcal{L}_{\\text{action}} + \\lambda_{\\text{r}} \\mathcal{L}_{\\text{reasoning}}\n \\]\n 其中 \\(\\mathcal{L}_{\\text{action}} = -\\sum_t \\log \\mathbb{P}(a_{i,t} \\mid o_i, a_{i,<t}; \\theta)\\)，\\(\\mathcal{L}_{\\text{reasoning}}\\) 为推理文本的负对数似然。\n### 论文使用数据集和训练资源\n- **数据集**：自建125,000条机器人操作轨迹，每条包含观察（图像+指令）、专家动作及教师生成的推理标注。\n- **基础模型**：2B参数的预训练VLA模型（具体架构未说明）。\n- **训练资源**：未明确提及硬件配置，但强调选择性微调可减少计算负担。\n### 论文使用的评估环境和评估指标\n- **环境**：SimplerEnv模拟器，测试两种机器人平台：\n - WidowX Robot（7自由度机械臂）\n - Google Robot（移动操作平台）\n- **评估指标**：\n - **任务成功率**（Success Rate）：任务完成的成功比例，核心指标。\n - **注意力可视化**：通过分析动作token的注意力热图，验证模型对相关物体的关注程度。\n - **变体设置**：包括环境变化（如物体位置扰动）下的鲁棒性测试。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>VinRobotics (越南), Max Planck Research School for Intelligent Systems (德国), University of Texas at Arlington (美国), Automation & Control Institute, TU Wien (奥地利), Austrian Institute of Technology (AIT) (奥地利)</p>\n<h3>论文概述</h3>\n<p>本文提出 ReFineVLA，一个通过教师引导的微调框架，为视觉-语言-动作（VLA）模型注入显式多模态推理能力。传统VLA模型学习观察-动作的直接映射，缺乏推理步骤，限制了在复杂任务中的泛化和可解释性。ReFineVLA利用专家教师模型（如Gemini）为机器人轨迹生成结构化的推理标注，通过选择性微调上层参数，联合优化动作预测和推理生成，提升模型对任务逻辑的理解和决策透明度。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出首个将显式多模态推理注入VLA模型的微调框架ReFineVLA，通过教师生成的自然语言推理链指导策略学习。</li><li>构建包含125,000个推理增强轨迹的数据集，覆盖观察分析、空间推理、任务规划等结构化推理步骤。</li><li>设计选择性微调策略，仅更新上层参数以保留预训练泛化能力，同时降低计算开销。</li><li>通过注意力图可视化验证，显示模型在推理注入后更关注语义相关物体而非局部动作线索。</li><li>在SimplerEnv基准上超越现有SOTA：WidowX任务平均成功率提升5.0%，变体聚合设置提升8.6%，视觉匹配任务提升1.7%。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>推理标注生成</strong>：使用教师模型（Gemini）为每个观察-动作对生成多模态推理标注，包含四部分结构：观察（物体识别）、情境分析（上下文理解）、空间推理（物体关系）、任务规划（动作步骤）。</li><li><strong>选择性微调</strong>：冻结VLA模型底层参数（如视觉编码器低层），仅微调上层Transformer块和策略头，避免破坏预训练特征。</li><li><strong>多目标联合训练</strong>：损失函数为动作预测损失（行为克隆）与推理生成损失（语言建模）的加权和，公式为：</li></ol>\n<p> \\[</p>\n<p> \\mathcal{L}_{\\text{ReFineVLA}} = \\mathcal{L}_{\\text{action}} + \\lambda_{\\text{r}} \\mathcal{L}_{\\text{reasoning}}</p>\n<p> \\]</p>\n<p> 其中 \\(\\mathcal{L}_{\\text{action}} = -\\sum_t \\log \\mathbb{P}(a_{i,t} \\mid o_i, a_{i,<t}; \\theta)\\)，\\(\\mathcal{L}_{\\text{reasoning}}\\) 为推理文本的负对数似然。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：自建125,000条机器人操作轨迹，每条包含观察（图像+指令）、专家动作及教师生成的推理标注。</li><li><strong>基础模型</strong>：2B参数的预训练VLA模型（具体架构未说明）。</li><li><strong>训练资源</strong>：未明确提及硬件配置，但强调选择性微调可减少计算负担。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>环境</strong>：SimplerEnv模拟器，测试两种机器人平台：</li></ul>\n<p> - WidowX Robot（7自由度机械臂）</p>\n<p> - Google Robot（移动操作平台）</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率</strong>（Success Rate）：任务完成的成功比例，核心指标。</p>\n<p> - <strong>注意力可视化</strong>：通过分析动作token的注意力热图，验证模型对相关物体的关注程度。</p>\n<p> - <strong>变体设置</strong>：包括环境变化（如物体位置扰动）下的鲁棒性测试。</p>\n<p>```</p>"
  },
  {
    "date": "2025-05-24",
    "title": "Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance",
    "link": "http://arxiv.org/abs/2505.18793",
    "summary_markdown": "论文研究单位\nAgiBot\n\n论文概述\n该论文提出了一种名为Genie Centurion (GCENT)的可扩展机器人策略训练数据收集范式，旨在解决传统遥操作数据收集成本高、效率低、扩展性差的问题。当机器人执行失败时，GCENT通过一个回退机制将系统恢复到之前的某个状态，然后由人类遥操作员提供纠正性演示来优化策略。该方法结合一个名为Task Sentinel的多模态大模型模块，该模块能自主预测任务步骤的完成情况并在必要时请求人类干预，从而实现一人对多机器人的高效监督。实验结果表明，GCENT在四个真实世界任务上，相比最先进的数据收集方法，任务成功率提升了高达40%，且仅使用不到一半的数据量即可达到相当的性能，证明了其在提升数据效率和降低人工成本方面的潜力。\n\n论文核心贡献点\n1. 提出了Genie Centurion (GCENT)框架，一个通过故障触发的纠正式干预进行高效机器人策略训练的统一系统，并辅以回退机制以增强状态空间的覆盖。\n2. 进行了广泛的真实世界实验，将GCENT与标准的被动和对抗性数据收集方法进行比较，证明了其在任务成功率和减少人类操作工作量上的显著优势。\n3. 提出了Task Sentinel模块，一个基于多模态大语言模型的机器人步骤检测模型，通过在策略自主执行过程中预测任务完成度来选择性地请求人类干预，实现了可扩展的单人多机器人监督。\n\n论文方法描述\nGCENT系统构建于AgiBot G01机器人平台和VR遥操作系统之上。其核心是一个持续迭代的数据闭环，包含三个主要阶段：\n1. **初始化**：通过少量人类遥操作的种子数据训练一个初始策略。\n2. **部署**：机器人使用当前策略自主执行任务。该阶段包含四个关键步骤：\n * **推理与监控**：策略模型执行动作，同时Task Sentinel模型监控任务状态。若任务步骤在预设时间内未完成，系统进入“等待干预”状态。\n * **回退与纠正**：当需要干预时，操作员可触发回退机制，将机器人状态恢复到过去3秒缓冲区中的一个历史点，然后手动提供纠正性演示。\n * **数据聚合**：任务完成后，系统自动记录并上传所有轨迹数据，特别是标记为“干预”模式的成功纠错轨迹，这些被视为高质量的监督样本。\n3. **再训练**：使用聚合后的新数据集微调策略模型和Task Sentinel模型，然后将更新后的模型重新部署，开始新一轮的迭代。\n\nTask Sentinel模型基于InternVL 2.5 2B，它接收当前图像和任务指令作为输入，输出一个二进制信号来判断当前任务步骤是否成功完成，从而决定是继续执行还是请求人工干预。\n\n论文使用数据集和训练资源\n* **数据集**：论文不使用公开数据集，而是通过GCENT框架在真实世界中自行收集。实验涉及四个任务：三明治组装、连接器插入、微波加热和打字。初始阶段通过20条轨迹进行预热，后续通过GCENT迭代收集数据，每次迭代包含20条演示。\n* **训练资源**：策略模型基于预训练的GO-1模型进行微调。每次迭代使用1个A800计算节点进行100个epoch的训练，耗时约16小时。Task Sentinel模型基于InternVL 2.5 2B模型，并添加了一个MLP分类头进行微调。\n\n论文使用的评估环境和评估指标\n* **评估环境**：所有数据收集和模型评估均在真实世界的AgiBot G01实体机器人平台上进行。\n* **评估指标**：\n * **任务平均得分**：一个0到1之间的连续值，1.0表示任务完全成功，部分完成的任务获得相应比例的分数，用于更精细地评估性能。\n * **干预率**：在数据收集过程中，需要人类干预的帧数占总帧数的百分比，用于衡量策略的自主性和人工监督成本。\n * **收集效率**：在单人多机器人场景下，通过（有效帧数 / 人工操作时间）计算的比率，用于量化数据产出与人类努力的比值，理想最大值为2.0（单人同时操作两台机器人）。",
    "summary_html": "<p>论文研究单位</p>\n<p>AgiBot</p>\n\n<p>论文概述</p>\n<p>该论文提出了一种名为Genie Centurion (GCENT)的可扩展机器人策略训练数据收集范式，旨在解决传统遥操作数据收集成本高、效率低、扩展性差的问题。当机器人执行失败时，GCENT通过一个回退机制将系统恢复到之前的某个状态，然后由人类遥操作员提供纠正性演示来优化策略。该方法结合一个名为Task Sentinel的多模态大模型模块，该模块能自主预测任务步骤的完成情况并在必要时请求人类干预，从而实现一人对多机器人的高效监督。实验结果表明，GCENT在四个真实世界任务上，相比最先进的数据收集方法，任务成功率提升了高达40%，且仅使用不到一半的数据量即可达到相当的性能，证明了其在提升数据效率和降低人工成本方面的潜力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了Genie Centurion (GCENT)框架，一个通过故障触发的纠正式干预进行高效机器人策略训练的统一系统，并辅以回退机制以增强状态空间的覆盖。</li><li>进行了广泛的真实世界实验，将GCENT与标准的被动和对抗性数据收集方法进行比较，证明了其在任务成功率和减少人类操作工作量上的显著优势。</li><li>提出了Task Sentinel模块，一个基于多模态大语言模型的机器人步骤检测模型，通过在策略自主执行过程中预测任务完成度来选择性地请求人类干预，实现了可扩展的单人多机器人监督。</li></ol>\n\n<p>论文方法描述</p>\n<p>GCENT系统构建于AgiBot G01机器人平台和VR遥操作系统之上。其核心是一个持续迭代的数据闭环，包含三个主要阶段：</p>\n<ol><li><strong>初始化</strong>：通过少量人类遥操作的种子数据训练一个初始策略。</li><li><strong>部署</strong>：机器人使用当前策略自主执行任务。该阶段包含四个关键步骤：</li></ol>\n<p> * <strong>推理与监控</strong>：策略模型执行动作，同时Task Sentinel模型监控任务状态。若任务步骤在预设时间内未完成，系统进入“等待干预”状态。</p>\n<p> * <strong>回退与纠正</strong>：当需要干预时，操作员可触发回退机制，将机器人状态恢复到过去3秒缓冲区中的一个历史点，然后手动提供纠正性演示。</p>\n<p> * <strong>数据聚合</strong>：任务完成后，系统自动记录并上传所有轨迹数据，特别是标记为“干预”模式的成功纠错轨迹，这些被视为高质量的监督样本。</p>\n<p>3. <strong>再训练</strong>：使用聚合后的新数据集微调策略模型和Task Sentinel模型，然后将更新后的模型重新部署，开始新一轮的迭代。</p>\n\n<p>Task Sentinel模型基于InternVL 2.5 2B，它接收当前图像和任务指令作为输入，输出一个二进制信号来判断当前任务步骤是否成功完成，从而决定是继续执行还是请求人工干预。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>* <strong>数据集</strong>：论文不使用公开数据集，而是通过GCENT框架在真实世界中自行收集。实验涉及四个任务：三明治组装、连接器插入、微波加热和打字。初始阶段通过20条轨迹进行预热，后续通过GCENT迭代收集数据，每次迭代包含20条演示。</p>\n<p>* <strong>训练资源</strong>：策略模型基于预训练的GO-1模型进行微调。每次迭代使用1个A800计算节点进行100个epoch的训练，耗时约16小时。Task Sentinel模型基于InternVL 2.5 2B模型，并添加了一个MLP分类头进行微调。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>* <strong>评估环境</strong>：所有数据收集和模型评估均在真实世界的AgiBot G01实体机器人平台上进行。</p>\n<p>* <strong>评估指标</strong>：</p>\n<p> * <strong>任务平均得分</strong>：一个0到1之间的连续值，1.0表示任务完全成功，部分完成的任务获得相应比例的分数，用于更精细地评估性能。</p>\n<p> * <strong>干预率</strong>：在数据收集过程中，需要人类干预的帧数占总帧数的百分比，用于衡量策略的自主性和人工监督成本。</p>\n<p> * <strong>收集效率</strong>：在单人多机器人场景下，通过（有效帧数 / 人工操作时间）计算的比率，用于量化数据产出与人类努力的比值，理想最大值为2.0（单人同时操作两台机器人）。</p>"
  },
  {
    "date": "2025-05-24",
    "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
    "link": "http://arxiv.org/abs/2505.18719",
    "summary_markdown": "论文研究单位\n- 清华大学深圳国际研究生院\n- 南洋理工大学\n\n论文概述\n- 论文提出了 VLA-RL，一个利用在线强化学习（RL）来改进预训练自回归视觉-语言-动作（VLA）模型的算法和系统框架。\n- 传统的VLA模型依赖模仿学习，在处理分布外（OOD）场景时会失败，因为其无法探索有限的离线数据中未覆盖的状态。\n- VLA-RL旨在通过在线探索和策略优化来解决这一局限性，从而提升模型在下游任务中的泛化能力和性能。\n- 实验表明，该方法能使OpenVLA-7B模型在LIBERO的40项挑战性任务上，性能超越最强基线4.5%，并能匹配先进商业模型（如π0-FAST）的性能。\n\n论文核心贡献点\n- 提出了VLA-RL，一个系统性的框架，首次将在线强化学习有效地应用于大规模预训练VLA模型的下游任务优化。\n- 将通用机器人操作轨迹形式化为多模态、多轮对话，从而可以在统一的视角下对自回归VLA进行轨迹级的RL训练。\n- 为了解决机器人动作空间中奖励稀疏的问题，引入了机器人过程奖励模型（RPRM）。该模型基于自动提取的任务片段上的伪奖励标签进行微调，用于提供密集的奖励信号。\n- 识别并验证了多项关键的系统性实现技术，包括课程选择策略、GPU均衡的向量化环境、批量解码和评论家预热，这些技术显著提升了训练的稳定性和效率。\n\n论文方法描述\n- **整体流程**：系统采用Actor-Critic框架，包含一个待训练的策略模型、一个同构的价值模型和一个冻结的机器人过程奖励模型（RPRM）。\n- **RL问题形式化**：将机器人操作任务定义为马尔可夫决策过程，其中状态是图像观测和文本指令的组合，动作是VLA模型生成的动作词元序列。这使得可以用标准RL算法进行优化。\n- **奖励密集化**：使用机器人过程奖励模型（RPRM）为环境提供的稀疏奖励补充密集的奖励信号。总奖励为两者之和。\n- **训练算法**：采用类似PPO的算法进行优化。首先在多个并行环境中进行数据收集（rollout），然后利用广义优势估计（GAE）计算优势函数，最后使用收集的数据和优势函数通过PPO更新策略和价值模型的参数。\n- **系统性实现**：通过课程学习选择任务、利用GPU均衡的向量化环境、批量解码以及对评论家模型进行预热等工程技巧，确保了训练过程的可扩展性和稳定性。\n\n论文使用数据集和训练资源\n- **数据集**：实验在LIBERO数据集上进行，该数据集包含40项具有挑战性的机器人操作任务。\n- **训练资源**：训练利用了多个GPU，并采用GPU均衡的向量化环境进行并行化，但原文未提供具体的GPU数量和训练时长等资源细节。\n\n论文使用的评估环境和评估指标\n- **评估环境**：评估在LIBERO模拟器环境中进行。\n- **评估指标**：主要评估指标是任务的成功率。论文通过比较不同方法在LIBERO任务集上的平均成功率来衡量性能，并与最强的监督微调基线以及先进的商业模型进行对比。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>清华大学深圳国际研究生院</li><li>南洋理工大学</li></ul>\n\n<p>论文概述</p>\n<ul><li>论文提出了 VLA-RL，一个利用在线强化学习（RL）来改进预训练自回归视觉-语言-动作（VLA）模型的算法和系统框架。</li><li>传统的VLA模型依赖模仿学习，在处理分布外（OOD）场景时会失败，因为其无法探索有限的离线数据中未覆盖的状态。</li><li>VLA-RL旨在通过在线探索和策略优化来解决这一局限性，从而提升模型在下游任务中的泛化能力和性能。</li><li>实验表明，该方法能使OpenVLA-7B模型在LIBERO的40项挑战性任务上，性能超越最强基线4.5%，并能匹配先进商业模型（如π0-FAST）的性能。</li></ul>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了VLA-RL，一个系统性的框架，首次将在线强化学习有效地应用于大规模预训练VLA模型的下游任务优化。</li><li>将通用机器人操作轨迹形式化为多模态、多轮对话，从而可以在统一的视角下对自回归VLA进行轨迹级的RL训练。</li><li>为了解决机器人动作空间中奖励稀疏的问题，引入了机器人过程奖励模型（RPRM）。该模型基于自动提取的任务片段上的伪奖励标签进行微调，用于提供密集的奖励信号。</li><li>识别并验证了多项关键的系统性实现技术，包括课程选择策略、GPU均衡的向量化环境、批量解码和评论家预热，这些技术显著提升了训练的稳定性和效率。</li></ul>\n\n<p>论文方法描述</p>\n<ul><li><strong>整体流程</strong>：系统采用Actor-Critic框架，包含一个待训练的策略模型、一个同构的价值模型和一个冻结的机器人过程奖励模型（RPRM）。</li><li><strong>RL问题形式化</strong>：将机器人操作任务定义为马尔可夫决策过程，其中状态是图像观测和文本指令的组合，动作是VLA模型生成的动作词元序列。这使得可以用标准RL算法进行优化。</li><li><strong>奖励密集化</strong>：使用机器人过程奖励模型（RPRM）为环境提供的稀疏奖励补充密集的奖励信号。总奖励为两者之和。</li><li><strong>训练算法</strong>：采用类似PPO的算法进行优化。首先在多个并行环境中进行数据收集（rollout），然后利用广义优势估计（GAE）计算优势函数，最后使用收集的数据和优势函数通过PPO更新策略和价值模型的参数。</li><li><strong>系统性实现</strong>：通过课程学习选择任务、利用GPU均衡的向量化环境、批量解码以及对评论家模型进行预热等工程技巧，确保了训练过程的可扩展性和稳定性。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>数据集</strong>：实验在LIBERO数据集上进行，该数据集包含40项具有挑战性的机器人操作任务。</li><li><strong>训练资源</strong>：训练利用了多个GPU，并采用GPU均衡的向量化环境进行并行化，但原文未提供具体的GPU数量和训练时长等资源细节。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：评估在LIBERO模拟器环境中进行。</li><li><strong>评估指标</strong>：主要评估指标是任务的成功率。论文通过比较不同方法在LIBERO任务集上的平均成功率来衡量性能，并与最强的监督微调基线以及先进的商业模型进行对比。</li></ul>"
  },
  {
    "date": "2025-05-22",
    "title": "ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems",
    "link": "http://arxiv.org/abs/2505.17295",
    "summary_markdown": "论文研究单位\nUniversity of Connecticut\n\n论文概述\n本文介绍了ScanBot，一个专为指令驱动、高精度表面扫描设计的多模态数据集。与关注抓取或导航等粗粒度任务的现有数据集不同，ScanBot专注于工业激光扫描的亚毫米级精度和连续路径等严格要求。该数据集包含机器人在12个不同物体上执行的激光扫描轨迹，涵盖6种由自然语言指令指导的任务类型，并同步了RGB、深度、激光轮廓、机器人位姿和关节状态等多模态数据，旨在推动能够进行精细运动控制和参数调整的智能扫描策略研究。\n\n论文核心贡献点\n1. 提出了首个面向工业级高精度表面扫描的指令驱动多模态数据集ScanBot。\n2. 定义了六种覆盖全表面扫描、几何特征定位、缺陷检测等不同层次的扫描任务，为智能感知规划提供了评估基准。\n3. 通过对现有先进的视觉-语言模型进行基准测试，揭示了它们在处理需要精细运动控制、参数调整和指令理解的连续扫描任务时存在的显著局限性。\n\n论文方法描述\n研究方法包括构建一个特定的硬件平台：UR3机器人搭载Keyence LJ-X8200激光轮廓仪、Intel RealSense D435i相机和GoPro相机。数据集包含12个物体，分为6个真实电子元件和6个具有明确几何特征的3D打印模型。数据收集过程通过迭代校准，针对不同物体的材质和反射特性，手动调整扫描仪的曝光、功率、量程等参数，并同步机器人的运动速度与扫描频率，以确保采集到高质量的多模态扫描数据。\n\n论文使用数据集和训练资源\n使用的数据集是作者自建的ScanBot数据集，包含12个物体上的896条扫描路径，涵盖6种任务类型。数据包含同步的RGB-D图像、激光轮廓、机器人位姿与关节状态以及扫描仪参数配置。论文未训练新模型，而是直接利用了预训练的多模态大语言模型进行评估，包括GPT-4.1、OpenAI o3、Gemini 2.5 Pro和Gemini 2.5 Flash。\n\n论文使用的评估环境和评估指标\n评估环境旨在测试模型的“感知-规划-执行”全流程能力，包含四个实验：1) 扫描仪参数选择，评估指标为参数预测的准确率。2) 指令区域定位，评估指标为预测边界框与人工标注的平均交并比。3) 扫描路径生成与三维重建，评估指标为预测路径起点/终点的欧几里得偏差，以及生成点云与真实点云之间的倒角距离。4) 整体重构质量，通过倒角距离间接反映。论文在ScanBot数据集上对GPT-4.1、OpenAI o3、Gemini 2.5 Pro和Gemini 2.5 Flash等模型进行了基准测试。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Connecticut</p>\n\n<p>论文概述</p>\n<p>本文介绍了ScanBot，一个专为指令驱动、高精度表面扫描设计的多模态数据集。与关注抓取或导航等粗粒度任务的现有数据集不同，ScanBot专注于工业激光扫描的亚毫米级精度和连续路径等严格要求。该数据集包含机器人在12个不同物体上执行的激光扫描轨迹，涵盖6种由自然语言指令指导的任务类型，并同步了RGB、深度、激光轮廓、机器人位姿和关节状态等多模态数据，旨在推动能够进行精细运动控制和参数调整的智能扫描策略研究。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了首个面向工业级高精度表面扫描的指令驱动多模态数据集ScanBot。</li><li>定义了六种覆盖全表面扫描、几何特征定位、缺陷检测等不同层次的扫描任务，为智能感知规划提供了评估基准。</li><li>通过对现有先进的视觉-语言模型进行基准测试，揭示了它们在处理需要精细运动控制、参数调整和指令理解的连续扫描任务时存在的显著局限性。</li></ol>\n\n<p>论文方法描述</p>\n<p>研究方法包括构建一个特定的硬件平台：UR3机器人搭载Keyence LJ-X8200激光轮廓仪、Intel RealSense D435i相机和GoPro相机。数据集包含12个物体，分为6个真实电子元件和6个具有明确几何特征的3D打印模型。数据收集过程通过迭代校准，针对不同物体的材质和反射特性，手动调整扫描仪的曝光、功率、量程等参数，并同步机器人的运动速度与扫描频率，以确保采集到高质量的多模态扫描数据。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用的数据集是作者自建的ScanBot数据集，包含12个物体上的896条扫描路径，涵盖6种任务类型。数据包含同步的RGB-D图像、激光轮廓、机器人位姿与关节状态以及扫描仪参数配置。论文未训练新模型，而是直接利用了预训练的多模态大语言模型进行评估，包括GPT-4.1、OpenAI o3、Gemini 2.5 Pro和Gemini 2.5 Flash。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境旨在测试模型的“感知-规划-执行”全流程能力，包含四个实验：1) 扫描仪参数选择，评估指标为参数预测的准确率。2) 指令区域定位，评估指标为预测边界框与人工标注的平均交并比。3) 扫描路径生成与三维重建，评估指标为预测路径起点/终点的欧几里得偏差，以及生成点云与真实点云之间的倒角距离。4) 整体重构质量，通过倒角距离间接反映。论文在ScanBot数据集上对GPT-4.1、OpenAI o3、Gemini 2.5 Pro和Gemini 2.5 Flash等模型进行了基准测试。</p>"
  },
  {
    "date": "2025-05-22",
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2505.17016",
    "summary_markdown": "论文研究单位\nUT Austin¹, Nankai University²\n\n论文概述\n现有视觉-语言-动作（VLA）模型的训练流程严重依赖离线专家演示数据和监督式模仿学习，这限制了模型在低数据场景下适应新任务和环境的能力。为解决此问题，本文提出了RIPT-VLA，一个简单且可扩展的基于强化学习的交互式后训练范式。该方法在预训练和监督微调之后，允许VLA模型与环境进行交互，并仅使用稀疏的二进制成功/失败奖励进行微调。RIPT-VLA旨在通过最小化的监督来解锁模型潜在的预训练知识，从而在新任务上实现高效适应和卓越性能。\n\n论文核心贡献点\n1. 提出了RIPT-VLA，一个新颖的VLA训练第三阶段，即基于强化学习的交互式后训练，补足了传统的预训练和监督微调两个阶段。\n2. 引入了一个稳定且高效的策略优化算法：动态采样留一近端策略优化（DS-LOOP-PPO）。该方法扩展了LOOP框架，通过动态推演采样构建非零优势样本的均匀批次，过滤掉零优势轨迹，实现了无需价值函数或奖励塑形的稳定训练。\n3. 在效率和性能上取得显著突破：在LIBERO基准上，RIPT-VLA将轻量级QueST模型性能提升21.2%，并将7B的OpenVLA-OFT模型的成功率提升至前所未有的97.5%。在仅有一个演示的极端低数据场景下，它能将一个成功率仅4%的SFT模型在15次迭代内提升至97%。\n4. 展示了所学策略的强大泛化能力，证明了其在不同任务、场景和初始状态下的有效性及鲁棒性。\n\n论文方法描述\nRIPT-VLA采用三阶段训练范式：1）大规模预训练，2）任务特定监督微调（SFT），3）交互式后训练。在第三阶段，预训练后的VLA模型在多任务环境中与环境交互，生成动作序列并接收稀疏的二进制成功奖励。为了优化模型，论文提出了DS-LOOP-PPO算法。该算法结合了REINFORCE留一（RLOO）优势估计和近端策略优化（PPO）的优点。其核心创新在于动态采样：持续进行推演采样，直到收集到足够数量的非零优势样本，以此构建用于策略更新的均匀批次。这一机制确保了训练的稳定性，特别是在策略成功率较高时，并适用于不同动作类型的VLA模型（如token化或连续动作头）。\n\n论文使用数据集和训练资源\n数据集：LIBERO benchmark（包括LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-100）和Meta-World-45。\n训练资源：使用8个NVIDIA A100 GPU。对于监督微调（SFT）阶段，每个任务通常包含约50个人工演示。\n\n论文使用的评估环境和评估指标\n评估环境：模拟环境，具体为LIBERO和Meta-World的模拟器。\n评估指标：主要评估指标是任务成功率（Success Rate, SR），报告时也使用绝对成功率或平均成功率。",
    "summary_html": "<p>论文研究单位</p>\n<p>UT Austin¹, Nankai University²</p>\n\n<p>论文概述</p>\n<p>现有视觉-语言-动作（VLA）模型的训练流程严重依赖离线专家演示数据和监督式模仿学习，这限制了模型在低数据场景下适应新任务和环境的能力。为解决此问题，本文提出了RIPT-VLA，一个简单且可扩展的基于强化学习的交互式后训练范式。该方法在预训练和监督微调之后，允许VLA模型与环境进行交互，并仅使用稀疏的二进制成功/失败奖励进行微调。RIPT-VLA旨在通过最小化的监督来解锁模型潜在的预训练知识，从而在新任务上实现高效适应和卓越性能。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了RIPT-VLA，一个新颖的VLA训练第三阶段，即基于强化学习的交互式后训练，补足了传统的预训练和监督微调两个阶段。</li><li>引入了一个稳定且高效的策略优化算法：动态采样留一近端策略优化（DS-LOOP-PPO）。该方法扩展了LOOP框架，通过动态推演采样构建非零优势样本的均匀批次，过滤掉零优势轨迹，实现了无需价值函数或奖励塑形的稳定训练。</li><li>在效率和性能上取得显著突破：在LIBERO基准上，RIPT-VLA将轻量级QueST模型性能提升21.2%，并将7B的OpenVLA-OFT模型的成功率提升至前所未有的97.5%。在仅有一个演示的极端低数据场景下，它能将一个成功率仅4%的SFT模型在15次迭代内提升至97%。</li><li>展示了所学策略的强大泛化能力，证明了其在不同任务、场景和初始状态下的有效性及鲁棒性。</li></ol>\n\n<p>论文方法描述</p>\n<p>RIPT-VLA采用三阶段训练范式：1）大规模预训练，2）任务特定监督微调（SFT），3）交互式后训练。在第三阶段，预训练后的VLA模型在多任务环境中与环境交互，生成动作序列并接收稀疏的二进制成功奖励。为了优化模型，论文提出了DS-LOOP-PPO算法。该算法结合了REINFORCE留一（RLOO）优势估计和近端策略优化（PPO）的优点。其核心创新在于动态采样：持续进行推演采样，直到收集到足够数量的非零优势样本，以此构建用于策略更新的均匀批次。这一机制确保了训练的稳定性，特别是在策略成功率较高时，并适用于不同动作类型的VLA模型（如token化或连续动作头）。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：LIBERO benchmark（包括LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-100）和Meta-World-45。</p>\n<p>训练资源：使用8个NVIDIA A100 GPU。对于监督微调（SFT）阶段，每个任务通常包含约50个人工演示。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：模拟环境，具体为LIBERO和Meta-World的模拟器。</p>\n<p>评估指标：主要评估指标是任务成功率（Success Rate, SR），报告时也使用绝对成功率或平均成功率。</p>"
  },
  {
    "date": "2025-05-22",
    "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving",
    "link": "http://arxiv.org/abs/2505.16278",
    "summary_markdown": "论文研究单位\n上海交通大学计算机科学与人工智能学院\n\n论文概述\n本文提出了DriveMoE，一种新颖的基于混合专家模型的端到端自动驾驶框架。该框架基于Drive-π0视觉-语言-动作模型，针对自动驾驶中的多视图处理冗余和多样化驾驶行为处理不足的问题，引入了场景专业化视觉MoE和技能专业化动作MoE。DriveMoE通过动态选择相关相机视图和激活特定技能的专家模块，显著提升了在复杂和罕见驾驶场景中的性能，并在Bench2Drive闭环评估中取得了最先进的结果。\n\n论文核心贡献点\n1. 将VLA基础模型π0从具身智能领域扩展到自动驾驶领域，构建了Drive-π0统一框架。\n2. 提出了首个将混合专家模型集成到感知和决策中的框架DriveMoE。\n3. 设计了场景专业化视觉MoE用于动态相机视图选择和技能专业化动作MoE用于特定行为规划。\n4. 在Bench2Drive闭环仿真基准上实现了最先进的性能，显著提高了对罕见驾驶行为的鲁棒性。\n\n论文方法描述\nDriveMoE基于Drive-π0基线模型，该模型使用预训练的Paligemma VLM作为骨干网络和基于流匹配的动作模块进行未来轨迹生成。针对多视图处理效率低和行为模式平均化问题，引入两个MoE模块：\n1. 场景专业化视觉MoE：训练一个轻量级路由器，根据驾驶上下文和未来目标点动态选择相关相机视图（如前视和少数侧/后视），减少冗余计算，模拟人类驾驶员的注意力机制。\n2. 技能专业化动作MoE：将流匹配解码器中的前馈网络替换为多个专家模块，每个专家专门处理特定驾驶技能（如车道跟随、变道、紧急制动）。路由器根据输入状态激活Top-1或Top-2专家，通过交叉熵损失和负载均衡正则化进行训练。\n训练采用两阶段策略：第一阶段使用真实专家标注稳定路由器训练；第二阶段转为自适应路由，增强模型泛化能力。\n\n论文使用数据集和训练资源\n数据集：CARLA模拟器（版本0.9.15.1）和Bench2Drive基准。使用Bench2Drive的基础训练集（1000个片段，950训练，50验证/测试），并在220条官方测试路线上评估。\n训练资源：基于预训练的Paligemma VLM权重进行微调，具体硬件未在提供片段中详细说明。\n\n论文使用的评估环境和评估指标\n评估环境：CARLA模拟器（版本0.9.15.1）中的Bench2Drive闭环基准，包含220条测试路线，每条路线包含一个挑战性案例。\n评估指标：\n- 驾驶评分（DS）：路线完成率与违规分数的乘积，综合评估任务完成和规则遵守。\n- 成功率（SR）：在规定时间内无违规成功完成路线的百分比。\n- 效率：车辆相对于周围交通的速度，衡量驾驶的进取性。\n- 舒适性：驾驶轨迹的平滑度。\n- 任务特定指标：针对并线、超车、紧急制动、让行和交通标志识别等具体驾驶技能进行评估。",
    "summary_html": "<p>论文研究单位</p>\n<p>上海交通大学计算机科学与人工智能学院</p>\n\n<p>论文概述</p>\n<p>本文提出了DriveMoE，一种新颖的基于混合专家模型的端到端自动驾驶框架。该框架基于Drive-π0视觉-语言-动作模型，针对自动驾驶中的多视图处理冗余和多样化驾驶行为处理不足的问题，引入了场景专业化视觉MoE和技能专业化动作MoE。DriveMoE通过动态选择相关相机视图和激活特定技能的专家模块，显著提升了在复杂和罕见驾驶场景中的性能，并在Bench2Drive闭环评估中取得了最先进的结果。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>将VLA基础模型π0从具身智能领域扩展到自动驾驶领域，构建了Drive-π0统一框架。</li><li>提出了首个将混合专家模型集成到感知和决策中的框架DriveMoE。</li><li>设计了场景专业化视觉MoE用于动态相机视图选择和技能专业化动作MoE用于特定行为规划。</li><li>在Bench2Drive闭环仿真基准上实现了最先进的性能，显著提高了对罕见驾驶行为的鲁棒性。</li></ol>\n\n<p>论文方法描述</p>\n<p>DriveMoE基于Drive-π0基线模型，该模型使用预训练的Paligemma VLM作为骨干网络和基于流匹配的动作模块进行未来轨迹生成。针对多视图处理效率低和行为模式平均化问题，引入两个MoE模块：</p>\n<ol><li>场景专业化视觉MoE：训练一个轻量级路由器，根据驾驶上下文和未来目标点动态选择相关相机视图（如前视和少数侧/后视），减少冗余计算，模拟人类驾驶员的注意力机制。</li><li>技能专业化动作MoE：将流匹配解码器中的前馈网络替换为多个专家模块，每个专家专门处理特定驾驶技能（如车道跟随、变道、紧急制动）。路由器根据输入状态激活Top-1或Top-2专家，通过交叉熵损失和负载均衡正则化进行训练。</li></ol>\n<p>训练采用两阶段策略：第一阶段使用真实专家标注稳定路由器训练；第二阶段转为自适应路由，增强模型泛化能力。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：CARLA模拟器（版本0.9.15.1）和Bench2Drive基准。使用Bench2Drive的基础训练集（1000个片段，950训练，50验证/测试），并在220条官方测试路线上评估。</p>\n<p>训练资源：基于预训练的Paligemma VLM权重进行微调，具体硬件未在提供片段中详细说明。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：CARLA模拟器（版本0.9.15.1）中的Bench2Drive闭环基准，包含220条测试路线，每条路线包含一个挑战性案例。</p>\n<p>评估指标：</p>\n<ul><li>驾驶评分（DS）：路线完成率与违规分数的乘积，综合评估任务完成和规则遵守。</li><li>成功率（SR）：在规定时间内无违规成功完成路线的百分比。</li><li>效率：车辆相对于周围交通的速度，衡量驾驶的进取性。</li><li>舒适性：驾驶轨迹的平滑度。</li><li>任务特定指标：针对并线、超车、紧急制动、让行和交通标志识别等具体驾驶技能进行评估。</li></ul>"
  },
  {
    "date": "2025-05-21",
    "title": "UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning",
    "link": "http://arxiv.org/abs/2505.15725",
    "summary_markdown": "论文研究单位\n北京航空航天大学人工智能研究院, 新加坡国立大学, MMLab@CUHK, 北京航空航天大学杭州国际创新研究院\n\n论文概述\n论文提出了一种新的无人机控制范式“Flying-on-a-Word”（Flow），旨在解决语言引导下的无人机精细化轨迹控制问题。与传统关注长时程路径规划的“飞得更远”任务不同，Flow任务聚焦于“飞得更好”，即执行短程、反应式的飞行行为。为支持该范式，论文构建了UAV-Flow，这是一个包含任务定义、大规模真实世界数据集、可部署控制框架和仿真评估套件的综合性基准测试。研究表明，通过模仿学习，无人机可以学习专业飞行员的飞行轨迹，并结合语言指令实现精准控制。论文在UAV-Flow基准上对视觉语言导航（VLN）和视觉-语言-动作（VLA）两种主流范式进行了评估，发现VLA模型在精细化控制任务上表现更优，并成功实现了VLA模型在真实世界开放环境中的首次部署。\n\n论文核心贡献点\n1. 提出了“Flying-on-a-Word”（Flow）任务，将语言引导无人机控制的研究重点从长时程规划转移到短时程、精细化的轨迹控制。\n2. 构建了UAV-Flow基准，这是第一个面向语言条件无人机精细化控制的真实世界基准。\n3. 收集并发布了一个大规模的真实世界数据集（UAV-Flow），包含超过3万条飞行轨迹，覆盖多种环境和指令类型，实现了视觉、语言和动作的精细对齐。\n4. 开发了一个地空协同部署框架，通过将计算任务卸载到地面站，实现了大规模模型在真实无人机上的实时部署。\n5. 构建了一个闭环仿真环境（UAV-Flow-Sim）和配套的评估协议，用于对算法进行系统化、可重复的评估。\n6. 实现并展示了首个在真实世界开放环境中部署的VLA（视觉-语言-动作）模型，用于语言条件下的无人机控制。\n\n论文方法描述\n1. **任务定义**：将Flow任务形式化为一个策略函数π_θ，该函数根据无人机当前6自由度状态S_t、第一人称视觉观测O_t和自然语言指令I，映射出低级控制动作a_t。任务评估运动意图理解（如“移动5米”）和空间上下文基础（如“飞到标记物右侧”）两种核心能力，并定义了基本运动指令和物体交互指令两种类型。\n2. **数据收集**：在三个大学校园环境中，由专业飞手操作搭载4K相机和RTK GPS的无人机，依据指令模板执行飞行。记录同步的视频和6自由度轨迹，并通过时间戳对齐。注释团队对有效片段进行语言标注，并利用LLM（如GPT）丰富指令表达，形成固定和开放两种词汇命令集。\n3. **模型适配**：将现有的VLN模型（如Seq2Seq, CMA, Travel）和VLA模型（如OpenVLA, Pi-0）适配到Flow任务。主要修改包括将VLN模型的输出从离散动作改为连续位姿回归，以及调整VLA模型的输入格式以融合无人机状态和语言指令，并预测6自由度位姿。\n4. **部署框架**：为解决无人机机载算力限制，设计了地空协同框架。无人机通过无线链路将视频流和状态数据传输至地面站，地面站完成模型推理后将控制指令发回。采用带有前瞻机制的全局对齐连续运动方案，以处理推理延迟并保证飞行平稳性。\n\n论文使用数据集和训练资源\n1. **数据集**：\n - UAV-Flow (真实世界): 包含30,692条飞行轨迹，采集于三个总面积5.02平方公里的校园环境，涵盖8种主要运动类型。多数轨迹长度在20米以内。\n - UAV-Flow-Sim (仿真): 包含10,109条轨迹，在基于Unreal Engine的校园环境中构建。另有一个包含273条轨迹的测试集用于系统评估。\n2. **训练资源**：\n - Seq2Seq-UAV/CMA-UAV: 1块 RTX 4090, Batch Size 32, 学习率 1e-4, 训练10个epoch。\n - Travel-UAV: 8块 A100, Batch Size 32, 学习率 5e-4, 使用LoRA (rank 32), 训练2个epoch。\n - OpenVLA-UAV: 8块 A100, Batch Size 32, 学习率 5e-4, 使用LoRA (rank 32), 训练200k步。\n - Pi-0-UAV: 8块 RTX 4090, Batch Size 16, 学习率 5e-5, 使用LoRA (rank 32), 训练12个epoch。\n\n论文使用的评估环境和评估指标\n1. **评估环境**：\n - 主要在闭环仿真环境（UAV-Flow-Sim）中进行，以实现系统化、可控制的算法比较。\n - 在真实世界中对模型进行了部署演示，以验证其在实际环境中的可行性，但因安全和环境复杂性未进行定量评估。\n2. **评估指标**：\n - **成功率**：通过人工检查预测轨迹在语义上是否满足指令来判定是否成功。\n - **归一化动态时间规整**：通过计算预测轨迹与参考轨迹之间的6维（位置x,y,z和姿态roll, pitch, yaw的余弦值）相似度，来评估轨迹的精确度和形状匹配程度。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京航空航天大学人工智能研究院, 新加坡国立大学, MMLab@CUHK, 北京航空航天大学杭州国际创新研究院</p>\n\n<p>论文概述</p>\n<p>论文提出了一种新的无人机控制范式“Flying-on-a-Word”（Flow），旨在解决语言引导下的无人机精细化轨迹控制问题。与传统关注长时程路径规划的“飞得更远”任务不同，Flow任务聚焦于“飞得更好”，即执行短程、反应式的飞行行为。为支持该范式，论文构建了UAV-Flow，这是一个包含任务定义、大规模真实世界数据集、可部署控制框架和仿真评估套件的综合性基准测试。研究表明，通过模仿学习，无人机可以学习专业飞行员的飞行轨迹，并结合语言指令实现精准控制。论文在UAV-Flow基准上对视觉语言导航（VLN）和视觉-语言-动作（VLA）两种主流范式进行了评估，发现VLA模型在精细化控制任务上表现更优，并成功实现了VLA模型在真实世界开放环境中的首次部署。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了“Flying-on-a-Word”（Flow）任务，将语言引导无人机控制的研究重点从长时程规划转移到短时程、精细化的轨迹控制。</li><li>构建了UAV-Flow基准，这是第一个面向语言条件无人机精细化控制的真实世界基准。</li><li>收集并发布了一个大规模的真实世界数据集（UAV-Flow），包含超过3万条飞行轨迹，覆盖多种环境和指令类型，实现了视觉、语言和动作的精细对齐。</li><li>开发了一个地空协同部署框架，通过将计算任务卸载到地面站，实现了大规模模型在真实无人机上的实时部署。</li><li>构建了一个闭环仿真环境（UAV-Flow-Sim）和配套的评估协议，用于对算法进行系统化、可重复的评估。</li><li>实现并展示了首个在真实世界开放环境中部署的VLA（视觉-语言-动作）模型，用于语言条件下的无人机控制。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li><strong>任务定义</strong>：将Flow任务形式化为一个策略函数π_θ，该函数根据无人机当前6自由度状态S_t、第一人称视觉观测O_t和自然语言指令I，映射出低级控制动作a_t。任务评估运动意图理解（如“移动5米”）和空间上下文基础（如“飞到标记物右侧”）两种核心能力，并定义了基本运动指令和物体交互指令两种类型。</li><li><strong>数据收集</strong>：在三个大学校园环境中，由专业飞手操作搭载4K相机和RTK GPS的无人机，依据指令模板执行飞行。记录同步的视频和6自由度轨迹，并通过时间戳对齐。注释团队对有效片段进行语言标注，并利用LLM（如GPT）丰富指令表达，形成固定和开放两种词汇命令集。</li><li><strong>模型适配</strong>：将现有的VLN模型（如Seq2Seq, CMA, Travel）和VLA模型（如OpenVLA, Pi-0）适配到Flow任务。主要修改包括将VLN模型的输出从离散动作改为连续位姿回归，以及调整VLA模型的输入格式以融合无人机状态和语言指令，并预测6自由度位姿。</li><li><strong>部署框架</strong>：为解决无人机机载算力限制，设计了地空协同框架。无人机通过无线链路将视频流和状态数据传输至地面站，地面站完成模型推理后将控制指令发回。采用带有前瞻机制的全局对齐连续运动方案，以处理推理延迟并保证飞行平稳性。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. <strong>数据集</strong>：</p>\n<p> - UAV-Flow (真实世界): 包含30,692条飞行轨迹，采集于三个总面积5.02平方公里的校园环境，涵盖8种主要运动类型。多数轨迹长度在20米以内。</p>\n<p> - UAV-Flow-Sim (仿真): 包含10,109条轨迹，在基于Unreal Engine的校园环境中构建。另有一个包含273条轨迹的测试集用于系统评估。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - Seq2Seq-UAV/CMA-UAV: 1块 RTX 4090, Batch Size 32, 学习率 1e-4, 训练10个epoch。</p>\n<p> - Travel-UAV: 8块 A100, Batch Size 32, 学习率 5e-4, 使用LoRA (rank 32), 训练2个epoch。</p>\n<p> - OpenVLA-UAV: 8块 A100, Batch Size 32, 学习率 5e-4, 使用LoRA (rank 32), 训练200k步。</p>\n<p> - Pi-0-UAV: 8块 RTX 4090, Batch Size 16, 学习率 5e-5, 使用LoRA (rank 32), 训练12个epoch。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - 主要在闭环仿真环境（UAV-Flow-Sim）中进行，以实现系统化、可控制的算法比较。</p>\n<p> - 在真实世界中对模型进行了部署演示，以验证其在实际环境中的可行性，但因安全和环境复杂性未进行定量评估。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - <strong>成功率</strong>：通过人工检查预测轨迹在语义上是否满足指令来判定是否成功。</p>\n<p> - <strong>归一化动态时间规整</strong>：通过计算预测轨迹与参考轨迹之间的6维（位置x,y,z和姿态roll, pitch, yaw的余弦值）相似度，来评估轨迹的精确度和形状匹配程度。</p>"
  },
  {
    "date": "2025-05-21",
    "title": "From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems",
    "link": "http://arxiv.org/abs/2505.15685",
    "summary_markdown": "### 论文研究单位\n- IHPC, Agency for Science, Technology and Research, Singapore\n- Nanyang Technological University, Singapore\n### 论文概述\n论文研究了三种基础模型（Foundation Models）在具身机器人系统中的集成策略：端到端视觉-语言-动作模型（VLAs）、模块化视觉-语言模型（VLM）管道，以及多模态大语言模型（MLLM）代理作为编排者。通过两个案例研究——指令基础（instruction grounding）和机器人操作（robotic manipulation）——评估了这些策略在复杂指令跟随和通用动作生成中的权衡。实验揭示了模型规模、泛化能力和数据效率之间的平衡点，为语言驱动的物理代理提供了设计见解。\n### 论文核心贡献点\n1. **范式分析**：系统分析了三种FM集成范式在共享具身任务上的能力与权衡。\n2. **资源发布**：发布了一个数据集和代码，用于评估指令基础和对象操作，涵盖跨模态推理和不同布局下的技能适应。\n3. **实践见解**：提供了对最先进VLAs和MLLMs的及时洞察，调查了它们的能力和失败模式，为实践者选择FM堆栈提供了实用权衡指导。\n4. **系统演示**：发布了一个完整的端到端抓娃娃机器人系统作为真实世界FM集成演示。\n### 论文方法描述\n1. **端到端VLA模型**：\n - **自回归模型**：逐步生成动作，基于当前感知输入和历史输出预测低级控制令牌（如关节角度）。\n - **扩散模型**：通过去噪轨迹生成动作，建模未来动作的分布而非逐步生成。\n - **优势与局限**：利用大规模预训练实现任务泛化，但受限于数据稀缺性和对新任务/环境的脆弱性。\n2. **模块化VLM管道**：\n - 专业化VLM处理感知，输出符号化场景信息（如边界框、分割掩码），下游规划器生成动作。\n - 优势：可解释性和高效率（模型参数仅100M-600M）。\n - 局限：交互刚性，感知错误传播无缓解。\n3. **多模态LLM代理作为编排者**：\n - MLLM作为认知枢纽，通过函数调用调用视觉工具，推理后输出高级动作基元。\n - 优势：视觉常识推理和指令跟随能力强。\n - 局限：资源密集型，部署挑战大。\n### 论文使用数据集和训练资源\n- **数据集**：\n - **杂乱桌面操作数据集**：163个演示片段，目标是在杂乱环境中拾取螺丝刀，使用UR5机械臂和RealSense相机收集。\n - **复杂指令基础数据集**：473条指令，包含隐式、属性显式和空间关系指令，用于跨模态消歧。\n- **训练资源**：\n - 部分微调：单块NVIDIA A6000 GPU（48GB VRAM），持续3天。\n - 全量微调：8×H100 GPU节点。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **真实环境**：WidowX机器人平台（真实世界任务）。\n - **仿真环境**：LIBERO基准测试（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。\n- **评估指标**：\n - **成功率（%）**：操作任务中的任务完成率，每个任务500次试验。\n - **宏平均准确率**：指令基础任务中物体定位和复杂指令理解的准确率。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>IHPC, Agency for Science, Technology and Research, Singapore</li><li>Nanyang Technological University, Singapore</li></ul>\n<h3>论文概述</h3>\n<p>论文研究了三种基础模型（Foundation Models）在具身机器人系统中的集成策略：端到端视觉-语言-动作模型（VLAs）、模块化视觉-语言模型（VLM）管道，以及多模态大语言模型（MLLM）代理作为编排者。通过两个案例研究——指令基础（instruction grounding）和机器人操作（robotic manipulation）——评估了这些策略在复杂指令跟随和通用动作生成中的权衡。实验揭示了模型规模、泛化能力和数据效率之间的平衡点，为语言驱动的物理代理提供了设计见解。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>范式分析</strong>：系统分析了三种FM集成范式在共享具身任务上的能力与权衡。</li><li><strong>资源发布</strong>：发布了一个数据集和代码，用于评估指令基础和对象操作，涵盖跨模态推理和不同布局下的技能适应。</li><li><strong>实践见解</strong>：提供了对最先进VLAs和MLLMs的及时洞察，调查了它们的能力和失败模式，为实践者选择FM堆栈提供了实用权衡指导。</li><li><strong>系统演示</strong>：发布了一个完整的端到端抓娃娃机器人系统作为真实世界FM集成演示。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>端到端VLA模型</strong>：</p>\n<p> - <strong>自回归模型</strong>：逐步生成动作，基于当前感知输入和历史输出预测低级控制令牌（如关节角度）。</p>\n<p> - <strong>扩散模型</strong>：通过去噪轨迹生成动作，建模未来动作的分布而非逐步生成。</p>\n<p> - <strong>优势与局限</strong>：利用大规模预训练实现任务泛化，但受限于数据稀缺性和对新任务/环境的脆弱性。</p>\n<p>2. <strong>模块化VLM管道</strong>：</p>\n<p> - 专业化VLM处理感知，输出符号化场景信息（如边界框、分割掩码），下游规划器生成动作。</p>\n<p> - 优势：可解释性和高效率（模型参数仅100M-600M）。</p>\n<p> - 局限：交互刚性，感知错误传播无缓解。</p>\n<p>3. <strong>多模态LLM代理作为编排者</strong>：</p>\n<p> - MLLM作为认知枢纽，通过函数调用调用视觉工具，推理后输出高级动作基元。</p>\n<p> - 优势：视觉常识推理和指令跟随能力强。</p>\n<p> - 局限：资源密集型，部署挑战大。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - <strong>杂乱桌面操作数据集</strong>：163个演示片段，目标是在杂乱环境中拾取螺丝刀，使用UR5机械臂和RealSense相机收集。</p>\n<p> - <strong>复杂指令基础数据集</strong>：473条指令，包含隐式、属性显式和空间关系指令，用于跨模态消歧。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 部分微调：单块NVIDIA A6000 GPU（48GB VRAM），持续3天。</p>\n<p> - 全量微调：8×H100 GPU节点。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>真实环境</strong>：WidowX机器人平台（真实世界任务）。</p>\n<p> - <strong>仿真环境</strong>：LIBERO基准测试（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率（%）</strong>：操作任务中的任务完成率，每个任务500次试验。</p>\n<p> - <strong>宏平均准确率</strong>：指令基础任务中物体定位和复杂指令理解的准确率。</p>"
  },
  {
    "date": "2025-05-21",
    "title": "Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization",
    "link": "http://arxiv.org/abs/2505.15660",
    "summary_markdown": "```markdown\n# 论文研究单位\nThe Hong Kong University of Science and Technology (Guangzhou), The University of Hong Kong, Sun Yat-sen University, The Hong Kong University of Science and Technology\n# 论文概述\n论文探讨了视觉-语言-动作（VLA）模型在跨任务泛化中的局限性，并引入了AGNOSTOS基准来严格评估零样本跨任务泛化能力。AGNOSTOS包含23个未见任务，分为两个难度级别。评估显示现有VLA模型泛化能力有限，为此提出了跨任务上下文操作（X-ICM）方法，结合大型语言模型（LLMs）的上下文学习能力和动态引导样本选择策略，以提升泛化性能。\n# 论文核心贡献点\n- 引入AGNOSTOS基准，首个系统性评估零样本跨任务泛化的基准，涵盖18个已见任务和23个未见任务（Level-1和Level-2）。\n- 提出X-ICM方法，通过上下文提示LLMs预测未见任务的动作序列，并利用动态引导样本选择策略提高泛化效果。\n- 在AGNOSTOS上评估多种VLA模型，揭示其局限性，并展示X-ICM的优越性，平均成功率提升显著。\n# 论文方法描述\nX-ICM方法包含两个核心模块：\n1. **动态引导样本选择模块**：训练扩散模型学习任务动态表示，通过预测最终观测从初始状态和任务描述中提取特征。计算已见任务和未见任务特征的余弦相似度，检索top-K相关演示。\n2. **跨任务上下文预测模块**：将检索的演示文本化（包括物体3D位置和关键动作序列），构建提示词输入LLM（如Qwen2.5-Instruct），生成未见任务的动作序列。\n# 论文使用数据集和训练资源\n- **数据集**：AGNOSTOS基准基于RLBench，包含18个已见任务（每个任务200个演示，共3600个）用于训练，23个未见任务（13个Level-1和10个Level-2）用于测试。\n- **训练资源**：X-ICM使用LLMs（Qwen2.5-Instruct的7B和72B参数版本），部署在A6000 GPU上（7B用2块，72B用8块）。扩散模型初始化自InstructPix2Pix，在已见演示上优化。\n# 论文使用的评估环境和评估指标\n- **评估环境**：RLBench模拟环境，支持可重现测试。\n- **评估指标**：对每个未见任务进行3次不同种子的测试运行，每次包含25次rollout，报告成功率的平均值和标准差（%）。比较不同模型在Level-1和Level-2任务上的平均成功率。\n```",
    "summary_html": "<p>```markdown</p>\n<h1>论文研究单位</h1>\n<p>The Hong Kong University of Science and Technology (Guangzhou), The University of Hong Kong, Sun Yat-sen University, The Hong Kong University of Science and Technology</p>\n<h1>论文概述</h1>\n<p>论文探讨了视觉-语言-动作（VLA）模型在跨任务泛化中的局限性，并引入了AGNOSTOS基准来严格评估零样本跨任务泛化能力。AGNOSTOS包含23个未见任务，分为两个难度级别。评估显示现有VLA模型泛化能力有限，为此提出了跨任务上下文操作（X-ICM）方法，结合大型语言模型（LLMs）的上下文学习能力和动态引导样本选择策略，以提升泛化性能。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>引入AGNOSTOS基准，首个系统性评估零样本跨任务泛化的基准，涵盖18个已见任务和23个未见任务（Level-1和Level-2）。</li><li>提出X-ICM方法，通过上下文提示LLMs预测未见任务的动作序列，并利用动态引导样本选择策略提高泛化效果。</li><li>在AGNOSTOS上评估多种VLA模型，揭示其局限性，并展示X-ICM的优越性，平均成功率提升显著。</li></ul>\n<h1>论文方法描述</h1>\n<p>X-ICM方法包含两个核心模块：</p>\n<ol><li><strong>动态引导样本选择模块</strong>：训练扩散模型学习任务动态表示，通过预测最终观测从初始状态和任务描述中提取特征。计算已见任务和未见任务特征的余弦相似度，检索top-K相关演示。</li><li><strong>跨任务上下文预测模块</strong>：将检索的演示文本化（包括物体3D位置和关键动作序列），构建提示词输入LLM（如Qwen2.5-Instruct），生成未见任务的动作序列。</li></ol>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>数据集</strong>：AGNOSTOS基准基于RLBench，包含18个已见任务（每个任务200个演示，共3600个）用于训练，23个未见任务（13个Level-1和10个Level-2）用于测试。</li><li><strong>训练资源</strong>：X-ICM使用LLMs（Qwen2.5-Instruct的7B和72B参数版本），部署在A6000 GPU上（7B用2块，72B用8块）。扩散模型初始化自InstructPix2Pix，在已见演示上优化。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>评估环境</strong>：RLBench模拟环境，支持可重现测试。</li><li><strong>评估指标</strong>：对每个未见任务进行3次不同种子的测试运行，每次包含25次rollout，报告成功率的平均值和标准差（%）。比较不同模型在Level-1和Level-2任务上的平均成功率。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-05-21",
    "title": "FLARE: Robot Learning with Implicit World Modeling",
    "link": "http://arxiv.org/abs/2505.15659",
    "summary_markdown": "论文研究单位\nNVIDIA, University of Maryland, College Park, Nanyang Technological University, University of Texas, Austin\n\n论文概述\nFLARE（Future Latent Representation Alignment）是一个新颖的框架，通过将预测性的潜在世界模型集成到机器人策略学习中。该方法通过将扩散变换器的特征与未来观察的潜在嵌入对齐，使扩散变换器策略能够预测未来观察的潜在表示，从而在生成动作的同时推理长期后果。FLARE仅需要对现有的视觉-语言-动作（VLA）模型进行最小的架构修改（增加少量token），却实现了显著的性能提升。在跨越单臂和人形机器人桌面操作的两个多任务模拟模仿学习基准测试中，FLARE达到了最先进的性能，比之前的策略学习基线高出26%。此外，FLARE能够利用人类第一人称视频演示进行共同训练，而无需动作标签，显著提升了策略对具有未见几何形状的新物体的泛化能力，每个物体仅需一个机器人演示。\n\n论文核心贡献点\n1. 提出了FLARE框架，通过未来潜在表示对齐，在扩散策略中实现隐式世界建模。\n2. 设计了一种动作感知的未来嵌入模型，通过Q-former模块压缩视觉-语言序列，生成紧凑的表示，并针对下游控制任务进行优化。\n3. 在多任务模拟基准上实现了SOTA性能，比基线高出26%。\n4. 实现了从无动作标签的人类第一人称视频中学习的能力，仅凭一个机器人演示即可泛化到新物体。\n5. 展示了FLARE在真实GR1人形机器人上的数据高效微调能力，每任务100个轨迹达到95%成功率。\n\n论文方法描述\n方法包括两个阶段：\n1. 预训练一个紧凑的动作感知观察嵌入模型：使用SigLIP-2的视觉和文本编码器，通过自注意力块融合多模态特征，再用Q-former压缩成32个可学习查询token，并端到端训练以预测机器人动作，确保潜在嵌入捕获任务相关信息。\n2. 联合训练扩散变换器策略：\n - 输入序列包含三部分：当前本体状态编码、加噪动作块编码、M个可学习未来token。\n - 在扩散变换器中间层（第6层）提取未来token的激活，通过MLP投影后，与未来观察的冻结视觉-语言嵌入进行余弦相似度对齐。\n - 总损失函数为动作流匹配损失与未来对齐损失的加权和（λ=0.2）。\n - 使用预训练嵌入模型作为目标，并通过指数移动平均（EMA，ρ=0.995）更新目标嵌入以适应下游任务分布。\n\n论文使用数据集和训练资源\n预训练数据：跨具身机器人数据集混合，包括GR00T N1和Open X-Embodiment的7个额外数据集，总计约2000小时机器人数据。\n下游任务：\n - 多任务基准：24个RoboCasa模拟任务和24个GR-1桌面操作任务。\n - 真实机器人评估：4个真实GR-1任务（Pick and Place等），每个任务8个初始帧和4个不同物体。\n - 人类视频实验：5个新颖几何形状物体，每个物体150个人类第一人称视频（GoPro采集）和10个机器人遥操作演示。\n训练资源：未明确提及硬件，但使用扩散变换器架构和梯度下降优化。\n\n论文使用的评估环境和评估指标\n评估环境：\n - 模拟环境：RoboCasa（厨房环境，3视角RGB图像）和GR-1桌面操作模拟（头戴摄像头，单视角RGB）。\n - 真实环境：真实GR-1人形机器人，桌面操作场景。\n评估指标：\n - 任务成功率：每个任务评估50个回合，报告最后5个检查点中的最大成功率。\n - 多任务平均成功率：RoboCasa 24任务平均和GR-1 24任务平均。\n - 泛化实验：新物体抓取成功率，部分成功（抓取但未放入篮子）计0.5分。\n - 数据效率：在不同训练轨迹数量（100/1000）下的性能对比。",
    "summary_html": "<p>论文研究单位</p>\n<p>NVIDIA, University of Maryland, College Park, Nanyang Technological University, University of Texas, Austin</p>\n\n<p>论文概述</p>\n<p>FLARE（Future Latent Representation Alignment）是一个新颖的框架，通过将预测性的潜在世界模型集成到机器人策略学习中。该方法通过将扩散变换器的特征与未来观察的潜在嵌入对齐，使扩散变换器策略能够预测未来观察的潜在表示，从而在生成动作的同时推理长期后果。FLARE仅需要对现有的视觉-语言-动作（VLA）模型进行最小的架构修改（增加少量token），却实现了显著的性能提升。在跨越单臂和人形机器人桌面操作的两个多任务模拟模仿学习基准测试中，FLARE达到了最先进的性能，比之前的策略学习基线高出26%。此外，FLARE能够利用人类第一人称视频演示进行共同训练，而无需动作标签，显著提升了策略对具有未见几何形状的新物体的泛化能力，每个物体仅需一个机器人演示。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了FLARE框架，通过未来潜在表示对齐，在扩散策略中实现隐式世界建模。</li><li>设计了一种动作感知的未来嵌入模型，通过Q-former模块压缩视觉-语言序列，生成紧凑的表示，并针对下游控制任务进行优化。</li><li>在多任务模拟基准上实现了SOTA性能，比基线高出26%。</li><li>实现了从无动作标签的人类第一人称视频中学习的能力，仅凭一个机器人演示即可泛化到新物体。</li><li>展示了FLARE在真实GR1人形机器人上的数据高效微调能力，每任务100个轨迹达到95%成功率。</li></ol>\n\n<p>论文方法描述</p>\n<p>方法包括两个阶段：</p>\n<ol><li>预训练一个紧凑的动作感知观察嵌入模型：使用SigLIP-2的视觉和文本编码器，通过自注意力块融合多模态特征，再用Q-former压缩成32个可学习查询token，并端到端训练以预测机器人动作，确保潜在嵌入捕获任务相关信息。</li><li>联合训练扩散变换器策略：</li></ol>\n<p> - 输入序列包含三部分：当前本体状态编码、加噪动作块编码、M个可学习未来token。</p>\n<p> - 在扩散变换器中间层（第6层）提取未来token的激活，通过MLP投影后，与未来观察的冻结视觉-语言嵌入进行余弦相似度对齐。</p>\n<p> - 总损失函数为动作流匹配损失与未来对齐损失的加权和（λ=0.2）。</p>\n<p> - 使用预训练嵌入模型作为目标，并通过指数移动平均（EMA，ρ=0.995）更新目标嵌入以适应下游任务分布。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>预训练数据：跨具身机器人数据集混合，包括GR00T N1和Open X-Embodiment的7个额外数据集，总计约2000小时机器人数据。</p>\n<p>下游任务：</p>\n<p> - 多任务基准：24个RoboCasa模拟任务和24个GR-1桌面操作任务。</p>\n<p> - 真实机器人评估：4个真实GR-1任务（Pick and Place等），每个任务8个初始帧和4个不同物体。</p>\n<p> - 人类视频实验：5个新颖几何形状物体，每个物体150个人类第一人称视频（GoPro采集）和10个机器人遥操作演示。</p>\n<p>训练资源：未明确提及硬件，但使用扩散变换器架构和梯度下降优化。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<p> - 模拟环境：RoboCasa（厨房环境，3视角RGB图像）和GR-1桌面操作模拟（头戴摄像头，单视角RGB）。</p>\n<p> - 真实环境：真实GR-1人形机器人，桌面操作场景。</p>\n<p>评估指标：</p>\n<p> - 任务成功率：每个任务评估50个回合，报告最后5个检查点中的最大成功率。</p>\n<p> - 多任务平均成功率：RoboCasa 24任务平均和GR-1 24任务平均。</p>\n<p> - 泛化实验：新物体抓取成功率，部分成功（抓取但未放入篮子）计0.5分。</p>\n<p> - 数据效率：在不同训练轨迹数量（100/1000）下的性能对比。</p>"
  },
  {
    "date": "2025-05-21",
    "title": "Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control",
    "link": "http://arxiv.org/abs/2505.15304",
    "summary_markdown": "论文研究单位\nHanyang University, Hyundai Motor Company, Seoul, Republic of Korea\n\n论文概述\n本文提出显著性感知量化模仿学习（SQIL）方法，用于高效机器人控制。深度神经网络策略模型（如视觉-语言-动作模型）在复杂决策中表现出色，但模型规模扩大导致计算开销增加，难以在资源受限设备（如机器人操作、自动驾驶）上部署。SQIL结合量化感知训练与选择性损失加权策略，通过显著性分数识别任务关键状态并在训练损失中强调这些状态，从而在低比特精度下保持决策保真度。实验表明，SQIL在多个模拟基准、真实任务和跨域任务（自动驾驶、物理模拟）中恢复全精度性能：例如，在机器人操作LIBERO基准上，4-bit权重量化模型在边缘GPU实现2.5倍加速和2.5倍节能；在自动驾驶NoCrash基准上，4-bit权重和激活量化策略保持全精度性能，在低端GPU实现3.7倍加速和3.1倍节能，展示了其在资源受限设备上部署大规模策略的潜力。\n\n论文核心贡献点\n首次系统研究量化模仿学习，识别任务关键状态的重要性，发现量化策略失败主要源于物理交互中的粗粒度控制，解释朴素量化失败原因。\n提出基于策略的关键状态检测方法SIS，利用策略动作敏感性识别任务关键状态，超越传统视觉-语言关键帧检测器。\n设计SQIL框架，将4-bit量化感知训练与SIS加权损失结合，实现2-4倍加速和节能，同时成功率保持在全精度基线1%以内。\n在机器人操作、自动驾驶和MuJoCo控制等跨域任务中进行广泛验证，证实SQIL在模拟和真实环境中的通用性。\n\n论文方法描述\nSQIL包含两个核心组件：显著性状态重要性评分（SIS）和量化鲁棒动作蒸馏（QRD）。SIS通过测量视觉扰动下的动作差异量化状态重要性，公式为 $S_{\\pi}(s_{t},k) = \\frac{1}{2} \\\\|\\pi(s_{t}) - \\pi(\\phi(s_{t},k)) \\\\|^{2}$，平均分数 $SIS^{s_{t}}_{\\pi}$ 越高表示状态越关键，如抓取或释放物体时刻；QRD在量化感知训练中，对SIS识别的高重要性状态施加更高损失权重，使量化策略更贴近全精度策略在关键状态下的决策。训练时从全精度策略初始化量化策略，计算SIS后，循环N个epoch遍历专家数据，更新量化参数。\n\n论文使用数据集和训练资源\n数据集包括LIBERO机器人操作基准（1700个模拟episode用于微调OpenVLA）、NoCrash自动驾驶基准（用于CILRS模型）和D4RL物理模拟基准（用于MuJoCo控制），并涉及真实世界任务。训练资源使用预训练模型如OpenVLA（970k episode预训练后LoRA微调）、CILRS和D4RL策略，量化至4-bit权重或权重-激活；训练超参数（如学习率）与全精度训练一致，硬件未明确指定。\n\n论文使用的评估环境和评估指标\n评估环境涵盖模拟环境（LIBERO、NoCrash、D4RL）和真实机器人任务，硬件平台包括边缘GPU（机器人操作）和低端GPU（自动驾驶）。主要评估指标为成功率（如LIBERO平均成功率），次要指标包括动作差异（L2-norm）、推理速度加速比（如2.5x）和节能比（如2.5x），并测试环境变化下的泛化性能。",
    "summary_html": "<p>论文研究单位</p>\n<p>Hanyang University, Hyundai Motor Company, Seoul, Republic of Korea</p>\n\n<p>论文概述</p>\n<p>本文提出显著性感知量化模仿学习（SQIL）方法，用于高效机器人控制。深度神经网络策略模型（如视觉-语言-动作模型）在复杂决策中表现出色，但模型规模扩大导致计算开销增加，难以在资源受限设备（如机器人操作、自动驾驶）上部署。SQIL结合量化感知训练与选择性损失加权策略，通过显著性分数识别任务关键状态并在训练损失中强调这些状态，从而在低比特精度下保持决策保真度。实验表明，SQIL在多个模拟基准、真实任务和跨域任务（自动驾驶、物理模拟）中恢复全精度性能：例如，在机器人操作LIBERO基准上，4-bit权重量化模型在边缘GPU实现2.5倍加速和2.5倍节能；在自动驾驶NoCrash基准上，4-bit权重和激活量化策略保持全精度性能，在低端GPU实现3.7倍加速和3.1倍节能，展示了其在资源受限设备上部署大规模策略的潜力。</p>\n\n<p>论文核心贡献点</p>\n<p>首次系统研究量化模仿学习，识别任务关键状态的重要性，发现量化策略失败主要源于物理交互中的粗粒度控制，解释朴素量化失败原因。</p>\n<p>提出基于策略的关键状态检测方法SIS，利用策略动作敏感性识别任务关键状态，超越传统视觉-语言关键帧检测器。</p>\n<p>设计SQIL框架，将4-bit量化感知训练与SIS加权损失结合，实现2-4倍加速和节能，同时成功率保持在全精度基线1%以内。</p>\n<p>在机器人操作、自动驾驶和MuJoCo控制等跨域任务中进行广泛验证，证实SQIL在模拟和真实环境中的通用性。</p>\n\n<p>论文方法描述</p>\n<p>SQIL包含两个核心组件：显著性状态重要性评分（SIS）和量化鲁棒动作蒸馏（QRD）。SIS通过测量视觉扰动下的动作差异量化状态重要性，公式为 $S_{\\pi}(s_{t},k) = \\frac{1}{2} \\\\|\\pi(s_{t}) - \\pi(\\phi(s_{t},k)) \\\\|^{2}$，平均分数 $SIS^{s_{t}}_{\\pi}$ 越高表示状态越关键，如抓取或释放物体时刻；QRD在量化感知训练中，对SIS识别的高重要性状态施加更高损失权重，使量化策略更贴近全精度策略在关键状态下的决策。训练时从全精度策略初始化量化策略，计算SIS后，循环N个epoch遍历专家数据，更新量化参数。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集包括LIBERO机器人操作基准（1700个模拟episode用于微调OpenVLA）、NoCrash自动驾驶基准（用于CILRS模型）和D4RL物理模拟基准（用于MuJoCo控制），并涉及真实世界任务。训练资源使用预训练模型如OpenVLA（970k episode预训练后LoRA微调）、CILRS和D4RL策略，量化至4-bit权重或权重-激活；训练超参数（如学习率）与全精度训练一致，硬件未明确指定。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境涵盖模拟环境（LIBERO、NoCrash、D4RL）和真实机器人任务，硬件平台包括边缘GPU（机器人操作）和低端GPU（自动驾驶）。主要评估指标为成功率（如LIBERO平均成功率），次要指标包括动作差异（L2-norm）、推理速度加速比（如2.5x）和节能比（如2.5x），并测试环境变化下的泛化性能。</p>"
  },
  {
    "date": "2025-05-21",
    "title": "EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy",
    "link": "http://arxiv.org/abs/2505.15206",
    "summary_markdown": "论文研究单位\n香港中文大学，慕尼黑工业大学\n\n论文概述\n本文提出EndoVLA，一种专为内窥镜连续机器人设计的双阶段视觉-语言-动作模型，用于实现内窥镜手术中的自主跟踪。该模型能够根据内窥镜图像和外科医生的指令执行息肉跟踪、异常黏膜区域描绘和环形标记跟随三项核心任务。通过结合监督微调和强化微调策略，解决了传统模型在复杂动态胃肠道环境中的泛化问题。\n\n论文核心贡献点\n1. 提出EndoVLA模型，首个针对连续机器人的端到端VLA框架，采用双阶段微调策略（DFT）提升性能和泛化能力。\n2. 构建EndoVLA-Motion数据集，包含6k图像-动作对，覆盖三种内窥镜任务，支持视觉-语言-运动建模。\n3. 实验证实模型在三项任务中达到SOTA性能，并在通用场景和复杂序列任务中实现零样本泛化。\n\n论文方法描述\n模型基于Qwen2-VL架构，使用LoRA进行高效微调。输入包括RGB图像和语言指令，输出目标边界框和离散动作。动作空间定义为5类：上右、上左、下左、下右和停止，对应电机旋转增量。通过双阶段微调：第一阶段监督微调（SFT）最小化边界框和动作预测误差；第二阶段强化微调（RFT）采用分组相对策略优化（GRPO），使用可验证奖励信号（如目标与焦点距离）进一步提升决策能力。\n\n论文使用数据集和训练资源\n使用自建EndoVLA-Motion数据集，包含6k图像-动作对，通过Olympus内窥镜在两种胃模型上采集。数据通过YOLOv5自动标注边界框并手动校准，按任务分为息肉跟踪（PP）、异常区域定位（AR）和圆形标记跟随（CC）。训练集占80%，评估集占20%。模型在Qwen2-VL基础上微调，使用LoRA适配器减少参数量，具体计算资源未明确说明。\n\n论文使用的评估环境和评估指标\n评估在真实机器人系统进行，包括内窥镜平台（Olympus内窥镜，双电机控制，400×400分辨率@30FPS）。指标包括：跟踪成功率（目标中心与焦点距离≤18像素即成功）、动作预测准确率、边界框定位精度（IoU），以及零样本泛化能力（在未见过的通用场景和序列任务中的表现）。实验涵盖运动预测、消融研究和真实机器人跟踪测试。",
    "summary_html": "<p>论文研究单位</p>\n<p>香港中文大学，慕尼黑工业大学</p>\n\n<p>论文概述</p>\n<p>本文提出EndoVLA，一种专为内窥镜连续机器人设计的双阶段视觉-语言-动作模型，用于实现内窥镜手术中的自主跟踪。该模型能够根据内窥镜图像和外科医生的指令执行息肉跟踪、异常黏膜区域描绘和环形标记跟随三项核心任务。通过结合监督微调和强化微调策略，解决了传统模型在复杂动态胃肠道环境中的泛化问题。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出EndoVLA模型，首个针对连续机器人的端到端VLA框架，采用双阶段微调策略（DFT）提升性能和泛化能力。</li><li>构建EndoVLA-Motion数据集，包含6k图像-动作对，覆盖三种内窥镜任务，支持视觉-语言-运动建模。</li><li>实验证实模型在三项任务中达到SOTA性能，并在通用场景和复杂序列任务中实现零样本泛化。</li></ol>\n\n<p>论文方法描述</p>\n<p>模型基于Qwen2-VL架构，使用LoRA进行高效微调。输入包括RGB图像和语言指令，输出目标边界框和离散动作。动作空间定义为5类：上右、上左、下左、下右和停止，对应电机旋转增量。通过双阶段微调：第一阶段监督微调（SFT）最小化边界框和动作预测误差；第二阶段强化微调（RFT）采用分组相对策略优化（GRPO），使用可验证奖励信号（如目标与焦点距离）进一步提升决策能力。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用自建EndoVLA-Motion数据集，包含6k图像-动作对，通过Olympus内窥镜在两种胃模型上采集。数据通过YOLOv5自动标注边界框并手动校准，按任务分为息肉跟踪（PP）、异常区域定位（AR）和圆形标记跟随（CC）。训练集占80%，评估集占20%。模型在Qwen2-VL基础上微调，使用LoRA适配器减少参数量，具体计算资源未明确说明。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估在真实机器人系统进行，包括内窥镜平台（Olympus内窥镜，双电机控制，400×400分辨率@30FPS）。指标包括：跟踪成功率（目标中心与焦点距离≤18像素即成功）、动作预测准确率、边界框定位精度（IoU），以及零样本泛化能力（在未见过的通用场景和序列任务中的表现）。实验涵盖运动预测、消融研究和真实机器人跟踪测试。</p>"
  },
  {
    "date": "2025-05-21",
    "title": "Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation",
    "link": "http://arxiv.org/abs/2505.15098",
    "summary_markdown": "### 论文研究单位\nJD Explore Academy, JD Company\n北京交通大学 (Beijing Jiaotong University)\n### 论文概述\n本文提出了一种名为Object-Focus Actor (OFA)的新型数据高效方法，用于机器人的泛化灵巧操作。该方法利用在灵巧操作任务中观察到的轨迹一致性，允许高效的策略训练。OFA采用分层流程：物体感知和姿态估计、预操作姿态到达和OFA策略执行。通过在真实世界中的七个任务进行综合实验，证明OFA在位置和背景泛化测试中显著优于基线方法。值得注意的是，OFA仅用10次演示就能实现稳健的性能，突显了其数据效率。\n### 论文核心贡献点\n1. 提出了Object-Focus Actor (OFA)框架，通过关注物体最终的一致性轨迹，实现数据高效的泛化灵巧操作。\n2. 设计了分层处理流程，包括物体感知与姿态估计、预操作姿态到达和物体中心策略学习，解决了物体位置泛化的挑战。\n3. 引入了Hand-Focus图像和相对本体感觉表示，使策略能专注于核心操作区域，减少对背景的敏感性。\n4. 在真实世界的七个任务上验证了方法的有效性，证明了在位置和背景泛化上的优越性能，并展示了仅需10次演示的数据效率。\n### 论文方法描述\nOFA方法包含三个核心模块：\n1. **物体感知与姿态估计**：使用GroundingDINO进行物体定位，SAM进行分割，FoundationPose进行6D姿态估计，基于物体的CAD模型和单目图像。\n2. **预操作姿态到达**：根据物体类别设置统一的预操作姿态偏移（旋转和平移），通过CuRobo规划器生成无碰撞的运动轨迹，引导机械手到达该姿态。\n3. **物体中心策略学习**：\n - 构建Hand-Focus图像：通过机器人正向运动学计算手部在相机坐标系中的投影区域，放大至包含手部和物体，并裁剪为统一尺寸。\n - 采用相对本体感觉：包括相对于预操作姿态的姿态（位置和轴角）和手指关节角度。\n - 使用相对动作块：策略预测未来k步的相对动作序列，包含相对姿态和手指角度。\n - 模型架构：基于条件变分自编码器（CVAE），类似ACT，输入为Hand-Focus图像、相对本体感觉，输出为相对动作块。\n - 训练损失：包括重建损失和KL散度正则化项。\n### 论文使用数据集和训练资源\n- **数据集**：针对7个灵巧操作任务（如抓杯子、拿马克杯、持条码扫描器等）收集人类演示数据，每个任务使用30次演示。演示过程中物体位置随机放置，机械手初始位置固定。\n- **硬件平台**：Realman双臂平台，配备6自由度机械臂、Inspire RH56BFX灵巧手和头部ZED2立体相机。\n- **训练资源**：未明确提及，但通过变分自编码器架构和批量处理进行策略学习，训练使用相对动作块和Hand-Focus图像。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在真实物理环境中进行实验，使用双臂机器人平台和固定相机。\n- **评估指标**：\n - **成功率**：任务完成的百分比，在不同条件下评估（如标准测试、位置泛化、背景变化）。\n - **泛化测试**：\n - 位置泛化：物体放置于训练数据未见的区域，评估10次测试。\n - 背景泛化：改变环境背景，评估10次测试。\n - **数据效率分析**：比较不同演示数量（10、20、30次）下的性能，每个条件评估10次。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>JD Explore Academy, JD Company</p>\n<p>北京交通大学 (Beijing Jiaotong University)</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为Object-Focus Actor (OFA)的新型数据高效方法，用于机器人的泛化灵巧操作。该方法利用在灵巧操作任务中观察到的轨迹一致性，允许高效的策略训练。OFA采用分层流程：物体感知和姿态估计、预操作姿态到达和OFA策略执行。通过在真实世界中的七个任务进行综合实验，证明OFA在位置和背景泛化测试中显著优于基线方法。值得注意的是，OFA仅用10次演示就能实现稳健的性能，突显了其数据效率。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了Object-Focus Actor (OFA)框架，通过关注物体最终的一致性轨迹，实现数据高效的泛化灵巧操作。</li><li>设计了分层处理流程，包括物体感知与姿态估计、预操作姿态到达和物体中心策略学习，解决了物体位置泛化的挑战。</li><li>引入了Hand-Focus图像和相对本体感觉表示，使策略能专注于核心操作区域，减少对背景的敏感性。</li><li>在真实世界的七个任务上验证了方法的有效性，证明了在位置和背景泛化上的优越性能，并展示了仅需10次演示的数据效率。</li></ol>\n<h3>论文方法描述</h3>\n<p>OFA方法包含三个核心模块：</p>\n<ol><li><strong>物体感知与姿态估计</strong>：使用GroundingDINO进行物体定位，SAM进行分割，FoundationPose进行6D姿态估计，基于物体的CAD模型和单目图像。</li><li><strong>预操作姿态到达</strong>：根据物体类别设置统一的预操作姿态偏移（旋转和平移），通过CuRobo规划器生成无碰撞的运动轨迹，引导机械手到达该姿态。</li><li><strong>物体中心策略学习</strong>：</li></ol>\n<p> - 构建Hand-Focus图像：通过机器人正向运动学计算手部在相机坐标系中的投影区域，放大至包含手部和物体，并裁剪为统一尺寸。</p>\n<p> - 采用相对本体感觉：包括相对于预操作姿态的姿态（位置和轴角）和手指关节角度。</p>\n<p> - 使用相对动作块：策略预测未来k步的相对动作序列，包含相对姿态和手指角度。</p>\n<p> - 模型架构：基于条件变分自编码器（CVAE），类似ACT，输入为Hand-Focus图像、相对本体感觉，输出为相对动作块。</p>\n<p> - 训练损失：包括重建损失和KL散度正则化项。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：针对7个灵巧操作任务（如抓杯子、拿马克杯、持条码扫描器等）收集人类演示数据，每个任务使用30次演示。演示过程中物体位置随机放置，机械手初始位置固定。</li><li><strong>硬件平台</strong>：Realman双臂平台，配备6自由度机械臂、Inspire RH56BFX灵巧手和头部ZED2立体相机。</li><li><strong>训练资源</strong>：未明确提及，但通过变分自编码器架构和批量处理进行策略学习，训练使用相对动作块和Hand-Focus图像。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在真实物理环境中进行实验，使用双臂机器人平台和固定相机。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：任务完成的百分比，在不同条件下评估（如标准测试、位置泛化、背景变化）。</p>\n<p> - <strong>泛化测试</strong>：</p>\n<p> - 位置泛化：物体放置于训练数据未见的区域，评估10次测试。</p>\n<p> - 背景泛化：改变环境背景，评估10次测试。</p>\n<p> - <strong>数据效率分析</strong>：比较不同演示数量（10、20、30次）下的性能，每个条件评估10次。</p>"
  },
  {
    "date": "2025-05-20",
    "title": "AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory",
    "link": "http://arxiv.org/abs/2505.14030",
    "summary_markdown": "### 论文研究单位\n香港大学（HKU）、TeleAI、清华大学（THU）、上海交通大学（SJTU）、香港大学上海智能计算中心\n### 论文概述\nAutoBio 是一个针对数字生物实验室的机器人自动化模拟框架和基准测试。现有视觉-语言-动作（VLA）模型的基准测试主要聚焦于家庭任务，缺乏对专业科学领域的评估。生物实验室实验具有结构化协议、高精度要求和多模态交互的特点，但现有模拟器在处理流体、数字界面和实验室特有物理机制方面存在局限。AutoBio 通过仪器数字化、物理插件和渲染技术，构建了一个高保真度的生物实验模拟环境，并设计了一套涵盖基础生物学操作的基准任务，用于评估VLA模型在科学工作流中的表现。\n### 论文核心贡献点\n1. **生物实验室专用模拟器**：开发了仪器数字化流程、物理插件（螺纹/棘轮机制、准静态液体）和基于物理的渲染（PBR）支持透明材料与动态界面。\n2. **生物学基础任务基准**：提供16项任务，分三个难度级别，支持轨迹合成和VLA模型集成，用于标准化评估实验协议中的机器人操作。\n3. **科学导向环境的VLA评估**：系统测试了两个SOTA VLA模型（π₀和RDT），揭示了当前模型在精度操作、指令遵循和视觉推理方面的显著差距。\n### 论文方法描述\n**模拟器架构**：\n- **AutoBio Assets**：通过3D高斯泼溅（3DGS）结合CAD细化流程，将真实实验室仪器（如离心机、热循环仪）转换为可交互的数字资产。\n- **AutoBio Physics**：基于MuJoCo引擎，开发定制物理插件，包括螺纹机制（使用圆形螺旋线的SDF模拟）、棘轮机制（离散位置反馈）、偏心机制（振荡运动模拟）和准静态液体（平面界面简化）。\n- **AutoBio Rendering**：支持两种后端——基础渲染（快速OpenGL渲染）和高级渲染（Blender PBR实现透明材料与动态纹理渲染），后者确保容器和液体的视觉保真度。\n- **反应式界面**：动态纹理渲染实现仪器控制面板的实时交互反馈。\n\n**基准设计**：\n- **任务分级**：简单级（如关闭热循环仪盖）、中等难度（如旋开离心管盖）、困难级（如操作混匀仪面板），任务涵盖场景初始化、示范合成、状态检查和VLA接口。\n- **机器人配置**：支持单臂（Aloha）和双臂（UR5e搭配Robotiq夹爪或DexHand），提供多相机视角（全局和手腕相机）。\n### 论文使用数据集和训练资源\n- **数据集**：从9个任务中生成100条示范轨迹（50Hz），总计792k帧（约4.4小时连续数据），格式为LeRobot数据集，发布于HuggingFace。\n- **训练资源**：在NVIDIA H800 GPU上训练，批次大小32，30,000步，单次运行10-14小时，总计约1,500 GPU小时。\n### 论文使用的评估环境和评估指标\n- **评估环境**：AutoBio模拟器，包含16个任务，分三个难度级别（每级3个测试任务），环境支持随机化参数（如目标位置）。\n- **评估指标**：\n - **任务成功率**：二进制评分（成功=1，失败=0），适用于大部分任务。\n - **相对进度分数**：用于\"操作混匀仪面板\"任务，反映部分完成度。\n - **指标计算**：100次测试 episodes的平均成功率（%），报告均值及标准误（如99.7±0.3）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>香港大学（HKU）、TeleAI、清华大学（THU）、上海交通大学（SJTU）、香港大学上海智能计算中心</p>\n<h3>论文概述</h3>\n<p>AutoBio 是一个针对数字生物实验室的机器人自动化模拟框架和基准测试。现有视觉-语言-动作（VLA）模型的基准测试主要聚焦于家庭任务，缺乏对专业科学领域的评估。生物实验室实验具有结构化协议、高精度要求和多模态交互的特点，但现有模拟器在处理流体、数字界面和实验室特有物理机制方面存在局限。AutoBio 通过仪器数字化、物理插件和渲染技术，构建了一个高保真度的生物实验模拟环境，并设计了一套涵盖基础生物学操作的基准任务，用于评估VLA模型在科学工作流中的表现。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>生物实验室专用模拟器</strong>：开发了仪器数字化流程、物理插件（螺纹/棘轮机制、准静态液体）和基于物理的渲染（PBR）支持透明材料与动态界面。</li><li><strong>生物学基础任务基准</strong>：提供16项任务，分三个难度级别，支持轨迹合成和VLA模型集成，用于标准化评估实验协议中的机器人操作。</li><li><strong>科学导向环境的VLA评估</strong>：系统测试了两个SOTA VLA模型（π₀和RDT），揭示了当前模型在精度操作、指令遵循和视觉推理方面的显著差距。</li></ol>\n<h3>论文方法描述</h3>\n<p><strong>模拟器架构</strong>：</p>\n<ul><li><strong>AutoBio Assets</strong>：通过3D高斯泼溅（3DGS）结合CAD细化流程，将真实实验室仪器（如离心机、热循环仪）转换为可交互的数字资产。</li><li><strong>AutoBio Physics</strong>：基于MuJoCo引擎，开发定制物理插件，包括螺纹机制（使用圆形螺旋线的SDF模拟）、棘轮机制（离散位置反馈）、偏心机制（振荡运动模拟）和准静态液体（平面界面简化）。</li><li><strong>AutoBio Rendering</strong>：支持两种后端——基础渲染（快速OpenGL渲染）和高级渲染（Blender PBR实现透明材料与动态纹理渲染），后者确保容器和液体的视觉保真度。</li><li><strong>反应式界面</strong>：动态纹理渲染实现仪器控制面板的实时交互反馈。</li></ul>\n\n<p><strong>基准设计</strong>：</p>\n<ul><li><strong>任务分级</strong>：简单级（如关闭热循环仪盖）、中等难度（如旋开离心管盖）、困难级（如操作混匀仪面板），任务涵盖场景初始化、示范合成、状态检查和VLA接口。</li><li><strong>机器人配置</strong>：支持单臂（Aloha）和双臂（UR5e搭配Robotiq夹爪或DexHand），提供多相机视角（全局和手腕相机）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：从9个任务中生成100条示范轨迹（50Hz），总计792k帧（约4.4小时连续数据），格式为LeRobot数据集，发布于HuggingFace。</li><li><strong>训练资源</strong>：在NVIDIA H800 GPU上训练，批次大小32，30,000步，单次运行10-14小时，总计约1,500 GPU小时。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：AutoBio模拟器，包含16个任务，分三个难度级别（每级3个测试任务），环境支持随机化参数（如目标位置）。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>任务成功率</strong>：二进制评分（成功=1，失败=0），适用于大部分任务。</p>\n<p> - <strong>相对进度分数</strong>：用于\"操作混匀仪面板\"任务，反映部分完成度。</p>\n<p> - <strong>指标计算</strong>：100次测试 episodes的平均成功率（%），报告均值及标准误（如99.7±0.3）。</p>"
  },
  {
    "date": "2025-05-20",
    "title": "InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning",
    "link": "http://arxiv.org/abs/2505.13888",
    "summary_markdown": "### 论文研究单位\n西南交通大学、电子科技大学、同济大学\n### 论文概述\n本文提出了一种名为InSpire（Intrinsic Spatial Reasoning）的方法，旨在提升视觉-语言-动作模型（VLAs）的空间推理能力，以减少虚假相关性对模型泛化性能的负面影响。现有VLAs倾向于在任务无关的视觉特征与动作之间建立虚假相关性，限制了其泛化能力。InSpire通过在语言指令前添加一个关于目标物体相对于机器人方向的视觉问答（VQA）问题，引导模型关注任务相关因素，并将模型的答案与真实动作对齐，从而增强空间推理。该方法无需额外数据或与其他大型模型交互，可作为插件式模块增强现有自回归VLAs。\n### 论文核心贡献点\n1. 提出InSpire，一种缓解虚假相关性对VLAs泛化性能负面影响的新方法。\n2. 无需使用额外数据或与其他大型模型交互，以即插即用的方式赋予VLAs空间推理能力。\n3. 在模拟和真实世界环境中进行了全面评估，验证了InSpire的有效性和灵活性。\n### 论文方法描述\n1. InSpire通过在语言指令前添加问题“In which direction is the [object] relative to the robot?”，引导VLA进行空间推理。\n2. 模型首先生成该问题的文本答案（如right/left/up/down/front/back/grasped），该答案与问题共同构成任务相关因素的文本表示。\n3. 随后将此文本表示作为额外输入，传递给同一VLA以生成最终动作。\n4. 训练时，使用基于规则的自动化标注方法，通过机器人末端执行器与目标物体的3D位置差计算真实空间关系标签，对空间推理答案和动作同时施加自回归损失。\n### 论文使用数据集和训练资源\n1. 数据集：LIBERO（包含LIBERO-90、Spatial、Object、Goal、Long子集）、CALVIN（长期语言条件操作任务）、以及真实世界数据（10个已见任务和5个未见任务）。\n2. 训练资源：基于miniVLA-VQ和π₀-FAST模型进行实验，学习率1e-5，训练步数50,000，使用3个随机种子。真实世界实验使用AGILEX PiPER 6DOF机械臂，每个任务10条演示轨迹。\n### 论文使用的评估环境和评估指标\n1. 评估环境：模拟环境（LIBERO和CALVIN）、真实世界环境（AGILEX PiPER 6DOF机械臂）。\n2. 评估指标：成功率（Success Rate），其中LIBERO和CALVIN分别对每个任务进行100次和500次试验；真实世界任务每个任务进行10次试验。CALVIN额外评估顺序任务完成的平均数量。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>西南交通大学、电子科技大学、同济大学</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为InSpire（Intrinsic Spatial Reasoning）的方法，旨在提升视觉-语言-动作模型（VLAs）的空间推理能力，以减少虚假相关性对模型泛化性能的负面影响。现有VLAs倾向于在任务无关的视觉特征与动作之间建立虚假相关性，限制了其泛化能力。InSpire通过在语言指令前添加一个关于目标物体相对于机器人方向的视觉问答（VQA）问题，引导模型关注任务相关因素，并将模型的答案与真实动作对齐，从而增强空间推理。该方法无需额外数据或与其他大型模型交互，可作为插件式模块增强现有自回归VLAs。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出InSpire，一种缓解虚假相关性对VLAs泛化性能负面影响的新方法。</li><li>无需使用额外数据或与其他大型模型交互，以即插即用的方式赋予VLAs空间推理能力。</li><li>在模拟和真实世界环境中进行了全面评估，验证了InSpire的有效性和灵活性。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>InSpire通过在语言指令前添加问题“In which direction is the [object] relative to the robot?”，引导VLA进行空间推理。</li><li>模型首先生成该问题的文本答案（如right/left/up/down/front/back/grasped），该答案与问题共同构成任务相关因素的文本表示。</li><li>随后将此文本表示作为额外输入，传递给同一VLA以生成最终动作。</li><li>训练时，使用基于规则的自动化标注方法，通过机器人末端执行器与目标物体的3D位置差计算真实空间关系标签，对空间推理答案和动作同时施加自回归损失。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li>数据集：LIBERO（包含LIBERO-90、Spatial、Object、Goal、Long子集）、CALVIN（长期语言条件操作任务）、以及真实世界数据（10个已见任务和5个未见任务）。</li><li>训练资源：基于miniVLA-VQ和π₀-FAST模型进行实验，学习率1e-5，训练步数50,000，使用3个随机种子。真实世界实验使用AGILEX PiPER 6DOF机械臂，每个任务10条演示轨迹。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li>评估环境：模拟环境（LIBERO和CALVIN）、真实世界环境（AGILEX PiPER 6DOF机械臂）。</li><li>评估指标：成功率（Success Rate），其中LIBERO和CALVIN分别对每个任务进行100次和500次试验；真实世界任务每个任务进行10次试验。CALVIN额外评估顺序任务完成的平均数量。</li></ol>"
  },
  {
    "date": "2025-05-19",
    "title": "SPKLIP: Aligning Spike Video Streams with Natural Language",
    "link": "http://arxiv.org/abs/2505.12656",
    "summary_markdown": "# 论文研究单位\n北京大学 National Key Laboratory for Multimedia Information Processing, School of Computer Science; 中国科学院大学人工智能学院; 电子科技大学英才学院; 北京大学人工智能研究院\n# 论文概述\nSPKLIP是首个专为脉冲视频流与自然语言对齐设计的端到端框架。它通过多模态对比学习，直接从脉冲事件流中实现高速动态场景的语义理解，解决了传统CLIP等模型在处理稀疏、异步的脉冲数据时性能下降的问题。\n# 论文核心贡献点\n- 提出了一种新型的脉冲-视频-语言对齐架构，包含专为处理稀疏、异步事件流设计的分层脉冲特征提取器，并通过脉冲-文本对比学习直接对齐原始脉冲视频与文本。\n- 开发了节能的全脉冲设计，将SNN组件集成到流程中，展示了其在神经形态硬件上的能效潜力，并在一个新的真实世界脉冲视频数据集上验证了其泛化能力。\n- 通过全面的实验，证明SPKLIP在脉冲-视频-语言对齐任务上显著优于传统视觉语言模型。\n# 论文方法描述\n- **分层脉冲特征提取器**：包含多尺度时序滤波和空间注意模块。多尺度时序滤波通过并行分支处理不同时间尺度的特征，使用可学习的时序掩码动态加权；空间注意模块通过注意力机制增强关键时间步并抑制噪声。\n- **时空注意力残差网络**：使用MAPResNet进行分层特征提取，结合残差块和多头自注意力池化，然后通过Transformer编码器建模长期时间依赖关系，最后通过全局特征池化得到紧凑表示。\n- **脉冲-文本对比学习**：使用对比损失函数对齐脉冲视频特征和文本特征。\n# 论文使用数据集和训练资源\n- **数据集**：基准脉冲数据集和一个新贡献的真实世界脉冲视频数据集。\n- **训练资源**：在论文提供的HTML原文中未明确提及具体的硬件配置和训练时间等资源细节。\n# 论文使用的评估环境和评估指标\n- **评估环境**：在提供的HTML原文中未详细说明具体的硬件和软件环境。\n- **评估指标**：论文使用了对比学习常用的指标，如零样本分类准确率，并在文本到视频检索等下游任务上评估模型性能。具体指标名称在原文中未明确列出。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>北京大学 National Key Laboratory for Multimedia Information Processing, School of Computer Science; 中国科学院大学人工智能学院; 电子科技大学英才学院; 北京大学人工智能研究院</p>\n<h1>论文概述</h1>\n<p>SPKLIP是首个专为脉冲视频流与自然语言对齐设计的端到端框架。它通过多模态对比学习，直接从脉冲事件流中实现高速动态场景的语义理解，解决了传统CLIP等模型在处理稀疏、异步的脉冲数据时性能下降的问题。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出了一种新型的脉冲-视频-语言对齐架构，包含专为处理稀疏、异步事件流设计的分层脉冲特征提取器，并通过脉冲-文本对比学习直接对齐原始脉冲视频与文本。</li><li>开发了节能的全脉冲设计，将SNN组件集成到流程中，展示了其在神经形态硬件上的能效潜力，并在一个新的真实世界脉冲视频数据集上验证了其泛化能力。</li><li>通过全面的实验，证明SPKLIP在脉冲-视频-语言对齐任务上显著优于传统视觉语言模型。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li><strong>分层脉冲特征提取器</strong>：包含多尺度时序滤波和空间注意模块。多尺度时序滤波通过并行分支处理不同时间尺度的特征，使用可学习的时序掩码动态加权；空间注意模块通过注意力机制增强关键时间步并抑制噪声。</li><li><strong>时空注意力残差网络</strong>：使用MAPResNet进行分层特征提取，结合残差块和多头自注意力池化，然后通过Transformer编码器建模长期时间依赖关系，最后通过全局特征池化得到紧凑表示。</li><li><strong>脉冲-文本对比学习</strong>：使用对比损失函数对齐脉冲视频特征和文本特征。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>数据集</strong>：基准脉冲数据集和一个新贡献的真实世界脉冲视频数据集。</li><li><strong>训练资源</strong>：在论文提供的HTML原文中未明确提及具体的硬件配置和训练时间等资源细节。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>评估环境</strong>：在提供的HTML原文中未详细说明具体的硬件和软件环境。</li><li><strong>评估指标</strong>：论文使用了对比学习常用的指标，如零样本分类准确率，并在文本到视频检索等下游任务上评估模型性能。具体指标名称在原文中未明确列出。</li></ul>"
  },
  {
    "date": "2025-05-18",
    "title": "RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction",
    "link": "http://arxiv.org/abs/2505.12224",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-05-16",
    "title": "Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions",
    "link": "http://arxiv.org/abs/2505.11214",
    "summary_markdown": "## 研究单位\n西湖大学与浙江大学\n## 论文概述\n本文提出了一种名为OE-VLA的视觉-语言-行动（VLA）模型，旨在解决传统VLA模型仅接受语言指令的限制，扩展其对开放式多模态指令的处理能力。通过统一架构，模型能处理包括图像、视频和文本在内的自由形式人类指令，从而增强人机交互的自然性。实验表明，OE-VLA在语言指令任务上与传统模型性能相当，并在多种开放式任务中表现优异，显著扩大了VLA模型的应用范围。\n## 论文核心贡献点\n1. 提出了OE-VLA，一种新型VLA模型，通过统一神经架构处理多样化开放式多模态人类指令。\n2. 提出了一种通用方法，利用现有数据集构建包含自由形式多模态指令的机器人数据集，并采用两阶段课程学习算法将基础模型微调为支持交错模态的VLA模型。\n3. 引入了基于CALVIN套件的两个新基准OE-CALVIN_base和OE-CALVIN_hard，包含多样化的开放式指令，用于评估模型性能。\n## 论文方法描述\n模型架构基于LLaVA-Next-Interleave，包含三个组件：视觉编码器（使用SigLIP-400M处理图像）、LLM主干（Qwen-1.5，最大上下文长度32k）和动作分词器（将连续动作离散化为256个bin，用语言token表示）。训练数据构建采用转换方法：从现有语言指令数据集中，随机抽取样本并转换为多模态形式，包括视觉对象规范（VOS）、光学指令跟随（OIF）、视觉目标达成（VGR）和视频演示学习（VDL）。训练流程采用两阶段课程学习：第一阶段使用多图像定位数据集增强空间感知；第二阶段在构建的多模态机器人数据上微调模型，样本格式统一为((<obs>, (text1, <img1>, text2, <img2>), <act>)。\n## 论文使用数据集和训练资源\n使用CALVIN数据集及其衍生基准OE-CALVIN_base和OE-CALVIN_hard进行训练和评估，后者包含约1000个评估序列，指令类型覆盖多模态场景。训练资源方面，模型提供了0.5B和7B参数版本，所有模型训练一个epoch以确保公平比较；硬件资源未明确提及，但7B模型需大规模计算支持。\n## 论文使用的评估环境和评估指标\n评估环境基于CALVIN测试套件，该套件包含34项任务的1000个评估序列，用于测试语言条件策略学习。新增的OE-CALVIN基准进一步提供两种难度（base和hard），测试多模态指令下的性能。评估指标为成功序列长度（success sequence length），包括5步长任务的平均成功率（LH-1至LH-5）和整体序列长度，衡量模型在连续子任务中的执行能力。",
    "summary_html": "<h2>研究单位</h2>\n<p>西湖大学与浙江大学</p>\n<h2>论文概述</h2>\n<p>本文提出了一种名为OE-VLA的视觉-语言-行动（VLA）模型，旨在解决传统VLA模型仅接受语言指令的限制，扩展其对开放式多模态指令的处理能力。通过统一架构，模型能处理包括图像、视频和文本在内的自由形式人类指令，从而增强人机交互的自然性。实验表明，OE-VLA在语言指令任务上与传统模型性能相当，并在多种开放式任务中表现优异，显著扩大了VLA模型的应用范围。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出了OE-VLA，一种新型VLA模型，通过统一神经架构处理多样化开放式多模态人类指令。</li><li>提出了一种通用方法，利用现有数据集构建包含自由形式多模态指令的机器人数据集，并采用两阶段课程学习算法将基础模型微调为支持交错模态的VLA模型。</li><li>引入了基于CALVIN套件的两个新基准OE-CALVIN_base和OE-CALVIN_hard，包含多样化的开放式指令，用于评估模型性能。</li></ol>\n<h2>论文方法描述</h2>\n<p>模型架构基于LLaVA-Next-Interleave，包含三个组件：视觉编码器（使用SigLIP-400M处理图像）、LLM主干（Qwen-1.5，最大上下文长度32k）和动作分词器（将连续动作离散化为256个bin，用语言token表示）。训练数据构建采用转换方法：从现有语言指令数据集中，随机抽取样本并转换为多模态形式，包括视觉对象规范（VOS）、光学指令跟随（OIF）、视觉目标达成（VGR）和视频演示学习（VDL）。训练流程采用两阶段课程学习：第一阶段使用多图像定位数据集增强空间感知；第二阶段在构建的多模态机器人数据上微调模型，样本格式统一为((<obs>, (text1, <img1>, text2, <img2>), <act>)。</p>\n<h2>论文使用数据集和训练资源</h2>\n<p>使用CALVIN数据集及其衍生基准OE-CALVIN_base和OE-CALVIN_hard进行训练和评估，后者包含约1000个评估序列，指令类型覆盖多模态场景。训练资源方面，模型提供了0.5B和7B参数版本，所有模型训练一个epoch以确保公平比较；硬件资源未明确提及，但7B模型需大规模计算支持。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>评估环境基于CALVIN测试套件，该套件包含34项任务的1000个评估序列，用于测试语言条件策略学习。新增的OE-CALVIN基准进一步提供两种难度（base和hard），测试多模态指令下的性能。评估指标为成功序列长度（success sequence length），包括5步长任务的平均成功率（LH-1至LH-5）和整体序列长度，衡量模型在连续子任务中的执行能力。</p>"
  },
  {
    "date": "2025-05-16",
    "title": "Conditioning Matters: Training Diffusion Policies is Faster Than You Think",
    "link": "http://arxiv.org/abs/2505.11123",
    "summary_markdown": "论文研究单位\n- 天津大学\n- 清华大学\n- 华为诺亚方舟实验室\n\n论文概述\n该论文指出了在训练视觉-语言-动作（VLA）模型时，条件扩散策略存在的一个核心挑战：当生成条件难以区分时，训练目标会退化为建模边际动作分布，这一现象被称为“损失坍塌”。为解决此问题，论文提出了一种名为Cocos（condition-conditioned source distribution）的通用解决方案。该方法通过修改条件流匹配中的源分布，使其依赖于条件输入，具体做法是利用一个自编码器从条件中提取语义，并将源分布的中心锚定在该语义上，同时保持固定的标准差。Cocos鼓励模型更强地整合条件信息，从而防止损失坍塌，显著提升了训练效率和最终性能。\n\n论文核心贡献点\n- 构建了带生成条件的流匹配数学框架，并证明了当条件难以区分时，策略网络会主动忽略这些条件。\n- 提出了Cocos方法，这是一种简单且有效的源分布修改技术，可以防止损失坍塌，并显著提高扩散策略的训练效率和性能。\n- 建立了一个全面的评估基准，涵盖了从模拟（LIBERO, MetaWorld）到真实世界（低成本SO-100和高性能xArm机器人）的多种任务设置，验证了Cocos作为一种通用、即插即用解决方案的有效性。\n\n论文方法描述\nCocos的核心思想是替换标准条件流匹配中与条件无关的源分布（通常是标准高斯分布）。具体实现上，该方法使用一个视觉-语言自编码器来压缩条件（如观察图像和语言指令）的表示，将其作为均值来构建一个条件化的高斯源分布 q(z\\|c)，其中c代表条件输入，而标准差则保持固定。通过这种方式，扩散过程的起点本身就包含了条件的语义信息，强制策略网络在训练的每一步都必须利用条件，从而从根本上解决了条件被忽略和损失坍塌的问题。\n\n论文使用数据集和训练资源\n- 数据集:\n - 模拟环境：LIBERO和MetaWorld基准中的70个任务。\n - 真实世界环境：在开源低成本SO-100机器人平台上的10个任务，以及在高性能xArm机器人平台上的10个任务。\n- 训练资源:\n - 计算资源：使用配备了8块NVIDIA A100 GPU的服务器进行实验。\n\n论文使用的评估环境和评估指标\n- 评估环境:\n - 模拟环境：LIBERO和MetaWorld。\n - 真实世界环境：配备摄像头的SO-100和xArm机器人手臂。\n- 评估指标:\n - 任务成功率：在不同任务上成功完成任务的比率。\n - 训练效率/收敛速度：达到特定性能水平（如π₀性能）所需的梯度步数或训练时间。论文中特别使用了达到目标性能所需的时间倍数来衡量加速比。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>天津大学</li><li>清华大学</li><li>华为诺亚方舟实验室</li></ul>\n\n<p>论文概述</p>\n<p>该论文指出了在训练视觉-语言-动作（VLA）模型时，条件扩散策略存在的一个核心挑战：当生成条件难以区分时，训练目标会退化为建模边际动作分布，这一现象被称为“损失坍塌”。为解决此问题，论文提出了一种名为Cocos（condition-conditioned source distribution）的通用解决方案。该方法通过修改条件流匹配中的源分布，使其依赖于条件输入，具体做法是利用一个自编码器从条件中提取语义，并将源分布的中心锚定在该语义上，同时保持固定的标准差。Cocos鼓励模型更强地整合条件信息，从而防止损失坍塌，显著提升了训练效率和最终性能。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>构建了带生成条件的流匹配数学框架，并证明了当条件难以区分时，策略网络会主动忽略这些条件。</li><li>提出了Cocos方法，这是一种简单且有效的源分布修改技术，可以防止损失坍塌，并显著提高扩散策略的训练效率和性能。</li><li>建立了一个全面的评估基准，涵盖了从模拟（LIBERO, MetaWorld）到真实世界（低成本SO-100和高性能xArm机器人）的多种任务设置，验证了Cocos作为一种通用、即插即用解决方案的有效性。</li></ul>\n\n<p>论文方法描述</p>\n<p>Cocos的核心思想是替换标准条件流匹配中与条件无关的源分布（通常是标准高斯分布）。具体实现上，该方法使用一个视觉-语言自编码器来压缩条件（如观察图像和语言指令）的表示，将其作为均值来构建一个条件化的高斯源分布 q(z\\|c)，其中c代表条件输入，而标准差则保持固定。通过这种方式，扩散过程的起点本身就包含了条件的语义信息，强制策略网络在训练的每一步都必须利用条件，从而从根本上解决了条件被忽略和损失坍塌的问题。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集:</li></ul>\n<p> - 模拟环境：LIBERO和MetaWorld基准中的70个任务。</p>\n<p> - 真实世界环境：在开源低成本SO-100机器人平台上的10个任务，以及在高性能xArm机器人平台上的10个任务。</p>\n<ul><li>训练资源:</li></ul>\n<p> - 计算资源：使用配备了8块NVIDIA A100 GPU的服务器进行实验。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境:</li></ul>\n<p> - 模拟环境：LIBERO和MetaWorld。</p>\n<p> - 真实世界环境：配备摄像头的SO-100和xArm机器人手臂。</p>\n<ul><li>评估指标:</li></ul>\n<p> - 任务成功率：在不同任务上成功完成任务的比率。</p>\n<p> - 训练效率/收敛速度：达到特定性能水平（如π₀性能）所需的梯度步数或训练时间。论文中特别使用了达到目标性能所需的时间倍数来衡量加速比。</p>"
  },
  {
    "date": "2025-05-14",
    "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware",
    "link": "http://arxiv.org/abs/2505.09601",
    "summary_markdown": "# 论文研究单位\n- 加州大学伯克利分校（University of California, Berkeley）\n- 丰田研究院（Toyota Research Institute）\n# 论文概述\n论文提出Real2Render2Real (R2R2R)，一种无需动力学仿真或机器人硬件即可扩展机器人训练数据的新方法。该方法通过智能手机捕获的物体扫描和单个人类演示视频，重建3D物体几何和外观，跟踪物体6-DoF运动，渲染数千个高视觉保真度的机器人无关演示数据。物理实验表明，使用单个人类演示的R2R2R数据训练的模型性能可与使用150个人类遥操作演示训练的模型相匹配。\n# 论文核心贡献点\n1. 提出Real2Render2Real框架：仅使用智能手机捕获的视频（多视角物体扫描和人类演示视频）合成多样化的物理接地观测-动作对，无需动力学仿真或机器人硬件。\n2. 验证数据兼容性：证明生成数据与现代视觉-语言-动作（VLA）和模仿学习策略（包括基于Transformer和扩散的架构）兼容。\n3. 实验性能展示：在1,050次物理机器人评估中，使用单个演示的R2R2R生成数据训练的策略性能匹配150次遥操作演示训练的策略，同时显著减少数据生成时间。\n# 论文方法描述\n方法包含三个主要阶段：\n1. 实际到仿真资产和轨迹提取：从真实智能手机捕获中提取刚性或铰接物体的几何和部件轨迹。\n2. 增强：随机化物体初始化并在适当时插值物体运动轨迹。\n3. 并行渲染：使用IsaacLab生成多样化的照片级真实机器人执行，可根据GPU数量和内存扩展。\n\n关键技术细节：\n- 资产提取：使用3D高斯泼溅(3DGS)重建物体几何和外观，通过GARField分割场景为语义部件，将高斯组转换为带纹理的三角形网格。\n- 轨迹提取：通过4D微分部分建模(4D-DPM)从视频中提取物体和部件的6-DoF运动，扩展支持单/多刚性物体及铰接物体。\n- 轨迹多样性合成：通过空间归一化和球面线性插值(Slerp)将轨迹适应新起点/终点姿态，保留语义意图。\n- 抓取采样：从演示视频中估计3D手部关键点，确定物体-手部交互，使用抗对偶抓取采样器生成候选抓取轴。\n- 微分逆运动学：使用PyRoki求解平滑关节空间轨迹，诱导所需物体运动，不依赖对象动力学或物理交互仿真。\n- 环境渲染：应用域随机化（光照、相机外参、物体初始位姿），使用IsaacLab的GPU并行执行、DLSS和网格资产实例化实现高吞吐量渲染。\n# 论文使用数据集和训练资源\n- 输入数据：智能手机多视角物体扫描和单个人类演示视频。\n- 训练数据规模：生成50、100、150和1,000个渲染轨迹用于策略训练。\n- 训练资源：\n - 扩散策略：单张NVIDIA GH200训练约3小时（100k步）。\n - π₀-FAST：单张NVIDIA GH200微调约11小时（30k步，LoRA秩=16）。\n- 数据生成吞吐量：单张NVIDIA RTX 4090每分钟平均渲染51个演示，比人类遥操作（每分钟1.7个）快27倍。\n# 论文使用的评估环境和评估指标\n- 评估环境：\n - 物理机器人：ABB YuMi IRB14000双手机器人（π₀-FAST预训练时未见的机器人 embodiment）。\n - 任务场景：5个操作任务涵盖单物体抓取、多物体交互、铰接物体操作和双手机器协调。\n - 环境设置：桌面准静态操作环境，假设物体低镜面反射度。\n- 评估指标：\n - 任务成功率：二进制成功指标（任务完成为1，否则为0），每个策略每任务15次试验。\n - 性能对比：基于数据生成时间和成功率的对比分析。\n - 统计测试：使用TOST（双侧单侧检验）评估R2R2R与遥操作数据的等效性（±5%裕度）。\n- 额外分析：包括轨迹插值消融（移除插值导致显著性能下降）、背景增强影响（过度增强损害性能）和仿真-真实协同训练比较。",
    "summary_html": "<h1>论文研究单位</h1>\n<ul><li>加州大学伯克利分校（University of California, Berkeley）</li><li>丰田研究院（Toyota Research Institute）</li></ul>\n<h1>论文概述</h1>\n<p>论文提出Real2Render2Real (R2R2R)，一种无需动力学仿真或机器人硬件即可扩展机器人训练数据的新方法。该方法通过智能手机捕获的物体扫描和单个人类演示视频，重建3D物体几何和外观，跟踪物体6-DoF运动，渲染数千个高视觉保真度的机器人无关演示数据。物理实验表明，使用单个人类演示的R2R2R数据训练的模型性能可与使用150个人类遥操作演示训练的模型相匹配。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出Real2Render2Real框架：仅使用智能手机捕获的视频（多视角物体扫描和人类演示视频）合成多样化的物理接地观测-动作对，无需动力学仿真或机器人硬件。</li><li>验证数据兼容性：证明生成数据与现代视觉-语言-动作（VLA）和模仿学习策略（包括基于Transformer和扩散的架构）兼容。</li><li>实验性能展示：在1,050次物理机器人评估中，使用单个演示的R2R2R生成数据训练的策略性能匹配150次遥操作演示训练的策略，同时显著减少数据生成时间。</li></ol>\n<h1>论文方法描述</h1>\n<p>方法包含三个主要阶段：</p>\n<ol><li>实际到仿真资产和轨迹提取：从真实智能手机捕获中提取刚性或铰接物体的几何和部件轨迹。</li><li>增强：随机化物体初始化并在适当时插值物体运动轨迹。</li><li>并行渲染：使用IsaacLab生成多样化的照片级真实机器人执行，可根据GPU数量和内存扩展。</li></ol>\n\n<p>关键技术细节：</p>\n<ul><li>资产提取：使用3D高斯泼溅(3DGS)重建物体几何和外观，通过GARField分割场景为语义部件，将高斯组转换为带纹理的三角形网格。</li><li>轨迹提取：通过4D微分部分建模(4D-DPM)从视频中提取物体和部件的6-DoF运动，扩展支持单/多刚性物体及铰接物体。</li><li>轨迹多样性合成：通过空间归一化和球面线性插值(Slerp)将轨迹适应新起点/终点姿态，保留语义意图。</li><li>抓取采样：从演示视频中估计3D手部关键点，确定物体-手部交互，使用抗对偶抓取采样器生成候选抓取轴。</li><li>微分逆运动学：使用PyRoki求解平滑关节空间轨迹，诱导所需物体运动，不依赖对象动力学或物理交互仿真。</li><li>环境渲染：应用域随机化（光照、相机外参、物体初始位姿），使用IsaacLab的GPU并行执行、DLSS和网格资产实例化实现高吞吐量渲染。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>输入数据：智能手机多视角物体扫描和单个人类演示视频。</li><li>训练数据规模：生成50、100、150和1,000个渲染轨迹用于策略训练。</li><li>训练资源：</li></ul>\n<p> - 扩散策略：单张NVIDIA GH200训练约3小时（100k步）。</p>\n<p> - π₀-FAST：单张NVIDIA GH200微调约11小时（30k步，LoRA秩=16）。</p>\n<ul><li>数据生成吞吐量：单张NVIDIA RTX 4090每分钟平均渲染51个演示，比人类遥操作（每分钟1.7个）快27倍。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：</li></ul>\n<p> - 物理机器人：ABB YuMi IRB14000双手机器人（π₀-FAST预训练时未见的机器人 embodiment）。</p>\n<p> - 任务场景：5个操作任务涵盖单物体抓取、多物体交互、铰接物体操作和双手机器协调。</p>\n<p> - 环境设置：桌面准静态操作环境，假设物体低镜面反射度。</p>\n<ul><li>评估指标：</li></ul>\n<p> - 任务成功率：二进制成功指标（任务完成为1，否则为0），每个策略每任务15次试验。</p>\n<p> - 性能对比：基于数据生成时间和成功率的对比分析。</p>\n<p> - 统计测试：使用TOST（双侧单侧检验）评估R2R2R与遥操作数据的等效性（±5%裕度）。</p>\n<ul><li>额外分析：包括轨迹插值消融（移除插值导致显著性能下降）、背景增强影响（过度增强损害性能）和仿真-真实协同训练比较。</li></ul>"
  },
  {
    "date": "2025-05-14",
    "title": "VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation",
    "link": "http://arxiv.org/abs/2505.09577",
    "summary_markdown": "论文研究单位\n- 中国科学院自动化研究所多模态人工智能系统重点实验室\n- 三星北京研究院\n- 北京智源人工智能研究院\n\n论文概述\n- 本文提出了VTLA（Vision-Tactile-Language-Action）模型，这是一个用于接触密集型操作任务（如插入操作）的多模态框架。该模型通过跨模态语言基础有效整合视觉和触觉输入，以生成稳健的策略。研究者构建了一个低成本的模拟多模态数据集，包含专门针对指尖插入任务的视觉-触觉-动作-指令对。此外，引入了直接偏好优化（DPO）来提供类似回归的监督，以弥合基于分类的下一个令牌预测损失与连续机器人任务之间的差距。实验结果表明，VTLA模型在未见过的销形状上实现了超过90%的成功率，并在真实世界的销插入实验中展示了出色的Sim2Real性能。\n\n论文核心贡献点\n- 提出了VTLA模型，这是一个集成视觉、触觉和语言输入的机器人操作策略生成框架。\n- 设计了视觉引导的时间增强令牌（VGTE），通过强调视觉先验并在标记化前融入时间融合，以解决VLM在视觉-触觉操作中的时间推理限制。\n- 引入了偏好学习（使用DPO）到VTLA中，以减轻对真实动作的过拟合，并通过模拟回归监督增强泛化能力。\n- 在真实世界插入实验中证明VTLA优于现有方法，展示了所设计模块的有效性。\n\n论文方法描述\n- VTLA模型基于视觉-语言模型（VLM）进行构建，通过两阶段训练流程进行优化。在第一阶段，使用模拟数据收集的视觉-触觉-动作-指令对进行监督微调（SFT），采用下一个令牌预测（NTP）损失。在第二阶段，应用直接偏好优化（DPO），通过比较模型生成的动作与真实动作的接近程度，创建偏好数据集，并优化模型以偏好更接近真实动作的响应。为了解决VLM在时间理解上的局限性，提出了视觉引导的时间增强令牌（VGTE），该方法在标记化前强调视觉先验并增强时间融合，从而提升跨模态时间推理能力。\n\n论文使用数据集和训练资源\n- 数据集：在NVIDIA Isaac Gym中构建的模拟多模态数据集，包含28,000个销插入样本，覆盖5种不同的销孔形状和0.6至2.0毫米的装配间隙。数据集包含视觉图像、左右触觉图像序列（2x2网格格式）、动作标签和文本指令。使用领域随机化技术增强Sim2Real泛化能力。\n- 训练资源：使用LlamaFactory框架进行训练。在SFT阶段，基础模型为Qwen2-VL 7B，学习率为5e-4，批量大小为64，训练10个周期。在DPO阶段，学习率为5e-6，批量大小为32，训练3个周期。视觉编码器和模态适配器参数被冻结，仅微调语言模型。\n\n论文使用的评估环境和评估指标\n- 评估环境：在模拟环境（Isaac Gym）中测试不同间隙和形状的销插入任务；真实世界实验使用配备Robotiq 2F-85夹爪的UR3机器人，手腕安装Intel RealSense D405相机捕获视觉，指尖使用GelStereo 2.0传感器获取触觉数据。\n- 评估指标：数据集评估使用目标收敛率（GCR，所有方向动作正确的百分比）和L1距离（x、y、rz方向）。任务评估使用成功率（插入成功的百分比）和平均尝试步数（完成插入的平均步数）。真实世界评估也记录成功率和步数。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>中国科学院自动化研究所多模态人工智能系统重点实验室</li><li>三星北京研究院</li><li>北京智源人工智能研究院</li></ul>\n\n<p>论文概述</p>\n<ul><li>本文提出了VTLA（Vision-Tactile-Language-Action）模型，这是一个用于接触密集型操作任务（如插入操作）的多模态框架。该模型通过跨模态语言基础有效整合视觉和触觉输入，以生成稳健的策略。研究者构建了一个低成本的模拟多模态数据集，包含专门针对指尖插入任务的视觉-触觉-动作-指令对。此外，引入了直接偏好优化（DPO）来提供类似回归的监督，以弥合基于分类的下一个令牌预测损失与连续机器人任务之间的差距。实验结果表明，VTLA模型在未见过的销形状上实现了超过90%的成功率，并在真实世界的销插入实验中展示了出色的Sim2Real性能。</li></ul>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了VTLA模型，这是一个集成视觉、触觉和语言输入的机器人操作策略生成框架。</li><li>设计了视觉引导的时间增强令牌（VGTE），通过强调视觉先验并在标记化前融入时间融合，以解决VLM在视觉-触觉操作中的时间推理限制。</li><li>引入了偏好学习（使用DPO）到VTLA中，以减轻对真实动作的过拟合，并通过模拟回归监督增强泛化能力。</li><li>在真实世界插入实验中证明VTLA优于现有方法，展示了所设计模块的有效性。</li></ul>\n\n<p>论文方法描述</p>\n<ul><li>VTLA模型基于视觉-语言模型（VLM）进行构建，通过两阶段训练流程进行优化。在第一阶段，使用模拟数据收集的视觉-触觉-动作-指令对进行监督微调（SFT），采用下一个令牌预测（NTP）损失。在第二阶段，应用直接偏好优化（DPO），通过比较模型生成的动作与真实动作的接近程度，创建偏好数据集，并优化模型以偏好更接近真实动作的响应。为了解决VLM在时间理解上的局限性，提出了视觉引导的时间增强令牌（VGTE），该方法在标记化前强调视觉先验并增强时间融合，从而提升跨模态时间推理能力。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：在NVIDIA Isaac Gym中构建的模拟多模态数据集，包含28,000个销插入样本，覆盖5种不同的销孔形状和0.6至2.0毫米的装配间隙。数据集包含视觉图像、左右触觉图像序列（2x2网格格式）、动作标签和文本指令。使用领域随机化技术增强Sim2Real泛化能力。</li><li>训练资源：使用LlamaFactory框架进行训练。在SFT阶段，基础模型为Qwen2-VL 7B，学习率为5e-4，批量大小为64，训练10个周期。在DPO阶段，学习率为5e-6，批量大小为32，训练3个周期。视觉编码器和模态适配器参数被冻结，仅微调语言模型。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：在模拟环境（Isaac Gym）中测试不同间隙和形状的销插入任务；真实世界实验使用配备Robotiq 2F-85夹爪的UR3机器人，手腕安装Intel RealSense D405相机捕获视觉，指尖使用GelStereo 2.0传感器获取触觉数据。</li><li>评估指标：数据集评估使用目标收敛率（GCR，所有方向动作正确的百分比）和L1距离（x、y、rz方向）。任务评估使用成功率（插入成功的百分比）和平均尝试步数（完成插入的平均步数）。真实世界评估也记录成功率和步数。</li></ul>"
  },
  {
    "date": "2025-05-13",
    "title": "From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2505.08548",
    "summary_markdown": "### 论文研究单位\n天津大学\n### 论文概述\n论文提出FSD（From Seeing to Doing）模型，通过空间关系推理生成中间视觉表示（如空间affordance boxes/points和visual traces），解决机器人操作中的泛化问题，克服现有VLA模型因数据稀缺性和异质性导致的零样本性能限制。\n### 论文核心贡献点\n1. 提出FSD模型，通过空间关系聚焦的视觉思维链（SrCoT）生成视觉辅助，实现推理与决策的桥接。\n2. 设计弱到强的分层数据构建管道，结合自我一致性机制对齐空间理解与生成。\n3. 构建VABench基准，用于评估复杂场景下的视觉辅助生成能力。\n4. 在8个空间推理基准和真实机器人任务中实现SOTA性能，零样本成功率显著提升。\n### 论文方法描述\n1. **视觉辅助定义**：使用三种中间表示：空间affordance boxes（目标区域）、spatial affordance points（精确点）和object-centric visual traces（操作轨迹序列），所有坐标归一化到[0, 1000]空间。\n2. **SrCoT机制**：分两阶段推理：描述阶段构建物体为中心的空间关系图，推理阶段基于该图逐步推导坐标，绑定对象与位置。\n3. **数据构建**：五级能力分层（区域定位→空间关系→空间推理→affordance生成→视觉轨迹生成），自动从BridgeDataV2等数据集生成300K样本。\n4. **自一致性对齐**：双向任务设计（图像→轨迹 vs. 轨迹→指令），增强坐标与视觉信号对齐。\n### 论文使用数据集和训练资源\n1. **数据集**：BridgeDataV2、RT-X、Droid等机器人数据集，结合通用VQA数据，自动标注空间关系和轨迹。\n2. **训练资源**：基于LLaVA-1.5架构，使用CLIP-ViT-L图像编码器和Vicuna-13B LLM，冻结视觉编码器，微调投影层和LLM，混合1.4M样本训练。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：SimplerEnv仿真环境（WidowX机器人）和真实xArm机器人平台。\n2. **评估指标**：\n - 空间推理基准：CVBench等5个数据集的18个子任务平均准确率。\n - 物体/区域参考：RoboRefIt和Where2Place的点在目标区域比例。\n - VABench：VABench-Point的准确率，VABench-VisualTrace的RMSE/MAE和GPT Score。\n - 机器人任务：零样本操作成功率（SimplerEnv和8个真实任务）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>天津大学</p>\n<h3>论文概述</h3>\n<p>论文提出FSD（From Seeing to Doing）模型，通过空间关系推理生成中间视觉表示（如空间affordance boxes/points和visual traces），解决机器人操作中的泛化问题，克服现有VLA模型因数据稀缺性和异质性导致的零样本性能限制。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出FSD模型，通过空间关系聚焦的视觉思维链（SrCoT）生成视觉辅助，实现推理与决策的桥接。</li><li>设计弱到强的分层数据构建管道，结合自我一致性机制对齐空间理解与生成。</li><li>构建VABench基准，用于评估复杂场景下的视觉辅助生成能力。</li><li>在8个空间推理基准和真实机器人任务中实现SOTA性能，零样本成功率显著提升。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>视觉辅助定义</strong>：使用三种中间表示：空间affordance boxes（目标区域）、spatial affordance points（精确点）和object-centric visual traces（操作轨迹序列），所有坐标归一化到[0, 1000]空间。</li><li><strong>SrCoT机制</strong>：分两阶段推理：描述阶段构建物体为中心的空间关系图，推理阶段基于该图逐步推导坐标，绑定对象与位置。</li><li><strong>数据构建</strong>：五级能力分层（区域定位→空间关系→空间推理→affordance生成→视觉轨迹生成），自动从BridgeDataV2等数据集生成300K样本。</li><li><strong>自一致性对齐</strong>：双向任务设计（图像→轨迹 vs. 轨迹→指令），增强坐标与视觉信号对齐。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：BridgeDataV2、RT-X、Droid等机器人数据集，结合通用VQA数据，自动标注空间关系和轨迹。</li><li><strong>训练资源</strong>：基于LLaVA-1.5架构，使用CLIP-ViT-L图像编码器和Vicuna-13B LLM，冻结视觉编码器，微调投影层和LLM，混合1.4M样本训练。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：SimplerEnv仿真环境（WidowX机器人）和真实xArm机器人平台。</li><li><strong>评估指标</strong>：</li></ol>\n<p> - 空间推理基准：CVBench等5个数据集的18个子任务平均准确率。</p>\n<p> - 物体/区域参考：RoboRefIt和Where2Place的点在目标区域比例。</p>\n<p> - VABench：VABench-Point的准确率，VABench-VisualTrace的RMSE/MAE和GPT Score。</p>\n<p> - 机器人任务：零样本操作成功率（SimplerEnv和8个真实任务）。</p>"
  },
  {
    "date": "2025-05-13",
    "title": "Training Strategies for Efficient Embodied Reasoning",
    "link": "http://arxiv.org/abs/2505.08243",
    "summary_markdown": "```markdown\n# 论文研究单位\nUC Berkeley, Stanford University, Physical Intelligence\n# 论文概述\n本文研究了机器人链式思维（ECoT）推理为何能提升视觉-语言-动作模型（VLA）的性能，并提出轻量级训练策略ECoT-Lite。传统ECoT虽能提升泛化能力，但存在推理数据标注成本高和推理速度慢（仅1-1.2Hz）的问题。通过实证分析，论文揭示了ECoT有效的核心机制，并设计出无需生成中间推理即可获得高性能且实现3倍推理加速的方案。\n# 论文核心贡献点\n1. 提出\"推理改进表示学习\"的核心机制，通过预训练或推理Dropout策略提升VLA内部表示质量。\n2. 开发四种轻量级训练方法：推理预训练/联合训练、推理Dropout、推理脚手架和思考令牌。\n3. 在LIBERO-90基准测试中达到SOTA（90.8%成功率），在BridgeData V2上超越标准VLA 10-19%，同时实现3倍推理加速。\n4. 揭示模型表达能力（Hyp.3）并非主要瓶颈，语义推理才是关键。\n# 论文方法描述\n1. **推理预训练/联合训练**：先在推理数据上预训练VLM，再微调动作预测（Hyp.1）。\n2. **推理Dropout**：训练中随机丢弃推理步骤，迫使模型直接预测动作（Hyp.1）。\n3. **推理脚手架**：训练时提供推理示例作为上下文，但不计算损失（Hyp.2）。\n4. **思考令牌**：添加无语义令牌增加计算量（Hyp.3），实验证明无效。\n# 论文使用数据集和训练资源\n- **数据集**：LIBERO-90（90项模拟操作任务）、BridgeData V2（真实机器人操作任务）。\n- **推理数据生成**：通过基础模型（如GPT-4V）标注轨迹的中间步骤（子任务分解、物体定位等）。\n- **训练资源**：NVIDIA H100 GPU，使用TensorRT-LLM FP8编译优化，训练步数与基线模型相当。\n# 论文使用的评估环境和评估指标\n- **仿真环境**：LIBERO-90标准测试及两个泛化变体（物体位置扰动+干扰物、空间关系泛化）。\n- **真实环境**：BridgeData V2的泛化评估，包含运动泛化、空间关系、未见物体等维度。\n- **评估指标**：任务成功率（50次试验/任务的平均值及标准误差），控制频率作为推理速度指标。\n- **关键结果**：ECoT-Lite在LIBERO-90达90.8%成功，比标准VLA高8.8%；推理速度达3.5Hz+（标准ECoT为1-1.2Hz）。\n```",
    "summary_html": "<p>```markdown</p>\n<h1>论文研究单位</h1>\n<p>UC Berkeley, Stanford University, Physical Intelligence</p>\n<h1>论文概述</h1>\n<p>本文研究了机器人链式思维（ECoT）推理为何能提升视觉-语言-动作模型（VLA）的性能，并提出轻量级训练策略ECoT-Lite。传统ECoT虽能提升泛化能力，但存在推理数据标注成本高和推理速度慢（仅1-1.2Hz）的问题。通过实证分析，论文揭示了ECoT有效的核心机制，并设计出无需生成中间推理即可获得高性能且实现3倍推理加速的方案。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出\"推理改进表示学习\"的核心机制，通过预训练或推理Dropout策略提升VLA内部表示质量。</li><li>开发四种轻量级训练方法：推理预训练/联合训练、推理Dropout、推理脚手架和思考令牌。</li><li>在LIBERO-90基准测试中达到SOTA（90.8%成功率），在BridgeData V2上超越标准VLA 10-19%，同时实现3倍推理加速。</li><li>揭示模型表达能力（Hyp.3）并非主要瓶颈，语义推理才是关键。</li></ol>\n<h1>论文方法描述</h1>\n<ol><li><strong>推理预训练/联合训练</strong>：先在推理数据上预训练VLM，再微调动作预测（Hyp.1）。</li><li><strong>推理Dropout</strong>：训练中随机丢弃推理步骤，迫使模型直接预测动作（Hyp.1）。</li><li><strong>推理脚手架</strong>：训练时提供推理示例作为上下文，但不计算损失（Hyp.2）。</li><li><strong>思考令牌</strong>：添加无语义令牌增加计算量（Hyp.3），实验证明无效。</li></ol>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>数据集</strong>：LIBERO-90（90项模拟操作任务）、BridgeData V2（真实机器人操作任务）。</li><li><strong>推理数据生成</strong>：通过基础模型（如GPT-4V）标注轨迹的中间步骤（子任务分解、物体定位等）。</li><li><strong>训练资源</strong>：NVIDIA H100 GPU，使用TensorRT-LLM FP8编译优化，训练步数与基线模型相当。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>仿真环境</strong>：LIBERO-90标准测试及两个泛化变体（物体位置扰动+干扰物、空间关系泛化）。</li><li><strong>真实环境</strong>：BridgeData V2的泛化评估，包含运动泛化、空间关系、未见物体等维度。</li><li><strong>评估指标</strong>：任务成功率（50次试验/任务的平均值及标准误差），控制频率作为推理速度指标。</li><li><strong>关键结果</strong>：ECoT-Lite在LIBERO-90达90.8%成功，比标准VLA高8.8%；推理速度达3.5Hz+（标准ECoT为1-1.2Hz）。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2025-05-12",
    "title": "ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning",
    "link": "http://arxiv.org/abs/2505.07395",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n论文作者为 Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang。未在提供的HTML原文中明确提及具体研究单位。\n## 论文概述\n本文提出了ReinboT（Reinforced robot GPT），一种新颖的端到端视觉-语言-动作（VLA）模型，旨在通过集成强化学习（RL）的最大化累积回报原理来增强机器人视觉语言操作能力。ReinboT通过预测密集回报来深入理解数据质量分布，使机器人能够生成更稳健、面向未来利益最大化的决策行为。在CALVIN混合质量数据集上的实验表明，ReinboT实现了最先进的性能，并在真实世界的任务中表现出卓越的少样本学习和分布外泛化能力。\n## 论文核心贡献点\n- 提出了ReinboT，一种新颖的端到端VLA模型，集成了RL回报最大化以增强机器人操作能力。\n- 引入了一种奖励密集化方法，使ReinboT能够深入理解数据质量以进行更稳健的学习。\n- 通过广泛的实验证明了ReinboT的最先进性能，显著优于模拟和真实世界任务中的基线模型。\n## 论文方法描述\nReinboT方法的核心是奖励密集化和端到端强化VLA模型设计。\n1. **奖励密集化**：将长时程操作轨迹自动分解为多个包含单一子目标的轨迹段，并设计一个包含四个组件的密集奖励来捕捉操作任务特性：\n - 子目标达成（r1）：通过MSE、SSIM和ORB算法计算当前状态与子目标状态的差异。\n - 任务进展（r2）：基于子目标序列在整体轨迹中的位置分配权重，越接近最终目标的序列权重越高。\n - 行为平滑性（r3）：惩罚过大的关节速度、加速度和动作变化，促进平滑自然的运动。\n - 任务完成（r4）：若轨迹成功完成指令则给予奖励。\n2. **端到端强化VLA模型**：基于GPT风格的Transformer，引入了三种预测标记嵌入（[RTG]、[ACTION]和[IMAGE]）来分别预测ReturnToGo、机器人动作和未来图像状态。模型使用CLIP编码语言指令，ViT和perceiver resampler处理图像状态，MLP处理本体感觉。训练时，利用期望分位数回归损失函数预测最大化回报，使模型在推理时能预测最大回报以指导执行更优的动作。\n## 论文使用数据集和训练资源\n- **数据集**：主要在CALVIN混合质量数据集上进行评估。\n- **训练资源**：未在提供的HTML原文中明确提及具体的计算资源（如GPU类型、数量、训练时长等）。\n## 论文使用的评估环境和评估指标\n- **评估环境**：在模拟环境（CALVIN数据集）和真实世界任务中进行评估。\n- **评估指标**：\n - 在CALVIN混合质量数据集上的任务成功率。\n - 少样本学习能力和分布外（OOD）泛化能力的评估。\n - 真实世界任务的成功率和性能表现。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>论文作者为 Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, Donglin Wang。未在提供的HTML原文中明确提及具体研究单位。</p>\n<h2>论文概述</h2>\n<p>本文提出了ReinboT（Reinforced robot GPT），一种新颖的端到端视觉-语言-动作（VLA）模型，旨在通过集成强化学习（RL）的最大化累积回报原理来增强机器人视觉语言操作能力。ReinboT通过预测密集回报来深入理解数据质量分布，使机器人能够生成更稳健、面向未来利益最大化的决策行为。在CALVIN混合质量数据集上的实验表明，ReinboT实现了最先进的性能，并在真实世界的任务中表现出卓越的少样本学习和分布外泛化能力。</p>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了ReinboT，一种新颖的端到端VLA模型，集成了RL回报最大化以增强机器人操作能力。</li><li>引入了一种奖励密集化方法，使ReinboT能够深入理解数据质量以进行更稳健的学习。</li><li>通过广泛的实验证明了ReinboT的最先进性能，显著优于模拟和真实世界任务中的基线模型。</li></ul>\n<h2>论文方法描述</h2>\n<p>ReinboT方法的核心是奖励密集化和端到端强化VLA模型设计。</p>\n<p>1. <strong>奖励密集化</strong>：将长时程操作轨迹自动分解为多个包含单一子目标的轨迹段，并设计一个包含四个组件的密集奖励来捕捉操作任务特性：</p>\n<p> - 子目标达成（r1）：通过MSE、SSIM和ORB算法计算当前状态与子目标状态的差异。</p>\n<p> - 任务进展（r2）：基于子目标序列在整体轨迹中的位置分配权重，越接近最终目标的序列权重越高。</p>\n<p> - 行为平滑性（r3）：惩罚过大的关节速度、加速度和动作变化，促进平滑自然的运动。</p>\n<p> - 任务完成（r4）：若轨迹成功完成指令则给予奖励。</p>\n<p>2. <strong>端到端强化VLA模型</strong>：基于GPT风格的Transformer，引入了三种预测标记嵌入（[RTG]、[ACTION]和[IMAGE]）来分别预测ReturnToGo、机器人动作和未来图像状态。模型使用CLIP编码语言指令，ViT和perceiver resampler处理图像状态，MLP处理本体感觉。训练时，利用期望分位数回归损失函数预测最大化回报，使模型在推理时能预测最大回报以指导执行更优的动作。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>：主要在CALVIN混合质量数据集上进行评估。</li><li><strong>训练资源</strong>：未在提供的HTML原文中明确提及具体的计算资源（如GPU类型、数量、训练时长等）。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：在模拟环境（CALVIN数据集）和真实世界任务中进行评估。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 在CALVIN混合质量数据集上的任务成功率。</p>\n<p> - 少样本学习能力和分布外（OOD）泛化能力的评估。</p>\n<p> - 真实世界任务的成功率和性能表现。</p>"
  },
  {
    "date": "2025-05-09",
    "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
    "link": "http://arxiv.org/abs/2505.06111",
    "summary_markdown": "### 论文研究单位\n论文作者来自多个机构，包括北京大学的1,2单位，以及北京大学的其他合作单位，例如第3单位。\n### 论文概述\n论文提出了UniVLA，一个用于学习跨具身视觉-语言-动作（VLA）策略的新框架。核心创新是通过一个潜在动作模型从视频中导出任务中心的动作表示，从而能够利用跨越广泛具身和视角的庞大数据。论文通过结合语言指令并在DINO特征空间中建立潜在动作模型，来减轻任务无关动态的影响。学习自互联网规模视频的通用策略可以通过高效的潜在动作解码部署到各种机器人。论文在多个操作和导航基准测试以及真实机器人部署中取得了最先进的结果。UniVLA在计算成本不到OpenVLA的1/20，下游数据不到1/10的情况下，取得了优于OpenVLA的性能。\n### 论文核心贡献点\n1. 提出了UniVLA，一个通过在统一的、具身不可知的动作空间中规划来实现通用策略的框架，能够通过从网络规模视频中学习来实现可扩展和高效的决策。\n2. 引入了一种从跨具身视频中提取任务相关潜在动作的新方法，将任务中心动态与无关的视觉变化解耦。定性和定量实验突出了其优点和相对于现有工作的优势。\n3. UniVLA在多个基准测试和真实机器人测试中取得了最先进的性能，在LIBERO基准上成功率比OpenVLA提高18.5%，导航任务中提高29.6%，真实世界部署中提高36.7%。\n### 论文方法描述\nUniVLA方法分为三个阶段：\n1. **任务中心潜在动作学习**：通过语言指令引导，从跨具身视频中无监督地提取任务相关的动作表示。使用VQ-VAE对连续帧对的逆动态进行离散化，得到量化的潜在动作。在DINOv2特征空间中操作以过滤视觉噪声，并解耦出任务中心动态。\n2. **通用策略预训练**：基于Prismatic-7B视觉语言模型，通过自回归方式预测离散化的潜在动作标记，训练一个具身不可知的通用策略。\n3. **部署后训练**：引入轻量级动作解码器（约10.8M参数），将潜在动作解码为可执行轨迹，实现对新机器人的高效适配。同时利用历史潜在动作输出作为提示输入，增强长时程任务表现。\n### 论文使用数据集和训练资源\n**数据集**：\n- 预训练：Open X-Embodiment（操作数据）、GNM（导航数据）和Ego4D（人类视频）。\n- 评估：LIBERO（操作）、CALVIN（操作）、SimplerEnv（操作）、Room2Room（导航）以及真实世界机器人任务。\n**训练资源**：\n- 预训练：960 A100小时，显著低于OpenVLA的21,500 A100小时。\n- 后训练：使用LoRA进行参数高效微调，新增约123M可训练参数。\n- 推理：在NVIDIA RTX 4090 GPU上实现10Hz实时闭环推理。\n### 论文使用的评估环境和评估指标\n**评估环境**：\n- 模拟环境：LIBERO、CALVIN、SimplerEnv和VLN-CE的Room2Room（R2R）。\n- 真实环境：配备7-DoF机械臂和Orbecc DABAI相机的Piper机器人。\n**评估指标**：\n- 操作任务：成功率（Success Rate）。\n- 导航任务：Oracle成功率（在VLN-CE中，代理到达目标3米内视为成功）。\n- 真实任务：成功率（Success Rate）和逐步得分（Step-wise Score，每个任务最高3分）。\n- 通用性评估：光照变化、视觉干扰物、新物体场景下的成功率。</think>\n### 论文研究单位\n论文作者来自多个机构，包括北京大学的1,2单位，以及北京大学的其他合作单位，例如第3单位。\n### 论文概述\n论文提出了UniVLA，一个用于学习跨具身视觉-语言-动作（VLA）策略的新框架。核心创新是通过一个潜在动作模型从视频中导出任务中心的动作表示，从而能够利用跨越广泛具身和视角的庞大数据。论文通过结合语言指令并在DINO特征空间中建立潜在动作模型，来减轻任务无关动态的影响。学习自互联网规模视频的通用策略可以通过高效的潜在动作解码部署到各种机器人。论文在多个操作和导航基准测试以及真实机器人部署中取得了最先进的结果。UniVLA在计算成本不到OpenVLA的1/20，下游数据不到1/10的情况下，取得了优于OpenVLA的性能。\n### 论文核心贡献点\n1. 提出了UniVLA，一个通过在统一的、具身不可知的动作空间中规划来实现通用策略的框架，能够通过从网络规模视频中学习来实现可扩展和高效的决策。\n2. 引入了一种从跨具身视频中提取任务相关潜在动作的新方法，将任务中心动态与无关的视觉变化解耦。定性和定量实验突出了其优点和相对于现有工作的优势。\n3. UniVLA在多个基准测试和真实机器人测试中取得了最先进的性能，在LIBERO基准上成功率比OpenVLA提高18.5%，导航任务中提高29.6%，真实世界部署中提高36.7%。\n### 论文方法描述\nUniVLA方法分为三个阶段：\n1. **任务中心潜在动作学习**：通过语言指令引导，从跨具身视频中无监督地提取任务相关的动作表示。使用VQ-VAE对连续帧对的逆动态进行离散化，得到量化的潜在动作。在DINOv2特征空间中操作以过滤视觉噪声，并解耦出任务中心动态。\n2. **通用策略预训练**：基于Prismatic-7B视觉语言模型，通过自回归方式预测离散化的潜在动作标记，训练一个具身不可知的通用策略。\n3. **部署后训练**：引入轻量级动作解码器（约10.8M参数），将潜在动作解码为可执行轨迹，实现对新机器人的高效适配。同时利用历史潜在动作输出作为提示输入，增强长时程任务表现。\n### 论文使用数据集和训练资源\n**数据集**：\n- 预训练：Open X-Embodiment（操作数据）、GNM（导航数据）和Ego4D（人类视频）。\n- 评估：LIBERO（操作）、CALVIN（操作）、SimplerEnv（操作）、Room2Room（导航）以及真实世界机器人任务。\n**训练资源**：\n- 预训练：960 A100小时，显著低于OpenVLA的21,500 A100小时。\n- 后训练：使用LoRA进行参数高效微调，新增约123M可训练参数。\n- 推理：在NVIDIA RTX 4090 GPU上实现10Hz实时闭环推理。\n### 论文使用的评估环境和评估指标\n**评估环境**：\n- 模拟环境：LIBERO、CALVIN、SimplerEnv和VLN-CE的Room2Room（R2R）。\n- 真实环境：配备7-DoF机械臂和Orbecc DABAI相机的Piper机器人。\n**评估指标**：\n- 操作任务：成功率（Success Rate）。\n- 导航任务：Oracle成功率（在VLN-CE中，代理到达目标3米内视为成功）。\n- 真实任务：成功率（Success Rate）和逐步得分（Step-wise Score，每个任务最高3分）。\n- 通用性评估：光照变化、视觉干扰物、新物体场景下的成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>论文作者来自多个机构，包括北京大学的1,2单位，以及北京大学的其他合作单位，例如第3单位。</p>\n<h3>论文概述</h3>\n<p>论文提出了UniVLA，一个用于学习跨具身视觉-语言-动作（VLA）策略的新框架。核心创新是通过一个潜在动作模型从视频中导出任务中心的动作表示，从而能够利用跨越广泛具身和视角的庞大数据。论文通过结合语言指令并在DINO特征空间中建立潜在动作模型，来减轻任务无关动态的影响。学习自互联网规模视频的通用策略可以通过高效的潜在动作解码部署到各种机器人。论文在多个操作和导航基准测试以及真实机器人部署中取得了最先进的结果。UniVLA在计算成本不到OpenVLA的1/20，下游数据不到1/10的情况下，取得了优于OpenVLA的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了UniVLA，一个通过在统一的、具身不可知的动作空间中规划来实现通用策略的框架，能够通过从网络规模视频中学习来实现可扩展和高效的决策。</li><li>引入了一种从跨具身视频中提取任务相关潜在动作的新方法，将任务中心动态与无关的视觉变化解耦。定性和定量实验突出了其优点和相对于现有工作的优势。</li><li>UniVLA在多个基准测试和真实机器人测试中取得了最先进的性能，在LIBERO基准上成功率比OpenVLA提高18.5%，导航任务中提高29.6%，真实世界部署中提高36.7%。</li></ol>\n<h3>论文方法描述</h3>\n<p>UniVLA方法分为三个阶段：</p>\n<ol><li><strong>任务中心潜在动作学习</strong>：通过语言指令引导，从跨具身视频中无监督地提取任务相关的动作表示。使用VQ-VAE对连续帧对的逆动态进行离散化，得到量化的潜在动作。在DINOv2特征空间中操作以过滤视觉噪声，并解耦出任务中心动态。</li><li><strong>通用策略预训练</strong>：基于Prismatic-7B视觉语言模型，通过自回归方式预测离散化的潜在动作标记，训练一个具身不可知的通用策略。</li><li><strong>部署后训练</strong>：引入轻量级动作解码器（约10.8M参数），将潜在动作解码为可执行轨迹，实现对新机器人的高效适配。同时利用历史潜在动作输出作为提示输入，增强长时程任务表现。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>：</p>\n<ul><li>预训练：Open X-Embodiment（操作数据）、GNM（导航数据）和Ego4D（人类视频）。</li><li>评估：LIBERO（操作）、CALVIN（操作）、SimplerEnv（操作）、Room2Room（导航）以及真实世界机器人任务。</li></ul>\n<p><strong>训练资源</strong>：</p>\n<ul><li>预训练：960 A100小时，显著低于OpenVLA的21,500 A100小时。</li><li>后训练：使用LoRA进行参数高效微调，新增约123M可训练参数。</li><li>推理：在NVIDIA RTX 4090 GPU上实现10Hz实时闭环推理。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>：</p>\n<ul><li>模拟环境：LIBERO、CALVIN、SimplerEnv和VLN-CE的Room2Room（R2R）。</li><li>真实环境：配备7-DoF机械臂和Orbecc DABAI相机的Piper机器人。</li></ul>\n<p><strong>评估指标</strong>：</p>\n<ul><li>操作任务：成功率（Success Rate）。</li><li>导航任务：Oracle成功率（在VLN-CE中，代理到达目标3米内视为成功）。</li><li>真实任务：成功率（Success Rate）和逐步得分（Step-wise Score，每个任务最高3分）。</li><li>通用性评估：光照变化、视觉干扰物、新物体场景下的成功率。</think></li></ul>\n<h3>论文研究单位</h3>\n<p>论文作者来自多个机构，包括北京大学的1,2单位，以及北京大学的其他合作单位，例如第3单位。</p>\n<h3>论文概述</h3>\n<p>论文提出了UniVLA，一个用于学习跨具身视觉-语言-动作（VLA）策略的新框架。核心创新是通过一个潜在动作模型从视频中导出任务中心的动作表示，从而能够利用跨越广泛具身和视角的庞大数据。论文通过结合语言指令并在DINO特征空间中建立潜在动作模型，来减轻任务无关动态的影响。学习自互联网规模视频的通用策略可以通过高效的潜在动作解码部署到各种机器人。论文在多个操作和导航基准测试以及真实机器人部署中取得了最先进的结果。UniVLA在计算成本不到OpenVLA的1/20，下游数据不到1/10的情况下，取得了优于OpenVLA的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了UniVLA，一个通过在统一的、具身不可知的动作空间中规划来实现通用策略的框架，能够通过从网络规模视频中学习来实现可扩展和高效的决策。</li><li>引入了一种从跨具身视频中提取任务相关潜在动作的新方法，将任务中心动态与无关的视觉变化解耦。定性和定量实验突出了其优点和相对于现有工作的优势。</li><li>UniVLA在多个基准测试和真实机器人测试中取得了最先进的性能，在LIBERO基准上成功率比OpenVLA提高18.5%，导航任务中提高29.6%，真实世界部署中提高36.7%。</li></ol>\n<h3>论文方法描述</h3>\n<p>UniVLA方法分为三个阶段：</p>\n<ol><li><strong>任务中心潜在动作学习</strong>：通过语言指令引导，从跨具身视频中无监督地提取任务相关的动作表示。使用VQ-VAE对连续帧对的逆动态进行离散化，得到量化的潜在动作。在DINOv2特征空间中操作以过滤视觉噪声，并解耦出任务中心动态。</li><li><strong>通用策略预训练</strong>：基于Prismatic-7B视觉语言模型，通过自回归方式预测离散化的潜在动作标记，训练一个具身不可知的通用策略。</li><li><strong>部署后训练</strong>：引入轻量级动作解码器（约10.8M参数），将潜在动作解码为可执行轨迹，实现对新机器人的高效适配。同时利用历史潜在动作输出作为提示输入，增强长时程任务表现。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>：</p>\n<ul><li>预训练：Open X-Embodiment（操作数据）、GNM（导航数据）和Ego4D（人类视频）。</li><li>评估：LIBERO（操作）、CALVIN（操作）、SimplerEnv（操作）、Room2Room（导航）以及真实世界机器人任务。</li></ul>\n<p><strong>训练资源</strong>：</p>\n<ul><li>预训练：960 A100小时，显著低于OpenVLA的21,500 A100小时。</li><li>后训练：使用LoRA进行参数高效微调，新增约123M可训练参数。</li><li>推理：在NVIDIA RTX 4090 GPU上实现10Hz实时闭环推理。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>：</p>\n<ul><li>模拟环境：LIBERO、CALVIN、SimplerEnv和VLN-CE的Room2Room（R2R）。</li><li>真实环境：配备7-DoF机械臂和Orbecc DABAI相机的Piper机器人。</li></ul>\n<p><strong>评估指标</strong>：</p>\n<ul><li>操作任务：成功率（Success Rate）。</li><li>导航任务：Oracle成功率（在VLN-CE中，代理到达目标3米内视为成功）。</li><li>真实任务：成功率（Success Rate）和逐步得分（Step-wise Score，每个任务最高3分）。</li><li>通用性评估：光照变化、视觉干扰物、新物体场景下的成功率。</li></ul>"
  },
  {
    "date": "2025-05-09",
    "title": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks",
    "link": "http://arxiv.org/abs/2505.05800",
    "summary_markdown": "- 论文研究单位\n纽约大学。\n\n- 论文概述\n该论文提出了3D-CAVLA模型，一种改进的视觉-语言-动作模型，旨在通过整合深度感知和3D上下文来提升对未见任务的泛化能力。模型基于OpenVLA-OFT架构，引入了思维链提示、深度特征提取和任务感知的兴趣区域（ROI）检测，以增强场景理解和空间推理能力。在LIBERO模拟环境中进行实验，结果显示3D-CAVLA在分布内任务的平均成功率达到98.1%，在未见任务上相比基线实现了8.8%的绝对提升。\n\n- 论文核心贡献点\n1. 提出3D-CAVLA模型：整合思维链提示、深度特征和ROI检测，提升视觉-语言-动作模型的场景感知和泛化能力。\n2. 引入轻量级深度编码器：从点云中提取3D特征，增强几何和空间理解。\n3. 开发任务感知ROI模块：通过实体识别和对象跟踪生成二进制掩码，聚焦于任务相关区域。\n4. 创建LIBERO-Unseen基准：包含10个未见任务，用于零样本评估，并公开代码和数据集。\n\n- 论文方法描述\n3D-CAVLA基于OpenVLA-OFT架构，核心改进包括：\n1. 思维链提示：使用GPT-4将任务指令分解为逐步步骤，提供中间推理以增强泛化。\n2. 深度特征集成：通过相机内参和深度图生成3D点云，输入轻量级PointNet风格的编码器（约1M参数），提取空间不变特征并与视觉、语言和本体感受特征融合。\n3. 任务感知ROI检测：结合命名实体识别、Molmo对象检测和SAMURAI跟踪，生成任务相关区域的二进制掩码，用于视觉特征池化，训练中随机使用25%以保留背景上下文。\n训练采用单块NVIDIA A100 GPU，批大小为8，使用LoRA微调策略。\n\n- 论文使用数据集和训练资源\n数据集：LIBERO基准（包含Spatial、Object、Goal、Long四个任务套件）和自定义的LIBERO-Unseen（10个未见任务）。\n训练资源：单块NVIDIA A100 GPU，批大小8。\n\n- 论文使用的评估环境和评估指标\n评估环境：LIBERO模拟环境。\n评估指标：任务成功率（%），包括分布内任务（LIBERO各套件）和零样本任务（LIBERO-Unseen）的评估。",
    "summary_html": "<ul><li>论文研究单位</li></ul>\n<p>纽约大学。</p>\n\n<ul><li>论文概述</li></ul>\n<p>该论文提出了3D-CAVLA模型，一种改进的视觉-语言-动作模型，旨在通过整合深度感知和3D上下文来提升对未见任务的泛化能力。模型基于OpenVLA-OFT架构，引入了思维链提示、深度特征提取和任务感知的兴趣区域（ROI）检测，以增强场景理解和空间推理能力。在LIBERO模拟环境中进行实验，结果显示3D-CAVLA在分布内任务的平均成功率达到98.1%，在未见任务上相比基线实现了8.8%的绝对提升。</p>\n\n<ul><li>论文核心贡献点</li></ul>\n<ol><li>提出3D-CAVLA模型：整合思维链提示、深度特征和ROI检测，提升视觉-语言-动作模型的场景感知和泛化能力。</li><li>引入轻量级深度编码器：从点云中提取3D特征，增强几何和空间理解。</li><li>开发任务感知ROI模块：通过实体识别和对象跟踪生成二进制掩码，聚焦于任务相关区域。</li><li>创建LIBERO-Unseen基准：包含10个未见任务，用于零样本评估，并公开代码和数据集。</li></ol>\n\n<ul><li>论文方法描述</li></ul>\n<p>3D-CAVLA基于OpenVLA-OFT架构，核心改进包括：</p>\n<ol><li>思维链提示：使用GPT-4将任务指令分解为逐步步骤，提供中间推理以增强泛化。</li><li>深度特征集成：通过相机内参和深度图生成3D点云，输入轻量级PointNet风格的编码器（约1M参数），提取空间不变特征并与视觉、语言和本体感受特征融合。</li><li>任务感知ROI检测：结合命名实体识别、Molmo对象检测和SAMURAI跟踪，生成任务相关区域的二进制掩码，用于视觉特征池化，训练中随机使用25%以保留背景上下文。</li></ol>\n<p>训练采用单块NVIDIA A100 GPU，批大小为8，使用LoRA微调策略。</p>\n\n<ul><li>论文使用数据集和训练资源</li></ul>\n<p>数据集：LIBERO基准（包含Spatial、Object、Goal、Long四个任务套件）和自定义的LIBERO-Unseen（10个未见任务）。</p>\n<p>训练资源：单块NVIDIA A100 GPU，批大小8。</p>\n\n<ul><li>论文使用的评估环境和评估指标</li></ul>\n<p>评估环境：LIBERO模拟环境。</p>\n<p>评估指标：任务成功率（%），包括分布内任务（LIBERO各套件）和零样本任务（LIBERO-Unseen）的评估。</p>"
  },
  {
    "date": "2025-05-08",
    "title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments",
    "link": "http://arxiv.org/abs/2505.05540",
    "summary_markdown": "### 论文研究单位\n- Manifold Research\n- Metarch.ai\n- Georgia Tech\n- MIT\n### 论文概述\n该论文介绍了MultiNet v0.2，一个用于评估视觉-语言-动作（VLA）模型在程序生成环境中零样本泛化能力的综合基准。研究聚焦于分析GPT-4o、GPT-4.1、OpenVLA、Pi0 Base和Pi0 FAST等最先进模型在Procgen基准上的性能，揭示了这些模型在处理分布外（OOD）任务时的关键局限性。\n### 论文核心贡献点\n1. 提出了一个系统化的基准测试框架，用于评估最先进模型在多种程序生成游戏环境中的性能。\n2. 提供了初始VLA和VLM模型的详细性能分析。\n3. 分析了架构选择、训练数据和输出处理技术对模型泛化能力的影响。\n4. 深入探讨了动作空间表示和图像复杂度等因素对模型性能的影响。\n### 论文方法描述\n1. **数据预处理**：将Procgen的专家强化学习轨迹转换为TensorFlow Datasets格式，并创建随机采样的测试集。\n2. **模型适配**：\n - **GPT模型**：使用Genesis提示工程框架将轨迹数据转换为结构化文本表示。\n - **OpenVLA**：接收原始图像输入和简洁的任务描述，限制自回归步骤为一步。\n - **Pi0模型**：使用单视图图像输入，零数组补充其他视图和本体感受状态。\n3. **推理基础设施**：针对不同模型配置专用硬件，包括NVIDIA L4、A100 GPU和OpenAI Batch API。\n4. **评估指标**：采用多维指标体系，包括Brier MAE、精度/召回率/F1分数、无效预测比例等。\n### 论文使用数据集和训练资源\n- **数据集**：Procgen基准的16个子数据集，包含专家RL代理的离线轨迹，测试集占比10%。\n- **训练资源**：\n - OpenVLA：使用HuggingFace提供的7B预训练权重。\n - Pi0模型：使用Openpi代码库中的检查点。\n - GPT模型：通过OpenAI Batch API进行推理。\n - 硬件：NVIDIA L4（OpenVLA）、单张A100 40GB（Pi0 Base）、四张A100（Pi0 Fast）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Procgen程序生成的16个不同2D游戏环境，每个环境具有独特的任务、动作空间和奖励结构。\n- **评估指标**：\n - **校准度指标**：Brier MAE（最大值2）、归一化Brier MAE、分位数过滤Brier MAE。\n - **分类指标**：微观/宏观精度、召回率、F1分数、精确匹配率。\n - **鲁棒性指标**：无效预测比例、最大相对Brier MAE。\n - **类别分析**：类别级精度/召回率/F1分数，用于检测模型偏差。\n - **特殊指标**：宏观召回率作为最代表性指标，因不受多数类影响。</think>\n### 论文研究单位\n- Manifold Research\n- Metarch.ai\n- Georgia Tech\n- MIT\n### 论文概述\n该论文介绍了MultiNet v0.2，一个用于评估视觉-语言-动作（VLA）模型在程序生成环境中零样本泛化能力的综合基准。研究聚焦于分析GPT-4o、GPT-4.1、OpenVLA、Pi0 Base和Pi0 FAST等最先进模型在Procgen基准上的性能，揭示了这些模型在处理分布外（OOD）任务时的关键局限性。\n### 论文核心贡献点\n1. 提出了一个系统化的基准测试框架，用于评估最先进模型在多种程序生成游戏环境中的性能。\n2. 提供了初始VLA和VLM模型的详细性能分析。\n3. 分析了架构选择、训练数据和输出处理技术对模型泛化能力的影响。\n4. 深入探讨了动作空间表示和图像复杂度等因素对模型性能的影响。\n### 论文方法描述\n1. **数据预处理**：将Procgen的专家强化学习轨迹转换为TensorFlow Datasets格式，并创建随机采样的测试集。\n2. **模型适配**：\n - **GPT模型**：使用Genesis提示工程框架将轨迹数据转换为结构化文本表示。\n - **OpenVLA**：接收原始图像输入和简洁的任务描述，限制自回归步骤为一步。\n - **Pi0模型**：使用单视图图像输入，零数组补充其他视图和本体感受状态。\n3. **推理基础设施**：针对不同模型配置专用硬件，包括NVIDIA L4、A100 GPU和OpenAI Batch API。\n4. **评估指标**：采用多维指标体系，包括Brier MAE、精度/召回率/F1分数、无效预测比例等。\n### 论文使用数据集和训练资源\n- **数据集**：Procgen基准的16个子数据集，包含专家RL代理的离线轨迹，测试集占比10%。\n- **训练资源**：\n - OpenVLA：使用HuggingFace提供的7B预训练权重。\n - Pi0模型：使用Openpi代码库中的检查点。\n - GPT模型：通过OpenAI Batch API进行推理。\n - 硬件：NVIDIA L4（OpenVLA）、单张A100 40GB（Pi0 Base）、四张A100（Pi0 Fast）。\n### 论文使用的评估环境和评估指标\n- **评估环境**：Procgen程序生成的16个不同2D游戏环境，每个环境具有独特的任务、动作空间和奖励结构。\n- **评估指标**：\n - **校准度指标**：Brier MAE（最大值2）、归一化Brier MAE、分位数过滤Brier MAE。\n - **分类指标**：微观/宏观精度、召回率、F1分数、精确匹配率。\n - **鲁棒性指标**：无效预测比例、最大相对Brier MAE。\n - **类别分析**：类别级精度/召回率/F1分数，用于检测模型偏差。\n - **特殊指标**：宏观召回率作为最代表性指标，因不受多数类影响。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>Manifold Research</li><li>Metarch.ai</li><li>Georgia Tech</li><li>MIT</li></ul>\n<h3>论文概述</h3>\n<p>该论文介绍了MultiNet v0.2，一个用于评估视觉-语言-动作（VLA）模型在程序生成环境中零样本泛化能力的综合基准。研究聚焦于分析GPT-4o、GPT-4.1、OpenVLA、Pi0 Base和Pi0 FAST等最先进模型在Procgen基准上的性能，揭示了这些模型在处理分布外（OOD）任务时的关键局限性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个系统化的基准测试框架，用于评估最先进模型在多种程序生成游戏环境中的性能。</li><li>提供了初始VLA和VLM模型的详细性能分析。</li><li>分析了架构选择、训练数据和输出处理技术对模型泛化能力的影响。</li><li>深入探讨了动作空间表示和图像复杂度等因素对模型性能的影响。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据预处理</strong>：将Procgen的专家强化学习轨迹转换为TensorFlow Datasets格式，并创建随机采样的测试集。</li><li><strong>模型适配</strong>：</li></ol>\n<p> - <strong>GPT模型</strong>：使用Genesis提示工程框架将轨迹数据转换为结构化文本表示。</p>\n<p> - <strong>OpenVLA</strong>：接收原始图像输入和简洁的任务描述，限制自回归步骤为一步。</p>\n<p> - <strong>Pi0模型</strong>：使用单视图图像输入，零数组补充其他视图和本体感受状态。</p>\n<ol><li><strong>推理基础设施</strong>：针对不同模型配置专用硬件，包括NVIDIA L4、A100 GPU和OpenAI Batch API。</li><li><strong>评估指标</strong>：采用多维指标体系，包括Brier MAE、精度/召回率/F1分数、无效预测比例等。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Procgen基准的16个子数据集，包含专家RL代理的离线轨迹，测试集占比10%。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - OpenVLA：使用HuggingFace提供的7B预训练权重。</p>\n<p> - Pi0模型：使用Openpi代码库中的检查点。</p>\n<p> - GPT模型：通过OpenAI Batch API进行推理。</p>\n<p> - 硬件：NVIDIA L4（OpenVLA）、单张A100 40GB（Pi0 Base）、四张A100（Pi0 Fast）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Procgen程序生成的16个不同2D游戏环境，每个环境具有独特的任务、动作空间和奖励结构。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>校准度指标</strong>：Brier MAE（最大值2）、归一化Brier MAE、分位数过滤Brier MAE。</p>\n<p> - <strong>分类指标</strong>：微观/宏观精度、召回率、F1分数、精确匹配率。</p>\n<p> - <strong>鲁棒性指标</strong>：无效预测比例、最大相对Brier MAE。</p>\n<p> - <strong>类别分析</strong>：类别级精度/召回率/F1分数，用于检测模型偏差。</p>\n<p> - <strong>特殊指标</strong>：宏观召回率作为最代表性指标，因不受多数类影响。</think></p>\n<h3>论文研究单位</h3>\n<ul><li>Manifold Research</li><li>Metarch.ai</li><li>Georgia Tech</li><li>MIT</li></ul>\n<h3>论文概述</h3>\n<p>该论文介绍了MultiNet v0.2，一个用于评估视觉-语言-动作（VLA）模型在程序生成环境中零样本泛化能力的综合基准。研究聚焦于分析GPT-4o、GPT-4.1、OpenVLA、Pi0 Base和Pi0 FAST等最先进模型在Procgen基准上的性能，揭示了这些模型在处理分布外（OOD）任务时的关键局限性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一个系统化的基准测试框架，用于评估最先进模型在多种程序生成游戏环境中的性能。</li><li>提供了初始VLA和VLM模型的详细性能分析。</li><li>分析了架构选择、训练数据和输出处理技术对模型泛化能力的影响。</li><li>深入探讨了动作空间表示和图像复杂度等因素对模型性能的影响。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据预处理</strong>：将Procgen的专家强化学习轨迹转换为TensorFlow Datasets格式，并创建随机采样的测试集。</li><li><strong>模型适配</strong>：</li></ol>\n<p> - <strong>GPT模型</strong>：使用Genesis提示工程框架将轨迹数据转换为结构化文本表示。</p>\n<p> - <strong>OpenVLA</strong>：接收原始图像输入和简洁的任务描述，限制自回归步骤为一步。</p>\n<p> - <strong>Pi0模型</strong>：使用单视图图像输入，零数组补充其他视图和本体感受状态。</p>\n<ol><li><strong>推理基础设施</strong>：针对不同模型配置专用硬件，包括NVIDIA L4、A100 GPU和OpenAI Batch API。</li><li><strong>评估指标</strong>：采用多维指标体系，包括Brier MAE、精度/召回率/F1分数、无效预测比例等。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：Procgen基准的16个子数据集，包含专家RL代理的离线轨迹，测试集占比10%。</li><li><strong>训练资源</strong>：</li></ul>\n<p> - OpenVLA：使用HuggingFace提供的7B预训练权重。</p>\n<p> - Pi0模型：使用Openpi代码库中的检查点。</p>\n<p> - GPT模型：通过OpenAI Batch API进行推理。</p>\n<p> - 硬件：NVIDIA L4（OpenVLA）、单张A100 40GB（Pi0 Base）、四张A100（Pi0 Fast）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：Procgen程序生成的16个不同2D游戏环境，每个环境具有独特的任务、动作空间和奖励结构。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>校准度指标</strong>：Brier MAE（最大值2）、归一化Brier MAE、分位数过滤Brier MAE。</p>\n<p> - <strong>分类指标</strong>：微观/宏观精度、召回率、F1分数、精确匹配率。</p>\n<p> - <strong>鲁棒性指标</strong>：无效预测比例、最大相对Brier MAE。</p>\n<p> - <strong>类别分析</strong>：类别级精度/召回率/F1分数，用于检测模型偏差。</p>\n<p> - <strong>特殊指标</strong>：宏观召回率作为最代表性指标，因不受多数类影响。</p>"
  },
  {
    "date": "2025-05-07",
    "title": "Vision-Language-Action Models: Concepts, Progress, Applications and Challenges",
    "link": "http://arxiv.org/abs/2505.04769",
    "summary_markdown": "论文研究单位\nCornell University, The Hong Kong University of Science and Technology, University of the Peloponnese\n\n论文概述\n本文是关于视觉-语言-动作(VLA)模型的综合性综述，系统分析了过去三年发布的80多个VLA模型。论文从概念基础、技术进展、应用领域和面临挑战五个主题维度，系统阐述了VLA模型如何统一感知、自然语言理解和具身动作。强调VLA模型在实现通用、自适应和具身智能体方面的变革性潜力，并为未来研究提供路线图。\n\n论文核心贡献点\n1. 建立了VLA模型的系统性综述框架，涵盖概念演进、技术进展和应用领域\n2. 追溯了VLA从跨模态学习架构到通用智能体的演变历程\n3. 分析了架构创新、参数高效训练策略和实时推理加速等关键技术进展\n4. 探索了人形机器人、自动驾驶、工业机器人等多样化应用场景\n5. 识别了实时控制、多模态动作表示等主要挑战并提出针对性解决方案\n\n论文方法描述\nVLA模型采用统一的标记化表示框架，通过前缀标记(prefix tokens)编码环境场景和语言指令，状态标记(state tokens)嵌入机器人配置信息，动作标记(action tokens)自回归生成控制信号。使用交叉注意力机制融合视觉、语言和状态标记，形成共享嵌入空间。最后通过基于Transformer的解码器逐步预测动作序列，实现从感知到执行的端到端学习。\n\n论文使用数据集和训练资源\n使用互联网规模数据集，整合视觉、语言和行为信息。训练策略包括模仿学习、强化学习和检索增强模块。采用CLIP等预训练视觉编码器和T5/GPT等语言模型作为基础组件。\n\n论文使用的评估环境和评估指标\n由于是综述论文，未指定具体实验环境。但强调评估需关注零样本泛化能力、实时推理效率、跨具身泛化能力和安全保证。提出需要新的基准测试方法来评估VLA系统的复杂推理和任务执行能力。",
    "summary_html": "<p>论文研究单位</p>\n<p>Cornell University, The Hong Kong University of Science and Technology, University of the Peloponnese</p>\n\n<p>论文概述</p>\n<p>本文是关于视觉-语言-动作(VLA)模型的综合性综述，系统分析了过去三年发布的80多个VLA模型。论文从概念基础、技术进展、应用领域和面临挑战五个主题维度，系统阐述了VLA模型如何统一感知、自然语言理解和具身动作。强调VLA模型在实现通用、自适应和具身智能体方面的变革性潜力，并为未来研究提供路线图。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>建立了VLA模型的系统性综述框架，涵盖概念演进、技术进展和应用领域</li><li>追溯了VLA从跨模态学习架构到通用智能体的演变历程</li><li>分析了架构创新、参数高效训练策略和实时推理加速等关键技术进展</li><li>探索了人形机器人、自动驾驶、工业机器人等多样化应用场景</li><li>识别了实时控制、多模态动作表示等主要挑战并提出针对性解决方案</li></ol>\n\n<p>论文方法描述</p>\n<p>VLA模型采用统一的标记化表示框架，通过前缀标记(prefix tokens)编码环境场景和语言指令，状态标记(state tokens)嵌入机器人配置信息，动作标记(action tokens)自回归生成控制信号。使用交叉注意力机制融合视觉、语言和状态标记，形成共享嵌入空间。最后通过基于Transformer的解码器逐步预测动作序列，实现从感知到执行的端到端学习。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>使用互联网规模数据集，整合视觉、语言和行为信息。训练策略包括模仿学习、强化学习和检索增强模块。采用CLIP等预训练视觉编码器和T5/GPT等语言模型作为基础组件。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>由于是综述论文，未指定具体实验环境。但强调评估需关注零样本泛化能力、实时推理效率、跨具身泛化能力和安全保证。提出需要新的基准测试方法来评估VLA系统的复杂推理和任务执行能力。</p>"
  },
  {
    "date": "2025-05-06",
    "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2505.03912",
    "summary_markdown": "### 论文研究单位\n西湖大学、浙江大学、西安交通大学、香港科技大学（广州）\n### 论文概述\n本文对双系统视觉-语言-动作（VLA）架构进行了简短综述和实证分析，并提出一个名为OpenHelix的开源双系统VLA模型，旨在解决现有开源双系统VLA工作不足的问题，并为性能分析和优化提供基础。项目旨在持续更新实验结论和性能改进的开源模型。\n### 论文核心贡献点\n1. 对现有双系统VLA架构的结构设计进行了总结和比较。\n2. 对双系统架构的核心设计元素进行了系统的实证评估，包括MLLM选择、策略选择、潜在特征表示选择、MLLM训练策略、策略训练策略、双系统集成策略和双系统异步策略。\n3. 提供了一个低成本的开源模型OpenHelix，用于进一步探索，并计划持续更新。\n4. 定义了双系统VLA的正式标准，指出System 1必须包含实时感知输入（如RGB图像）。\n### 论文方法描述\nOpenHelix采用双系统架构：\n- 高层MLLM（System 2）：使用LLaVA1.0作为视觉语言模型，接收第三人称RGB图像、任务指令和可学习的<ACT>标记作为输入。通过提示调优（Prompt-tuning）训练MLLM，仅训练lm-head层，保持其他模型参数固定，以减少训练成本并保留多模态推理能力。输出<ACT>标记的最终层特征嵌入作为低层策略的潜在目标。\n- 低层策略（System 1）：使用预训练的3D Diffusion Actor（3DDA）作为策略，接收高层MLLM的潜在目标、3D场景标记和本体感觉信息，通过交叉注意力层迭代预测动作噪声以产生精确的动作轨迹和夹爪状态。使用线性层替换3D Diffusion Actor的文本编码器，对齐高层MLLM输出嵌入与低层策略输入维度。\n- 双系统集成：使用MLP投影仪连接高层和低层组件，采用两阶段训练策略：首先冻结高层MLLM，训练MLP投影仪和低层策略，然后解冻高层MLLM进行联合训练。引入辅助任务（预测动作的位置、旋转、开/关），强制模型捕获更多视觉信息，确保潜在嵌入包含视觉和文本信息。\n- 异步策略：采用同步训练和异步测试，在测试时高层MLLM推理频率低于低层策略。\n### 论文使用数据集和训练资源\n数据集：CALVIN环境及其变体CALVIN-E（用于语言指令泛化测试）和CALVIN-D（用于动态场景测试，物体以四种不同方式移动）。\n训练资源：未明确提及具体硬件，但使用预训练的LLaVA1.0（7B）和3D Diffusion Actor作为基础模型，采用提示调优和两阶段投影仪训练策略。\n### 论文使用的评估环境和评估指标\n评估环境：CALVIN模拟环境（标准ABC-D场景）、CALVIN-E（增强语言指令）和CALVIN-D（动态场景，物体移动）。\n评估指标：连续完成任务成功率（百分比），平均任务长度。评估时使用前100个任务进行快速消融实验，最终评估扩展至1000个任务。指标越高表示性能越好（符号↑）。</think>\n### 论文研究单位\n西湖大学、浙江大学、西安交通大学、香港科技大学（广州）\n### 论文概述\n本文对双系统视觉-语言-动作（VLA）架构进行了简短综述和实证分析，并提出一个名为OpenHelix的开源双系统VLA模型，旨在解决现有开源双系统VLA工作不足的问题，并为性能分析和优化提供基础。项目旨在持续更新实验结论和性能改进的开源模型。\n### 论文核心贡献点\n1. 对现有双系统VLA架构的结构设计进行了总结和比较。\n2. 对双系统架构的核心设计元素进行了系统的实证评估，包括MLLM选择、策略选择、潜在特征表示选择、MLLM训练策略、策略训练策略、双系统集成策略和双系统异步策略。\n3. 提供了一个低成本的开源模型OpenHelix，用于进一步探索，并计划持续更新。\n4. 定义了双系统VLA的正式标准，指出System 1必须包含实时感知输入（如RGB图像）。\n### 论文方法描述\nOpenHelix采用双系统架构：\n- 高层MLLM（System 2）：使用LLaVA1.0作为视觉语言模型，接收第三人称RGB图像、任务指令和可学习的<ACT>标记作为输入。通过提示调优（Prompt-tuning）训练MLLM，仅训练lm-head层，保持其他模型参数固定，以减少训练成本并保留多模态推理能力。输出<ACT>标记的最终层特征嵌入作为低层策略的潜在目标。\n- 低层策略（System 1）：使用预训练的3D Diffusion Actor（3DDA）作为策略，接收高层MLLM的潜在目标、3D场景标记和本体感觉信息，通过交叉注意力层迭代预测动作噪声以产生精确的动作轨迹和夹爪状态。使用线性层替换3D Diffusion Actor的文本编码器，对齐高层MLLM输出嵌入与低层策略输入维度。\n- 双系统集成：使用MLP投影仪连接高层和低层组件，采用两阶段训练策略：首先冻结高层MLLM，训练MLP投影仪和低层策略，然后解冻高层MLLM进行联合训练。引入辅助任务（预测动作的位置、旋转、开/关），强制模型捕获更多视觉信息，确保潜在嵌入包含视觉和文本信息。\n- 异步策略：采用同步训练和异步测试，在测试时高层MLLM推理频率低于低层策略。\n### 论文使用数据集和训练资源\n数据集：CALVIN环境及其变体CALVIN-E（用于语言指令泛化测试）和CALVIN-D（用于动态场景测试，物体以四种不同方式移动）。\n训练资源：未明确提及具体硬件，但使用预训练的LLaVA1.0（7B）和3D Diffusion Actor作为基础模型，采用提示调优和两阶段投影仪训练策略。\n### 论文使用的评估环境和评估指标\n评估环境：CALVIN模拟环境（标准ABC-D场景）、CALVIN-E（增强语言指令）和CALVIN-D（动态场景，物体移动）。\n评估指标：连续完成任务成功率（百分比），平均任务长度。评估时使用前100个任务进行快速消融实验，最终评估扩展至1000个任务。指标越高表示性能越好（符号↑）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>西湖大学、浙江大学、西安交通大学、香港科技大学（广州）</p>\n<h3>论文概述</h3>\n<p>本文对双系统视觉-语言-动作（VLA）架构进行了简短综述和实证分析，并提出一个名为OpenHelix的开源双系统VLA模型，旨在解决现有开源双系统VLA工作不足的问题，并为性能分析和优化提供基础。项目旨在持续更新实验结论和性能改进的开源模型。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>对现有双系统VLA架构的结构设计进行了总结和比较。</li><li>对双系统架构的核心设计元素进行了系统的实证评估，包括MLLM选择、策略选择、潜在特征表示选择、MLLM训练策略、策略训练策略、双系统集成策略和双系统异步策略。</li><li>提供了一个低成本的开源模型OpenHelix，用于进一步探索，并计划持续更新。</li><li>定义了双系统VLA的正式标准，指出System 1必须包含实时感知输入（如RGB图像）。</li></ol>\n<h3>论文方法描述</h3>\n<p>OpenHelix采用双系统架构：</p>\n<ul><li>高层MLLM（System 2）：使用LLaVA1.0作为视觉语言模型，接收第三人称RGB图像、任务指令和可学习的<ACT>标记作为输入。通过提示调优（Prompt-tuning）训练MLLM，仅训练lm-head层，保持其他模型参数固定，以减少训练成本并保留多模态推理能力。输出<ACT>标记的最终层特征嵌入作为低层策略的潜在目标。</li><li>低层策略（System 1）：使用预训练的3D Diffusion Actor（3DDA）作为策略，接收高层MLLM的潜在目标、3D场景标记和本体感觉信息，通过交叉注意力层迭代预测动作噪声以产生精确的动作轨迹和夹爪状态。使用线性层替换3D Diffusion Actor的文本编码器，对齐高层MLLM输出嵌入与低层策略输入维度。</li><li>双系统集成：使用MLP投影仪连接高层和低层组件，采用两阶段训练策略：首先冻结高层MLLM，训练MLP投影仪和低层策略，然后解冻高层MLLM进行联合训练。引入辅助任务（预测动作的位置、旋转、开/关），强制模型捕获更多视觉信息，确保潜在嵌入包含视觉和文本信息。</li><li>异步策略：采用同步训练和异步测试，在测试时高层MLLM推理频率低于低层策略。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：CALVIN环境及其变体CALVIN-E（用于语言指令泛化测试）和CALVIN-D（用于动态场景测试，物体以四种不同方式移动）。</p>\n<p>训练资源：未明确提及具体硬件，但使用预训练的LLaVA1.0（7B）和3D Diffusion Actor作为基础模型，采用提示调优和两阶段投影仪训练策略。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：CALVIN模拟环境（标准ABC-D场景）、CALVIN-E（增强语言指令）和CALVIN-D（动态场景，物体移动）。</p>\n<p>评估指标：连续完成任务成功率（百分比），平均任务长度。评估时使用前100个任务进行快速消融实验，最终评估扩展至1000个任务。指标越高表示性能越好（符号↑）。</think></p>\n<h3>论文研究单位</h3>\n<p>西湖大学、浙江大学、西安交通大学、香港科技大学（广州）</p>\n<h3>论文概述</h3>\n<p>本文对双系统视觉-语言-动作（VLA）架构进行了简短综述和实证分析，并提出一个名为OpenHelix的开源双系统VLA模型，旨在解决现有开源双系统VLA工作不足的问题，并为性能分析和优化提供基础。项目旨在持续更新实验结论和性能改进的开源模型。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>对现有双系统VLA架构的结构设计进行了总结和比较。</li><li>对双系统架构的核心设计元素进行了系统的实证评估，包括MLLM选择、策略选择、潜在特征表示选择、MLLM训练策略、策略训练策略、双系统集成策略和双系统异步策略。</li><li>提供了一个低成本的开源模型OpenHelix，用于进一步探索，并计划持续更新。</li><li>定义了双系统VLA的正式标准，指出System 1必须包含实时感知输入（如RGB图像）。</li></ol>\n<h3>论文方法描述</h3>\n<p>OpenHelix采用双系统架构：</p>\n<ul><li>高层MLLM（System 2）：使用LLaVA1.0作为视觉语言模型，接收第三人称RGB图像、任务指令和可学习的<ACT>标记作为输入。通过提示调优（Prompt-tuning）训练MLLM，仅训练lm-head层，保持其他模型参数固定，以减少训练成本并保留多模态推理能力。输出<ACT>标记的最终层特征嵌入作为低层策略的潜在目标。</li><li>低层策略（System 1）：使用预训练的3D Diffusion Actor（3DDA）作为策略，接收高层MLLM的潜在目标、3D场景标记和本体感觉信息，通过交叉注意力层迭代预测动作噪声以产生精确的动作轨迹和夹爪状态。使用线性层替换3D Diffusion Actor的文本编码器，对齐高层MLLM输出嵌入与低层策略输入维度。</li><li>双系统集成：使用MLP投影仪连接高层和低层组件，采用两阶段训练策略：首先冻结高层MLLM，训练MLP投影仪和低层策略，然后解冻高层MLLM进行联合训练。引入辅助任务（预测动作的位置、旋转、开/关），强制模型捕获更多视觉信息，确保潜在嵌入包含视觉和文本信息。</li><li>异步策略：采用同步训练和异步测试，在测试时高层MLLM推理频率低于低层策略。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：CALVIN环境及其变体CALVIN-E（用于语言指令泛化测试）和CALVIN-D（用于动态场景测试，物体以四种不同方式移动）。</p>\n<p>训练资源：未明确提及具体硬件，但使用预训练的LLaVA1.0（7B）和3D Diffusion Actor作为基础模型，采用提示调优和两阶段投影仪训练策略。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：CALVIN模拟环境（标准ABC-D场景）、CALVIN-E（增强语言指令）和CALVIN-D（动态场景，物体移动）。</p>\n<p>评估指标：连续完成任务成功率（百分比），平均任务长度。评估时使用前100个任务进行快速消融实验，最终评估扩展至1000个任务。指标越高表示性能越好（符号↑）。</p>"
  },
  {
    "date": "2025-05-06",
    "title": "RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration",
    "link": "http://arxiv.org/abs/2505.03673",
    "summary_markdown": "论文研究单位\nState Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; Beijing Academy of Artificial Intelligence (BAAI).\n\n论文概述\n论文提出了RoboOS，第一个开源的基于脑-小脑分层架构的具身框架，旨在解决现有机器人系统在跨具身适应性、任务调度和动态错误纠正方面的局限性。该系统通过其三个核心组件：用于高级决策的具身脑模型、用于技能执行的小脑技能库以及用于状态同步的实时共享内存，实现了从单智能体到多智能体智能的转变。RoboOS支持边缘-云部署，并在餐厅、家庭和超市等多种真实场景中验证了其对单臂、双臂、人形和轮式等不同具身机器人的协作能力。\n\n论文核心贡献点\n1. 提出了RoboOS，首个基于脑-小脑分层架构的开源具身框架，推动了多智能体智能的发展。\n2. 精心设计了RoboOS的三个核心组件：具身脑模型、小脑技能库和实时共享内存，并优化了边缘-云通信与分布式推理。\n3. 在餐厅、家庭、超市等真实世界场景中进行了广泛的实验，验证了系统在跨具身协作方面的适应性和性能，支持多种机器人平台。\n\n论文方法描述\nRoboOS采用脑-小脑分层架构。具身脑模型是一个部署在云端的多模态大语言模型，负责多智能体任务规划、工具调用和错误纠正。小脑技能库是部署在机器人终端的模块化工具包，支持操作、导航和专业化技能的插拔式执行。实时共享内存通过Redis维护空间、时间和机器人状态的同步。系统的工作流程包括：全局任务分解、拓扑子任务分配、分布式代理执行和动态内存更新四个步骤。此外，系统通过FlagScale框架支持边缘-云部署，实现高效通信与大规模推理。\n\n论文使用数据集和训练资源\n训练数据集分为三类：VLM数据集（如General-873k, ScanView-318k）、机器人数据集（如Planning-700k, Affordance-373k）和RoboOS增强数据集（Multi-Robot-45k, Robotic-Agent-144k）。训练采用三阶段策略，首先在3M样本上训练基础能力，然后在2.3M样本上优化机器人专项技能，最后在249k样本上适应多智能体协作任务。训练资源为20台服务器，每台配备8块A800 GPU，使用Zero3分布式策略。\n\n论文使用的评估环境和评估指标\n评估环境包括基准测试和真实世界演示。基准测试使用了Where2Place（指向预测）和ShareRobot（可供性与轨迹预测）。真实世界演示在餐厅、家庭和超市三个场景中进行，展示了不同机器人（如Unitree G1, AgileX双臂, Realman单臂）的协作。评估指标包括：用于多机器人规划的准确率（Accuracy-Rate）；用于指向预测的准确率（AR）；用于可供性预测的平均精度均值；以及用于轨迹预测的离散弗雷歇距离（DFD）、豪斯多夫距离（HD）和均方根误差（RMSE）。",
    "summary_html": "<p>论文研究单位</p>\n<p>State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; Beijing Academy of Artificial Intelligence (BAAI).</p>\n\n<p>论文概述</p>\n<p>论文提出了RoboOS，第一个开源的基于脑-小脑分层架构的具身框架，旨在解决现有机器人系统在跨具身适应性、任务调度和动态错误纠正方面的局限性。该系统通过其三个核心组件：用于高级决策的具身脑模型、用于技能执行的小脑技能库以及用于状态同步的实时共享内存，实现了从单智能体到多智能体智能的转变。RoboOS支持边缘-云部署，并在餐厅、家庭和超市等多种真实场景中验证了其对单臂、双臂、人形和轮式等不同具身机器人的协作能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了RoboOS，首个基于脑-小脑分层架构的开源具身框架，推动了多智能体智能的发展。</li><li>精心设计了RoboOS的三个核心组件：具身脑模型、小脑技能库和实时共享内存，并优化了边缘-云通信与分布式推理。</li><li>在餐厅、家庭、超市等真实世界场景中进行了广泛的实验，验证了系统在跨具身协作方面的适应性和性能，支持多种机器人平台。</li></ol>\n\n<p>论文方法描述</p>\n<p>RoboOS采用脑-小脑分层架构。具身脑模型是一个部署在云端的多模态大语言模型，负责多智能体任务规划、工具调用和错误纠正。小脑技能库是部署在机器人终端的模块化工具包，支持操作、导航和专业化技能的插拔式执行。实时共享内存通过Redis维护空间、时间和机器人状态的同步。系统的工作流程包括：全局任务分解、拓扑子任务分配、分布式代理执行和动态内存更新四个步骤。此外，系统通过FlagScale框架支持边缘-云部署，实现高效通信与大规模推理。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>训练数据集分为三类：VLM数据集（如General-873k, ScanView-318k）、机器人数据集（如Planning-700k, Affordance-373k）和RoboOS增强数据集（Multi-Robot-45k, Robotic-Agent-144k）。训练采用三阶段策略，首先在3M样本上训练基础能力，然后在2.3M样本上优化机器人专项技能，最后在249k样本上适应多智能体协作任务。训练资源为20台服务器，每台配备8块A800 GPU，使用Zero3分布式策略。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境包括基准测试和真实世界演示。基准测试使用了Where2Place（指向预测）和ShareRobot（可供性与轨迹预测）。真实世界演示在餐厅、家庭和超市三个场景中进行，展示了不同机器人（如Unitree G1, AgileX双臂, Realman单臂）的协作。评估指标包括：用于多机器人规划的准确率（Accuracy-Rate）；用于指向预测的准确率（AR）；用于可供性预测的平均精度均值；以及用于轨迹预测的离散弗雷歇距离（DFD）、豪斯多夫距离（HD）和均方根误差（RMSE）。</p>"
  },
  {
    "date": "2025-05-06",
    "title": "Task Reconstruction and Extrapolation for $π_0$ using Text Latent",
    "link": "http://arxiv.org/abs/2505.03500",
    "summary_markdown": "### 论文研究单位\nIndependent\n### 论文概述\n该论文研究了视觉-语言-动作模型在任务外推方面的局限性，发现这些模型虽然在演示过的任务上表现良好，但在需要组合不同任务中的技能以解决新任务时常常失败。作者提出了一种名为“text latent”的方法，这是一种从模型内部隐藏状态中提取的任务特定向量。通过将“text latent”注入模型的残差流，可以重构相应的任务行为。进一步地，通过线性插值混合不同任务的“text latent”（Text Latent Interpolation, TLI），可以组合子技能以生成新颖的行为，从而实现任务外推。研究揭示了π₀模型内部编码了独立且可组合的技能表示，但其本身无法自主地组合这些表示。此外，论文还提出并验证了一个新的基准测试集libero-ood，并发现当前最先进的VLAs普遍存在“空间过拟合”问题，即将对象名称与其在演示场景中的空间位置相关联，而非真正理解对象。\n### 论文核心贡献点\n1. **引入“text latent”**：提出并验证了一种从模型文本标记的隐藏状态中提取的任务特定向量，它编码了完成任务所需的核心语义知识。\n2. **任务重构**：证明了通过将“text latent”注入模型的残差流，可以在没有原始任务提示的情况下重构任务行为，成功率超过80%。\n3. **提出Text Latent Interpolation (TLI)**：开发了一种通过在时间步上线性插值两个“text latent”来组合不同任务技能的方法，使得模型能够完成需要技能拼接的泛化任务。\n4. **提出libero-ood基准**：创建了一个包含20个外推任务的新基准测试集，用于评估VLAs的组合泛化能力，这些任务需要模型拼接已学习但未曾组合过的子轨迹。\n5. **显著提升外推性能**：通过应用TLI方法，将π₀模型在libero-ood基准上的成功率从9%提升至83%，证明了其内在的可组合潜力。\n6. **发现VLAs的共性缺陷**：通过在多个SOTA VLAs上测试，发现它们普遍存在“空间过拟合”问题，即模型将对象名称与训练数据中的特定空间位置绑定，缺乏真正的对象和目标理解能力。\n### 论文方法描述\n1. **Preliminary**：首先阐述了基于Transformer的VLAs的工作原理，即视觉、语言和本体感觉信息被编码为嵌入，并依次通过L层Transformer，最终由这些嵌入和隐藏状态生成动作。\n2. **Text Latent 提取**：对于一个给定的任务，通过运行模型在多个演示轨迹上，收集所有时间步和所有Transformer层中对应文本标记的隐藏状态。然后对这些状态进行元素平均，得到一个形状为 `(L-1) x \\|T\\|x d` 的张量，即该任务的“text latent”。\n3. **Text Latent Interpolation (TLI)**：为了解决需要组合两个基础任务技能的新任务，该方法在每个时间步 `i` 计算一个插值系数 `α = i / λ`（其中 `λ` 是预设的过渡步数）。然后，将两个基础任务的“text latent” (`𝒯¹`, `𝒯²`) 按该系数进行加权，并将结果注入到当前文本标记的隐藏状态中。这使得模型在任务开始时执行任务1的行为，随着时间推进，行为逐渐平滑过渡到任务2。\n### 论文使用数据集和训练资源\n1. **数据集**：\n * **标准基准**：LIBERO仿真环境中的三个任务套件，包括 `libero-goal`, `libero-object`, `libero-spatial`。\n * **新基准**：作者提出的 `libero-ood`，包含 `libero-goal-ood` 和 `libero-spatial-ood` 两个子集，共20个任务。这些任务的设计理念是，完成任务所需的抓取和放置动作分别在训练任务中出现过，但它们的组合是全新的。\n2. **训练/计算资源**：\n * “text latent” 的提取基于每个任务的20个演示轨迹。\n * 所有实验均在 **Nvidia RTX 4090** GPU上完成。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：所有实验均在 **LIBERO仿真环境**中进行。\n2. **评估指标**：\n * 主要指标为 **任务成功率**。对于每个任务，执行10次独立的评估（使用不同随机种子）。每个任务套体的最终成功率是总100次尝试中成功次数的比例。\n * 辅以 **定性分析**，通过可视化模型的行为轨迹来分析其决策过程，特别是“空间过拟合”现象。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Independent</p>\n<h3>论文概述</h3>\n<p>该论文研究了视觉-语言-动作模型在任务外推方面的局限性，发现这些模型虽然在演示过的任务上表现良好，但在需要组合不同任务中的技能以解决新任务时常常失败。作者提出了一种名为“text latent”的方法，这是一种从模型内部隐藏状态中提取的任务特定向量。通过将“text latent”注入模型的残差流，可以重构相应的任务行为。进一步地，通过线性插值混合不同任务的“text latent”（Text Latent Interpolation, TLI），可以组合子技能以生成新颖的行为，从而实现任务外推。研究揭示了π₀模型内部编码了独立且可组合的技能表示，但其本身无法自主地组合这些表示。此外，论文还提出并验证了一个新的基准测试集libero-ood，并发现当前最先进的VLAs普遍存在“空间过拟合”问题，即将对象名称与其在演示场景中的空间位置相关联，而非真正理解对象。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>引入“text latent”</strong>：提出并验证了一种从模型文本标记的隐藏状态中提取的任务特定向量，它编码了完成任务所需的核心语义知识。</li><li><strong>任务重构</strong>：证明了通过将“text latent”注入模型的残差流，可以在没有原始任务提示的情况下重构任务行为，成功率超过80%。</li><li><strong>提出Text Latent Interpolation (TLI)</strong>：开发了一种通过在时间步上线性插值两个“text latent”来组合不同任务技能的方法，使得模型能够完成需要技能拼接的泛化任务。</li><li><strong>提出libero-ood基准</strong>：创建了一个包含20个外推任务的新基准测试集，用于评估VLAs的组合泛化能力，这些任务需要模型拼接已学习但未曾组合过的子轨迹。</li><li><strong>显著提升外推性能</strong>：通过应用TLI方法，将π₀模型在libero-ood基准上的成功率从9%提升至83%，证明了其内在的可组合潜力。</li><li><strong>发现VLAs的共性缺陷</strong>：通过在多个SOTA VLAs上测试，发现它们普遍存在“空间过拟合”问题，即模型将对象名称与训练数据中的特定空间位置绑定，缺乏真正的对象和目标理解能力。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>Preliminary</strong>：首先阐述了基于Transformer的VLAs的工作原理，即视觉、语言和本体感觉信息被编码为嵌入，并依次通过L层Transformer，最终由这些嵌入和隐藏状态生成动作。</li><li><strong>Text Latent 提取</strong>：对于一个给定的任务，通过运行模型在多个演示轨迹上，收集所有时间步和所有Transformer层中对应文本标记的隐藏状态。然后对这些状态进行元素平均，得到一个形状为 <code>(L-1) x \\|T\\|x d</code> 的张量，即该任务的“text latent”。</li><li><strong>Text Latent Interpolation (TLI)</strong>：为了解决需要组合两个基础任务技能的新任务，该方法在每个时间步 <code>i</code> 计算一个插值系数 <code>α = i / λ</code>（其中 <code>λ</code> 是预设的过渡步数）。然后，将两个基础任务的“text latent” (<code>𝒯¹</code>, <code>𝒯²</code>) 按该系数进行加权，并将结果注入到当前文本标记的隐藏状态中。这使得模型在任务开始时执行任务1的行为，随着时间推进，行为逐渐平滑过渡到任务2。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：</p>\n<p> * <strong>标准基准</strong>：LIBERO仿真环境中的三个任务套件，包括 <code>libero-goal</code>, <code>libero-object</code>, <code>libero-spatial</code>。</p>\n<p> * <strong>新基准</strong>：作者提出的 <code>libero-ood</code>，包含 <code>libero-goal-ood</code> 和 <code>libero-spatial-ood</code> 两个子集，共20个任务。这些任务的设计理念是，完成任务所需的抓取和放置动作分别在训练任务中出现过，但它们的组合是全新的。</p>\n<p>2. <strong>训练/计算资源</strong>：</p>\n<p> * “text latent” 的提取基于每个任务的20个演示轨迹。</p>\n<p> * 所有实验均在 <strong>Nvidia RTX 4090</strong> GPU上完成。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：所有实验均在 <strong>LIBERO仿真环境</strong>中进行。</li><li><strong>评估指标</strong>：</li></ol>\n<p> * 主要指标为 <strong>任务成功率</strong>。对于每个任务，执行10次独立的评估（使用不同随机种子）。每个任务套体的最终成功率是总100次尝试中成功次数的比例。</p>\n<p> * 辅以 <strong>定性分析</strong>，通过可视化模型的行为轨迹来分析其决策过程，特别是“空间过拟合”现象。</p>"
  },
  {
    "date": "2025-05-06",
    "title": "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data",
    "link": "http://arxiv.org/abs/2505.03233",
    "summary_markdown": "论文研究单位\nGalbot, Peking University, The University of Hong Kong, Beijing Academy of Artificial Intelligence\n\n论文概述\n论文提出了GraspVLA，一个基于十亿规模合成数据预训练的抓取基础模型。该模型通过合成数据生成大规模数据集SynGrasp-1B，并采用渐进式动作生成（PAG）机制，将视觉-语言模型与动作专家结合，实现零样本泛化和少样本适应能力。\n\n论文核心贡献点\n1. 提出了一种完全依赖合成动作数据进行预训练的新范式，显著减少了真实世界动作数据的采集负担。\n2. 构建了十亿帧规模的机器人抓取数据集SynGrasp-1B，是首个此类规模的数据集。\n3. 提出渐进式动作生成（PAG）方法，将合成动作与互联网数据联合训练，使GraspVLA能够泛化到新颖的物体类别。\n4. 实验证明GraspVLA具有强大的零样本泛化能力和高效的少样本适应性。\n\n论文方法描述\nGraspVLA集成了一个视觉-语言模型（VLM）和一个基于流匹配的动作专家，通过渐进式动作生成（PAG）机制连接。PAG将感知任务（视觉定位和抓取姿态预测）作为动作生成的中间步骤，形成一个因果推断的思维链（CoT）过程。模型在合成数据集和互联网 grounding数据集上进行联合训练，合成数据提供详细的物体交互几何信息，互联网数据提供丰富的语义知识。\n\n论文使用数据集和训练资源\n数据集：SynGrasp-1B，包含1000万条轨迹、1万多个物体、240个类别、1000种桌面纹理和1200种背景纹理，总计10亿帧。\n训练资源：使用160块NVIDIA 4090 GPU，训练耗时10天。\n\n论文使用的评估环境和评估指标\n评估环境：真实世界实验使用Franka Panda机械臂和两个Intel RealSense摄像头，工作空间为40cm×50cm×20cm；仿真基准在LIBERO上进行。\n评估指标：成功率（3次尝试内成功抓取的百分比）、SPL（Success weighted by Path Length，考虑路径效率的加权成功率）。",
    "summary_html": "<p>论文研究单位</p>\n<p>Galbot, Peking University, The University of Hong Kong, Beijing Academy of Artificial Intelligence</p>\n\n<p>论文概述</p>\n<p>论文提出了GraspVLA，一个基于十亿规模合成数据预训练的抓取基础模型。该模型通过合成数据生成大规模数据集SynGrasp-1B，并采用渐进式动作生成（PAG）机制，将视觉-语言模型与动作专家结合，实现零样本泛化和少样本适应能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种完全依赖合成动作数据进行预训练的新范式，显著减少了真实世界动作数据的采集负担。</li><li>构建了十亿帧规模的机器人抓取数据集SynGrasp-1B，是首个此类规模的数据集。</li><li>提出渐进式动作生成（PAG）方法，将合成动作与互联网数据联合训练，使GraspVLA能够泛化到新颖的物体类别。</li><li>实验证明GraspVLA具有强大的零样本泛化能力和高效的少样本适应性。</li></ol>\n\n<p>论文方法描述</p>\n<p>GraspVLA集成了一个视觉-语言模型（VLM）和一个基于流匹配的动作专家，通过渐进式动作生成（PAG）机制连接。PAG将感知任务（视觉定位和抓取姿态预测）作为动作生成的中间步骤，形成一个因果推断的思维链（CoT）过程。模型在合成数据集和互联网 grounding数据集上进行联合训练，合成数据提供详细的物体交互几何信息，互联网数据提供丰富的语义知识。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：SynGrasp-1B，包含1000万条轨迹、1万多个物体、240个类别、1000种桌面纹理和1200种背景纹理，总计10亿帧。</p>\n<p>训练资源：使用160块NVIDIA 4090 GPU，训练耗时10天。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：真实世界实验使用Franka Panda机械臂和两个Intel RealSense摄像头，工作空间为40cm×50cm×20cm；仿真基准在LIBERO上进行。</p>\n<p>评估指标：成功率（3次尝试内成功抓取的百分比）、SPL（Success weighted by Path Length，考虑路径效率的加权成功率）。</p>"
  },
  {
    "date": "2025-05-06",
    "title": "Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets",
    "link": "http://arxiv.org/abs/2505.03174",
    "summary_markdown": "论文研究单位\nMachine Intelligence, Interaction, and Imagination (Mi^3) Laboratory, University of California, Merced\n\n论文概述\n指令-动作（IA）数据对对于训练机器人系统，特别是自动驾驶汽车（AV）非常有价值，但人工标注数据成本高且效率低下。本文探索了利用移动应用的全球定位系统（GPS）数据和自然语言处理（NLP）技术来自动生成大规模的IA指令与响应，无需人工生成或事后标记数据。通过在试点数据收集中驾驶至不同目的地并收集来自GPS应用的语音指令，论文展示了一种收集和分类多样化指令集的方法，并辅以视频数据形成完整的视觉-语言-动作三元组。论文详细介绍了一个完全自动化的数据收集原型系统ADVLAT-Engine，并将收集到的GPS语音指令分为八种不同的类别，突显了从免费移动应用中可获取的指令广度和参考性。\n\n论文核心贡献点\n1. 探索了三种常见的导航应用（Apple Maps, Google Maps, Waze），分析了它们在口头指令输出上的差异，并评估了其在参考性视觉-语言导航模型中的潜在用途。\n2. 提出了一个自动化流程，用于创建包含视频（视觉）、指令（语言）和车辆轨迹（动作）的标注数据，以供视觉-语言-动作（VLA）模型使用。\n3. 设计并实现了一个名为ADVLAT-Engine（Autonomous Driving Vision-Language-Action Triad Engine）的原型系统，用于自动化收集视觉-语言-动作三元组数据。\n4. 将GPS语音指令归纳为八个类别：道路名称、位置距离、静态物体、转向、基本方位、地点名称、车道信息和灯光信息，并分析了不同应用中这些指令的分布。\n\n论文方法描述\n1. **试点数据收集**：通过在加利福尼亚州的住宅区、乡村、商业区、城市街道和高速公路等5条不同路线上驾驶，从Apple Maps、Google Maps和Waze收集了233条语音指令。对收集到的指令进行人工分析，根据其参考性特征进行分类。\n2. **自动化数据收集系统（ADVLAT-Engine）**：\n - **数据模态**：系统同步收集三个模态的数据：面向车外的视频流（视觉）、GPS应用的音频输出（语言指令）和车辆的经纬度坐标（动作）。\n - **语音转录**：使用OpenAI的Whisper模型将GPS应用的音频指令转换为带时间戳的文本，作为语言指令和事件标识符。\n - **数据同步**：将视频帧、GPS轨迹点和转录的文本指令进行时间同步，形成视觉-语言-动作三元组数据。该系统可扩展以集成LiDAR或CAN总线等其他模态。\n\n论文使用数据集和训练资源\n1. **数据集**：论文使用了一个自建的试点数据集，包含从Apple Maps、Google Maps和Waze收集的233条语音指令，这些指令来自在加利福尼亚州5条不同路线上的驾驶过程。该数据集用于指令分类和方法演示，而非训练一个完整的模型。\n2. **训练资源**：论文的重点是数据整理方法，并未描述对一个端到端导航模型的训练过程。在数据处理流程中，使用了预训练的OpenAI Whisper模型进行语音转文本。论文未提及具体的硬件资源（如GPU）或训练时间。\n\n论文使用的评估环境和评估指标\n本文没有提出一个需要进行传统性能评估的导航模型，其评估重点在于所提出的数据整理方法和系统原型。\n1. **评估方式**：\n - **定性演示**：通过图示（如图2和图3）展示了ADVLAT-Engine原型系统的工作流程和输出样本，证明其能够生成同步的视觉-语言-动作三元组。\n - **统计分析**：对试点数据集进行分类和统计分析，例如，使用表格（如表III和表IV）展示不同导航应用中各类指令的出现频率和多属性指令的组合模式，以量化分析指令的多样性和结构。\n2. **评估指标**：未使用导航任务的传统指标（如任务完成率、路径长度等）。评估指标主要是指令类别的分布频率和多属性组合的计数，用以证明GPS指令的丰富性和可自动分类性。",
    "summary_html": "<p>论文研究单位</p>\n<p>Machine Intelligence, Interaction, and Imagination (Mi^3) Laboratory, University of California, Merced</p>\n\n<p>论文概述</p>\n<p>指令-动作（IA）数据对对于训练机器人系统，特别是自动驾驶汽车（AV）非常有价值，但人工标注数据成本高且效率低下。本文探索了利用移动应用的全球定位系统（GPS）数据和自然语言处理（NLP）技术来自动生成大规模的IA指令与响应，无需人工生成或事后标记数据。通过在试点数据收集中驾驶至不同目的地并收集来自GPS应用的语音指令，论文展示了一种收集和分类多样化指令集的方法，并辅以视频数据形成完整的视觉-语言-动作三元组。论文详细介绍了一个完全自动化的数据收集原型系统ADVLAT-Engine，并将收集到的GPS语音指令分为八种不同的类别，突显了从免费移动应用中可获取的指令广度和参考性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>探索了三种常见的导航应用（Apple Maps, Google Maps, Waze），分析了它们在口头指令输出上的差异，并评估了其在参考性视觉-语言导航模型中的潜在用途。</li><li>提出了一个自动化流程，用于创建包含视频（视觉）、指令（语言）和车辆轨迹（动作）的标注数据，以供视觉-语言-动作（VLA）模型使用。</li><li>设计并实现了一个名为ADVLAT-Engine（Autonomous Driving Vision-Language-Action Triad Engine）的原型系统，用于自动化收集视觉-语言-动作三元组数据。</li><li>将GPS语音指令归纳为八个类别：道路名称、位置距离、静态物体、转向、基本方位、地点名称、车道信息和灯光信息，并分析了不同应用中这些指令的分布。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li><strong>试点数据收集</strong>：通过在加利福尼亚州的住宅区、乡村、商业区、城市街道和高速公路等5条不同路线上驾驶，从Apple Maps、Google Maps和Waze收集了233条语音指令。对收集到的指令进行人工分析，根据其参考性特征进行分类。</li><li><strong>自动化数据收集系统（ADVLAT-Engine）</strong>：</li></ol>\n<p> - <strong>数据模态</strong>：系统同步收集三个模态的数据：面向车外的视频流（视觉）、GPS应用的音频输出（语言指令）和车辆的经纬度坐标（动作）。</p>\n<p> - <strong>语音转录</strong>：使用OpenAI的Whisper模型将GPS应用的音频指令转换为带时间戳的文本，作为语言指令和事件标识符。</p>\n<p> - <strong>数据同步</strong>：将视频帧、GPS轨迹点和转录的文本指令进行时间同步，形成视觉-语言-动作三元组数据。该系统可扩展以集成LiDAR或CAN总线等其他模态。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li><strong>数据集</strong>：论文使用了一个自建的试点数据集，包含从Apple Maps、Google Maps和Waze收集的233条语音指令，这些指令来自在加利福尼亚州5条不同路线上的驾驶过程。该数据集用于指令分类和方法演示，而非训练一个完整的模型。</li><li><strong>训练资源</strong>：论文的重点是数据整理方法，并未描述对一个端到端导航模型的训练过程。在数据处理流程中，使用了预训练的OpenAI Whisper模型进行语音转文本。论文未提及具体的硬件资源（如GPU）或训练时间。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>本文没有提出一个需要进行传统性能评估的导航模型，其评估重点在于所提出的数据整理方法和系统原型。</p>\n<p>1. <strong>评估方式</strong>：</p>\n<p> - <strong>定性演示</strong>：通过图示（如图2和图3）展示了ADVLAT-Engine原型系统的工作流程和输出样本，证明其能够生成同步的视觉-语言-动作三元组。</p>\n<p> - <strong>统计分析</strong>：对试点数据集进行分类和统计分析，例如，使用表格（如表III和表IV）展示不同导航应用中各类指令的出现频率和多属性指令的组合模式，以量化分析指令的多样性和结构。</p>\n<p>2. <strong>评估指标</strong>：未使用导航任务的传统指标（如任务完成率、路径长度等）。评估指标主要是指令类别的分布频率和多属性组合的计数，用以证明GPS指令的丰富性和可自动分类性。</p>"
  },
  {
    "date": "2025-05-04",
    "title": "Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions",
    "link": "http://arxiv.org/abs/2505.02152",
    "summary_markdown": "### 论文研究单位\n- 上海交通大学\n- 加州大学伯克利分校\n- 北卡罗来纳大学教堂山分校\n### 论文概述\n本文提出了一种名为Interleave-VLA的新型机器人学习范式，旨在通过交错的图像-文本指令增强机器人操作。现有的文本指令机器人策略在泛化到未见场景时存在局限，而交错图像-文本指令能提供更丰富、偏差更小的上下文，使机器人更好地处理基于上下文视觉定位的未见任务。论文介绍了首个能够理解交错指令并直接生成连续动作序列的Interleave-VLA范式，以及一个包含21万次交互的大规模真实世界交错数据集Open Interleaved X-Embodiment。实验表明，该方法在域外泛化上比文本基线高2倍，并支持零样本泛化到草图等多样化指令。\n### 论文核心贡献点\n1. **提出Interleave-VLA范式**：首个支持交错图像-文本指令输入的机器人学习范式，通过最小修改扩展现有VLA模型（如π₀和OpenVLA），实现强零样本泛化。\n2. **构建大规模数据集**：开发了自动化流程，将Open X-Embodiment的文本指令转换为交错指令，生成包含21万次交互和1300万帧的真实世界数据集。\n3. **揭示注意力幻觉问题**：分析表明文本VLA的泛化失败常源于注意力偏差、扩散和泄漏，而交错指令通过上下文视觉定位有效缓解此问题。\n### 论文方法描述\n- **范式设计**：Interleave-VLA包含三个核心组件：\n 1. **轻量适配模块**：在tokenizer中引入特殊分隔符（如<BOI>和<EOI>），使现有VLA无需架构变更即可处理交错输入。\n 2. **可扩展训练流程**：利用交错数据集训练，保留原始目标和超参数。\n 3. **多功能推理接口**：支持文本或交错指令，允许使用实时摄像头裁剪图、网络图片或手绘草图。\n- **数据集构建**：自动化流程包括三步：\n 1. **指令解析**：用Qwen2.5提取关键对象。\n 2. **开放词汇检测**：用OWLv2定位并裁剪目标对象（准确率82.6%）。\n 3. **数据验证**：用Qwen2.5-VL验证并结合Segment Anything优化，准确率提升至95.6%。\n### 论文使用数据集和训练资源\n- **数据集**：基于Open X-Embodiment构建的Open Interleaved X-Embodiment数据集，包含21万次交互、1300万帧、3500个独特物体和多种任务。数据增强包括随机混合网络图像。\n- **训练资源**：使用π₀和OpenVLA作为基础模型，在交错数据集上微调，无需修改超参数。具体GPU集群细节未明确说明，但实验涉及大规模训练。\n### 论文使用的评估环境和评估指标\n- **仿真环境**：SimplerEnv（WidowX配置），评估域内（4个任务）和域外任务（视觉泛化、语义泛化各3个任务）。\n- **真实机器人**：FANUC LRMate 200iD/7L机械臂，SMC夹爪，测试食品抓取和厨具放置任务。\n- **评估指标**：\n - **成功率**：任务完成比例。\n - **物体选择准确率**：正确选择目标物体的比例。\n - **泛化性能**：域外任务的相对提升（如2×增益）。\n - **零样本能力**：对草图、裁剪图等未见指令类型的准确率。</think>\n### 论文研究单位\n上海交通大学, UC Berkeley, UNC Chapel Hill\n### 论文概述\n本文提出了一种名为Interleave-VLA的新型机器人学习范式，旨在通过交错的图像-文本指令增强机器人操作能力。现有基于纯文本指令的机器人策略在泛化到未见场景时存在局限，而交错图像-文本指令能提供更丰富、偏差更小的上下文，使机器人更好地处理基于上下文视觉定位的未见任务。论文介绍了首个能够理解交错指令并直接生成连续动作序列的Interleave-VLA范式，以及一个包含21万次交互的大规模真实世界交错数据集Open Interleaved X-Embodiment。实验表明，该方法在域外泛化上比文本基线高2倍，并支持零样本泛化到草图等多样化指令。\n### 论文核心贡献点\n1. 提出Interleave-VLA范式：首个支持交错图像-文本指令输入的机器人学习范式，通过最小修改扩展现有VLA模型（如π₀和OpenVLA），实现强零样本泛化。\n2. 构建大规模数据集：开发了自动化流程，将Open X-Embodiment的文本指令转换为交错指令，生成包含21万次交互和1300万帧的真实世界数据集。\n3. 揭示注意力幻觉问题：分析表明文本VLA的泛化失败常源于注意力偏差、扩散和泄漏，而交错指令通过上下文视觉定位有效缓解此问题。\n### 论文方法描述\nInterleave-VLA包含三个核心组件：\n1. 轻量适配模块：在tokenizer中引入特殊分隔符（如<BOI>和<EOI>），使现有VLA无需架构变更即可处理交错输入。\n2. 可扩展训练流程：利用交错数据集训练，保留原始目标和超参数。\n3. 多功能推理接口：支持文本或交错指令，允许使用实时摄像头裁剪图、网络图片或手绘草图。\n数据集构建流程包括三步：指令解析（用Qwen2.5提取关键对象）、开放词汇检测（用OWLv2定位并裁剪目标对象，准确率82.6%）、数据验证（用Qwen2.5-VL验证并结合Segment Anything优化，准确率提升至95.6%）。\n### 论文使用数据集和训练资源\n数据集为Open Interleaved X-Embodiment，基于Open X-Embodiment构建，包含21万次交互、1300万帧、3500个独特物体和多种任务。数据增强包括随机混合网络图像。训练资源使用π₀和OpenVLA作为基础模型，在交错数据集上微调，无需修改超参数。\n### 论文使用的评估环境和评估指标\n仿真环境为SimplerEnv（WidowX配置），评估域内（4个任务）和域外任务（视觉泛化、语义泛化各3个任务）。真实机器人为FANUC LRMate 200iD/7L机械臂，SMC夹爪，测试食品抓取和厨具放置任务。评估指标包括成功率、物体选择准确率、泛化性能（域外任务相对提升）和零样本能力（对草图、裁剪图等未见指令类型的准确率）。",
    "summary_html": "<h3>论文研究单位</h3>\n<ul><li>上海交通大学</li><li>加州大学伯克利分校</li><li>北卡罗来纳大学教堂山分校</li></ul>\n<h3>论文概述</h3>\n<p>本文提出了一种名为Interleave-VLA的新型机器人学习范式，旨在通过交错的图像-文本指令增强机器人操作。现有的文本指令机器人策略在泛化到未见场景时存在局限，而交错图像-文本指令能提供更丰富、偏差更小的上下文，使机器人更好地处理基于上下文视觉定位的未见任务。论文介绍了首个能够理解交错指令并直接生成连续动作序列的Interleave-VLA范式，以及一个包含21万次交互的大规模真实世界交错数据集Open Interleaved X-Embodiment。实验表明，该方法在域外泛化上比文本基线高2倍，并支持零样本泛化到草图等多样化指令。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>提出Interleave-VLA范式</strong>：首个支持交错图像-文本指令输入的机器人学习范式，通过最小修改扩展现有VLA模型（如π₀和OpenVLA），实现强零样本泛化。</li><li><strong>构建大规模数据集</strong>：开发了自动化流程，将Open X-Embodiment的文本指令转换为交错指令，生成包含21万次交互和1300万帧的真实世界数据集。</li><li><strong>揭示注意力幻觉问题</strong>：分析表明文本VLA的泛化失败常源于注意力偏差、扩散和泄漏，而交错指令通过上下文视觉定位有效缓解此问题。</li></ol>\n<h3>论文方法描述</h3>\n<ul><li><strong>范式设计</strong>：Interleave-VLA包含三个核心组件：</li></ul>\n<p> 1. <strong>轻量适配模块</strong>：在tokenizer中引入特殊分隔符（如<BOI>和<EOI>），使现有VLA无需架构变更即可处理交错输入。</p>\n<p> 2. <strong>可扩展训练流程</strong>：利用交错数据集训练，保留原始目标和超参数。</p>\n<p> 3. <strong>多功能推理接口</strong>：支持文本或交错指令，允许使用实时摄像头裁剪图、网络图片或手绘草图。</p>\n<ul><li><strong>数据集构建</strong>：自动化流程包括三步：</li></ul>\n<p> 1. <strong>指令解析</strong>：用Qwen2.5提取关键对象。</p>\n<p> 2. <strong>开放词汇检测</strong>：用OWLv2定位并裁剪目标对象（准确率82.6%）。</p>\n<p> 3. <strong>数据验证</strong>：用Qwen2.5-VL验证并结合Segment Anything优化，准确率提升至95.6%。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：基于Open X-Embodiment构建的Open Interleaved X-Embodiment数据集，包含21万次交互、1300万帧、3500个独特物体和多种任务。数据增强包括随机混合网络图像。</li><li><strong>训练资源</strong>：使用π₀和OpenVLA作为基础模型，在交错数据集上微调，无需修改超参数。具体GPU集群细节未明确说明，但实验涉及大规模训练。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>仿真环境</strong>：SimplerEnv（WidowX配置），评估域内（4个任务）和域外任务（视觉泛化、语义泛化各3个任务）。</li><li><strong>真实机器人</strong>：FANUC LRMate 200iD/7L机械臂，SMC夹爪，测试食品抓取和厨具放置任务。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>成功率</strong>：任务完成比例。</p>\n<p> - <strong>物体选择准确率</strong>：正确选择目标物体的比例。</p>\n<p> - <strong>泛化性能</strong>：域外任务的相对提升（如2×增益）。</p>\n<p> - <strong>零样本能力</strong>：对草图、裁剪图等未见指令类型的准确率。</think></p>\n<h3>论文研究单位</h3>\n<p>上海交通大学, UC Berkeley, UNC Chapel Hill</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为Interleave-VLA的新型机器人学习范式，旨在通过交错的图像-文本指令增强机器人操作能力。现有基于纯文本指令的机器人策略在泛化到未见场景时存在局限，而交错图像-文本指令能提供更丰富、偏差更小的上下文，使机器人更好地处理基于上下文视觉定位的未见任务。论文介绍了首个能够理解交错指令并直接生成连续动作序列的Interleave-VLA范式，以及一个包含21万次交互的大规模真实世界交错数据集Open Interleaved X-Embodiment。实验表明，该方法在域外泛化上比文本基线高2倍，并支持零样本泛化到草图等多样化指令。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出Interleave-VLA范式：首个支持交错图像-文本指令输入的机器人学习范式，通过最小修改扩展现有VLA模型（如π₀和OpenVLA），实现强零样本泛化。</li><li>构建大规模数据集：开发了自动化流程，将Open X-Embodiment的文本指令转换为交错指令，生成包含21万次交互和1300万帧的真实世界数据集。</li><li>揭示注意力幻觉问题：分析表明文本VLA的泛化失败常源于注意力偏差、扩散和泄漏，而交错指令通过上下文视觉定位有效缓解此问题。</li></ol>\n<h3>论文方法描述</h3>\n<p>Interleave-VLA包含三个核心组件：</p>\n<ol><li>轻量适配模块：在tokenizer中引入特殊分隔符（如<BOI>和<EOI>），使现有VLA无需架构变更即可处理交错输入。</li><li>可扩展训练流程：利用交错数据集训练，保留原始目标和超参数。</li><li>多功能推理接口：支持文本或交错指令，允许使用实时摄像头裁剪图、网络图片或手绘草图。</li></ol>\n<p>数据集构建流程包括三步：指令解析（用Qwen2.5提取关键对象）、开放词汇检测（用OWLv2定位并裁剪目标对象，准确率82.6%）、数据验证（用Qwen2.5-VL验证并结合Segment Anything优化，准确率提升至95.6%）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集为Open Interleaved X-Embodiment，基于Open X-Embodiment构建，包含21万次交互、1300万帧、3500个独特物体和多种任务。数据增强包括随机混合网络图像。训练资源使用π₀和OpenVLA作为基础模型，在交错数据集上微调，无需修改超参数。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>仿真环境为SimplerEnv（WidowX配置），评估域内（4个任务）和域外任务（视觉泛化、语义泛化各3个任务）。真实机器人为FANUC LRMate 200iD/7L机械臂，SMC夹爪，测试食品抓取和厨具放置任务。评估指标包括成功率、物体选择准确率、泛化性能（域外任务相对提升）和零样本能力（对草图、裁剪图等未见指令类型的准确率）。</p>"
  },
  {
    "date": "2025-04-28",
    "title": "NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks",
    "link": "http://arxiv.org/abs/2504.19854",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n新加坡科技与设计大学，Lambda Labs\n## 论文概述\n本文提出了NORA（Neural Orchestrator for Robotic Autonomy），一个3B参数的小型开源通用视觉-语言-动作（VLA）模型，用于处理具身任务。NORA旨在减少现有大型VLA模型的高计算开销，同时保持强大的任务执行能力。模型基于Qwen-2.5-VL-3B多模态模型，并利用FAST+标记器进行高效的动作序列生成。实验表明，NORA在多个现实世界和模拟任务中优于现有的大型VLA模型，以显著降低的计算开销实现了更好的任务性能，使其成为实时机器人自主性更实用的解决方案。\n## 论文核心贡献点\n1. 提出了NORA，一个3B参数的VLA模型，基于Qwen-2.5-VL-3B主干，采用高效的动作解码策略压缩高度相关的动作标记，确保在各种机器人任务中的鲁棒性能。\n2. 进行了全面的实验，分析了不同动作预测策略的影响，包括单步与分块动作预测的详细比较，证明了其设计在提高动作生成效率方面的有效性。\n3. 开源了完整的NORA框架，包括模型检查点、训练策略和评估协议，以促进可重复性并推动可扩展的视觉-语言-动作模型在机器人学领域的进一步研究。\n## 论文方法描述\nNORA的架构基于预训练的视觉-语言模型（VLM）M，该模型自回归地预测从时间t到t+N的动作块，编码未来的动作。输入包括自然语言任务指令c和t时间的n帧视觉观测ot，形成整体输入Xt=[ot,c]。动作块at:t+N由FAST+机器人标记器在训练时编码的离散标记序列R=[rt,...,rt+N]表示。VLM M基于Xt条件预测动作块的标记序列R，然后使用FAST+_decode将标记序列解码为连续动作。模型选择了Qwen-2.5-VL作为主干，因其3B的小参数规模，并通过FAST+标记器引入了2048个额外标记来增强VLM标记器的词汇量。观测ot保持为单个视觉帧，动作块大小选择为1。NORA使用标准的下一标记预测损失语言建模目标进行训练。\n## 论文使用数据集和训练资源\n模型在Open X-Embodiment数据集上进行预训练，该数据集包含970k真实世界机器人演示，来自不同机器人执行的各种任务，包括BridgeV2和DROID等子集。训练在单个8x H100 GPU节点上进行约三周，总计约4000 H100 GPU小时。使用批量大小为256，采用AdamW优化器进行110万次梯度更新。应用了前50k步的线性预热，峰值学习率为5e-5，随后采用余弦衰减至零。为增强训练效率和减少内存占用，使用了FlashAttention并以bf16精度训练。此外，训练了NORA-Long变体，使用动作块大小为5，在相同预训练数据集上预训练了90万步。\n## 论文使用的评估环境和评估指标\n评估在两种环境中进行：真实世界的WidowX机器人平台和LIBERO模拟基准。真实世界评估包括9个多样任务，每个任务进行10次试验，评估指令理解、空间推理和多任务运动规划能力。模拟基准包括30个程序生成的解耦任务，涉及空间布局（LIBERO-Spatial）、物体（LIBERO-Object）、任务目标（LIBERO-Goal）和10个长视野纠缠任务（LIBERO-Long）。所有策略在相同设置下进行500次试验评估。主要评估指标为成功率，定义为成功完成任务的比例，计算公式为：% success rate := (100 E_{τ~D_eval} 1[task τ is successfully completed])%。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>新加坡科技与设计大学，Lambda Labs</p>\n<h2>论文概述</h2>\n<p>本文提出了NORA（Neural Orchestrator for Robotic Autonomy），一个3B参数的小型开源通用视觉-语言-动作（VLA）模型，用于处理具身任务。NORA旨在减少现有大型VLA模型的高计算开销，同时保持强大的任务执行能力。模型基于Qwen-2.5-VL-3B多模态模型，并利用FAST+标记器进行高效的动作序列生成。实验表明，NORA在多个现实世界和模拟任务中优于现有的大型VLA模型，以显著降低的计算开销实现了更好的任务性能，使其成为实时机器人自主性更实用的解决方案。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出了NORA，一个3B参数的VLA模型，基于Qwen-2.5-VL-3B主干，采用高效的动作解码策略压缩高度相关的动作标记，确保在各种机器人任务中的鲁棒性能。</li><li>进行了全面的实验，分析了不同动作预测策略的影响，包括单步与分块动作预测的详细比较，证明了其设计在提高动作生成效率方面的有效性。</li><li>开源了完整的NORA框架，包括模型检查点、训练策略和评估协议，以促进可重复性并推动可扩展的视觉-语言-动作模型在机器人学领域的进一步研究。</li></ol>\n<h2>论文方法描述</h2>\n<p>NORA的架构基于预训练的视觉-语言模型（VLM）M，该模型自回归地预测从时间t到t+N的动作块，编码未来的动作。输入包括自然语言任务指令c和t时间的n帧视觉观测ot，形成整体输入Xt=[ot,c]。动作块at:t+N由FAST+机器人标记器在训练时编码的离散标记序列R=[rt,...,rt+N]表示。VLM M基于Xt条件预测动作块的标记序列R，然后使用FAST+_decode将标记序列解码为连续动作。模型选择了Qwen-2.5-VL作为主干，因其3B的小参数规模，并通过FAST+标记器引入了2048个额外标记来增强VLM标记器的词汇量。观测ot保持为单个视觉帧，动作块大小选择为1。NORA使用标准的下一标记预测损失语言建模目标进行训练。</p>\n<h2>论文使用数据集和训练资源</h2>\n<p>模型在Open X-Embodiment数据集上进行预训练，该数据集包含970k真实世界机器人演示，来自不同机器人执行的各种任务，包括BridgeV2和DROID等子集。训练在单个8x H100 GPU节点上进行约三周，总计约4000 H100 GPU小时。使用批量大小为256，采用AdamW优化器进行110万次梯度更新。应用了前50k步的线性预热，峰值学习率为5e-5，随后采用余弦衰减至零。为增强训练效率和减少内存占用，使用了FlashAttention并以bf16精度训练。此外，训练了NORA-Long变体，使用动作块大小为5，在相同预训练数据集上预训练了90万步。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<p>评估在两种环境中进行：真实世界的WidowX机器人平台和LIBERO模拟基准。真实世界评估包括9个多样任务，每个任务进行10次试验，评估指令理解、空间推理和多任务运动规划能力。模拟基准包括30个程序生成的解耦任务，涉及空间布局（LIBERO-Spatial）、物体（LIBERO-Object）、任务目标（LIBERO-Goal）和10个长视野纠缠任务（LIBERO-Long）。所有策略在相同设置下进行500次试验评估。主要评估指标为成功率，定义为成功完成任务的比例，计算公式为：% success rate := (100 E_{τ~D_eval} 1[task τ is successfully completed])%。</p>"
  },
  {
    "date": "2025-04-22",
    "title": "$π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization",
    "link": "http://arxiv.org/abs/2504.16054",
    "summary_markdown": "# 论文总结\n## 论文研究单位\n- Physical Intelligence\n## 论文概述\n- 本文提出了一个名为 π₀.₅ 的视觉-语言-动作（VLA）模型，旨在通过在异构任务上进行协同训练，实现机器人在开放世界中的泛化能力。\n- 模型的核心思想是结合来自多种信息源的知识，包括多种机器人平台的数据、网络数据、高级语义预测以及人类语言指令。\n- 论文声称，π₀.₅ 是首个能够在训练中从未见过的全新家庭环境中，执行长达10到15分钟的长时程、灵巧操作任务（如清理厨房或卧室）的端到端学习系统。\n## 论文核心贡献点\n- 提出了一个新的 VLA 模型 π₀.₅ 及其相应的训练配方。\n- 设计了一个协同训练框架，该框架能够有效地整合和利用异构数据源，以提升模型的泛化能力。\n- 实现了一个层次化的推理架构，模型在执行任务时首先预测高级语义子任务，然后基于该子任务生成低级动作。\n- 通过详尽的实验，首次在真实、未见过的家庭环境中，展示了端到端学习系统完成长时程复杂操作任务的能力。\n## 论文方法描述\n- **模型架构**:\n - 模型基于一个统一的 Transformer，能够处理多模态输入（图像、文本、机器人状态）并输出多模态结果（文本子任务、连续动作）。\n - 引入了“动作专家”模块，类似于混合专家模型，专门用于生成连续的动作序列，以提高效率和性能。\n - 将策略分解为 p(动作, 子任务 \\|观测, 指令) = p(动作 \\|观测, 子任务) * p(子任务 \\|观测, 指令)，实现高层次与低层次推理的解耦。\n- **训练过程**:\n - **第一阶段：预训练**\n - 目标：利用大规模多样化的数据混合来适应模型。\n - 数据来源：包括移动机械臂数据（约400小时）、其他非移动机器人数据、实验室跨具身数据、高级语义子任务预测数据、人类语言指令以及网络多模态数据。\n - 动作表示：在此阶段使用离散令牌来表示动作。\n - **第二阶段：后训练**\n - 目标：专门针对移动操作任务进行微调。\n - 数据来源：专注于与任务最相关的移动机械臂数据和人类口头语言指令。\n - 动作表示：改用流匹配来表示连续的动作分布，并引入动作专家，以实现更精细的实时控制。\n- **推理过程**:\n - 采用层次化推理，模型在每一步首先预测一个高级语义子任务（如“拿起盘子”）。\n - 然后，模型以该子任务为条件，通过动作专家预测下一步的低级动作块。\n## 论文使用数据集和训练资源\n- **数据集**:\n - 未使用单一命名数据集，而是采用异构数据混合。\n - 核心数据包括在多个真实家庭中收集的约400小时的移动机械臂操作数据。\n - 辅助数据包括：其他机器人数据、实验室跨具身任务数据、高级语义子任务预测数据、人类语言指令和网络数据（如图像说明、问答、对象定位）。\n- **训练资源**:\n - 论文未明确说明硬件配置，但训练如此规模的视觉-语言-动作模型和数据集需要大量的计算资源（如大规模GPU集群）。\n## 论文使用的评估环境和评估指标\n- **评估环境**:\n - 主要在真实世界的、全新的家庭环境中进行评估，这些环境在训练期间模型从未见过。\n - 测试场景包括全新的厨房和卧室。\n - 评估任务是长时程、多阶段的复杂操作，例如“清理整个厨房”。\n- **评估指标**:\n - 主要指标是任务成功率，通过一个详细的任务评估标准来判断。\n - 评估模型在执行长达10-15分钟任务中的表现。\n - 包含了与其他视觉-语言-动作（VLA）模型的定量比较，以评估其相对性能和泛化能力。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<ul><li>Physical Intelligence</li></ul>\n<h2>论文概述</h2>\n<ul><li>本文提出了一个名为 π₀.₅ 的视觉-语言-动作（VLA）模型，旨在通过在异构任务上进行协同训练，实现机器人在开放世界中的泛化能力。</li><li>模型的核心思想是结合来自多种信息源的知识，包括多种机器人平台的数据、网络数据、高级语义预测以及人类语言指令。</li><li>论文声称，π₀.₅ 是首个能够在训练中从未见过的全新家庭环境中，执行长达10到15分钟的长时程、灵巧操作任务（如清理厨房或卧室）的端到端学习系统。</li></ul>\n<h2>论文核心贡献点</h2>\n<ul><li>提出了一个新的 VLA 模型 π₀.₅ 及其相应的训练配方。</li><li>设计了一个协同训练框架，该框架能够有效地整合和利用异构数据源，以提升模型的泛化能力。</li><li>实现了一个层次化的推理架构，模型在执行任务时首先预测高级语义子任务，然后基于该子任务生成低级动作。</li><li>通过详尽的实验，首次在真实、未见过的家庭环境中，展示了端到端学习系统完成长时程复杂操作任务的能力。</li></ul>\n<h2>论文方法描述</h2>\n<ul><li><strong>模型架构</strong>:</li></ul>\n<p> - 模型基于一个统一的 Transformer，能够处理多模态输入（图像、文本、机器人状态）并输出多模态结果（文本子任务、连续动作）。</p>\n<p> - 引入了“动作专家”模块，类似于混合专家模型，专门用于生成连续的动作序列，以提高效率和性能。</p>\n<p> - 将策略分解为 p(动作, 子任务 \\|观测, 指令) = p(动作 \\|观测, 子任务) * p(子任务 \\|观测, 指令)，实现高层次与低层次推理的解耦。</p>\n<ul><li><strong>训练过程</strong>:</li></ul>\n<p> - <strong>第一阶段：预训练</strong></p>\n<p> - 目标：利用大规模多样化的数据混合来适应模型。</p>\n<p> - 数据来源：包括移动机械臂数据（约400小时）、其他非移动机器人数据、实验室跨具身数据、高级语义子任务预测数据、人类语言指令以及网络多模态数据。</p>\n<p> - 动作表示：在此阶段使用离散令牌来表示动作。</p>\n<p> - <strong>第二阶段：后训练</strong></p>\n<p> - 目标：专门针对移动操作任务进行微调。</p>\n<p> - 数据来源：专注于与任务最相关的移动机械臂数据和人类口头语言指令。</p>\n<p> - 动作表示：改用流匹配来表示连续的动作分布，并引入动作专家，以实现更精细的实时控制。</p>\n<ul><li><strong>推理过程</strong>:</li></ul>\n<p> - 采用层次化推理，模型在每一步首先预测一个高级语义子任务（如“拿起盘子”）。</p>\n<p> - 然后，模型以该子任务为条件，通过动作专家预测下一步的低级动作块。</p>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> - 未使用单一命名数据集，而是采用异构数据混合。</p>\n<p> - 核心数据包括在多个真实家庭中收集的约400小时的移动机械臂操作数据。</p>\n<p> - 辅助数据包括：其他机器人数据、实验室跨具身任务数据、高级语义子任务预测数据、人类语言指令和网络数据（如图像说明、问答、对象定位）。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - 论文未明确说明硬件配置，但训练如此规模的视觉-语言-动作模型和数据集需要大量的计算资源（如大规模GPU集群）。</p>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - 主要在真实世界的、全新的家庭环境中进行评估，这些环境在训练期间模型从未见过。</p>\n<p> - 测试场景包括全新的厨房和卧室。</p>\n<p> - 评估任务是长时程、多阶段的复杂操作，例如“清理整个厨房”。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - 主要指标是任务成功率，通过一个详细的任务评估标准来判断。</p>\n<p> - 评估模型在执行长达10-15分钟任务中的表现。</p>\n<p> - 包含了与其他视觉-语言-动作（VLA）模型的定量比较，以评估其相对性能和泛化能力。</p>"
  },
  {
    "date": "2025-04-01",
    "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning",
    "link": "http://arxiv.org/abs/2504.00907",
    "summary_markdown": "### 论文研究单位\nGeorgia Institute of Technology 和 Meta FAIR\n### 论文概述\n本文提出 Ask-to-Act 任务，研究具身智能体在部分可观测的家居环境中，如何通过最少且相关的澄清问题解决模糊指令，完成单或多物体重排任务。作者提出 AutoAsk 方法，利用强化学习（RL）微调多模态大语言模型（MLLM）作为视觉-语言-行动（VLA）策略，通过大型语言模型（LLM）生成奖励信号，无需大规模人类演示或手动设计奖励。实验表明，该方法在未见场景和任务上显著优于零样本基线（如 GPT-4o）和监督微调模型。\n### 论文核心贡献点\n- 引入 Ask-to-Act 任务，评估智能体通过提问解决模糊指令的能力，涵盖属性识别、空间推理、物体大小等七类子任务。\n- 提出 AutoAsk 方法，首次实现基于 LLM 生成奖励的在线 RL 训练 VLA 策略，使智能体能同时行动和提问。\n- 验证 LLM 生成奖励的有效性，消除对人工奖励或人类数据的依赖，在未见场景和任务上成功率提升 10.4%-16.5%。\n- 展示训练策略在问题预算下的可调性，平衡成功率和提问效率。\n### 论文方法描述\nAutoAsk 基于 LLaVA-OneVision 0.5B 架构，适配为 VLA 策略：\n- **策略架构**：输入包括任务指令、历史观察、动作和用户回复。视觉观察通过 Perceiver 模型下采样至 4 个 token，以支持长历史序列处理。\n- **训练机制**：使用 DD-PPO（分布式 PPO）进行在线 RL，训练 50M 步。奖励函数由 LLM（Llama-3）生成，包含成功奖励、子目标奖励、有用问题奖励、超预算惩罚和步数惩罚。\n- **奖励生成**：LLM 根据特权环境状态、任务指令和智能体动作，评估提问是否有效解决模糊性，输出二进制奖励。\n- **实现细节**：异步处理奖励请求，使用 8 个 A40 GPU，vLLM 服务器加速推理。\n### 论文使用数据集和训练资源\n- **数据集**：在 Habitat 3.0 中构建，使用 ReplicaCAD 数据集的 83 个场景（63 个训练，20 个评估）和 Google Scanned Objects 的 42 个物体类别。任务包括单物体和多物体重排，覆盖无模糊性、属性识别、空间推理等七类。\n- **训练资源**：使用 8 个 NVIDIA A40 GPU，分布式训练 5000 万步。奖励生成模型为 Llama-3 8B，通过 vLLM 服务异步处理。\n### 论文使用的评估环境和评估指标\n- **评估环境**：部分可观测的 Habitat 3.0 模拟器，智能体以 Spot 机器人形态执行任务。评估分两维度：Unseen Scenes（新场景和物体布局）和 Unseen Tasks（新模糊任务组合）。\n- **评估指标**：\n - Success Rate (SR)：任务完成率。\n - Ambiguity-Resolution Efficiency Score (ARS)：衡量任务成功的同时最小化提问数量，计算公式为 $\\frac{\\mathds{1}_{\\text{success}}}{1+\\text{abs}(q_{\\text{relevant}}-K)+q_{\\text{irrelevant}}}$。\n - Question Ratio (QR)：实际提问数与最小需求问题数的比率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Georgia Institute of Technology 和 Meta FAIR</p>\n<h3>论文概述</h3>\n<p>本文提出 Ask-to-Act 任务，研究具身智能体在部分可观测的家居环境中，如何通过最少且相关的澄清问题解决模糊指令，完成单或多物体重排任务。作者提出 AutoAsk 方法，利用强化学习（RL）微调多模态大语言模型（MLLM）作为视觉-语言-行动（VLA）策略，通过大型语言模型（LLM）生成奖励信号，无需大规模人类演示或手动设计奖励。实验表明，该方法在未见场景和任务上显著优于零样本基线（如 GPT-4o）和监督微调模型。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>引入 Ask-to-Act 任务，评估智能体通过提问解决模糊指令的能力，涵盖属性识别、空间推理、物体大小等七类子任务。</li><li>提出 AutoAsk 方法，首次实现基于 LLM 生成奖励的在线 RL 训练 VLA 策略，使智能体能同时行动和提问。</li><li>验证 LLM 生成奖励的有效性，消除对人工奖励或人类数据的依赖，在未见场景和任务上成功率提升 10.4%-16.5%。</li><li>展示训练策略在问题预算下的可调性，平衡成功率和提问效率。</li></ul>\n<h3>论文方法描述</h3>\n<p>AutoAsk 基于 LLaVA-OneVision 0.5B 架构，适配为 VLA 策略：</p>\n<ul><li><strong>策略架构</strong>：输入包括任务指令、历史观察、动作和用户回复。视觉观察通过 Perceiver 模型下采样至 4 个 token，以支持长历史序列处理。</li><li><strong>训练机制</strong>：使用 DD-PPO（分布式 PPO）进行在线 RL，训练 50M 步。奖励函数由 LLM（Llama-3）生成，包含成功奖励、子目标奖励、有用问题奖励、超预算惩罚和步数惩罚。</li><li><strong>奖励生成</strong>：LLM 根据特权环境状态、任务指令和智能体动作，评估提问是否有效解决模糊性，输出二进制奖励。</li><li><strong>实现细节</strong>：异步处理奖励请求，使用 8 个 A40 GPU，vLLM 服务器加速推理。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：在 Habitat 3.0 中构建，使用 ReplicaCAD 数据集的 83 个场景（63 个训练，20 个评估）和 Google Scanned Objects 的 42 个物体类别。任务包括单物体和多物体重排，覆盖无模糊性、属性识别、空间推理等七类。</li><li><strong>训练资源</strong>：使用 8 个 NVIDIA A40 GPU，分布式训练 5000 万步。奖励生成模型为 Llama-3 8B，通过 vLLM 服务异步处理。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：部分可观测的 Habitat 3.0 模拟器，智能体以 Spot 机器人形态执行任务。评估分两维度：Unseen Scenes（新场景和物体布局）和 Unseen Tasks（新模糊任务组合）。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - Success Rate (SR)：任务完成率。</p>\n<p> - Ambiguity-Resolution Efficiency Score (ARS)：衡量任务成功的同时最小化提问数量，计算公式为 $\\frac{\\mathds{1}_{\\text{success}}}{1+\\text{abs}(q_{\\text{relevant}}-K)+q_{\\text{irrelevant}}}$。</p>\n<p> - Question Ratio (QR)：实际提问数与最小需求问题数的比率。</p>"
  },
  {
    "date": "2025-03-30",
    "title": "OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model",
    "link": "http://arxiv.org/abs/2503.23463",
    "summary_markdown": "论文研究单位\nTechnical University of Munich, Ludwig Maximilian University of Munich\n\n论文概述\n本文提出了OpenDriveVLA，一个专为端到端自动驾驶设计的视觉-语言-动作（VLA）模型。该模型基于开源预训练的大型视觉语言模型，根据3D环境感知、自车状态和驾驶员指令生成可靠的驾驶动作。为了弥合驾驶视觉表示与语言嵌入之间的模态鸿沟，论文提出了一种分层视觉-语言对齐过程，将2D和3D结构化视觉标记投影到统一的语义空间。此外，OpenDriveVLA通过自回归智能体-环境-自车交互过程，对自车、周围智能体和静态道路元素之间的动态关系进行建模，确保在空间和行为上都得到充分信息的轨迹规划。在nuScenes数据集上的大量实验表明，OpenDriveVLA在开环轨迹规划和驾驶相关问答任务上取得了最先进的结果。\n\n论文核心贡献点\n- 提出了OpenDriveVLA，一个端到端的视觉-语言-动作模型，能够根据多模态输入生成可靠的驾驶轨迹。\n- 引入了分层视觉-语言特征对齐模块，将结构化的2D和3D视觉标记投影到统一的语义嵌入空间，以促进语言引导的轨迹生成。\n- 设计了智能体-环境-自车交互过程，以捕获自车、动态智能体和静态地图元素之间的交互，显著提高了复杂交通场景下的运动预测准确性和轨迹可靠性。\n- 在nuScenes数据集上的大量实验表明，OpenDriveVLA在开环规划和驾驶相关问答任务上建立了新的最先进结果，持续优于先前的基于LLM和端到端自动驾驶方法。\n\n论文方法描述\nOpenDriveVLA的总体架构始于预训练的视觉编码器，从多视角图像中提取环境表示。视觉模块首先使用共享的2D主干网络从每张图像中提取多尺度2D特征，然后聚合这些特征并提升到鸟瞰图（BEV）空间。为了获得结构化的环境表示，模型采用了三个视觉查询模块：全局场景采样器、智能体查询转换器和地图查询转换器。全局场景采样器从多视角2D特征中编码周围驾驶场景上下文，智能体查询转换器检测和跟踪场景内的动态智能体，地图查询转换器提取静态结构信息，如车道边界和可行驶区域。这些模块的输出构成了后续阶段的视觉环境表示。\n为了弥合提取的视觉标记与预训练LLM词嵌入空间之间的模态鸿沟，模型采用了分层视觉-语言对齐策略。为每种类型的视觉标记（场景、智能体、地图）引入了特定的投影器。在训练过程中，视觉编码器和预训练LLM保持冻结，只有标记特定的投影器是可训练的。通过最大化预测文本与真实文本的对齐概率来实现前向对齐。\n为了嵌入高级驾驶知识并增强模型的推理能力，论文引入了专门的驾驶指令调优阶段。通过使用精心策划的驾驶指令问答数据集，将语言领域的驾驶知识注入模型。该数据集涵盖了广泛的驾驶相关推理，包括感知理解、运动预测、注意力分配、动作推理和高级决策。在指令调优过程中，视觉编码器保持冻结，而标记特定的投影器和LLM被设置为可训练。\n为了实现可靠的轨迹规划，论文引入了条件智能体运动预测任务，作为3D智能体-环境-自车交互建模的代理任务。给定场景和地图标记以及自车状态，LLM根据每个检测到的智能体的投影视觉嵌入预测其未来运动。未来运动表示为一系列航点，学习目标被形式化为在给定场景上下文、地图结构和自车状态的条件下，最大化航点序列的条件概率。\n在这个阶段，OpenDriveVLA将未来的驾驶动作规划为未来几秒内的航点序列。每个航点代表自车在时间步t的2D坐标。为了使LLM能够进行自回归生成，航点首先被标记化为离散的文本标记序列。生成过程随后被转换为一个因果序列预测任务，其中每个标记的预测都基于视觉感知标记、自车状态和驾驶命令。\n\n论文使用数据集和训练资源\n论文使用的数据集是nuScenes数据集。训练资源方面，论文提到了使用预训练的开源视觉语言模型作为基础，并在包含多视角图像、3D检测、跟踪、分割数据以及驾驶相关问答对的定制数据集上进行训练。具体的计算资源（如GPU数量）未在提供的文本中明确说明。\n\n论文使用的评估环境和评估指标\n评估在nuScenes数据集上进行。评估环境包括开环轨迹规划和驾驶相关的问答任务。评估指标方面，对于开环轨迹规划，使用了轨迹规划的准确性和可靠性指标；对于驾驶相关的问答任务，使用了问答准确率指标。论文提到模型在这些指标上取得了最先进的结果，但未提供具体的指标名称或数值。",
    "summary_html": "<p>论文研究单位</p>\n<p>Technical University of Munich, Ludwig Maximilian University of Munich</p>\n\n<p>论文概述</p>\n<p>本文提出了OpenDriveVLA，一个专为端到端自动驾驶设计的视觉-语言-动作（VLA）模型。该模型基于开源预训练的大型视觉语言模型，根据3D环境感知、自车状态和驾驶员指令生成可靠的驾驶动作。为了弥合驾驶视觉表示与语言嵌入之间的模态鸿沟，论文提出了一种分层视觉-语言对齐过程，将2D和3D结构化视觉标记投影到统一的语义空间。此外，OpenDriveVLA通过自回归智能体-环境-自车交互过程，对自车、周围智能体和静态道路元素之间的动态关系进行建模，确保在空间和行为上都得到充分信息的轨迹规划。在nuScenes数据集上的大量实验表明，OpenDriveVLA在开环轨迹规划和驾驶相关问答任务上取得了最先进的结果。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出了OpenDriveVLA，一个端到端的视觉-语言-动作模型，能够根据多模态输入生成可靠的驾驶轨迹。</li><li>引入了分层视觉-语言特征对齐模块，将结构化的2D和3D视觉标记投影到统一的语义嵌入空间，以促进语言引导的轨迹生成。</li><li>设计了智能体-环境-自车交互过程，以捕获自车、动态智能体和静态地图元素之间的交互，显著提高了复杂交通场景下的运动预测准确性和轨迹可靠性。</li><li>在nuScenes数据集上的大量实验表明，OpenDriveVLA在开环规划和驾驶相关问答任务上建立了新的最先进结果，持续优于先前的基于LLM和端到端自动驾驶方法。</li></ul>\n\n<p>论文方法描述</p>\n<p>OpenDriveVLA的总体架构始于预训练的视觉编码器，从多视角图像中提取环境表示。视觉模块首先使用共享的2D主干网络从每张图像中提取多尺度2D特征，然后聚合这些特征并提升到鸟瞰图（BEV）空间。为了获得结构化的环境表示，模型采用了三个视觉查询模块：全局场景采样器、智能体查询转换器和地图查询转换器。全局场景采样器从多视角2D特征中编码周围驾驶场景上下文，智能体查询转换器检测和跟踪场景内的动态智能体，地图查询转换器提取静态结构信息，如车道边界和可行驶区域。这些模块的输出构成了后续阶段的视觉环境表示。</p>\n<p>为了弥合提取的视觉标记与预训练LLM词嵌入空间之间的模态鸿沟，模型采用了分层视觉-语言对齐策略。为每种类型的视觉标记（场景、智能体、地图）引入了特定的投影器。在训练过程中，视觉编码器和预训练LLM保持冻结，只有标记特定的投影器是可训练的。通过最大化预测文本与真实文本的对齐概率来实现前向对齐。</p>\n<p>为了嵌入高级驾驶知识并增强模型的推理能力，论文引入了专门的驾驶指令调优阶段。通过使用精心策划的驾驶指令问答数据集，将语言领域的驾驶知识注入模型。该数据集涵盖了广泛的驾驶相关推理，包括感知理解、运动预测、注意力分配、动作推理和高级决策。在指令调优过程中，视觉编码器保持冻结，而标记特定的投影器和LLM被设置为可训练。</p>\n<p>为了实现可靠的轨迹规划，论文引入了条件智能体运动预测任务，作为3D智能体-环境-自车交互建模的代理任务。给定场景和地图标记以及自车状态，LLM根据每个检测到的智能体的投影视觉嵌入预测其未来运动。未来运动表示为一系列航点，学习目标被形式化为在给定场景上下文、地图结构和自车状态的条件下，最大化航点序列的条件概率。</p>\n<p>在这个阶段，OpenDriveVLA将未来的驾驶动作规划为未来几秒内的航点序列。每个航点代表自车在时间步t的2D坐标。为了使LLM能够进行自回归生成，航点首先被标记化为离散的文本标记序列。生成过程随后被转换为一个因果序列预测任务，其中每个标记的预测都基于视觉感知标记、自车状态和驾驶命令。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>论文使用的数据集是nuScenes数据集。训练资源方面，论文提到了使用预训练的开源视觉语言模型作为基础，并在包含多视角图像、3D检测、跟踪、分割数据以及驾驶相关问答对的定制数据集上进行训练。具体的计算资源（如GPU数量）未在提供的文本中明确说明。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估在nuScenes数据集上进行。评估环境包括开环轨迹规划和驾驶相关的问答任务。评估指标方面，对于开环轨迹规划，使用了轨迹规划的准确性和可靠性指标；对于驾驶相关的问答任务，使用了问答准确率指标。论文提到模型在这些指标上取得了最先进的结果，但未提供具体的指标名称或数值。</p>"
  },
  {
    "date": "2025-03-27",
    "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2503.22020",
    "summary_markdown": "### 论文研究单位\nNVIDIA、斯坦福大学和麻省理工学院。\n### 论文概述\n该论文提出了CoT-VLA，一种视觉-语言-动作模型，它通过引入显式的视觉思维链（CoT）推理来增强模型的性能。CoT-VLA首先生成一个代表未来子目标的图像，然后基于当前观测和生成的子目标图像来生成一个短动作序列以实现该子目标。该方法允许模型在执行动作前进行“视觉思考”，从而提升在复杂操作任务中的表现。论文通过在模拟基准和真实世界实验中进行广泛评估，证明了视觉思维链推理能提升VLA性能，并且在多个机器人平台和任务上实现了最先进的性能。\n### 论文核心贡献点\n1. 引入了一种通过子目标图像生成作为机器人控制中间推理步骤的视觉思维链推理方法。\n2. 提出了CoT-VLA系统，该系统集成了视觉思维链推理，并采用了一种混合注意力机制，将用于像素和文本生成的因果注意力与用于动作预测的全注意力相结合。\n3. 在模拟和真实世界环境中进行了全面评估，证明视觉思维链推理提高了VLA性能，其系统在多个机器人平台和任务上实现了最先进的性能。\n### 论文方法描述\n1. **视觉思维链推理框架**：模型操作分为两个连续阶段。首先，预测一个子目标图像作为中间视觉推理步骤；然后，生成一个动作序列以实现该子目标状态。\n2. **基础视觉-语言模型**：基于VILA-U构建，这是一个能够理解和生成图像和文本标记的统一多模态基础模型。\n3. **训练程序**：包括视觉标记预测（使用因果注意力）和动作标记预测（使用全注意力）。训练目标结合了动作损失和视觉损失。\n4. **混合注意力机制**：对图像或文本生成使用因果注意力，对动作生成使用全注意力。\n5. **动作分块**：预测动作序列而不是单个动作，每个动作被离散化为256个区间。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - 机器人演示数据：Open X-Embodiment数据集（OpenX）的子集，包括第三人称摄像头视图和单臂末端执行器控制（7自由度）。\n - 无动作视频数据：EPIC-KITCHENS和Something-Something V2数据集。\n2. **训练资源**：使用7B的VILA-U模型作为基础模型，所有图像处理在256x256分辨率下进行。\n3. **训练阶段**：\n - **预训练阶段**：在机器人演示数据和无动作视频数据上对模型进行预训练。\n - **适应阶段**：在下游任务部署的机器人设置上收集的任务演示对模型进行微调。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - **模拟基准测试**：LIBERO模拟基准测试。\n - **真实世界实验**：Bridge-V2和Franka-Tabletop真实机器人实验。\n2. **评估指标**：\n - 在LIBERO基准测试中，报告了在不同任务套件（空间、对象、目标、长程）上的平均成功率和标准误差。\n - 在真实世界任务中，通过成功率来评估性能。\n - 与基线方法（如Diffusion Policy、Octo、OpenVLA）进行了比较。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>NVIDIA、斯坦福大学和麻省理工学院。</p>\n<h3>论文概述</h3>\n<p>该论文提出了CoT-VLA，一种视觉-语言-动作模型，它通过引入显式的视觉思维链（CoT）推理来增强模型的性能。CoT-VLA首先生成一个代表未来子目标的图像，然后基于当前观测和生成的子目标图像来生成一个短动作序列以实现该子目标。该方法允许模型在执行动作前进行“视觉思考”，从而提升在复杂操作任务中的表现。论文通过在模拟基准和真实世界实验中进行广泛评估，证明了视觉思维链推理能提升VLA性能，并且在多个机器人平台和任务上实现了最先进的性能。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>引入了一种通过子目标图像生成作为机器人控制中间推理步骤的视觉思维链推理方法。</li><li>提出了CoT-VLA系统，该系统集成了视觉思维链推理，并采用了一种混合注意力机制，将用于像素和文本生成的因果注意力与用于动作预测的全注意力相结合。</li><li>在模拟和真实世界环境中进行了全面评估，证明视觉思维链推理提高了VLA性能，其系统在多个机器人平台和任务上实现了最先进的性能。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>视觉思维链推理框架</strong>：模型操作分为两个连续阶段。首先，预测一个子目标图像作为中间视觉推理步骤；然后，生成一个动作序列以实现该子目标状态。</li><li><strong>基础视觉-语言模型</strong>：基于VILA-U构建，这是一个能够理解和生成图像和文本标记的统一多模态基础模型。</li><li><strong>训练程序</strong>：包括视觉标记预测（使用因果注意力）和动作标记预测（使用全注意力）。训练目标结合了动作损失和视觉损失。</li><li><strong>混合注意力机制</strong>：对图像或文本生成使用因果注意力，对动作生成使用全注意力。</li><li><strong>动作分块</strong>：预测动作序列而不是单个动作，每个动作被离散化为256个区间。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：</p>\n<p> - 机器人演示数据：Open X-Embodiment数据集（OpenX）的子集，包括第三人称摄像头视图和单臂末端执行器控制（7自由度）。</p>\n<p> - 无动作视频数据：EPIC-KITCHENS和Something-Something V2数据集。</p>\n<ol><li><strong>训练资源</strong>：使用7B的VILA-U模型作为基础模型，所有图像处理在256x256分辨率下进行。</li><li><strong>训练阶段</strong>：</li></ol>\n<p> - <strong>预训练阶段</strong>：在机器人演示数据和无动作视频数据上对模型进行预训练。</p>\n<p> - <strong>适应阶段</strong>：在下游任务部署的机器人设置上收集的任务演示对模型进行微调。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - <strong>模拟基准测试</strong>：LIBERO模拟基准测试。</p>\n<p> - <strong>真实世界实验</strong>：Bridge-V2和Franka-Tabletop真实机器人实验。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - 在LIBERO基准测试中，报告了在不同任务套件（空间、对象、目标、长程）上的平均成功率和标准误差。</p>\n<p> - 在真实世界任务中，通过成功率来评估性能。</p>\n<p> - 与基线方法（如Diffusion Policy、Octo、OpenVLA）进行了比较。</p>"
  },
  {
    "date": "2025-03-26",
    "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation",
    "link": "http://arxiv.org/abs/2503.20384",
    "summary_markdown": "### 论文研究单位\n南京大学、香港理工大学、北京大学计算机学院多媒体信息处理国家重点实验室、北京智源人工智能研究院、香港科技大学。\n### 论文概述\n该论文提出了一种名为MoLe-VLA的新型视觉-语言-动作模型，旨在通过动态跳过大型语言模型中的部分层来提高机器人操作的效率。该方法受到神经科学中“浅层脑假说”和混合专家模型的启发，将每个LLM层视为一个专家，并设计了一个时空感知路由器来根据机器人的当前状态选择性地激活相关层，以模仿大脑在认知和因果推理中不同的信号通路。此外，为了弥补因层跳过而损失的LLM认知能力，论文还提出了一种认知自知识蒸馏方法来增强模型对任务需求的理解并生成相关的动作序列。\n### 论文核心贡献点\n1. 受浅层脑假说启发，开发了MoLe框架，模仿人脑信号流，通过路由器实现动态层激活以提高模型效率。\n2. 提出了一种新型层决策路由器STAR，充分利用机器人输入的时空信息做出更准确的激活决策。\n3. 引入了一种自知识蒸馏范式CogKD，以恢复稀疏LLM中因层跳过而丢失的认知信息，提升整体性能。\n### 论文方法描述\n论文方法主要包括三个核心部分：\n1. **混合层架构**：将LLM的每一层视为一个独立的专家，设计路由器动态选择执行哪些层。输入嵌入`x_k`经过路由器生成二进制门控向量`G_mol(x)`，仅top-k个值为1的层`π_k`被执行，其余层被跳过，输出`h_k`根据公式`h_k = G_k * π_k(h_{k-1}) + (1 - G_k) * h_{k-1}`计算。\n2. **时空感知路由器（STAR）**：为克服传统路由器无法捕捉动态具身智能任务中关键时空信息的限制，STAR独立处理来自视觉输入的空间特征和来自文本输入的时间依赖，将其组合成统一表示，以对齐LLM层选择与当前环境需求。\n3. **认知自知识蒸馏（CogKD）**：为了补偿层跳过导致的认知表达能力下降，使用原始全层模型作为教师，MoLe层跳过模型作为学生。引入可学习的“认知token”来整合视觉token和语言指导，通过分析认知token与学生token的相似性来识别关键信息，并自适应地重新加权蒸馏过程。\n### 论文使用数据集和训练资源\n数据集：RLBench仿真环境数据集和真实世界任务数据集。\n训练资源：论文未明确提及具体硬件，但训练过程在多个VLA模型上进行了端到端的实现，使用了扩散动作头，通过最小化预测噪声与真实噪声之间的均方误差进行优化。优化目标包括任务损失`L_task`和可能的负载平衡损失。\n### 论文使用的评估环境和评估指标\n评估环境：RLBench模拟环境和真实世界环境（使用Franka机械臂设置）。\n评估指标：主要指标是任务成功率，计算为在十个任务上的平均成功率。此外，还评估了计算效率，包括推理延迟（或频率）和计算成本（如FLOPs或延迟倍数）。在真实世界中，评估了模型在一系列操作任务上的定性表现。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>南京大学、香港理工大学、北京大学计算机学院多媒体信息处理国家重点实验室、北京智源人工智能研究院、香港科技大学。</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种名为MoLe-VLA的新型视觉-语言-动作模型，旨在通过动态跳过大型语言模型中的部分层来提高机器人操作的效率。该方法受到神经科学中“浅层脑假说”和混合专家模型的启发，将每个LLM层视为一个专家，并设计了一个时空感知路由器来根据机器人的当前状态选择性地激活相关层，以模仿大脑在认知和因果推理中不同的信号通路。此外，为了弥补因层跳过而损失的LLM认知能力，论文还提出了一种认知自知识蒸馏方法来增强模型对任务需求的理解并生成相关的动作序列。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>受浅层脑假说启发，开发了MoLe框架，模仿人脑信号流，通过路由器实现动态层激活以提高模型效率。</li><li>提出了一种新型层决策路由器STAR，充分利用机器人输入的时空信息做出更准确的激活决策。</li><li>引入了一种自知识蒸馏范式CogKD，以恢复稀疏LLM中因层跳过而丢失的认知信息，提升整体性能。</li></ol>\n<h3>论文方法描述</h3>\n<p>论文方法主要包括三个核心部分：</p>\n<ol><li><strong>混合层架构</strong>：将LLM的每一层视为一个独立的专家，设计路由器动态选择执行哪些层。输入嵌入<code>x_k</code>经过路由器生成二进制门控向量<code>G_mol(x)</code>，仅top-k个值为1的层<code>π_k</code>被执行，其余层被跳过，输出<code>h_k</code>根据公式<code>h_k = G_k * π_k(h_{k-1}) + (1 - G_k) * h_{k-1}</code>计算。</li><li><strong>时空感知路由器（STAR）</strong>：为克服传统路由器无法捕捉动态具身智能任务中关键时空信息的限制，STAR独立处理来自视觉输入的空间特征和来自文本输入的时间依赖，将其组合成统一表示，以对齐LLM层选择与当前环境需求。</li><li><strong>认知自知识蒸馏（CogKD）</strong>：为了补偿层跳过导致的认知表达能力下降，使用原始全层模型作为教师，MoLe层跳过模型作为学生。引入可学习的“认知token”来整合视觉token和语言指导，通过分析认知token与学生token的相似性来识别关键信息，并自适应地重新加权蒸馏过程。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：RLBench仿真环境数据集和真实世界任务数据集。</p>\n<p>训练资源：论文未明确提及具体硬件，但训练过程在多个VLA模型上进行了端到端的实现，使用了扩散动作头，通过最小化预测噪声与真实噪声之间的均方误差进行优化。优化目标包括任务损失<code>L_task</code>和可能的负载平衡损失。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：RLBench模拟环境和真实世界环境（使用Franka机械臂设置）。</p>\n<p>评估指标：主要指标是任务成功率，计算为在十个任务上的平均成功率。此外，还评估了计算效率，包括推理延迟（或频率）和计算成本（如FLOPs或延迟倍数）。在真实世界中，评估了模型在一系列操作任务上的定性表现。</p>"
  },
  {
    "date": "2025-03-25",
    "title": "Gemini Robotics: Bringing AI into the Physical World",
    "link": "http://arxiv.org/abs/2503.20020",
    "summary_markdown": "论文研究单位\nGoogle DeepMind\n\n论文概述\n本文介绍了Gemini Robotics系列模型，该系列模型基于Gemini 2.0构建，旨在将人工智能能力扩展至物理世界。该系列包含两个模型：Gemini Robotics-ER，一个增强的视觉-语言模型（VLM），专注于具身推理能力；以及Gemini Robotics，一个视觉-语言-动作（VLA）通用模型，能够直接控制机器人执行复杂操作任务。论文展示了这两个模型在多种任务上的能力，包括物体检测、3D理解、机器人控制（零样本和少样本）以及长时程灵巧操作，并讨论了相关的安全考虑。\n\n论文核心贡献点\n1. ERQA基准：一个开源的具身推理问答基准，包含400个多选题，用于评估模型在物理世界的空间、轨迹、动作和状态估计等方面的理解能力。\n2. Gemini Robotics-ER：一个基于Gemini 2.0 Flash增强的视觉-语言模型，显著提升了在2D指向、3D检测等具身推理任务上的性能，并能通过代码生成或上下文学习实现零样本和少样本机器人控制。\n3. Gemini Robotics：一个视觉-语言-动作（VLA）模型，通过在大型、多样的机器人动作数据集上微调Gemini Robotics-ER得到，能够直接预测机器人动作，执行复杂操作任务，对视觉、指令和动作变化具有鲁棒性。\n4. 模型适应性与专业化：展示了Gemini Robotics可通过进一步微调快速适应新任务（100个样本）、新机器人形态（双臂平台、高自由度人形机器人）以及长时程灵巧任务（折纸、玩卡牌游戏等）。\n5. 负责任的发展与安全：探讨了训练大型机器人模型的社会效益、风险及缓解措施，与Google AI原则对齐。\n\n论文方法描述\n1. Gemini Robotics-ER方法：基于Gemini 2.0 Flash进行专业化训练，增强其在2D/3D空间理解、多视角对应、物体检测和指向等具身推理任务上的能力。支持通过API调用其感知功能（如检测物体、预测抓取点）进行零样本控制。\n2. Gemini Robotics方法：由云端VLA主干（基于Gemini Robotics-ER的蒸馏版本，延迟优化至160ms）和本地动作解码器组成，端到端延迟约250ms，支持50Hz控制频率。通过在大规模多模态数据（包括机器人动作、网络文档、代码、图像、音频、视频和具身推理数据）上微调，模型能够预测动作块（action chunks），实现平滑、反应式的运动。\n3. 零样本与少样本控制：Gemini 2.0和Gemini Robotics-ER可通过生成代码调用机器人API（移动夹爪、获取状态）或通过上下文学习（10个演示）直接输出末端执行器轨迹，实现机器人控制，后者在双臂和灵巧任务上表现更优。\n\n论文使用数据集和训练资源\n数据集：\n- ERQA：自建基准，含400个多选题，图像源自OXE、UMI Data、MECCANO等。\n- 机器人动作训练：大规模、多样化的ALOHA 2机器人遥操作数据集，历时12个月收集，包含数千小时、数千种任务的专家演示，覆盖不同技能、物体和难度。\n- 多模态数据：包括网络文档、代码、图像、音频、视频和具身推理问答数据，用于提升模型理解和泛化能力。\n训练资源：\n- Gemini Robotics-ER基于Gemini 2.0 Flash专业化训练。\n- Gemini Robotics主干为Gemini Robotics-ER的蒸馏版本，优化了推理延迟（<160ms）。\n- 基线模型：π0重新实现和多任务扩散策略，均在相同数据混合上训练至收敛，使用Nvidia RTX 4090 GPU本地运行。\n\n论文使用的评估环境和评估指标\n评估环境：\n- 实验在真实ALOHA 2机器人上进行，任务场景包括家庭、厨房、办公室等。\n- 部分任务在模拟环境中评估（如零样本控制实验）。\n- 评估涵盖分布内、视觉泛化、指令泛化和动作泛化场景。\n评估指标：\n- ERQA：多选题准确率，比较Gemini 2.0与GPT-4o、Claude等模型。\n- 指向任务：在Paco-LVIS、Pixmo-Point、Where2Place基准上的准确率（点在掩码内为1，否则为0）。\n- 3D检测：在SUN-RGBD上的AP@15指标。\n- 机器人任务成功率：二进制任务成功率（平均成功率、各任务成功率）。\n- 泛化任务：进度分数（Progress Score，连续度量）和成功率，用于评估模型对视觉、指令和动作变化的适应性。",
    "summary_html": "<p>论文研究单位</p>\n<p>Google DeepMind</p>\n\n<p>论文概述</p>\n<p>本文介绍了Gemini Robotics系列模型，该系列模型基于Gemini 2.0构建，旨在将人工智能能力扩展至物理世界。该系列包含两个模型：Gemini Robotics-ER，一个增强的视觉-语言模型（VLM），专注于具身推理能力；以及Gemini Robotics，一个视觉-语言-动作（VLA）通用模型，能够直接控制机器人执行复杂操作任务。论文展示了这两个模型在多种任务上的能力，包括物体检测、3D理解、机器人控制（零样本和少样本）以及长时程灵巧操作，并讨论了相关的安全考虑。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>ERQA基准：一个开源的具身推理问答基准，包含400个多选题，用于评估模型在物理世界的空间、轨迹、动作和状态估计等方面的理解能力。</li><li>Gemini Robotics-ER：一个基于Gemini 2.0 Flash增强的视觉-语言模型，显著提升了在2D指向、3D检测等具身推理任务上的性能，并能通过代码生成或上下文学习实现零样本和少样本机器人控制。</li><li>Gemini Robotics：一个视觉-语言-动作（VLA）模型，通过在大型、多样的机器人动作数据集上微调Gemini Robotics-ER得到，能够直接预测机器人动作，执行复杂操作任务，对视觉、指令和动作变化具有鲁棒性。</li><li>模型适应性与专业化：展示了Gemini Robotics可通过进一步微调快速适应新任务（100个样本）、新机器人形态（双臂平台、高自由度人形机器人）以及长时程灵巧任务（折纸、玩卡牌游戏等）。</li><li>负责任的发展与安全：探讨了训练大型机器人模型的社会效益、风险及缓解措施，与Google AI原则对齐。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>Gemini Robotics-ER方法：基于Gemini 2.0 Flash进行专业化训练，增强其在2D/3D空间理解、多视角对应、物体检测和指向等具身推理任务上的能力。支持通过API调用其感知功能（如检测物体、预测抓取点）进行零样本控制。</li><li>Gemini Robotics方法：由云端VLA主干（基于Gemini Robotics-ER的蒸馏版本，延迟优化至160ms）和本地动作解码器组成，端到端延迟约250ms，支持50Hz控制频率。通过在大规模多模态数据（包括机器人动作、网络文档、代码、图像、音频、视频和具身推理数据）上微调，模型能够预测动作块（action chunks），实现平滑、反应式的运动。</li><li>零样本与少样本控制：Gemini 2.0和Gemini Robotics-ER可通过生成代码调用机器人API（移动夹爪、获取状态）或通过上下文学习（10个演示）直接输出末端执行器轨迹，实现机器人控制，后者在双臂和灵巧任务上表现更优。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<ul><li>ERQA：自建基准，含400个多选题，图像源自OXE、UMI Data、MECCANO等。</li><li>机器人动作训练：大规模、多样化的ALOHA 2机器人遥操作数据集，历时12个月收集，包含数千小时、数千种任务的专家演示，覆盖不同技能、物体和难度。</li><li>多模态数据：包括网络文档、代码、图像、音频、视频和具身推理问答数据，用于提升模型理解和泛化能力。</li></ul>\n<p>训练资源：</p>\n<ul><li>Gemini Robotics-ER基于Gemini 2.0 Flash专业化训练。</li><li>Gemini Robotics主干为Gemini Robotics-ER的蒸馏版本，优化了推理延迟（<160ms）。</li><li>基线模型：π0重新实现和多任务扩散策略，均在相同数据混合上训练至收敛，使用Nvidia RTX 4090 GPU本地运行。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ul><li>实验在真实ALOHA 2机器人上进行，任务场景包括家庭、厨房、办公室等。</li><li>部分任务在模拟环境中评估（如零样本控制实验）。</li><li>评估涵盖分布内、视觉泛化、指令泛化和动作泛化场景。</li></ul>\n<p>评估指标：</p>\n<ul><li>ERQA：多选题准确率，比较Gemini 2.0与GPT-4o、Claude等模型。</li><li>指向任务：在Paco-LVIS、Pixmo-Point、Where2Place基准上的准确率（点在掩码内为1，否则为0）。</li><li>3D检测：在SUN-RGBD上的AP@15指标。</li><li>机器人任务成功率：二进制任务成功率（平均成功率、各任务成功率）。</li><li>泛化任务：进度分数（Progress Score，连续度量）和成功率，用于评估模型对视觉、指令和动作变化的适应性。</li></ul>"
  },
  {
    "date": "2025-03-25",
    "title": "Boosting Robotic Manipulation Generalization with Minimal Costly Data",
    "link": "http://arxiv.org/abs/2503.19516",
    "summary_markdown": "### 论文研究单位\n美团\n### 论文概述\n机器人操纵任务中，视觉-语言-行动模型的性能依赖于大量多样化的演示数据，但数据收集成本高昂，特别是涉及物理交互的数据。论文发现，操纵任务中的空间推理阶段数据易于低成本收集，且对模型泛化至关重要。为解决数据成本和覆盖度不足的问题，论文提出了RoboTron-Craft，一个成本效益高的物理真实轨迹生成管道，以及RoboTron-Platter方法，该方法通过解耦任务阶段，利用大量低成本的空间推理阶段数据来增强模型泛化，从而最大化利用昂贵但稀缺的物理交互阶段数据。\n### 论文核心贡献点\n1. 提出了RoboTron-Craft，一个分阶段且具有成本效益的轨迹生成管道，并揭示了模型性能与轨迹多样性之间的缩放定律。\n2. 提出了RoboTron-Platter方法，利用额外的低成本空间推理阶段轨迹来提高模型在零样本场景中的泛化性能。\n3. 证明了空间推理阶段数据可以作为催化剂，最大化昂贵的物理交互阶段数据在VLA模型训练中的贡献。\n4. 实验表明该方法在零样本场景中将任务成功率提高了41%，并能有效将模型技能迁移到新目标物体上。\n### 论文方法描述\nRoboTron-Platter方法首先将机器人的完整任务轨迹根据末端执行器与目标的距离划分为两个阶段：空间推理阶段和物理交互阶段。然后，构建一个混合数据集，该数据集包含一定数量的完整轨迹和大量额外收集的独立空间推理阶段轨迹。通过调整这两类数据的比例，对VLA模型进行模仿学习训练。这种分阶段的训练策略使模型能更有效地学习空间搜索和物理操作两种不同模式的技能，从而提升泛化能力。空间推理阶段的数据通过在真实抓取姿态上添加随机偏移来生成，避免了耗时的物理仿真，大幅降低了数据收集成本。\n### 论文使用数据集和训练资源\n1. **数据集**:\n * **自建数据集**：通过RoboTron-Craft管道生成，包含多种几何形状的日常物品在不同场景下的拾取-放置任务轨迹。\n * **CALVIN数据集**：用于验证方法通用性的公开基准数据集。\n2. **仿真环境**:\n * **平台**：NVIDIA Isaac Sim，配合Isaac Lab框架。\n * **场景**：包含多个工作台的房间，目标物体随机散落，场景纹理可随机化。\n * **智能体**：一个7-DoF Franka Emika Panda机械臂（固定基座）。\n * **观察**：使用手腕相机、头部相机和桌子相机的RGB图像，深度图仅在训练中用作监督。\n3. **训练资源**:\n * 论文未明确指定GPU型号和训练时间等计算资源细节。\n### 论文使用的评估环境和评估指标\n1. **评估环境**:\n * 在RoboTron-Craft基准提供的基于Isaac Sim的评估环境中进行。\n * 评估场景分为测试场景（模型见过的场景类型）和零样本场景（全新场景）。\n * 评估模型在分布外（OOD）的新目标物体上的泛化能力。\n2. **评估指标**:\n * **任务成功率**：主要指标。一次尝试成功抓取并放置目标物体记为1分，搬运过程中掉落记为0.5分，否则为0分。报告平均成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>美团</p>\n<h3>论文概述</h3>\n<p>机器人操纵任务中，视觉-语言-行动模型的性能依赖于大量多样化的演示数据，但数据收集成本高昂，特别是涉及物理交互的数据。论文发现，操纵任务中的空间推理阶段数据易于低成本收集，且对模型泛化至关重要。为解决数据成本和覆盖度不足的问题，论文提出了RoboTron-Craft，一个成本效益高的物理真实轨迹生成管道，以及RoboTron-Platter方法，该方法通过解耦任务阶段，利用大量低成本的空间推理阶段数据来增强模型泛化，从而最大化利用昂贵但稀缺的物理交互阶段数据。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了RoboTron-Craft，一个分阶段且具有成本效益的轨迹生成管道，并揭示了模型性能与轨迹多样性之间的缩放定律。</li><li>提出了RoboTron-Platter方法，利用额外的低成本空间推理阶段轨迹来提高模型在零样本场景中的泛化性能。</li><li>证明了空间推理阶段数据可以作为催化剂，最大化昂贵的物理交互阶段数据在VLA模型训练中的贡献。</li><li>实验表明该方法在零样本场景中将任务成功率提高了41%，并能有效将模型技能迁移到新目标物体上。</li></ol>\n<h3>论文方法描述</h3>\n<p>RoboTron-Platter方法首先将机器人的完整任务轨迹根据末端执行器与目标的距离划分为两个阶段：空间推理阶段和物理交互阶段。然后，构建一个混合数据集，该数据集包含一定数量的完整轨迹和大量额外收集的独立空间推理阶段轨迹。通过调整这两类数据的比例，对VLA模型进行模仿学习训练。这种分阶段的训练策略使模型能更有效地学习空间搜索和物理操作两种不同模式的技能，从而提升泛化能力。空间推理阶段的数据通过在真实抓取姿态上添加随机偏移来生成，避免了耗时的物理仿真，大幅降低了数据收集成本。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>:</p>\n<p> * <strong>自建数据集</strong>：通过RoboTron-Craft管道生成，包含多种几何形状的日常物品在不同场景下的拾取-放置任务轨迹。</p>\n<p> * <strong>CALVIN数据集</strong>：用于验证方法通用性的公开基准数据集。</p>\n<p>2. <strong>仿真环境</strong>:</p>\n<p> * <strong>平台</strong>：NVIDIA Isaac Sim，配合Isaac Lab框架。</p>\n<p> * <strong>场景</strong>：包含多个工作台的房间，目标物体随机散落，场景纹理可随机化。</p>\n<p> * <strong>智能体</strong>：一个7-DoF Franka Emika Panda机械臂（固定基座）。</p>\n<p> * <strong>观察</strong>：使用手腕相机、头部相机和桌子相机的RGB图像，深度图仅在训练中用作监督。</p>\n<p>3. <strong>训练资源</strong>:</p>\n<p> * 论文未明确指定GPU型号和训练时间等计算资源细节。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>:</p>\n<p> * 在RoboTron-Craft基准提供的基于Isaac Sim的评估环境中进行。</p>\n<p> * 评估场景分为测试场景（模型见过的场景类型）和零样本场景（全新场景）。</p>\n<p> * 评估模型在分布外（OOD）的新目标物体上的泛化能力。</p>\n<p>2. <strong>评估指标</strong>:</p>\n<p> * <strong>任务成功率</strong>：主要指标。一次尝试成功抓取并放置目标物体记为1分，搬运过程中掉落记为0.5分，否则为0分。报告平均成功率。</p>"
  },
  {
    "date": "2025-03-20",
    "title": "IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes",
    "link": "http://arxiv.org/abs/2503.17406",
    "summary_markdown": "论文研究单位\n卡内基梅隆大学机器人研究所\n\n论文概述\n论文提出了一种名为IRef-VLA的新基准数据集，用于解决在3D场景中，当语言指令不完美或存在歧义时的交互式参考定位问题。该任务旨在评估和促进开发能够理解自然语言、将其与物理世界关联、并能处理语言不完美性的智能体，特别是在室内导航等机器人应用中。论文通过创建大规模、多样化的数据集并定义一个扩展任务来推动这一领域的发展。\n\n论文核心贡献点\n1. 提出了IRef-VLA，一个大规模的3D场景参考定位基准数据集，是当前最大的真实世界数据集，包含超过11.5K个扫描的3D房间、760万条启发式生成的语义关系和470万条参考陈述。\n2. 数据集包含了丰富的标注信息，如语义对象和房间标注、密集的场景图、可自由导航的空间标注。\n3. 引入了“带有不完美参考的参考定位”这一新任务，要求模型不仅能定位对象，还要能检测引用对象是否存在，并在不存在时生成有效的替代建议。\n4. 数据集中特别增加了包含不完美或歧义语言的陈述，以评估模型的鲁棒性。\n\n论文方法描述\n1. 数据处理：整合了ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes和Unity生成的多个数据集，并进行标准化处理，包括提取场景点云、对象语义类别、边界框和颜色信息，同时生成可自由导航的空间区域。\n2. 场景图生成：基于对象的边界框，通过启发式方法计算八种类型的空间语义关系（如上方、附近、之间），为每个区域生成密集的场景图。\n3. 语言生成：采用基于模板的方法生成参考陈述。生成的语句遵循三个原则：视图无关、无歧义、最简（遵循格莱斯准则）。通过使用关系谓词的同义词来增加语言的多样性。\n4. 不完美语言生成：通过修改现有真实陈述中的目标对象、锚点对象或关系等属性，生成描述不存在对象的不完美参考陈述，以模拟真实世界中语言与场景不匹配的情况。\n\n论文使用数据集和训练资源\n数据集：\n- 来源：ScanNet, Matterport3D, HM3D, 3RScan, ARKitScenes, Unity scenes。\n- 规模：7,635个场景，超过11.5K个区域，286K个对象（跨477个类别），760万个空间关系，470万条参考陈述。\n训练资源：\n- 训练模型：MVT (Multi-View Transformer) 和 3D-VisTA。\n- 训练划分：对ScanNet数据使用官方ScanNet/ReferIt3D的训练/验证划分，其余场景按80%/20%划分训练集和验证集。\n- 训练策略：分别训练了在ScanNet子集上和完整IRef-VLA数据集上的模型，以评估数据规模对泛化能力的影响。模型训练至损失收敛。\n\n论文使用的评估环境和评估指标\n评估环境：\n- 在IRef-VLA数据集自身的验证集上进行评估。\n- 为了测试泛化能力，将在IRef-VLA上训练的模型进行零样本迁移，在ReferIt3D的测试集（包括合成语句的Sr3D和人类语句的Nr3D）上进行评估。\n评估指标：\n- 标准参考定位任务：使用准确率（Accuracy）。\n- 不完美参考定位任务：\n - 对象存在性检测子任务：使用二分类指标，包括真阳性、假阳性、真阴性、假阴性，并计算F1分数。\n - 替代建议子任务：提出了一种启发式相似性评分系统（score_sim），通过加权计算原始陈述与建议替代陈述在对象类别、属性和空间关系等方面的匹配程度来量化替代建议的质量。",
    "summary_html": "<p>论文研究单位</p>\n<p>卡内基梅隆大学机器人研究所</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为IRef-VLA的新基准数据集，用于解决在3D场景中，当语言指令不完美或存在歧义时的交互式参考定位问题。该任务旨在评估和促进开发能够理解自然语言、将其与物理世界关联、并能处理语言不完美性的智能体，特别是在室内导航等机器人应用中。论文通过创建大规模、多样化的数据集并定义一个扩展任务来推动这一领域的发展。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了IRef-VLA，一个大规模的3D场景参考定位基准数据集，是当前最大的真实世界数据集，包含超过11.5K个扫描的3D房间、760万条启发式生成的语义关系和470万条参考陈述。</li><li>数据集包含了丰富的标注信息，如语义对象和房间标注、密集的场景图、可自由导航的空间标注。</li><li>引入了“带有不完美参考的参考定位”这一新任务，要求模型不仅能定位对象，还要能检测引用对象是否存在，并在不存在时生成有效的替代建议。</li><li>数据集中特别增加了包含不完美或歧义语言的陈述，以评估模型的鲁棒性。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>数据处理：整合了ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes和Unity生成的多个数据集，并进行标准化处理，包括提取场景点云、对象语义类别、边界框和颜色信息，同时生成可自由导航的空间区域。</li><li>场景图生成：基于对象的边界框，通过启发式方法计算八种类型的空间语义关系（如上方、附近、之间），为每个区域生成密集的场景图。</li><li>语言生成：采用基于模板的方法生成参考陈述。生成的语句遵循三个原则：视图无关、无歧义、最简（遵循格莱斯准则）。通过使用关系谓词的同义词来增加语言的多样性。</li><li>不完美语言生成：通过修改现有真实陈述中的目标对象、锚点对象或关系等属性，生成描述不存在对象的不完美参考陈述，以模拟真实世界中语言与场景不匹配的情况。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<ul><li>来源：ScanNet, Matterport3D, HM3D, 3RScan, ARKitScenes, Unity scenes。</li><li>规模：7,635个场景，超过11.5K个区域，286K个对象（跨477个类别），760万个空间关系，470万条参考陈述。</li></ul>\n<p>训练资源：</p>\n<ul><li>训练模型：MVT (Multi-View Transformer) 和 3D-VisTA。</li><li>训练划分：对ScanNet数据使用官方ScanNet/ReferIt3D的训练/验证划分，其余场景按80%/20%划分训练集和验证集。</li><li>训练策略：分别训练了在ScanNet子集上和完整IRef-VLA数据集上的模型，以评估数据规模对泛化能力的影响。模型训练至损失收敛。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ul><li>在IRef-VLA数据集自身的验证集上进行评估。</li><li>为了测试泛化能力，将在IRef-VLA上训练的模型进行零样本迁移，在ReferIt3D的测试集（包括合成语句的Sr3D和人类语句的Nr3D）上进行评估。</li></ul>\n<p>评估指标：</p>\n<ul><li>标准参考定位任务：使用准确率（Accuracy）。</li><li>不完美参考定位任务：</li></ul>\n<p> - 对象存在性检测子任务：使用二分类指标，包括真阳性、假阳性、真阴性、假阴性，并计算F1分数。</p>\n<p> - 替代建议子任务：提出了一种启发式相似性评分系统（score_sim），通过加权计算原始陈述与建议替代陈述在对象类别、属性和空间关系等方面的匹配程度来量化替代建议的质量。</p>"
  },
  {
    "date": "2025-03-20",
    "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse",
    "link": "http://arxiv.org/abs/2503.16365",
    "summary_markdown": "```markdown\n论文研究单位\n- Peking University\n- BIGAI\n\n论文概述\n本文介绍了JARVIS-VLA，一个在Minecraft环境中通过键盘和鼠标执行视觉游戏的视觉-语言-动作（VLA）模型。该模型基于ActVLP（从视觉语言后训练中行动）范式，首先对基础模型进行后训练以增强其世界知识、视觉识别和空间定位能力，然后通过模仿学习在轨迹数据集上进行训练。\n\n论文核心贡献点\n1. 提出了ActVLP训练范式，通过在非轨迹视觉语言任务上的后训练增强基础模型能力。\n2. 开发了JARVIS-VLA模型，首个在Minecraft中能遵循人类指令执行超过1000种原子任务的VLA模型。\n3. 实验表明ActVLP范式比最佳基线提升了40%的性能。\n4. 开源了代码、模型和数据集。\n\n论文方法描述\n1. 模型结构：基于Llava或Qwen2-VL架构，包含视觉编码器、图像投影模块和语言模型Transformer。支持多图像输入以处理部分可观测环境。\n2. 训练流程：\n - 阶段一：在文本世界知识数据集上后训练语言模型（冻结视觉模块）。\n - 阶段二：在多模态视觉语言对齐和空间定位数据集上联合微调视觉编码器和语言模型。\n - 阶段三：在轨迹数据集上通过模仿学习训练动作生成（冻结视觉模块）。\n3. 动作表示：将离散动作合并为统一类别，连续动作分桶后映射为离散token。重用语言模型中51个最不常用token表示动作。\n\n论文使用数据集和训练资源\n1. 非轨迹数据集：\n - 世界知识问答数据集：约277K条目。\n - 视觉语言对齐数据集：35K关键帧生成的问答对。\n - 空间定位数据集：404K条目。\n2. 轨迹数据集：超过7.4M帧Minecraft游戏数据，包含人类玩家、YouTube视频和现有代理的轨迹。\n3. 训练资源：32块A800-80G GPU，使用DeepSpeed ZeRO-1优化。\n\n论文使用的评估环境和评估指标\n1. 评估环境：Minecraft 1.16.5，隐藏人类玩家不可见信息（如位置和物品栏）。\n2. 基准：MCU Benchmark，包含四类任务（挖矿、击杀、合成、熔炼），每类至少5个任务。\n3. 评估指标：任务成功率（每个任务执行至少30次），以及各类别平均成功率。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位</p>\n<ul><li>Peking University</li><li>BIGAI</li></ul>\n\n<p>论文概述</p>\n<p>本文介绍了JARVIS-VLA，一个在Minecraft环境中通过键盘和鼠标执行视觉游戏的视觉-语言-动作（VLA）模型。该模型基于ActVLP（从视觉语言后训练中行动）范式，首先对基础模型进行后训练以增强其世界知识、视觉识别和空间定位能力，然后通过模仿学习在轨迹数据集上进行训练。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了ActVLP训练范式，通过在非轨迹视觉语言任务上的后训练增强基础模型能力。</li><li>开发了JARVIS-VLA模型，首个在Minecraft中能遵循人类指令执行超过1000种原子任务的VLA模型。</li><li>实验表明ActVLP范式比最佳基线提升了40%的性能。</li><li>开源了代码、模型和数据集。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>模型结构：基于Llava或Qwen2-VL架构，包含视觉编码器、图像投影模块和语言模型Transformer。支持多图像输入以处理部分可观测环境。</li><li>训练流程：</li></ol>\n<p> - 阶段一：在文本世界知识数据集上后训练语言模型（冻结视觉模块）。</p>\n<p> - 阶段二：在多模态视觉语言对齐和空间定位数据集上联合微调视觉编码器和语言模型。</p>\n<p> - 阶段三：在轨迹数据集上通过模仿学习训练动作生成（冻结视觉模块）。</p>\n<p>3. 动作表示：将离散动作合并为统一类别，连续动作分桶后映射为离散token。重用语言模型中51个最不常用token表示动作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. 非轨迹数据集：</p>\n<p> - 世界知识问答数据集：约277K条目。</p>\n<p> - 视觉语言对齐数据集：35K关键帧生成的问答对。</p>\n<p> - 空间定位数据集：404K条目。</p>\n<ol><li>轨迹数据集：超过7.4M帧Minecraft游戏数据，包含人类玩家、YouTube视频和现有代理的轨迹。</li><li>训练资源：32块A800-80G GPU，使用DeepSpeed ZeRO-1优化。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：Minecraft 1.16.5，隐藏人类玩家不可见信息（如位置和物品栏）。</li><li>基准：MCU Benchmark，包含四类任务（挖矿、击杀、合成、熔炼），每类至少5个任务。</li><li>评估指标：任务成功率（每个任务执行至少30次），以及各类别平均成功率。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2025-03-18",
    "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
    "link": "http://arxiv.org/abs/2503.14734",
    "summary_markdown": "**论文研究单位**\nNVIDIA\n\n**论文概述**\n该论文提出了GR00T N1，一个用于通用人形机器人的开源基础模型。它是一个视觉-语言-动作（VLA）模型，采用双系统架构：一个视觉-语言模块（系统2）用于理解环境和任务目标，以及一个扩散Transformer模块（系统1）用于生成实时运动动作。该模型在一个异构数据混合体上进行了训练，包括真实机器人轨迹、人类视频和合成生成的数据集。GR00T N1在多个机器人形态上的标准模拟基准测试中优于最先进的模仿学习基线。此外，该模型已部署在Fourier GR-1人形机器人上，用于语言条件下的双手操作任务，并表现出强大的性能和高数据效率。\n\n**论文核心贡献点**\n1. 提出GR00T N1，一个开源的、面向通用人形机器人的基础模型。\n2. 设计了一个集成了基于VLM的推理模块（系统2）和基于DiT的动作模块（系统1）的组合式模型，并在统一的框架中进行端到端联合训练。\n3. 开发了一种有效的预训练策略，利用人类视频、模拟和神经生成的数据以及真实机器人演示的混合体，以实现泛化和鲁棒性。\n4. 训练了一个大规模、多任务、语言条件的策略，该策略支持广泛的机器人形态，并能通过数据高效的后训练快速适应新任务。\n5. 在多个机器人形态的模拟基准测试中实现了优于最先进模仿学习基线的性能，并在真实的GR-1人形机器人上展示了强大的性能。\n\n**论文方法描述**\nGR00T N1是一个视觉-语言-动作（VLA）模型，总参数量为22亿（其中VLM部分为13.4亿）。其双系统架构受人类认知过程的启发。\n* **系统2（视觉-语言模块）**：使用预训练的Eagle-2 VLM，处理机器人的视觉感知和语言指令，以10Hz的频率运行。它将图像和语言输入编码为一系列token，并从中提取特征。\n* **系统1（扩散Transformer模块）**：一个经过流匹配训练的扩散Transformer，以120Hz的频率运行，负责生成高频的闭环电机动作。它接受来自VLM的输出token、机器人的本体感觉状态和带噪动作作为输入，并输出去噪后的动作。\n* **状态与动作编码/解码**：针对不同的机器人形态，使用特定的MLP将不同维度的状态和动作投影到共享的嵌入维度，然后由DiT处理。动作解码器将DiT的输出映射回特定形态的动作空间。\n* **训练策略**：\n * **预训练**：在一个由真实机器人数据、人类视频数据（通过学习的潜在动作标注）和合成数据（神经轨迹和模拟轨迹）组成的异构数据混合体上进行端到端训练。\n * **后训练**：在单个形态对应的数据集上微调预训练好的模型，冻结VLM的语言部分，仅微调模型的其余部分。\n * **数据生成**：\n * **潜在动作**：在视频数据上训练VQ-VAE模型，从连续的图像帧中提取潜在动作表示，用作动作标签。\n * **神经轨迹**：在真实世界数据上微调开源的图像到视频生成模型，以生成新颖语言提示下的新轨迹，从而大规模扩增数据。\n * **模拟轨迹**：使用DexMimicGen系统，通过演示转换和回放在模拟环境中，将少量人类演示自动扩展为大规模数据集。\n\n**论文使用数据集和训练资源**\n* **数据集**：训练数据由一个数据金字塔构成：\n * **真实世界数据（顶层）**：包括内部收集的GR-1人形机器人遥操作数据（88小时）、Open X-Embodiment数据集（RT-1, Bridge-v2, Language Table, DROID, MUTEX, RoboSet, Plex）和AgiBot-Alpha数据集（14万条轨迹）。\n * **合成数据（中层）**：包括神经轨迹（从视频生成模型生成，约827小时）和模拟轨迹（使用DexMimicGen在RoboCasa框架中生成，约78万条轨迹，相当于6500小时）。\n * **人类视频数据（底层）**：包括大规模的Ego4D、Ego-Exo4D、Assembly-101、EPIC-KITCHENS、HOI4D、HoloAssist和RH20T-Human等数据集。\n* **训练资源**：使用NVIDIA OSMO平台进行训练。训练集群配备了通过NVIDIA Quantum-2 InfiniBand连接的H100 GPU。最多使用了1024个GPU。GR00T-N1-2B模型的预训练大约消耗了50,000个H100 GPU小时。神经轨迹生成大约在3600个L40 GPU上花费了105,000个GPU小时（约1.5天）。\n\n**论文使用的评估环境和评估指标**\n* **评估环境**：\n * **模拟基准**：包括三个不同的基准测试集：\n 1. RoboCasa Kitchen（24项任务）：在模拟厨房环境中进行基础感觉运动技能测试。\n 2. DexMimicGen（来自先前工作）：一系列桌面操作任务。\n 3. 新开发的桌面操作任务套件：旨在紧密模仿真实世界的任务设置，包括在多种物体和容器组合之间重新排列物体。\n * **真实世界基准**：在Fourier GR-1人形机器人上进行的一系列桌面操作任务，以评估模型从有限数量的人类演示中获取新技能的能力。\n* **评估指标**：\n * **成功率**：任务成功完成的百分比。\n * **平均成功率**：跨任务或评估回合的平均成功率。\n * **数据效率**：在特定成功率水平下所需的人类演示数量或训练数据量。\n * 通过这些指标，GR00T N1在模拟和真实世界的评估中均显示出比最先进的模仿学习基线更高的性能。",
    "summary_html": "<p><strong>论文研究单位</strong></p>\n<p>NVIDIA</p>\n\n<p><strong>论文概述</strong></p>\n<p>该论文提出了GR00T N1，一个用于通用人形机器人的开源基础模型。它是一个视觉-语言-动作（VLA）模型，采用双系统架构：一个视觉-语言模块（系统2）用于理解环境和任务目标，以及一个扩散Transformer模块（系统1）用于生成实时运动动作。该模型在一个异构数据混合体上进行了训练，包括真实机器人轨迹、人类视频和合成生成的数据集。GR00T N1在多个机器人形态上的标准模拟基准测试中优于最先进的模仿学习基线。此外，该模型已部署在Fourier GR-1人形机器人上，用于语言条件下的双手操作任务，并表现出强大的性能和高数据效率。</p>\n\n<p><strong>论文核心贡献点</strong></p>\n<ol><li>提出GR00T N1，一个开源的、面向通用人形机器人的基础模型。</li><li>设计了一个集成了基于VLM的推理模块（系统2）和基于DiT的动作模块（系统1）的组合式模型，并在统一的框架中进行端到端联合训练。</li><li>开发了一种有效的预训练策略，利用人类视频、模拟和神经生成的数据以及真实机器人演示的混合体，以实现泛化和鲁棒性。</li><li>训练了一个大规模、多任务、语言条件的策略，该策略支持广泛的机器人形态，并能通过数据高效的后训练快速适应新任务。</li><li>在多个机器人形态的模拟基准测试中实现了优于最先进模仿学习基线的性能，并在真实的GR-1人形机器人上展示了强大的性能。</li></ol>\n\n<p><strong>论文方法描述</strong></p>\n<p>GR00T N1是一个视觉-语言-动作（VLA）模型，总参数量为22亿（其中VLM部分为13.4亿）。其双系统架构受人类认知过程的启发。</p>\n<p>* <strong>系统2（视觉-语言模块）</strong>：使用预训练的Eagle-2 VLM，处理机器人的视觉感知和语言指令，以10Hz的频率运行。它将图像和语言输入编码为一系列token，并从中提取特征。</p>\n<p>* <strong>系统1（扩散Transformer模块）</strong>：一个经过流匹配训练的扩散Transformer，以120Hz的频率运行，负责生成高频的闭环电机动作。它接受来自VLM的输出token、机器人的本体感觉状态和带噪动作作为输入，并输出去噪后的动作。</p>\n<p>* <strong>状态与动作编码/解码</strong>：针对不同的机器人形态，使用特定的MLP将不同维度的状态和动作投影到共享的嵌入维度，然后由DiT处理。动作解码器将DiT的输出映射回特定形态的动作空间。</p>\n<p>* <strong>训练策略</strong>：</p>\n<p> * <strong>预训练</strong>：在一个由真实机器人数据、人类视频数据（通过学习的潜在动作标注）和合成数据（神经轨迹和模拟轨迹）组成的异构数据混合体上进行端到端训练。</p>\n<p> * <strong>后训练</strong>：在单个形态对应的数据集上微调预训练好的模型，冻结VLM的语言部分，仅微调模型的其余部分。</p>\n<p> * <strong>数据生成</strong>：</p>\n<p> * <strong>潜在动作</strong>：在视频数据上训练VQ-VAE模型，从连续的图像帧中提取潜在动作表示，用作动作标签。</p>\n<p> * <strong>神经轨迹</strong>：在真实世界数据上微调开源的图像到视频生成模型，以生成新颖语言提示下的新轨迹，从而大规模扩增数据。</p>\n<p> * <strong>模拟轨迹</strong>：使用DexMimicGen系统，通过演示转换和回放在模拟环境中，将少量人类演示自动扩展为大规模数据集。</p>\n\n<p><strong>论文使用数据集和训练资源</strong></p>\n<p>* <strong>数据集</strong>：训练数据由一个数据金字塔构成：</p>\n<p> * <strong>真实世界数据（顶层）</strong>：包括内部收集的GR-1人形机器人遥操作数据（88小时）、Open X-Embodiment数据集（RT-1, Bridge-v2, Language Table, DROID, MUTEX, RoboSet, Plex）和AgiBot-Alpha数据集（14万条轨迹）。</p>\n<p> * <strong>合成数据（中层）</strong>：包括神经轨迹（从视频生成模型生成，约827小时）和模拟轨迹（使用DexMimicGen在RoboCasa框架中生成，约78万条轨迹，相当于6500小时）。</p>\n<p> * <strong>人类视频数据（底层）</strong>：包括大规模的Ego4D、Ego-Exo4D、Assembly-101、EPIC-KITCHENS、HOI4D、HoloAssist和RH20T-Human等数据集。</p>\n<p>* <strong>训练资源</strong>：使用NVIDIA OSMO平台进行训练。训练集群配备了通过NVIDIA Quantum-2 InfiniBand连接的H100 GPU。最多使用了1024个GPU。GR00T-N1-2B模型的预训练大约消耗了50,000个H100 GPU小时。神经轨迹生成大约在3600个L40 GPU上花费了105,000个GPU小时（约1.5天）。</p>\n\n<p><strong>论文使用的评估环境和评估指标</strong></p>\n<p>* <strong>评估环境</strong>：</p>\n<p> * <strong>模拟基准</strong>：包括三个不同的基准测试集：</p>\n<p> 1. RoboCasa Kitchen（24项任务）：在模拟厨房环境中进行基础感觉运动技能测试。</p>\n<p> 2. DexMimicGen（来自先前工作）：一系列桌面操作任务。</p>\n<p> 3. 新开发的桌面操作任务套件：旨在紧密模仿真实世界的任务设置，包括在多种物体和容器组合之间重新排列物体。</p>\n<p> * <strong>真实世界基准</strong>：在Fourier GR-1人形机器人上进行的一系列桌面操作任务，以评估模型从有限数量的人类演示中获取新技能的能力。</p>\n<p>* <strong>评估指标</strong>：</p>\n<p> * <strong>成功率</strong>：任务成功完成的百分比。</p>\n<p> * <strong>平均成功率</strong>：跨任务或评估回合的平均成功率。</p>\n<p> * <strong>数据效率</strong>：在特定成功率水平下所需的人类演示数量或训练数据量。</p>\n<p> * 通过这些指标，GR00T N1在模拟和真实世界的评估中均显示出比最先进的模仿学习基线更高的性能。</p>"
  },
  {
    "date": "2025-03-15",
    "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis",
    "link": "http://arxiv.org/abs/2503.14526",
    "summary_markdown": "# 论文研究单位\n北卡罗来纳大学教堂山分校计算机科学系、Robotics and AI Institute、华盛顿大学电气与计算机工程系\n# 论文概述\nReBot是一种通过真实到模拟再到真实的机器人视频合成技术来扩展机器人学习的方法。该方法首先在模拟环境中重放真实世界的机器人轨迹以多样化被操作物体，然后将模拟的运动与修复后的真实世界背景集成，合成物理上逼真且时间上一致的机器人视频。ReBot旨在解决真实世界数据收集成本高昂的问题，并通过模拟的扩展性和最小化模拟到现实的差距来增强视觉-语言-动作（VLA）模型的性能。\n# 论文核心贡献点\n1. 提出首个真实到模拟再到真实的机器人数据集扩展方法，用于将VLA模型适配到目标领域。\n2. 结合了真实数据和模拟的优势，利用模拟的可扩展性，同时通过真实数据来最小化模拟到现实的差距。\n3. 全自动化的数据流程，无需人工干预。\n4. 在模拟和真实世界环境中的广泛实验证明，ReBot显著提升了VLA模型的性能和鲁棒性，例如在SimplerEnv上OpenVLA的性能提升21.8%，真实世界任务成功率提高20%。\n# 论文方法描述\nReBot包含三个关键步骤：\n1. 真实到模拟轨迹重放：在模拟环境中自动设置数字孪生，重放真实世界机器人轨迹以获取操纵新物体的模拟运动。通过分析夹具动作序列确定物体和容器的位置，并验证重放轨迹的成功性。\n2. 真实世界背景修复：使用GroundedSAM2分割和跟踪真实世界视频中的机器人和物体，然后使用ProPainter移除这些特定元素，获得与任务无关的真实世界背景。\n3. 模拟到真实视频合成：将模拟的运动与修复后的真实背景集成，合成新的机器人视频帧。通过替换原始语言指令中的物体和容器描述来生成新的指令。\n# 论文使用数据集和训练资源\n数据集：BridgeData V2、DROID、自建的220个真实世界片段、Objaverse的厨房资产。\n模拟环境：Isaac Sim 4.1，基于Isaac Lab实现。\n训练资源：4块NVIDIA A6000 GPU，对Octo进行全微调，批次大小256，学习率4e-5；对OpenVLA进行LoRA微调，批次大小32，学习率5e-4。\n# 论文使用的评估环境和评估指标\n评估环境：\n- 模拟环境：SimplerEnv，使用WidowX 250 6DOF机器人臂。\n- 真实环境：Franka Panda 7DoF机器人臂配备Robotiq 2F-85夹具。\n评估指标：\n- 视频质量：使用VBench评估时间质量（主体一致性、背景一致性、运动平滑度）和逐帧质量（成像质量）。\n- 任务性能：在模拟和真实环境中评估VLA模型的任务成功率和泛化性能，包括领域内和跨域任务的表现。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>北卡罗来纳大学教堂山分校计算机科学系、Robotics and AI Institute、华盛顿大学电气与计算机工程系</p>\n<h1>论文概述</h1>\n<p>ReBot是一种通过真实到模拟再到真实的机器人视频合成技术来扩展机器人学习的方法。该方法首先在模拟环境中重放真实世界的机器人轨迹以多样化被操作物体，然后将模拟的运动与修复后的真实世界背景集成，合成物理上逼真且时间上一致的机器人视频。ReBot旨在解决真实世界数据收集成本高昂的问题，并通过模拟的扩展性和最小化模拟到现实的差距来增强视觉-语言-动作（VLA）模型的性能。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出首个真实到模拟再到真实的机器人数据集扩展方法，用于将VLA模型适配到目标领域。</li><li>结合了真实数据和模拟的优势，利用模拟的可扩展性，同时通过真实数据来最小化模拟到现实的差距。</li><li>全自动化的数据流程，无需人工干预。</li><li>在模拟和真实世界环境中的广泛实验证明，ReBot显著提升了VLA模型的性能和鲁棒性，例如在SimplerEnv上OpenVLA的性能提升21.8%，真实世界任务成功率提高20%。</li></ol>\n<h1>论文方法描述</h1>\n<p>ReBot包含三个关键步骤：</p>\n<ol><li>真实到模拟轨迹重放：在模拟环境中自动设置数字孪生，重放真实世界机器人轨迹以获取操纵新物体的模拟运动。通过分析夹具动作序列确定物体和容器的位置，并验证重放轨迹的成功性。</li><li>真实世界背景修复：使用GroundedSAM2分割和跟踪真实世界视频中的机器人和物体，然后使用ProPainter移除这些特定元素，获得与任务无关的真实世界背景。</li><li>模拟到真实视频合成：将模拟的运动与修复后的真实背景集成，合成新的机器人视频帧。通过替换原始语言指令中的物体和容器描述来生成新的指令。</li></ol>\n<h1>论文使用数据集和训练资源</h1>\n<p>数据集：BridgeData V2、DROID、自建的220个真实世界片段、Objaverse的厨房资产。</p>\n<p>模拟环境：Isaac Sim 4.1，基于Isaac Lab实现。</p>\n<p>训练资源：4块NVIDIA A6000 GPU，对Octo进行全微调，批次大小256，学习率4e-5；对OpenVLA进行LoRA微调，批次大小32，学习率5e-4。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>评估环境：</p>\n<ul><li>模拟环境：SimplerEnv，使用WidowX 250 6DOF机器人臂。</li><li>真实环境：Franka Panda 7DoF机器人臂配备Robotiq 2F-85夹具。</li></ul>\n<p>评估指标：</p>\n<ul><li>视频质量：使用VBench评估时间质量（主体一致性、背景一致性、运动平滑度）和逐帧质量（成像质量）。</li><li>任务性能：在模拟和真实环境中评估VLA模型的任务成功率和泛化性能，包括领域内和跨域任务的表现。</li></ul>"
  },
  {
    "date": "2025-03-17",
    "title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation",
    "link": "http://arxiv.org/abs/2503.13446",
    "summary_markdown": "### 论文研究单位\n北京邮电大学, 南洋理工大学, 清华大学\n### 论文概述\n论文提出了一种名为MoManipVLA的策略迁移框架，用于将预训练的视觉-语言-动作（VLA）模型从固定基座操作迁移到移动操作任务。该框架利用VLA模型生成具有高泛化能力的末端执行器路径点，并设计了一个双层轨迹优化框架来生成物理可行的移动基座和机械臂的联合轨迹，从而实现跨任务和环境的零样本调整，使移动操作策略能够执行多样化的家庭任务。\n### 论文核心贡献点\n提出了一种高效的策略适配框架MoManipVLA，用于将固定基座的VLA模型迁移到移动操作任务。\n设计了移动运动规划目标，包括末端执行器的可达性、轨迹平滑性和碰撞避免，以最大化轨迹的物理可行性。\n提出了一个双层目标优化框架，上层优化预测基座移动路径点以增强后续的操纵策略空间，下层优化选择最优的末端执行器轨迹以完成操纵任务。\n在OVMM基准测试和真实世界实验中验证了方法的有效性和高效性，相比最先进的移动操作技术，成功率高4.2%，并且由于预训练VLA模型的强大泛化能力，真实世界部署仅需50个专家样本。\n### 论文方法描述\nMoManipVLA首先使用预训练的VLA模型来生成高泛化的末端执行器路径点。\n然后，设计运动规划目标来评估基座和机械臂轨迹的物理可行性，包括可达性成本（基于逆运动学求解迭代次数）、平滑性成本（关节角和基座姿态的连续变化）和碰撞成本（基于ESDF的距离查询）。\n最后，采用一个双层轨迹优化框架来高效地求解轨迹：上层优化预测基座移动的路径点以扩展操纵策略空间，下层优化在给定路径点下选择最优的末端执行器轨迹以满足VLA模型生成的目标。该框架通过联合优化移动基座和机械臂的动作，使得VLA模型为固定基座生成的路径点在移动操作场景中变得可行。\n### 论文使用数据集和训练资源\n论文在OVMM基准测试和真实世界环境中进行了实验。\n真实世界部署仅需50个专家样本进行训练，这得益于预训练VLA模型的强大泛化能力。\n### 论文使用的评估环境和评估指标\n评估环境：OVMM基准测试和真实世界环境。\n评估指标：任务成功率。实验结果表明，MoManipVLA在OVMM上实现了比最先进方法高4.2%的成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京邮电大学, 南洋理工大学, 清华大学</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为MoManipVLA的策略迁移框架，用于将预训练的视觉-语言-动作（VLA）模型从固定基座操作迁移到移动操作任务。该框架利用VLA模型生成具有高泛化能力的末端执行器路径点，并设计了一个双层轨迹优化框架来生成物理可行的移动基座和机械臂的联合轨迹，从而实现跨任务和环境的零样本调整，使移动操作策略能够执行多样化的家庭任务。</p>\n<h3>论文核心贡献点</h3>\n<p>提出了一种高效的策略适配框架MoManipVLA，用于将固定基座的VLA模型迁移到移动操作任务。</p>\n<p>设计了移动运动规划目标，包括末端执行器的可达性、轨迹平滑性和碰撞避免，以最大化轨迹的物理可行性。</p>\n<p>提出了一个双层目标优化框架，上层优化预测基座移动路径点以增强后续的操纵策略空间，下层优化选择最优的末端执行器轨迹以完成操纵任务。</p>\n<p>在OVMM基准测试和真实世界实验中验证了方法的有效性和高效性，相比最先进的移动操作技术，成功率高4.2%，并且由于预训练VLA模型的强大泛化能力，真实世界部署仅需50个专家样本。</p>\n<h3>论文方法描述</h3>\n<p>MoManipVLA首先使用预训练的VLA模型来生成高泛化的末端执行器路径点。</p>\n<p>然后，设计运动规划目标来评估基座和机械臂轨迹的物理可行性，包括可达性成本（基于逆运动学求解迭代次数）、平滑性成本（关节角和基座姿态的连续变化）和碰撞成本（基于ESDF的距离查询）。</p>\n<p>最后，采用一个双层轨迹优化框架来高效地求解轨迹：上层优化预测基座移动的路径点以扩展操纵策略空间，下层优化在给定路径点下选择最优的末端执行器轨迹以满足VLA模型生成的目标。该框架通过联合优化移动基座和机械臂的动作，使得VLA模型为固定基座生成的路径点在移动操作场景中变得可行。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>论文在OVMM基准测试和真实世界环境中进行了实验。</p>\n<p>真实世界部署仅需50个专家样本进行训练，这得益于预训练VLA模型的强大泛化能力。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：OVMM基准测试和真实世界环境。</p>\n<p>评估指标：任务成功率。实验结果表明，MoManipVLA在OVMM上实现了比最先进方法高4.2%的成功率。</p>"
  },
  {
    "date": "2025-03-13",
    "title": "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2503.10631",
    "summary_markdown": "论文研究单位\n北京大学，北京人工智能研究院，香港中文大学\n\n论文概述\n该论文提出了HybridVLA，一个统一的视觉-语言-动作（VLA）模型框架，它将扩散和自回归动作生成方法整合到单个大型语言模型中。通过协作训练配方和动作集成机制，该模型旨在吸收扩散动作的连续性和自回归的上下文推理能力，实现两种范式的相互增强，从而提高机器人操作的鲁棒性和性能。HybridVLA在大规模跨具身机器人数据集上进行了预训练，并在模拟和真实世界任务中均实现了最先进的性能。\n\n论文核心贡献点\n1. 提出了HybridVLA，一个统一的模型，将扩散和自回归动作生成无缝集成在单个大型语言模型中，有效结合了扩散动作的连续性和自回归生成的上下文推理能力。\n2. 引入了一种协作训练配方，通过共享的LLM主干连接两种动作生成方法，使得它们能够相互促进，并提出了一种协作动作集成机制，根据自回归动作令牌的置信度自适应地融合两种动作预测，增强了操作鲁棒性。\n3. 提出的方法在多种任务上实现了最先进的性能，并在多种未见过的配置中表现出强大的泛化能力。\n\n论文方法描述\nHybridVLA采用预训练的视觉-语言模型（VLM）作为基础架构。其核心方法包括：\n1. 令牌序列构建：设计了一种统一的令牌序列来组织多模态输入（视觉、语言、机器人状态）、扩散令牌和自回归令牌，通过特殊的标记令牌（<BOD>和<EOD>）连接不同部分，以协调两种生成范式。\n2. 协作训练：使用混合目标函数（L_hybrid = L_dif + L_ce）联合训练模型。扩散部分使用均方误差损失预测噪声，自回归部分使用交叉熵损失预测离散动作令牌。通过共享LLM主干进行反向传播，使模型同时吸收连续动作表示和语义推理表示。\n3. 协作动作集成：在推理时，模型同时生成扩散和自回归两种动作。根据自回归令牌的平均置信度（c^ar）进行自适应融合：当置信度高于阈值θ（0.96）时，对两种动作取平均；否则，仅使用扩散动作。扩散过程采用DDIM采样，仅需4步即可完成。\n\n论文使用数据集和训练资源\n预训练数据集：Open X-Embodiment, DROID, ROBOMIND，总计760K条轨迹，超过3300万帧，在A800 GPU上训练超过10,000小时。\n微调数据集：自收集的模拟数据（RLBench）和真实世界数据。\n训练资源：未明确提及具体GPU数量，但提及了A800 GPU和大量训练时长。\n\n论文使用的评估环境和评估指标\n评估环境：\n1. 模拟环境：RLBench。\n2. 真实世界环境：单臂和双臂机器人任务设置。\n评估指标：\n1. 成功率：在RLBench上报告平均成功率。\n2. 推理速度：以Hz为单位报告模型的推理频率。\n3. 泛化能力：在未见过的操作物体、背景、空间位置和光照条件下评估性能的稳定性。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京大学，北京人工智能研究院，香港中文大学</p>\n\n<p>论文概述</p>\n<p>该论文提出了HybridVLA，一个统一的视觉-语言-动作（VLA）模型框架，它将扩散和自回归动作生成方法整合到单个大型语言模型中。通过协作训练配方和动作集成机制，该模型旨在吸收扩散动作的连续性和自回归的上下文推理能力，实现两种范式的相互增强，从而提高机器人操作的鲁棒性和性能。HybridVLA在大规模跨具身机器人数据集上进行了预训练，并在模拟和真实世界任务中均实现了最先进的性能。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了HybridVLA，一个统一的模型，将扩散和自回归动作生成无缝集成在单个大型语言模型中，有效结合了扩散动作的连续性和自回归生成的上下文推理能力。</li><li>引入了一种协作训练配方，通过共享的LLM主干连接两种动作生成方法，使得它们能够相互促进，并提出了一种协作动作集成机制，根据自回归动作令牌的置信度自适应地融合两种动作预测，增强了操作鲁棒性。</li><li>提出的方法在多种任务上实现了最先进的性能，并在多种未见过的配置中表现出强大的泛化能力。</li></ol>\n\n<p>论文方法描述</p>\n<p>HybridVLA采用预训练的视觉-语言模型（VLM）作为基础架构。其核心方法包括：</p>\n<ol><li>令牌序列构建：设计了一种统一的令牌序列来组织多模态输入（视觉、语言、机器人状态）、扩散令牌和自回归令牌，通过特殊的标记令牌（<BOD>和<EOD>）连接不同部分，以协调两种生成范式。</li><li>协作训练：使用混合目标函数（L_hybrid = L_dif + L_ce）联合训练模型。扩散部分使用均方误差损失预测噪声，自回归部分使用交叉熵损失预测离散动作令牌。通过共享LLM主干进行反向传播，使模型同时吸收连续动作表示和语义推理表示。</li><li>协作动作集成：在推理时，模型同时生成扩散和自回归两种动作。根据自回归令牌的平均置信度（c^ar）进行自适应融合：当置信度高于阈值θ（0.96）时，对两种动作取平均；否则，仅使用扩散动作。扩散过程采用DDIM采样，仅需4步即可完成。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>预训练数据集：Open X-Embodiment, DROID, ROBOMIND，总计760K条轨迹，超过3300万帧，在A800 GPU上训练超过10,000小时。</p>\n<p>微调数据集：自收集的模拟数据（RLBench）和真实世界数据。</p>\n<p>训练资源：未明确提及具体GPU数量，但提及了A800 GPU和大量训练时长。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ol><li>模拟环境：RLBench。</li><li>真实世界环境：单臂和双臂机器人任务设置。</li></ol>\n<p>评估指标：</p>\n<ol><li>成功率：在RLBench上报告平均成功率。</li><li>推理速度：以Hz为单位报告模型的推理频率。</li><li>泛化能力：在未见过的操作物体、背景、空间位置和光照条件下评估性能的稳定性。</li></ol>"
  },
  {
    "date": "2025-03-12",
    "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games",
    "link": "http://arxiv.org/abs/2503.09527",
    "summary_markdown": "论文研究单位\n阿里巴巴集团 (Alibaba Group)\n\n论文概述\n本文提出了CombatVLA，一个专为3D动作角色扮演游戏(ARPGs)战斗任务优化的高效视觉-语言-动作(VLA)模型。该模型是一个3B参数规模的模型，通过在一个动作追踪器收集的视频-动作对上进行训练，数据被格式化为动作思维序列。CombatVLA可以无缝集成到一个动作执行框架中，通过截断的动作思维策略实现高效推理。实验结果表明，CombatVLA不仅在战斗理解基准上优于所有现有模型，而且在游戏战斗中实现了50倍的加速，并且其任务成功率高于人类玩家。\n\n论文核心贡献点\n1. 动作追踪器：开发了一个在游戏后台运行以记录玩家动作的Python工具，用于加速数据收集。\n2. 战斗理解基准：建立了一个名为CUBench的战斗理解基准，通过视觉问答格式评估模型在收集、理解和推理三个任务中的表现。\n3. AoT数据集：引入了一个包含三种类型（粗粒度视频AoT、细粒度帧AoT和帧截断AoT）的三阶段AoT数据集，使模型能够逐步学习战斗技能。\n4. CombatVLA模型：CombatVLA采用渐进学习范式进行训练，并受自适应动作加权损失的约束，在战斗理解基准上取得了最佳性能。\n5. 动作执行框架：将CombatVLA集成到一个可在PC上运行的代理框架中，通过截断策略实现了50倍的加速。\n\n论文方法描述\n1. 动作思维构建：受思维链提示的启发，将动作追踪器收集的数据转换为动作思维数据。模型响应被格式化为JSON，包含[action]（如“按空格”）和[explanation]（用于描述敌人当前状态、动作的物理意义等）。此外，引入特殊token<TRUNC>表示输出截断以增加推理速度。\n2. 三阶段渐进学习：\n - 阶段1：粗粒度视频-AoT调优。该训练阶段的目标是帮助模型理解战斗环境，使学习更容易，并稳定训练。每个视频由n帧组成，帧率设置为m帧/秒。\n - 阶段2：细粒度帧-AoT调优。创建动作-帧对齐的数据对，称为帧-AoT，从当前动作的时间戳追溯k帧。例如，如果k帧显示敌人准备攻击，模型可能决定闪避。\n - 阶段3：细粒度帧-截断-AoT调优。重新组织AoT数据，通过引入特殊token<TRUNC>来减轻因AoT引入而导致的时间增加。在实时操作中，任何<TRUNC>之后的响应都会被截断。\n3. 自适应动作加权损失：\n - CombatVLA使用三种损失进行训练：语言建模损失、动作对齐损失和模态对比损失，以解决动作分布不平衡问题。\n - 基于模型输出的动作是否与标签中的动作匹配，调整视觉和动作嵌入之间的距离。定义了一个基于预定义动作序列的优先级感知匹配准则，并根据匹配结果动态适应损失函数：对于匹配的对，最小化余弦距离；对于不匹配的对，最大化其分离，同时通过动作对齐损失强制执行动作预测准确性。\n\n论文使用数据集和训练资源\n1. 数据集：收集了来自《黑神话：悟空》的200小时人类游戏记录动作。这些数据由六名已完成所有游戏关卡的玩家组成的团队，在两周内进行标注。\n2. 训练资源：CombatVLA是一个3B模型。在训练过程中，冻结了视觉编码器的参数，并微调了语言模型的参数。具体的硬件训练资源未在提供的文本中提及。\n\n论文使用的评估环境和评估指标\n1. 评估环境：在真实PC游戏环境中进行评估，CombatVLA集成到一个操作真实PC的代理框架中。\n2. 评估基准：使用论文提出的战斗理解基准，该基准围绕三个能力（收集、理解和推理）构建，包含914个数据点（39.4%收集，22.3%理解，38.3%推理）。\n3. 评估指标：\n - 在CUBench上的表现，与现有模型（如GPT-4o和Qwen2.5-VL）进行比较。\n - 执行速度：在游戏战斗中达到50倍加速，与现有基于VLM的游戏代理（如Cradle和VARP框架）进行比较。\n - 任务成功率：与人类玩家比较，CombatVLA的任务成功率高于人类。\n - 实验包括主结果、消融研究和定性可视化。",
    "summary_html": "<p>论文研究单位</p>\n<p>阿里巴巴集团 (Alibaba Group)</p>\n\n<p>论文概述</p>\n<p>本文提出了CombatVLA，一个专为3D动作角色扮演游戏(ARPGs)战斗任务优化的高效视觉-语言-动作(VLA)模型。该模型是一个3B参数规模的模型，通过在一个动作追踪器收集的视频-动作对上进行训练，数据被格式化为动作思维序列。CombatVLA可以无缝集成到一个动作执行框架中，通过截断的动作思维策略实现高效推理。实验结果表明，CombatVLA不仅在战斗理解基准上优于所有现有模型，而且在游戏战斗中实现了50倍的加速，并且其任务成功率高于人类玩家。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>动作追踪器：开发了一个在游戏后台运行以记录玩家动作的Python工具，用于加速数据收集。</li><li>战斗理解基准：建立了一个名为CUBench的战斗理解基准，通过视觉问答格式评估模型在收集、理解和推理三个任务中的表现。</li><li>AoT数据集：引入了一个包含三种类型（粗粒度视频AoT、细粒度帧AoT和帧截断AoT）的三阶段AoT数据集，使模型能够逐步学习战斗技能。</li><li>CombatVLA模型：CombatVLA采用渐进学习范式进行训练，并受自适应动作加权损失的约束，在战斗理解基准上取得了最佳性能。</li><li>动作执行框架：将CombatVLA集成到一个可在PC上运行的代理框架中，通过截断策略实现了50倍的加速。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>动作思维构建：受思维链提示的启发，将动作追踪器收集的数据转换为动作思维数据。模型响应被格式化为JSON，包含[action]（如“按空格”）和[explanation]（用于描述敌人当前状态、动作的物理意义等）。此外，引入特殊token<TRUNC>表示输出截断以增加推理速度。</li><li>三阶段渐进学习：</li></ol>\n<p> - 阶段1：粗粒度视频-AoT调优。该训练阶段的目标是帮助模型理解战斗环境，使学习更容易，并稳定训练。每个视频由n帧组成，帧率设置为m帧/秒。</p>\n<p> - 阶段2：细粒度帧-AoT调优。创建动作-帧对齐的数据对，称为帧-AoT，从当前动作的时间戳追溯k帧。例如，如果k帧显示敌人准备攻击，模型可能决定闪避。</p>\n<p> - 阶段3：细粒度帧-截断-AoT调优。重新组织AoT数据，通过引入特殊token<TRUNC>来减轻因AoT引入而导致的时间增加。在实时操作中，任何<TRUNC>之后的响应都会被截断。</p>\n<p>3. 自适应动作加权损失：</p>\n<p> - CombatVLA使用三种损失进行训练：语言建模损失、动作对齐损失和模态对比损失，以解决动作分布不平衡问题。</p>\n<p> - 基于模型输出的动作是否与标签中的动作匹配，调整视觉和动作嵌入之间的距离。定义了一个基于预定义动作序列的优先级感知匹配准则，并根据匹配结果动态适应损失函数：对于匹配的对，最小化余弦距离；对于不匹配的对，最大化其分离，同时通过动作对齐损失强制执行动作预测准确性。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：收集了来自《黑神话：悟空》的200小时人类游戏记录动作。这些数据由六名已完成所有游戏关卡的玩家组成的团队，在两周内进行标注。</li><li>训练资源：CombatVLA是一个3B模型。在训练过程中，冻结了视觉编码器的参数，并微调了语言模型的参数。具体的硬件训练资源未在提供的文本中提及。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：在真实PC游戏环境中进行评估，CombatVLA集成到一个操作真实PC的代理框架中。</li><li>评估基准：使用论文提出的战斗理解基准，该基准围绕三个能力（收集、理解和推理）构建，包含914个数据点（39.4%收集，22.3%理解，38.3%推理）。</li><li>评估指标：</li></ol>\n<p> - 在CUBench上的表现，与现有模型（如GPT-4o和Qwen2.5-VL）进行比较。</p>\n<p> - 执行速度：在游戏战斗中达到50倍加速，与现有基于VLM的游戏代理（如Cradle和VARP框架）进行比较。</p>\n<p> - 任务成功率：与人类玩家比较，CombatVLA的任务成功率高于人类。</p>\n<p> - 实验包括主结果、消融研究和定性可视化。</p>"
  },
  {
    "date": "2025-03-11",
    "title": "MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2503.08007",
    "summary_markdown": "### 论文研究单位\nZhejiang University, China; MiLAB, Westlake University, China; AIM Lab, Faculty of IT, Monash University, Australia; HKUST(GZ), China\n### 论文概述\n本文提出了MoRE（Mixture of Robotic Experts），一种用于四足机器人的新型视觉-语言-动作（VLA）模型。该模型旨在通过强化学习（RL）微调大规模VLA模型，以适应大量混合质量数据。MoRE将多个低秩适应（LoRA）模块作为专家集成到密集的多模态大语言模型中，形成稀疏激活的专家混合模型，有效适应各种下游任务。模型采用基于RL的训练目标作为Q函数，利用自动收集的混合质量数据，提升数据效率和模型性能。\n### 论文核心贡献点\n- 提出MoRE，首个探索MoE架构在大规模端到端VLA模型中应用的工作，在多任务设置中展示高成功率和泛化能力。\n- 引入基于RL的训练目标，包含自动收集的次优轨迹，有效增强大规模VLA模型的数据效率和性能。\n- 在仿真和现实世界中进行广泛实验，研究MoRE在各种设置下的性能。\n### 论文方法描述\n- **模型架构**：基于Fuyu 8B解码器-Only Transformer，支持任意分辨率和图像数量的输入作为MLLM主干，生成动作令牌。\n- **混合LoRA专家**：将多个LoRA模块作为专家集成到解码器层的FFN中，每个专家包含共享FFN参数和LoRA适配器，通过路由器动态选择专家进行任务适应。\n- **训练目标**：采用自回归离散Q学习目标，将任务建模为MDP，模型作为Q函数训练，利用任务的结构特性（如视界无关回报和关键点有限性）从混合质量数据中学习。\n### 论文使用数据集和训练资源\n- **数据集**：使用QUARD数据集的6个具有挑战性的任务，分为“简单”、“中等”和“困难”三个难度级别，包含广泛和次优数据。\n- **训练资源**：基于Fuyu 8B模型，使用LoRA微调技术，未指定具体硬件。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在QUARD数据集的仿真环境中进行测试，并在现实世界四足机器人上部署验证。\n- **评估指标**：主要使用成功率（success rate），每个任务评估25个情节的平均性能。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Zhejiang University, China; MiLAB, Westlake University, China; AIM Lab, Faculty of IT, Monash University, Australia; HKUST(GZ), China</p>\n<h3>论文概述</h3>\n<p>本文提出了MoRE（Mixture of Robotic Experts），一种用于四足机器人的新型视觉-语言-动作（VLA）模型。该模型旨在通过强化学习（RL）微调大规模VLA模型，以适应大量混合质量数据。MoRE将多个低秩适应（LoRA）模块作为专家集成到密集的多模态大语言模型中，形成稀疏激活的专家混合模型，有效适应各种下游任务。模型采用基于RL的训练目标作为Q函数，利用自动收集的混合质量数据，提升数据效率和模型性能。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出MoRE，首个探索MoE架构在大规模端到端VLA模型中应用的工作，在多任务设置中展示高成功率和泛化能力。</li><li>引入基于RL的训练目标，包含自动收集的次优轨迹，有效增强大规模VLA模型的数据效率和性能。</li><li>在仿真和现实世界中进行广泛实验，研究MoRE在各种设置下的性能。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>模型架构</strong>：基于Fuyu 8B解码器-Only Transformer，支持任意分辨率和图像数量的输入作为MLLM主干，生成动作令牌。</li><li><strong>混合LoRA专家</strong>：将多个LoRA模块作为专家集成到解码器层的FFN中，每个专家包含共享FFN参数和LoRA适配器，通过路由器动态选择专家进行任务适应。</li><li><strong>训练目标</strong>：采用自回归离散Q学习目标，将任务建模为MDP，模型作为Q函数训练，利用任务的结构特性（如视界无关回报和关键点有限性）从混合质量数据中学习。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：使用QUARD数据集的6个具有挑战性的任务，分为“简单”、“中等”和“困难”三个难度级别，包含广泛和次优数据。</li><li><strong>训练资源</strong>：基于Fuyu 8B模型，使用LoRA微调技术，未指定具体硬件。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在QUARD数据集的仿真环境中进行测试，并在现实世界四足机器人上部署验证。</li><li><strong>评估指标</strong>：主要使用成功率（success rate），每个任务评估25个情节的平均性能。</li></ul>"
  },
  {
    "date": "2025-03-10",
    "title": "PointVLA: Injecting the 3D World into Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2503.07511",
    "summary_markdown": "### 论文研究单位\n美的集团、上海大学、华东师范大学\n### 论文概述\nPointVLA提出了一种无需重新训练即可将3D点云注入预训练视觉-语言-动作模型（VLA）的框架。该方法冻结原始动作专家模块，通过轻量级模块化块注入3D特征，并利用跳块分析确定对性能影响最小的网络层进行特征注入，显著提升了模型的空间推理能力。\n### 论文核心贡献点\n1. **高效3D注入机制**：通过冻结预训练VLA并选择性注入3D特征，避免大规模重新训练。\n2. **跳块分析技术**：识别动作专家中可跳过的非关键层（如第11-31层），最小化对预训练表示的干扰。\n3. **独特优势**：\n - 少样本多任务学习（每任务仅需20个样本）。\n - 区分真实物体与照片（利用3D信息避免\"物体幻觉\"）。\n - 高度适应性（适应训练中未见的桌面高度变化）。\n - 长时任务处理（如动态传送带物体抓取）。\n### 论文方法描述\n1. **架构设计**：\n - 冻结预训练的VLA主干（如DexVLA）和动作专家（如ScaleDP-1B）。\n - 点云通过分层卷积编码器提取特征，与动作嵌入对齐后注入选定层。\n2. **跳块分析**：\n - 通过跳过单层/多层实验，确定可替换的层（如第11-31层）。\n - 仅训练5个新增注入块，保持计算高效。\n3. **训练流程**：\n - 微调VLM以学习新语言指令，冻结大部分预训练权重。\n - 使用DexVLA的stage-1预训练权重初始化。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - **真实世界**：\n - 双手UR5e平台（3个Realsense相机：2个D435i腕部相机 + 1个L515顶部相机）。\n - 双手AgileX平台（与UR5e相似配置）。\n - 4个少样本任务（充电手机、擦拭盘子、放置面包、运输水果），每任务20个样本。\n - **仿真**：RoboTwin平台（14自由度移动双手机器人）。\n2. **训练资源**：\n - 计算资源未明确说明。\n - 使用DexVLA的stage-1预训练权重，沿用其stage-2超参数（chunk size=50）。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 真实机器人（UR5e和AgileX）及RoboTwin仿真平台。\n2. **评估指标**：\n - **少样本多任务**：任务平均成功率（%）。\n - **长时任务（如装配线打包）**：平均成功步长。\n - **真实-照片区分**：是否误抓取屏幕中的物体图像。\n - **高度适应性**：桌面高度变化下的抓取成功率。\n - **仿真基准**：各任务成功率（20/50样本下，测试100次均值±标准差）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>美的集团、上海大学、华东师范大学</p>\n<h3>论文概述</h3>\n<p>PointVLA提出了一种无需重新训练即可将3D点云注入预训练视觉-语言-动作模型（VLA）的框架。该方法冻结原始动作专家模块，通过轻量级模块化块注入3D特征，并利用跳块分析确定对性能影响最小的网络层进行特征注入，显著提升了模型的空间推理能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>高效3D注入机制</strong>：通过冻结预训练VLA并选择性注入3D特征，避免大规模重新训练。</li><li><strong>跳块分析技术</strong>：识别动作专家中可跳过的非关键层（如第11-31层），最小化对预训练表示的干扰。</li><li><strong>独特优势</strong>：</li></ol>\n<p> - 少样本多任务学习（每任务仅需20个样本）。</p>\n<p> - 区分真实物体与照片（利用3D信息避免\"物体幻觉\"）。</p>\n<p> - 高度适应性（适应训练中未见的桌面高度变化）。</p>\n<p> - 长时任务处理（如动态传送带物体抓取）。</p>\n<h3>论文方法描述</h3>\n<p>1. <strong>架构设计</strong>：</p>\n<p> - 冻结预训练的VLA主干（如DexVLA）和动作专家（如ScaleDP-1B）。</p>\n<p> - 点云通过分层卷积编码器提取特征，与动作嵌入对齐后注入选定层。</p>\n<p>2. <strong>跳块分析</strong>：</p>\n<p> - 通过跳过单层/多层实验，确定可替换的层（如第11-31层）。</p>\n<p> - 仅训练5个新增注入块，保持计算高效。</p>\n<p>3. <strong>训练流程</strong>：</p>\n<p> - 微调VLM以学习新语言指令，冻结大部分预训练权重。</p>\n<p> - 使用DexVLA的stage-1预训练权重初始化。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：</p>\n<p> - <strong>真实世界</strong>：</p>\n<p> - 双手UR5e平台（3个Realsense相机：2个D435i腕部相机 + 1个L515顶部相机）。</p>\n<p> - 双手AgileX平台（与UR5e相似配置）。</p>\n<p> - 4个少样本任务（充电手机、擦拭盘子、放置面包、运输水果），每任务20个样本。</p>\n<p> - <strong>仿真</strong>：RoboTwin平台（14自由度移动双手机器人）。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - 计算资源未明确说明。</p>\n<p> - 使用DexVLA的stage-1预训练权重，沿用其stage-2超参数（chunk size=50）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - 真实机器人（UR5e和AgileX）及RoboTwin仿真平台。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - <strong>少样本多任务</strong>：任务平均成功率（%）。</p>\n<p> - <strong>长时任务（如装配线打包）</strong>：平均成功步长。</p>\n<p> - <strong>真实-照片区分</strong>：是否误抓取屏幕中的物体图像。</p>\n<p> - <strong>高度适应性</strong>：桌面高度变化下的抓取成功率。</p>\n<p> - <strong>仿真基准</strong>：各任务成功率（20/50样本下，测试100次均值±标准差）。</p>"
  },
  {
    "date": "2025-03-06",
    "title": "Refined Policy Distillation: From VLA Generalists to RL Experts",
    "link": "http://arxiv.org/abs/2503.05833",
    "summary_markdown": "论文研究单位\nUniversity of Technology Nuremberg, Germany, Department of Computer Science & Artificial Intelligence\n\n论文概述\n该论文解决了视觉-语言-动作（VLA）模型的问题，这些模型虽然通用，但其成功率通常不如专家策略，且在设置变更时需要微调。作者提出了一种新的强化学习（RL）方法，称为精炼策略蒸馏（RPD）。RPD 结合了在策略强化学习和行为克隆，将大型通用 VLA 模型的知识提炼并精炼为小型、高性能的特定任务专家策略。该方法通过使用教师 VLA 的动作来指导学生策略的强化学习探索，从而提高样本效率和收敛速度。\n\n论文核心贡献点\n1. 提出了一种名为精炼策略蒸馏（RPD）的新方法，用于通过在策略强化学习将大型 VLA 提炼并精炼为紧凑的特定任务专家策略。\n2. 在多个任务上证明了 RPD 相比标准强化学习基线具有更高的样本效率和稳定性，包括稠密和稀疏奖励设置。\n3. 证明了 RPD 在分布外任务变体（如 VLA 未训练过的任务）和相机视角变化情况下的泛化能力。\n4. 通过在 ManiSkill3 上对微调的 Octo 和 OpenVLA 进行评估，提供了当前 VLA 在模拟环境中性能的见解。\n\n论文方法描述\n该方法的核心是扩展了近端策略优化（PPO）算法，并修改了其损失函数。RPD 的损失函数由两部分组成：标准的 PPO 损失（用于从环境中学习）和一个行为克隆（BC）损失。BC 损失采用均方误差（MSE）形式，用于最小化 PPO 学生策略的动作均值与 VLA 教师策略动作期望之间的差距。在训练过程中，RL 学生策略在与环境交互时，其探索过程会受到 VLA 教师策略的引导，从而提高样本效率，并最终能够通过环境交互超越教师策略的性能，实现策略精炼。\n\n论文使用数据集和训练资源\n数据集：使用了 ManiSkill3 模拟环境中的八个机器人操作任务。为了在模拟环境中评估 RPD，作者使用 ManiSkill3 提供的 RL 生成的专家演示数据对 Octo 和 OpenVLA 两个 VLA 模型进行了微调。其中两个任务（PullCubeTool 和 PokeCube）被保留为分布外测试任务，不参与 VLA 的微调。\n训练资源：训练使用了 Erlangen National HPC Center (NHR@FAU) 提供的高性能计算资源。具体的 GPU 资源包括用于运行 OpenVLA 的四个 Nvidia H100 节点，以及用于运行 Octo 的单个 Nvidia A40 GPU。\n\n论文使用的评估环境和评估指标\n评估环境：主要在 ManiSkill3 模拟环境中进行评估。为了测试泛化能力，还在分布外任务和改变相机视角的任务变体上进行了测试。\n评估指标：主要评估指标是任务成功率 和平均奖励。实验同时考虑了稠密奖励和稀疏奖励两种设置。成功率直接衡量任务完成的百分比，而平均奖励则反映了策略在每一步获得的总回报。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Technology Nuremberg, Germany, Department of Computer Science & Artificial Intelligence</p>\n\n<p>论文概述</p>\n<p>该论文解决了视觉-语言-动作（VLA）模型的问题，这些模型虽然通用，但其成功率通常不如专家策略，且在设置变更时需要微调。作者提出了一种新的强化学习（RL）方法，称为精炼策略蒸馏（RPD）。RPD 结合了在策略强化学习和行为克隆，将大型通用 VLA 模型的知识提炼并精炼为小型、高性能的特定任务专家策略。该方法通过使用教师 VLA 的动作来指导学生策略的强化学习探索，从而提高样本效率和收敛速度。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种名为精炼策略蒸馏（RPD）的新方法，用于通过在策略强化学习将大型 VLA 提炼并精炼为紧凑的特定任务专家策略。</li><li>在多个任务上证明了 RPD 相比标准强化学习基线具有更高的样本效率和稳定性，包括稠密和稀疏奖励设置。</li><li>证明了 RPD 在分布外任务变体（如 VLA 未训练过的任务）和相机视角变化情况下的泛化能力。</li><li>通过在 ManiSkill3 上对微调的 Octo 和 OpenVLA 进行评估，提供了当前 VLA 在模拟环境中性能的见解。</li></ol>\n\n<p>论文方法描述</p>\n<p>该方法的核心是扩展了近端策略优化（PPO）算法，并修改了其损失函数。RPD 的损失函数由两部分组成：标准的 PPO 损失（用于从环境中学习）和一个行为克隆（BC）损失。BC 损失采用均方误差（MSE）形式，用于最小化 PPO 学生策略的动作均值与 VLA 教师策略动作期望之间的差距。在训练过程中，RL 学生策略在与环境交互时，其探索过程会受到 VLA 教师策略的引导，从而提高样本效率，并最终能够通过环境交互超越教师策略的性能，实现策略精炼。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：使用了 ManiSkill3 模拟环境中的八个机器人操作任务。为了在模拟环境中评估 RPD，作者使用 ManiSkill3 提供的 RL 生成的专家演示数据对 Octo 和 OpenVLA 两个 VLA 模型进行了微调。其中两个任务（PullCubeTool 和 PokeCube）被保留为分布外测试任务，不参与 VLA 的微调。</p>\n<p>训练资源：训练使用了 Erlangen National HPC Center (NHR@FAU) 提供的高性能计算资源。具体的 GPU 资源包括用于运行 OpenVLA 的四个 Nvidia H100 节点，以及用于运行 Octo 的单个 Nvidia A40 GPU。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：主要在 ManiSkill3 模拟环境中进行评估。为了测试泛化能力，还在分布外任务和改变相机视角的任务变体上进行了测试。</p>\n<p>评估指标：主要评估指标是任务成功率 和平均奖励。实验同时考虑了稠密奖励和稀疏奖励两种设置。成功率直接衡量任务完成的百分比，而平均奖励则反映了策略在每一步获得的总回报。</p>"
  },
  {
    "date": "2025-03-06",
    "title": "VLA Model-Expert Collaboration for Bi-directional Manipulation Learning",
    "link": "http://arxiv.org/abs/2503.04163",
    "summary_markdown": "```markdown\n# 论文研究单位\n中国科学院自动化研究所多模态人工智能系统重点实验室，以及中国科学院大学人工智能学院。\n# 论文概述\n该论文提出了一种VLA模型-专家协作框架，旨在解决当前视觉-语言-动作（VLA）模型在多任务泛化方面的局限性。该框架通过引入有限数量的专家动作来增强VLA模型的性能，从而在降低专家工作量的同时，提升模型的可靠性和泛化能力。更重要的是，在协作过程中收集的操纵数据可用于进一步微调VLA模型，同时人类专家也能通过交互提升技能，形成一个双向的学习循环。实验结果在MetaWorld环境中验证了该框架在协作操纵和学习方面的有效性，显著提高了任务成功率。\n# 论文核心贡献点\n- 提出了一种半自主操纵方法，通过VLA模型与专家的协作完成任务，这是首次对VLA模型与专家协作的探索性研究。\n- 实现了一个双向学习过程：VLA模型可以利用协作中收集的操纵数据进行自我完善，同时专家也能逐渐适应VLA模型的行为模式。\n- 在MetaWorld的MT10和MT50基准测试中进行了验证。当VLA模型与专家的动作比例为4:1时，VLA模型的平均成功率分别提升了6.2%和13.5%，而人类专家的操作步骤减少了82.24%。\n# 论文方法描述\n- **VLA模型通用结构**: VLA模型将视觉输入（v_i）和语言指令（l_i）作为条件，通过各自的编码器处理后进行融合，最终预测出机器人的动作（a_i）。对于离散动作输出的模型，预测结果会被映射回连续的动作空间。\n- **专家策略**: 研究中定义了两种专家策略。一种是基于规则的策略，在MetaWorld模拟环境中提供近最优解；另一种是人类用户策略，由参与者直接控制机械臂，其表现可能因熟练度而异。\n- **专家-VLA协作**: 该过程包含“协作操纵”和“协作学习”两个阶段。在协作操纵中，VLA模型连续执行N步后，由专家介入执行一步，循环此模式直至任务完成。在协作学习中，将专家介入时产生的（视觉、语言、动作）数据存入缓冲区，当缓冲区满后，用这些数据对VLA模型进行监督式微调。\n# 论文使用数据集和训练资源\n- **数据集**: 使用MetaWorld模拟环境中的ML10（10个任务）和ML50（50个任务）基准数据集。模型微调数据是通过基于规则的策略为每个任务采集50条演示轨迹，每条轨迹最多500个步骤。\n- **训练资源**: VLA模型使用800K个采样数据进行微调。训练期间对视觉输入应用了随机裁剪、亮度/对比度/饱和度/色调调整等多种数据增强技术。动作输出根据模型类型（离散/连续）进行了归一化处理。\n# 论文使用的评估环境和评估指标\n- **评估环境**: MetaWorld模拟环境，具体在ML10和ML50多任务基准上进行评估。\n- **评估指标**:\n - **成功率**: 对每个任务进行50次随机初始化的测试，统计任务成功完成的比率。\n - **操作步骤数**: 记录并比较在纯专家策略和协作策略下，专家执行任务所需的平均动作步数，以量化专家工作量的减少。\n```",
    "summary_html": "<p>```markdown</p>\n<h1>论文研究单位</h1>\n<p>中国科学院自动化研究所多模态人工智能系统重点实验室，以及中国科学院大学人工智能学院。</p>\n<h1>论文概述</h1>\n<p>该论文提出了一种VLA模型-专家协作框架，旨在解决当前视觉-语言-动作（VLA）模型在多任务泛化方面的局限性。该框架通过引入有限数量的专家动作来增强VLA模型的性能，从而在降低专家工作量的同时，提升模型的可靠性和泛化能力。更重要的是，在协作过程中收集的操纵数据可用于进一步微调VLA模型，同时人类专家也能通过交互提升技能，形成一个双向的学习循环。实验结果在MetaWorld环境中验证了该框架在协作操纵和学习方面的有效性，显著提高了任务成功率。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出了一种半自主操纵方法，通过VLA模型与专家的协作完成任务，这是首次对VLA模型与专家协作的探索性研究。</li><li>实现了一个双向学习过程：VLA模型可以利用协作中收集的操纵数据进行自我完善，同时专家也能逐渐适应VLA模型的行为模式。</li><li>在MetaWorld的MT10和MT50基准测试中进行了验证。当VLA模型与专家的动作比例为4:1时，VLA模型的平均成功率分别提升了6.2%和13.5%，而人类专家的操作步骤减少了82.24%。</li></ul>\n<h1>论文方法描述</h1>\n<ul><li><strong>VLA模型通用结构</strong>: VLA模型将视觉输入（v_i）和语言指令（l_i）作为条件，通过各自的编码器处理后进行融合，最终预测出机器人的动作（a_i）。对于离散动作输出的模型，预测结果会被映射回连续的动作空间。</li><li><strong>专家策略</strong>: 研究中定义了两种专家策略。一种是基于规则的策略，在MetaWorld模拟环境中提供近最优解；另一种是人类用户策略，由参与者直接控制机械臂，其表现可能因熟练度而异。</li><li><strong>专家-VLA协作</strong>: 该过程包含“协作操纵”和“协作学习”两个阶段。在协作操纵中，VLA模型连续执行N步后，由专家介入执行一步，循环此模式直至任务完成。在协作学习中，将专家介入时产生的（视觉、语言、动作）数据存入缓冲区，当缓冲区满后，用这些数据对VLA模型进行监督式微调。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li><strong>数据集</strong>: 使用MetaWorld模拟环境中的ML10（10个任务）和ML50（50个任务）基准数据集。模型微调数据是通过基于规则的策略为每个任务采集50条演示轨迹，每条轨迹最多500个步骤。</li><li><strong>训练资源</strong>: VLA模型使用800K个采样数据进行微调。训练期间对视觉输入应用了随机裁剪、亮度/对比度/饱和度/色调调整等多种数据增强技术。动作输出根据模型类型（离散/连续）进行了归一化处理。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li><strong>评估环境</strong>: MetaWorld模拟环境，具体在ML10和ML50多任务基准上进行评估。</li><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>成功率</strong>: 对每个任务进行50次随机初始化的测试，统计任务成功完成的比率。</p>\n<p> - <strong>操作步骤数</strong>: 记录并比较在纯专家策略和协作策略下，专家执行任务所需的平均动作步数，以量化专家工作量的减少。</p>\n<p>```</p>"
  },
  {
    "date": "2025-03-05",
    "title": "OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction",
    "link": "http://arxiv.org/abs/2503.03734",
    "summary_markdown": "论文研究单位\n论文概述\n论文核心贡献点\n论文方法描述\n论文使用数据集和训练资源\n论文使用的评估环境和评估指标",
    "summary_html": "<p>论文研究单位</p>\n<p>论文概述</p>\n<p>论文核心贡献点</p>\n<p>论文方法描述</p>\n<p>论文使用数据集和训练资源</p>\n<p>论文使用的评估环境和评估指标</p>"
  },
  {
    "date": "2025-03-05",
    "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning",
    "link": "http://arxiv.org/abs/2503.03480",
    "summary_markdown": "### 论文研究单位\n\nInstitute for Artificial Intelligence, Peking University; PKU-PsiBot Joint Lab; State Key Laboratory of General Artificial Intelligence, Peking University.\n### 论文概述\n\n该论文探讨了如何将安全约束明确地整合到视觉-语言-动作模型（VLA）中。VLA作为通用机器人策略具有巨大潜力，但在实际部署中存在极端安全挑战，可能对环境、机器人自身和人类造成伤害。作者提出了一种集成安全方法（ISA），系统地建模安全需求，主动激发多样化的不安全行为，通过安全强化学习有效约束VLA策略，并通过有针对性的评估来严格确保其安全性。该方法利用约束马尔可夫决策过程（CMDP）范式，从最小-最大角度优化VLA以应对激发的安全风险。实验表明，通过ISA对齐的策略实现了有效的安全-性能权衡，与最先进方法相比，安全违规累积成本降低了83.58%，同时任务成功率提高了3.85%，并在长尾风险缓解和极端故障场景处理上表现出强大的安全保证，以及对分布外扰动的鲁棒泛化能力。\n### 论文核心贡献点\n\n1. 集成安全方法（ISA）的探索：系统地研究了VLA安全对齐的集成方法，包括安全建模、风险激发、策略约束和安全保证四个关键方面。\n2. 环境：为弥补VLA安全评估的空白，引入了Safety-CHORES测试平台。该基准内置了细粒度的安全约束，融合了导航和操作的多样化长视野任务，并通过大规模程序生成的场景和专门针对安全关键组件的设计，比传统基准更有效地暴露VLA的脆弱性。\n3. 实验验证与关键发现：通过广泛的实验证明，ISA对齐的策略实现了有效的安全-性能权衡，平均安全性比最先进方法提高83.58%，同时维持任务性能（+3.85%）；具有强大的安全保证，特别是在缓解长尾风险和处理极端故障场景方面；以及学习到的安全行为对各种分布外扰动的鲁棒泛化能力。\n### 论文方法描述\n\n1. 建模安全：定义场景、规范和任务的表示，使用状态-动作谓词和轨迹级谓词来形式化安全约束。\n2. 激发风险：利用大规模程序生成的室内场景和3D资产库，结合安全关键组件（如角落、盲点、易碎品集合、关键点、危险设备）来系统化地激发多样的不安全行为。\n3. 约束策略：将安全谓词转换为成本信号，采用基于拉格朗日的SafeRL技术，通过最小-最大优化过程在满足安全约束的前提下最大化任务奖励，动态调整策略参数和拉格朗日乘子以平衡安全和性能。\n4. 保证安全：通过测试时安全、长尾安全和极端故障安全三个维度进行综合评估，验证模型的安全性能和鲁棒性。\n### 论文使用数据集和训练资源\n\n数据集：Safety-CHORES，基于AI2THOR模拟器构建，包含150K多样化室内场景（由ProcTHOR生成）和800K 3D资产（来自Objaverse），设计了包含安全关键组件的导航和操作任务。\n训练资源：使用SPOC-DINOv2作为初始预训练VLA模型，在复杂任务（如Safety-Fetch）上训练2500万步，简单任务（如Safety-ObjNav和Safety-PickUp）上训练1500万步。\n### 论文使用的评估环境和评估指标\n\n评估环境：主要在Safety-CHORES基准上进行评估，该基准包含Safety-ObjNav、Safety-PickUp和Safety-Fetch三类任务。同时，在AI2THOR、iTHOR、ProcTHOR等现有基准上进行对比实验。评估还考虑了分布外扰动（颜色、光照、材质变化）和极端故障场景（如任务不可能完成时）。\n评估指标：任务成功率（SR），衡量任务完成的程度；累积成本（CC），量化整个轨迹中所有安全违规的总成本，计算为每一步违反各类安全约束的成本之和。",
    "summary_html": "<h3>论文研究单位</h3>\n\n<p>Institute for Artificial Intelligence, Peking University; PKU-PsiBot Joint Lab; State Key Laboratory of General Artificial Intelligence, Peking University.</p>\n<h3>论文概述</h3>\n\n<p>该论文探讨了如何将安全约束明确地整合到视觉-语言-动作模型（VLA）中。VLA作为通用机器人策略具有巨大潜力，但在实际部署中存在极端安全挑战，可能对环境、机器人自身和人类造成伤害。作者提出了一种集成安全方法（ISA），系统地建模安全需求，主动激发多样化的不安全行为，通过安全强化学习有效约束VLA策略，并通过有针对性的评估来严格确保其安全性。该方法利用约束马尔可夫决策过程（CMDP）范式，从最小-最大角度优化VLA以应对激发的安全风险。实验表明，通过ISA对齐的策略实现了有效的安全-性能权衡，与最先进方法相比，安全违规累积成本降低了83.58%，同时任务成功率提高了3.85%，并在长尾风险缓解和极端故障场景处理上表现出强大的安全保证，以及对分布外扰动的鲁棒泛化能力。</p>\n<h3>论文核心贡献点</h3>\n\n<ol><li>集成安全方法（ISA）的探索：系统地研究了VLA安全对齐的集成方法，包括安全建模、风险激发、策略约束和安全保证四个关键方面。</li><li>环境：为弥补VLA安全评估的空白，引入了Safety-CHORES测试平台。该基准内置了细粒度的安全约束，融合了导航和操作的多样化长视野任务，并通过大规模程序生成的场景和专门针对安全关键组件的设计，比传统基准更有效地暴露VLA的脆弱性。</li><li>实验验证与关键发现：通过广泛的实验证明，ISA对齐的策略实现了有效的安全-性能权衡，平均安全性比最先进方法提高83.58%，同时维持任务性能（+3.85%）；具有强大的安全保证，特别是在缓解长尾风险和处理极端故障场景方面；以及学习到的安全行为对各种分布外扰动的鲁棒泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n\n<ol><li>建模安全：定义场景、规范和任务的表示，使用状态-动作谓词和轨迹级谓词来形式化安全约束。</li><li>激发风险：利用大规模程序生成的室内场景和3D资产库，结合安全关键组件（如角落、盲点、易碎品集合、关键点、危险设备）来系统化地激发多样的不安全行为。</li><li>约束策略：将安全谓词转换为成本信号，采用基于拉格朗日的SafeRL技术，通过最小-最大优化过程在满足安全约束的前提下最大化任务奖励，动态调整策略参数和拉格朗日乘子以平衡安全和性能。</li><li>保证安全：通过测试时安全、长尾安全和极端故障安全三个维度进行综合评估，验证模型的安全性能和鲁棒性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n\n<p>数据集：Safety-CHORES，基于AI2THOR模拟器构建，包含150K多样化室内场景（由ProcTHOR生成）和800K 3D资产（来自Objaverse），设计了包含安全关键组件的导航和操作任务。</p>\n<p>训练资源：使用SPOC-DINOv2作为初始预训练VLA模型，在复杂任务（如Safety-Fetch）上训练2500万步，简单任务（如Safety-ObjNav和Safety-PickUp）上训练1500万步。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n\n<p>评估环境：主要在Safety-CHORES基准上进行评估，该基准包含Safety-ObjNav、Safety-PickUp和Safety-Fetch三类任务。同时，在AI2THOR、iTHOR、ProcTHOR等现有基准上进行对比实验。评估还考虑了分布外扰动（颜色、光照、材质变化）和极端故障场景（如任务不可能完成时）。</p>\n<p>评估指标：任务成功率（SR），衡量任务完成的程度；累积成本（CC），量化整个轨迹中所有安全违规的总成本，计算为每一步违反各类安全约束的成本之和。</p>"
  },
  {
    "date": "2025-03-04",
    "title": "RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour",
    "link": "http://arxiv.org/abs/2503.02572",
    "summary_markdown": "# 论文研究单位\nSkolkovo Institute of Science and Technology Moscow\n# 论文概述\nRaceVLA提出了一种创新的自主竞速无人机导航方法，通过利用视觉-语言-动作（VLA）模型来模拟人类类似的行为。该模型处理来自无人机机载摄像头的第一人称视角（FPV）视频流和自然语言指令，以生成控制无人机的速度向量和偏航角速度信号。该研究旨在使无人机能够根据实时环境反馈调整其导航策略，模仿人类飞行员的决策过程。模型在收集的竞速无人机数据集上进行微调，在复杂的无人机竞速环境中展现出强大的泛化能力。\n# 论文核心贡献点\n1. 提出了RaceVLA，这是第一个专门为自主竞速无人机设计的基于VLA的系统。\n2. 在运动泛化（75.0 vs. 60.0）和语义泛化（45.5 vs. 36.3）方面优于OpenVLA，并在所有评估轴（视觉、运动、物理、语义）上均优于RT-2。\n3. 利用动态相机特性训练模型，使其对相机位置变化具有鲁棒性，从而显著提升了运动泛化能力。\n4. 实现了高速场景下的有效导航，平均速度为1.04 m/s，最高速度为2.02 m/s，展示了系统的实时决策能力。\n5. 开源了RaceVLA的代码库、预训练权重和数据集，促进了相关领域的研究。\n# 论文方法描述\nRaceVLA的核心是一个基于OpenVLA模型进行微调的VLA模型，其输入是来自无人机机载摄像头的FPV帧和自然语言指令，输出是直接控制无人机的四维动作向量。\n- 模型架构：在OpenVLA-7b模型基础上，使用低秩适应（LoRA）技术进行微调，将原始的7维机械臂动作向量适配为适用于无人机控制的4维速度向量（Vx, Vy, Vz, omega）。\n- 系统操作：无人机导航遵循一个迭代过程，系统实时处理连续的FPV帧和语言指令来计算并执行下一步飞行动作，而不是采用传统的基于路径点的导航方式。\n- 硬件平台：使用定制的8英寸竞速无人机，配备Speedy Bee F405飞控（运行ArduPilot固件）、RealSense T265摄像头以及用于定位和计算的机载Intel NUC计算机。\n- 软件流程：VLA模型运行在远程服务器上，通过Flask API接收无人机传来的实时帧，处理后将导航命令发回无人机。定位由OpenVINS视觉惯性里程计系统提供，整个系统在机器人操作系统（ROS）框架下运行。\n# 论文使用数据集和训练资源\n- 数据集规模：包含200个片段（episodes）和大约20,000张图像。\n- 数据集内容：涵盖了多种竞速任务，如穿过拱形门、方形门和在环形赛道上飞行。每个数据样本包括位置数据、速度分量（Vx, Vy, Vz）、偏航角变化和同步的视觉帧。\n- 数据采集：使用Vicon运动捕捉系统以60Hz的频率记录无人机速度，使用T265摄像头以30Hz的频率捕捉图像。\n- 训练模型：对OpenVLA-7b模型进行微调。\n- 训练技术：采用参数高效的LoRA技术，使用rank-32的适配器来优化内存使用。\n- 训练配置：批次大小为16，学习率为5e-4，共进行7000个梯度步，未使用图像增强。\n- 训练资源：使用一块NVIDIA A100 GPU进行训练。\n# 论文使用的评估环境和评估指标\n- 评估环境：\n 1. 在无人机竞速赛道上进行任务评估，包括穿越单个门、根据指令选择特定门、在变化的配置下完成连续多门穿越任务。\n 2. 在预定义的竞速赛道上进行多次飞行测试。\n 3. 评估模型的泛化能力，涵盖四个维度：视觉（新背景、干扰物）、运动（新物体位置和姿态）、物理（新物体形状和尺寸）和语义（新指令和目标物体）。\n- 评估指标：\n 1. 泛化成功率：在视觉、运动、物理和语义四个维度上的任务成功百分比。\n 2. 飞行参数：包括平均速度、最大速度、速度标准差、平均偏航角速度和任务平均持续时间。\n 3. 性能对比：与基准模型OpenVLA和RT-2在不同泛化维度上的性能比较。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Skolkovo Institute of Science and Technology Moscow</p>\n<h1>论文概述</h1>\n<p>RaceVLA提出了一种创新的自主竞速无人机导航方法，通过利用视觉-语言-动作（VLA）模型来模拟人类类似的行为。该模型处理来自无人机机载摄像头的第一人称视角（FPV）视频流和自然语言指令，以生成控制无人机的速度向量和偏航角速度信号。该研究旨在使无人机能够根据实时环境反馈调整其导航策略，模仿人类飞行员的决策过程。模型在收集的竞速无人机数据集上进行微调，在复杂的无人机竞速环境中展现出强大的泛化能力。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出了RaceVLA，这是第一个专门为自主竞速无人机设计的基于VLA的系统。</li><li>在运动泛化（75.0 vs. 60.0）和语义泛化（45.5 vs. 36.3）方面优于OpenVLA，并在所有评估轴（视觉、运动、物理、语义）上均优于RT-2。</li><li>利用动态相机特性训练模型，使其对相机位置变化具有鲁棒性，从而显著提升了运动泛化能力。</li><li>实现了高速场景下的有效导航，平均速度为1.04 m/s，最高速度为2.02 m/s，展示了系统的实时决策能力。</li><li>开源了RaceVLA的代码库、预训练权重和数据集，促进了相关领域的研究。</li></ol>\n<h1>论文方法描述</h1>\n<p>RaceVLA的核心是一个基于OpenVLA模型进行微调的VLA模型，其输入是来自无人机机载摄像头的FPV帧和自然语言指令，输出是直接控制无人机的四维动作向量。</p>\n<ul><li>模型架构：在OpenVLA-7b模型基础上，使用低秩适应（LoRA）技术进行微调，将原始的7维机械臂动作向量适配为适用于无人机控制的4维速度向量（Vx, Vy, Vz, omega）。</li><li>系统操作：无人机导航遵循一个迭代过程，系统实时处理连续的FPV帧和语言指令来计算并执行下一步飞行动作，而不是采用传统的基于路径点的导航方式。</li><li>硬件平台：使用定制的8英寸竞速无人机，配备Speedy Bee F405飞控（运行ArduPilot固件）、RealSense T265摄像头以及用于定位和计算的机载Intel NUC计算机。</li><li>软件流程：VLA模型运行在远程服务器上，通过Flask API接收无人机传来的实时帧，处理后将导航命令发回无人机。定位由OpenVINS视觉惯性里程计系统提供，整个系统在机器人操作系统（ROS）框架下运行。</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集规模：包含200个片段（episodes）和大约20,000张图像。</li><li>数据集内容：涵盖了多种竞速任务，如穿过拱形门、方形门和在环形赛道上飞行。每个数据样本包括位置数据、速度分量（Vx, Vy, Vz）、偏航角变化和同步的视觉帧。</li><li>数据采集：使用Vicon运动捕捉系统以60Hz的频率记录无人机速度，使用T265摄像头以30Hz的频率捕捉图像。</li><li>训练模型：对OpenVLA-7b模型进行微调。</li><li>训练技术：采用参数高效的LoRA技术，使用rank-32的适配器来优化内存使用。</li><li>训练配置：批次大小为16，学习率为5e-4，共进行7000个梯度步，未使用图像增强。</li><li>训练资源：使用一块NVIDIA A100 GPU进行训练。</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：</li></ul>\n<p> 1. 在无人机竞速赛道上进行任务评估，包括穿越单个门、根据指令选择特定门、在变化的配置下完成连续多门穿越任务。</p>\n<p> 2. 在预定义的竞速赛道上进行多次飞行测试。</p>\n<p> 3. 评估模型的泛化能力，涵盖四个维度：视觉（新背景、干扰物）、运动（新物体位置和姿态）、物理（新物体形状和尺寸）和语义（新指令和目标物体）。</p>\n<ul><li>评估指标：</li></ul>\n<p> 1. 泛化成功率：在视觉、运动、物理和语义四个维度上的任务成功百分比。</p>\n<p> 2. 飞行参数：包括平均速度、最大速度、速度标准差、平均偏航角速度和任务平均持续时间。</p>\n<p> 3. 性能对比：与基准模型OpenVLA和RT-2在不同泛化维度上的性能比较。</p>"
  },
  {
    "date": "2025-03-04",
    "title": "Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding",
    "link": "http://arxiv.org/abs/2503.02310",
    "summary_markdown": "# 论文研究单位\n* The Hong Kong University of Science and Technology (Guangzhou)\n* Westlake University\n* Zhejiang University\n* Monash University\n# 论文概述\n该论文针对集成了动作分块技术的视觉-语言-动作模型在推理时因自回归解码而效率低下的问题，提出了一个名为PD-VLA的并行解码框架。该方法将传统的自回归解码过程重塑为一个非线性方程组，并通过并行定点迭代法进行求解。此方法在不改变模型架构、无需重新训练的情况下，显著提升了动作序列的生成速度，同时保持了模型的性能。PD-VLA框架易于部署，并能与其他加速技术无缝协同工作。\n# 论文核心贡献点\n1. 提出了首个面向集成动作分块的VLA模型的并行解码框架，在保持模型性能的同时，消除了自回归解码的效率瓶颈。\n2. 设计了一种仅针对解码过程的加速策略，实现了模型免修改、免训练的友好部署，并能与现有加速方法无缝结合。\n3. 通过在仿真和真实世界平台上进行广泛的实验和消融研究，对该方法进行了全面的实证验证。\n# 论文方法描述\n该方法构建于一个基础的视觉-语言-动作模型（LLaVA-VLA）之上，该模型以图像和语言指令作为输入，通过视觉编码器处理图像，再由大语言模型生成动作令牌。在此基础上，集成了动作分块技术，模型一次预测未来m个时间步的动作序列（m=5），以提升控制性能。\n核心创新是并行解码机制，它将传统的顺序令牌预测问题转化为一个非线性方程组的求解问题。具体而言，利用Jacobi定点迭代法，在每一轮迭代中，根据上一轮的所有令牌预测结果，并行地更新整个动作序列的所有令牌。在推理阶段，该方法通过将模型中的因果注意力掩码替换为双向注意力掩码来打破令牌间的序列依赖，从而实现并行计算，大幅缩短了生成一个动作块所需的时间。\n# 论文使用数据集和训练资源\n* 数据集：论文提供的HTML原文中未明确指出所使用的具体数据集名称。\n* 训练资源：论文提供的HTML原文中未明确指出所使用的具体计算资源（如GPU型号、训练时长等）。\n# 论文使用的评估环境和评估指标\n* 评估环境：\n * 仿真环境：在模拟器中进行实验。\n * 真实世界环境：在实际的机械臂平台上进行任务验证。\n* 评估指标：\n * 任务成功率：衡量模型执行指定任务的成功比例。\n * 执行频率：衡量模型生成动作的速度，通过与基础VLA模型比较的加速倍数（如2.52倍）来量化。",
    "summary_html": "<h1>论文研究单位</h1>\n<p>* The Hong Kong University of Science and Technology (Guangzhou)</p>\n<p>* Westlake University</p>\n<p>* Zhejiang University</p>\n<p>* Monash University</p>\n<h1>论文概述</h1>\n<p>该论文针对集成了动作分块技术的视觉-语言-动作模型在推理时因自回归解码而效率低下的问题，提出了一个名为PD-VLA的并行解码框架。该方法将传统的自回归解码过程重塑为一个非线性方程组，并通过并行定点迭代法进行求解。此方法在不改变模型架构、无需重新训练的情况下，显著提升了动作序列的生成速度，同时保持了模型的性能。PD-VLA框架易于部署，并能与其他加速技术无缝协同工作。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出了首个面向集成动作分块的VLA模型的并行解码框架，在保持模型性能的同时，消除了自回归解码的效率瓶颈。</li><li>设计了一种仅针对解码过程的加速策略，实现了模型免修改、免训练的友好部署，并能与现有加速方法无缝结合。</li><li>通过在仿真和真实世界平台上进行广泛的实验和消融研究，对该方法进行了全面的实证验证。</li></ol>\n<h1>论文方法描述</h1>\n<p>该方法构建于一个基础的视觉-语言-动作模型（LLaVA-VLA）之上，该模型以图像和语言指令作为输入，通过视觉编码器处理图像，再由大语言模型生成动作令牌。在此基础上，集成了动作分块技术，模型一次预测未来m个时间步的动作序列（m=5），以提升控制性能。</p>\n<p>核心创新是并行解码机制，它将传统的顺序令牌预测问题转化为一个非线性方程组的求解问题。具体而言，利用Jacobi定点迭代法，在每一轮迭代中，根据上一轮的所有令牌预测结果，并行地更新整个动作序列的所有令牌。在推理阶段，该方法通过将模型中的因果注意力掩码替换为双向注意力掩码来打破令牌间的序列依赖，从而实现并行计算，大幅缩短了生成一个动作块所需的时间。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>* 数据集：论文提供的HTML原文中未明确指出所使用的具体数据集名称。</p>\n<p>* 训练资源：论文提供的HTML原文中未明确指出所使用的具体计算资源（如GPU型号、训练时长等）。</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>* 评估环境：</p>\n<p> * 仿真环境：在模拟器中进行实验。</p>\n<p> * 真实世界环境：在实际的机械臂平台上进行任务验证。</p>\n<p>* 评估指标：</p>\n<p> * 任务成功率：衡量模型执行指定任务的成功比例。</p>\n<p> * 执行频率：衡量模型生成动作的速度，通过与基础VLA模型比较的加速倍数（如2.52倍）来量化。</p>"
  },
  {
    "date": "2025-03-03",
    "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
    "link": "http://arxiv.org/abs/2503.01378",
    "summary_markdown": "论文研究单位\nSkolkovo Institute of Science and Technology\n\n论文概述\n该论文介绍了CognitiveDrone，一个专为无人机（UAV）复杂认知任务设计的新型视觉-语言-行动（VLA）模型。该模型在包含超过8,000条模拟飞行轨迹的数据集上训练，涵盖Human Recognition、Symbol Understanding和Reasoning三大类别，能够根据第一人称视觉输入和文本指令生成实时的4D动作命令。为提升在复杂场景下的表现，论文进一步提出了CognitiveDrone-R1，它集成了一个额外的视觉-语言模型（VLM）推理模块，以在高频控制前简化任务指令。论文还发布了开源评估基准CognitiveDroneBench，用于评估无人机在认知任务中的表现。实验结果表明，CognitiveDrone-R1在关键认知任务上的成功率比基础模型有显著提升。\n\n论文核心贡献点\n1. 提出了CognitiveDrone，一个专为解决复杂无人机认知任务而设计的VLA模型。\n2. 开发了CognitiveDrone-R1，一个集成了VLM推理模块的增强版系统，用于提升任务理解和推理能力。\n3. 创建并发布了CognitiveDroneBench，这是首个专门用于评估无人机认知任务的开源基准测试。\n4. 构建并开源了一个包含8,062个样本的数据集，覆盖Human Recognition、Symbol Understanding和Reasoning三个认知类别。\n5. 公开了完整的代码库、模型权重、训练和推理代码以及基准环境。\n\n论文方法描述\n系统架构基于开源的OpenVLA模型，一个拥有70亿参数的VLA模型，用于从第一人称视觉输入和文本指令生成平滑的4D（Vx, Vy, Vz, omega）无人机控制命令。该模型在包含8,000多个模拟飞行片段的自定义数据集上进行了微调，使其对无人机飞行物理和认知任务有深刻理解。为处理更复杂的认知任务，CognitiveDrone-R1架构增加了一个独立的70亿参数VLM（基于Qwen2.5-VL）作为推理模块。VLM以较低频率（约2Hz）运行，负责处理和简化任务指令，然后将简化后的指令传递给以较高频率（10Hz）运行的VLA模型，后者负责生成实时的控制信号。这种设计结合了VLA的快速反应控制和VLM的深层推理能力。\n\n论文使用数据集和训练资源\n数据集：论文收集了一个包含8,062条连续轨迹样本的自定义数据集，分为三个认知类别：Human Recognition（根据文本提示识别特定人物并导航至相应门）、Symbol Understanding（识别数字、字母、公司徽标、动物符号）和Reasoning（进行逻辑推断，如解决数学问题或根据属性关联对象）。数据以RLDS格式组织，记录了无人机在模拟环境中的速度和姿态。数据集被划分为训练集和测试集。\n训练资源：模型在四块NVIDIA A100 GPU上进行训练。基础模型为OpenVLA-7b，并采用参数高效的LoRA方法（秩32适配器）进行微调。训练配置包括64的批次大小、5e-4的学习率、4000个梯度步长，并禁用了图像增强。\n\n论文使用的评估环境和评估指标\n评估环境：使用名为CognitiveDroneBench的开源基准进行评估。该基准基于Gazebo物理模拟器，集成了ArduPilot插件以真实模拟无人机动力学。环境中构建了一个带有多个连续认知检查点的赛道。无人机在每个检查点会接收到第一人称视角图像和文本指令，需要通过解决认知任务来选择正确的门并穿越。\n评估指标：主要评估指标是成功率。无人机成功穿越正确的门得1分。每个任务类别（Human Recognition, Symbol Understanding, Reasoning）的最终得分是该类别累积得分除以最大可能得分。此外，还计算了所有任务样本的总体平均成功率，以衡量模型的综合性能。",
    "summary_html": "<p>论文研究单位</p>\n<p>Skolkovo Institute of Science and Technology</p>\n\n<p>论文概述</p>\n<p>该论文介绍了CognitiveDrone，一个专为无人机（UAV）复杂认知任务设计的新型视觉-语言-行动（VLA）模型。该模型在包含超过8,000条模拟飞行轨迹的数据集上训练，涵盖Human Recognition、Symbol Understanding和Reasoning三大类别，能够根据第一人称视觉输入和文本指令生成实时的4D动作命令。为提升在复杂场景下的表现，论文进一步提出了CognitiveDrone-R1，它集成了一个额外的视觉-语言模型（VLM）推理模块，以在高频控制前简化任务指令。论文还发布了开源评估基准CognitiveDroneBench，用于评估无人机在认知任务中的表现。实验结果表明，CognitiveDrone-R1在关键认知任务上的成功率比基础模型有显著提升。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了CognitiveDrone，一个专为解决复杂无人机认知任务而设计的VLA模型。</li><li>开发了CognitiveDrone-R1，一个集成了VLM推理模块的增强版系统，用于提升任务理解和推理能力。</li><li>创建并发布了CognitiveDroneBench，这是首个专门用于评估无人机认知任务的开源基准测试。</li><li>构建并开源了一个包含8,062个样本的数据集，覆盖Human Recognition、Symbol Understanding和Reasoning三个认知类别。</li><li>公开了完整的代码库、模型权重、训练和推理代码以及基准环境。</li></ol>\n\n<p>论文方法描述</p>\n<p>系统架构基于开源的OpenVLA模型，一个拥有70亿参数的VLA模型，用于从第一人称视觉输入和文本指令生成平滑的4D（Vx, Vy, Vz, omega）无人机控制命令。该模型在包含8,000多个模拟飞行片段的自定义数据集上进行了微调，使其对无人机飞行物理和认知任务有深刻理解。为处理更复杂的认知任务，CognitiveDrone-R1架构增加了一个独立的70亿参数VLM（基于Qwen2.5-VL）作为推理模块。VLM以较低频率（约2Hz）运行，负责处理和简化任务指令，然后将简化后的指令传递给以较高频率（10Hz）运行的VLA模型，后者负责生成实时的控制信号。这种设计结合了VLA的快速反应控制和VLM的深层推理能力。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：论文收集了一个包含8,062条连续轨迹样本的自定义数据集，分为三个认知类别：Human Recognition（根据文本提示识别特定人物并导航至相应门）、Symbol Understanding（识别数字、字母、公司徽标、动物符号）和Reasoning（进行逻辑推断，如解决数学问题或根据属性关联对象）。数据以RLDS格式组织，记录了无人机在模拟环境中的速度和姿态。数据集被划分为训练集和测试集。</p>\n<p>训练资源：模型在四块NVIDIA A100 GPU上进行训练。基础模型为OpenVLA-7b，并采用参数高效的LoRA方法（秩32适配器）进行微调。训练配置包括64的批次大小、5e-4的学习率、4000个梯度步长，并禁用了图像增强。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：使用名为CognitiveDroneBench的开源基准进行评估。该基准基于Gazebo物理模拟器，集成了ArduPilot插件以真实模拟无人机动力学。环境中构建了一个带有多个连续认知检查点的赛道。无人机在每个检查点会接收到第一人称视角图像和文本指令，需要通过解决认知任务来选择正确的门并穿越。</p>\n<p>评估指标：主要评估指标是成功率。无人机成功穿越正确的门得1分。每个任务类别（Human Recognition, Symbol Understanding, Reasoning）的最终得分是该类别累积得分除以最大可能得分。此外，还计算了所有任务样本的总体平均成功率，以衡量模型的综合性能。</p>"
  },
  {
    "date": "2025-02-27",
    "title": "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success",
    "link": "http://arxiv.org/abs/2502.19645",
    "summary_markdown": "论文研究单位\n斯坦福大学\n\n论文概述\n该论文研究了如何优化视觉-语言-动作模型（VLA）的微调过程，以提升其在机器人任务中的速度和成功率。以OpenVLA为基础模型，系统性地比较了不同动作解码方案、动作表示和学习目标对微调效果的影响。研究发现，并行解码与动作分块结合、连续动作表示和L1回归目标能同时提升推理效率、策略性能和模型的输入输出灵活性。基于此，提出了OpenVLA-OFT方法，在LIBERO仿真基准上取得了97.1%的平均成功率，比基线提升20.6%，同时实现26倍的动作生成加速。在真实的ALOHA双手机器人上，OpenVLA-OFT+成功执行了高频率的灵巧操作任务，平均成功率比其他方法和从头训练的策略高出15%。\n\n论文核心贡献点\n1. 系统性分析了VLA微调的关键设计选择，包括动作解码策略（自回归 vs 并行）、动作表示（离散 vs 连续）和学习目标（下一词预测 vs L1回归 vs 扩散模型）。\n2. 提出了一种优化的微调配方（OpenVLA-OFT），整合了并行解码、动作分块、连续动作表示和L1回归，共同提升推理效率、任务性能和输入输出灵活性。\n3. 在LIBERO基准测试中，OpenVLA-OFT实现了新的最先进性能，平均成功率达97.1%，并将动作生成吞吐量提高了26倍。\n4. 在ALOHA真实机器人上，通过加入FiLM增强语言理解（OFT+），成功执行了高频率双臂任务，平均成功率优于其他VLAs和从零开始训练的模仿学习策略高达15%。\n\n论文方法描述\n1. 并行解码与动作分块：将自回归解码改为并行解码，使用双向注意力掩码，使模型在一次前向传播中预测整个动作序列。动作分块通过在输入中添加额外的空动作嵌入实现，使模型一次性预测多个未来时间步的动作。\n2. 连续动作表示：用MLP动作头替换离散标记的softmax输出层，直接预测连续的归一化动作值。\n3. L1回归目标：最小化预测动作与真实动作之间的L1距离，替代下一词预测或扩散模型。\n4. FiLM增强：在视觉变换器块中，通过语言嵌入计算缩放和偏移向量，对视觉特征进行仿射变换，以增强模型对语言指令的关注。\n5. 多模态输入处理：支持多视角图像和机器人本体感觉输入，通过共享投影器将视觉特征映射到语言嵌入空间，并与语言标记和状态嵌入连接。\n\n论文使用数据集和训练资源\n数据集：\n1. LIBERO仿真基准：包含四个任务套件（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long），每个套件包含10个任务，每个任务500个专家演示，用于评估泛化能力。\n2. ALOHA真实机器人任务：设计了四个任务（折叠短裤、折叠T恤、用勺子舀取、放置物品），包含20-300个演示，用于测试双手机器人的灵巧操作和语言跟随能力。\n训练资源：\n1. GPU：使用8块NVIDIA A100或H100（80GB）GPU进行微调。\n2. 训练步数：非扩散方法训练50-150K步，扩散方法训练100-250K步。\n3. 批大小：64-128（LIBERO实验），32（ALOHA实验）。\n4. 动作块大小：LIBERO实验设为8，ALOHA实验设为25。\n\n论文使用的评估环境和评估指标\n评估环境：\n1. LIBERO仿真环境：使用Franka Emika Panda机械臂，评估任务包括空间布局、物体操作、目标导向和长时程任务。\n2. ALOHA真实机器人：双手机器人平台，包含两个ViperX 300S机械臂、三个摄像头视角（一个俯视、两个腕部摄像头）和14维关节角状态输入，控制频率为25 Hz。\n评估指标：\n1. 任务成功率：在LIBERO中，对每个任务执行50次试验，计算任务完成百分比；在ALOHA中，根据任务评分标准计算平均完成度。\n2. 推理效率：\n - 吞吐量（Hz）：每秒生成的动作数量。\n - 延迟：生成单个动作或动作块所需的时间。",
    "summary_html": "<p>论文研究单位</p>\n<p>斯坦福大学</p>\n\n<p>论文概述</p>\n<p>该论文研究了如何优化视觉-语言-动作模型（VLA）的微调过程，以提升其在机器人任务中的速度和成功率。以OpenVLA为基础模型，系统性地比较了不同动作解码方案、动作表示和学习目标对微调效果的影响。研究发现，并行解码与动作分块结合、连续动作表示和L1回归目标能同时提升推理效率、策略性能和模型的输入输出灵活性。基于此，提出了OpenVLA-OFT方法，在LIBERO仿真基准上取得了97.1%的平均成功率，比基线提升20.6%，同时实现26倍的动作生成加速。在真实的ALOHA双手机器人上，OpenVLA-OFT+成功执行了高频率的灵巧操作任务，平均成功率比其他方法和从头训练的策略高出15%。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>系统性分析了VLA微调的关键设计选择，包括动作解码策略（自回归 vs 并行）、动作表示（离散 vs 连续）和学习目标（下一词预测 vs L1回归 vs 扩散模型）。</li><li>提出了一种优化的微调配方（OpenVLA-OFT），整合了并行解码、动作分块、连续动作表示和L1回归，共同提升推理效率、任务性能和输入输出灵活性。</li><li>在LIBERO基准测试中，OpenVLA-OFT实现了新的最先进性能，平均成功率达97.1%，并将动作生成吞吐量提高了26倍。</li><li>在ALOHA真实机器人上，通过加入FiLM增强语言理解（OFT+），成功执行了高频率双臂任务，平均成功率优于其他VLAs和从零开始训练的模仿学习策略高达15%。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>并行解码与动作分块：将自回归解码改为并行解码，使用双向注意力掩码，使模型在一次前向传播中预测整个动作序列。动作分块通过在输入中添加额外的空动作嵌入实现，使模型一次性预测多个未来时间步的动作。</li><li>连续动作表示：用MLP动作头替换离散标记的softmax输出层，直接预测连续的归一化动作值。</li><li>L1回归目标：最小化预测动作与真实动作之间的L1距离，替代下一词预测或扩散模型。</li><li>FiLM增强：在视觉变换器块中，通过语言嵌入计算缩放和偏移向量，对视觉特征进行仿射变换，以增强模型对语言指令的关注。</li><li>多模态输入处理：支持多视角图像和机器人本体感觉输入，通过共享投影器将视觉特征映射到语言嵌入空间，并与语言标记和状态嵌入连接。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<ol><li>LIBERO仿真基准：包含四个任务套件（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long），每个套件包含10个任务，每个任务500个专家演示，用于评估泛化能力。</li><li>ALOHA真实机器人任务：设计了四个任务（折叠短裤、折叠T恤、用勺子舀取、放置物品），包含20-300个演示，用于测试双手机器人的灵巧操作和语言跟随能力。</li></ol>\n<p>训练资源：</p>\n<ol><li>GPU：使用8块NVIDIA A100或H100（80GB）GPU进行微调。</li><li>训练步数：非扩散方法训练50-150K步，扩散方法训练100-250K步。</li><li>批大小：64-128（LIBERO实验），32（ALOHA实验）。</li><li>动作块大小：LIBERO实验设为8，ALOHA实验设为25。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ol><li>LIBERO仿真环境：使用Franka Emika Panda机械臂，评估任务包括空间布局、物体操作、目标导向和长时程任务。</li><li>ALOHA真实机器人：双手机器人平台，包含两个ViperX 300S机械臂、三个摄像头视角（一个俯视、两个腕部摄像头）和14维关节角状态输入，控制频率为25 Hz。</li></ol>\n<p>评估指标：</p>\n<ol><li>任务成功率：在LIBERO中，对每个任务执行50次试验，计算任务完成百分比；在ALOHA中，根据任务评分标准计算平均完成度。</li><li>推理效率：</li></ol>\n<p> - 吞吐量（Hz）：每秒生成的动作数量。</p>\n<p> - 延迟：生成单个动作或动作块所需的时间。</p>"
  },
  {
    "date": "2025-02-26",
    "title": "ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration",
    "link": "http://arxiv.org/abs/2502.19250",
    "summary_markdown": "论文研究单位\nMidea Group, East China Normal University, Shanghai University\n\n论文概述\n该论文解决了模仿学习在机器人操作中的局限性，即其对大量人类演示数据的依赖以及对新物体的泛化能力不足。为此，研究者提出了ObjectVLA，一个视觉-语言-动作（VLA）模型，旨在实现无需演示的端到端开放世界物体操作。该方法通过共同训练机器人交互数据和图像-文本数据，利用定位元数据作为桥梁，建立物体名称、视觉特征与机器人动作之间的隐式联系，从而使机器人能够零样本地操作训练时未见过的物体。\n\n论文核心贡献点\n1. 提出了一个统一的流水线，通过共同训练机器人交互数据和视觉-语言数据，实现了端到端的物体级泛化。\n2. 利用定位元数据（如边界框）来连接视觉、语言和动作，使模型能够理解并操作全新的物体。\n3. 在真实的机器人平台上验证了方法的有效性，实现了对100个未见物体的泛化，成功率为64%。\n4. 提出了一种低成本、高效的方法：仅需使用智能手机拍摄少量图像并进行短暂的持续学习，即可让模型快速适应新物体，显著降低了对大规模人类演示的需求。\n\n论文方法描述\n该方法的核心是构建一个混合数据集并对VLA模型进行共同训练。\n1. 数据构建：首先，构建一个图像-文本数据集，包含100个机器人训练数据中未出现的物体，每个物体拍摄20张不同姿态的图像。文本指令使用固定模板（如“检测物体的边界框”），答案是对应的边界框坐标。其次，为机器人交互数据构建推理数据。通过DinoX等开放词汇目标检测器识别指令中的目标物体并生成其边界框，然后使用特定模板将物体名称和边界框信息编码为一段推理文本。\n2. 训练策略：采用基于扩散的VLA模型（DiVLA）作为基础模型。将机器人交互数据和构建的图像-文本数据以10:1的比例进行共同训练。推理文本在每次动作执行前被注入到模型中，从而将视觉-语言理解与机器人动作执行关联起来。\n\n论文使用数据集和训练资源\n1. 数据集：机器人交互数据（包含移动、推动、旋转、分拣等任务的远程操作演示数据）和自建的图像-文本数据集（包含100个OOD物体的2000张图像及其边界框描述）。\n2. 训练资源：使用8块NVIDIA A800 GPU进行训练。优化器为Adam，学习率为2e-5，全局批量大小为128，共训练50,000步。\n\n论文使用的评估环境和评估指标\n1. 评估环境：真实的Franka Emika机器人平台，配备7自由度机械臂和夹爪。视觉输入来自两个外部ZED相机和一个腕部Realsense 435i相机。\n2. 评估指标：主要指标是任务成功率。具体而言，对于每个物体进行多次试验（通常为3次或4次），记录成功完成的试验次数或百分比。例如，“移动到物体”任务要求模型在所有四次试验中都正确移向目标物体才算成功。",
    "summary_html": "<p>论文研究单位</p>\n<p>Midea Group, East China Normal University, Shanghai University</p>\n\n<p>论文概述</p>\n<p>该论文解决了模仿学习在机器人操作中的局限性，即其对大量人类演示数据的依赖以及对新物体的泛化能力不足。为此，研究者提出了ObjectVLA，一个视觉-语言-动作（VLA）模型，旨在实现无需演示的端到端开放世界物体操作。该方法通过共同训练机器人交互数据和图像-文本数据，利用定位元数据作为桥梁，建立物体名称、视觉特征与机器人动作之间的隐式联系，从而使机器人能够零样本地操作训练时未见过的物体。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一个统一的流水线，通过共同训练机器人交互数据和视觉-语言数据，实现了端到端的物体级泛化。</li><li>利用定位元数据（如边界框）来连接视觉、语言和动作，使模型能够理解并操作全新的物体。</li><li>在真实的机器人平台上验证了方法的有效性，实现了对100个未见物体的泛化，成功率为64%。</li><li>提出了一种低成本、高效的方法：仅需使用智能手机拍摄少量图像并进行短暂的持续学习，即可让模型快速适应新物体，显著降低了对大规模人类演示的需求。</li></ol>\n\n<p>论文方法描述</p>\n<p>该方法的核心是构建一个混合数据集并对VLA模型进行共同训练。</p>\n<ol><li>数据构建：首先，构建一个图像-文本数据集，包含100个机器人训练数据中未出现的物体，每个物体拍摄20张不同姿态的图像。文本指令使用固定模板（如“检测物体的边界框”），答案是对应的边界框坐标。其次，为机器人交互数据构建推理数据。通过DinoX等开放词汇目标检测器识别指令中的目标物体并生成其边界框，然后使用特定模板将物体名称和边界框信息编码为一段推理文本。</li><li>训练策略：采用基于扩散的VLA模型（DiVLA）作为基础模型。将机器人交互数据和构建的图像-文本数据以10:1的比例进行共同训练。推理文本在每次动作执行前被注入到模型中，从而将视觉-语言理解与机器人动作执行关联起来。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：机器人交互数据（包含移动、推动、旋转、分拣等任务的远程操作演示数据）和自建的图像-文本数据集（包含100个OOD物体的2000张图像及其边界框描述）。</li><li>训练资源：使用8块NVIDIA A800 GPU进行训练。优化器为Adam，学习率为2e-5，全局批量大小为128，共训练50,000步。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：真实的Franka Emika机器人平台，配备7自由度机械臂和夹爪。视觉输入来自两个外部ZED相机和一个腕部Realsense 435i相机。</li><li>评估指标：主要指标是任务成功率。具体而言，对于每个物体进行多次试验（通常为3次或4次），记录成功完成的试验次数或百分比。例如，“移动到物体”任务要求模型在所有四次试验中都正确移向目标物体才算成功。</li></ol>"
  },
  {
    "date": "2025-02-24",
    "title": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design",
    "link": "http://arxiv.org/abs/2502.17034",
    "summary_markdown": "### 论文研究单位\nSkolkovo Institute of Science and Technology, Intelligent Space Robotics Laboratory, Center for Digital Engineering.\n### 论文概述\n该论文提出了一个名为 Evolution 6.0 的概念，这是一个由生成式人工智能驱动的自主机器人系统。当机器人缺乏完成人类请求任务所需的工具时，它可以自主设计所需工具并学习如何使用它们以实现目标。该系统整合了视觉语言模型（VLMs）、视觉语言动作模型（VLA）和文本到3D生成模型，用于工具设计和任务执行，旨在提高机器人在不可预测环境中的适应性和操作灵活性。\n### 论文核心贡献点\n- 提出了 Evolution 6.0 框架，实现机器人通过生成式设计自主进化和创造工具。\n- 构建了一个包含工具生成模块和动作生成模块的系统，实现了从场景理解到工具制造和任务执行的闭环。\n- 集成了 QwenVLM、OpenVLA 和 Llama-Mesh 模型，分别用于环境理解、动作规划和3D工具生成。\n- 通过实验验证了系统的有效性，工具生成成功率达到90%，动作生成在物理和视觉泛化上达到83.5%。\n### 论文方法描述\n系统包含两个核心模块：\n- **工具生成模块**:\n - 通过相机等传感器捕获机器人场景输入。\n - 使用 Qwen2-VL-2B-Instruct 模型分析环境并生成工具设计的文本描述（如“创建一个3D刀具模型”）。\n - 将描述输入到自回归语言模型 Llama-Mesh 中，生成3D网格格式的工具模型。\n - 对网格进行渲染和验证，然后转换为G-Code，通过3D打印机制造出实体工具。\n- **动作生成模块**:\n - 基于一个经过微调的 OpenVLA 模型。\n - 接收第三方视角的相机画面和自然语言任务指令作为输入。\n - 输出一个7维的动作向量（3个平移位移，3个角位移和1个夹爪动作），直接控制机器人机械臂。\n - 系统以5Hz的频率迭代运行，实时处理环境变化并调整动作。\n### 论文使用数据集和训练资源\n- **数据集**:\n - 使用 UR10 机械臂和 Logitech C920e 相机收集。\n - 包含20个数据片段，平均分为两个任务：切蛋糕和抓取并放置蛋糕。\n - 数据格式化为RLDS（强化学习数据集）结构，包含机器人位姿、状态和同步的视觉反馈。\n- **训练资源**:\n - 对 OpenVLA-7b 模型进行微调，采用参数高效的 LoRA 方法，rank为32。\n - 训练设置包括：批量大小为16，学习率为 5e-4，训练4000个梯度步。\n - 在一块 NVIDIA A100 GPU 上进行训练。\n - 推理时，QwenVLM 和 Llama-Mesh 模型被优化为 int8 精度，在一块 NVIDIA RTX 4090 24Gb GPU 上运行。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - 评估分为两个阶段。\n - 第一阶段评估工具生成模块，在10个不同的机器人场景中进行。\n - 第二阶段评估动作生成模块，在一台 UR-10 机器人实物上进行，通过第三方视角观察。\n - 第二阶段包含五种评估场景：与训练数据相同的场景、物理泛化（目标物体尺寸或颜色变化）、运动泛化（物体位置变化）、语义泛化（新的指令）和视觉泛化（场景变化或引入干扰物）。\n- **评估指标**:\n - **工具生成**: 使用成功率和平均推理时间作为指标。\n - **动作生成**: 使用任务成功率作为主要指标，分别在五种场景下进行评估。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Skolkovo Institute of Science and Technology, Intelligent Space Robotics Laboratory, Center for Digital Engineering.</p>\n<h3>论文概述</h3>\n<p>该论文提出了一个名为 Evolution 6.0 的概念，这是一个由生成式人工智能驱动的自主机器人系统。当机器人缺乏完成人类请求任务所需的工具时，它可以自主设计所需工具并学习如何使用它们以实现目标。该系统整合了视觉语言模型（VLMs）、视觉语言动作模型（VLA）和文本到3D生成模型，用于工具设计和任务执行，旨在提高机器人在不可预测环境中的适应性和操作灵活性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了 Evolution 6.0 框架，实现机器人通过生成式设计自主进化和创造工具。</li><li>构建了一个包含工具生成模块和动作生成模块的系统，实现了从场景理解到工具制造和任务执行的闭环。</li><li>集成了 QwenVLM、OpenVLA 和 Llama-Mesh 模型，分别用于环境理解、动作规划和3D工具生成。</li><li>通过实验验证了系统的有效性，工具生成成功率达到90%，动作生成在物理和视觉泛化上达到83.5%。</li></ul>\n<h3>论文方法描述</h3>\n<p>系统包含两个核心模块：</p>\n<ul><li><strong>工具生成模块</strong>:</li></ul>\n<p> - 通过相机等传感器捕获机器人场景输入。</p>\n<p> - 使用 Qwen2-VL-2B-Instruct 模型分析环境并生成工具设计的文本描述（如“创建一个3D刀具模型”）。</p>\n<p> - 将描述输入到自回归语言模型 Llama-Mesh 中，生成3D网格格式的工具模型。</p>\n<p> - 对网格进行渲染和验证，然后转换为G-Code，通过3D打印机制造出实体工具。</p>\n<ul><li><strong>动作生成模块</strong>:</li></ul>\n<p> - 基于一个经过微调的 OpenVLA 模型。</p>\n<p> - 接收第三方视角的相机画面和自然语言任务指令作为输入。</p>\n<p> - 输出一个7维的动作向量（3个平移位移，3个角位移和1个夹爪动作），直接控制机器人机械臂。</p>\n<p> - 系统以5Hz的频率迭代运行，实时处理环境变化并调整动作。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> - 使用 UR10 机械臂和 Logitech C920e 相机收集。</p>\n<p> - 包含20个数据片段，平均分为两个任务：切蛋糕和抓取并放置蛋糕。</p>\n<p> - 数据格式化为RLDS（强化学习数据集）结构，包含机器人位姿、状态和同步的视觉反馈。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - 对 OpenVLA-7b 模型进行微调，采用参数高效的 LoRA 方法，rank为32。</p>\n<p> - 训练设置包括：批量大小为16，学习率为 5e-4，训练4000个梯度步。</p>\n<p> - 在一块 NVIDIA A100 GPU 上进行训练。</p>\n<p> - 推理时，QwenVLM 和 Llama-Mesh 模型被优化为 int8 精度，在一块 NVIDIA RTX 4090 24Gb GPU 上运行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - 评估分为两个阶段。</p>\n<p> - 第一阶段评估工具生成模块，在10个不同的机器人场景中进行。</p>\n<p> - 第二阶段评估动作生成模块，在一台 UR-10 机器人实物上进行，通过第三方视角观察。</p>\n<p> - 第二阶段包含五种评估场景：与训练数据相同的场景、物理泛化（目标物体尺寸或颜色变化）、运动泛化（物体位置变化）、语义泛化（新的指令）和视觉泛化（场景变化或引入干扰物）。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>工具生成</strong>: 使用成功率和平均推理时间作为指标。</p>\n<p> - <strong>动作生成</strong>: 使用任务成功率作为主要指标，分别在五种场景下进行评估。</p>"
  },
  {
    "date": "2025-02-20",
    "title": "Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration",
    "link": "http://arxiv.org/abs/2502.14795",
    "summary_markdown": "论文研究单位\nICML 2025\n作者信息为匿名占位符，具体机构未在文档中提供。\n\n论文概述\n本文档是国际机器学习会议（ICML 2025）的论文提交和格式指南。\n它提供了关于电子提交流程、论文格式、匿名要求、页面限制和最终版本准备的详细说明。\n文档本身作为一个模板，展示了正确的格式规范，包括摘要、章节、图表、算法和参考文献的样式。\n\n论文核心贡献点\n提供了ICML 2025的完整提交流程，强调匿名提交和PDF格式要求。\n明确规定了论文的格式要求，包括页面尺寸、字体类型（10点Times字体）、页边距、图表和表格的排版规则。\n详述了作者信息在初审阶段和最终版本中的不同处理方式，以确保双盲评审的公正性。\n包含了关于参考文献格式、自我引用规范以及附录处理的指导。\n\n论文方法描述\n该文档并非研究性论文，其“方法”在于阐述符合会议要求的标准化论文准备与提交流程。\n提交方法要求作者通过会议网站以单一PDF文件提交论文，该文件需包含正文、参考文献和附录。\n使用LaTeX的作者需通过指定特定命令（如 `\\usepackage[accepted]{icml2025}`）来切换初审版本和最终版本。\nWord用户需手动调整格式以满足最终版本的要求。\n\n论文使用数据集和训练资源\n本文档是格式说明，不涉及任何具体的机器学习数据集或训练过程。\n文档中提到的“Breast”、“Cleveland”等数据集（见表1）仅为展示表格格式的示例。\n所涉及的“资源”主要是文档准备工具，如LaTeX（及其特定的样式文件 `icml2025.sty` 和 bibliography style 文件 `icml2025.bst`）、Microsoft Word，以及用于生成PDF的工具。\n\n论文使用的评估环境和评估指标\n本文档不包含对机器学习模型的评估。\n其“评估标准”是指对提交论文的格式和内容进行合规性检查。\n评估指标包括：文件格式（PDF）、字体类型、页面限制（正文8页）、总文件大小（不超过10MB）以及是否遵守匿名性规定。\n文档中的表格示例（如表1）所展示的“分类准确率”和“Better?”列，旨在说明表格中呈现结果的规范格式，而非实际的研究评估结果。",
    "summary_html": "<p>论文研究单位</p>\n<p>ICML 2025</p>\n<p>作者信息为匿名占位符，具体机构未在文档中提供。</p>\n\n<p>论文概述</p>\n<p>本文档是国际机器学习会议（ICML 2025）的论文提交和格式指南。</p>\n<p>它提供了关于电子提交流程、论文格式、匿名要求、页面限制和最终版本准备的详细说明。</p>\n<p>文档本身作为一个模板，展示了正确的格式规范，包括摘要、章节、图表、算法和参考文献的样式。</p>\n\n<p>论文核心贡献点</p>\n<p>提供了ICML 2025的完整提交流程，强调匿名提交和PDF格式要求。</p>\n<p>明确规定了论文的格式要求，包括页面尺寸、字体类型（10点Times字体）、页边距、图表和表格的排版规则。</p>\n<p>详述了作者信息在初审阶段和最终版本中的不同处理方式，以确保双盲评审的公正性。</p>\n<p>包含了关于参考文献格式、自我引用规范以及附录处理的指导。</p>\n\n<p>论文方法描述</p>\n<p>该文档并非研究性论文，其“方法”在于阐述符合会议要求的标准化论文准备与提交流程。</p>\n<p>提交方法要求作者通过会议网站以单一PDF文件提交论文，该文件需包含正文、参考文献和附录。</p>\n<p>使用LaTeX的作者需通过指定特定命令（如 <code>\\usepackage[accepted]{icml2025}</code>）来切换初审版本和最终版本。</p>\n<p>Word用户需手动调整格式以满足最终版本的要求。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>本文档是格式说明，不涉及任何具体的机器学习数据集或训练过程。</p>\n<p>文档中提到的“Breast”、“Cleveland”等数据集（见表1）仅为展示表格格式的示例。</p>\n<p>所涉及的“资源”主要是文档准备工具，如LaTeX（及其特定的样式文件 <code>icml2025.sty</code> 和 bibliography style 文件 <code>icml2025.bst</code>）、Microsoft Word，以及用于生成PDF的工具。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>本文档不包含对机器学习模型的评估。</p>\n<p>其“评估标准”是指对提交论文的格式和内容进行合规性检查。</p>\n<p>评估指标包括：文件格式（PDF）、字体类型、页面限制（正文8页）、总文件大小（不超过10MB）以及是否遵守匿名性规定。</p>\n<p>文档中的表格示例（如表1）所展示的“分类准确率”和“Better?”列，旨在说明表格中呈现结果的规范格式，而非实际的研究评估结果。</p>"
  },
  {
    "date": "2025-02-20",
    "title": "ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2502.14420",
    "summary_markdown": "论文研究单位\nMidea Group, East China Normal University, Shanghai University, Beijing Innovation Center of Humanoid Robotics, Tsinghua University\n\n论文概述\n论文提出了ChatVLA，一个统一的视觉-语言-动作模型框架，用于同时实现多模态理解和机器人控制。研究指出现有VLA模型存在两个关键挑战：spurious forgetting（伪遗忘），即机器人训练覆盖了关键的视觉-文本对齐；和task interference（任务干扰），即控制和理解任务的参数空间冲突导致性能下降。ChatVLA通过分阶段对齐训练和混合专家架构来解决这些问题，在保持参数效率的同时实现双重能力。\n\n论文核心贡献点\n对现有VLA方法进行深入分析，证明其在多模态理解和机器人控制两方面的局限性\n引入ChatVLA框架，统一了对话能力、多模态理解和机器人控制于单一神经网络\n在多模态理解基准和真实机器人任务上进行广泛实验评估\n在包含25个多样化任务的真实家庭环境中进行机器人实验验证性能\n\n论文方法描述\n采用分阶段对齐训练策略：第一阶段专注于机器人控制数据训练，第二阶段增量整合多模态数据以重新激活视觉-文本对齐\n引入混合专家架构：在MLP层使用MoE设计，使控制和理解任务共享注意力层以促进知识转移，同时隔离任务特定MLP以最小化干扰\n架构基于双重编码理论，共享层处理跨任务知识，分离层处理任务特定知识\n\n论文使用数据集和训练资源\n机器人数据集D_robot包含专家演示轨迹，每个演示由状态-动作对序列组成\n多模态理解数据集D_v-t包含视觉图像和对应问题或标题的图像-文本对\n使用基准包括TextVQA、DocVQA、MMMU、MME、MMStar等13个多模态理解基准\n模型基于2B参数规模，相比ECoT使用3.5倍更少的VLM骨干参数\n\n论文使用的评估环境和评估指标\n真实机器人环境：包含25个任务的多任务设置，涵盖浴室、厨房、桌面等多样化家庭环境技能\n多模态理解基准：包括视觉问答任务和一般理解任务，涵盖数学、OCR等能力\n评估指标：机器人任务成功率、平均成功长度、多模态理解准确率、MMMU得分、MMStar得分等",
    "summary_html": "<p>论文研究单位</p>\n<p>Midea Group, East China Normal University, Shanghai University, Beijing Innovation Center of Humanoid Robotics, Tsinghua University</p>\n\n<p>论文概述</p>\n<p>论文提出了ChatVLA，一个统一的视觉-语言-动作模型框架，用于同时实现多模态理解和机器人控制。研究指出现有VLA模型存在两个关键挑战：spurious forgetting（伪遗忘），即机器人训练覆盖了关键的视觉-文本对齐；和task interference（任务干扰），即控制和理解任务的参数空间冲突导致性能下降。ChatVLA通过分阶段对齐训练和混合专家架构来解决这些问题，在保持参数效率的同时实现双重能力。</p>\n\n<p>论文核心贡献点</p>\n<p>对现有VLA方法进行深入分析，证明其在多模态理解和机器人控制两方面的局限性</p>\n<p>引入ChatVLA框架，统一了对话能力、多模态理解和机器人控制于单一神经网络</p>\n<p>在多模态理解基准和真实机器人任务上进行广泛实验评估</p>\n<p>在包含25个多样化任务的真实家庭环境中进行机器人实验验证性能</p>\n\n<p>论文方法描述</p>\n<p>采用分阶段对齐训练策略：第一阶段专注于机器人控制数据训练，第二阶段增量整合多模态数据以重新激活视觉-文本对齐</p>\n<p>引入混合专家架构：在MLP层使用MoE设计，使控制和理解任务共享注意力层以促进知识转移，同时隔离任务特定MLP以最小化干扰</p>\n<p>架构基于双重编码理论，共享层处理跨任务知识，分离层处理任务特定知识</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>机器人数据集D_robot包含专家演示轨迹，每个演示由状态-动作对序列组成</p>\n<p>多模态理解数据集D_v-t包含视觉图像和对应问题或标题的图像-文本对</p>\n<p>使用基准包括TextVQA、DocVQA、MMMU、MME、MMStar等13个多模态理解基准</p>\n<p>模型基于2B参数规模，相比ECoT使用3.5倍更少的VLM骨干参数</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>真实机器人环境：包含25个任务的多任务设置，涵盖浴室、厨房、桌面等多样化家庭环境技能</p>\n<p>多模态理解基准：包括视觉问答任务和一般理解任务，涵盖数学、OCR等能力</p>\n<p>评估指标：机器人任务成功率、平均成功长度、多模态理解准确率、MMMU得分、MMStar得分等</p>"
  },
  {
    "date": "2025-02-19",
    "title": "VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation",
    "link": "http://arxiv.org/abs/2502.13508",
    "summary_markdown": "论文研究单位\nWestlake University, Zhejiang University, Xi'an Jiaotong University\n\n论文概述\n该论文提出了VLAS，一个用于定制化机器人操作的新型端到端视觉-语言-动作模型。现有的视觉-语言-动作模型（VLA）主要依赖文本指令，忽略了更自然的语音交互方式，而传统的语音集成方法需要独立的自动语音识别（ASR）系统，这会增加系统复杂性并引入误差传播。VLAS通过在机器人策略模型内部直接整合语音识别来解决这些问题，使机器人能够通过内部的语音-文本对齐来理解口语命令并生成相应动作。此外，论文设计了语音检索增强生成（Voice RAG）范式，以有效处理需要特定个体知识的任务。实验表明，VLAS能够通过多样化的语音命令有效完成机器人操作任务，提供无缝且个性化的交互体验。\n\n论文核心贡献点\n1. 提出了VLAS，这是第一个为机器人操作集成了语音指令的视觉-语言-动作模型，无需外部语音识别系统，实现了更自然的机器人通信。\n2. 设计了Voice RAG范式，使VLAS能够有效处理需要特定个体知识的定制化任务。\n3. 除了机器人策略模型，还引入了VLAS-Base，它扩展了广泛使用的视觉-语言模型LLaVA以接受语音指令。该模型对其他涉及语音输入的下游任务也具有价值。同时，为社区研究提供了两个新数据集SQA和CSI。\n\n论文方法描述\nVLAS模型构建在LLaVA模型之上，并采用三阶段训练范式。\n1. **架构**：模型包含视觉编码器（CLIP ViT）、语音编码器和一个LLaMA LLM主干。视觉观察和语音指令被各自的编码器处理后，通过多层感知机（MLP）投影到统一的语言空间。语音检索增强生成（Voice RAG）模块通过说话人识别提取声纹，从外部数据库检索个性化知识，并将其作为背景知识整合到模型输入中。所有嵌入拼接后输入LLM，以自回归方式预测离散化的动作标记，然后通过反标记化器转换为连续动作值。\n2. **数据收集**：为了支持语音指令调优，论文构建了两个新数据集。SQA数据集通过TTS工具将LLaVA的视觉问答对中的文本问题转换为语音，包含185K个样本。CSI数据集通过TTS将CALVIN机器人操作数据集中的文本指令转换为语音，包含约194K个音频样本。\n3. **训练范式**：\n - **阶段一：语音对齐**。在LibriSpeech-360语音识别数据集上微调语音编码器与LLM之间的MLP，实现语音与文本的粗粒度模态对齐。\n - **阶段二：语音问答微调**。在SQA数据集、原始VQA数据集和LibriSpeech-100数据集上微调除预训练编码器外的所有网络组件，得到能够处理多模态输入的VLAS-Base基础模型。\n - **阶段三：机器人操作微调**。在CSI机器人操作数据集上通过行为克隆对VLAS-Base进行进一步微调，使其能够根据图像、文本或语音指令生成机器人操作轨迹。\n\n论文使用数据集和训练资源\n1. **数据集**：\n - **SQA**：论文构建的语音问答数据集，包含185K个样本，涵盖1152种不同声音。\n - **CSI**：论文构建的带语音指令的CALVIN数据集，包含389个文本指令及其对应的约194K个语音样本。\n - **CALVIN**：用于评估机器人操作的基准数据集。\n - **LibriSpeech**：用于语音对齐和微调的语音识别数据集（LibriSpeech-360和LibriSpeech-100）。\n - **LLaVA VQA**：用于多模态训练的原始视觉问答数据集。\n - **Berkeley UR5 demonstration dataset** 和 **cup-picking dataset**：用于在真实世界UR5机械臂上进行微调。\n2. **训练资源**：\n - 模型基于预训练的LLaVA（其LLM主干为Vicuna，一个LLaMA的微调版本）。\n - 语音编码器使用预训练的Whisper模型。\n - 语音合成使用ESPnet工具和预训练的VITS TTS模型。\n - 对比模型包括VLA、Roboflamingo、OpenVLA等，并使用Whisper large-v2作为ASR基线。\n\n论文使用的评估环境和评估指标\n1. **评估环境**：\n - **模拟环境**：CALVIN基准，这是一个包含1000个长期任务（每个任务由5个连续子任务组成）的模拟环境。\n - **真实世界环境**：配备UR5机械臂的物理实验平台。\n2. **评估指标**：\n - **CALVIN基准**：长期成功率（LH-1到LH-5），表示成功完成1到5个子任务的任务百分比，以及平均任务序列长度。\n - **语音识别**：在LibriSpeech测试集上使用词错误率（WER）。\n - **定制化任务基准**：论文构建的新基准上的成功率，包括所有权任务、用户偏好任务、复合任务及其多阶段变体。\n - **多模态理解**：在VQA-v2、VizWiz、SQA-I、VQA-T、POPE、GQA等视觉问答基准上的准确率。在自建的语音问答基准（SGQA）上使用准确率。\n - **语音问答**：在自建的SGQA基准上使用准确率。",
    "summary_html": "<p>论文研究单位</p>\n<p>Westlake University, Zhejiang University, Xi'an Jiaotong University</p>\n\n<p>论文概述</p>\n<p>该论文提出了VLAS，一个用于定制化机器人操作的新型端到端视觉-语言-动作模型。现有的视觉-语言-动作模型（VLA）主要依赖文本指令，忽略了更自然的语音交互方式，而传统的语音集成方法需要独立的自动语音识别（ASR）系统，这会增加系统复杂性并引入误差传播。VLAS通过在机器人策略模型内部直接整合语音识别来解决这些问题，使机器人能够通过内部的语音-文本对齐来理解口语命令并生成相应动作。此外，论文设计了语音检索增强生成（Voice RAG）范式，以有效处理需要特定个体知识的任务。实验表明，VLAS能够通过多样化的语音命令有效完成机器人操作任务，提供无缝且个性化的交互体验。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了VLAS，这是第一个为机器人操作集成了语音指令的视觉-语言-动作模型，无需外部语音识别系统，实现了更自然的机器人通信。</li><li>设计了Voice RAG范式，使VLAS能够有效处理需要特定个体知识的定制化任务。</li><li>除了机器人策略模型，还引入了VLAS-Base，它扩展了广泛使用的视觉-语言模型LLaVA以接受语音指令。该模型对其他涉及语音输入的下游任务也具有价值。同时，为社区研究提供了两个新数据集SQA和CSI。</li></ol>\n\n<p>论文方法描述</p>\n<p>VLAS模型构建在LLaVA模型之上，并采用三阶段训练范式。</p>\n<ol><li><strong>架构</strong>：模型包含视觉编码器（CLIP ViT）、语音编码器和一个LLaMA LLM主干。视觉观察和语音指令被各自的编码器处理后，通过多层感知机（MLP）投影到统一的语言空间。语音检索增强生成（Voice RAG）模块通过说话人识别提取声纹，从外部数据库检索个性化知识，并将其作为背景知识整合到模型输入中。所有嵌入拼接后输入LLM，以自回归方式预测离散化的动作标记，然后通过反标记化器转换为连续动作值。</li><li><strong>数据收集</strong>：为了支持语音指令调优，论文构建了两个新数据集。SQA数据集通过TTS工具将LLaVA的视觉问答对中的文本问题转换为语音，包含185K个样本。CSI数据集通过TTS将CALVIN机器人操作数据集中的文本指令转换为语音，包含约194K个音频样本。</li><li><strong>训练范式</strong>：</li></ol>\n<p> - <strong>阶段一：语音对齐</strong>。在LibriSpeech-360语音识别数据集上微调语音编码器与LLM之间的MLP，实现语音与文本的粗粒度模态对齐。</p>\n<p> - <strong>阶段二：语音问答微调</strong>。在SQA数据集、原始VQA数据集和LibriSpeech-100数据集上微调除预训练编码器外的所有网络组件，得到能够处理多模态输入的VLAS-Base基础模型。</p>\n<p> - <strong>阶段三：机器人操作微调</strong>。在CSI机器人操作数据集上通过行为克隆对VLAS-Base进行进一步微调，使其能够根据图像、文本或语音指令生成机器人操作轨迹。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>1. <strong>数据集</strong>：</p>\n<p> - <strong>SQA</strong>：论文构建的语音问答数据集，包含185K个样本，涵盖1152种不同声音。</p>\n<p> - <strong>CSI</strong>：论文构建的带语音指令的CALVIN数据集，包含389个文本指令及其对应的约194K个语音样本。</p>\n<p> - <strong>CALVIN</strong>：用于评估机器人操作的基准数据集。</p>\n<p> - <strong>LibriSpeech</strong>：用于语音对齐和微调的语音识别数据集（LibriSpeech-360和LibriSpeech-100）。</p>\n<p> - <strong>LLaVA VQA</strong>：用于多模态训练的原始视觉问答数据集。</p>\n<p> - <strong>Berkeley UR5 demonstration dataset</strong> 和 <strong>cup-picking dataset</strong>：用于在真实世界UR5机械臂上进行微调。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - 模型基于预训练的LLaVA（其LLM主干为Vicuna，一个LLaMA的微调版本）。</p>\n<p> - 语音编码器使用预训练的Whisper模型。</p>\n<p> - 语音合成使用ESPnet工具和预训练的VITS TTS模型。</p>\n<p> - 对比模型包括VLA、Roboflamingo、OpenVLA等，并使用Whisper large-v2作为ASR基线。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - <strong>模拟环境</strong>：CALVIN基准，这是一个包含1000个长期任务（每个任务由5个连续子任务组成）的模拟环境。</p>\n<p> - <strong>真实世界环境</strong>：配备UR5机械臂的物理实验平台。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - <strong>CALVIN基准</strong>：长期成功率（LH-1到LH-5），表示成功完成1到5个子任务的任务百分比，以及平均任务序列长度。</p>\n<p> - <strong>语音识别</strong>：在LibriSpeech测试集上使用词错误率（WER）。</p>\n<p> - <strong>定制化任务基准</strong>：论文构建的新基准上的成功率，包括所有权任务、用户偏好任务、复合任务及其多阶段变体。</p>\n<p> - <strong>多模态理解</strong>：在VQA-v2、VizWiz、SQA-I、VQA-T、POPE、GQA等视觉问答基准上的准确率。在自建的语音问答基准（SGQA）上使用准确率。</p>\n<p> - <strong>语音问答</strong>：在自建的SGQA基准上使用准确率。</p>"
  },
  {
    "date": "2025-02-14",
    "title": "Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation",
    "link": "http://arxiv.org/abs/2502.10040",
    "summary_markdown": "论文研究单位\n北京航空航天大学, 瑞典皇家理工学院, 北京人形机器人创新中心\n\n论文概述\n本文针对长时程机器人操作任务中模仿学习面临的复合误差和泛化能力差的问题，提出了一个名为扩散轨迹引导策略（DTP）的框架。该框架通过一个扩散模型，根据视觉和语言指令生成2D轨迹，并利用这些轨迹引导策略学习，以减少长序列任务中的误差累积。DTP是一个两阶段方法：首先训练一个扩散轨迹模型来生成任务相关的轨迹，然后在第二阶段利用这些轨迹来优化模仿策略。在CALVIN基准测试中，该方法在未使用外部预训练的情况下，成功率比最先进的基线高出25%，并在真实机器人实验中显著提升了性能。\n\n论文核心贡献点\n1. 提出了扩散轨迹引导策略（DTP），一个新颖的模仿学习框架，利用扩散轨迹模型引导长时程机器人操作任务的策略学习。\n2. 利用机器人视频数据预训练了一个生成式的视觉-语言扩散模型，通过充分利用可用的机器人数据来增强模仿策略的训练效率。该方法还可以与大规模预训练方法结合，作为一个简单有效的插件来提升性能。\n3. 在模拟和真实世界环境中进行了广泛的实验，以评估DTP在不同设置下的性能。\n\n论文方法描述\n该方法是一个两阶段框架：\n第一阶段：训练一个扩散轨迹模型（DTM）。该模型接收初始观察图像和语言指令作为输入，输出任务完成所需的未来末端执行器在2D图像空间的粒子轨迹。轨迹数据是通过将机器人末端执行器的3D世界坐标根据相机内外参投影到2D图像平面得到的。模型采用去噪扩散概率模型（DDPM），通过迭代去噪过程生成轨迹，训练目标是最小化噪声预测的均方误差（MSE）。\n第二阶段：训练扩散轨迹引导策略（DTP）。该策略以一个因果Transformer为骨干网络，将语言指令、历史观察序列、机器人状态以及第一阶段生成的扩散轨迹作为输入，输出机器人动作。为了降低计算开销，设计了一个扩散轨迹重采样器模块来压缩轨迹序列的token数量。最终的损失函数是轨迹预测损失、动作预测损失和视频预测损失的总和。\n\n论文使用数据集和训练资源\n数据集：\n- CALVIN基准数据集：一个用于评估长时程语言条件操作任务的模拟环境，包含4个相似但不同的环境（A、B、C、D）和34个基本操作任务。\n- 真实世界数据集：通过一个Franka Emika Panda机器人远程操作收集了1,512个演示数据，涵盖5个独立任务和一个由3个子任务组成的长时程任务。\n训练资源：\n- 模拟环境训练：使用8块NVIDIA RTX 3090 GPU，在CALVIN数据集上训练耗时约1.5天至5天不等。\n- 真实机器人训练：使用4块NVIDIA RTX 3090 GPU，训练耗时约1天。\n\n论文使用的评估环境和评估指标\n评估环境：\n- 模拟环境：CALVIN基准测试，分为在训练/测试环境相同（D→D）和在训练环境A、B、C上训练，在未见过的环境D上测试（ABC→D）两种设置。\n- 真实机器人环境：一个配备Robotiq夹爪和三个Intel RealSense D435i相机的Franka Emika Panda机器人。\n评估指标：\n- 任务成功率：评估机器人连续完成1到5个指令任务的成功率。\n- 平均序列长度：在由5个任务组成的序列中，平均能成功完成的任务数量，用于衡量模型的长期任务执行能力。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京航空航天大学, 瑞典皇家理工学院, 北京人形机器人创新中心</p>\n\n<p>论文概述</p>\n<p>本文针对长时程机器人操作任务中模仿学习面临的复合误差和泛化能力差的问题，提出了一个名为扩散轨迹引导策略（DTP）的框架。该框架通过一个扩散模型，根据视觉和语言指令生成2D轨迹，并利用这些轨迹引导策略学习，以减少长序列任务中的误差累积。DTP是一个两阶段方法：首先训练一个扩散轨迹模型来生成任务相关的轨迹，然后在第二阶段利用这些轨迹来优化模仿策略。在CALVIN基准测试中，该方法在未使用外部预训练的情况下，成功率比最先进的基线高出25%，并在真实机器人实验中显著提升了性能。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了扩散轨迹引导策略（DTP），一个新颖的模仿学习框架，利用扩散轨迹模型引导长时程机器人操作任务的策略学习。</li><li>利用机器人视频数据预训练了一个生成式的视觉-语言扩散模型，通过充分利用可用的机器人数据来增强模仿策略的训练效率。该方法还可以与大规模预训练方法结合，作为一个简单有效的插件来提升性能。</li><li>在模拟和真实世界环境中进行了广泛的实验，以评估DTP在不同设置下的性能。</li></ol>\n\n<p>论文方法描述</p>\n<p>该方法是一个两阶段框架：</p>\n<p>第一阶段：训练一个扩散轨迹模型（DTM）。该模型接收初始观察图像和语言指令作为输入，输出任务完成所需的未来末端执行器在2D图像空间的粒子轨迹。轨迹数据是通过将机器人末端执行器的3D世界坐标根据相机内外参投影到2D图像平面得到的。模型采用去噪扩散概率模型（DDPM），通过迭代去噪过程生成轨迹，训练目标是最小化噪声预测的均方误差（MSE）。</p>\n<p>第二阶段：训练扩散轨迹引导策略（DTP）。该策略以一个因果Transformer为骨干网络，将语言指令、历史观察序列、机器人状态以及第一阶段生成的扩散轨迹作为输入，输出机器人动作。为了降低计算开销，设计了一个扩散轨迹重采样器模块来压缩轨迹序列的token数量。最终的损失函数是轨迹预测损失、动作预测损失和视频预测损失的总和。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<ul><li>CALVIN基准数据集：一个用于评估长时程语言条件操作任务的模拟环境，包含4个相似但不同的环境（A、B、C、D）和34个基本操作任务。</li><li>真实世界数据集：通过一个Franka Emika Panda机器人远程操作收集了1,512个演示数据，涵盖5个独立任务和一个由3个子任务组成的长时程任务。</li></ul>\n<p>训练资源：</p>\n<ul><li>模拟环境训练：使用8块NVIDIA RTX 3090 GPU，在CALVIN数据集上训练耗时约1.5天至5天不等。</li><li>真实机器人训练：使用4块NVIDIA RTX 3090 GPU，训练耗时约1天。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ul><li>模拟环境：CALVIN基准测试，分为在训练/测试环境相同（D→D）和在训练环境A、B、C上训练，在未见过的环境D上测试（ABC→D）两种设置。</li><li>真实机器人环境：一个配备Robotiq夹爪和三个Intel RealSense D435i相机的Franka Emika Panda机器人。</li></ul>\n<p>评估指标：</p>\n<ul><li>任务成功率：评估机器人连续完成1到5个指令任务的成功率。</li><li>平均序列长度：在由5个任务组成的序列中，平均能成功完成的任务数量，用于衡量模型的长期任务执行能力。</li></ul>"
  },
  {
    "date": "2025-02-13",
    "title": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation",
    "link": "http://arxiv.org/abs/2502.09268",
    "summary_markdown": "论文研究单位\nZhejiang University, Westlake University\n\n论文概述\n该论文提出了GEVRM（Goal-Expressive Video Generation Model for Robust Visual Manipulation），一个旨在增强视觉-语言-动作（VLA）模型在部署期间面对不可避免的外部扰动时的鲁棒性的新颖框架。该方法集成了经典的内部模型控制（IMC）原理，通过文本引导的视频生成模型生成具有高度表现力的未来视觉规划目标，并利用原型对比学习来对齐状态表示。这种设计使模型能够隐式地推断和区分来自外部环境的扰动，从而在标准与受扰动的CALVIN基准以及真实机器人任务中实现稳健的性能。\n\n论文核心贡献点\n- 引入了GEVRM，一个新颖的闭环VLA模型，它将IMC原理整合到机器人视觉运动控制中，以增强对干扰的鲁棒性。\n- 研究了如何利用文本引导的视频生成模型获得高度表现力的目标，并通过原型对比学习对齐状态表示来抵抗部署时的外部扰动。\n- 通过广泛的实验验证了GEVRM的有效性，表明它在标准与受扰动的CALVIN基准上显著优于以前的最先进方法，并且在真实世界机器人操作中，所生成目标状态的表现力相较于基线方法有显著改善。\n\n论文方法描述\n方法分为三个核心部分：\n1. 机器人行为规划器：采用基于DiT（Diffusion Transformer）的文本引导视频扩散模型。模型使用2D VAE进行空间压缩（8倍下采样），使用3D VAE进行时间压缩（4倍下采样），并实施一个随机掩蔽机制（75%权重分配给初始帧），通过Rectified Flow训练，以提升对物理世界规律的理解。\n2. 状态对齐以模拟响应：使用基于ResNet-34的视觉编码器将目标状态和当前状态映射到潜在空间，并通过原型对比学习进行对齐。该方法将来自同一轨迹的状态视为正样本对，不同轨迹的状态视为负样本对，优化一个温度参数δ以分配概率，从而在潜在空间中模拟机器人对扰动的响应。\n3. 目标引导的动作预测：设计一个以生成的高表现力目标为条件的扩散策略，通过逆动力学和对比学习目标进行联合优化，将目标与当前内部编码信号解码为机器人可执行的鲁棒动作。\n\n论文使用数据集和训练资源\n- 机器人行为规划器使用大规模的文本-视频对进行训练，数据来源于带语言标签的网络视频片段和带文本注释的机器人序列决策数据。\n- 机器人动作预测模型使用少量的玩耍数据（视频-动作序列对）进行训练，该数据不依赖语言标签。\n- 论文中未明确提及具体的训练资源，如GPU数量或训练时长。\n\n论文使用的评估环境和评估指标\n- 评估环境：CALVIN基准（包含标准条件和引入外部扰动如光照变化和视频流噪声的场景）以及真实世界的机器人操作任务。\n- 评估指标：对于目标生成，评估其表现力（expressiveness）；对于动作执行，评估在标准与扰动基准上的任务成功率，并与先前的最先进方法进行性能比较。",
    "summary_html": "<p>论文研究单位</p>\n<p>Zhejiang University, Westlake University</p>\n\n<p>论文概述</p>\n<p>该论文提出了GEVRM（Goal-Expressive Video Generation Model for Robust Visual Manipulation），一个旨在增强视觉-语言-动作（VLA）模型在部署期间面对不可避免的外部扰动时的鲁棒性的新颖框架。该方法集成了经典的内部模型控制（IMC）原理，通过文本引导的视频生成模型生成具有高度表现力的未来视觉规划目标，并利用原型对比学习来对齐状态表示。这种设计使模型能够隐式地推断和区分来自外部环境的扰动，从而在标准与受扰动的CALVIN基准以及真实机器人任务中实现稳健的性能。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>引入了GEVRM，一个新颖的闭环VLA模型，它将IMC原理整合到机器人视觉运动控制中，以增强对干扰的鲁棒性。</li><li>研究了如何利用文本引导的视频生成模型获得高度表现力的目标，并通过原型对比学习对齐状态表示来抵抗部署时的外部扰动。</li><li>通过广泛的实验验证了GEVRM的有效性，表明它在标准与受扰动的CALVIN基准上显著优于以前的最先进方法，并且在真实世界机器人操作中，所生成目标状态的表现力相较于基线方法有显著改善。</li></ul>\n\n<p>论文方法描述</p>\n<p>方法分为三个核心部分：</p>\n<ol><li>机器人行为规划器：采用基于DiT（Diffusion Transformer）的文本引导视频扩散模型。模型使用2D VAE进行空间压缩（8倍下采样），使用3D VAE进行时间压缩（4倍下采样），并实施一个随机掩蔽机制（75%权重分配给初始帧），通过Rectified Flow训练，以提升对物理世界规律的理解。</li><li>状态对齐以模拟响应：使用基于ResNet-34的视觉编码器将目标状态和当前状态映射到潜在空间，并通过原型对比学习进行对齐。该方法将来自同一轨迹的状态视为正样本对，不同轨迹的状态视为负样本对，优化一个温度参数δ以分配概率，从而在潜在空间中模拟机器人对扰动的响应。</li><li>目标引导的动作预测：设计一个以生成的高表现力目标为条件的扩散策略，通过逆动力学和对比学习目标进行联合优化，将目标与当前内部编码信号解码为机器人可执行的鲁棒动作。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>机器人行为规划器使用大规模的文本-视频对进行训练，数据来源于带语言标签的网络视频片段和带文本注释的机器人序列决策数据。</li><li>机器人动作预测模型使用少量的玩耍数据（视频-动作序列对）进行训练，该数据不依赖语言标签。</li><li>论文中未明确提及具体的训练资源，如GPU数量或训练时长。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：CALVIN基准（包含标准条件和引入外部扰动如光照变化和视频流噪声的场景）以及真实世界的机器人操作任务。</li><li>评估指标：对于目标生成，评估其表现力（expressiveness）；对于动作执行，评估在标准与扰动基准上的任务成功率，并与先前的最先进方法进行性能比较。</li></ul>"
  },
  {
    "date": "2025-02-07",
    "title": "Survey on Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2502.06851",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2025-02-09",
    "title": "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control",
    "link": "http://arxiv.org/abs/2502.05855",
    "summary_markdown": "论文研究单位\n* Midea Group\n* East China Normal University\n* Shanghai University\n\n论文概述\n* 论文提出了DexVLA，一种新颖的视觉-语言-动作（VLA）模型框架，旨在增强模型在跨多种机器人具身上执行复杂、长视野任务的效率和泛化能力。该框架通过一个十亿参数级别的扩散专家和一个三阶段的具身课程学习策略来解决当前VLA模型在动作表示和训练效率上的瓶颈。\n\n论文核心贡献点\n* **十亿参数扩散专家**：提出了一个基于扩散模型的新颖动作专家，模型规模达到十亿参数。该专家采用多头架构，每个头对应一个特定的机器人具身，以实现有效的跨具身学习，显著提升了模型学习复杂运动技能的能力。\n* **具身课程学习**：设计了一个三阶段的训练策略，使模型能够逐步学习更难的任务。\n 1. **跨具身预训练**：仅使用跨具身数据预训练扩散专家，学习与具身无关的低级运动技能。\n 2. **具身特定对齐**：将抽象的视觉-语言表示与特定机器人的物理约束对齐。\n 3. **任务特定适应**：使模型能够快速适应新任务，掌握复杂的长视野任务。\n* **子步骤推理**：通过训练模型生成长视野任务的子步骤语言描述，使其能够内部进行高级规划，无需外部高层策略模型（如SayCan）即可完成复杂任务。\n\n论文方法描述\n* **模型架构**：\n * 基于Transformer语言模型骨干，使用Qwen2-VL作为基础VLM。\n * VLM生成推理标记和动作标记。\n * 动作标记通过一个投影模块（两个线性层和LayerNorm）传递，以将其嵌入空间与扩散专家的输入对齐。\n * 推理标记通过FiLM层注入到扩散专家中，用于引导动作生成。\n * 扩散专家是一个参数量为1B的基于Transformer的扩散模型，采用多头输出结构以处理不同具身的数据。\n* **训练策略**：\n * 训练损失是扩散损失和下一个标记预测损失的加权和。\n * 训练过程分为三个阶段：\n 1. **第一阶段**：仅使用跨具身数据预训练扩散专家，此时不涉及VLM。使用ResNet-50作为图像编码器，DistilBERT作为语言嵌入模型。\n 2. **第二阶段**：使用具身特定数据，联合训练VLM、投影层和扩散专家（冻结VLM的视觉编码器），对齐具身特定的动作空间、视觉和语言。\n 3. **第三阶段**：使用高质量的任务特定演示数据对模型进行后训练，使其掌握复杂任务。在此阶段，模型学习生成子步骤推理。\n * 在长视野任务的训练数据中，每五秒标注一次子步骤指令。\n\n论文使用数据集和训练资源\n* **数据集**：\n * 包含约100小时的机器人演示数据，涵盖91个不同的任务。\n * 数据来源于多种机器人平台，包括Agilex（42.7%）、单臂Franka（34.7%）等，实现了跨具身学习。\n* **训练资源**：\n * 模型可以在单个Nvidia A6000 GPU上以60Hz的频率运行。\n * 训练优化器为AdamW（β₁=0.9, β₂=0.95）。\n * 三个训练阶段的训练轮数均为5个epoch。\n * 第一阶段仅训练扩散专家的速度比训练整个VLA模型快2.78倍。\n\n论文使用的评估环境和评估指标\n* **评估环境**：\n * 在四种不同的真实世界机器人配置上进行了评估：\n 1. **Franka with gripper**：7自由度，配备Robotiq夹爪。\n 2. **Franka with dexterous hand**：配备Inspire灵巧手，共12个自由度。\n 3. **Bimanual UR5e**：双UR5e机器人，共14个自由度。\n 4. **Bimanual AgileX**：双Agilex臂，移动平台，共14个自由度。\n * 每种配置都包含多个摄像头视角（手腕相机、顶部相机等）。\n * 也在LIBERO模拟基准上进行了评估。\n* **评估指标**：\n * **主要指标**：对于每个任务，执行10次试验，并报告平均分数。分数根据任务完成度进行归一化处理。\n * **详细评分标准**：针对不同任务有明确的评分细则。例如，洗衣折叠（满分4分）、衬衫折叠（满分3分）、收拾桌子（每正确处理一个物品得1分）等。\n * **比较基线**：与OpenVLA、Octo、Diffusion Policy、π₀等最先进的模型进行了性能比较。",
    "summary_html": "<p>论文研究单位</p>\n<p>* Midea Group</p>\n<p>* East China Normal University</p>\n<p>* Shanghai University</p>\n\n<p>论文概述</p>\n<p>* 论文提出了DexVLA，一种新颖的视觉-语言-动作（VLA）模型框架，旨在增强模型在跨多种机器人具身上执行复杂、长视野任务的效率和泛化能力。该框架通过一个十亿参数级别的扩散专家和一个三阶段的具身课程学习策略来解决当前VLA模型在动作表示和训练效率上的瓶颈。</p>\n\n<p>论文核心贡献点</p>\n<p>* <strong>十亿参数扩散专家</strong>：提出了一个基于扩散模型的新颖动作专家，模型规模达到十亿参数。该专家采用多头架构，每个头对应一个特定的机器人具身，以实现有效的跨具身学习，显著提升了模型学习复杂运动技能的能力。</p>\n<p>* <strong>具身课程学习</strong>：设计了一个三阶段的训练策略，使模型能够逐步学习更难的任务。</p>\n<p> 1. <strong>跨具身预训练</strong>：仅使用跨具身数据预训练扩散专家，学习与具身无关的低级运动技能。</p>\n<p> 2. <strong>具身特定对齐</strong>：将抽象的视觉-语言表示与特定机器人的物理约束对齐。</p>\n<p> 3. <strong>任务特定适应</strong>：使模型能够快速适应新任务，掌握复杂的长视野任务。</p>\n<p>* <strong>子步骤推理</strong>：通过训练模型生成长视野任务的子步骤语言描述，使其能够内部进行高级规划，无需外部高层策略模型（如SayCan）即可完成复杂任务。</p>\n\n<p>论文方法描述</p>\n<p>* <strong>模型架构</strong>：</p>\n<p> * 基于Transformer语言模型骨干，使用Qwen2-VL作为基础VLM。</p>\n<p> * VLM生成推理标记和动作标记。</p>\n<p> * 动作标记通过一个投影模块（两个线性层和LayerNorm）传递，以将其嵌入空间与扩散专家的输入对齐。</p>\n<p> * 推理标记通过FiLM层注入到扩散专家中，用于引导动作生成。</p>\n<p> * 扩散专家是一个参数量为1B的基于Transformer的扩散模型，采用多头输出结构以处理不同具身的数据。</p>\n<p>* <strong>训练策略</strong>：</p>\n<p> * 训练损失是扩散损失和下一个标记预测损失的加权和。</p>\n<p> * 训练过程分为三个阶段：</p>\n<p> 1. <strong>第一阶段</strong>：仅使用跨具身数据预训练扩散专家，此时不涉及VLM。使用ResNet-50作为图像编码器，DistilBERT作为语言嵌入模型。</p>\n<p> 2. <strong>第二阶段</strong>：使用具身特定数据，联合训练VLM、投影层和扩散专家（冻结VLM的视觉编码器），对齐具身特定的动作空间、视觉和语言。</p>\n<p> 3. <strong>第三阶段</strong>：使用高质量的任务特定演示数据对模型进行后训练，使其掌握复杂任务。在此阶段，模型学习生成子步骤推理。</p>\n<p> * 在长视野任务的训练数据中，每五秒标注一次子步骤指令。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>* <strong>数据集</strong>：</p>\n<p> * 包含约100小时的机器人演示数据，涵盖91个不同的任务。</p>\n<p> * 数据来源于多种机器人平台，包括Agilex（42.7%）、单臂Franka（34.7%）等，实现了跨具身学习。</p>\n<p>* <strong>训练资源</strong>：</p>\n<p> * 模型可以在单个Nvidia A6000 GPU上以60Hz的频率运行。</p>\n<p> * 训练优化器为AdamW（β₁=0.9, β₂=0.95）。</p>\n<p> * 三个训练阶段的训练轮数均为5个epoch。</p>\n<p> * 第一阶段仅训练扩散专家的速度比训练整个VLA模型快2.78倍。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>* <strong>评估环境</strong>：</p>\n<p> * 在四种不同的真实世界机器人配置上进行了评估：</p>\n<p> 1. <strong>Franka with gripper</strong>：7自由度，配备Robotiq夹爪。</p>\n<p> 2. <strong>Franka with dexterous hand</strong>：配备Inspire灵巧手，共12个自由度。</p>\n<p> 3. <strong>Bimanual UR5e</strong>：双UR5e机器人，共14个自由度。</p>\n<p> 4. <strong>Bimanual AgileX</strong>：双Agilex臂，移动平台，共14个自由度。</p>\n<p> * 每种配置都包含多个摄像头视角（手腕相机、顶部相机等）。</p>\n<p> * 也在LIBERO模拟基准上进行了评估。</p>\n<p>* <strong>评估指标</strong>：</p>\n<p> * <strong>主要指标</strong>：对于每个任务，执行10次试验，并报告平均分数。分数根据任务完成度进行归一化处理。</p>\n<p> * <strong>详细评分标准</strong>：针对不同任务有明确的评分细则。例如，洗衣折叠（满分4分）、衬衫折叠（满分3分）、收拾桌子（每正确处理一个物品得1分）等。</p>\n<p> * <strong>比较基线</strong>：与OpenVLA、Octo、Diffusion Policy、π₀等最先进的模型进行了性能比较。</p>"
  },
  {
    "date": "2025-02-08",
    "title": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation",
    "link": "http://arxiv.org/abs/2502.05485",
    "summary_markdown": "# 论文总结\n## 论文研究单位\nNVIDIA, University of Washington, University of Southern California\n## 论文概述\n本文提出了一种名为HAMSTER（Hierarchical Action Models with Separated TEpRpresentations）的分层视觉-语言-动作（VLA）模型，用于开放世界机器人操作。该模型通过高层视觉语言模型（VLM）预测粗粒度的2D路径作为中间表示，然后由低层3D感知策略根据路径生成精确动作，以提升泛化能力和操作鲁棒性。实验表明，该方法在七个泛化维度上平均成功率比OpenVLA提升20%，相当于相对增益50%。\n## 论文核心贡献点\n1. 提出分层VLA架构HAMSTER，将VLM的2D路径预测与3D低层策略解耦，降低对昂贵机器人数据的依赖。\n2. 证明高层VLM可利用离域数据（如无动作视频、仿真数据）进行微调，实现跨域泛化。\n3. 低层策略结合3D感知与路径引导，专注于局部几何控制，显著提升视觉和语义泛化能力。\n## 论文方法描述\n1. **高层VLM设计**：使用VILA-1.5-13B作为基础模型，输入RGB图像和语言指令，输出机器人末端执行器的2D路径序列（包含位置和夹爪状态）。\n2. **离域数据微调**：构建多源数据集，包括770k像素点预测任务（RoboPoint）、320k仿真轨迹（RLBench）和110k真实机器人轨迹（Bridge/DROID），通过监督损失最大化路径预测的对数似然。\n3. **路径简化**：采用Ramer-Douglas-Peucker算法压缩长路径点，保留关键点。\n4. **低层策略设计**：采用RVT-2或3D-DA架构，将2D路径以彩色轨迹叠加在输入图像上，作为额外条件通道，策略学习基于路径生成动作。\n5. **分层执行**：VLM以低频率生成路径，低层策略以高频率执行动作，支持异步操作。\n## 论文使用数据集和训练资源\n- **离域数据**：RoboPoint（770k样本）、RLBench仿真（320k样本）、Bridge/DROID真实数据（110k样本）。\n- **同域数据**：真实机器人收集的小规模任务特定数据集，包含100-200条轨迹，用于训练低层策略。\n- **计算资源**：高层VLM在8×A100 GPU上训练，低层策略在4×A100 GPU上训练。\n## 论文使用的评估环境和评估指标\n- **评估环境**：真实世界（Franka Emika机器人，配备RGB-D相机）和仿真环境（RLBench）。\n- **任务**：七类泛化任务，包括新物体、新场景、新背景、新机器人构型、新指令、长视野操作和动态干扰。\n- **评估指标**：任务成功率（Success Rate）、路径预测准确率（Path Accuracy）和跨域泛化性能（Cross-domain Generalization）。",
    "summary_html": "<h1>论文总结</h1>\n<h2>论文研究单位</h2>\n<p>NVIDIA, University of Washington, University of Southern California</p>\n<h2>论文概述</h2>\n<p>本文提出了一种名为HAMSTER（Hierarchical Action Models with Separated TEpRpresentations）的分层视觉-语言-动作（VLA）模型，用于开放世界机器人操作。该模型通过高层视觉语言模型（VLM）预测粗粒度的2D路径作为中间表示，然后由低层3D感知策略根据路径生成精确动作，以提升泛化能力和操作鲁棒性。实验表明，该方法在七个泛化维度上平均成功率比OpenVLA提升20%，相当于相对增益50%。</p>\n<h2>论文核心贡献点</h2>\n<ol><li>提出分层VLA架构HAMSTER，将VLM的2D路径预测与3D低层策略解耦，降低对昂贵机器人数据的依赖。</li><li>证明高层VLM可利用离域数据（如无动作视频、仿真数据）进行微调，实现跨域泛化。</li><li>低层策略结合3D感知与路径引导，专注于局部几何控制，显著提升视觉和语义泛化能力。</li></ol>\n<h2>论文方法描述</h2>\n<ol><li><strong>高层VLM设计</strong>：使用VILA-1.5-13B作为基础模型，输入RGB图像和语言指令，输出机器人末端执行器的2D路径序列（包含位置和夹爪状态）。</li><li><strong>离域数据微调</strong>：构建多源数据集，包括770k像素点预测任务（RoboPoint）、320k仿真轨迹（RLBench）和110k真实机器人轨迹（Bridge/DROID），通过监督损失最大化路径预测的对数似然。</li><li><strong>路径简化</strong>：采用Ramer-Douglas-Peucker算法压缩长路径点，保留关键点。</li><li><strong>低层策略设计</strong>：采用RVT-2或3D-DA架构，将2D路径以彩色轨迹叠加在输入图像上，作为额外条件通道，策略学习基于路径生成动作。</li><li><strong>分层执行</strong>：VLM以低频率生成路径，低层策略以高频率执行动作，支持异步操作。</li></ol>\n<h2>论文使用数据集和训练资源</h2>\n<ul><li><strong>离域数据</strong>：RoboPoint（770k样本）、RLBench仿真（320k样本）、Bridge/DROID真实数据（110k样本）。</li><li><strong>同域数据</strong>：真实机器人收集的小规模任务特定数据集，包含100-200条轨迹，用于训练低层策略。</li><li><strong>计算资源</strong>：高层VLM在8×A100 GPU上训练，低层策略在4×A100 GPU上训练。</li></ul>\n<h2>论文使用的评估环境和评估指标</h2>\n<ul><li><strong>评估环境</strong>：真实世界（Franka Emika机器人，配备RGB-D相机）和仿真环境（RLBench）。</li><li><strong>任务</strong>：七类泛化任务，包括新物体、新场景、新背景、新机器人构型、新指令、长视野操作和动态干扰。</li><li><strong>评估指标</strong>：任务成功率（Success Rate）、路径预测准确率（Path Accuracy）和跨域泛化性能（Cross-domain Generalization）。</li></ul>"
  },
  {
    "date": "2025-02-08",
    "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy",
    "link": "http://arxiv.org/abs/2502.05450",
    "summary_markdown": "- 论文研究单位\nSKL-MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n\n- 论文概述\n本文提出了一种名为ConRFT（基于一致性策略的强化微调方法）的新型两阶段框架，用于微调预训练的视觉-语言-动作（VLA）模型，以解决真实世界机器人操作任务中的挑战。该方法包括一个离线阶段和一个在线阶段，均使用统一的基于一致性的训练目标。离线阶段整合了行为克隆和Q学习，以从少量演示中提取策略并稳定值估计。在线阶段通过一致性策略进一步微调VLA模型，并结合人类干预以确保安全探索和高样本效率。\n\n- 论文核心贡献点\n1. 提出ConRFT，一种新颖的两阶段微调管道，在离线和在线阶段均使用统一的基于一致性的训练目标。\n2. 提出Cal-ConRFT用于离线阶段，通过整合离线强化学习与基于一致性的行为克隆损失，专注于利用少量演示提取高效策略和值函数，提供稳定的初始化。\n3. 提出HIL-ConRFT用于在线阶段，保留与离线阶段相同的损失结构以实现快速策略适应，同时利用人类干预确保真实世界环境中的安全探索和高样本效率。\n\n- 论文方法描述\nConRFT方法包含两个阶段：\n1. 离线微调：使用Cal-ConRFT，该方法结合了校准Q学习（Cal-QL）和一个行为克隆（BC）损失。Cal-QL通过减少时间差误差和正则化项来处理分布外动作，而BC损失直接最小化模型动作与演示动作之间的差异。训练目标是一个统一的基于一致性的目标，旨在学习一个一致性策略作为VLA模型的动作头，该策略将来自单位高斯的随机动作映射到以专家动作分布为条件的动作。一致性策略通过离散化扩散时间步并最小化预测动作与添加噪声的专家动作之间的欧几里得距离来进行训练。\n2. 在线微调：使用HIL-ConRFT，该方法应用与离线阶段相同的基于一致性的损失函数，同时结合来自预收集演示、策略转换和人类干预的数据进行训练。人类操作员可以通过遥操作工具（如SpaceMouse）干预机器人策略，以确保安全和高效的探索。\n\n- 论文使用数据集和训练资源\n数据集：使用包含20-30条轨迹的小型预收集离线数据集，并在真实世界环境中进行在线交互。\n训练资源：基于预训练的VLA模型进行微调，具体模型和硬件资源未明确提及。\n评估环境：八个不同的真实世界操作任务，包括拾取和放置（如香蕉、勺子、面包）、操作物体（如抽屉、烤面包机）以及组装复杂物体（如椅子轮、中国结）。\n评估指标：成功率（%）和任务长度（episode length）。ConRFT在45-90分钟的在线微调后平均成功率达到96.3%，相比监督微调方法成功率提升144%，任务长度缩短1.9倍。",
    "summary_html": "<ul><li>论文研究单位</li></ul>\n<p>SKL-MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China</p>\n<p>School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China</p>\n\n<ul><li>论文概述</li></ul>\n<p>本文提出了一种名为ConRFT（基于一致性策略的强化微调方法）的新型两阶段框架，用于微调预训练的视觉-语言-动作（VLA）模型，以解决真实世界机器人操作任务中的挑战。该方法包括一个离线阶段和一个在线阶段，均使用统一的基于一致性的训练目标。离线阶段整合了行为克隆和Q学习，以从少量演示中提取策略并稳定值估计。在线阶段通过一致性策略进一步微调VLA模型，并结合人类干预以确保安全探索和高样本效率。</p>\n\n<ul><li>论文核心贡献点</li></ul>\n<ol><li>提出ConRFT，一种新颖的两阶段微调管道，在离线和在线阶段均使用统一的基于一致性的训练目标。</li><li>提出Cal-ConRFT用于离线阶段，通过整合离线强化学习与基于一致性的行为克隆损失，专注于利用少量演示提取高效策略和值函数，提供稳定的初始化。</li><li>提出HIL-ConRFT用于在线阶段，保留与离线阶段相同的损失结构以实现快速策略适应，同时利用人类干预确保真实世界环境中的安全探索和高样本效率。</li></ol>\n\n<ul><li>论文方法描述</li></ul>\n<p>ConRFT方法包含两个阶段：</p>\n<ol><li>离线微调：使用Cal-ConRFT，该方法结合了校准Q学习（Cal-QL）和一个行为克隆（BC）损失。Cal-QL通过减少时间差误差和正则化项来处理分布外动作，而BC损失直接最小化模型动作与演示动作之间的差异。训练目标是一个统一的基于一致性的目标，旨在学习一个一致性策略作为VLA模型的动作头，该策略将来自单位高斯的随机动作映射到以专家动作分布为条件的动作。一致性策略通过离散化扩散时间步并最小化预测动作与添加噪声的专家动作之间的欧几里得距离来进行训练。</li><li>在线微调：使用HIL-ConRFT，该方法应用与离线阶段相同的基于一致性的损失函数，同时结合来自预收集演示、策略转换和人类干预的数据进行训练。人类操作员可以通过遥操作工具（如SpaceMouse）干预机器人策略，以确保安全和高效的探索。</li></ol>\n\n<ul><li>论文使用数据集和训练资源</li></ul>\n<p>数据集：使用包含20-30条轨迹的小型预收集离线数据集，并在真实世界环境中进行在线交互。</p>\n<p>训练资源：基于预训练的VLA模型进行微调，具体模型和硬件资源未明确提及。</p>\n<p>评估环境：八个不同的真实世界操作任务，包括拾取和放置（如香蕉、勺子、面包）、操作物体（如抽屉、烤面包机）以及组装复杂物体（如椅子轮、中国结）。</p>\n<p>评估指标：成功率（%）和任务长度（episode length）。ConRFT在45-90分钟的在线微调后平均成功率达到96.3%，相比监督微调方法成功率提升144%，任务长度缩短1.9倍。</p>"
  },
  {
    "date": "2025-02-06",
    "title": "Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture",
    "link": "http://arxiv.org/abs/2502.04558",
    "summary_markdown": "论文研究单位\n塔夫茨大学计算机科学系\n\n论文概述\n该论文旨在解决视觉-语言-动作（VLA）模型作为机器人通用策略时存在的“黑箱”问题和环境敏感性。为了提升其可解释性和鲁棒性，作者提出将VLA模型与认知架构（CA）相结合。具体方法是通过探测技术，从开源的OpenVLA模型的隐藏层中提取符号化状态（如物体属性、关系和动作状态），并将这些符号信息集成到DIARC认知架构中，实现对模型内部状态的实时监控和高层逻辑推理。\n\n论文核心贡献点\n1. 通过线性探测实验，证明了OpenVLA模型在其隐藏层中编码了丰富的符号化状态信息，包括物体关系、物体属性以及动作状态。\n2. 对OpenVLA模型Llama 2主干的所有33个隐藏层进行了系统性的探测分析，评估了不同层次对符号信息的编码能力。\n3. 设计并实现了一个集成的DIARC-OpenVLA系统，该系统利用探测到的符号状态进行实时状态监控和可视化，为构建更可解释、更可靠的机器人系统提供了基础。\n\n论文方法描述\n1. 集成系统构建：构建了DIARC-OpenVLA集成系统。用户通过DIARC的图形界面发送自然语言指令，指令通过WebSocket服务器传递给OpenVLA。OpenVLA在LIBERO模拟环境中执行动作，同时其指定隐藏层的激活值被提取出来。\n2. 探测实验设计：\n - 探测器训练：为OpenVLA的33个隐藏层分别训练两个线性探测器（一个用于物体状态，一个用于动作状态）。探测器是一个多标签分类器，使用二元交叉熵损失进行训练，将4096维的隐藏层激活向量映射到一组符号状态的二元预测值。\n - 数据收集：在LIBERO-spatial数据集的10个抓取与放置任务中，让OpenVLA执行并收集数据。每个任务收集5个成功的轨迹。在每个时间步t，记录第ℓ层的隐藏激活h_t和对应的环境真实符号状态y_t，形成训练对(h_t, object_state_t)和(h_t, action_state_t)。\n - 数据预处理：采用轨迹级别的训练/测试集划分以避免时间泄露。移除了在训练集中标签值变化极小（真值占比低于1%或高于99%）的符号状态标签。\n\n论文使用数据集和训练资源\n1. 数据集：LIBERO-spatial数据集。该数据集包含10个模拟的抓取与放置任务，任务形式为“捡起某个具有特定空间关系的黑碗，并将其放到盘子上”。总共收集了10个任务 * 5个轨迹/任务 = 50个成功轨迹的数据。\n2. 训练资源：论文中未明确提及训练探测器时所使用的具体硬件资源（如GPU型号）和训练时长。\n\n论文使用的评估环境和评估指标\n1. 评估环境：在LIBERO-spatial模拟环境中收集的测试集上对训练好的探测器进行评估。\n2. 评估指标：采用准确率作为主要评估指标。对于每一种符号谓词（如on, left-of, grasped），计算该谓词所有实例（如on(bowl, plate), on(plate, table)）的二元分类准确率的平均值。结果以热力图形式展示，呈现了不同隐藏层在不同符号谓词上的预测准确率。",
    "summary_html": "<p>论文研究单位</p>\n<p>塔夫茨大学计算机科学系</p>\n\n<p>论文概述</p>\n<p>该论文旨在解决视觉-语言-动作（VLA）模型作为机器人通用策略时存在的“黑箱”问题和环境敏感性。为了提升其可解释性和鲁棒性，作者提出将VLA模型与认知架构（CA）相结合。具体方法是通过探测技术，从开源的OpenVLA模型的隐藏层中提取符号化状态（如物体属性、关系和动作状态），并将这些符号信息集成到DIARC认知架构中，实现对模型内部状态的实时监控和高层逻辑推理。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>通过线性探测实验，证明了OpenVLA模型在其隐藏层中编码了丰富的符号化状态信息，包括物体关系、物体属性以及动作状态。</li><li>对OpenVLA模型Llama 2主干的所有33个隐藏层进行了系统性的探测分析，评估了不同层次对符号信息的编码能力。</li><li>设计并实现了一个集成的DIARC-OpenVLA系统，该系统利用探测到的符号状态进行实时状态监控和可视化，为构建更可解释、更可靠的机器人系统提供了基础。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>集成系统构建：构建了DIARC-OpenVLA集成系统。用户通过DIARC的图形界面发送自然语言指令，指令通过WebSocket服务器传递给OpenVLA。OpenVLA在LIBERO模拟环境中执行动作，同时其指定隐藏层的激活值被提取出来。</li><li>探测实验设计：</li></ol>\n<p> - 探测器训练：为OpenVLA的33个隐藏层分别训练两个线性探测器（一个用于物体状态，一个用于动作状态）。探测器是一个多标签分类器，使用二元交叉熵损失进行训练，将4096维的隐藏层激活向量映射到一组符号状态的二元预测值。</p>\n<p> - 数据收集：在LIBERO-spatial数据集的10个抓取与放置任务中，让OpenVLA执行并收集数据。每个任务收集5个成功的轨迹。在每个时间步t，记录第ℓ层的隐藏激活h_t和对应的环境真实符号状态y_t，形成训练对(h_t, object_state_t)和(h_t, action_state_t)。</p>\n<p> - 数据预处理：采用轨迹级别的训练/测试集划分以避免时间泄露。移除了在训练集中标签值变化极小（真值占比低于1%或高于99%）的符号状态标签。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：LIBERO-spatial数据集。该数据集包含10个模拟的抓取与放置任务，任务形式为“捡起某个具有特定空间关系的黑碗，并将其放到盘子上”。总共收集了10个任务 * 5个轨迹/任务 = 50个成功轨迹的数据。</li><li>训练资源：论文中未明确提及训练探测器时所使用的具体硬件资源（如GPU型号）和训练时长。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：在LIBERO-spatial模拟环境中收集的测试集上对训练好的探测器进行评估。</li><li>评估指标：采用准确率作为主要评估指标。对于每一种符号谓词（如on, left-of, grasped），计算该谓词所有实例（如on(bowl, plate), on(plate, table)）的二元分类准确率的平均值。结果以热力图形式展示，呈现了不同隐藏层在不同符号谓词上的预测准确率。</li></ol>"
  },
  {
    "date": "2025-02-04",
    "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2502.02175",
    "summary_markdown": "### 论文研究单位\nUniversity of Sydney, Australia; Shanghai Jiao Tong University, China.\n### 论文概述\n本文针对视觉-语言-动作（VLA）模型在实时机器人控制中计算成本高昂的问题，提出了一种名为VLA-Cache的训练无关推理加速方法。该方法利用机器人操作中连续帧之间的时间冗余，通过自适应地缓存和复用视觉上未发生显著变化的静态token的键值（KV）表示，来减少计算开销。同时，为保持动作的精确性，VLA-Cache会过滤掉对任务至关重要但视觉上静态的token，确保对这些区域进行重新计算。此外，还引入了一种分层自适应token复用策略，根据解码器各层的注意力集中程度动态调整复用比例。实验表明，VLA-Cache在多个模拟环境和真实机器人上实现了最高1.7倍的推理速度提升和15%的控制频率增加，而对任务成功率的影响微乎其微。\n### 论文核心贡献点\n1. 提出了一种训练无关、即插即用的VLA推理加速框架VLA-Cache，通过跨帧token缓存减少计算冗余。\n2. 设计了一种基于视觉相似性的静态token选择机制，并结合解码器注意力分数过滤掉任务相关的语义重要token，以平衡效率与精度。\n3. 引入了一种分层自适应的token复用策略，根据模型不同层的注意力集中程度动态调整token复用率，进一步优化计算效率。\n4. 在多个主流VLA模型（OpenVLA, CogAct, OpenVLA-OFT）、两个模拟基准（LIBERO, SIMPLER）和一个真实机器人系统（Kinova Jaco2）上进行了广泛验证，证明了方法的通用性和有效性。\n### 论文方法描述\nVLA-Cache的核心方法分为三个步骤：\n1. 静态Token选择：将图像划分为多个不重叠的块，计算当前帧与前一帧对应块之间的余弦相似度。选择相似度超过预设阈值τ且最稳定的前k个块作为静态视觉token候选集。\n2. 保留任务相关信息：认识到部分视觉上静态但对任务至关重要的token（如机械臂末端、目标物体）不应被复用。为此，该方法利用VLA解码器中语言指令对视觉token的交叉注意力分数来评估每个视觉token的任务相关性。过滤掉注意力分数高的任务相关token，得到最终可复用的token集。\n3. 分层自适应Token复用：观察到Transformer模型不同层的注意力分布不同，深层注意力更为集中。该方法基于每层注意力熵的变化计算一个复用比例α，对注意力更集中的层允许更高比例的token复用，从而在保证关键信息更新的同时最大化计算收益。\n在推理过程中，对于被判定为可复用的token，直接使用其前一时刻缓存的KV值；对于动态变化或任务相关的token，则重新计算其KV值。\n### 论文使用数据集和训练资源\n- **数据集与评估基准**:\n - LIBERO：一个包含四种任务套件（Spatial, Object, Goal, Long）的机器人操作模拟基准。\n - SIMPLER：一个包含Visual Matching和Variant Aggregation两种设置的模拟环境，用于缩小模拟与现实的差距。\n - 真实世界任务：在Kinova Jaco2机械臂上执行了四个自定义任务（PickPot, PlaceCube, PutSausage, WipeTable）。\n- **训练资源**:\n - VLA-Cache本身是一个训练无关的推理优化方法。\n - 实验使用了预训练好的VLA模型：OpenVLA, OpenVLA-OFT, CogAct。\n - 真实机器人实验中，使用LoRA对OpenVLA进行了微调。\n - 所有实验均在NVIDIA RTX 4090 GPU上进行。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - 模拟环境：LIBERO和SIMPLER模拟器。\n - 真实世界环境：配备一个前置摄像头的Kinova Jaco2七自由度机械臂。\n- **评估指标**:\n - Success Rate (SR)：任务成功率，衡量任务完成的百分比。\n - Control Frequency (Hz)：控制频率，衡量模型每秒能生成多少次动作，反映系统的实时响应能力。\n - FLOPs (Teraflops)：浮点运算次数，用于衡量理论计算量。\n - CUDA Latency (ms)：CUDA延迟，指在GPU上完成一次推理所需的时间，反映实际推理速度。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>University of Sydney, Australia; Shanghai Jiao Tong University, China.</p>\n<h3>论文概述</h3>\n<p>本文针对视觉-语言-动作（VLA）模型在实时机器人控制中计算成本高昂的问题，提出了一种名为VLA-Cache的训练无关推理加速方法。该方法利用机器人操作中连续帧之间的时间冗余，通过自适应地缓存和复用视觉上未发生显著变化的静态token的键值（KV）表示，来减少计算开销。同时，为保持动作的精确性，VLA-Cache会过滤掉对任务至关重要但视觉上静态的token，确保对这些区域进行重新计算。此外，还引入了一种分层自适应token复用策略，根据解码器各层的注意力集中程度动态调整复用比例。实验表明，VLA-Cache在多个模拟环境和真实机器人上实现了最高1.7倍的推理速度提升和15%的控制频率增加，而对任务成功率的影响微乎其微。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种训练无关、即插即用的VLA推理加速框架VLA-Cache，通过跨帧token缓存减少计算冗余。</li><li>设计了一种基于视觉相似性的静态token选择机制，并结合解码器注意力分数过滤掉任务相关的语义重要token，以平衡效率与精度。</li><li>引入了一种分层自适应的token复用策略，根据模型不同层的注意力集中程度动态调整token复用率，进一步优化计算效率。</li><li>在多个主流VLA模型（OpenVLA, CogAct, OpenVLA-OFT）、两个模拟基准（LIBERO, SIMPLER）和一个真实机器人系统（Kinova Jaco2）上进行了广泛验证，证明了方法的通用性和有效性。</li></ol>\n<h3>论文方法描述</h3>\n<p>VLA-Cache的核心方法分为三个步骤：</p>\n<ol><li>静态Token选择：将图像划分为多个不重叠的块，计算当前帧与前一帧对应块之间的余弦相似度。选择相似度超过预设阈值τ且最稳定的前k个块作为静态视觉token候选集。</li><li>保留任务相关信息：认识到部分视觉上静态但对任务至关重要的token（如机械臂末端、目标物体）不应被复用。为此，该方法利用VLA解码器中语言指令对视觉token的交叉注意力分数来评估每个视觉token的任务相关性。过滤掉注意力分数高的任务相关token，得到最终可复用的token集。</li><li>分层自适应Token复用：观察到Transformer模型不同层的注意力分布不同，深层注意力更为集中。该方法基于每层注意力熵的变化计算一个复用比例α，对注意力更集中的层允许更高比例的token复用，从而在保证关键信息更新的同时最大化计算收益。</li></ol>\n<p>在推理过程中，对于被判定为可复用的token，直接使用其前一时刻缓存的KV值；对于动态变化或任务相关的token，则重新计算其KV值。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集与评估基准</strong>:</li></ul>\n<p> - LIBERO：一个包含四种任务套件（Spatial, Object, Goal, Long）的机器人操作模拟基准。</p>\n<p> - SIMPLER：一个包含Visual Matching和Variant Aggregation两种设置的模拟环境，用于缩小模拟与现实的差距。</p>\n<p> - 真实世界任务：在Kinova Jaco2机械臂上执行了四个自定义任务（PickPot, PlaceCube, PutSausage, WipeTable）。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - VLA-Cache本身是一个训练无关的推理优化方法。</p>\n<p> - 实验使用了预训练好的VLA模型：OpenVLA, OpenVLA-OFT, CogAct。</p>\n<p> - 真实机器人实验中，使用LoRA对OpenVLA进行了微调。</p>\n<p> - 所有实验均在NVIDIA RTX 4090 GPU上进行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - 模拟环境：LIBERO和SIMPLER模拟器。</p>\n<p> - 真实世界环境：配备一个前置摄像头的Kinova Jaco2七自由度机械臂。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - Success Rate (SR)：任务成功率，衡量任务完成的百分比。</p>\n<p> - Control Frequency (Hz)：控制频率，衡量模型每秒能生成多少次动作，反映系统的实时响应能力。</p>\n<p> - FLOPs (Teraflops)：浮点运算次数，用于衡量理论计算量。</p>\n<p> - CUDA Latency (ms)：CUDA延迟，指在GPU上完成一次推理所需的时间，反映实际推理速度。</p>"
  },
  {
    "date": "2025-02-03",
    "title": "Scalable, Training-Free Visual Language Robotics: A Modular Multi-Model Framework for Consumer-Grade GPUs",
    "link": "http://arxiv.org/abs/2502.01071",
    "summary_markdown": "论文研究单位\nCNRS-AIST JRL (Joint Robotics Laboratory), National Institute of Advanced Industrial Science and Technology (AIST), Japan。\n\n论文概述\n现有的视觉-语言-动作（VLA）模型因高昂的计算成本、需要大量重新训练以及可扩展性有限而受到阻碍，限制了其广泛应用。本文提出了SVLR（可扩展视觉-语言机器人技术），一个开源、模块化且无需训练的框架，旨在解决这些问题。SVLR集成了多个轻量级开源AI模型（如Mini-InternVL、CLIPSeg、Phi-3和all-MiniLM），用于处理视觉和语言输入，以识别环境中的物体并响应自然语言指令生成动作序列。该框架的关键优势在于其可扩展性，无需重新训练，只需添加文本描述和任务定义即可轻松集成新的机器人任务和机器人。实验表明，SVLR可在NVIDIA RTX 2070（移动版）GPU上有效运行，并在抓取与放置任务中展现出良好的性能。\n\n论文核心贡献点\n1. 提出了一个名为SVLR的开源、模块化、无需训练的机器人控制框架，旨在提高视觉-语言机器人技术的可扩展性和可及性。\n2. 通过结合轻量级的开源AI模型，使整个框架能够在消费级GPU（如NVIDIA RTX 2070）上运行，降低了对高端硬件的依赖。\n3. 设计了一种模块化架构，允许用户仅通过添加文本描述和预编程任务来轻松扩展新的机器人和任务，无需对模型进行重新训练。\n4. 集成了句子相似度模型，用于校准大型语言模型（LLM）的输出与预定义任务及感知模块检测到的物体，提高了系统的鲁棒性和任务执行的可靠性。\n\n论文方法描述\nSVLR框架接收用户的语言指令和环境图像作为输入，通过四个主要组件进行处理：\n1. 机器人信息模块：以文本形式描述机器人的能力，并提供预编程的任务集。\n2. 感知模块：首先使用视觉-语言模型（VLM）根据图像生成场景中物体的文本描述，然后使用零样本分割模型为每个物体生成掩码，并计算其质心，最后将像素坐标转换为世界坐标系。\n3. 大型语言模型（LLM）与提示生成：提示生成器将用户指令、感知模块提供的环境信息和机器人的能力描述整合成一个提示。LLM据此生成一系列高层次的、格式化为“action_name: [parameters]”的动作描述。\n4. 动作管理器：解析LLM的输出，使用句子相似度模型将输出的动作名称和参数与机器人信息模块中的预定义任务及感知模块检测到的物体名称进行匹配。匹配成功后，执行相应的预编程任务，并将命令发送给机器人控制器。\n\n论文使用数据集和训练资源\n该论文提出了一个无需训练的框架，没有为该特定任务使用或创建任何数据集。所有使用的AI模型（Mini-InternVL, CLIPSeg, Phi-3, all-MiniLM）均为预训练的开源模型，框架直接利用其现有能力，无需进一步训练或微调。\n\n论文使用的评估环境和评估指标\n评估环境：\n* **硬件**：UR10机器人臂，Robotiq 2F-140夹爪，一个安装在夹爪上的标准网络摄像头。\n* **计算平台**：NVIDIA RTX 2070（移动版）GPU，配备8GB显存。\n\n评估指标与任务：\n* **任务**：主要针对抓取与放置任务进行评估，以演示框架理解自然语言指令、进行推理和执行动作序列的能力。\n* **评估方式**：评估是定性的，通过展示框架成功执行多种不同复杂度的语言指令（如“clean the bottle”, “Put the can into the green box and the bottle into the red box”）的实例来证明其有效性。文中未提及使用如成功率或平均执行时间等定量指标。",
    "summary_html": "<p>论文研究单位</p>\n<p>CNRS-AIST JRL (Joint Robotics Laboratory), National Institute of Advanced Industrial Science and Technology (AIST), Japan。</p>\n\n<p>论文概述</p>\n<p>现有的视觉-语言-动作（VLA）模型因高昂的计算成本、需要大量重新训练以及可扩展性有限而受到阻碍，限制了其广泛应用。本文提出了SVLR（可扩展视觉-语言机器人技术），一个开源、模块化且无需训练的框架，旨在解决这些问题。SVLR集成了多个轻量级开源AI模型（如Mini-InternVL、CLIPSeg、Phi-3和all-MiniLM），用于处理视觉和语言输入，以识别环境中的物体并响应自然语言指令生成动作序列。该框架的关键优势在于其可扩展性，无需重新训练，只需添加文本描述和任务定义即可轻松集成新的机器人任务和机器人。实验表明，SVLR可在NVIDIA RTX 2070（移动版）GPU上有效运行，并在抓取与放置任务中展现出良好的性能。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一个名为SVLR的开源、模块化、无需训练的机器人控制框架，旨在提高视觉-语言机器人技术的可扩展性和可及性。</li><li>通过结合轻量级的开源AI模型，使整个框架能够在消费级GPU（如NVIDIA RTX 2070）上运行，降低了对高端硬件的依赖。</li><li>设计了一种模块化架构，允许用户仅通过添加文本描述和预编程任务来轻松扩展新的机器人和任务，无需对模型进行重新训练。</li><li>集成了句子相似度模型，用于校准大型语言模型（LLM）的输出与预定义任务及感知模块检测到的物体，提高了系统的鲁棒性和任务执行的可靠性。</li></ol>\n\n<p>论文方法描述</p>\n<p>SVLR框架接收用户的语言指令和环境图像作为输入，通过四个主要组件进行处理：</p>\n<ol><li>机器人信息模块：以文本形式描述机器人的能力，并提供预编程的任务集。</li><li>感知模块：首先使用视觉-语言模型（VLM）根据图像生成场景中物体的文本描述，然后使用零样本分割模型为每个物体生成掩码，并计算其质心，最后将像素坐标转换为世界坐标系。</li><li>大型语言模型（LLM）与提示生成：提示生成器将用户指令、感知模块提供的环境信息和机器人的能力描述整合成一个提示。LLM据此生成一系列高层次的、格式化为“action_name: [parameters]”的动作描述。</li><li>动作管理器：解析LLM的输出，使用句子相似度模型将输出的动作名称和参数与机器人信息模块中的预定义任务及感知模块检测到的物体名称进行匹配。匹配成功后，执行相应的预编程任务，并将命令发送给机器人控制器。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>该论文提出了一个无需训练的框架，没有为该特定任务使用或创建任何数据集。所有使用的AI模型（Mini-InternVL, CLIPSeg, Phi-3, all-MiniLM）均为预训练的开源模型，框架直接利用其现有能力，无需进一步训练或微调。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<p>* <strong>硬件</strong>：UR10机器人臂，Robotiq 2F-140夹爪，一个安装在夹爪上的标准网络摄像头。</p>\n<p>* <strong>计算平台</strong>：NVIDIA RTX 2070（移动版）GPU，配备8GB显存。</p>\n\n<p>评估指标与任务：</p>\n<p>* <strong>任务</strong>：主要针对抓取与放置任务进行评估，以演示框架理解自然语言指令、进行推理和执行动作序列的能力。</p>\n<p>* <strong>评估方式</strong>：评估是定性的，通过展示框架成功执行多种不同复杂度的语言指令（如“clean the bottle”, “Put the can into the green box and the bottle into the red box”）的实例来证明其有效性。文中未提及使用如成功率或平均执行时间等定量指标。</p>"
  },
  {
    "date": "2025-01-31",
    "title": "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent",
    "link": "http://arxiv.org/abs/2501.18867",
    "summary_markdown": "### 论文研究单位\n论文作者为 Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen。论文中未明确提及具体的研究单位或机构。\n### 论文概述\n该论文提出了UP-VLA (Unified Understanding and Prediction Model for Embodied Agent)，一种用于具身智能体的统一理解与预测模型。作者指出，现有的视觉-语言-动作 (VLA) 模型通常依赖于预训练的视觉-语言模型 (VLM)，虽然VLMs提供了丰富的语义知识，但它们往往关注高层语义内容而忽略了低层特征，限制了模型捕获对机器人控制至关重要的详细视觉和空间信息的能力。为了解决这一问题，UP-VLA引入了一种新的训练范式，将多模态理解和未来预测目标相结合。该模型旨在同时增强高层语义理解和低层空间理解，实验证明其在模拟和真实世界的操作任务中均取得了显著的性能提升，特别是在Calvin ABC-D基准上相比之前的最先进方法实现了33%的改进。\n### 论文核心贡献点\n1. 受近期对VLMs局限性研究的启发，将包含丰富细节信息和动态上下文的视频数据集集成到VLA模型的预训练中，以增强其能力。\n2. 引入了一种新的VLA模型训练范式，结合了视觉-语言理解和未来预测目标，使模型能够捕获具身智能体所需的高级语义和低层视觉模式。\n3. 在模拟和真实世界的操作任务中实现了成功率的显著提升。此外，通过消融研究验证了两种预训练方法的有效性。\n### 论文方法描述\nUP-VLA方法的核心是构建一个统一的模型，该模型能够处理多模态理解、视觉预测和动作学习。\n1. **模型骨干**:\n - 使用Phi-1.5作为基础大语言模型。\n - 对于多模态理解任务，使用CLIP-ViT作为连续视觉编码器，将图像特征投影到语言嵌入空间。\n - 对于视觉预测任务，使用VQ-GAN作为离散图像编码器，直接预测未来的离散图像token。\n\n2. **连接视觉预测与多模态理解**:\n - 通过统一的提示和注意力机制，使模型能够处理多种任务。\n - **多模态理解**: 图像的连续token被放置在语言token之前，并通过修改因果注意力掩码，使图像token之间可以相互关注。\n - **未来视觉预测**: 当前观测的离散图像token被放置在语言指令之后，使模型在关注所有先前信息后预测未来图像token。\n\n3. **通过联合预测与理解增强动作学习**:\n - 模型在执行动作时，不仅输出动作序列，还输出对未来观测的预测。\n - 输入指令被扩展，加入了模型自身生成的场景描述 `L' = [E_1(O_t'), π_θ^{MMU}(O_t, L_prompt), L]`，使得动作决策能够同时利用高层次的语义理解和对当前场景的低层次描述。\n - 最终动作通过一个小型策略头生成，该策略头由一个单层注意力模块 (MAP) 和一个线性层 (MLP) 组成。\n\n4. **训练策略**:\n - **训练流程**: 分为两个阶段。第一阶段是预测与理解预训练，混合使用机器人数据（Bridge数据集，25k演示）用于未来预测和图文对数据（LLaVA-tuning-665k）用于增强理解能力。第二阶段是带动作的微调，在下游具身任务上对模型进行微调，同时继续混合图文对数据以保留多模态理解能力。\n - **训练目标**: 包含三个建模目标：用于多模态理解的语言建模、用于视觉预测的图像建模、以及用于具身任务的动作建模。\n### 论文使用数据集和训练资源\n- **数据集**:\n - **预训练**: 使用了Bridge数据集（包含25k个机械臂演示轨迹）进行未来预测任务的训练；使用了LLaVA-tuning-665k数据集（包含665k个图像-文本对）进行多模态理解能力的训练。\n - **微调与评估**: 在Calvin数据集上进行下游任务的微调和评估。附录中提到了一个用于真实世界评估的Manipulation Dataset。\n- **训练资源**:\n - 论文未明确提及所使用的具体硬件资源（如GPU类型和数量）。模型的骨干网络从Show-o模型进行初始化。\n### 论文使用的评估环境和评估指标\n- **评估环境**:\n - **模拟环境**: 在Calvin模拟环境中进行评估，具体测试了ABC->D和ABCD->D泛化基准。\n - **真实世界环境**: 在真实的机器人平台上进行评估，区分了`real-seen`（已见任务）、`real-unseen`（未见任务，测试语义泛化能力）和`real-precise`（需要精确控制的任务）三种设置。\n- **评估指标**:\n - **成功率**: 主要评估指标是任务执行的成功率。在Calvin基准上，衡量的是长序列任务的成功率；在真实世界任务中，衡量的是单个或序列任务的成功完成情况。论文报告的33%性能提升是在Calvin ABC-D基准上的成功率提升。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>论文作者为 Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen。论文中未明确提及具体的研究单位或机构。</p>\n<h3>论文概述</h3>\n<p>该论文提出了UP-VLA (Unified Understanding and Prediction Model for Embodied Agent)，一种用于具身智能体的统一理解与预测模型。作者指出，现有的视觉-语言-动作 (VLA) 模型通常依赖于预训练的视觉-语言模型 (VLM)，虽然VLMs提供了丰富的语义知识，但它们往往关注高层语义内容而忽略了低层特征，限制了模型捕获对机器人控制至关重要的详细视觉和空间信息的能力。为了解决这一问题，UP-VLA引入了一种新的训练范式，将多模态理解和未来预测目标相结合。该模型旨在同时增强高层语义理解和低层空间理解，实验证明其在模拟和真实世界的操作任务中均取得了显著的性能提升，特别是在Calvin ABC-D基准上相比之前的最先进方法实现了33%的改进。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>受近期对VLMs局限性研究的启发，将包含丰富细节信息和动态上下文的视频数据集集成到VLA模型的预训练中，以增强其能力。</li><li>引入了一种新的VLA模型训练范式，结合了视觉-语言理解和未来预测目标，使模型能够捕获具身智能体所需的高级语义和低层视觉模式。</li><li>在模拟和真实世界的操作任务中实现了成功率的显著提升。此外，通过消融研究验证了两种预训练方法的有效性。</li></ol>\n<h3>论文方法描述</h3>\n<p>UP-VLA方法的核心是构建一个统一的模型，该模型能够处理多模态理解、视觉预测和动作学习。</p>\n<p>1. <strong>模型骨干</strong>:</p>\n<p> - 使用Phi-1.5作为基础大语言模型。</p>\n<p> - 对于多模态理解任务，使用CLIP-ViT作为连续视觉编码器，将图像特征投影到语言嵌入空间。</p>\n<p> - 对于视觉预测任务，使用VQ-GAN作为离散图像编码器，直接预测未来的离散图像token。</p>\n\n<p>2. <strong>连接视觉预测与多模态理解</strong>:</p>\n<p> - 通过统一的提示和注意力机制，使模型能够处理多种任务。</p>\n<p> - <strong>多模态理解</strong>: 图像的连续token被放置在语言token之前，并通过修改因果注意力掩码，使图像token之间可以相互关注。</p>\n<p> - <strong>未来视觉预测</strong>: 当前观测的离散图像token被放置在语言指令之后，使模型在关注所有先前信息后预测未来图像token。</p>\n\n<p>3. <strong>通过联合预测与理解增强动作学习</strong>:</p>\n<p> - 模型在执行动作时，不仅输出动作序列，还输出对未来观测的预测。</p>\n<p> - 输入指令被扩展，加入了模型自身生成的场景描述 <code>L' = [E_1(O_t'), π_θ^{MMU}(O_t, L_prompt), L]</code>，使得动作决策能够同时利用高层次的语义理解和对当前场景的低层次描述。</p>\n<p> - 最终动作通过一个小型策略头生成，该策略头由一个单层注意力模块 (MAP) 和一个线性层 (MLP) 组成。</p>\n\n<p>4. <strong>训练策略</strong>:</p>\n<p> - <strong>训练流程</strong>: 分为两个阶段。第一阶段是预测与理解预训练，混合使用机器人数据（Bridge数据集，25k演示）用于未来预测和图文对数据（LLaVA-tuning-665k）用于增强理解能力。第二阶段是带动作的微调，在下游具身任务上对模型进行微调，同时继续混合图文对数据以保留多模态理解能力。</p>\n<p> - <strong>训练目标</strong>: 包含三个建模目标：用于多模态理解的语言建模、用于视觉预测的图像建模、以及用于具身任务的动作建模。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>:</li></ul>\n<p> - <strong>预训练</strong>: 使用了Bridge数据集（包含25k个机械臂演示轨迹）进行未来预测任务的训练；使用了LLaVA-tuning-665k数据集（包含665k个图像-文本对）进行多模态理解能力的训练。</p>\n<p> - <strong>微调与评估</strong>: 在Calvin数据集上进行下游任务的微调和评估。附录中提到了一个用于真实世界评估的Manipulation Dataset。</p>\n<ul><li><strong>训练资源</strong>:</li></ul>\n<p> - 论文未明确提及所使用的具体硬件资源（如GPU类型和数量）。模型的骨干网络从Show-o模型进行初始化。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>:</li></ul>\n<p> - <strong>模拟环境</strong>: 在Calvin模拟环境中进行评估，具体测试了ABC->D和ABCD->D泛化基准。</p>\n<p> - <strong>真实世界环境</strong>: 在真实的机器人平台上进行评估，区分了<code>real-seen</code>（已见任务）、<code>real-unseen</code>（未见任务，测试语义泛化能力）和<code>real-precise</code>（需要精确控制的任务）三种设置。</p>\n<ul><li><strong>评估指标</strong>:</li></ul>\n<p> - <strong>成功率</strong>: 主要评估指标是任务执行的成功率。在Calvin基准上，衡量的是长序列任务的成功率；在真实世界任务中，衡量的是单个或序列任务的成功完成情况。论文报告的33%性能提升是在Calvin ABC-D基准上的成功率提升。</p>"
  },
  {
    "date": "2025-01-28",
    "title": "Improving Vision-Language-Action Model with Online Reinforcement Learning",
    "link": "http://arxiv.org/abs/2501.16664",
    "summary_markdown": "论文研究单位\n清华大学、加州大学伯克利分校、上海期智研究院\n\n论文概述\n论文提出了一种名为iRe-VLA的框架，通过在线强化学习来改进视觉-语言-动作模型。传统的VLA模型通过监督微调训练，但在与环境交互中进一步改进是开放问题。直接应用在线RL到大型VLA模型存在训练不稳定和计算负担大的问题。iRe-VLA通过迭代RL和监督学习阶段来有效改进VLA模型，利用RL的探索性好处同时保持监督学习的稳定性。\n\n论文核心贡献点\n- 提出iRe-VLA方法，迭代进行在线RL和SL两个阶段\n- 在RL阶段冻结VLM参数，只训练轻量级动作头，保持训练稳定\n- 在SL阶段对整个模型进行微调，充分利用大型模型的表达能力\n- 两阶段方法持续增强VLA性能，稳定训练，且计算更高效\n- 在MetaWorld、Franka-Kitchen和真实世界Panda操作任务集上验证了方法有效性\n\n论文方法描述\n模型架构：使用BLIP-2 3B作为VLM主干，添加轻量级动作头，包括token learner和MLP。使用LoRA进行参数高效微调。\n学习流程：\n- Stage 0：在专家数据集上进行监督微调\n- Stage 1（RL阶段）：冻结VLM参数，仅训练动作头参数\n- Stage 2（SL阶段）：在专家数据集和在线收集的数据集上对整个模型进行监督微调\n- 迭代执行Stage 1和Stage 2\n\n论文使用数据集和训练资源\n数据集：MetaWorld、Franka-Kitchen、真实世界Panda操作任务集\n训练资源：未明确说明具体硬件配置，但方法设计考虑了计算效率\n\n论文使用的评估环境和评估指标\n评估环境：两个模拟基准和真实世界操作套件\n评估指标：任务成功率（二元奖励，R=1表示成功，R=0表示失败）",
    "summary_html": "<p>论文研究单位</p>\n<p>清华大学、加州大学伯克利分校、上海期智研究院</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为iRe-VLA的框架，通过在线强化学习来改进视觉-语言-动作模型。传统的VLA模型通过监督微调训练，但在与环境交互中进一步改进是开放问题。直接应用在线RL到大型VLA模型存在训练不稳定和计算负担大的问题。iRe-VLA通过迭代RL和监督学习阶段来有效改进VLA模型，利用RL的探索性好处同时保持监督学习的稳定性。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出iRe-VLA方法，迭代进行在线RL和SL两个阶段</li><li>在RL阶段冻结VLM参数，只训练轻量级动作头，保持训练稳定</li><li>在SL阶段对整个模型进行微调，充分利用大型模型的表达能力</li><li>两阶段方法持续增强VLA性能，稳定训练，且计算更高效</li><li>在MetaWorld、Franka-Kitchen和真实世界Panda操作任务集上验证了方法有效性</li></ul>\n\n<p>论文方法描述</p>\n<p>模型架构：使用BLIP-2 3B作为VLM主干，添加轻量级动作头，包括token learner和MLP。使用LoRA进行参数高效微调。</p>\n<p>学习流程：</p>\n<ul><li>Stage 0：在专家数据集上进行监督微调</li><li>Stage 1（RL阶段）：冻结VLM参数，仅训练动作头参数</li><li>Stage 2（SL阶段）：在专家数据集和在线收集的数据集上对整个模型进行监督微调</li><li>迭代执行Stage 1和Stage 2</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：MetaWorld、Franka-Kitchen、真实世界Panda操作任务集</p>\n<p>训练资源：未明确说明具体硬件配置，但方法设计考虑了计算效率</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：两个模拟基准和真实世界操作套件</p>\n<p>评估指标：任务成功率（二元奖励，R=1表示成功，R=0表示失败）</p>"
  },
  {
    "date": "2025-01-25",
    "title": "An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation",
    "link": "http://arxiv.org/abs/2501.15068",
    "summary_markdown": "* **论文研究单位**\n * JD Explore Academy, China\n * Jingdong Technology Information Technology Co., Ltd\n * University of Science and Technology of China\n * Shenzhen University\n * Haier Group\n * Tsinghua University\n * D-robotics\n * RealMan Intelligent Technology (Jiangsu/Beijing) Co., Ltd\n\n* **论文概述**\n 该论文针对当前端到端具身操作模型在新环境和任务中适应性差、数据需求量大的问题，提出了一种数据驱动的原子技能库构建方法。该方法通过将复杂的端到端任务分解为更细粒度、可复用的原子技能，旨在解决传统方法面临的“数据爆炸”挑战，从而实现数据高效的具身操作。\n\n* **论文核心贡献点**\n 1. 提出了一种新颖的“三轮”框架，结合了负责任务分解的VLP（视觉-语言-规划）模块和负责技能执行的VLA（视觉-语言-动作）模块，以数据驱动的方式构建原子技能库。\n 2. 基于此框架，实现了一个能够有效进行任务分解和实时规划的VLP智能体，一个将子任务映射到通用原子技能定义的语义抽象策略，以及通过微调VLA来实现原子技能库构建的实用方法。\n 3. 率先尝试通过原子技能库方法解决具身操作实际应用中的数据爆炸问题。大量真实世界实验表明，该方法能显著降低数据成本，同时展现出卓越的任务执行能力。\n\n* **论文方法描述**\n 该方法是一个由三个核心模块构成的“三轮”系统：\n 1. **VLP Agent Wheel (视觉-语言-规划轮)**：该模块集成了视觉感知、语言理解和空间智能。它使用Prismatic VLM生成场景描述，Dino-X进行物体检测，SAM-2进行实例分割，并通过基于规则的算法判断空间关系。最后，将这些信息连同任务指令输入GPT-4，进行任务分解和规划。\n 2. **VLA Wheel (视觉-语言-动作轮)**：该模块负责执行具体的原子技能。其性能，特别是可塑性和适应性，决定了原子技能的粒度。论文使用了RDT-1B模型作为基础，通过收集少量轨迹数据进行微调，以快速掌握特定的原子技能。\n 3. **Atomic Skill Wheel (原子技能轮)**：该模块是整个方法的核心，负责原子技能库的构建和更新。VLP模块将一系列任务分解为子任务，高层语义抽象模块（基于LLM如GPT-4）将这些子任务抽象为一组原子技能定义。随后通过数据收集和VLA微调来实现这些技能。面对新任务时，若所需技能已存在于库中，则直接执行；若有缺失，则仅需针对缺失技能进行数据收集和微调，实现技能库的动态扩展。\n\n* **论文使用数据集和训练资源**\n * **训练资源**：VLA模型（RDT-1B）的微调在40块A800 80GB GPU上进行。\n * **数据集**：微调RDT-1B模型共使用了8000条轨迹数据，包含6000条开源轨迹和2000条专有轨迹。专有轨迹数据由采用Mobile ALOHA系统设计的机器人采集。评估中针对不同任务采集了特定数据量，例如，“倒水”任务采集了27个端到端演示（3个瓶子位置 x 3个杯子位置 x 3次演示），并分解为9个“抓取”技能和9个“倾倒”技能的演示数据。\n\n* **论文使用的评估环境和评估指标**\n * **评估环境**：在真实世界环境中进行实验评估，使用的机器人平台是ALOHA双臂机器人。\n * **评估任务**：设计了四项复杂的操作任务进行评估：1) 将香蕉放到盘子上，2) 将水从瓶子倒入杯子，3) 将笔放入笔筒，4) 按特定顺序（如红-绿-蓝）移动积木。\n * **评估指标**：主要评估指标是任务成功率，即成功完成的试验次数占总试验次数的比例。为测试泛化能力，实验分别在分布内和分布外的物体位置组合下进行，每个组合测试10次。\n * **对比基线**：与端到端的Octo和RDT-1B模型进行对比，对比设置包括“End-to-end”（原始端到端方法）、“Ours”（数据量少于端到端）和“Ours-plus”（数据量与端到端相同但数据分布更广）。",
    "summary_html": "<p>* <strong>论文研究单位</strong></p>\n<p> * JD Explore Academy, China</p>\n<p> * Jingdong Technology Information Technology Co., Ltd</p>\n<p> * University of Science and Technology of China</p>\n<p> * Shenzhen University</p>\n<p> * Haier Group</p>\n<p> * Tsinghua University</p>\n<p> * D-robotics</p>\n<p> * RealMan Intelligent Technology (Jiangsu/Beijing) Co., Ltd</p>\n\n<p>* <strong>论文概述</strong></p>\n<p> 该论文针对当前端到端具身操作模型在新环境和任务中适应性差、数据需求量大的问题，提出了一种数据驱动的原子技能库构建方法。该方法通过将复杂的端到端任务分解为更细粒度、可复用的原子技能，旨在解决传统方法面临的“数据爆炸”挑战，从而实现数据高效的具身操作。</p>\n\n<p>* <strong>论文核心贡献点</strong></p>\n<p> 1. 提出了一种新颖的“三轮”框架，结合了负责任务分解的VLP（视觉-语言-规划）模块和负责技能执行的VLA（视觉-语言-动作）模块，以数据驱动的方式构建原子技能库。</p>\n<p> 2. 基于此框架，实现了一个能够有效进行任务分解和实时规划的VLP智能体，一个将子任务映射到通用原子技能定义的语义抽象策略，以及通过微调VLA来实现原子技能库构建的实用方法。</p>\n<p> 3. 率先尝试通过原子技能库方法解决具身操作实际应用中的数据爆炸问题。大量真实世界实验表明，该方法能显著降低数据成本，同时展现出卓越的任务执行能力。</p>\n\n<p>* <strong>论文方法描述</strong></p>\n<p> 该方法是一个由三个核心模块构成的“三轮”系统：</p>\n<p> 1. <strong>VLP Agent Wheel (视觉-语言-规划轮)</strong>：该模块集成了视觉感知、语言理解和空间智能。它使用Prismatic VLM生成场景描述，Dino-X进行物体检测，SAM-2进行实例分割，并通过基于规则的算法判断空间关系。最后，将这些信息连同任务指令输入GPT-4，进行任务分解和规划。</p>\n<p> 2. <strong>VLA Wheel (视觉-语言-动作轮)</strong>：该模块负责执行具体的原子技能。其性能，特别是可塑性和适应性，决定了原子技能的粒度。论文使用了RDT-1B模型作为基础，通过收集少量轨迹数据进行微调，以快速掌握特定的原子技能。</p>\n<p> 3. <strong>Atomic Skill Wheel (原子技能轮)</strong>：该模块是整个方法的核心，负责原子技能库的构建和更新。VLP模块将一系列任务分解为子任务，高层语义抽象模块（基于LLM如GPT-4）将这些子任务抽象为一组原子技能定义。随后通过数据收集和VLA微调来实现这些技能。面对新任务时，若所需技能已存在于库中，则直接执行；若有缺失，则仅需针对缺失技能进行数据收集和微调，实现技能库的动态扩展。</p>\n\n<p>* <strong>论文使用数据集和训练资源</strong></p>\n<p> * <strong>训练资源</strong>：VLA模型（RDT-1B）的微调在40块A800 80GB GPU上进行。</p>\n<p> * <strong>数据集</strong>：微调RDT-1B模型共使用了8000条轨迹数据，包含6000条开源轨迹和2000条专有轨迹。专有轨迹数据由采用Mobile ALOHA系统设计的机器人采集。评估中针对不同任务采集了特定数据量，例如，“倒水”任务采集了27个端到端演示（3个瓶子位置 x 3个杯子位置 x 3次演示），并分解为9个“抓取”技能和9个“倾倒”技能的演示数据。</p>\n\n<p>* <strong>论文使用的评估环境和评估指标</strong></p>\n<p> * <strong>评估环境</strong>：在真实世界环境中进行实验评估，使用的机器人平台是ALOHA双臂机器人。</p>\n<p> * <strong>评估任务</strong>：设计了四项复杂的操作任务进行评估：1) 将香蕉放到盘子上，2) 将水从瓶子倒入杯子，3) 将笔放入笔筒，4) 按特定顺序（如红-绿-蓝）移动积木。</p>\n<p> * <strong>评估指标</strong>：主要评估指标是任务成功率，即成功完成的试验次数占总试验次数的比例。为测试泛化能力，实验分别在分布内和分布外的物体位置组合下进行，每个组合测试10次。</p>\n<p> * <strong>对比基线</strong>：与端到端的Octo和RDT-1B模型进行对比，对比设置包括“End-to-end”（原始端到端方法）、“Ours”（数据量少于端到端）和“Ours-plus”（数据量与端到端相同但数据分布更广）。</p>"
  },
  {
    "date": "2025-01-16",
    "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2501.09747",
    "summary_markdown": "### 论文研究单位\nPhysical Intelligence, UC Berkeley, Stanford\n### 论文概述\n论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。\n### 论文核心贡献点\n1. 提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。\n2. 发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。\n3. 验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。\n4. 实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。\n### 论文方法描述\n1. 动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。\n2. DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。\n3. 系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。\n4. 展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。\n5. 逆向过程：通过逆BPE和逆DCT重建连续动作。\n### 论文使用数据集和训练资源\n1. 评估任务：\n - Libero模拟基准（空间、目标、长时任务）\n - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）\n - DROID零样本桌面操作（15Hz）\n2. 训练数据：\n - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。\n - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。\n3. 计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。\n### 论文使用的评估环境和评估指标\n1. 评估环境：\n - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。\n - 模拟环境：Libero任务套件。\n - 零样本测试：三个大学校园的全新桌面场景。\n2. 评估指标：\n - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。\n - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。\n - 训练效率：收敛速度和计算时间。\n - 泛化能力：零样本环境下的策略鲁棒性。</think>\n### 论文研究单位\nPhysical Intelligence, UC Berkeley, Stanford\n### 论文概述\n论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。\n### 论文核心贡献点\n1. 提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。\n2. 发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。\n3. 验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。\n4. 实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。\n### 论文方法描述\n1. 动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。\n2. DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。\n3. 系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。\n4. 展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。\n5. 逆向过程：通过逆BPE和逆DCT重建连续动作。\n### 论文使用数据集和训练资源\n1. 评估任务：\n - Libero模拟基准（空间、目标、长时任务）\n - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）\n - DROID零样本桌面操作（15Hz）\n2. 训练数据：\n - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。\n - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。\n3. 计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。\n### 论文使用的评估环境和评估指标\n1. 评估环境：\n - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。\n - 模拟环境：Libero任务套件。\n - 零样本测试：三个大学校园的全新桌面场景。\n2. 评估指标：\n - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。\n - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。\n - 训练效率：收敛速度和计算时间。\n - 泛化能力：零样本环境下的策略鲁棒性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Physical Intelligence, UC Berkeley, Stanford</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。</li><li>发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。</li><li>验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。</li><li>实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。</li><li>DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。</li><li>系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。</li><li>展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。</li><li>逆向过程：通过逆BPE和逆DCT重建连续动作。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. 评估任务：</p>\n<p> - Libero模拟基准（空间、目标、长时任务）</p>\n<p> - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）</p>\n<p> - DROID零样本桌面操作（15Hz）</p>\n<p>2. 训练数据：</p>\n<p> - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。</p>\n<p> - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。</p>\n<p>3. 计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. 评估环境：</p>\n<p> - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。</p>\n<p> - 模拟环境：Libero任务套件。</p>\n<p> - 零样本测试：三个大学校园的全新桌面场景。</p>\n<p>2. 评估指标：</p>\n<p> - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。</p>\n<p> - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。</p>\n<p> - 训练效率：收敛速度和计算时间。</p>\n<p> - 泛化能力：零样本环境下的策略鲁棒性。</think></p>\n<h3>论文研究单位</h3>\n<p>Physical Intelligence, UC Berkeley, Stanford</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。</li><li>发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。</li><li>验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。</li><li>实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li>动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。</li><li>DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。</li><li>系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。</li><li>展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。</li><li>逆向过程：通过逆BPE和逆DCT重建连续动作。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. 评估任务：</p>\n<p> - Libero模拟基准（空间、目标、长时任务）</p>\n<p> - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）</p>\n<p> - DROID零样本桌面操作（15Hz）</p>\n<p>2. 训练数据：</p>\n<p> - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。</p>\n<p> - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。</p>\n<p>3. 计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. 评估环境：</p>\n<p> - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。</p>\n<p> - 模拟环境：Libero任务套件。</p>\n<p> - 零样本测试：三个大学校园的全新桌面场景。</p>\n<p>2. 评估指标：</p>\n<p> - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。</p>\n<p> - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。</p>\n<p> - 训练效率：收敛速度和计算时间。</p>\n<p> - 泛化能力：零样本环境下的策略鲁棒性。</p>"
  },
  {
    "date": "2025-01-12",
    "title": "Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing",
    "link": "http://arxiv.org/abs/2501.06919",
    "summary_markdown": "论文研究单位\nSkoltech, Moscow, Russia\n\n论文概述\n本文介绍了Shake-VLA，一个基于视觉-语言-动作（VLA）模型的系统，旨在通过双手机器人操作实现自动化鸡尾酒调制。该系统集成了视觉模块以检测配料瓶和读取标签，语音转文本模块以解读用户命令，以及一个语言模型来生成特定的机器人指令。系统利用力矩传感器精确测量倒出液体的量，并包含检索增强生成（RAG）模块和异常检测机制以适应配方和处理配料可用性问题。\n\n论文核心贡献点\n提出了一个完整的双手机器人鸡尾酒调制系统Shake-VLA，该系统集成了视觉、语言和动作模型。通过实验验证了系统各模块的高性能，包括语音识别（93%成功率）、视觉检测（91%成功率）和异常检测（95%成功率）。整个系统在完整的鸡尾酒制作任务中达到了100%的成功率，展示了其在复杂交互式任务中的实际应用潜力。\n\n论文方法描述\n系统架构分为五个核心模块。1) 视觉模块：使用YOLOV8进行物体检测，EasyOCR读取标签，将环境信息结构化为JSON格式。2) 语音交互模块：使用OpenAI Whisper-1将语音命令转为文本，使用gTTS将系统响应转为语音。3) 检索增强生成模块（RAG）：利用FAISS和OpenAI的embedding模型检索相关配方，并由GPT-4o生成详细的制作步骤。4) 异常检测模块：比对配方要求与视觉模块检测到的配料，识别缺失或不匹配的物品，并提出替换建议。5) 语言模块：使用GPT-4o将用户输入和配方信息转化为具体的机器人指令，通过调用预定义的API函数（如take_bottle, pour_liquid）控制双臂机器人执行动作。\n\n论文使用数据集和训练资源\n系统集成了多个预训练模型，包括用于物体检测的YOLOV8、用于文本识别的EasyOCR、用于语音识别的OpenAI Whisper-1和用于指令生成的GPT-4o。RAG模块使用了自建的配方数据库，并利用OpenAI的embedding模型和FAISS向量数据库进行检索。实验硬件包括一个配备2F-Robotiq夹爪的UR3机械臂、一个UR3e机械臂、一个罗技摄像头以及一个用于测量液体重量的力传感器。研究由RSF基金（No. 24-41-02039）支持。\n\n论文使用的评估环境和评估指标\n评估在包含一个UR3机械臂、一个UR3e机械臂、摄像头和力传感器的真实世界环境中进行。视觉模块使用20个不同标签（包含俄语和英语）的瓶子进行测试。语音模块在嘈杂环境下测试了30个命令。异常检测模块进行了20次试验。评估指标主要基于成功率：语音识别成功率为93%，视觉模块在杂乱环境下的检测成功率为91%，异常检测差异识别率为95%，整个系统在完整鸡尾酒制作任务中的整体成功率为100%。",
    "summary_html": "<p>论文研究单位</p>\n<p>Skoltech, Moscow, Russia</p>\n\n<p>论文概述</p>\n<p>本文介绍了Shake-VLA，一个基于视觉-语言-动作（VLA）模型的系统，旨在通过双手机器人操作实现自动化鸡尾酒调制。该系统集成了视觉模块以检测配料瓶和读取标签，语音转文本模块以解读用户命令，以及一个语言模型来生成特定的机器人指令。系统利用力矩传感器精确测量倒出液体的量，并包含检索增强生成（RAG）模块和异常检测机制以适应配方和处理配料可用性问题。</p>\n\n<p>论文核心贡献点</p>\n<p>提出了一个完整的双手机器人鸡尾酒调制系统Shake-VLA，该系统集成了视觉、语言和动作模型。通过实验验证了系统各模块的高性能，包括语音识别（93%成功率）、视觉检测（91%成功率）和异常检测（95%成功率）。整个系统在完整的鸡尾酒制作任务中达到了100%的成功率，展示了其在复杂交互式任务中的实际应用潜力。</p>\n\n<p>论文方法描述</p>\n<p>系统架构分为五个核心模块。1) 视觉模块：使用YOLOV8进行物体检测，EasyOCR读取标签，将环境信息结构化为JSON格式。2) 语音交互模块：使用OpenAI Whisper-1将语音命令转为文本，使用gTTS将系统响应转为语音。3) 检索增强生成模块（RAG）：利用FAISS和OpenAI的embedding模型检索相关配方，并由GPT-4o生成详细的制作步骤。4) 异常检测模块：比对配方要求与视觉模块检测到的配料，识别缺失或不匹配的物品，并提出替换建议。5) 语言模块：使用GPT-4o将用户输入和配方信息转化为具体的机器人指令，通过调用预定义的API函数（如take_bottle, pour_liquid）控制双臂机器人执行动作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>系统集成了多个预训练模型，包括用于物体检测的YOLOV8、用于文本识别的EasyOCR、用于语音识别的OpenAI Whisper-1和用于指令生成的GPT-4o。RAG模块使用了自建的配方数据库，并利用OpenAI的embedding模型和FAISS向量数据库进行检索。实验硬件包括一个配备2F-Robotiq夹爪的UR3机械臂、一个UR3e机械臂、一个罗技摄像头以及一个用于测量液体重量的力传感器。研究由RSF基金（No. 24-41-02039）支持。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估在包含一个UR3机械臂、一个UR3e机械臂、摄像头和力传感器的真实世界环境中进行。视觉模块使用20个不同标签（包含俄语和英语）的瓶子进行测试。语音模块在嘈杂环境下测试了30个命令。异常检测模块进行了20次试验。评估指标主要基于成功率：语音识别成功率为93%，视觉模块在杂乱环境下的检测成功率为91%，异常检测差异识别率为95%，整个系统在完整鸡尾酒制作任务中的整体成功率为100%。</p>"
  },
  {
    "date": "2025-01-09",
    "title": "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation",
    "link": "http://arxiv.org/abs/2501.05014",
    "summary_markdown": "### 论文研究单位\n莫斯科斯科尔科沃科学技术学院\n### 论文概述\n该论文提出了UAV-VLA（视觉-语言-动作）系统，这是一个旨在通过简单文本指令为空中机器人生成大规模飞行任务的工具。该系统集成了卫星图像处理、视觉语言模型（VLM）和大型语言模型（GPT），能够将用户的自然语言请求转换为具体的飞行路径和动作计划。系统利用卫星图像提供的丰富上下文信息来增强决策和任务规划。实验表明，该方法生成的飞行路径长度与人工结果相差22%，在K-最近邻（KNN）方法下，地图上目标定位的平均欧氏距离误差为34.22米。同时，UAV-VLA系统生成所有飞行计划仅需5分24秒，比有经验的操作员快6.5倍。\n### 论文核心贡献点\n- 提出了一个大规模的视觉-语言-动作（VLA）系统，能够根据文本任务请求和卫星图像生成路径-动作集。\n- 引入了UAV-VLPA-nano-30基准数据集，用于在全球范围内评估VLA系统的性能。\n- 在UAV-VLPA-nano-30基准上验证了该系统，其性能可与人类专家生成的路径和动作计划相媲美。\n### 论文方法描述\n该方法包含三个核心模块：\n1. **目标提取GPT模块**：解析用户的语言指令，提取出一组目标任务。\n2. **目标搜索VLM模块**：利用Molmo模型在卫星图像中识别出这些目标，并输出它们在图像中的像素坐标。\n3. **动作生成GPT模块**：将像素坐标通过元数据转换为全球地理坐标，并结合任务细节，使用MAVProxy工具生成具体的无人机动作序列。\n整个流程实现了从自然语言指令到无人机可执行任务的端到端转换，包括指令解析、目标检测和坐标转换。\n### 论文使用数据集和训练资源\n- **数据集**：使用新提出的UAV-VLPA-nano-30基准数据集。该数据集包含30张来自USGS EarthExplorer平台的高分辨率卫星图像，覆盖美国多样化的城市、郊区和自然环境。图像分辨率为1.5米/像素，拍摄于春夏季节的白天。\n- **训练资源**：实验未涉及模型从零开始的训练，而是使用了预训练模型。具体评估环境为一台配备RTX 4090 GPU (24GB VRAM)和Intel Core i9-13900K CPU的PC。由于内存限制，使用了量化后的Molmo-7B-D BnB 4-bit模型进行目标搜索。\n### 论文使用的评估环境和评估指标\n- **评估环境**：系统生成的飞行计划与人类专家在Mission Planner软件中手动创建的计划进行比较。测试任务为：“创建一个飞行计划，让四轴飞行器在100米高度绕飞每个建筑物，然后返回起始点并降落”。\n- **评估指标**：\n - **路径长度**：比较系统生成路径与人工生成路径的总长度。\n - **定位误差**：使用均方根误差（RMSE）衡量生成路径点与人工路径点之间的空间偏差。采用了三种方法计算误差：顺序方法、动态时间规整（DTW）和K-最近邻（KNN）。\n - **效率**：比较系统生成任务所需时间与人类操作员所需时间。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>莫斯科斯科尔科沃科学技术学院</p>\n<h3>论文概述</h3>\n<p>该论文提出了UAV-VLA（视觉-语言-动作）系统，这是一个旨在通过简单文本指令为空中机器人生成大规模飞行任务的工具。该系统集成了卫星图像处理、视觉语言模型（VLM）和大型语言模型（GPT），能够将用户的自然语言请求转换为具体的飞行路径和动作计划。系统利用卫星图像提供的丰富上下文信息来增强决策和任务规划。实验表明，该方法生成的飞行路径长度与人工结果相差22%，在K-最近邻（KNN）方法下，地图上目标定位的平均欧氏距离误差为34.22米。同时，UAV-VLA系统生成所有飞行计划仅需5分24秒，比有经验的操作员快6.5倍。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个大规模的视觉-语言-动作（VLA）系统，能够根据文本任务请求和卫星图像生成路径-动作集。</li><li>引入了UAV-VLPA-nano-30基准数据集，用于在全球范围内评估VLA系统的性能。</li><li>在UAV-VLPA-nano-30基准上验证了该系统，其性能可与人类专家生成的路径和动作计划相媲美。</li></ul>\n<h3>论文方法描述</h3>\n<p>该方法包含三个核心模块：</p>\n<ol><li><strong>目标提取GPT模块</strong>：解析用户的语言指令，提取出一组目标任务。</li><li><strong>目标搜索VLM模块</strong>：利用Molmo模型在卫星图像中识别出这些目标，并输出它们在图像中的像素坐标。</li><li><strong>动作生成GPT模块</strong>：将像素坐标通过元数据转换为全球地理坐标，并结合任务细节，使用MAVProxy工具生成具体的无人机动作序列。</li></ol>\n<p>整个流程实现了从自然语言指令到无人机可执行任务的端到端转换，包括指令解析、目标检测和坐标转换。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：使用新提出的UAV-VLPA-nano-30基准数据集。该数据集包含30张来自USGS EarthExplorer平台的高分辨率卫星图像，覆盖美国多样化的城市、郊区和自然环境。图像分辨率为1.5米/像素，拍摄于春夏季节的白天。</li><li><strong>训练资源</strong>：实验未涉及模型从零开始的训练，而是使用了预训练模型。具体评估环境为一台配备RTX 4090 GPU (24GB VRAM)和Intel Core i9-13900K CPU的PC。由于内存限制，使用了量化后的Molmo-7B-D BnB 4-bit模型进行目标搜索。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：系统生成的飞行计划与人类专家在Mission Planner软件中手动创建的计划进行比较。测试任务为：“创建一个飞行计划，让四轴飞行器在100米高度绕飞每个建筑物，然后返回起始点并降落”。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>路径长度</strong>：比较系统生成路径与人工生成路径的总长度。</p>\n<p> - <strong>定位误差</strong>：使用均方根误差（RMSE）衡量生成路径点与人工路径点之间的空间偏差。采用了三种方法计算误差：顺序方法、动态时间规整（DTW）和K-最近邻（KNN）。</p>\n<p> - <strong>效率</strong>：比较系统生成任务所需时间与人类操作员所需时间。</p>"
  },
  {
    "date": "2025-01-08",
    "title": "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding",
    "link": "http://arxiv.org/abs/2501.04693",
    "summary_markdown": "论文研究单位\nBerkeley AI Research (BAIR), UC Berkeley, USA\n\n论文概述\n论文提出了一种名为FuSe的新方法，通过自然语言作为跨模态锚定，将预训练的通用机器人策略微调以支持异构传感器模态（如触觉和听觉）。该方法使机器人能够在部分可观测环境中执行需要多模态推理的任务，例如在视觉遮挡时依赖触觉和听觉。实验表明FuSe在真实世界任务中将成功率提高了20%以上。\n\n论文核心贡献点\n- 提出FuSe微调方法，利用语言锚定将异构传感器数据整合到预训练的通用机器人策略中\n- 引入多模态对比损失和语言生成损失，连接视觉、触觉、听觉等模态与策略的语义知识\n- 实现零样本多模态提示能力，如跨模态对象描述和组合推理\n- 验证方法在多种架构上的通用性，包括基于Transformer的Octo模型和视觉-语言-动作(VLA)模型\n- 开源包含26,866条轨迹的多模态数据集，涵盖视觉、触觉、音频和动作数据\n\n论文方法描述\nFuSe方法包含以下关键组件：\n- 触觉编码器：使用预训练的TVL编码器处理触觉图像\n- 音频编码器：将原始音频波形转换为频谱图，通过ResNet26编码\n- 辅助损失函数：多模态对比损失最大化不同视图间的互信息；语言生成损失预测高阶语义\n- 最终损失函数：L = L_BC + βL_gen + λL_contrast，其中β=1，λ=1\n- 语言重述：使用ChatGPT生成多样化的指令模板以增强泛化能力\n\n论文使用数据集和训练资源\n- 数据集：自建多模态机器人轨迹数据集，包含26,866条轨迹，涵盖视觉(640x480)、触觉(DIGIT传感器320x240)、音频(44.1kHz)、本体感觉和语言指令\n- 训练资源：在TPU v5e-128 pod上训练50,000步，批量大小1024，使用余弦学习率调度器，峰值学习率3×10^-4\n\n论文使用的评估环境和评估指标\n- 评估环境：WidowX 250 6-DoF机械臂，配备第三人称和手腕RGB相机、两个DIGIT触觉传感器、麦克风和9-DoF IMU\n- 任务：桌面抓取、购物袋抓取和按钮按压三个任务，其中购物袋任务模拟视觉遮挡场景\n- 评估指标：任务成功率，包括到达和抓取成功率，在训练和未见对象上分别评估\n- 额外评估：多模态提示准确率、组合任务完成率和消融研究性能比较",
    "summary_html": "<p>论文研究单位</p>\n<p>Berkeley AI Research (BAIR), UC Berkeley, USA</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为FuSe的新方法，通过自然语言作为跨模态锚定，将预训练的通用机器人策略微调以支持异构传感器模态（如触觉和听觉）。该方法使机器人能够在部分可观测环境中执行需要多模态推理的任务，例如在视觉遮挡时依赖触觉和听觉。实验表明FuSe在真实世界任务中将成功率提高了20%以上。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出FuSe微调方法，利用语言锚定将异构传感器数据整合到预训练的通用机器人策略中</li><li>引入多模态对比损失和语言生成损失，连接视觉、触觉、听觉等模态与策略的语义知识</li><li>实现零样本多模态提示能力，如跨模态对象描述和组合推理</li><li>验证方法在多种架构上的通用性，包括基于Transformer的Octo模型和视觉-语言-动作(VLA)模型</li><li>开源包含26,866条轨迹的多模态数据集，涵盖视觉、触觉、音频和动作数据</li></ul>\n\n<p>论文方法描述</p>\n<p>FuSe方法包含以下关键组件：</p>\n<ul><li>触觉编码器：使用预训练的TVL编码器处理触觉图像</li><li>音频编码器：将原始音频波形转换为频谱图，通过ResNet26编码</li><li>辅助损失函数：多模态对比损失最大化不同视图间的互信息；语言生成损失预测高阶语义</li><li>最终损失函数：L = L_BC + βL_gen + λL_contrast，其中β=1，λ=1</li><li>语言重述：使用ChatGPT生成多样化的指令模板以增强泛化能力</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：自建多模态机器人轨迹数据集，包含26,866条轨迹，涵盖视觉(640x480)、触觉(DIGIT传感器320x240)、音频(44.1kHz)、本体感觉和语言指令</li><li>训练资源：在TPU v5e-128 pod上训练50,000步，批量大小1024，使用余弦学习率调度器，峰值学习率3×10^-4</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：WidowX 250 6-DoF机械臂，配备第三人称和手腕RGB相机、两个DIGIT触觉传感器、麦克风和9-DoF IMU</li><li>任务：桌面抓取、购物袋抓取和按钮按压三个任务，其中购物袋任务模拟视觉遮挡场景</li><li>评估指标：任务成功率，包括到达和抓取成功率，在训练和未见对象上分别评估</li><li>额外评估：多模态提示准确率、组合任务完成率和消融研究性能比较</li></ul>"
  },
  {
    "date": "2025-01-07",
    "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
    "link": "http://arxiv.org/abs/2501.03841",
    "summary_markdown": "### 论文研究单位\nCFCS, School of CS, Peking University; PKU-AgiBot Lab; AgiBot\n### 论文概述\n该论文提出了一种名为OmniManip的通用机器人操作系统，旨在解决非结构化环境下的机器人操控难题。现有视觉语言模型（VLM）虽擅长高层推理，但缺乏精细操控所需的细粒度3D空间理解能力，而将VLM微调为视觉-语言-动作模型（VLA）又面临数据成本高和泛化性差的问题。为应对这些挑战，论文提出了一种新颖的、以物体为中心的表示方法，通过将交互原语（点和方向）定义在物体的标准空间中，作为空间约束来桥接VLM的高层推理与底层操控。系统设计为一个双闭环架构：一个用于高层规划，通过原语重采样、交互渲染和VLM检查实现闭环；另一个用于底层执行，通过6D姿态跟踪实现闭环，从而在无需微调VLM的情况下实现鲁棒、实时的控制。\n### 论文核心贡献点\n1. 提出了一种新颖的以物体为中心的交互表示，有效桥接了VLM的高层常识推理与底层机器人操控之间的鸿沟。\n2. 首次提出了一个规划和执行双闭环的开放词汇操控系统，且整个过程无需对VLM进行微调。\n3. 通过大量实验证明了该方法在多样化操控任务上具有强大的零样本泛化能力，并展示了其在自动化大规模仿真数据生成方面的潜力。\n### 论文方法描述\n该方法将复杂的机器人任务分解为多个阶段，每个阶段由带有空间约束的物体交互原语来定义。\n1. **任务分解与原语定义**：利用视觉基础模型（VFM）和VLM从指令中识别相关物体并分解任务。交互原语被定义为物体标准空间中的交互点和交互方向。空间约束则定义了主动物体与被动物体之间交互原语的距离和角度关系。\n2. **原语与约束提取**：首先使用单视图3D生成网络和通用6D物体姿态估计模型对物体进行网格重建和标准化。然后，通过VLM结合视觉提示定位交互点（包括可见和不可见的点），并通过LLM对沿主轴采样的交互方向进行任务相关性评分，生成一个有序的、带有约束的交互原语列表。\n3. **双闭环系统**：\n - **闭环规划**：引入了一个基于重采样、渲染和检查的自校正机制（RRC）。系统渲染当前交互配置，交由VLM验证。根据验证结果（成功、失败或需优化），系统决定接受、尝试下一个约束或进入细化阶段进行更精细的重采样，从而有效缓解VLM的幻觉问题。\n - **闭环执行**：当空间约束确定后，任务执行被表述为一个优化问题，目标是在满足空间约束、碰撞避免和路径平滑等损失函数的条件下，计算最优的末端执行器目标姿态。通过6D姿态跟踪器实时更新姿态，形成执行闭环。\n### 论文使用数据集和训练资源\n该方法不依赖于特定任务的训练数据集，采用零样本学习方式。它利用了多种预训练模型作为组件，包括：\n- 视觉语言模型（VLM）和大型语言模型（LLM）用于高层推理和验证。\n- Omni6DPose用于通用6D物体姿态估计。\n- 单视图3D生成网络用于从单张图像生成物体网格。\n实验中使用的机器人硬件包括Franka Emika和Kinova Gen3机械臂，以及各种日常物体。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在真实世界的实验平台上进行评估，使用了Franka Emika和Kinova Gen3两种机械臂，并包含了12种不同的日常操作任务，涵盖了刚体物体操控（如倒茶、插花）和铰接物体操控（如开关抽屉）。\n- **评估指标**：主要采用任务成功率作为核心量化指标。对于每个任务，记录在多次尝试中成功完成的次数，并计算总体的平均成功率。例如，在12项任务上，OmniManip的闭环版本总成功率为68.3%，显著优于其他基线方法。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>CFCS, School of CS, Peking University; PKU-AgiBot Lab; AgiBot</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种名为OmniManip的通用机器人操作系统，旨在解决非结构化环境下的机器人操控难题。现有视觉语言模型（VLM）虽擅长高层推理，但缺乏精细操控所需的细粒度3D空间理解能力，而将VLM微调为视觉-语言-动作模型（VLA）又面临数据成本高和泛化性差的问题。为应对这些挑战，论文提出了一种新颖的、以物体为中心的表示方法，通过将交互原语（点和方向）定义在物体的标准空间中，作为空间约束来桥接VLM的高层推理与底层操控。系统设计为一个双闭环架构：一个用于高层规划，通过原语重采样、交互渲染和VLM检查实现闭环；另一个用于底层执行，通过6D姿态跟踪实现闭环，从而在无需微调VLM的情况下实现鲁棒、实时的控制。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种新颖的以物体为中心的交互表示，有效桥接了VLM的高层常识推理与底层机器人操控之间的鸿沟。</li><li>首次提出了一个规划和执行双闭环的开放词汇操控系统，且整个过程无需对VLM进行微调。</li><li>通过大量实验证明了该方法在多样化操控任务上具有强大的零样本泛化能力，并展示了其在自动化大规模仿真数据生成方面的潜力。</li></ol>\n<h3>论文方法描述</h3>\n<p>该方法将复杂的机器人任务分解为多个阶段，每个阶段由带有空间约束的物体交互原语来定义。</p>\n<ol><li><strong>任务分解与原语定义</strong>：利用视觉基础模型（VFM）和VLM从指令中识别相关物体并分解任务。交互原语被定义为物体标准空间中的交互点和交互方向。空间约束则定义了主动物体与被动物体之间交互原语的距离和角度关系。</li><li><strong>原语与约束提取</strong>：首先使用单视图3D生成网络和通用6D物体姿态估计模型对物体进行网格重建和标准化。然后，通过VLM结合视觉提示定位交互点（包括可见和不可见的点），并通过LLM对沿主轴采样的交互方向进行任务相关性评分，生成一个有序的、带有约束的交互原语列表。</li><li><strong>双闭环系统</strong>：</li></ol>\n<p> - <strong>闭环规划</strong>：引入了一个基于重采样、渲染和检查的自校正机制（RRC）。系统渲染当前交互配置，交由VLM验证。根据验证结果（成功、失败或需优化），系统决定接受、尝试下一个约束或进入细化阶段进行更精细的重采样，从而有效缓解VLM的幻觉问题。</p>\n<p> - <strong>闭环执行</strong>：当空间约束确定后，任务执行被表述为一个优化问题，目标是在满足空间约束、碰撞避免和路径平滑等损失函数的条件下，计算最优的末端执行器目标姿态。通过6D姿态跟踪器实时更新姿态，形成执行闭环。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>该方法不依赖于特定任务的训练数据集，采用零样本学习方式。它利用了多种预训练模型作为组件，包括：</p>\n<ul><li>视觉语言模型（VLM）和大型语言模型（LLM）用于高层推理和验证。</li><li>Omni6DPose用于通用6D物体姿态估计。</li><li>单视图3D生成网络用于从单张图像生成物体网格。</li></ul>\n<p>实验中使用的机器人硬件包括Franka Emika和Kinova Gen3机械臂，以及各种日常物体。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在真实世界的实验平台上进行评估，使用了Franka Emika和Kinova Gen3两种机械臂，并包含了12种不同的日常操作任务，涵盖了刚体物体操控（如倒茶、插花）和铰接物体操控（如开关抽屉）。</li><li><strong>评估指标</strong>：主要采用任务成功率作为核心量化指标。对于每个任务，记录在多次尝试中成功完成的次数，并计算总体的平均成功率。例如，在12项任务上，OmniManip的闭环版本总成功率为68.3%，显著优于其他基线方法。</li></ul>"
  },
  {
    "date": "2025-01-07",
    "title": "Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis",
    "link": "http://arxiv.org/abs/2501.03565",
    "summary_markdown": "### 论文研究单位\n中国科学技术大学生命科学与医学部、苏州高等研究院、斯坦福大学、科大讯飞医疗事业部、中国科学技术大学附属第一医院放射科。\n### 论文概述\n针对3D医学图像零样本诊断中现有视觉-语言对齐方法存在的模态间隙问题，提出Bridged Semantic Alignment (BrgSA)框架。该框架通过大型语言模型对医疗报告进行语义总结，并设计跨模态知识交互模块，利用跨模态知识库作为语义桥梁，缩小图像与文本特征之间的间隙，提升对齐效果。\n### 论文核心贡献点\n- 提出BrgSA框架，包含语义报告总结和跨模态知识交互模块，有效弥合视觉与文本特征间隙。\n- 构建扩展基准数据集CT-RATE-LT，涵盖15种低频异常，用于评估长尾疾病的零样本诊断能力。\n- 在CT-RATE、RAD-ChestCT等数据集上实现state-of-the-art性能，显著提升低频异常诊断和报告-体素检索任务效果。\n### 论文方法描述\n- **语义总结**：利用大型语言模型（如GPT-4 Turbo）提取医疗报告关键信息，生成固定模板总结（如\"There is [abnormality]\"），并采用双输入策略（原始报告+总结）平衡信息完整性与语义一致性。\n- **跨模态知识交互（CMKI）**：引入跨模态知识库（CMKB）作为共享语义桥梁，通过注意力权重重建图像和文本特征（公式7），结合重构损失（MSE）和对比损失（InfoNCE）优化特征对齐（公式8-11）。\n### 论文使用数据集和训练资源\n- **数据集**：CT-RATE（47,149训练样本）、CT-RATE-LT（扩展15种低频异常）、RAD-ChestCT（3,630样本）、INSPECT（3,214 CTPA样本）。\n- **训练资源**：单块NVIDIA A800 GPU，PyTorch框架，学习率5e-5，批量大小64，CMKB大小K=2048，损失权重α=0.5、β=1、γ=1。\n### 论文使用的评估环境和评估指标\n- **评估环境**：单块NVIDIA A800 GPU。\n- **评估指标**：\n - 多标签分类：AUC、Accuracy、F1、Precision、mAP、Recall@1、Precision@3。\n - 检索任务：体素-体素检索用MAP@Q（Q={5,10,50}），报告-体素检索用Recall@P（P={5,10,50,100}）。\n - 统计显著性：AUC用DeLong测试，其他指标用bootstrap或t-test（p<0.001）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>中国科学技术大学生命科学与医学部、苏州高等研究院、斯坦福大学、科大讯飞医疗事业部、中国科学技术大学附属第一医院放射科。</p>\n<h3>论文概述</h3>\n<p>针对3D医学图像零样本诊断中现有视觉-语言对齐方法存在的模态间隙问题，提出Bridged Semantic Alignment (BrgSA)框架。该框架通过大型语言模型对医疗报告进行语义总结，并设计跨模态知识交互模块，利用跨模态知识库作为语义桥梁，缩小图像与文本特征之间的间隙，提升对齐效果。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出BrgSA框架，包含语义报告总结和跨模态知识交互模块，有效弥合视觉与文本特征间隙。</li><li>构建扩展基准数据集CT-RATE-LT，涵盖15种低频异常，用于评估长尾疾病的零样本诊断能力。</li><li>在CT-RATE、RAD-ChestCT等数据集上实现state-of-the-art性能，显著提升低频异常诊断和报告-体素检索任务效果。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>语义总结</strong>：利用大型语言模型（如GPT-4 Turbo）提取医疗报告关键信息，生成固定模板总结（如\"There is [abnormality]\"），并采用双输入策略（原始报告+总结）平衡信息完整性与语义一致性。</li><li><strong>跨模态知识交互（CMKI）</strong>：引入跨模态知识库（CMKB）作为共享语义桥梁，通过注意力权重重建图像和文本特征（公式7），结合重构损失（MSE）和对比损失（InfoNCE）优化特征对齐（公式8-11）。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：CT-RATE（47,149训练样本）、CT-RATE-LT（扩展15种低频异常）、RAD-ChestCT（3,630样本）、INSPECT（3,214 CTPA样本）。</li><li><strong>训练资源</strong>：单块NVIDIA A800 GPU，PyTorch框架，学习率5e-5，批量大小64，CMKB大小K=2048，损失权重α=0.5、β=1、γ=1。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：单块NVIDIA A800 GPU。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - 多标签分类：AUC、Accuracy、F1、Precision、mAP、Recall@1、Precision@3。</p>\n<p> - 检索任务：体素-体素检索用MAP@Q（Q={5,10,50}），报告-体素检索用Recall@P（P={5,10,50,100}）。</p>\n<p> - 统计显著性：AUC用DeLong测试，其他指标用bootstrap或t-test（p<0.001）。</p>"
  },
  {
    "date": "2025-01-06",
    "title": "Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches",
    "link": "http://arxiv.org/abs/2501.03151",
    "summary_markdown": "论文研究单位\n加纳海岸角技术大学电气与电子工程系，加纳矿业与技术大学。\n\n论文概述\n本文是一篇综述性论文，探讨了如何利用大型语言模型（LLMs）实现人工通用智能（AGI）。文章首先肯定了基于大规模预训练基础模型的生成式AI系统在多个领域展现出的强大能力，但同时也指出当前最先进的LLMs在认知能力上仍然表面和脆弱，其通才能力受到严重限制。为了使LLMs达到人类水平的通用智能，论文认为需要解决四个基础性问题：具身、符号接地、因果性和记忆。本文旨在讨论这些基础问题，并综述在LLMs中实现这些概念的最先进方法，最终提出一个有机地结合这些原则以实现AGI的框架。\n\n论文核心贡献点\n1. 系统性地论证了当前LLMs虽功能强大，但在实现真正的AGI方面仍存在根本性局限，如缺乏对物理世界的直接体验和对世界深层因果关系的理解。\n2. 明确并定义了四个实现LLMs通才能力所必需的认知基础原则：具身、符号接地、因果性和记忆，并阐述了每个原则对于模拟人类认知的重要性。\n3. 针对上述四个原则，分别进行了深入的文献回顾和方法综述。例如，在具身部分讨论了目标感知、情境感知、自我感知和刻意行动的实现方法；在符号接地部分对比了知识图谱、本体论驱动和嵌入等方法。\n4. 提出了一个整合了具身、接地、因果性和记忆四大原则的通用主义AI（AGI）认知框架，强调了这些原则之间的相互作用和协同效应，为构建更接近人类智能的AI系统提供了理论指导。\n\n论文方法描述\n本文采用概念分析和文献综述的方法论。首先，通过分析当前LLMs的能力与局限性来构建问题框架。其次，从生物学和人类认知科学中汲取灵感，识别出支撑通用智能的四个核心原则。接着，论文分章节对每个原则进行深入探讨，解释其理论基础，并分类和总结了在LLMs中实现这些原则的技术方法。例如，对于具身，回顾了物理智能体和虚拟环境模拟等方案；对于符号接地，分析了基于知识图谱和嵌入学习的方法。最后，综合各部分内容，构建了一个统一的AGI概念框架。\n\n论文使用数据集和训练资源\n作为一篇综述论文，本文本身不涉及在特定数据集上的模型训练或资源消耗。然而，文章全面回顾和分类了相关研究中使用的各种数据集和仿真环境。例如，在具身智能部分，文章提到了如EgoExoLearn、Holoassist等第一人称数据集，以及AirSim、AI2-THOR、CARLA等用于训练和测试智能体的3D仿真环境。在其他部分，也引用了用于符号接地的知识图谱和用于记忆机制研究的数据集。\n\n论文使用的评估环境和评估指标\n作为一篇综述论文，本文没有提出新的评估环境或特定的评估指标。文章的工作是分析和总结现有文献中用于评估AGI核心组件的各类环境和标准。例如，在具身智能的评估中，文章讨论了在物理或虚拟环境中任务的成功率、导航效率和物体操作的准确性等指标。对于符号接地，评估可能涉及语义一致性、减少幻觉等。对于记忆机制，评估指标可能包括在长对话中保持信息的准确性和检索相关性。论文本身通过对这些现有评估方法的总结，来评述不同实现路径的有效性和局限性。",
    "summary_html": "<p>论文研究单位</p>\n<p>加纳海岸角技术大学电气与电子工程系，加纳矿业与技术大学。</p>\n\n<p>论文概述</p>\n<p>本文是一篇综述性论文，探讨了如何利用大型语言模型（LLMs）实现人工通用智能（AGI）。文章首先肯定了基于大规模预训练基础模型的生成式AI系统在多个领域展现出的强大能力，但同时也指出当前最先进的LLMs在认知能力上仍然表面和脆弱，其通才能力受到严重限制。为了使LLMs达到人类水平的通用智能，论文认为需要解决四个基础性问题：具身、符号接地、因果性和记忆。本文旨在讨论这些基础问题，并综述在LLMs中实现这些概念的最先进方法，最终提出一个有机地结合这些原则以实现AGI的框架。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>系统性地论证了当前LLMs虽功能强大，但在实现真正的AGI方面仍存在根本性局限，如缺乏对物理世界的直接体验和对世界深层因果关系的理解。</li><li>明确并定义了四个实现LLMs通才能力所必需的认知基础原则：具身、符号接地、因果性和记忆，并阐述了每个原则对于模拟人类认知的重要性。</li><li>针对上述四个原则，分别进行了深入的文献回顾和方法综述。例如，在具身部分讨论了目标感知、情境感知、自我感知和刻意行动的实现方法；在符号接地部分对比了知识图谱、本体论驱动和嵌入等方法。</li><li>提出了一个整合了具身、接地、因果性和记忆四大原则的通用主义AI（AGI）认知框架，强调了这些原则之间的相互作用和协同效应，为构建更接近人类智能的AI系统提供了理论指导。</li></ol>\n\n<p>论文方法描述</p>\n<p>本文采用概念分析和文献综述的方法论。首先，通过分析当前LLMs的能力与局限性来构建问题框架。其次，从生物学和人类认知科学中汲取灵感，识别出支撑通用智能的四个核心原则。接着，论文分章节对每个原则进行深入探讨，解释其理论基础，并分类和总结了在LLMs中实现这些原则的技术方法。例如，对于具身，回顾了物理智能体和虚拟环境模拟等方案；对于符号接地，分析了基于知识图谱和嵌入学习的方法。最后，综合各部分内容，构建了一个统一的AGI概念框架。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>作为一篇综述论文，本文本身不涉及在特定数据集上的模型训练或资源消耗。然而，文章全面回顾和分类了相关研究中使用的各种数据集和仿真环境。例如，在具身智能部分，文章提到了如EgoExoLearn、Holoassist等第一人称数据集，以及AirSim、AI2-THOR、CARLA等用于训练和测试智能体的3D仿真环境。在其他部分，也引用了用于符号接地的知识图谱和用于记忆机制研究的数据集。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>作为一篇综述论文，本文没有提出新的评估环境或特定的评估指标。文章的工作是分析和总结现有文献中用于评估AGI核心组件的各类环境和标准。例如，在具身智能的评估中，文章讨论了在物理或虚拟环境中任务的成功率、导航效率和物体操作的准确性等指标。对于符号接地，评估可能涉及语义一致性、减少幻觉等。对于记忆机制，评估指标可能包括在长对话中保持信息的准确性和检索相关性。论文本身通过对这些现有评估方法的总结，来评述不同实现路径的有效性和局限性。</p>"
  },
  {
    "date": "2024-12-29",
    "title": "CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance",
    "link": "http://arxiv.org/abs/2412.20451",
    "summary_markdown": "论文研究单位\n上海大学, 美的集团, 华东师范大学\n\n论文概述\n该论文提出了CoA-VLA（Chain-of-Affordance）框架，通过引入视觉-文本链式可供性推理来增强视觉-语言-动作模型。该方法将机器人操作分解为四种可供性：对象可供性、抓取可供性、空间可供性和移动可供性，并通过动态选择机制整合到策略学习中，以提升机器人在复杂任务中的泛化能力和鲁棒性。\n\n论文核心贡献点\n1. 提出了一种新颖的链式可供性（Chain-of-Affordance）推理框架，将任务分解为四个关键可供性步骤。\n2. 设计了视觉-文本双模态可供性表示，并通过创新的共注入模块整合到扩散策略中。\n3. 实现了动态可供性选择机制，根据本体感受状态自适应选择相关可供性，减少计算冗余。\n4. 构建了自动化数据生成流水线，利用多种工具合成大规模可供性标注数据。\n\n论文方法描述\n1. 可供性定义：将任务推理分解为四个序列步骤：对象可供性（识别目标物体及位置）、抓取可供性（确定抓取点）、空间可供性（确定放置区域）、移动可供性（规划无碰撞路径）。\n2. 表示形式：文本可供性通过自然语言描述（如\"抓取把手\"），视觉可供性通过图像叠加标注（如边界框、轨迹线）。\n3. 融合机制：文本可供性经MLP标记化，视觉可供性通过ViT-Small编码，两者经Transformer模块融合后通过FiLM层注入扩散模型。\n4. 动态选择：利用本体感受状态（如夹爪开合度）动态选择当前步骤所需的可供性，避免冗余计算。\n\n论文使用数据集和训练资源\n1. 数据集：使用Droid数据集过滤39K轨迹作为预训练数据，在7个真实任务上收集692条轨迹进行微调。\n2. 训练资源：基于DiffusionVLA框架，冻结VLM参数，采用LoRA微调。预训练5个epoch（学习率2e-5），微调5个epoch（初始学习率2e-6，余弦衰减）。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n - 真实机器人：Franka机械臂配置两个ZED相机和一个手腕Realsense 435i相机\n - 仿真环境：LIBERO基准包含4个任务套件（Spatial/Object/Goal/Long）\n2. 评估指标：\n - 任务成功率：真实机器人每个任务11次尝试的成功次数\n - 平均成功率：多任务学习的整体表现\n - 视觉泛化能力：在干扰物和光照变化下的成功率\n - 标准误差：仿真基准报告成功率的标准误差</think>\n论文研究单位\n上海大学, 美的集团, 华东师范大学\n\n论文概述\n该论文提出了CoA-VLA（Chain-of-Affordance）框架，通过引入视觉-文本链式可供性推理来增强视觉-语言-动作模型。该方法将机器人操作分解为四种可供性：对象可供性、抓取可供性、空间可供性和移动可供性，并通过动态选择机制整合到策略学习中，以提升机器人在复杂任务中的泛化能力和鲁棒性。\n\n论文核心贡献点\n1. 提出了一种新颖的链式可供性（Chain-of-Affordance）推理框架，将任务分解为四个关键可供性步骤。\n2. 设计了视觉-文本双模态可供性表示，并通过创新的共注入模块整合到扩散策略中。\n3. 实现了动态可供性选择机制，根据本体感受状态自适应选择相关可供性，减少计算冗余。\n4. 构建了自动化数据生成流水线，利用多种工具合成大规模可供性标注数据。\n\n论文方法描述\n1. 可供性定义：将任务推理分解为四个序列步骤：对象可供性（识别目标物体及位置）、抓取可供性（确定抓取点）、空间可供性（确定放置区域）、移动可供性（规划无碰撞路径）。\n2. 表示形式：文本可供性通过自然语言描述（如\"抓取把手\"），视觉可供性通过图像叠加标注（如边界框、轨迹线）。\n3. 融合机制：文本可供性经MLP标记化，视觉可供性通过ViT-Small编码，两者经Transformer模块融合后通过FiLM层注入扩散模型。\n4. 动态选择：利用本体感受状态（如夹爪开合度）动态选择当前步骤所需的可供性，避免冗余计算。\n\n论文使用数据集和训练资源\n1. 数据集：使用Droid数据集过滤39K轨迹作为预训练数据，在7个真实任务上收集692条轨迹进行微调。\n2. 训练资源：基于DiffusionVLA框架，冻结VLM参数，采用LoRA微调。预训练5个epoch（学习率2e-5），微调5个epoch（初始学习率2e-6，余弦衰减）。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n - 真实机器人：Franka机械臂配置两个ZED相机和一个手腕Realsense 435i相机\n - 仿真环境：LIBERO基准包含4个任务套件（Spatial/Object/Goal/Long）\n2. 评估指标：\n - 任务成功率：真实机器人每个任务11次尝试的成功次数\n - 平均成功率：多任务学习的整体表现\n - 视觉泛化能力：在干扰物和光照变化下的成功率\n - 标准误差：仿真基准报告成功率的标准误差",
    "summary_html": "<p>论文研究单位</p>\n<p>上海大学, 美的集团, 华东师范大学</p>\n\n<p>论文概述</p>\n<p>该论文提出了CoA-VLA（Chain-of-Affordance）框架，通过引入视觉-文本链式可供性推理来增强视觉-语言-动作模型。该方法将机器人操作分解为四种可供性：对象可供性、抓取可供性、空间可供性和移动可供性，并通过动态选择机制整合到策略学习中，以提升机器人在复杂任务中的泛化能力和鲁棒性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种新颖的链式可供性（Chain-of-Affordance）推理框架，将任务分解为四个关键可供性步骤。</li><li>设计了视觉-文本双模态可供性表示，并通过创新的共注入模块整合到扩散策略中。</li><li>实现了动态可供性选择机制，根据本体感受状态自适应选择相关可供性，减少计算冗余。</li><li>构建了自动化数据生成流水线，利用多种工具合成大规模可供性标注数据。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>可供性定义：将任务推理分解为四个序列步骤：对象可供性（识别目标物体及位置）、抓取可供性（确定抓取点）、空间可供性（确定放置区域）、移动可供性（规划无碰撞路径）。</li><li>表示形式：文本可供性通过自然语言描述（如\"抓取把手\"），视觉可供性通过图像叠加标注（如边界框、轨迹线）。</li><li>融合机制：文本可供性经MLP标记化，视觉可供性通过ViT-Small编码，两者经Transformer模块融合后通过FiLM层注入扩散模型。</li><li>动态选择：利用本体感受状态（如夹爪开合度）动态选择当前步骤所需的可供性，避免冗余计算。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：使用Droid数据集过滤39K轨迹作为预训练数据，在7个真实任务上收集692条轨迹进行微调。</li><li>训练资源：基于DiffusionVLA框架，冻结VLM参数，采用LoRA微调。预训练5个epoch（学习率2e-5），微调5个epoch（初始学习率2e-6，余弦衰减）。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> - 真实机器人：Franka机械臂配置两个ZED相机和一个手腕Realsense 435i相机</p>\n<p> - 仿真环境：LIBERO基准包含4个任务套件（Spatial/Object/Goal/Long）</p>\n<p>2. 评估指标：</p>\n<p> - 任务成功率：真实机器人每个任务11次尝试的成功次数</p>\n<p> - 平均成功率：多任务学习的整体表现</p>\n<p> - 视觉泛化能力：在干扰物和光照变化下的成功率</p>\n<p> - 标准误差：仿真基准报告成功率的标准误差</think></p>\n<p>论文研究单位</p>\n<p>上海大学, 美的集团, 华东师范大学</p>\n\n<p>论文概述</p>\n<p>该论文提出了CoA-VLA（Chain-of-Affordance）框架，通过引入视觉-文本链式可供性推理来增强视觉-语言-动作模型。该方法将机器人操作分解为四种可供性：对象可供性、抓取可供性、空间可供性和移动可供性，并通过动态选择机制整合到策略学习中，以提升机器人在复杂任务中的泛化能力和鲁棒性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种新颖的链式可供性（Chain-of-Affordance）推理框架，将任务分解为四个关键可供性步骤。</li><li>设计了视觉-文本双模态可供性表示，并通过创新的共注入模块整合到扩散策略中。</li><li>实现了动态可供性选择机制，根据本体感受状态自适应选择相关可供性，减少计算冗余。</li><li>构建了自动化数据生成流水线，利用多种工具合成大规模可供性标注数据。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>可供性定义：将任务推理分解为四个序列步骤：对象可供性（识别目标物体及位置）、抓取可供性（确定抓取点）、空间可供性（确定放置区域）、移动可供性（规划无碰撞路径）。</li><li>表示形式：文本可供性通过自然语言描述（如\"抓取把手\"），视觉可供性通过图像叠加标注（如边界框、轨迹线）。</li><li>融合机制：文本可供性经MLP标记化，视觉可供性通过ViT-Small编码，两者经Transformer模块融合后通过FiLM层注入扩散模型。</li><li>动态选择：利用本体感受状态（如夹爪开合度）动态选择当前步骤所需的可供性，避免冗余计算。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：使用Droid数据集过滤39K轨迹作为预训练数据，在7个真实任务上收集692条轨迹进行微调。</li><li>训练资源：基于DiffusionVLA框架，冻结VLM参数，采用LoRA微调。预训练5个epoch（学习率2e-5），微调5个epoch（初始学习率2e-6，余弦衰减）。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> - 真实机器人：Franka机械臂配置两个ZED相机和一个手腕Realsense 435i相机</p>\n<p> - 仿真环境：LIBERO基准包含4个任务套件（Spatial/Object/Goal/Long）</p>\n<p>2. 评估指标：</p>\n<p> - 任务成功率：真实机器人每个任务11次尝试的成功次数</p>\n<p> - 平均成功率：多任务学习的整体表现</p>\n<p> - 视觉泛化能力：在干扰物和光照变化下的成功率</p>\n<p> - 标准误差：仿真基准报告成功率的标准误差</p>"
  },
  {
    "date": "2024-12-24",
    "title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks",
    "link": "http://arxiv.org/abs/2412.18194",
    "summary_markdown": "1. **拆解用户需求**\n\n* **角色：** 我需要扮演一名论文阅读专家。\n* **输入：** 一篇 Arxiv 论文的 HTML 内容。它已被截断，但涵盖了核心部分：标题、作者、摘要、介绍、相关工作、方法（VLABench 部分）、实验和结论。\n* **任务：** 总结论文的要点。\n* **格式：** 严格的 Markdown 格式。无加粗。无额外文本。\n* **结构：** 输出必须包含这些确切的标题：\n * 论文研究单位\n * 论文概述\n * 论文核心贡献点\n * 论文方法描述\n * 论文使用数据集和训练资源\n * 论文使用的评估环境和评估指标\n\n2. **初步梳理与信息提取（心智草稿）**\n\n* **论文研究单位:**\n * 查找作者单位。作者列表显示“School of Computer Science, Fudan University”以及相应的电子邮件 `@fudan.edu.cn`。这很直接。答案：复旦大学计算机科学学院。\n\n* **论文概述:**\n * 阅读摘要和引言。\n * 摘要：“我们提出了 VLABench，一个开源基准，用于评估通用的语言条件操作（LCM）任务学习...VLABench 提供了 100 个精心设计的任务类别...与之前的基准相比，VLABench 有四个关键方面...”\n * 引言：“语言条件操作是具身智能的一个基本挑战...为了更好地定义适合基础模型的语言条件操作任务的类型，并为进一步推进机器人学研究提供一个标准化的评估套件，我们引入了 VLABench。”\n * 核心思想：当前的语言条件操作基准不足以评估像视觉-语言-动作（VLA）模型这样的大型基础模型。它们缺乏长期推理、常识转移、复杂的自然语言指令以及对模型能力进行多方面评估的需求。因此，作者创建了 VLABench 来填补这一空白。\n * 我的概述总结：VLABench 是一个新的大规模基准，旨在评估语言条件机器人操作任务，特别是对基础模型（如 VLAs）的评估。它设计了 100 个类别、超过 2000 个物体的任务，重点关注长期推理、常识转移和隐式指令等要求，从视觉、语言、知识和行动等多个维度对模型进行评估。\n\n* **论文核心贡献点:**\n * 查找类似“我们贡献如下”的部分。引言的末尾有这样一个列表。\n * 要点 1：“我们提出了 VLABench，第一个旨在全面评估 VLAs 和 VLMs 在机器人操作任务中的能力的基准...”\n * 要点 2：“我们在标准化评估框架内定义了 100 个新颖的 LCM 任务，这些任务专为基础模型的能力量身定制...”\n * 要点 3：“我们提供了一个可扩展的数据构建框架和一个标准化的评估数据集。这种自动化数据构建方法促进了未来关于机器人预训练数据的研究。”\n * 要点 4：“我们的实验表明，当前的预训练 VLAs 尚未表现出在 LLMs 中观察到的强泛化能力...”\n * 这些是四个明确陈述的贡献。我将直接列出它们。\n\n* **论文方法描述:**\n * 这一节更宽泛一些。它关乎“VLABench”是如何构建的。\n * 查看 3. VLABench 章节：\n * **任务描述（3.1）：** 任务被分为 60 个基元任务和 40 个复合任务。基元任务评估网格/纹理理解、空间理解、常识、语义理解和物理定律等特定维度。复合任务需要多步骤规划和长期推理（平均超过 500 个时间步）。\n * **基准（3.2）：** 评估针对三种方法进行：VLA（对已见/未见物体进行评估）、基础模型工作流（零样本）和 VLM（使用领域特定语言进行非交互式和交互式评估）。\n * **仿真（3.3）：** 基于 Mujoco 和 dm_control。高度模块化。资产库有 163 个类别、2164 个物品，来自 RoboCasa、Objaverse 和文本到 3D 的生成。默认机器人是 Franka Emika Panda（7 自由度）。\n * **数据集构建（3.4）：** 为确保多样性而进行域随机化（物体位置、比例、纹理、光照）。通过一个利用技能库、启发式信息和 RRT/SLERP 轨迹规划的可扩展自动化框架生成轨迹。使用 GPT-4 进行指令增强以创建自然的、隐式的指令。\n * 我需要把这些内容整合成一个连贯的段落，涵盖任务设计、仿真环境、数据收集和评估方法论。\n\n* **论文使用数据集和训练资源:**\n * 这与前一点有重叠，但我会特别关注“是什么”和“有多少”。\n * **数据集：** 由论文的自动化框架构建。包含 100 个任务类别的轨迹。\n * **资产：** 163 个类别、2164 个 3D 物体。\n * **训练资源：** 论文提到使用此数据集微调了像 OpenVLA、Octo 和 RDT-1B 这样的预训练 VLA。他们使用“每个任务类别 100 条轨迹，总计 1600 条轨迹”进行微调。所以，训练资源是他们自定义的数据集和强大的 GPU（尽管具体型号未提及，但这是隐含的）。我将陈述他们创建的数据集，并列出用于微调的模型。\n\n* **论文使用的评估环境和评估指标:**\n * **环境：** 基于 Mujoco 和 dm_control 的仿真环境。在仿真中使用 Franka Emika Panda 机械臂。支持多相机视图和点云。\n * **指标：** 这是关键的。论文定义了几个指标。\n * **Progress Score (PS)：** 首要指标。公式为 `PS = α * (n_correct / N) + (1-α) * (m_done / M)`。它平衡了正确决策与任务进度。默认的 α 为 0.2。\n * **对于 VLM：** 更详细的指标：\n * Skill Recall Rate\n * Parameter Recall Rate\n * Skill & Parameter Recall Rate\n * Precise Matching Rate (PM)\n * 评估设置各不相同：VLA（已见/未见物体），工作流（零样本），VLM（交互式/非交互式）。\n * 我将清晰地列出环境（Mujoco，Franka Panda）和指标（PS 为主要指标，外加 VLM 的特定指标）。\n\n3. **最终审查和规范格式**\n\n* 我将把提取的要点组合在所需的 Markdown 标题下。\n* 我将使用项目符号以求清晰，特别是在“贡献”和“方法”部分。\n* 我将确保语言简洁准确，并直接基于论文内容。\n* 我将再次检查，确保没有加粗，也没有像“这是总结：”这样的额外文本。输出应该*就是*这个总结。\n* 结构将是：\n ```markdown\n### 论文研究单位\n ...\n### 论文概述\n ...\n### 论文核心贡献点\n - ...\n - ...\n### 论文方法描述\n ...\n### 论文使用数据集和训练资源\n ...\n### 论文使用的评估环境和评估指标\n ...\n ```\n* 这个结构满足了用户的所有要求。内容直接从源文本中提取和总结。我现在将生成最终输出。</think>\n### 论文研究单位\n复旦大学计算机科学学院\n### 论文概述\nVLABench是一个大规模开源基准，旨在评估通用语言条件操作（LCM）任务的学习，特别是针对基于基础模型的方法。该基准提供了100个精心设计的任务类别，包含超过2000个物体，具有强随机性。VLABench的核心特点是强调需要世界知识与常识迁移的任务、包含隐含人类意图的自然语言指令、需要多步推理的长视野任务，以及对动作策略和语言模型能力的双重评估。它旨在从视觉、语言、知识迁移、规划和行动等多个维度，全面评估模型的综合技能学习与泛化能力。\n### 论文核心贡献点\n- 提出了VLABench，首个专为全面评估视觉-语言-动作模型（VLA）和视觉-语言模型（VLM）在机器人操作任务中能力而设计的大规模基准。\n- 在标准化评估框架内定义了100个新颖的LCM任务，这些任务专为评估基础模型在语义、视觉、空间、物理、知识迁移和推理等方面的能力而设计。\n- 提供了一个可扩展的数据构建框架和一个标准化的评估数据集，该自动化数据构建方法有助于未来关于机器人预训练数据的研究。\n- 通过实验证明，当前最先进的预训练VLA尚未展现出在大型语言模型中观察到的强泛化能力，现有最先进的VLM在具身场景中也存在局限性。\n### 论文方法描述\nVLABench的构建方法包含以下几个部分：\n- **任务设计**：任务分为60个基元任务和40个复合任务。基元任务用于评估网格与纹理理解、空间理解、常识与世界知识、语义理解和物理定律等特定能力维度。复合任务则结合多种技能，需要长期任务规划和多步逻辑推理，平均轨迹长度超过500个时间步。\n- **仿真环境**：基于Mujoco和dm_control构建，因其轻量级、高性能和物理真实感而入选。框架高度模块化，支持多种机器人形态，标准评估使用7自由度的Franka Emika Panda机械臂。\n- **资产库构建**：构建了包含163个类别、共2164个物体的资产库。资产部分继承自RoboCasa和Objaverse，部分通过在线3D模型网站和生成式AI模型（如Tripo.AI和Runway.AI）创建。\n- **数据集构建**：为实现数据多样性和规模化，实现了多种域随机化（如物体位置、纹理、光照）。开发了一个高效的自动化数据收集管道，该管道利用自定义的技能库和先验信息（如点云、抓取点）通过RRT和SLERP算法生成轨迹，并使用GPT-4进行指令增强，以生成自然且隐含的指令。\n### 论文使用数据集和训练资源\n- **数据集**：通过上述自动化框架构建，为每个任务类别提供高质量的轨迹数据。\n- **训练资源**：使用该数据集对OpenVLA、Octo和RDT-1B等预训练VLA模型进行微调。实验中，从每个任务类别采样100条轨迹，总计1600条轨迹进行微调，以确保任务间的平衡表示。\n### 论文使用的评估环境和评估指标\n- **评估环境**：在基于Mujoco的仿真环境中进行评估。标准评估设置使用Franka Emika Panda机械臂，其末端执行器的位姿通过欧几里得空间中的3D坐标和四元数表示，并通过逆运动学解析为关节角。\n- **评估指标**：\n - **Progress Score (PS)**：主要评估指标，计算公式为 `PS = α * (n_correct / N) + (1-α) * (m_done / M)`，其中α默认为0.2。该指标综合了正确决策的比率（`n_correct / N`）和任务进度的比率（`m_done / M`），用于更细致的评估。\n - **VLM评估指标**：采用了更详细的评分方法，包括技能召回率、参数召回率、技能与参数召回率和精确匹配率。\n- **评估设置**：针对VLA模型，设置了已见物体和未见物体两种泛化评估场景；对于基础模型工作流，进行零样本转移能力评估；对于VLM，则进行交互式和非交互式两种方式的综合能力评估。",
    "summary_html": "<p>1. <strong>拆解用户需求</strong></p>\n\n<p>* <strong>角色：</strong> 我需要扮演一名论文阅读专家。</p>\n<p>* <strong>输入：</strong> 一篇 Arxiv 论文的 HTML 内容。它已被截断，但涵盖了核心部分：标题、作者、摘要、介绍、相关工作、方法（VLABench 部分）、实验和结论。</p>\n<p>* <strong>任务：</strong> 总结论文的要点。</p>\n<p>* <strong>格式：</strong> 严格的 Markdown 格式。无加粗。无额外文本。</p>\n<p>* <strong>结构：</strong> 输出必须包含这些确切的标题：</p>\n<p> * 论文研究单位</p>\n<p> * 论文概述</p>\n<p> * 论文核心贡献点</p>\n<p> * 论文方法描述</p>\n<p> * 论文使用数据集和训练资源</p>\n<p> * 论文使用的评估环境和评估指标</p>\n\n<p>2. <strong>初步梳理与信息提取（心智草稿）</strong></p>\n\n<p>* <strong>论文研究单位:</strong></p>\n<p> * 查找作者单位。作者列表显示“School of Computer Science, Fudan University”以及相应的电子邮件 <code>@fudan.edu.cn</code>。这很直接。答案：复旦大学计算机科学学院。</p>\n\n<p>* <strong>论文概述:</strong></p>\n<p> * 阅读摘要和引言。</p>\n<p> * 摘要：“我们提出了 VLABench，一个开源基准，用于评估通用的语言条件操作（LCM）任务学习...VLABench 提供了 100 个精心设计的任务类别...与之前的基准相比，VLABench 有四个关键方面...”</p>\n<p> * 引言：“语言条件操作是具身智能的一个基本挑战...为了更好地定义适合基础模型的语言条件操作任务的类型，并为进一步推进机器人学研究提供一个标准化的评估套件，我们引入了 VLABench。”</p>\n<p> * 核心思想：当前的语言条件操作基准不足以评估像视觉-语言-动作（VLA）模型这样的大型基础模型。它们缺乏长期推理、常识转移、复杂的自然语言指令以及对模型能力进行多方面评估的需求。因此，作者创建了 VLABench 来填补这一空白。</p>\n<p> * 我的概述总结：VLABench 是一个新的大规模基准，旨在评估语言条件机器人操作任务，特别是对基础模型（如 VLAs）的评估。它设计了 100 个类别、超过 2000 个物体的任务，重点关注长期推理、常识转移和隐式指令等要求，从视觉、语言、知识和行动等多个维度对模型进行评估。</p>\n\n<p>* <strong>论文核心贡献点:</strong></p>\n<p> * 查找类似“我们贡献如下”的部分。引言的末尾有这样一个列表。</p>\n<p> * 要点 1：“我们提出了 VLABench，第一个旨在全面评估 VLAs 和 VLMs 在机器人操作任务中的能力的基准...”</p>\n<p> * 要点 2：“我们在标准化评估框架内定义了 100 个新颖的 LCM 任务，这些任务专为基础模型的能力量身定制...”</p>\n<p> * 要点 3：“我们提供了一个可扩展的数据构建框架和一个标准化的评估数据集。这种自动化数据构建方法促进了未来关于机器人预训练数据的研究。”</p>\n<p> * 要点 4：“我们的实验表明，当前的预训练 VLAs 尚未表现出在 LLMs 中观察到的强泛化能力...”</p>\n<p> * 这些是四个明确陈述的贡献。我将直接列出它们。</p>\n\n<p>* <strong>论文方法描述:</strong></p>\n<p> * 这一节更宽泛一些。它关乎“VLABench”是如何构建的。</p>\n<p> * 查看 3. VLABench 章节：</p>\n<p> * <strong>任务描述（3.1）：</strong> 任务被分为 60 个基元任务和 40 个复合任务。基元任务评估网格/纹理理解、空间理解、常识、语义理解和物理定律等特定维度。复合任务需要多步骤规划和长期推理（平均超过 500 个时间步）。</p>\n<p> * <strong>基准（3.2）：</strong> 评估针对三种方法进行：VLA（对已见/未见物体进行评估）、基础模型工作流（零样本）和 VLM（使用领域特定语言进行非交互式和交互式评估）。</p>\n<p> * <strong>仿真（3.3）：</strong> 基于 Mujoco 和 dm_control。高度模块化。资产库有 163 个类别、2164 个物品，来自 RoboCasa、Objaverse 和文本到 3D 的生成。默认机器人是 Franka Emika Panda（7 自由度）。</p>\n<p> * <strong>数据集构建（3.4）：</strong> 为确保多样性而进行域随机化（物体位置、比例、纹理、光照）。通过一个利用技能库、启发式信息和 RRT/SLERP 轨迹规划的可扩展自动化框架生成轨迹。使用 GPT-4 进行指令增强以创建自然的、隐式的指令。</p>\n<p> * 我需要把这些内容整合成一个连贯的段落，涵盖任务设计、仿真环境、数据收集和评估方法论。</p>\n\n<p>* <strong>论文使用数据集和训练资源:</strong></p>\n<p> * 这与前一点有重叠，但我会特别关注“是什么”和“有多少”。</p>\n<p> * <strong>数据集：</strong> 由论文的自动化框架构建。包含 100 个任务类别的轨迹。</p>\n<p> * <strong>资产：</strong> 163 个类别、2164 个 3D 物体。</p>\n<p> * <strong>训练资源：</strong> 论文提到使用此数据集微调了像 OpenVLA、Octo 和 RDT-1B 这样的预训练 VLA。他们使用“每个任务类别 100 条轨迹，总计 1600 条轨迹”进行微调。所以，训练资源是他们自定义的数据集和强大的 GPU（尽管具体型号未提及，但这是隐含的）。我将陈述他们创建的数据集，并列出用于微调的模型。</p>\n\n<p>* <strong>论文使用的评估环境和评估指标:</strong></p>\n<p> * <strong>环境：</strong> 基于 Mujoco 和 dm_control 的仿真环境。在仿真中使用 Franka Emika Panda 机械臂。支持多相机视图和点云。</p>\n<p> * <strong>指标：</strong> 这是关键的。论文定义了几个指标。</p>\n<p> * <strong>Progress Score (PS)：</strong> 首要指标。公式为 <code>PS = α * (n_correct / N) + (1-α) * (m_done / M)</code>。它平衡了正确决策与任务进度。默认的 α 为 0.2。</p>\n<p> * <strong>对于 VLM：</strong> 更详细的指标：</p>\n<p> * Skill Recall Rate</p>\n<p> * Parameter Recall Rate</p>\n<p> * Skill & Parameter Recall Rate</p>\n<p> * Precise Matching Rate (PM)</p>\n<p> * 评估设置各不相同：VLA（已见/未见物体），工作流（零样本），VLM（交互式/非交互式）。</p>\n<p> * 我将清晰地列出环境（Mujoco，Franka Panda）和指标（PS 为主要指标，外加 VLM 的特定指标）。</p>\n\n<p>3. <strong>最终审查和规范格式</strong></p>\n\n<p>* 我将把提取的要点组合在所需的 Markdown 标题下。</p>\n<p>* 我将使用项目符号以求清晰，特别是在“贡献”和“方法”部分。</p>\n<p>* 我将确保语言简洁准确，并直接基于论文内容。</p>\n<p>* 我将再次检查，确保没有加粗，也没有像“这是总结：”这样的额外文本。输出应该*就是*这个总结。</p>\n<p>* 结构将是：</p>\n<p> ```markdown</p>\n<h3>论文研究单位</h3>\n<p> ...</p>\n<h3>论文概述</h3>\n<p> ...</p>\n<h3>论文核心贡献点</h3>\n<p> - ...</p>\n<p> - ...</p>\n<h3>论文方法描述</h3>\n<p> ...</p>\n<h3>论文使用数据集和训练资源</h3>\n<p> ...</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p> ...</p>\n<p> ```</p>\n<p>* 这个结构满足了用户的所有要求。内容直接从源文本中提取和总结。我现在将生成最终输出。</think></p>\n<h3>论文研究单位</h3>\n<p>复旦大学计算机科学学院</p>\n<h3>论文概述</h3>\n<p>VLABench是一个大规模开源基准，旨在评估通用语言条件操作（LCM）任务的学习，特别是针对基于基础模型的方法。该基准提供了100个精心设计的任务类别，包含超过2000个物体，具有强随机性。VLABench的核心特点是强调需要世界知识与常识迁移的任务、包含隐含人类意图的自然语言指令、需要多步推理的长视野任务，以及对动作策略和语言模型能力的双重评估。它旨在从视觉、语言、知识迁移、规划和行动等多个维度，全面评估模型的综合技能学习与泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了VLABench，首个专为全面评估视觉-语言-动作模型（VLA）和视觉-语言模型（VLM）在机器人操作任务中能力而设计的大规模基准。</li><li>在标准化评估框架内定义了100个新颖的LCM任务，这些任务专为评估基础模型在语义、视觉、空间、物理、知识迁移和推理等方面的能力而设计。</li><li>提供了一个可扩展的数据构建框架和一个标准化的评估数据集，该自动化数据构建方法有助于未来关于机器人预训练数据的研究。</li><li>通过实验证明，当前最先进的预训练VLA尚未展现出在大型语言模型中观察到的强泛化能力，现有最先进的VLM在具身场景中也存在局限性。</li></ul>\n<h3>论文方法描述</h3>\n<p>VLABench的构建方法包含以下几个部分：</p>\n<ul><li><strong>任务设计</strong>：任务分为60个基元任务和40个复合任务。基元任务用于评估网格与纹理理解、空间理解、常识与世界知识、语义理解和物理定律等特定能力维度。复合任务则结合多种技能，需要长期任务规划和多步逻辑推理，平均轨迹长度超过500个时间步。</li><li><strong>仿真环境</strong>：基于Mujoco和dm_control构建，因其轻量级、高性能和物理真实感而入选。框架高度模块化，支持多种机器人形态，标准评估使用7自由度的Franka Emika Panda机械臂。</li><li><strong>资产库构建</strong>：构建了包含163个类别、共2164个物体的资产库。资产部分继承自RoboCasa和Objaverse，部分通过在线3D模型网站和生成式AI模型（如Tripo.AI和Runway.AI）创建。</li><li><strong>数据集构建</strong>：为实现数据多样性和规模化，实现了多种域随机化（如物体位置、纹理、光照）。开发了一个高效的自动化数据收集管道，该管道利用自定义的技能库和先验信息（如点云、抓取点）通过RRT和SLERP算法生成轨迹，并使用GPT-4进行指令增强，以生成自然且隐含的指令。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：通过上述自动化框架构建，为每个任务类别提供高质量的轨迹数据。</li><li><strong>训练资源</strong>：使用该数据集对OpenVLA、Octo和RDT-1B等预训练VLA模型进行微调。实验中，从每个任务类别采样100条轨迹，总计1600条轨迹进行微调，以确保任务间的平衡表示。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：在基于Mujoco的仿真环境中进行评估。标准评估设置使用Franka Emika Panda机械臂，其末端执行器的位姿通过欧几里得空间中的3D坐标和四元数表示，并通过逆运动学解析为关节角。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>Progress Score (PS)</strong>：主要评估指标，计算公式为 <code>PS = α * (n_correct / N) + (1-α) * (m_done / M)</code>，其中α默认为0.2。该指标综合了正确决策的比率（<code>n_correct / N</code>）和任务进度的比率（<code>m_done / M</code>），用于更细致的评估。</p>\n<p> - <strong>VLM评估指标</strong>：采用了更详细的评分方法，包括技能召回率、参数召回率、技能与参数召回率和精确匹配率。</p>\n<ul><li><strong>评估设置</strong>：针对VLA模型，设置了已见物体和未见物体两种泛化评估场景；对于基础模型工作流，进行零样本转移能力评估；对于VLM，则进行交互式和非交互式两种方式的综合能力评估。</li></ul>"
  },
  {
    "date": "2024-12-20",
    "title": "QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning",
    "link": "http://arxiv.org/abs/2412.15576",
    "summary_markdown": "# 论文研究单位\n西湖大学MiLAB、浙江大学、北京邮电大学\n# 论文概述\n本文解决了在四足视觉-语言-动作(QUAR-VLA)任务中部署多模态大语言模型(MLLM)时固有的推理延迟挑战。研究发现，传统的参数缩减技术最终会在动作指令调优阶段损害语言基础模型的性能，因此不适合此目的。论文引入了一种名为QUART-Online的新型无延迟四足MLLM模型，旨在提高推理效率而不降低语言基础模型的性能。通过引入动作块离散化(ACD)，压缩原始动作表示空间，将连续动作值映射到更小的离散代表性向量集合，同时保留关键信息。随后，对MLLM进行微调，将视觉、语言和压缩动作集成到统一语义空间。实验结果表明，QUART-Online与现有MLLM系统协同工作，以50Hz频率实现实时推理，与底层控制器频率同步，显著提升各类任务成功率65%。\n# 论文核心贡献点\n- 发现传统参数缩减技术在动作指令调优阶段会损害语言基础模型性能\n- 提出QUART-Online无延迟四足多模态大语言模型\n- 引入动作块离散化(ACD)方法，有效压缩动作表示空间\n- 将视觉、语言和压缩动作集成到统一语义空间的微调方法\n- 实现50Hz实时推理频率，与底层控制器频率同步\n- 在多个任务上平均成功率提升65%\n# 论文方法描述\nQUART-Online包含两个核心组件：\n1. 动作块离散化(ACD)\n - 将序列连续动作值映射到更小的离散代表性向量集合\n - 使用多层量化方法，输入数据由每个量化器顺序量化，明确管理每个量化步骤的残差\n - 包含动作编码器：使用N_L层1D卷积在时间轴上编码动作步骤，确保时间压缩和步骤连贯性\n - 量化过程：使用可学习码本B∈R^(N_q×K×D)，其中包含N_q层码本，每层有K个码字\n - 反量化过程：从最后一层码本开始，逐层累积到第一层，重建原始动作表示\n\n2. 动作块对齐\n - 微调MLLM将视觉、语言和压缩动作集成到统一语义空间\n - 推理时MLLM先输出预测的压缩动作标记，再解码为连续轨迹供机器人执行\n\n整体流程：输入图像和语言指令→标记化→MLLM处理→输出压缩动作标记→反量化→连续动作轨迹\n# 论文使用数据集和训练资源\n- 数据集：QUARD benchmark，包含多种导航和全身操作任务\n- 训练资源：8B参数的MLLM模型，使用GPU资源进行训练\n# 论文使用的评估环境和评估指标\n- 评估环境：QUARD benchmark，分为Easy、Medium、Hard三个难度级别\n- 评估指标：\n - 时间每动作步（频率，Hz）\n - 成功率（对每个任务进行50次实验的平均值）\n - 未见过视觉元素(U_v)和未见过语言指令(U_l)的性能评估\n - 包含6种具体任务类型：Distinguish、Go to、Go avoid、Go through、Crawl、Unload",
    "summary_html": "<h1>论文研究单位</h1>\n<p>西湖大学MiLAB、浙江大学、北京邮电大学</p>\n<h1>论文概述</h1>\n<p>本文解决了在四足视觉-语言-动作(QUAR-VLA)任务中部署多模态大语言模型(MLLM)时固有的推理延迟挑战。研究发现，传统的参数缩减技术最终会在动作指令调优阶段损害语言基础模型的性能，因此不适合此目的。论文引入了一种名为QUART-Online的新型无延迟四足MLLM模型，旨在提高推理效率而不降低语言基础模型的性能。通过引入动作块离散化(ACD)，压缩原始动作表示空间，将连续动作值映射到更小的离散代表性向量集合，同时保留关键信息。随后，对MLLM进行微调，将视觉、语言和压缩动作集成到统一语义空间。实验结果表明，QUART-Online与现有MLLM系统协同工作，以50Hz频率实现实时推理，与底层控制器频率同步，显著提升各类任务成功率65%。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>发现传统参数缩减技术在动作指令调优阶段会损害语言基础模型性能</li><li>提出QUART-Online无延迟四足多模态大语言模型</li><li>引入动作块离散化(ACD)方法，有效压缩动作表示空间</li><li>将视觉、语言和压缩动作集成到统一语义空间的微调方法</li><li>实现50Hz实时推理频率，与底层控制器频率同步</li><li>在多个任务上平均成功率提升65%</li></ul>\n<h1>论文方法描述</h1>\n<p>QUART-Online包含两个核心组件：</p>\n<p>1. 动作块离散化(ACD)</p>\n<p> - 将序列连续动作值映射到更小的离散代表性向量集合</p>\n<p> - 使用多层量化方法，输入数据由每个量化器顺序量化，明确管理每个量化步骤的残差</p>\n<p> - 包含动作编码器：使用N_L层1D卷积在时间轴上编码动作步骤，确保时间压缩和步骤连贯性</p>\n<p> - 量化过程：使用可学习码本B∈R^(N_q×K×D)，其中包含N_q层码本，每层有K个码字</p>\n<p> - 反量化过程：从最后一层码本开始，逐层累积到第一层，重建原始动作表示</p>\n\n<p>2. 动作块对齐</p>\n<p> - 微调MLLM将视觉、语言和压缩动作集成到统一语义空间</p>\n<p> - 推理时MLLM先输出预测的压缩动作标记，再解码为连续轨迹供机器人执行</p>\n\n<p>整体流程：输入图像和语言指令→标记化→MLLM处理→输出压缩动作标记→反量化→连续动作轨迹</p>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>数据集：QUARD benchmark，包含多种导航和全身操作任务</li><li>训练资源：8B参数的MLLM模型，使用GPU资源进行训练</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：QUARD benchmark，分为Easy、Medium、Hard三个难度级别</li><li>评估指标：</li></ul>\n<p> - 时间每动作步（频率，Hz）</p>\n<p> - 成功率（对每个任务进行50次实验的平均值）</p>\n<p> - 未见过视觉元素(U_v)和未见过语言指令(U_l)的性能评估</p>\n<p> - 包含6种具体任务类型：Distinguish、Go to、Go avoid、Go through、Crawl、Unload</p>"
  },
  {
    "date": "2024-12-18",
    "title": "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models",
    "link": "http://arxiv.org/abs/2412.14058",
    "summary_markdown": "论文研究单位\n清华大学, 字节跳动研究, 中科院自动化所模式识别国家重点实验室, 上海交通大学, 新加坡国立大学\n\n论文概述\n论文探讨了构建视觉-语言-动作模型（VLAs）以实现通用机器人策略的关键设计因素。通过系统研究不同VLA架构、骨干网络选择、数据使用策略，回答了为何选择VLAs、如何构建VLAs、选择哪种骨干网络以及何时使用跨具身数据集等问题。论文提出了一个名为RoboVLMs的新框架，用于将VLMs转换为VLAs，并在多个基准测试中实现了最先进的性能。\n\n论文核心贡献点\n1. 系统分析了构建VLAs的四个关键问题：为何选择VLAs、如何构建VLAs、选择哪种VLM骨干网络以及何时使用跨具身数据集。\n2. 提出了RoboVLMs框架，一个灵活、易于使用的开源框架，支持将任何VLM轻松集成到VLA中，并允许自由组合各种设计选择。\n3. 通过超过600个不同的实验，验证了VLAs的有效性，并在三个模拟任务和真实世界实验中实现了最先进的性能。\n4. 开源了所有细节，包括代码、模型、数据集和工具包，以及详细的训练和评估方案。\n\n论文方法描述\n1. VLA结构研究：比较了四种VLA结构，包括单步建模、交错连续动作建模和策略头连续动作建模，以确定最佳结构。\n2. 骨干网络选择：研究了8种不同的VLM骨干网络，评估其在机器人操作任务中的适用性。\n3. 数据使用策略：探讨了预训练、微调和后训练三种数据使用策略，以确定何时使用跨具身数据集。\n4. RoboVLMs框架：提供了一个统一的框架，允许无缝集成任何VLM，并支持各种设计选择的组合。\n\n论文使用数据集和训练资源\n1. 数据集：CALVIN（多任务桌面操作）、SimplerEnv（真实到模拟环境）和自收集的真实世界机器人操作数据集（包含100个操作任务和74K轨迹）。\n2. 训练资源：使用了不同规模的VLM骨干网络（3B到9B参数），并在不同数据规模（10%、100%、500%）下进行了实验。\n\n论文使用的评估环境和评估指标\n1. 评估环境：\n - 模拟环境：CALVIN和SimplerEnv。\n - 真实世界环境：使用Kinova Gen3机器人臂和Robotiq 2F-85夹爪的真实机器人实验。\n2. 评估指标：\n - CALVIN：连续任务成功率（1至5个任务）和平均任务长度。\n - SimplerEnv：任务成功率。\n - 真实世界：平均成功率，包括简单设置、未见干扰物、未见目标对象、未见背景和新颖技能描述。",
    "summary_html": "<p>论文研究单位</p>\n<p>清华大学, 字节跳动研究, 中科院自动化所模式识别国家重点实验室, 上海交通大学, 新加坡国立大学</p>\n\n<p>论文概述</p>\n<p>论文探讨了构建视觉-语言-动作模型（VLAs）以实现通用机器人策略的关键设计因素。通过系统研究不同VLA架构、骨干网络选择、数据使用策略，回答了为何选择VLAs、如何构建VLAs、选择哪种骨干网络以及何时使用跨具身数据集等问题。论文提出了一个名为RoboVLMs的新框架，用于将VLMs转换为VLAs，并在多个基准测试中实现了最先进的性能。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>系统分析了构建VLAs的四个关键问题：为何选择VLAs、如何构建VLAs、选择哪种VLM骨干网络以及何时使用跨具身数据集。</li><li>提出了RoboVLMs框架，一个灵活、易于使用的开源框架，支持将任何VLM轻松集成到VLA中，并允许自由组合各种设计选择。</li><li>通过超过600个不同的实验，验证了VLAs的有效性，并在三个模拟任务和真实世界实验中实现了最先进的性能。</li><li>开源了所有细节，包括代码、模型、数据集和工具包，以及详细的训练和评估方案。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>VLA结构研究：比较了四种VLA结构，包括单步建模、交错连续动作建模和策略头连续动作建模，以确定最佳结构。</li><li>骨干网络选择：研究了8种不同的VLM骨干网络，评估其在机器人操作任务中的适用性。</li><li>数据使用策略：探讨了预训练、微调和后训练三种数据使用策略，以确定何时使用跨具身数据集。</li><li>RoboVLMs框架：提供了一个统一的框架，允许无缝集成任何VLM，并支持各种设计选择的组合。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：CALVIN（多任务桌面操作）、SimplerEnv（真实到模拟环境）和自收集的真实世界机器人操作数据集（包含100个操作任务和74K轨迹）。</li><li>训练资源：使用了不同规模的VLM骨干网络（3B到9B参数），并在不同数据规模（10%、100%、500%）下进行了实验。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 评估环境：</p>\n<p> - 模拟环境：CALVIN和SimplerEnv。</p>\n<p> - 真实世界环境：使用Kinova Gen3机器人臂和Robotiq 2F-85夹爪的真实机器人实验。</p>\n<p>2. 评估指标：</p>\n<p> - CALVIN：连续任务成功率（1至5个任务）和平均任务长度。</p>\n<p> - SimplerEnv：任务成功率。</p>\n<p> - 真实世界：平均成功率，包括简单设置、未见干扰物、未见目标对象、未见背景和新颖技能描述。</p>"
  },
  {
    "date": "2024-12-18",
    "title": "RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation",
    "link": "http://arxiv.org/abs/2412.13877",
    "summary_markdown": "论文研究单位\nBeijing Innovation Center of Humanoid Robotics, State Key Laboratory of Multimedia Information Processing (Peking University), and Beijing Academy of Artificial Intelligence.\n\n论文概述\n论文提出了RoboMIND，一个用于机器人操作的大规模、多形态智能规范数据集。该数据集通过人类遥操作收集，包含107k个演示轨迹，涵盖479个不同任务和96种物体类别。RoboMIND在统一的数据收集平台和标准化协议下构建，涉及四种不同的机器人形态：Franka Emika Panda、双灵巧手的X-Humannoid Tien Kung人形机器人、AgileX双臂机器人和UR5e。数据集还包括5k个真实世界失败演示，每个都附有详细原因，以及一个在Isaac Sim模拟器中复现真实世界任务的数字孪生环境。论文通过使用单任务模仿学习方法和视觉-语言-动作（VLA）大模型进行大量实验，验证了数据集的质量和多样性。\n\n论文核心贡献点\n1. 提出了RoboMIND，一个大规模、多形态的机器人操作数据集，包含107k个演示轨迹、479个任务、96个物体类别和38种操作技能。\n2. 数据集通过人类遥操作收集，并在统一平台和标准化协议下构建，确保数据一致性和可靠性。\n3. 涵盖四种机器人形态（单臂、双臂、人形），提供了多样化的机器人形态数据。\n4. 包含5k个失败案例轨迹，每个附带详细失败原因，支持策略学习中的失败反思和纠正。\n5. 创建了数字孪生环境，便于低成本收集额外训练数据和高效评估。\n6. 通过广泛的实验证明了数据集的有效性，包括单任务模仿学习和多任务VLA模型的评估。\n\n论文方法描述\n数据集构建方法：采用人类遥操作技术收集数据，针对不同机器人类型使用特定设备（如3D打印组件、辅助臂控制、动作捕捉服）。开发了一个智能数据平台，包含数据收集、存储、预处理、分类和标注五个主要功能模块。数据以标准化H5格式存储，包含多视图RGB-D数据、机器人本体状态信息、末端执行器状态信息和遥操作身体状态信息。数据预处理基于预定义标准进行质量保证检查，包括初始检查、详细检查和数据过滤与问题记录。数据分类采用以任务为中心的协议，每个任务由机器人类型、操作技能、涉及物体和场景描述四个关键组件定义。数据标注提供任务级语言描述和10k轨迹的帧级细粒度语言描述，结合Gemini自动生成和人工修正。\n\n论文使用数据集和训练资源\n数据集：RoboMIND，包含107k个演示轨迹，涉及479个任务和96个物体类别。训练资源：实验使用真实机器人设置，包括配备Intel RealSense D435i相机的Franka Emika Panda、配备Orbbec相机的Tien Kung人形机器人、配备Orbbec Astra相机的AgileX Cobot Magic V2.0，以及配备顶部Intel RealSense D435i相机的UR5e。数字孪生环境基于NVIDIA Isaac Sim构建。\n\n论文使用的评估环境和评估指标\n评估环境：真实世界的机器人实验设置，针对Franka使用顶部、左侧和右侧三个视角的相机，针对Tien Kung和AgileX使用内置摄像头，针对UR使用外部顶部相机。评估指标：任务成功率。每个模型进行十次测试，记录成功或失败情况，计算成功次数与总测试次数的比率作为成功率。",
    "summary_html": "<p>论文研究单位</p>\n<p>Beijing Innovation Center of Humanoid Robotics, State Key Laboratory of Multimedia Information Processing (Peking University), and Beijing Academy of Artificial Intelligence.</p>\n\n<p>论文概述</p>\n<p>论文提出了RoboMIND，一个用于机器人操作的大规模、多形态智能规范数据集。该数据集通过人类遥操作收集，包含107k个演示轨迹，涵盖479个不同任务和96种物体类别。RoboMIND在统一的数据收集平台和标准化协议下构建，涉及四种不同的机器人形态：Franka Emika Panda、双灵巧手的X-Humannoid Tien Kung人形机器人、AgileX双臂机器人和UR5e。数据集还包括5k个真实世界失败演示，每个都附有详细原因，以及一个在Isaac Sim模拟器中复现真实世界任务的数字孪生环境。论文通过使用单任务模仿学习方法和视觉-语言-动作（VLA）大模型进行大量实验，验证了数据集的质量和多样性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了RoboMIND，一个大规模、多形态的机器人操作数据集，包含107k个演示轨迹、479个任务、96个物体类别和38种操作技能。</li><li>数据集通过人类遥操作收集，并在统一平台和标准化协议下构建，确保数据一致性和可靠性。</li><li>涵盖四种机器人形态（单臂、双臂、人形），提供了多样化的机器人形态数据。</li><li>包含5k个失败案例轨迹，每个附带详细失败原因，支持策略学习中的失败反思和纠正。</li><li>创建了数字孪生环境，便于低成本收集额外训练数据和高效评估。</li><li>通过广泛的实验证明了数据集的有效性，包括单任务模仿学习和多任务VLA模型的评估。</li></ol>\n\n<p>论文方法描述</p>\n<p>数据集构建方法：采用人类遥操作技术收集数据，针对不同机器人类型使用特定设备（如3D打印组件、辅助臂控制、动作捕捉服）。开发了一个智能数据平台，包含数据收集、存储、预处理、分类和标注五个主要功能模块。数据以标准化H5格式存储，包含多视图RGB-D数据、机器人本体状态信息、末端执行器状态信息和遥操作身体状态信息。数据预处理基于预定义标准进行质量保证检查，包括初始检查、详细检查和数据过滤与问题记录。数据分类采用以任务为中心的协议，每个任务由机器人类型、操作技能、涉及物体和场景描述四个关键组件定义。数据标注提供任务级语言描述和10k轨迹的帧级细粒度语言描述，结合Gemini自动生成和人工修正。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：RoboMIND，包含107k个演示轨迹，涉及479个任务和96个物体类别。训练资源：实验使用真实机器人设置，包括配备Intel RealSense D435i相机的Franka Emika Panda、配备Orbbec相机的Tien Kung人形机器人、配备Orbbec Astra相机的AgileX Cobot Magic V2.0，以及配备顶部Intel RealSense D435i相机的UR5e。数字孪生环境基于NVIDIA Isaac Sim构建。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：真实世界的机器人实验设置，针对Franka使用顶部、左侧和右侧三个视角的相机，针对Tien Kung和AgileX使用内置摄像头，针对UR使用外部顶部相机。评估指标：任务成功率。每个模型进行十次测试，记录成功或失败情况，计算成功次数与总测试次数的比率作为成功率。</p>"
  },
  {
    "date": "2024-12-16",
    "title": "Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning",
    "link": "http://arxiv.org/abs/2412.11974",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-12-13",
    "title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies",
    "link": "http://arxiv.org/abs/2412.10345",
    "summary_markdown": "论文研究单位\nUniversity of Maryland, College Park; Microsoft Research; Capital One\n\n论文概述\n该论文提出了TraceVLA，一种通过视觉轨迹提示增强通用机器人策略时空感知能力的模型。该方法通过在图像上叠加机器人历史移动的视觉轨迹作为提示，让VLA模型更好地理解时空动态，从而在复杂操作任务中提升性能。作者开发了7B和4B两种规模的VLA模型，并在模拟和真实机器人任务上进行了广泛评估。\n\n论文核心贡献点\n- 提出视觉轨迹提示技术：通过点跟踪算法生成历史移动轨迹，并作为视觉提示叠加在原始图像上，为VLA模型提供时空记忆。\n- 构建数据集与模型：收集了150K条机器人轨迹数据，用于微调OpenVLA，并开发了基于Phi-3-Vision的紧凑4B模型。\n- 验证有效性：在模拟环境(SimplerEnv的137个配置)和真实机器人(WidowX的4个任务)上均达到SOTA性能，分别比OpenVLA提升10%和3.5倍。\n\n论文方法描述\n1. 视觉轨迹生成：\n - 使用Co-Tracker对历史图像序列进行密集点轨迹跟踪(K=40网格)。\n - 筛选活跃轨迹：计算点位移，保留移动总位移超过阈值κ的轨迹。\n - 随机采样M=5条轨迹，叠加在当前观测图像上作为视觉提示。\n\n2. 模型架构设计：\n - 双图像输入：同时输入原始图像和叠加轨迹的图像，用特殊分隔符连接。\n - 训练时采用dropout机制：以概率α替换轨迹图为原始图，提升鲁棒性。\n\n3. 训练与推理优化：\n - 训练：对OpenVLA和Phi-3-Vision进行5个epoch的微调。\n - 推理：稀疏查询Co-Tracker，每20步重计算密集轨迹，平衡效率与性能。\n\n论文使用数据集和训练资源\n- 数据集：BridgeData-v2, Google RT-1, Open X-Embodiment (970K轨迹)，以及自采WidowX数据(120条)，总计150K带轨迹标注的轨迹。\n- 训练资源：7B模型使用32×H100，batch size=4096，30个epoch；4B模型可在RTX4090/A5000上微调。\n- 超参数：时间窗口N=6，网格大小K=40，采样轨迹数M=5。\n\n论文使用的评估环境和评估指标\n- 仿真环境：SimplerEnv的Google机器人任务，包含视觉匹配(visual matching)和变体聚合(variant aggregation)两种设置，覆盖137个环境配置。\n- 真实环境：WidowX-250机器人，8个操作任务(含4个未见任务)，使用256×256 RGB摄像头。\n- 评估指标：任务成功率(%)，具体包括环境适应下的平均成功率，以及未见任务的泛化性能对比。",
    "summary_html": "<p>论文研究单位</p>\n<p>University of Maryland, College Park; Microsoft Research; Capital One</p>\n\n<p>论文概述</p>\n<p>该论文提出了TraceVLA，一种通过视觉轨迹提示增强通用机器人策略时空感知能力的模型。该方法通过在图像上叠加机器人历史移动的视觉轨迹作为提示，让VLA模型更好地理解时空动态，从而在复杂操作任务中提升性能。作者开发了7B和4B两种规模的VLA模型，并在模拟和真实机器人任务上进行了广泛评估。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提出视觉轨迹提示技术：通过点跟踪算法生成历史移动轨迹，并作为视觉提示叠加在原始图像上，为VLA模型提供时空记忆。</li><li>构建数据集与模型：收集了150K条机器人轨迹数据，用于微调OpenVLA，并开发了基于Phi-3-Vision的紧凑4B模型。</li><li>验证有效性：在模拟环境(SimplerEnv的137个配置)和真实机器人(WidowX的4个任务)上均达到SOTA性能，分别比OpenVLA提升10%和3.5倍。</li></ul>\n\n<p>论文方法描述</p>\n<p>1. 视觉轨迹生成：</p>\n<p> - 使用Co-Tracker对历史图像序列进行密集点轨迹跟踪(K=40网格)。</p>\n<p> - 筛选活跃轨迹：计算点位移，保留移动总位移超过阈值κ的轨迹。</p>\n<p> - 随机采样M=5条轨迹，叠加在当前观测图像上作为视觉提示。</p>\n\n<p>2. 模型架构设计：</p>\n<p> - 双图像输入：同时输入原始图像和叠加轨迹的图像，用特殊分隔符连接。</p>\n<p> - 训练时采用dropout机制：以概率α替换轨迹图为原始图，提升鲁棒性。</p>\n\n<p>3. 训练与推理优化：</p>\n<p> - 训练：对OpenVLA和Phi-3-Vision进行5个epoch的微调。</p>\n<p> - 推理：稀疏查询Co-Tracker，每20步重计算密集轨迹，平衡效率与性能。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：BridgeData-v2, Google RT-1, Open X-Embodiment (970K轨迹)，以及自采WidowX数据(120条)，总计150K带轨迹标注的轨迹。</li><li>训练资源：7B模型使用32×H100，batch size=4096，30个epoch；4B模型可在RTX4090/A5000上微调。</li><li>超参数：时间窗口N=6，网格大小K=40，采样轨迹数M=5。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>仿真环境：SimplerEnv的Google机器人任务，包含视觉匹配(visual matching)和变体聚合(variant aggregation)两种设置，覆盖137个环境配置。</li><li>真实环境：WidowX-250机器人，8个操作任务(含4个未见任务)，使用256×256 RGB摄像头。</li><li>评估指标：任务成功率(%)，具体包括环境适应下的平均成功率，以及未见任务的泛化性能对比。</li></ul>"
  },
  {
    "date": "2024-12-09",
    "title": "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks",
    "link": "http://arxiv.org/abs/2412.06224",
    "summary_markdown": "### 论文研究单位\n北京大学计算机学院CFCS、Galbot、北京智源研究院\n### 论文概述\nUni-NaVid是一个基于视频的视觉-语言-动作（VLA）模型，旨在统一多种具身导航任务。模型仅需接收自然语言指令和RGB视频流作为输入，端到端输出底层机器人动作，支持连续环境中的高效导航。通过收集360万条多任务导航样本并进行训练，Uni-NaVid在多个导航基准测试中实现了最先进性能，并在真实世界实验中展示了有效性和泛化能力。\n### 论文核心贡献点\n1. 提出首个统一处理多种具身导航任务（包括VLN、ObjectNav、EQA和Human Following）的VLA模型，仅需RGB视频输入。\n2. 设计在线视觉token合并机制，按时间分组压缩视觉信息（当前、短期、长期token），推理速度提升至5 Hz，满足实时部署需求。\n3. 引入前瞻性动作规划，一次性生成未来4步动作序列，减少推理延迟。\n4. 收集并构建包含360万导航样本和230万互联网VQA/视频字幕样本的多任务数据集，促进跨任务学习协同。\n5. 在模拟基准和真实机器人平台上实现SOTA性能，验证模型的泛化能力。\n### 论文方法描述\n模型架构分为三部分：\n1. **观察编码**：使用EVA-CLIP视觉编码器将每帧视频转换为256个视觉token（维度C）。\n2. **在线视觉token合并**：\n - **分组**：按时间分为当前token（帧T）、短期token（帧[T-B, T)）、长期token（帧[1, T-B)），其中B=64。\n - **空间压缩**：使用GridPool操作，当前token压缩率α_curr=2（输出64个token），短期token α_short=8（输出4个token），长期token α_long=16（输出1个token）。\n - **在线更新**：新帧到达时，仅处理最新帧和最旧短期帧，重用历史token。\n - **长期token优化**：基于余弦相似度（阈值τ=0.95）合并相似长期token，防止线性增长，合并公式为加权平均。\n3. **动作规划**：视觉token经MLP投影后与语言token拼接，输入LLM生成4个动作（FORWARD、TURN-LEFT、TURN-RIGHT、STOP），动作空间兼容连续环境（30度旋转或25cm移动）。\n### 论文使用数据集和训练资源\n- **导航数据集**：收集360万条样本，覆盖四个任务：\n - VLN-CE（R2R/RxR连续环境版）\n - ObjectNav（HM3D数据集）\n - EQA（MP3D-EQA）\n - Human Following（自建Habitat 3.0基准）\n- **辅助数据集**：230万互联网视频数据（VQA和视频字幕），增强场景理解和sim-to-real泛化。\n- **训练资源**：使用多GPU训练（具体型号未明确），训练策略联合优化多任务损失，未公开详细计算资源。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - **模拟环境**：Habitat仿真器，包含VLN-CE、HM3D、MP3D-EQA等基准。\n - **真实世界**：配备RGB摄像头的机器人平台，部署非阻塞导航系统。\n- **评估指标**：\n - **VLN**：SPL（路径长度加权成功率）、nDTW（标准化轨迹扭曲）。\n - **ObjectNav**：成功率（Success Rate）、SPL。\n - **EQA**：准确率（Accuracy）。\n - **Human Following**：距离保持误差（Distance Maintenance Error）。\n - **整体**：任务特定指标和推理速度（目标5 Hz）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学计算机学院CFCS、Galbot、北京智源研究院</p>\n<h3>论文概述</h3>\n<p>Uni-NaVid是一个基于视频的视觉-语言-动作（VLA）模型，旨在统一多种具身导航任务。模型仅需接收自然语言指令和RGB视频流作为输入，端到端输出底层机器人动作，支持连续环境中的高效导航。通过收集360万条多任务导航样本并进行训练，Uni-NaVid在多个导航基准测试中实现了最先进性能，并在真实世界实验中展示了有效性和泛化能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出首个统一处理多种具身导航任务（包括VLN、ObjectNav、EQA和Human Following）的VLA模型，仅需RGB视频输入。</li><li>设计在线视觉token合并机制，按时间分组压缩视觉信息（当前、短期、长期token），推理速度提升至5 Hz，满足实时部署需求。</li><li>引入前瞻性动作规划，一次性生成未来4步动作序列，减少推理延迟。</li><li>收集并构建包含360万导航样本和230万互联网VQA/视频字幕样本的多任务数据集，促进跨任务学习协同。</li><li>在模拟基准和真实机器人平台上实现SOTA性能，验证模型的泛化能力。</li></ol>\n<h3>论文方法描述</h3>\n<p>模型架构分为三部分：</p>\n<ol><li><strong>观察编码</strong>：使用EVA-CLIP视觉编码器将每帧视频转换为256个视觉token（维度C）。</li><li><strong>在线视觉token合并</strong>：</li></ol>\n<p> - <strong>分组</strong>：按时间分为当前token（帧T）、短期token（帧[T-B, T)）、长期token（帧[1, T-B)），其中B=64。</p>\n<p> - <strong>空间压缩</strong>：使用GridPool操作，当前token压缩率α_curr=2（输出64个token），短期token α_short=8（输出4个token），长期token α_long=16（输出1个token）。</p>\n<p> - <strong>在线更新</strong>：新帧到达时，仅处理最新帧和最旧短期帧，重用历史token。</p>\n<p> - <strong>长期token优化</strong>：基于余弦相似度（阈值τ=0.95）合并相似长期token，防止线性增长，合并公式为加权平均。</p>\n<p>3. <strong>动作规划</strong>：视觉token经MLP投影后与语言token拼接，输入LLM生成4个动作（FORWARD、TURN-LEFT、TURN-RIGHT、STOP），动作空间兼容连续环境（30度旋转或25cm移动）。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>导航数据集</strong>：收集360万条样本，覆盖四个任务：</li></ul>\n<p> - VLN-CE（R2R/RxR连续环境版）</p>\n<p> - ObjectNav（HM3D数据集）</p>\n<p> - EQA（MP3D-EQA）</p>\n<p> - Human Following（自建Habitat 3.0基准）</p>\n<ul><li><strong>辅助数据集</strong>：230万互联网视频数据（VQA和视频字幕），增强场景理解和sim-to-real泛化。</li><li><strong>训练资源</strong>：使用多GPU训练（具体型号未明确），训练策略联合优化多任务损失，未公开详细计算资源。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - <strong>模拟环境</strong>：Habitat仿真器，包含VLN-CE、HM3D、MP3D-EQA等基准。</p>\n<p> - <strong>真实世界</strong>：配备RGB摄像头的机器人平台，部署非阻塞导航系统。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>VLN</strong>：SPL（路径长度加权成功率）、nDTW（标准化轨迹扭曲）。</p>\n<p> - <strong>ObjectNav</strong>：成功率（Success Rate）、SPL。</p>\n<p> - <strong>EQA</strong>：准确率（Accuracy）。</p>\n<p> - <strong>Human Following</strong>：距离保持误差（Distance Maintenance Error）。</p>\n<p> - <strong>整体</strong>：任务特定指标和推理速度（目标5 Hz）。</p>"
  },
  {
    "date": "2024-12-05",
    "title": "NaVILA: Legged Robot Vision-Language-Action Model for Navigation",
    "link": "http://arxiv.org/abs/2412.04453",
    "summary_markdown": "### 论文研究单位\nUC San Diego, USC, NVIDIA\n### 论文概述\nNaVILA是一个用于腿式机器人导航的视觉-语言-动作模型，旨在解决视觉-语言导航问题。该模型采用两级框架：高级视觉-语言-动作（VLA）模型生成包含空间信息的中级动作（如“向前移动75cm”），低级视觉运动策略则执行这些动作。这种方法使机器人能够通过人类语言指令导航复杂场景，并在模拟和真实环境中实现了高性能，显著提升了现有基准的导航成功率。\n### 论文核心贡献点\n- 提出了一个两级框架NaVILA，统一了VLA模型与运动技能，通过中级语言动作桥接高级推理和低级控制。\n- VLA模型输出自然语言形式的中级动作，而不是直接预测低级关节动作，增强了跨机器人平台的可移植性。\n- 利用YouTube人类旅行视频数据训练模型，首次实现直接从真实视频中学习连续环境导航，提升泛化能力。\n- 开发了新基准VLN-CE-Isaac，基于Isaac Sim，包含更现实的场景、低级控制和物理交互。\n- 在真实机器人部署中取得88%的整体成功率，其中复杂指令成功率达75%，展示了在挑战环境中的鲁棒性。\n### 论文方法描述\n- **高级VLA模型**：基于VILA视觉-语言模型，处理单视图图像和历史帧，生成导航动作。设计特定导航提示，区分当前观察和历史帧，使用文本描述内存信息。采用监督微调（SFT）混合数据，包括仿真数据、真实视频数据和一般VQA数据。\n- **从人类视频学习**：处理2000个YouTube旅行视频，通过熵采样生成20K轨迹，使用MASt3R估计相机姿态，提取步进动作，并利用VLM和LLM生成自然语言指令。\n- **低级运动策略**：使用PPO算法训练视觉运动策略，输入为LiDAR点云生成的身高图和本体感觉数据（如关节位置、速度），输出为期望关节位置。策略在Isaac Sim中通过Isaac Lab以单阶段方式训练，避免策略蒸馏，提高效率。身高图处理包括体素网格化和最大滤波，以增强环境感知。\n- **训练推理**：VLA模型在VILA第二阶段基础上微调一个epoch，所有模块未冻结；推理时使用正则表达式解析器提取动作类型和参数。运动策略以高频率运行，支持实时控制。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 仿真导航数据：R2R-CE和RxR-CE，使用Habitat模拟器生成动作序列。\n - 真实视频数据：2000个YouTube人类旅行视频，处理成20K轨迹，通过MASt3R估计姿态。\n - 辅助导航数据：EnvDrop增强指令、ScanQA 3D扫描QA对、导航轨迹摘要任务。\n - 一般VQA数据：来自多个通用视觉问答数据集，以保持模型泛化能力。\n- **训练资源**：\n - VLA训练：基于VILA模型，在混合数据上微调一个epoch，使用标准GPU资源（具体型号未提及）。\n - 运动策略训练：在Isaac Sim中使用Isaac Lab，采用PPO算法，在RTX 4090 GPU上达到60K FPS训练吞吐量。\n - 推理：VLA模型运行在低频率，运动策略实时运行，整体系统部署在机器人硬件上。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 经典VLN基准：R2R-CE和RxR-CE的Val-Unseen分割。\n - 新基准：VLN-CE-Isaac，基于Isaac Sim，包含详细机器人关节运动和物理交互。\n - 真实世界场景：在Unitree Go2、Unitree H1和Booster T1机器人上测试，包括实验室、住宅和户外不平地形。\n- **评估指标**：\n - 导航误差（NE，数值越低越好）。\n - 目标成功率（OS，数值越高越好）。\n - 成功率（SR，数值越高越好）。\n - 路径长度加权成功率（SPL，数值越高越好）。\n - nDTW（用于RxR-CE，数值越高越好）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>UC San Diego, USC, NVIDIA</p>\n<h3>论文概述</h3>\n<p>NaVILA是一个用于腿式机器人导航的视觉-语言-动作模型，旨在解决视觉-语言导航问题。该模型采用两级框架：高级视觉-语言-动作（VLA）模型生成包含空间信息的中级动作（如“向前移动75cm”），低级视觉运动策略则执行这些动作。这种方法使机器人能够通过人类语言指令导航复杂场景，并在模拟和真实环境中实现了高性能，显著提升了现有基准的导航成功率。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一个两级框架NaVILA，统一了VLA模型与运动技能，通过中级语言动作桥接高级推理和低级控制。</li><li>VLA模型输出自然语言形式的中级动作，而不是直接预测低级关节动作，增强了跨机器人平台的可移植性。</li><li>利用YouTube人类旅行视频数据训练模型，首次实现直接从真实视频中学习连续环境导航，提升泛化能力。</li><li>开发了新基准VLN-CE-Isaac，基于Isaac Sim，包含更现实的场景、低级控制和物理交互。</li><li>在真实机器人部署中取得88%的整体成功率，其中复杂指令成功率达75%，展示了在挑战环境中的鲁棒性。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>高级VLA模型</strong>：基于VILA视觉-语言模型，处理单视图图像和历史帧，生成导航动作。设计特定导航提示，区分当前观察和历史帧，使用文本描述内存信息。采用监督微调（SFT）混合数据，包括仿真数据、真实视频数据和一般VQA数据。</li><li><strong>从人类视频学习</strong>：处理2000个YouTube旅行视频，通过熵采样生成20K轨迹，使用MASt3R估计相机姿态，提取步进动作，并利用VLM和LLM生成自然语言指令。</li><li><strong>低级运动策略</strong>：使用PPO算法训练视觉运动策略，输入为LiDAR点云生成的身高图和本体感觉数据（如关节位置、速度），输出为期望关节位置。策略在Isaac Sim中通过Isaac Lab以单阶段方式训练，避免策略蒸馏，提高效率。身高图处理包括体素网格化和最大滤波，以增强环境感知。</li><li><strong>训练推理</strong>：VLA模型在VILA第二阶段基础上微调一个epoch，所有模块未冻结；推理时使用正则表达式解析器提取动作类型和参数。运动策略以高频率运行，支持实时控制。</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 仿真导航数据：R2R-CE和RxR-CE，使用Habitat模拟器生成动作序列。</p>\n<p> - 真实视频数据：2000个YouTube人类旅行视频，处理成20K轨迹，通过MASt3R估计姿态。</p>\n<p> - 辅助导航数据：EnvDrop增强指令、ScanQA 3D扫描QA对、导航轨迹摘要任务。</p>\n<p> - 一般VQA数据：来自多个通用视觉问答数据集，以保持模型泛化能力。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - VLA训练：基于VILA模型，在混合数据上微调一个epoch，使用标准GPU资源（具体型号未提及）。</p>\n<p> - 运动策略训练：在Isaac Sim中使用Isaac Lab，采用PPO算法，在RTX 4090 GPU上达到60K FPS训练吞吐量。</p>\n<p> - 推理：VLA模型运行在低频率，运动策略实时运行，整体系统部署在机器人硬件上。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 经典VLN基准：R2R-CE和RxR-CE的Val-Unseen分割。</p>\n<p> - 新基准：VLN-CE-Isaac，基于Isaac Sim，包含详细机器人关节运动和物理交互。</p>\n<p> - 真实世界场景：在Unitree Go2、Unitree H1和Booster T1机器人上测试，包括实验室、住宅和户外不平地形。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 导航误差（NE，数值越低越好）。</p>\n<p> - 目标成功率（OS，数值越高越好）。</p>\n<p> - 成功率（SR，数值越高越好）。</p>\n<p> - 路径长度加权成功率（SPL，数值越高越好）。</p>\n<p> - nDTW（用于RxR-CE，数值越高越好）。</p>"
  },
  {
    "date": "2024-12-02",
    "title": "Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control",
    "link": "http://arxiv.org/abs/2412.01034",
    "summary_markdown": "论文研究单位\nHanyang University, Hyundai Motor Company\n\n论文概述\n本文提出了一种量化感知模仿学习框架QAIL，用于资源高效的机器人控制。现有的基于深度神经网络的策略模型在机器人操作和自动驾驶中表现出色，但计算成本高且内存需求大。虽然量化可以降低这些成本，但直接量化会导致性能显著下降，因为量化误差在动作序列中会累积，影响决策质量。本文通过将量化集成到模仿学习的微调过程中，并结合量化鲁棒行为克隆技术，使低精度策略能够在资源受限的设备上保持高性能，同时实现显著的加速和能效提升。\n\n论文核心贡献点\n1. 提出了量化感知模仿学习(QAIL)框架，将量化操作集成到模仿学习的微调过程中，增强策略对低比特精度误差的鲁棒性。\n2. 设计了量化鲁棒行为克隆(QBC)方法，通过最小化量化策略与全精度策略之间的动作分布差异来减少累积误差。\n3. 开发了加权QBC(wQBC)变体，基于状态重要性对损失进行加权，特别适用于长时程任务。\n4. 在机器人操作、自动驾驶和物理模拟三个领域验证了方法有效性，实现了接近全精度模型的性能，同时获得2.5×至3.7×的加速和显著的能耗降低。\n\n论文方法描述\n方法由两个主要组件构成：\n1. QAIL：结合专家演示数据集(D_E)和全精度策略生成数据(D_FP)，在量化感知设置下通过模仿学习目标训练量化策略。使用STE(直通估计器)处理量化操作的梯度。\n2. QBC：引入KL散度损失函数，对齐量化策略π^q_θ和全精度策略π^FP的动作分布，减少序列决策中的量化误差累积。总损失函数为L^total = L^QAIL + λL^QBC。\n3. wQBC：通过基于扰动的显著性分数评估状态重要性，对QBC损失进行加权，使模型关注关键决策状态。\n\n论文使用数据集和训练资源\n数据集：\n- 机器人操作：LIBERO基准(包括LIBERO-Spatial, LIBERO-Object等)\n- 自动驾驶：NoCrash基准(包括NoCrash-Busy, NoCrash-Dense)\n- 物理模拟：D4RL基准(MuJoCo环境)\n训练资源：NVIDIA A100 GPU(80GB)和NVIDIA RTX 4090 GPU(24GB)\n\n论文使用的评估环境和评估指标\n评估环境：\n- 机器人操作：LIBERO模拟环境\n- 自动驾驶：CARLA模拟器中的NoCrash基准\n- 物理模拟：MuJoCo控制环境\n评估指标：\n- 机器人操作：任务成功率\n- 自动驾驶：驾驶得分、碰撞率、路线完成率\n- 物理模拟：归一化平均回报\n- 效率指标：推理延迟、吞吐量、能耗\n- 硬件平台：Jetson Orin边缘GPU、低端GPU、设备CPU",
    "summary_html": "<p>论文研究单位</p>\n<p>Hanyang University, Hyundai Motor Company</p>\n\n<p>论文概述</p>\n<p>本文提出了一种量化感知模仿学习框架QAIL，用于资源高效的机器人控制。现有的基于深度神经网络的策略模型在机器人操作和自动驾驶中表现出色，但计算成本高且内存需求大。虽然量化可以降低这些成本，但直接量化会导致性能显著下降，因为量化误差在动作序列中会累积，影响决策质量。本文通过将量化集成到模仿学习的微调过程中，并结合量化鲁棒行为克隆技术，使低精度策略能够在资源受限的设备上保持高性能，同时实现显著的加速和能效提升。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了量化感知模仿学习(QAIL)框架，将量化操作集成到模仿学习的微调过程中，增强策略对低比特精度误差的鲁棒性。</li><li>设计了量化鲁棒行为克隆(QBC)方法，通过最小化量化策略与全精度策略之间的动作分布差异来减少累积误差。</li><li>开发了加权QBC(wQBC)变体，基于状态重要性对损失进行加权，特别适用于长时程任务。</li><li>在机器人操作、自动驾驶和物理模拟三个领域验证了方法有效性，实现了接近全精度模型的性能，同时获得2.5×至3.7×的加速和显著的能耗降低。</li></ol>\n\n<p>论文方法描述</p>\n<p>方法由两个主要组件构成：</p>\n<ol><li>QAIL：结合专家演示数据集(D_E)和全精度策略生成数据(D_FP)，在量化感知设置下通过模仿学习目标训练量化策略。使用STE(直通估计器)处理量化操作的梯度。</li><li>QBC：引入KL散度损失函数，对齐量化策略π^q_θ和全精度策略π^FP的动作分布，减少序列决策中的量化误差累积。总损失函数为L^total = L^QAIL + λL^QBC。</li><li>wQBC：通过基于扰动的显著性分数评估状态重要性，对QBC损失进行加权，使模型关注关键决策状态。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<ul><li>机器人操作：LIBERO基准(包括LIBERO-Spatial, LIBERO-Object等)</li><li>自动驾驶：NoCrash基准(包括NoCrash-Busy, NoCrash-Dense)</li><li>物理模拟：D4RL基准(MuJoCo环境)</li></ul>\n<p>训练资源：NVIDIA A100 GPU(80GB)和NVIDIA RTX 4090 GPU(24GB)</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<ul><li>机器人操作：LIBERO模拟环境</li><li>自动驾驶：CARLA模拟器中的NoCrash基准</li><li>物理模拟：MuJoCo控制环境</li></ul>\n<p>评估指标：</p>\n<ul><li>机器人操作：任务成功率</li><li>自动驾驶：驾驶得分、碰撞率、路线完成率</li><li>物理模拟：归一化平均回报</li><li>效率指标：推理延迟、吞吐量、能耗</li><li>硬件平台：Jetson Orin边缘GPU、低端GPU、设备CPU</li></ul>"
  },
  {
    "date": "2024-11-29",
    "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
    "link": "http://arxiv.org/abs/2412.00174",
    "summary_markdown": "论文研究单位\nSenseTime Research, S-Lab, Nanyang Technological University\n\n论文概述\n论文介绍了SOLAMI，这是一个用于与3D自主角色进行沉浸式交互的首个端到端社交视觉-语言-动作（VLA）建模框架。该框架旨在赋予3D角色社交智能，使其能够感知、理解用户的多模态输入（语音和动作）并生成相应的多模态响应（语音和动作）。SOLAMI从三个方面构建了3D自主角色：1）社交VLA架构：一个统一的框架，用于根据用户输入生成多模态响应；2）交互式多模态数据：一个名为SynMSI的合成数据集，以解决数据稀缺问题；3）沉浸式VR界面：一个允许用户与角色进行沉浸式交互的VR系统。广泛的定量实验和用户研究表明，该框架能够产生更精确、自然且符合用户期望的角色响应，同时具有更低的延迟。\n\n论文核心贡献点\n1. 一种新的VLA架构，用于建模角色的行为系统以实现沉浸式社交交互。\n2. 一个专用的合成流水线，能够利用现有的运动数据集自动生成大规模的多模态交互数据SynMSI。\n3. 一个沉浸式VR接口，供用户通过语音和动作与各种角色进行交互。\n\n论文方法描述\nSOLAMI是一个基于仅解码器LLM骨干网络的端到端社交VLA模型。\n- **架构**：用户的语音和动作首先被转换为离散的语音和动作标记。然后，LLM基于用户的输入标记和角色设置预测角色的输出响应标记。生成的标记随后被解码为相应的语音和动作。模型使用特殊标记来分隔不同模态序列，并以多轮对话的方式进行交互。运动表示采用SMPL-X关节旋转，并为身体、手部和相对变换设计了独立的VQ-VAE量化器以实现更高的重建精度。语音则使用SpeechTokenizer将语音解构为语义标记，并支持通过短语音样本进行实例语音克隆。\n- **训练**：采用三阶段训练策略。第一阶段训练运动量化器，然后冻结。第二阶段进行多任务预训练，以实现运动与文本、语音与文本之间的模态对齐。第三阶段使用合成的多模态交互数据集进行指令微调，使模型能够处理长序列、多轮对话，该阶段采用对响应的下一个token预测进行监督，并比较了全参数微调和LoRA微调方法。\n\n论文使用数据集和训练资源\n- **数据集**：为解决数据稀缺问题，论文提出了SynMSI数据集，其生成流程包括：1）从网络平台收集5.3K个话题；2）使用GPT-4o基于话题和角色设置生成多轮对话文本脚本；3）利用精心策划的运动数据库（包含HumanML3D, Inter-X, DLP-MoCap等，共46K个运动-文本对）通过文本嵌入检索最相关的运动；4）根据检索到的运动优化语音脚本，并使用TTS/语音克隆技术生成角色特定的语音。最终获得了6.3K个多轮多模态对话项。\n- **训练资源**：模型在2个NVIDIA H800 GPU上进行训练和推理。\n\n论文使用的评估环境和评估指标\n- **评估环境**：开发了一个VR接口进行评估。前端使用Oculus Quest 3头显，其内置的全身追踪系统用于捕捉用户动作，并通过麦克风捕捉语音。后端由2个H800 GPU驱动的服务器构成，负责运行SOLAMI及基线模型。用户的动作被重定向到SMPL-X模型，生成的角色面部动画由UniTalker方法驱动，最终渲染到3D角色模型上。实验还包括定量评估和VR用户研究。\n- **评估指标**：\n - **运动指标**：FID (Frechet Inception Distance)、Diversity、PA-MPJPE (Procrustes Aligned Mean Per Joint Position Error)、Angle Error。\n - **语音指标**：VC Similarity (Voice Cloning Similarity)、WER (Word Error Rate)。\n - **效率指标**：Inference Time (推理时间)、Latency (延迟)。\n - **用户研究**：通过VR界面进行定性评估，以衡量交互的自然度、精确度和用户满意度。",
    "summary_html": "<p>论文研究单位</p>\n<p>SenseTime Research, S-Lab, Nanyang Technological University</p>\n\n<p>论文概述</p>\n<p>论文介绍了SOLAMI，这是一个用于与3D自主角色进行沉浸式交互的首个端到端社交视觉-语言-动作（VLA）建模框架。该框架旨在赋予3D角色社交智能，使其能够感知、理解用户的多模态输入（语音和动作）并生成相应的多模态响应（语音和动作）。SOLAMI从三个方面构建了3D自主角色：1）社交VLA架构：一个统一的框架，用于根据用户输入生成多模态响应；2）交互式多模态数据：一个名为SynMSI的合成数据集，以解决数据稀缺问题；3）沉浸式VR界面：一个允许用户与角色进行沉浸式交互的VR系统。广泛的定量实验和用户研究表明，该框架能够产生更精确、自然且符合用户期望的角色响应，同时具有更低的延迟。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>一种新的VLA架构，用于建模角色的行为系统以实现沉浸式社交交互。</li><li>一个专用的合成流水线，能够利用现有的运动数据集自动生成大规模的多模态交互数据SynMSI。</li><li>一个沉浸式VR接口，供用户通过语音和动作与各种角色进行交互。</li></ol>\n\n<p>论文方法描述</p>\n<p>SOLAMI是一个基于仅解码器LLM骨干网络的端到端社交VLA模型。</p>\n<ul><li><strong>架构</strong>：用户的语音和动作首先被转换为离散的语音和动作标记。然后，LLM基于用户的输入标记和角色设置预测角色的输出响应标记。生成的标记随后被解码为相应的语音和动作。模型使用特殊标记来分隔不同模态序列，并以多轮对话的方式进行交互。运动表示采用SMPL-X关节旋转，并为身体、手部和相对变换设计了独立的VQ-VAE量化器以实现更高的重建精度。语音则使用SpeechTokenizer将语音解构为语义标记，并支持通过短语音样本进行实例语音克隆。</li><li><strong>训练</strong>：采用三阶段训练策略。第一阶段训练运动量化器，然后冻结。第二阶段进行多任务预训练，以实现运动与文本、语音与文本之间的模态对齐。第三阶段使用合成的多模态交互数据集进行指令微调，使模型能够处理长序列、多轮对话，该阶段采用对响应的下一个token预测进行监督，并比较了全参数微调和LoRA微调方法。</li></ul>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li><strong>数据集</strong>：为解决数据稀缺问题，论文提出了SynMSI数据集，其生成流程包括：1）从网络平台收集5.3K个话题；2）使用GPT-4o基于话题和角色设置生成多轮对话文本脚本；3）利用精心策划的运动数据库（包含HumanML3D, Inter-X, DLP-MoCap等，共46K个运动-文本对）通过文本嵌入检索最相关的运动；4）根据检索到的运动优化语音脚本，并使用TTS/语音克隆技术生成角色特定的语音。最终获得了6.3K个多轮多模态对话项。</li><li><strong>训练资源</strong>：模型在2个NVIDIA H800 GPU上进行训练和推理。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li><strong>评估环境</strong>：开发了一个VR接口进行评估。前端使用Oculus Quest 3头显，其内置的全身追踪系统用于捕捉用户动作，并通过麦克风捕捉语音。后端由2个H800 GPU驱动的服务器构成，负责运行SOLAMI及基线模型。用户的动作被重定向到SMPL-X模型，生成的角色面部动画由UniTalker方法驱动，最终渲染到3D角色模型上。实验还包括定量评估和VR用户研究。</li><li><strong>评估指标</strong>：</li></ul>\n<p> - <strong>运动指标</strong>：FID (Frechet Inception Distance)、Diversity、PA-MPJPE (Procrustes Aligned Mean Per Joint Position Error)、Angle Error。</p>\n<p> - <strong>语音指标</strong>：VC Similarity (Voice Cloning Similarity)、WER (Word Error Rate)。</p>\n<p> - <strong>效率指标</strong>：Inference Time (推理时间)、Latency (延迟)。</p>\n<p> - <strong>用户研究</strong>：通过VR界面进行定性评估，以衡量交互的自然度、精确度和用户满意度。</p>"
  },
  {
    "date": "2024-11-29",
    "title": "RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World",
    "link": "http://arxiv.org/abs/2412.00171",
    "summary_markdown": "论文研究单位\n未在提供的HTML原文中找到相关信息。\n\n论文概述\n未在提供的HTML原文中找到相关信息。原文仅包含一个工作流程图，展示了从真实机器人收集数据到模型评估的步骤。\n\n论文核心贡献点\n未在提供的HTML原文中找到相关信息。\n\n论文方法描述\n根据原文中的流程图，论文方法包含一个循环流程：1. 在真实机器人上收集示范数据；2. 数据处理；3. 数据标注；4. 模型训练；5. 在真实机器人上进行评估。\n\n论文使用数据集和训练资源\n未在提供的HTML原文中找到相关信息。\n\n论文使用的评估环境和评估指标\n根据原文中的流程图，评估在真实机器人上进行。具体的评估指标未在原文中提及。",
    "summary_html": "<p>论文研究单位</p>\n<p>未在提供的HTML原文中找到相关信息。</p>\n\n<p>论文概述</p>\n<p>未在提供的HTML原文中找到相关信息。原文仅包含一个工作流程图，展示了从真实机器人收集数据到模型评估的步骤。</p>\n\n<p>论文核心贡献点</p>\n<p>未在提供的HTML原文中找到相关信息。</p>\n\n<p>论文方法描述</p>\n<p>根据原文中的流程图，论文方法包含一个循环流程：1. 在真实机器人上收集示范数据；2. 数据处理；3. 数据标注；4. 模型训练；5. 在真实机器人上进行评估。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>未在提供的HTML原文中找到相关信息。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>根据原文中的流程图，评估在真实机器人上进行。具体的评估指标未在原文中提及。</p>"
  },
  {
    "date": "2024-11-29",
    "title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2411.19650",
    "summary_markdown": "### 论文研究单位\n清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。\n### 论文概述\n该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。\n### 论文核心贡献点\n- 提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。\n- 系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。\n- 引入了一种简单而有效的自适应动作集成算法，用于时间融合。\n- 在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。\n### 论文方法描述\n模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。\n### 论文使用数据集和训练资源\n- 数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。\n- 预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。\n- 微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。\n- 训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。\n### 论文使用的评估环境和评估指标\n- 评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。\n- 评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。</think>\n### 论文研究单位\n清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。\n### 论文概述\n该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。\n### 论文核心贡献点\n- 提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。\n- 系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。\n- 引入了一种简单而有效的自适应动作集成算法，用于时间融合。\n- 在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。\n### 论文方法描述\n模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。\n### 论文使用数据集和训练资源\n- 数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。\n- 预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。\n- 微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。\n- 训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。\n### 论文使用的评估环境和评估指标\n- 评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。\n- 评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。</p>\n<h3>论文概述</h3>\n<p>该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。</li><li>系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。</li><li>引入了一种简单而有效的自适应动作集成算法，用于时间融合。</li><li>在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<p>模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。</li><li>预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。</li><li>微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。</li><li>训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。</li><li>评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。</think></li></ul>\n<h3>论文研究单位</h3>\n<p>清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。</p>\n<h3>论文概述</h3>\n<p>该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。</li><li>系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。</li><li>引入了一种简单而有效的自适应动作集成算法，用于时间融合。</li><li>在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。</li></ul>\n<h3>论文方法描述</h3>\n<p>模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。</li><li>预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。</li><li>微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。</li><li>训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。</li><li>评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。</li></ul>"
  },
  {
    "date": "2024-11-28",
    "title": "GRAPE: Generalizing Robot Policy via Preference Alignment",
    "link": "http://arxiv.org/abs/2411.19309",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-11-18",
    "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
    "link": "http://arxiv.org/abs/2411.13587",
    "summary_markdown": "### 论文研究单位\nRochester Institute of Technology, University of Missouri - Kansas City, U.S. Naval Research Laboratory, Lamar University, Meta AI, University of Rochester, Rutgers University\n### 论文概述\n信息未在提供的HTML原文中。\n### 论文核心贡献点\n信息未在提供的HTML原文中。\n### 论文方法描述\n信息未在提供的HTML原文中。\n### 论文使用数据集和训练资源\n信息未在提供的HTML原文中。\n### 论文使用的评估环境和评估指标\n信息未在提供的HTML原文中。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Rochester Institute of Technology, University of Missouri - Kansas City, U.S. Naval Research Laboratory, Lamar University, Meta AI, University of Rochester, Rutgers University</p>\n<h3>论文概述</h3>\n<p>信息未在提供的HTML原文中。</p>\n<h3>论文核心贡献点</h3>\n<p>信息未在提供的HTML原文中。</p>\n<h3>论文方法描述</h3>\n<p>信息未在提供的HTML原文中。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>信息未在提供的HTML原文中。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>信息未在提供的HTML原文中。</p>"
  },
  {
    "date": "2024-11-15",
    "title": "Visual-Linguistic Agent: Towards Collaborative Contextual Object Reasoning",
    "link": "http://arxiv.org/abs/2411.10252",
    "summary_markdown": "# 论文研究单位\nCarnegie Mellon University, Zhejiang University, Beijing Institute of Petrochemical Technology, TikTok, Sealand Technology Inc., Singapore Management University\n# 论文概述\n论文提出视觉-语言代理（VLA）框架，结合多模态大语言模型（MLLMs）的推理能力和传统目标检测模型的定位能力。传统目标检测模型定位精确但缺乏上下文连贯性，MLLMs擅长关系推理但定位不准确。VLA通过MLLM作为语言代理与专门的视觉代理协作，利用空间和上下文关系推理评估检测结果，并通过分类视觉代理提供纠正反馈，实现更准确和上下文相关的目标检测。\n# 论文核心贡献点\n1. 提出视觉-语言代理（VLA）协作框架，利用MLLMs的推理能力增强目标检测的上下文连贯性。\n2. MLLM作为语言代理与目标检测视觉代理、分类视觉代理协作，通过空间关系推理过滤错误检测，最大化MLLM的推理能力同时增强定位精度。\n3. 在COCO数据集上实验显示VLA使AP50:95提升3%，为多模态目标检测树立新基准。\n# 论文方法描述\nVLA框架包含三个阶段：\n1. 场景理解与目标检测：视觉代理（如YOLO）处理图像生成边界框和类别标签，语言代理（MLLM）基于视觉特征生成上下文图像描述。\n2. 理性分析与错误过滤：语言代理根据场景理解和常识知识评估每个检测的合理性，标记错误检测结果。\n3. 目标错误纠正：针对标记的错误检测，分类视觉代理（如CLIP）执行细粒度分类修正错误标签。\n代理间使用JSON格式交换结构化数据，最终结果保存为COCO风格JSON文件。\n# 论文使用数据集和训练资源\n数据集：COCO数据集\n多模态语言模型：GPT-4o, Claude 3.5 Sonnet, LLaVA, Gemini 1.5\n目标检测模型：Faster R-CNN, YOLOX, YOLOv11, DETR, DINO\n分类视觉代理：CLIP\n# 论文使用的评估环境和评估指标\n评估环境：COCO数据集\n评估指标：\n检测指标：平均精度均值（mAP），具体包括AP50:95、AP50、AP75\n目标尺寸相关指标：APs（小目标）、APm（中目标）、APl（大目标）\n错误纠正指标：纠正率（Correction Rate），评估错误检测的修复能力",
    "summary_html": "<h1>论文研究单位</h1>\n<p>Carnegie Mellon University, Zhejiang University, Beijing Institute of Petrochemical Technology, TikTok, Sealand Technology Inc., Singapore Management University</p>\n<h1>论文概述</h1>\n<p>论文提出视觉-语言代理（VLA）框架，结合多模态大语言模型（MLLMs）的推理能力和传统目标检测模型的定位能力。传统目标检测模型定位精确但缺乏上下文连贯性，MLLMs擅长关系推理但定位不准确。VLA通过MLLM作为语言代理与专门的视觉代理协作，利用空间和上下文关系推理评估检测结果，并通过分类视觉代理提供纠正反馈，实现更准确和上下文相关的目标检测。</p>\n<h1>论文核心贡献点</h1>\n<ol><li>提出视觉-语言代理（VLA）协作框架，利用MLLMs的推理能力增强目标检测的上下文连贯性。</li><li>MLLM作为语言代理与目标检测视觉代理、分类视觉代理协作，通过空间关系推理过滤错误检测，最大化MLLM的推理能力同时增强定位精度。</li><li>在COCO数据集上实验显示VLA使AP50:95提升3%，为多模态目标检测树立新基准。</li></ol>\n<h1>论文方法描述</h1>\n<p>VLA框架包含三个阶段：</p>\n<ol><li>场景理解与目标检测：视觉代理（如YOLO）处理图像生成边界框和类别标签，语言代理（MLLM）基于视觉特征生成上下文图像描述。</li><li>理性分析与错误过滤：语言代理根据场景理解和常识知识评估每个检测的合理性，标记错误检测结果。</li><li>目标错误纠正：针对标记的错误检测，分类视觉代理（如CLIP）执行细粒度分类修正错误标签。</li></ol>\n<p>代理间使用JSON格式交换结构化数据，最终结果保存为COCO风格JSON文件。</p>\n<h1>论文使用数据集和训练资源</h1>\n<p>数据集：COCO数据集</p>\n<p>多模态语言模型：GPT-4o, Claude 3.5 Sonnet, LLaVA, Gemini 1.5</p>\n<p>目标检测模型：Faster R-CNN, YOLOX, YOLOv11, DETR, DINO</p>\n<p>分类视觉代理：CLIP</p>\n<h1>论文使用的评估环境和评估指标</h1>\n<p>评估环境：COCO数据集</p>\n<p>评估指标：</p>\n<p>检测指标：平均精度均值（mAP），具体包括AP50:95、AP50、AP75</p>\n<p>目标尺寸相关指标：APs（小目标）、APm（中目标）、APl（大目标）</p>\n<p>错误纠正指标：纠正率（Correction Rate），评估错误检测的修复能力</p>"
  },
  {
    "date": "2024-11-04",
    "title": "Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks",
    "link": "http://arxiv.org/abs/2411.05821",
    "summary_markdown": "### 论文研究单位\nMetarch.ai\n### 论文概述\n本文提出了一个全面的评估框架和基准测试套件，用于评估视觉-语言-动作（VLA）模型在机器人学习任务中的性能。研究对GPT-4o、OpenVLA和JAT三种最先进的VLM和VLA模型进行了详细分析，覆盖了来自Open-X-Embodiment集合的20个多样化数据集。研究揭示了当前VLA模型在不同任务和机器人平台上的性能变化显著，其中GPT-4o通过复杂的提示工程展现出最一致的性能，而所有模型在需要多步规划的复杂操作任务中均表现不佳，且模型性能对动作空间特性和环境因素高度敏感。\n### 论文核心贡献点\n1. **首个大规模VLA基准测试**：推出了MultiNet v0.1，这是首个针对大规模通用动作模型的基准测试。\n2. **系统性评估框架**：建立了一个全面的评估框架，包括数据集管理、模型配置和性能指标。\n3. **模型性能分析**：提供了对三种代表性VLA模型的详细性能分析，揭示了它们的优势和局限。\n4. **开源工具**：发布了开源软件基础设施，用于下载、管理和利用基准数据。\n5. **跨模态映射框架**：提出了一个通用框架，用于将VLM映射到其他模态类别，特别是动作空间。\n### 论文方法描述\n1. **数据集处理**：从Open-X-Embodiment数据集中筛选并处理了53个数据集，最终对20个数据集进行评估。数据集涵盖多种机器人平台和任务类型，使用RLDS格式存储。\n2. **模型配置**：\n - **JAT**：采用零样本设置，对图像进行4通道处理（RGB复制红色通道作为Alpha），观察值和动作拼接为单一张量。\n - **GPT-4o**：构建综合提示，包括浮点观察状态、主图像观察、自然语言指令和动作空间描述，对不兼容输出进行错误处理。\n - **OpenVLA**：标准化夹爪命令（二进制/三元离散化或连续归一化），处理特殊数据集（如UCSD和ETH），并排除终端张量。\n3. **评估指标**：主要使用平均均方误差（AMSE）、归一化AMSE（NAMSE）和完成率，通过比较预测动作与真实动作来评估模型性能。\n4. **推理基础设施**：JAT和GPT使用GCP e2-standard-8实例，OpenVLA使用配备NVIDIA L4 GPU的GCP g2-standard-8实例。\n### 论文使用数据集和训练资源\n1. **数据集**：Open-X-Embodiment数据集，包含来自21个机构的超过100万条真实机器人轨迹，涵盖22种不同的机器人形态。评估版本v0.1使用了53个数据集，完整训练数据约32TB。\n2. **训练资源**：未明确提及模型训练过程，但详细描述了推理阶段的计算资源，包括GCP实例类型和GPU配置（如NVIDIA L4）。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：基于Google Cloud Platform（GCP）的虚拟机实例，具体包括：\n - JAT和GPT：e2-standard-8实例（8 vCPU，32 GB内存）。\n - OpenVLA：g2-standard-8实例（NVIDIA L4 GPU，24 GB显存）。\n2. **评估指标**：\n - **平均均方误差（AMSE）**：计算数据集中所有时间步的MSE平均值，用于跨数据集性能比较。\n - **归一化AMSE（NAMSE）**：对每个模型的MSE进行最小-最大归一化后取平均，用于 equitable 跨数据集比较。\n - **完成率**：通过比较预测与真实最终动作评估任务完成情况，作为任务完成能力的近似度量。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Metarch.ai</p>\n<h3>论文概述</h3>\n<p>本文提出了一个全面的评估框架和基准测试套件，用于评估视觉-语言-动作（VLA）模型在机器人学习任务中的性能。研究对GPT-4o、OpenVLA和JAT三种最先进的VLM和VLA模型进行了详细分析，覆盖了来自Open-X-Embodiment集合的20个多样化数据集。研究揭示了当前VLA模型在不同任务和机器人平台上的性能变化显著，其中GPT-4o通过复杂的提示工程展现出最一致的性能，而所有模型在需要多步规划的复杂操作任务中均表现不佳，且模型性能对动作空间特性和环境因素高度敏感。</p>\n<h3>论文核心贡献点</h3>\n<ol><li><strong>首个大规模VLA基准测试</strong>：推出了MultiNet v0.1，这是首个针对大规模通用动作模型的基准测试。</li><li><strong>系统性评估框架</strong>：建立了一个全面的评估框架，包括数据集管理、模型配置和性能指标。</li><li><strong>模型性能分析</strong>：提供了对三种代表性VLA模型的详细性能分析，揭示了它们的优势和局限。</li><li><strong>开源工具</strong>：发布了开源软件基础设施，用于下载、管理和利用基准数据。</li><li><strong>跨模态映射框架</strong>：提出了一个通用框架，用于将VLM映射到其他模态类别，特别是动作空间。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>数据集处理</strong>：从Open-X-Embodiment数据集中筛选并处理了53个数据集，最终对20个数据集进行评估。数据集涵盖多种机器人平台和任务类型，使用RLDS格式存储。</li><li><strong>模型配置</strong>：</li></ol>\n<p> - <strong>JAT</strong>：采用零样本设置，对图像进行4通道处理（RGB复制红色通道作为Alpha），观察值和动作拼接为单一张量。</p>\n<p> - <strong>GPT-4o</strong>：构建综合提示，包括浮点观察状态、主图像观察、自然语言指令和动作空间描述，对不兼容输出进行错误处理。</p>\n<p> - <strong>OpenVLA</strong>：标准化夹爪命令（二进制/三元离散化或连续归一化），处理特殊数据集（如UCSD和ETH），并排除终端张量。</p>\n<ol><li><strong>评估指标</strong>：主要使用平均均方误差（AMSE）、归一化AMSE（NAMSE）和完成率，通过比较预测动作与真实动作来评估模型性能。</li><li><strong>推理基础设施</strong>：JAT和GPT使用GCP e2-standard-8实例，OpenVLA使用配备NVIDIA L4 GPU的GCP g2-standard-8实例。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：Open-X-Embodiment数据集，包含来自21个机构的超过100万条真实机器人轨迹，涵盖22种不同的机器人形态。评估版本v0.1使用了53个数据集，完整训练数据约32TB。</li><li><strong>训练资源</strong>：未明确提及模型训练过程，但详细描述了推理阶段的计算资源，包括GCP实例类型和GPU配置（如NVIDIA L4）。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：基于Google Cloud Platform（GCP）的虚拟机实例，具体包括：</p>\n<p> - JAT和GPT：e2-standard-8实例（8 vCPU，32 GB内存）。</p>\n<p> - OpenVLA：g2-standard-8实例（NVIDIA L4 GPU，24 GB显存）。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - <strong>平均均方误差（AMSE）</strong>：计算数据集中所有时间步的MSE平均值，用于跨数据集性能比较。</p>\n<p> - <strong>归一化AMSE（NAMSE）</strong>：对每个模型的MSE进行最小-最大归一化后取平均，用于 equitable 跨数据集比较。</p>\n<p> - <strong>完成率</strong>：通过比较预测与真实最终动作评估任务完成情况，作为任务完成能力的近似度量。</p>"
  },
  {
    "date": "2024-11-05",
    "title": "VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation",
    "link": "http://arxiv.org/abs/2411.03540",
    "summary_markdown": "### 论文研究单位\nRobotics Institute, Carnegie Mellon University\n### 论文概述\n论文介绍了VLA-3D，一个用于3D语义场景理解和导航任务的大规模真实世界数据集。该数据集旨在解决现有视觉-语言模型在室内导航任务中因空间推理和语义理解不足而面临的挑战，特别是在包含大量细粒度物体的复杂且变化的场景中。VLA-3D通过提供丰富的3D场景数据、语义关系和指称语言，帮助开发更鲁棒和交互式的具身智能体。\n### 论文核心贡献点\n1. 提出了VLA-3D，一个目前最大的真实世界3D视觉-语言-行动数据集，包含超过11.5K个室内场景、23.5M个语义关系和9.7M个指称语句。\n2. 为每个场景提供了大规模的密集场景图，有助于处理场景变化和识别相似物体，这是该数据集区别于其他数据集的关键特征之一。\n3. 包含了可导航的自由空间标注，使得模型不仅能引用物体，还能引用空间或路径。\n4. 生成的指称语言语句是视图无关、无歧义且最小化的，更贴近人类自然语言习惯。\n5. 公开了完整的数据集生成代码和可视化工具，以促进研究发展。\n### 论文方法描述\n数据集的构建包含三个主要步骤：\n1. **3D扫描处理**：整合ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes和Unity等数据源的3D扫描数据，生成点云。为每个物体提取语义类别、边界框和主要颜色，并计算水平可穿越的自由空间区域。\n2. **场景图生成**：基于物体的3D边界框，使用启发式方法为每个区域内的物体对或三元组计算八种类型的语义空间关系（如above, below, near, between等），构建结构化的场景图。\n3. **语言生成**：基于生成的场景图，使用模板方法合成指称语言语句。这些语句遵循视图无关、无歧义和最小化的原则，并利用关系谓词的同义词增加多样性。\n### 论文使用数据集和训练资源\n* **使用数据集**：该论文主要介绍了一个新的数据集VLA-3D，其本身由多个现有数据集构成，包括ScanNet、Matterport3D、Habitat-Matterport 3D (HM3D)、3RScan、ARKitScenes以及Unity生成的场景。最终数据集包含7635个场景，超过11.5K个区域（房间），超过28.6万个物体，以及477个不同的物体类别。\n* **训练资源**：论文未提及在VLA-3D数据集上从头训练模型的计算资源（如GPU数量、训练时间）。评估时直接使用了MVT和3D-VisTA模型的预训练检查点。\n### 论文使用的评估环境和评估指标\n* **评估环境**：在VLA-3D数据集的测试集上，对两个当前最先进的开源模型（MVT和3D-VisTA）的预训练权重进行了直接评估，以验证数据集的挑战性。评估内容是在未见过的复杂场景上执行指称物体定位任务。\n* **评估指标**：主要的评估指标是**准确率**，即模型根据语言描述正确识别出目标物体的百分比。论文还将模型在VLA-3D上的性能与它们在标准基准数据集Nr3D和Sr3D上的表现进行了对比。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>Robotics Institute, Carnegie Mellon University</p>\n<h3>论文概述</h3>\n<p>论文介绍了VLA-3D，一个用于3D语义场景理解和导航任务的大规模真实世界数据集。该数据集旨在解决现有视觉-语言模型在室内导航任务中因空间推理和语义理解不足而面临的挑战，特别是在包含大量细粒度物体的复杂且变化的场景中。VLA-3D通过提供丰富的3D场景数据、语义关系和指称语言，帮助开发更鲁棒和交互式的具身智能体。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了VLA-3D，一个目前最大的真实世界3D视觉-语言-行动数据集，包含超过11.5K个室内场景、23.5M个语义关系和9.7M个指称语句。</li><li>为每个场景提供了大规模的密集场景图，有助于处理场景变化和识别相似物体，这是该数据集区别于其他数据集的关键特征之一。</li><li>包含了可导航的自由空间标注，使得模型不仅能引用物体，还能引用空间或路径。</li><li>生成的指称语言语句是视图无关、无歧义且最小化的，更贴近人类自然语言习惯。</li><li>公开了完整的数据集生成代码和可视化工具，以促进研究发展。</li></ol>\n<h3>论文方法描述</h3>\n<p>数据集的构建包含三个主要步骤：</p>\n<ol><li><strong>3D扫描处理</strong>：整合ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes和Unity等数据源的3D扫描数据，生成点云。为每个物体提取语义类别、边界框和主要颜色，并计算水平可穿越的自由空间区域。</li><li><strong>场景图生成</strong>：基于物体的3D边界框，使用启发式方法为每个区域内的物体对或三元组计算八种类型的语义空间关系（如above, below, near, between等），构建结构化的场景图。</li><li><strong>语言生成</strong>：基于生成的场景图，使用模板方法合成指称语言语句。这些语句遵循视图无关、无歧义和最小化的原则，并利用关系谓词的同义词增加多样性。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>* <strong>使用数据集</strong>：该论文主要介绍了一个新的数据集VLA-3D，其本身由多个现有数据集构成，包括ScanNet、Matterport3D、Habitat-Matterport 3D (HM3D)、3RScan、ARKitScenes以及Unity生成的场景。最终数据集包含7635个场景，超过11.5K个区域（房间），超过28.6万个物体，以及477个不同的物体类别。</p>\n<p>* <strong>训练资源</strong>：论文未提及在VLA-3D数据集上从头训练模型的计算资源（如GPU数量、训练时间）。评估时直接使用了MVT和3D-VisTA模型的预训练检查点。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>* <strong>评估环境</strong>：在VLA-3D数据集的测试集上，对两个当前最先进的开源模型（MVT和3D-VisTA）的预训练权重进行了直接评估，以验证数据集的挑战性。评估内容是在未见过的复杂场景上执行指称物体定位任务。</p>\n<p>* <strong>评估指标</strong>：主要的评估指标是<strong>准确率</strong>，即模型根据语言描述正确识别出目标物体的百分比。论文还将模型在VLA-3D上的性能与它们在标准基准数据集Nr3D和Sr3D上的表现进行了对比。</p>"
  },
  {
    "date": "2024-11-04",
    "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
    "link": "http://arxiv.org/abs/2411.02359",
    "summary_markdown": "### 论文研究单位\n清华大学自动化系与字节跳动\n### 论文概述\n论文提出了一种名为DeeR-VLA的动态推理框架，用于解决多模态大语言模型在机器人控制中的计算效率问题。该方法基于一个观察：机器人执行任务时，大多数情况是相对简单的，可以用较小的模型处理，只有少数复杂情况才需要完整的大模型。DeeR-VLA通过在多模态大语言模型中引入多个中间退出点，根据当前情况动态选择合适的模型规模，从而在保持性能的同时显著降低计算和内存需求。\n### 论文核心贡献点\n- 提出动态早期退出框架DeeR-VLA，首次将动态推理应用于机器人多模态大语言模型\n- 设计基于动作一致性的退出判据，替代传统基于置信度的方法\n- 开发预算执行算法，可根据计算和内存约束自动优化退出阈值\n- 在CALVIN基准上实现5.2-6.5倍计算减少和2-6倍GPU内存减少，同时保持竞争性能\n### 论文方法描述\n- 多退出架构：将LLM层分为N个连续组，每组后添加退出点，通过最大池化聚合特征\n- 动作预测头：使用轻量级LSTM处理历史信息，预测7自由度末端执行器动作（6连续+1离散）\n- 退出判据：比较相邻退出点的动作预测L2距离，当小于阈值时提前退出\n- 预算执行：将阈值选择建模为优化问题，在满足计算/内存约束下最大化任务成功率\n- 训练策略：随机采样所有退出点特征进行训练，使用辅助动作头优化模型\n### 论文使用数据集和训练资源\n- 数据集：CALVIN Long-Horizon Multi-Task Language Control (LH-MTLC)基准\n- 训练资源：8×NVIDIA A100 GPU，训练时间约24小时\n### 论文使用的评估环境和评估指标\n- 评估环境：CALVIN LH-MTLC的三个挑战子集\n- 评估指标：任务成功率、平均计算成本、峰值计算成本、GPU内存使用量",
    "summary_html": "<h3>论文研究单位</h3>\n<p>清华大学自动化系与字节跳动</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为DeeR-VLA的动态推理框架，用于解决多模态大语言模型在机器人控制中的计算效率问题。该方法基于一个观察：机器人执行任务时，大多数情况是相对简单的，可以用较小的模型处理，只有少数复杂情况才需要完整的大模型。DeeR-VLA通过在多模态大语言模型中引入多个中间退出点，根据当前情况动态选择合适的模型规模，从而在保持性能的同时显著降低计算和内存需求。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出动态早期退出框架DeeR-VLA，首次将动态推理应用于机器人多模态大语言模型</li><li>设计基于动作一致性的退出判据，替代传统基于置信度的方法</li><li>开发预算执行算法，可根据计算和内存约束自动优化退出阈值</li><li>在CALVIN基准上实现5.2-6.5倍计算减少和2-6倍GPU内存减少，同时保持竞争性能</li></ul>\n<h3>论文方法描述</h3>\n<ul><li>多退出架构：将LLM层分为N个连续组，每组后添加退出点，通过最大池化聚合特征</li><li>动作预测头：使用轻量级LSTM处理历史信息，预测7自由度末端执行器动作（6连续+1离散）</li><li>退出判据：比较相邻退出点的动作预测L2距离，当小于阈值时提前退出</li><li>预算执行：将阈值选择建模为优化问题，在满足计算/内存约束下最大化任务成功率</li><li>训练策略：随机采样所有退出点特征进行训练，使用辅助动作头优化模型</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li>数据集：CALVIN Long-Horizon Multi-Task Language Control (LH-MTLC)基准</li><li>训练资源：8×NVIDIA A100 GPU，训练时间约24小时</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li>评估环境：CALVIN LH-MTLC的三个挑战子集</li><li>评估指标：任务成功率、平均计算成本、峰值计算成本、GPU内存使用量</li></ul>"
  },
  {
    "date": "2024-11-01",
    "title": "CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision",
    "link": "http://arxiv.org/abs/2411.00508",
    "summary_markdown": "待生成",
    "summary_html": "<p>待生成</p>"
  },
  {
    "date": "2024-10-21",
    "title": "The Duality of Generative AI and Reinforcement Learning in Robotics: A Review",
    "link": "http://arxiv.org/abs/2410.16411",
    "summary_markdown": "论文研究单位\n- 瑞士南方应用科学与艺术大学（SUPSI）IDSIA部门\n- 印度比哈尔邦IIIT机电与自动化工程系\n- 意大利米兰比科卡大学信息、系统与传播系\n- 意大利米兰理工大学机械工程系\n\n论文概述\n本文是一篇综述论文，探讨了生成式AI（如大型语言模型LLM、视觉语言模型VLM、扩散模型、世界模型）与强化学习（RL）在机器人学中的双重整合。论文系统分析了生成式AI作为工具增强RL任务（如奖励信号生成、状态表示学习、规划探索），以及RL方法训练、微调和蒸馏生成式策略（如视觉-语言-动作模型）的互补关系。基于169篇文献，论文提出了新的分类法，并讨论了当前挑战、最佳实践和未来研究方向。\n\n论文核心贡献点\n1. 全面回顾了基于Transformer和扩散模型的生成式模型与RL在机器人学中的交叉研究。\n2. 首次分析了生成式AI工具如何改进RL，以及RL如何改进生成式策略的双重性。\n3. 识别了使用生成式AI模型作为RL工具时的最佳实践和挑战。\n4. 详细分类了基于RL的预训练、微调和策略蒸馏方法。\n5. 提出了三个新的研究方向，包括基于人类反馈的RL、执行者-评论者基础模型和约束感知生成模型。\n6. 提出了统一的分类法和持续更新的GitHub资源库。\n\n论文方法描述\n论文提出的新分类法包括两个主要维度：\n1. 生成式AI作为RL的工具：按基础模型（LLM、VLM、扩散模型、世界模型、视频预测模型）、输入输出模态（文本、图像、轨迹等）和任务（奖励信号设计、状态表示学习、规划与探索）分类。\n2. RL用于生成式策略：按RL预训练（变换器策略、扩散策略、基元生成器）、RL微调（策略特定方法、策略无关方法）和策略蒸馏（从通用到专家、从专家到通用）分类。\n\n论文使用数据集和训练资源\n- 数据集：模拟环境（如NVIDIA Isaac Gym、ManiSkill2、MetaWorld）、真实机器人任务（如操作任务）、演示数据集（如Alfred Dataset）、文本环境（如BabyAI-Text）和视频数据集（如Ego4D）。\n- 训练资源：多数研究在模拟环境中训练，部分结合真实实验验证；代码可用性在表格中明确标注，常用框架包括GPT-4、CLIP、扩散策略等。\n\n论文使用的评估环境和评估指标\n- 评估环境：包括模拟环境（如MuJoCo、Isaac Gym）和真实世界机器人实验（如操作任务）。\n- 评估指标：任务成功率、奖励信号强度、探索效率、策略泛化能力、状态表示质量等，具体指标因任务而异（如奖励函数设计任务中评估奖励对齐度）。",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>瑞士南方应用科学与艺术大学（SUPSI）IDSIA部门</li><li>印度比哈尔邦IIIT机电与自动化工程系</li><li>意大利米兰比科卡大学信息、系统与传播系</li><li>意大利米兰理工大学机械工程系</li></ul>\n\n<p>论文概述</p>\n<p>本文是一篇综述论文，探讨了生成式AI（如大型语言模型LLM、视觉语言模型VLM、扩散模型、世界模型）与强化学习（RL）在机器人学中的双重整合。论文系统分析了生成式AI作为工具增强RL任务（如奖励信号生成、状态表示学习、规划探索），以及RL方法训练、微调和蒸馏生成式策略（如视觉-语言-动作模型）的互补关系。基于169篇文献，论文提出了新的分类法，并讨论了当前挑战、最佳实践和未来研究方向。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>全面回顾了基于Transformer和扩散模型的生成式模型与RL在机器人学中的交叉研究。</li><li>首次分析了生成式AI工具如何改进RL，以及RL如何改进生成式策略的双重性。</li><li>识别了使用生成式AI模型作为RL工具时的最佳实践和挑战。</li><li>详细分类了基于RL的预训练、微调和策略蒸馏方法。</li><li>提出了三个新的研究方向，包括基于人类反馈的RL、执行者-评论者基础模型和约束感知生成模型。</li><li>提出了统一的分类法和持续更新的GitHub资源库。</li></ol>\n\n<p>论文方法描述</p>\n<p>论文提出的新分类法包括两个主要维度：</p>\n<ol><li>生成式AI作为RL的工具：按基础模型（LLM、VLM、扩散模型、世界模型、视频预测模型）、输入输出模态（文本、图像、轨迹等）和任务（奖励信号设计、状态表示学习、规划与探索）分类。</li><li>RL用于生成式策略：按RL预训练（变换器策略、扩散策略、基元生成器）、RL微调（策略特定方法、策略无关方法）和策略蒸馏（从通用到专家、从专家到通用）分类。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：模拟环境（如NVIDIA Isaac Gym、ManiSkill2、MetaWorld）、真实机器人任务（如操作任务）、演示数据集（如Alfred Dataset）、文本环境（如BabyAI-Text）和视频数据集（如Ego4D）。</li><li>训练资源：多数研究在模拟环境中训练，部分结合真实实验验证；代码可用性在表格中明确标注，常用框架包括GPT-4、CLIP、扩散策略等。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：包括模拟环境（如MuJoCo、Isaac Gym）和真实世界机器人实验（如操作任务）。</li><li>评估指标：任务成功率、奖励信号强度、探索效率、策略泛化能力、状态表示质量等，具体指标因任务而异（如奖励函数设计任务中评估奖励对齐度）。</li></ul>"
  },
  {
    "date": "2024-10-21",
    "title": "VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making",
    "link": "http://arxiv.org/abs/2410.15885",
    "summary_markdown": "### 论文研究单位\n浙江大学计算机科学与技术学院、中国电信数字生活科技有限公司、浙江实验室、都柏林三一学院\n### 论文概述\n本文提出MIMO-VLA（VLASCD），一个统一的多输入多输出（MIMO）训练架构，用于同时进行对话生成和决策制定。研究发现现有的多输入单输出（MISO）架构（如LLMs和VLAs）在MIMO场景中存在根本限制，任务间会因共享输出通道产生相互排斥效应，导致优化不平衡和性能下降。MIMO-VLA通过并行多任务输出消除了任务间干扰，支持高效并行处理，在CARLA自动驾驶平台上验证了其有效性。\n### 论文核心贡献点\n1. 首次证明现有MISO模型（如LLMs和VLAs）在处理MIMO任务时存在根本性缺陷\n2. 提出MIMO-VLA统一MIMO训练架构，集成多项技术：\n - 生成连续动作值的计算模块和损失项\n - 利用丰富视觉信息的图像重建损失\n - 保持对话能力同时增强决策准确性的标签平滑策略\n3. 广泛实验证明MIMO-VLA在决策准确性上超越SOTA基线，同时完全保留实时对话功能\n### 论文方法描述\n模型架构：\n- 基于LLaMA-7B骨干网络，支持文本、图像和数值向量三种输入模态\n- 图像分割为补丁通过2D卷积编码，动作值通过MLP编码\n- 文本使用预训练嵌入层，所有模态嵌入按固定顺序拼接\n- 输出端支持文本响应和连续动作决策两种模态\n\n训练过程：\n- 使用LoRA微调，仅更新Q和V投影模块（占0.06%参数）\n- 联合训练三个损失函数：\n - 文本生成：采用标签平滑的交叉熵损失防止过拟合\n - 动作预测：直接预测连续动作值的MSE损失\n - 图像重建：像素级欧氏距离的辅助损失\n- 实现梯度空间隔离机制，不同损失影响不同位置编码\n### 论文使用数据集和训练资源\n数据集：\n- 在CARLA 0.9.10平台使用EGADS框架收集专家轨迹数据\n- 总量5.69GB，包含13,761帧驾驶场景\n- 每帧配有从50个预设问题中随机选择的问答对\n\n训练资源：\n- 使用LoRA技术高效微调大型语言模型\n- 在town03地图上采集训练数据\n- 动作空间为2维（加速度和转向），范围分别为[-3,3]和[-0.2,0.2]\n### 论文使用的评估环境和评估指标\n评估环境：\n- gym-carla环境（基于CARLA 0.9.10的OpenAI Gym兼容环境）\n- 训练集town03，测试集包括town03和town04（泛化测试）\n\n评估指标：\n对话能力评估：\n- GPT-4o对50个随机环境-问题对进行评分（0-10分）\n- 分级标准：不可接受(<3)、可接受(3≤score<6)、良好(≥6)\n\n决策能力评估：\n- 碰撞率(CR, %, ↓)\n- 偏离道路率(OR, %, ↓)\n- 任务完成率(ER, %, ↑)\n- 平均安全驾驶距离(ASD, 米, ↑)\n- 平均奖励(AR, 分数, ↑)\n- 驾驶分数(DS = ER × AR, ↑)",
    "summary_html": "<h3>论文研究单位</h3>\n<p>浙江大学计算机科学与技术学院、中国电信数字生活科技有限公司、浙江实验室、都柏林三一学院</p>\n<h3>论文概述</h3>\n<p>本文提出MIMO-VLA（VLASCD），一个统一的多输入多输出（MIMO）训练架构，用于同时进行对话生成和决策制定。研究发现现有的多输入单输出（MISO）架构（如LLMs和VLAs）在MIMO场景中存在根本限制，任务间会因共享输出通道产生相互排斥效应，导致优化不平衡和性能下降。MIMO-VLA通过并行多任务输出消除了任务间干扰，支持高效并行处理，在CARLA自动驾驶平台上验证了其有效性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>首次证明现有MISO模型（如LLMs和VLAs）在处理MIMO任务时存在根本性缺陷</li><li>提出MIMO-VLA统一MIMO训练架构，集成多项技术：</li></ol>\n<p> - 生成连续动作值的计算模块和损失项</p>\n<p> - 利用丰富视觉信息的图像重建损失</p>\n<p> - 保持对话能力同时增强决策准确性的标签平滑策略</p>\n<p>3. 广泛实验证明MIMO-VLA在决策准确性上超越SOTA基线，同时完全保留实时对话功能</p>\n<h3>论文方法描述</h3>\n<p>模型架构：</p>\n<ul><li>基于LLaMA-7B骨干网络，支持文本、图像和数值向量三种输入模态</li><li>图像分割为补丁通过2D卷积编码，动作值通过MLP编码</li><li>文本使用预训练嵌入层，所有模态嵌入按固定顺序拼接</li><li>输出端支持文本响应和连续动作决策两种模态</li></ul>\n\n<p>训练过程：</p>\n<ul><li>使用LoRA微调，仅更新Q和V投影模块（占0.06%参数）</li><li>联合训练三个损失函数：</li></ul>\n<p> - 文本生成：采用标签平滑的交叉熵损失防止过拟合</p>\n<p> - 动作预测：直接预测连续动作值的MSE损失</p>\n<p> - 图像重建：像素级欧氏距离的辅助损失</p>\n<ul><li>实现梯度空间隔离机制，不同损失影响不同位置编码</li></ul>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：</p>\n<ul><li>在CARLA 0.9.10平台使用EGADS框架收集专家轨迹数据</li><li>总量5.69GB，包含13,761帧驾驶场景</li><li>每帧配有从50个预设问题中随机选择的问答对</li></ul>\n\n<p>训练资源：</p>\n<ul><li>使用LoRA技术高效微调大型语言模型</li><li>在town03地图上采集训练数据</li><li>动作空间为2维（加速度和转向），范围分别为[-3,3]和[-0.2,0.2]</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境：</p>\n<ul><li>gym-carla环境（基于CARLA 0.9.10的OpenAI Gym兼容环境）</li><li>训练集town03，测试集包括town03和town04（泛化测试）</li></ul>\n\n<p>评估指标：</p>\n<p>对话能力评估：</p>\n<ul><li>GPT-4o对50个随机环境-问题对进行评分（0-10分）</li><li>分级标准：不可接受(<3)、可接受(3≤score<6)、良好(≥6)</li></ul>\n\n<p>决策能力评估：</p>\n<ul><li>碰撞率(CR, %, ↓)</li><li>偏离道路率(OR, %, ↓)</li><li>任务完成率(ER, %, ↑)</li><li>平均安全驾驶距离(ASD, 米, ↑)</li><li>平均奖励(AR, 分数, ↑)</li><li>驾驶分数(DS = ER × AR, ↑)</li></ul>"
  },
  {
    "date": "2024-10-21",
    "title": "A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM",
    "link": "http://arxiv.org/abs/2410.15549",
    "summary_markdown": "论文研究单位\n韩国电子与电信研究所（ETRI）\n\n论文概述\n本文旨在解决现有视觉-语言-动作（VLA）模型因计算需求高而难以实现高效实时性能的挑战。为此，作者提出了一个受双过程理论启发的层次化框架——双过程VLA（DP-VLA）。该框架利用一个大型System 2模型（L-Sys2）处理复杂的推理和决策，同时使用一个小型System 1模型（S-Sys1）负责实时的运动控制和感官处理。通过让L-Sys2以低频运行，降低了计算开销，而S-Sys1则确保了任务的快速准确执行。在RoboCasa数据集上的实验结果表明，DP-VLA实现了更快的推理速度和更高的任务成功率，为高级机器人应用提供了一个可扩展的解决方案。\n\n论文核心贡献点\n- 高效的机器人操控：该框架确保了精确且响应迅速的操控，在速度和准确性上均有提升。\n- 可扩展的设计：它允许无缝升级到先进的视觉-语言模型（VLM），而无需修改整体系统。\n- 实验验证：通过实验，作者证明了DP-VLA在RoboCasa模拟环境中的有效性，其性能优于以往的VLA方法。\n\n论文方法描述\n该方法受人类认知心理学中的双过程理论启发，将VLA模型分解为两个子系统：\n1. 大型System 2模型（L-Sys2）：作为高级决策者，它接收环境的全局视觉图像和用户的语言指令。它是一个大型模型（如VLM或VLA），仅在任务指令更新时运行，用于处理复杂的推理和任务规划，并生成一个包含逻辑和分析信息的潜特征 `z_i`，该特征作为S-Sys1的指导信号。\n2. 小型System 1模型（S-Sys1）：作为实时运动控制器，它是一个小型机器人策略模型（如BC-Transformer）。它持续接收来自环境的多样化感官输入（如多摄像头视图、机器人状态）以及来自L-Sys2的固定潜特征 `z_i`，并据此预测机器人的实时动作 `a_t`。\n这种分层设计的主要原因在于：大型模型无法进行实时推理；训练大型VLA模型需要海量数据和昂贵硬件；分离设计允许S-Sys1处理更精细的控制输入；并且可以独立升级L-Sys2以利用最新VLM的进步。\n\n论文使用数据集和训练资源\n- 数据集：RoboCasa数据集，具体使用了其中通过MimicGen算法基于人类演示自动生成的Generated-3000子集。该子集包含72,000个剧集，涵盖24项在厨房环境中执行的原子任务，如拾取与放置（PnP）、开门和关门等。\n- 训练资源：所有实验均在RTX 6000 Ada GPU上进行。L-Sys2使用预训练的OpenVLA模型，并在RoboCasa数据集上进行了微调。S-Sys1使用BC-Transformer模型从零开始训练，其语言编码器利用了预训练的CLIP。训练的批大小为128。\n\n论文使用的评估环境和评估指标\n- 评估环境：遵循RoboCasa的评估协议，通过 rollout 训练好的模型进行测试。评估重点在于测试模型在具有未见过的厨房风格和未见过的物体实例的环境中的表现。\n- 评估指标：\n - 任务成功率：在24项任务上的平均成功率，用于评估模型完成各种任务的能力。\n - 推理速度：平均推理时间（秒），具体测量的是一个episode中从第1帧到第50帧的平均推理时间，用于评估模型的实时性能。",
    "summary_html": "<p>论文研究单位</p>\n<p>韩国电子与电信研究所（ETRI）</p>\n\n<p>论文概述</p>\n<p>本文旨在解决现有视觉-语言-动作（VLA）模型因计算需求高而难以实现高效实时性能的挑战。为此，作者提出了一个受双过程理论启发的层次化框架——双过程VLA（DP-VLA）。该框架利用一个大型System 2模型（L-Sys2）处理复杂的推理和决策，同时使用一个小型System 1模型（S-Sys1）负责实时的运动控制和感官处理。通过让L-Sys2以低频运行，降低了计算开销，而S-Sys1则确保了任务的快速准确执行。在RoboCasa数据集上的实验结果表明，DP-VLA实现了更快的推理速度和更高的任务成功率，为高级机器人应用提供了一个可扩展的解决方案。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>高效的机器人操控：该框架确保了精确且响应迅速的操控，在速度和准确性上均有提升。</li><li>可扩展的设计：它允许无缝升级到先进的视觉-语言模型（VLM），而无需修改整体系统。</li><li>实验验证：通过实验，作者证明了DP-VLA在RoboCasa模拟环境中的有效性，其性能优于以往的VLA方法。</li></ul>\n\n<p>论文方法描述</p>\n<p>该方法受人类认知心理学中的双过程理论启发，将VLA模型分解为两个子系统：</p>\n<ol><li>大型System 2模型（L-Sys2）：作为高级决策者，它接收环境的全局视觉图像和用户的语言指令。它是一个大型模型（如VLM或VLA），仅在任务指令更新时运行，用于处理复杂的推理和任务规划，并生成一个包含逻辑和分析信息的潜特征 <code>z_i</code>，该特征作为S-Sys1的指导信号。</li><li>小型System 1模型（S-Sys1）：作为实时运动控制器，它是一个小型机器人策略模型（如BC-Transformer）。它持续接收来自环境的多样化感官输入（如多摄像头视图、机器人状态）以及来自L-Sys2的固定潜特征 <code>z_i</code>，并据此预测机器人的实时动作 <code>a_t</code>。</li></ol>\n<p>这种分层设计的主要原因在于：大型模型无法进行实时推理；训练大型VLA模型需要海量数据和昂贵硬件；分离设计允许S-Sys1处理更精细的控制输入；并且可以独立升级L-Sys2以利用最新VLM的进步。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ul><li>数据集：RoboCasa数据集，具体使用了其中通过MimicGen算法基于人类演示自动生成的Generated-3000子集。该子集包含72,000个剧集，涵盖24项在厨房环境中执行的原子任务，如拾取与放置（PnP）、开门和关门等。</li><li>训练资源：所有实验均在RTX 6000 Ada GPU上进行。L-Sys2使用预训练的OpenVLA模型，并在RoboCasa数据集上进行了微调。S-Sys1使用BC-Transformer模型从零开始训练，其语言编码器利用了预训练的CLIP。训练的批大小为128。</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<ul><li>评估环境：遵循RoboCasa的评估协议，通过 rollout 训练好的模型进行测试。评估重点在于测试模型在具有未见过的厨房风格和未见过的物体实例的环境中的表现。</li><li>评估指标：</li></ul>\n<p> - 任务成功率：在24项任务上的平均成功率，用于评估模型完成各种任务的能力。</p>\n<p> - 推理速度：平均推理时间（秒），具体测量的是一个episode中从第1帧到第50帧的平均推理时间，用于评估模型的实时性能。</p>"
  },
  {
    "date": "2024-10-17",
    "title": "Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand",
    "link": "http://arxiv.org/abs/2410.14022",
    "summary_markdown": "### 论文研究单位\n瑞士洛桑联邦理工学院机械工程学院\n### 论文概述\n该论文提出了一种结合微调的视觉-语言-动作（VLA）模型与扩散模型的混合控制方法，用于实现人形手的灵巧操作。VLA模型负责基于语言指令的高级规划，具有高泛化性；扩散模型则处理低级交互，为特定物体和环境提供所需的精度和鲁棒性。通过在训练数据中加入切换信号，实现了两个模型之间基于事件的转换，应用于通过语言指令指定目标物体和放置位置的抓取-放置任务。该方法在13自由度的ADAPT Hand 2上进行了部署，这是首次将VLA模型应用于多指手。实验表明，该混合方法的成功率超过80%，而仅使用VLA模型的成功率低于40%。\n### 论文核心贡献点\n1. 提出了一种混合控制框架，结合了VLA模型的高级规划能力和扩散模型的低级精确控制能力。\n2. 设计了一种基于事件信号的模型切换机制，实现了VLA模型和扩散模型之间的无缝转换。\n3. 首次成功地将VLA模型应用于控制多指灵巧手。\n4. 在真实的抓取-放置任务中，验证了混合框架的有效性，成功率远超仅使用VLA模型的基线。\n5. 展示了系统具备多种期望行为：多模态抓取、错误恢复能力以及通过硬件柔顺性带来的交互稳定性。\n### 论文方法描述\n1. **框架设计**：系统在VLA模型和扩散策略模型之间切换。VLA模型（微调的openVLA）根据语言指令和视觉反馈控制手臂的运动，使手部接近目标物体，其输出中的一个标量信号（抓取百分比）被重新用作切换事件信号。扩散策略模型负责精确的抓取动作，当其完成抓取后，通过另一个事件信号将控制权交还给VLA模型。\n2. **机器人平台**：使用定制的ADAPT Hand 2，一个13自由度、具有串联弹性驱动（SEA）的拟人化灵巧手，安装在UR5机械臂上。这种柔顺性设计允许机器人安全地与环境和物体交互。\n3. **模型输入**：VLA模型的视觉输入是来自一个固定摄像头和一个手腕摄像头的两幅图像，经过缩放后垂直拼接成一幅图像输入。扩散策略同样使用这两个摄像头的图像作为输入。\n4. **模型选择**：扩散策略模型是针对特定物体训练的，因此系统使用一个基于语言输入的查找表来为每个物体选择对应的扩散策略模型。\n### 论文使用数据集和训练资源\n1. **数据集**：所有数据均通过真实机器人遥操作收集。\n * **VLA数据**：记录完整的抓取-放置任务，但仅使用抓取动作前后的轨迹进行训练。针对每个物体和目标位置的组合，记录了20次试验。\n * **扩散策略数据**：仅记录抓取动作部分。对每个物体（辣椒、磁带、纸），记录了30到40次试验，包含不同的抓取策略（如滑动抓取、直接抓取）和失败恢复的演示。\n2. **训练资源**：\n * **VLA模型训练**：在配备单个A100-80GB GPU的集群虚拟机上进行微调，直至动作精度超过95%。\n * **扩散策略训练**：在自定义GPU上训练1500个周期。\n * **推理运行**：在Nvidia RTX 4090 GPU上运行，实现约5Hz的控制频率。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n * **硬件**：ADAPT Hand 2灵巧手与UR5机械臂。\n * **任务**：对三种不同几何形状的物体（红辣椒、磁带、一张纸）执行抓取-放置任务，放置到两个指定位置（黄色或紫色盘子）之一。\n * **传感器**：一个固定于世界坐标的摄像头和一个安装在手腕的摄像头。\n2. **评估指标**：\n * **VLA接近精度**：测量VLA控制手部移动到目标物体附近后，手部中心与物体中心在xy平面的偏移距离。\n * **扩散策略成功率**：测试当手部初始位置与物体有不同偏移（5, 10, 15cm）时的抓取成功率。\n * **多模态抓取能力**：评估模型能否根据物体在桌面上的不同位置选择合适的抓取策略（如滑动抓取或直接抓取）。\n * **任务整体成功率**：采用一个5级评分系统来评估完整的抓取-放置任务：1.00（完全成功）、0.75（未能放在正确盘子）、0.50（未能放在错误盘子）、0.25（未能抓取正确物体）、0.00（接近了错误物体）。\n * **对比基准**：将提出的VLA+扩散模型与仅使用VLA模型（使用预编程的抓取序列作为1-DoF夹爪代理）的方法进行对比。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>瑞士洛桑联邦理工学院机械工程学院</p>\n<h3>论文概述</h3>\n<p>该论文提出了一种结合微调的视觉-语言-动作（VLA）模型与扩散模型的混合控制方法，用于实现人形手的灵巧操作。VLA模型负责基于语言指令的高级规划，具有高泛化性；扩散模型则处理低级交互，为特定物体和环境提供所需的精度和鲁棒性。通过在训练数据中加入切换信号，实现了两个模型之间基于事件的转换，应用于通过语言指令指定目标物体和放置位置的抓取-放置任务。该方法在13自由度的ADAPT Hand 2上进行了部署，这是首次将VLA模型应用于多指手。实验表明，该混合方法的成功率超过80%，而仅使用VLA模型的成功率低于40%。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了一种混合控制框架，结合了VLA模型的高级规划能力和扩散模型的低级精确控制能力。</li><li>设计了一种基于事件信号的模型切换机制，实现了VLA模型和扩散模型之间的无缝转换。</li><li>首次成功地将VLA模型应用于控制多指灵巧手。</li><li>在真实的抓取-放置任务中，验证了混合框架的有效性，成功率远超仅使用VLA模型的基线。</li><li>展示了系统具备多种期望行为：多模态抓取、错误恢复能力以及通过硬件柔顺性带来的交互稳定性。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>框架设计</strong>：系统在VLA模型和扩散策略模型之间切换。VLA模型（微调的openVLA）根据语言指令和视觉反馈控制手臂的运动，使手部接近目标物体，其输出中的一个标量信号（抓取百分比）被重新用作切换事件信号。扩散策略模型负责精确的抓取动作，当其完成抓取后，通过另一个事件信号将控制权交还给VLA模型。</li><li><strong>机器人平台</strong>：使用定制的ADAPT Hand 2，一个13自由度、具有串联弹性驱动（SEA）的拟人化灵巧手，安装在UR5机械臂上。这种柔顺性设计允许机器人安全地与环境和物体交互。</li><li><strong>模型输入</strong>：VLA模型的视觉输入是来自一个固定摄像头和一个手腕摄像头的两幅图像，经过缩放后垂直拼接成一幅图像输入。扩散策略同样使用这两个摄像头的图像作为输入。</li><li><strong>模型选择</strong>：扩散策略模型是针对特定物体训练的，因此系统使用一个基于语言输入的查找表来为每个物体选择对应的扩散策略模型。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：所有数据均通过真实机器人遥操作收集。</p>\n<p> * <strong>VLA数据</strong>：记录完整的抓取-放置任务，但仅使用抓取动作前后的轨迹进行训练。针对每个物体和目标位置的组合，记录了20次试验。</p>\n<p> * <strong>扩散策略数据</strong>：仅记录抓取动作部分。对每个物体（辣椒、磁带、纸），记录了30到40次试验，包含不同的抓取策略（如滑动抓取、直接抓取）和失败恢复的演示。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> * <strong>VLA模型训练</strong>：在配备单个A100-80GB GPU的集群虚拟机上进行微调，直至动作精度超过95%。</p>\n<p> * <strong>扩散策略训练</strong>：在自定义GPU上训练1500个周期。</p>\n<p> * <strong>推理运行</strong>：在Nvidia RTX 4090 GPU上运行，实现约5Hz的控制频率。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> * <strong>硬件</strong>：ADAPT Hand 2灵巧手与UR5机械臂。</p>\n<p> * <strong>任务</strong>：对三种不同几何形状的物体（红辣椒、磁带、一张纸）执行抓取-放置任务，放置到两个指定位置（黄色或紫色盘子）之一。</p>\n<p> * <strong>传感器</strong>：一个固定于世界坐标的摄像头和一个安装在手腕的摄像头。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> * <strong>VLA接近精度</strong>：测量VLA控制手部移动到目标物体附近后，手部中心与物体中心在xy平面的偏移距离。</p>\n<p> * <strong>扩散策略成功率</strong>：测试当手部初始位置与物体有不同偏移（5, 10, 15cm）时的抓取成功率。</p>\n<p> * <strong>多模态抓取能力</strong>：评估模型能否根据物体在桌面上的不同位置选择合适的抓取策略（如滑动抓取或直接抓取）。</p>\n<p> * <strong>任务整体成功率</strong>：采用一个5级评分系统来评估完整的抓取-放置任务：1.00（完全成功）、0.75（未能放在正确盘子）、0.50（未能放在错误盘子）、0.25（未能抓取正确物体）、0.00（接近了错误物体）。</p>\n<p> * <strong>对比基准</strong>：将提出的VLA+扩散模型与仅使用VLA模型（使用预编程的抓取序列作为1-DoF夹爪代理）的方法进行对比。</p>"
  },
  {
    "date": "2024-10-15",
    "title": "Latent Action Pretraining from Videos",
    "link": "http://arxiv.org/abs/2410.11758",
    "summary_markdown": "好的，我已阅读并理解了论文内容。以下是根据要求生成的Markdown格式论文总结：\n\n```markdown\n### 论文研究单位\nKAIST、华盛顿大学、微软研究院、NVIDIA、艾伦人工智能研究所。\n### 论文概述\n论文提出了一种名为“潜动作预训练”（Latent Action Pretraining, LAPA）的无监督方法，用于在无需真实机器人动作标签的情况下预训练视觉-语言-动作（VLA）模型。该方法旨在从大规模无标签的互联网视频中学习机器人技能，克服现有VLA模型对人工遥操作数据的依赖，从而扩展数据来源和规模。LAPA通过学习离散的潜动作表示，并在少量机器人动作数据上微调，实现了对未见任务、对象和指令的泛化能力，性能超越了现有技术。\n### 论文核心贡献点\n1. 首次提出无监督的VLA预训练框架LAPA，利用无动作标签的视频数据。\n2. 实验表明LAPA在跨任务、跨环境和跨形态设置中显著优于现有基线方法，并在真实任务中超越当前最优VLA模型OpenVLA（+6.22%）。\n3. 证明了仅使用人类操作视频进行预训练的有效性，为利用网络规模数据构建机器人基础模型开辟了道路。\n4. 展示了LAPA可作为世界模型生成神经轨迹，支持闭环评估。\n### 论文方法描述\n方法分为三个阶段：\n1. **潜动作量化（Latent Action Quantization）**：使用基于VQ-VAE的目标训练编码器-解码器模型，从连续视频帧（当前帧xt和未来帧xt+H）中学习离散的潜动作表示zt，通过量化损失和码本实现动作标记化。\n2. **潜预训练（Latent Pretraining）**：将预训练的视觉-语言模型（VLM）的编码器作为逆动力学模型，预测语言指令和当前观察xt对应的潜动作zt，冻结视觉编码器并训练语言模型参数。\n3. **动作微调（Action Finetuning）**：在小规模机器人动作数据集上微调模型，替换潜动作头为真实动作头（如末端执行器增量动作），映射潜空间到机器人动作，冻结视觉编码器。\n### 论文使用数据集和训练资源\n- **数据集**：\n - 预训练：BridgeV2（机器人轨迹）、Open-X-Embodiment（多机器人数据集）、Something-Something V2（人类操作视频）。\n - 微调：Language Table（仿真推积木）、SIMPLER（仿真7-DOF WidowX臂）、真实世界桌面操作任务（7-DOF Franka Panda臂，3任务）。\n- **训练资源**：\n - 潜动作量化模型：使用NSVQ避免梯度崩溃，码本替换技术。\n - VLM骨干：7B Large World Model（LWM-Chat-1M）。\n - 预训练计算：8 H100 GPU训练34小时（272 GPU小时），比OpenVLA（21,500 A100小时）高效30倍。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n - 仿真：Language Table（2-DOF推任务）、SIMPLER（多任务仿真）。\n - 真实世界：Franka Emika Panda机械臂执行“拾取物体”、“覆盖物体”、“敲击物体”任务。\n- **评估指标**：\n - 平均成功率（Average Success Rate），标准差（StdErr），针对seen/unseen对象、组合和指令设置。\n - 任务部分成功率（Partial Success Criterion）。\n - 生成轨迹质量（通过闭环 rollout 评估）。\n```",
    "summary_html": "<p>好的，我已阅读并理解了论文内容。以下是根据要求生成的Markdown格式论文总结：</p>\n\n<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>KAIST、华盛顿大学、微软研究院、NVIDIA、艾伦人工智能研究所。</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为“潜动作预训练”（Latent Action Pretraining, LAPA）的无监督方法，用于在无需真实机器人动作标签的情况下预训练视觉-语言-动作（VLA）模型。该方法旨在从大规模无标签的互联网视频中学习机器人技能，克服现有VLA模型对人工遥操作数据的依赖，从而扩展数据来源和规模。LAPA通过学习离散的潜动作表示，并在少量机器人动作数据上微调，实现了对未见任务、对象和指令的泛化能力，性能超越了现有技术。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>首次提出无监督的VLA预训练框架LAPA，利用无动作标签的视频数据。</li><li>实验表明LAPA在跨任务、跨环境和跨形态设置中显著优于现有基线方法，并在真实任务中超越当前最优VLA模型OpenVLA（+6.22%）。</li><li>证明了仅使用人类操作视频进行预训练的有效性，为利用网络规模数据构建机器人基础模型开辟了道路。</li><li>展示了LAPA可作为世界模型生成神经轨迹，支持闭环评估。</li></ol>\n<h3>论文方法描述</h3>\n<p>方法分为三个阶段：</p>\n<ol><li><strong>潜动作量化（Latent Action Quantization）</strong>：使用基于VQ-VAE的目标训练编码器-解码器模型，从连续视频帧（当前帧xt和未来帧xt+H）中学习离散的潜动作表示zt，通过量化损失和码本实现动作标记化。</li><li><strong>潜预训练（Latent Pretraining）</strong>：将预训练的视觉-语言模型（VLM）的编码器作为逆动力学模型，预测语言指令和当前观察xt对应的潜动作zt，冻结视觉编码器并训练语言模型参数。</li><li><strong>动作微调（Action Finetuning）</strong>：在小规模机器人动作数据集上微调模型，替换潜动作头为真实动作头（如末端执行器增量动作），映射潜空间到机器人动作，冻结视觉编码器。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> - 预训练：BridgeV2（机器人轨迹）、Open-X-Embodiment（多机器人数据集）、Something-Something V2（人类操作视频）。</p>\n<p> - 微调：Language Table（仿真推积木）、SIMPLER（仿真7-DOF WidowX臂）、真实世界桌面操作任务（7-DOF Franka Panda臂，3任务）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> - 潜动作量化模型：使用NSVQ避免梯度崩溃，码本替换技术。</p>\n<p> - VLM骨干：7B Large World Model（LWM-Chat-1M）。</p>\n<p> - 预训练计算：8 H100 GPU训练34小时（272 GPU小时），比OpenVLA（21,500 A100小时）高效30倍。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> - 仿真：Language Table（2-DOF推任务）、SIMPLER（多任务仿真）。</p>\n<p> - 真实世界：Franka Emika Panda机械臂执行“拾取物体”、“覆盖物体”、“敲击物体”任务。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> - 平均成功率（Average Success Rate），标准差（StdErr），针对seen/unseen对象、组合和指令设置。</p>\n<p> - 任务部分成功率（Partial Success Criterion）。</p>\n<p> - 生成轨迹质量（通过闭环 rollout 评估）。</p>\n<p>```</p>"
  },
  {
    "date": "2024-10-10",
    "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2410.08001",
    "summary_markdown": "# 论文研究单位\n上海交通大学、香港大学、AgiBot、上海人工智能实验室\n# 论文概述\n论文提出RoboDual，一个协同双系统框架，结合通才策略和专才策略的优点用于机器人操作。通才策略基于大规模预训练视觉-语言-动作模型提供泛化和高级任务理解，专才策略基于轻量级扩散变换器提供高效实时控制。该框架解决了通才模型推理效率低和专才模型泛化能力弱的问题，实现性能提升和部署效率优化。\n# 论文核心贡献点\n- 提出RoboDual双系统协同框架，整合通才策略的泛化能力与专才策略的效率和精度\n- 设计基于扩散变换器的专才策略，通过统一条件机制自适应融合多模态输入和通才输出\n- 通过仿真和真实世界实验验证双系统方法在性能、泛化性和效率上优于单一模型\n# 论文方法描述\n- 通才模型：基于OpenVLA架构，使用LoRA微调，自回归生成离散动作块和潜在表示作为专才条件\n- 专才模型：采用扩散变换器架构，通过交叉注意力机制融合多感官输入、本体状态、通才离散动作和潜在表示\n- 训练协议：通才最小化负对数似然损失，专才最小化动作去噪均方误差\n- 推理协议：通才低频生成高级计划，专才高频执行精确控制，通过移位窗口条件保持时序一致性\n# 论文使用数据集和训练资源\n- 仿真数据集：CALVIN ABC→D基准测试\n- 真实世界数据集：ALOHA平台收集的单指令和多指令任务演示\n- 训练资源：8个NVIDIA A100 GPU，专才模型2000万可训练参数，训练时间8 GPU小时\n# 论文使用的评估环境和评估指标\n- 评估环境：CALVIN仿真环境、ALOHA真实机器人平台\n- 评估指标：任务成功率、平均任务完成长度、连续任务完成率、位置变化/视觉干扰/新背景/新物体泛化成功率\n- 效率指标：训练时间、推理延迟（控制频率3.8倍提升）",
    "summary_html": "<h1>论文研究单位</h1>\n<p>上海交通大学、香港大学、AgiBot、上海人工智能实验室</p>\n<h1>论文概述</h1>\n<p>论文提出RoboDual，一个协同双系统框架，结合通才策略和专才策略的优点用于机器人操作。通才策略基于大规模预训练视觉-语言-动作模型提供泛化和高级任务理解，专才策略基于轻量级扩散变换器提供高效实时控制。该框架解决了通才模型推理效率低和专才模型泛化能力弱的问题，实现性能提升和部署效率优化。</p>\n<h1>论文核心贡献点</h1>\n<ul><li>提出RoboDual双系统协同框架，整合通才策略的泛化能力与专才策略的效率和精度</li><li>设计基于扩散变换器的专才策略，通过统一条件机制自适应融合多模态输入和通才输出</li><li>通过仿真和真实世界实验验证双系统方法在性能、泛化性和效率上优于单一模型</li></ul>\n<h1>论文方法描述</h1>\n<ul><li>通才模型：基于OpenVLA架构，使用LoRA微调，自回归生成离散动作块和潜在表示作为专才条件</li><li>专才模型：采用扩散变换器架构，通过交叉注意力机制融合多感官输入、本体状态、通才离散动作和潜在表示</li><li>训练协议：通才最小化负对数似然损失，专才最小化动作去噪均方误差</li><li>推理协议：通才低频生成高级计划，专才高频执行精确控制，通过移位窗口条件保持时序一致性</li></ul>\n<h1>论文使用数据集和训练资源</h1>\n<ul><li>仿真数据集：CALVIN ABC→D基准测试</li><li>真实世界数据集：ALOHA平台收集的单指令和多指令任务演示</li><li>训练资源：8个NVIDIA A100 GPU，专才模型2000万可训练参数，训练时间8 GPU小时</li></ul>\n<h1>论文使用的评估环境和评估指标</h1>\n<ul><li>评估环境：CALVIN仿真环境、ALOHA真实机器人平台</li><li>评估指标：任务成功率、平均任务完成长度、连续任务完成率、位置变化/视觉干扰/新背景/新物体泛化成功率</li><li>效率指标：训练时间、推理延迟（控制频率3.8倍提升）</li></ul>"
  },
  {
    "date": "2024-09-12",
    "title": "HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers",
    "link": "http://arxiv.org/abs/2410.05273",
    "summary_markdown": "论文研究单位\n清华大学交叉信息研究院、加州大学伯克利分校、上海期智研究院。\n\n论文概述\n论文提出了一种名为HiRT（分层机器人Transformer）的框架，旨在通过结合大型视觉-语言模型（VLM）的语义理解能力和轻量级策略模型的快速反应能力，解决现有大型VLA模型因计算开销大导致的控制频率低和推理延迟高的问题。HiRT通过分层策略，让VLM在低频率下运行以捕获时不变特征，同时由一个高频的基于视觉的策略在VLM特征的引导下进行实时交互，从而在准静态和动态任务中实现高性能和高频率控制。\n\n论文核心贡献点\n1. 提出了一种分层交互式模仿学习框架HiRT，通过解耦慢速的语义理解和快速的实时控制，显著提升了机器人控制频率并降低延迟。\n2. 设计了一种新颖的条件化策略，包括FiLM条件层、交叉注意力层和前缀调优，有效整合VLM的高级语义信息到轻量级低层策略中。\n3. 在模拟和真实世界的静态和动态操作任务上进行了广泛实验，证明了HiRT相比基线方法在成功率、控制频率和动态任务性能上的显著提升。\n\n论文方法描述\nHiRT框架包含两个主要模块：理解模块和执行模块。理解模块使用预训练的InstructBLIP（7B）模型，将视觉观察和语言指令编码为融合了多模态信息的潜在特征向量，该过程通过ViT编码图像，Q-Former融合视觉与语言特征，再由LLM处理并通过一个MAP模块聚合输出。执行模块是一个紧凑的基于视觉的策略模型，它以更高的频率处理近期的视觉观察历史，并利用理解模块最新更新的潜在特征作为条件。具体条件化策略包括：在CNN架构（如EfficientNet）中使用FiLM层对特征进行缩放和偏置；在Transformer架构中，通过额外的交叉注意力层注入VLM潜在信息；在动作头中，将VLM潜在向量作为前缀与视觉特征一同输入到MAP模块。训练时，两个模块联合优化；推理时，VLM以较低频率异步更新潜在特征，指导高频率运行的执行模块生成动作。\n\n论文使用数据集和训练资源\n论文使用了现有的多任务机器人操作数据集进行训练和评估。训练在NVIDIA A100 GPU上进行。\n\n论文使用的评估环境和评估指标\n评估在模拟环境（如Calvin任务）和真实世界的静态及动态操作任务中进行。评估指标包括任务成功率（Success Rate）和控制频率（Hz）。在静态任务中，HiRT在将控制频率提高一倍的同时保持了与基线相当的成功率；在真实世界动态操作任务中，HiRT将成功率从48%提升至75%。",
    "summary_html": "<p>论文研究单位</p>\n<p>清华大学交叉信息研究院、加州大学伯克利分校、上海期智研究院。</p>\n\n<p>论文概述</p>\n<p>论文提出了一种名为HiRT（分层机器人Transformer）的框架，旨在通过结合大型视觉-语言模型（VLM）的语义理解能力和轻量级策略模型的快速反应能力，解决现有大型VLA模型因计算开销大导致的控制频率低和推理延迟高的问题。HiRT通过分层策略，让VLM在低频率下运行以捕获时不变特征，同时由一个高频的基于视觉的策略在VLM特征的引导下进行实时交互，从而在准静态和动态任务中实现高性能和高频率控制。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种分层交互式模仿学习框架HiRT，通过解耦慢速的语义理解和快速的实时控制，显著提升了机器人控制频率并降低延迟。</li><li>设计了一种新颖的条件化策略，包括FiLM条件层、交叉注意力层和前缀调优，有效整合VLM的高级语义信息到轻量级低层策略中。</li><li>在模拟和真实世界的静态和动态操作任务上进行了广泛实验，证明了HiRT相比基线方法在成功率、控制频率和动态任务性能上的显著提升。</li></ol>\n\n<p>论文方法描述</p>\n<p>HiRT框架包含两个主要模块：理解模块和执行模块。理解模块使用预训练的InstructBLIP（7B）模型，将视觉观察和语言指令编码为融合了多模态信息的潜在特征向量，该过程通过ViT编码图像，Q-Former融合视觉与语言特征，再由LLM处理并通过一个MAP模块聚合输出。执行模块是一个紧凑的基于视觉的策略模型，它以更高的频率处理近期的视觉观察历史，并利用理解模块最新更新的潜在特征作为条件。具体条件化策略包括：在CNN架构（如EfficientNet）中使用FiLM层对特征进行缩放和偏置；在Transformer架构中，通过额外的交叉注意力层注入VLM潜在信息；在动作头中，将VLM潜在向量作为前缀与视觉特征一同输入到MAP模块。训练时，两个模块联合优化；推理时，VLM以较低频率异步更新潜在特征，指导高频率运行的执行模块生成动作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>论文使用了现有的多任务机器人操作数据集进行训练和评估。训练在NVIDIA A100 GPU上进行。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估在模拟环境（如Calvin任务）和真实世界的静态及动态操作任务中进行。评估指标包括任务成功率（Success Rate）和控制频率（Hz）。在静态任务中，HiRT在将控制频率提高一倍的同时保持了与基线相当的成功率；在真实世界动态操作任务中，HiRT将成功率从48%提升至75%。</p>"
  },
  {
    "date": "2024-10-07",
    "title": "LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation",
    "link": "http://arxiv.org/abs/2410.05191",
    "summary_markdown": "```markdown\n### 论文研究单位\nUniversity of Alberta, Edmonton, AB, Canada\nThe University of Tokyo, Tokyo, Japan\n### 论文概述\n本文提出了LADEV，一个语言驱动的测试和评估平台，用于机器人操作中的视觉-语言-动作（VLA）模型。该平台通过自然语言输入自动生成仿真环境，并对任务指令进行转述，以全面评估VLA模型的性能。相比现有方法如SimplerEnv（需手动调整环境且忽略语言输入变化），LADEV提高了测试效率并填补了语言多样性评估的空白。实验使用超过4000个场景测试了七个VLA模型，证明了其有效性。\n### 论文核心贡献点\n- 提出语言驱动方法，从自然语言描述自动生成仿真环境，减少手动配置需求。\n- 实现任务指令转述机制，生成多样化的语言输入以评估模型的语言鲁棒性。\n- 引入批量评估方法，通过单一输入生成多个测试场景，支持大规模测试。\n- 对多个VLA模型进行全面评估，涵盖四个任务和4000+场景，建立性能基线。\n### 论文方法描述\n- **语言驱动的仿真环境生成**：\n 使用LLM将自然语言描述（对象数量、细节、环境设置）转换为仿真器兼容配置。过程分两步：对象配置（基于YCB和SimplerEnv数据集选择模型）和环境设置（调整光照和相机位置）。采用少样本上下文学习确保输出格式兼容。\n- **自然语言任务指令转述**：\n 生成阶段用LLM基于原始指令创建k个变体；验证阶段用sentence BERT计算语义相似度，超过阈值则保留有效指令。\n- **批量评估**：\n LLM生成n个场景描述和原始指令，每个指令转述为k个变体，共n×k个输入，实现高效大规模测试。\n### 论文使用数据集和训练资源\n- **数据集**：\n YCB对象数据集（65个对象，视为未见对象）和SimplerEnv默认数据集（18个对象，视为训练集覆盖对象）。\n- **训练资源**：\n 实验使用GPT-4o作为LLM，服务器配置为AMD 5955WX CPU和两个NVIDIA RTX A6000 GPU。平台未训练新模型，仅用于评估。\n### 论文使用的评估环境和评估指标\n- **评估环境**：\n 基于SimplerEnv，构建于SAPIEN仿真器和ManiSkill2基准。\n- **评估任务**：\n 四个机器人操作任务：拾取物体、移动物体A到物体B附近、将物体A放在物体B上、将物体A放入物体B内。\n- **评估指标**：\n 成功率（任务完成百分比），评估条件包括对象数量（1-5个）、任务指令（基本vs转述）、对象类型（SimplerEnv vs YCB）、环境条件（默认光照、变化光照、相机位置变化）。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>University of Alberta, Edmonton, AB, Canada</p>\n<p>The University of Tokyo, Tokyo, Japan</p>\n<h3>论文概述</h3>\n<p>本文提出了LADEV，一个语言驱动的测试和评估平台，用于机器人操作中的视觉-语言-动作（VLA）模型。该平台通过自然语言输入自动生成仿真环境，并对任务指令进行转述，以全面评估VLA模型的性能。相比现有方法如SimplerEnv（需手动调整环境且忽略语言输入变化），LADEV提高了测试效率并填补了语言多样性评估的空白。实验使用超过4000个场景测试了七个VLA模型，证明了其有效性。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出语言驱动方法，从自然语言描述自动生成仿真环境，减少手动配置需求。</li><li>实现任务指令转述机制，生成多样化的语言输入以评估模型的语言鲁棒性。</li><li>引入批量评估方法，通过单一输入生成多个测试场景，支持大规模测试。</li><li>对多个VLA模型进行全面评估，涵盖四个任务和4000+场景，建立性能基线。</li></ul>\n<h3>论文方法描述</h3>\n<ul><li><strong>语言驱动的仿真环境生成</strong>：</li></ul>\n<p> 使用LLM将自然语言描述（对象数量、细节、环境设置）转换为仿真器兼容配置。过程分两步：对象配置（基于YCB和SimplerEnv数据集选择模型）和环境设置（调整光照和相机位置）。采用少样本上下文学习确保输出格式兼容。</p>\n<ul><li><strong>自然语言任务指令转述</strong>：</li></ul>\n<p> 生成阶段用LLM基于原始指令创建k个变体；验证阶段用sentence BERT计算语义相似度，超过阈值则保留有效指令。</p>\n<ul><li><strong>批量评估</strong>：</li></ul>\n<p> LLM生成n个场景描述和原始指令，每个指令转述为k个变体，共n×k个输入，实现高效大规模测试。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：</li></ul>\n<p> YCB对象数据集（65个对象，视为未见对象）和SimplerEnv默认数据集（18个对象，视为训练集覆盖对象）。</p>\n<ul><li><strong>训练资源</strong>：</li></ul>\n<p> 实验使用GPT-4o作为LLM，服务器配置为AMD 5955WX CPU和两个NVIDIA RTX A6000 GPU。平台未训练新模型，仅用于评估。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：</li></ul>\n<p> 基于SimplerEnv，构建于SAPIEN仿真器和ManiSkill2基准。</p>\n<ul><li><strong>评估任务</strong>：</li></ul>\n<p> 四个机器人操作任务：拾取物体、移动物体A到物体B附近、将物体A放在物体B上、将物体A放入物体B内。</p>\n<ul><li><strong>评估指标</strong>：</li></ul>\n<p> 成功率（任务完成百分比），评估条件包括对象数量（1-5个）、任务指令（基本vs转述）、对象类型（SimplerEnv vs YCB）、环境条件（默认光照、变化光照、相机位置变化）。</p>\n<p>```</p>"
  },
  {
    "date": "2024-10-02",
    "title": "Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust",
    "link": "http://arxiv.org/abs/2410.01971",
    "summary_markdown": "```markdown\n### 论文研究单位\n普林斯顿大学机械与航空航天工程系\n### 论文概述\n本文提出了一种名为“Bring Your Own VLA”（BYOVLA）的运行时干预方案，旨在提高视觉-语言-动作（VLA）模型在视觉干扰下的鲁棒性。VLAs虽然在大规模数据上训练，但对任务无关的视觉细节（如干扰物或背景颜色）非常敏感。该方法通过动态识别VLA模型敏感的输入图像区域，并使用自动化图像编辑工具对任务无关区域进行最小化修改来降低模型的敏感性，且无需对模型进行微调或访问模型权重。\n### 论文核心贡献点\n- 提出了BYOVLA，一种通用的、无需训练的运行时干预框架，适用于任何现成的VLA模型。\n- 设计了一种视觉敏感性探针，通过扰动图像不同区段并测量动作输出的变化，来直接评估VLA对特定区域的敏感度。\n- 在包含干扰物和背景变化的多种操作任务中，BYOVLA将任务成功率提高了20-40%，显著优于现有基线方法（如GradCAM）。\n- 方法仅需VLM和分割模型识别任务无关区域，并自动进行选择性图像编辑，无需重新训练VLA。\n### 论文方法描述\nBYOVLA方法包含三个主要步骤：\n1. **定位任务无关对象**：使用GPT-4o等视觉语言模型（VLM）根据语言指令识别初始图像中的任务无关区域，并通过Grounded-SAM2等分割模型获得像素级掩码。\n2. **应用视觉敏感性探针**：对于每个任务无关区域，通过添加高斯模糊（针对对象干扰）或高斯噪声（针对背景干扰）来扰动图像，计算扰动前后VLA输出动作序列的加权L2范数变化（公式2），量化模型对该区域的敏感度。\n3. **确定敏感度阈值并转换图像**：若某区域的敏感度值高于预设阈值（对象干扰为0.002m，背景干扰为0.001m），则进行干预。对于对象干扰，使用Inpaint Anything模型进行修复移除；对于背景干扰，直接修改RGB像素值。\n### 论文使用数据集和训练资源\n- **数据集**：BridgeV2数据集（用于确定敏感度阈值）和自定义的厨房环境任务。\n- **VLA模型**：Octo-Base和OpenVLA（作为BYOVLA的测试平台）。\n- **预训练模型**：GPT-4o（用于识别任务无关区域）、Grounded-SAM2（用于分割）、Inpaint Anything（用于图像修复）。\n- **训练资源**：未对VLA模型进行训练或微调；BYOVLA作为运行时模块，计算资源消耗主要来自VLM和图像编辑工具的推理开销。\n### 论文使用的评估环境和评估指标\n- **评估环境**：硬件实验在物理机器人平台上进行，包含语言指令的操控任务，环境设置包括任务无关的干扰对象（如额外物体）和背景变化（如颜色改变）。\n- **评估指标**：任务成功率（Task Success Rate），即完成指定语言指令的比例。实验比较了原始VLA、BYOVLA增强的VLA以及基线方法（如GradCAM）在干扰环境下的表现。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>普林斯顿大学机械与航空航天工程系</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为“Bring Your Own VLA”（BYOVLA）的运行时干预方案，旨在提高视觉-语言-动作（VLA）模型在视觉干扰下的鲁棒性。VLAs虽然在大规模数据上训练，但对任务无关的视觉细节（如干扰物或背景颜色）非常敏感。该方法通过动态识别VLA模型敏感的输入图像区域，并使用自动化图像编辑工具对任务无关区域进行最小化修改来降低模型的敏感性，且无需对模型进行微调或访问模型权重。</p>\n<h3>论文核心贡献点</h3>\n<ul><li>提出了BYOVLA，一种通用的、无需训练的运行时干预框架，适用于任何现成的VLA模型。</li><li>设计了一种视觉敏感性探针，通过扰动图像不同区段并测量动作输出的变化，来直接评估VLA对特定区域的敏感度。</li><li>在包含干扰物和背景变化的多种操作任务中，BYOVLA将任务成功率提高了20-40%，显著优于现有基线方法（如GradCAM）。</li><li>方法仅需VLM和分割模型识别任务无关区域，并自动进行选择性图像编辑，无需重新训练VLA。</li></ul>\n<h3>论文方法描述</h3>\n<p>BYOVLA方法包含三个主要步骤：</p>\n<ol><li><strong>定位任务无关对象</strong>：使用GPT-4o等视觉语言模型（VLM）根据语言指令识别初始图像中的任务无关区域，并通过Grounded-SAM2等分割模型获得像素级掩码。</li><li><strong>应用视觉敏感性探针</strong>：对于每个任务无关区域，通过添加高斯模糊（针对对象干扰）或高斯噪声（针对背景干扰）来扰动图像，计算扰动前后VLA输出动作序列的加权L2范数变化（公式2），量化模型对该区域的敏感度。</li><li><strong>确定敏感度阈值并转换图像</strong>：若某区域的敏感度值高于预设阈值（对象干扰为0.002m，背景干扰为0.001m），则进行干预。对于对象干扰，使用Inpaint Anything模型进行修复移除；对于背景干扰，直接修改RGB像素值。</li></ol>\n<h3>论文使用数据集和训练资源</h3>\n<ul><li><strong>数据集</strong>：BridgeV2数据集（用于确定敏感度阈值）和自定义的厨房环境任务。</li><li><strong>VLA模型</strong>：Octo-Base和OpenVLA（作为BYOVLA的测试平台）。</li><li><strong>预训练模型</strong>：GPT-4o（用于识别任务无关区域）、Grounded-SAM2（用于分割）、Inpaint Anything（用于图像修复）。</li><li><strong>训练资源</strong>：未对VLA模型进行训练或微调；BYOVLA作为运行时模块，计算资源消耗主要来自VLM和图像编辑工具的推理开销。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<ul><li><strong>评估环境</strong>：硬件实验在物理机器人平台上进行，包含语言指令的操控任务，环境设置包括任务无关的干扰对象（如额外物体）和背景变化（如颜色改变）。</li><li><strong>评估指标</strong>：任务成功率（Task Success Rate），即完成指定语言指令的比例。实验比较了原始VLA、BYOVLA增强的VLA以及基线方法（如GradCAM）在干扰环境下的表现。</li></ul>\n<p>```</p>"
  },
  {
    "date": "2024-09-29",
    "title": "RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2409.19590",
    "summary_markdown": "论文研究单位\nMulti-Scale Medical Robotics Centre, Ltd., The Chinese University of Hong Kong; Humanoids and Human-Centered Mechatronics (HHCM), Istituto Italiano di Tecnologia; Department of Surgery, The Chinese University of Hong Kong.\n\n论文概述\n本文介绍了RoboNurse-VLA，一个基于视觉-语言-动作模型的机器人洗手护士系统。该系统通过集成Segment Anything Model 2 (SAM 2)和Llama 2语言模型，旨在解决外科手术中器械传递的挑战，如准确抓取和传递结构复杂或难以抓取的器械。RoboNurse-VLA能够根据外科医生的语音命令，实现手术器械的实时高精度抓取和传递。通过广泛的评估，该系统在手术器械传递任务上表现出优越性能，即使面对未见过的器械和具有挑战性的物品也能达到高成功率，展现了其在自主手术辅助中的潜力。\n\n论文核心贡献点\n开发了一种创新的VLA模型，并首次应用于外科手术器械传递任务，其性能超越了现有最先进的模型。\n设计了一个基于SAM 2的视觉模块，并将其与VLA模型集成，实现了对具有挑战性物品的语言引导抓取。\n有效解决了区分相似物品、优化抓取姿势和传递姿势等关键挑战。\n\n论文方法描述\nRoboNurse-VLA系统架构包含三个主要部分：一个基于SAM 2的视觉模块、一个将视觉特征映射到语言嵌入空间的投影器（MLP）、以及一个预训练的Llama 2 7B参数语言模型作为核心主干。\n输入包括深度相机的视觉数据和外科医生的语音命令。语音命令通过Gladia ASR转录为文本。\n在视觉模块中，一个预训练的YOLOv8模型首先检测目标器械和外科医生的手，为SAM 2提供边界框。SAM 2随后对这两个物体进行分割。\nSAM 2图像编码器输出的特征和掩码解码器生成的掩码，通过一个MLP投影后，作为输入提供给Llama 2。Llama 2根据文本提示和视觉信息生成机器人动作指令，这些指令被解析后发送给UR5机械臂执行。\n在训练阶段，视觉模块被冻结，使用LoRA（Low-Rank Adaptation）方法对OpenVLA中的Llama 2模型进行微调。\n\n论文使用数据集和训练资源\n数据集：\n1. 一个包含6种视觉上相似手术器械和手部图像的自定义数据集，共700张，分辨率为6000x4000像素，使用LabelImg工具手动标注。\n2. 一个包含648个演示轨迹的数据集，用于洗手护士传递任务，数据采集时考虑了物体放置、相机角度、手部姿势和布局的多样性。\n训练资源：模型微调在单个A100 GPU上花费了20小时完成。\n\n论文使用的评估环境和评估指标\n评估环境：\n硬件包括麦克风、一张放置6种手术器械的桌子、Intel RealSense Depth Camera D415深度相机、一个UR5机械臂和一台用于实时推理的工作站。控制系统为30Hz的关节位置控制。\n评估任务：\n零样本性能测试：测试模型在未经训练的情况下，根据指令举起或传递单个或多个器械的能力。\n微调后性能测试：评估模型在标准场景、手部高度变化、手部姿势变化下的传递性能。\n泛化能力测试：评估模型对未见过的器械和难以抓取的物品（如乒乓球）的处理能力。\n评估指标：\n主要指标为任务成功率，以百分比形式表示。\n辅助模型评估指标：Gladia ASR的准确率；YOLOv8检测器的平均精度和召回率；SAM 2分割的Dice分数、IoU（交并比）和MAE（平均绝对误差）。",
    "summary_html": "<p>论文研究单位</p>\n<p>Multi-Scale Medical Robotics Centre, Ltd., The Chinese University of Hong Kong; Humanoids and Human-Centered Mechatronics (HHCM), Istituto Italiano di Tecnologia; Department of Surgery, The Chinese University of Hong Kong.</p>\n\n<p>论文概述</p>\n<p>本文介绍了RoboNurse-VLA，一个基于视觉-语言-动作模型的机器人洗手护士系统。该系统通过集成Segment Anything Model 2 (SAM 2)和Llama 2语言模型，旨在解决外科手术中器械传递的挑战，如准确抓取和传递结构复杂或难以抓取的器械。RoboNurse-VLA能够根据外科医生的语音命令，实现手术器械的实时高精度抓取和传递。通过广泛的评估，该系统在手术器械传递任务上表现出优越性能，即使面对未见过的器械和具有挑战性的物品也能达到高成功率，展现了其在自主手术辅助中的潜力。</p>\n\n<p>论文核心贡献点</p>\n<p>开发了一种创新的VLA模型，并首次应用于外科手术器械传递任务，其性能超越了现有最先进的模型。</p>\n<p>设计了一个基于SAM 2的视觉模块，并将其与VLA模型集成，实现了对具有挑战性物品的语言引导抓取。</p>\n<p>有效解决了区分相似物品、优化抓取姿势和传递姿势等关键挑战。</p>\n\n<p>论文方法描述</p>\n<p>RoboNurse-VLA系统架构包含三个主要部分：一个基于SAM 2的视觉模块、一个将视觉特征映射到语言嵌入空间的投影器（MLP）、以及一个预训练的Llama 2 7B参数语言模型作为核心主干。</p>\n<p>输入包括深度相机的视觉数据和外科医生的语音命令。语音命令通过Gladia ASR转录为文本。</p>\n<p>在视觉模块中，一个预训练的YOLOv8模型首先检测目标器械和外科医生的手，为SAM 2提供边界框。SAM 2随后对这两个物体进行分割。</p>\n<p>SAM 2图像编码器输出的特征和掩码解码器生成的掩码，通过一个MLP投影后，作为输入提供给Llama 2。Llama 2根据文本提示和视觉信息生成机器人动作指令，这些指令被解析后发送给UR5机械臂执行。</p>\n<p>在训练阶段，视觉模块被冻结，使用LoRA（Low-Rank Adaptation）方法对OpenVLA中的Llama 2模型进行微调。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集：</p>\n<ol><li>一个包含6种视觉上相似手术器械和手部图像的自定义数据集，共700张，分辨率为6000x4000像素，使用LabelImg工具手动标注。</li><li>一个包含648个演示轨迹的数据集，用于洗手护士传递任务，数据采集时考虑了物体放置、相机角度、手部姿势和布局的多样性。</li></ol>\n<p>训练资源：模型微调在单个A100 GPU上花费了20小时完成。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境：</p>\n<p>硬件包括麦克风、一张放置6种手术器械的桌子、Intel RealSense Depth Camera D415深度相机、一个UR5机械臂和一台用于实时推理的工作站。控制系统为30Hz的关节位置控制。</p>\n<p>评估任务：</p>\n<p>零样本性能测试：测试模型在未经训练的情况下，根据指令举起或传递单个或多个器械的能力。</p>\n<p>微调后性能测试：评估模型在标准场景、手部高度变化、手部姿势变化下的传递性能。</p>\n<p>泛化能力测试：评估模型对未见过的器械和难以抓取的物品（如乒乓球）的处理能力。</p>\n<p>评估指标：</p>\n<p>主要指标为任务成功率，以百分比形式表示。</p>\n<p>辅助模型评估指标：Gladia ASR的准确率；YOLOv8检测器的平均精度和召回率；SAM 2分割的Dice分数、IoU（交并比）和MAE（平均绝对误差）。</p>"
  },
  {
    "date": "2024-09-19",
    "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation",
    "link": "http://arxiv.org/abs/2409.12514",
    "summary_markdown": "```markdown\n论文研究单位\n华东师范大学、美的集团AI实验室、北京人形机器人创新中心、雪城大学、上海大学\n\n论文概述\n针对现有视觉-语言-动作（VLA）模型推理速度慢且依赖大规模机器人数据预训练的问题，提出TinyVLA紧凑模型系列，通过结合轻量级多模态模型与扩散策略解码器，实现快速推理和数据高效学习，无需预训练阶段即可适应新指令和环境。\n\n论文核心贡献点\n1. 提出结合轻量视觉-语言模型与扩散模型的新架构，支持快速推理和强泛化。\n2. 在模拟和真实机器人上进行广泛实验，涵盖单臂和双臂任务。\n3. 证明无需大规模机器人数据即可训练高性能VLA模型，实现数据效率。\n\n论文方法描述\n1. 使用参数少于10亿的预训练视觉-语言模型（如基于Pythia的LLaVA变体）初始化策略网络。\n2. 机器人数据微调时冻结预训练权重，采用LoRA低秩适应技术，仅训练5%参数。\n3. 添加扩散策略解码器，通过去噪过程直接输出连续动作序列，避免自回归动作令牌生成。\n\n论文使用数据集和训练资源\n1. 模拟：MetaWorld数据集的50个任务。\n2. 真实机器人：单臂Franka Panda（5个任务）和双臂UR5（3个任务），每个任务收集100条轨迹。\n3. 训练资源：A6000 GPU，预训练VLM用公开视觉-语言数据集，无需机器人数据预训练。\n\n论文使用的评估环境和评估指标\n1. 环境：MetaWorld模拟器、Franka Panda真实机器人、UR5双臂机器人。\n2. 指标：任务成功率、平均成功率、推理延迟（毫秒），评估多任务学习、未见指令泛化、空间/视觉/背景泛化能力。\n```",
    "summary_html": "<p>```markdown</p>\n<p>论文研究单位</p>\n<p>华东师范大学、美的集团AI实验室、北京人形机器人创新中心、雪城大学、上海大学</p>\n\n<p>论文概述</p>\n<p>针对现有视觉-语言-动作（VLA）模型推理速度慢且依赖大规模机器人数据预训练的问题，提出TinyVLA紧凑模型系列，通过结合轻量级多模态模型与扩散策略解码器，实现快速推理和数据高效学习，无需预训练阶段即可适应新指令和环境。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出结合轻量视觉-语言模型与扩散模型的新架构，支持快速推理和强泛化。</li><li>在模拟和真实机器人上进行广泛实验，涵盖单臂和双臂任务。</li><li>证明无需大规模机器人数据即可训练高性能VLA模型，实现数据效率。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>使用参数少于10亿的预训练视觉-语言模型（如基于Pythia的LLaVA变体）初始化策略网络。</li><li>机器人数据微调时冻结预训练权重，采用LoRA低秩适应技术，仅训练5%参数。</li><li>添加扩散策略解码器，通过去噪过程直接输出连续动作序列，避免自回归动作令牌生成。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>模拟：MetaWorld数据集的50个任务。</li><li>真实机器人：单臂Franka Panda（5个任务）和双臂UR5（3个任务），每个任务收集100条轨迹。</li><li>训练资源：A6000 GPU，预训练VLM用公开视觉-语言数据集，无需机器人数据预训练。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>环境：MetaWorld模拟器、Franka Panda真实机器人、UR5双臂机器人。</li><li>指标：任务成功率、平均成功率、推理延迟（毫秒），评估多任务学习、未见指令泛化、空间/视觉/背景泛化能力。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2024-09-05",
    "title": "OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving",
    "link": "http://arxiv.org/abs/2409.03272",
    "summary_markdown": "论文研究单位\n未在提供的文本中明确说明。\n\n论文概述\n该论文提出了OccLLaMA，一个用于自动驾驶的占用-语言-动作生成式世界模型。该模型使用语义占用作为通用的3D视觉表示，并通过一个自回归模型统一视觉、语言和动作等多种模态。OccLLaMA旨在模仿人类基于内部3D视觉表示模拟未来状态并规划行动的能力，从而在一个统一的框架内处理场景理解、4D占用预测和运动规划等多项任务。\n\n论文核心贡献点\n1. 提出了OccLLaMA，一个以语义占用为视觉表示，并通过统一的多模态词汇表和基于LLaMA的增强自回归模型来处理多任务的生成式世界模型。\n2. 引入了一种新颖的场景标记器，该标记器考虑了占用场景的稀疏性和类别不平衡，能够高效地对其进行离散化和重建。\n3. 在多个任务上进行了广泛的实验，包括4D占用预测、运动规划和视觉问答，均取得了具有竞争力的性能，展示了其作为自动驾驶基础模型的潜力。\n\n论文方法描述\n1. 场景标记器：\n - 编码器：采用稀疏编码策略，将非空气体素视为一维伪点云，并通过类似PointPillars的嵌入和Swin Transformer块生成鸟瞰图特征。\n - 量化：通过向量量化将特征映射到可学习的码本中，以获得离散表示。\n - 解码器：使用卷积和上采样层恢复密集的3D体素特征。为解决类别不平衡问题，分别设计了轻量级的体素头和类别头，其中体素头为类别头提供占用掩码。\n - 损失函数：结合了用于几何和语义重建的交叉熵损失、Lovasz-Softmax损失以及用于码本学习的嵌入损失。\n2. 生成式世界模型：\n - 统一词汇表：为场景、文本和动作分别构建词汇表，并与一些特殊功能标记（如`<occ>`、`</occ>`、`<act>`、`</act>`）合并，构成一个统一的多模态词汇表。\n - 下一个词元/场景预测：对语言和动作序列使用标准的带因果掩码的下一个词元预测。对于场景序列，采用下一个场景预测机制，通过可学习的场景查询和空间注意力，在一次前向传播中预测整个场景，以捕捉空间关系并降低计算成本。\n3. 训练阶段：\n - 场景标记器训练：独立训练场景标记器以学习场景码本。\n - 占用-语言-动作预训练：在统一词汇表上对LLaMA进行预训练，执行下一个词元/场景预测任务。\n - 指令微调：使用指令-响应对对模型进行微调，使其能够遵循特定指令完成不同任务。\n\n论文使用数据集和训练资源\n1. 数据集：nuScenes。\n2. 训练资源：模型基于LLaMA-2-7B构建，在32个A100 GPU上进行训练。\n\n论文使用的评估环境和评估指标\n1. 4D Occupancy Forecasting：\n - 评估指标：交并比（IOU）和平均交并比（MIOU）。\n - 评估细节：在1秒、2秒和3秒的未来时间点进行预测，并计算与真实情况的IOU和MIOU。\n2. Motion Planning：\n - 评估环境：CARLA仿真器中的闭环评估器。\n - 评估指标：L2误差、碰撞率和任务成功率。\n3. Visual Question Answering：\n - 评估环境：遵循DR-Bench的评估协议。\n - 评估指标：BLEU、ROUGE、METEOR和CIDEr。",
    "summary_html": "<p>论文研究单位</p>\n<p>未在提供的文本中明确说明。</p>\n\n<p>论文概述</p>\n<p>该论文提出了OccLLaMA，一个用于自动驾驶的占用-语言-动作生成式世界模型。该模型使用语义占用作为通用的3D视觉表示，并通过一个自回归模型统一视觉、语言和动作等多种模态。OccLLaMA旨在模仿人类基于内部3D视觉表示模拟未来状态并规划行动的能力，从而在一个统一的框架内处理场景理解、4D占用预测和运动规划等多项任务。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了OccLLaMA，一个以语义占用为视觉表示，并通过统一的多模态词汇表和基于LLaMA的增强自回归模型来处理多任务的生成式世界模型。</li><li>引入了一种新颖的场景标记器，该标记器考虑了占用场景的稀疏性和类别不平衡，能够高效地对其进行离散化和重建。</li><li>在多个任务上进行了广泛的实验，包括4D占用预测、运动规划和视觉问答，均取得了具有竞争力的性能，展示了其作为自动驾驶基础模型的潜力。</li></ol>\n\n<p>论文方法描述</p>\n<p>1. 场景标记器：</p>\n<p> - 编码器：采用稀疏编码策略，将非空气体素视为一维伪点云，并通过类似PointPillars的嵌入和Swin Transformer块生成鸟瞰图特征。</p>\n<p> - 量化：通过向量量化将特征映射到可学习的码本中，以获得离散表示。</p>\n<p> - 解码器：使用卷积和上采样层恢复密集的3D体素特征。为解决类别不平衡问题，分别设计了轻量级的体素头和类别头，其中体素头为类别头提供占用掩码。</p>\n<p> - 损失函数：结合了用于几何和语义重建的交叉熵损失、Lovasz-Softmax损失以及用于码本学习的嵌入损失。</p>\n<p>2. 生成式世界模型：</p>\n<p> - 统一词汇表：为场景、文本和动作分别构建词汇表，并与一些特殊功能标记（如<code><occ></code>、<code></occ></code>、<code><act></code>、<code></act></code>）合并，构成一个统一的多模态词汇表。</p>\n<p> - 下一个词元/场景预测：对语言和动作序列使用标准的带因果掩码的下一个词元预测。对于场景序列，采用下一个场景预测机制，通过可学习的场景查询和空间注意力，在一次前向传播中预测整个场景，以捕捉空间关系并降低计算成本。</p>\n<p>3. 训练阶段：</p>\n<p> - 场景标记器训练：独立训练场景标记器以学习场景码本。</p>\n<p> - 占用-语言-动作预训练：在统一词汇表上对LLaMA进行预训练，执行下一个词元/场景预测任务。</p>\n<p> - 指令微调：使用指令-响应对对模型进行微调，使其能够遵循特定指令完成不同任务。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：nuScenes。</li><li>训练资源：模型基于LLaMA-2-7B构建，在32个A100 GPU上进行训练。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>1. 4D Occupancy Forecasting：</p>\n<p> - 评估指标：交并比（IOU）和平均交并比（MIOU）。</p>\n<p> - 评估细节：在1秒、2秒和3秒的未来时间点进行预测，并计算与真实情况的IOU和MIOU。</p>\n<p>2. Motion Planning：</p>\n<p> - 评估环境：CARLA仿真器中的闭环评估器。</p>\n<p> - 评估指标：L2误差、碰撞率和任务成功率。</p>\n<p>3. Visual Question Answering：</p>\n<p> - 评估环境：遵循DR-Bench的评估协议。</p>\n<p> - 评估指标：BLEU、ROUGE、METEOR和CIDEr。</p>"
  },
  {
    "date": "2024-08-19",
    "title": "CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving",
    "link": "http://arxiv.org/abs/2408.10845",
    "summary_markdown": "论文研究单位\nTuring Inc.\n\n论文概述\n该论文提出了CoVLA (Comprehensive Vision-Language-Action) 数据集，旨在解决自动驾驶领域中视觉-语言-动作（VLA）模型因缺乏大规模、多模态标注数据而发展受限的问题。该数据集通过可扩展的自动化处理流程，从超过1000小时的原始驾驶数据中，采样并生成了包含10,000个真实世界驾驶场景（超过80小时视频）的综合数据集。每个场景都包含了来自前置摄像头的视频帧、通过传感器融合估计的准确车辆未来轨迹，以及结合规则和视觉语言模型（VLM）自动生成的详细自然语言描述。论文还基于该数据集训练了CoVLA-Agent模型，验证了其在轨迹预测和场景描述任务上的有效性。\n\n论文核心贡献点\n1. 介绍了CoVLA数据集，这是一个为多样化驾驶场景提供轨迹目标和详细逐帧情况描述的大规模数据集。\n2. 提出了一种可扩展的方法，通过传感器融合精确估计轨迹，并自动生成包含关键驾驶信息的帧级文本字幕。\n3. 开发了CoVLA-Agent，这是一个基于CoVLA数据集的可解释的端到端自动驾驶VLA模型。该模型能够一致地生成驾驶场景描述和预测轨迹，为构建更可靠的自动驾驶系统铺平了道路。\n\n论文方法描述\n1. **数据收集与采样**：使用配备前置摄像头、CAN总线、GNSS和IMU的车辆，在东京地区收集了超过1000小时的原始数据。通过对转向角、加速度、转向灯等特征进行加权采样，从筛选后的数据中选出10,000个时长30秒的多样化场景。\n2. **自动标注**：\n * **轨迹**：使用卡尔曼滤波器融合GNSS和IMU数据，为每个时间戳生成未来3秒（60帧）的车辆轨迹，并通过启发式方法过滤掉不稳定的轨迹。\n * **物体**：使用深度学习模型检测交通灯的状态，并通过传感器融合检测并跟踪前车。\n3. **自动字幕生成**：\n * 采用两阶段方法：首先，基于车辆状态和检测结果生成规则式字幕；然后，使用预训练的VideoLLaMA 2模型处理3秒的视频窗口以生成更丰富的描述。\n * 为减少VLM的幻觉，将规则式字幕作为事实性约束提供给VLM，指导其补充额外信息。\n\n论文使用数据集和训练资源\n1. **数据集**：使用自建的CoVLA-Dataset，包含10,000个场景（600万帧）。数据集按70%/15%/15%的比例划分为训练集、验证集和测试集。\n2. **训练数据**：从数据集中采样得到302,989个训练样本、64,153个验证样本和64,920个测试样本，采样频率为2Hz。\n3. **模型**：CoVLA-Agent，其架构包括Llama-2 (7B)作为语言模型，CLIP ViT-L作为视觉编码器。\n4. **计算资源**：数据集的自动字幕生成过程使用了8个NVIDIA H100 GPU，在一天内完成。\n\n论文使用的评估环境和评估指标\n1. **评估任务**：主要评估CoVLA-Agent在交通场景描述生成和轨迹预测两个任务上的性能。\n2. **评估条件**：在两种条件下评估轨迹预测的准确性：一种是使用模型自身预测的字幕（predicted caption condition），另一种是使用真实的标注字幕（ground truth caption condition）。\n3. **评估指标**：使用平均位移误差（ADE）和最终位移误差（FDE）来量化预测轨迹与真实轨迹之间的偏差。ADE是所有预测时间步的欧氏距离平均值，FDE是预测终点与真实终点的欧氏距离。",
    "summary_html": "<p>论文研究单位</p>\n<p>Turing Inc.</p>\n\n<p>论文概述</p>\n<p>该论文提出了CoVLA (Comprehensive Vision-Language-Action) 数据集，旨在解决自动驾驶领域中视觉-语言-动作（VLA）模型因缺乏大规模、多模态标注数据而发展受限的问题。该数据集通过可扩展的自动化处理流程，从超过1000小时的原始驾驶数据中，采样并生成了包含10,000个真实世界驾驶场景（超过80小时视频）的综合数据集。每个场景都包含了来自前置摄像头的视频帧、通过传感器融合估计的准确车辆未来轨迹，以及结合规则和视觉语言模型（VLM）自动生成的详细自然语言描述。论文还基于该数据集训练了CoVLA-Agent模型，验证了其在轨迹预测和场景描述任务上的有效性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>介绍了CoVLA数据集，这是一个为多样化驾驶场景提供轨迹目标和详细逐帧情况描述的大规模数据集。</li><li>提出了一种可扩展的方法，通过传感器融合精确估计轨迹，并自动生成包含关键驾驶信息的帧级文本字幕。</li><li>开发了CoVLA-Agent，这是一个基于CoVLA数据集的可解释的端到端自动驾驶VLA模型。该模型能够一致地生成驾驶场景描述和预测轨迹，为构建更可靠的自动驾驶系统铺平了道路。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li><strong>数据收集与采样</strong>：使用配备前置摄像头、CAN总线、GNSS和IMU的车辆，在东京地区收集了超过1000小时的原始数据。通过对转向角、加速度、转向灯等特征进行加权采样，从筛选后的数据中选出10,000个时长30秒的多样化场景。</li><li><strong>自动标注</strong>：</li></ol>\n<p> * <strong>轨迹</strong>：使用卡尔曼滤波器融合GNSS和IMU数据，为每个时间戳生成未来3秒（60帧）的车辆轨迹，并通过启发式方法过滤掉不稳定的轨迹。</p>\n<p> * <strong>物体</strong>：使用深度学习模型检测交通灯的状态，并通过传感器融合检测并跟踪前车。</p>\n<p>3. <strong>自动字幕生成</strong>：</p>\n<p> * 采用两阶段方法：首先，基于车辆状态和检测结果生成规则式字幕；然后，使用预训练的VideoLLaMA 2模型处理3秒的视频窗口以生成更丰富的描述。</p>\n<p> * 为减少VLM的幻觉，将规则式字幕作为事实性约束提供给VLM，指导其补充额外信息。</p>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li><strong>数据集</strong>：使用自建的CoVLA-Dataset，包含10,000个场景（600万帧）。数据集按70%/15%/15%的比例划分为训练集、验证集和测试集。</li><li><strong>训练数据</strong>：从数据集中采样得到302,989个训练样本、64,153个验证样本和64,920个测试样本，采样频率为2Hz。</li><li><strong>模型</strong>：CoVLA-Agent，其架构包括Llama-2 (7B)作为语言模型，CLIP ViT-L作为视觉编码器。</li><li><strong>计算资源</strong>：数据集的自动字幕生成过程使用了8个NVIDIA H100 GPU，在一天内完成。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li><strong>评估任务</strong>：主要评估CoVLA-Agent在交通场景描述生成和轨迹预测两个任务上的性能。</li><li><strong>评估条件</strong>：在两种条件下评估轨迹预测的准确性：一种是使用模型自身预测的字幕（predicted caption condition），另一种是使用真实的标注字幕（ground truth caption condition）。</li><li><strong>评估指标</strong>：使用平均位移误差（ADE）和最终位移误差（FDE）来量化预测轨迹与真实轨迹之间的偏差。ADE是所有预测时间步的欧氏距离平均值，FDE是预测终点与真实终点的欧氏距离。</li></ol>"
  },
  {
    "date": "2024-07-25",
    "title": "Unified Lexical Representation for Interpretable Visual-Language Alignment",
    "link": "http://arxiv.org/abs/2407.17827",
    "summary_markdown": "好的，我将以论文阅读专家的身份，根据提供的Arxiv论文HTML原文，总结论文的要点。我将遵循你的要求，只提供Markdown格式文本，不使用加粗，不输出其他内容，并将总结分为你指定的几个部分。\n\n1. **论文研究单位**:\n * 我将从作者信息中提取研究机构。根据HTML，作者单位是复旦大学（Fudan University）和亚马逊云科技（Amazon Web Services）。\n\n2. **论文概述**:\n * 我将概括论文的背景、问题陈述和主要目标。这篇论文介绍了一个名为LexVLA的框架，用于视觉-语言对齐（VLA），旨在通过学习统一的词汇表示来解决现有方法（如CLIP）的可解释性差和训练复杂的问题。\n\n3. **论文核心贡献点**:\n * 我将从引言的“We contribute”部分提取贡献点。\n * 贡献1：利用单模态预训练模型（用于视觉的DINOv2，用于文本的Llama 2），利用它们独特的优势（如局部特征和上下文预测能力）进行VLA。\n * 贡献2：提出了一种统一的词汇表示方法，但为视觉和文本模态使用独立的码本，以避免削弱预训练能力。\n * 贡献3：引入了过度使用惩罚（overuse penalty），以鼓励稀疏嵌入并防止无意义激活。\n * 贡献4：在更少的多模态训练数据下实现了卓越的检索性能，并使用提出的PatchDis指标获得了更好的可解释性。\n\n4. **论文方法描述**:\n * 我将总结方法的关键步骤和组件，基于第3节“LexVLA”。\n * **词汇表示**: 使用共享词汇表但为视觉和文本模态设置不同的码本。码本初始化为Llama 2的输出码本；文本码本被冻结，视觉码本被微调。表示被稀疏化，通过阈值化（值大于1/√V）。\n * **词汇编码器**:\n * **文本编码器**: 使用Llama 2，并通过上下文提示（如“关键词提取”）使其执行词汇预测，而不是简单的标题嵌入。\n * **视觉编码器**: 使用DINOv2作为冻结的骨干网络。特征通过一个适配器（自注意力+2个MLP）处理，然后映射到视觉词汇表示。\n * **训练**: 使用标准的对比学习目标，并加入过度使用惩罚。训练采用增量微调：对于文本，使用LoRA；对于视觉，只训练适配器和映射器。\n\n5. **论文使用数据集和训练资源**:\n * 我将从“实验”部分（第4节）中提取数据集和计算细节。\n * **数据集**: 训练在CC-12M上进行。在Flickr30K和MSCOCO上进行零样本跨模态检索评估。\n * **训练资源**: 文本编码器基于Llama 2（7B），视觉骨干网络为DINOv2（ViT-L/14）。使用单个8x A100 GPU进行训练。提及使用LoRA进行文本微调以减少资源消耗。\n\n6. **论文使用的评估环境和评估指标**:\n * 我将从“实验”部分提取评估设置和指标。\n * **评估环境**: 标准零样本跨模态检索设置（图像到文本和文本到图像检索）。\n * **评估指标**: 在Flickr30K和MSCOCO上的检索指标（R@1, R@5, R@10, mean Recall）。此外，引入了一个新的可解释性指标“PatchDis”来量化模型在未经过细粒度任务训练的情况下的块级对齐能力。</think>\n### 论文研究单位\n复旦大学 和 亚马逊云科技\n### 论文概述\n本文提出了一种名为LexVLA的视觉-语言对齐框架，旨在通过学习统一的词汇表示来解决现有方法（如CLIP）存在的可解释性差和训练复杂的问题。传统CLIP的潜在特征对齐缺乏表示清晰度和相似度评分的明确性，而词汇表示虽然自然且可解释，但难以学习。LexVLA利用单模态预训练模型的独特优势，通过简单的训练配置，实现了更优的跨模态检索性能和模型可解释性。\n### 论文核心贡献点\n1. 强调利用单模态预训练模型进行视觉-语言对齐，以受益于其无法通过对比目标学习的独特属性。例如，使用DINOv2因其局部化特征和使用Llama 2因其上下文能力。\n2. 有效学习统一词汇表示，为视觉和语言模态采用独特的码本，以避免削弱预训练能力。\n3. 提出过度使用惩罚，以鼓励稀疏嵌入并防止无意义激活。\n4. 以更少的多模态训练数据，享受卓越的检索性能，并在全局监督信号下，实现更好的块级可解释的VLA模型，使用提出的PatchDis指标在量上超越CLIP风格和词汇方法。\n### 论文方法描述\nLexVLA包含三个核心部分：\n1. **词汇表示**：使用共享词汇表但为文本和图像模态设置不同的码本。码本初始化为Llama 2的输出码本；文本码本被冻结以保留其语义，视觉码本则被微调以适应视觉特征。词汇表示被限制为非负且l2归一化，并通过值阈值化（大于1/√V）实现稀疏性。\n2. **词汇编码器**：\n * **词汇文本编码器**：使用Llama 2作为基础。并非直接对文本标题嵌入，而是通过上下文提示（例如，“...的关键词是：”）使其执行词汇预测任务。输出标记的嵌入与文本码本计算点积注意力，经elu1p激活和l2归一化后得到全局词汇表示。\n * **词汇视觉编码器**：使用DINOv2作为冻结的视觉骨干网络。输入图像被展平为块，通过DINOv2提取特征序列。特征通过一个适配器（包含自注意力层和两个多层感知机）进行处理，然后映射到视觉词汇表示。\n3. **训练LexVLA**：采用标准的对比学习目标来对齐图像-文本对。训练中加入了过度使用惩罚项，以防止频繁激活无意义词汇。训练采用增量微调策略：对于文本，使用LoRA适配器微调Llama 2；对于视觉，冻结DINOv2骨干，仅训练适配器和映射器，视觉码本用文本码本初始化并进行微调。\n### 论文使用数据集和训练资源\n**数据集**：模型在CC-12M数据集上进行训练。在Flickr30K和MSCOCO数据集上进行零样本跨模态检索评估。\n**训练资源**：文本编码器基于Llama 2（7B参数），视觉骨干网络为DINOv2（ViT-L/14架构）。训练在单个8x A100 GPU上进行。对文本模型使用LoRA进行微调以减少计算资源消耗。\n### 论文使用的评估环境和评估指标\n**评估环境**：在标准的零样本跨模态检索设置下评估模型性能，包括图像到文本检索和文本到图像检索任务。\n**评估指标**：主要在Flickr30K和MSCOCO数据集上使用检索指标，包括R@1, R@5, R@10和平均召回率。此外，论文引入了一个新的可解释性评估指标PatchDis，用于在未经过细粒度任务（如分割或检测）训练的VLA模型上量化其块级对齐能力。",
    "summary_html": "<p>好的，我将以论文阅读专家的身份，根据提供的Arxiv论文HTML原文，总结论文的要点。我将遵循你的要求，只提供Markdown格式文本，不使用加粗，不输出其他内容，并将总结分为你指定的几个部分。</p>\n\n<p>1. <strong>论文研究单位</strong>:</p>\n<p> * 我将从作者信息中提取研究机构。根据HTML，作者单位是复旦大学（Fudan University）和亚马逊云科技（Amazon Web Services）。</p>\n\n<p>2. <strong>论文概述</strong>:</p>\n<p> * 我将概括论文的背景、问题陈述和主要目标。这篇论文介绍了一个名为LexVLA的框架，用于视觉-语言对齐（VLA），旨在通过学习统一的词汇表示来解决现有方法（如CLIP）的可解释性差和训练复杂的问题。</p>\n\n<p>3. <strong>论文核心贡献点</strong>:</p>\n<p> * 我将从引言的“We contribute”部分提取贡献点。</p>\n<p> * 贡献1：利用单模态预训练模型（用于视觉的DINOv2，用于文本的Llama 2），利用它们独特的优势（如局部特征和上下文预测能力）进行VLA。</p>\n<p> * 贡献2：提出了一种统一的词汇表示方法，但为视觉和文本模态使用独立的码本，以避免削弱预训练能力。</p>\n<p> * 贡献3：引入了过度使用惩罚（overuse penalty），以鼓励稀疏嵌入并防止无意义激活。</p>\n<p> * 贡献4：在更少的多模态训练数据下实现了卓越的检索性能，并使用提出的PatchDis指标获得了更好的可解释性。</p>\n\n<p>4. <strong>论文方法描述</strong>:</p>\n<p> * 我将总结方法的关键步骤和组件，基于第3节“LexVLA”。</p>\n<p> * <strong>词汇表示</strong>: 使用共享词汇表但为视觉和文本模态设置不同的码本。码本初始化为Llama 2的输出码本；文本码本被冻结，视觉码本被微调。表示被稀疏化，通过阈值化（值大于1/√V）。</p>\n<p> * <strong>词汇编码器</strong>:</p>\n<p> * <strong>文本编码器</strong>: 使用Llama 2，并通过上下文提示（如“关键词提取”）使其执行词汇预测，而不是简单的标题嵌入。</p>\n<p> * <strong>视觉编码器</strong>: 使用DINOv2作为冻结的骨干网络。特征通过一个适配器（自注意力+2个MLP）处理，然后映射到视觉词汇表示。</p>\n<p> * <strong>训练</strong>: 使用标准的对比学习目标，并加入过度使用惩罚。训练采用增量微调：对于文本，使用LoRA；对于视觉，只训练适配器和映射器。</p>\n\n<p>5. <strong>论文使用数据集和训练资源</strong>:</p>\n<p> * 我将从“实验”部分（第4节）中提取数据集和计算细节。</p>\n<p> * <strong>数据集</strong>: 训练在CC-12M上进行。在Flickr30K和MSCOCO上进行零样本跨模态检索评估。</p>\n<p> * <strong>训练资源</strong>: 文本编码器基于Llama 2（7B），视觉骨干网络为DINOv2（ViT-L/14）。使用单个8x A100 GPU进行训练。提及使用LoRA进行文本微调以减少资源消耗。</p>\n\n<p>6. <strong>论文使用的评估环境和评估指标</strong>:</p>\n<p> * 我将从“实验”部分提取评估设置和指标。</p>\n<p> * <strong>评估环境</strong>: 标准零样本跨模态检索设置（图像到文本和文本到图像检索）。</p>\n<p> * <strong>评估指标</strong>: 在Flickr30K和MSCOCO上的检索指标（R@1, R@5, R@10, mean Recall）。此外，引入了一个新的可解释性指标“PatchDis”来量化模型在未经过细粒度任务训练的情况下的块级对齐能力。</think></p>\n<h3>论文研究单位</h3>\n<p>复旦大学 和 亚马逊云科技</p>\n<h3>论文概述</h3>\n<p>本文提出了一种名为LexVLA的视觉-语言对齐框架，旨在通过学习统一的词汇表示来解决现有方法（如CLIP）存在的可解释性差和训练复杂的问题。传统CLIP的潜在特征对齐缺乏表示清晰度和相似度评分的明确性，而词汇表示虽然自然且可解释，但难以学习。LexVLA利用单模态预训练模型的独特优势，通过简单的训练配置，实现了更优的跨模态检索性能和模型可解释性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>强调利用单模态预训练模型进行视觉-语言对齐，以受益于其无法通过对比目标学习的独特属性。例如，使用DINOv2因其局部化特征和使用Llama 2因其上下文能力。</li><li>有效学习统一词汇表示，为视觉和语言模态采用独特的码本，以避免削弱预训练能力。</li><li>提出过度使用惩罚，以鼓励稀疏嵌入并防止无意义激活。</li><li>以更少的多模态训练数据，享受卓越的检索性能，并在全局监督信号下，实现更好的块级可解释的VLA模型，使用提出的PatchDis指标在量上超越CLIP风格和词汇方法。</li></ol>\n<h3>论文方法描述</h3>\n<p>LexVLA包含三个核心部分：</p>\n<ol><li><strong>词汇表示</strong>：使用共享词汇表但为文本和图像模态设置不同的码本。码本初始化为Llama 2的输出码本；文本码本被冻结以保留其语义，视觉码本则被微调以适应视觉特征。词汇表示被限制为非负且l2归一化，并通过值阈值化（大于1/√V）实现稀疏性。</li><li><strong>词汇编码器</strong>：</li></ol>\n<p> * <strong>词汇文本编码器</strong>：使用Llama 2作为基础。并非直接对文本标题嵌入，而是通过上下文提示（例如，“...的关键词是：”）使其执行词汇预测任务。输出标记的嵌入与文本码本计算点积注意力，经elu1p激活和l2归一化后得到全局词汇表示。</p>\n<p> * <strong>词汇视觉编码器</strong>：使用DINOv2作为冻结的视觉骨干网络。输入图像被展平为块，通过DINOv2提取特征序列。特征通过一个适配器（包含自注意力层和两个多层感知机）进行处理，然后映射到视觉词汇表示。</p>\n<p>3. <strong>训练LexVLA</strong>：采用标准的对比学习目标来对齐图像-文本对。训练中加入了过度使用惩罚项，以防止频繁激活无意义词汇。训练采用增量微调策略：对于文本，使用LoRA适配器微调Llama 2；对于视觉，冻结DINOv2骨干，仅训练适配器和映射器，视觉码本用文本码本初始化并进行微调。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>：模型在CC-12M数据集上进行训练。在Flickr30K和MSCOCO数据集上进行零样本跨模态检索评估。</p>\n<p><strong>训练资源</strong>：文本编码器基于Llama 2（7B参数），视觉骨干网络为DINOv2（ViT-L/14架构）。训练在单个8x A100 GPU上进行。对文本模型使用LoRA进行微调以减少计算资源消耗。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>：在标准的零样本跨模态检索设置下评估模型性能，包括图像到文本检索和文本到图像检索任务。</p>\n<p><strong>评估指标</strong>：主要在Flickr30K和MSCOCO数据集上使用检索指标，包括R@1, R@5, R@10和平均召回率。此外，论文引入了一个新的可解释性评估指标PatchDis，用于在未经过细粒度任务（如分割或检测）训练的VLA模型上量化其块级对齐能力。</p>"
  },
  {
    "date": "2024-07-11",
    "title": "Robotic Control via Embodied Chain-of-Thought Reasoning",
    "link": "http://arxiv.org/abs/2407.08693",
    "summary_markdown": "### 论文研究单位\nUC Berkeley, University of Warsaw, Stanford University\n### 论文概述\n论文提出了一种名为\"具身思维链推理\"（Embodied Chain-of-Thought Reasoning, ECoT）的方法，用于提升视觉-语言-动作模型（VLAs）在机器人控制中的泛化能力。传统VLAs直接从观测映射到动作，而ECoT通过在预测动作前引入多步文本推理（包括任务规划、子任务分解、运动描述和视觉特征定位），迫使模型在行动前\"仔细思考\"和\"仔细观察\"。实验表明，ECoT在无需额外机器人数据的情况下，将OpenVLA模型在挑战性泛化任务上的绝对成功率提高了28%，并增强了策略的可解释性和人机交互能力。\n### 论文核心贡献点\n1. 提出ECoT方法：将视觉-语言-动作模型训练为在预测动作前执行多步推理，结合高级语义推理（如子任务规划）和低级具身推理（如物体边界框和夹爪位置）。\n2. 设计可扩展的合成数据生成管道：利用预训练模型（如Prismatic VLM、Grounding DINO、Gemini）自动为大规模机器人数据集标注推理链，无需人工标注。\n3. 实验验证ECoT有效性：在Bridge V2数据集上训练的模型，在真实机器人泛化任务中成功率显著超越基线（包括OpenVLA、RT-2-X），并提升策略可解释性和交互式纠错能力。\n4. 推理效率优化：提出同步/异步执行策略，减少推理延迟，使ECoT实用化。\n### 论文方法描述\n1. **推理步骤设计**：\n - 高级推理：任务重述（TASK）、高层计划（PLAN）、当前子任务（SUBTASK）。\n - 具身推理：低级运动指令（MOVE，如\"左移\"）、夹爪像素位置（GRIPPER）、物体边界框（OBJECTS）。\n - 强制模型关注多模态输入，将推理与视觉和机器人状态对齐。\n2. **数据生成流程**：\n - 使用Prismatic VLM生成场景描述。\n - Grounding DINO检测物体边界框，关联文本片段。\n - 从机器人本体感觉计算运动原语（729种模板化动作）。\n - OWLv2和SAM检测夹爪位置，拟合投影矩阵。\n - Gemini整合信息生成完整推理链（计划、子任务、解释）。\n - 在Bridge V2的250万+转换数据上运行，耗时7天。\n3. **训练与推理**：\n - 将推理和动作离散为tokens，微调OpenVLA（基于Llama 2-7B和Prismatic视觉编码器）。\n - 推理时采用N步冻结（如每5步更新高级推理）或异步并行（双实例）加速。\n### 论文使用数据集和训练资源\n1. **数据集**：\n - 主要训练数据：Bridge V2数据集（60k演示，250万+转换）。\n - 评估任务：自定义泛化任务集（空间关系、未见过物体、新指令），共314次试验。\n - 泛化测试包括分布内（ID）和分布外（OOD）视角场景。\n2. **训练资源**：\n - 基础模型：OpenVLA（Prismatic VLM + Llama 2-7B）。\n - 数据生成依赖：Prismatic-7B、Grounding DINO、Gemini 1.0、OWLv2、SAM。\n - 计算资源：未明确说明，但强调生成管线可扩展（7天处理Bridge V2）。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：\n - 机器人平台：6-DoF WidowX机械臂（Bridge V2标准配置）。\n - 传感器：单视角第三人称摄像头。\n - 场景：真实世界环境，控制相机角度、光照和背景。\n2. **评估指标**：\n - 任务成功率：主要指标，报告平均值±标准误（StdErr）。\n - 泛化能力：按任务类型分类评估（ID/OOD物体、空间关系、新指令）。\n - 可解释性：定性分析推理链与失败案例。\n - 交互纠错：单次语言干预后的成功率提升。\n - 推理效率：比较不同加速方法的吞吐量（相对加速比）。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>UC Berkeley, University of Warsaw, Stanford University</p>\n<h3>论文概述</h3>\n<p>论文提出了一种名为\"具身思维链推理\"（Embodied Chain-of-Thought Reasoning, ECoT）的方法，用于提升视觉-语言-动作模型（VLAs）在机器人控制中的泛化能力。传统VLAs直接从观测映射到动作，而ECoT通过在预测动作前引入多步文本推理（包括任务规划、子任务分解、运动描述和视觉特征定位），迫使模型在行动前\"仔细思考\"和\"仔细观察\"。实验表明，ECoT在无需额外机器人数据的情况下，将OpenVLA模型在挑战性泛化任务上的绝对成功率提高了28%，并增强了策略的可解释性和人机交互能力。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出ECoT方法：将视觉-语言-动作模型训练为在预测动作前执行多步推理，结合高级语义推理（如子任务规划）和低级具身推理（如物体边界框和夹爪位置）。</li><li>设计可扩展的合成数据生成管道：利用预训练模型（如Prismatic VLM、Grounding DINO、Gemini）自动为大规模机器人数据集标注推理链，无需人工标注。</li><li>实验验证ECoT有效性：在Bridge V2数据集上训练的模型，在真实机器人泛化任务中成功率显著超越基线（包括OpenVLA、RT-2-X），并提升策略可解释性和交互式纠错能力。</li><li>推理效率优化：提出同步/异步执行策略，减少推理延迟，使ECoT实用化。</li></ol>\n<h3>论文方法描述</h3>\n<p>1. <strong>推理步骤设计</strong>：</p>\n<p> - 高级推理：任务重述（TASK）、高层计划（PLAN）、当前子任务（SUBTASK）。</p>\n<p> - 具身推理：低级运动指令（MOVE，如\"左移\"）、夹爪像素位置（GRIPPER）、物体边界框（OBJECTS）。</p>\n<p> - 强制模型关注多模态输入，将推理与视觉和机器人状态对齐。</p>\n<p>2. <strong>数据生成流程</strong>：</p>\n<p> - 使用Prismatic VLM生成场景描述。</p>\n<p> - Grounding DINO检测物体边界框，关联文本片段。</p>\n<p> - 从机器人本体感觉计算运动原语（729种模板化动作）。</p>\n<p> - OWLv2和SAM检测夹爪位置，拟合投影矩阵。</p>\n<p> - Gemini整合信息生成完整推理链（计划、子任务、解释）。</p>\n<p> - 在Bridge V2的250万+转换数据上运行，耗时7天。</p>\n<p>3. <strong>训练与推理</strong>：</p>\n<p> - 将推理和动作离散为tokens，微调OpenVLA（基于Llama 2-7B和Prismatic视觉编码器）。</p>\n<p> - 推理时采用N步冻结（如每5步更新高级推理）或异步并行（双实例）加速。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>1. <strong>数据集</strong>：</p>\n<p> - 主要训练数据：Bridge V2数据集（60k演示，250万+转换）。</p>\n<p> - 评估任务：自定义泛化任务集（空间关系、未见过物体、新指令），共314次试验。</p>\n<p> - 泛化测试包括分布内（ID）和分布外（OOD）视角场景。</p>\n<p>2. <strong>训练资源</strong>：</p>\n<p> - 基础模型：OpenVLA（Prismatic VLM + Llama 2-7B）。</p>\n<p> - 数据生成依赖：Prismatic-7B、Grounding DINO、Gemini 1.0、OWLv2、SAM。</p>\n<p> - 计算资源：未明确说明，但强调生成管线可扩展（7天处理Bridge V2）。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>1. <strong>评估环境</strong>：</p>\n<p> - 机器人平台：6-DoF WidowX机械臂（Bridge V2标准配置）。</p>\n<p> - 传感器：单视角第三人称摄像头。</p>\n<p> - 场景：真实世界环境，控制相机角度、光照和背景。</p>\n<p>2. <strong>评估指标</strong>：</p>\n<p> - 任务成功率：主要指标，报告平均值±标准误（StdErr）。</p>\n<p> - 泛化能力：按任务类型分类评估（ID/OOD物体、空间关系、新指令）。</p>\n<p> - 可解释性：定性分析推理链与失败案例。</p>\n<p> - 交互纠错：单次语言干预后的成功率提升。</p>\n<p> - 推理效率：比较不同加速方法的吞吐量（相对加速比）。</p>"
  },
  {
    "date": "2024-07-10",
    "title": "Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs",
    "link": "http://arxiv.org/abs/2407.07775",
    "summary_markdown": "论文研究单位\nGoogle DeepMind\n\n论文概述\n该论文研究了一种名为多模态指令导航（MIN）的任务及其变体，即带演示巡游的多模态指令导航（MINT）。在此任务中，机器人接收一个环境的演示巡游视频和一个多模态用户指令（可包含文本和图像），然后导航到指令所指示的目标位置。为了解决MINT任务，论文提出了一种名为\\method的分层视觉-语言-动作（VLA）导航策略。\n\n论文核心贡献点\n1. 提出了一种新的机器人导航范式：MIN及其变体MINT，使机器人能理解包含自然语言和图像的多模态指令，并执行有用的导航。\n2. 提出了\\method作为解决MINT任务的方案，该方案结合了长上下文VLM的环境理解和常识推理能力，以及基于拓扑图的鲁棒底层导航策略。\n\n论文方法描述\n\\method是一个分层的导航策略，包含在线和离线组件。\n1. 离线拓扑图生成：首先，使用演示巡游视频（可通过远程操作或智能手机录制），通过COLMAP等运动恢复结构方法为每个视频帧估算6自由度相机位姿，并据此构建一个拓扑图G。图中每个顶点对应一个视频帧，如果目标顶点在源顶点前方且距离在2米内，则添加一条有向边。\n2. 高层目标查找：在线执行时，高层策略使用一个长上下文多模态VLM（如Gemini Pro 1.5），输入演示巡游视频、可选的叙述文本和用户的多模态指令（文本和/或图像）。VLM通过分析视频内容和指令，输出一个目标帧的索引g。\n3. 底层目标到达：底层策略接收目标帧索引g、机器人当前相机观察O和离线构建的拓扑图G。在每个时间步，该策略首先通过一个分层的视觉定位系统在图中定位机器人，获取其当前位姿和起始顶点。然后，使用Dijkstra算法计算从起始顶点到目标顶点的最短路径。最后，计算出通往路径上下一个顶点的相对位姿，并输出一个路点动作（纵向平移Δx、横向平移Δy和垂直轴旋转Δθ）供机器人执行。\n\n论文使用数据集和训练资源\n1. 数据集：实验在一个836平方米的真实世界办公环境和一个家庭式环境中进行。演示巡游通过远程操作机器人或使用智能手机录制。用户指令部分通过众包收集了57条，分为四类：无需推理、需要推理、小物体和多模态指令。\n2. 训练资源：论文未提及对模型进行端到端的训练。高层策略使用预训练的长上下文VLM（Gemini 1.5 Pro），不涉及模型训练。底层策略依赖经典算法（如COLMAP、PnP、Dijkstra、MPC），无需数据驱动的训练过程。\n\n论文使用的评估环境和评估指标\n1. 评估环境：主要评估环境是一个面积为836平方米、有日常物品杂乱摆放的真实办公室。此外，还构建了该办公室的高保真度NeRF模拟环境用于大规模评估，并在一个家庭式环境中进行了概念验证实验。\n2. 评估指标：主要评估指标是成功率，分为三个层次：高层目标查找成功率、底层目标到达成功率和端到端成功率。此外，还使用了SPL（Success Rate weighted by Path Length），一个综合考虑了成功率和路径效率的指标。",
    "summary_html": "<p>论文研究单位</p>\n<p>Google DeepMind</p>\n\n<p>论文概述</p>\n<p>该论文研究了一种名为多模态指令导航（MIN）的任务及其变体，即带演示巡游的多模态指令导航（MINT）。在此任务中，机器人接收一个环境的演示巡游视频和一个多模态用户指令（可包含文本和图像），然后导航到指令所指示的目标位置。为了解决MINT任务，论文提出了一种名为\\method的分层视觉-语言-动作（VLA）导航策略。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种新的机器人导航范式：MIN及其变体MINT，使机器人能理解包含自然语言和图像的多模态指令，并执行有用的导航。</li><li>提出了\\method作为解决MINT任务的方案，该方案结合了长上下文VLM的环境理解和常识推理能力，以及基于拓扑图的鲁棒底层导航策略。</li></ol>\n\n<p>论文方法描述</p>\n<p>\\method是一个分层的导航策略，包含在线和离线组件。</p>\n<ol><li>离线拓扑图生成：首先，使用演示巡游视频（可通过远程操作或智能手机录制），通过COLMAP等运动恢复结构方法为每个视频帧估算6自由度相机位姿，并据此构建一个拓扑图G。图中每个顶点对应一个视频帧，如果目标顶点在源顶点前方且距离在2米内，则添加一条有向边。</li><li>高层目标查找：在线执行时，高层策略使用一个长上下文多模态VLM（如Gemini Pro 1.5），输入演示巡游视频、可选的叙述文本和用户的多模态指令（文本和/或图像）。VLM通过分析视频内容和指令，输出一个目标帧的索引g。</li><li>底层目标到达：底层策略接收目标帧索引g、机器人当前相机观察O和离线构建的拓扑图G。在每个时间步，该策略首先通过一个分层的视觉定位系统在图中定位机器人，获取其当前位姿和起始顶点。然后，使用Dijkstra算法计算从起始顶点到目标顶点的最短路径。最后，计算出通往路径上下一个顶点的相对位姿，并输出一个路点动作（纵向平移Δx、横向平移Δy和垂直轴旋转Δθ）供机器人执行。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：实验在一个836平方米的真实世界办公环境和一个家庭式环境中进行。演示巡游通过远程操作机器人或使用智能手机录制。用户指令部分通过众包收集了57条，分为四类：无需推理、需要推理、小物体和多模态指令。</li><li>训练资源：论文未提及对模型进行端到端的训练。高层策略使用预训练的长上下文VLM（Gemini 1.5 Pro），不涉及模型训练。底层策略依赖经典算法（如COLMAP、PnP、Dijkstra、MPC），无需数据驱动的训练过程。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：主要评估环境是一个面积为836平方米、有日常物品杂乱摆放的真实办公室。此外，还构建了该办公室的高保真度NeRF模拟环境用于大规模评估，并在一个家庭式环境中进行了概念验证实验。</li><li>评估指标：主要评估指标是成功率，分为三个层次：高层目标查找成功率、底层目标到达成功率和端到端成功率。此外，还使用了SPL（Success Rate weighted by Path Length），一个综合考虑了成功率和路径效率的指标。</li></ol>"
  },
  {
    "date": "2024-06-27",
    "title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents",
    "link": "http://arxiv.org/abs/2407.00114",
    "summary_markdown": "论文研究单位\n北京大学人工智能研究院、北京大学元培学院、北京通用人工智能研究院（BIGAI）、加州大学洛杉矶分校。\n\n论文概述\n本文提出了OmniJARVIS，一种新颖的视觉-语言-动作（VLA）模型，用于在Minecraft中构建开放世界指令跟随智能体。与先前工作不同，OmniJARVIS通过统一的多模态交互数据（视觉、语言和动作）的令牌化，实现了强大的推理和高效的决策能力。\n\n论文核心贡献点\n1. 提出了OmniJARVIS模型，能够通过联合建模多模态交互数据中的视觉、语言和动作，在开放世界环境中进行推理、规划和行动。\n2. 提出了一种自监督学习方法，用于学习行为编码器以对动作进行令牌化，并训练一个模仿学习策略解码器，以从行为令牌生成控制命令。\n3. 在开放世界Minecraft中进行了广泛的评估，证明了OmniJARVIS在各种任务上的熟练度，并提供了深入的分析以揭示有价值的见解。\n\n论文方法描述\n1. 行为令牌化：修改了GROOT的基于VAE的自监督学习方法，使用有限标量量化器（FSQ）替换高斯潜在表示，生成长度为5、码本大小为15360的离散令牌。\n2. 多模态交互数据构建：将OpenAI的Minecraft游戏数据集转换为所需的多模态交互数据格式，包括指令、记忆、思维、观察（视觉）和行为轨迹（动作）。\n3. 模型架构与训练：基于预训练的多模态语言模型（MLM），将其词汇表扩展以包含行为令牌。使用前缀语言建模目标进行自回归训练。\n\n论文使用数据集和训练资源\n1. 数据集：OpenAI发布的Minecraft承包商数据。\n2. 训练资源：未明确提及。\n\n论文使用的评估环境和评估指标\n1. 评估环境：开放世界的Minecraft。\n2. 评估指标：在原子任务、程序化任务和开放式指令跟随任务上的表现。",
    "summary_html": "<p>论文研究单位</p>\n<p>北京大学人工智能研究院、北京大学元培学院、北京通用人工智能研究院（BIGAI）、加州大学洛杉矶分校。</p>\n\n<p>论文概述</p>\n<p>本文提出了OmniJARVIS，一种新颖的视觉-语言-动作（VLA）模型，用于在Minecraft中构建开放世界指令跟随智能体。与先前工作不同，OmniJARVIS通过统一的多模态交互数据（视觉、语言和动作）的令牌化，实现了强大的推理和高效的决策能力。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了OmniJARVIS模型，能够通过联合建模多模态交互数据中的视觉、语言和动作，在开放世界环境中进行推理、规划和行动。</li><li>提出了一种自监督学习方法，用于学习行为编码器以对动作进行令牌化，并训练一个模仿学习策略解码器，以从行为令牌生成控制命令。</li><li>在开放世界Minecraft中进行了广泛的评估，证明了OmniJARVIS在各种任务上的熟练度，并提供了深入的分析以揭示有价值的见解。</li></ol>\n\n<p>论文方法描述</p>\n<ol><li>行为令牌化：修改了GROOT的基于VAE的自监督学习方法，使用有限标量量化器（FSQ）替换高斯潜在表示，生成长度为5、码本大小为15360的离散令牌。</li><li>多模态交互数据构建：将OpenAI的Minecraft游戏数据集转换为所需的多模态交互数据格式，包括指令、记忆、思维、观察（视觉）和行为轨迹（动作）。</li><li>模型架构与训练：基于预训练的多模态语言模型（MLM），将其词汇表扩展以包含行为令牌。使用前缀语言建模目标进行自回归训练。</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<ol><li>数据集：OpenAI发布的Minecraft承包商数据。</li><li>训练资源：未明确提及。</li></ol>\n\n<p>论文使用的评估环境和评估指标</p>\n<ol><li>评估环境：开放世界的Minecraft。</li><li>评估指标：在原子任务、程序化任务和开放式指令跟随任务上的表现。</li></ol>"
  },
  {
    "date": "2024-06-28",
    "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
    "link": "http://arxiv.org/abs/2406.20095",
    "summary_markdown": "1. **论文研究单位**\n * Stony Brook University\n * University of Wisconsin-Madison\n\n2. **论文概述**\n 论文提出了LLaRA（Large Language and Robotics Assistant）框架，该框架将机器人动作策略构建为视觉文本对话，通过一种称为“视觉运动指令微调”的过程，将预训练的视觉语言模型（VLM）高效地转换为强大的视觉语言动作（VLA）模型。LLaRA的核心思想是，将现有的行为克隆数据集转换为对话风格的指令微调数据，并通过自监督的方式定义六个辅助任务来增强该数据集，而无需任何额外的动作标注。这使得在有限的数据量下，微调后的VLM也能产生有意义的机器人动作决策，并在多个模拟和真实世界的任务中实现了最先进的性能。\n\n3. **论文核心贡献点**\n * 提出了一种将机器人操作任务表述为自然语言指令-响应对的方法，从而能够成功地将VLM指令微调为策略。\n * 将机器人动作与图像像素坐标对齐，使得预训练VLM到机器人领域的迁移更高效，特别是在训练数据有限时。\n * 识别并生成了能以自监督方式进一步增强机器人策略学习的辅助数据。\n\n4. **论文方法描述**\n LLaRA框架包含三个主要部分：\n 1. **数据生成**：设计了一个自动化流程，将现有的行为克隆（BC）数据集转换为“对话风格”的指令微调数据集（称为Instruct-BC，或inBC）。具体来说，将状态-动作对转换为单轮对话，其中当前视觉观察和文本任务描述作为用户指令，而机器人的动作（例如，2D图像坐标和旋转角度）以自然语言格式作为模型响应。\n 2. **多图像处理**：为处理任务描述中的多图像输入，该方法通过目标检测将参考图像转换为文本描述，形成`D-inBC`数据集，使单图像VLM能够处理多模态指令。\n 3. **数据增强**：为了解决数据稀缺问题，提出了六种自监督的辅助数据集（定位、检测、动作预测、未来预测、空间关系、时间关系），这些数据集可以从现有轨迹中自动生成，无需额外标注，用于提升模型的空间和时间理解能力。\n 4. **模型训练与推理**：基于LLaVA架构，使用生成的指令数据对预训练VLM进行微调。在推理时，模型接收当前观察、任务描述和动作历史作为输入，生成文本形式的动作，该动作随后被解析并映射到机器人的控制空间。\n\n5. **论文使用数据集和训练资源**\n * **数据集**：\n * 模拟实验：使用了VIMA-Bench环境，包含17个任务，并使用了VIMA数据集的三个子集（VIMA-0.8k, VIMA-8k, VIMA-80k）进行训练和评估。\n * 真实世界实验：构建了一个新的真实世界环境，使用一个带有夹爪的机械臂和固定的RGB相机。设计了三个未见过的任务（移动物体到碗中、按指定角度旋转物体、将物体放置在托盘上）。\n * **训练资源**：论文未明确说明具体的计算资源（如GPU类型和数量），但提到了在VIMA-8k数据集上训练2个epoch。\n\n6. **论文使用的评估环境和评估指标**\n * **评估环境**：\n * 模拟环境：VIMA-Bench，一个模拟的桌面机器人操作环境。\n * 真实世界环境：一个带有机械臂、夹爪和固定相机的真实世界设置，物体放置在平面上。\n * **评估指标**：\n * **成功率**：在模拟和真实世界实验中，主要使用任务成功率作为评估指标。对于VIMA-Bench，报告了三个难度级别（L1, L2, L3）的平均成功率。对于真实世界任务，报告了每个任务的20次随机初始化尝试的平均成功率。</think>\n1. **论文研究单位**\n * Stony Brook University\n * University of Wisconsin-Madison\n\n2. **论文概述**\n 论文提出了LLaRA（Large Language and Robotics Assistant）框架，该框架将机器人动作策略构建为视觉文本对话，通过一种称为“视觉运动指令微调”的过程，将预训练的视觉语言模型（VLM）高效地转换为强大的视觉语言动作（VLA）模型。LLaRA的核心思想是，将现有的行为克隆数据集转换为对话风格的指令微调数据，并通过自监督的方式定义六个辅助任务来增强该数据集，而无需任何额外的动作标注。这使得在有限的数据量下，微调后的VLM也能产生有意义的机器人动作决策，并在多个模拟和真实世界的任务中实现了最先进的性能。\n\n3. **论文核心贡献点**\n * 提出了一种将机器人操作任务表述为自然语言指令-响应对的方法，从而能够成功地将VLM指令微调为策略。\n * 将机器人动作与图像像素坐标对齐，使得预训练VLM到机器人领域的迁移更高效，特别是在训练数据有限时。\n * 识别并生成了能以自监督方式进一步增强机器人策略学习的辅助数据。\n\n4. **论文方法描述**\n LLaRA框架包含三个主要部分：\n 1. **数据生成**：设计了一个自动化流程，将现有的行为克隆（BC）数据集转换为“对话风格”的指令微调数据集（称为Instruct-BC，或inBC）。具体来说，将状态-动作对转换为单轮对话，其中当前视觉观察和文本任务描述作为用户指令，而机器人的动作（例如，2D图像坐标和旋转角度）以自然语言格式作为模型响应。\n 2. **多图像处理**：为处理任务描述中的多图像输入，该方法通过目标检测将参考图像转换为文本描述，形成`D-inBC`数据集，使单图像VLM能够处理多模态指令。\n 3. **数据增强**：为了解决数据稀缺问题，提出了六种自监督的辅助数据集（定位、检测、动作预测、未来预测、空间关系、时间关系），这些数据集可以从现有轨迹中自动生成，无需额外标注，用于提升模型的空间和时间理解能力。\n 4. **模型训练与推理**：基于LLaVA架构，使用生成的指令数据对预训练VLM进行微调。在推理时，模型接收当前观察、任务描述和动作历史作为输入，生成文本形式的动作，该动作随后被解析并映射到机器人的控制空间。\n\n5. **论文使用数据集和训练资源**\n * **数据集**：\n * 模拟实验：使用了VIMA-Bench环境，包含17个任务，并使用了VIMA数据集的三个子集（VIMA-0.8k, VIMA-8k, VIMA-80k）进行训练和评估。\n * 真实世界实验：构建了一个新的真实世界环境，使用一个带有夹爪的机械臂和固定的RGB相机。设计了三个未见过的任务（移动物体到碗中、按指定角度旋转物体、将物体放置在托盘上）。\n * **训练资源**：论文未明确说明具体的计算资源（如GPU类型和数量），但提到了在VIMA-8k数据集上训练2个epoch。\n\n6. **论文使用的评估环境和评估指标**\n * **评估环境**：\n * 模拟环境：VIMA-Bench，一个模拟的桌面机器人操作环境。\n * 真实世界环境：一个带有机械臂、夹爪和固定相机的真实世界设置，物体放置在平面上。\n * **评估指标**：\n * **成功率**：在模拟和真实世界实验中，主要使用任务成功率作为评估指标。对于VIMA-Bench，报告了三个难度级别（L1, L2, L3）的平均成功率。对于真实世界任务，报告了每个任务的20次随机初始化尝试的平均成功率。",
    "summary_html": "<p>1. <strong>论文研究单位</strong></p>\n<p> * Stony Brook University</p>\n<p> * University of Wisconsin-Madison</p>\n\n<p>2. <strong>论文概述</strong></p>\n<p> 论文提出了LLaRA（Large Language and Robotics Assistant）框架，该框架将机器人动作策略构建为视觉文本对话，通过一种称为“视觉运动指令微调”的过程，将预训练的视觉语言模型（VLM）高效地转换为强大的视觉语言动作（VLA）模型。LLaRA的核心思想是，将现有的行为克隆数据集转换为对话风格的指令微调数据，并通过自监督的方式定义六个辅助任务来增强该数据集，而无需任何额外的动作标注。这使得在有限的数据量下，微调后的VLM也能产生有意义的机器人动作决策，并在多个模拟和真实世界的任务中实现了最先进的性能。</p>\n\n<p>3. <strong>论文核心贡献点</strong></p>\n<p> * 提出了一种将机器人操作任务表述为自然语言指令-响应对的方法，从而能够成功地将VLM指令微调为策略。</p>\n<p> * 将机器人动作与图像像素坐标对齐，使得预训练VLM到机器人领域的迁移更高效，特别是在训练数据有限时。</p>\n<p> * 识别并生成了能以自监督方式进一步增强机器人策略学习的辅助数据。</p>\n\n<p>4. <strong>论文方法描述</strong></p>\n<p> LLaRA框架包含三个主要部分：</p>\n<p> 1. <strong>数据生成</strong>：设计了一个自动化流程，将现有的行为克隆（BC）数据集转换为“对话风格”的指令微调数据集（称为Instruct-BC，或inBC）。具体来说，将状态-动作对转换为单轮对话，其中当前视觉观察和文本任务描述作为用户指令，而机器人的动作（例如，2D图像坐标和旋转角度）以自然语言格式作为模型响应。</p>\n<p> 2. <strong>多图像处理</strong>：为处理任务描述中的多图像输入，该方法通过目标检测将参考图像转换为文本描述，形成<code>D-inBC</code>数据集，使单图像VLM能够处理多模态指令。</p>\n<p> 3. <strong>数据增强</strong>：为了解决数据稀缺问题，提出了六种自监督的辅助数据集（定位、检测、动作预测、未来预测、空间关系、时间关系），这些数据集可以从现有轨迹中自动生成，无需额外标注，用于提升模型的空间和时间理解能力。</p>\n<p> 4. <strong>模型训练与推理</strong>：基于LLaVA架构，使用生成的指令数据对预训练VLM进行微调。在推理时，模型接收当前观察、任务描述和动作历史作为输入，生成文本形式的动作，该动作随后被解析并映射到机器人的控制空间。</p>\n\n<p>5. <strong>论文使用数据集和训练资源</strong></p>\n<p> * <strong>数据集</strong>：</p>\n<p> * 模拟实验：使用了VIMA-Bench环境，包含17个任务，并使用了VIMA数据集的三个子集（VIMA-0.8k, VIMA-8k, VIMA-80k）进行训练和评估。</p>\n<p> * 真实世界实验：构建了一个新的真实世界环境，使用一个带有夹爪的机械臂和固定的RGB相机。设计了三个未见过的任务（移动物体到碗中、按指定角度旋转物体、将物体放置在托盘上）。</p>\n<p> * <strong>训练资源</strong>：论文未明确说明具体的计算资源（如GPU类型和数量），但提到了在VIMA-8k数据集上训练2个epoch。</p>\n\n<p>6. <strong>论文使用的评估环境和评估指标</strong></p>\n<p> * <strong>评估环境</strong>：</p>\n<p> * 模拟环境：VIMA-Bench，一个模拟的桌面机器人操作环境。</p>\n<p> * 真实世界环境：一个带有机械臂、夹爪和固定相机的真实世界设置，物体放置在平面上。</p>\n<p> * <strong>评估指标</strong>：</p>\n<p> * <strong>成功率</strong>：在模拟和真实世界实验中，主要使用任务成功率作为评估指标。对于VIMA-Bench，报告了三个难度级别（L1, L2, L3）的平均成功率。对于真实世界任务，报告了每个任务的20次随机初始化尝试的平均成功率。</think></p>\n<p>1. <strong>论文研究单位</strong></p>\n<p> * Stony Brook University</p>\n<p> * University of Wisconsin-Madison</p>\n\n<p>2. <strong>论文概述</strong></p>\n<p> 论文提出了LLaRA（Large Language and Robotics Assistant）框架，该框架将机器人动作策略构建为视觉文本对话，通过一种称为“视觉运动指令微调”的过程，将预训练的视觉语言模型（VLM）高效地转换为强大的视觉语言动作（VLA）模型。LLaRA的核心思想是，将现有的行为克隆数据集转换为对话风格的指令微调数据，并通过自监督的方式定义六个辅助任务来增强该数据集，而无需任何额外的动作标注。这使得在有限的数据量下，微调后的VLM也能产生有意义的机器人动作决策，并在多个模拟和真实世界的任务中实现了最先进的性能。</p>\n\n<p>3. <strong>论文核心贡献点</strong></p>\n<p> * 提出了一种将机器人操作任务表述为自然语言指令-响应对的方法，从而能够成功地将VLM指令微调为策略。</p>\n<p> * 将机器人动作与图像像素坐标对齐，使得预训练VLM到机器人领域的迁移更高效，特别是在训练数据有限时。</p>\n<p> * 识别并生成了能以自监督方式进一步增强机器人策略学习的辅助数据。</p>\n\n<p>4. <strong>论文方法描述</strong></p>\n<p> LLaRA框架包含三个主要部分：</p>\n<p> 1. <strong>数据生成</strong>：设计了一个自动化流程，将现有的行为克隆（BC）数据集转换为“对话风格”的指令微调数据集（称为Instruct-BC，或inBC）。具体来说，将状态-动作对转换为单轮对话，其中当前视觉观察和文本任务描述作为用户指令，而机器人的动作（例如，2D图像坐标和旋转角度）以自然语言格式作为模型响应。</p>\n<p> 2. <strong>多图像处理</strong>：为处理任务描述中的多图像输入，该方法通过目标检测将参考图像转换为文本描述，形成<code>D-inBC</code>数据集，使单图像VLM能够处理多模态指令。</p>\n<p> 3. <strong>数据增强</strong>：为了解决数据稀缺问题，提出了六种自监督的辅助数据集（定位、检测、动作预测、未来预测、空间关系、时间关系），这些数据集可以从现有轨迹中自动生成，无需额外标注，用于提升模型的空间和时间理解能力。</p>\n<p> 4. <strong>模型训练与推理</strong>：基于LLaVA架构，使用生成的指令数据对预训练VLM进行微调。在推理时，模型接收当前观察、任务描述和动作历史作为输入，生成文本形式的动作，该动作随后被解析并映射到机器人的控制空间。</p>\n\n<p>5. <strong>论文使用数据集和训练资源</strong></p>\n<p> * <strong>数据集</strong>：</p>\n<p> * 模拟实验：使用了VIMA-Bench环境，包含17个任务，并使用了VIMA数据集的三个子集（VIMA-0.8k, VIMA-8k, VIMA-80k）进行训练和评估。</p>\n<p> * 真实世界实验：构建了一个新的真实世界环境，使用一个带有夹爪的机械臂和固定的RGB相机。设计了三个未见过的任务（移动物体到碗中、按指定角度旋转物体、将物体放置在托盘上）。</p>\n<p> * <strong>训练资源</strong>：论文未明确说明具体的计算资源（如GPU类型和数量），但提到了在VIMA-8k数据集上训练2个epoch。</p>\n\n<p>6. <strong>论文使用的评估环境和评估指标</strong></p>\n<p> * <strong>评估环境</strong>：</p>\n<p> * 模拟环境：VIMA-Bench，一个模拟的桌面机器人操作环境。</p>\n<p> * 真实世界环境：一个带有机械臂、夹爪和固定相机的真实世界设置，物体放置在平面上。</p>\n<p> * <strong>评估指标</strong>：</p>\n<p> * <strong>成功率</strong>：在模拟和真实世界实验中，主要使用任务成功率作为评估指标。对于VIMA-Bench，报告了三个难度级别（L1, L2, L3）的平均成功率。对于真实世界任务，报告了每个任务的20次随机初始化尝试的平均成功率。</p>"
  },
  {
    "date": "2024-06-21",
    "title": "Learning Efficient and Robust Language-conditioned Manipulation using Textual-Visual Relevancy and Equivariant Language Mapping",
    "link": "http://arxiv.org/abs/2406.15677",
    "summary_markdown": "论文研究单位\n布朗大学、东北大学\n\n论文概述\n论文针对现有语言条件操作方法在处理未见场景时表现脆弱且需要大量数据或昂贵微调的问题，提出了一种名为GEM（Grounded Equivariant Manipulation）的新方法。GEM通过结合预训练视觉-语言模型的文本-视觉相关性和等变语言映射，实现了高效且鲁棒的操作策略。该方法在仅使用10-20个演示的情况下，就能达到与CLIPort和VIMA等基线相当或更高的性能，并展现出对未见物体和姿态更强的鲁棒性。\n\n论文核心贡献点\n1. 提出了一种构建块级文本和视觉相关性图的新方法，用于实现鲁棒的开放词汇识别。\n2. 分析了语言条件操作问题中潜在的对称性，并设计了名为语言可引导核的新型神经网络，用于构建语言条件等变网络。\n3. 在模拟和真实世界的一系列挑战性任务上，证明了其最先进的泛化鲁棒性和样本效率，仅需10%-20%的训练数据。\n\n论文方法描述\n方法包含三个主要模块。首先，相关性提取模块使用多视图图像和语言指令作为输入，利用预训练的CLIP模型生成像素级的相关性图，包括通过CLIP特征相似性计算的文本相关性图和通过演示数据检索增强语义的视觉相关性图。其次，语言条件抓取模块采用正交投影的RGB-D图像和语言指令，通过一个双分支网络输出抓取动作图：上分支是包含可学习动作模型和相关性提取器的视觉-语言编码器，下分支是等变语言映射，它将语言指令通过UNet映射为核，并在离散的旋转空间中进行变换以生成语言可引导核，从而实现物体级别的SE(2)等变性。最后，语言条件放置模块与抓取模块类似，但使用图像裁剪条件核来输出放置动作图。整个系统将动作表示为SE(2)坐标中的像素位置。\n\n论文使用数据集和训练资源\n数据集: CLIPort, VIMA\n训练资源: 由美国海军研究办公室（ONR）资助，编号为N000142412784，暗示使用了GPU等计算资源。\n\n论文使用的评估环境和评估指标\n评估环境: 模拟环境、真实世界\n评估指标: 成功率、数据效率（所需演示数量）、泛化鲁棒性（对未见物体和姿态的表现）。",
    "summary_html": "<p>论文研究单位</p>\n<p>布朗大学、东北大学</p>\n\n<p>论文概述</p>\n<p>论文针对现有语言条件操作方法在处理未见场景时表现脆弱且需要大量数据或昂贵微调的问题，提出了一种名为GEM（Grounded Equivariant Manipulation）的新方法。GEM通过结合预训练视觉-语言模型的文本-视觉相关性和等变语言映射，实现了高效且鲁棒的操作策略。该方法在仅使用10-20个演示的情况下，就能达到与CLIPort和VIMA等基线相当或更高的性能，并展现出对未见物体和姿态更强的鲁棒性。</p>\n\n<p>论文核心贡献点</p>\n<ol><li>提出了一种构建块级文本和视觉相关性图的新方法，用于实现鲁棒的开放词汇识别。</li><li>分析了语言条件操作问题中潜在的对称性，并设计了名为语言可引导核的新型神经网络，用于构建语言条件等变网络。</li><li>在模拟和真实世界的一系列挑战性任务上，证明了其最先进的泛化鲁棒性和样本效率，仅需10%-20%的训练数据。</li></ol>\n\n<p>论文方法描述</p>\n<p>方法包含三个主要模块。首先，相关性提取模块使用多视图图像和语言指令作为输入，利用预训练的CLIP模型生成像素级的相关性图，包括通过CLIP特征相似性计算的文本相关性图和通过演示数据检索增强语义的视觉相关性图。其次，语言条件抓取模块采用正交投影的RGB-D图像和语言指令，通过一个双分支网络输出抓取动作图：上分支是包含可学习动作模型和相关性提取器的视觉-语言编码器，下分支是等变语言映射，它将语言指令通过UNet映射为核，并在离散的旋转空间中进行变换以生成语言可引导核，从而实现物体级别的SE(2)等变性。最后，语言条件放置模块与抓取模块类似，但使用图像裁剪条件核来输出放置动作图。整个系统将动作表示为SE(2)坐标中的像素位置。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>数据集: CLIPort, VIMA</p>\n<p>训练资源: 由美国海军研究办公室（ONR）资助，编号为N000142412784，暗示使用了GPU等计算资源。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境: 模拟环境、真实世界</p>\n<p>评估指标: 成功率、数据效率（所需演示数量）、泛化鲁棒性（对未见物体和姿态的表现）。</p>"
  },
  {
    "date": "2024-06-13",
    "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
    "link": "http://arxiv.org/abs/2406.09246",
    "summary_markdown": "### 论文研究单位\n斯坦福大学、UC Berkeley、丰田研究院、Google Deepmind、Physical Intelligence、MIT。\n### 论文概述\n提出OpenVLA，一个7B参数的开源视觉-语言-动作模型（VLA），在970k真实机器人演示数据上训练。基于Llama 2语言模型，融合DINOv2和SigLIP视觉编码器，旨在解决现有VLAs闭源和微调效率低的问题。\n### 论文核心贡献点\n1. 开源高性能VLA模型，在29个任务上超越RT-2-X（55B参数）16.5%成功率。\n2. 首次系统探索VLA高效微调方法，支持LoRA参数微调。\n3. 实现量化推理，4-bit量化保持性能不变，内存减半。\n4. 发布完整代码库和训练基础设施，支持大规模训练。\n5. 在多机器人泛化和语言条件任务上表现最优。\n### 论文方法描述\n架构：采用Prismatic VLM框架，包含视觉编码器（DINOv2+SigLIP特征融合）、投影层（2层MLP）和LLM主干（Llama 2）。动作通过离散化为256个bin映射到语言token，训练目标为下一token预测。关键设计：微调视觉编码器、输入分辨率224×224px、训练27个epoch、固定学习率2e-5。\n### 论文使用数据集和训练资源\n数据集：Open X-Embodiment数据集，过滤后包含970k轨迹，覆盖多机器人形态。训练资源：64个A100 GPU训练14天，总计算量21,500 A100小时，批大小2048，采用FSDP并行。\n### 论文使用的评估环境和评估指标\n环境：真实机器人平台包括WidowX（BridgeData V2任务）和Google Robot（移动操作任务），模拟环境LIBERO。指标：任务成功率（success rate），测试视觉、运动、物理和语义泛化维度，以及多对象语言条件能力。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>斯坦福大学、UC Berkeley、丰田研究院、Google Deepmind、Physical Intelligence、MIT。</p>\n<h3>论文概述</h3>\n<p>提出OpenVLA，一个7B参数的开源视觉-语言-动作模型（VLA），在970k真实机器人演示数据上训练。基于Llama 2语言模型，融合DINOv2和SigLIP视觉编码器，旨在解决现有VLAs闭源和微调效率低的问题。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>开源高性能VLA模型，在29个任务上超越RT-2-X（55B参数）16.5%成功率。</li><li>首次系统探索VLA高效微调方法，支持LoRA参数微调。</li><li>实现量化推理，4-bit量化保持性能不变，内存减半。</li><li>发布完整代码库和训练基础设施，支持大规模训练。</li><li>在多机器人泛化和语言条件任务上表现最优。</li></ol>\n<h3>论文方法描述</h3>\n<p>架构：采用Prismatic VLM框架，包含视觉编码器（DINOv2+SigLIP特征融合）、投影层（2层MLP）和LLM主干（Llama 2）。动作通过离散化为256个bin映射到语言token，训练目标为下一token预测。关键设计：微调视觉编码器、输入分辨率224×224px、训练27个epoch、固定学习率2e-5。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>数据集：Open X-Embodiment数据集，过滤后包含970k轨迹，覆盖多机器人形态。训练资源：64个A100 GPU训练14天，总计算量21,500 A100小时，批大小2048，采用FSDP并行。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>环境：真实机器人平台包括WidowX（BridgeData V2任务）和Google Robot（移动操作任务），模拟环境LIBERO。指标：任务成功率（success rate），测试视觉、运动、物理和语义泛化维度，以及多对象语言条件能力。</p>"
  },
  {
    "date": "2024-06-06",
    "title": "RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation",
    "link": "http://arxiv.org/abs/2406.04339",
    "summary_markdown": "### 论文研究单位\n北京大学、AI²Robotics、北京智源人工智能研究院（BAAI）\n### 论文概述\n该论文提出了RoboMamba，一个高效的视觉-语言-行动（VLA）模型，用于机器人推理和操作。RoboMamba基于Mamba架构，旨在解决现有VLA模型在复杂任务推理能力不足和计算成本高的问题。模型集成了视觉编码器和Mamba语言模型，通过两阶段训练策略，使其同时具备通用常识、机器人相关推理能力和低级操作技能，并保持了高效的微调和推理速度。\n### 论文核心贡献点\n1. 提出RoboMamba，一个高效的端到端机器人VLA模型，它将视觉编码器与线性复杂度的Mamba语言模型相结合。\n2. 设计了一个高效的微调策略，仅通过微调一个简单的策略头（约占模型总参数的0.1%）即可赋予模型SE(3)姿态预测能力。\n3. 实验表明，RoboMamba在多个通用和机器人推理基准上表现出色，并在模拟和真实世界实验中展示了令人印象深刻的姿态预测结果，推理速度是现有VLA模型的3倍。\n### 论文方法描述\nRoboMamba的架构包括CLIP视觉编码器、一个多层感知机（MLP）作为跨模态连接器，以及一个Mamba语言模型。训练分为两个阶段：\n1. **阶段一：通用和机器人相关推理训练**\n - **1.1 对齐预训练**：冻结CLIP编码器和Mamba，仅使用558K图像-文本对数据集训练MLP连接器，对齐视觉特征与语言嵌入。\n - **1.2 指令协同训练**：解冻Mamba和连接器，结合通用指令数据集（如LLaVA混合指令数据集）和机器人指令数据集（如RoboVQA）进行训练，提升模型的视觉常识和机器人相关推理能力。\n2. **阶段二：机器人操作微调**\n - 冻结RoboMamba的所有参数，仅添加一个简单的策略头（包含两个MLP，分别用于预测末端执行器的位置和方向），该策略头仅占模型总参数的0.1%。\n - 使用SAPIEN模拟器生成的10K末端执行器姿态预测数据集进行微调，采用位置L1损失和方向反余弦损失。该过程仅需几分钟即可在单个A100 GPU上完成。\n### 论文使用数据集和训练资源\n**数据集**:\n- **阶段一**：LLaVA-LCS 558K（对齐预训练）、LLaVA混合指令数据集、ShareGPT4V-SFT数据集、LLaVA-Next数据集（指令协同训练）、300K RoboVQA样本（指令协同训练）。\n- **阶段二**：使用SAPIEN模拟器和PartNet-Mobility数据集收集的10K末端执行器姿态预测数据，用于操作微调。\n- **评估**：推理评估在VQAv2、OKVQA、RoboVQA、GQA等多个基准上进行；操作评估在SAPIEN模拟器中进行，测试集包含1.1K样本（20个已见任务和10个未见任务）。\n**训练资源**:\n- 所有实验均在NVIDIA A100 GPU上进行。\n- 阶段一训练使用AdamW优化器，学习率4e-5，混合精度（fp16）。\n- 阶段二微调学习率1e-5，权重衰减0.1，使用fp32精度。\n### 论文使用的评估环境和评估指标\n**评估环境**:\n- **模拟环境**：使用SAPIEN物理模拟引擎，与PartNet-Mobility数据集中的可动物体进行交互。机器人模型为配备吸盘夹爪的Franka Panda机器人。\n- **真实世界环境**：使用真实的Franka Panda机器人对多个可动物体进行操作。\n**评估指标**:\n- **推理能力**：在多个视觉问答基准上使用标准指标（如BLEU-4用于RoboVQA）进行评估。\n- **操作能力**：\n - **成功率**：在模拟环境中，成功操作样本数占总测试样本数的比例。一个操作被视为成功，当且仅当物体交互前后的关节状态变化超过0.1米的阈值。\n - **位置误差（L1 loss）**：预测位置与真实位置之间的平均绝对误差。\n - **方向误差**：预测旋转矩阵与真实旋转矩阵之间的反余弦误差。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>北京大学、AI²Robotics、北京智源人工智能研究院（BAAI）</p>\n<h3>论文概述</h3>\n<p>该论文提出了RoboMamba，一个高效的视觉-语言-行动（VLA）模型，用于机器人推理和操作。RoboMamba基于Mamba架构，旨在解决现有VLA模型在复杂任务推理能力不足和计算成本高的问题。模型集成了视觉编码器和Mamba语言模型，通过两阶段训练策略，使其同时具备通用常识、机器人相关推理能力和低级操作技能，并保持了高效的微调和推理速度。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出RoboMamba，一个高效的端到端机器人VLA模型，它将视觉编码器与线性复杂度的Mamba语言模型相结合。</li><li>设计了一个高效的微调策略，仅通过微调一个简单的策略头（约占模型总参数的0.1%）即可赋予模型SE(3)姿态预测能力。</li><li>实验表明，RoboMamba在多个通用和机器人推理基准上表现出色，并在模拟和真实世界实验中展示了令人印象深刻的姿态预测结果，推理速度是现有VLA模型的3倍。</li></ol>\n<h3>论文方法描述</h3>\n<p>RoboMamba的架构包括CLIP视觉编码器、一个多层感知机（MLP）作为跨模态连接器，以及一个Mamba语言模型。训练分为两个阶段：</p>\n<p>1. <strong>阶段一：通用和机器人相关推理训练</strong></p>\n<p> - <strong>1.1 对齐预训练</strong>：冻结CLIP编码器和Mamba，仅使用558K图像-文本对数据集训练MLP连接器，对齐视觉特征与语言嵌入。</p>\n<p> - <strong>1.2 指令协同训练</strong>：解冻Mamba和连接器，结合通用指令数据集（如LLaVA混合指令数据集）和机器人指令数据集（如RoboVQA）进行训练，提升模型的视觉常识和机器人相关推理能力。</p>\n<p>2. <strong>阶段二：机器人操作微调</strong></p>\n<p> - 冻结RoboMamba的所有参数，仅添加一个简单的策略头（包含两个MLP，分别用于预测末端执行器的位置和方向），该策略头仅占模型总参数的0.1%。</p>\n<p> - 使用SAPIEN模拟器生成的10K末端执行器姿态预测数据集进行微调，采用位置L1损失和方向反余弦损失。该过程仅需几分钟即可在单个A100 GPU上完成。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p><strong>数据集</strong>:</p>\n<ul><li><strong>阶段一</strong>：LLaVA-LCS 558K（对齐预训练）、LLaVA混合指令数据集、ShareGPT4V-SFT数据集、LLaVA-Next数据集（指令协同训练）、300K RoboVQA样本（指令协同训练）。</li><li><strong>阶段二</strong>：使用SAPIEN模拟器和PartNet-Mobility数据集收集的10K末端执行器姿态预测数据，用于操作微调。</li><li><strong>评估</strong>：推理评估在VQAv2、OKVQA、RoboVQA、GQA等多个基准上进行；操作评估在SAPIEN模拟器中进行，测试集包含1.1K样本（20个已见任务和10个未见任务）。</li></ul>\n<p><strong>训练资源</strong>:</p>\n<ul><li>所有实验均在NVIDIA A100 GPU上进行。</li><li>阶段一训练使用AdamW优化器，学习率4e-5，混合精度（fp16）。</li><li>阶段二微调学习率1e-5，权重衰减0.1，使用fp32精度。</li></ul>\n<h3>论文使用的评估环境和评估指标</h3>\n<p><strong>评估环境</strong>:</p>\n<ul><li><strong>模拟环境</strong>：使用SAPIEN物理模拟引擎，与PartNet-Mobility数据集中的可动物体进行交互。机器人模型为配备吸盘夹爪的Franka Panda机器人。</li><li><strong>真实世界环境</strong>：使用真实的Franka Panda机器人对多个可动物体进行操作。</li></ul>\n<p><strong>评估指标</strong>:</p>\n<ul><li><strong>推理能力</strong>：在多个视觉问答基准上使用标准指标（如BLEU-4用于RoboVQA）进行评估。</li><li><strong>操作能力</strong>：</li></ul>\n<p> - <strong>成功率</strong>：在模拟环境中，成功操作样本数占总测试样本数的比例。一个操作被视为成功，当且仅当物体交互前后的关节状态变化超过0.1米的阈值。</p>\n<p> - <strong>位置误差（L1 loss）</strong>：预测位置与真实位置之间的平均绝对误差。</p>\n<p> - <strong>方向误差</strong>：预测旋转矩阵与真实旋转矩阵之间的反余弦误差。</p>"
  },
  {
    "date": "2024-05-31",
    "title": "Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations",
    "link": "http://arxiv.org/abs/2406.00121",
    "summary_markdown": "### 论文研究单位\nCUHK, ByteDance Inc., University of California, Merced\n### 论文概述\n本文旨在解决现有基于文本的图像编辑方法在处理用户模糊、高层次的编辑意图（称为\"编辑提示\"）时效果不佳的问题，这种情况下存在从抽象意图到具体指令的\"创造力差距\"。为此，论文提出了一个新任务——图像编辑推荐（IER），其目标是从一张输入图像和一个简单的编辑提示出发，自动生成一组多样化且具创意的编辑指令。为了实现该任务，作者构建了一个名为Creativity-Vision Language Assistant (Creativity-VLA)的多模态框架，并引入了一个独特的\"token-for-localization\"机制，以同时支持全局和局部编辑推荐。实验证明，该模型能生成既富有创意又与图像内容和用户初始提示高度相关的编辑建议。\n### 论文核心贡献点\n1. 提出了图像编辑推荐（IER）任务，旨在将用户模糊的编辑意图转化为多样化且可执行的创意编辑指令。\n2. 构建了一个包含16,000个样本的专用创意指令数据集，该数据集通过结合GPT-4的思维链（CoT）提示技术与人工校对流程创建，旨在激发模型的发散性想象。\n3. 开发了Creativity-VLA模型，这是一个基于LLaVA的视觉语言助手，它将模型的输出解耦为文本建议和编辑位置，并引入\"token-for-localization\"机制，使模型能够为编辑指令推荐具体的图像区域，从而支持全局和局部两种编辑模式。\n### 论文方法描述\nCreativity-VLA模型基于LLaVA-7B构建。其核心创新在于将模型的输出解耦为两个部分：文本编辑建议（`O_sug`）和编辑位置（`O_loc`）。为实现这一点，模型在词汇表中引入了一个特殊标记`<EDIT>`。在生成过程中，模型首先输出文本建议，然后输出`<EDIT>`标记。该标记的嵌入向量会输入一个专门的定位解码器，该解码器由一个MLP投影层和一个三层Transformer组成。定位解码器将`<EDIT>`标记的嵌入与来自CLIP视觉编码器的图像特征进行交叉注意力计算，最终预测出执行编辑的边界框位置。模型采用端到端方式训练，总损失函数由文本生成的交叉熵损失和定位的L1损失与GIoU损失加权组成，其中视觉编码器参数被冻结。\n### 论文使用数据集和训练资源\n论文构建了一个包含16,000个编辑指令的内部数据集，内容涵盖人类、动物、室内外场景和商品特写等，其中全局和局部指令各占50%。数据集的构建分为四步：1）使用RAM、Grounding-DINO、SAM和BLIP-2进行详细的视觉理解；2）利用带思维链提示的GPT-4，根据编辑提示（如\"奢华\"）联想相关概念；3）提示GPT-4生成JSON格式的编辑指令；4）进行去重和人工整理，确保指令的有效性。训练时，模型基于LLaVA-7B架构，在自定义数据集上进行了3个epoch的微调。\n### 论文使用的评估环境和评估指标\n评估环境方面，论文使用InstructDiffusion作为全局编辑工具，GLIGEN作为局部编辑工具，以执行生成的编辑指令。对比基线包括直接编辑方法（MagicBrush, InstructDiffusion）和指令生成方法（LLaVA-v1.5, GPT-4V）。评估指标主要采用用户偏好评估，即由人工用户在提示对齐度、图像对齐度、视觉质量和多样性四个维度上对不同方法生成的编辑结果进行评分（分数越低表示越好）。此外，还使用CLIP分数作为辅助的自动化评估指标，用于衡量生成图像与文本指令之间的一致性。",
    "summary_html": "<h3>论文研究单位</h3>\n<p>CUHK, ByteDance Inc., University of California, Merced</p>\n<h3>论文概述</h3>\n<p>本文旨在解决现有基于文本的图像编辑方法在处理用户模糊、高层次的编辑意图（称为\"编辑提示\"）时效果不佳的问题，这种情况下存在从抽象意图到具体指令的\"创造力差距\"。为此，论文提出了一个新任务——图像编辑推荐（IER），其目标是从一张输入图像和一个简单的编辑提示出发，自动生成一组多样化且具创意的编辑指令。为了实现该任务，作者构建了一个名为Creativity-Vision Language Assistant (Creativity-VLA)的多模态框架，并引入了一个独特的\"token-for-localization\"机制，以同时支持全局和局部编辑推荐。实验证明，该模型能生成既富有创意又与图像内容和用户初始提示高度相关的编辑建议。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了图像编辑推荐（IER）任务，旨在将用户模糊的编辑意图转化为多样化且可执行的创意编辑指令。</li><li>构建了一个包含16,000个样本的专用创意指令数据集，该数据集通过结合GPT-4的思维链（CoT）提示技术与人工校对流程创建，旨在激发模型的发散性想象。</li><li>开发了Creativity-VLA模型，这是一个基于LLaVA的视觉语言助手，它将模型的输出解耦为文本建议和编辑位置，并引入\"token-for-localization\"机制，使模型能够为编辑指令推荐具体的图像区域，从而支持全局和局部两种编辑模式。</li></ol>\n<h3>论文方法描述</h3>\n<p>Creativity-VLA模型基于LLaVA-7B构建。其核心创新在于将模型的输出解耦为两个部分：文本编辑建议（<code>O_sug</code>）和编辑位置（<code>O_loc</code>）。为实现这一点，模型在词汇表中引入了一个特殊标记<code><EDIT></code>。在生成过程中，模型首先输出文本建议，然后输出<code><EDIT></code>标记。该标记的嵌入向量会输入一个专门的定位解码器，该解码器由一个MLP投影层和一个三层Transformer组成。定位解码器将<code><EDIT></code>标记的嵌入与来自CLIP视觉编码器的图像特征进行交叉注意力计算，最终预测出执行编辑的边界框位置。模型采用端到端方式训练，总损失函数由文本生成的交叉熵损失和定位的L1损失与GIoU损失加权组成，其中视觉编码器参数被冻结。</p>\n<h3>论文使用数据集和训练资源</h3>\n<p>论文构建了一个包含16,000个编辑指令的内部数据集，内容涵盖人类、动物、室内外场景和商品特写等，其中全局和局部指令各占50%。数据集的构建分为四步：1）使用RAM、Grounding-DINO、SAM和BLIP-2进行详细的视觉理解；2）利用带思维链提示的GPT-4，根据编辑提示（如\"奢华\"）联想相关概念；3）提示GPT-4生成JSON格式的编辑指令；4）进行去重和人工整理，确保指令的有效性。训练时，模型基于LLaVA-7B架构，在自定义数据集上进行了3个epoch的微调。</p>\n<h3>论文使用的评估环境和评估指标</h3>\n<p>评估环境方面，论文使用InstructDiffusion作为全局编辑工具，GLIGEN作为局部编辑工具，以执行生成的编辑指令。对比基线包括直接编辑方法（MagicBrush, InstructDiffusion）和指令生成方法（LLaVA-v1.5, GPT-4V）。评估指标主要采用用户偏好评估，即由人工用户在提示对齐度、图像对齐度、视觉质量和多样性四个维度上对不同方法生成的编辑结果进行评分（分数越低表示越好）。此外，还使用CLIP分数作为辅助的自动化评估指标，用于衡量生成图像与文本指令之间的一致性。</p>"
  },
  {
    "date": "2024-05-27",
    "title": "A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation",
    "link": "http://arxiv.org/abs/2405.17418",
    "summary_markdown": "```markdown\n### 论文研究单位\n北京大学计算机学院，多媒体信息处理国家重点实验室\n### 论文概述\n本文提出了一种自校正视觉-语言-动作模型（SC-VLA），用于机器人的系统操作。该框架模仿人类思维的快系统和慢系统：快系统直接预测末端执行器的SE(3)位姿以实现快速决策，慢系统通过思维链训练策略反思和纠正失败动作。此外，引入连续策略学习方法，将成功纠正的样本用于增强快系统的适应性，减少专家干预。模型在仿真和真实任务中均验证了其有效性和鲁棒性。\n### 论文核心贡献点\n1. 提出了SC-VLA框架，集成了快系统的直接动作预测能力和慢系统的反思纠错能力。\n2. 为慢系统设计了思维链训练策略，使模型能检测失败原因、请求专家反馈并逐步生成校正动作。\n3. 引入了连续策略学习方法，利用成功纠正的样本通过指数移动平均技术提升快系统的稳定性，降低对专家干预的依赖。\n### 论文方法描述\n1. **快系统**：使用LLaMA-Adapter V2作为基础模型，通过参数高效微调将位姿预测转化为语言建模问题，保留多模态大语言模型的推理能力。输入包括RGB图像和任务描述，输出为离散化的6-DoF位姿。\n2. **慢系统**：基于结束状态图像和机器人状态识别失败类型（位置、旋转或组合错误），并动态请求专家反馈：\n - 位置专家（如Where2Act）生成可供性图。\n - 旋转专家（如Anygrasp）预测潜在旋转。\n - 推理专家（如GPT-4V）细化接触点。\n 通过思维链推理整合错误原因和专家反馈，重新生成动作。\n3. **连续策略学习**：使用指数移动平均更新模型参数（α=0.999），结合成功纠正样本微调适配器，实现知识从慢系统到快系统的转移。\n### 论文使用数据集和训练资源\n1. **数据集**：在SAPIEN仿真器中生成12k成功样本、15k失败样本和60k校正提示；使用PartNet-Mobility数据集进行开环实验；RLBench任务收集100个episodes per task用于闭环训练。\n2. **训练资源**：基于80GB A100 GPU，使用Adam优化器（学习率2e-5）训练10个epochs，耗时约3小时。\n### 论文使用的评估环境和评估指标\n1. **评估环境**：开环实验在SAPIEN仿真器中测试，使用PartNet-Mobility数据集；闭环实验在RLBench环境中进行；真实世界实验部署于Franka Panda机械臂，进行sim-to-real迁移测试。\n2. **评估指标**：任务成功率（success rate），基于预定义条件（如任务完成状态）评估，包括开环位姿准确率和闭环任务成功率。\n```",
    "summary_html": "<p>```markdown</p>\n<h3>论文研究单位</h3>\n<p>北京大学计算机学院，多媒体信息处理国家重点实验室</p>\n<h3>论文概述</h3>\n<p>本文提出了一种自校正视觉-语言-动作模型（SC-VLA），用于机器人的系统操作。该框架模仿人类思维的快系统和慢系统：快系统直接预测末端执行器的SE(3)位姿以实现快速决策，慢系统通过思维链训练策略反思和纠正失败动作。此外，引入连续策略学习方法，将成功纠正的样本用于增强快系统的适应性，减少专家干预。模型在仿真和真实任务中均验证了其有效性和鲁棒性。</p>\n<h3>论文核心贡献点</h3>\n<ol><li>提出了SC-VLA框架，集成了快系统的直接动作预测能力和慢系统的反思纠错能力。</li><li>为慢系统设计了思维链训练策略，使模型能检测失败原因、请求专家反馈并逐步生成校正动作。</li><li>引入了连续策略学习方法，利用成功纠正的样本通过指数移动平均技术提升快系统的稳定性，降低对专家干预的依赖。</li></ol>\n<h3>论文方法描述</h3>\n<ol><li><strong>快系统</strong>：使用LLaMA-Adapter V2作为基础模型，通过参数高效微调将位姿预测转化为语言建模问题，保留多模态大语言模型的推理能力。输入包括RGB图像和任务描述，输出为离散化的6-DoF位姿。</li><li><strong>慢系统</strong>：基于结束状态图像和机器人状态识别失败类型（位置、旋转或组合错误），并动态请求专家反馈：</li></ol>\n<p> - 位置专家（如Where2Act）生成可供性图。</p>\n<p> - 旋转专家（如Anygrasp）预测潜在旋转。</p>\n<p> - 推理专家（如GPT-4V）细化接触点。</p>\n<p> 通过思维链推理整合错误原因和专家反馈，重新生成动作。</p>\n<p>3. <strong>连续策略学习</strong>：使用指数移动平均更新模型参数（α=0.999），结合成功纠正样本微调适配器，实现知识从慢系统到快系统的转移。</p>\n<h3>论文使用数据集和训练资源</h3>\n<ol><li><strong>数据集</strong>：在SAPIEN仿真器中生成12k成功样本、15k失败样本和60k校正提示；使用PartNet-Mobility数据集进行开环实验；RLBench任务收集100个episodes per task用于闭环训练。</li><li><strong>训练资源</strong>：基于80GB A100 GPU，使用Adam优化器（学习率2e-5）训练10个epochs，耗时约3小时。</li></ol>\n<h3>论文使用的评估环境和评估指标</h3>\n<ol><li><strong>评估环境</strong>：开环实验在SAPIEN仿真器中测试，使用PartNet-Mobility数据集；闭环实验在RLBench环境中进行；真实世界实验部署于Franka Panda机械臂，进行sim-to-real迁移测试。</li><li><strong>评估指标</strong>：任务成功率（success rate），基于预定义条件（如任务完成状态）评估，包括开环位姿准确率和闭环任务成功率。</li></ol>\n<p>```</p>"
  },
  {
    "date": "2024-05-23",
    "title": "A Survey on Vision-Language-Action Models for Embodied AI",
    "link": "http://arxiv.org/abs/2405.14093",
    "summary_markdown": "论文研究单位\n- The Chinese University of Hong Kong, Hong Kong, China\n- Huawei Noah's Ark Lab, Shenzhen, China\n\n论文概述\n这篇论文是首个关于具身AI的视觉-语言-动作模型（VLAs）的全面综述。VLA是能够处理视觉和语言多模态输入以生成机器人动作来完成具身任务的模型。论文提出了VLA的广义定义，建立了详细的分类体系，并回顾了该领域的最新进展。论文涵盖了VLA的组件、低层控制策略、高层任务规划器，以及相关的数据集和基准，最后讨论了当前挑战和未来方向。\n\n论文核心贡献点\n- 提供了首个全面的VLA模型综述\n- 提出了基于当前机器人系统分层框架的分类法，包括低层控制策略和高层任务规划器\n- 总结了训练和评估VLAs的必要资源，包括数据集、模拟器和基准\n- 概述了当前面临的挑战和有前景的未来研究方向\n\n论文方法描述\n论文采用系统性的综述方法，将VLA研究分为三个主要方向：\n1. VLA组件：包括强化学习、预训练视觉表示、视频表示、动力学学习、世界模型和推理\n2. 低层控制策略：包括非Transformer控制策略、基于Transformer的控制策略、多模态指令控制策略、3D视觉控制策略、基于扩散的控制策略、运动规划策略、基于点的动作策略和大型VLA\n3. 高层任务规划器：分为整体式任务规划器（端到端规划器、带3D视觉的端到端规划器、基础规划器）和模块化任务规划器（基于语言的规划器、基于代码的规划器）\n\n论文使用数据集和训练资源\n论文总结了多种数据集和训练资源：\n- 真实世界机器人数据集：如Franka Kitchen, CALVIN, xArm7数据集\n- 模拟环境数据集：如Meta-World, Adroit, DeepMind Control Suite, Habitat, TriFinger, Minecraft\n- 自动化数据集收集方法\n- 人类数据集\n- 任务规划基准\n- 具身问答基准\n- 模拟器：如Habitat, Unity等\n\n论文使用的评估环境和评估指标\n论文在以下环境中进行评估：\n- 模拟环境：Meta-World, Franka Kitchen, Adroit, DeepMind Control Suite, Habitat, TriFinger, Minecraft\n- 真实机器人环境：Franka机械臂、xArm7等\n- 任务类型：操作任务（如抓取、放置、开门）和导航任务\n\n评估指标包括：\n- 任务成功率\n- 泛化能力到未见任务的表现\n- 多任务学习性能\n- 长视野任务完成度\n- 动作预测准确性",
    "summary_html": "<p>论文研究单位</p>\n<ul><li>The Chinese University of Hong Kong, Hong Kong, China</li><li>Huawei Noah's Ark Lab, Shenzhen, China</li></ul>\n\n<p>论文概述</p>\n<p>这篇论文是首个关于具身AI的视觉-语言-动作模型（VLAs）的全面综述。VLA是能够处理视觉和语言多模态输入以生成机器人动作来完成具身任务的模型。论文提出了VLA的广义定义，建立了详细的分类体系，并回顾了该领域的最新进展。论文涵盖了VLA的组件、低层控制策略、高层任务规划器，以及相关的数据集和基准，最后讨论了当前挑战和未来方向。</p>\n\n<p>论文核心贡献点</p>\n<ul><li>提供了首个全面的VLA模型综述</li><li>提出了基于当前机器人系统分层框架的分类法，包括低层控制策略和高层任务规划器</li><li>总结了训练和评估VLAs的必要资源，包括数据集、模拟器和基准</li><li>概述了当前面临的挑战和有前景的未来研究方向</li></ul>\n\n<p>论文方法描述</p>\n<p>论文采用系统性的综述方法，将VLA研究分为三个主要方向：</p>\n<ol><li>VLA组件：包括强化学习、预训练视觉表示、视频表示、动力学学习、世界模型和推理</li><li>低层控制策略：包括非Transformer控制策略、基于Transformer的控制策略、多模态指令控制策略、3D视觉控制策略、基于扩散的控制策略、运动规划策略、基于点的动作策略和大型VLA</li><li>高层任务规划器：分为整体式任务规划器（端到端规划器、带3D视觉的端到端规划器、基础规划器）和模块化任务规划器（基于语言的规划器、基于代码的规划器）</li></ol>\n\n<p>论文使用数据集和训练资源</p>\n<p>论文总结了多种数据集和训练资源：</p>\n<ul><li>真实世界机器人数据集：如Franka Kitchen, CALVIN, xArm7数据集</li><li>模拟环境数据集：如Meta-World, Adroit, DeepMind Control Suite, Habitat, TriFinger, Minecraft</li><li>自动化数据集收集方法</li><li>人类数据集</li><li>任务规划基准</li><li>具身问答基准</li><li>模拟器：如Habitat, Unity等</li></ul>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>论文在以下环境中进行评估：</p>\n<ul><li>模拟环境：Meta-World, Franka Kitchen, Adroit, DeepMind Control Suite, Habitat, TriFinger, Minecraft</li><li>真实机器人环境：Franka机械臂、xArm7等</li><li>任务类型：操作任务（如抓取、放置、开门）和导航任务</li></ul>\n\n<p>评估指标包括：</p>\n<ul><li>任务成功率</li><li>泛化能力到未见任务的表现</li><li>多任务学习性能</li><li>长视野任务完成度</li><li>动作预测准确性</li></ul>"
  },
  {
    "date": "2024-05-09",
    "title": "Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations",
    "link": "http://arxiv.org/abs/2405.06039",
    "summary_markdown": "论文研究单位\nIntelligent Space Robotics Laboratory, Center for Digital Engineering, Skolkovo Institute of Science and Technology, Moscow, Russia\n\n论文概述\n该研究介绍了Bi-VLA（视觉-语言-动作）模型，一个为双手机器人灵巧操作设计的新系统，无缝集成了视觉场景理解、将人类指令翻译为可执行代码的语言理解以及物理动作生成。系统通过一系列家务任务评估其功能，包括根据人类要求制作指定的沙拉。\n\n论文核心贡献点\n提出了一个名为Bi-VLA的视觉-语言-动作模型系统，专门用于协调双手机器人完成复杂操作。系统在语言模块生成可执行代码的成功率达到100%，视觉模块检测特定食材的成功率达到96.06%，整体任务执行成功率达到83.4%。\n\n论文方法描述\n系统接收用户任务请求和相机图像。语言模块使用Starling-LM-7B-alpha模型，首先进行语义规划，然后通过代码生成器将计划翻译为可执行的Python API函数调用。这些API函数（如`move_to_object`, `grasp`, `cut`等）控制机器人的具体动作。视觉语言模块集成Qwen-VL模型，用于场景理解、检测物体并返回其2D像素坐标。系统通过相机标定参数和畸变校正模型（Brown-Conrady模型）将检测到的2D像素坐标转换为3D世界坐标，从而指导机器人手臂进行精确操作。\n\n论文使用数据集和训练资源\n该研究采用了一种数据需求量较低的方法，无需对核心模型进行训练或微调，而是集成了预训练的Starling-LM-7B-alpha（语言模块）和Qwen-VL（视觉语言模块）。评估任务是在真实世界中设置的制作沙拉场景，而非使用标准化的公开数据集。文中未详细说明其底层预训练模型的训练资源。\n\n论文使用的评估环境和评估指标\n评估环境为一个真实世界的家务任务场景，具体任务是根据指令制作沙拉。评估指标包括：语言模块生成正确可执行代码的成功率（100%）、视觉模块检测特定食材的成功率（96.06%）以及系统正确执行用户请求任务的整体成功率（83.4%）。",
    "summary_html": "<p>论文研究单位</p>\n<p>Intelligent Space Robotics Laboratory, Center for Digital Engineering, Skolkovo Institute of Science and Technology, Moscow, Russia</p>\n\n<p>论文概述</p>\n<p>该研究介绍了Bi-VLA（视觉-语言-动作）模型，一个为双手机器人灵巧操作设计的新系统，无缝集成了视觉场景理解、将人类指令翻译为可执行代码的语言理解以及物理动作生成。系统通过一系列家务任务评估其功能，包括根据人类要求制作指定的沙拉。</p>\n\n<p>论文核心贡献点</p>\n<p>提出了一个名为Bi-VLA的视觉-语言-动作模型系统，专门用于协调双手机器人完成复杂操作。系统在语言模块生成可执行代码的成功率达到100%，视觉模块检测特定食材的成功率达到96.06%，整体任务执行成功率达到83.4%。</p>\n\n<p>论文方法描述</p>\n<p>系统接收用户任务请求和相机图像。语言模块使用Starling-LM-7B-alpha模型，首先进行语义规划，然后通过代码生成器将计划翻译为可执行的Python API函数调用。这些API函数（如<code>move_to_object</code>, <code>grasp</code>, <code>cut</code>等）控制机器人的具体动作。视觉语言模块集成Qwen-VL模型，用于场景理解、检测物体并返回其2D像素坐标。系统通过相机标定参数和畸变校正模型（Brown-Conrady模型）将检测到的2D像素坐标转换为3D世界坐标，从而指导机器人手臂进行精确操作。</p>\n\n<p>论文使用数据集和训练资源</p>\n<p>该研究采用了一种数据需求量较低的方法，无需对核心模型进行训练或微调，而是集成了预训练的Starling-LM-7B-alpha（语言模块）和Qwen-VL（视觉语言模块）。评估任务是在真实世界中设置的制作沙拉场景，而非使用标准化的公开数据集。文中未详细说明其底层预训练模型的训练资源。</p>\n\n<p>论文使用的评估环境和评估指标</p>\n<p>评估环境为一个真实世界的家务任务场景，具体任务是根据指令制作沙拉。评估指标包括：语言模块生成正确可执行代码的成功率（100%）、视觉模块检测特定食材的成功率（96.06%）以及系统正确执行用户请求任务的整体成功率（83.4%）。</p>"
  }
]