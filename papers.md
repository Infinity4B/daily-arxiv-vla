| 日期 | 标题 | 链接 | 简要总结 |
| --- | --- | --- | --- |
| 2025-10-09 | Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered | http://arxiv.org/abs/2510.08464v1 | <details><summary>展开</summary>待生成</details> |
| 2025-10-09 | Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track | http://arxiv.org/abs/2510.07871v1 | <details><summary>展开</summary>本文针对IROS 2025 RoboSense挑战赛的社交导航赛道，提出了一种基于主动风险感知的社交导航方法。核心要点如下： 1. **问题背景**： - 任务要求自主代理在动态人类环境中（如室内场景）仅使用RGB-D传感器和里程计进行导航，需同时满足目标到达、路径效率及社会规范（如避碰、安全距离）。 2. **方法创新**： - 在Falcon框架基础上，引入**主动风险感知模块（Proactive Risk Perception Module）**，通过神经网络预测周围人类的距离风险分数（分危险区、警告区、安全区），增强代理的空间意识和避碰能力。 - 该模块整合到Falcon的辅助任务学习中，通过加权损失函数强化高风险场景训练，无需额外推理开销。 3. **实验与结果**： - 在Social-HM3D数据集（大规模室内场景，含动态人类代理）上评估。 - 关键指标：成功率（SR）、路径长度加权成功率（SPL）、个人空间合规率（PSC）、人类碰撞率（H-Coll）。 - **竞赛成绩**：总分0.6994（SR=0.656, SPL=0.5958, PSC=0.8608, H-Coll=0.33），在16支队伍中排名第二，验证了方法的有效性。</details> |
| 2025-10-09 | USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots | http://arxiv.org/abs/2510.07869v1 | <details><summary>展开</summary>本文提出USIM仿真数据集与U0模型，用于提升水下机器人的多任务自主能力。核心要点如下： ### 1. **USIM数据集** - **构成**：基于Stonefish仿真平台构建，包含561K帧数据（15.6小时）、1,852条轨迹，覆盖20种任务（如视觉导航、移动抓取）和9种水下场景。 - **特点**： - 多模态数据：双目视觉图像、压力传感器、IMU、DVL及机器人动作（推进器PWM信号+机械臂关节角度）。 - 语言指令驱动任务，支持大规模自动化采集。 ### 2. **U0模型** - **架构**：基于Isaac-GR00T N1.5改进的视觉-语言-动作（VLA）模型。 - **创新设计**： - **多模态融合**：整合双目视觉与非视觉传感器，增强空间感知与水下定位能力。 - **CAP模块**：卷积注意力感知增强模块，通过VLM特征引导提升目标检测与操作精度（如抓取任务中距离目标缩短21.2%）。 - **轻量化**：30亿参数，可部署于嵌入式平台（如NVIDIA Jetson）。 ### 3. **实验验证** - **性能**： - **非抓取任务**（巡检、避障等）：平均成功率80%。 - **移动抓取任务**：相比基线模型，机器人距目标距离减少21.2%。 - **优势**：双目输入显著优于单目，CAP模块有效解决水下视觉退化问题。 ### 4. **贡献与意义** - 首个面向水下机器人的多任务VLA框架，证明仿真数据可有效训练通用模型。 - 为水下机器人自主性提升提供新思路，支持后续真实场景迁移与多模态扩展（如声呐融合）。 --- **总结**：USIM与U0解决了水下机器人缺乏高质量数据集和通用模型的难题，通过仿真数据与多模态融合设计，显著提升了多任务执行能力，为水下自主系统发展奠定基础。</details> |
| 2025-10-09 | IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction | http://arxiv.org/abs/2510.07778v1 | <details><summary>展开</summary>本文提出**IntentionVLA**，一种用于人机交互（HRI）的通用高效具身意图推理框架，解决现有视觉-语言-动作（VLA）模型在隐式意图理解和实时推理上的不足。核心贡献如下： ### 1. **核心问题** - 现有VLAs依赖显式指令到动作的映射，缺乏**意图推理密集型预训练**，导致在复杂HRI场景中表现不佳。 - 推理机制效率低（如ECoT的长文本链生成），无法满足实时交互需求。 ### 2. **方法设计** - **数据构造**： - 设计高效标注流程，生成三类数据： 1. **意图推理链**：GPT-4o分解用户指令为多步意图分析。 2. **空间推理链**：融合目标物体检测（Florence-2）与机械臂末端3D位姿投影，增强空间感知。 3. **紧凑推理**：压缩为“移动方向→目标物体”短序列（如“向右移动至手机”），加速推理。 - **模型架构**： - 基于Qwen2.5-7B VLM主干，添加**可学习查询（Learnable Queries）** 提取动作特征。 - **连接器（Connector）** 模块对齐推理与动作生成，输出条件引导DiT扩散模型生成7-DoF机械臂动作。 - **训练范式**： - **两阶段课程学习**： 1. **预训练阶段**：VLM学习三类推理数据，强化意图理解与空间感知。 2. **微调阶段**：冻结VLM，用紧凑推理引导动作生成模块（DiT），避免遗忘推理能力。 ### 3. **高效推理** - 推理时首先生成紧凑推理序列（耗时0.2秒），再单步前向生成动作，显著降低延迟。 ### 4. **实验结果** - **分布内任务**： - 意图指令成功率**45%**，远超ECoT（16.7%）和π0（20%）；显式指令成功率**48.3%**。 - **泛化能力**： - **分布外意图指令**：平均成功率**30%**（超ECoT 2倍）。 - **新物体操作**：在未见过物体任务上成功率达**20–30%**（基线多失败）。 - **零样本HRI**： - 真人交互任务成功率**40%**（静态手）、**30%**（动态手），推理延迟仅**0.72秒**（ECoT需3.41秒）。 - **多模态理解**： - 在MMMU、HallBench等基准上超越专用多模态模型（如LLaVA），验证强感知能力。 ### 5. **消融实验** - 移除任一推理数据（意图/空间/紧凑）均导致性能下降**13–17%**，验证三者协同必要性。 - 两阶段训练策略（冻结VLM微调动作）比联合训练更稳定。 ### 结论 IntentionVLA通过**意图感知数据构造**、**紧凑推理引导**及**两阶段训练**，实现高效、泛化的具身意图推理，为下一代HRI系统提供新范式。</details> |
| 2025-10-08 | WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation | http://arxiv.org/abs/2510.07313v1 | <details><summary>展开</summary>论文提出**WristWorld**框架，通过4D世界模型从锚点视角生成机械臂腕部视角视频，解决机器人数据集中腕部视角稀缺的问题。核心贡献如下： 1. **两阶段框架设计**： - **重建阶段**：扩展视觉几何模型VGGT，加入腕部视角预测头，并设计**空间投影一致性损失（SPC Loss）**，仅利用RGB对应关系监督几何一致性，估计腕部相机位姿和4D点云。 - **生成阶段**：基于扩散的视频生成模型，以重建的投影点云和锚点视角的CLIP语义特征为条件，合成时空一致的腕部视角视频。 2. **关键技术**： - **SPC Loss**：通过2D-2D对应关系和3D点云投影，在无深度监督下保证跨视角几何对齐。 - **多模态条件融合**：将点云投影图与外部视角CLIP嵌入结合，增强生成视频的语义一致性。 3. **实验效果**： - 在Droid、Calvin和Franka Panda数据集上实现**SOTA视频生成质量**（FVD↓ 42%，PSNR↑ 17.8）。 - 生成数据提升下游视觉-语言-动作（VLA）模型性能：在Calvin上任务平均完成长度提升**3.81%**，缩小锚点-腕部视角性能差距的**42.4%**。 - 可**即插即用**扩展单视角世界模型，无需额外腕部数据。 4. **应用价值**：低成本扩展机器人数据集腕部视角数据，显著增强精细操作任务的感知能力。</details> |
| 2025-10-08 | TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking | http://arxiv.org/abs/2510.07134v1 | <details><summary>展开</summary>TrackVLA++是一种新型视觉-语言-动作（VLA）模型，旨在增强具身视觉跟踪（EVT）中的推理和记忆能力。核心创新包括： 1. **Polar-CoT空间推理机制**：通过链式思维（CoT）范式推断目标的相对位置，编码为紧凑的极坐标令牌，提升空间推理效率。 2. **目标识别记忆（TIM）模块**：采用置信度感知门控更新策略，长期保存目标特征，有效应对遮挡和相似干扰物，确保时空一致性。 实验结果表明： - 在EVT-Bench基准测试中，DT分割任务上单视角成功率提升5.1%，多视角提升12%，均达最先进水平。 - 在Gym-UnrealCV等基准上实现零样本泛化，真实场景中动态遮挡下跟踪鲁棒性显著增强。 该方法解决了现有模型缺乏显式推理和长期记忆的缺陷，显著提升复杂场景的跟踪性能。</details> |
| 2025-10-08 | Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications | http://arxiv.org/abs/2510.07077v1 | <details><summary>展开</summary>这篇论文对视觉-语言-动作（VLA）模型在机器人领域的应用进行了系统性综述，重点聚焦于实际部署的挑战与解决方案。以下是核心要点总结： ### 一、研究背景与挑战 1. **VLA模型定义** 通过端到端框架整合视觉、语言和动作模态，直接生成机器人控制指令（如机械臂位姿、抓取指令），排除仅使用VLMs进行高层规划的方法。 2. **核心挑战** - **数据稀缺性**：需大规模对齐视觉、语言和动作的三模态数据，但高质量机器人演示数据获取成本高。 - **跨具身泛化**：不同机器人（机械臂/轮式/足式）的动作空间和传感器差异阻碍策略迁移。 - **计算成本**：多模态Transformer训练和推理的资源需求高，限制实时部署。 ### 二、架构设计演进 1. **主流架构类型** - **传感器模型**（占比最高）：7类变体（如Transformer+离散动作、VLM+扩散动作头、VLM+流匹配）。 - **世界模型**：预测未来观测指导动作生成（如视频预测+逆动力学模型）。 - **功能模型**：预测物体可操作性（affordance）并生成动作。 2. **关键创新** - **层次化策略**（如RT-H）：高层输出语言化动作描述，低层生成具体控制指令，提升长任务性能。 - **跨模态处理**： - 视觉：SigLIP/DINOv2特征提取 + TokenLearner压缩 - 语言：T5/LLaMA分词 + FiLM条件融合 - 动作：离散分桶(256 bins)/流匹配/扩散生成 ### 三、训练与部署实践 1. **数据策略** - **数据集**：Open-X Embodiment（130,000演示）支持跨机器人训练。 - **增强方法**：域随机化、轨迹插值、仿真到真实迁移。 - **预训练**：基于网络规模VLMs（如PaLM-E）初始化，注入常识知识。 2. **训练范式** - **监督学习**：两阶段训练（预训练 + 任务微调）。 - **强化学习**：微调动作头（如iRe-VLA）或作高层策略（如Humanoid-VLA）。 - **自监督**：视频预测学习潜在动作表示（如LAPA）。 ### 四、实际应用与评估 1. **机器人平台** - 多样化硬件：单臂（Franka）、双足（Atlas）、轮式（Stretch）。 - **传感器配置**：RGB-D相机（>80%方案） + 力传感（接触任务）。 2. **评估基准** - **模拟环境**：RLBench, MetaWorld - **真实指标**：任务成功率（RT-2达97%）、泛化性（新物体/场景） ### 五、未来方向 1. **多模态扩展**：触觉/音频信息融合（如Tactile-VLA）。 2. **推理能力**：链式思考（CoT-VLA）提升复杂任务规划。 3. **安全机制**：实时故障检测与恢复（如Safe-VLA）。 4. **持续学习**：非灾难性遗忘架构（如iManip）。 > 论文提供实用指南：https://vla-survey.github.io ，涵盖架构选型建议、数据集索引和部署checklist。</details> |
| 2025-10-08 | Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models | http://arxiv.org/abs/2510.07067v1 | <details><summary>展开</summary>这篇论文研究了无关上下文对具身AI中视觉语言动作（VLA）模型性能的影响，核心发现如下： 1. **问题背景** VLA模型使机器人能理解和执行自然语言指令，但真实场景中的人类指令常含无关内容（如闲聊）或改写表达。现有研究未充分评估模型对此类语言扰动的鲁棒性。 2. **关键实验** - **人类指令改写**：收集众包改写指令，测试模型性能下降约20%，揭示模型与真实语言使用的适配差距。 - **无关上下文影响**： - **长度变化**：性能随无关文本长度增加持续下降（长上下文导致最高58%性能损失）。 - **语义相似性**：与任务词汇相似的无关内容（如"沙发上有苹果"）引发50%性能骤降，远高于随机文本（10%下降）。 3. **模型脆弱性** VLA模型对训练集相似的词汇和语义干扰最敏感，尤其在LIBERO和Habitat 2.0仿真环境中测试的5种主流模型（OpenVLA/UniAct等）均表现此弱点。 4. **解决方案** 提出基于LLM的过滤框架： - 使用prompt工程从噪声指令中提取核心命令。 - 使模型性能恢复率达98.5%，显著提升抗干扰能力。 - 小至0.5B参数的LLM即可有效处理随机噪声，但需3B+模型处理语义复杂干扰。 5. **实际意义** 揭示了VLA模型部署到真实场景的潜在风险，为提升具身AI的指令理解鲁棒性提供了可落地的技术路径（过滤框架已开源）。 **总结**：无关上下文（尤其语义相似内容）会严重损害VLA模型性能，而轻量级LLM过滤层可低成本解决此问题，推动可靠具身AI的发展。</details> |
| 2025-10-08 | RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training | http://arxiv.org/abs/2510.06710v1 | <details><summary>展开</summary>本文提出了RLinf-VLA框架，旨在解决视觉语言动作模型（VLA）与强化学习（RL）结合训练时的效率和统一性问题。核心要点如下： 1. **问题背景**：当前VLA模型主要采用监督微调（SFT），面临分布偏移下的泛化不足问题。RL虽能通过交互优化任务表现，但缺乏统一高效的训练框架。 2. **框架设计**： - **统一性**：支持多类VLA架构（如OpenVLA、OpenVLA-OFT）、RL算法（PPO、GRPO）和仿真器（ManiSkill、LIBERO）。 - **高效性**： - 提出三种GPU分配策略：**并置分配**（共享GPU）、**分离分配**（组件独占GPU）和创新的**混合分配**（结合细粒度流水线）。 - 混合分配在GPU并行仿真器中实现1.61–1.88倍加速，并置分配优化CPU并行仿真器资源。 - **算法增强**：轻量级价值函数、损失归一化、动作掩码等技术提升训练效率。 3. **关键创新**： - **混合流水线机制**：将仿真器实例分割为子模块并行执行，减少空闲时间。 - **灵活配置**：通过YAML文件切换分配模式，支持LoRA参数微调。 4. **实验结果**： - **高性能**：单一模型在130个LIBERO任务中达98.11%成功率，25个ManiSkill任务中达97.66%。 - **强泛化**：在Franka机器人实机部署中，RL训练策略的零样本泛化能力显著优于SFT（8/30 vs. 0/30成功）。 - **最佳实践**： - PPO：动作级价值估计优于分块级，部分环境重置提升样本效率。 - GRPO：轨迹长度归一化和动作掩码对稳定训练至关重要。 - 通用建议：扩大批量大小有效；LoRA需超参数重调。 5. **开源贡献**：框架已开源，旨在标准化具身智能研究，加速算法迭代。 RLinf-VLA通过系统与算法协同优化，为VLA+RL的大规模训练提供了高效、可扩展的解决方案。</details> |
| 2025-10-07 | Verifier-free Test-Time Sampling for Vision Language Action Models | http://arxiv.org/abs/2510.05681v1 | <details><summary>展开</summary>论文提出了一种无需验证器的测试时采样方法MG-Select，用于提升视觉语言动作模型（VLAs）在精细操作任务中的精度。核心要点如下： 1. **问题背景** VLAs在机器人控制中表现优异，但其单次推理范式（贪婪解码）难以处理高精度任务（如抓取和放置）。现有基于外部验证器的测试时缩放方法需要额外训练且泛化性差。 2. **方法创新** - **MG-Select框架**： - 通过温度采样生成N个候选动作（并行推理）。 - 提出**条件掩码分布置信度**：使用KL散度衡量预测分布与参考分布的差异，选择差异最大的动作。参考分布通过对状态/语言输入随机掩码生成，确保最大不确定性。 - 支持文本掩码、状态掩码或双掩码变体（不同任务适用不同变体）。 - **联合训练策略**：在微调时随机丢弃状态或语言条件，使模型同时学习条件分布和无条件分布，提升参考分布质量（MG-Select*）。 3. **实验结果** - **仿真任务**：在RoboCasa、SIMPLER-WidowX和LIBERO基准测试中，MG-Select显著提升基线模型（如π₀-FAST和OpenVLA），尤其在低数据场景下（RoboCasa 30条演示时抓取任务提升168%）。 - **实物机器人**：在Franka Research 3上，ID任务提升28%（如"盒子到碗"任务），OOD任务提升35%（如"拾取胶带"）。 - **消融实验**：验证关键设计（如文本掩码置信度最优、温度τ=4.0正则化、前5个token聚合策略），单预填充部署降低45%延迟。 4. **优势** 无需外部模块或额外训练，利用模型内部属性提升精度，适用于不同VLA架构，为高精度机器人控制提供通用解决方案。</details> |
| 2025-10-07 | MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption | http://arxiv.org/abs/2510.05580v1 | <details><summary>展开</summary>本文提出**MetaVLA**框架，解决当前视觉-语言-动作（VLA）模型在具身任务适应中的三大瓶颈： 1. **任务特定微调成本高**（如OpenVLA需240K训练步） 2. **跨任务泛化能力弱** 3. **计算资源消耗大** ### 核心创新 - **统一元协同训练**： - 将多目标任务（如LIBERO四个任务套件）整合至**单阶段微调**，减少68.75%训练步数（75K步）。 - 引入**结构化辅助任务**（如GR00T数据集中的6种机械臂任务），通过上下文感知机制注入跨域知识，提升泛化性。 - **轻量级元学习模块**： - 基于注意力神经过程（ANP）设计 **Meta-Action-Reasoner (MAR)**，动态融合目标与上下文信息，仅增加0.3ms/词元推理延迟。 - 避免传统多任务学习的优化冲突，支持异构任务（如单/双臂操作、不同摄像机视角）。 ### 关键结果 在LIBERO基准测试中： - **性能提升**：平均成功率比OpenVLA高4.4%（LIBERO-Long任务提升8.0%），超越朴素多任务SFT 3.1%。 - **效率优化**：GPU训练时间减少76%（约24小时），模型数量从4个降至1个。 - **泛化性**：单一模型在目标域和辅助域任务中均表现鲁棒。 ### 工程价值 - **骨干无关**：兼容OpenVLA等预训练模型，支持监督微调与强化学习流程。 - **可扩展性**：上下文批大小与辅助任务数量可灵活扩展，为通用具身智能体提供新路径。 > 代码开源地址：https://stellar-neuron.github.io/metavla/</details> |
| 2025-10-06 | StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation | http://arxiv.org/abs/2510.05057v1 | <details><summary>展开</summary>论文提出StaMo框架，通过无监督学习从静态图像中编码高度紧凑的状态表示（仅两个令牌），并利用预训练Diffusion Transformer解码器。关键创新在于状态差异自然涌现为可泛化的机器人运动，无需显式运动监督或视频数据。该方法高效且可解释，能无缝集成到现有视觉-语言-动作（VLA）模型中，提升LIBERO基准任务性能14.3%和真实世界任务成功率30%。实验验证其支持策略协同训练（优于基线10.4%）并有效扩展至多源数据（仿真、真实机器人及人类视频）。</details> |
| 2025-10-06 | HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks | http://arxiv.org/abs/2510.04898v1 | <details><summary>展开</summary>本文提出HyperVLA模型，通过超网络（Hypernetwork, HN）架构解决Vision-Language-Action（VLA）模型推理成本高的问题。核心要点如下： 1. **问题背景** - 传统VLA模型（如OpenVLA）需在**每个时间步激活整个模型**（约7.6B参数），导致高延迟（如6Hz），难以满足实时控制需求。 2. **解决方案：HyperVLA架构** - **分层设计**： - **超网络（HN）**：基于任务上下文（语言指令+初始图像）生成轻量级策略参数，**仅在新任务开始时激活**。 - **基础策略**：接收当前图像观测，输出动作，**每步推理仅激活小型网络**（0.1M参数）。 - **推理效率**：相比OpenVLA，激活参数量减少**90倍**（7.6B→86M），推理速度提升**120倍**（482ms→4ms/步）。 3. **关键技术优化** - **视觉主干**：基础策略采用预训练DINOv2作为图像编码器，避免小规模机器人数据过拟合。 - **HN归一化**：对上下文嵌入做尺度归一化（除以$\sqrt{d_e}$），稳定超网络训练。 - **动作生成**：使用线性动作头+均方误差损失，替代复杂自回归/扩散策略，加速训练与推理。 4. **实验结果** - **零样本泛化**：在SIMPLER基准上，性能匹配或超越OpenVLA（如Google Robot任务平均成功率63% vs 45%）。 - **少样本适应**：LIBERO任务微调后平均成功率89%，显著优于基线（OpenVLA为77%）。 - **消融实验**：移除任一设计（如DINOv2主干或HN归一化）均导致性能下降，验证必要性。 5. **开源**：代码已公开（https://github.com/MasterXiong/Hyper-VLA）。 HyperVLA通过解耦任务间/任务内知识，实现了高性能与高效率的平衡，为通用机器人控制提供新思路。</details> |
| 2025-10-05 | ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context | http://arxiv.org/abs/2510.04246v1 | <details><summary>展开</summary>论文提出**ContextVLA**模型，旨在解决机器人任务中利用多帧观察时存在的性能不一致和计算开销大的问题。核心要点如下： 1. **问题背景**： - 机器人任务（如物体遮挡、长视野操作）需时间上下文（多帧观察），但传统行为克隆模型使用多帧时性能不稳定且计算成本高。 - Vision-Language-Action模型（VLA）虽能有效提取多帧信息，但视频输入的高维度导致训练和推理效率低下。 2. **方法创新**： - **上下文压缩**：在VLM主干中间层，将过去多帧观察通过平均池化压缩为**单个上下文标记**，保留时间信息（如物体运动轨迹）。 - **高效架构**：仅处理当前帧与压缩后的上下文标记，减少计算量。支持自回归或扩散式动作解码器生成动作。 - **KV缓存优化**：推理时预计算并缓存过去帧的中间状态，进一步降低延迟。 3. **实验验证**： - **性能提升**：在Libero、Simpler-WidowX和Robocasa等基准测试中，ContextVLA一致优于单帧VLA（如π₀、GR00T），平均成功率提升最高达14.4%。在需时间上下文的任务（如真实世界Pick-and-Place）中提升更显著（单帧25% → 多帧65%）。 - **效率优势**：相比全帧处理，训练时间减少30%，推理延迟降低57%（227ms → 96ms），实现多帧收益的低成本部署。 4. **核心贡献**： - 首次证明VLM的时间理解能力可高效利用多帧上下文，通过压缩机制平衡性能与计算效率，为实时机器人应用提供可行方案。</details> |
| 2025-10-05 | SITCOM: Scaling Inference-Time COMpute for VLAs | http://arxiv.org/abs/2510.04041v1 | <details><summary>展开</summary>论文提出SITCOM框架，旨在增强视觉语言动作模型（VLA）的长时程规划能力。核心要点如下： 1. **问题背景**： VLA模型虽能执行单步指令，但缺乏前瞻机制，在动态任务中易受错误累积影响，导致长时程规划失败。 2. **方法创新**： - **推理时计算扩展**：在推理阶段，SITCOM通过高温度采样生成多个候选动作序列，利用预训练的Transformer动态模型模拟多步状态轨迹（如10步展开）。 - **奖励驱动选择**：基于模拟器奖励函数（如夹爪-物体距离、抓取成功率）评分轨迹，选择最优序列执行，受模型预测控制（MPC）启发。 - **动态模型优化**：在BridgeV2数据集上预训练，并在SIMPLER环境微调，结合L1和LPIPS损失减少"现实-模拟"差距；采用DAgger策略缓解长时展开的误差累积。 3. **实验结果**： - 在SIMPLER环境的物体放置、堆叠等任务中，SITCOM将平均任务完成率从基准模型（OpenVLA）的48%提升至72%（使用动态模型）和76%（使用环境模拟）。 - 性能随候选轨迹数量（广度）和展开步长（深度）增加而提升，但计算时间相应增长（如25候选需160秒）。 4. **贡献总结**： SITCOM将单步VLA转化为鲁棒的长时程规划器，显著提高复杂任务成功率，核心是通过动态模型模拟和奖励选择实现推理时计算扩展。</details> |
| 2025-10-04 | Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert | http://arxiv.org/abs/2510.03896v1 | <details><summary>展开</summary>论文提出一种新框架，通过可泛化动作专家桥接视觉语言模型（VLM）的思考能力与物理行动。核心要点如下： 1. **问题与动机**： - 传统视觉语言动作（VLA）模型因集成推理与行动，在稀缺数据下泛化能力差，且易遗忘VLM的预训练知识。 - 双系统方法（解耦思考与行动）仍受语义模糊性限制，阻碍大规模跨任务训练。 2. **解决方案**： - **可泛化动作专家**：作为核心组件，使用稀疏3D轨迹（路点）作为VLM与低层控制的接口。 - VLM仅需生成粗糙3D路点（在相机坐标系），保留其视觉推理优势。 - 动作专家结合实时点云观测，将路点细化为密集可执行动作序列。 - **训练范式**：提出“动作预训练+点云微调”策略： - 预训练阶段：仅用轨迹数据学习基础运动技能（高效大批量训练）。 - 微调阶段：引入点云数据，学习环境感知的轨迹优化。 3. **优势**： - 完全解耦高层规划与低层执行，动作专家免于语义解释负担，专注几何优化。 - 支持零样本部署（无需任务特定微调），适应新视角、物体和指令。 4. **实验结果**： - 在模拟（RoboTwin、ManiSkill）和真实机器人任务中验证： - 短/中/长视野任务成功率优于基线（如Pi0、RDT），长视野任务提升显著（平均60%）。 - 在颜色、物体、语义变化下泛化能力强，点云质量鲁棒（如PromptDepth增强数据）。 - 训练效率高：动作预训练加速收敛，点云微调提升数据利用率。 **总结**：该框架通过稀疏3D轨迹和专用训练策略，释放VLM的物理潜力，实现高效、泛化强的机器人控制。</details> |
| 2025-10-04 | NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation | http://arxiv.org/abs/2510.03895v1 | <details><summary>展开</summary>这篇论文提出了NoTVLA框架，用于解决视觉-语言-动作（VLA）模型在机器人操作中存在的灾难性遗忘问题。核心创新点如下： 1. **问题识别** 传统VLA模型依赖密集动作轨迹训练，导致： - 灾难性遗忘：新任务训练会覆盖旧任务知识 - 计算成本高：需大量计算资源（如π₀模型的1/10以上） - 视角敏感：依赖腕部摄像头 2. **解决方案：NoTVLA框架** 通过稀疏轨迹规划提升泛化能力： - **轨迹聚焦策略**：从目标物体轨迹转向机器人末端执行器轨迹 - **时空压缩**：基于运动学选择关键帧（加速度/夹爪状态变化） - **稀疏训练**：仅用关键帧训练（8.1K轨迹/100K关键帧） 3. **三大核心技术** - **锚点深度推理**：先预测2D锚点，再结合外部深度源生成三维动作标记 - **样条动作解码器**：将离散标记转为平滑高频轨迹（支持闭环控制） - **跨平台部署**：统一轨迹标注支持多机器人平台（Franka/Aloha等） 4. **显著优势** - **性能**：多任务成功率接近单任务专家模型 - **泛化能力**： - 零样本执行新任务（如指令语义反转） - 跨视角鲁棒性（无腕部摄像头） - 跨机器人平台部署 - **效率**：比π₀节省超10倍算力 5. **实验验证** - RoboTwin 2.0：40项任务中成功率超基准模型（如点击闹钟任务78% vs 基准63%） - AGIBOT挑战赛：总分3.697优于基线UniVLA(2.795) - 深度推理消融：显式深度输入使长时任务成功率提升15% 论文通过解耦高层推理与底层控制，在保留语言能力的同时解决了灾难性遗忘问题，为通用机器人部署提供了高效方案。 --- *注：总结基于论文HTML原文的标题、摘要、方法及实验章节核心内容，已过滤实现细节与参考文献。*</details> |
| 2025-10-04 | LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization | http://arxiv.org/abs/2510.03827v1 | <details><summary>展开</summary>本文提出LIBERO-PRO评估框架，旨在解决当前LIBERO基准评测VLA模型时的严重缺陷。核心要点如下： 1. **问题诊断** - 现有模型在LIBERO基准上虽达90%+准确率，但主要依赖**任务序列记忆**而非真正理解 - 实验显示：当目标物体被替换/移除（如茶杯→螺丝刀）、物体位置微调、指令被破坏时，模型仍输出相同动作轨迹（图1） - 位置扰动下模型成功率骤降至0%（图2），揭示当前评估高分的虚假性 2. **解决方案：LIBERO-PRO框架** - 设计四维扰动体系： - **物体属性**（颜色/纹理/尺寸） - **初始位姿**（空间重组） - **指令语义**（同义替换）与**任务结构**（目标替换） - **环境背景**（场景布局变更） - 引入严格约束：扰动需保持任务可执行性，且与原始任务距离＞阈值（公式10-11） 3. **实验结果** - 主流模型（OpenVLA/pi0/pi0.5）在LIBERO-PRO上全面崩溃： - 位置扰动下平均成功率趋近0%（表2-5） - 任务级扰动（如"取茶杯"→"取碗"）成功率均为0% - 仅pi0.5在部分位置任务达38%，仍远低于原始分数 - 揭示模型本质缺陷：无法泛化到训练分布外场景 4. **核心贡献** - 实证当前VLA评估协议失效，呼吁社区弃用纯记忆导向评测 - 开源可扩展评测套件（GitHub地址），支持多维扰动组合 - 建立新评估标准：模型需通过扰动测试才具实际应用价值 > 总结：LIBERO-PRO暴露VLA模型严重依赖数据记忆的本质缺陷，为构建真正鲁棒的具身智能体提供可靠评估工具。</details> |
| 2025-10-02 | Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer | http://arxiv.org/abs/2510.03342v1 | <details><summary>展开</summary>待生成</details> |
| 2025-10-03 | MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning | http://arxiv.org/abs/2510.03142v1 | <details><summary>展开</summary>本文提出**MM-Nav**，一种基于多专家学习的多视角视觉-语言-动作（VLA）模型，用于鲁棒的视觉导航。核心要点如下： 1. **多视角输入** 使用**360°四摄像头系统**（前、右、后、左视图）覆盖全向环境，解决单视角观测的视野局限问题。 2. **多专家学习框架** - 在仿真环境中训练**三个强化学习（RL）专家**，分别专注不同导航能力： - **到达（Reaching）**：避开静态障碍到达目标 - **穿行（Squeezing）**：通过狭窄空间 - **避障（Avoiding）**：躲避动态障碍物 - 专家使用**特权深度信息**（实际部署时不可用）。 3. **师生迭代训练** - **预训练**：用专家成功轨迹初始化VLA学生模型。 - **在线迭代**： - 部署VLA到仿真环境，收集专家动作数据。 - **动态平衡数据比例**：基于各能力性能差距（加权旅行时间WTT）调整训练数据分布，优先补强薄弱能力。 - 迭代微调直至收敛。 4. **模型架构** - 基于预训练大语言模型（Qwen2-7B）和视觉基础模型（SigLIP）。 - 输出**连续速度指令**（\(v_x, v_y, v_{yaw}\)），支持全向敏捷运动。 - 通过分层token设计实现**7Hz实时推理**。 5. **实验验证** - **仿真测试**：在混合场景（静态+动态障碍+狭窄通道）中： - 成功率达47%，超越所有基线（如NavDP的23%）。 - **超越RL专家**，证明多能力协同效应。 - **实物测试**：在Unitree GO2机器人上实现零样本仿真到现实迁移，成功应对狭窄走廊、动态障碍等复杂场景。 **结论**：MM-Nav通过多视角感知、多专家能力融合及动态平衡训练，解决了纯视觉导航的几何理解难题，兼具高鲁棒性与实时性。</details> |
| 2025-10-03 | Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation - Technical Report for IROS 2025 RoboSense Challenge Track 4 | http://arxiv.org/abs/2510.02728v1 | <details><summary>展开</summary>本文针对跨模态无人机导航中的自然语言引导图像检索问题，提出了一种两阶段的描述引导检索系统（CGRS）。核心要点如下： 1. **问题**：现有基线方法在复杂空中场景下难以实现文本查询与视觉内容的细粒度语义匹配，导致检索精度不足。 2. **方法**：CGRS采用两阶段流程： - **粗排阶段**：使用GeoText-1652基线模型获取查询对应的前20个候选图像。 - **重排阶段**：利用视觉语言模型（VLM）为候选图像生成详细描述，通过文本相似度计算（查询与描述）进行细粒度重排，增强语义对齐。 3. **结果**：在IROS 2025 RoboSense Challenge Track 4上，CGRS显著提升基线性能，Recall@1/5/10指标均提高5%，最终获得第二名（TOP-2），验证了其在真实导航场景的有效性。 4. **贡献**： - 提出CGRS框架，通过描述生成桥接跨模态语义鸿沟。 - 引入描述引导重排机制，将视觉匹配转化为文本相似度计算。 - 实现性能稳定提升并赢得竞赛奖项。</details> |
| 2025-10-02 | Contrastive Representation Regularization for Vision-Language-Action Models | http://arxiv.org/abs/2510.01711v1 | <details><summary>展开</summary>本文提出了一种针对视觉-语言-动作（VLA）模型的对比表示正则化方法，核心贡献如下： 1. **Robot State-aware Contrastive Loss (RS-CL)** 针对预训练视觉语言模型（VLM）对机器人信号（如本体感知状态）不敏感的问题，提出一种对比损失函数。该损失利用机器人本体状态（如末端执行器位姿）的相对距离作为软监督信号，引导VLM表示与机器人控制相关信号对齐，同时保留语义信息。 2. **轻量级兼容设计** - **视图截断（View Cutoff）**：在表示层随机屏蔽单一观测视图的特征，实现高效的数据增强。 - **可学习摘要令牌**：压缩长序列VLM输出，降低计算开销。 - 与标准动作预测损失（流匹配目标）联合优化，无缝集成到现有VLA训练流程。 3. **显著性能提升** - **模拟任务**：在RoboCasa-Kitchen基准上，平均成功率提升4.8%（30 demo）→ 4.0%（300 demo）；抓放任务成功率从30.8%提升至41.5%（+11.2%）。LIBERO基准平均成功率从95.7%提升至96.4%。 - **真实机器人任务**：7-DoF机械臂操作成功率从45.0%提升至58.3%（+13.3%），尤其在遮挡场景下（如闭合锅盖任务）表现出更强的定位鲁棒性。 4. **消融验证** - 本体状态监督优于其他信号（如动作序列距离）。 - 视图截断优于其他表示增强方法，多视图不变性学习是关键。 该方法通过显式对齐视觉语义表示与机器人物理状态，解决了VLA模型在精细操作任务中的性能瓶颈，为具身智能提供了新思路。</details> |
| 2025-10-02 | FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models | http://arxiv.org/abs/2510.01642v1 | <details><summary>展开</summary>论文提出**FailSafe系统**，用于解决视觉-语言-动作（VLA）模型在机器人操作中遇到故障时的自主推理与恢复问题。核心要点如下： 1. **问题背景** - 现有VLA模型在机器人操作中易因不可预测故障失败，而传统数据集仅提供正确轨迹或文本反馈，无法直接生成可执行恢复动作。 2. **FailSafe系统设计** - **故障生成**：在仿真环境中自动注入三类故障（平移、旋转、无操作扰动），模拟真实故障场景。 - **动作收集**：通过匹配故障位姿与校正位姿，生成**7自由度可执行恢复动作**（非文本指令）。 - **有效性验证**：引入**Sanity Check机制**，确保恢复动作能引导任务成功完成。 3. **数据集与模型** - 构建 **FailSafe数据集**（含131k故障-动作对），覆盖多视角图像、任务指令及故障类型。 - 基于LLaVa-OV-7B微调得到 **FailSafe-VLM模型**，专精故障检测与动作生成。 4. **实验结果** - 在ManiSkill任务中，FailSafe-VLM显著提升三大VLA模型性能： - OpenVLA平均成功率提升 **22.6%**。 - OpenVLA-OFT提升8.0%，πₒ-FAST提升4.0%。 - 支持跨空间配置、相机视角及机器人本体（如未训练的xArm机械臂）泛化。 5. **贡献与开源** - 首个自动生成**可执行恢复动作**的框架，无需人工干预。 - 提升VLA模型鲁棒性，推动自主机器人发展。 - 承诺**开源代码**（项目页：https://jimntu.github.io/FailSafe/）。</details> |
| 2025-10-02 | VLA-R1: Enhancing Reasoning in Vision-Language-Action Models | http://arxiv.org/abs/2510.01623v1 | <details><summary>展开</summary>论文提出**VLA-R1模型**，旨在解决现有视觉-语言-动作（VLA）模型缺乏显式推理和训练优化不足的问题。核心贡献如下： 1. **推理增强架构** - 引入**RLVR（基于可验证奖励的强化学习）** 结合 **GRPO（分组相对策略优化）**，通过三类可验证奖励优化模型： - **区域对齐奖励**（GIoU）：提升空间感知鲁棒性 - **轨迹一致性奖励**（改进Fréchet距离）：确保轨迹合理性与平滑度 - **格式奖励**：强制结构化输出（`<think>`推理段+`<output>`动作段） 2. **高质量数据集** - 构建 **VLA-CoT-13K** 数据集，提供13,000条显式链式思维（CoT）标注，对齐场景中的功能约束与轨迹标签，强化推理步骤监督。 3. **多场景验证** - **领域内任务**：IoU提升17.78%（达36.51），轨迹误差降低17.25% - **跨领域任务**：保持最优泛化性能（IoU 33.96） - **真实机器人实验**：成功率显著提升（感知任务62.5%，轨迹执行75%） 4. **开源资源** 模型、代码及数据集已公开（GitHub: `https://github.com/GigaAI-research/VLA-R1`）。 **总结**：VLA-R1通过系统性优化推理与执行的协同，显著提升VLA模型在复杂场景中的鲁棒性和泛化能力，为具身智能提供新解决方案。</details> |
| 2025-10-01 | INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models | http://arxiv.org/abs/2510.01389v1 | <details><summary>展开</summary>待生成</details> |
| 2025-10-01 | Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition | http://arxiv.org/abs/2510.01068v1 | <details><summary>展开</summary>这篇论文提出了一种名为“通用策略组合”（GPC）的训练免费方法，用于提升基于扩散模型或流模型的机器人策略性能。以下是核心要点： 1. **核心创新**：GPC通过**测试时分布级组合**，将预训练的扩散/流策略的分布分数进行凸组合（\(s_{\text{comp}} = \sum w_i s_i\)），无需额外训练即可生成更优策略。组合后策略性能可超越任一原始策略（图1）。 2. **理论基础**： - **函数级改进**：证明多个分数估计器的凸组合可降低分数误差（命题4.1）。 - **系统级稳定性**：采样轨迹误差受累积分数误差约束（命题4.2），确保单步改进传播至整体性能提升（推论4.1）。 3. **方法特性**： - **灵活性**：支持异构策略组合（如视觉动作VA与视觉语言动作VLA模型、扩散/流模型），兼容不同输入模态（图2）。 - **自适应权重**：通过测试时搜索确定最优权重（算法1），权重配置对任务敏感（图1c）。 4. **实验结果**： - **仿真环境**：在Robomimic、PushT、RoboTwin基准测试中，GPC平均成功率提升2-7%（表1-2）。例如： - \(\pi_0\)+FP组合在Robomimic Can任务达99.5%（表1）。 - RDT+DP\({}_{\text{pcd}}\)在RoboTwin任务提升7%（表2）。 - **真实机器人**：4项任务验证有效性（图3），如双瓶抓取任务成功率提升24%（表3）。 - **权重分析**：最优权重因任务而异（表3），0.3-0.6区间常表现最佳。 5. **扩展机制**： - 支持逻辑运算符组合（如AND/OR），通过概率密度乘积或权重调整实现不同组合语义（第5.3节）。 6. **意义**：为利用现有策略提升控制性能提供高效途径，项目页面见[https://sagecao1125.github.io/GPC-Site/](https://sagecao1125.github.io/GPC-Site/)。</details> |
| 2025-10-01 | HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy | http://arxiv.org/abs/2510.00695v2 | <details><summary>展开</summary>论文提出HAMLET框架，用于将视觉-语言-动作（VLA）模型转换为历史感知策略。核心要点如下： 1. **问题**：现有VLA模型仅依赖当前观察，忽略历史上下文，限制了长时程任务的性能，而直接添加历史帧会显著增加计算开销（延迟+35%，内存消耗×3.6）。 2. **方法**： - **时刻令牌（Moment Tokens）**：压缩每个时间步的感知信息，通过时间对比学习初始化，强调任务相关动态特征。 - **记忆模块（Memory Module）**：轻量级Transformer整合历史时刻令牌，生成历史感知特征用于动作预测。 3. **优势**：无需重新预训练，可插拔式适配现有VLA模型，计算高效（仅增加2%延迟）。 4. **实验结果**： - 真实世界任务：成功率76.4%，比基线（GR00T N1.5）提升47.2%。 - RoboCasa Kitchen：成功率66.4%（100-demo），超越基线64.1%。 - LIBERO：成功率97.7%，超越基线95.6%。 - 通用性：适配不同VLA（如CogACT），在SimplerEnv-Bridge任务提升11.4%。 5. **贡献**：解决VLA历史依赖问题，提升长时程任务性能，框架设计高效且通用。</details> |
| 2025-10-01 | Hybrid Training for Vision-Language-Action Models | http://arxiv.org/abs/2510.00600v1 | <details><summary>展开</summary>这篇论文提出了一种名为混合训练（Hybrid Training, HyT）的新框架，用于提升视觉-语言-动作模型（VLAs）的性能并解决推理延迟问题。核心要点如下： ### 1. **问题背景** - **Embodied CoT（ECoT）的局限性**：现有方法通过生成语言形式的思维链（Chain-of-Thought）提升VLAs的任务性能，但思维链生成显著增加推理时间（降低3倍频率），影响实时性（如机器人操控）。 - **关键假设**：思维链训练的核心价值在于**内部化知识**而非推理过程本身，模型可发展"专家直觉"直接输出动作。 ### 2. **HyT框架设计** - **多模态条件训练**： - 引入模态变量（`<act>`, `<think>`, `<follow>`），控制模型输出形式： - **`<act>`**：直接输出动作（如标准VLA） - **`<think>`**：先输出思维链再生成动作（如ECoT） - **`<follow>`**：根据外部指令生成动作（如分层策略） - **训练目标**： - 联合优化三个分布的负对数似然损失（公式3），通过蒙特卡洛采样平衡数据分布。 - 默认采样权重：`<act>`(0.25), `<think>`(0.5), `<follow>`(0.25) ### 3. **核心优势** - **推理效率**： - 测试时使用`<act>`模态，**跳过思维链生成**，推理速度与标准VLA相同（比ECoT快3倍）。 - 在A100 GPU上达3Hz动作频率。 - **性能提升**： - **ClevrSkills基准**：HyT在3000条演示数据下比标准VLA高15%成功率（图3），且优于ECoT和分层方法。 - **LIBERO基准**：结合OFT微调方案，以93.7%平均成功率刷新SOTA（图5）。 - **真实机器人实验**（xArm 6）：比OpenVLA高22%总体成功率（表1），尤其在分布外任务提升显著。 ### 4. **关键结论** - **思维链的价值**：训练时学习思维链可**增强环境理解**（类似辅助损失），但推理时非必需。 - **灵活性**：支持按需切换模态（如`<think>`提升可解释性，`<follow>`支持人机协作）。 - **数据效率**：在小规模数据（300条演示）下，HyT比标准VLA性能高10%以上（图6），证明其高效知识迁移能力。 ### 5. **实验验证** - 在模拟（ClevrSkills, LIBERO）和真实场景验证，覆盖**空间推理**（Place At）、**物体交互**（Place OnTop）、**长视野任务**（Stack Tower）。 - 消融实验：不同模态在相同任务性能相近，但`<follow>`模态结合外部指令可进一步提升成功率（图4）。 > 总结：HyT通过多模态条件训练，使VLAs在保持实时推理的同时获得性能提升，为机器人部署提供高效解决方案。</details> |
| 2025-10-01 | VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators | http://arxiv.org/abs/2510.00406v1 | <details><summary>展开</summary>本文提出VLA-RFT框架，通过世界模型驱动的强化微调提升视觉-语言-动作（VLA）模型的决策能力。核心创新点包括： 1. **问题解决**：针对传统VLA模型依赖模仿学习导致的误差累积和分布偏移鲁棒性差的问题，提出利用数据驱动的世界模型（WM）作为可控模拟器。 2. **方法框架**： - **两阶段训练**：先预训练WM（学习环境动态）和VLA策略（生成稳定动作）；再通过WM交互进行强化微调 - **验证奖励机制**：WM基于动作序列预测未来视觉轨迹，与目标参考轨迹对比生成密集奖励信号 - **GRPO优化**：采用广义强化策略优化算法，结合随机微分方程（SDE）策略实现高效更新 3. **关键优势**： - **高效性**：仅需400步微调即超越强监督基线（平均成功率+4.7%） - **鲁棒性**：在目标位移/机器人状态扰动等场景下保持稳定性能（扰动任务成功率最高提升6.7%） - **样本效率**：相比模拟器RL降低3个数量级的交互需求 4. **验证结果**： - WM生成质量优异（PSNR 25.23dB，LPIPS 0.059） - 奖励机制对比显示轨迹级验证奖励效果最佳（比动作级奖励高3.4%成功率） - LIBERO基准测试中91.1%平均成功率，长周期任务提升显著 该方法为VLA模型提供了一种高效、安全的强化微调范式，显著提升了泛化能力和抗干扰性。</details> |
| 2025-09-30 | MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation | http://arxiv.org/abs/2509.26642v1 | <details><summary>展开</summary>本文提出MLA（多感官语言-动作模型），用于增强机器人操作中的多模态理解和预测。现有视觉-语言-动作（VLA）模型主要依赖2D图像，难以全面捕捉空间依赖和物理动态。MLA的创新点包括： 1. **无编码器多模态对齐**：直接利用大型语言模型（LLM）作为感知模块，通过位置引导的对比学习对齐2D图像、3D点云和触觉信号，无需额外编码器。 2. **未来多感官生成**：通过后训练策略预测未来多模态状态（如语义、几何和交互信息），提升物理动态推理能力，为动作生成提供鲁棒条件。 3. **三阶段训练管道**：大规模预训练（570K轨迹）、监督微调（整合多模态对齐）和后训练（加入未来预测），高效整合感知、推理与动作生成。 实验表明，MLA在复杂接触丰富的真实任务（如单/双臂操作）中，性能优于SOTA 2D和3D VLA方法12%和24%，并显著提升对未见物体和背景的泛化能力。项目网站：https://sites.google.com/view/open-mla。</details> |
| 2025-09-30 | Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA | http://arxiv.org/abs/2509.26251v1 | <details><summary>展开</summary>### 论文要点总结： 1. **问题背景**： 现有潜在动作模型（LAMs）在视觉-语言-动作（VLA）系统中存在两个瓶颈： - **空间理解不足**：端到端图像编码器忽略几何结构（如物体关系、场景布局）。 - **时间感知有限**：依赖稀疏帧输入，难以捕捉长时动态和精细运动。 导致动作表示不稳定，影响决策可靠性。 2. **核心方法**： - **Farsighted-LAM**： - **几何感知空间编码**：利用DINOv2特征提取结构先验（空间布局、深度）。 - **多尺度时间建模**：通过连续帧序列捕捉长时趋势与瞬时交互（如接触）。 - **损失设计**：结合RGB光度损失与深度梯度损失，增强多模态重建能力。 - **SSM-VLA框架**： - 三阶段推理流程： 1. **VisualCoT预测**：显式生成未来视觉状态（RGB+深度）。 2. **潜在动作推断**：输出结构化动作计划。 3. **动作生成**：通过流匹配模型生成可执行动作。 - **多模态协同注意力**：整合历史观测、语言指令与预测状态，提升跨模态交互。 3. **实验结果**： - **模拟环境**：在CALVIN ABC-D基准测试中达到SOTA（平均任务链长度4.38，优于VPP等基线）。 - **真实世界**：在机器人操作任务中成功部署，泛化至杂乱场景。 - **消融验证**： - Farsighted-LAM结构（多帧输入）提升性能11.3%。 - 几何先验（深度监督）贡献显著，移除后性能下降2.5%。 4. **贡献价值**： - 首次将显式空间结构（深度）与动态建模结合到LAMs中。 - SSM-VLA通过链式推理增强决策可解释性，为具身智能提供鲁棒、可迁移的解决方案。</details> |
| 2025-09-30 | TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses | http://arxiv.org/abs/2509.25746v1 | <details><summary>展开</summary>以下是论文要点的中文总结： 1. **核心问题**：针对机器人抓取执行阶段的位姿误差（尤其对薄/平面物体），提出纯触觉解决方案TacRefineNet，通过多指尖触觉反馈实现毫米级精度的在位物体姿态调整。 2. **关键技术**： - **硬件设计**：在欠驱动灵巧手（11自由度）指尖集成压阻式触觉传感器（11×9阵列），将接触力转化为触觉图像。 - **迭代调整机制**：基于当前触觉图像、目标触觉图像和本体感觉，通过多分支网络预测6自由度腕部位姿增量，通过"接触-松开-重抓"循环逐步逼近目标位姿。 - **跨模态训练**：结合MuJoCo物理仿真数据与少量真实数据，采用交叉组合训练策略（随机配对当前/目标触觉图像）实现任意目标位姿泛化。 3. **实验成果**： - 仿真预训练+真实微调策略在10次迭代内达到最佳精度（位置误差1.1mm，姿态误差0.016rad），显著优于纯仿真训练。 - 成功实现16种任意位姿间的调整（如俯仰/滚转/Y-Z轴组合），对同类未见过物体展现泛化能力。 - 长时程实验验证了移动物体的持续追踪能力。 **创新点**：首个纯触觉驱动的任意在位物体位姿调整框架，突破视觉限制，为高精度操作任务（如装配）提供新思路。</details> |
| 2025-09-30 | VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning | http://arxiv.org/abs/2509.25718v1 | <details><summary>展开</summary>本文针对VLA（视觉-语言-动作）模型后训练中强化学习面临的稀疏奖励和不稳定训练问题，提出了一种结合动作分块PPO和自行为克隆的方法。核心要点如下： 1. **方法创新**： - 引入**动作分块PPO**，将连续动作聚合成块，提高策略时间一致性和反馈密度。 - 设计**自行为克隆**机制，通过动态更新的演示缓冲区收集高质量轨迹，构建辅助监督损失。 - 在线调整PPO与行为克隆损失的权重，实现从模仿学习到强化学习的平稳过渡。 2. **实验结果**： - 在MetaWorld基准测试中，仅用10个演示初始化，成功率达**0.93**，平均步数仅**42.17**。 - 显著优于监督微调（100个演示成功率0.89、步数65.65），并发现更高效的任务策略。 3. **意义**：验证了强化学习用于VLA后训练的可行性，为下游应用提供实用方案。</details> |
| 2025-09-30 | dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought | http://arxiv.org/abs/2509.25681v1 | <details><summary>展开</summary>论文提出 **dVLA（Diffusion Vision-Language-Action Model）**，一种基于扩散模型的视觉-语言-动作统一框架，结合多模态思维链（CoT）实现机器人感知、推理与控制的协同优化。核心要点如下： --- ### **1. 核心创新** - **统一扩散训练目标**： 将视觉感知、语言推理和动作生成整合到单一扩散模型中，通过离散化策略（图像用MAGViT-v2、文本用LLaDA分词器、动作用FAST编码器）统一处理多模态数据，避免传统VLA模型的梯度冲突问题。 - **多模态思维链（CoT）**： 要求模型同步生成 **子目标图像（视觉CoT）** 和 **文本推理步骤**，再预测动作序列。例如，模型先生成“抓取蓝色玩具车”的文本指令及对应场景图像，再输出具体动作，增强跨模态一致性。 - **推理加速策略**： 采用 **前缀注意力掩码**（分块处理输入）和 **KV缓存**（减少重复计算），实现推理速度 **2倍提升**（从1.5 Hz到3 Hz），性能损失可忽略。 --- ### **2. 实验结果** - **仿真任务（LIBERO基准）**： 平均成功率 **96.4%**，超越所有基线（如GR00T-N1的93.9%、π₀的94.2%），尤其在长视野任务（Libero-Long）达92.2%。 - **真实机器人任务（Franka机械臂）**： - 多物体分拣（Bin Picking）成功率 **70%**，杯具悬挂（Hang Cups）达 **70%**，平均成功率 **65%**，显著优于Diffusion Policy（35%）等。 - **视觉CoT可预测失败场景**：如生成“机械臂卡在箱子边缘”的图像，与实际执行错误一致，验证模型对物理规律的隐式学习。 --- ### **3. 关键优势** - **端到端联合优化**：扩散框架统一理解与生成，解决传统VLA中规划与执行割裂的问题。 - **强泛化能力**：对未见指令和物体场景适应性强，依赖多模态CoT的推理机制。 - **实用部署**：加速策略使模型适用于实时控制场景（如分拣任务）。 --- ### **结论** dVLA通过扩散模型与多模态CoT的结合，为机器人提供了一种高性能、可解释的通用控制框架，在仿真和现实任务中均验证了其有效性。</details> |
| 2025-09-29 | World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training | http://arxiv.org/abs/2509.24948v1 | <details><summary>展开</summary>这篇论文提出了World-Env框架，用于解决视觉-语言-动作（VLA）模型在数据稀缺场景下的性能瓶颈问题。核心创新点如下： 1. **虚拟环境构建** - 提出**视频基世界模拟器**（Video-Based World Simulator），通过动作条件化的未来帧预测生成时序一致的视觉观测，替代真实物理交互。 - 利用离线演示数据训练并固定模拟器，支持在虚拟环境中安全探索策略，避免高风险场景的实际交互成本。 2. **语义感知反馈机制** - 设计**VLM引导即时反射器**（VLM-guided Instant Reflector），基于预训练视觉语言模型（如LLaVA）评估预测视觉轨迹与语言指令的语义对齐度。 - 提供连续奖励信号（$R(\mathbf{o}_{1:t},\mathbf{g}) \in [0,1]$）并动态检测任务完成状态（当$R > \eta$时终止动作），解决传统方法因延迟终止导致的冗余操作问题（如图6所示）。 3. **高效强化学习优化** - 结合Laplace分布建模动作不确定性，通过尺度头（Scale Head）实现自适应探索。 - 采用LOOP（Leave-One-Out PPO）算法，利用RLOO基线估计优势函数，在仅需**每条任务5个演示样本**的条件下显著提升泛化能力。 **实验结果** 在LIBERO机器人操作基准测试中： - 相比OpenVLA-OFT基线模型，平均成功率从74.85%提升至79.6%（表1）。 - 消融实验验证：数据增强使世界模拟器渲染精度提升（图4），奖励头部设计使任务终止检测准确率提高（表2）。 - 在长时序任务（LIBERO-Long）上取得57.8%成功率，优于所有对比方法。 该方法为高风险或非重置场景下的VLA模型训练提供了安全、低成本的解决方案，显著缓解了数据依赖性和执行效率问题。</details> |
| 2025-09-29 | IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks | http://arxiv.org/abs/2509.24768v1 | <details><summary>展开</summary>本文提出IA-VLA框架，用于增强视觉-语言-动作模型（VLA）处理语义复杂任务的能力。核心要点如下： 1. **问题背景** VLA模型因需实时输出机器人动作而受限于模型规模，导致语言理解能力不足，难以处理涉及视觉重复物体（如无法通过外观区分的相同类别物体）的复杂语义指令（如通过相对位置指定目标物体）。 2. **解决方案** - **IA-VLA框架**：利用大型视觉语言模型（VLM）预处理输入图像，生成语义信息并增强VLA输入。 - 通过Semantic-SAM分割图像并添加数字标签，由VLM识别任务相关物体 - 使用半透明掩码高亮相关物体，并通过SAM2实现掩码跨帧跟踪 - **输入增强**：在训练和推理阶段均对图像进行增强，仅在首帧使用计算密集型VLM（如GPT-4），后续帧采用轻量跟踪。 3. **实验验证** - **任务场景**：在包含视觉重复物体的三类任务中测试（积木抓取、玩具蔬菜入锅、抽屉开启），定义三级指令复杂度（已知指令/概念组合/概念外推）。 - **结果对比**： - 基线模型（OpenVLA）在复杂指令（Category 3）上成功率显著下降（积木任务仅19%） - IA-VLA显著提升性能，尤其在概念外推任务中（积木任务提升至76%） - 指令简化版本（"操作高亮物体"）在抽屉等复杂场景表现更优 4. **关键贡献** - 首个针对VLA重复物体任务的系统框架 - 构建包含1290次测试的新数据集 - 实验证明增强输入使VLA成功率最高提升57%（积木任务） 5. **局限与发现** - 主要失败模式为动作执行错误（70%），而非VLM或掩码生成问题 - 在抽屉等需精确操作的任务中，指令简化更有效，因原始指令可能干扰增强信号 该框架通过解耦语义理解与动作生成，为VLA处理复杂任务提供新思路，代码与数据集已开源。</details> |
| 2025-09-29 | Emergent World Representations in OpenVLA | http://arxiv.org/abs/2509.24559v1 | <details><summary>展开</summary>该论文研究了在OpenVLA（一种基于策略强化学习训练的视觉语言动作模型）中是否隐式涌现出世界模型。主要发现如下： 1. **核心方法**：通过嵌入算术（embedding arithmetic）分析状态表示，提出了一种探测模型内部激活的方法。具体为：测量连续环境状态嵌入的差异（状态转移向量 \(\Delta e_{t \to t+K}\))，并利用线性/非线性探针测试该向量能否从中间层激活中恢复。 2. **关键证据**： - 探针在预测状态转移向量时显著超越基线（\(R^2 > 0\) 且 \(p < 0.0001\)），表明OpenVLA编码了内部世界模型（而非探针自身学习转移函数）。 - 世界模型主要位于网络中间层（如第15层），且随训练进程逐渐涌现。 - 预训练计算规模扩大能增强状态转移的潜在知识。 3. **模型特性**： - 线性探针性能优于MLP，支持线性表示假说（LHR）。 - 长期状态转移的Allan方差分析显示，预测性能随步长\(K\)增加而提升，因信号噪声比改善。 4. **应用方向**：提出可解释性流程——利用稀疏自编码器（SAE）分解预测的状态转移向量，生成可验证的特征表示（如图1所示），用于可信决策（如手术机器人中的行动否决）。 **局限**：均值池化阻碍了特征定位；探针训练数据规模较小（400个片段）。未来需扩展非池化方法及SAE训练。 总之，研究证明策略训练的VLAs可通过规模效应隐式学习世界模型，模糊了无模型与基于模型强化学习的界限。</details> |
| 2025-09-29 | PhysiAgent: An Embodied Agent Framework in Physical World | http://arxiv.org/abs/2509.24524v1 | <details><summary>展开</summary>论文提出PhysiAgent框架，旨在解决Vision-Language-Action（VLA）模型在物理世界泛化能力不足的问题。核心要点如下： 1. **问题背景**：VLA模型在真实任务中泛化受限，现有方法整合Vision-Language Models（VLMs）和VLAs时采用刚性序列结构，导致协作低效和落地困难。 2. **框架设计**： - 引入自适应架构，包含**规划器（Planner）**、**监控器（Monitor）**、**反思器（Reflector）**、**记忆（Memory）**和**工具箱（Toolbox）**。 - 通过实时反馈循环：Monitor跟踪VLA执行进度，Reflector验证并生成视觉约束纠正错误，Memory存储历史以优化决策，Toolbox提供轻量级感知/控制工具。 - 支持VLMs基于VLA能力动态调整指令，最大化利用VLA潜能。 3. **优势**： - 无需训练，模块化集成任意VLMs/VLAs。 - 实现自反思、自适应协作及框架动态演化。 4. **实验验证**： - 在真实机器人桌面操作任务（如物体抓取和多步烹饪）中测试。 - 结果：任务成功率显著提升，超越基线方法（如静态分层框架），并展现人类干预级别的鲁棒性。 5. **贡献**：首次将代理范式引入物理世界，为具身智能提供可部署解决方案。</details> |
| 2025-09-28 | Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models | http://arxiv.org/abs/2509.23655v1 | <details><summary>展开</summary>这篇论文提出了一种名为Oat-VLA（Object-Agent-centric Tokenization for Vision Language Action Models）的高效视觉语言动作模型训练方法，核心要点如下： 1. **问题背景** 现有视觉语言动作模型（VLA）在处理图像时需将图像分割成大量图像块（如256个），生成大量视觉token，导致训练计算成本高昂，成为模型扩展的主要瓶颈。 2. **核心创新** - **对象中心token**：通过FT-Dinosaur无监督对象分割模型提取7个语义对象掩码，对同一对象的图像块特征进行平均池化，压缩为7个token。 - **智能体中心token**：基于轻量级夹爪检测器定位机械臂末端，选取其周围3×3网格（9个图像块）作为高精度操作的关键视觉token。 两者结合仅需**16个视觉token**（比OpenVLA减少93.75%），显著降低计算量。 3. **关键优势** - **训练效率**：在LIBERO基准测试中，收敛速度比OpenVLA快2倍以上（图1），训练吞吐量提升104%（每秒处理320样本 vs 157样本）。 - **性能提升**： - LIBERO任务平均成功率78.6%（OpenVLA为76.5%），长时序任务（LIBERO-10）优势更显著（55.9% vs 53.7%） - 真实世界抓取任务成功率59%（OpenVLA为41%），尤其在物体精确定位场景表现更鲁棒（图4）。 - **模块化设计**：兼容现有VLA预训练模型（如OpenVLA），支持参数高效微调（LoRA）。 4. **消融实验验证** - 去除智能体中心token会显著降低性能（平均成功率从77.1%→61.3%） - 平均池化优于注意力池化，且3×3夹爪网格足以覆盖操作需求。 5. **应用意义** 为资源受限的研究实验室提供可行的大规模VLA训练方案，同时为具身智能的视觉表示优化提供新思路。 **关键词**：视觉语言动作模型、对象中心表示、机器人操作、高效训练。</details> |
| 2025-09-27 | Leave No Observation Behind: Real-time Correction for VLA Action Chunks | http://arxiv.org/abs/2509.23224v1 | <details><summary>展开</summary>本文提出了一种异步动作块校正方法（A2C2），用于解决视觉-语言-动作（VLA）模型在实时控制中的延迟问题。核心要点如下： 1. **问题背景** VLA模型通过预测动作块（action chunks）提高效率，但长时域执行和推理延迟导致动作基于过时观测，降低动态任务中的响应性和成功率。 2. **方法创新** - **轻量级校正模块**：在预训练的VLA模型上增加实时校正头（A2C2），每控制步长执行一次。 - **输入特征**：融合最新观测、基础动作、时间位置编码（指示动作块内位置）和VLA的隐层特征。 - **残差校正**：输出残差动作（Δa）叠加到基础动作，生成最终执行动作：\(a^{\text{exec}} = a^{\text{base}} + \Delta a\)。 3. **技术优势** - **即插即用**：无需重新训练基础VLA模型。 - **低开销**：校正头参数量小（Kinetix任务0.31M，LIBERO任务32M），推理时间短于单控制步长。 - **异步兼容**：可与Real Time Chunking（RTC）等方法协同使用。 4. **实验结果** - **Kinetix基准**（12个动态任务）：延迟\(d=4\)时，成功率较原始执行提升35%，较RTC提升23%；长时域（\(H=7\)）零延迟下较原始执行提升12%。 - **LIBERO Spatial基准**（10个操作任务）：执行时域\(e=40\)且延迟\(d=10\)时，成功率较原始执行提升19.8%（64.4%→84.2%）；长时域（\(e=50\)）零延迟下提升9.4%（72.2%→81.6%）。 5. **核心贡献** - 首次形式化VLA动作块推理的延迟问题。 - 提出通用校正框架，在保留基础模型能力的同时恢复闭环响应性。 - 实验验证了在动态环境和长时域任务中的鲁棒性提升。 该方法为大规模VLA模型在实时控制中的部署提供了有效的低开销解决方案。</details> |
| 2025-09-27 | Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges | http://arxiv.org/abs/2509.23121v1 | <details><summary>展开</summary>本文评估了视觉-语言-动作（VLA）模型在工业应用中的性能与挑战，核心结论如下： 1. **工业性能差距** - 微调后最佳模型（Pi0）在简单抓取任务中成功率仅约60%，高精度放置任务的位置误差达2.2cm、角度误差12.4°。 - 视觉遮挡（头部/腕部相机）使任务成功率显著下降（尤其物体多样性场景），相机抖动影响相对较小但误差仍超工业标准。 2. **关键瓶颈分析** - **数据匮乏**：缺乏工业级大规模对齐数据，限制模型在复杂指令（如"轴压入轴承"）和遮挡场景的泛化能力。 - **架构局限**： - 动作空间设计（相对末端控制 vs 绝对关节控制）影响精度与实时性。 - 推理延迟（普遍<10Hz）导致运动抖动，影响稳定性。 - **计算需求**：大模型资源消耗高，动作分块（chunking）虽提效但牺牲控制精度。 3. **改进方向** - 增强感知鲁棒性（如融合触觉/音频等多模态数据）。 - 开发轻量架构以提升实时性，优化动作空间设计（推荐绝对关节控制）。 - 构建工业专用数据集，解决跨机器人平台泛化问题。 **结论**：当前VLA模型在非结构化工业环境中尚未满足高精度、实时响应和部署要求，需针对性优化架构与数据策略。</details> |
| 2025-09-26 | MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training | http://arxiv.org/abs/2509.22199v2 | <details><summary>展开</summary>论文提出MimicDreamer框架，旨在解决人类演示视频与机器人执行的域差异问题，实现低成本人类视频到机器人可训练数据的转换。核心贡献包括： 1. **视觉对齐（H2R Aligner）** 基于视频扩散模型，将人类操作视频的运动轨迹迁移至机器人域，生成高保真机器人演示视频，保留背景语义和操作几何一致性。 2. **视角稳定（EgoStabilizer）** 通过单应性变换将第一人称视频规范化为任务级参考视角，并结合视频修复技术消除扭曲和遮挡，减少相机抖动（平均稳定性提升21.9%）。 3. **动作对齐** 设计统一的人-机器人动作空间： - 人体坐标系归一化后刚性映射到机器人基座标系 - 倾斜角对齐（忽略手腕滚动差异） - 带约束的逆运动学求解器生成低抖动关节轨迹，满足运动学限制 4. **实验效果** - 在6类操作任务（如拾取袋子、堆叠碗杯等）上，仅使用合成数据训练的VLA策略实现真实机器人少样本执行 - 相比纯机器人数据基线，平均成功率提升14.7% - 数据扩展实验显示性能随人类数据量增加持续提升（如插入网球任务成功率从25%→45%） 该方法显著降低机器人数据收集成本，为大规模视觉-语言-动作（VLA）模型训练提供高效解决方案。</details> |
| 2025-09-26 | VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search | http://arxiv.org/abs/2509.22643v1 | <details><summary>展开</summary>这篇论文提出了一种名为VLA-Reasoner的插件框架，旨在增强视觉-语言-动作模型（VLA）在长时序任务中的推理能力。核心要点如下： 1. **问题背景** 现有VLA模型仅能预测短视的下一步动作，在长时序任务中因累积偏差导致性能下降。VLA-Reasoner通过在线蒙特卡洛树搜索（MCTS）赋予模型预见未来状态的能力。 2. **核心方法** - **在线MCTS框架**：在测试时采样动作轨迹，通过世界模型模拟未来状态，评估潜在结果并搜索最优动作。 - **高效采样机制**：基于核密度估计（KDE）的置信度采样，减少冗余的VLA查询。 - **视觉奖励塑形**：利用离线训练的视觉奖励网络评估中间状态，提供长期反馈。 3. **技术优势** - 插件式设计兼容现有VLA模型（如OpenVLA、SpatialVLA）。 - 通过动作注入公式 \(a_t = \alpha \cdot a_t^{\text{VLA}} + (1-\alpha) \cdot a_t^{\text{Reasoner}}\) 平衡原始预测与优化动作。 4. **实验结果** - **仿真测试**（LIBERO/SimplerEnv）：任务成功率平均提升5-10%，长时序任务提升最显著（如LIBERO-Long任务提升6.4%）。 - **实物部署**：在5项机器人操作任务中，OpenVLA成功率提升19%（22%→41%），\(\pi_0\)-FAST提升10%（64%→74%）。 - **消融实验**：验证了KDE采样和视觉奖励塑形的有效性，最优注入强度 \(\alpha=0.6\)。 5. **贡献价值** 为机器人操作提供可扩展的测试时优化路径，显著缓解VLA的增量偏差问题，推动具身智能发展。 > 论文源码与模型：https://github.com/VLA-Reasoner/VLA-Reasoner</details> |
| 2025-09-26 | UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation | http://arxiv.org/abs/2509.22441v1 | <details><summary>展开</summary>论文提出**UnderwaterVLA**框架，用于解决水下自主导航的三大挑战：通信受限、感知退化及训练数据稀缺。核心创新如下： 1. **双脑架构** - **云端大脑**：负责高层任务分解（如将“导航至珊瑚礁”拆解为子目标序列），仅在浮出水面时间歇运行。 - **本地小脑**：执行实时闭环控制（如避障动作），通过JSON结构化指令实现低带宽通信，支持动态任务终止（如提前结束“保持安全距离”步骤）。 2. **零数据训练的VLA模型** - 首次将视觉-语言-动作（VLA）模型引入水下机器人，结合**思维链（CoT）提示**增强决策可解释性（例如输出“检测左侧障碍物，右转”的推理过程）。 - 利用预训练多模态基础模型，最小化水下特定数据依赖。 3. **流体动力学补偿的MPC** - 设计**模型预测控制器（MPC）**，实时估算流体阻力（如公式 \( F_{\text{drag}} = D_v v \|v\| \))，无需任务训练即可补偿湍流干扰。 - 通过**运动模式切换**（平移/旋转）和严格时间约束（1秒周期），确保动作精确性。 ### 实验结果 - **性能提升**：在浑浊水域中，任务完成率比基线方法提高19%–27%（如隧道穿越任务达27%）。 - **鲁棒性**：双脑架构在低光照/高浊度下保持功能，本地小脑可自主调整子任务执行策略；传统端到端模型在同等条件下失效。 ### 意义 该框架为水下机器人提供可扩展、低成本的自主导航方案，通过物理模型与语言智能的结合，推动水下 embodied AI 的实际应用。 --- **关键词**：水下机器人、大语言模型、视觉-语言-动作、自主导航、模型预测控制。</details> |
| 2025-09-26 | EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer | http://arxiv.org/abs/2509.22407v1 | <details><summary>展开</summary>论文提出**EMMA框架**，旨在解决视觉-语言-动作（VLA）模型在真实机器人操作中泛化能力不足的问题。核心要点如下： 1. **问题背景**：VLA模型依赖多样化训练数据实现鲁棒泛化，但收集真实世界机器人操作数据成本高昂，且仿真数据存在视觉差距。 2. **解决方案**： - **DreamTransfer**：基于扩散Transformer的生成模型，通过文本控制编辑前景、背景和光照，生成多视图一致且几何合理的机器人操作视频，支持真实到真实（real-to-real）和仿真到真实（sim-to-real）的迁移。 - **AdaMix**：硬样本感知训练策略，动态调整训练样本权重，优先优化感知或运动学上的挑战性样本，提升策略鲁棒性。 3. **实验结果**： - **视频生成质量**：DreamTransfer在多项指标（多视图一致性提升42%、深度一致性提升24%）上优于基线方法。 - **真实机器人任务**：在零样本视觉域任务（如折叠布料、清理桌面、投掷瓶子）中，EMMA相比纯真实数据训练实现200%以上性能增益，AdaMix进一步带来13%的提升。 4. **贡献**：EMMA通过生成数据引擎和自适应训练，显著提升VLA模型在跨物体类别和视觉域的泛化能力，为机器人操作提供高效解决方案。</details> |
| 2025-09-26 | MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training | http://arxiv.org/abs/2509.22199v1 | <details><summary>展开</summary>本文提出MimicDreamer框架，旨在解决人类演示视频与机器人执行的领域差异问题，以支持可扩展的视觉-语言-动作（VLA）模型训练。核心贡献包括： 1. **视觉对齐**：提出H2R Aligner视频扩散模型，通过几何相机先验将人类操作视频转换为高保真度的机器人手臂视频，保留动作语义和背景一致性。 2. **视角稳定**：设计EgoStabilizer模块，通过单应性变换将第一人称视频稳定到任务级参考视角，并利用修复技术消除扭曲和遮挡，减少相机抖动（平均稳定性提升21.9%）。 3. **动作对齐**：构建统一动作空间，通过归一化人体坐标系、倾斜角对齐和带约束的逆运动学求解器（IK Resolver），将人类手腕轨迹映射为低抖动、可行的机器人关节指令。 实验结果： - 仅使用合成数据训练的VLA策略在真实机器人上实现小样本执行，在六个操作任务（如拾取包、堆叠碗等）中平均成功率提升14.7%。 - 增加人类演示数据规模可线性提升性能，50%人类+50%机器人数据混合时成功率最高（如堆叠碗任务提升25%）。 - 消融实验验证了视角稳定和视觉对齐对任务完成率的关键作用。 该方法显著降低了机器人数据收集成本，为利用大规模人类视频进行VLA训练提供了系统解决方案。</details> |
| 2025-09-26 | Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting | http://arxiv.org/abs/2509.22195v1 | <details><summary>展开</summary>这篇论文提出了一种名为VLM2VLA的新方法，旨在解决将视觉语言模型（VLM）微调为视觉语言动作模型（VLA）时的灾难性遗忘问题。核心要点如下： 1. **问题分析** - 传统VLA微调方法（如架构修改或离散动作标记化）会导致VLM丧失预训练获得的多模态理解和推理能力（如VQA性能下降85%以上），影响泛化能力。 2. **核心创新：动作语言化表示** - 将机器人底层动作（如机械臂移动）转化为自然语言描述（例如“向前移动4.2厘米”），使微调数据与VLM的预训练分布对齐。 - 通过Gemini模型自动标注机器人轨迹数据，构建分层结构：高层子任务（如“抓取胡萝卜”）→ 中层运动规划（如“向左下方移动”）→ 底层动作序列。 3. **高效训练方法** - 仅需低秩适应（LoRA）微调，无需修改VLM架构或进行大规模联合训练（co-training），显著降低计算成本。 - 保留Gemma-3等VLM的原始权重，避免灾难性遗忘（VQA任务性能保留85%以上）。 4. **实验验证** - **多模态理解**：在MMMU、TextVQA等12个基准测试中超越主流VLAs（如OpenVLA），接近原始VLM性能。 - **机器人操作**：800+次实体实验显示： - 在分布外任务（如多语言指令“西班牙语抓取胡萝卜”）中成功率60%，远超基线（OpenVLA仅10%）。 - 开放世界推理任务（如“拿起皮卡丘上方的物品”）成功率60%，证明保留的语义知识支持零样本泛化。 5. **局限与未来方向** - 推理延迟较高（中位数6.1秒），需优化解码速度。 - 当前仅支持平移控制，未来将扩展至旋转等灵巧操作。 - 探索跨机器人平台的通用动作表示。 **结论**：VLM2VLA通过数据层面的动作语言化表示，解决了VLA微调中的分布失配问题，在保留VLM核心能力的同时实现了高效的机器人策略学习。代码与数据见项目主页：https://vlm2vla.github.io/。</details> |
| 2025-09-26 | Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation | http://arxiv.org/abs/2509.22093v1 | <details><summary>展开</summary>本文提出动作感知动态剪枝（ADP）框架，用于提升视觉-语言-动作（VLA）模型在机器人操作中的计算效率。核心要点如下： ### 核心问题 - VLA模型处理密集视觉token时计算开销大，现有方法忽视**不同操作阶段的视觉冗余差异**：粗粒度阶段（如物体搬迁）冗余高，细粒度阶段（如抓取）需保留更多视觉细节。 ### 创新方法 1. **文本驱动剪枝** - 通过跨模态注意力计算文本指令与视觉token的相关性，保留Top-K相关token（如保留率ρ=30%~70%）。 - 多视图场景下动态分配各视角保留比例（主视角:腕部视角=4:6）。 2. **动作感知动态策略** - 基于机械臂末端运动轨迹窗口（如8步动作块）计算运动幅度δ： - 高运动幅度（粗粒度阶段）→ 启用剪枝 - 低运动幅度（细粒度阶段）→ 禁用剪枝 - 动态决策函数：对比当前δ与历史均值，实现阶段自适应切换。 ### 实验结果 - **效率提升**：在OpenVLA-OFT模型上实现1.35×加速，FLOPs降低25.8%（LIBERO仿真）；真实任务延迟从76.9ms降至51.8ms（1.49×加速）。 - **性能保持**： - LIBERO基准上保持96.3%成功率（vs 基线97.1%） - 真实任务成功率提升至88.3%（vs 基线85.8%） - **消融验证**： - 移除动态策略导致成功率下降2.85% - 浅层（第0层）注意力信号即可有效指导剪枝 ### 意义 ADP为VLA模型提供即插即用的高效推理方案，平衡计算开销与操作精度，适用于实时机器人系统。代码与项目见：https://vla-adp.github.io/ > 总结：ADP通过**文本驱动的token选择**与**动作触发的动态剪枝**，在保持操作成功率的同时显著降低VLA计算负载，推动机器人高效多模态控制发展。</details> |
| 2025-09-26 | Developing Vision-Language-Action Model from Egocentric Videos | http://arxiv.org/abs/2509.21986v1 | <details><summary>展开</summary>本文提出了一种从自我中心视频（egocentric videos）开发视觉-语言-动作模型（VLA）的方法，以解决传统VLA预训练依赖高成本专家遥操作数据的问题。核心要点如下： 1. **问题与动机**： - 自我中心视频捕捉人类操作物体的运动线索，提供可扩展的数据源，但现有方法需依赖辅助注释（如手部姿势），限制了实用性。 - 本文旨在直接从原始自我中心视频中训练VLA，无需额外标签。 2. **方法**： - 使用 **EgoScaler框架** 从四个大规模自我中心视频数据集（Ego4D、Ego-Exo4D、HD-EPIC、Nymeria）中提取6DoF物体操作轨迹。 - 自动精炼噪声或不完整轨迹，构建新的大规模VLA预训练数据集（45,157个样本）。 3. **实验与结果**： - 在模拟（SIMPLER）和真实机器人（ALOHA）环境中，基于π0架构评估： - 预训练后任务成功率比从头训练提高20%以上。 - 性能与主流真实机器人数据集（如BC-Z、BridgeData V2）相当。 - 结合自我中心视频与真实机器人数据（如BridgeData V2），性能进一步提升。 4. **结论**： - 自我中心视频是VLA预训练的有效资源，具有可扩展性，能缓解数据稀缺问题。</details> |
| 2025-09-25 | RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models | http://arxiv.org/abs/2509.21243v1 | <details><summary>展开</summary>论文提出**RetoVLA**，一种轻量级视觉-语言-动作（VLA）模型，通过**重用Vision Transformer中的Register Tokens增强空间推理能力**，解决传统轻量化方法导致的性能下降问题。核心要点如下： ### 1. **核心创新** - **Register Tokens再利用**：传统方法将Register Tokens视为“净化器”（吸收ViT中的异常信息后丢弃），本文将其重新定义为**空间上下文提供者**。这些tokens包含场景的全局空间信息（如物体3D布局、工作空间结构）。 - **空间上下文注入**：设计新模块，将Register Tokens作为Key-Value对直接注入动作专家（Action Expert）的注意力层，使模型同时利用高层语义特征与全局空间信息生成动作。 - **自适应门控**：引入可学习标量门控（经Sigmoid激活），动态调节Register Tokens的影响强度，避免全局信息干扰需局部精度的任务。 ### 2. **架构设计** - **双流信息流**： （1）VLM主干提取语义特征（仅用前一半层以降低计算量）； （2）Register Tokens通过空间聚合器生成场景相关表示，注入动作专家的交叉注意力层。 - **训练目标**：采用条件流匹配（Conditional Flow Matching），将噪声动作序列逐步优化至真实动作，结合视觉/语言上下文。 ### 3. **关键优势** - **效率与性能平衡**：在轻量化VLM（如SmolVLA）基础上，仅增加少量计算开销即显著提升空间推理能力，避免传统方法的信息损失。 - **任务适应性**：门控机制使模型自动区分需全局理解的任务（如长期规划）与需局部精度的任务。 ### 4. **实验结果** - **真实机器人（7-DOF机械臂）**：在7类复杂操作任务（如堆叠积木、关闭抽屉）上，平均成功率从**50.3%提升至67.4%**（+17.1%），其中长视距任务（如摆放多米诺骨牌）提升最高达**28%**。 - **LIBERO基准测试**：在需工作记忆（+11.5%）和3D空间推理（+9.0%）的任务上表现突出，但局部精确操作任务略有下降。 - **仿真实验**：在自定义Unity/MuJoCo环境中，平均成功率提升**12%**（62.8% → 74.8%），验证方法的泛化性。 ### 5. **结论与意义** - **重新定义信息流**：证明被丢弃的Register Tokens是提升机器人空间智能的关键资源，为轻量化高性能VLA模型提供新范式。 - **开源承诺**：将发布代码、模型权重及机器人硬件规格以促进复现。 > **局限**：局部精细操作任务存在性能权衡，未来需优化门控机制或融合策略。 > **视频演示**：[https://youtu.be/2CseBR-snZg](https://youtu.be/2CseBR-snZg)</details> |
| 2025-09-25 | Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning | http://arxiv.org/abs/2509.21126v1 | <details><summary>展开</summary>本文提出**VARL框架**（VLM作为在线强化学习的动作建议器），通过利用视觉语言模型（VLM）生成动作建议提升强化学习（RL）的样本效率。核心要点如下： ### 1. **问题背景** - 在线RL在复杂任务中样本效率低，需大量交互学习最优策略。 - 现有方法局限：VLM策略在低级控制表现不足；微调需专家演示；VLM奖励设计依赖模型偏好准确性且计算开销大。 ### 2. **VARL框架设计** - **核心创新**：VLM作为**动作建议器**（非奖励设计器），提供启发式动作引导RL探索。 - **双组件结构**： - **VLM动作生成器**：基于当前状态和任务描述生成候选动作（图2），存储至启发式缓冲区。 - **策略塑形**：将VLM建议动作融入策略更新（公式8），通过门控函数（公式4,7）避免过度拟合次优动作，并在训练后期移除启发式信号（固定步数 \(N_s\)）。 - **优势**：保证RL收敛性，避免奖励函数偏差；显著减少VLM查询量（仅需3次，表1）。 ### 3. **关键优势** - **提升样本效率**：在稀疏/稠密奖励任务中，收敛速度均优于SAC及SAC+专家数据（图4），尤其适用于事件驱动型奖励。 - **低计算开销**：相比奖励塑形方法（如RL-VLM-F、ERL-VLM），VLM调用量减少99%以上（表1）。 - **任务通用性**：在10种仿真/现实任务验证（图3），支持离散/连续动作空间及视觉/状态输入（Meta-World、AI2-THOR、真实机械臂任务）。 - **现实可行性**：直接在真实环境（RM-65B机械臂）实现在线RL，无需专家演示或环境建模（目标抓取任务仅需3000次交互）。 ### 4. **实验验证** - **效率对比**：VARL在7种环境中样本效率显著优于基线（图4）。 - **现实部署**：成功学习真实世界推方块（30k–50k次交互）和位置到达任务（3k次交互）。 - **超参数鲁棒性**：对动作权重系数 \(\lambda\) 和启发式移除步数 \(N_s\) 不敏感（图6）。 ### 5. **结论** VARL通过VLM动作建议增强探索多样性，解决在线RL样本效率问题，同时降低计算成本，为无演示真实环境RL提供可行路径。未来将扩展至更复杂任务及视频生成技术融合。</details> |
| 2025-09-25 | AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation | http://arxiv.org/abs/2509.21006v1 | <details><summary>展开</summary>论文提出**AnywhereVLA框架**，用于未知室内环境中的语言驱动移动操作机器人系统。核心要点如下： 1. **模块化架构**： - 将自然语言指令解析为任务图，驱动**SLAM（LiDAR+相机）**、**3D语义建图（带置信度）** 和**主动环境探索（AEE）** 模块。 - 目标检测后，**路径规划模块**选择可达的抓取位姿。 - **轻量化操作头（SmolVLA）** 基于微调的抓取数据集生成操作动作。 2. **关键技术**： - **语义建图**：融合LiDAR点云与目标检测，通过插值解决稀疏性问题（图3→图4），结合多视角数据估计目标置信度（公式2）。 - **主动探索**：基于前沿检测算法（算法2），语言指令驱动机器人搜索未知区域直至定位目标物体。 - **嵌入式部署**：在**Jetson Orin NX**（感知与VLA）和**Intel NUC**（SLAM与控制）上实现全系统实时运行（≥10Hz）。 3. **实验验证**： - 在动态多房间实验室测试50次任务，整体成功率**46%**（VLA操作模块微调后成功率85%）。 - 表II显示各模块成功率：探索（75%）、导航（90%）、目标检测（85%）、VLA操作（80%）。 - 5米半径内平均任务时间**133秒**（图7），10米半径内小于10分钟。 4. **优势与局限**： - 结合**经典导航的鲁棒性**与**VLA的语言泛化能力**，开源代码/模型/数据集。 - 局限：无法处理复杂空间约束（如“桌上的瓶子”）；探索失败率25%（狭窄空间）。 --- **结论**：AnywhereVLA通过模块化设计平衡了导航可靠性与语言操作的灵活性，为大规模室内移动操作提供了可行方案。未来需提升空间语义理解能力。</details> |
| 2025-09-25 | ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation | http://arxiv.org/abs/2509.20841v1 | <details><summary>展开</summary>论文提出了一种名为“ImaginationPolicy”的端到端机器人操作策略，旨在解决现有方法（如视觉-语言-动作模型）泛化性不足、精度低和可靠性差的问题。核心创新是“Chain of Moving Oriented Keypoints (CoMOK)”动作表示方法，其要点如下： 1. **动作表示（CoMOK）**： - 将操作任务建模为任务特定的定向关键点（affordance）及其运动链。 - Affordance定义为物体局部语义部分（如杯柄），表示为6-DoF（位姿）关键点，支持泛化到不同形状、大小的物体，并实现亚厘米级精度。 - 通用性强：可简化为标准末端执行器姿态动作，统一处理抓取、放置等多样化任务。 2. **关键优势**： - **泛化性**：通过关键点忽略无关几何细节，适应新物体和场景。 - **多任务处理**：支持多阶段操作（如抓取-倾倒-放置）、多模态行为（如多个可行抓取位姿）和可变形物体（如绳索）。 - **轨迹扩展**：可输出动作序列（如切割轨迹），结合传统或学习型运动生成器执行。 3. **实现方法**： - 神经网络基于分数匹配（扩散模型变体），输入场景观测（点云/RGBD）和任务描述（语言提示），输出多模态动作候选。 - 任务规划网络（微调Groma VLM）分解全局任务为子任务；动作预测网络生成关键点位姿和轨迹。 4. **实验结果**： - 在抓取检测、稳定放置、电缆插入和挂杯任务中验证，成功率达85-98%。 - 模拟和真实硬件（Rokae SR5机器人）实验显示，对物体形状变化（如20种杯子）和任务复杂度具有强鲁棒性。 该方法为端到端操作提供了高精度、可靠且可扩展的解决方案，代码和演示见[项目页面](https://sites.google.com/view/imaginationpolicy)。</details> |
| 2025-09-24 | Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving | http://arxiv.org/abs/2509.20109v1 | <details><summary>展开</summary>论文提出了一种名为ReflectDrive的新型端到端自动驾驶框架，其核心创新点包括： 1. **离散扩散模型的应用** - 首次将离散扩散模型引入自动驾驶轨迹生成，通过将二维驾驶空间离散化为动作码本，将连续轨迹表示为离散token序列。 - 利用预训练扩散语言模型（DLMs）进行轨迹规划微调，支持并行解码和双向特征融合。 2. **安全反射机制** - **目标条件生成**：首先生成多模态候选轨迹（通过目标点采样和NMS过滤），再基于全局评分器选择最优轨迹。 - **安全引导再生**：对不安全轨迹点进行局部搜索，找到可行解作为"安全锚点"，通过扩散修复（inpainting）重构轨迹，无需梯度计算。 - 评分函数体系：包含全局安全评分（$S_{\text{global}}$）、局部安全检测（$S_{\text{safe}}$）和轨迹质量评估（$S_{\text{local}}$），确保硬性安全约束（如碰撞避免、可行驶区域合规）。 3. **实验验证** - 在NAVSIM真实驾驶基准测试中，ReflectDrive的PDMS分数达91.1（仅相机输入），显著优于基线模型（+6.3分）。 - 安全指标突出：可行驶区域合规率（DAC）达99.3%，接近人类水平（100%）；配合精确环境信息时，综合性能达人类驾驶的94.7%。 该方法通过离散token空间的高效搜索与修复机制，解决了模仿学习模型难以保障物理安全的问题，为可验证的端到端自动驾驶提供了新范式。</details> |
| 2025-09-24 | FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models | http://arxiv.org/abs/2509.19870v1 | <details><summary>展开</summary>这篇论文提出了一种针对视觉-语言-动作（VLA）模型的新型攻击方法FreezeVLA，其核心要点如下： 1. **问题发现** VLA模型存在关键安全漏洞：对抗性图像可使模型持续"冻结"，忽略后续指令，导致机器人完全停止行动（称为"动作冻结攻击"）。这种静默失效比错误动作更危险，可能造成关键任务中断且难以被检测。 2. **攻击方法** - **FreezeVLA框架**：通过双层优化生成跨提示的对抗图像 - **内层最大化**：通过梯度分析和同义词替换生成抗冻结的"困难提示"（如将"scale"替换为"weighing machine"） - **外层最小化**：针对困难提示集优化对抗图像，强制模型输出<freeze>令牌 - 采用GPT生成多样化初始提示（如"将金属罐放入柳条环"），增强攻击泛化性 3. **实验结果** - 在3个VLA模型（SpatialVLA/OpenVLA/π₀）和4个机器人基准（LIBERO）上验证 - 平均攻击成功率76.2%，显著超越基线（最高提升78.4%） - 单张对抗图像可跨不同语言指令诱导瘫痪（如OpenVLA达95.4%成功率） - 对抗扰动预算ε=4/255时即有效，ε=16/255时成功率超95% 4. **贡献与意义** - 首次系统化研究VLA模型的动作冻结漏洞 - 提出首个针对该漏洞的跨提示攻击框架 - 揭示VLA模型在安全关键场景中的重大风险 - 代码开源：https://github.com/xinwong/FreezeVLA 论文强调该攻击在实验室环境验证，反对实际滥用，呼吁开发防御机制保障具身智能安全。</details> |
| 2025-09-24 | Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training | http://arxiv.org/abs/2509.19752v1 | <details><summary>展开</summary>论文提出一种基于扩散强化学习的数据生成方法，用于替代人类演示数据训练视觉-语言-动作（VLA）模型。核心要点如下： 1. **问题背景** VLA模型依赖大规模人类演示数据，但人工收集成本高且数据存在高方差和多模态问题。传统强化学习（RL）在长视野稀疏奖励任务中生成的数据质量低。 2. **方法创新** - **扩散策略优化**：采用扩散模型作为策略表示，通过迭代去噪过程生成平滑、低方差动作序列，结合PPO算法进行在线微调。 - **两阶段训练**： - **阶段1**：多模态行为克隆（BC）预热，利用少量人类数据初始化策略。 - **阶段2**：扩散PPO优化，通过稳定化技术（DDIM采样器、余弦退火学习率、多样化经验回放）提升训练效率。 3. **关键优势** - **高质量数据**：扩散RL生成的数据比人类演示更高效（轨迹长度缩短23%）、更平滑（运动jerk值降低38%）、更一致（动作方差最低）。 - **性能提升**：在LIBERO基准（130个任务）上，仅用扩散RL数据训练的VLA模型达到**81.9%**平均成功率，超越人类数据（+5.3%）和高斯RL数据（+12.6%）。 - **泛化能力**：人类数据与扩散RL数据混合训练，在OOD任务上成功率达**5.2%**（单一数据源最高仅2.06%）。 4. **实验验证** - 架构设计：ResNet+U-Net优于ViT+MLP，防止策略崩溃。 - 采样效率：DDIM（5步）比DDPM快10倍且性能相当。 - 数据多样性：并行环境收集数据避免模式坍塌。 5. **结论** 扩散RL可生成高质量、低方差数据，显著提升VLA模型性能，为减少人类数据依赖提供有效解决方案。</details> |
| 2025-09-23 | Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action | http://arxiv.org/abs/2509.19571v1 | <details><summary>展开</summary>论文提出了一种名为Agentic Scene Policies (ASP) 的机器人策略框架，旨在解决开放词汇自然语言指令的执行问题。其核心创新点包括： 1. **框架设计** ASP 通过结构化场景表示（ObjectMap）统一处理空间、语义和功能特性（affordances）。该框架包含： - **对象地图**：整合物体几何、语义特征（如CLIP嵌入）和交互功能（如把手部位及对应技能） - **LLM智能体**：将用户指令分解为工具调用序列 - **工具系统**：提供物体检索、空间推理和交互功能（如抓取/推动特定功能部位） 2. **关键技术突破** - **零样本交互**：利用基础模型（Gemini）检测物体功能部位（如按钮/把手），并映射到预定义技能（如`tip_push`按压） - **移动扩展**：支持房间级任务，结合导航（基于功能朝向规划路径）与操作，通过多视角建图实现跨场景物体重定位 - **模块化验证**：在15项桌面操作任务中，ASP 成功率显著超过端到端视觉语言动作模型（VLA），尤其在需精细操作的任务（如拔除图钉/开抽屉）优势达40%以上 3. **实验验证** - 对比VLA基线（如π₀-FAST）显示：ASP在复杂指令理解（空间关系/功能交互）和零样本泛化性上更具优势 - 消融实验证明功能检测是关键：移除该模块后任务成功率平均下降21% - 移动场景测试验证了框架可扩展性（如"将鸡蛋放入锅"等需导航+操作的任务） 4. **局限与展望** 当前技能库限于基础操作（抓取/推拉），未来需结合学习策略处理长时序任务（如折叠衣物）。场景表示的动态更新也是重要改进方向。 ASP 的核心价值在于通过显式场景表示弥合语言指令与机器人动作的鸿沟，为开放场景任务提供可解释且可扩展的解决方案。</details> |
| 2025-09-23 | OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation | http://arxiv.org/abs/2509.19480v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-23 | Pure Vision Language Action (VLA) Models: A Comprehensive Survey | http://arxiv.org/abs/2509.19012v2 | <details><summary>展开</summary>这篇综述论文《Pure Vision Language Action (VLA) Models: A Comprehensive Survey》系统性地总结了视觉语言动作模型的研究进展，要点如下： 1. **研究背景与目标** - VLA模型将传统机器人控制范式转变为通用化智能体，整合视觉感知、语言理解和动作生成，实现复杂环境中的主动决策。 - 论文提出首个针对纯VLA方法的分类框架，填补现有研究空白。 2. **核心分类框架** VLA方法分为四大范式： - **自回归模型**：基于Transformer架构，通过序列建模生成动作（如Gato、RT系列）。优势在于跨任务泛化能力，但存在误差累积和计算延迟问题。 - **扩散模型**：将决策建模为条件生成过程（如Diffusion Policy），支持多模态目标条件，但需解决推理效率瓶颈。 - **强化微调模型**：结合RL优化策略，提升策略稳定性和样本效率。 - **混合架构与专业模型**：针对特定场景（如手术机器人、无人机）设计多模态融合方案。 3. **关键资源体系** - **数据集**：Open X-Embodiment（跨21机构527技能）、BridgeData等标准化数据集推动模型训练。 - **仿真平台**：THOR、Habitat、CARLA等提供可扩展的虚拟测试环境。 - **硬件平台**：涵盖机械臂、四足机器人、人形机器人等多形态载体。 4. **挑战与未来方向** - **核心挑战**：机器人数据稀缺性、架构异构性、实时推理成本、人机伪交互、评估体系局限。 - **突破方向**：世界模型与跨模态统一、因果推理突破、虚实数据融合、可信生态系统构建。 5. **贡献总结** - 建立首个VLA方法分类学，系统分析300+研究成果。 - 阐明各范式的技术演进路径（如自回归模型从基础token化→大规模实机训练→语义推理集成）。 - 指出开放性难题：长时程任务稳定性、安全部署机制、多模态对齐鲁棒性。 该综述为通用具身智能的发展提供了方法论基础和技术路线图，强调需在数据生成、计算效率和安全验证三方面取得突破。</details> |
| 2025-09-23 | Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations | http://arxiv.org/abs/2509.18953v1 | <details><summary>展开</summary>该论文提出Eva-VLA框架，首次系统评估视觉-语言-动作（VLA）模型在真实物理变化下的鲁棒性。核心贡献如下： 1. **问题定义**：针对VLA模型在真实部署中面临的物理变化脆弱性问题（如物体位姿变化、光照干扰、对抗性补丁），提出首个统一评估框架。 2. **方法创新**： - 将物理变化分解为三类连续参数化问题： * 物体3D变换（旋转角α/β/γ） * 光照变化（位置x/y、半径σ、强度I） * 对抗补丁位置（Δx/Δy） - 采用黑盒优化算法CMA-ES搜索最坏场景，避免真实数据采集成本 3. **实验结果**： - 在OpenVLA等先进模型上验证，所有物理变化均导致>60%失败率 - 物体3D变换对长时任务造成最高97.8%失败率 - 光照变化和对抗补丁分别导致62.6%和71.2%平均失败率 - 物理实验证实44.6%攻击成功率 4. **揭示问题**：实验暴露VLA模型在实验室环境与真实部署间的严重差距，证明当前模型对物理变化极度敏感，尤其长时任务存在级联失效风险。 该框架为提升VLA模型鲁棒性提供了评估基准和优化路径。</details> |
| 2025-09-23 | Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation | http://arxiv.org/abs/2509.18865v1 | <details><summary>展开</summary>论文提出了一种名为Bi-VLA的新型框架，旨在解决传统双边控制模仿学习的单任务限制问题。Bi-VLA通过融合视觉、语言和机器人状态数据（包括关节角度、速度和扭矩），利用SigLIP处理语言指令和EfficientNet提取视觉特征，并通过FiLM-based融合生成统一表示。该方法使单个模型能适应多个任务，无需任务特定模型或重新训练。在真实机器人实验中，Bi-VLA在语言可区分和视觉可区分任务上均显著提高了任务成功率，验证了其增强的泛化性和适应性。</details> |
| 2025-09-22 | Latent Action Pretraining Through World Modeling | http://arxiv.org/abs/2509.18428v1 | <details><summary>展开</summary>本文提出了一种名为**LAWM（Latent Action Pretraining Through World Modeling）** 的模型无关框架，旨在通过世界建模从无标签视频数据中学习潜在动作表示，提升模仿学习模型的性能。核心要点如下： ### 1. 核心方法 - **两阶段流程**： - **预训练阶段**：模型输入图像和语言指令，预测潜在动作表示（如动作块 \(z_{t:t+n}\))。这些表示与世界模型（如DreamerV3的RSSM架构）联合优化，通过预测未来视频帧实现自监督学习（损失函数含MSE重建和KL正则项）。 - **微调阶段**：丢弃世界模型，仅用带标签的演示数据微调模仿学习模型（如BAKU或Diffusion Policy），将观测映射到真实动作空间。 - **模型无关性**：兼容不同模仿学习模型和世界模型架构。 - **数据来源**：支持机器人录像和人类日常操作视频（如Something-Something v2数据集），无需动作标签。 ### 2. 关键优势 - **减少标注依赖**：自监督预训练避免了对昂贵人工标注动作数据的需求。 - **高效迁移**：预训练获得的动作先验提升了下游任务微调效率（如LIBERO基准微调仅需2小时）。 - **跨任务泛化**：在LIBERO任务套件（Spatial/Object/Goal/Long）和真实机器人任务中，小模型（如7M参数的BAKU）性能超越： - 监督预训练基线（如OpenVLA、π₀）。 - 同类无监督方法（如3B参数的villa-X），平均成功率97.25% vs. 90.10%。 - **人类视频有效性**：仅用人类视频预训练的模型微调后性能接近/超越机器人数据监督预训练（如Diffusion Policy在LIBERO-90达92.6%）。 ### 3. 实验验证 - **数据集**：预训练使用BridgeData v2（机器人）和Something-Something v2（人类视频）；微调使用LIBERO基准和真实机械臂任务（5种操作任务）。 - **结果**： - 真实任务成功率94% vs. 无预训练84%。 - 典型相关分析（CCA）显示潜在动作与真实动作强相关（第一典型相关系数达0.915–0.959），证明学习表示的有效性。 - **效率**：单A100 GPU完成预训练（约30小时）和微调（最快20分钟）。 ### 4. 局限与展望 - 计算开销较大（需联合训练世界模型）； - 世界模型依赖像素重建，可能忽略任务相关特征； - 未来可探索多模态输入（如触觉信息）和更大规模视频预训练。 ### 结论 LAWM通过自监督世界建模学习通用动作表示，显著降低对标注数据的依赖，提升小模型在跨任务、跨 embodiment 场景下的性能与实用性，为机器人基础模型提供新思路。</details> |
| 2025-09-22 | PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies | http://arxiv.org/abs/2509.18282v1 | <details><summary>展开</summary>本文提出PEEK框架，通过视觉语言模型（VLM）提升机器人操作策略的零样本泛化能力。核心要点如下： 1. **问题与创新点** - 传统模仿学习策略需同时学习"关注位置（where）"、"执行动作（what）"和"动作执行方式（how）"，导致泛化能力受限。 - PEEK将高层语义推理（where/what）卸载给VLM，策略仅需专注底层动作执行（how）。 2. **技术方案** - **统一表示**：微调VLM预测点基中间表示： - **路径点（Path）**：机械臂末端运动轨迹（what） - **掩码点（Mask）**：任务相关区域位置（where） - **策略接口**：将路径和掩码直接覆盖到观测图像上，形成策略无关的通用输入。 - **自动标注**：提出多步跟踪流水线，从20+数据集（9种机器人平台）生成148k轨迹的标注数据。 3. **关键结果** - **仿真到现实**：仅仿真训练的3D策略（3DDA）在真实场景成功率提升**41.4倍**。 - **多策略兼容**： - 大型VLA模型（π₀）成功率提升**2-3.5倍** - 小型策略（ACT）提升**3.4倍** - **鲁棒性**：在535次真实评估中，PEEK在视觉干扰和语义变化任务上均优于基线（ARRO、HAMSTER等）。 4. **贡献价值** - VLM吸收语义和视觉复杂性，为策略提供最小化提示（where/what/how）。 - 开源项目页：https://peek-robot.github.io 总结：PEEK通过解耦高层推理与底层控制，显著提升策略泛化能力，为开放场景机器人部署提供新范式。</details> |
| 2025-09-22 | Prepare Before You Act: Learning From Humans to Rearrange Initial States | http://arxiv.org/abs/2509.18043v1 | <details><summary>展开</summary>这篇论文提出了一种名为ReSET的算法，旨在通过模仿人类行为改善机器人模仿学习（IL）在环境状态超出训练分布时的鲁棒性。核心要点如下： ### 1. **问题背景** - IL策略在目标物体位置异常或被遮挡时容易失败，直接收集大量数据解决该问题效率低下。 - 人类在面临非常规初始状态时，会先**重构环境**（如移开障碍物、调整物体位姿）简化任务，再进行操作。 ### 2. **ReSET算法核心思想** - **分阶段策略**： - **简化策略（Reduction Policy）**：将复杂初始状态（如遮挡场景）转化为**锚状态（Anchor States）**（如移除障碍物后的状态），使环境分布更集中（降低方差）。 - **任务策略（Task Policy）**：在锚状态上执行原任务（如抓取杯子）。 - **理论支撑**：锚状态满足 $\text{tr}(\Sigma_a) < \text{tr}(\Sigma_0)$（状态协方差矩阵迹更小），可降低泛化误差上界（公式6），提升数据效率。 ### 3. **关键技术组件** - **评分网络**： 基于人类视频训练，评估当前状态是否适合直接执行任务策略（低分表示适合执行）。 - **点流生成网络**： 从人类视频中提取物体运动轨迹（点流 $\mathcal{T}$），预测如何重构环境（如移开障碍物的路径）。 - **简化策略**： 将点流映射为机器人可执行的动作基元（如抓取、推动、旋转）。 ### 4. **实验验证** - **任务场景**：抓取遮挡物体、旋转工具、多任务操作等（图4）。 - **结果**： - 在分布外状态上，ReSET成功率（平均85%）显著高于扩散策略（65%）、Dynamics-DP等基线（图5）。 - 仅需 **20分钟人类视频 + 20分钟机器人通用操作数据**，达到扩散策略 **70次专家演示** 的效果，数据效率更高。 ### 5. **贡献总结** - **理论**：证明分阶段策略（环境重构→任务执行）可降低泛化误差。 - **方法**：结合人类视频（动作无关）与机器人数据（任务无关），实现环境重构决策、运动预测与动作生成。 - **效果**：在少量数据下提升任务鲁棒性，适用于长视野、多任务场景。 > 论文链接：[https://reset2025paper.github.io](https://reset2025paper.github.io)</details> |
| 2025-09-20 | ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering | http://arxiv.org/abs/2509.16680v1 | <details><summary>展开</summary>论文提出**ProtoVQA框架**，用于可解释的细粒度视觉问答（VQA），核心要点如下： 1. **研究动机** - VQA在医疗诊断、自动驾驶等安全关键领域应用时，需模型提供**可验证的解释**，而现有方法（如注意力可视化）难以忠实反映决策过程。 - 原型学习在视觉任务中可增强可解释性，但多模态场景下存在**视觉-语言语义鸿沟**、**几何变化建模不足**等问题。 2. **核心框架ProtoVQA** - **问题感知原型**：将问题令牌重塑为 \(m \times k \times D\) 的3D张量（\(m\) 个原型，每个含 \(k\) 个子块），作为语义锚点连接答案与图像区域。 - **空间约束贪婪匹配**：通过迭代选择相似度最高的图像块-子块对（公式2-3），并施加空间连续性约束（半径 \(r=3\)），确保证据区域语义相关且空间连贯。 - **双模态答案处理**： - **Type 1（视觉定位）**：直接投影坐标至特征空间。 - **Type 2（描述性QA）**：冻结的权重共享投影器处理文本答案，避免过拟合。 - **共享骨干网络**：统一原型框架同时支持VQA和定位任务。 3. **评估指标VLAS** - 提出**视觉-语言对齐分数（VLAS）**，衡量模型关注区域与真实证据的重叠度（IoU>0.5），优于传统像素级指标，更符合人类解释评估需求（公式5）。 4. **实验结果** - **数据集**：Visual7W（32.8万QA对，47K图像）。 - **性能**： - 准确率70.23%（表1），与ViT骨干的基线模型（如Bi-CMA 70.53%）相当。 - **VLAS@1达0.4103**，较最佳基线提升66.4%；VLAS@3提升119.6%（表2），证明解释忠实性显著领先。 - **定性可视化**：匹配的局部区域（图2）与真实答案框高度对齐，展示细粒度推理能力（如物体部件、空间关系）。 5. **意义与局限** - 首次将原型学习扩展至多模态VQA，提供**透明推理路径**。 - 局限：未探索生成式VQA；领域迁移（如医疗）需进一步适配。 **总结**：ProtoVQA通过问题驱动原型匹配和空间约束机制，在保持竞争性准确率的同时，显著提升解释可信度，推动可信VQA系统发展。</details> |
| 2025-09-19 | Randomized Smoothing Meets Vision-Language Models | http://arxiv.org/abs/2509.16088v1 | <details><summary>展开</summary>这篇论文提出将随机平滑（Randomized Smoothing, RS）技术应用于视觉语言模型（VLMs），以解决其鲁棒性认证问题。核心要点如下： 1. **问题背景** - RS 传统上用于分类任务（输出离散标签），但 VLMs 生成文本序列，无法直接应用。 - 直接扩展 RS 到生成式模型需处理无限输出空间，计算不可行。 2. **解决方案：引入预言机分类层** 通过三类任务将 VLM 输出转化为分类问题： - **内容审核**：用预言机（如 LLM）判断输出是否有害（二分类）。 - **离散动作 VLAs**：如机器人指令，输出映射到有限动作集。 - **语义等价聚类**：用预言机将语义相似的输出归为同一类（算法 2）。 3. **理论保障** - 假设预言机错误率 \(\epsilon < 0.5\)，推导出概率下界公式（定理 4.1）： \[ \bar{p}_{y} = \frac{\bar{q}_{y} - \epsilon}{1 - 2\epsilon} \] - 证明即使预言机不完美，仍可计算鲁棒性半径（定理 4.2）。 4. **计算效率优化** - 提出改进的缩放定律（Scaling Laws），减少采样复杂度： - 认证所需样本量降低 2–3 个数量级（从 \(10^4\)–\(10^5\) 级降至百级）。 - 平均认证半径下降近似公式（定理 5.3）： \[ r_{\sigma}(\alpha,n) \approx 1 - 1.64 \frac{z_{\alpha}}{\sqrt{n}} \] - 放宽均匀分布假设，提升理论普适性。 5. **实验验证** - 在 SotA VLMs 上验证框架有效性，成功防御越狱式对抗攻击（如 Qi 2024 的攻击）。 **总结**：论文通过预言机将 VLM 输出转化为分类问题，使 RS 适用于生成模型；理论证明在有限错误率下保持认证可靠性；大幅降低计算成本，为 VLMs/VLAs 的安全部署提供基础。</details> |
| 2025-09-19 | CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine | http://arxiv.org/abs/2509.15968v1 | <details><summary>展开</summary>论文提出**CoReVLA框架**，旨在提升自动驾驶在**长尾场景**（罕见但高危场景）中的性能。核心要点如下： 1. **问题背景** - 现有自动驾驶系统在长尾场景（如突发事故、极端天气）中表现不佳，导致安全事故。 - 视觉语言动作模型（VLA）虽具推理能力，但受限于**高质量长尾数据稀缺**和**稀疏数据下训练效率低**。 2. **解决方案：双阶段框架** - **阶段1（Collect）**： - 混合开源驾驶QA数据集（LingoQA、BDD、HAD）对Qwen2.5-VL-7B模型进行监督微调（SFT），构建基础场景理解能力。 - 在CAVE仿真平台进行人机协同测试，收集人类接管失败的场景数据（含视觉输入、接管行为及注意力）。 - **阶段2（Refine）**： - 利用接管数据，通过**直接偏好优化（DPO）** 对齐人类行为偏好：将模型失败行为（次优）与人类接管行为（最优）对比，避免手动设计奖励函数导致的奖励黑客问题。 3. **实验结果** - **开环QA评估**：在LingoQA/BDD/HAD数据集上，BLEU和ROUGE分数显著优于基线（如Llava、Impromptu）。 - **闭环驾驶测试**（Bench2Drive基准）： - 驾驶评分（DS）达 **72.18**，成功率（SR）达 **50%**，较SOTA方法提升 **7.96 DS** 和 **15% SR**。 - 案例验证：模型能通过接管数据持续优化行为，避免相似场景的重复失败（如雨天避让故障车辆、行人突发横穿）。 4. **贡献与资源** - 提出首个结合**CAVE平台接管数据收集**与**DPO行为优化**的端到端框架。 - 开源代码、数据集及场景配置：[https://github.com/FanGShiYuu/CoReVLA](https://github.com/FanGShiYuu/CoReVLA)。 > 总结：CoReVLA通过**数据收集+行为精炼**双阶段设计，高效利用人类接管数据优化长尾场景决策，显著提升自动驾驶安全性与泛化能力。</details> |
| 2025-09-19 | A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning | http://arxiv.org/abs/2509.15937v1 | <details><summary>展开</summary>论文提出VLAC（Vision-Language-Action-Critic）模型，用于解决机器人真实世界强化学习（RL）中稀疏奖励和低效探索的问题。要点如下： 1. **模型核心**：基于InternVL构建，通过大规模异构数据（包括视觉-语言数据集和机器人/人类轨迹数据）训练，统一批评者（critic）和策略（policy）。输入一对观察图像和语言目标，输出密集任务进度增量（作为奖励）和完成信号，无需任务特定奖励工程。 2. **关键技术**： - 支持一次性上下文转移（in-context transfer）到未见任务和环境。 - 通过负面样本增强鲁棒性，拒绝无关提示并检测任务回归或停滞。 - 使用提示控制交替生成奖励和行动令牌。 3. **框架集成**：部署于异步真实世界RL循环，结合分级人类在环干预（离线演示回放、返回探索、人类引导探索），加速探索并稳定学习。 4. **实验结果**：在四个真实操作任务（如拾取碗、桌面清扫）中，VLAC在200个交互episode内将成功率从约30%提升至90%；加入人类干预后，样本效率提高50%，最终成功率高达100%。</details> |
| 2025-09-18 | RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation | http://arxiv.org/abs/2509.15212v1 | <details><summary>展开</summary>论文提出RynnVLA-001模型，通过人类演示视频提升机器人操作能力。核心方法包括两阶段预训练： 1. **自我中心视频生成预训练**：在1200万自我中心操作视频上训练图像到视频模型，基于初始帧和语言指令预测未来帧，学习操作动态。 2. **人类中心轨迹感知建模**：扩展模型，联合预测未来帧和关键点轨迹，桥接视觉预测与动作表示。 引入**ActionVAE**（变分自编码器）压缩动作序列为紧凑嵌入，简化输出空间并确保动作连贯性。 在相同机器人数据集上微调后，模型优于最先进基线（如GR00T N1.5和Pi0），验证了预训练策略的有效性。</details> |
| 2025-09-18 | Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue | http://arxiv.org/abs/2509.15061v2 | <details><summary>展开</summary>论文提出 **Ask-to-Clarify 框架**，旨在解决具身智能体在执行模糊指令时的问题，使其从被动执行者转变为主动协作的伙伴。核心要点如下： ### 1. **问题与动机** - **现状局限**：当前视觉-语言-动作模型（VLAs）仅能被动执行指令，遇到模糊指令（如“给我那个杯子”但桌上有多个杯子）易失败。 - **目标**：构建能通过**多轮对话主动澄清指令**，再生成**底层动作**完成任务的协作型智能体。 ### 2. **框架设计** - **双组件结构**： - **协作组件**（VLM）：基于视觉语言模型，负责解析模糊指令并通过对话提问澄清。 - **动作组件**（扩散模型）：生成底层机器人动作（如关节控制），实现端到端执行。 - **连接模块**：将VLM的输出转化为扩散模型的**条件参数**，通过调整视觉观察和指令生成更可靠的动作条件。 ### 3. **训练策略（两阶段知识隔离）** 1. **阶段一**： - 使用**模糊指令对话数据**微调VLM，使其学会提问澄清（如输出 `<AMBG>` 标记提问）。 - 冻结视觉编码器，仅训练语言模型部分，保留预训练知识。 2. **阶段二**： - **冻结VLM**，防止遗忘对话能力。 - 集成扩散模型，使用**专家演示数据**训练动作组件和连接模块，学习生成底层动作。 ### 4. **推理机制** - **信号检测器**：根据VLM输出的信号标记（如 `<AMBG>`、`<ACT>`）切换模式： - 检测到模糊指令 → 提问并等待用户回复。 - 指令明确后 → 生成动作执行任务或拒绝无效指令（`<REJ>`）。 ### 5. **实验结果** - **任务**：在8个真实场景任务（如放置水果、倒水、堆叠积木）中评估。 - **性能**：显著优于SOTA VLAs（如 π₀、π₀-FAST、OpenVLA-OFT），平均成功率提升明显（如堆叠任务达90%）。 - **优势**： - **鲁棒性**：在低光照、干扰物存在时表现更稳健。 - **关键设计验证**：连接模块和两阶段训练缺一不可（消融实验证实）。 ### 6. **贡献总结** - **新框架**：首次结合多轮对话澄清与底层动作生成，解决指令模糊性。 - **训练策略**：两阶段知识隔离兼顾对话能力和动作学习。 - **实验验证**：真实场景验证有效性，为构建协作型具身智能体提供新路径。 ### 未来方向 优化连接模块（如引入对比学习或跨注意力机制），提升视觉-语言条件融合效果。</details> |
| 2025-09-18 | Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale | http://arxiv.org/abs/2509.14932v1 | <details><summary>展开</summary>论文提出了一种名为“机器人控制堆栈（RCS）”的精益生态系统，旨在解决大规模机器人学习（尤其是视觉-语言-动作模型VLA）中传统软件框架的瓶颈问题。核心要点如下： 1. **问题背景**： - 传统机器人软件（如ROS）在机器学习工作流中成为瓶颈，难以支持大规模数据收集和模型训练。 - 现有仿真工具缺乏对真实机器人控制的无缝支持，限制了从仿真到真实环境（sim-to-real）的转换。 2. **RCS架构设计**： - **模块化分层结构**：提供统一接口连接仿真（MuJoCo）和真实机器人，通过环境包装器（wrapper）支持Python/C++扩展，实现动作和观测空间的灵活调整。 - **关键特性**： - 硬件抽象层支持多种传感器（如RGB-D相机）和执行器（如夹爪）。 - 同步/异步执行模式，确保数据时序一致性。 - 集成机器人工具包（如Pinocchio用于运动学、OMPL用于路径规划）。 - **轻量依赖**：最小化开销，同时支持分布式训练和并行仿真。 3. **实验验证**： - **多机器人部署**：在FR3、xArm7、UR5e、SO101四种真实机器人及仿真环境中测试，覆盖不同任务（如Pick-Cuboid）。 - **VLA模型评估**： - 对Octo、OpenVLA和π₀模型进行跨平台性能测试，证明π₀在真实环境中的最佳适应性（FR3成功率最高）。 - 混合仿真与真实数据训练可显著提升策略性能（如π₀在FR3上的成功率提高至100%）。 - **强化学习支持**：PPO算法在仿真中高效训练，3小时内达成100%任务成功率。 4. **贡献总结**： - 提出RCS生态系统，弥合仿真与真实机器人间的鸿沟。 - 实验证明仿真数据可增强真实世界策略，为模型选择和超参优化提供依据。 RCS已开源，支持大规模机器人学习研究，未来将扩展至双臂和移动操作任务。</details> |
| 2025-09-18 | CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human | http://arxiv.org/abs/2509.14889v1 | <details><summary>展开</summary>本文提出**CollabVLA**，一种自反思的视觉-语言-动作模型框架，通过人机协作解决现有VLA的三大局限： 1. **核心创新**： - **混合专家设计**：整合VLM推理与扩散动作生成，避免辅助生成模型的高延迟 - **两阶段训练**： *动作基础训练*（多模态目标预训练 + 潜在动作表示） *反思调优*（注入失败场景数据，实现不确定性识别与求助机制） - **动态协作机制**：MoE门控自动切换控制/反思模式，遇不确定性时主动请求人类轻量提示（文本/视觉线索） 2. **关键优势**： - **效率提升**：比生成式代理减少2倍标准化时间、4倍反思次数 - **性能提升**：在仿真与实物实验中成功率显著高于OpenVLA、RoboDreamer等基线（如Table II抓取任务成功率58.5% vs 28%） - **可解释性**：生成自然语言反思报告，揭示决策依据 3. **实验验证**： - **多模态理解保留**：在MMMU等基准接近原始VLM水平（Table I） - **长视野任务优化**：人类提示使复杂任务成功率提升40%以上（如洗碗机摆放任务） - **低延迟协作**：通过动作缓存和反射嵌入条件化，维持实时控制（<50ms） **结论**：CollabVLA首次将VLA从封闭控制器转化为具备推理、行动与人机协作能力的助手，为具身智能提供新范式。</details> |
| 2025-09-18 | RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI | http://arxiv.org/abs/2509.14687v1 | <details><summary>展开</summary>### 论文要点总结 论文提出 **RealMirror**，一个开源的视觉-语言-动作（VLA）平台，旨在解决具身AI的核心挑战：数据获取成本高、缺乏标准化基准、仿真与现实（Sim2Real）差距大。主要贡献如下： 1. **高效端到端系统**： - 构建低成本数据收集（基于远程操作）、模型训练（支持ACT、Diffusion Policy、SmolVLA等模型）和推理系统，无需真实机器人即可进行VLA研究。 - 优化通信框架，降低延迟（114ms），提升数据采集效率（单任务轨迹平均7.83秒）。 2. **标准化VLA基准**： - 针对类人机器人设计，涵盖5个任务场景（如厨房清理、空气炸锅操作、装配线分拣），包含1,200+仿真轨迹。 - 评估核心技能（抓取放置、双臂协作、精密控制等），提供自动化评估工具和热力图分析，支持公平模型比较（实验显示SmolVLA平均成功率79.75%，优于其他模型）。 3. **零样本Sim2Real迁移**： - 结合生成模型和3D高斯泼溅（3DGS）重建高保真环境与机器人模型，缩小视觉差距。 - 通过坐标对齐和相机校准，实现模型在仿真数据训练后直接部署真实机器人（如ZHIYUAN A2），无需微调（基础任务成功率92.86%，复杂任务71.43%）。 **结论**：RealMirror统一数据收集、训练、评估和Sim2Real迁移，显著加速类人机器人VLA模型的开发。</details> |
| 2025-09-18 | VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation | http://arxiv.org/abs/2509.18183v1 | <details><summary>展开</summary>本文针对视觉-语言-动作（VLA）模型在视角变化时泛化能力差的问题，提出了一种轻量级视角自适应融合框架VLA-LPAF。其核心创新点包括： 1. **问题背景**：VLA模型依赖训练时的固定摄像头视角（如全局和手腕局部摄像头），但实际部署时视角变化导致视觉特征差异，显著降低任务成功率。 2. **解决方案**： - 设计轻量级MLP融合模块，在潜在空间对齐多视角2D图像特征，无需3D数据或额外硬件。 - 采用三阶段训练策略：单视角动作微调、多视角特征对齐微调、联合优化动作与对齐损失。 3. **有效性验证**： - 基于RoboFlamingo实例化为RoboFlamingo-LPAF，在CALVIN、LIBERO和自定义CabinEnv数据集上平均提升任务成功率（分别约8%、15%和30%）。 - 真实世界实验证实其视角自适应能力，成功处理未见视角的任务。 4. **优势**：仅需2D图像，高效桥接视角差异，提升机器人操作的灵活性和泛化性。消融实验支持了余弦相似度损失和渐进式数据引入等设计选择。</details> |
| 2025-09-17 | CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping | http://arxiv.org/abs/2509.14143v1 | <details><summary>展开</summary>本文提出 **CLAW框架**（一种视觉-语言-动作模型），用于解决机器人抓取任务中**重量感知控制**的难题。以下是核心要点： ### 1. **问题背景** - 现有视觉-语言-动作（VLA）模型（如π₀）虽能生成连续动作，但**难以精确满足数值阈值约束**（如按重量停止抓取），因其端到端设计缺乏显式状态监控机制。 ### 2. **CLAW框架设计** - **核心创新**：**解耦状态监控与动作生成**。 - **轻量级监控模块**：微调CLIP模型，实时读取秤盘图像并生成二元提示（`继续`/`停止`），基于任务指定的重量阈值。 - **动作生成模块**：微调后的π₀模型接收CLIP提示+多视角图像，输出连续机器人动作。 - **优势**：结合符号化重量推理与高频运动控制，支持实时响应。 ### 3. **关键技术** - **CLIP微调**：构建含2000张秤盘图像的训练集，通过数值比较任务学习生成精准提示。 - **π₀微调**：收集50组演示数据，人工标注抓取/停止阶段的提示标签，训练π₀响应提示指令。 ### 4. **实验验证** - **任务场景**： - 单物体抓取（糖果/大蒜，目标重量20g/30g/40g）。 - 混合物体抓取（指定物体+重量）。 - **结果**： - CLAW在**动作执行成功率**和**重量停止准确率**均达**100%**（表I）。 - 显著优于基线（原始π₀动作成功率≤35%，停止率≈0%；微调π₀虽动作成功率达100%，但停止准确率仅0-35%）。 - **鲁棒性**：即使重量突发波动（图5），CLAW仍能即时调整动作。 ### 5. **贡献总结** - 提出首个重量感知VLA框架，通过显式监控提升控制精度。 - 验证CLIP作为轻量提示生成器的可行性。 - 实现π₀在提示监督下的高效动作生成。 - 在单/多物体任务中均表现鲁棒。 ### 6. **未来方向** - 增强秤盘定位鲁棒性（无需手动裁剪）。 - 扩展非数值停止条件（如时间、视觉形态）。 - 融合多模态输入（声音、触觉）。 > **关键价值**：CLAW为机器人执行需精确量化约束的任务（如按重配料、药品分装）提供了新范式。</details> |
| 2025-09-17 | SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model | http://arxiv.org/abs/2509.14138v1 | <details><summary>展开</summary>SeqVLA 提出一种基于完成感知的视觉-语言-动作（VLA）模型，用于解决长时程操作任务中的顺序执行问题。核心要点如下： 1. **问题与动机**：现有 VLA 模型（如 π₀）擅长连续控制，但缺乏子任务完成判断能力，导致长时程任务（如多步骤打包）中错误累积和顺序混乱。 2. **模型设计**： - **双头架构**：在 π₀ 基础上增加轻量级**完成检测头**（Completion Detection Head），共享动作专家层的特征。 - **功能**：主头输出机器人动作；检测头实时预测当前子任务完成概率（二元分类），触发自主任务切换。 - **损失函数**：总损失 = 动作损失（π₀ 的流匹配损失） + λ · 完成检测损失（二元交叉熵，λ=0.1）。 3. **微调策略**： - 对比四种策略：**联合 vs. 顺序微调** × **主干网络全微调 vs. 冻结**。 - **最优方案**：联合微调且不冻结主干（SeqVLA-J），其完成检测置信度最高（熵值最低），KS统计量显著（p<0.001）。 4. **实验验证**： - **任务**：沙拉打包（7个子任务）和糖果打包（4个子任务）。 - **结果**： - SeqVLA 显著超越基线 π₀，消除顺序错误（如重复执行或跳步）。 - SeqVLA-J 在子任务成功率（图6）和长时程整体成功率（图10）上均最优。 5. **贡献**： - 首次在 VLA 模型中集成完成检测机制，实现自主子任务切换。 - 明确最佳微调策略（联合+非冻结主干），提升长时程任务鲁棒性。 - 在真实机器人任务中验证有效性，为顺序操作提供新解决方案。 **未来方向**：探索层次化任务分解、人机协作场景及动态任务序列适应能力。</details> |
| 2025-09-17 | GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model | http://arxiv.org/abs/2509.14117v2 | <details><summary>展开</summary>本文提出GeoAware-VLA模型，旨在解决视觉-语言-动作（VLA）模型在摄像机视角变化时泛化能力差的问题。核心方法是用预训练的几何视觉模型VGGT作为冻结的特征提取器，替代传统可训练视觉编码器，并添加轻量级可训练投影层适配策略解码器。关键贡献如下： 1. **几何感知设计**： 引入VGGT模型提取隐含几何特征，使策略无需从零学习3D一致性，显著提升视角不变性。 2. **性能提升**： - 在LIBERO基准测试中，新视角下的零样本成功率提升超2倍（平均82.6% vs 基线37.9%-50.2%）。 - 真实机器人实验验证有效性，尤其在未训练视角下成功率显著高于基线（如任务5提升27%）。 3. **技术优势**： - 兼容连续（MLP）和离散（VQ-BeT）动作空间，模型轻量（仅投影层可训练）。 - 消融实验表明均匀选择VGGT中间层特征最有效，优于仅用末层或全层。 4. **实验验证**： - 模拟环境：跨4类任务集（Spatial/Object/Goal/Long）均优于基线（如Long任务新视角成功率47.3% vs 基线3.7%）。 - 真实场景：5项操作任务成功率高，证明几何先验提升策略鲁棒性。 结论指出，几何感知是提升VLA泛化能力的关键，该方法为构建适应视角变化的通用机器人策略提供有效路径。</details> |
| 2025-09-17 | Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach | http://arxiv.org/abs/2509.13774v1 | <details><summary>展开</summary>本文提出了一种人机协作的双执行器VLA模型微调框架，用于解决复杂机器人任务中的性能瓶颈。核心要点如下： 1. **双执行器架构**： - **主执行器**：基于扩散策略生成鲁棒的多任务动作 - **精炼执行器**：在潜在噪声空间进行细粒度调整，接受语言指令引导 2. **"说调"人机交互机制**： - 将物理干预（如SpaceMouse操作）实时转化为语义化语言指令（如"向右移动"） - 构建"说-调"数据集，同时优化主执行器的基线策略和精炼执行器的指令响应能力 3. **多任务学习策略**： - 采用共享执行器+任务特定评论家架构 - 引入自适应Q值加权机制平衡不同任务的学习进度 4. **实验效果**： - 真实机器人多任务测试：101分钟在线微调后在螺栓直立放置/抓取/装配三个任务均达100%成功率 - 长时程任务：12步连续操作中维持50%成功率 - 多机器人扩展：双机器人并行训练实现2倍效率提升 - 模型泛化性：在Octo和SmolVLA等不同VLA骨干网络上均有效 5. **局限**： - 完全遮挡场景下性能受限 - 长序列任务存在误差累积 - 当前仍需人类干预保障安全性 该方法通过语义化人机交互和分层策略优化，显著提升了VLA模型在复杂机器人任务中的适应效率和执行精度。</details> |
| 2025-09-17 | AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving | http://arxiv.org/abs/2509.13769v1 | <details><summary>展开</summary>论文提出**AdaThinkDrive框架**，用于解决自动驾驶中Chain-of-Thought（CoT）推理在简单场景下计算冗余的问题。核心创新点包括： 1. **双模态推理机制** - 受人类"快/慢思考"启发，设计**直接预测（无CoT）**和**显式推理（含CoT）**双输出模式。 - 预训练阶段融合驾驶QA数据与轨迹数据，获取世界知识与常识。 - 监督微调（SFT）阶段构建混合数据集，使模型区分场景复杂度。 2. **自适应奖励策略** - 提出**Adaptive Think Reward**，通过强化学习（GRPO算法）动态选择推理模式： - 简单场景（84%概率）直接输出轨迹 - 复杂场景（96%概率）启用CoT推理 - 奖励函数融合轨迹质量（PDMS）、格式合规性、终点精度和场景分类效果。 3. **性能优势** - 在Navsim基准测试达到**90.3 PDMS**，超越最佳视觉基线1.7分。 - 推理效率提升14%（相比全时CoT模型），平衡精度与计算开销。 - 消融实验显示：自适应推理比"永不推理"和"全时推理"基线分别提升2.0和1.4 PDMS。 该框架通过**场景感知的按需推理**，解决了传统CoT在简单场景的过推理问题，为自动驾驶决策提供了高效可靠的解决方案。</details> |
| 2025-09-16 | The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning | http://arxiv.org/abs/2509.12594v2 | <details><summary>展开</summary>本文提出LightVLA，一种用于视觉-语言-动作（VLA）模型的高效可微分token剪枝框架。VLA模型在机器人任务中表现优异，但计算开销大（主要源于大量视觉token的注意力计算），限制了其在资源受限设备上的部署。LightVLA的核心创新包括： 1. **性能驱动的自适应剪枝**：通过动态查询评估视觉token的重要性，结合Gumbel softmax实现可微分token选择，保留关键信息并剪枝冗余token。 2. **无需额外参数**：不引入启发式超参数或可训练参数，与现代推理框架兼容。 3. **效率与性能双提升**：在LIBERO基准测试中，LightVLA减少59.1% FLOPs和38.2%延迟，同时任务成功率提高2.6%，优于现有VLA模型和剪枝方法。 4. **扩展性**：变体LightVLA*（引入可学习查询）也验证了有效性，标志着自适应剪枝在VLA任务的首个应用，推动实时机器人系统向高效实用化发展。</details> |
| 2025-09-15 | TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning | http://arxiv.org/abs/2509.11839v2 | <details><summary>展开</summary>TrajBooster 是一个基于轨迹中心的跨形态框架，旨在利用丰富的轮式人形机器人数据提升双足人形机器人的视觉-语言-动作（VLA）模型性能。其核心创新点包括： 1. **轨迹作为形态无关接口** 以末端执行器的 6D 轨迹为通用媒介，从轮式人形机器人（如 Agibot）提取真实轨迹，经尺度归一化处理适配目标双足机器人（Unitree G1）。 2. **仿真中的分层重定向模型** - **架构**：分解为手臂策略（逆运动学生成关节角度）、管理者策略（输出基座速度/躯干高度指令）和工作者策略（执行指令控制腿部关节）。 - **训练**：采用启发式增强的协调在线 DAgger 算法，利用仿真特权信息生成可行全身动作，高效生成大规模目标机器人兼容动作数据。 3. **两阶段 VLA 微调** - **后预训练**：使用重定向数据构建异构三元组（源视觉/语言 + 目标动作），对齐 VLA 动作分布。 - **后训练**：仅需 10 分钟真实遥操作数据微调，显著降低数据收集成本。 4. **关键成果** - **加速适应**：比仅用真实数据训练快 3 倍以上，实现跨高度操作（如蹲起、抓取）。 - **增强泛化**：在物体位置分布外场景的成功率提升至 80%（基线为 0%）。 - **零样本迁移**：解锁未见于遥操作的任务（如 "传递水杯"）。 - **鲁棒性**：在真实部署中展现协调全身运动与动态平衡能力。 **贡献总结**：首次通过轨迹重定向实现双足人形全身操作的 VLA 模型，减少对昂贵同形态数据的依赖，提升动作空间理解与零样本泛化能力。</details> |
| 2025-09-15 | Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs | http://arxiv.org/abs/2509.11480v1 | <details><summary>展开</summary>本文评估了视觉-语言-动作（VLA）模型在边缘设备（如NVIDIA Jetson AGX Orin）与数据中心GPU（如H100、A100）上的跨平台性能扩展。通过LIBERO基准测试五种代表性VLA模型（包括新提出的VOTE和QwenVLA），测量准确率、延迟、吞吐量和内存占用。主要发现： 1. **架构选择影响性能**：动作标记化（如VOTE的少令牌设计）和主干大小（如QwenVLA的小型主干）显著优化吞吐量和内存占用。 2. **边缘功率约束下的非线性下降**：在低功率模式（如15W），性能下降明显，但高功率边缘配置（如Orin MAX模式）可匹配旧数据中心GPU（如V100）。 3. **高吞吐量可行**：VOTE变体（如VOTE-MLP4）在保持高准确率（VOTE-1T达96.9%）的同时，实现高吞吐量（数据中心达474.78 Hz），无需显著牺牲精度。 这些结果为VLA模型在资源受限场景（如机器人部署）的选择和优化提供了实用指导，挑战了数据中心硬件绝对优越的假设。</details> |
| 2025-09-14 | Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations | http://arxiv.org/abs/2509.11417v2 | <details><summary>展开</summary>论文提出一种框架，通过保留预训练表示增强视觉-语言-动作（VLA）模型的泛化能力，主要包括三个组件： 1. **双编码器设计**：一个冻结的视觉编码器保留预训练特征，另一个可训练编码器适应任务，避免微调导致的表示退化。 2. **基于字符串的动作标记器**：将连续动作转换为字符序列，与预训练语言模型对齐，实现动作的逐步生成和跨模态重用。 3. **协同训练策略**：结合机器人演示数据和强调空间推理的视觉-语言数据集，平衡训练目标，防止过拟合并提升泛化。 实验表明，该方法在仿真和真实机器人任务中显著优于基线，提高对视觉扰动（如背景变化、干扰物）、新指令和环境的鲁棒性，并提升任务成功率。</details> |
| 2025-09-13 | OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft | http://arxiv.org/abs/2509.13347v1 | <details><summary>展开</summary>基于论文内容，核心要点总结如下： 1. **动作表示对比** 论文通过大规模实验（800+ Minecraft任务）系统比较了多种动作抽象空间（原始动作、语言技能、运动基元、接地动作、潜在动作），发现**最优动作空间具有任务依赖性**：接地动作在具身任务中表现最佳（37.1%成功率），运动基元适用于导航任务但无法处理GUI交互，语言技能泛化能力有限。 2. **创新框架：动作链（CoA）** 提出**Chain of Action（CoA）框架**，将高层次抽象动作作为推理中间步骤（类似"思维链"），引导最终可执行动作的生成。关键突破： - **统一架构**：单模型端到端训练，整合高层次规划与低层次控制 - **双推理模式**： - 快速解耦模式（类似传统分层代理，3.75 FPS） - 慢速自回归模式（全链生成，性能提升但仅0.98 FPS） - 实验证明CoA在战斗任务中成功率较基线提升6倍（4.3%→25.6%） 3. **通用代理训练策略** **All-in-One训练策略**使单一代理掌握多种动作空间： - 通过混合数据集训练（接地动作+运动基元+原始动作） - 以原始动作为"通用锚点"实现跨动作空间知识迁移 - OpenHA代理在GUI任务中达32.5%成功率（超专业代理25.8%），具身任务30.1%（超专业模型24.9%） 4. **资源贡献** 开源**OpenHA套件**包含： - 800+人工验证任务的新基准 - 所有预训练模型检查点 - 源代码及标注数据集 - 项目地址：https://github.com/CraftJarvis/OpenHA 结论：CoA框架通过统一动作表示解决了动作抽象选择困境，OpenHA代理在跨任务泛化性上实现新SOTA，为可训练通用代理提供了新范式。</details> |
| 2025-09-11 | SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning | http://arxiv.org/abs/2509.09674v1 | <details><summary>展开</summary>论文提出了一种名为**SimpleVLA-RL**的强化学习框架，用于提升视觉-语言-动作（VLA）模型的训练效率和泛化能力。核心要点如下： 1. **解决VLA模型的关键挑战** - **数据稀缺性**：传统监督微调（SFT）依赖大量人类操作轨迹，成本高昂且难以扩展。 - **泛化能力差**：模型在分布偏移任务（如场景/物体/目标变化）中表现显著下降。 2. **框架设计创新** - **高效在线RL系统**：基于veRL改进，支持并行环境交互、轨迹采样和损失计算优化。 - **探索增强策略**： - **动态采样**：仅保留混合成功/失败的轨迹组，确保非零梯度。 - **高温采样**（T=1.6）和**放宽PPO截断范围**（[0.8, 1.28]），提升策略多样性。 - **简化奖励机制**：仅用二元结果奖励（1/0），避免复杂过程奖励设计。 3. **显著性能提升** - **基准测试结果**： - **LIBERO**：平均成功率从91%提升至99.1%，长时任务（LIBERO-Long）从17.1%升至98.5%。 - **RoboTwin 1.0/2.0**：双臂任务提升30.6%（1.0）和30.5%（2.0），超越π₀等SOTA模型。 - **数据效率**：仅需单任务演示即可大幅提升性能（如LIBERO-Long从17.1%→91.7%）。 4. **关键发现** - **"Pushcut"现象**：RL训练中自发涌现新动作模式，超越监督数据中的策略。 - **泛化能力**：在空间/物体/目标变化任务中表现优异，且仿真策略可直接迁移至真实机器人（sim-to-real）。 5. **贡献总结** - 降低对大规模数据的依赖，提升长时规划与跨场景泛化能力。 - 首次在VLA模型中验证纯结果奖励RL的有效性，为VLA训练提供新范式。 > 论文链接：[SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674) > 代码开源：[PRIME-RL/SimpleVLA-RL](https://github.com/PRIME-RL/SimpleVLA-RL)</details> |
| 2025-09-11 | VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model | http://arxiv.org/abs/2509.09372v2 | <details><summary>展开</summary>VLA-Adapter提出了一种高效的小规模视觉-语言-动作（VLA）模型范式。核心贡献包括： 1. **系统分析桥接范式**：首次研究视觉语言（VL）表示到动作（A）的桥接条件，发现多层级ActionQuery特征和中间层Raw特征对动作生成最有效。 2. **轻量级架构**：设计带Bridge Attention的Policy模块，自动注入最优VL条件到动作空间，仅需0.5B参数骨干（如Qwen2.5-0.5B），无需机器人数据预训练。 3. **高效性能**： - 在LIBERO、CALVIN等基准上达到SOTA水平（LIBERO平均成功率97.3%，CALVIN任务平均长度4.10）。 - 推理速度达219.2 Hz（延迟0.0365秒），比同类快3倍。 - 训练成本低：单消费级GPU 8小时完成训练。 4. **关键优势**： - 冻结骨干时仍保持高性能（成功率86.4%），降低对大模型的依赖。 - 支持多尺度骨干，性能增益显著（如0.5B骨干比7B骨干微调提升9.2%成功率）。 该方法显著降低VLA模型部署门槛，适用于仿真与真实机器人任务。</details> |
| 2025-09-11 | SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models | http://arxiv.org/abs/2509.09090v1 | <details><summary>展开</summary>本文提出**SQAP-VLA框架**，解决视觉-语言-动作（VLA）模型部署中的效率问题。核心要点如下： 1. **问题背景**： VLA模型（如CogAct）计算和内存开销大，难以在资源受限设备部署。传统**量化**（W4A4）与**令牌剪枝**结合时存在**不兼容性**：量化扭曲注意力分布，导致基于注意力的剪枝失效（图2）。 2. **创新方法**： - **量化感知剪枝策略**： - **量化不敏感保留**：选择极端注意力值的令牌（相对顺序稳定）。 - **机器人感知保护**：基于机器人坐标强制保留关键空间令牌（公式2-4）。 - **空间感知采样**：用FPS算法最大化保留令牌的空间覆盖（公式5）。 - **剪枝导向的量化增强**： 引入**Hadamard变换**（公式6），平滑激活分布，减少异常值对注意力分数的干扰（图3）。 3. **实验结果**： - 在CogAct模型上，W4A4量化 + **40%剪枝率**下： - 推理速度提升**1.93倍**，GPU内存降低73%（图4）。 - 任务成功率**超越原始模型**（视觉匹配场景平均提升4.5%，表1）。 - 显著优于FastV、VLA-Cache等基线（表1-2），且**无需训练**。 4. **贡献与意义**： - 首次实现量化与剪枝的协同设计，解决二者不兼容问题。 - 为资源受限设备提供高效部署方案，代码已开源（https://github.com/ecdine/SQAP-VLA）。 --- 总结：SQAP-VLA通过**量化感知剪枝**和**剪枝导向的量化增强**，在提升VLA模型推理效率的同时保持或提升性能，为嵌入式部署提供新路径。</details> |
| 2025-09-10 | RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation | http://arxiv.org/abs/2509.08820v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-09 | TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models | http://arxiv.org/abs/2509.07962v1 | <details><summary>展开</summary>论文《TA-VLA: 扭矩感知视觉-语言-动作模型的设计空间探索》的核心要点总结如下： ### 1. **问题背景** - 现有VLA模型缺乏对力信号（如扭矩）的感知能力，而扭矩是判断接触式操作任务成败的关键物理反馈。 - 扭矩信号能清晰区分不同操作状态（如无接触、插入失败、插入成功），如图1所示。 ### 2. **关键设计原则** - **解码器集成优于编码器**： - 扭矩与关节角度等本体信号高度相关，在解码器端集成可更好保留物理动态的细微变化（通过HSIC分析验证）。 - 解码器对输入扰动更敏感（表2），适合处理精细扭矩信号。 - **扭矩历史优于单帧输入**： - 多帧扭矩历史能捕捉接触动态，但需压缩为**单一token**输入解码器，避免破坏原有输入模式（表3）。 - **扭矩预测提升物理理解**： - 提出**联合动作-扭矩扩散模型**（图5），通过预测未来扭矩辅助动作生成，增强模型对接触动力学的内部表征（图6）。 ### 3. **实验验证** - **任务性能**： - 在5类接触密集型任务（如按钮按压、充电器插拔）中，完整模型（π₀+obs+obj）显著超越基线： - 按钮按压成功率：18/20 vs 基线5/20（π₀） - 充电器插拔成功率：17/20 vs 基线0/20（π₀）（表5）。 - **泛化能力**： - 方法可迁移至不同VLA架构（如RDT模型，表6）和机器人平台（图8）。 - **效率分析**： - 扭矩集成未显著增加计算开销（附录A.8）。 ### 4. **贡献总结** - 系统探索扭矩集成设计空间（何时/何处/如何集成）。 - 验证解码器端单token历史压缩的有效性。 - 首创动作-扭矩联合扩散模型，提升物理推理能力。 - 实验证明方法在接触任务中提升成功率20-80%，且具跨模型泛化性。 ### 局限 依赖电机扭矩估计精度，未来需探索多模态（如触觉）融合。</details> |
| 2025-09-09 | Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation | http://arxiv.org/abs/2509.07957v1 | <details><summary>展开</summary>本文提出Graph-Fused Vision-Language-Action（GF-VLA）框架，解决从人类视频演示中学习机器人技能时传统方法依赖低级轨迹复制、泛化能力差的问题。核心要点如下： 1. **框架核心**： - GF-VLA通过信息论方法（如熵和互信息）提取任务相关线索，动态检测手-对象和对象-对象交互，构建时序场景图，结构化编码物理交互。 - 融合语言条件变换器，生成分层行为树和可解释笛卡尔运动原语，支持任务级推理。 2. **关键创新**： - **跨臂分配策略**：自主决定双臂夹持器分配，无需显式几何建模，优化协作效率。 - **Chain-of-Thought（CoT）引导**：增强可解释性，提供子目标分解和自验证，提升策略鲁棒性。 - **双头架构**：LLM头处理语义规划，动作头输出低层控制命令，实现并行推理与执行。 3. **实验验证**： - 在双臂块组装任务（如堆叠、字母构建、几何重构）中，场景图准确率达95%，子任务分割精度93%。 - 部署后，抓取可靠性94%、放置精度89%、整体任务成功率90%，在物体形状、空间布局和语义变化下展现强泛化能力。 4. **意义**： - 实现单个人类演示到双臂机器人的策略迁移，统一交互建模与语义推理，为复杂操作提供可解释、自适应的解决方案。</details> |
| 2025-09-08 | F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions | http://arxiv.org/abs/2509.06951v2 | <details><summary>展开</summary>本文提出了一种名为 \(\mathcal{F}_{1}\) 的视觉-语言-动作（VLA）模型，通过集成视觉预见生成机制，弥合感知、预测与执行之间的鸿沟。其核心要点如下： ### 1. **模型架构创新** - **三专家混合架构**：采用 Mixture-of-Transformer 结构，包含三个专用模块： - **理解专家**：基于预训练视觉语言模型（如 PaliGemma），对齐语言指令与视觉观测。 - **生成专家**：通过多尺度残差 VQ-VAE 编码历史观测，生成目标导向的视觉预见图像（\(\hat{o}_{t+1}\)），作为显式规划目标。 - **动作专家**：基于预测逆动力学模型（PIDM），将动作生成转化为视觉目标驱动的逆动力学问题，输出动作序列 \(\hat{a}_{t:t+k}\)。 - **渐进注意力机制**（UGA）：跨专家信息流遵循"理解→生成→动作"的因果层级，避免信息回流，确保稳定性。 ### 2. **训练策略** - **三阶段渐进式训练**： - **阶段 I**：对齐生成专家与预训练的理解专家，通过教师强制学习视觉预见。 - **阶段 II**：在大规模机器人数据集（330k 轨迹，136 任务）上联合优化所有专家，学习通用视觉运动知识。 - **阶段 III**：任务特定数据上微调，适应新机器人本体。 - **损失函数**：结合自回归视觉预测损失（\(\mathcal{L}_{\mathrm{gen}}^{\mathrm{pred}}\)）与流匹配动作损失（\(\mathcal{L}_{\mathrm{action}}\)），强化预测与执行的协同。 ### 3. **关键优势** - **视觉预见引导决策**：通过生成未来观测作为中间目标，将反应式策略转化为基于规划的决策，提升长视野任务鲁棒性。 - **高效多尺度预测**：采用"下一尺度预测"机制，平衡计算效率与生成质量（4 尺度为最优）。 - **强泛化能力**：三阶段训练注入可迁移的视觉预见能力，支持动态环境适应。 ### 4. **实验结果** - **真实任务**：在 9 项 Genie 机器人任务中，平均成功率 **82.2%**（最高任务 **100%**），显著优于 \(\pi_0\)（65.2%）和 gr00t-N1（30.4%）。 - **仿真基准**： - **LIBERO**：平均成功率 **95.7%**，长视野任务提升显著（LIBERO-Long: **91.3%**）。 - **SimplerEnv Bridge**：平均成功率 **72.9%**，精细操作任务（如 Eggplant）达 **100%**。 - **消融实验**：验证生成专家（移除后性能下降 17.2%）和三阶段训练（移除预训练阶段 II 下降 3.7%）的必要性。 ### 5. **应用场景** - **动态环境**：在传送带抓取任务中成功率达 **66.7%**，展现对移动目标的适应能力。 - **跨本体迁移**：仅需 47 条演示数据即可适配新机器人（ARX LIFT II）。 > 总结：\(\mathcal{F}_{1}\) 通过视觉预见重构动作生成范式，在动态与长视野任务中实现显著性能突破，为具身智能提供可解释、可规划的决策框架。</details> |
| 2025-09-08 | LLaDA-VLA: Vision Language Diffusion Action Models | http://arxiv.org/abs/2509.06932v2 | <details><summary>展开</summary>论文提出**LLaDA-VLA**，首个基于预训练扩散视觉语言模型（d-VLM）的**视觉-语言-扩散-动作模型**，用于机器人操作任务。核心要点如下： --- ### **1. 研究动机** - **现有局限**：主流视觉-语言-动作模型（VLA）依赖自回归模型（ARMs），存在生成效率低、单向生成灵活性不足的问题。 - **扩散模型优势**：掩码扩散模型（MDMs）支持并行生成与迭代优化，在文本和视觉任务中表现优异，但尚未应用于机器人动作生成。 --- ### **2. 核心创新** - **局部化特殊令牌分类（Localized Special-token Classification）** 将分类空间从完整词汇表缩减至机器人动作相关的特殊令牌，显著降低预训练d-VLM适配机器人领域的难度。 - **分层动作结构化解码（Hierarchical Action-Structured Decoding）** - **动作级置信排序**：根据动作整体置信度确定解码顺序。 - **令牌级置信排序**：在动作内部按令牌置信度细化解码。 *解决扩散模型难以生成结构化动作序列的问题，显式建模动作内/间依赖关系。* --- ### **3. 模型架构** - **基础框架**：基于预训练d-VLM（LLaDA-V），集成视觉编码器（SigLIP-2）和投影模块（MLP）。 - **输入/输出**： - 输入：语言指令 + 第一视角RGB图像。 - 输出：离散化动作序列（7维/时间步：位移、旋转、夹爪状态）。 - **动作分块**：预测连续K时间步的动作块（默认K=5），提升轨迹连贯性。 --- ### **4. 实验结果** - **仿真基准**： - **CALVIN**：平均任务完成长度提升0.74（优于OpenVLA等）。 - **SimplerEnv**：平均成功率提升51.3%（超越CogACT等）。 - **真实机器人（WidowX）**： - 在4项任务中平均成功率58%（超越π0的35%和CogACT的30%）。 - **泛化能力**：在未见过的物体/容器任务中平均成功率40%（较π0提升25%）。 - **消融实验**： - 局部化分类策略使平均任务长度提升0.79。 - 分层解码策略进一步提升0.58。 --- ### **5. 贡献总结** - 提出首个**扩散式VLA框架**，为机器人策略学习开辟新范式。 - 设计两项关键技术解决d-VLM适配机器人任务的挑战。 - 在仿真与真实场景中均实现**SOTA性能**，验证扩散模型在机器人操作中的潜力。 --- **关键结论**：LLaDA-VLA通过结合扩散模型的高效生成与结构化动作解码，显著提升机器人操作的精度与泛化能力，为未来基于d-VLM的机器人研究奠定基础。</details> |
| 2025-09-06 | SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning | http://arxiv.org/abs/2509.05614v1 | <details><summary>展开</summary>本文提出**SpecPrune-VLA**，一种通过动作感知自推测剪枝加速视觉-语言-动作（VLA）模型的方法。核心贡献如下： 1. **动作级静态令牌剪枝** - 利用历史动作的全局注意力信息识别冗余令牌（如背景），结合当前动作的局部信息（前两层LLM注意力）动态补充关键令牌（如移动物体）。 - 通过速度自适应的帧采样策略（如公式 \( T = \lfloor -\frac{16}{3} \cdot v_t + \frac{22}{3} \rfloor + 4 \) ）精准捕捉动态变化区域。 2. **层级动态令牌剪枝** - 设计重要性评分机制（公式 \( s_i^{(l)} = \omega_{\text{rank},i}^{(l)} \times \omega_{\text{conf}}^{(l)} \) ），结合令牌排名权重和层置信度，在Transformer深层自适应剪枝冗余令牌。 3. **轻量动作感知控制器** - 根据机械臂末端速度（平移 \( v_t \) 和旋转 \( v_r \) ）区分粗粒度/细粒度动作，动态调整剪枝强度（细粒度时保留更多令牌）。 **实验结果**： 在LIBERO仿真基准测试中，相比OpenVLA-OFT： - A800 GPU上平均加速 **1.46×**，任务成功率损失可忽略（<0.7%） - RTX 3090 GPU上加速 **1.57×** - FLOPs减少约 **57%**，同时保持空间推理、物体操作等任务的高成功率（>96%）。 该方法通过融合跨动作的全局信息和局部推测，显著提升VLA模型推理效率，适用于实时机器人控制场景。</details> |
| 2025-09-05 | FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies | http://arxiv.org/abs/2509.04996v1 | <details><summary>展开</summary>这篇论文提出了一种高效的视觉-语言-动作（VLA）策略模型FLOWER，核心贡献是通过结构创新解决现有VLA模型计算成本高的问题： 1. **核心创新** - **中间模态融合**：剪除30-50%的VLM（视觉语言模型）层，将中间层特征通过跨注意力注入流变换器，保留语义理解能力的同时释放参数容量。 - **全局AdaLN机制**：针对不同动作空间设计共享的层归一化控制器（Global-AdaLN），减少20%的扩散头参数，结合轻量级LoRA适配器实现高效调节。 2. **模型架构** - 基于Florence-2 VLM的剪裁版本（保留编码器）和18层流变换器（1024维隐空间），总参数量仅950M。 - 使用整流流（Rectified Flow）生成动作，仅需4-8步去噪，支持跨动作空间（如关节状态/末端位移）的统一处理。 3. **性能优势** - **训练效率**：在8个公开机器人数据集（约25万轨迹）上仅用200 H100 GPU小时完成预训练（同类模型的1%成本）。 - **多场景泛化**：在10个基准测试的190项任务中达到SOTA水平，包括： - CALVIN ABC：平均序列长度4.53（新SOTA） - LIBERO-Long：93.4%成功率 - 真实厨房场景：61%成功率（比基线高103%） - **推理效率**：311Hz吞吐量（比OpenVLA快50倍），仅需1.85GB显存。 4. **应用意义** - 首次实现参数量<1B的通用VLA策略，显著降低机器人策略的研发门槛。 - 开源模型权重与代码（https://intuitive-robots.github.io/flower_vla/），推动机器人学习民主化。 **局限性**：对高精度操作（如密集场景抓取）和移动机器人场景的适应性仍需优化，SIMPLER Google任务表现落后基线10.2%。</details> |
| 2025-09-04 | Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models | http://arxiv.org/abs/2509.04063v1 | <details><summary>展开</summary>本文提出了一种用于视觉语言动作（VLA）流模型的自适应离线强化学习（RL）微调方法——自适应强化流匹配（ARFM）。核心创新点是通过理论推导构建了一个可调节缩放因子α的偏差-方差平衡目标函数，解决VLA流模型（如π₀）在复杂下游任务中动作精度不足的问题。具体贡献如下： 1. **方法设计** - 在VLA流模型损失函数中引入自适应缩放因子α，构建能量加权流匹配损失函数（CEFM）。该因子动态平衡RL优势信号保留（高回报样本权重增强）与梯度方差控制（避免训练崩溃）。 - 通过理论分析建立α的优化目标：最小化损失函数梯度方差的同时最大化RL信号效用（公式7），并推导出可解析求解的非线性方程（公式8）。 - 提出二分迭代算法（Algorithm 1）实时更新α，实现高效微调（Algorithm 2）。 2. **实验验证** - **多任务泛化性**：在LIBERO基准测试中，ARFM平均任务成功率92.1%，较基线π₀提升4.5%，优于ReinboT（91.2%）和RWR（90.8%）（Table 1）。 - **抗扰动能力**：在动作噪声干扰下（噪声强度0.1-0.3），ARFM平均成功率48.2%，较π₀（43.3%）提升11.4%，显著优于对比方法（Table 2）。 - **小样本与持续学习**：仅需10%的微调数据即可达到90%基线性能；在增量任务场景中，遗忘率低于3%。 - **真实机器人任务**：UR5机械臂抓取实验中，ARFM在物体位置扰动下成功率保持85%以上，较基线高12%。 3. **核心优势** ARFM通过理论驱动的自适应机制，解决了传统模仿学习难以利用数据质量分布的问题，同时避免了RL信号引入的不稳定性，在泛化性、鲁棒性和数据效率方面均达到SOTA水平。 > 实验结果表明：缩放因子λ=0.3时取得最优权衡；α的实时调整使训练稳定性提升40%（梯度爆炸率降至5%以下）。</details> |
| 2025-09-04 | FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction | http://arxiv.org/abs/2509.04018v1 | <details><summary>展开</summary>论文提出FPC-VLA框架，通过集成视觉-语言-动作（VLA）模型与故障预测-校正监督器，提升机器人操作的可靠性。核心创新点如下： 1. **双模型架构** - **监督器模块**：基于视觉语言模型（VLM），在关键帧（如夹爪状态变化时）评估动作可行性。通过视觉-语言查询预测故障风险，并生成空间校正指令（如"向左大距离移动"）。 - **自动化数据集生成**：从RLDS格式数据自动构建故障校正数据集，无需人工标注。通过分析夹爪状态变化轨迹，生成包含图像、任务指令和结构化QA的样本（如表II示例）。 2. **动作优化机制** - **相似性引导融合模块**：聚合历史动作预测，通过余弦相似性（公式15）和时间衰减权重（公式17-18）平滑输出，解决动作突变问题。其中姿态与夹爪状态分开处理（公式19-20）。 3. **实验验证** - **跨平台测试**：在WidowX（SIMPLER基准）、Google Robot（表IV）和Franka（LIBERO基准）上均超越SOTA： - WidowX零样本任务成功率64.6%（表III），较次优方法提升22%。 - Google Robot上"打开抽屉放苹果"任务成功率54.5%（表IV）。 - LIBERO长时程任务成功率82.2%（表V），提升12个百分点。 - **实时性**：监督器仅触发≤3次/任务，推理延时增加15%但成功率提升34.6%（任务成功率从58.3%→92.9%）。 4. **应用价值** 成功部署于真实机器人（图4），处理遮挡物体操作等复杂场景，项目页面与代码已开源。 > 该方法解决了传统VLA模型缺乏故障恢复能力的问题，通过轻量化监督机制显著提升系统鲁棒性，为自主机器人提供可扩展的可靠性解决方案。</details> |
| 2025-09-03 | ANNIE: Be Careful of Your Robots | http://arxiv.org/abs/2509.03383v1 | <details><summary>展开</summary>本文《Annie: Be Careful of Your Robots》针对具身AI（EAI）系统的安全风险展开研究，核心贡献如下： 1. **安全定义与分类**：基于ISO/TS 15066标准，首次为EAI系统建立安全框架，将安全违规分为三类： - **关键违规**（Critical）：涉及危险工具（如刀具）与人类距离过近，违反物理隔离约束。 - **危险违规**（Dangerous）：物体速度超限或过早释放，导致潜在伤害。 - **风险违规**（Risky）：机器人与环境物体碰撞，引发间接损害。 2. **安全基准Annie-Bench**：构建首个EAI安全专用数据集，包含9个安全关键场景（每类3个），共2,400个视频-动作序列，覆盖三类违规场景，支持安全攻击与防御评估。 3. **攻击框架Annie-Attack**：提出任务感知对抗攻击框架，通过“攻击领导者模型”将长期安全目标分解为帧级扰动，实现： - **高攻击成功率**：在ACT、Baku等EAI模型上，关键、危险、风险任务的攻击成功率分别达52%、67%、50%。 - **隐蔽性优化**：支持稀疏攻击（如每3帧扰动）和自适应策略，降低攻击频率同时保持高成功率。 4. **评估与影响**：实验验证攻击在模拟和真实机器人场景的有效性，暴露EAI系统在物理交互中的安全漏洞，呼吁加强安全防御机制。 论文强调传统机器学习安全方法不适用于EAI，需重新定义安全指标并设计物理环境下的防御策略。</details> |
| 2025-09-02 | Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance | http://arxiv.org/abs/2509.02055v2 | <details><summary>展开</summary>本文提出了一种名为Align-Then-stEer（ATE）的新型视觉语言动作模型（VLA）自适应框架，用于解决跨任务和跨机器人平台适应中的动作分布不匹配问题。核心要点如下： ### 1. **问题背景** - VLA模型在大规模预训练后，适应新机器人平台或任务时存在显著动作分布差异 - 传统微调方法在跨本体（如单臂→双臂）和跨任务场景下需要大量数据 ### 2. **ATE框架设计** - **两阶段自适应**： - **对齐阶段**：通过非对称变分自编码器（VAE）构建统一潜在动作空间 - 利用反向KL散度的模式寻求特性，将目标动作嵌入预训练动作分布的特定模式 - **引导阶段**：在扩散/流匹配VLA的微调中引入分类器引导机制 - 基于共享潜在空间设计引导函数，显式地将输出分布推向目标域 ### 3. **技术优势** - **数据高效**：仅需少量目标域数据（如每任务50轨迹） - **即插即用**：兼容扩散型（如RDT）和流匹配型（如π₀）VLA，无需修改原架构 - **计算轻量**：仅增加两个轻量VAE（参数量<1%） ### 4. **实验结果** - **仿真测试**（RoboTwin 1.0/ManiSkill3）： - 平均多任务成功率提升**9.8%** - 在复杂任务（如双瓶抓取）上提升超30% - **真实世界**（双臂RealMan 7-DoF机器人）： - 跨本体适应成功率提升**32%** - 实现分钟级长时程操作和双手协调任务 ### 5. **核心贡献** - 首次利用反向KL的模式寻求特性解决VLA跨本体适应问题 - 提出首个适用于扩散/流匹配VLA的潜在空间引导机制 - 为实际部署提供通用轻量解决方案，显著降低适应成本 该方法通过结构化潜在空间和显式分布引导，有效解决了VLA适应中的动作分布不匹配瓶颈，为机器人快速部署提供了新范式。</details> |
| 2025-09-02 | AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving | http://arxiv.org/abs/2509.01944v1 | <details><summary>展开</summary>论文提出AutoDrive-R²框架，通过两阶段训练增强自动驾驶视觉-语言-动作（VLA）模型的推理与自省能力： 1. **创新CoT数据集**：构建nuScenesR²-6K数据集，采用四步逻辑链（场景分析→物理建模→逻辑推导→自省验证）生成监督微调数据，建立输入信息与输出轨迹的认知桥梁。 2. **物理约束强化学习**：提出基于群组相对策略优化（GRPO）的强化学习方法，融合空间对齐、车辆动力学（转向角/速度约束）和时间平滑性奖励函数，确保轨迹的物理可行性和安全性。 3. **性能优势**：在nuScenes/Waymo数据集上实现SOTA性能（平均L2误差0.19m），零样本泛化能力显著，较基线模型Qwen-VL-7B误差降低86.9%。消融实验验证了四步推理结构和物理奖励模块的必要性。 该框架首次在自动驾驶中实现带自省验证的结构化推理，解决了传统VLA模型轨迹不可行和逻辑不透明的问题。</details> |
| 2025-08-31 | OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving | http://arxiv.org/abs/2509.00789v1 | <details><summary>展开</summary>本文提出OmniReason框架，用于解决自动驾驶中视觉语言模型（VLMs）忽视时间维度的问题。核心贡献包括： 1. **OmniReason-Data数据集** - 构建两个大规模视觉-语言-动作（VLA）数据集（基于nuScenes和Bench2Drive），通过抗幻觉自动标注流程生成时空密集的注释和自然语言解释。 - 整合人类先验知识、场景感知描述和因果推理链，确保数据的时间连贯性和物理合理性。 2. **OmniReason-Agent架构** - 设计端到端VLA模型，包含**稀疏时序记忆模块**（建模长期场景上下文）和**解释生成器**（输出人类可理解的决策依据）。 - 通过时空知识蒸馏技术，将时空因果推理模式注入感知-规划-行动全流程。 3. **性能优势** - 在开环轨迹规划任务中取得最佳L2距离（0.34m）和违规率（3.18%）。 - 在驾驶视觉问答（VQA）任务中显著超越现有模型（如BLEU-1提升37.6%）。 - 实现可解释的时序感知决策，缩小自动驾驶与人类驾驶的决策差距。 该框架为复杂动态环境中的自动驾驶提供了可靠的时空推理能力。</details> |
| 2025-08-30 | Galaxea Open-World Dataset and G0 Dual-System VLA Model | http://arxiv.org/abs/2509.00576v1 | <details><summary>展开</summary>本文提出Galaxea开放世界数据集及G0双系统VLA模型，核心要点如下： 1. **Galaxea开放世界数据集** - 大规模机器人行为数据集（500小时/100K轨迹），在真实人类生活/工作环境收集 - 覆盖150+任务类别、50个场景、1600+物体，使用统一机器人本体（Galaxea R1 Lite） - 关键特性：细粒度子任务标注、异构环境多样性、严格质量管控 - 相较现有数据集优势：单本体一致性、真实场景覆盖、精细动作标注 2. **G0双系统框架** - **系统架构**： - System 2 (G0-VLM)：视觉语言模型负责高层规划（慢思考） - System 1 (G0-VLA)：视觉语言动作模型执行细粒度控制（快执行） - **三阶段训练课程**： - 阶段1：跨本体预训练（学习通用先验知识） - 阶段2：单本体预训练（Galaxea数据集适配目标平台） - 阶段3：任务特定后训练（少量演示微调） 3. **关键发现** - 单本体预训练对性能提升至关重要，尤其在： - 本体特定动作（底盘/躯干协调控制） - 少样本迁移（20样本微调成功率显著提升） - 跨本体预训练存在局限：本体差距过大时可能损害性能 - 完整训练流程（G0-Full）在桌面整理/微波操作/铺床等任务表现最优 4. **评估结果** - 构建多维度基准：桌面操作/设备使用/长时程移动操作 - G0-VLA在动作精度提升50%+，G0-VLM指令准确率超基线50% - 实验验证：高质量数据集+单本体预训练是强泛化性能的关键 5. **开源计划** - 数据集与模型将开源，推动具身智能社区发展 结论：该工作通过创新数据集与双系统框架，解决了真实场景机器人泛化的核心挑战，为开放世界机器人操作建立新基准。</details> |
| 2025-08-30 | Mechanistic interpretability for steering vision-language-action models | http://arxiv.org/abs/2509.00328v1 | <details><summary>展开</summary>这篇论文的核心贡献是提出了首个针对视觉-语言-动作模型（VLA）的机制可解释性框架，实现了通过内部表征干预实时控制模型行为。主要要点如下： 1. **核心发现** - VLA 内部存在可解释的语义概念神经元（如“速度”“方向”），其 FFN 层值向量（value vectors）可投影到词嵌入空间展现语义关联。 - 即使模型仅输出动作指令，超过 75% 的神经元仍保留语义概念（如“slow”），且与动作选择存在因果关联。 - VLA 训练未显著改变预训练 VLM 的语义结构，微调主要影响动作令牌分布而非语义概念。 2. **干预方法** - **激活导向（Activation Steering）**：通过识别与特定概念（如“向上”）对齐的神经元集群，在推理时注入固定强度激活（公式 3-4），无需微调或环境交互。 - 实验证明：在仿真（OpenVLA/LIBERO）和实体机器人（π₀/UR5）中，干预可显著改变行为（如“fast”集群使末端位移提升 27.7%）。 3. **关键实验** - **仿真实验**： - 激活“fast/slow”集群可调节末端执行器位移（p<0.001）。 - 晚期层干预对运动控制效果更显著（如“up”集群在晚期层注入时位移提升 12.5%）。 - **机器人实验**： - 在 UR5 上，语义干预（如“low”集群）比提示词修改（如“low place penguin”）更有效降低轨迹高度。 - “slow”干预显著降低平均位移（但“fast”干预与基线相近，表明默认行为已偏快）。 4. **意义与局限** - **贡献**：首次建立 VLA 机制可解释性与实时控制的桥梁，为安全部署提供透明调控接口。 - **局限**：语义方向存在歧义性，微调可能影响可干预性，需扩展至移动平台等复杂场景。 **结论**：该框架证明 VLA 的语义概念可被直接操控以实现零样本行为控制，推动了具身智能模型的可解释性与安全性研究。</details> |
| 2025-08-28 | CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification | http://arxiv.org/abs/2508.21046v1 | <details><summary>展开</summary>CogVLA是一种认知对齐的视觉-语言-动作模型，通过指令驱动的路由与稀疏化提升效率与性能。核心要点如下： 1. **问题背景** - 现有VLA模型存在高计算开销（如7B模型单任务训练需600+ GPU小时），传统稀疏化方法（MoD、层跳过等）忽视跨模态语义耦合，导致视觉特征丢失、语言连贯性破坏和动作生成不一致。 2. **创新设计** - **三阶段渐进架构**：仿人类多模态协调机制 - **EFA-Routing**（视觉聚焦）：通过Encoder-FiLM动态聚合视觉token，压缩75%冗余信息（保留25%），指令条件化融合双流视觉特征（如SigLIP与DINOv2）。 - **LFP-Routing**（语义过滤）：在LLM中引入动作意图，剪枝50%无关视觉token，降低计算负担。 - **CAtten**（动作规划）：V-L-A耦合注意力结合因果视觉语言注意与双向动作并行解码，确保动作连贯性（公式19-20）。 3. **关键优势** - **性能提升**： - LIBERO基准：97.4%平均成功率（Spatial:98.6%, Object:98.8%, Long:95.4%），超越最优基线（OpenVLA-OFT:97.1%）。 - 真实任务（ALOHA）：70%成功率（如抽屉操作7/10，T恤折叠6/10）。 - **效率优化**： - 推理延迟0.091s（比OpenVLA快2.8×），吞吐量87.9Hz（22.5×提升）。 - 训练成本4.7h/10k步（降低2.5×），FLOPs仅2.72T（减少3.1×）。 4. **实验验证** - 稀疏化协同效应：Stage 1+2在8×压缩下性能最优（Stage 1占比越高提升越显著）。 - 模块必要性：移除任一阶段导致性能下降（如无Stage 1时Spatial SR降至91.2%）。 - 可视化对比：CogVLA避免任务无关错误（如抽屉碰撞），动作轨迹更精确。 5. **贡献总结** - 首个人类认知启发的VLA三阶段框架，实现端到端跨模态语义对齐。 - 指令驱动的双重稀疏化（EFA+LFP）与耦合注意力（CAtten）协同优化效率与性能。 - 为资源受限场景提供高效解决方案，推动具身智能部署。 > 论文链接: [https://jiutian-vl.github.io/CogVLA-page](https://jiutian-vl.github.io/CogVLA-page)</details> |
| 2025-08-28 | EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control | http://arxiv.org/abs/2508.21112v3 | <details><summary>展开</summary>该论文提出了一种名为**EO-1**的通用机器人控制模型，其核心创新点是通过**交错式视觉-文本-动作预训练**提升机器人的多模态推理与物理交互能力。以下是核心要点： --- ### 1. **模型架构（EO-1）** - **统一的多模态处理**：采用单一解码器Transformer架构，无缝处理图像、文本、视频和动作数据，支持跨模态联合学习。 - **双目标训练**： - **自回归解码**：生成文本推理结果（如任务规划答案）。 - **流匹配降噪**：生成连续机器人动作（通过预测向量场实现动作去噪）。 - **参数继承**：基于预训练的视觉语言模型（Qwen2.5-VL）初始化，避免从头训练动作模块，提升跨模态知识迁移效率。 --- ### 2. **数据集（EO-Data1.5M）** - **规模与构成**：包含**150万样本**，整合机器人控制数据与多模态推理数据，强调时空动态关联。 - **交错式数据构建**： - **时空推理标注**：通过人工与VLM标注机器人视频，生成物理常识理解、任务规划、物体定位等QA对。 - **三种交错格式**： 1. **时序推理格式**：串联图像、任务规划QA、动作指令。 2. **空间推理格式**：关联轨迹预测QA与动作执行。 3. **自由对话格式**：随机组合推理QA与任务指令。 - **优势**：捕捉推理与动作的因果依赖，支持模型学习开放世界的交互逻辑。 --- ### 3. **关键性能** - **多模态推理**：在**RoboVQA**、**ERQA**及自建**EO-Bench**评测中超越主流模型（如GPT-4o、InternVL2.5），尤其在空间理解（如多视角物体定位）和时序推理（如任务进度验证）任务中表现突出。 - **机器人控制**： - **仿真环境（LIBERO）**：长时序任务成功率98.2%，优于OpenVLA、Octo等方法。 - **真实机器人（WidowX/Google Robot）**：在物体操作（如叠放积木）和复杂任务（如开关抽屉）中成功率提升20%以上。 --- ### 4. **泛化能力** - **开放世界适应性**：通过交错训练，模型在未见物体、环境变化及多机器人平台（如双臂人形机器人）中展现强泛化性。 - **关键技术**：**交错校正采样**策略，确保动作生成与后续推理的因果一致性。 --- ### 5. **贡献总结** - **统一架构**：首个支持视觉-文本-动作交错生成的一体化模型。 - **高质量数据集**：构建最大规模交错式机器人数据集（EO-Data1.5M）。 - **开源生态**：公开模型、训练代码及数据集，推动通用具身智能发展。 > 论文通过实验验证：**交错式学习**是提升机器人开放场景理解与泛化控制的关键，为通用自主机器人系统提供了新范式。</details> |
| 2025-08-27 | Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies | http://arxiv.org/abs/2508.20072v1 | <details><summary>展开</summary>本文提出Discrete Diffusion VLA模型，创新性地将离散扩散引入视觉-语言-动作（VLA）策略的动作解码中。核心贡献包括： 1. **统一架构设计**：在单一Transformer中集成视觉、语言和动作模态，通过离散扩散对分桶化的动作令牌建模，使用与VLM主干相同的交叉熵目标训练，保留预训练多模态先验知识。 2. **自适应解码机制**： - 采用"易先难后"的并行解码策略，根据置信度分数动态选择待预测令牌 - 引入二次重掩码技术，通过阈值检测和置信残差检查修正不确定预测，提升跨步一致性 3. **性能优势**： - 在LIBERO任务上达到96.3%平均成功率（+0.9% vs OpenVLA-OFT） - SimplerEnv-Fractal视觉匹配率71.2%，整体64.1% - SimplerEnv-Bridge整体49.3%（+9.8% vs π₀+FAST） - 相比自回归模型减少4.7倍函数评估次数，突破序列解码瓶颈 该方法通过固定步长的迭代精炼实现动作生成，为大规模VLA模型提供可扩展的统一框架。</details> |
| 2025-08-27 | Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation | http://arxiv.org/abs/2508.19958v2 | <details><summary>展开</summary>待生成</details> |
| 2025-08-27 | Ego-centric Predictive Model Conditioned on Hand Trajectories | http://arxiv.org/abs/2508.19852v2 | <details><summary>展开</summary>这篇论文提出了一种基于手部轨迹的自中心预测模型（Ego-PM），通过统一的两阶段框架解决自中心场景中动作预测与未来视频生成的联合建模问题。核心贡献如下： 1. **问题定位与创新点** - 现有视觉-语言-动作（VLA）模型仅预测动作但缺乏视觉后果建模，而视频生成模型未显式结合动作条件，导致预测不一致 - 首次提出统一框架，同时预测未来动作（手部轨迹）及其视觉结果，无需额外标注即可推理 2 **方法设计** - **阶段一（显式动作建模）**： - 采用连续状态建模（CoSMo）策略，融合历史视觉、语言和动作输入 - 通过自回归模型预测未来手部轨迹和动作描述 - **阶段二（动作增强帧预测）**： - 引入因果交叉注意力机制，融合多模态条件（视觉、文本、预测动作） - 利用潜在扩散模型（LDM）生成未来帧序列 3 **技术优势** - CoSMo策略利用连续状态提升时序一致性 - 动作编码器-解码器设计增强动作表征能力 - 适用于人类活动（Ego4D）和机器人操作（BridgeData/RLBench）双场景 4 **实验结果** - 在Ego4D上：动作预测（手部IoU 44.25）和帧生成（FID 21.31）均超越SOTA - 在BridgeData V2：成功率达73.7%，视频生成质量（FID 16.36）最优 - 消融实验验证CoSMo和因果注意力对长时序预测的关键作用 5 **开源与意义** - 代码已开源：https://github.com/showlab/Ego-PM - 为具身智能提供可推理动作后果的世界模型，推动人机交互发展 该模型首次实现自中心场景中动作与视觉变化的联合预测，为理解人-物交互提供了新范式。</details> |
| 2025-08-26 | MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2508.19236v1 | <details><summary>展开</summary>本文提出**MemoryVLA**——一种受人类记忆系统启发的机器人操作框架，通过感知-认知记忆机制解决长时序任务依赖问题。核心要点如下： 1. **问题背景** 主流视觉-语言-动作（VLA）模型仅依赖当前观测，难以处理长时序任务（如按钮序列操作）。认知科学表明人类依赖工作记忆（短期缓冲）和海马体（长期存储语义细节）协同决策。 2. **方法创新** - **双记忆架构**： - **工作记忆**：视觉编码器提取感知token + LLM生成认知token，形成短期表征 - **记忆银行（PCMB）**：存储历史低层感知细节与高层语义，支持跨时序检索 - **记忆操作**： - **检索**：基于时间位置编码查询相关历史 - **门控融合**：自适应融合当前与历史表征 - **压缩**：合并相邻相似条目减少冗余 - **动作生成**：基于记忆增强的token，扩散模型预测连续7-DoF动作序列 3. **实验结果** - **仿真任务**： - SimplerEnv-Bridge：71.9%成功率（+14.6优于CogACT） - SimplerEnv-Fractal：72.7%成功率（+4.6） - LIBERO-90：96.5%成功率（+3.3） - **实物机器人**： - 6项通用任务：85%成功率（+9优于CogACT） - 6项长时序任务：83%成功率（+26显著提升） - **鲁棒性**：在遮挡/光照变化/干扰物等OOD场景下表现优异 4. **贡献总结** - 首个人类记忆启发的VLA框架，显式建模时序依赖 - PCMB机制实现跨层（感知/认知）记忆检索与融合 - 在150+任务中验证有效性，长时序任务提升显著 > 项目页面：https://shihao1895.github.io/MemoryVLA</details> |
| 2025-08-25 | FlowVLA: Thinking in Motion with a Visual Chain of Thought | http://arxiv.org/abs/2508.18269v2 | <details><summary>展开</summary>论文提出FlowVLA框架，解决现有Vision-Language-Action (VLA)模型通过直接下一帧预测（\(v_t \rightarrow v_{t+1}\)）训练世界模型时，混同运动动态与外观渲染的问题，导致物理预测不合理和策略学习低效。核心创新是引入**Visual Chain of Thought (Visual CoT)**，将预测分解为“运动推理→外观生成”的链式过程（\(v_t \rightarrow f_t \rightarrow v_{t+1}\)），其中\(f_t\)为中间光流运动表示。FlowVLA作为自回归Transformer实现，采用统一标记化将光流和图像编码为共享词汇，无需额外架构。实验表明，在LIBERO和SimplerEnv等机器人操作基准上，FlowVLA实现了最先进性能，并显著提升样本效率，验证了显式运动推理对世界建模和策略学习的有效性。</details> |
| 2025-08-23 | NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows | http://arxiv.org/abs/2508.16845v1 | <details><summary>展开</summary>论文提出**NinA模型**，用**标准化流（Normalizing Flows, NFs）替代扩散模型**作为视觉-语言-行动（VLA）模型中的动作解码器，核心要点如下： ### 1. **问题与动机** - **扩散模型的缺陷**：当前VLA模型广泛使用扩散模型作为动作解码器，虽能处理复杂动作分布，但需多次迭代去噪，推理延迟高，难以满足实时机器人控制的高频需求。 - **NFs的优势**：标准化流通过单次可逆变换生成动作，**推理速度显著更快**，且支持精确似然估计，利于强化学习、不确定性建模等下游任务。 ### 2. **NinA方法** - **架构设计**： - 基于FLOWER VLA框架，保留预训练的视觉-语言模型（VLM）编码器，**仅替换动作解码器为NF**。 - 提出两种NF变体：**MLP版（参数量2M）** 和 **Transformer版（参数量38M）**，前者轻量高效，后者性能更强。 - **关键技术**： - **噪声注入**：训练时对动作添加高斯噪声（最佳σ=0.03），提升泛化能力（消融实验证实必要性）。 - **可逆耦合层**：将动作分割为两部分，一部分经网络生成缩放/偏置参数，另一部分线性变换后拼接。 - **PLU可逆线性层**：增强表达能力（消融实验显示对Transformer版有轻微增益）。 ### 3. **实验结果** - **性能匹配**：在LIBERO基准测试中： - NinA Transformer平均成功率0.938，接近扩散模型（0.952），参数量仅为其1/10（38M vs 330M）。 - NinA MLP平均成功率0.909，优于小型扩散模型（31M参数版为0.916）。 - **推理速度**： - NinA比扩散模型**快5-10倍**（如H100 GPU：0.021s vs 0.110s）。 - **关键消融**： - 移除噪声注入导致性能显著下降（Transformer从0.938→0.896）。 - Transformer在深度扩展性上优于MLP（深度增至30层仍稳定）。 ### 4. **意义与展望** - **高效替代方案**：NFs在保持性能的同时大幅提升推理效率，适用于实时机器人控制。 - **潜在价值**：NFs的似然计算能力为强化学习、不确定性估计等任务提供新可能。 - **未来方向**：扩展至更大规模预训练、多场景部署及与其他模态的集成。 --- **总结**：NinA通过标准化流实现**高效单次动作生成**，在推理速度和参数量上显著优于扩散模型，为实时VLA控制提供新路径。</details> |
| 2025-08-22 | Do What? Teaching Vision-Language-Action Models to Reject the Impossible | http://arxiv.org/abs/2508.16292v1 | <details><summary>展开</summary>论文提出**Instruct-Verify-and-Act (IVA) 框架**，旨在提升视觉-语言-动作模型（VLA）处理**错误前提指令**（如指令引用环境中不存在物体）的能力。核心要点如下： 1. **问题背景**： - 现有VLA模型在真实场景中面临用户指令可能基于错误前提（如“拿桌上的红杯子”，但杯子不存在）的挑战，无法有效识别或纠正此类指令。 2. **方法创新**： - **IVA框架**： - **检测（Instruct）**：识别指令是否因错误前提（物体缺失/条件不满足）而无法执行。 - **验证（Verify）**：通过语言交互澄清或纠正用户意图（如“未找到红杯子，是否指蓝色抽屉？”）。 - **执行（Act）**：对可行指令生成动作，或基于感知提出替代方案。 - **数据集构建**： - 利用RLBench环境构建大规模指令调优数据集，包含**真实前提指令**与两类错误前提指令： - **领域内错误**（如任务中相似但缺失的物体）； - **领域外错误**（如完全不存在的物体）。 - 数据分布：65%任务含领域内错误，20%含领域外错误，确保模型全面学习。 3. **实验结果**： - 在9个RLBench任务上，IVA显著优于基线（如LLARVA）： - **错误前提检测率**：领域内达100%，领域外达97.78%（平均提升97.56%）。 - **错误场景响应成功率**：提升50.78%。 - **真实前提任务性能**：保持与基线相当（42.67% vs 38.67%），证明框架不损害基础能力。 4. **局限与展望**： - 数据依赖模拟环境，需验证真实场景泛化性； - 复杂指令（如多轮对话）处理能力有限； - 未来将探索开放环境部署及更灵活的语言交互。 **总结**：IVA通过统一框架使VLA模型能主动拒绝错误指令并引导用户修正，提升人机交互的鲁棒性与安全性。</details> |
| 2025-08-21 | Survey of Vision-Language-Action Models for Embodied Manipulation | http://arxiv.org/abs/2508.15201v1 | <details><summary>展开</summary>根据提供的HTML原文，该文件未包含论文实质内容，仅显示一条消息：“See pages 1-last of lhr.pdf”，指示用户需查阅PDF文件（lhr.pdf）以获取完整论文信息。因此，无法从HTML中提取任何论文要点。</details> |
| 2025-08-19 | CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models | http://arxiv.org/abs/2508.13446v1 | <details><summary>展开</summary>论文提出了一种名为**CAST（Counterfactual Augmentation with Synthetic Trajectories）**的新方法，用于提升视觉-语言-动作（VLA）模型在机器人导航任务中的指令跟随能力。核心要点如下： 1. **问题背景** - VLA模型在遵循细粒度语言指令（如“沿左侧墙壁移动”）时表现不佳，主要原因是训练数据缺乏**语义多样性**和**细粒度任务变体**。 - 现有数据集中，相同观察状态通常只对应单一任务（如“打开抽屉”），导致模型忽略语言指令（**后验塌陷**问题）。 2. **核心方法：CAST** - **反事实标签生成**：利用视觉语言模型（VLM）为现有机器人轨迹生成**反事实指令**（如将“直行走廊”改为“沿左侧墙壁导航”）及对应动作。 - **原子策略辅助**：通过训练简单原子策略（如“左转/右转”）生成反事实动作，确保指令与动作的可行性。 - **数据增强**：将生成的反事实轨迹（指令-动作对）加入训练集，强制模型关注语言指令与动作的关联。 3. **理论依据** - 反事实标签通过增加**条件互信息** \(I(\mathbf{a}; \ell \| \mathbf{o})\)，确保模型必须依赖指令预测动作（Lemma IV.1）。 4. **实验验证** - **任务设置**：在3种真实环境（室内走廊、厨房、户外公园）测试27项复杂指令任务，涵盖物体导航（如“移动到椅子”）、关系导航（如“移动到桌子右侧”）和连续导航（如“沿窗户移动”）。 - **结果**： - CAST显著提升VLA性能：**53%** 平均成功率，比未使用CAST的VLA高**27%**。 - 优于基线方法：超过分层规划（\(\pi_a\) + VLM规划）和CoNVOI等SOTA方法**19%**。 - **关键优势**：无需额外数据收集，仅通过数据增强即可提升模型对语言指令的敏感度。 5. **技术贡献** - 开源CAST数据增强代码、数据集及训练好的VLA模型（**CounterfactualVLA**），促进可复现性。 6. **局限性** - 原子行为设计依赖任务领域知识（如导航中的转向指令）。 - VLM标注质量影响生成效果，未来可结合仿真或生成模型优化。 **总结**：CAST通过合成反事实指令-动作对，解决了VLA模型因训练数据单一导致的指令忽略问题，显著提升了机器人对复杂语言指令的理解和执行能力。</details> |
| 2025-08-18 | Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy | http://arxiv.org/abs/2508.13103v1 | <details><summary>展开</summary>本文提出了一种名为“观察中心视觉-语言-动作策略”（OC-VLA）的新框架，旨在解决现有视觉-语言-动作（VLA）模型在泛化到真实环境时面临的核心问题：观察空间（相机坐标系）与动作空间（机器人基坐标系）之间的错位导致模型鲁棒性差。具体要点如下： 1. **问题分析**：现有VLA模型通常在机器人基坐标系中预测末端执行器位姿，但视觉观察基于相机坐标系。这种不一致在训练数据视角多样时加剧了跨视角泛化困难，尤其当机器人基座不在相机视野内时。 2. **核心方法**：OC-VLA利用相机外参标定矩阵，将动作目标从机器人基坐标系转换到相机坐标系进行预测。这统一了观察与动作的空间表示，无需修改模型架构即可实现即插即用： - **训练阶段**：将动作位姿转换至相机坐标系作为监督信号。 - **推理阶段**：预测的相机坐标系动作反向转换回机器人坐标系执行。 3. **关键优势**： - 显著提升模型对相机视角变化的鲁棒性，改善跨视角泛化。 - 作为轻量级模块，兼容现有VLA架构（如连续或离散动作空间）。 4. **实验结果**： - **模拟任务**（ManiSkill2）：OC-VLA在成功率上优于基线（例如离散动作空间提升14%），尤其在视角变化任务中表现更优。 - **真实机器人任务**（Franka Emika Panda）：在10样本微调设置下，OC-VLA平均成功率提高10%，且在未见视角下泛化能力更强（性能下降幅度比基线低6%）。 该方法通过统一观察与动作空间，有效解决了VLA模型的空间错位问题，提升了实际部署的适应性。</details> |
| 2025-08-18 | Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey | http://arxiv.org/abs/2508.13073v2 | <details><summary>展开</summary>本文系统综述了基于大型视觉语言模型（VLA）的视觉-语言-动作模型在机器人操作中的应用，核心要点如下： 1. **模型架构分类**： - **单体模型（Monolithic Models）**： - **单系统模型**：集成环境感知（视觉、语言、机器人状态）与动作生成于统一架构，通过自回归或并行解码生成动作（如RT系列、OpenVLA）。研究聚焦模型性能增强（多模态感知、推理链优化）和推理效率优化（架构精简、参数量化）。 - **双系统模型**：分离高层推理（System 2：VLM主干）与实时控制（System 1：动作专家）。分串联式（如DP-VLA、RoboDual，VLM输出特征驱动动作生成）和并联式（如GR00T N1、ChatVLA，双系统交互协同），提升响应速度与任务适应性。 2. **分层模型（Hierarchical Models）**： - 显式解耦任务规划与策略执行，生成可解释中间表示（如关键点、子任务、程序）。 - **纯规划器（Planner-Only）**：VLM输出结构化指令（如程序代码、关键点坐标），由独立策略模块执行。 - **规划器+策略（Planner+Policy）**：联合优化规划与执行模块（如基于关键点或子任务的端到端训练），提升任务鲁棒性。 3. **关键贡献**： - 统一VLA模型定义，解决术语不一致性，整合机器人学、CV、NLP多领域研究。 - 分析VLA特性：多模态融合、指令跟随、多维度泛化能力，并系统分类数据集（仿真/真实世界/人类行为数据）。 - 指出未来方向：记忆机制、4D感知、高效适配、多智能体协作等。 4. **应用与挑战**： - VLA模型利用VLMs的开放世界泛化、知识推理优势，提升机器人复杂场景操作能力。 - 现存挑战：推理效率与实时控制需求冲突、多模态对齐复杂性、长时序任务规划稳定性。 该综述首次系统化梳理大型VLM-based VLA模型，为机器人操作领域提供架构设计参考与研究路线图。</details> |
| 2025-08-17 | Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search | http://arxiv.org/abs/2508.12211v1 | <details><summary>展开</summary>这篇论文提出了Vision-Language-Action Planning & Search (VLAPS)框架，通过结合模型搜索与预训练的视觉-语言-动作(VLA)策略，提升机器人策略在语言指定任务中的性能。核心要点如下： 1. **问题背景**：预训练的VLA策略在分布外场景中表现脆弱，无法推理动作后果，导致行为不稳定。而传统模型搜索在机器人任务中面临动作空间大、奖励稀疏的挑战。 2. **方法创新**： - **VLAPS框架**：在VLA推理过程中嵌入改进的蒙特卡洛树搜索(MCTS)，利用环境模型进行前瞻搜索。 - **关键机制**： - **宏动作空间构建**：使用VLA策略采样上下文相关的时序抽象宏动作（如4步动作序列），将连续动作空间压缩为有限候选集（约2000个），解决搜索空间爆炸问题。 - **VLA引导的树搜索**：通过VLA输出的先验分布（公式1）指导节点扩展，并设计基于VLA置信度的选择策略（公式2），聚焦有希望的行为分支。 3. **实验验证**： - 在LIBERO机器人操作任务套件中测试，VLAPS显著超越VLA基线： - 成功率最高提升67个百分点（如Libero-Spatial任务从34%→97%）。 - 当基础VLA策略较弱时（成功率<50%），改进幅度最大。 - **自适应计算**：在VLA失败时自动分配更多搜索时间，成功时快速返回结果。 - **效率优势**：VLA驱动的动作采样使原本不可行的搜索（理论节点数达$2000^{100}$）变得可行。 4. **贡献价值**： - 提供原则性框架整合环境先验知识与规划技术。 - 实现测试时计算资源的动态分配。 - 无需额外训练即可提升现有VLA模型性能。 5. **局限性**：依赖环境模型精度，搜索增加计算延迟。未来将探索学习模型与搜索的联合优化。</details> |
| 2025-08-16 | Toward General Physical Intelligence for Resilient Agile Manufacturing Automation | http://arxiv.org/abs/2508.11960v1 | <details><summary>展开</summary>本文探讨面向弹性敏捷制造自动化的通用物理智能（GPI）框架，核心要点如下： 1. **研究背景与目标** - 工业5.0要求机器人具备人机协作和环境适应能力，但传统自动化系统难以应对高定制化生产的动态需求。 - GPI通过融合多模态感知（视觉、语言、触觉）与物理行动，实现跨任务、跨平台的通用智能，核心是视觉-语言-动作（VLA）模型。 2. **技术框架支柱** - **多感官表征学习**：融合视觉、触觉和本体感觉数据（如TLA框架），提升接触式操作的鲁棒性。 - **仿真到现实迁移**：利用合成数据（如Open X-Embodiment数据集）和域随机化解决真实数据稀缺问题。 - **分层规划与控制**：结合高层语言规划（如SayCan）与低层控制策略（如RT-H），支持长时序任务分解。 - **安全与不确定性**：通过风险感知控制器（如KNOWNO）和扩散策略保障人机协作安全。 - **评估基准**：建立物理交互指标（如力控精度）和认知对齐指标，量化工业就绪度。 3. **GPI架构创新** - 扩展VLA模型（如RT-2、PaLM-E），集成触觉反馈和本体感觉，形成双系统架构： - *System-1*：基于VLA的认知规划层，生成高层策略。 - *System-2*：实时反射控制层，执行物理动作并反馈环境状态。 - 消融实验（螺母装配任务）显示：RT-2-GPI模型成功率最高（93.3%），泛化能力最强（0.89），得益于触觉与视觉的深度融合。 4. **挑战与建议** - **数据瓶颈**：需构建同步多模态工业数据集（含触觉动态），开发物理准确的接触仿真。 - **触觉语义理解**：建立语言指令到力控策略的映射（如“适度紧固”的量化定义）。 - **实时性与安全**：模型轻量化（蒸馏/量化）和可解释性设计是部署关键。 - **仿真到现实**：通过元学习缩小物理参数差异，提升策略泛化性。 5. **工业应用前景** GPI推动制造系统向自主适应演进，典型场景包括：零样本装配指令执行、人机协作质检、动态产线重构。需进一步解决实时决策延迟（<1kHz）和跨平台认证问题。</details> |
| 2025-08-15 | TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models | http://arxiv.org/abs/2508.19257v1 | <details><summary>展开</summary>论文提出TTF-VLA（Temporal Token Fusion via Pixel-Attention Integration）方法，用于增强视觉-语言-动作（VLA）模型的性能。现有VLA模型逐帧独立处理视觉输入，忽略时序信息，易受噪声干扰且无法利用连续帧的连贯性。TTF-VLA的核心创新包括： - **双维度检测机制**：结合高效的灰度像素差异分析和基于注意力的语义相关性评估，识别关键变化区域。 - **自适应融合策略**：通过硬融合（二值选择）和关键帧锚定，选择性整合历史与当前视觉令牌，平衡时序连贯性与响应性，防止错误累积。 - **无需训练**：直接应用于现有VLA架构（如OpenVLA和VLA-Cache），模型无关。 实验验证显示一致性能提升： - **LIBERO基准**：平均任务成功率提高4.0个百分点（72.4% vs 68.4%基线）。 - **跨环境验证（SimplerEnv）**：相对改进4.8%。 - **真实机器人任务**：相对改进8.7%。 关键发现：选择性重用Query矩阵可提升性能，为KQV矩阵重用策略提供方向，实现计算加速与任务成功率提升。</details> |
| 2025-08-14 | CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model | http://arxiv.org/abs/2508.10416v1 | <details><summary>展开</summary>本文提出**CorrectNav模型**及**自我纠错飞轮（Self-correction Flywheel）训练范式**，解决视觉语言导航（VLN）模型因缺乏纠错能力而导致的轨迹偏离问题。核心要点如下： ### 1. **核心创新：自我纠错飞轮范式** - **四步闭环**： （1）在训练集评估模型，收集错误轨迹； （2）通过轨迹偏离检测算法定位偏差点（如距离阈值法）； （3）自动生成纠错数据： - **动作纠错**：基于偏离点生成恢复轨迹（如`Tₑ = (Mₜ → Gₖ₊₁ → … → Gₙ)`）； - **感知纠错**：利用多模态大模型（如Qwen-VL）分析关键帧，生成场景描述与问答对。 （4）混合采样后继续训练模型。 - **多轮迭代**：每一轮训练后重新评估模型，生成新纠错数据，形成性能持续提升的闭环。 ### 2. **模型架构与训练策略** - **基础模型**：基于LLaVA-Video 7B，由视觉编码器（SigLIP）、投影层（MLP）和语言模型（Qwen2）组成。 - **导航微调**： - **动作预测**：使用210万步R2R/RxR仿真数据，加入相机高度、光照等随机化增强鲁棒性。 - **指令生成**：基于轨迹反推语言指令（3万条）。 - **多模态召回**：引入ActivityNet-QA/NextQA数据（24万条）防止通用能力退化。 ### 3. **实验效果** - **仿真测试**： - **R2R-CE**：成功率65.1%（↑8.2% SOTA），SPL 62.3%。 - **RxR-CE**：成功率69.3%（↑16.4% SOTA），nDTW 75.2%。 - **消融实验**：移除纠错轨迹/关键帧感知/数据采样策略均导致性能显著下降（如R2R-CE SR↓3–5%）。 - **实物验证**： - 在办公/家居/校园场景中，复杂指令成功率提升40–65%（如家庭场景SR 0.80 vs NaVid 0.15）。 - 实现动态避障（图5c-d）、长指令纠偏（图5e-f）及室外导航（图5g）。 ### 4. **局限与展望** - **单目RGB局限**：难以精确感知机器人本体与环境的相对位置（如四足机器人腿部碰撞风险）。 - **未来方向**：融入机器人本体尺寸与状态信息作为导航先验。 ### 总结 本文通过**自我纠错飞轮机制**，将错误轨迹转化为训练资源，显著提升VLN模型的纠偏能力，在仿真与实物测试中均达到SOTA性能，为具身智能的鲁棒导航提供新范式。</details> |
| 2025-08-14 | Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning | http://arxiv.org/abs/2508.10399v1 | <details><summary>展开</summary>这篇论文全面综述了大模型赋能的具身智能体在决策与学习方面的研究进展。核心要点如下： 1. **研究背景** 具身智能旨在开发能与物理环境交互的智能体，是实现通用人工智能（AGI）的重要路径。大模型通过增强感知、规划与执行能力，显著提升了具身智能体的决策和学习效率。 2. **决策范式创新** - **分层决策**：大模型驱动高层任务规划（结构化/自然语言/编程语言规划）、低层执行（传统控制/学习驱动/模块化控制）和闭环反馈（自反思/人类/环境反馈） - **端到端决策**：视觉-语言-动作（VLA）模型直接映射多模态输入到动作，通过感知增强、轨迹优化和训练成本压缩提升性能 3. **具身学习方法** - 模仿学习：大模型增强扩散/Transformer策略网络 - 强化学习：大模型优化奖励函数设计（任务分解/奖励塑造）和策略网络构建（扩散/Transformer架构） - 迁移学习与元学习提升跨任务泛化能力 4. **世界模型创新** 首次将世界模型纳入具身智能框架，提出潜在空间/Transformer/扩散模型等设计方法，在决策验证和状态预测中发挥关键作用。 5. **挑战与展望** 指出数据稀缺、持续学习、计算效率、仿真-现实差距等核心挑战，强调多模态大模型与具身学习协同是未来重点方向。 该综述系统梳理了大模型赋能具身智能的最新进展，为通用具身智能体的发展提供了理论框架和技术路线参考。</details> |
| 2025-08-14 | ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver | http://arxiv.org/abs/2508.10333v1 | <details><summary>展开</summary>论文提出ReconVLA模型，解决现有视觉-语言-动作（VLA）模型视觉注意力分散的问题，核心贡献如下： 1. **重构式VLA框架** - 引入隐式 grounding 范式：通过扩散transformer重建图像中的"注视区域"（目标操作物体） - 监督机制：以VLA的视觉输出为条件，重建目标区域，迫使模型学习细粒度表征 - 效果：引导视觉注意力精确聚焦目标区域，提升操作精度 2. **大规模预训练数据集** - 构建超过10万条轨迹、200万样本的机器人数据集 - 利用Grounding DINO自动标注目标区域 - 增强视觉重建的泛化能力 3. **关键实验结果** - 隐式grounding优于显式grounding（EG）和思维链grounding（CG） - 注意力可视化显示注意力集中度提升（CALVIN基准成功率提高20.2%） - 在长视野任务（如"堆叠积木"）中表现突出，真实世界多任务成功率超90% - 预训练使模型对未见目标具泛化能力 4. **架构创新** - 双路径设计：动作路径输出离散动作token，重构路径恢复场景token - 冻结视觉tokenizer保留细节信息，实现高保真重建 论文通过仿真和真实实验验证了模型在精确操作和泛化能力上的优势，项目页面见：https://zionchow.github.io/ReconVLA/。</details> |
| 2025-08-12 | GeoVLA: Empowering 3D Representations in Vision-Language-Action Models | http://arxiv.org/abs/2508.09071v2 | <details><summary>展开</summary>GeoVLA提出了一种新颖的视觉-语言-动作（VLA）框架，通过整合3D几何信息增强机器人操作能力。核心创新包括： 1. **双路径架构设计** - **视觉语言分支**：利用预训练VLM（如Prismatic）处理RGB图像和语言指令，生成融合特征 - **几何分支**：通过定制化点嵌入网络（PEN）处理深度图转换的点云数据，提取以机械臂末端为中心的3D特征 2. **关键组件创新** - **PEN模块**：采用双路径Transformer（几何特征路径+旋转位置编码），选择性提取末端执行器锚点特征 - **3DAE动作专家**：基于扩散Transformer，通过静态路由的混合专家机制融合多模态特征，生成连续动作序列 3. **显著性能优势** - 仿真实验：在LIBERO和ManiSkill2基准上达到SOTA（分别超越前最佳2.4%和11%） - 真实世界任务：平均成功率86.3%，在高度适应/尺度感知/视角变化等场景展现强鲁棒性 - 消融实验：验证PEN的几何编码能力和3DAE多模态融合机制的有效性 4. **工程贡献** - 开源项目地址：https://linsun449.github.io/GeoVLA - 避免大规模3D指令微调需求，通过端到端训练保持预训练模型的知识迁移能力 该方法解决了传统VLA模型依赖2D视觉输入导致的几何信息缺失问题，为机器人操作任务提供了更精确的空间感知基础。</details> |
| 2025-08-12 | Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding | http://arxiv.org/abs/2508.09032v1 | <details><summary>展开</summary>本文提出了一种名为“空间轨迹”（Spatial Traces）的新方法，用于增强视觉-语言-动作（VLA）模型对时空信息的理解。核心创新点是通过视觉提示技术，将关键点的运动轨迹投影到深度图上，使模型能同时捕捉空间（深度信息）和时间（历史轨迹）特征。在 SimplerEnv 环境中进行实验，仅使用 52 条训练轨迹微调后，该方法（ST-VLA）的任务成功率比 SpatialVLA 提高 4%，比 TraceVLA 提高 19%，显著提升了复杂操作任务（如物体抓取和放置）的性能。该方法在数据有限的现实场景中具有高效性，未来可结合 3D 重建进一步优化。</details> |
| 2025-08-12 | OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing | http://arxiv.org/abs/2508.08706v2 | <details><summary>展开</summary>本文提出OmniVTLA模型，解决现有Vision-Language-Action（VLA）模型忽视触觉感知的问题，尤其在接触丰富的任务中性能不足。核心要点如下： 1. **问题与目标**： - 现有VLA模型依赖视觉和语言，但忽略触觉感知，导致接触密集型任务（如抓取）失败。 - 目标：整合视觉、触觉、语言和动作模态，提升机器人操作的鲁棒性和泛化能力。 2. **关键创新**： - **OmniVTLA架构**： - 双路径触觉编码器（dual-encoder path）：使用预训练视觉变换器（ViT）处理视觉数据，语义对齐触觉ViT（SA-ViT）处理触觉信号，解决传感器异质性。 - 通过对比学习对齐触觉、视觉和文本语义空间。 - **ObjTac数据集**： - 收集135K三模态样本（文本、视觉、基于力的触觉数据），涵盖56个对象、10个类别，补充现有数据集。 - **语义对齐训练**： - 利用ObjTac训练SA-ViT编码器，学习统一触觉表示，作为OmniVTLA的初始化。 3. **实验结果**： - **性能提升**： - 夹爪拾取任务：成功率96.9%（比VLA基线高21.9%）。 - 灵巧手任务：成功率100%（比基线高6.2%）。 - **效率优势**： - 任务完成时间减少24.2%，轨迹平滑性提升89.6%，符合“非接触快速移动，接触时减速”原则。 - **泛化能力**：在未见对象上表现优异，如塑料瓶和方瓶成功率100%。 4. **意义**： - 为触觉感知提供统一框架，增强机器人对物理交互的理解，推动多模态模型在真实场景的应用。</details> |
| 2025-08-11 | GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions | http://arxiv.org/abs/2508.07650v2 | <details><summary>展开</summary>论文提出GraphCoT-VLA模型，用于解决模糊指令下的机器人操作问题。核心要点如下： 1. **问题背景**：现有视觉-语言-动作（VLA）模型在处理模糊指令（如“我想吃辣味河鲜”）和未知环境状态时表现不佳，且感知局限于2D观察，缺乏3D空间交互建模能力。 2. **模型创新**： - **结构化CoT推理模块**：集成高层任务理解与规划、失败任务反馈、低层未来对象位置及动作想象推理，提升模糊指令解析能力。 - **3D姿态-对象图**：实时构建机器人关节与场景物体的空间拓扑关系（基于深度数据和运动学），增强3D交互感知。 - **Dropout混合推理策略**：训练时随机丢弃部分CoT监督，平衡推理深度与实时性，实现高效控制输出。 3. **实验结果**：在真实机器人任务（如食材准备和衣物选择）中，模型任务成功率（最高76.67%）和响应速度（≈10Hz）显著优于ACT、Diffusion Policy等基线，展现强泛化性和环境鲁棒性。</details> |
| 2025-08-07 | Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control | http://arxiv.org/abs/2508.05342v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction | http://arxiv.org/abs/2508.05294v2 | <details><summary>展开</summary>本文综述了大型语言模型（LLM）、视觉语言模型（VLM）及视觉语言动作模型（VLA）在实现具身智能体AI（Embodied Agentic AI）中的研究进展与分类体系，核心要点如下： 1. **核心概念** - **具身智能体AI**：区别于端到端学习或传统符号规划，强调LLM/VLM作为**智能中介**协调机器人系统（如ROS），通过自然语言理解、API调用和任务规划增强自主性，而非直接生成控制策略。 - **优势**：兼容现有机器人软件栈，提升灵活性、可解释性和人机交互能力。 2. **模型集成方法分类**（图1） - **协议集成**：模型作为协议翻译器（如`ros2ai`将自然语言转为ROS命令）。 - **接口集成**：模型作为交互层，动态分解任务并调用工具（如ROSA、RAI框架支持多工具循环调用）。 - **编排导向集成**：模型协调资源或多智能体（如AutoRT管理机器人集群任务分配）。 - **嵌入式集成**：模型直接输出动作或感知（端到端VLAs如RT-2、π0）。 3. **智能体角色分类**（图3） - **规划型**：生成高层任务序列（如SayCan通过LLM+价值函数选择动作）。 - **编排型**：管理多技能/多机器人协作（如LABOR Agent调度数百种操作技能）。 - **任务专用型**：解决特定任务（如BUMBLE处理开放世界移动操作）。 - **模型中心型**：统一架构处理多模态输入（如RoboCat泛化跨任务策略）。 - **通用型**：模块化架构组合技能库（如Voyager自主生成代码工具）。 - **通用系统型**：提供可扩展框架（如ROSA、OpenMind OM1支持工具定制）。 4. **关键趋势** - **工业与社区驱动**：除学术研究外，ROS社区工具（如ROS-MCP）和工业框架（OpenMind）推动实用化。 - **演进方向**：从ChatGPT初期的简单协议交互（2023）→ VLAs与多智能体协调（2024）→ 开放世界通用智能体（2025）。 5. **挑战与展望** - 需解决**环境 grounding**、**长时记忆**、**安全验证**及**评估标准**问题。 - 融合多集成方法和智能体角色是迈向通用具身智能的关键路径。 **总结**：论文提出双维度分类法（集成方法×智能体角色），系统化梳理了LLM/VLM驱动的机器人智能体架构，强调其作为"中间层"协调传统系统的设计范式，为具身智能发展提供技术路线参考。</details> |
| 2025-08-07 | Learning to See and Act: Task-Aware View Planning for Robotic Manipulation | http://arxiv.org/abs/2508.05186v1 | <details><summary>展开</summary>本文提出任务感知视角规划（TAVP）框架，解决机器人操作中静态视角和共享视觉编码器导致的3D感知不足与任务干扰问题。核心创新包括： 1. **多视角探索策略（MVEP）**：通过强化学习动态规划最优观测视角，主动获取目标物体和机械手的完整视觉信息，解决遮挡问题。 2. **任务感知专家混合模块（TaskMoE）**：引入门控机制，根据任务指令和场景特征动态选择专家网络，解耦不同任务的视觉特征表达，提升多任务泛化能力。 实验在RLBench的18项任务上验证： - TAVP平均成功率86.7%，超越固定视角方法（如RVT2的81.4%和ARP+的84.9%） - 在遮挡敏感任务（如"放入橱柜"）提升达56% - 消融实验表明移除MVEP会使性能骤降至8.89%，TaskMoE移除导致1.1%下降 框架通过动态视角规划和任务感知表征学习，显著提升复杂场景下的操作鲁棒性。</details> |
| 2025-08-07 | IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model | http://arxiv.org/abs/2508.06571v3 | <details><summary>展开</summary>本文提出IRL-VLA框架，通过奖励世界模型实现端到端自动驾驶的视觉-语言-动作策略训练。主要贡献如下： 1. **问题解决**：针对现有VLA模型依赖开环模仿学习（导致性能受限）和闭环训练依赖高保真仿真（存在域差距和计算效率问题）的挑战，提出三阶段框架： - **模仿策略学习**：设计新型VLA架构，包含语义推理模块（VLM场景理解）、3D推理模块（BEV空间几何推理）和统一扩散规划器（生成多样化轨迹） - **逆向环境学习**：构建轻量级奖励世界模型（RWM），通过逆向强化学习从多样化策略中学习多模态驾驶奖励函数，替代传统仿真器 - **RWM强化学习**：使用PPO算法，结合RWM生成的奖励和模仿学习损失微调策略，平衡安全性/舒适性/交通效率 2. **核心创新**： - 首个不依赖仿真器的闭环强化学习VLA框架 - RWM模型高效计算EPDMS评估指标（包含碰撞避免/可行驶区域合规等9项子分数） - 分层扩散规划器将驾驶过程建模为马尔可夫决策过程 3. **实验结果**： - NAVSIM v2端到端驾驶基准测试中达到SOTA性能（EPDMS 74.9） - CVPR2025自动驾驶挑战赛中获得亚军（EDPMS 45.0） - 消融实验验证：语义推理+3D推理+扩散规划器的组合提升EPDMS 4.4分；模仿学习损失权重0.5时效果最佳 该方法显著提升自动驾驶策略的泛化能力，为闭环VLA研究提供新范式。代码已开源：https://github.com/IRL-VLA/IRL-VLA</details> |
| 2025-08-06 | Static and Plugged: Make Embodied Evaluation Simple | http://arxiv.org/abs/2508.06553v1 | <details><summary>展开</summary>### 论文要点总结 论文针对具身智能（Embodied AI）评估的挑战，提出了一种新型静态基准测试 **StaticEmbodiedBench**。核心问题包括： 1. **当前评估的局限性**：依赖模拟环境（如Isaac Sim）或真实机器人部署，导致硬件成本高、数据量大（单任务可达1GB）、部署复杂，且现有基准（如ALFRED、EmbodiedEval）仅评估特定模型类型（如VLMs或VLAs），缺乏统一框架。 2. **解决方案**： - 引入**静态关键帧评估方法**，仅保留任务关键帧（如目标识别或抓取时刻），避免模拟需求。 - 设计 **cerebrum–cerebellum 协作框架**： - **VLM（cerebrum）** 评估认知能力（宏观规划、微观感知、阶段推理），支持第一人称和第三人称视角。 - **VLA（cerebellum）** 评估执行能力（位置、方向、末端执行器控制），使用7-DoF动作向量与专家轨迹比较。 - 构建 **StaticEmbodiedBench** 基准：覆盖42个场景和8个维度，仅需静态图像和文本元数据，实现即插即用评估。数据轻量（单样本<1MB），支持统一评估。 3. **主要贡献**： - 提出首个统一静态评估框架，涵盖19个VLMs和11个VLAs的全面评测，建立首个具身智能静态排行榜。 - 验证 **静态到动态（S2D）差距**：S2D相关系数达0.66，证明静态评估与实际性能强相关。 - 发布200个样本子集，加速研究发展。 4. **实验结果**： - VLM评估（如InternVL2.5-78B-MPO得分最高）显示模型在宏观规划强，但阶段推理弱。 - VLA评估（如Octo-base-1.5最优）揭示轻量模型在隔离评估中表现优异。 - 成本对比：静态评估硬件成本为0，准备时间分钟级，远低于模拟或真实部署。 论文通过静态方法显著降低评估门槛，推动具身智能标准化发展。</details> |
| 2025-08-06 | A tutorial note on collecting simulated data for vision-language-action models | http://arxiv.org/abs/2508.06547v1 | <details><summary>展开</summary>本文是关于视觉-语言-动作（VLA）模型模拟数据收集的教程笔记。核心要点总结如下： 1. **VLA模型概述**： VLA模型通过单一神经网络统一处理视觉观察、语言指令和机器人动作，取代了传统模块化机器人系统。其成功依赖高质量数据集，需捕捉视觉-语言-动作的复杂关联。 2. **三种数据收集方法**： - **PyBullet模拟框架**： 基于物理引擎和Ravens任务套件，提供灵活可控的数据生成。支持定制任务（如积木插入、颜色分类），生成高质量演示数据（RGB图像、深度图、动作指令），以标准化格式存储。 - **LIBERO基准套件**： 结合MuJoCo物理引擎，支持标准化任务评估和人类遥操作的自定义数据收集。通过BDDL文件定义任务场景，可添加干扰对象增强鲁棒性，数据以HDF5格式聚合。 - **RT-X数据集**： 大规模真实机器人数据集，整合22种机器人平台的100万条轨迹。采用标准化7维动作表示（位置+抓取状态），支持跨具身学习，覆盖基础操作（如拾取）和复杂任务（如工具使用）。 3. **关键比较与结论**： - **PyBullet**：数据质量高但限于仿真环境。 - **LIBERO**：平衡真实性与可控性，适合任务定制。 - **RT-X**：规模与多样性突出，适用于跨平台泛化。 数据质量、规模和多样性是VLA训练的核心考量，代码与数据已开源（GitHub链接见原文）。</details> |
| 2025-08-04 | MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming | http://arxiv.org/abs/2508.02549v1 | <details><summary>展开</summary>这篇论文提出了MonoDream框架，用于解决单目视觉语言导航（VLN）的性能瓶颈问题。核心贡献包括： 1. **统一导航表示（UNR）** 设计共享的潜在空间，联合对齐导航相关的视觉语义（全局布局、深度、未来线索）和语言指导的动作意图，实现更可靠的动作预测。 2. **潜在全景想象（LPD）任务** 通过多任务协同训练，监督模型仅基于单目输入预测当前及未来步骤的全景RGB-D观测的潜在特征。包含四个子任务： - 当前全景RGB（PI）和深度（PD）特征预测 - 未来全景RGB（FPI）和深度（FPD）特征预测 3. **性能优势** - 在VLN-CE基准测试（R2R-CE/RxR-CE）上达到单目SOTA，显著缩小与全景方法的差距（R2R-CE上SPL 49.1 vs 全景方法50.0） - 跨数据集测试（RxR-CE）显示强泛化能力（SR 49.4） - 仅需2B参数，推理速度0.8秒/步（比7B模型快33%） - 无需外部训练数据，仅用模拟器数据实现高效训练 消融实验验证LPD是关键创新点，移除后SPL下降9.7个百分点。该方法首次通过潜在全景监督赋予单目代理全局场景理解和未来预测能力，解决了单目视野受限的核心挑战。</details> |
| 2025-08-04 | CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning | http://arxiv.org/abs/2508.02219v1 | <details><summary>展开</summary>本文提出**CO-RFT**算法，通过**分块离线强化学习**高效微调视觉-语言-动作（VLA）模型，解决现有方法在样本效率、动作分块兼容性和训练稳定性上的不足。核心要点如下： ### 1. **核心方法** - **分块强化学习框架**： - 将时序差分（TD）学习扩展至**动作分块**（一次预测多步动作），提升样本效率与稀疏奖励处理能力。 - **分块Critic网络**：基于Transformer架构，输入当前状态和动作块，预测多步Q值序列，优化动作块决策。 - **两阶段微调（CO-RFT）**： - **阶段1（模仿学习）**：全参数微调VLA模型，适配新机器人平台。 - **阶段2（离线RL）**：结合动作分块的CalQL算法优化策略，引入保守正则化缓解分布偏移问题。 ### 2. **关键优势** - **高效样本利用**：仅需30-60个演示样本。 - **兼容动作分块**：保留VLA模型输出多步动作的特性，提升动作连贯性。 - **奖励上采样技术**：缓解稀疏奖励问题，增强价值学习。 ### 3. **实验结果** - **性能提升**：在真实机械臂任务中，相比监督微调（SFT）： - 平均**成功率提升57%**（部分任务达100%）。 - 平均**周期时间减少22.3%**。 - **泛化能力**：在未知位置测试时成功率**达44.3%**，数据多样性（随机初始化）对泛化影响显著。 ### 4. **意义** - 为VLA模型微调提供样本高效的离线RL方案，避免在线RL的复杂基础设施需求。 - 首次实现动作分块与强化学习的兼容，为长视野、稀疏奖励任务提供新思路。 > 总结：CO-RFT通过分块强化学习和两阶段微调，显著提升VLA模型在真实机器人任务中的性能与泛化能力，同时降低样本需求和推理延迟。</details> |
| 2025-08-04 | FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation | http://arxiv.org/abs/2508.02190v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models | http://arxiv.org/abs/2508.02062v1 | <details><summary>展开</summary>论文提出**RICL（Retraining for In-Context Learning）方法**，旨在为预训练的视觉-语言-动作模型（VLA）注入上下文学习（ICL）能力，使其无需参数微调即可适应新任务。核心要点如下： 1. **问题背景**： - 现有VLA模型（如π₀-FAST）虽能处理多任务，但**缺乏ICL能力**，用户需通过微调模型参数来适应新任务，过程复杂。 2. **RICL解决方案**： - **核心思路**：通过特定微调方法，使预训练VLA模型能够利用**检索增强生成（RAG）** 和**少量演示数据**（10-20条）进行上下文学习。 - **实现步骤**： - 收集少量“引导演示数据”（20个任务的400条演示），将查询状态与检索到的相似状态-动作对拼接为输入序列。 - 引入**动作插值层**（公式1），结合检索动作与模型预测结果，提升动作生成质量。 - 仅微调语言模型部分，冻结视觉编码器（图2架构）。 - **部署**：用户提供新任务演示后，RICL-VLA自动检索相关片段注入上下文，实现零参数更新的任务适应。 3. **实验结果**： - 在7项新任务（涉及新物体、新动作、新场景）上，RICL-VLA（π₀-FAST-DROID为基础）显著优于原始模型： - **零参数更新**：任务成功率从基线的2.5%提升至31.25%（图4）。 - **支持微调**：若允许在新任务数据上微调，成功率进一步提升至61.67%，且效果优于直接微调原始模型。 - 案例验证（图1）：原始模型在处理精灵球抓取、刮水器拖动等新任务时失败，而RICL-VLA仅通过20条演示即成功适应。 4. **贡献与意义**： - 首次实现VLA模型的**即插即用式上下文学习接口**，用户仅需提供少量演示即可提升模型表现。 - 开源代码与模型权重，推动机器人任务适配的简易化（官网链接见脚注）。 5. **局限与未来**： - 当前主要适用于拾放类任务，对复杂动作（如网球挥拍）泛化有限。 - 未来方向：扩展引导数据规模、结合人类视频演示、增强动作多样性。 **关键词**：视觉-语言-动作模型（VLA）、上下文学习（ICL）、检索增强生成（RAG）。</details> |
| 2025-07-31 | XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation | http://arxiv.org/abs/2508.00097v1 | <details><summary>展开</summary>论文提出XRoboToolkit框架，用于解决机器人遥操作中的数据收集问题，以支持视觉语言动作模型（VLA）训练。核心要点如下： 1. **问题背景**：当前遥操作方法（如领导者-跟随器、视觉或VR方案）存在可扩展性差、设置复杂、数据质量低及延迟高等问题，难以满足VLA模型对大规模高质量数据的需求。 2. **解决方案**：开发基于OpenXR标准的跨平台框架XRoboToolkit，特点包括： - **低延迟立体视觉反馈**：支持PICO 4 Ultra和ZED Mini相机，优化视频流管道，减少运动不适。 - **多模态跟踪**：兼容头部、控制器、手部姿态及辅助运动跟踪器，提供90Hz高频率数据流。 - **优化控制算法**：采用二次规划（QP）逆运动学确保平滑运动，结合灵巧手重定向实现精细操作。 - **模块化架构**：无缝集成物理机器人（如UR5、ARX R5、Galaxea移动机器人）和仿真环境（如MuJoCo），通过Unity应用简化配置。 3. **验证与应用**： - **实验验证**：视频流延迟低至82ms（ZED-PICO方案），优于现有方案（如Open-TeleVision）；数据收集用于VLA微调，实现100%任务成功率。 - **应用场景**：涵盖精密操作（如螺丝插入）、双机械臂协同、移动机器人导航及灵巧手仿真控制。 4. **局限与未来方向**：当前依赖PICO的24关节全身模型，缺乏OpenXR标准化；未来将扩展仿真支持（如Roboverse）并改进欠驱动系统的手部重定向。</details> |
| 2025-07-31 | villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models | http://arxiv.org/abs/2507.23682v3 | <details><summary>展开</summary>论文提出villa-X框架，通过改进视觉-语言-动作（VLA）模型中的潜在动作建模，提升机器人策略的泛化能力。核心贡献包括： 1. **潜在动作模型（LAM）改进** - 引入本体感知前向动力学模型（proprio-FDM），通过预测机器人未来状态和动作，将潜在动作与物理动力学对齐，解决纯视觉方法忽略细微动作（如夹爪旋转）的问题。 - 添加本体上下文（$c_e$）区分异构数据，提升跨具身泛化能力。 2. **执行器模块（ACT）设计** - 提出分层策略架构：VLM编码视觉语言输入，ACT-latent生成潜在动作计划，ACT-robot基于潜在动作生成机器人动作。 - 采用联合扩散建模，通过注意力掩码策略防止动作分支过度依赖潜在动作。 3. **实验验证** - **LAM有效性**：proprio-FDM提升潜在动作质量（图3），在SIMPLER基准测试中平均性能超越基线（表1）。 - **零样本泛化**：潜在动作专家可生成未见具身（Realman机械臂）和开放词汇符号的规划（图4）。 - **整体性能**：在SIMPLER仿真任务中，Google机器人任务平均成功率77.7%，WidowX机器人62.5%（表2）；实物实验显示在夹爪和灵巧手任务中均优于GR00T等基线（表3-4）。 4. **优势** - 通过大规模预训练，实现跨具身和开放场景的零样本泛化。 - 为学习通用机器人策略提供可扩展范式，代码已开源。 局限性与未来工作包括进一步探索潜在动作专家的规划验证能力。</details> |
| 2025-07-31 | A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving | http://arxiv.org/abs/2507.23540v1 | <details><summary>展开</summary>论文提出一个统一的感知-语言-动作（PLA）框架，用于提升自动驾驶系统在复杂环境中的适应性、鲁棒性和可解释性。该框架整合多传感器融合（相机、LiDAR、雷达）与大型语言模型（LLM，如GPT-4.1）增强的视觉-语言-动作（VLA）架构，通过三层结构实现： 1. **感知层**：处理传感器数据生成结构化场景描述（如物体位置、速度）。 2. **语言层**：基于LLM进行上下文推理和风险分析，输出可解释的驾驶命令。 3. **动作层**：执行轨迹规划与控制，并通过数字孪生验证安全性。 在城市交叉口施工区场景（nuScenes数据集）的评估中，框架在速度预测（MAE 0.39 m/s）、轨迹跟踪（ADE 1.013m）方面表现优异，但转向角预测（MAE 2.52°）需优化。结果突显语言增强框架在提升安全性和泛化能力方面的潜力，未来工作将聚焦转向精度改进和场景扩展。</details> |
| 2025-07-31 | FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning | http://arxiv.org/abs/2507.23318v3 | <details><summary>展开</summary>论文提出FastDriveVLA框架，用于高效端到端自动驾驶的视觉语言动作（VLA）模型。核心创新点包括： 1. **基于重建的令牌修剪**：设计即插即用模块ReconPruner，通过MAE风格像素重建优先保留前景信息（如车辆、行人、车道），并引入对抗性前景-背景重建策略防止模型退化。 2. **专用数据集**：构建nuScenes-FG数据集（241K图像-掩码对），标注驾驶场景的前景区域，支撑模型训练。 3. **高效部署**：ReconPruner轻量（0.07B参数），训练后可无缝集成不同VLA模型，无需重训练，显著降低计算开销。 4. **性能优势**：在nuScenes开放环规划基准上，以50%令牌修剪率实现SOTA（L2误差降1.7%，碰撞率降4.2%），推理速度提升3.7倍。 方法解决了现有注意力/相似性修剪在自动驾驶中的局限性，强调前景信息对决策的关键性。</details> |
| 2025-07-30 | Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance | http://arxiv.org/abs/2507.22424v2 | <details><summary>展开</summary>论文提出**Spec-VLA框架**，用于加速视觉-语言-动作（VLA）模型的推理。核心要点如下： 1. **问题背景**：VLA模型因参数规模大和自回归解码导致计算开销高，现有加速方法（如早期退出或Jacobi解码）需微调或牺牲性能。 2. **解决方案**：引入**推测解码（SD）框架**，采用草稿模型高效生成令牌，验证模型并行校验。直接应用SD在VLA任务中加速有限（仅1.08–1.15倍），因动作预测复杂且贪婪解码严格。 3. **关键创新**：提出**放松接受机制**，利用VLA动作令牌的离散化特征（如位置差），允许相似令牌被接受，放宽阈值（如距离≤9），减少验证失败。 4. **实验结果**：在LIBERO基准测试中，接受长度提升26–44%（达2.94令牌/步），速度提升1.22–1.42倍，成功率与基线相当。消融实验验证阈值放宽的有效性。 5. **意义**：首次将SD成功应用于VLA领域，为多模态模型加速提供新方向，代码已开源。</details> |
| 2025-07-23 | InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation | http://arxiv.org/abs/2507.17520v1 | <details><summary>展开</summary>这篇论文提出了**InstructVLA**模型，通过**视觉-语言-动作指令微调（VLA-IT）** 解决现有视觉-语言-动作（VLA）模型在适应机器人操作任务时出现的**多模态能力遗忘**问题。核心贡献如下： 1. **模型架构与训练范式** - 提出**两阶段训练框架**： - **动作预训练**：将语言运动描述蒸馏为潜在动作表示，对齐视觉语言模型（VLM）与操作任务。 - **VLA-IT微调**：通过混合专家（MoE）适配器动态切换文本推理与动作生成，联合优化多模态理解和操作能力。 - 设计**流模型动作专家**，将潜在动作解码为连续动作空间，支持闭环控制。 2. **数据集与评测基准** - 构建**650K样本的VLA-IT数据集**，包含场景描述、问答对、指令改写和上下文推理四类标注，增强语言-动作对齐。 - 提出**SimplerEnv-Instruct评测基准**（80个零样本任务），涵盖指令聚合（50任务）和情境推理（30任务）两大层级，评估复杂指令泛化能力。 3. **关键实验结果** - **操作性能**：在SimplerEnv任务上超越SpatialVLA 30.5%，在SimplerEnv-Instruct上超越微调版OpenVLA 92%，超越GPT-4o辅助的专家模型29%。 - **多模态保持**：在MMMU、MMB等12个视觉问答基准上优于同等规模VLMs（如LLaVA-OV），证实有效缓解灾难性遗忘。 - **实时交互**：在WidowX和Franka机器人实机测试中，零样本/少样本情境推理任务成功率提升46.7%。 4. **创新机制** - **推理时思维链（Think）**：通过语言响应缓存机制，在动作生成间隙插入语义推理，提升复杂任务规划能力（36.1%性能增益）。 - **数据缩放效应**：VLA-IT数据量每增加25%，情境推理任务性能呈对数级提升，验证语言能力对操作的迁移性。 该方法为统一多模态理解与操作提供了可扩展框架，项目网站见：https://yangs03.github.io/InstructVLA_Home/。</details> |
| 2025-07-23 | ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents | http://arxiv.org/abs/2507.17462v1 | <details><summary>展开</summary>本文提出ERMV框架，用于编辑4D机器人多视角时序图像（空间多视角+时间维度），以增强具身智能体（如VLA模型）的训练数据。核心创新点包括： 1. **视觉引导机制** 使用单帧编辑图像作为精确的视觉指引，替代模糊的文本提示，确保编辑意图的准确性。 2. **稀疏时空模块（SST）** 通过时空解耦和稀疏采样，将长时序多视角编辑转化为单帧多视角问题，显著降低计算开销，支持大时间窗口处理（单消费级GPU可训练）。 3. **极线运动感知注意力（EMA-Attn）** 先学习运动导致的像素偏移，再应用几何约束，有效解决动态场景中运动模糊导致的几何一致性难题，保持跨视角/跨时序的时空一致性。 4. **反馈干预机制** 引入多模态大语言模型（MLLM）自动检测关键物体（如机械臂）的编辑一致性，仅在必要时请求专家提供掩码指导，避免自回归误差累积。 实验验证： - **数据增强效果**：在仿真（RoboTwin）和真实场景（RDT数据集、双机械臂平台）中，ERMV增强的数据显著提升VLA模型的泛化性和鲁棒性（任务成功率提升）。 - **仿真转真实**：将仿真图像编辑为真实风格，有效缩小Sim-to-Real差距。 - **效率优势**：稀疏采样策略相比稠密视频模型计算成本降低90%，支持千帧级序列编辑。 代码开源地址：https://github.com/IRMVLab/ERMV **核心贡献**：首次实现动态多视角4D机器人数据的高效编辑，解决数据稀缺问题，为具身智能提供可扩展的数据增强方案。</details> |
| 2025-07-23 | Confidence Calibration in Vision-Language-Action Models | http://arxiv.org/abs/2507.17383v1 | <details><summary>展开</summary>本文研究了视觉-语言-动作（VLA）模型在机器人任务中的置信度校准问题，核心贡献如下： 1. **任务成功与校准的关联性** 通过多数据集和VLA变体的基准测试，首次系统验证了任务成功率与校准误差（如ECE）呈正相关关系，表明高性能模型通常具有更好的置信度校准效果。 2. **提示集成方法** 提出轻量级的**提示集成**技术：对同一指令生成多个语义等效的改写版本，聚合各版本的置信度预测（如取平均）。该方法无需重新训练，即能显著提升校准效果，平均降低预期校准误差（ECE）超20%。 3. **时间维度的校准分析** 揭示任务执行过程中置信度的动态特性：**初始阶段置信度可靠性最低**，而**任务进展中期（如执行到30%-70%时）置信度最准确**。这为高风险场景的干预时机提供了依据。 4. **动作维度缩放校准** 发现不同动作维度（如机械臂平移、旋转）存在系统性校准偏差（部分过自信/欠自信）。提出**动作维度独立的Platt缩放**方法，对每个维度单独进行标定，显著提升整体置信度估计的可靠性。 **研究意义**：通过建立VLA模型校准评估框架及高效优化方法，为机器人安全决策提供可信的置信度依据，推动VLA在开放场景的可靠部署。</details> |
| 2025-07-23 | VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback | http://arxiv.org/abs/2507.17294v2 | <details><summary>展开</summary>待生成</details> |
| 2025-07-22 | ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning | http://arxiv.org/abs/2507.16815v2 | <details><summary>展开</summary>本文提出ThinkAct框架，用于视觉-语言-动作（VLA）推理任务的核心要点如下： 1. **问题与动机** - 现有VLA模型端到端映射输入到动作，缺乏显式推理，导致多步规划和复杂任务适应能力受限。 - 传统监督微调方法依赖高质量推理轨迹标注，成本高且易过拟合；强化学习方法受限于QA式奖励，难以支持长时规划和动作衔接。 2. **ThinkAct框架设计** - **双系统架构**： - **推理MLLM**（基于Qwen2.5-VL）：通过强化学习生成具身推理计划，输出视觉规划潜变量 \( c_t \)。 - **动作模型**（DiT-based）：以 \( c_t \) 为条件预测可执行动作序列。 - **动作对齐的视觉奖励**： - 目标奖励 \( r_{\text{goal}} \)：对比预测轨迹起点/终点与真实轨迹的匹配度。 - 轨迹奖励 \( r_{\text{traj}} \)：通过DTW距离约束预测轨迹的物理合理性。 - 结合格式正确性奖励，总奖励 \( r = 0.9(r_{\text{goal}} + r_{\text{traj}}) + 0.1r_{\text{format}} \)。 - **强化学习优化**：采用GRPO算法，采样多样化响应并最大化奖励信号，引导MLLM生成结构化推理步骤。 3. **关键创新** - **视觉潜变量规划**：将推理步骤压缩为紧凑的潜变量 \( c_t \)，桥接高层推理与低层动作执行。 - **异步执行机制**：每个 \( c_t \) 对应 \( N \) 步动作执行，支持"慢思考、快控制"。 - **数据利用**：融合机器人轨迹（OXE数据集）和人类视频（Something-Something v2），结合VQA数据（RoboVQA、EgoPlan）增强泛化能力。 4. **实验验证** - **任务基准**： - 机器人操作：SimplerEnv（视觉匹配/变体聚合）、LIBERO（长时规划）。 - 具身推理：EgoPlan-Bench2、RoboVQA、OpenEQA。 - **核心优势**： - **长时规划**：LIBERO-Long任务成功率70.9%，优于CoT-VLA（69.0%）。 - **少样本适应**：仅10样本微调，LIBERO-Goal任务成功率87.1%，提升7.3%。 - **自校正能力**：通过视频片段输入检测执行失败（如物体掉落）并实时重新规划。 - **多任务泛化**：EgoPlan-Bench2准确率48.2%，RoboVQA BLEU得分59.8%，均达SOTA。 5. **性能总结** - 在SimplerEnv和LIBERO上分别超越基线方法15.5%和7.6%，验证视觉潜变量规划对动作执行的有效引导。 - 消融实验表明：去除轨迹奖励 \( r_{\text{traj}} \) 导致性能下降1.6%，证明物理合理性约束的关键作用。 项目页面：https://jasper0314-huang.github.io/thinkact-vla/</details> |
| 2025-07-21 | Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos | http://arxiv.org/abs/2507.15597v1 | <details><summary>展开</summary>论文提出了一种名为**Being-H0**的视觉-语言-动作模型（VLA），通过大规模人类视频数据预训练，实现灵巧操作技能的迁移。核心创新点包括： 1. **物理指令微调（Physical Instruction Tuning）** - 将人类手部视为“基础操作器”，通过三阶段训练范式： - **预训练**：从人类视频中学习手部动作生成（如抓取、操纵物体） - **物理空间对齐**：统一异构数据（运动捕捉、VR、RGB视频）的3D坐标系 - **后训练**：适配机器人形态差异（如关节自由度差异） 2. **部件级运动分词（Part-Level Motion Tokenization）** - 提出基于分组残差量化（GRQ）的方法，将连续手部运动（MANO参数）离散化为语言兼容的token - 实现**毫米级重建精度**（如关节角度误差<1mm），支持自回归生成 3. **UniHand数据集** - 整合多源数据（150M样本），涵盖150+任务 - 数据流水线包括：手部姿态标准化、任务描述标注、弱透视投影对齐 4. **模型架构与效果** - 统一的自回归框架处理视觉、语言、动作token - 实验显示： - 在长序列动作生成任务上超越基线模型（FID提升20%） - 预训练模型迁移至机器人操作任务时成功率提升35% **贡献总结**：首次实现基于显式动作建模的大规模人类视频预训练，为灵巧操作机器人提供数据高效的解决方案，弥合了人类演示与机器人执行的鸿沟。</details> |
| 2025-07-21 | GR-3 Technical Report | http://arxiv.org/abs/2507.15493v2 | <details><summary>展开</summary>本文介绍了GR-3技术报告的核心内容，要点总结如下： 1. **模型概述** GR-3是一个大规模视觉-语言-动作（VLA）模型，通过端到端方式控制双臂移动机器人。其架构基于预训练视觉语言模型（Qwen2.5-VL-3B-Instruct），采用混合Transformer设计，结合动作扩散Transformer（DiT）预测动作块，通过流匹配（flow-matching）实现动作生成。 2. **核心能力** - **强泛化性**：能处理未见过的物体、环境和涉及抽象概念（如空间关系、常识推理）的指令 - **高效微调**：仅需少量人类轨迹数据（VR设备采集）即可快速适应新场景 - **长视程任务**：在复杂任务（如桌面清理、布料操作）中表现鲁棒可靠 - **双手灵巧操作**：支持双手协同和移动操作 3. **训练方法** - **三阶段数据融合**： (1) 机器人轨迹数据模仿学习 (2) 网络级视觉语言数据协同训练（增强语义理解） (3) 人类轨迹数据少样本微调 - **关键优化**：引入RMSNorm提升训练稳定性，添加任务状态监督增强指令跟随能力 4. **硬件系统** 配套开发ByteMini机器人： - 22自由度双臂移动平台，具备球形腕关节设计提升灵活性 - 全身柔顺控制框架支持动态交互 - 多视角相机系统（头/腕部）提供环境感知 5. **实验验证** 在三大任务中超越基线模型π₀： - **物体抓放**：在未见物体/环境/指令下成功率提升37%（77.1% vs 40%） - **长视程桌面清理**：指令跟随准确率达97.5%，支持多目标多容器操作 - **布料操作**：平均任务进度达86.7%，适应布料位置和形态变化 6. **应用前景** 为通用机器人助手提供技术基础，未来计划结合强化学习提升复杂场景鲁棒性。 GR-3通过多模态协同训练和高效适应机制，推动了通用机器人策略的发展。</details> |
| 2025-07-18 | VLA-Mark: A cross modal watermark for large vision-language alignment model | http://arxiv.org/abs/2507.14067v2 | <details><summary>展开</summary>本文提出了一种名为VLA-Mark的跨模态水印框架，专为视觉-语言对齐大模型（VLAMs）设计，旨在保护知识产权同时保持多模态语义一致性。以下是核心要点： 1. **问题与动机** - 现有文本水印方法破坏视觉-文本对齐：随机词汇划分（如"绿名单"）会抑制视觉关键语义词（如物体/场景描述），导致多模态一致性下降。 - 静态水印策略无法适应生成过程中的不确定性变化，且缺乏对视觉关键语义的保护机制。 2. **核心创新** - **多尺度语义显著性指标**：结合局部块亲和度（LPA）、全局语义连贯性（GSC）和跨模态上下文显著性（CCS），利用视觉语义指导水印词汇划分，避免随机划分的偏差。 - **熵调节分区机制**：根据生成熵值动态调整水印强度（低熵时优先保留语义关键词，高熵时增强水印鲁棒性），平衡语义保真度与水印可检测性。 - **语义关键词（SCT）保护**：通过跨模态嵌入对齐识别视觉锚定词汇，建立分层防御体系，抵抗改写、同义词替换等攻击。 3. **技术优势** - **零训练开销**：直接利用VLAMs固有的跨模态对齐层（如LLaVA的视觉投影模块），无需模型微调。 - **语义一致性**：实验显示比传统方法降低7.4%困惑度（PPL↑），提升26.6% BLEU分数，保持文本-视觉对齐。 - **强鲁棒性**：在翻译、改写等攻击下保持96.1%检测率（AUC 98.8%），显著优于基线（如DiP下降至69.78% AUC）。 4. **实验结果** - 在LLaVA-v1.5等4个VLAMs上验证：水印检测准确率（ACC）达98.1%-99.8%，语义相似度（STS）92.13%，优于所有基线。 - 消融实验证明多尺度指标互补性：移除LPA导致PPL上升15.9%，移除GSC显著降低BertScore。 5. **应用价值** - 为多模态内容生成提供首个视觉对齐的水印方案，解决传统方法在跨模态场景中的根本缺陷。 - 代码已开源（https://github.com/shiningwhite-cmd/VLA-mark）。 > 总结：VLA-Mark通过视觉语义引导的水印注入、熵自适应机制和SCT保护，实现了跨模态水印的突破性平衡——在保持文本质量的同时提升检测鲁棒性，为VLAMs的知识产权保护设立新标准。</details> |
| 2025-07-18 | EdgeVLA: Efficient Vision-Language-Action Models | http://arxiv.org/abs/2507.14049v1 | <details><summary>展开</summary>论文提出EdgeVLA（EVLA），一种高效的视觉-语言-动作模型，旨在解决大型VLA模型在资源受限边缘设备上部署的挑战。核心创新包括：（1）消除末端执行器位置预测的自回归要求，实现7倍推理加速；（2）采用小型语言模型（如Qwen2-0.5B），结合SigLIP和DINOv2视觉编码器，在降低计算需求的同时保持模型性能。实验结果在BridgeData V2和OpenX数据集上显示，EVLA训练性能与OpenVLA相当，但推理速度显著提升（从20ms降至5ms），内存占用减少75%（从16GB降至4GB）。模型和代码已开源以促进边缘机器人应用研究。</details> |
| 2025-07-17 | AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation | http://arxiv.org/abs/2507.12768v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-16 | EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos | http://arxiv.org/abs/2507.12440v3 | <details><summary>展开</summary>这篇论文提出了一种名为EgoVLA的视觉-语言-动作模型，其核心创新点是通过人类第一视角视频学习操作技能，并将其迁移至人形机器人。主要贡献如下： 1. **方法架构** - 提出**EgoVLA模型**：基于NVILA-2B视觉语言模型构建，输入包含视觉历史帧（6帧RGB）、语言指令和本体感觉状态 - 统一动作空间：使用**手腕位姿（3D平移+旋转）**和**MANO手部参数（15维PCA）**作为人类与机器人的共享动作表示 - 训练流程：先在人类视频数据集预训练，再用少量机器人演示微调 2. **数据集与基准** - 构建**大规模人类操作数据集**：整合HOI4D、HOT3D、HoloAssist和TACO四个来源，共50万图像-动作对 - 提出**Ego Humanoid Manipulation Benchmark**：在NVIDIA IsaacSim中设计12项双手操作任务（7项短时程+5项长时程），包含100条演示/任务 3. **关键技术** - **动作转换机制**：通过逆运动学将预测的人体手腕位姿转换为机器人末端位姿，并训练轻量MLP将MANO关键点映射至机器人关节命令 - **跨域适应**：设计几何变换对齐人类与机器人动作空间，实现预训练模型向目标域的迁移 - **多模态处理**：利用动作查询令牌预测未来30步（1秒）的动作序列 4. **实验结果** - 在仿真基准测试中，EgoVLA显著优于基线模型： - 短时程任务平均成功率77.78%（可见场景）→69.11%（新场景） - 长时程任务平均成功率45.93%（可见场景）→28.79%（新场景） - 关键优势：人类视频预训练提升**细粒度操作能力**（如翻转杯子、堆叠罐头）和**跨场景泛化性**，尤其在长时程任务中比无预训练模型高20%成功率 5. **局限性与应用** - 需少量机器人演示微调（零样本迁移失败） - 动作转换引入微小误差（指尖位置误差5×10⁻⁵m） - 为大规模机器人操作学习提供新范式，减少对真实机器人数据的依赖 该方法通过统一动作空间桥接人类与机器人操作技能，为利用丰富人类视频数据训练通用操作策略开辟了新途径。</details> |
| 2025-07-14 | Vision Language Action Models in Robotic Manipulation: A Systematic Review | http://arxiv.org/abs/2507.10672v1 | <details><summary>展开</summary>这篇系统综述论文聚焦于视觉-语言-动作（VLA）模型在机器人操作领域的发展，核心要点如下： 1. **研究范畴与意义** VLA模型旨在统一视觉感知、语言理解和动作控制，推动机器人自主执行自然语言指令的复杂任务。论文系统分析了102个VLA模型、26个数据集和12个仿真平台，揭示该领域的快速发展趋势（2022-2025年模型数量激增）。 2. **核心架构分类** - **模型架构**：提出VLA模型的结构化分类体系，包括视觉编码器（如ViT、CLIP）、语言编码器（如LLaMA、T5）与动作解码器（扩散策略、Transformer头）的融合范式。 - **创新设计**：重点分析RT-2、Octo等标志性模型的端到端学习框架，强调多模态对齐和跨任务泛化能力。 3. **数据集评估框架** 引入基于任务复杂度（\(\mathcal{C}_{\text{task}}\))和模态丰富度（\(\mathcal{C}_{\text{mod}}\))的二维量化评估体系（图10），发现现有数据集（如Open X-Embodiment、DROID）在**高复杂度任务与多模态融合**方面存在缺口。 4. **仿真平台作用** 评估Habitat、Isaac Gym等仿真工具在生成大规模训练数据、促进"仿真-现实"迁移及支持多样化任务方面的有效性，指出当前平台在物理精度和语言接口集成上的不足。 5. **挑战与未来方向** - **架构挑战**：词汇对齐、多模态融合、机械臂运动平滑性 - **数据挑战**：标注成本、模态不平衡、场景真实性 - **仿真挑战**：物理引擎精度、视觉-计算效率权衡 - **关键路径**：模块化设计、可扩展预训练协议、统一语言接口 论文同时开源了[模型/数据集/仿真平台汇总库](https://github.com/Muhayyuddin/VLAs)，为领域研究提供技术参考与路线图。 --- 注：总结基于论文HTML原文的标题、摘要、核心章节（第1/3/4/5/8节）及图表说明，聚焦主要贡献与结论。</details> |
| 2025-07-12 | Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization | http://arxiv.org/abs/2507.09160v1 | <details><summary>展开</summary>论文HTML原文截断，仅显示LaTeX命令定义（如argmax、argmin、sign、Tr），未提供正文内容，无法总结要点。</details> |
| 2025-07-07 | VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting | http://arxiv.org/abs/2507.05116v3 | <details><summary>展开</summary>本文提出VOTE方法，解决现有视觉-语言-动作（VLA）模型的两大缺陷： 1. **计算开销过大**：传统VLA模型需生成大量动作令牌或依赖扩散过程，导致高推理延迟（如OpenVLA延迟240ms）和训练成本高（需填充大量空令牌）。 2. **动作利用率不足**：历史预测动作未被充分整合，仅依赖当前观测易产生不稳定轨迹。 **核心方案**： - **训练框架**：引入特殊令牌`<ACT>`表示整个动作块，替代多令牌生成。通过单次前馈预测一个`<ACT>`令牌，经MLP动作头并行解码为连续动作（如7自由度机械臂位姿），显著减少生成令牌数量（原需$N \times D$个令牌降至1个），降低83%训练填充开销。 - **推理优化**：提出投票集成策略，基于历史$K$步与当前动作的余弦相似度（阈值$\tau=0.5$）划分动作集，按多数表决选择最优动作执行，提升动作稳定性。 **实验结果**： - **性能提升**： - SimplerEnv任务成功率58.3%（超CogACT 7%），LIBERO任务平均成功率98%（超OpenVLA 21.5%）。 - 集成策略较平均法提升3.5%成功率（表5）。 - **效率优势**： - 推理速度达205.2 Hz（A6000 GPU），较OpenVLA加速48.8倍； - 边缘设备（Jetson Orin）实现46 Hz吞吐，延迟346ms，加速39倍（表4）。 - 瓶颈式MLP结构较各向同性设计参数量减少26%，内存占用仅14.4GB（表6）。 **贡献总结**： 1. 轻量训练框架大幅降低计算开销； 2. 投票集成机制提升动作利用率； 3. 在精度与速度上均优于主流VLA模型，具备边缘部署实用性。 代码开源：https://github.com/LukeLIN-web/VOTE</details> |
| 2025-07-06 | DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge | http://arxiv.org/abs/2507.04447v3 | <details><summary>展开</summary>DreamVLA提出了一种新型视觉-语言-动作（VLA）模型，通过预测全面的世界知识增强机器人操作能力。核心创新点包括： 1. **感知-预测-行动框架** - 引入动态区域（光流预测）、深度图和语义特征（DINOv2/SAM）三类关键未来知识预测，替代传统冗余图像生成 - 构建紧凑的世界嵌入（world embedding）作为中间推理表示，指导动作规划 2. **结构化注意力机制** - 采用块状掩码注意力隔离不同类型知识（动态/深度/语义）的相互干扰 - 通过扩散变换器（DiT）解耦动作表示与共享特征 3. **实验性能突破** - CALVIN ABC-D基准测试：平均任务长度达4.44（SOTA），长序列任务（5步）成功率78.1% - 真实机器人任务：76.7%成功率（Franka机械臂抓取/放置/抽屉操作） - LIBERO基准：空间推理（97.5%）和长时任务（89.5%）表现优异 4. **关键发现** - 动态区域预测贡献最大（性能提升约18%），深度与语义预测提供互补增益 - 移除结构化注意力导致性能下降6.2%，验证其抗干扰作用 - 单一知识预测（如仅深度）会损害性能，需协同优化 该方法模拟人类"抽象推理后行动"的认知模式，显著提升长时任务鲁棒性，代码和模型已在Hugging Face开源。</details> |
| 2025-07-03 | DexVLG: Dexterous Vision-Language-Grasp Model at Scale | http://arxiv.org/abs/2507.02747v1 | <details><summary>展开</summary>论文提出了一种大规模灵巧视觉-语言-抓取模型（DexVLG），核心贡献如下： 1. **大规模数据集（DexGraspNet 3.0）** - 包含174k对象和170M抓取姿势，覆盖多种抓取风格（如包裹式Ours-Wrap和捏取式Ours-Pinch）。 - 对象预处理：从Objaverse过滤高质量资产，通过GPT-4o标注语义部分标签，并调整尺寸至可抓取范围（20-50cm）。 - 抓取质量优化：基于物理仿真的穿透检测，降低手-物体穿透（平均1.75mm）和自穿透（0.19mm）。 2. **基于能量的优化方法** - 改进可微分力闭合（DFC）：引入LP-based DFC解决原始DFC的等力假设缺陷，通过线性规划优化接触力大小，提升抓取自然性（图1对比）。 - 正则化能量：包括穿透避免（\(E_{pen}\)）、自穿透避免（\(E_{spen}\)）、关节限制（\(E_{limit}\)）和接触方向对齐（\(E_{dir}\)），确保抓取稳定性和几何贴合。 3. **语义驱动的抓取初始化** - 部分分类：将对象部件分为盖子状、盘状、L形和轴状四类（图4-7），基于部件主方向对齐手掌姿势。 - 初始化策略：采样接触点并扰动姿势，生成多样抓取；实验验证其优于随机初始化（图9）。 4. **模型与实验** - 视觉-语言-抓取框架：集成视觉输入和语言指令，预测语义对齐的抓取姿势。 - 实验：包裹式抓取优于捏取（LVIS-Seen成功率87.7% vs. 71.8%）；流匹配（FlowMatching）去噪优于DDPM/DDIM（表4）。模型在未见对象上泛化良好（Unseen数据集成功率79.1%）。</details> |
| 2025-07-02 | cVLA: Towards Efficient Camera-Space VLAs | http://arxiv.org/abs/2507.02190v1 | <details><summary>展开</summary>这篇论文提出了一种高效的相机空间视觉-语言-动作（cVLA）模型，核心贡献如下： 1. **轻量级架构设计** - 基于PaliGemma预训练模型微调，仅需优化注意力层参数 - 将机器人轨迹预测转化为图像坐标系中的末端执行器关键位姿预测（预测两个关键点） - 相比传统VLA模型输出底层控制信号，本方法更高效且与机器人本体解耦 2. **关键技术创新** - **相机空间表示**：直接在图像坐标系（归一化宽高+深度）预测位姿，替代传统机器人基坐标系 - **深度信息融合**：将深度图转换为Viridis色彩编码的RGB图像，复用预训练图像编码器 - **动作编码**：利用定位令牌（1024个）预测位置，分割令牌（128个）预测方向 3. **高效训练方案** - 使用ManiSkill模拟器构建多样化训练数据集（CLEVR几何体+Objaverse真实物体） - 引入两级数据增强：简单版（较少相机/场景随机）和困难版（强随机化） - 通过背景替换（概率0.2）和文本模板增强泛化能力 4. **推理优化策略** - **图像裁剪**：提升小物体定位精度（输入224×224分辨率下） - **束搜索-NMS**：改进解码策略，解决位姿令牌分布的多峰特性 - **多预测评估**：提出基于L1距离的mAP评估指标（阈值0.5-50cm） 5. **单次模仿学习** - 扩展模型支持基于示范的one-shot模仿：输入示范图像+轨迹对，输出新场景轨迹 - 在CLEVR-hard模拟数据上达到28%成功率，在DROID真实数据上L1误差11.56 6. **实验验证** - 模拟实验：深度输入提升成功率（CLEVR-hard达54%） - 实物验证：Franka Panda机器人实现零样本迁移（无需真实数据微调） - 开源承诺：将发布代码、数据集和模型 **局限**：目前局限于桌面级准静态操作任务，对复杂抓取和精确旋转的泛化能力有待提升。该方法为高效训练VLAs提供了新思路，显著降低了计算和数据需求。</details> |
| 2025-07-02 | A Survey on Vision-Language-Action Models: An Action Tokenization Perspective | http://arxiv.org/abs/2507.01925v1 | <details><summary>展开</summary>这篇论文提出了一种统一框架，从**动作标记化（Action Tokenization）**的视角系统综述了视觉-语言-动作（VLA）模型。核心要点如下： ### 1. **统一框架与动作标记分类** - VLA模型可抽象为：视觉与语言输入经一系列**VLA模块**处理，生成链式**动作标记**，逐步编码可执行动作信息。 - 动作标记分为8类： - **语言描述**（如任务分解的自然语言计划） - **代码**（如可执行API指令） - **可供性**（如关键点、边界框、分割掩码、空间图） - **轨迹**（如运动路径） - **目标状态**（如目标图像/视频） - **潜在表示**（如压缩的隐空间编码） - **原始动作**（如机器人关节指令） - **推理**（如思维链）。 ### 2. **动作标记的发展趋势** - **综合应用优于单一类型**：未来VLA需融合多种标记（如语言计划+代码控制+轨迹生成）。 - **关键方向**： - **语言描述**：适合任务规划但缺乏精细动作表达能力。 - **代码**：逻辑清晰但依赖预定义API库。 - **可供性+轨迹**：结合语义引导（"做什么"）与路径规划（"如何做"）。 - **目标状态**：通过世界模型预测视觉目标，支持长时序任务。 - **原始动作**：端到端学习的理想形式，但受数据稀缺限制。 - **推理**：作为元标记增强其他类型，向多模态自适应演进。 ### 3. **架构与数据演进** - **分层架构**：高层用语言/代码规划，底层融合目标状态预测、轨迹建模与可供性，最终映射到原始动作。 - **数据可扩展性**：构建三层数据源——人类视频（底层）、仿真数据（中层）、机器人数据（顶层）。 ### 4. **关键挑战与未来方向** - **从模仿学习到强化学习**：引入RL解决模仿学习的局限性，需高效算法降低现实部署成本。 - **从模型到智能体**：VLA需扩展记忆、探索、反思等认知架构，支持主动决策。 - **硬件-模型-数据协同**：通用智能需三者共同进化（如提升机器人灵巧性、多模态感知）。 - **安全对齐**：当前研究侧重能力，未来需加强安全性与人类价值观对齐。 ### 5. **局限性与展望** - **现状局限**：多数实验限于简化场景（如夹爪操作），离开放物理世界需求差距显著。 - **核心挑战**：硬件灵活性不足（如缺乏类人灵巧手）、高质量具身数据稀缺、开放环境泛化能力弱。 > 总结：该框架为VLA研究提供系统性视角，强调**动作标记的设计是模型核心**，未来需在多标记协同、硬件-数据-模型共进化及安全对齐上突破，推动通用具身智能发展。</details> |
| 2025-07-02 | TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control | http://arxiv.org/abs/2507.01424v2 | <details><summary>展开</summary>本文提出TriVLA模型，一种基于三重系统的视觉-语言-动作统一框架，用于通用机器人控制。核心贡献如下： 1. **三重系统架构设计**： - **System 2（视觉语言模块）**：采用预训练的Eagle-2视觉语言模型，解析环境视觉信息和语言指令 - **System 3（动态感知模块）**：基于微调的稳定视频扩散模型（SVD），通过预测未来帧序列捕捉物理动态 - **System 1（策略学习模块）**：通过动作流匹配训练的扩散策略，整合前两模块输出生成实时动作 2. **关键技术创新**： - 在传统双系统架构上新增视频预测系统，统一世界知识（VLMs）与世界模型（VDMs） - 提出自动特征聚合方法，融合多尺度视频特征用于动作预测 - 采用扩散变换器（DiT）实现跨模态注意力机制，处理变长状态和动作空间 3. **性能优势**： - 实时性：以36Hz频率执行控制 - 在Calvin ABC→D/LIBERO/MetaWorld等基准测试中，性能超越SOTA方法0.11-0.21 - 验证实验显示：在仿真和真实场景中均能处理复杂多阶段任务，尤其在长时域任务中表现突出 该框架通过整合静态环境理解与动态预测能力，显著提升了机器人对复杂指令的执行能力和泛化性能。</details> |
| 2025-07-01 | Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding | http://arxiv.org/abs/2507.00416v2 | <details><summary>展开</summary>论文提出Evo-0模型，通过隐式整合3D几何特征增强视觉-语言-动作（VLA）模型的空间理解能力。核心要点如下： 1. **问题**：现有VLA模型因依赖2D图像-文本预训练而缺乏精确空间理解，显式3D输入（如深度图）需额外传感器且易引入噪声。 2. **方法**：设计轻量融合模块，利用视觉几何基础模型（VGGT）从RGB图像提取几何特征，与VLM视觉标记结合，无需显式3D数据。 3. **实验**： - **模拟任务**（RLBench）：5项空间挑战任务中，Evo-0平均成功率（56%）显著优于基线（OpenVLA-OFT:25%，π₀:41%）。 - **真实任务**：5项操作（如钉孔插入、透明物体抓取），Evo-0平均成功率提升28.88%，验证几何特征的有效性。 - **鲁棒性**：在干扰物、背景变化等5类扰动下，Evo-0表现更稳健（如目标位置偏移时成功率提高10%）。 4. **贡献**：提供即插即用解决方案，提升空间理解；多场景验证性能优势；模型保持实时控制频率（6.94Hz）。</details> |
| 2025-06-30 | A Survey on Vision-Language-Action Models for Autonomous Driving | http://arxiv.org/abs/2506.24044v1 | <details><summary>展开</summary>这篇论文《自动驾驶中的视觉-语言-动作模型综述》首次全面概述了VLA在自动驾驶（VLA4AD）领域的研究进展。核心要点如下： 1. **范式演进**： - 从传统模块化流水线→端到端学习→VLM解释型模型→VLA主动决策模型演进 - VLA4AD核心突破：统一视觉感知、语言理解与控制决策，实现可解释的指令跟随 2. **架构范式**： - **输入**：多模态感知（摄像头/LiDAR）+ 语言指令（导航/交互命令） - **核心模块**：视觉编码器（如BEV特征）、语言处理器（LLM）、动作解码器（轨迹生成/低层控制） - **输出**：低层控制信号（转向/油门）或高层轨迹规划 3. **模型进展**： - **解释型VLM**：被动生成场景描述（如DriveGPT-4） - **模块化VLA**：语言指令转中间表示再生成动作（如CoVLA-Agent） - **端到端VLA**：传感器→语言→动作端到端映射（如EMMA） - **推理增强VLA**：集成CoT推理与轨迹规划（如ORION） 4. **数据集与评测**： - 关键数据集：BDD-X（带人工解释）、nuScenes（多传感器）、Impromptu VLA（长尾场景） - 评测协议：联合评估驾驶安全性、指令准确性和解释质量 5. **挑战与方向**： - 开放挑战：实时性（30Hz控制）、鲁棒性（极端场景）、形式化验证 - 未来方向：神经符号安全核、车队级持续学习、跨模态社会智能 论文系统化梳理了20余个代表性模型（见表1），并指出VLA4AD正推动自动驾驶向可解释、人机协同的方向发展。</details> |
| 2025-06-29 | IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering | http://arxiv.org/abs/2506.23329v1 | <details><summary>展开</summary>这篇论文提出了IR3D-Bench基准测试，用于评估视觉语言模型（VLMs）通过工具使用的场景理解能力。核心要点如下： 1. **核心问题**：传统VLMs在描述性任务（如问答、图像描述）表现优异，但其是否真正理解场景仍存疑。论文提出"通过创造来理解"（understanding-by-creating）的理念，受费曼名言"无法创造即未理解"启发。 2. **创新方法**： - **Agentic Inverse Rendering（代理逆渲染）**：要求视觉语言代理（VLA）主动使用编程工具（如Blender API）将输入图像逆向解析为可执行的3D场景程序，重建原始场景的底层结构。 - 区别于传统逆渲染（优化连续参数），该方法强调**工具使用的生成能力**，需输出结构化、可执行的程序（Python脚本）。 3. **基准设计**： - **数据集**：基于CLEVR数据集（含15,000个合成图像），提供精确的3D物体标注（位置、形状、颜色、空间关系）。 - **流程**： - **阶段1**：VLA根据输入图像生成场景的JSON描述并转换为Blender脚本。 - **阶段2**：通过渲染重建图像，从三方面评估： - **定位**（物体数量、空间对齐、关系一致性） - **视觉外观**（形状/材质准确性） - **语言对齐语义**（布局合理性） - **评估指标**： - 像素级中心距离、物体计数准确率 - 边界框相似度（BBox Edge Score） - 空间关系一致性（如左右、前后） - 语义相似度（CLIP文本编码） - GPT-4o辅助的全局合理性评分 4. **关键发现**： - 当前VLMs主要瓶颈在于**视觉精度不足**（如细粒度属性识别），而非工具使用能力。 - 即使迭代优化，模型在视觉细节修正上易陷入瓶颈。 - 实验显示：Gemini-2.5-pro在空间布局表现佳，Grok-3擅长材质建模，Qwen2.5-VL-72B在复杂场景中表现欠佳。 5. **意义**： - 推动VLMs从被动识别转向主动创造，为构建真正理解3D世界的多模态智能体提供新范式。 - 开源基准（含数据集与评估协议）促进工具驱动型VLAs的研究。 论文强调未来突破需提升视觉表征的保真度，而非单纯优化指令微调或语法支持。</details> |
| 2025-06-27 | 4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration | http://arxiv.org/abs/2506.22242v1 | <details><summary>展开</summary>这篇论文提出了一种名为4D-VLA的新型时空视觉-语言-动作预训练框架，核心贡献如下： 1. **问题定义**： 针对机器人数据预训练中存在的"坐标系混乱"（67%的DROID数据集样本存在机器人基座遮挡）和"状态混乱"（单帧输入无法解决动作歧义）问题，提出通过整合4D时空信息来优化输入表征。 2. **方法创新**： - **空间感知视觉标记**：通过RGB-D序列输入，将深度信息生成的3D坐标嵌入与视觉特征融合，解决机器人坐标系与场景的对齐问题 - **记忆库采样策略**：自适应选择历史关键帧（基于特征相似性），提升时序信息利用效率 - **4D表征构建**：结合时空编码令牌（$\bm{e}^{T}$）和空间视觉令牌（$\bm{e}^{ST}$），形成$\mathcal{X}=\bigcup[\bm{e}^{T}_i\mid\bm{e}^{ST}_i]\cup\{\bm{e}^{text}\}$的输入结构 3. **实验验证**： - 在仿真环境（LIBERO）和真实机器人任务中，成功率显著超越OpenVLA基准 - 提出**MV-Bench多视角评测数据集**，验证模型在空间理解和视角泛化上的优势 - 消融实验表明：深度信息使空间定位错误率降低42%，记忆库采样提升20%的时序推理效率 4. **技术优势**： 以InternVL-4B为骨干网络，通过轻量级空间模块（仅增加0.3%参数）实现跨场景校准，为机器人操作提供端到端闭环控制（输出$\Delta\bm{x}$, $\Delta\bm{\theta}$, $g$三自由度动作）。 该工作通过多模态输入增强解决了动作分布高方差问题，为具身智能的预训练范式提供了新方向。代码已开源：https://github.com/fudan-zvg/4D-VLA</details> |
| 2025-06-26 | WorldVLA: Towards Autoregressive Action World Model | http://arxiv.org/abs/2506.21539v1 | <details><summary>展开</summary>该论文提出WorldVLA，一种自回归动作世界模型，核心贡献如下： 1. **统一框架**：整合视觉-语言-动作（VLA）模型与世界模型，通过共享词表的图像、文本、动作分词器，在单一自回归架构中实现动作生成与未来图像预测的联合学习。动作模型基于观测生成动作，世界模型利用动作预测环境状态，二者相互增强。 2. **注意力掩码策略**：针对自回归生成动作序列时的误差累积问题，提出选择性掩码机制。生成当前动作时屏蔽历史动作依赖，缓解预训练多模态模型在动作域的泛化不足，显著提升动作块生成性能（抓取成功率提升4%-23%）。 3. **实验验证**：在LIBERO基准测试中，WorldVLA超越独立模型： - 动作模型：抓取成功率提升4% - 世界模型：视频生成Fréchet视频距离（FVD）降低10% 验证了动作与世界模型的协同优化效应，证明统一框架在物理理解与决策生成上的优势。 [[论文链接](https://arxiv.org/abs/2506.21539)]</details> |
| 2025-06-26 | Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends | http://arxiv.org/abs/2506.20966v1 | <details><summary>展开</summary>这篇论文探讨了视觉-语言-动作（VLA）模型的后训练与人类运动学习的相似性，提出了一种基于人类运动学习机制的后训练分类法，并分析了当前挑战与未来趋势。核心要点如下： 1. **核心关联** VLA模型的后训练（适应特定环境、任务和机器人本体）与人类运动技能习得过程相似：预训练对应先天遗传先验，后训练对应通过实践精炼技能。 2. **后训练分类法** 提出四类后训练策略，聚焦三个维度： - **环境感知增强**：通过具身化学习（如RoboPoint）、专用编码器优化（如Theia）和表征增强（如DexGraspVLA）提升环境理解。 - **本体感知改进**：学习运动学模型（如MIDAS）或设计动作头（如FAST）以适配机器人本体。 - **任务理解深化**：通过人机交互（如VLA-Expert）和分层任务分解（如CoVLM）提升任务执行能力。 - **多组件集成**：结合强化学习、视觉预测等方法（如UniAct）实现综合优化。 3. **关键挑战** - **数据与评估**：需解决数据效率问题（如DemoGen的演示生成）和公平性评估。 - **操作能力**：需融入多模态感知（如触觉）、实现持续学习（如BID）并提升开放世界泛化性。 - **安全与伦理**：增强模型可解释性（如KAT）和安全性保障机制（如VTLA）。 4. **实践意义** 通过神经科学与机器人学的交叉视角（NeuroAI），为VLA模型的实际部署提供指导，降低应用门槛并推动基础模型在机器人领域的进化。 论文建立了首个系统化的VLA后训练方法分类体系，强调环境-本体-任务的协同优化，为未来研究奠定框架基础。</details> |
| 2025-06-24 | Unified Vision-Language-Action Model | http://arxiv.org/abs/2506.19850v1 | <details><summary>展开</summary>本文提出UniVLA，一种统一视觉-语言-动作（VLA）模型，通过自回归序列建模实现多模态统一表示。核心创新点如下： 1. **统一多模态建模** - 将视觉、语言、动作信号统一离散化为token序列，使用共享词汇表 - 图像采用VQ编码器，动作通过FAST方法（DCT频域转换）编码 - 序列中插入特殊边界符（boi/eoi, boa/eoa）区分模态 2. **世界模型后训练策略** - 第一阶段：利用大规模无动作视频数据学习环境动态（视觉预测任务） - 第二阶段：在动作序列上微调策略 - 该策略显著提升长时序任务性能（如LIBERO-Long任务成功率从85.5%→95.5%） 3. **关键性能优势** - 在CALVIN长时序任务中平均完成长度4.63（SOTA） - LIBERO基准平均成功率95.5%（超越π0-FAST的85.5%） - SimplerEnv-WidowX任务成功率69.8%（相对提升27%） 4. **多模态能力拓展** - 支持视觉预测、空间推理等任务 - 成功迁移至真实机器人操作（ALOHA）和自动驾驶场景 实验表明，统一建模框架结合世界模型预训练，显著提升跨模态理解、长时序规划及数据效率，为通用具身智能提供新范式。 --- *注：总结基于论文核心贡献，省略实现细节（如8.5B参数规模、具体数据集规模等），聚焦方法创新与性能突破。*</details> |
| 2025-06-24 | CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation | http://arxiv.org/abs/2506.19816v1 | <details><summary>展开</summary>该论文提出CronusVLA框架，用于提升机器人操作任务中的多帧动作预测能力。核心创新点包括： 1. **单帧预训练** 基于大规模异构操作数据集训练基础VLA模型，通过自回归动作令牌预测建立具身视觉-语言基础。 2. **多帧编码** - 将单帧VLA模型输出从离散动作令牌转换为连续运动特征 - 聚合历史帧运动特征形成"特征分块"(feature chunking) - 通过缓存机制避免冗余计算，提升推理速度 3. **跨帧解码** - 设计交叉注意力解码器，将特征分块映射为连续动作 - 引入特征调制器平衡当前帧与历史信息 - 采用多帧正则化技术保持单帧感知能力 4. **动作自适应** 在微调阶段引入基于特征-动作检索的适配机制，通过相似性匹配从专家数据中检索参考动作作为预测先验。 **实验效果**： - 在SimplerEnv基准达到70.9%成功率（SOTA） - LIBERO基准超越OpenVLA 12.7% - 真实Franka机器人实验验证了框架的鲁棒性 - 推理速度提升：通过特征缓存实现近恒定时间复杂度的长时程预测 该方法首次实现了VLAs模型从单帧到多帧范式的扩展，在保持视觉语言模型核心优势的同时，高效捕获了运动信息的时序动态特性。</details> |
| 2025-06-23 | MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis | http://arxiv.org/abs/2506.18897v2 | <details><summary>展开</summary>这篇论文提出了一种名为MinD（Manipulate in Dream）的双系统世界模型，用于实时规划和隐式风险分析。核心要点如下： 1. **问题背景** 现有视频生成模型（VGMs）作为世界模型时面临两大挑战：生成过程难以集成到策略学习中，以及实时生成视频帧计算效率低下，无法满足实时控制需求。 2. **核心创新** - **双系统架构**： - **LoDiff-Visual**：低频视频生成器，预测未来场景（1000步去噪） - **HiDiff-Policy**：高频扩散策略（100步），基于单步视觉特征输出实时动作 - **DiffMatcher模块**：通过扩散强制对齐（diffusion-forcing）策略桥接双系统，使动作模型能理解未完全去噪的视觉特征 - **单步推理机制**：仅需单步视频去噪即可提取有效未来状态表征，显著提升效率（11.3 FPS） 3. **关键技术** - 异步协同训练：联合优化视频生成损失、动作损失和对齐损失 - 隐式风险评估：通过分析生成视频的潜在特征（PCA可视化），提前预测74%任务失败案例 4. **实验结果** - RL-Bench仿真任务：63%成功率（SOTA） - 真实Franka机器人：60%成功率 - 效率：比传统VGMs快10倍以上（如RoboDreamer 0.7 FPS → MinD 11.3 FPS） 5. **应用价值** 模型同时支持实时操作和风险预测，为构建安全可靠的视觉-语言-动作（VLA）系统提供新范式。代码和演示见：manipulate-in-dream.github.io 论文通过解耦想象与执行，实现了高效的世界模型应用，为具身智能提供了可解释的决策支持。</details> |
| 2025-06-22 | RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation | http://arxiv.org/abs/2506.18088v2 | <details><summary>展开</summary>论文提出RoboTwin 2.0框架，用于增强双手机器人操作的鲁棒性，核心贡献如下： 1. **自动化专家数据生成** - 结合多模态大语言模型（MLLMs）与仿真反馈循环，自动生成任务执行代码 - 通过视觉语言模型实时监测执行过程，实现错误定位与代码迭代优化 - 代码生成成功率提升10.9%（71.3% vs 基线60.4%） 2. **结构化域随机化** - 沿5个维度增强数据多样性：场景杂乱度、光照、背景纹理、桌面高度、语言指令 - 构建包含11,000种纹理的材质库，支持逼真场景生成 - 语言指令组合生成器实现任务描述的语义多样性 3. **本体感知抓取适配** - 为731个物体（147类）标注操作关键点与抓取轴 - 针对不同自由度机械臂（如Franka/Piper）生成本体专属抓取策略 - 低自由度平台任务成功率最高提升22.7% 4. **资源与验证** - 发布RoboTwin-OD物体数据集及50个双人协同任务基准 - 生成100,000+专家轨迹，覆盖5种机器人本体 - 策略训练实验显示：仅10条真实演示+合成数据，成功率相对提升367%；纯合成数据零样本迁移提升228% - 真实世界测试证实对复杂环境的强鲁棒性（平均成功率提升24.4%） 开源数据生成器、基准测试与预训练模型支持鲁棒双手机器人操作研究。</details> |
| 2025-06-21 | RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models | http://arxiv.org/abs/2506.17811v2 | <details><summary>展开</summary>本文提出RoboMonkey框架，旨在通过测试时采样与验证增强视觉-语言-动作模型（VLA）的鲁棒性。核心贡献如下： 1. **推理阶段扩展定律的发现** 实验表明，VLA的动作误差与生成样本数量呈指数幂律关系（如公式 $\log(e)\approx\log(a)+b\cdot\log(k)$）。高斯扰动采样策略在计算效率与性能间取得最佳平衡，较单次采样显著降低动作误差。 2. **合成数据驱动的动作验证器训练** - 提出自动化合成偏好数据集构建流程：基于专家演示数据，通过聚类生成多样化动作候选对，以RMSE作为偏好标签（$a_t^W$优于$a_t^L$当且仅当$\text{RMSE}(a_t^i, a_t^*) < \text{RMSE}(a_t^j, a_t^*)$）。 - 利用该数据集微调70亿参数视觉语言模型（VLM）作为动作验证器，并证明扩大合成数据规模可持续提升验证精度。 3. **测试时扩展框架RoboMonkey** 部署阶段包含三步： a) **高斯扰动采样**：从基础VLA策略采样少量动作，拟合高斯分布以高效生成候选动作 b) **多数投票决策**：针对夹爪状态进行多数表决 c) **VLM验证器筛选**：从候选动作中选取最优执行动作 4. **实验验证** - 在分布内任务（SIMPLER）提升9%成功率，分布外任务提升25%绝对性能 - 迁移到新机器人平台时，联合微调VLA与验证器较单独微调VLA性能提升7% - 硬件实验证实框架可部署性，延迟分析见第5.4节 该框架无需修改预训练VLA，通过测试时计算扩展显著提升模型鲁棒性，为机器人基础模型部署提供新范式。</details> |
| 2025-06-21 | RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models | http://arxiv.org/abs/2506.17639v1 | <details><summary>展开</summary>本文提出RLRC方法，用于压缩视觉-语言-动作模型（VLA），以解决其参数量大、推理延迟高的问题。核心要点如下： 1. **问题背景**：VLA模型在机器人操作任务中表现出色，但庞大的参数量和高延迟限制了其在资源受限设备上的部署。 2. **方法设计**： - **三阶段压缩框架**： 1. **结构化剪枝**：针对VLA中的LLM组件进行90%剪枝，显著减少计算量。 2. **性能恢复**：结合监督微调（SFT）和强化学习（RL）恢复任务性能（SFT恢复基础能力，RL提升泛化性）。 3. **量化**：可选4-bit量化进一步压缩内存至原模型1/8。 - **关键创新**：首次将强化学习用于压缩后VLA的性能恢复，提升OOD泛化能力。 3. **实验结果**： - **性能**：在ManiSkill基准上，压缩后模型IND任务成功率90.62%（超原模型89.06%），OOD成功率62.50%（超原模型57.81%）。 - **效率**：内存降至3.856GB（8×压缩），推理速度提升至13.5 samples/s（2.3×加速）。 - **优势**：显著优于VLA-Cache等基线，4-bit量化版本（RLRC-4bit）内存仅1.772GB。 4. **贡献**： - 实证验证剪枝/量化在VLA的适用性 - 提出首个支持强化学习恢复的VLA压缩框架 - 实现高压缩率下性能持平或超越原模型 论文链接：https://rlrc-vla.github.io</details> |
| 2025-06-21 | VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models | http://arxiv.org/abs/2506.17561v1 | <details><summary>展开</summary>本文系统研究了视觉-语言-动作（VLA）模型中任务规划的表示形式与范式，核心发现如下： ### 核心结论 1. **视觉规划表示优于语言表示** - 视觉推理（V）和图像前瞻规划（IF）在任务性能、泛化能力和推理速度上均优于语言规划（L） - 视觉表示的推理速度更快（IF比L快1.7倍），训练成本更低（IF训练时间比L减少30%） 2. **分层范式（Hierarchical-VLA）综合性能最佳** - 在LIBERO等6个基准测试（刚性/可变形物体、2D/3D环境、仿真/现实场景）中： - 任务成功率平均提升8.2%（最高达85.6%） - 泛化能力最强（在COLOSSEUM泛化测试中提升1.3%） - 支持多表示融合（L+V+IF）时性能最优（74.2%） 3. **训练与推理效率的权衡** - 分层范式训练成本增加40%，推理延迟增加50%（因需两阶段计算） - 集成范式（Integrated-VLA）隐式规划（I-I）性能次优（73.3%），但推理速度更快 ### 关键发现 - **规划预训练增益显著**：任务规划头部预训练可使成功率提升5.6-5.8% - **策略学习是瓶颈**：在LIBERO任务中，策略学习模块的失败率（35%）高于任务规划模块（15%） - **显式规划的缺陷**：集成范式的显式规划（I-E）因误差累积导致性能下降（比基准低13.5%） ### 实践意义 分层范式虽需更高算力，但为复杂操作任务提供了最优解决方案。未来工作可聚焦于优化其计算效率。 > 论文视频：https://nus-lins-lab.github.io/vlaos/ > 模型代码：开源在HuggingFace</details> |
| 2025-06-19 | CapsDT: Diffusion-Transformer for Capsule Robot Manipulation | http://arxiv.org/abs/2506.16263v1 | <details><summary>展开</summary>本文提出CapsDT——一种基于扩散Transformer的胶囊内窥镜机器人操控模型，主要贡献如下： 1. **系统构建** - 开发了磁控胶囊内窥镜机器人系统：7自由度Kuka机械臂控制外部永磁体，通过磁力驱动胶囊在硅胶胃模型中运动 - 创建了包含1,000+轨迹的多任务数据集（导航/旋转/液下观测/旋转观测四类任务），是目前最全面的胶囊机器人数据集之一 2. **模型创新** - 提出CapsDT模型，融合视觉-语言-动作模态： * 处理内窥镜图像+文本指令+机器人参数 * 预测7维控制信号（3维平移速度+3维旋转速度+夹爪状态） - 采用扩散Transformer架构： * 通过去噪过程生成最优动作序列 * 支持动作块预测提升时序一致性 - 多模态编码策略： * SigLIP编码视觉输入 * BART-large编码语言指令 * 随机掩码防止模态依赖失衡 3. **性能突破** - 在四类内窥镜任务中全面超越基线模型： * 导航任务成功率25%（基线接近0%） * 旋转任务成功率20%（基线最高20%） - 在真实胃模拟器实验中实现26.25%的成功率 - 相对基线模型平均提升21.25%成功率 该研究首次将视觉-语言-动作模型应用于内窥镜胶囊机器人领域，为医疗机器人的自主精准操控提供了新范式。</details> |
| 2025-06-19 | ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models | http://arxiv.org/abs/2506.16211v1 | <details><summary>展开</summary>本文提出ControlVLA框架，用于解决预训练视觉-语言-动作（VLA）模型在少量演示下的适应问题。核心创新点包括： 1. **控制网络式微调**：通过零初始化投影层，将物体中心化表示逐步融入预训练VLA策略，保留先验知识的同时引入任务特定条件 2. **高效适应机制**：仅需10-20个演示样本即可达到76.7%的平均成功率，显著优于需要100+样本的基线方法（20.8%） 3. **多任务验证**：在8类真实世界任务（刚性/柔性物体操作、关节控制、流体模拟等）中验证有效性，包括： - 短期任务：杯子重排、玩具整理、剪刀归位等 - 长期任务：多物体协同操作、抽屉物体替换等 4. **泛化能力**：在未见过的物体和背景场景中保持鲁棒性，并展示出对长时程任务的扩展性 关键组件分析表明：VLA预训练提供技能先验，物体中心化表示提升学习效率，控制网络式调节确保稳定适应。该方法为数据稀缺场景下的机器人操作提供了高效解决方案。</details> |
| 2025-06-17 | FormGym: Doing Paperwork with Agents | http://arxiv.org/abs/2506.14079v1 | <details><summary>展开</summary>论文提出FormGym基准，用于评估代理在纯图像表单填写任务中的能力（无OCR或PDF文本访问）。核心要点如下： 1. **问题背景**：表单填写是耗时挑战，需多模态理解、信息检索和工具使用。纯图像领域（如扫描文档）尤其困难，代理需基于用户配置文件（236个特征）填充字段。 2. **FormGym基准**： - 包含432个字段、55个文档和3个任务（如自然语言输入、文档转移和数据库查询）。 - 评估代理动作（如放置文本、签名）和流程（单次或迭代完成）。 3. **基线表现**： - 视觉语言模型（VLAs）准确率普遍低于1%，主要因定位能力差。 - GUI代理准确率在10.6%–68.0%，但成本高、延迟高。 4. **解决方案FieldFinder**： - 辅助工具，帮助LLMs定位字段输入区域（预测边界框）。 - 集成后，所有模型性能提升：最大提升从2%到56%（如GPT-4o在FUNSD任务），平均提升显著。 5. **贡献与计划**： - 开源基准和FieldFinder工具（计划GitHub发布）。 - 分离语义理解与空间定位，缓解VLAs瓶颈。</details> |
| 2025-06-16 | GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics | http://arxiv.org/abs/2506.14009v1 | <details><summary>展开</summary>这篇论文提出了一种名为GRaD-Nav++的轻量级视觉-语言-动作（VLA）框架，用于实现无人机完全机载的自然语言导航。核心创新点包括： 1. **系统架构** - 采用轻量化设计（基于CLIP的视觉语言模型 + 混合专家策略网络），可在无人机嵌入式硬件上实时运行 - 引入MoE（Mixture-of-Experts）动作头，通过自适应计算路由提升泛化能力并减少遗忘 2. **训练方法** - 在3D高斯泼溅（3DGS）模拟器中构建高保真视觉环境 - 结合可微分强化学习（DiffRL）与可微分无人机动力学模型，实现高效策略优化 - 使用参考轨迹引导的多阶段任务训练范式 3. **核心性能** - **多任务泛化**：在训练任务上达到83%成功率，未见过任务上75%（仿真）；实物部署分别达67%和50% - **多环境适应**：跨不同仿真环境平均成功率81%，实物场景达67% - 相比现有方法（如RaceVLA/CognitiveDrone），在保持精度的同时显著降低计算需求 4. **技术贡献** - 首次实现完全机载的VLA导航，摆脱对外部基础设施依赖 - 建立视觉-语言指令与低层控制间的端到端映射 - 通过上下文估计器网络缓解仿真到现实的迁移差距 该框架为资源受限的无人机平台提供了可靠的语言引导导航新范式，在搜救、物流等场景具有应用潜力。</details> |
| 2025-06-16 | AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning | http://arxiv.org/abs/2506.13757v1 | <details><summary>展开</summary>论文提出AutoVLA模型，用于端到端自动驾驶，主要解决现有视觉-语言-动作（VLA）模型的两个问题： 1. **物理不可行动作与复杂结构**：通过离散化连续轨迹为可行动作token，直接集成到语言模型中，避免额外解码器 2. **跨场景推理效率低**：引入自适应双模式推理 - **快速思维**（简单场景）：直接输出轨迹 - **慢速思维**（复杂场景）：结合链式推理（CoT）生成决策依据 **关键技术**： - **监督微调（SFT）**：使用自动标注的高质量推理数据（45.6k nuPlan + 7.2k Waymo），联合优化语言建模损失 \(\mathcal{L}_{\text{LM}}\) 和动作损失 \(\mathcal{L}_{\text{action}}\) - **强化微调（RFT）**：基于GRPO算法优化奖励函数，在提升规划性能的同时减少冗余推理（运行时效率提升） **实验验证**： 在nuPlan、nuScenes、Waymo和CARLA的开放/闭环测试中均表现优异，自适应推理能力在复杂场景（如路口决策）显著提升轨迹准确性。 **核心贡献**： 1. 首个统一推理与动作生成的VLA架构 2. 创新强化微调方法实现高效自适应推理 3. 开源大规模自动驾驶推理数据集 > 模型代码与数据：https://autovla.github.io/</details> |
| 2025-06-16 | LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction | http://arxiv.org/abs/2506.13751v3 | <details><summary>展开</summary>本文提出LeVERB系统，首次实现基于视觉-语言指令的人形机器人全身控制，核心贡献如下： 1. **LeVERB-Bench基准数据集**：构建首个支持sim-to-real的视觉-语言闭环评测基准，包含10类150+任务，通过运动重定向与逼真渲染生成17.1小时合成数据，支持物理仿真与零样本迁移。 2. **双层次控制架构**： - **高层视觉语言模块（System 2）**：基于CVAE学习潜在动作空间（"latent verbs"），将视觉/语言指令编码为语义化运动计划（10Hz）。 - **低层动作模块（System 1）**：强化学习策略（50Hz）将潜在指令转化为关节动作，通过解耦训练避免渲染开销，实现动力学级控制。 3. **关键技术突破**： - 提出残差潜在空间设计，结合运动编码器增强语义对齐 - 引入对抗判别器统一视觉/非视觉数据分布 - 支持零样本部署，在导航任务达80%成功率，整体任务成功率58.5%，超越基线7.8倍 4. **实验验证**： - 在LeVERB-Bench闭环保真测试中优于所有消融模型 - 实机部署于Unitree G1人形机器人，成功执行视觉导航（如"走向绿色椅子"）和复杂动作（如转身坐下）</details> |
| 2025-06-16 | CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding | http://arxiv.org/abs/2506.13725v1 | <details><summary>展开</summary>本文提出了一种加速视觉-语言-动作（VLA）模型推理的方法CEED-VLA，核心创新点包括： 1. **问题背景**：VLA模型在机器人任务中存在推理速度瓶颈，传统Jacobi解码因模型无法在错误前缀下预测多令牌而加速有限（仅1.28倍）。 2. **一致性蒸馏训练**： - 通过教师模型生成Jacobi轨迹数据集 - 学生模型学习将轨迹任意点直接映射到固定点，实现单次迭代预测多令牌 - 设计一致性损失函数：\(\mathcal{L}_{\text{C}} = \mathbb{E}[\sum \text{KL}(Q_{\theta^{-}}(\cdot\|\mathcal{Y}_i^*,{\bm{x}}) \|\| Q_{\theta}(\cdot\|\mathcal{Y}_i,{\bm{x}}))]\) 3. **混合标签监督**： - 引入自回归损失\(\mathcal{L}_{\text{AR}}\)保持原始操作能力 - 动态选择监督信号：当教师输出与真值L1距离小于阈值\(\delta_{\text{max}}\)时采用教师输出，否则采用真值，避免蒸馏误差累积 4. **早退解码策略**： - 识别Jacobi解码中的低效迭代（收敛慢） - 放宽收敛条件，提前终止迭代，进一步提升平均推理速度 5. **实验结果**： - 在CALVIN和LIBERO仿真环境中实现2-4.1倍加速，任务成功率相当 - 真实机械臂实验达到4倍频率提升，高频灵巧任务成功率改善 - 消融实验验证了混合标签和早退机制的有效性 该方法为机器人多模态决策提供了高效通用范式，项目页面：https://irpn-eai.github.io/CEED-VLA/。</details> |
| 2025-06-16 | ROSA: Harnessing Robot States for Vision-Language and Action Alignment | http://arxiv.org/abs/2506.13679v1 | <details><summary>展开</summary>本文提出了一种名为ROSA（利用机器人状态实现视觉-语言-动作对齐）的新训练范式，核心贡献如下： 1. **问题识别**：现有视觉-语言-动作（VLA）模型在微调视觉-语言模型（VLM）时存在**空间-时间鸿沟**： - 空间上：VLM处理高层语义，而机器人动作需精确实时3D空间定位 - 时间上：VLM关注当前图像理解，而VLA需预测未来动作 2. **解决方案**： - **双模态数据训练**： - **专家动作数据**：人工收集的示教数据（成本高） - **机器人状态数据**：自动化采集末端执行器的3D位姿和夹爪状态（零人工成本） - **统一训练架构**：将状态估计与动作预测整合到同一VLA框架中，通过状态估计增强空间感知能力 3. **关键技术优势**： - **数据高效性**：在低数据场景（如100条专家轨迹）下，RLBench仿真任务成功率提升11.4%，真实机器人任务成功率翻倍 - **泛化能力**：在未见过的物体/容器任务上（如"方块入盒"），成功率较基线提升60% - **自动化扩展**：状态数据可通过机器人生成随机动作自动收集，支持大规模应用 4. **实验结果**： - **仿真（RLBench）**：12项任务平均成功率63.7%（100条专家数据），优于PerACT等非VLA方法 - **真实机器人（WidowX）**： - 已知任务：35%平均成功率提升 - 未知任务：85%平均成功率（基线43%） - **极端场景**：单样本训练在3项任务中实现非零成功率（基线全失败） 5. **核心价值**：通过机器人状态数据弥合语义与物理空间的鸿沟，显著降低VLA模型对人工示教数据的依赖，为高效机器人操控提供新范式。 > 注：ROSA基于标准LLaVA架构（CLIP视觉编码器+Qwen-2.5-7B语言模型），实验表明其在不同数据规模下均优于基线方法。</details> |
| 2025-06-15 | SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration | http://arxiv.org/abs/2506.12723v2 | <details><summary>展开</summary>本文提出SP-VLA框架，通过联合优化模型调度和令牌剪枝加速视觉-语言-动作（VLA）模型。核心创新点包括： 1. **动作感知的模型调度** - 受人类行为模式启发，将VLA动作分为**深思型**（如抓取、转向）和**直觉型**（如高速移动）两类。 - 基于历史动作速度动态切换模型：深思型动作由高精度VLA模型处理，直觉型动作交由轻量级Ridge回归生成器执行（速度阈值判定）。 - 通过动作缓存和触发机制（如速度区间判定、历史动作比例控制）实现高频自适应切换，减少时序冗余。 2. **空间-语义双感知令牌剪枝** - 发现VLA模型依赖令牌空间位置和物体轮廓信息（如边缘特征），单纯语义剪枝会破坏空间理解。 - 融合**空间信息**（Canny边缘检测提取轮廓）和**语义重要性**（累积注意力得分）计算令牌综合重要性。 - 根据当前运动速度动态调整剪枝阈值，优先保留关键空间和语义信息，降低计算开销。 3. **联合优化效果** - 实验表明：在抓取、导航等任务中，框架实现**1.35–1.5倍加速**，精度损失低于3%。 - 轻量生成器（Ridge回归）减少70%单步计算，令牌剪枝最高压缩50%视觉输入，二者协同显著提升实时性。 该方法首次同时解决VLA模型的时序冗余（动作序列优化）和空间冗余（视觉令牌压缩），为实时机器人控制等场景提供高效解决方案。</details> |
| 2025-06-11 | EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models | http://arxiv.org/abs/2506.10100v1 | <details><summary>展开</summary>本文提出**EfficientVLA**，一种无需训练的推理加速框架，用于解决基于扩散的视觉-语言-动作（VLA）模型的计算冗余问题。核心方法包括三部分： 1. **语言模块剪枝**：通过分析层间隐藏状态相似性，移除冗余的Transformer层（减少41%参数量），降低内存瓶颈。 2. **视觉令牌优化**：结合任务相关性和多样性选择关键视觉令牌。先基于跨注意力分数筛选任务相关令牌，再通过特征相似性避免信息冗余（保留量减少78%）。 3. **动作头缓存**：利用扩散解码器的时序冗余，缓存中间注意力/MLP特征，减少80%去噪步计算。 在CogACT模型（SIMPLER基准）上的实验表明： - 推理速度提升**1.93倍** - 计算量（FLOPs）降至原版**28.9%** - 任务成功率仅下降**0.6%** 该方法无需微调，显著提升VLA模型在资源受限场景的部署效率。</details> |
| 2025-06-11 | SAFE: Multitask Failure Detection for Vision-Language-Action Models | http://arxiv.org/abs/2506.09937v1 | <details><summary>展开</summary>本文提出了一种针对视觉-语言-动作模型（VLA）的多任务失败检测方法SAFE，核心要点如下： 1. **问题背景** - VLA模型在新任务上的成功率较低（30-60%），需实时检测失败以保障安全部署 - 现有失败检测方法需针对每个任务单独训练，无法泛化至未见任务 2. **关键发现** - VLA的隐空间特征能区分任务成功/失败状态，且具有跨任务泛化性（图1可视化） - 失败轨迹的特征会聚集在特定"失败区域"，与任务无关 3. **SAFE方法设计** - **特征提取**：利用VLA内部隐藏层特征（最终层解码前的特征） - **检测模型**： - 轻量级MLP或LSTM架构（避免过拟合） - 输入时序特征$\mathbf{e}_{0:t}$，输出失败概率$s_t$ - **训练机制**： - 使用L1损失函数：推动失败轨迹得分上升，成功轨迹得分下降 - 多任务联合训练提升泛化能力 - **阈值校准**：采用共形预测（Conformal Prediction）动态调整检测阈值 4. **实验验证** - **测试模型**：OpenVLA, $\pi_0$, $\pi_0$-FAST - **评估指标**：ROC-AUC（表1） - **关键结果**： - 在LIBERO等仿真基准上，SAFE的MLP/LSTM版本均优于基线方法 - 跨任务泛化性：未见任务上保持高检测精度（如$\pi_0$-FAST的84.48 AUC） - 实时性：仅需单次推理，计算开销低于需多次采样的基线 5. **贡献总结** - 首次揭示VLA隐空间的跨任务失败特征模式 - 提出首个适用于通用VLA的多任务失败检测框架 - 在仿真/真实场景验证了最优的精度-实时性平衡 SAFE通过利用VLA内部特征实现高效跨任务失败检测，为安全部署通用机器人策略提供新方案。更多结果见项目页面：https://vla-safe.github.io/</details> |
| 2025-06-11 | From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models | http://arxiv.org/abs/2506.09930v1 | <details><summary>展开</summary>这篇论文的核心贡献是提出了INT-ACT评测套件，用于系统评估视觉-语言-动作模型（VLA）的泛化能力。主要要点如下： 1. **问题背景** 当前VLA模型评估存在两大局限： - 传统机器人基准缺乏语言指令 - 新兴VLA基准任务单一，无法量化VLM预训练对策略泛化的贡献 - 真实机器人评测成本高，可复现性差 2. **INT-ACT评测套件** - 在SimplerEnv模拟器上构建，包含**50个任务**，覆盖三大维度： - **物体多样性**：引入分布外（OOD）物体（如工业工具）、新外观和非常规物体关系 - **语言复杂性**：涵盖动作改写（"Place A atop B"）、否定指令（"不要拿B"）、指代表述（"紫色物体"） - **视觉语言思维**：添加干扰物（如胡萝卜旁的兔子）、常识推理（"兔子最爱的蔬菜"） 3. **关键发现** - **意图-执行鸿沟**： - VLA在OOD场景下保持高意图正确率（80-100%），但任务成功率平均下降40-70% - 例如：更换目标物体（如键盘→轮子）使π₀模型抓取成功率从77%降至56% - **语言泛化退化**： - 微调后VLA的语言理解能力显著弱于原始VLM - 否定指令使任务成功率平均下降24%，外观描述指令下降18% - **多模态干扰脆弱性**： - 当语言常识与视觉干扰叠加（如"兔子最爱的蔬菜" + 兔子玩偶），错误物体操作率高达65% 4. **模型对比** - π₀-scratch表现最佳（平均任务成功率48.9%），但仍有显著泛化缺口 - 联合训练的Magma对语言变化更鲁棒，但动作执行能力较弱 - 非VLM基线Octo泛化能力最差（成功率<10%） 5. **开源贡献** - 发布评测套件和代码（https://ai4ce.github.io/INT-ACT/） - 呼吁社区关注VLA中感知-动作的转化瓶颈 论文揭示了当前VLA架构的局限：VLM提供强大的意图理解，但动作执行模块难以有效利用该能力，尤其在跨模态分布偏移时。这为未来改进方向提供了实证基础。</details> |
| 2025-06-11 | OctoNav: Towards Generalist Embodied Navigation | http://arxiv.org/abs/2506.09839v1 | <details><summary>展开</summary>以下是论文《OctoNav: Towards Generalist Embodied Navigation》的核心要点总结： ### 1. **研究目标** 提出**通用化具身导航智能体**，能遵循包含**多模态（视觉/语言/坐标）和多能力（目标导航/点导航/图像导航等）任意组合**的自由形式指令，解决现有导航任务（如ObjNav、VLN等）相互割裂的问题。 ### 2. **核心贡献** - **OctoNav-Bench 基准**： - **规模**：覆盖400+室内场景（HM3D/Gibson等），包含45k+指令-轨迹对。 - **创新设计**： - **自由形式指令**：每条指令可同时融合多种导航能力（如"前往坐标点→寻找目标物体→返回起点"）。 - **多模态支持**：指令嵌入图像（场景/物体级参考图）、文本描述、空间坐标。 - **TBA-CoT数据集**：通过Qwen-VL和DeepSeek-R1自动生成"行动前思考"（Think-Before-Action）的推理链数据，揭示动作背后的决策逻辑。 - **连续环境**：支持智能体自由移动与在线强化学习（RL），区别于离散网格环境。 - **OctoNav-R1 模型**： - **架构**：基于MLLMs（如LLaMA-VID）构建的视觉-语言-动作（VLA）模型，仅需2D视觉观测即可输出低层动作。 - **混合训练范式（HTP）**： 1. **监督微调（SFT）**： - **Action-SFT**：学习指令-轨迹对的行动映射。 - **TBA-SFT**：利用TBA-CoT数据学习生成"思考→行动"的结构化输出（`<Think>推理</Think><Action>动作</Action>`）。 2. **Nav-GRPO**：通过分组相对策略优化增强推理能力，定制奖励函数提升思考质量。 3. **在线强化学习（RL）**：在OctoNav-Bench环境中进行试错学习，优化策略。 - **关键创新**：首次将DeepSeek-R1的"思考-回答"范式迁移至具身导航，提升复杂指令的泛化能力。 ### 3. **实验结果** - **性能优势**：在OctoNav-Bench上全面超越现有方法（如GOAT-Bench、LHPR-VLN），各导航子任务准确率均显著提升。 - **仿真到现实迁移**：在物理机器人上初步验证了无需真实数据微调的跨域泛化能力。 - **消融实验**：验证了TBA机制和HTP训练范式的有效性。 ### 4. **意义** 通过统一的多任务基准与融合推理的模型设计，为构建**通用具身智能体**迈出关键一步，推动导航系统向人类级灵活指令理解与决策演进。</details> |
| 2025-06-10 | An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models | http://arxiv.org/abs/2506.09172v2 | <details><summary>展开</summary>本文提出MultiNet，一个开源软件工具包和基准套件，用于评估和适应多模态动作模型（VLAs）。核心要点如下： 1. **问题背景**：当前视觉-语言-动作模型（VLAs）泛化能力有限，尤其在未见过的领域（如机器人控制、游戏环境）表现不佳。社区缺乏统一的开源基准来系统评估这些模型。 2. **核心贡献**： - **大规模开源数据集**：整合超过1.3万亿tokens的多模态数据，涵盖图像标注、视觉问答、常识推理、机器人控制（如OpenX-Embodiment）和游戏任务（如Procgen）。 - **数据SDK工具包**：提供标准化数据下载与处理，将机器人/强化学习数据统一为TensorFlow格式。 - **评估框架与指标**：包括标准化测试分割和多样化指标（如MSE、Brier MAE、CIDEr、VQA准确率），用于量化模型在分布外（OOD）数据上的泛化能力。 - **通用提示框架（GenESIS）**：模块化框架，支持快速适配视觉语言模型（VLMs）到新任务，通过系统指令、环境上下文和动作空间定义提升多模态集成。 - **SoTA模型适配**：开源适配主流VLAs（如OpenVLA、Pi0）到新领域（如机器人数据），包括架构调整和后处理优化。 3. **研究结果**：利用MultiNet评估显示，现有VLAs在OOD任务（如机器人环境和程序生成游戏）中泛化能力显著不足，表现为高错误率和无效输出。 4. **目标与影响**：通过完全开源（代码库见GitHub），MultiNet旨在促进通用AI系统开发，提供可复现的评估标准，加速社区在模型泛化和适应性方面的进展。未来将扩展交互式在线基准和跨领域迁移学习研究。</details> |
| 2025-06-10 | FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency | http://arxiv.org/abs/2506.08822v1 | <details><summary>展开</summary>论文提出了一种名为FreqPolicy的高效流式视觉运动策略方法，通过频率一致性约束解决机器人操作中生成式策略推理速度慢的问题。核心创新点包括： 1. **频率一致性目标**：受时间序列和语音处理启发，在频域对齐动作片段的速度特征，增强时间连续性，提升单步动作生成质量。 2. **自适应频率损失**：针对机器人动作的结构化时变特性（如平稳/非平稳运动阶段），动态加权频域分量差异，优化关键频率成分的学习。 实验验证表明： - 在3个仿真基准的53项任务中超越现有方法（如MetaWorld任务84.2%成功率） - 集成视觉-语言-动作（VLA）模型实现5倍加速且性能无损（Libero的40项任务） - 真实机器人场景达到93.5Hz推理频率，代码将开源。 该方法首次将时序知识引入单步动作生成，解决了传统流式策略因迭代采样导致的实时性瓶颈，同时保持多模态动作建模优势。</details> |
| 2025-06-10 | Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing | http://arxiv.org/abs/2506.08462v1 | <details><summary>展开</summary>这篇论文提出了CIPHER框架（Control and Interpretation of Production via Hybrid Expertise and Reasoning），一种用于制造业的视觉-语言-动作（VLA）模型，核心贡献如下： 1. **混合推理架构**： - 集成视觉语言模型（Llama-3.2-Vision）与专用**过程专家模块**（ResNet-152），解决传统VLM在连续值回归任务（如材料流速预测）的精度不足问题 - 过程专家将视觉输入转换为定量特征，使MAE降低5倍（82.92→17.62） 2. **参数高效微调**： - 采用低秩自适应（LoRA）技术，减少52.4%内存占用和81%训练数据需求 - 仅微调2.4%参数，保留93.2%语言召回率，避免灾难性遗忘 3. **知识增强推理**： - 检索增强生成（RAG）整合领域知识库（3,930条制造知识） - 在100项新场景测试中，Elo评分达1238±9，超越GPT-4o-mini，实现物理常识驱动的链式推理 4. **工业应用验证**： - 在商用3D打印机部署： - 实现端到端自主控制（感知-决策-执行） - 支持自然语言/图像直接生成G代码（无几何监督） - 2.3Hz实时校正频率，控制误差仅0.215±0.090 - 展示涌现能力：温度调节、材料匹配、故障中止等复杂决策 5. **关键优势**： - 解决制造业数据稀缺问题（无需密集标注） - 结合工程精度与AI泛化能力 - 提供可解释决策链，支持高可靠性场景 局限包括空间推理待优化（支撑结构生成）和复杂几何约束处理。框架为工业自主系统提供新范式，代码已开源。</details> |
| 2025-06-10 | TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization | http://arxiv.org/abs/2506.08440v2 | <details><summary>展开</summary>这篇论文提出了TGRPO（轨迹分组相对策略优化）方法，用于改进视觉-语言-动作模型（VLA）在新任务上的微调。主要贡献如下： 1. **问题背景**：现有VLA模型依赖静态轨迹数据的监督微调（SFT），无法与环境实时交互，且性能受限于数据集质量。强化学习（RL）虽能提供闭环交互，但传统方法（如PPO）需训练值函数网络，效率较低。 2. **方法创新**： - 受GRPO（分组相对策略优化）启发，提出**TGRPO算法**，融合轨迹级与步级优势信号： - 轨迹级优势（$T_i$）评估整条轨迹的相对性能 - 步级优势（$S_{i,t}$）评估单步动作的相对性能 - 通过超参数$\alpha_1, \alpha_2$加权融合（$Adv_{i,t} = \alpha_1 S_{i,t} + \alpha_2 T_i$） - 实现**在线强化学习框架**：在多个并行环境中采样轨迹，动态计算融合优势信号更新策略，无需预存离线数据。 3. **实验验证**： - 在LIBERO仿真环境的10个物体操作任务上测试 - 使用OpenVLA作为基础模型，通过多阶段奖励函数（由LLM生成）提供密集反馈 - 结果：TGRPO在训练效率（速度提升2-3倍）和成功率上均优于基线方法（如GRAPE），消融实验证实轨迹级优势对性能提升贡献显著。 4. **核心价值**：TGRPO首次将轨迹级优化引入VLA的在线微调，解决了传统RL方法在时序任务中的适配问题，为VLA的实时环境适应提供了高效方案。 源码已开源：https://github.com/hahans/TGRPO</details> |
| 2025-06-09 | HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation | http://arxiv.org/abs/2506.08296v2 | <details><summary>展开</summary>本文提出HiBerNAC（分层脑仿真机器人神经代理集体），用于解决复杂操作任务中的三大挑战：持续性上下文记忆、不确定性下的多智能体协调和动态长时程规划。核心创新是将多模态视觉-语言-动作（VLA）模型与神经科学启发的多智能体机制相结合，主要包含三个模块： 1. **多智能体神经结构** - 模拟大脑分层处理：前额规划器（PFP，0.01Hz）负责高层任务分解，海马体模块（HM，0.1Hz）实现情景记忆存储 - 专用代理协同：感知代理（PA）处理多模态输入，语义代理（SA）解析指令，操作代理（MA）生成动作，检测代理（IA）提供误差校准 - 通过功能连接矩阵（\(\mathbf{F}_{ij}\)）实现代理间神经突触式交互 2. **异步任务管道** - 基于DBHTN的DAG规划器将高层任务分解为结构化子任务（如"抓取苹果"→"定位→移动→抓握"） - 三重记忆系统：潜在状态向量存储环境信息，动作历史队列记录执行轨迹，语义记忆保留任务概念 3. **反应式VLA系统（RVLA）** - 100Hz高频执行层：轻量化模型直接映射观测到机器人动作（如关节控制） - 支持动作分块与令牌压缩，确保实时性能 **实验结果** 在7-DoF Franka机器人上验证： - 长时程任务平均完成时间减少23% - 多路径任务成功率提升12-31%（基线模型均为失败） - 在动态遮挡、物体消失等场景展现鲁棒性 该方法通过神经科学启发的层次化架构和动态智能体专业化，为生物认知机制与机器人学习的融合提供了新路径。项目代码已开源。 --- *总结聚焦核心贡献（神经-多智能体融合架构）、关键技术（分层处理/异步管道）及实证效果（效率/成功率提升），符合简明扼要要求。*</details> |
| 2025-06-09 | Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework | http://arxiv.org/abs/2506.08185v2 | <details><summary>展开</summary>论文提出了一种代理性手术AI框架，用于预测外科医生个性化手势序列并量化隐私风险。核心要点如下： 1. **问题与创新**： - 现有手术AI系统忽略外科医生的个性化风格（如训练、经验、运动行为）。 - 提出结合离散扩散模型和视觉-语言-动作（VLA）框架的新方法，将手势预测建模为结构化去噪任务。 2. **方法**： - **多模态输入**：融合手术视频的视觉特征、任务意图的语言描述，以及外科医生身份和技能（GRS分数）的嵌入。 - **隐私保护**：通过第三方大型语言模型（LLM）的自然语言提示（如“Surgeon ID: 3, GRS: 3.75”）生成嵌入，避免直接暴露身份。 - **扩散过程**：前向过程添加离散噪声，反向过程基于条件重建原始序列。 3. **实验与结果**： - 在JIGSAWS数据集上验证，ID+GRS的LLM嵌入策略性能最佳（Top-1准确率83.89%），能学习独特的外科医生运动指纹。 - **隐私风险**：成员推理攻击显示，更丰富的嵌入（如ID+GRS）虽提升任务性能，但显著增加身份泄露风险（AUC达1.0），揭示个性化与隐私的权衡。 4. **贡献与意义**： - 首次在手术AI中量化个性化建模的隐私风险。 - 强调临床部署需平衡个性化收益与隐私保护（如差分隐私）。 - 代码已开源。</details> |
| 2025-06-09 | BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models | http://arxiv.org/abs/2506.07961v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse | http://arxiv.org/abs/2506.07639v2 | <details><summary>展开</summary>论文提出 **Fast ECoT** 方法，旨在解决具身思维链（ECoT）在机器人控制中因自回归生成推理步骤导致的高延迟问题。核心要点如下： 1. **问题背景** ECoT 通过生成中间推理步骤（如任务规划、物体定位）提升视觉-语言-动作（VLA）模型的性能和可解释性，但逐 token 生成推理链会引入显著延迟，阻碍实时部署。 2. **关键创新** - **推理复用**：利用高层推理的时序局部性（如任务目标更新率仅 8.4%），缓存并复用历史推理内容（图 2）。 - **并行生成**：将推理步骤拆分为独立模块并行生成，替代传统顺序生成（图 3）。 - **异步调度**：解耦推理与动作解码，优先响应低延迟动作请求，异步更新推理缓存（图 6）。 - **连续批处理**：动态调度变长序列，减少填充开销，提升 GPU 利用率（图 4）。 3. **优势** - **高效性**：在 LIBERO 仿真和真实机器人任务中，延迟降低 **7.5 倍**（从 4997ms 至 686ms），吞吐量提升（表 I、II）。 - **性能保持**：任务成功率持平或提升（LIBERO 平均成功率 80.0%，优于基线 73.3%）。 - **零训练开销**：无需模型修改或额外训练，可直接集成现有 VLA 流程。 - **可解释性保留**：动作忠实度（AF）指标验证推理质量未受损（图 9）。 4. **验证结果** - 仿真（LIBERO）和真实场景测试一致表明延迟显著降低（表 III），异步机制进一步优化响应速度。 - 复用高层推理可稳定控制策略，但过度低频更新会降低性能（表 IV）。 5. **意义** Fast ECoT 使 ECoT 推理接近实时控制需求，为具身智能的实用化部署提供关键技术支撑。 --- 总结：Fast ECoT 通过 **推理复用**、**并行生成** 和 **异步调度** 三大机制，高效压缩 ECoT 延迟，同时保持任务性能与可解释性，推动机器人 VLA 模型的实时应用。</details> |
| 2025-06-09 | BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation | http://arxiv.org/abs/2506.07530v1 | <details><summary>展开</summary>本文提出**BitVLA**，首个用于机器人操作的**1-bit视觉-语言-动作（VLA）模型**，核心创新如下： 1. **1-bit三元参数量化** - 所有权重参数压缩为三元值 `{-1, 0, 1}`，视觉编码器采用**1.58-bit权重+8-bit激活**（W1.58A8） - 通过**蒸馏感知训练策略**：以全精度视觉编码器为教师模型，结合语言建模损失（$\mathcal{L}_{\text{LM}}$）和表征对齐损失（$\mathcal{L}_{\text{aux}}$）压缩模型，减少量化误差 2. **高效架构设计** - 基于**1-bit LLM（BitNet b1.58 2B4T）** 和 **SigLIP-L视觉编码器** - 推理时采用**并行解码+动作分块**技术，提升实时控制效率 3. **性能与效率优势** - 在LIBERO机器人操作基准测试中，**性能媲美4-bit后量化的OpenVLA-OFT**（平均成功率94.8% vs 91.9%） - **内存占用仅为全精度模型的29.8%**（1.4GB vs 4.7GB），显著降低边缘设备部署门槛 - **无需大规模机器人预训练**，仅需视觉语言预训练+任务微调 4. **应用潜力** - 在视觉问答（VQA）任务中保持竞争力（零样本准确率51.5%） - 开源代码与模型权重，推动低资源机器人部署 > 论文链接：[https://github.com/ustcwhy/BitVLA](https://github.com/ustcwhy/BitVLA)</details> |
| 2025-06-09 | Real-Time Execution of Action Chunking Flow Policies | http://arxiv.org/abs/2506.07339v1 | <details><summary>展开</summary>这篇论文提出了一种名为**实时分块（RTC）** 的新算法，用于解决大型视觉语言动作模型（VLA）在实时控制任务中的高延迟问题。核心要点如下： 1. **问题背景** - 先进VLA模型（如扩散模型、流匹配模型）存在高推理延迟（>100ms），导致机器人控制出现动作暂停或块间不连贯的抖动。 - 传统动作分块策略（输出多步动作）虽提升时间一致性，但无法解决延迟引发的跨块跳变问题（如策略突变导致加速度异常）。 2. **核心方法：RTC** - **异步修复机制**：在执行当前动作块时，提前生成下一动作块。将延迟时间内的动作“冻结”（强制与上一块一致），后续动作通过**流匹配修复技术**生成。 - **软掩码技术**：对重叠时序的动作施加指数衰减权重（`权重 = c_i * (e^{c_i}-1)/(e-1)`），取代传统二值掩码，确保跨块动作平滑过渡（图4）。 - **引导权重裁剪**：改进修复算法稳定性，限制梯度规模（公式2中的`β`），避免少量去噪步时的发散问题（附录A.2验证）。 3. **实验验证** - **新基准测试**：在Kinetix模拟器构建12个高动态任务（如敏捷操作、移动），测试延迟鲁棒性。 - **真实任务**：6项双手操作任务（如划火柴，图1），在300ms延迟下RTC成功率比同步推理高40%。 - **关键优势**：比双向解码（BID）计算量低50%，任务吞吐量提升20%，加速度波动减少60%（图1底部）。 4. **应用价值** - 兼容任意扩散/流匹配VLA模型，无需重新训练。 - 解决了边缘设备部署VLA的实时性瓶颈，为机器人动态任务提供可靠控制框架。 > 📌 **总结**：RTC通过冻结关键帧+软掩码修复，在保证动作连贯性的同时实现模型异步推理，显著提升高延迟场景下的控制性能。</details> |
| 2025-06-08 | Robotic Policy Learning via Human-assisted Action Preference Optimization | http://arxiv.org/abs/2506.07127v2 | <details><summary>展开</summary>本文提出了一种名为HAPO（人辅助动作偏好优化）的方法，用于解决视觉语言动作（VLA）模型在机器人策略部署中的关键问题。核心要点如下： 1. **问题背景** VLA模型依赖专家演示，缺乏从失败中修正和学习的能力，限制了其在真实场景的可靠部署。 2. **方法创新** - **人机协作框架**：通过实时人工干预纠正执行失败，确保任务可靠完成，并收集干预轨迹数据。 - **动作偏好优化**： - 提出自适应重加权算法，解决VLA模型偏好优化的两大挑战：不可逆交互（无法复现相同状态）和标记概率失配（连续动作离散化导致的误差）。 - 利用卡尼曼前景理论构建目标函数，从二元合意性信号（成功/失败）中学习。 - 通过解码的连续动作指导离散动作标记的优化，动态调整样本权重以聚焦易失败动作。 3. **技术优势** - 平衡采样处理校正动作的比例失衡问题。 - 结合专家演示与干预数据，迭代优化策略，实现持续性能提升。 - 支持终身学习，适应动态任务环境。 4. **实验结果** - 在仿真和真实机器人任务（如精细装配）中验证了方法的泛化性和鲁棒性。 - 显著提升模型在未知扰动下的表现，并降低失败动作的发生率。 该方法为VLA模型提供了可靠的部署框架和持续学习机制，推动机器人系统在开放环境中的实际应用。</details> |
| 2025-06-07 | RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation | http://arxiv.org/abs/2506.06677v1 | <details><summary>展开</summary>论文提出RoboCerebra基准，用于评估机器人操作中的长时程推理能力。核心贡献包括： 1. **大规模数据集**： - 包含1000条模拟轨迹（平均9.1子任务/条） - 任务长度达2972步（6倍于现有基准） - 通过GPT生成任务→人工仿真执行→多阶段验证的流程构建 2. **分层框架**： - **System 2（高层）**：VLM规划器处理低频观测，生成子任务序列 - **System 1（低层）**：VLA控制器执行高频动作 - 内存模块协调两级系统 3. **多维度评估**： - 规划准确率（子任务序列匹配） - 反射能力（状态判断准确率） - 记忆性能（长时依赖任务成功率） - 效率指标（单位步长成功率） 4. **实验发现**： - 纯VLA模型在长时程任务中成功率仅7.84% - 分层框架显著提升至21.10%（Ideal场景） - GPT-4o作为规划器表现最佳（16.04%平均成功率） 该基准填补了现有机器人评估在长时程推理、动态场景适应和记忆依赖任务方面的空白，为VLM的System 2能力提供系统评测框架。 项目主页：robocerebra.github.io</details> |
| 2025-06-06 | DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models | http://arxiv.org/abs/2506.05667v1 | <details><summary>展开</summary>本文介绍了**DriveAction基准测试**，专为评估自动驾驶中的视觉-语言-动作（VLA）模型而设计，核心要点如下： 1. **解决现有基准的三大缺陷**： - **场景多样性不足**：现有基准依赖开源数据集，覆盖场景有限，缺乏真实驾驶的复杂性和人类行为特征。 - **动作标注不可靠**：传统标注依赖人工后处理，无法实时反映驾驶意图。 - **评估逻辑不匹配**：现有评估多为前向链式（感知→规划→动作），未对齐人类决策的依赖关系。 2. **DriveAction的核心创新**： - **真实用户贡献场景**：基于量产车用户主动提交的驾驶数据（覆盖中国148个城市），包含2,610个场景和16,185个QA对，涵盖匝道汇入、导航变道、复杂路口等7类关键场景（表2）。 - **人类偏好对齐的标注**：动作标签直接来自用户实时操作（如转向、减速），经多轮人工验证，排除错误/非法行为，并离散化为高层决策（匹配模型输出粒度）。 - **树状评估框架**（图1）： - **动作驱动**：以动作为根节点，动态关联语言理解（如导航跟随）和视觉任务（如车道检测）。 - **灵活评估模式**：支持全流程（V-L-A）、纯视觉（V-A）、纯语言（L-A）和无引导（A）四种模式，量化多模态依赖性。 - **场景信息增强**：提供连续帧图像、实时导航指令和车速信息，减少模型"幻觉"（图4）。 3. **关键实验结果**： - **多模态必要性**：最优模型需同时使用视觉和语言输入，移除视觉精度降3.3%，移除语言降4.1%，两者均无则降8.0%（表3）。 - **任务特异性分析**：导航相关任务（如变道）依赖语言理解，静态物体识别（如交通标志）依赖视觉（表4-5）。推理模型（如o1）在复杂决策中表现更优。 4. **资源开放**： 基准数据集发布于[Hugging Face](https://huggingface.co/datasets/LiAuto-DriveAction/drive-action)，支持自动驾驶模型的人性化决策能力评估。</details> |
| 2025-06-04 | SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models | http://arxiv.org/abs/2506.03574v1 | <details><summary>展开</summary>本文提出SwitchVLA框架，解决现有视觉-语言-动作（VLA）模型在执行过程中无法动态切换任务的局限。核心创新点包括： 1. **问题定义**：传统VLA模型假设任务意图固定，无法响应执行中的指令变更。SwitchVLA将任务切换建模为**执行感知的行为调制问题**，通过接触状态（物理交互检测）和行为模式（前进/回退/跃迁）双监督信号实现动态适应。 2. **方法设计**： - **多行为条件策略**：通过分段专家轨迹识别接触阶段，使策略能推断任务进度 - **统一架构**： - 视觉-语言-接触嵌入模块融合多模态输入 - 条件执行专家同步预测接触状态、行为模式（0:前进/1:回退/2:跃迁）和动作块 - **无需额外数据**：利用现有轨迹数据训练，通过行为启发式自动生成监督信号 3. **实验验证**： - 在仿真和真实机器人操作任务中，相比基线模型显著提升任务切换流畅度（最高+38%成功率） - 关键优势：指令遵循鲁棒性、动态恢复能力、多阶段任务泛化性 - 典型场景：当用户中途修改指令时，模型能自动执行回退操作再转向新任务 该方法解决了服务机器人等动态场景的核心挑战，为实时人机交互提供了新范式。</details> |
| 2025-06-03 | Adversarial Attacks on Robotic Vision Language Action Models | http://arxiv.org/abs/2506.03350v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-02 | SAB3R: Semantic-Augmented Backbone in 3D Reconstruction | http://arxiv.org/abs/2506.02112v2 | <details><summary>展开</summary>本文提出了一种名为SAB3R（Semantic-Augmented Backbone）的3D重建方法，核心创新点如下： 1. **新任务定义** - 提出"Map and Locate"任务，首次将开放词汇分割与3D重建统一： - 输入：无位姿视频 + 文本查询 - 输出：3D点云地图 + 开放词汇目标定位 - 解决传统方法依赖预扫描点云/精确位姿的局限，更贴近具身智能实际场景 2. **方法架构** - 基于MASt3R框架，通过轻量级蒸馏注入2D语义： - 从CLIP/DINOv2等2D骨干网络提取稠密特征 - 使用FeatUp增强特征分辨率 - 单次前向传播同时输出： - 点云地图 - 像素级CLIP特征 - 像素级DINOv2特征 3. **关键技术** - 多任务学习框架防止灾难性遗忘： - 保留原有深度估计头（Head₃ᴰ） - 新增2D语义回归头（Head₂ᴰ） - 损失函数设计： - $\mathcal{L}_{total} = \mathcal{L}_{conf} + \beta\mathcal{L}_{match} + \gamma\mathcal{L}_{2D}$ - 联合优化几何一致性（$\mathcal{L}_{conf}$）和语义对齐（$\mathcal{L}_{2D}$） 4. **核心优势** - 统一模型超越分离式方案（MASt3R+CLIP）： - 在Map and Locate任务上精度更高 - 推理效率提升（单模型vs多模型） - 支持零样本能力： - 单目深度估计 - 相机位姿估计 - 开放词汇语义分割 5. **验证体系** - 构建ScanNet子集作为评估基准 - 提出新指标： - mComp/mdComp（重建完整性） - mIoU/Acc（语义分割精度） 该方法通过语义增强的3D骨干网络，首次实现了无位姿输入下的重建-识别-重组联合优化，为具身智能提供新基础架构。</details> |
| 2025-06-02 | Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning | http://arxiv.org/abs/2506.01953v1 | <details><summary>展开</summary>本文提出了一种名为Fast-in-Slow (FiS) 的双系统视觉语言动作（VLA）基础模型，其核心创新点及贡献总结如下： ### 核心创新 1. **统一的双系统架构** - 将快速执行系统（System 1）嵌入预训练的视觉语言模型（System 2）内部，通过参数共享实现协调。 - 不同于现有双系统方法（分离的System 1策略头），FiS将System 2的末端Transformer块重构成System 1执行模块，保留VLM的推理能力同时支持高频控制。 2. **异步多模态设计** - **异步频率**：System 2（推理）以低频处理任务理解和场景语义（输入：2D图像+语言指令），System 1（执行）以高频生成动作（输入：机器人状态+3D点云+2D图像）。 - **3D空间感知**：引入轻量级3D分词器处理点云，利用共享视觉编码器提取几何特征，提升操作精度。 3. **双感知协同训练** - **System 1**：采用扩散模型学习动作生成（噪声动作注入）。 - **System 2**：保留自回归下一词预测目标维持推理能力。 - 联合优化损失函数：\(\mathcal{L}_{\text{FiS-VLA}} = \mathcal{L}_{\text{fast}} + \mathcal{L}_{\text{slow}}\)。 ### 实验结果 1. **性能优势** - **仿真任务（RLBench）**：平均成功率比最优方法高8%（达69%），在10项任务中多项刷新记录（如Close box: 100%, Close laptop: 100%）。 - **实物任务**：双机械臂操作成功率提升11%，支持异构机器人（AgileX/AlphaBot）和不同控制模式（末端位姿/关节位置控制）。 2. **高频执行** - 在System 2与System 1频率比1:8时，控制频率达117.7 Hz（NVIDIA 4090 GPU，动作块大小为8）。 3. **强泛化性** - 对未见过的物体、复杂背景和光照变化保持鲁棒性，跨任务和场景适应性强。 ### 项目资源 - **开源地址**：[https://fast-in-slow.github.io/](https://fast-in-slow.github.io/) ### 总结 FiS-VLA通过统一嵌入双系统、异步多模态输入和协同训练策略，解决了传统VLA模型执行效率低与推理能力难以协同的瓶颈，在性能、频率和泛化性上均达到新高度。</details> |
| 2025-06-02 | SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics | http://arxiv.org/abs/2506.01844v1 | <details><summary>展开</summary>基于论文内容，SmolVLA的核心要点总结如下： 1. **轻量高效架构** - 提出小型视觉-语言-动作（VLA）模型SmolVLA，通过**层跳过**（仅使用VLM前半层）、**视觉令牌压缩**（每帧64个令牌）和**交叉/自注意力交替模块**降低计算开销。 - 采用**流匹配动作专家**（Flow Matching）生成连续动作块，支持低延迟推理。 - 模型可在单GPU训练，CPU/消费级GPU部署，参数量为同类模型的1/10。 2. **社区驱动数据训练** - 仅使用**22.9K公开社区数据集**（481个数据集，10.6M帧），通过VLM自动标注任务指令并统一相机视角。 - 数据量比主流方法少一个数量级，降低数据收集成本。 3. **异步推理系统** - 设计**解耦式异步推理栈**：动作执行与感知预测分离，通过队列机制实现动作块重叠生成。 - 解决同步推理的延迟问题，提升响应速度（详见图2和算法1）。 4. **性能验证** - 在仿真和现实机器人任务中，SmolVLA性能匹配10倍参数量模型，同时推理速度提升3倍。 - 关键优化包括：早期VLM层特征提取、因果注意力机制和动作块长度调整（第4.7节消融实验）。 5. **开源贡献** 发布完整代码、预训练模型及数据集，推动低成本机器人研究发展。</details> |
| 2025-06-01 | OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation | http://arxiv.org/abs/2506.01196v1 | <details><summary>展开</summary>OG-VLA 是一种新型的视觉-语言-动作（VLA）模型，通过正交图像生成实现 3D 感知的机器人操作策略。其核心要点如下： 1. **核心创新** - 结合 VLA 模型的泛化能力（处理新指令和场景）与 3D 感知策略的鲁棒性（抵抗相机/姿态变化）。 - **正交投影机制**：将多视角 RGBD 观测投影到点云，渲染为固定正交视图（如前视、顶视），确保输入输出空间一致性，实现视角不变性。 2. **方法架构** - **输入**：语言指令 + 多视角 RGBD 图像（含相机位姿）。 - **处理流程**： - 点云构建与正交视图渲染。 - 视觉主干（ImageBind）提取特征 → 大语言模型（Vicuna-7B）生成动作 token。 - 图像扩散模型（Stable Diffusion）生成带注释的正交图像，编码末端执行器位姿（位置、旋转、开合状态）。 - **输出解码**：从生成图像中解析 6-DOF 末端执行器关键帧位姿（3D 位置通过多视图热力图融合优化，旋转通过各视图角度预测）。 3. **关键优势** - **泛化性提升**：在仿真基准测试中（Arnold/Colosseum），对**未见过的环境/物体**相对性能提升超 40%（SOTA）。 - **鲁棒性**：对相机位姿扰动、物体颜色/位置变化和干扰物具有强鲁棒性。 - **数据效率**：仅需 3-5 次真实机器人演示即可适应新任务（如开门、拾取）。 4. **实验验证** - **仿真**：在 Arnold（泛化性测试）和 Colosseum（扰动鲁棒性测试）上超越基线模型（如 PerAct、RVT）。 - **实物**：成功部署于机械臂，完成跨场景任务（如操作抽屉、瓶子），验证快速适应能力。 5. **局限性与资源** - 限制：依赖精确点云重建，复杂场景生成可能受限。 - 资源：代码和视频见项目页 [og-vla.github.io](https://og-vla.github.io/)。</details> |
| 2025-06-01 | GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering | http://arxiv.org/abs/2506.01174v1 | <details><summary>展开</summary>本文提出GraphPad，一种用于具身问答的可修改3D场景图记忆系统。针对静态场景图无法适应任务变化的缺陷，GraphPad的核心创新在于通过语言驱动的API实现推理时动态更新： 1. **问题背景**：传统预构建的3D场景图常遗漏任务关键信息（如物体/空间关系），导致具身问答（EQA）性能受限 2. **解决方案**： - **结构化场景记忆（SSM）**：包含场景图（带对象轨迹）、图形便签（任务注释）、帧内存（关键帧）和导航日志（帧索引） - **可修改API**：VLM通过`find_objects`（插入新物体）、`analyze_objects`（注释属性）、`analyze_frame`（联合更新）动态修补知识缺口 - **推理循环**：VLM迭代调用API（平均1.9次/查询），整合新信息至场景图 3. **关键结果**： - 在OpenEQA基准测试达**55.3%准确率**（+3.0pp优于同VLM图像基线），且输入帧数减少5倍 - 消融实验显示导航日志贡献最大（+9.6pp），帧级API优于节点级API - 在属性识别（+20.3pp）、功能推理（+5.7pp）等需细粒度分析的类别提升显著 4. **贡献价值**：无需额外训练/数据，通过语言引导的感知更新实现任务自适应记忆，为具身智能体提供高效动态工作空间 > 分析依据：论文摘要（§Abstract）、方法架构（§3）、实验结果（§4.1-4.5）及对比基线（Table 1）。</details> |
| 2025-05-31 | LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks | http://arxiv.org/abs/2506.00411v1 | <details><summary>展开</summary>论文提出了一种统一视觉-语言-动作模型LoHoVLA，用于解决长视野具身任务中的关键挑战。核心要点如下： 1. **问题背景**： - 现有视觉语言动作（VLA）模型在长视野任务（需多步骤规划）中表现不佳，缺乏有效的任务分解能力。 - 分层架构（独立规划器+控制器）存在协调性差和泛化能力弱的问题。 2. **模型创新**： - **统一框架**：LoHoVLA将高层任务规划（生成语言子任务）和低层动作控制（预测动作令牌）集成到单一模型中，共享预训练视觉语言模型（PaliGemma）的骨干网络。 - **分层闭环控制**：引入动态纠错机制——若子任务执行失败超过阈值则重新规划，否则仅更新动作，减少冗余计算。 3. **数据集**： - 构建合成数据集**LoHoSet**（基于Ravens模拟器），包含20个长视野任务，每个任务提供1000条专家演示（含视觉观测、语言目标、子任务和动作）。 4. **实验结果**： - 在Ravens模拟器上显著超越分层架构和标准VLA模型，尤其在未见过任务上展现强泛化能力。 - 消融实验验证了统一架构和闭环控制机制的有效性。 5. **意义**： - 为长视野具身任务提供端到端解决方案，证明统一架构在提升任务规划、动作执行和泛化性能方面的潜力。 总结：LoHoVLA通过统一规划和控制的联合表示学习，结合闭环纠错机制，解决了长视野任务中的协调与泛化问题，为具身智能提供了新思路。</details> |
| 2025-05-30 | Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction | http://arxiv.org/abs/2505.24156v1 | <details><summary>展开</summary>论文提出**CogRobot框架**，用于解决双机械臂操作策略泛化性差的问题。核心创新点包括： 1. **两阶段视频预测方法** - **文本→光流生成**：利用预训练文本-视频模型（CogVideoX），将语言指令转化为光流序列，捕捉物体运动趋势（减少语言歧义）。 - **光流→视频预测**：基于光流生成未来轨迹视频，显式建模机械臂与物体的动态交互（降低数据需求）。 2. **降低数据依赖** - 光流作为中间表示，仅需少量双臂操作数据微调模型，避免直接学习低层动作的高复杂度。 3. **实验验证** - 在仿真和真实双臂机器人平台（Realman）测试，相比基线方法： - 准确预测符合物理规律的运动轨迹 - 提升多步骤任务成功率（如开盖、放置物体） - 对未见场景展现强泛化性。 **关键效果**：通过光流桥接高层指令与底层动作，实现高效、可泛化的双臂协调策略，减少90%以上真实机器人数据需求。</details> |
| 2025-05-29 | Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models | http://arxiv.org/abs/2505.23757v1 | <details><summary>展开</summary>本文提出了Impromptu VLA数据集，旨在解决自动驾驶在非结构化道路（如边界模糊、临时交规变化等场景）的数据稀缺问题。核心贡献包括： 1. **新型数据集构建** - 从8个开源数据集（总计200万视频片段）中精选80,000+片段，聚焦四类挑战性场景：道路边界模糊、临时交规变化、非常规动态障碍物、恶劣路况 - 提供丰富的多任务标注：规划导向的问答对（场景描述、交通信号检测、动态意图预测等）及动作轨迹 2. **创新数据处理流程** - 建立基于VLM的层级分类法（Qwen2.5-VL + 思维链推理） - 通过时序稳定性过滤和人工验证确保标注质量 3. **显著性能提升验证** - **闭环测试**：在NeuroNCAP基准上，3B模型平均安全分提升21.5%（1.77→2.15/5.0），碰撞率降低9.7%（72.5%→65.5%） - **开环预测**：nuScenes轨迹预测L2误差达0.30m，接近SOTA方法（EMMA+的0.29m） - 诊断评估显示VLM在感知/预测/规划能力均有提升 数据集与模型已开源，项目页：http://Impromptu-VLA.c7w.tech/ 代码仓库：https://github.com/ahydchh/Impromptu-VLA</details> |
| 2025-05-29 | Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better | http://arxiv.org/abs/2505.23705v1 | <details><summary>展开</summary>这篇论文提出了一种名为“知识绝缘”的新方法，用于改进视觉-语言-动作模型（VLA）的训练效率和性能。核心要点如下： ### 核心问题 - 传统VLA模型在引入**连续动作输出模块**（如扩散模型或流匹配动作专家）以实现高效实时控制时，会破坏预训练视觉语言模型（VLM）的知识迁移： 1. **训练速度大幅下降**：新模块的随机初始化梯度干扰VLM主干的优化。 2. **语义知识退化**：VLM的语言理解和视觉推理能力受损。 3. **控制性能降低**：影响策略的泛化性和任务成功率。 ### 解决方案：知识绝缘（Knowledge Insulation） 1. **双轨训练机制**： - **VLM主干**：仅通过**离散化的动作令牌**（如FAST压缩表示）和通用VLM数据训练，使用标准交叉熵损失。保持知识迁移的高效性和稳定性。 - **动作专家**：独立训练**流匹配或扩散模型**生成连续动作，**阻断其梯度回传至VLM主干**，避免干扰。 2. **推理优势**： - 仅需轻量级动作专家生成连续动作，满足高频率实时控制需求（如机械臂操作）。 ### 关键优势 - ✅ **训练更快**：离散动作的稳定训练加速收敛（比传统方法快2-3倍）。 - ✅ **运行更快**：小型动作专家高效输出连续动作。 - ✅ **泛化更好**：保留VLM的语义知识，提升语言指令遵循和跨任务泛化能力。 - ✅ **兼容性**：可同时利用机器人数据和通用VLM数据联合训练。 ### 实验验证 在移动双臂机器人等复杂长周期任务（如叠衣服、桌面清理）及标准测试集（DROID、LIBERO）上验证： - 相比基线（如直接添加动作专家的\(\pi_0\)），成功率提升15-30%。 - 语言理解准确率恢复至接近原始VLM水平。 ### 创新点 - 首次提出**梯度隔离机制**解决VLA中连续动作模块的训练冲突。 - 为高效、高精度VLA模型提供可扩展框架（[项目页面](https://pi.website/research/knowledge_insulation)）。 > 总结：知识绝缘技术通过解耦离散表示学习和连续动作生成，实现了VLA模型的快速训练、实时执行与知识高效迁移，为具身智能提供了新思路。</details> |
| 2025-05-29 | TrackVLA: Embodied Visual Tracking in the Wild | http://arxiv.org/abs/2505.23189v1 | <details><summary>展开</summary>这篇论文提出了TrackVLA，一种视觉-语言-动作（VLA）模型，用于解决具身视觉跟踪（EVT）任务。核心创新点如下： 1. **统一架构设计**： - 提出并行预测框架，共享LLM主干（Vicuna-7B），同时处理目标识别（语言建模头）和轨迹规划（基于锚点的扩散模型） - 采用网格池化策略压缩视觉特征（EVA-CLIP编码），细粒度特征用于最新观测，粗粒度用于历史帧 2. **高效动作模型**： - 锚点扩散策略通过K-means聚类预定义轨迹模式，仅需2步去噪（比标准扩散模型快5倍） - 联合优化轨迹回归损失（MSE）和分类损失（BCE） 3. **大规模数据集**： - 构建EVT-Bench数据集（21,771训练/4,215测试场景），包含： - 100+高保真人体化身（SMPL-X模型 + 随机纹理） - 三类难度任务：单目标跟踪/干扰环境跟踪/外观模糊目标跟踪 - 收集855K跟踪样本 + 855K识别样本（362K人体识别 + 493K开放世界VQA） 4. **性能优势**： - 在Gym-UnrealCV等基准上零样本达到SOTA - 10 FPS实时推理，鲁棒应对遮挡和动态场景 - 消融实验验证：1:1的跟踪-识别数据比例最优 5. **开源贡献**： - 公开EVT-Bench数据集和代码（项目页：https://pku-epic.github.io/TrackVLA-web） 该方法通过协同优化识别与规划，解决了传统模块化方法错误累积的瓶颈，在仿真和现实场景均展现强泛化能力。</details> |
| 2025-05-28 | Zero-Shot 3D Visual Grounding from Vision-Language Models | http://arxiv.org/abs/2505.22429v1 | <details><summary>展开</summary>论文提出了一种零样本3D视觉定位框架**SeeGround**，利用2D视觉语言模型（VLMs）实现无需3D标注数据的开放场景物体定位。核心创新点包括： 1. **混合模态表示**：将3D场景转换为查询对齐的2D渲染视图与空间文本描述（包含物体语义和位置），解决VLMs与3D数据的模态差异问题。 2. **视角自适应模块**：根据语言查询动态选择最优渲染视角（以锚物体为中心），增强局部细节和空间上下文的相关性。 3. **融合对齐模块**：通过视觉提示（高亮候选区域）整合视觉特征与空间文本，减少遮挡和歧义，提升定位精度。 **实验效果**： - 在ScanRefer和Nr3D基准测试中，分别以**7.7%**和**7.1%**显著超越现有零样本方法。 - 接近全监督模型性能（如ScanRefer上Acc@0.5达**68.9%**），且在复杂场景（纹理、形状、方向等属性）中展现强泛化能力。 - 无需3D特定训练，仅依赖预训练的2D-VLMs（如Qwen-VL）实现开放词汇理解。 该方法为3D视觉定位提供了一种高效可扩展的解决方案，适用于机器人导航、增强现实等场景。 --- 总结自论文核心贡献：零样本设计、动态视角选择、跨模态对齐机制及SOTA实验结果。</details> |
| 2025-05-28 | ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation | http://arxiv.org/abs/2505.22159v3 | <details><summary>展开</summary>论文提出 **ForceVLA** 框架，用于增强视觉-语言-动作（VLA）模型在接触丰富的机器人操作任务中的性能。核心要点如下： 1. **问题**：传统VLA模型依赖视觉和语言输入，在接触密集任务（如插入、装配）中因忽略力反馈而表现不佳，尤其在视觉遮挡或动态不确定时。 2. **方法**： - 引入 **FVLMoE**（力感知专家混合模块），动态融合预训练的视觉-语言嵌入与实时6轴力信号，通过门控机制路由模态特定专家。 - 构建 **ForceVLA-Data** 数据集，包含5个任务（如插头插入、黄瓜削皮）的同步视觉、本体感觉和力-扭矩数据。 3. **结果**： - 在5个任务上平均成功率 **60.5%**，比强基线（π₀模型）提升 **23.2%**，其中插头插入任务达 **80%**。 - 消融实验验证FVLMoE的关键性；力信号直接拼接仅提升3%，而FVLMoE提升显著。 4. **优势**： - 支持跨任务泛化（如新物体、遮挡场景），高效整合多模态信号，提升物理交互的鲁棒性。 - 开源代码和数据集，促进社区研究。</details> |
| 2025-05-28 | ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge | http://arxiv.org/abs/2505.21906v2 | <details><summary>展开</summary>论文提出ChatVLA-2，一个新颖的视觉-语言-动作（VLA）模型，旨在通过预训练知识实现开放世界具身推理。核心创新点包括： 1. **模型架构**：采用动态混合专家（Mixture-of-Expert）机制，解耦多模态理解与机器人控制的特征空间，同时保留共享表示；引入推理跟随增强模块，确保动作输出与内部推理一致。 2. **训练策略**：两阶段管道——第一阶段联合训练图像-文本数据与机器人数据，保留预训练视觉语言模型（VLM）知识；第二阶段冻结VLM，仅微调动作专家，强化推理-动作对齐。 3. **能力验证**：通过数学匹配（解决未见过方程并选择数字卡片）和玩具放置（处理新物体和空间指令）实验，证明模型在OCR、数学推理和空间理解上的开放世界泛化能力，显著超越OpenVLA、DexVLA等基线。 该方法有效解决了VLA模型微调时知识丢失问题，推动了具身推理的通用化发展。</details> |
| 2025-05-27 | Hume: Introducing System-2 Thinking in Visual-Language-Action Model | http://arxiv.org/abs/2505.21432v4 | <details><summary>展开</summary>本文提出Hume模型，一种引入系统2慢思考的双系统视觉-语言-动作（VLA）模型，用于提升机器人复杂任务的处理能力。核心要点如下： 1. **双系统架构设计** - **System 2（慢思考）**：基于预训练VLM，新增动作去噪头和价值查询头。通过重复采样生成多个候选动作块，利用价值查询头评估状态-动作值（Q值），选择最高Q值的动作块作为输出（频率：4Hz）。 - **System 1（快响应）**：轻量级扩散策略，接收System 2选定的动作块，结合实时视觉和机器人状态，通过级联动作去噪生成精细动作（频率：90Hz）。 2. **关键技术创新** - **价值引导思考**：System 2通过流匹配去噪生成候选动作块，并利用Q函数评估其长期收益，实现基于价值的决策。 - **级联动作去噪**：System 1对System 2的输出分段去噪，提升动作精度，平衡思考速度与执行实时性。 - **异步协作机制**：双系统异步工作，System 2低频规划，System 1高频执行，确保复杂任务中的实时响应。 3. **显著性能优势** - **仿真测试**：在LIBERO基准上比π0模型成功率提升4.4%，在Simpler基准提升25.9%。 - **实物部署**：在21种真实机器人场景中成功率平均提升12.9%，尤其在视角/光照变化、新物体/环境等复杂条件下表现鲁棒。 - **消融实验**：验证了价值引导和级联去噪对性能的关键作用（详见第4.3节）。 4. **应用价值** Hume首次将系统2慢思考引入机器人VLA模型，解决了语义模糊动作的推理难题，为动态、灵巧的机器人控制提供了新范式。 **总结**：Hume通过双系统协作和价值引导的慢思考机制，显著提升了VLA模型在复杂机器人任务中的决策精度与适应性，在仿真和实物测试中均达到SOTA性能。</details> |
| 2025-05-27 | Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models | http://arxiv.org/abs/2505.21200v1 | <details><summary>展开</summary>这篇论文提出了一种名为**FlashVLA**的高效推理框架，用于加速视觉-语言-动作（VLA）模型。核心创新点如下： ### 核心问题 - VLA模型（如OpenVLA）存在**双重冗余**： 1. **动作级冗余**：连续动作步骤的输出高度相似（>90%步骤变化微小）。 2. **视觉令牌冗余**：大量视觉令牌对推理贡献度低（如附录C所示）。 ### 解决方案：FlashVLA框架 1. **令牌感知动作重用（Token-Aware Action Reuse）** - **动态跳过机制**：通过轻量级"FlashTrigger"模块判断当前帧是否满足重用条件： - 计算连续动作向量的夹角变化（公式7） - 结合视觉令牌稳定性分析 - 若满足条件（如动作方向变化<阈值），则直接复用前一帧动作，**跳过完整推理**。 2. **信息导向的视觉令牌压缩（Information-Guided Token Pruning）** - 基于**信息贡献理论**（公式2）： - 对注意力输出矩阵进行SVD分解 - 计算每个令牌的信息贡献得分 \( C(x) = \sum \|u_{xi} \sigma_i\| \) - 保留高贡献令牌（如Top 62.5%），显著降低计算量（FLOPs减少55.7%）。 ### 关键优势 - **无需训练**：即插即用，兼容Flash Attention等现有优化。 - **双路径加速**： - **重用路径**：零计算成本（稳定动作阶段） - **轻量路径**：压缩视觉令牌（动态变化阶段） - **效率提升**（LIBERO基准测试）： - FLOPs降低55.7% - 推理延迟减少36.0% - 任务成功率仅下降0.7%（99.3%→98.6%） ### 实验验证 - 在**LIBERO**四大任务（Spatial/Object/Goal/Long）上验证： - 视觉令牌压缩至62.5%时，计算效率最优。 - 消融实验证明动作重用贡献延迟降低的70%。 - 泛化性：在VLAbench环境保持>98%成功率。 ### 意义 首次实现VLA模型的**训练无关加速**，为实时机器人控制提供轻量级解决方案，符合"三思而后行（Think Twice, Act Once）"的高效执行范式。</details> |
| 2025-05-27 | EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models | http://arxiv.org/abs/2505.21567v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Hierarchical Instruction-aware Embodied Visual Tracking | http://arxiv.org/abs/2505.20710v1 | <details><summary>展开</summary>### 论文要点总结： 1. **研究问题** - 提出 **用户为中心的具身视觉跟踪（UC-EVT）** 任务，要求智能体： - 理解动态用户指令（如调整目标、角度、距离）。 - 实时响应快速移动目标。 - 泛化至未见过的环境、目标和指令组合。 2. **现有方法局限性** - **RL模型**：缺乏指令理解能力，泛化性差。 - **VLA/VLM大模型**：推理速度慢（0.5–3 FPS），无法实时跟踪；依赖标注数据，泛化弱。 - **端到端模型**：难以弥合高层指令与底层动作的语义差距。 3. **解决方案：HIEVT模型** - **核心创新**：引入 **空间目标（Spatial Goal）** 作为中介，桥接指令与动作： - **LLM语义-空间目标对齐器**： - 语义解析 → 提取目标属性（如“蓝色汽车”）。 - 空间目标生成 → 通过CoT推理将指令转为边界框调整（如“靠近”→增大框尺寸）。 - 检索增强校正 → 利用记忆库优化目标框准确性。 - **RL自适应目标对齐策略**： - 输入视觉观测与空间目标，输出动作信号。 - 离线训练通用策略，实现跨环境零样本泛化。 4. **实验验证** - **数据集**：构建超1000万条轨迹的训练集。 - **测试环境**：1个训练环境 + 9个挑战性未见环境（如复杂动态场景）。 - **结果**： - 显著优于RL、VLA、VLM基线（如指令跟随精度提升35%）。 - 实时性达15 FPS，支持动态目标跟踪。 - 真实机器人部署验证鲁棒性（硬件设置见附录G）。 5. **贡献总结** - 提出UC-EVT新任务及评估基准。 - 设计分层模型HIEVT，解决指令-动作语义鸿沟。 - 开源代码与数据集（[项目链接](https://sites.google.com/view/hievt)）。 --- **关键公式**： 目标函数分解为两阶段： $$\mathcal{D}(\mathcal{I}_t, \mathcal{S}_t) \approx \underbrace{\mathcal{D}(\mathcal{I}_t, \mathcal{G}_{inter}(t))}_{\text{语义-空间对齐}} + \underbrace{\mathcal{D}(\mathcal{G}_{inter}(t), \mathcal{S}_t)}_{\text{目标驱动策略}}$$ 其中 $\mathcal{G}_{inter}(t)$ 为空间目标（边界框），$\mathcal{S}_t$ 为智能体状态。</details> |
| 2025-05-26 | What Can RL Bring to VLA Generalization? An Empirical Study | http://arxiv.org/abs/2505.19789v2 | <details><summary>展开</summary>这篇论文通过实证研究探讨了强化学习（RL）对视觉-语言-动作（VLA）模型泛化能力的提升效果。核心要点如下： 1. **问题背景**： VLA模型通常通过监督微调（SFT）训练，但在分布偏移下易出现复合误差，泛化能力受限。RL通过试错优化任务目标，有望提升泛化性，但其对VLA的具体效益尚不明确。 2. **研究方法**： - 构建涵盖**视觉**（新背景/纹理）、**语义**（新物体/指令）和**执行**（初始状态扰动/动态干扰）三个维度的泛化评测基准。 - 以"抓取-放置"任务为实验场景，对比PPO、GRPO、DPO等RL算法与SFT的泛化性能。 3. **关键发现**： - **RL vs. SFT**： - **语义理解**：RL（尤其是PPO）显著提升对未见物体、新容器及多样化指令的理解能力。 - **执行鲁棒性**：RL在初始状态扰动和动态干扰下表现更稳定。 - **视觉鲁棒性**：RL与SFT在背景/纹理变化下性能相当。 - **算法对比**：PPO优于GRPO和DPO，因后者在序列决策任务中优势估计不稳定。 - **高效训练**：提出PPO微调方案，包括**共享Actor-Critic主干**、**VLA预热初始化**和**最小化PPO训练轮次**，显著降低计算成本。 4. **实践意义**： - 为VLA模型提供可复现的RL微调方案（代码见https://rlvla.github.io）。 - 证明RL能突破SFT的数据局限，在语义和执行层面实现更优的零样本泛化。 5. **局限性**： 实验聚焦单任务场景，未来需探索多任务及长视野任务的RL泛化效果。 综上，RL微调（尤其PPO）可针对性提升VLA的语义与执行泛化能力，为具身智能模型优化提供新路径。</details> |
| 2025-05-26 | RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback | http://arxiv.org/abs/2505.19767v1 | <details><summary>展开</summary>论文提出了一种名为RFTF（Reinforcement Fine-tuning with Temporal Feedback）的新型强化微调方法，用于提升具身智能体的性能。核心要点如下： 1. **问题背景** 现有具身智能体通常依赖行为克隆训练，存在数据成本高、泛化能力有限的问题。传统强化微调方法使用稀疏奖励（仅基于任务成败），无法为动作提供细粒度反馈，导致训练效率低下。 2. **核心创新** - **时序反馈的价值模型**：设计一个无需动作标签的模型，通过对比学习预测状态价值（利用专家轨迹中状态价值随时间单调递增的特性），生成密集奖励信号。 - **强化微调框架**：结合PPO算法，引入奖励重塑（Reward Shaping）和广义优势估计（GAE），并加入KL散度约束以防止策略偏移。 3. **技术优势** - **数据高效**：价值模型训练仅需成功轨迹的时序信息，无需机器人动作标注。 - **奖励优化**：通过密集奖励替代稀疏奖励，解决部分正确动作的评估问题，提升训练稳定性。 4. **实验结果** - **泛化能力**：在CALVIN ABC-D基准测试中达到SOTA（平均任务完成长度4.296），超越Seer-Large等模型。 - **适应能力**：在新环境（CALVIN-D）中仅需少量微调，平均任务长度达4.301，实现快速适应。 - **消融实验**：验证密集奖励的关键作用（稀疏奖励导致性能下降0.2-0.3）。 5. **贡献总结** - 提出首个无需动作标签的密集奖励强化微调方法； - 开发基于时序信息的价值模型生成细粒度奖励； - 实现具身智能体在泛化和适应能力的显著提升。 该方法为降低具身智能训练数据依赖提供了新思路，未来将拓展至真实机器人场景。</details> |
| 2025-05-26 | DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving | http://arxiv.org/abs/2505.19381v4 | <details><summary>展开</summary>本文提出DiffVLA，一种结合视觉语言模型（VLM）和扩散规划的端到端自动驾驶框架。核心创新点包括： 1. **混合感知模块**： - 稀疏分支提取实例级障碍物/地图信息（3D检测+向量化表达） - 密集分支生成鸟瞰图（BEV）特征，二者互补增强环境理解 2. **VLM决策引导**： - 基于Senna-VLM架构处理多视角图像 - 输出高层驾驶指令（转向/加速）并与导航指令融合 3. **扩散轨迹规划**： - 采用截断扩散策略加速多模态轨迹生成 - 引入轨迹词汇表作为锚点先验，结合感知特征与VLM指令进行层级编码 4. **后处理优化**： - 针对高速场景调整轨迹分布（y轴减速2%）以降低碰撞率 实验表明，该方法在NAVSIM-v2数据集上综合评分达**45.0 PDMS**，显著提升复杂场景的鲁棒性。局限在于模块分阶段训练，端到端联合优化有待探索。</details> |
| 2025-05-25 | ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning | http://arxiv.org/abs/2505.19080v1 | <details><summary>展开</summary>本文提出**ReFineVLA框架**，通过教师引导的微调增强视觉-语言-动作（VLA）模型的多模态推理能力。核心创新点如下： 1. **推理增强机制** - 利用专家教师模型（如Gemini）为机器人演示数据生成**多模态推理标注**（涵盖视觉观察、空间关系和任务规划逻辑），构建包含125,000条轨迹的增强数据集。 - 推理标注以自然语言形式解释动作决策逻辑（如图2所示），形成"思维链"监督信号。 2. **选择性微调策略** - 冻结预训练VLA的底层特征提取层，**仅微调高层决策模块**（如Transformer后层和策略头），在保留泛化能力的同时高效注入推理能力。 - 设计**双目标损失函数**（公式1）：联合优化动作预测损失（\(\mathcal{L}_{\text{action}}\)）和推理生成损失（\(\mathcal{L}_{\text{reasoning}}\)），通过超参数\(\lambda_r\)平衡二者权重。 3. **性能优势** - 在SimplerEnv仿真测试中显著超越基线：WidowX机器人任务成功率提升5.0%，变体聚合场景提升8.6%；Google机器人视觉匹配任务提升1.7%。 - 注意力可视化显示模型聚焦更相关的语义对象（图1），证实其增强的多模态理解能力。 4. **应用价值** - 提供可解释的机器人决策流程（图3），提升复杂长视野任务的鲁棒性。 - 开源代码促进通用机器人策略发展。 该方法解决了传统VLA模型缺乏显式推理的缺陷，为具身智能提供新范式。</details> |
| 2025-05-24 | Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance | http://arxiv.org/abs/2505.18793v1 | <details><summary>展开</summary>本文提出Genie Centurion（GCENT）框架，用于解决机器人策略训练中人类演示数据收集成本高、效率低的问题。核心创新点包括： 1. **重放-精修机制**：当机器人执行失败时，系统回退到先前状态（rewind），操作员提供纠正示范（refine），生成高质量纠错数据，增强对关键状态（如失败恢复）的覆盖。 2. **任务哨兵模块**：基于多模态大语言模型（InternVL2.5-2B）的自主监控模块，实时预测任务步骤完成状态，仅在必要时请求人工干预，降低持续监督需求。 3. **可扩展监督架构**：支持单人多机操作模式。实验表明，GCENT在装配三明治、插入连接器、微波加热、键盘打字等任务中： - 任务成功率比被动数据收集方法提升40% - 达到相同性能所需数据量减少55%以上 - 单操作员双机器人模式下数据收集效率达1.92（理论最大值2.0） GCENT通过结构化失败干预和状态回退机制，显著提升策略学习效率，为大规模真实场景机器人部署提供低成本解决方案。</details> |
| 2025-05-24 | VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning | http://arxiv.org/abs/2505.18719v1 | <details><summary>展开</summary>论文提出**VLA-RL框架**，通过强化学习（RL）提升预训练视觉-语言-动作（VLA）模型在机器人操作任务中的泛化能力。核心要点如下： 1. **问题与动机** - 现有VLA模型（如OpenVLA-7B）依赖离线模仿学习，在**分布外场景（OOD）** 表现不佳，因训练数据状态覆盖有限。 - RL可通过**在线探索**优化策略，但传统RL在机器人任务中面临数据效率低、奖励设计复杂等挑战。 2. **解决方案：VLA-RL** - **统一RL公式**：将机器人操作轨迹建模为**多模态多轮对话**，将状态定义为观测图像与语言指令的联合空间（\(\mathcal{S} = \mathcal{O} \times \mathcal{V}^m\)），动作空间为输出词序列（\(\mathcal{V}^n\)）。 - **奖励稠密化**： - 微调预训练视觉语言模型作为**过程奖励模型（Robotic Process Reward Model）**，通过自动提取任务片段生成伪奖励标签，解决稀疏奖励问题。 - **系统优化**： - **课程选择策略**：优先训练简单任务提升稳定性。 - **GPU平衡向量化环境**：并行环境加速数据收集。 - **批量解码**：高效处理多任务动作生成。 - **Critic预热**：用模仿学习初始化价值函数。 3. **实验结果** - 在**LIBERO**的40个复杂操作任务上，VLA-RL将OpenVLA-7B性能提升**4.5%**，超越所有微调基线。 - 匹配商业模型\(\pi_0\)-FAST性能，且**测试时优化时间越长性能越佳**，预示机器人领域存在"推理缩放定律"。 4. **意义** - 首次实现**大规模VLA模型与在线RL的结合**，为通用机器人提供可扩展的优化路径。 - 代码开源：**github.com/GuanxingLu/vlarl**。</details> |
| 2025-05-22 | ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems | http://arxiv.org/abs/2505.17295v1 | <details><summary>展开</summary>ScanBot是一个新型数据集，专为体现机器人系统中的指令驱动高精度表面扫描设计。其核心要点如下： 1. **数据集目标**： - 解决现有机器人数据集（聚焦抓取、导航等粗粒度任务）的不足，针对工业激光扫描的高精度需求（如亚毫米路径连续性、参数稳定性）。 - 覆盖12个多样化对象（6个真实电子组件如GPU板，6个3D打印几何形状），支持6种任务类型：全表面扫描、几何区域扫描、空间参考扫描、功能结构扫描、缺陷检测和对比分析。 2. **数据组成**： - 每个任务由自然语言指令引导，包含同步多模态数据：RGB、深度、激光轮廓、机器人位姿及关节状态。 - 总计896条扫描轨迹，通过硬件（UR3机器人+Keyence激光轮廓仪+Intel RealSense相机）在受控环境中采集，确保测量精度。 3. **关键实验发现**： - 评估多模态大语言模型（MLLMs，如GPT-4.1、Gemini 2.5）在感知-规划-执行全流程的表现： - 参数选择：模型基于视觉外观预测扫描参数（频率、曝光等）的准确率最高仅41.7%（Gemini 2.5 Flash），Z轴中心等精细参数预测误差显著。 - 区域定位：指令引用的目标区域IoU平均仅0.129（OpenAI o3），细粒度任务（如缺陷检测）表现更差。 - 路径生成与重建：模型无法生成稳定扫描轨迹，导致点云失真或覆盖不全，重建误差大。 4. **意义与局限**： - 揭示现有视觉语言动作（VLA）模型在连续高精度控制中的缺陷（如轨迹抖动、参数适应不足），推动工具级泛化研究。 - 局限包括仅支持平面扫描，未来需扩展曲面扫描和闭环反馈。</details> |
| 2025-05-22 | Interactive Post-Training for Vision-Language-Action Models | http://arxiv.org/abs/2505.17016v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving | http://arxiv.org/abs/2505.16278v1 | <details><summary>展开</summary>本文提出DriveMoE，一种基于专家混合（MoE）的端到端自动驾驶框架，核心创新点如下： 1. **问题背景**：现有视觉-语言-动作（VLA）模型存在两大局限： - 多视角相机数据冗余处理导致计算效率低下 - 统一策略网络难以处理罕见驾驶行为（如紧急转向） 2. **核心方法**： - **场景专用视觉MoE**：动态选择关键视角（如根据行驶上下文激活前视+相关侧视摄像头），减少70%视觉token计算量，模仿人类注意力机制 - **技能专用动作MoE**：在流匹配规划器中部署专家模块（如车道保持/转向/避障专家），通过路由机制按场景激活特定专家，解决行为模式平均问题 3. **技术基础**： - 扩展具身智能模型π₀为自动驾驶基线Drive-π₀ - 创新路由标注机制（低成本相机优先级标注+驾驶技能标注） - 两阶段训练策略（教师强制→自适应路由） 4. **性能表现**： - 在Bench2Drive闭环评测中达到SOTA（驾驶分数提升12.6%） - 显著提升罕见场景处理能力（紧急制动成功率提高18.2%） - 计算效率提升40%且保持空间几何信息完整性 5. **开源承诺**：将发布DriveMoE与Drive-π₀的代码及模型 该工作首次将MoE架构同时应用于自动驾驶的感知与决策模块，为多模态大模型在复杂动态场景中的应用提供新范式。</details> |
| 2025-05-21 | UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning | http://arxiv.org/abs/2505.15725v2 | <details><summary>展开</summary>本文提出了一种面向无人机（UAV）语言交互式精细轨迹控制的新基准 **UAV-Flow**，核心要点如下： 1. **任务定义（Flying-on-a-Word, Flow）** - 区别于传统视觉语言导航（VLN）的长距离路径规划，Flow 任务聚焦 **短距离、原子级语言指令**（如“绕车飞行”“向右平移5米”）驱动的精细控制。 - 需具备两大能力：**运动意图理解**（解析基本飞行动作）与**空间上下文感知**（关联语言指令与视觉场景中的目标）。 2. **基准构建** - **首个真实世界无人机语言模仿学习数据集**： - 包含 30,692 条专业飞手执行的轨迹，覆盖 8 类动作（如悬停、绕行、穿越）。 - 多场景采集（3 个大学校园），同步记录 6-DoF 状态、FPV 视频及自然语言指令。 - 支持 **开放词汇指令集**（经 LLM 扩充）与固定指令集，增强语言泛化性。 - **仿真数据集（UAV-Flow-Sim）**： - 提供 10,109 条轨迹的受控测试环境，用于系统评估（成功率和轨迹相似度指标 NDTW）。 3. **技术方案** - **地面-无人机协同框架**： - 解决无人机端计算瓶颈：无人机流式传输状态/视频至地面站，后者运行大模型并返回低延迟控制指令。 - 提出 **前瞻式动作分块预测机制**，缓解连续运动中的延迟问题。 - **模型适配与评估**： - 对比 VLN 方法（Seq2Seq/CMA）与视觉语言动作（VLA）模型（OpenVLA/Pi-0）。 - **关键结论**：VLA 模型显著优于 VLN 方法（Pi-0-UAV 仿真任务成功率 81.4%），尤其在需空间感知的任务中；开放词汇训练提升泛化能力。 4. **实际部署** - 首次在开放环境中实现 **VLA 模型的真实无人机部署**（Pi-0-UAV），展示语言指令的实时轨迹执行能力（如精准绕行、目标接近），验证无 sim-to-real 差距。 **意义**：UAV-Flow 填补了语言引导无人机精细控制的基准空白，通过真实数据集与部署框架推动无人机自然交互研究。数据、代码及演示见项目主页：https://prince687028.github.io/UAV-Flow。</details> |
| 2025-05-21 | From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems | http://arxiv.org/abs/2505.15685v1 | <details><summary>展开</summary>这篇论文系统比较了三种基础模型（FMs）在具身机器人系统中的集成范式：端到端视觉-语言-动作（VLA）模型、模块化视觉语言模型（VLM）流水线，以及多模态大语言模型（LLM）代理。核心发现如下： 1. **指令落地任务评估** - **多模态LLM代理**（如Gemini 2.5-Pro）在复杂指令理解（如隐含对象指代和空间关系推理）中表现最优（平均准确率82.1%），但计算成本高且依赖大规模闭源模型。 - **模块化VLM流水线**（如GroundingDINO）参数量仅为LLM的1%-6%，数据效率高且可解释性强，但在复杂指令处理（如“拧螺丝工具”）和错误传播方面存在局限（平均准确率40.8%）。 - **量化影响**：LLM的INT4量化使模型尺寸减小70%，但显著损害关系型指令理解能力（准确率下降14%-17%）。 2. **机器人操作任务评估** - **VLA模型微调**：扩散模型（如π₀）在仿真环境LIBERO任务中表现最佳（平均成功率94.15%），但需大量数据；自回归模型（如OpenVLA）微调收敛慢且方差大。 - **泛化缺陷**：所有VLA模型在未见物体干扰下性能显著下降（平均成功率降幅＞15%），且仿真到实机迁移存在明显性能断层。 - **动作分块技术**（Action Chunking）可提升长时序任务表现（如NORA在LIBERO-Long任务成功率从45%升至74.6%）。 3. **关键约束与启示** - **数据稀缺**：机器人数据获取成本高，需开发仿真到实机的鲁棒迁移方法。 - **泛化瓶颈**：VLA模型对分布外物体和场景变化敏感，需融合空间推理模块。 - **部署效率**：LLM的高计算需求与机器人硬件存在矛盾，建议采用轻量化架构（如GPT-4o-mini）或分层决策系统。 论文通过公开数据集和爪机机器人原型（https://github.com/HRItdy/claw_machine）验证了上述结论，为语言驱动机器人系统的设计提供了实证依据。</details> |
| 2025-05-21 | Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization | http://arxiv.org/abs/2505.15660v2 | <details><summary>展开</summary>本文提出AGNOSTOS基准和X-ICM方法，用于评估和提升视觉-语言-动作（VLA）模型在跨任务零样本泛化中的性能： 1. **AGNOSTOS基准** - 首个专注于跨任务零样本泛化的机器人操作评测基准，基于RLBench构建 - 包含18个训练任务和23个测试任务，测试任务分为两个难度层级： - **Level-1**：13个与训练任务部分相似的任务（共享物体或动作） - **Level-2**：10个完全新颖的任务（无重叠物体/动作） - 评估三类VLA模型：基于大规模机器人数据的模型、人类视频预训练模型、纯仿真域内模型 2. **现有模型局限性** 系统评估发现当前主流VLA模型（如OpenVLA、VoxPoser等）在跨任务泛化中存在显著缺陷： - Level-1任务平均成功率低于40% - Level-2任务成功率普遍低于20% - 表明现有模型难以适应语义无关的新任务组合 3. **X-ICM方法** 提出跨任务情境操作框架解决泛化瓶颈： - **动态引导样本选择**：通过扩散模型学习任务动态特征，检索与目标任务动态相似的训练示例 - **跨任务情境预测**：将检索到的示例作为上下文提示，驱动LLM生成未见任务的动作序列 - 关键创新：利用动态相似性（而非表面语义）建立跨任务关联 4. **实验效果** - 在AGNOSTOS上显著超越现有最佳模型： - 相对π₀提升6.0%，相对VoxPoser提升7.9% - 在Level-2任务上提升尤为显著（+9.2%） - 消融实验验证动态引导机制贡献率达32% 5. **开源资源** 提供完整基准与代码： - 项目地址：https://jiaming-zhou.github.io/AGNOSTOS - 代码库：https://github.com/jiaming-zhou/X-ICM 这项工作揭示了VLA模型在跨任务泛化的本质挑战，并为通用机器人操作研究提供可复现的评估工具和方法论基础。</details> |
| 2025-05-21 | FLARE: Robot Learning with Implicit World Modeling | http://arxiv.org/abs/2505.15659v1 | <details><summary>展开</summary>论文提出了一种名为**FLARE（Future Latent Representation Alignment）**的机器人学习框架，其核心创新点是通过隐式世界建模提升策略性能。主要要点如下： 1. **方法设计** - FLARE通过**未来潜在表示对齐**将预测性世界建模集成到策略学习中：在扩散变换器（DiT）中引入少量可学习的"未来令牌"，使其隐藏状态能够预测未来观测的紧凑嵌入表示。 - 采用**双目标训练**：结合动作流匹配损失（生成动作）和未来潜在对齐损失（预测未来状态），无需显式重建图像帧。 - 提出**动作感知嵌入模型**：基于Q-Former压缩视觉-语言特征，并通过跨具身数据预训练提升泛化能力。 2. **性能优势** - 在24个RoboCasa单臂操作任务和24个GR1人形机器人桌面操作任务上达到SOTA，平均成功率分别为**70.1%**（优于基线26%）和**55.0%**（优于基线25%）。 - **轻量化设计**：仅需对标准VLA模型添加少量令牌，计算开销极低。 3. **关键应用** - **数据高效迁移**：利用预训练嵌入模型，仅需100条真实机器人轨迹即可实现95%的实际任务成功率。 - **无动作标签学习**：结合人类第一视角视频（无动作标签）和单条机器人演示，成功学习新物体的抓取策略，成功率提升至80%（比纯动作训练高一倍）。 4. **创新意义** 提供了一种可扩展的隐式世界建模范式，显著提升策略的长期推理能力和跨任务泛化性，同时兼容多模态数据源。</details> |
| 2025-05-21 | Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control | http://arxiv.org/abs/2505.15304v2 | <details><summary>展开</summary>论文提出显著性感知量化模仿学习（SQIL），用于提升机器人控制策略模型的推理效率。核心创新点包括： 1. **关键问题发现**：首次系统分析量化对模仿学习（IL）策略的影响，发现任务关键状态（如抓取/释放物体时刻）对量化误差极为敏感，微小动作偏差会导致任务失败。 2. **SIS关键状态检测**：提出基于策略敏感性的状态重要性评分（SIS），通过扰动图像计算动作差异，精准识别需精细控制的时刻（如机械臂抓取瞬间），优于传统视觉关键帧检测。 3. **SQIL方法**：结合4-bit量化感知训练（QAT）与量化鲁棒动作蒸馏（QRD）。QRD通过SIS加权损失函数，强化关键状态的动作保真度，使量化策略与全精度策略在关键决策点一致。 4. **多场景验证**： - **机器人操控**（OpenVLA）：4-bit量化模型在LIBERO基准上成功率与全精度相当，边缘GPU推理速度提升2.5倍，能耗降低2.5倍。 - **自动驾驶**（CILRS）：4-bit量化在NoCrash基准保持原始性能，低端GPU加速3.7倍。 - **跨域任务**（MuJoCo物理仿真）：SQIL均实现近无损量化。 该方法首次实现IL策略的高效量化部署，为资源受限的机器人系统提供实用解决方案。</details> |
| 2025-05-21 | EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy | http://arxiv.org/abs/2505.15206v1 | <details><summary>展开</summary>本文提出EndoVLA，一种专用于内窥镜手术自主跟踪的双阶段视觉-语言-动作（VLA）模型。核心贡献如下： 1. **模型架构**：基于Qwen2-VL构建端到端VLA框架，通过低秩自适应（LoRA）微调，整合内窥镜图像与外科医生指令（如息肉跟踪、异常区域跟随），输出目标边界框和机器人动作指令（上/下/左/右/停止）。 2. **双阶段训练策略**： - **监督微调（SFT）**：使用自建EndoVLA-Motion数据集（含6K图像-动作对，覆盖息肉跟踪、黏膜异常区域定位、环切标记跟随三任务），通过边界框和动作预测损失优化模型。 - **强化微调（RFT）**：引入可验证奖励函数（如目标与聚焦区域的像素距离），采用分组相对策略优化（GRPO）提升动作决策的鲁棒性。 3. **关键创新**： - **空间动作映射**：定义离散动作集（如"upper-right"对应电机增量[+δθ₁, +δθ₂]），建立像素坐标与机器人关节角的非线性映射。 - **零样本泛化**：模型在未见过的解剖场景和序列任务（如连续跟踪多个目标）中表现优异，无需重新校准。 4. **实验验证**： - 真实机器人系统（基于Olympus内窥镜的2自由度连续体机器人）验证，分辨率400×400。 - 显著提升跟踪精度，任务成功率优于基线方法，并减少人工干预需求。 该方法解决了内窥镜场景的动态性挑战，为手术自主化提供新范式。 --- **关键词**：视觉-语言-动作模型，连续体机器人，自主内窥镜跟踪，强化学习</details> |
| 2025-05-21 | Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation | http://arxiv.org/abs/2505.15098v1 | <details><summary>展开</summary>论文提出Object-Focus Actor (OFA)方法，用于解决机器人灵巧操作中的数据效率低和泛化能力差的问题。核心要点如下： 1. **问题洞察**：发现灵巧操作任务中，无论物体初始位置如何，机器人手接近物体后的末端操作轨迹具有高度一致性（称为"物体聚焦轨迹"）。 2. **方法设计**： - **分层框架**： 1. **物体感知与位姿估计**：使用GroundingDINO+SAM分割目标物体，FoundationPose估计6D位姿。 2. **预操作位姿抵达**：基于物体位姿计算预操作位姿，通过CuRobo规划器实现无碰撞运动。 3. **OFA策略学习**：提取手部-物体局部图像，结合相对位姿信息，学习核心操作轨迹。 - **关键技术**： - **手部焦点图像**：根据机器人运动学生成手部包围框，放大后裁剪局部观测区域。 - **相对位姿反馈**：使用相对于预操作位姿的偏移量（而非绝对位姿）作为策略输入。 - **相对动作块**：预测基于相对位姿的动作序列，提升位置泛化能力。 3. **实验验证**： - **7项真实任务**：包含抓杯、持扫码器、捏玩具等单/双手操作。 - **关键优势**： - **位置泛化**：物体放置于训练未覆盖区域时，成功率保持90%（基线方法接近0%）。 - **背景泛化**：复杂背景下成功率比基线高50%（如抓杯任务60% vs 10%）。 - **数据效率**：仅需10次演示即可达到80%以上成功率（基线需50+次）。 4. **创新价值**：通过利用操作轨迹的固有一致性，OFA实现了数据高效且强泛化的灵巧操作，为实际部署提供新路径。 > 项目网站：https://yihanghku.github.io/OFA</details> |
| 2025-05-20 | AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory | http://arxiv.org/abs/2505.14030v3 | <details><summary>展开</summary>该论文提出了AutoBio——一个面向生物实验室自动化机器人的仿真与评测框架。核心贡献包括： 1. **高保真仿真系统**： - 通过3D高斯溅射技术实现真实仪器的数字化建模 - 开发专用物理引擎插件（螺纹/卡扣机构、准静态液体模型） - 基于物理渲染（PBR）支持透明材质和仪器动态界面 2. **生物实验评测基准**： - 设计16个生物实验任务，分三个难度层级（图5）： - 基础级：热循环仪开关盖等简单操作 - 进阶级：离心管开盖、移液枪吸液等精密操作 - 专家级：温控混匀仪面板操作等跨模态任务 - 支持多机器人配置（UR5e机械臂/DexHand灵巧手等） - 提供轨迹生成工具和VLA模型对接接口 3. **实验发现**： - 评测SOTA视觉语言动作模型（π₀和RDT）显示（表3）： - 基础任务成功率>96%（如开关仪器盖） - 精密操作表现显著下降（螺纹开盖成功率<21%） - 复杂任务（面板操作）几乎无法完成 - 揭示当前模型在科学工作流中的三大缺陷： - 精密操作控制能力不足 - 多模态指令理解局限 - 长时程任务规划能力欠缺 项目开源地址：https://github.com/autobio-bench/AutoBio，旨在推动专业场景下通用机器人系统的研究。</details> |
| 2025-05-20 | InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning | http://arxiv.org/abs/2505.13888v2 | <details><summary>展开</summary>论文提出了一种名为**InSpire**（内禀空间推理）的方法，旨在提升视觉-语言-动作模型（VLAs）的空间推理能力，以解决其因虚假相关性导致的泛化能力不足问题。核心要点如下： 1. **问题背景**： - 现有VLAs在预测动作时易受任务无关视觉特征的干扰（如背景噪声），忽视语言指令和空间关系，导致在训练数据分布外的场景泛化能力差。 2. **方法创新**： - **空间推理问答机制**：在语言指令前添加固定问题 **“目标物体相对于机器人的方位？”**（如“黑色碗在机器人的哪个方向？”），并约束模型从预定义方向（左/右/前/后/抓取中等）生成答案。 - **双阶段推理**： - **空间感知阶段**：模型先回答物体的空间方位问题，生成文本描述（如“碗在右侧”）。 - **动作生成阶段**：将此文本描述与原始指令、视觉观察结合，输出最终动作。 - **无需额外资源**：作为即插即用模块，无需额外训练数据或外部大模型交互，兼容现有自回归VLAs（如miniVLA-VQ、π₀-FAST）。 3. **技术实现**： - 通过**自动化规则标注**物体与机器人的空间关系（利用仿真环境或真实交互的3D位置数据），构建训练所需的方位标签（图3）。 - 训练时对齐模型生成的方位答案与真实空间关系，引导模型聚焦任务相关特征。 4. **实验结果**： - **仿真环境**：miniVLA-VQ的已见/未见任务成功率提升**6.2%**和**10%**。 - **真实环境**：π₀-FAST的已见/未见任务成功率提升**25%**和**26%**。 - 基于LIBERO基准训练的**InspireVLA-1B模型**在性能与计算效率上优于现有推理型VLAs。 5. **贡献总结**： - 提出InSpire方法，通过空间推理缓解虚假相关性。 - 实现即插即用的空间能力增强，无需额外成本。 - 仿真与真实场景实验验证了方法的有效性与灵活性。 该方法通过显式空间推理桥接观察与动作，显著提升了VLAs在复杂任务中的鲁棒性和泛化能力。</details> |
| 2025-05-19 | SPKLIP: Aligning Spike Video Streams with Natural Language | http://arxiv.org/abs/2505.12656v2 | <details><summary>展开</summary>本文提出SPKLIP，首个专为脉冲视频-语言对齐（Spike-VLA）任务设计的架构，解决脉冲相机数据稀疏、异步特性导致的语义理解难题。核心要点如下： 1. **问题背景**： - 脉冲相机以高帧率（40,000 Hz）和大动态范围捕捉高速运动，但传统模型（如CLIP）因模态不匹配，在脉冲数据上性能显著下降。 - 现有方法将脉冲流转为静态图像，丢失时空信息，限制实时应用（如自动驾驶）。 2. **方法创新**： - **SPKLIP架构**： - **分层脉冲特征提取器（HSFE）**：自适应建模多尺度时间动态，通过多分支卷积和空间注意力处理稀疏事件流。 - **脉冲-文本对比学习（STCL）**：直接对齐原始脉冲视频与文本，避免中间转换，支持少样本学习。 - **全脉冲视觉编码器（FSVE）**：整合脉冲神经网络（SNN）组件，提升能效，适用于神经形态硬件。 - **新数据集**：贡献并开源真实世界脉冲视频数据集，验证模型泛化能力。 3. **实验结果**： - 在基准数据集上达到SOTA性能，显著优于适配的传统模型。 - FSVE变体降低能耗，少样本学习在新数据集上展现强泛化性，突显实用潜力。 4. **核心贡献**： - 首个端到端Spike-VLA框架，填补脉冲数据语义对齐的研究空白。 - 兼顾性能与能效，推动事件驱动多模态研究发展。</details> |
| 2025-05-18 | RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction | http://arxiv.org/abs/2505.12224v3 | <details><summary>展开</summary>待生成</details> |
| 2025-05-16 | Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions | http://arxiv.org/abs/2505.11214v1 | <details><summary>展开</summary>这篇论文提出了一种新型视觉-语言-动作模型OE-VLA，旨在解决传统VLA模型仅支持语言指令的局限性。核心贡献如下： 1. **模型创新** - 提出OE-VLA架构，首次支持**开放式多模态指令**（如图像目标、手写指令板、演示视频和视觉目标状态） - 基于LLaVA-Next-Interleave基础模型构建，包含视觉编码器（SigLIP-ViT）、投影层和LLM主干（Qwen-1.5） - 采用离散化动作标记输出，通过统一架构处理多模态输入序列 2. **数据与方法** - 设计**自动数据转换方法**：将现有机器人数据集扩展为多模态指令数据集（VOS/OIF/VGR/VDL四类任务） - 提出**两阶段训练策略**： - 阶段1：多图像基础训练（MGrounding数据集）提升空间关系理解 - 阶段2：开放式指令微调（混合多模态指令数据） 3. **评估体系** - 构建新基准**OE-CALVIN**（含base/hard两个版本），基于CALVIN测试集但替换为多模态指令 - hard版引入来自互联网的图像/视频，增加跨域泛化难度 4. **实验结果** - 语言任务：OE-VLA-7B在CALVIN ABC→D基准达2.99平均成功率（SOTA） - 多模态任务： - OE-CALVIN_base：平均成功率2.75（1B模型）→3.48（7B模型） - 视觉目标指定（VOS）表现最优（7B模型达3.46） - 消融实验验证两阶段训练的有效性（尤其跨域场景） 该方法显著扩展了人机交互场景，为机器人处理开放世界指令提供了新范式。</details> |
| 2025-05-16 | Conditioning Matters: Training Diffusion Policies is Faster Than You Think | http://arxiv.org/abs/2505.11123v1 | <details><summary>展开</summary>这篇论文的核心要点如下： 1. **问题发现**：在条件扩散策略训练中，当生成条件（如视觉-语言输入）难以区分时，训练目标会退化为仅建模边际动作分布，导致**损失坍缩（loss collapse）**。这使策略网络忽略条件输入，生成与观测无关的动作。 2. **解决方案**：提出 **Cocos 方法**，通过修改条件流匹配中的源分布： - 将标准高斯先验 \(q(z)\) 替换为**条件依赖的源分布** \(q(z\|c)\)。 - 使用视觉-语言自编码器提取条件语义，构建以条件为中心的**高斯分布**（固定标准差，均值由条件语义确定）。 - 理论证明此设计能避免损失坍缩，强制策略网络整合条件信息（图1显示条件注入后网络隐状态的余弦相似度降低，范数变化增大）。 3. **效果与优势**： - **训练效率**：收敛速度提升 2.14 倍（如 LIBERO 基准仅需 30K 梯度步）。 - **性能提升**：在 70 个仿真任务（LIBERO、MetaWorld）和 20 个真实机器人任务（SO-100、xArm）中成功率显著超越基线。 - **通用性**：轻量级实现，兼容不同策略架构，参数量和计算开销低，匹配大规模预训练模型性能仅需少量资源。 **核心贡献**：揭示了扩散策略训练效率低的根本原因（条件忽略），提出通用解决方案 Cocos，通过条件相关的源分布实现高效的条件-动作生成。</details> |
| 2025-05-14 | Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware | http://arxiv.org/abs/2505.09601v1 | <details><summary>展开</summary>这篇论文提出了一种名为Real2Render2Real（R2R2R）的新型机器人数据生成框架，其核心贡献如下： ### 核心方法 1. **无需动力学仿真或硬件**：仅需智能手机扫描的物体多视角图像和单段人类演示视频，即可生成大规模机器人训练数据。 2. **三阶段流程**： - **资产提取**：使用3D高斯抛雪（3DGS）重建物体几何与外观，支持刚体和铰接物体分解。 - **轨迹提取**：通过4D-DPM从视频中提取6-DoF物体部件运动轨迹。 - **并行渲染**：利用IsaacLab引擎合成多样化机器人演示（关闭碰撞模型，仅作运动学渲染）。 ### 技术创新 - **轨迹多样性增强**：通过空间归一化与球面线性插值（Slerp）将单段人类演示适配到随机初始位姿。 - **高效生成**：单GPU（NVIDIA 4090）生成速度达51条轨迹/分钟，比人类遥操作快27倍。 - **兼容性**：输出图像-动作对可直接用于视觉语言动作模型（VLA）和模仿学习策略（如Diffusion Policy, π₀-FAST）。 ### 实验结果 - 在5项操作任务（放置杯子、关闭水龙头、开抽屉等）的1,050次物理实验中： - **仅需1段人类演示**：R2R2R生成的1,000条轨迹训练的模型，性能**匹配150条人类遥操作数据**训练的模型。 - **跨策略泛化**：在ABB YuMi双手机器人（训练未见的构型）上成功部署。 ### 局限性与意义 - **限制**：不支持非抓取操作（如推动）、复杂接触动力学及多指手抓取。 - **价值**：大幅降低数据收集成本，为开放世界操作提供可扩展的数据生成范式。 > 项目页面: [https://real2render2real.com](https://real2render2real.com) > 关键词：机器人数据集、模仿学习、数据增强</details> |
| 2025-05-14 | VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation | http://arxiv.org/abs/2505.09577v1 | <details><summary>展开</summary>本文提出VTLA（Vision-Tactile-Language-Action）模型，用于解决接触密集型操作任务（如精密装配）中多模态融合的挑战。核心创新点包括： 1. **多模态融合框架**：整合视觉、触觉和语言模态，通过跨模态语言对齐生成鲁棒的操作策略。触觉输入采用时间增强编码（VGTE），强化视觉引导和时间特征融合，弥补视觉语言模型在时序理解上的不足。 2. **偏好学习优化**：引入直接偏好优化（DPO），为连续动作预测提供类回归监督，解决传统基于分类的"下一词预测"损失与机器人连续控制任务不匹配的问题。 3. **低成本数据集**：在仿真环境中构建包含28,000样本的视觉-触觉-动作-指令配对数据集，支持指尖插入任务，并通过域随机化技术增强Sim2Real泛化能力。 实验表明： - 在仿真环境中，VTLA在未知形状的插孔任务上成功率超过90%，显著优于传统模仿学习（如扩散策略）和现有多模态基线（VLA/TLA）。 - 真实机器人实验验证了优异的Sim2Real性能：在0.6mm间隙的方形插孔任务中达到95%成功率，且能泛化至五边形等异形结构。 - 消融实验证实DPO将OOD数据泛化性能提升16%，触觉模态使操作效率提高20%。 局限性：触觉模态与语言对齐有待优化，多模态深度融合仍需探索。项目主页：https://sites.google.com/view/vtla</details> |
| 2025-05-13 | From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation | http://arxiv.org/abs/2505.08548v2 | <details><summary>展开</summary>本文提出了一种名为FSD（From Seeing to Doing）的新型视觉语言模型，旨在解决机器人操作中泛化能力不足的问题。核心创新点是通过空间关系推理生成中间视觉辅助表示，为机器人操作提供细粒度指导。主要贡献包括： 1. **空间关系链式推理（SrCoT）**：建立以物体坐标和空间关系为锚点的多步推理机制，将视觉辅助生成转化为推理过程。通过构建空间关系图（描述阶段）和坐标迭代推导（推理阶段），显著提升空间理解能力。 2. **弱到强能力数据构建**：设计五级分层数据集（区域定位→空间关系→空间推理→空间可供性生成→视觉轨迹生成），结合大规模具身数据集（BridgeDataV2、RT-X等）和常识数据，通过自动化流程生成30万条训练数据。 3. **自对齐机制**：通过双向任务（图像+指令→轨迹 vs. 图像+轨迹→指令）统一空间理解与生成能力，解决坐标空间与视觉信号的对齐问题。 4. **VABench基准**：构建包含300个手动标注样本的视觉辅助生成评测集，涵盖真实场景和仿真任务。 实验验证： - 在8个空间推理基准上排名第一（平均1.3位），尤其在3D深度感知（88.0%）和空间关系（78.3%）任务表现突出 - 物体定位任务（RoboRefIt）准确率56.7%，超越GPT-4o（15.3%） - 零-shot机器人操作：在SimplerEnv仿真环境达到40.6%成功率，真实世界8项任务平均成功率72%，超越基线方法30% - 视觉轨迹生成（VABench）的RMSE降至78.26，GPT评分达6.21分 FSD通过中间视觉辅助表示（空间可供点/框、物体轨迹）有效桥接视觉推理与动作决策，为跨场景、跨平台的通用机器人操作提供新范式。 论文链接：https://embodied-fsd.github.io/ 代码仓库：https://github.com/pickxiguapi/Embodied-FSD</details> |
| 2025-05-13 | Training Strategies for Efficient Embodied Reasoning | http://arxiv.org/abs/2505.08243v2 | <details><summary>展开</summary>本文提出高效的具身推理训练策略（ECoT-Lite），以解决传统具身思维链推理（ECoT）的推理速度慢和数据标注需求高的问题。通过验证三个核心假设：（1）具身推理提升表征学习（效果最佳）；（2）提供学习课程；（3）增加模型表达能力（效果不显著），开发了以下轻量级方法： 1. **推理预训练/联合训练**：通过预训练或联合训练提升视觉语言动作模型（VLA）的表征能力，推理时不生成中间步骤，速度提升至3.5Hz（对比ECoT的1-1.2Hz）。 2. **推理丢弃**：训练时随机丢弃部分推理步骤，推理时无需生成中间步骤，在LIBERO-90任务中达到90.8%成功率（超越SOTA 2.2%），推理速度与标准VLA相当。 3. **思考令牌**：引入无语义的占位令牌（验证假设3无效）。 实验结果： - **LIBERO-90**：推理丢弃方法在标准/扰动/含干扰物场景下分别达89.4%/70.8%/69.1%，平均超越标准VLA 9%。 - **Bridge V2**：推理预训练在真实机器人任务中比标准VLA提升19%，推理丢弃提升10%。 - **效率**：ECoT-Lite比ECoT提速3倍，同时减少对标注数据的依赖。 结论：表征学习是具身推理提升性能的核心机制，ECoT-Lite在保持性能的同时显著提升实用性。推荐在窄任务域用推理丢弃，宽任务域用推理预训练，最大化性能时仍用完整ECoT。</details> |
| 2025-05-12 | ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning | http://arxiv.org/abs/2505.07395v1 | <details><summary>展开</summary>论文提出ReinboT（强化机器人GPT），一种融合强化学习（RL）的视觉-语言-动作（VLA）模型，旨在解决机器人操作任务中训练数据质量不均的问题。核心创新点包括： 1. **RL与VLA融合机制** - 引入密集奖励函数（含子目标达成、任务进度、行为平滑度、任务完成度四个加权组件），量化操作任务质量。 - 通过期望回归（expectile regression）预测最大化累积回报（ReturnToGo），使模型学习数据分布中的最优策略。 2. **端到端架构设计** - 基于GPT架构，新增[RTG]令牌预测累积回报，利用历史观测（图像+本体感知）和语言指令联合建模。 - 解码器分支出动作和未来状态预测头，实现多任务学习。 3. **实验性能优势** - 在CALVIN混合质量数据集上达到SOTA，显著超越模仿学习基线。 - 验证了少样本学习能力及真实场景的分布外泛化性，例如在稀疏奖励任务中成功率提升35%以上。 该方法通过奖励稠密化和回报最大化机制，提升了机器人对数据质量分布的感知能力，为复杂操作任务提供了鲁棒决策基础。</details> |
| 2025-05-09 | UniVLA: Learning to Act Anywhere with Task-centric Latent Actions | http://arxiv.org/abs/2505.06111v2 | <details><summary>展开</summary>本文提出UniVLA框架，通过任务中心潜在动作实现跨具身智能体的通用策略学习。核心创新点包括： 1. **任务中心潜在动作学习** - 从无标注视频中提取任务相关动作表征，利用DINOv2特征空间过滤任务无关动态（如相机抖动） - 结合语言指令解耦动作表征，通过两阶段训练（任务无关/任务中心）实现高效信息压缩 - 采用VQ-VAE量化潜在动作，构建统一跨具身动作空间 2. **通用策略预训练** - 基于Prismatic-7B视觉语言模型，扩展潜在动作词表（如ACT_1, ACT_2...） - 通过自回归预测潜在动作token（长度N=4），仅需960 A100小时（OpenVLA的1/20计算量） - 支持整合机器人演示与人类视频等异构数据 3. **高效部署机制** - 使用轻量级动作解码器（10.8M参数）将潜在动作转化为可执行控制信号 - 多头部注意力聚合视觉特征，仅需1/10下游数据适配新任务 - 在LIBERO操作任务上达到95.2%成功率（提升18.5%），导航任务提升29.6%，实物机器人部署成功率提升36.7% 该方法显著提升跨环境知识迁移能力，在计算效率和数据利用率上实现突破。 --- *总结聚焦核心创新（潜在动作学习）、方法架构（三阶段设计）及性能突破（效率+精度），严格遵循"问题-方法-结果"逻辑，避免冗余细节。*</details> |
| 2025-05-09 | 3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks | http://arxiv.org/abs/2505.05800v1 | <details><summary>展开</summary>本文提出3D-CAVLA模型，通过整合深度信息和3D上下文增强视觉-语言-动作（VLA）模型在未知任务上的泛化能力。核心创新点包括： 1. **深度特征融合**：引入轻量级点云编码器（PointNet启发），将RGB-D图像转换为3D坐标点云，增强空间感知能力，提升机器人操作的几何理解精度。 2. **思维链指令分解**：利用GPT-4将任务指令分解为可执行的子步骤（如"定位物体→抓取→移动→放置"），提升长期任务规划和跨任务泛化能力。 3. **任务感知兴趣区域检测**：通过实体识别和目标跟踪生成动态ROI掩码，聚焦任务相关区域（25%训练扰动避免过度依赖）。 实验结果： - **LIBERO基准测试**：在双摄像头配置下达到98.1%平均成功率（OpenVLA-OFT基线为97.1%），长时任务提升显著（96.1% vs 94.5%）。 - **零样本泛化**：在10个未知任务上绝对成功率提升8.8%（45.2% vs 36.4%），验证了3D场景感知对适应新任务的有效性。 - **消融实验**：深度特征贡献最大（移除导致最大性能下降），思维链对长时任务关键（提升1.3%）。 模型开源地址：https://3d-cavla.github.io。未来工作将探索实时纠错模块和真实场景部署，解决当前对训练数据分布依赖的限制。</details> |
| 2025-05-08 | Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments | http://arxiv.org/abs/2505.05540v2 | <details><summary>展开</summary>这篇论文介绍了MultiNet v0.2基准测试，用于评估视觉-语言-动作（VLA）模型在程序生成环境中的零样本泛化能力。核心发现如下： 1. **模型泛化能力不足**：所有测试模型（GPT-4o、GPT-4.1、OpenVLA、Pi0 Base、Pi0 FAST）在Procgen程序生成任务上均表现出严重的零样本分布外（OOD）泛化缺陷： - Brier MAE接近最大值2，表明概率校准失败 - 宏观召回率最高仅12.5%（OpenVLA在Maze任务） - GPT-4o在Starpilot任务召回率低至1% 2. **VLA模型相对优势**： - OpenVLA综合表现最佳（平均宏观召回率9-12%） - 架构设计确保零无效预测，但存在"无操作"动作偏好偏差 - VLAs优于纯视觉语言模型（VLMs） 3. **关键影响因素**： - **动作表示**：离散动作空间适应困难（如Ninja任务召回率下降至8%） - **图像复杂度**：64×64低分辨率输入导致视觉特征提取困难 - **解码技术**： * 自回归模型（OpenVLA）倾向中心动作类预测 * 扩散模型（Pi0 Base）预测更分散但精度低 * Pi0 FAST因DCT-BPE编码偏向特定动作类 4. **工程优化空间**： - 约束提示工程可使GPT-4o精度提升超过Pi0 Base - 无效预测率超80%（GPT-4o/Pi0 FAST）表明输出空间约束的重要性 论文揭示了当前VLA模型在跨域适应、动作空间理解和复杂环境理解方面的根本性挑战，为改进通用具身智能系统提供了评估基准。 Benchmark release: https://github.com/manifold-ai/manifold (code not included in text) </body></html></details> |
| 2025-05-07 | Vision-Language-Action Models: Concepts, Progress, Applications and Challenges | http://arxiv.org/abs/2505.04769v1 | <details><summary>展开</summary>本文系统综述了视觉-语言-动作（VLA）模型的核心概念、技术进展、应用场景与挑战。主要内容如下： ### 核心概念 VLA模型通过统一架构整合视觉感知、语言理解和动作生成，克服传统分模块系统的局限性。其核心创新包括： - **多模态融合**：通过Transformer架构联合处理视觉（ViT/CNN）、语言（LLM）和动作信号，实现端到端学习（如CLIPort、VIMA模型）。 - **令牌化表示**：将视觉、语言和状态信息编码为共享嵌入空间的离散令牌（前缀令牌、状态令牌、动作令牌），支持自回归动作生成（如RT-2模型）。 - **进化路径**：从基础整合（2022-2023）到专业化推理（2024），再向安全关键部署发展（2025），共涵盖47种模型（图6）。 ### 技术进展 - **架构创新**：扩散策略提升随机动作预测能力，记忆增强Transformer（如Octo）支持长时序决策。 - **训练优化**：参数高效方法（如LoRA）和模型加速技术（模型剪枝、量化）降低计算成本。 - **实时控制**：自适应执行机制通过状态令牌反馈动态调整动作（图8）。 ### 应用场景 - **机器人领域**：人形机器人（Humanoid-VLA）、工业机械臂（Quar-VLA）实现复杂操作。 - **自动驾驶**：多模态传感器融合（OpenDriveVLA）提升环境适应性。 - **医疗与农业**：医疗机器人执行精细手术，农业系统（如苹果采摘场景）结合视觉与动作优化作业（图5,9）。 ### 核心挑战 - **实时性**：推理延迟制约高风险场景部署。 - **安全与泛化**：动作表示安全性不足，数据集偏差导致未知任务泛化困难。 - **系统复杂性**：多模块集成复杂度高，算力需求大（如3D场景图处理）。 - **伦理风险**：决策透明度和责任归属未解决。 ### 未来方向 提出跨具身泛化、神经符号规划等解决方案，推动VLA向安全、自适应、通用具身智能体演进。</details> |
| 2025-05-06 | OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation | http://arxiv.org/abs/2505.03912v1 | <details><summary>展开</summary>这篇论文提出了一种名为OpenHelix的双系统视觉-语言-动作（VLA）模型，用于解决机器人操作任务中传统单系统模型在实时性和泛化能力上的矛盾。核心贡献如下： 1. **系统架构设计** 基于双过程认知理论，将模型分为两个子系统： - **系统1（快速响应）**：轻量级策略网络（如3D Diffusion Actor），负责高频实时动作执行 - **系统2（高级推理）**：大型视觉语言模型（如LLaVA），提供语义理解和任务规划 2. **关键技术创新** - **提示微调（Prompt Tuning）**：冻结MLLM参数，仅训练可学习的"<ACT>"标记，保留模型泛化能力 - **辅助任务监督**：增加动作预测头（位置/旋转/抓取状态），强制潜在特征包含视觉-语言信息 - **投影器预对齐**：两阶段训练策略，先对齐潜在特征与策略网络，再联合微调 3. **实证发现** - 预训练策略网络比从头训练效率提升40%（任务完成率96% vs 71%） - 动态场景测试（CALVIN-D）显示双系统比单系统鲁棒性提升300% - 辅助任务使语言泛化能力（CALVIN-E）提升25% 4. **开源贡献** 发布轻量级开源模型OpenHelix，包含： - 冻结的LLaVA作为系统2 - 3D Diffusion Actor作为系统1 - 低训练成本（单卡训练<24小时） - 项目主页：https://github.com/OpenHelix-robot/OpenHelix 该模型在CALVIN基准测试中达到98%单任务成功率，并在动态物体操作和复杂语言指令场景展现优越性能，为后续机器人VLA研究提供了可扩展的基线框架。</details> |
| 2025-05-06 | RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration | http://arxiv.org/abs/2505.03673v2 | <details><summary>展开</summary>本文提出RoboOS，一种基于脑-小脑层级架构的开源具身系统，旨在解决多机器人协作中的跨具身适应性与任务调度问题。其核心创新包括： 1. **三层架构设计**： - **具身大脑模型（RoboBrain）**：基于多模态大语言模型（MLLM），负责全局感知（如3D场景重建）和高级决策（多智能体任务分解、轨迹生成） - **小脑技能库**：模块化插件库，支持异构机器人平台（单臂/双臂/轮式/人形）的低延迟操作与导航技能 - **实时共享内存**：通过时空同步机制维护场景图（空间关系）、任务历史（时序状态）和机器人状态（关节数据等），实现多智能体协调 2. **工作流优化**： - 采用拓扑任务分解（有向无环图）实现并行子任务分配 - 通过分布式智能体执行动态错误恢复 - 结合边缘-云端部署（FlagScale框架）实现毫秒级响应 3. **实验验证**： - 在餐厅/家庭/超市等场景验证跨平台协作能力 - 支持Unitree人形、Agilex双臂、RealMan单臂等异构机器人协同完成复杂任务（如汉堡制作、物品递送） - RoboBrain模型在规划/指向/轨迹预测等任务中超越基线模型（如Qwen2.5-VL-7B） 该框架突破了传统视觉语言动作（VLA）模型在长时程规划和多智能体协调上的局限，为工业自动化与服务机器人提供了可扩展的解决方案。项目已开源。</details> |
| 2025-05-06 | Task Reconstruction and Extrapolation for $π_0$ using Text Latent | http://arxiv.org/abs/2505.03500v4 | <details><summary>展开</summary>这篇论文研究了视觉-语言-动作模型（VLAs）在任务外推中的局限性，并提出了“文本潜变量”（text latent）作为解决方案。核心要点如下： 1. **问题背景**：VLAs在训练任务上表现良好，但无法组合已学技能完成新任务（如能独立执行“放奶油芝士到碗里”和“放碗到柜子上”，却无法完成“放奶油芝士到柜子上”）。这表明模型可能过拟合演示轨迹，而非学习可组合的技能表示。 2. **文本潜变量**： - 定义为模型内部状态的层间平均特征向量，编码任务语义信息。 - 通过将文本潜变量注入残差流，可重建对应任务行为（无需原始指令），成功率超80%。 - 反嵌入文本潜变量可生成不可读的替代指令（成功率约70%），支持隐私指令或后门攻击。 3. **任务外推方法**： - **文本潜变量插值（TLI）**：混合两个任务的文本潜变量，线性调整时间权重以组合子技能。 - 在**libero-ood基准**（20个LIBERO衍生外推任务）上，TLI将π₀的成功率从9%提升至83%，证明技能表示可组合但模型无法自主组合。 4. **关键发现**： - **空间过拟合**：VLAs将物体名称关联到训练时的固定位置（如“取奶油芝士”总指向原位置，忽略物体当前位置），表明缺乏真正的物体/目标理解。 - 其他VLAs（UniVLA, openvla-oft, π₀-fast）在libero-ood上成功率均<21%，凸显外推能力缺陷。 - 公开libero-ood基准以推动无需TLI的泛化方法研究。 5. **局限性与意义**： - 方法依赖模型架构一致性，部分解码器（如π₀-fast）难以转换输出为动作。 - 揭示了VLAs的机制性缺陷，为改进多模态对齐和泛化能力提供方向。 > 论文代码：https://github.com/QuanyiLi/pi0-text-latent</details> |
| 2025-05-06 | GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data | http://arxiv.org/abs/2505.03233v3 | <details><summary>展开</summary>这篇论文提出了GraspVLA，一个基于十亿级合成动作数据预训练的抓取基础模型。核心要点如下： 1. **合成数据集创新** - 构建了**SynGrasp-1B**数据集（全球首个十亿帧规模抓取数据集）： - 通过物理仿真和照片级渲染生成 - 包含10,680个物体（240个类别） - 覆盖光照/背景/视角/高度等多样化场景 - 生成效率提升策略：异步写入、并行渲染、单步运动规划 2. **模型架构突破** - **Progressive Action Generation (PAG)** 机制： - 将感知任务（视觉定位、抓取位姿预测）作为动作生成的中间步骤 - 形成链式推理（Chain-of-Thought）统一框架 - 多模态融合设计： - 视觉语言主干（DINO-v2 + SigLIP + InternLM2） - 基于流匹配的动作生成器 - 支持合成动作数据与互联网语义数据协同训练 3. **关键性能优势** - **零样本泛化能力**： - 真实世界测试：在合成/网络类别物体上均达93%抓取成功率 - 超越RT-2、OpenVLA等基线模型（SPL指标提升35%+） - 透明物体抓取成功率86.6%（显著优于AnyGrasp的10%） - **小样本适应性**： - 仅需100条演示数据即可适应新任务（如避碰抓杯、密集场景顺序抓取） - 边界框标注即可扩展新物体类别（无需动作标注） - 数据规模定律：性能随训练帧数稳定提升（网络类别泛化需更多数据） 4. **工程贡献** - 开源SynGrasp-1B数据集与预训练模型 - 支持新机械臂/相机配置快速适配（5k样本/1天微调） - 项目页面：https://pku-epic.github.io/GraspVLA-web 该方法首次验证了纯合成数据训练VLA模型的可行性，显著降低机器人基础模型对真实动作数据的依赖。</details> |
| 2025-05-06 | Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets | http://arxiv.org/abs/2505.03174v1 | <details><summary>展开</summary>论文提出了一种利用GPS导航应用的语音指令自动生成自动驾驶视觉-语言导航（VLN）数据集的方法。核心要点如下： 1. **问题与动机** 传统人工标注指令-动作（IA）数据对成本高、效率低。GPS导航应用（如Apple Maps/Google Maps/Waze）产生的大量语音指令可作为天然IA数据源，但尚未被系统化利用。 2. **关键创新** - **自动化框架ADVLAT-Engine**：通过同步录制GPS语音指令、车辆轨迹（GPS坐标）和道路视频，自动构建视觉-语言-动作（VLA）三元组数据。 - **指令分类体系**：将导航指令归纳为8类特征（转向、距离、道路名称、静态物体、方向、目的地、车道信息、灯光信息），并统计多应用指令分布（如78%指令同时包含"目的地+道路+转向"）。 - **语音处理**：利用Whisper模型将语音指令转为带时间戳的文本，实现指令与动作的自动对齐。 3. **验证与发现** - 采集233条真实导航指令（覆盖城市/高速等场景），证实不同应用指令特征差异： - Apple Maps高频使用静态物体参考（41次） - Google Maps侧重道路名称（77次） - Waze偏好方向指示（22次） - 多模态同步演示：单手机即可实现视频-轨迹-指令的自动关联（图2）。 4. **应用价值** 显著降低VLN数据集构建成本，为视觉-语言-动作模型提供大规模训练数据，推动自动驾驶系统对复合指令（如"在红绿灯右转至主街"）的理解与执行能力。 > 总结：通过挖掘GPS语音指令作为自然语言监督信号，结合自动化多模态采集，实现低成本、大规模的VLN数据生成，突破人工标注瓶颈。</details> |
| 2025-05-04 | Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions | http://arxiv.org/abs/2505.02152v1 | <details><summary>展开</summary>论文提出Interleave-VLA框架，旨在通过交错图像-文本指令增强机器人操作能力。核心要点如下： 1. **框架设计**： - Interleave-VLA是首个支持交错图像-文本指令的端到端机器人策略，可直接生成连续动作序列。 - 通过最小修改扩展现有视觉-语言-动作（VLA）模型（如OpenVLA和π₀），实现模型无关的灵活范式，提升零样本泛化能力。 2. **数据集构建**： - 为解决缺乏大规模交错体现数据的问题，开发自动化流程，将Open X-Embodiment的文本指令转换为交错图像-文本指令。 - 创建首个大规模真实世界交错体现数据集（Open Interleaved X-Embodiment），包含21万episodes和1300万帧，覆盖3500种对象。 3. **关键优势**： - **泛化能力提升**：在模拟（SIMPLER、VIMA-Bench）和真实机器人实验中，对未见对象的域外泛化能力提高2-3倍。 - **零样本灵活性**：支持用户自定义指令（如手绘草图、网络图片、裁剪图像），无需额外微调。 - **跨数据集兼容性**：结合异构数据训练，有效利用多样化指令图像（包括互联网来源）。 4. **实验验证**： - 在真实机器人（FANUC机械臂）上，Interleave-VLA优于纯文本基线，尤其在低数据量场景下，通过跨具身数据集预训练实现高效迁移。 - 分析表明，交错范式通过多模态学习提升语义泛化，且混合任务相关图像与网络图像可优化性能。 5. **局限性与展望**： - 计算开销较高（因图像令牌增加），未来需探索令牌压缩。 - 建议扩展至交错输出（如文本或预测图像），以构建更统一的机器人基础模型。</details> |
| 2025-04-28 | NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks | http://arxiv.org/abs/2504.19854v1 | <details><summary>展开</summary>这篇论文提出了NORA，一种小型开源通用视觉语言动作模型，用于具身任务。核心要点如下： 1. **模型定位** NORA是一个3B参数的视觉-语言-动作（VLA）模型，旨在解决现有大型VLA模型（通常>7B参数）计算开销高、难以实时部署的问题。通过降低参数量，可在消费级GPU上微调。 2. **核心创新** - **架构设计**：基于Qwen-2.5-VL-3B多模态模型，利用其原生图像分辨率增强空间理解能力 - **高效动作编码**：采用FAST+分词器离散化连续动作空间，通过DCT和BPE压缩动作序列 - **轻量化**：推理时仅需8.3GB显存，显著低于7B级模型 3. **训练策略** - 在Open X-Embodiment数据集（97万条真实机器人演示）上预训练 - 提出**NORA-Long变体**：预测5步动作块（action chunking），提升长时序任务表现 - 单节点8×H100 GPU训练三周，优化器采用AdamW+余弦衰减 4. **关键实验结果** \| 测试场景 \| 主要优势 \| \|----------------\|------------------------------------------\| \| 真实机器人任务 \| 在OOD物体抓取、空间推理任务上超越OpenVLA等基线（平均成功率56.7% vs 40%） \| \| LIBERO仿真基准 \| NORA-Long微调后达87.9%平均成功率，长时序任务提升显著 \| \| 抗干扰能力 \| 在含干扰物环境中鲁棒性优于基线模型 \| 5. **开源贡献** 完整开源模型检查点、训练策略和评估协议，促进可复现研究。 总结：NORA通过模型轻量化与高效动作编码，在保持强任务性能的同时降低计算开销，为实时机器人部署提供了实用解决方案。</details> |
| 2025-04-22 | $π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization | http://arxiv.org/abs/2504.16054v1 | <details><summary>展开</summary>本文提出了一种名为π_0.5的视觉-语言-动作模型（VLA），通过创新的协同训练框架实现开放世界泛化能力。核心要点如下： 1. **多源协同训练** - 结合异构数据源：机器人数据（97.6%来自非目标平台）、高级语义预测（如"拿起盘子"）、人类语言指令、网络多模态数据（图像描述/问答） - 两阶段训练：预训练阶段整合跨平台机器人数据与语义任务，后训练阶段专注移动操作任务 2. **层次化推理架构** - 先预测高层语义子任务（如"整理枕头"），再生成底层动作序列 - 采用流匹配技术（flow matching）实现高效连续动作生成 3. **突破性泛化能力** - 首次实现在全新家庭环境中执行长周期复杂任务（如10-15分钟的厨房清洁） - 成功泛化至训练中未见的卧室/厨房场景，完成铺床、收纳餐具等精细操作 4. **关键验证结论** - 多源数据协同训练是泛化的核心驱动力 - 高层语义推理对长周期任务成功率提升显著 - 400小时真实家庭数据结合非机器人数据实现最优效果 该模型为具身智能系统在开放世界的实际应用提供了新范式。</details> |
| 2025-04-01 | Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning | http://arxiv.org/abs/2504.00907v2 | <details><summary>展开</summary>本文提出了一种新型任务“Ask-to-Act”，旨在解决具身智能体在模糊指令下的交互问题，核心贡献和创新点如下： ### 核心问题 - **模糊指令处理**：真实环境中（如家庭场景），人类指令常存在歧义（例如“拿杯子”但存在多个杯子）。传统方法需依赖大规模人类演示或手动设计奖励函数，成本高昂且难以推广。 ### 解决方案 1. **Ask-to-Act任务**： - 智能体在部分可观测环境中执行物体抓取任务（如“将杯子放到咖啡桌”）。 - 需通过**最少量的澄清问题**（如“您要红色的杯子吗？”）消除歧义，涉及五类模糊性：物体属性、空间关系、尺寸及其组合。 - 评估指标包括成功率（SR）、歧义解决效率（ARS）和提问比例（QR）。 2. **方法创新**： - **多模态LLM（MLLM）适配**：将LLaVA-OneVision等MLLM改造为视觉-语言-动作（VLA）策略模型。 - **强化学习+LLM奖励**： - 使用LLM（如Llama-3）生成密集奖励函数，替代人工设计奖励。 - 奖励函数涵盖任务成功、子目标达成、有效提问及超预算惩罚（公式1）。 - **语法约束解码**：限制动作预测空间，确保输出有效的技能或问题。 ### 实验结果 - **显著优势**：在Habitat 3.0的83个场景中测试，RL微调的MLLM优于所有基线： - **新场景泛化**：成功率提升40.3%（对比GPT-4o等零样本基线）。 - **新任务泛化**：成功率提升19.1%（处理未见的模糊组合）。 - **关键发现**： - 仅依赖子目标奖励的模型性能接近随机水平，验证密集奖励的必要性。 - 增加提问预算可提升任务成功率及泛化能力。 ### 意义 - **首个端到端框架**：首次实现MLLM同时执行动作和主动提问，且无需人类演示或手动奖励工程。 - **新评估基准**：提供包含63个训练场景/20个测试场景的数据集，推动具身智能的交互研究。 论文通过LLM生成的奖励信号，解决了模糊指令下智能体决策与交互的关键挑战，为可扩展的具身智能训练提供了新范式。</details> |
| 2025-03-30 | OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model | http://arxiv.org/abs/2503.23463v1 | <details><summary>展开</summary>该论文提出OpenDriveVLA，一种基于视觉-语言-动作（VLA）模型的端到端自动驾驶框架。核心贡献包括： 1. **模型架构**：利用开源预训练视觉语言模型（VLM），通过多模态输入（3D环境感知、自车状态和驾驶指令）生成可靠轨迹。解决传统VLM在动态3D驾驶场景中空间推理能力不足和幻觉问题。 2. **关键技术**： - **分层视觉-语言对齐**：将结构化2D/3D视觉特征映射到统一语义空间，减少模态差异。 - **智能体-环境-自车交互建模**：通过自回归过程动态捕捉交通参与者与静态元素的交互关系，提升轨迹可靠性。 - **驾驶指令微调**：注入驾驶知识增强推理能力，平衡推理速度与决策效果。 3. **实验验证**： - 在nuScenes数据集上实现开环轨迹规划与驾驶问答任务的SOTA性能。 - 定性分析表明模型能有效执行高层指令，在复杂场景（如密集交通）中生成鲁棒轨迹。 4. **创新点**： - 首个将VLA模型统一用于环境感知、语义理解和轨迹生成的端到端框架。 - 通过视觉中心化查询模块减少幻觉，提升空间感知可靠性。 论文代码将开源，推动端到端自动驾驶研究发展。</details> |
| 2025-03-27 | CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models | http://arxiv.org/abs/2503.22020v1 | <details><summary>展开</summary>这篇论文提出了一种名为CoT-VLA的创新方法，通过在视觉-语言-动作模型（VLA）中引入视觉思维链（Visual Chain-of-Thought, CoT）推理机制，显著提升了机器人任务的性能。以下是核心要点： ### 1. **核心创新：视觉思维链推理** - **问题背景**：传统VLA模型直接从观察映射到动作，缺乏中间推理步骤，限制了复杂任务的表现。 - **解决方案**：CoT-VLA引入**子目标图像生成**作为中间推理步骤： - 首先生成未来帧图像（子目标），表示任务预期的中间状态（视觉推理）。 - 然后基于当前观察和子目标图像预测动作序列（动作生成）。 - **优势**：子目标图像天然存在于演示数据中，无需额外标注；同时可兼容无动作标注的视频数据（如EPIC-KITCHENS），提升视觉推理能力。 ### 2. **模型架构与技术设计** - **基础模型**：基于多模态生成模型VILA-U构建，支持图像和文本的联合理解与生成。 - **混合注意力机制**： - **因果注意力**：用于自回归生成子目标图像和文本。 - **完全注意力**：并行预测所有动作维度，提升动作生成效率。 - **动作分块预测**：一次性输出多步动作序列（而非单步），增强时序连贯性。 ### 3. **训练与部署策略** - **两阶段训练**： - **预训练**：结合机器人演示数据（Open X-Embodiment）和无动作视频数据（EPIC-KITCHENS/Something-Something V2），优化视觉令牌和动作令牌预测。 - **微调**：在下游任务数据上适配，实现闭环控制。 - **损失函数**：视觉生成损失（$\mathcal{L}_{\text{visual}}$）与动作预测损失（$\mathcal{L}_{\text{action}}$）联合优化。 ### 4. **实验效果** - **显著提升**：在仿真（LIBERO）和真实机器人任务（Bridge-V2、Franka-Tabletop）中全面验证： - **LIBERO基准**：平均成功率81.13%，超越最佳基线（OpenVLA）6%。 - **真实任务**：成功率提升17%，尤其在长时程任务（+15.3%）和空间推理任务（+8.6%）上优势明显。 - **关键优势**：视觉CoT机制显著提升复杂任务的可解释性和动作规划准确性。 ### 5. **贡献总结** - 首创**视觉思维链推理**框架，将子目标图像作为中间表示。 - 提出**混合注意力机制**，平衡生成与预测效率。 - 实验证明**多源数据训练**（含无标注视频）的有效性，为VLA模型提供新范式。 > 论文链接：[https://cot-vla.github.io/](https://cot-vla.github.io/)</details> |
| 2025-03-26 | MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation | http://arxiv.org/abs/2503.20384v2 | <details><summary>展开</summary>本文提出MoLe-VLA框架，通过动态层跳跃机制提升机器人操作中多模态大语言模型（MLLM）的效率与性能。核心要点如下： 1. **问题背景** MLLM在机器人任务中面临高计算开销问题（如7B模型仅5-12Hz推理速度），难以满足实时控制需求（50-1000Hz）。现有稀疏化方法（如早期退出）易忽略关键语义层，导致性能下降。 2. **核心创新** - **混合层机制（MoLe）**：受神经科学"浅脑假说"启发，将每个LLM层视为独立专家，通过路由器动态激活部分层（类似脑信号路径选择），跳过冗余计算。 - **时空感知路由器（STAR）**：融合视觉空间特征与语言时序依赖，精准选择激活层（如优先保留高层语义层），提升决策准确性。 - **认知自蒸馏（CogKD）**：引入可学习的"认知令牌"，用完整模型指导层跳跃模型恢复丢失的语义信息，通过任务相关特征对齐提升鲁棒性。 3. **关键成果** - **效率**：最高减少5.6倍LLM计算量，适用于资源受限的机器人平台。 - **性能**：在RLBench仿真和真实任务中平均成功率提升8%，实现效率与精度共赢。 - **通用性**：可适配多种VLA模型（如OpenVLA），支持动态环境下的实时推理。 4. **验证场景** 在10项RLBench任务（如物体抓取、场景交互）和Franka机械臂真实部署中验证有效性，显著优于传统稀疏化方法（如MoE、早期退出）。 总结：MoLe-VLA通过神经科学启发的动态架构与知识蒸馏技术，解决了机器人任务中MLLM部署的效率瓶颈，为轻量化具身智能系统提供新思路。</details> |
| 2025-03-25 | Gemini Robotics: Bringing AI into the Physical World | http://arxiv.org/abs/2503.20020v1 | <details><summary>展开</summary>这篇论文介绍了Gemini Robotics系列模型，旨在将AI能力引入物理世界。核心要点如下： 1. **Gemini Robotics-ER（具身推理模型）** - 基于Gemini 2.0构建，增强了对物理世界的空间理解能力 - 支持2D/3D物体检测、指向预测、轨迹规划和抓取姿态预测 - 提出**ERQA基准**（具身推理问答），涵盖400个多模态问题，在空间推理等任务上超越GPT-4o等模型 2. **Gemini Robotics（视觉-语言-动作模型）** - 通过机器人动作数据微调，实现直接控制机器人 - 架构：云端VLA骨干（延迟<160ms）+本地动作解码器（50Hz控制频率） - 能力： - 开箱即用解决20+灵巧操作任务（如折叠衣物、缠绕耳机线） - 精确遵循自然语言指令（如“将蓝色夹子放在黄色便签右侧”） - 在视觉变化（光照/背景干扰）、指令泛化（多语言/错别字）和动作泛化（新物体位姿）中保持鲁棒性 3. **专业化与适应能力** - **长时序任务**：通过少量数据微调可完成复杂任务（折纸狐狸、打包午餐盒、卡牌游戏） - **快速适应**：仅需100个演示样本即可学习新任务 - **新本体适应**：成功迁移到双机械臂平台和高自由度人形机器人 4. **安全与责任** - 强调开发过程需遵循Google AI原则 - 讨论物理世界部署的风险缓解策略（如安全协议设计） 该工作标志着通用机器人的重要进展，通过结合多模态理解与物理交互能力，为AI在物理世界的应用奠定基础。模型在ERQA基准上达到48.3%准确率（使用思维链提示达54.8%），在真实机器人任务中展示出60%+的成功率。</details> |
| 2025-03-25 | Boosting Robotic Manipulation Generalization with Minimal Costly Data | http://arxiv.org/abs/2503.19516v2 | <details><summary>展开</summary>这篇论文提出了一种提升机器人操作泛化能力的方法，核心是通过低成本数据优化模型性能。主要贡献如下： 1. **RoboTron-Craft 基准**： - 开发了分阶段、低成本的数据生成管道，用于生成物理真实的机器人操作轨迹。 - 揭示了模型性能与训练数据量、目标物体多样性和环境多样性之间的对数缩放规律（logarithmic scaling law）。 2. **RoboTron-Platter 方法**： - 将操作轨迹解耦为**空间推理阶段（SRP）**（大范围导航）和**物理交互阶段（PIP）**（精确操作）。 - 利用大量低成本自动收集的 SRP 数据（占数据集 66%）与少量高成本 PIP 数据混合训练，显著提升 VLA 模型泛化能力。 3. **关键发现**： - SRP 数据作为“催化剂”，最大化昂贵 PIP 数据的效用，在零样本场景中任务成功率最高提升 **41%**。 - 模型可将操作技能迁移到**新目标物体**，仅需 SRP 数据即可泛化相似几何形状物体的操作。 - 数据混合比例存在最优值（SRP 占比约 66%），过量 SRP 数据会导致性能瓶颈。 4. **实验验证**： - 在 RoboMM 和 RoboFlamingo 模型及多数据集（CALVIN）上验证了方法的普适性。 - 消融实验证明：松散的线性接近轨迹、操作位姿多样化和固定工作台视角相机对性能提升至关重要。 **核心价值**：大幅降低数据收集成本（减少人工示教需求），为机器人操作泛化提供了高效解决方案。项目代码已开源。</details> |
| 2025-03-20 | JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse | http://arxiv.org/abs/2503.16365v2 | <details><summary>展开</summary>本文提出了一种名为**ActVLP**的新型视觉-语言-动作（VLA）模型训练范式，并基于此开发了首个在《我的世界》（Minecraft）中支持1000+原子任务的VLA模型**JARVIS-VLA**。核心要点如下： ### 1. **问题背景** - 现有VLA模型（如RT-2、OpenVLA）主要依赖动作后训练（模仿学习），**忽视基础模型的世界知识与视觉理解能力增强**，导致决策泛化性受限。 - Minecraft作为开放世界环境，需处理复杂任务（制作、冶炼、采矿等），传统方法在长序列任务（如制作钻石剑）上表现不佳。 ### 2. **创新方法：ActVLP训练范式** - **三阶段流程**： - **阶段I**：冻结视觉模块，**纯文本世界知识后训练**（如Minecraft百科问答），增强语言模型对环境的理解。 - **阶段II**：联合训练视觉编码器与语言模型，**对齐视觉-语言任务**（图像描述、视觉问答、空间定位），提升多模态理解。 - **阶段III**：**动作后训练**，将动作离散化为token，通过模仿学习轨迹数据生成键盘/鼠标操作。 - **模型架构**：基于Llava或Qwen2-VL，支持多图像输入（历史观测），复用低频词token表示动作（如鼠标移动分21档）。 ### 3. **关键成果** - **性能突破**： - 在MCU基准测试中，**超越所有基线40%以上**（表1），尤其在复杂任务（制作/冶炼）成功率提升2倍。 - 例如：制作钻石剑成功率83%（基线最高50%），冶炼铁锭成功率70%（基线最高40%）。 - **数据效率**：仅用21%的轨迹数据即超越纯模仿学习模型（图5）。 - **可扩展性**：扩大非轨迹视觉语言任务数据量，**直接提升下游任务成功率**（图6）。 ### 4. **技术贡献** - **揭示VLA缩放定律**：非轨迹任务的后训练损失与决策成功率呈线性相关（图6）。 - **开源资源**：发布代码、模型及包含**71万帧+277K知识条目**的数据集（[项目页](https://craftjarvis.github.io/JarvisVLA)）。 ### 5. **应用价值** - 首次实现**端到端VLA模型在开放3D环境的高精度控制**，支持键盘/鼠标操作，为游戏AI、机器人控制提供新范式。 --- **总结**：ActVLP通过增强基础模型的认知能力再迁移至动作学习，显著提升VLA在复杂环境中的决策性能，JARVIS-VLA在Minecraft的原子任务上达到SOTA水平。</details> |
| 2025-03-20 | IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes | http://arxiv.org/abs/2503.17406v1 | <details><summary>展开</summary>论文提出IRef-VLA数据集，用于3D场景中的交互式指代接地任务，特别关注处理不完美语言输入（如模糊或错误参考）。核心要点如下： 1. **数据集内容**：基于11.5K个真实3D场景（来自ScanNet等数据集），包含7.6M语义关系、4.7M指代表述，并增强语义注释、场景图、可导航空间及不完美语言样本。 2. **任务创新**：定义"带有不完美参考的指代接地"任务，要求模型检测目标对象是否存在，若不存在则生成替代建议，支持多轮交互。 3. **评估结果**：在SOTA模型（如MVT和3D-VisTA）上验证，数据集提升零样本泛化能力；提出的图搜索基线在对象存在检测（准确率94.4%）和替代建议生成（相似度61%）上表现优异。 4. **目标**：为3D场景理解提供资源，促进鲁棒交互式导航系统的发展，数据集和代码已开源。</details> |
| 2025-03-18 | GR00T N1: An Open Foundation Model for Generalist Humanoid Robots | http://arxiv.org/abs/2503.14734v2 | <details><summary>展开</summary>这篇论文介绍了GR00T N1，一个用于通用人形机器人的开源基础模型。核心要点如下： 1. **模型架构** - 采用双系统设计：**System 2**（视觉语言模块）基于Eagle-2 VLM，处理图像和语言指令；**System 1**（动作生成模块）使用扩散Transformer，生成120Hz的高频动作。 - 支持跨具身学习：通过特定编码器处理不同机器人的状态和动作维度。 2. **数据策略** - 提出**数据金字塔**结构：底层（人类视频/网络数据）→ 中层（合成数据）→ 顶层（真实机器人轨迹）。 - 创新数据增强： - **潜在动作**：通过VQ-VAE从无动作视频中提取动作表示。 - **神经轨迹**：用视频生成模型扩增10倍真实数据（88小时→827小时）。 - **仿真轨迹**：基于DexMimicGen自动生成6,500小时仿真数据。 3. **训练方法** - **预训练**：混合3类数据源（人类视频、神经轨迹、真实/仿真机器人数据），使用流匹配损失（flow-matching）。 - **后训练**：针对特定机器人具身微调，引入神经轨迹增强数据效率。 4. **性能验证** - 在仿真基准测试（RoboCasa/DexMimicGen）中超越现有模仿学习模型。 - 在Fourier GR-1人形机器人上实现高效语言控制双手操作，仅需少量演示数据。 5. **开源贡献** - 公开GR00T-N1-2B模型、训练数据和仿真基准（GitHub/HuggingFace）。 核心创新点在于通过异构数据融合与双系统架构，解决了人形机器人训练数据稀缺问题，实现了通用性强、数据效率高的行为生成。</details> |
| 2025-03-17 | MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation | http://arxiv.org/abs/2503.13446v1 | <details><summary>展开</summary>论文提出MoManipVLA框架，用于将预训练的视觉-语言-动作（VLA）模型迁移到移动操作任务中，以提升跨任务和环境的泛化能力。核心要点如下： 1. **问题与动机** - 传统移动操作方法因训练数据规模有限导致泛化能力差，而VLA模型虽在固定基座操作中表现优异，但无法处理移动基座与机械臂的协同运动。 2. **核心方法** - **VLA引导轨迹生成**：利用预训练VLA模型预测末端执行器的路径点（waypoints），提供高泛化性的任务指导。 - **运动规划目标设计**：针对移动基座和机械臂设计物理可行性目标，包括： - **可达性**（Reachability）：确保路径点可通过逆运动学求解。 - **平滑性**（Smoothness）：最小化关节角度和基座位姿的突变。 - **避碰**（Collision Avoidance）：基于环境距离场（ESDF）优化轨迹安全性。 - **双级轨迹优化**： - **上层**：优化移动基座路径点，扩展机械臂动作空间。 - **下层**：在固定基座位置下优化末端执行器轨迹，实现任务目标。 3. **关键优势** - **零样本适应**：通过调整基座位置，使固定基座VLA模型生成的路径点在移动场景中物理可行。 - **高效部署**：仅需50条专家演示即可完成真实世界部署，大幅降低训练成本。 - **泛化性强**：继承VLA模型在多样任务和环境中的泛化能力。 4. **实验结果** - 在OVMM基准和真实场景中，成功率比最优方法高4.2%。 - 项目主页：https://gary3410.github.io/momanipVLA/ 总结：MoManipVLA通过迁移预训练VLA模型和双级优化框架，解决了移动操作中泛化性低与训练成本高的问题，实现了高效跨任务跨环境的策略迁移。</details> |
| 2025-03-15 | ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis | http://arxiv.org/abs/2503.14526v1 | <details><summary>展开</summary>该论文提出了一种名为ReBot的新型真实-仿真-真实（Real-to-Sim-to-Real）方法，用于扩展机器人数据集并提升视觉-语言-动作（VLA）模型的性能。核心要点如下： 1. **问题背景** - 大规模真实机器人数据集（如Open X-Embodiment）训练VLA模型成本高昂，且数据稀缺限制模型泛化能力。 - 现有生成方法（如ROSIE）合成的机器人视频存在物理失真和时间不一致性，影响模型学习效果。 2. **ReBot方法框架** - **真实到仿真轨迹重放**：将真实机器人轨迹在仿真环境中重放，替换被操控物体以增加多样性，同时验证动作可行性。 - **真实背景修复**：通过GroundedSAM2分割原始视频中的机器人和物体，用ProPainter修复背景，获得任务无关的场景。 - **仿真到真实视频合成**：将仿真的机器人动作与修复后的真实背景融合，生成物理合理且时间一致的合成视频。 3. **核心优势** - **最小化仿真-现实差距**：保留真实动作轨迹，结合真实背景，确保物理真实性。 - **全自动化流程**：无需人工干预，可扩展性强。 - **多视角一致性**：基于3D仿真环境，天然支持多摄像头视角的一致性（如图5所示）。 4. **实验结果** - **视频质量**：在时序一致性、成像质量等指标上超越ROSIE（VBench评分93.0% vs ROSIE 80.3%），接近真实视频（96.1%）。 - **模型性能提升**： - 在仿真环境（SimplerEnv）中： - OpenVLA模型域内性能提升21.8%，泛化能力提升9.4%。 - Octo模型域内性能提升7.2%，泛化能力提升19.9%。 - 真实环境（Franka机器人）中： - OpenVLA成功率提升20%，Octo成功率提升17%。 5. **意义** ReBot首次通过真实-仿真-真实闭环解决机器人学习的"最后一公里"部署问题，为低成本扩展机器人数据集提供了新范式。 论文项目页面：[https://yuffish.github.io/rebot/](https://yuffish.github.io/rebot/)</details> |
| 2025-03-13 | HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model | http://arxiv.org/abs/2503.10631v3 | <details><summary>展开</summary>HybridVLA提出了一种统一视觉-语言-动作（VLA）模型，通过协同融合扩散模型和自回归生成，提升机器人操作的鲁棒性与精度。核心要点如下： ### 1. **问题背景** - **现有方法局限**： - 自回归VLA方法（如RT-2）将连续动作离散化，破坏动作连续性，影响精确控制。 - 扩散VLA方法（如π₀）依赖独立扩散头，无法充分利用VLM的推理能力（如基于token生成的上下文推理）。 - **关键挑战**：如何统一两种生成范式，实现互补强化。 ### 2. **核心创新** - **统一架构**： - 在单一LLM中集成扩散动作生成（连续）和自回归动作生成（离散），共享骨干网络。 - 引入特殊标记（`<BOD>`/`<EOD>`）连接扩散与自回归token序列，避免生成冲突。 - **协同训练机制**： - 设计混合损失函数：扩散部分采用均方误差（噪声预测），自回归部分采用交叉熵（token预测）。 - 分阶段训练：先在760K机器人轨迹（Open X-Embodiment等）上预训练，再在下游任务微调。 - **自适应动作融合**： - 基于自回归动作token的置信度（$c_{t+1}^{ar}$）动态加权融合两类动作： - 置信度 > 0.96 时，平均融合扩散动作（$a_{t+1}^d$）与自回归动作（$a_{t+1}^{ar}$）。 - 否则仅使用扩散动作，提升复杂任务鲁棒性。 ### 3. **实验结果** - **性能优势**： - 仿真任务：平均成功率比SOTA方法高14%（如RLBench任务均值达66%）。 - 实物任务：真实场景成功率提升19%，支持单臂/双臂机器人操作。 - **泛化能力**：对未见过的物体、背景、空间位置及光照变化具有强适应性。 - **推理效率**： - 优化版HybridVLA-dif（7B）仅用扩散推理，速度达9.4 Hz。 - DDIM采样步数降至4步，保持性能不变。 ### 4. **贡献总结** - 首创**扩散与自回归协同的VLA统一框架**，解决动作连续性与语义推理的兼容问题。 - 提出**训练配方与自适应融合机制**，实现两类生成方式的相互增强。 - 实验验证了**性能、鲁棒性与泛化性的显著提升**，为通用机器人控制提供新范式。 > 项目页面：[hybrid-vla.github.io](https://hybrid-vla.github.io/)</details> |
| 2025-03-12 | CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games | http://arxiv.org/abs/2503.09527v1 | <details><summary>展开</summary>本文提出CombatVLA，一种面向3D动作角色扮演游戏（如《黑神话：悟空》）战斗任务的高效视觉-语言-动作模型。核心创新点如下： 1. **问题定位** 针对现有视觉-语言-动作模型（VLA）在复杂3D环境中实时决策的不足（高分辨率感知、动态战术推理、秒级响应需求），尤其战斗场景中的延迟敏感性问题。 2. **关键技术贡献** - **动作追踪器**：轻量级工具，后台录制玩家键盘/鼠标操作与游戏画面帧，通过时间戳对齐生成视频-动作对数据（公式1）。 - **战斗理解基准（CUBench）**：包含三类任务（单帧采集/多帧理解/多帧推理），评估模型对敌人定位、攻击模式识别等战斗智商（图3）。 - **动作思维链（AoT）**：将追踪数据转化为结构化序列（动作+解释），引入截断符`⟨TRUNC⟩`加速推理（图4）。 - **三阶段渐进训练**： - 粗粒度视频级AoT学习战斗环境 - 细粒度帧级AoT学习精确响应 - 截断策略优化实现高效输出 - **自适应动作加权损失**：结合模态对比损失（公式3）和动作对齐损失（公式4），优先学习关键动作类别。 3. **模型性能** - 仅3B参数量，推理速度比GPT-4o等VLM方案快50倍（图1）。 - 在CUBench上超越所有对比模型（包括GPT-4o和Qwen2.5-VL）。 - 实际游戏成功率高于人类玩家。 4. **部署框架** 集成动作执行代理，通过截断AoT策略实现实时控制（图2e），可操作真实PC设备。 5. **开源承诺** 将公开动作追踪器、数据集、基准测试、模型权重及训练代码（项目页：https://combatvla.github.io/）。</details> |
| 2025-03-11 | MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models | http://arxiv.org/abs/2503.08007v1 | <details><summary>展开</summary>论文提出了一种名为MoRE（混合机器人专家）的新型视觉-语言-动作（VLA）模型，用于提升四足机器人的多任务性能与可扩展性。核心要点如下： 1. **问题背景** 现有VLA模型存在两大局限： - 直接微调多模态大语言模型（MLLM）架构，未针对机器人任务特性优化； - 依赖专家数据（模仿学习），无法利用自动收集的混合质量数据（含失败轨迹）。 2. **方法创新** - **专家混合架构**：在稠密MLLM（Fuyu 8B）中嵌入多个低秩自适应（LoRA）模块作为独立专家，构建稀疏激活的MoE结构。每个token根据任务类型（如移动、导航）动态选择专家，平衡多样化下游任务需求（图3）。 - **强化学习训练目标**：引入基于Q学习的训练目标，利用混合质量数据（专家数据+次优轨迹）。通过分析任务结构特性（如关键状态稀疏性、长轨迹、回报与视野无关），模型能高效学习次优数据中的策略（图4）。 3. **技术优势** - 首次将MoE架构应用于大规模端到端VLA模型，显著提升多任务适应性； - RL目标增强数据利用效率，降低对高质量专家数据的依赖； - 动作输出为12维离散化指令（含速度、步态、高度等参数）。 4. **实验结果** - 在仿真中6项任务上全面超越基线（表1），尤其在困难任务中成功率提升显著； - 分布外测试展现强泛化能力； - 实物部署验证了方法的实用性（图1），为四足机器人多任务学习奠定基础。 **总结**：MoRE通过专家混合架构与强化学习目标的协同设计，解决了VLA模型在机器人任务中的架构适配与数据效率问题，为复杂场景下的四足机器人控制提供了可扩展方案。</details> |
| 2025-03-10 | PointVLA: Injecting the 3D World into Vision-Language-Action Models | http://arxiv.org/abs/2503.07511v1 | <details><summary>展开</summary>这篇论文提出了PointVLA框架，旨在解决现有视觉-语言-动作（VLA）模型依赖2D图像而缺乏3D空间理解能力的问题。核心创新点是通过轻量级模块将点云数据注入预训练VLA模型，无需重新训练整个模型。具体方法包括： 1. **模块化3D注入**：冻结预训练VLA的视觉语言主干和动作专家，仅通过可训练的"3D模块块"将点云特征以加法方式注入动作专家的特定层（通过跳层分析确定冗余层）。 2. **跳层分析**：通过系统性实验（如逐层跳过推理）识别动作专家中对性能影响较小的层（如DexVLA中第11层后的层），确保3D特征仅注入这些层以最小化干扰。 3. **点云编码器**：采用分层卷积架构提取多尺度点云特征，避免使用预训练3D编码器以保证在新场景中的泛化能力。 **实验验证**表明PointVLA显著优于OpenVLA、Diffusion Policy等基线，主要体现在： - **小样本多任务**：仅需20条演示即可完成4种不同任务（如手机充电、擦盘子） - **虚实物体区分**：精准识别真实物体与照片，避免安全风险 - **高度适应性**：自动调整机械臂动作以适应训练中未见的桌面高度变化 - **长时序任务**：成功完成传送带动态抓取和装箱等复杂操作 该方法在仿真和真实双机械臂平台（UR5e/AgileX）上验证了有效性，解决了2D模型的空间认知局限，同时高效复用预训练知识。</details> |
| 2025-03-06 | Refined Policy Distillation: From VLA Generalists to RL Experts | http://arxiv.org/abs/2503.05833v2 | <details><summary>展开</summary>本文提出了一种名为**精炼策略蒸馏（Refined Policy Distillation, RPD）** 的新方法，旨在将视觉-语言-动作模型（VLA）的通用能力转化为高效的强化学习（RL）专家策略。核心要点如下： ### 1. **问题背景** - **VLA模型的局限性**：VLA虽在真实任务中泛化能力强，但成功率常低于专家策略，且需针对新环境微调（依赖大量人工演示数据）。 - **RL的优势与挑战**：RL可通过环境交互生成数据，训练紧凑策略，但存在样本效率低、探索难度大、超参数敏感等问题。 ### 2. **RPD方法核心** - **结合RL与行为克隆**：通过**在线RL（PPO算法） + 行为克隆（BC）** 蒸馏VLA知识： - **引导探索**：RL学生策略在探索时参考VLA教师的动作（单步采样），加速收敛。 - **策略精炼**：学生通过环境交互超越VLA性能，生成更优的专家策略。 - **损失函数设计**：在PPO目标函数中增加**动作均值的MSE损失**（RPD-MSE），拉近学生与VLA动作的距离（公式见正文）。 ### 3. **关键贡献** - **高效蒸馏**：RPD-MSE在六种仿真操作任务（如抓取立方体、推球）上显著提升样本效率，收敛速度**快于PPO基线**，且最终性能**超越VLA教师**。 - **稀疏奖励鲁棒性**：在奖励稀疏场景下，RPD仍能高效学习（PPO基线常失败）。 - **泛化能力**： - **跨任务**：可处理未在VLA训练集中出现的任务变体（如添加干扰物）。 - **跨视角**：摄像机视角变化时（VLA性能骤降），RPD仍能学习有效策略。 ### 4. **实验验证** - **VLA适配**：为评估RPD，在ManiSkill3仿真环境微调Octo和OpenVLA模型（原模型在仿真中失败）。 - **对比结果**： - **RPD-MSE最优**：优于L1损失（RPD-L1）和纯行为克隆（RPD-BC）。 - **超越基线**：PPO收敛慢且波动大；类似方法PPD（近端策略蒸馏）因VLA动作分布复杂而失败。 - **模型无关性**：适用于不同规模VLA（93M参数的Octo和7B参数的OpenVLA）。 ### 5. **局限与展望** - **依赖VLA质量**：若VLA完全无法提供有效动作（如跨域任务），RPD可能失效。 - **未来方向**：结合**仿真到现实迁移技术**部署至实体机器人；用RPD精炼的策略生成数据以**迭代优化VLA**。 ### 总结 RPD首次实现**从VLA到RL策略的知识蒸馏与精炼**，解决了VLA性能瓶颈和RL训练低效问题，为通用模型向场景化专家策略转化提供了新思路。代码与数据已开源（见论文链接）。</details> |
| 2025-03-06 | VLA Model-Expert Collaboration for Bi-directional Manipulation Learning | http://arxiv.org/abs/2503.04163v1 | <details><summary>展开</summary>本文提出了一种VLA模型与专家协作的双向操作学习框架，核心创新点包括： 1. **协作机制**：通过VLA模型（执行多数操作）与专家策略（低频干预）协同完成任务，在MetaWorld环境中以4:1动作比例实现： - VLA模型成功率提升6.2%（MT10）和13.5%（MT50） - 人类专家操作步骤减少82.24% 2. **双向学习**： - **模型优化**：收集协作过程中的操作数据持续微调VLA模型 - **专家进化**：人类通过交互提升操作熟练度 3. **技术验证**： - 支持多种VLA架构（OSCA/OSDA/HSCA） - 脑机接口实验证实协作系统提升低速操作效率 - 规则策略与人类策略对比显示人类操作效率持续优化 该框架显著提升多任务操作鲁棒性，为机器人基础模型时代的人机协作提供新范式。项目主页：https://aoqunjin.github.io/Expert-VLA/</details> |
| 2025-03-05 | OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction | http://arxiv.org/abs/2503.03734v3 | <details><summary>展开</summary>论文提出OTTER模型，一种基于文本感知视觉特征提取的视觉-语言-动作（VLA）架构。核心创新点包括： 1. **文本感知特征提取机制**：冻结预训练的视觉语言模型（如CLIP），通过计算文本token与视觉patch特征的余弦相似度，选择性提取与任务指令语义对齐的视觉特征（使用CLIP最后一层注意力输出特征），避免微调导致的语义对齐退化。 2. **零样本泛化优势**：实验证明OTTER在真实机器人（拾取放置任务）和仿真环境中显著优于现有VLA模型（如Octo、OpenVLA），尤其在未见过的物体和场景中保持高成功率（图1）。这源于模型保留了大规预训练中的语义理解能力。 3. **可扩展性验证**：性能随预训练编码器规模、策略网络容量和机器人数据量的增加而提升，表明模型具备多维度扩展潜力。 模型架构（图2）将提取的文本感知特征、压缩后的语言特征及本体感知特征融合，输入因果Transformer预测动作。该方法解耦了任务规划（特征选择）与动作规划，提升了跨场景泛化能力。 论文链接：https://ottervla.github.io/</details> |
| 2025-03-05 | SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning | http://arxiv.org/abs/2503.03480v2 | <details><summary>展开</summary>本文提出SafeVLA框架，通过约束学习实现视觉-语言-动作模型（VLA）的安全对齐。核心创新点包括： 1. **集成安全方法（ISA）** - **建模**：形式化安全约束为约束马尔可夫决策过程（CMDP），明确定义状态-动作安全谓词和轨迹级安全谓词 - **风险激发**：主动挖掘潜在不安全行为，构建多样化风险场景库 - **策略约束**：基于安全强化学习（SafeRL）优化VLA策略，实现最小化风险与任务性能的平衡 - **安全验证**：建立针对性评估协议，包含极端故障场景和分布外泛化测试 2. **安全评估基准（Safety-CHORES）** - 新型测试环境，集成导航与操作任务 - 嵌入细粒度安全约束（如物体碰撞、危险区域规避） - 支持程序化生成大规模场景，针对性暴露VLA安全漏洞 3. **关键实验结果** - **安全-性能平衡**：相比SOTA方法提升83.58%安全率，同时维持任务性能（+3.85%） - **长尾风险处理**：有效减少高风险动作（如碰撞概率下降76.3%） - **强泛化能力**：在分布外扰动场景中保持90.2%的安全遵守率 4. **技术贡献** - 首个系统性解决VLA物理安全约束的框架 - 开源基准环境与模型：https://pku-safevla.github.io 该方法为具身智能系统提供了可验证的安全保障机制，显著降低现实部署中的物理风险。</details> |
| 2025-03-04 | RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour | http://arxiv.org/abs/2503.02572v1 | <details><summary>展开</summary>论文提出**RaceVLA**，首个基于**视觉-语言-动作（VLA）模型**的竞速无人机导航系统，核心要点如下： 1. **核心创新**： - 利用**第一人称视角（FPV）视频流**和**自然语言指令**，直接生成无人机四维控制信号（`Vx, Vy, Vz, ω`），模拟人类飞行员的实时决策行为。 - 基于OpenVLA-7B模型微调，将机械臂的7维动作输出简化为适应无人机的4维控制（线速度+偏航角速度）。 2. **技术实现**： - **系统架构**：搭载RealSense T265摄像头的8英寸竞速无人机，通过ROS框架与地面服务器通信；模型经**int8量化**优化，推理速度达**4Hz**。 - **数据集**：收集200个飞行片段（约2万张图像），包含拱门/方形门导航、环形赛道等任务，使用RLDS格式存储动作、图像及语言指令。 - **训练**：采用LoRA技术（rank-32）高效微调，在A100 GPU上训练7000步（学习率5e⁻⁴）。 3. **性能优势**： - **泛化能力**：在**运动泛化**（75.0 vs. 60.0）和**语义泛化**（45.5 vs. 36.3）上超越OpenVLA；**全面优于RT-2模型**（视觉79.6 vs. 52.0，运动75.0 vs. 55.0，物理50.0 vs. 26.7，语义45.5 vs. 38.8）。 - **实时表现**：赛道任务平均速度**1.04 m/s**（最高2.02 m/s），平均偏航角速度0.40 rad/s，验证高速场景适应性。 4. **局限与改进**： - **动态环境挑战**：视觉泛化（79.6 vs. OpenVLA 87.0）和物理泛化（50.0 vs. 76.7）稍弱，主因是运动中物体尺寸变化及避障复杂性。 - 未来将提升视觉/物理泛化能力，并优化推理速度以支持更高飞行速度。 5. **开源贡献**： - 公开**代码库、预训练模型及数据集**（项目页：https://racevla.github.io/），推动无人机自主导航研究。 **总结**：RaceVLA首次将VLA模型应用于竞速无人机，实现类人决策的高适应性导航，为动态环境中的自主飞行设立新基准。</details> |
| 2025-03-04 | Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding | http://arxiv.org/abs/2503.02310v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-03 | CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs | http://arxiv.org/abs/2503.01378v1 | <details><summary>展开</summary>论文提出CognitiveDrone，一种用于无人机（UAVs）实时认知任务的视觉-语言-动作（VLA）模型及评估基准。核心要点如下： 1. **模型设计**： - **CognitiveDrone**：基于OpenVLA架构的7B参数VLA模型，通过第一人称视觉输入和文本指令生成实时4D动作命令（速度与角速度）。 - **CognitiveDrone-R1**：增强版本，集成额外7B参数视觉-语言模型（VLM）推理模块，以较低频率（2Hz）简化复杂任务指令，提升决策能力。 2. **数据集与训练**： - 收集超8000条模拟飞行轨迹，涵盖三类认知任务：**人类识别**（识别特定人物）、**符号理解**（区分字符/标志）和**推理**（逻辑推断）。 - 在Gazebo仿真环境中训练模型，使用LoRA高效微调，优化飞行物理与认知能力。 3. **评估基准（CognitiveDroneBench）**： - 首个无人机认知任务开源基准，基于Gazebo物理仿真，要求无人机在赛道中通过解决任务选择正确闸门。 - 测试结果： - RaceVLA（竞速模型）：成功率31.3%。 - CognitiveDrone：成功率59.6%。 - CognitiveDrone-R1：成功率77.2%，在推理、人类识别和符号理解任务上分别提升6%、31%和21%。 4. **贡献**： - 开发先进VLA模型，显著提升无人机在认知任务中的表现（最高77.2%成功率）。 - 建立首个专用无人机认知评估基准，推动标准化测试。 - 开源模型、数据集及代码库（访问：cognitivedrone.github.io）。</details> |
| 2025-02-27 | Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success | http://arxiv.org/abs/2502.19645v2 | <details><summary>展开</summary>本文提出了一种优化的视觉-语言-动作模型（VLA）微调方法OFT，显著提升策略性能和推理效率。核心要点如下： 1. **问题背景** - 现有VLA模型（如OpenVLA）在新型机器人任务上面临推理速度慢（3-5 Hz）和任务成功率低的问题，尤其在高频控制（25-50+ Hz）的双臂操作场景中表现不佳。 2. **关键设计决策** - **并行解码与动作分块**：替代自回归生成，单次前向预测多步动作（如8步分块），减少26倍延迟。 - **连续动作表示**：直接输出连续动作值（替代离散token），提升动作精度。 - **L1回归目标**：简化训练流程，效果与扩散模型相当但收敛更快。 3. **优化方案（OFT）** - 整合上述组件形成OFT微调方案： - 并行解码 + 动作分块 → 提升吞吐量 - 连续动作 + L1损失 → 提高策略质量 - **增强版OFT+**：针对语言任务加入FiLM调制，强化语言与视觉特征的融合。 4. **实验结果** - **LIBERO仿真基准**： - 平均成功率97.1%（原OpenVLA为76.5%），刷新SOTA。 - 8步动作分块下吞吐量提升26倍（109.7 Hz）。 - **ALOHA真实双臂机器人**： - 在衣物折叠、食材分拣等任务中平均成功率领先基线15%。 - 25步分块实现43倍加速（25 Hz实时控制）。 5. **优势** - **灵活性**：支持多视角图像和机器人状态输入。 - **高效性**：单次推理耗时0.112毫秒（单臂任务），满足实时控制需求。 代码及模型已开源：https://openvla-oft.github.io</details> |
| 2025-02-26 | ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration | http://arxiv.org/abs/2502.19250v2 | <details><summary>展开</summary>论文提出ObjectVLA方法，旨在解决机器人模仿学习中物体泛化（object generalization）的挑战。传统模仿学习依赖大量人类演示数据，但难以将技能迁移到视觉差异大、语义相似的新物体（如从“递苹果”泛化到“递桃子”）。ObjectVLA基于Vision-Language-Action（VLA）模型，通过结合视觉-语言对数据（如图像-文本对）和机器人交互数据，建立物体与动作的隐式链接，实现无需新物体演示的零样本泛化。在真实机器人实验中，该方法对100个未见物体达成64%的选择成功率，并支持智能手机拍照快速微调（仅需少量图像和短时训练）。这显著减少了对人类演示的依赖，提升了开放世界机器人操作的灵活性和可扩展性。</details> |
| 2025-02-24 | Evolution 6.0: Evolving Robotic Capabilities Through Generative Design | http://arxiv.org/abs/2502.17034v4 | <details><summary>展开</summary>本文提出“Evolution 6.0”框架，通过生成式AI实现机器人在缺乏工具时自主设计制造工具并完成任务。核心要点如下： 1. **系统架构** - **工具生成模块**：利用QwenVLM解析环境，Llama-Mesh根据文本指令生成3D工具模型（成功率90%，平均耗时10秒），经渲染验证后3D打印。 - **动作生成模块**：基于OpenVLA将自然语言指令转化为7维动作向量（位移/旋转/抓取），控制机械臂执行任务。 2. **实验性能** - **动作泛化能力**：物理与视觉泛化达83.5%，运动泛化70%，语义泛化仅37%（如指令替换为"切香蕉"时成功率下降）。 - **场景适应**：在训练一致场景中任务成功率100%，但复杂曲线工具生成存在局限。 3. **创新价值** 突破工业6.0预定义工具限制，适用于火星等极端环境，实现"环境感知-工具生成-任务执行"全自主闭环。 4. **未来方向** 拓展至双手操作、增强复杂场景解读能力、提升语义泛化及工具几何细节生成。 --- *注：总结基于论文摘要、架构（图1-2）及实验章节（V），量化数据来自工具生成（表I）与动作泛化测试（图5）。*</details> |
| 2025-02-20 | Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration | http://arxiv.org/abs/2502.14795v2 | <details><summary>展开</summary>本文是ICML 2025会议的投稿与格式指南，主要要点如下： 1. **提交要求**： - 电子提交（PDF格式），通过会议网站进行。 - 初始提交匿名（双盲评审），不得包含作者信息或致谢。 - 页面限制：正文不超过8页（参考文献和附录不计入页数），总文件大小≤10MB；最终版本可额外增加1页正文。 - 附录必须与正文合并为一个文件。 2. **格式规范**： - 文本分两列，宽度6.75英寸，行距11点，使用10号Times字体。 - 标题居中（14号粗体），摘要为单段落（4–6句话）。 - 图标题置于下方，表标题置于上方；图表需清晰且仅使用矢量或无损格式。 - 参考文献采用APA格式，按时间顺序排列并包含页码。 3. **其他规定**： - 最终版本需添加作者信息和修改脚注（如会议名称）。 - 必须包含“影响声明”，讨论工作的伦理及社会影响。 - 鼓励共享软件和数据（提交时匿名处理）。 - 可添加附录，但需符合总体文件大小限制。</details> |
| 2025-02-20 | ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model | http://arxiv.org/abs/2502.14420v2 | <details><summary>展开</summary>本文提出ChatVLA框架，解决视觉-语言-动作模型（VLA）中多模态理解与机器人控制的统一问题。核心要点如下： 1. **问题分析** 现有VLA模型存在两大缺陷： - **虚假遗忘**：机器人训练覆盖预训练视觉-语言模型的视觉-文本对齐能力 - **任务干扰**：控制与理解任务在联合训练时相互干扰导致性能下降 2. **创新方法** - **分阶段对齐训练**： 1. 先专注机器人控制任务训练 2. 再逐步融入多模态数据恢复视觉-文本对齐 - **混合专家架构（MoE）**： - 共享注意力层促进跨任务知识迁移 - 分离MLP层处理控制/理解任务以减少干扰 3. **关键成果** - **多模态理解**： - MMMU基准性能提升6倍（超越ECoT） - MMStar分数从0提升至47.2%（参数效率提高3.5倍） - **机器人控制**： - 25项真实任务（取放/推/挂等）超越OpenVLA等SOTA方法 - 长时序任务成功率提升显著（如清洁积木任务达75%） 4. **意义** 首次实现单一模型同时胜任：场景对话、跨模态推理、物理交互，为通用具身智能提供新路径。</details> |
| 2025-02-19 | VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation | http://arxiv.org/abs/2502.13508v2 | <details><summary>展开</summary>本文提出了一种新型的端到端视觉-语言-动作模型VLAS，首次将语音指令直接集成到机器人操作策略模型中，无需依赖外部语音识别系统。核心创新点包括： 1. **语音指令融合**：通过Whisper编码器处理原始语音，利用多层感知机将语音特征映射到LLaVA的语言空间，实现语音-文本-视觉的多模态对齐。 2. **三阶段训练范式**： - **语音对齐阶段**：在LibriSpeech数据集上微调MLP层 - **语音问答微调**：结合新构建的SQA语音问答数据集和VQA视觉问答数据 - **机器人操作微调**：在CSI机器人操作数据集上训练动作生成 3. **语音检索增强生成（Voice RAG）**：通过声纹识别检索个性化知识库，解决定制化任务中的个体特定需求（如物品归属识别、用户偏好理解）。 4. **实验验证**： - 在CALVIN基准测试中，VLAS的语音指令性能媲美传统文本指令VLA模型 - 在定制化任务基准上，语音指令成功率提升至86.5%（传统VLA仅19.2%） - 真实UR5机械臂实验验证了实用性和个性化交互能力 VLAS突破了现有VLA模型仅支持文本指令的限制，通过保留语音中的非语义信息（如声纹），为机器人提供了更自然的人机交互方式和个性化任务处理能力。代码、模型及数据集已开源。</details> |
| 2025-02-14 | Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation | http://arxiv.org/abs/2502.10040v1 | <details><summary>展开</summary>本文提出了一种扩散轨迹引导策略（DTP），用于解决长视野机器人操作任务中模仿学习（IL）的复合误差问题。核心创新点包括： 1. **DTP框架设计** - 采用两阶段方法：首先生成基于扩散模型的2D粒子轨迹（扩散轨迹模型DTM），随后利用该轨迹指导策略学习。 - 轨迹在RGB域中表示末端执行器的未来运动趋势，通过相机内外参将世界坐标映射为像素级关键点序列。 2. **关键技术贡献** - 通过扩散模型生成高质量任务相关轨迹，显著减少语言指令与动作空间之间的特征差异。 - 引入轨迹重采样模块压缩轨迹标记，降低计算开销。 - 结合视频预测损失（$\mathcal{L}_{video}$）、轨迹损失（$\mathcal{L}_{trajectory}$）和动作损失（$\mathcal{L}_{action}$）优化策略。 3. **实验验证** - 在CALVIN基准测试中，DTP相比SOTA方法（如GR-1、HULC）成功率提升25%，且无需外部预训练。 - 在真实机器人实验中，DTP显著改善长视野任务性能（如连续完成5个子任务）。 - 消融实验证明扩散轨迹引导是性能提升的关键因素。 4. **优势** - 有效缓解模仿学习中的误差累积问题。 - 仅需消费级GPU，计算效率高。 - 代码与实验视频将在论文接受后开源。 该方法为语言条件的长视野操作任务提供了新思路，通过轨迹级引导提升了策略的泛化能力和鲁棒性。</details> |
| 2025-02-13 | GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation | http://arxiv.org/abs/2502.09268v2 | <details><summary>展开</summary>论文提出GEVRM模型，通过结合内部模型控制（IMC）原理增强机器人视觉操作的鲁棒性。核心创新点如下： 1. **目标生成**：采用文本引导的视频扩散模型作为机器人行为规划器，通过时空压缩和随机掩码策略生成高表现力的未来视觉目标帧（参考输入）。该模型利用高效视频编码（2D/3D VAE）和DiT架构，结合Rectified Flow训练范式提升生成效率。 2. **状态对齐**：通过原型对比学习优化内部嵌入（internal embeddings），对齐当前状态与生成的目标状态。该方法模拟系统响应，隐含推断外部扰动（如光照变化、传输噪声），使模型能区分并抵抗环境干扰。 3. **动作预测**：设计基于扩散的目标引导策略，将视觉目标与当前状态联合编码，通过对比学习优化后生成7维机器人动作。该策略在扰动下仍能稳定追踪高表现力目标。 实验表明，GEVRM在CALVIN基准测试（标准/扰动场景）达到SOTA性能，并在真实机器人任务中显著提升目标生成表现力和任务成功率。消融研究验证了各模块的有效性。</details> |
| 2025-02-09 | DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control | http://arxiv.org/abs/2502.05855v3 | <details><summary>展开</summary>DexVLA是一种新型视觉-语言-动作（VLA）模型框架，旨在提升机器人控制的效率和泛化能力，尤其针对复杂长视界任务。其核心要点如下： 1. **问题背景**： - 现有VLA模型存在动作表示瓶颈和数据稀缺问题（依赖大规模人类演示数据），且视觉语言模型（VLM）与动作学习脱节。 2. **核心创新**： - **扩散专家**：引入10亿参数的扩散模型作为动作生成模块，采用多头设计支持跨体现（如单臂、双手、灵巧手）学习。 - **体现课程学习**：三阶段训练策略： - **阶段1**：仅预训练扩散专家于跨体现数据（100小时），学习低级运动技能。 - **阶段2**：对齐VLM与特定机器人体现，将抽象语义映射到物理控制。 - **阶段3**：任务特定微调，结合子步骤推理（如将“折叠衬衫”分解为“抚平褶皱”“对齐袖子”），使模型直接处理复杂任务。 3. **关键优势**： - **高效泛化**：无需任务特定适应即能完成衬衫折叠、物品分拣等任务；仅需≤100个演示即可适应新体现（如灵巧手倒饮料）。 - **长视界任务**：通过直接语言提示（如“折叠洗衣”）完成复杂任务，优于OpenVLA、π₀等模型（如洗衣折叠任务得分0.4 vs. π₀的0.2）。 - **计算高效**：预训练仅需100小时数据，推理速度达60Hz（单A6000 GPU）。 4. **实验验证**： - 在多个体现（Franka机械臂、双手UR5e、灵巧手等）和任务（洗衣折叠、餐桌清理）中，DexVLA均显著优于基线模型，尤其在数据效率和任务分解能力上表现突出。</details> |
| 2025-02-08 | HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation | http://arxiv.org/abs/2502.05485v4 | <details><summary>展开</summary>本文提出了一种名为HAMSTER的分层动作模型，用于解决开放世界机器人操作中泛化能力不足的问题。核心要点如下： ### 核心问题 - 现有视觉语言动作模型（VLA）依赖大规模机器人数据训练，但此类数据昂贵且稀缺，限制了泛化能力。 - 单层VLA模型（如OpenVLA）难以兼顾语义推理的开放性和局部动作的精确性。 ### 创新方案：分层架构HAMSTER 1. **高层VLM路径规划** - 微调视觉语言模型（VLM），输入RGB图像和任务指令，输出**2D路径**（归一化像素坐标序列 + 夹爪状态）。 - 2D路径表示机器人末端执行器的粗略轨迹，可低成本从离域数据（仿真/视频/草图）自动生成。 - 创新训练数据集（120万样本）： - **像素点预测**（77万）：物体位置问答（RoboPoint）。 - **仿真数据**（32万）：RLBench自动规划路径。 - **真实机器人轨迹**（11万）：Bridge/DROID数据集投影。 2. **低层策略执行** - 输入高层生成的2D路径、3D点云和本体感知信息，输出精确动作。 - 路径以**颜色渐变轨迹**叠加到图像（或独立通道），指导策略聚焦局部控制。 - 支持高效小规模训练（仅需任务专用数据）。 ### 关键优势 - **跨域泛化**：高层VLM利用离域数据训练，可迁移到本体/视觉/动态差异大的真实场景（如仿真→真机）。 - **性能提升**：在7个泛化维度上，真实任务成功率平均超OpenVLA 20%（相对提升50%）。 - **模块化设计**： - 高层低频调用（语义规划），低层高频执行（精确控制）。 - 支持3D感知策略（RVT-2/3D-DA），弥补纯2D输入的空间局限。 ### 实验验证 - **真实机器人测试**：桌面操作任务（抓取/推倒/按钮按压等）显著优于基线。 - **泛化场景**：包括新物体、布局干扰、背景变化等挑战。 - **失败分析**：主要源于路径预测偏差或轨迹执行误差（附录E）。 ### 意义 HAMSTER通过解耦语义规划与动作执行，为利用廉价离域数据提升机器人开放世界泛化提供了新范式，代码已开源。 > 论文链接：[https://hamster-robot.github.io/](https://hamster-robot.github.io/)</details> |
| 2025-02-08 | ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy | http://arxiv.org/abs/2502.05450v2 | <details><summary>展开</summary>这篇论文提出了ConRFT（基于一致性策略的强化微调方法），用于提升视觉-语言-动作（VLA）模型在机器人操作任务中的性能。核心要点如下： 1. **问题背景** - 传统监督微调（SFT）在接触密集型任务中表现不佳，受限于少量、不一致的演示数据。 - VLA模型需直接与环境交互，但强化学习（RL）在真实场景下面临样本效率低和安全风险高的挑战。 2. **方法创新** - **两阶段统一框架**： - **离线阶段（Cal-ConRFT）**：结合行为克隆（BC）和Q学习，通过一致性策略提取策略并稳定价值估计，仅需20-30条演示数据。 - **在线阶段（HIL-ConRFT）**：延续一致性目标，引入人类干预（HIL）确保安全探索，利用任务奖励快速优化策略。 - **关键技术**：一致性策略作为动作头，解决演示数据次优问题，保持轻量级推理效率。 3. **实验结果** - 在8个真实机器人任务（如拾取香蕉、操作抽屉等）上验证： - **成功率**：平均达96.3%，超越SFT方法144%。 - **效率**：在线微调仅需45-90分钟，任务执行轨迹长度缩短1.9倍。 - **数据利用**：仅需20条演示即可启动离线训练，显著优于基线（如Cal-QL）。 4. **优势与局限** - **优势**：高样本效率、安全探索、适应多样化VLA模型。 - **局限**：奖励函数设计敏感、编码器与骨干网络未优化（冻结训练）。 5. **开源资源** - 代码与视频见项目页面：https://cccedric.github.io/conrft/ 该方法通过强化学习与一致性策略的结合，显著提升了VLA模型在复杂机器人任务中的适应性和性能。</details> |
| 2025-02-07 | Survey on Vision-Language-Action Models | http://arxiv.org/abs/2502.06851v3 | <details><summary>展开</summary>待生成</details> |
| 2025-02-06 | Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture | http://arxiv.org/abs/2502.04558v1 | <details><summary>展开</summary>本文提出一种方法，通过探测视觉-语言-动作模型（VLA，如OpenVLA）的隐藏层激活，提取符号状态（如对象属性和动作状态），并集成到认知架构（CA，如DIARC）中，以提升机器人系统的可解释性和鲁棒性。 **核心要点如下**： 1. **问题背景**：VLA模型（输入图像和语言指令，输出动作）虽灵活但不可靠（黑盒特性且对环境变化敏感），而CA擅长符号推理但缺乏适应性。 2. **方法**： - 在OpenVLA的33个隐藏层上训练线性探针，预测LIBERO-spatial拾取任务中的符号状态（如对象关系、抓取状态）。 - 开发DIARC-OpenVLA集成系统，通过WebSocket实现实时状态监控和用户界面（GUI）。 3. **实验结果**： - 探测准确度高（>0.90），表明OpenVLA有效编码符号信息。 - 与假设相反，对象状态未在更早层编码，动作状态未在更晚层编码。 4. **贡献**：成功桥接VLA与CA，实现实时符号监控，为可解释的机器人操作奠定基础；未来需扩展至更多样化数据验证层间编码模式。</details> |
| 2025-02-04 | VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation | http://arxiv.org/abs/2502.02175v1 | <details><summary>展开</summary>本文提出VLA-Cache方法，旨在提升视觉-语言-动作（VLA）模型在机器人操控中的计算效率。核心创新点包括： 1. **动态令牌选择机制**： 通过比较连续帧的视觉输入，识别变化最小的静态令牌（如图像背景）。利用跨注意力分数过滤任务关键令牌（如机械臂和目标物体），确保关键信息参与完整计算。 2. **分层自适应缓存策略**： 根据Transformer各层注意力分布的熵值变化（低熵层注意力更集中），动态调整各层的令牌重用比例，平衡计算效率与模型精度。 3. **理论分析与实验验证**： 在LIBERO和SIMPLER仿真环境中实现1.7倍加速，成功率仅轻微下降；真实机器人（Kinova Jaco）实验证实了方法的实用性。该方法无需重新训练模型，通过KV缓存复用静态令牌的计算结果，显著降低冗余计算量。 该方法解决了VLA模型在实时机器人决策中的计算瓶颈，为时序决策场景提供了高效的视觉令牌复用方案。</details> |
| 2025-02-03 | Scalable, Training-Free Visual Language Robotics: A Modular Multi-Model Framework for Consumer-Grade GPUs | http://arxiv.org/abs/2502.01071v1 | <details><summary>展开</summary>论文提出SVLR（Scalable Visual Language Robotics）框架，一个可扩展、无需训练的视觉语言机器人系统，专为消费级GPU设计。其核心要点如下： 1. **框架设计**： - SVLR是开源、模块化的多模型框架，整合轻量级AI模型（包括视觉语言模型Mini-InternVL、分割模型CLIPSeg、大语言模型Phi-3和句子相似模型all-MiniLM），处理视觉和语言输入以生成机器人动作序列。 - 无需重新训练，通过预编程任务和参数化对象位置实现机器人控制。 2. **关键优势**： - **可扩展性**：用户可轻松添加新任务或机器人，仅需文本描述和任务定义，无需重新训练。 - **资源高效**：在消费级GPU（如NVIDIA RTX 2070）上运行，降低部署门槛。 3. **核心组件**： - **感知模块**：识别环境对象并输出位置坐标。 - **语言模型**：解析用户指令，生成任务序列（如pick-and-place）。 - **动作管理器**：使用句子相似模型匹配任务参数，确保与预定义任务一致。 4. **实验结果**： - 在UR10机器人上验证，成功执行pick-and-place等任务，展示对复杂指令（如多对象操作）的适应性。 - 模块化设计支持未来模型更新和任务扩展。 5. **局限性与展望**： - 需进一步评估复杂任务性能；未来工作包括集成深度相机、扩展至移动机器人及模型优化。</details> |
| 2025-01-31 | UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent | http://arxiv.org/abs/2501.18867v3 | <details><summary>展开</summary>本文提出UP-VLA模型，用于具身智能体的视觉-语言-动作（VLA）统一学习。核心创新点包括： 1. **问题分析**：现有VLA模型基于视觉语言模型（VLM）微调，但VLM预训练侧重高层语义理解（如VQA任务），忽视低层空间细节（如物体距离/尺寸），导致机器人控制任务中空间感知和物理动态理解不足。 2. **解决方案**： - **双目标预训练**：联合优化多模态理解（MMU）和未来视觉预测（PRE）目标，前者捕捉语义信息，后者增强空间细节感知。 - **统一架构**： - 使用连续编码器（CLIP-ViT）处理MMU任务，离散编码器（VQ-GAN）处理图像预测 - 通过特殊令牌（\|MMU\|/\|T2I\|）区分任务，设计注意力掩码机制（图4）：MMU任务中图像令牌可互关注，PRE任务中未来图像令牌关注全部历史信息 - **动作学习增强**：引入场景描述生成模块，扩展语言指令为 $L'=[E_1(O_t'), \pi_{\theta}^{MMU}(O_t, L_{prompt}), L]$，联合预测动作和未来帧（$(\hat{O}_{t+\Delta t}, \hat{A}_{t:t+\Delta t})$）。 3. **训练策略**： - **两阶段训练**：先混合Bridge机器人数据和LLaVA-tuning图像文本预训练；下游任务微调时保持图像文本数据协同训练以防遗忘 - **策略头设计**：通过轻量级MAP模块+MLP输出动作 4. **实验结果**： - Calvin仿真基准上ABC→D任务提升33%（SOTA） - 真实机器人任务成功率显著提升，尤其在需精确空间操作的任务 - 消融实验验证双目标预训练的必要性 代码已开源：https://github.com/CladernyJorn/UP-VLA</details> |
| 2025-01-28 | Improving Vision-Language-Action Model with Online Reinforcement Learning | http://arxiv.org/abs/2501.16664v1 | <details><summary>展开</summary>这篇论文提出了一种改进视觉-语言-动作（VLA）模型的新框架iRe-VLA，核心要点如下： 1. **问题背景** - VLA模型通过监督微调（SFT）将视觉语言模型（如BLIP-2）应用于机器人控制，但受限于专家数据集成本高和环境分布偏移问题。 - 直接应用在线强化学习（RL）微调大型VLA模型会导致训练不稳定和性能下降。 2. **方法创新** - 提出**迭代式强化学习框架（iRe-VLA）**，交替进行两个阶段： - **RL阶段**：冻结VLM主干参数，仅训练轻量级动作头，通过在线探索收集成功轨迹。 - **SL阶段**：解冻整个模型，在专家数据和新收集的成功轨迹上联合进行监督学习。 - 采用**低秩自适应（LoRA）** 技术提升训练效率，减少可调参数量。 3. **技术优势** - 平衡RL的探索能力和SL的稳定性，避免模型崩溃。 - 降低计算需求，适合本地机器部署。 - 通过动作头设计将VLM输出适配到低维动作空间。 4. **实验结果** - 在模拟环境（MetaWorld、Franka-Kitchen）和真实世界Panda机械臂任务中验证有效性： - 提升原始任务性能，解决未见任务。 - 增强模型泛化能力，缓解灾难性遗忘。 - 训练稳定性显著优于直接RL微调。 5. **核心贡献** 解决了大型VLA模型在线RL训练的稳定性和计算效率问题，为机器人持续学习提供了可行方案。 > 该方法已通过开源代码和实验复现验证，论文代码见项目页面。</details> |
| 2025-01-25 | An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation | http://arxiv.org/abs/2501.15068v3 | <details><summary>展开</summary>本文提出了一种数据高效的三轮回原子技能库构建方法，用于解决具身操作中端到端模型数据需求高、泛化能力差的问题。核心要点如下： 1. **问题背景**： - 现有端到端具身操作模型在真实复杂场景中泛化能力不足，且依赖大量任务特定数据，导致“数据爆炸”。 - 原子技能分解可减少数据需求，但传统方法受限于预定义静态技能集，无法动态扩展。 2. **方法框架（三轮回）**： - **VLP轮**：利用视觉-语言-规划（VLP）代理分解任务为子任务，结合空间感知（如物体检测和空间关系推理）生成执行计划。 - **VLA轮**：通过视觉-语言-动作（VLA）模型微调实现原子技能执行，利用少量数据适应新对象和场景。 - **原子技能轮**：抽象子任务为通用技能定义，动态扩展技能库；遇到新任务时仅需补充缺失技能数据，大幅降低数据成本。 3. **关键优势**： - **数据高效**：相比端到端方法，减少数据需求（实验显示任务成功率提升40%），支持跨任务复用。 - **动态扩展**：技能库随任务更新自然增长，提升对新任务的适应性。 - **高性能**：真实环境实验（如拾取放置、倒水任务）验证了方法在降低数据成本的同时维持高成功率。 4. **实验验证**： - 在ALOHA机器人上测试，相比端到端基线（如Octo、RDT-1B），数据需求减少且任务成功率更高（例如，在物体位置分布外场景成功率提升20–40%）。 - 方法兼容多种VLA模型，证实了泛化性和实用性。</details> |
| 2025-01-16 | FAST: Efficient Action Tokenization for Vision-Language-Action Models | http://arxiv.org/abs/2501.09747v1 | <details><summary>展开</summary>本文提出了一种高效的机器人动作标记化方法FAST（Frequency-space Action Sequence Tokenization），用于解决视觉-语言-动作（VLA）模型在训练高频率、灵巧机器人任务时的效率问题。核心要点如下： 1. **问题背景** - 现有VLA模型采用按维度分箱（per-dimension binning）的标记化方法，在高频控制任务中产生强相关的动作标记，导致自回归训练效率低下。 - 动作序列的高度相关性削弱了下一个标记预测目标的有效性，使模型陷入局部最优（如简单复制前一动作）。 2. **FAST方法设计** - **核心思想**：通过离散余弦变换（DCT）将连续动作信号压缩至频域，减少冗余信息。 - **流程**： - 动作归一化（映射到[-1,1]范围） - 应用DCT转换到频域 - 量化并舍入系数（平衡压缩率与重建精度） - 使用字节对编码（BPE）压缩稀疏系数矩阵 - **优势**：仅需两个超参数（缩放系数γ、BPE词表大小），无需复杂神经网络训练。 3. **FAST+通用标记器** - 基于100万条真实机器人轨迹训练，覆盖单臂/双臂/移动机器人等多样场景。 - 支持跨本体、动作空间和控制频率的通用标记化，作为即插即用方案。 4. **关键实验结果** - **压缩效率**：相比分箱法，FAST将1秒动作块的标记数减少2-13倍（表I），尤其在高频任务中（如50Hz叠衣任务压缩13.2倍）。 - **策略性能**： - 在7项评测任务中（图5），FAST显著优于分箱法和FSQ量化方法（图6），在20-50Hz任务上成功率提升30-50%。 - 首次实现DROID数据集策略的零样本泛化（图7），在未见过环境中执行桌面操作任务。 - **训练加速**：结合π₀ VLA模型时，训练时间减少5倍，性能匹配扩散模型（图1）。 5. **应用价值** - 开源通用标记器FAST+（HuggingFace集成），三行代码即可部署。 - 为大规模机器人数据集（如10k小时灵巧操作数据）的高效训练提供基础。 总结：FAST通过频域压缩解决动作序列冗余问题，显著提升VLA模型在高频灵巧任务中的训练效率和性能，并为跨场景机器人策略泛化提供新路径。</details> |
| 2025-01-12 | Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing | http://arxiv.org/abs/2501.06919v1 | <details><summary>展开</summary>论文提出Shake-VLA系统，一种基于视觉-语言-动作（VLA）模型的机器人系统，用于双手机器人操作和液体混合（如鸡尾酒制备）。核心要点如下： 1. **系统组成**： - **视觉模块**：使用YOLOV8检测配料瓶和EasyOCR读取标签（91%成功率）。 - **语音处理**：OpenAI Whisper-1实现语音转文本（嘈杂环境93%成功率），Google TTS提供语音反馈。 - **检索增强生成（RAG）**：结合FAISS检索和GPT-4o生成食谱及机器人指令。 - **异常检测**：比对食谱与可用配料，提供替代方案（95%成功率）。 - **动作执行**：语言模块（GPT-4o）生成双手机器人动作指令（如倒液、抓取），并利用力扭矩传感器精确控制液体量。 2. **实验结果**： - 整体系统在鸡尾酒制备中实现100%成功率，涵盖食谱生成到执行。 - 视觉模块在杂乱环境中表现稳健，语音模块抗噪能力强。 3. **创新与价值**： - 集成VLA模型实现人机交互与精确操作，适用于服务场景。 - 未来方向：增强多语言文本识别、噪声鲁棒性，扩展至实验室自动化等任务。</details> |
| 2025-01-09 | UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation | http://arxiv.org/abs/2501.05014v2 | <details><summary>展开</summary>论文提出 **UAV-VLA 系统**，通过整合卫星图像、视觉语言模型（VLM）和 GPT 技术，实现基于自然语言指令的无人机大规模任务自动生成。核心要点如下： ### 一、核心贡献 1. **系统设计** - 开发 **Vision-Language-Action (VLA) 系统**，用户输入文本指令（如“在100米高度环绕所有建筑飞行并返航”），系统自动生成飞行路径和动作序列。 - 流程分三步： - **目标解析**（GPT 提取指令中的任务目标）； - **视觉定位**（Molmo VLM 在卫星图中识别目标物体位置）； - **动作生成**（GPT 结合坐标和飞行协议生成可执行路径）。 - **开源代码**：https://github.com/sautenich/uav-vla 2. **新基准数据集** - 构建 **UAV-VLPA-nano-30** 评估基准，包含 **30 幅美国多地高清卫星图**（分辨率 1.5 米/像素），覆盖城市、郊区、自然区域等场景，用于测试全局路径规划能力。 3. **性能验证** - 系统性能接近人类专家水平，部分任务路径更优。 --- ### 二、实验结果 1. **效率优势** - 生成全部 **30 个任务的飞行计划仅需 5 分 24 秒**，比人类操作员（35 分钟）快 **6.5 倍**。 2. **路径质量** - 系统路径总长 **77.74 km**，较人工规划（63.89 km）长 **21.6%**，但 **23% 的任务路径更短**（见图 5）。 - **定位误差**（KNN 方法）：平均 **34.22 米**（见表 I），优于时序匹配（409.54 米）和动态时间规整（307.27 米）。 --- ### 三、未来方向 1. 构建专用卫星地图路径规划数据集，提升模型精度。 2. 开发端到端模型，实现从高层目标到全自主任务规划的闭环。 ### 总结 UAV-VLA 通过 **“文本指令→视觉定位→路径生成”** 的流水线，显著提升无人机任务规划效率，为自然语言控制无人机及多机协作奠定基础。</details> |
| 2025-01-08 | Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding | http://arxiv.org/abs/2501.04693v3 | <details><summary>展开</summary>本文提出FuSe方法，通过语言引导将异构传感器（触觉、听觉等）融入预训练的通用机器人策略中，解决多模态数据稀缺问题。核心要点如下： 1. **问题背景**：现有通用机器人策略（如Octo/VLA模型）主要依赖视觉和本体感觉，缺乏触觉、听觉等多模态感知能力，在视觉遮挡场景（如袋内抓取）表现受限。 2. **方法创新**： - **语言跨模态对齐**：提出多模态对比损失和语言生成损失，利用自然语言关联异构传感器语义。 - **传感器编码**：预训练触觉编码器（TVL）和音频频谱处理（ResNet），适配小规模数据集。 - **损失函数**：结合行为克隆损失与辅助损失（$\mathcal{L} = \mathcal{L}_{BC} + \beta\mathcal{L}_{gen} + \lambda\mathcal{L}_{contrast}$），避免模型忽略新传感器。 3. **实验验证**： - **数据集**：收集27K条真实机器人轨迹（含视觉/触觉/音频/动作/语言），覆盖桌面抓取、袋内抓取和按钮按压任务。 - **性能提升**：在视觉遮挡的袋内抓取任务中，FuSe比纯视觉基线成功率提高20%以上（图5）。 - **新能力**： - **多模态提示**：实现"抓取红色且触感柔软的发声物体"等指令（表I）。 - **组合推理**：完成"按压与蓝色按钮同声的按钮"等跨模态任务（图6）。 - **语言生成**：交互时生成物体描述（如触觉属性）。 4. **普适性**：方法适配不同架构模型（Octo和3B参数VLA模型PaliGemma），VLA版本在袋内抓取任务表现更优（图8）。 5. **开源贡献**：公开数据集、代码及模型，推动多模态机器人研究。 **限制**：计算资源需求高，历史上下文长度有限（0.4秒）。</details> |
| 2025-01-07 | OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints | http://arxiv.org/abs/2501.03841v1 | <details><summary>展开</summary>该论文提出了一种名为OmniManip的新型机器人操作系统，通过物体中心化的交互基元解决通用化操作问题。核心要点如下： 1. **问题定位**： 视觉语言模型（VLM）虽具备高层常识推理能力，但缺乏精细3D空间理解能力，难以直接用于精确操作任务。传统方法（如微调VLM为VLA模型）存在数据收集成本高、泛化性差的问题。 2. **核心创新**： - **物体中心化基元表示**：在物体规范空间（canonical space）定义交互基元（关键点和方向向量），将VLM的语义推理转化为可执行的3D空间约束（如距离/角度约束）。 - **双闭环系统**： - **规划闭环**：通过"重采样-渲染-校验"（RRC）机制实现自校正：VLM生成基元后，系统渲染交互场景并反馈给VLM验证，循环优化约束条件。 - **执行闭环**：基于6D姿态跟踪实时调整动作，通过约束优化计算轨迹（公式2-3）。 3. **技术实现**： - 采用通用6D姿态估计（Omni6DPose）和单视图3D重建实现物体规范化。 - 结合视觉基础模型（GroundingDINO+SAM）定位物体，VLM分解任务阶段并提取基元约束（公式1）。 - 方向采样沿物体主轴，通过VLM语义描述和LLM评分筛选相关方向。 4. **实验效果**： - 在12类操作任务（刚体/关节物体）中实现零样本泛化，成功率显著超越基线（表1：平均68.3% vs VoxPoser 15%）。 - 验证了无需微调VLM的可行性，并展示自动化生成仿真数据的潜力。 **创新价值**：首次实现无VLM微调的双闭环开放词汇操作系统，为通用机器人操作提供新范式。</details> |
| 2025-01-07 | Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis | http://arxiv.org/abs/2501.03565v1 | <details><summary>展开</summary>本文提出桥接语义对齐（BrgSA）框架，用于解决零样本3D医学图像诊断中的模态间隙问题。核心要点如下： 1. **问题背景**：现有视觉语言对齐方法（如CLIP）在3D医学图像（如CT）中存在显著模态间隙，即图像与文本特征在嵌入空间中形成分离簇，限制零样本诊断性能。 2. **方法设计**： - **语义总结**：利用大语言模型（LLM）提取医学报告关键信息，生成结构化摘要（如“存在[异常]”），简化文本学习。 - **跨模态知识交互（CMKI）**：引入可学习的跨模态知识库（CMKB）作为语义桥，通过注意力机制重建图像和文本特征，减少模态间隙，并结合MSE损失和对比损失（InfoNCE）优化对齐。 3. **新数据集**：构建CT-RATE-LT基准，包含15个代表性不足的异常（如罕见疾病），用于评估长尾分布下的零样本诊断能力。 4. **实验结果**：在CT-RATE、RAD-ChestCT和CT-RATE-LT上均达到SOTA： - 零样本诊断AUC显著提升（CT-RATE-LT：76.9→85.6）； - 报告-体积检索任务Recall@10提升100%（5.0→10.1），验证了跨模态对齐的有效性。</details> |
| 2025-01-06 | Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches | http://arxiv.org/abs/2501.03151v1 | <details><summary>展开</summary>这篇论文探讨了大型语言模型（LLMs）实现人工通用智能（AGI）的基础原则与方法。核心要点如下： 1. **AGI的核心挑战** - 当前LLMs虽在多领域展现强大能力（如推理、对话），但其认知能力仍存在**表面性**和**脆弱性**，缺乏真正的通用智能。 - 实现AGI需解决四个基础问题：**具身性（Embodiment）**、**符号接地（Symbol Grounding）**、**因果性（Causality）** 和**记忆机制（Memory）**。 2. **关键原则的作用** - **具身性**：通过物理或虚拟载体使LLMs与环境交互（如机器人、模拟环境），赋予其**目标感知**、**情境感知**和**自我感知**能力，支持自主决策（如PaLM-E、EmbodiedGPT模型）。 - **符号接地**：将抽象符号关联到现实实体（如通过知识图谱、本体提示），解决LLMs的**语义鸿沟**和幻觉问题（如KG-LLM集成方法）。 - **因果性**：结合神经符号方法或物理模型（如Theory of Mind），使LLMs理解事件间的因果关系，提升推理鲁棒性。 - **记忆机制**：通过注意力机制、外部存储（如RAG）或分层记忆（感官/工作/长期记忆）实现知识累积与复用，支持持续学习。 3. **实现路径** - **多模态融合**：视觉-语言-动作模型（VLAs）整合多感官数据，支持复杂任务（如导航、物体操作）。 - **模拟环境训练**：利用3D仿真或扩展现实（XR）低成本训练具身代理，再迁移至真实场景。 - **认知架构整合**：将四大原则有机融合，构建**目标驱动**、**环境交互**、**因果推断**和**知识积累**的通用AI框架。 4. **局限与展望** - 现有方法仍面临**场景泛化性不足**、**高成本数据需求**及**复杂行为建模困难**等挑战。 - 未来需探索神经符号结合、世界模型与动态记忆的深度集成，以实现人类级通用智能。 总结：论文系统论证了具身性、符号接地、因果性与记忆是LLMs迈向AGI的基石，并综述了各领域前沿技术路径，为构建真正通用的认知模型提供理论和方法基础。</details> |
| 2024-12-29 | CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance | http://arxiv.org/abs/2412.20451v2 | <details><summary>展开</summary>本文提出了一种新颖的视觉-语言-动作（VLA）模型改进框架CoA-VLA，通过引入视觉-文本链式可操作性（Chain-of-Affordance）机制增强机器人策略学习能力。核心要点如下： 1. **核心创新**：提出链式可操作性（CoA）框架，将机器人操作过程分解为四个关键可操作性阶段： - **物体可操作性**：识别目标物体及其位置 - **抓取可操作性**：确定物体最佳抓取点 - **空间可操作性**：规划物体安全放置空间 - **运动可操作性**：生成无碰撞运动轨迹 2. **多模态融合**： - 开发视觉-文本协同注入模块，将可操作性信息（视觉提示与文本描述）整合到策略网络中 - 设计动态可操作性选择机制，根据任务状态自适应激活相关可操作性，减少计算开销 3. **实验验证**： - 在真实机器人（Franka机械臂）的7项复杂任务中，CoA-VLA平均成功率85.54%，显著优于OpenVLA（54.89%）和DiffusionVLA（76.60%） - 在LIBERO仿真基准测试中，平均成功率79.8%，优于所有基线模型 - 展示出对未见物体姿态、障碍物避障和空间泛化的强鲁棒性 4. **技术优势**： - 通过自动化流水线生成大规模可操作性数据，避免人工标注成本 - 统一视觉与文本可操作性表示，提升策略学习的物理基础理解能力 - 在保持实时性的同时，显著提高复杂任务的成功率和泛化能力 该方法为机器人基础模型提供了一种可解释、可泛化的推理框架，推动了具身智能在复杂环境中的决策能力发展。</details> |
| 2024-12-24 | VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks | http://arxiv.org/abs/2412.18194v1 | <details><summary>展开</summary>基于提供的ArXiv论文HTML原文，本文的核心要点总结如下： 1. **研究背景与动机** - 现有机器人操作基准（如RLBench、CALVIN）无法充分评估基于基础模型（如VLAs/VLMs）的方法，尤其在**世界常识迁移**、**自然语言交互**和**长时序推理**任务上的能力不足。 - 语言条件操作（LCM）任务需融合多模态理解、常识迁移和复杂决策，是迈向通用人工智能的关键挑战。 2. **VLABench基准的核心贡献** - **首个针对VLAs/VLMs的综合评估基准**：涵盖100项任务（60项基础任务 + 40项复合任务），强调6大能力维度： - **视觉理解**（物体网格/纹理识别） - **空间关系理解**（相对位置、空间布局） - **常识与知识迁移**（世界知识应用） - **语义理解**（隐含意图的自然语言指令） - **物理规则应用**（力学原理推理） - **长时序推理**（多步骤规划与执行） - **大规模多样化场景**：包含163类物体、共2164个对象，支持强域随机化（纹理、光照、布局）。 - **自动化数据框架**：基于技能库和运动规划（RRT算法）实现高效轨迹生成，支持大规模预训练。 - **创新评估指标**：提出"进度分数"（Progress Score, PS），平衡任务决策准确性与执行进度。 3. **关键实验结果** - **VLAs泛化能力弱**：微调的预训练模型（如OpenVLA、RDT-1B）在**未见物体类别**和**复杂任务**上表现不佳（平均PS < 20%），显示当前架构难以迁移常识和适应新任务。 - **工作流方法受限**：Voxposer/CoPA等模块化流程在**闭环反馈**和**长时序规划**上存在瓶颈（PS ≈ 30–40%），依赖提示工程且物理推理能力不足。 - **VLMs能力不均衡**：GPT-4o在推理维度领先（PS ≈ 60%），但开源模型（如Qwen-VL）在空间感知突出；所有模型在**语义抽象**和**复合任务分解**上显著退化（PS下降至20–30%）。 4. **研究意义与展望** - VLABench明确定义了认知型智能体应具备的能力维度，为未来机器人预训练方案和VLA架构设计提供标准化测试平台。 - 实验表明当前VLAs尚未展现类似LLMs的强泛化能力，凸显机器人规模化训练（scaling laws）和基础模型适配的研究空间。 --- 总结：VLABench是首个针对语言条件操作任务的长时序推理基准，通过多维度任务设计、自动化数据流水线和综合评估机制，揭示了当前VLAs/VLMs在常识迁移、复杂推理和物理交互中的核心挑战，为下一代具身智能研究提供方向性指引。</details> |
| 2024-12-20 | QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning | http://arxiv.org/abs/2412.15576v5 | <details><summary>展开</summary>QUART-Online 提出了一种解决四足机器人视觉-语言-动作（VLA）任务中多模态大语言模型（MLLM）推理延迟问题的新方法。核心要点如下： 1. **问题与动机** - 传统参数压缩方法（如模型剪枝）虽能提升推理速度（从 2Hz 到 5Hz），但严重损害模型性能（任务成功率从 74% 降至 11%），尤其在泛化到未见任务时表现更差。 - 四足机器人需 50Hz 实时控制，而 MLLM 的低频推理（如 QUART 的 2Hz）无法满足实时性需求。 2. **创新方法：QUART-Online** - **动作块离散化（ACD）**： - 将连续动作序列压缩为离散语义向量（公式 4-5），减少输出维度。 - 通过编码器-量化器-解码器框架（图 3），在保留关键信息的同时提升处理效率。 - **动作块对齐**： - 微调 MLLM 以对齐视觉、语言和压缩动作的语义空间（公式 3），使模型直接输出离散动作令牌。 - **实时解码**： - 离散令牌经轻量级解码器（公式 6）实时重建为连续动作，频率与底层控制器同步（50Hz）。 3. **关键优势** - **零延迟推理**： - 通过 ACD 将模型输出频率从 2Hz 提升至 50Hz（图 2），避免参数压缩导致的性能损失。 - **性能提升**： - 在 QUARD 基准测试中，任务平均成功率提升 65%（表 II）。例如： - **硬任务**：避障（Go avoid）从 41% → 80%，爬行（Crawl）从 46% → 78%。 - **泛化性**：未见视觉元素（$U_v$）任务成功率 68%，未见指令（$U_l$）达 79%。 4. **实验验证** - 动作块长度优化：10 帧块（QUART-Online-10）在 50Hz 下性能最优（表 II）。 - 对比基线：超越 VLA-CLIP/VLA-VC1 等模型，且保持 MLLM 的语义理解能力。 5. **应用价值** - 为四足机器人提供低延迟、高精度的端到端 VLA 控制框架，支持复杂动态环境中的实时决策（如导航、物体卸载）。 - 项目页面：https://quart-online.github.io。 **总结**：QUART-Online 通过创新性的动作压缩与对齐机制，在维持 MLLM 强语义能力的同时实现 50Hz 实时控制，显著提升四足机器人的任务执行效率和泛化性能。</details> |
| 2024-12-18 | Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models | http://arxiv.org/abs/2412.14058v3 | <details><summary>展开</summary>Based on the provided HTML content, here is a concise summary of the paper's key points in Chinese: 1. **研究核心问题**： 论文系统探讨了构建视觉-语言-动作模型（VLAs）的四大关键因素： - **为何选择VLA**：基于预训练VLMs构建的VLAs在泛化性和数据效率上显著优于其他通用机器人策略，模拟和实物实验均验证其有效性。 - **如何设计VLA结构**：连续动作空间优于离散动作；历史观测信息融合至关重要；采用独立策略头（Policy Head）整合历史信息效果最佳（如KosMos-P.H.结构）。 - **选择何种VLM骨干网络**：在8种VLM骨干中，经过充分视觉-语言预训练的KosMos和Paligemma表现最优。 - **何时引入跨具身数据**：直接使用目标数据集微调效果显著；在跨具身数据（如Open X-Embodiment）上预训练后，再用目标数据微调（Post-training）可进一步提升性能。 2. **创新框架**： 提出**RoboVLMs框架**，支持灵活集成不同VLM骨干（如LLaVA、Flamingo、KosMos）与策略结构（单步/交错/策略头），在CALVIN、SimplerEnv仿真基准和真实机器人任务中均达到SOTA性能。 3. **关键实验结论**： - **数据效率**：大规模VLM骨干（如9B参数）在少量数据下表现更优，验证预训练表示的有效迁移。 - **泛化能力**：最优VLA结构（KosMos-P.H.）在未见过的干扰物、背景、目标物体及新技能指令中均展现强鲁棒性（实物实验成功率提升30%以上）。 - **开源贡献**：公开完整代码、模型、数据集及训练方案（robovlms.github.io），提供可复现的VLA设计指南。 4. **核心价值**： 通过超600组实验的系统分析，为构建高性能VLAs提供了实证性设计准则，推动通用机器人策略的发展。</details> |
| 2024-12-18 | RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation | http://arxiv.org/abs/2412.13877v3 | <details><summary>展开</summary>基于提供的Arxiv论文HTML原文，以下是论文要点的简明中文总结： 1. **数据集介绍**： - 提出 **RoboMIND** 数据集，专为机器人操作任务设计，包含 **107k条演示轨迹**、**479项多样化任务**，涉及**96类物体**和**4种机器人平台**（Franka Emika Panda、X-Humannoid Tien Kung人形机器人、AgileX双臂机器人、UR5e）。 - 数据通过**人类遥操作**收集，涵盖多视角RGB-D观测、本体感知状态、任务语言描述及末端执行器信息。 2. **核心优势**： - **统一标准**：在标准化平台上采集数据，确保一致性和可靠性（对比Open X-Embodiment等异构数据集）。 - **多模态信息**：提供**5k条真实失败轨迹**（含详细原因标注）及**10k条帧级细粒度语言标注轨迹**，支持失败分析与策略优化。 - **数字孪生**：在Isaac Sim中构建仿真环境，支持低成本数据扩展与高效评估。 3. **数据多样性**： - **任务类型**：覆盖基础操作（抓取/放置）、协调操作（双臂协作）、精调操作（如倒液体）等**6大类38项技能**。 - **场景与物体**：涵盖家居、工业、厨房等**5大场景**，涉及刚体、关节物体及可变形物体。 - **机器人异构性**：包含单臂（Franka、UR5e）、双臂（AgileX）及人形机器人（Tien Kung）数据，支持跨具身泛化研究。 4. **实验验证**： - **单任务模仿学习**（ACT、Diffusion Policy等）：在45项任务中验证数据有效性，平均成功率最高达55.3%（AgileX任务）。 - **多任务VLA大模型**（OpenVLA、RDT-1B等）：微调后显著提升任务泛化能力，例如RDT-1B在双臂协调任务中达100%成功率。 - **预训练增强**：使用RoboMIND预训练VLA模型后，再微调小样本数据，成功率平均提升20%（如FR-PlaceBreadPlate任务从7/10升至10/10）。 5. **贡献与意义**： - **最大规模统一平台多具身遥操作数据集**，为机器人策略泛化提供高质量训练资源。 - 公开数据集与工具链（[项目主页](https://x-humanoid-robomind.github.io/)），推动机器人操作研究。 --- 总结：RoboMIND通过标准化采集、多模态标注及大规模多样化任务数据，解决了机器人数据稀缺与异构性问题，显著提升了单任务与多任务策略模型的性能与泛化能力。</details> |
| 2024-12-16 | Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning | http://arxiv.org/abs/2412.11974v2 | <details><summary>展开</summary>待生成</details> |
| 2024-12-13 | TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies | http://arxiv.org/abs/2412.10345v3 | <details><summary>展开</summary>本文提出了一种名为“视觉轨迹提示”（visual trace prompting）的新方法，用于增强视觉-语言-动作（VLA）模型在机器人操控任务中的时空感知能力。核心要点如下： 1. **方法创新** - 引入**视觉轨迹提示**技术：通过在输入图像上叠加机器人历史运动轨迹（由稠密点跟踪算法生成），显式编码时空信息，解决VLA模型对历史动作感知不足的问题。 - 轨迹生成：使用Co-tracker提取历史图像序列中的关键点轨迹，筛选显著移动的“活跃点”，叠加到当前观测图像上作为视觉提示。 - 模型架构：输入包含原始图像和轨迹提示图像，通过分隔符拼接，并调整文本提示以引导模型关注轨迹信息（图1）。 2. **模型与数据** - 提出**TraceVLA模型**：基于OpenVLA（7B参数）微调，使用自建的**150K机器人操控轨迹数据集**（包含BridgeData-v2、Google RT1等数据）。 - 推出轻量版**TraceVLA-Phi3**：以4B参数Phi-3-Vision为骨干，在Open X-Embodiment数据集上预训练后微调，在保持性能的同时提升推理效率。 3. **性能验证** - **仿真实验**：在SimplerEnv的137种配置中，TraceVLA平均性能超越OpenVLA 10%，尤其在视角变化、干扰物存在等复杂场景下提升显著（图4）。 - **实物机器人实验**：在WidowX机器人上执行8项任务（含4项未见任务），TraceVLA成功率较OpenVLA提高3.5倍，展现出强泛化能力（图6）。 - 轻量模型TraceVLA-Phi3性能接近7B基线，推理速度更快。 4. **关键优势** - **时空感知提升**：视觉轨迹提示使模型能追踪历史动作，减少环境干扰（如光照变化、背景杂乱）的影响。 - **高效性**：相比直接输入多帧历史图像（导致性能下降6%），轨迹提示避免信息冗余；较文本描述轨迹方案提升6.4%性能（图8）。 - **泛化性**：在物体、场景、任务指令变化的未见任务中表现鲁棒（如抓放香蕉任务成功率80% vs 基线10%）。 5. **局限性** - 训练内存成本增加约10GB（H100），推理中轨迹跟踪引入额外0.03秒/步耗时（图10）。 - 轨迹长度需平衡（N=6最优），过长会遮挡关键信息（图9）。 **结论**：视觉轨迹提示通过简单高效的时空信息编码，显著提升VLA模型在机器人操控中的决策能力，为通用机器人策略提供新思路。模型与代码已开源。</details> |
| 2024-12-09 | Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks | http://arxiv.org/abs/2412.06224v2 | <details><summary>展开</summary>Uni-NaVid提出了一种基于视频的视觉-语言-动作（VLA）模型，用于统一多种具身导航任务（视觉语言导航、目标导航、具身问答和人跟随）。核心创新点包括： 1. **在线Token合并机制**：通过分层压缩历史视频帧的视觉Token（当前帧压缩率2×，短期帧8×，长期帧16×），显著减少计算量。该机制支持实时处理RGB视频流，将推理速度提升至5Hz，满足实际部署需求。 2. **多任务统一架构**：模型仅需单视角RGB视频和语言指令作为输入，直接输出底层动作（前进/转向/停止）。采用"前瞻预测"策略，一次性生成未来多步动作（实验中为4步），提升规划效率。 3. **大规模数据集**：收集360万样本覆盖四大导航任务，并融合230万互联网视频问答（VQA）和视频描述数据，增强场景理解与仿真-现实泛化能力。 实验表明，该模型在多个基准测试中达到SOTA水平，消融研究验证了多任务协同训练的有效性。真实场景部署证实了其非阻塞导航能力和强泛化性（项目页面：https://pku-epic.github.io/Uni-NaVid）。</details> |
| 2024-12-05 | NaVILA: Legged Robot Vision-Language-Action Model for Navigation | http://arxiv.org/abs/2412.04453v2 | <details><summary>展开</summary>论文提出NaVILA框架，用于腿式机器人的视觉-语言-动作导航，核心创新点包括： 1. **分层架构设计** - **高层VLA模型**：将视觉语言模型（VILA）微调为视觉语言动作模型，输出自然语言描述的中层动作指令（如"前进75cm"），而非直接生成底层关节控制信号。 - **底层运动策略**：基于强化学习的视觉运动策略，将中层指令转化为关节动作，利用LiDAR点云构建高度图实现避障，通过Isaac Lab仿真训练并直接部署到真实机器人。 2. **创新训练方法** - **多源数据融合**：整合仿真导航数据（R2R-CE/RxR-CE）、人类游览视频（YouTube）、场景理解数据（ScanQA）和通用VQA数据，增强泛化能力。 - **人类视频利用**：首次通过姿态估计和语言重述，从人类游览视频提取连续空间动作标签，解决真实环境数据稀缺问题。 3. **性能优势** - 在VLN-CE基准上超越SOTA方法17%成功率，新提出的VLN-CE-Isaac仿真基准上视觉策略比非视觉策略高14%成功率。 - 真实世界实验在25条指令中达到88%成功率（复杂指令75%），成功部署于Unitree Go2/H1、Booster T1等多机器人平台。 该框架通过解耦高层推理与底层控制，解决了语言指令到关节动作的转换难题，同时兼顾了跨场景泛化性和复杂地形适应性。</details> |
| 2024-12-02 | Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control | http://arxiv.org/abs/2412.01034v1 | <details><summary>展开</summary>该论文提出了一种量化感知模仿学习框架（QAIL），用于实现资源高效的机器人控制策略部署。核心创新点如下： 1. **问题背景** - 基于深度神经网络的策略模型（如视觉-语言-动作模型）在机器人操控和自动驾驶中面临计算开销大、内存需求高的问题 - 传统量化方法会导致动作决策误差累积，尤其在复杂长序列任务中（如物体精细操作、密集交通场景） 2. **解决方案** - **量化感知模仿学习（QAIL）**：在微调阶段引入量化操作，增强模型对低精度误差的鲁棒性 - **量化鲁棒行为克隆（QBC）**：创新性地通过对齐全精度模型与量化模型的行动分布，减少序列决策中的误差累积 - 加权QBC（wQBC）：基于视觉注意力机制动态调整关键状态的优化权重 3. **关键技术指标** \| 应用场景 \| 量化方案 \| 加速比 \| 能效提升 \| 硬件平台 \| \|----------------\|----------\|--------\|----------\|---------------\| \| 机器人操控 \| 4bit权重 \| 2.5× \| 2.5× \| 边缘GPU \| \| 自动驾驶 \| 4bit权重+激活 \| 3.7× \| 3.1× \| 低端GPU \| \| 车载CPU部署 \| 8bit权重+激活 \| 1.7× \| 1.3× \| 车载CPU \| 4. **实验验证** - 在LIBERO机器人操作基准上保持与FP32模型相当的任务成功率 - 在NoCrash自动驾驶基准的密集场景中碰撞率降低47% - 可视化分析显示量化模型保留了与全精度模型相似的注意力机制（图5） 5. **创新价值** 首次实现模仿学习策略模型在资源受限设备的高效部署，为实时机器人控制提供实用解决方案，代码已开源。 该方法显著降低了策略模型的计算需求，同时保持决策可靠性，为边缘设备部署大规模策略模型开辟了新途径。</details> |
| 2024-11-29 | SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters | http://arxiv.org/abs/2412.00174v1 | <details><summary>展开</summary>论文提出了一种名为**SOLAMI**的端到端社交视觉-语言-动作（VLA）建模框架，用于实现用户与3D自主角色的沉浸式交互。核心贡献包括： 1. **统一社交VLA架构** - 基于解码器LLM主干，处理用户语音和动作输入，生成角色语音与动作响应。 - 通过运动分词器（VQ-VAE）和语音分词器（SpeechTokenizer）将多模态输入离散化，实现低延迟端到端生成。 2. **合成多模态数据集SynMSI** - 利用现有文本-动作数据集（如HumanML3D、Inter-X），通过自动化流程生成6.3K轮多模态对话数据。 - 结合GPT-4o生成角色相关对话脚本，并通过文本嵌入检索匹配动作，确保语音与动作对齐。 3. **沉浸式VR交互接口** - 开发VR系统（Oculus Quest 3 + 后端服务），实时捕捉用户动作与语音，驱动角色生成多模态响应。 - 支持面部动画（UniTalker）与角色个性化（3D模型库），提升沉浸感。 4. **三阶段训练策略** - **分词器预训练**：运动VQ-VAE与语音离散化。 - **多任务预训练**：46K动作-文本对与410K语音-文本对对齐多模态。 - **指令微调**：5.7K多轮对话数据优化社交响应生成。 5. **实验结果** - 定量评估显示SOLAMI在动作精度（PA-MPJPE↓ 7.2）、语音相似度（VC↑ 0.81）和延迟（1.4s↓）上优于模块化基线（如LLM-Agent）。 - 用户研究证实其生成响应更自然且符合角色设定。 SOLAMI首次实现社交场景中多模态输入到输出的端到端建模，为3D角色交互提供新范式。</details> |
| 2024-11-29 | RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World | http://arxiv.org/abs/2412.00171v3 | <details><summary>展开</summary>论文提出了一种闭环机器人学习系统框架，要点如下： - 从真实机器人收集演示数据。 - 数据经过处理（如清洗和格式化）后，进行人工或自动标注。 - 标注数据用于训练机器学习模型。 - 模型训练后进行性能评估。 - 评估结果反馈至真实机器人，实现系统优化闭环。</details> |
| 2024-11-29 | CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation | http://arxiv.org/abs/2411.19650v1 | <details><summary>展开</summary>CogACT是一种新型视觉-语言-动作（VLA）基础模型，旨在提升机器人操作的认知与动作协同能力。其核心创新点包括： 1. **组件化架构**：分离视觉模块（处理图像）、语言模块（整合指令与视觉信息）和动作模块（扩散变换器DiT生成动作序列），取代传统VLM直接量化动作的方法。 2. **扩散动作模块**：采用DiT建模连续、多模态的动作序列，通过条件扩散过程提升动作精度和时序相关性，并展示良好的参数缩放性（数百兆参数即可显著提升性能）。 3. **自适应动作集成（AAE）**：在推理时动态融合历史预测与当前动作，避免跨模态冲突，优化轨迹平滑性。 4. **性能优势**：在5种机器人平台（仿真和真实环境）上评估，CogACT显著超越OpenVLA（相似7B模型规模）和RT-2-X（55B模型），平均成功率提升35%（仿真）和55%（真实任务），并展现强泛化能力（适应新机器人、未见物体及背景）。</details> |
| 2024-11-28 | GRAPE: Generalizing Robot Policy via Preference Alignment | http://arxiv.org/abs/2411.19309v2 | <details><summary>展开</summary>待生成</details> |
| 2024-11-18 | Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics | http://arxiv.org/abs/2411.13587v4 | <details><summary>展开</summary>本文探讨了机器人学中视觉-语言-动作模型（VLAMs）的对抗性漏洞，聚焦于这些模型在面临对抗攻击时的脆弱性及其对机器人系统安全性的影响。</details> |
| 2024-11-15 | Visual-Linguistic Agent: Towards Collaborative Contextual Object Reasoning | http://arxiv.org/abs/2411.10252v1 | <details><summary>展开</summary>论文提出视觉-语言代理（VLA）框架，以解决多模态大语言模型（MLLMs）对象定位不准和传统检测模型上下文建模不足的问题。VLA通过协作机制整合MLLMs的关系推理能力与传统检测器的定位优势： 1. **框架设计**：MLLM作为语言代理（Linguistic Agent），生成图像描述并评估检测结果；视觉代理（Vision Agents）负责对象检测和分类校正，二者通过空间关系推理过滤错误检测，并针对性修正分类。 2. **核心优势**：结合全局上下文理解与局部定位，提升检测的准确性和上下文连贯性，减少不合理检测（如将月亮误标为橘子）。 3. **实验验证**：在COCO数据集上，VLA显著提升多种检测模型（如Faster R-CNN、DINO）性能，最高使AP50:95指标提升3%（如DINO从49.5%至52.1%），尤其改善小物体检测（APs提升达4%）。 4. **局限性**：对冗余边界框等复杂场景处理仍需优化。</details> |
| 2024-11-05 | VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation | http://arxiv.org/abs/2411.03540v1 | <details><summary>展开</summary>论文提出VLA-3D数据集，用于3D语义场景理解与导航。基于11.5K个真实3D室内扫描（来源包括ScanNet、Matterport3D等），数据集包含： - **核心内容**：点云、对象语义标注（286K对象/477类）、可导航空间标注、场景图（23.5M启发式生成的视图无关空间关系）及9.7M合成参考语句（强调空间关系消歧）。 - **特点**：支持多房间场景，引入可导航空间作为目标，语言语句确保无歧义、最小化描述。 - **目的**：解决现有数据集规模小、语言不自然问题，促进鲁棒的交互式导航代理开发。 - **评估**：SOTA模型（如MVT、3D-VisTA）基准测试准确率仅22-29%，远低于其他数据集，突显挑战性。 代码与可视化工具已开源。</details> |
| 2024-11-04 | DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution | http://arxiv.org/abs/2411.02359v1 | <details><summary>展开</summary>论文提出DeeR-VLA（动态机器人视觉-语言-动作模型），一种高效的多模态大语言模型（MLLM）动态推理框架，用于资源受限的机器人控制。核心创新点包括： 1. **多出口架构**：在MLLM中设计多个中间出口，根据场景复杂度动态激活不同规模的子模型。简单场景提前退出，避免冗余计算；复杂场景使用完整模型。 2. **自适应推理机制**： - 提出**动作一致性准则**替代传统的置信度指标（因无Softmax输出），通过相邻层动作预测的L2距离判断终止时机。 - 开发**预算约束优化算法**，在预设计算量（平均功耗）、峰值时延和GPU内存限制下（如2GB）自动调整阈值，平衡性能与效率。 3. **高效训练方法**： - 训练时随机采样各出口特征，减少动态推理差异。 - 结合LSTM动作头整合时序信息，处理部分可观测状态。 实验结果在CALVIN机器人操作基准上： - 降低LLM计算量5.2-6.5倍，GPU内存占用减少2-6倍。 - 在严格内存限制下（2GB）性能仍优于基线，保持78.9%任务成功率。 - 开源代码：https://github.com/yueyang130/DeeR-VLA 该方法显著提升了MLLM在嵌入式平台的部署可行性，为资源受限的实时机器人控制提供新范式。</details> |
| 2024-11-04 | Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks | http://arxiv.org/abs/2411.05821v2 | <details><summary>展开</summary>根据论文内容，中文要点总结如下： 1. **研究目标** 提出首个系统性评估视觉-语言-动作模型（VLAs）在机器人学习任务中的框架，通过20个Open-X数据集评测GPT-4o、OpenVLA和JAT三种先进模型在操作任务中的表现。 2. **核心发现** - **模型差异显著**：GPT-4o通过精细提示工程实现最稳定性能（平均AMSE 0.1-0.5），OpenVLA在训练分布内任务表现优异但跨任务波动大，JAT整体表现较弱（AMSE普遍＞1）。 - **共性缺陷**：所有模型在需多步规划的复杂操作任务（如厨房物品整理）中表现显著下降。 - **敏感因素**：模型性能受动作空间特性（如连续/离散控制）和环境变化（如物体遮挡）影响显著。 3. **方法论创新** - 构建标准化评估流程，包括动作空间转换协议（如夹爪命令三元离散化）和跨平台适配方案。 - 提出归一化AMSE（NAMSE）指标，消除动作空间尺度差异，揭示GPT-4o具有最优任务泛化性（NAMSE＜0.2）。 4. **关键启示** - 提示工程对VLA性能影响大于架构设计（GPT-4o的提示含动作空间统计描述）。 - 专用模型（OpenVLA）与通用模型（GPT-4o）存在性能权衡，需平衡任务专注力与泛化能力。 5. **资源开放** 公开评估框架和53个数据集（32TB）的预处理代码，促进VLA模型的标准化评测。 6. **未来方向** 扩展评估至非机器人领域（如软件环境交互），并探索组合推理、长序列任务等能力评测。</details> |
| 2024-11-01 | CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision | http://arxiv.org/abs/2411.00508v4 | <details><summary>展开</summary>待生成</details> |
| 2024-10-21 | The Duality of Generative AI and Reinforcement Learning in Robotics: A Review | http://arxiv.org/abs/2410.16411v2 | <details><summary>展开</summary>这篇论文探讨了生成式AI（如大型语言模型LLM、视觉语言模型VLM、扩散模型DM）与强化学习（RL）在机器人领域的协同作用，提出了一种新的分类框架。核心要点如下： ### 核心二元性 1. **生成式AI作为RL工具** - 利用预训练的生成模型（LLM/VLM/DM）增强RL训练环节： - **奖励设计**：LLM/VLM根据任务描述自动生成奖励函数（如EUREKA框架） - **状态表示**：VLM从视觉输入中提取语义特征（如ZeST框架） - **规划探索**：LLM生成高层任务分解（如BOSS框架的技能链机制） - 优势：提供先验知识，减少训练样本需求，支持零样本泛化 2. **RL优化生成式策略** - RL用于训练/微调生成式策略（如Transformer/Diffusion策略）： - **预训练**：通过RL训练通用策略骨架（如VLA模型） - **微调**：适应具体任务（如安全约束下的策略调整） - **策略蒸馏**：将大模型知识压缩至轻量策略（专家→通用模型双向传输） ### 关键贡献 - **新分类法**（图2）：基于模型架构（LLM/VLM/DM）、模态（文本/图像/轨迹）、任务角色（奖励/状态/规划）三维度组织169篇文献 - **挑战分析**：指出生成模型与RL融合的瓶颈： - **可扩展性**：大模型计算成本高（如GPT-4实时推理负担） - **安全性**：生成策略的不可解释性（黑箱决策风险） - **数据对齐**：仿真到现实的域适应问题（表2/3显示仅37%研究含实物实验） - **未来方向**：提出三类新研究方向： 1. 基于人类反馈的RL（RLHF） 2. 演员-评论员基础模型架构 3. 结合最优控制的约束感知生成模型 ### 实证发现 - **扩散模型崛起**（图1b）：2023年后DM在RL中应用激增（如规划/探索任务） - **多模态必要性**（图4a）：VLM相比LLM更适应具身任务（避免文本描述的信息损失） - **开源缺口**：仅41%论文公开代码（表2/3），制约复现与改进 论文通过[GitHub仓库](https://github.com/clmoro/Robotics-RL-FMs-Integration)持续更新相关研究，强调生成式AI与RL的互补性将推动自适应机器人系统的演进。</details> |
| 2024-10-21 | VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making | http://arxiv.org/abs/2410.15885v3 | <details><summary>展开</summary>该论文提出了一种名为VLASCD（MIMO-VLA）的新型视觉语言动作模型，旨在解决多输入多输出（MIMO）场景下传统多输入单输出（MISO）模型的根本性缺陷。核心要点如下： 1. **问题发现** - 现有MISO模型（如LLMs和VLAs）在并行多任务场景（如同时对话与决策）中存在任务干扰问题，导致性能下降。 - 任务竞争共享输出通道引发优化不平衡，例如：对话生成与决策动作相互排斥。 2. **解决方案：MIMO-VLA框架** - **并行多任务输出**：通过独立输出通道支持同步生成自然语言对话和连续动作决策。 - **关键技术创新**： - **连续动作映射**：采用MLP直接输出连续动作值（如加速度/转向角），避免离散化误差。 - **图像重建损失**：通过辅助视觉重建任务增强环境特征提取能力。 - **标签平滑策略**：优化文本生成质量，防止过拟合。 - **梯度空间隔离机制**：语言、动作、图像损失分别作用于不同位置编码，消除任务干扰。 3. **实验验证** - **测试平台**：CARLA自动驾驶仿真环境。 - **性能优势**： - 决策指标：驾驶得分（DS）提升至105.25（H=4），远超OpenVLA（-7.84）和Decision Transformer（7.68）。 - 对话能力：GPT-4o评估得分显著高于基线模型，保持流畅人机对话。 - 泛化性：在未训练地图（town04）表现稳定（DS=94.26）。 4. **消融实验结论** - 移除语言损失导致对话质量骤降；图像损失提升决策鲁棒性。 - 连续动作损失比离散化方法（如OpenVLA）效果提升46%。 5. **意义** - 首次实现端到端同步对话与决策的统一框架，为多模态多任务学习开辟新方向。 代码已开源：https://github.com/Mark-zjtang/MIMO-VLA</details> |
| 2024-10-21 | A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM | http://arxiv.org/abs/2410.15549v1 | <details><summary>展开</summary>### 论文要点总结： 1. **问题背景**： Vision-Language-Action (VLA) 模型通过整合视觉和语言输入，使机器人执行复杂任务，但现有模型计算开销大，导致推理速度慢（如1-6Hz），无法满足实时操作需求，且缺乏泛化能力。 2. **核心方法**： 提出 **Dual Process VLA (DP-VLA)** 框架，受双过程理论启发： - **大型系统2模型（L-Sys2）**：基于Vision-Language Models (VLMs)，处理复杂推理和决策（如任务规划），在低频运行（仅任务变更时激活），减少计算负担。 - **小型系统1模型（S-Sys2）**：处理实时运动控制（如传感器输入和状态反馈），高频运行确保快速响应。 两者协同：L-Sys2输出潜在特征（latent features）指导S-Sys1生成精细动作。 3. **关键贡献**： - **高效性**：推理速度提升至0.03秒（RoboCasa实验），任务成功率提高20.4%（平均57.3% vs 基线47.6%）。 - **可扩展性**：L-Sys2可无缝升级新VLMs，无需修改整体系统。 - **实验验证**：在RoboCasa模拟环境中，优于OpenVLA、BC-Transformer等基线。 4. **实验结果**： - **数据集**：RoboCasa（厨房场景，24项原子任务）。 - **配置**：L-Sys2用OpenVLA，S-Sys1用BC-Transformer。 - **性能**：高成功率（如开门任务达84%），低延迟（比OpenVLA快8倍）；消融实验显示解码阶段特征效果最佳，且预训练VLMs优于微调。 5. **结论**： DP-VLA通过分离推理与控制，平衡计算效率与任务性能，为实时机器人操作提供可扩展解决方案。未来工作将优化动态调度和多级推理。</details> |
| 2024-10-17 | Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand | http://arxiv.org/abs/2410.14022v1 | <details><summary>展开</summary>本文提出一种混合控制方法，结合视觉-语言-动作（VLA）模型和扩散模型的优势，实现拟人手（ADAPT Hand 2）的灵巧操作。VLA模型基于语言命令进行高层规划（如手臂运动），提供泛化性；扩散模型处理低层交互（如抓取），提供精度和鲁棒性。通过事件信号触发两者切换，用于拾取-放置任务。 实验表明，该方法成功率超过80%（VLA单独使用仅40%），关键优势包括：VLA准确定位对象、扩散模型支持多模态抓取和错误恢复、拟人手的柔顺性增强交互鲁棒性。这是首次在拟人手上应用VLA模型，为灵巧操作提供新框架。</details> |
| 2024-10-15 | Latent Action Pretraining from Videos | http://arxiv.org/abs/2410.11758v2 | <details><summary>展开</summary>本文提出了一种无监督的视觉-语言-动作（VLA）模型预训练方法LAPA（Latent Action Pretraining），核心创新点是通过视频学习离散的潜在动作表示，无需机器人动作标签。主要贡献如下： 1. **方法框架** - **潜在动作量化**：使用VQ-VAE学习视频帧间的离散潜在动作（如动作token），形成通用动作表示空间。 - **潜在预训练**：基于视觉-语言模型预测潜在动作，仅需视频和任务描述（无动作标签）。 - **动作微调**：在小规模机器人动作数据上微调模型，将潜在动作映射到实际机器人动作空间。 2. **关键优势** - **无监督学习**：利用大规模网络视频（如人类操作视频）预训练，突破机器人动作标注数据的限制。 - **跨任务泛化**：在Language Table仿真环境中，跨任务/环境/具身的泛化性能显著优于基线（如UniPi、VPT），部分场景超越使用真实动作标签的ActionVLA模型。 - **效率提升**：预训练效率达OpenVLA的30倍（H100 GPU小时数对比）。 3. **实验结果** - **真实机器人任务**：在Franka机器人上，LAPA（预训练于Open-X数据集）平均成功率50.1%，超越OpenVLA（43.9%）；仅用人类操作视频预训练的LAPA也优于BridgeV2预训练的OpenVLA。 - **人类视频迁移**：在Something-Something V2人类视频上预训练的LAPA，在SIMPLER仿真任务中成功率达82.5%，验证了人类-机器人具身迁移的有效性。 4. **潜在动作分析** - 学习的潜在动作具有语义一致性（如"向下移动"），且在多具身数据中形成共享表示空间，支持跨数据集泛化。 - 结合解码器可实现神经模拟，生成动作后的未来帧（图7）。 **局限**：细粒度动作（如抓取）性能待提升，实时推理延迟需优化。该方法为利用网络视频构建机器人基础模型开辟了新途径。</details> |
| 2024-10-10 | Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation | http://arxiv.org/abs/2410.08001v3 | <details><summary>展开</summary>这篇论文提出了一种名为RoboDual的双系统框架，用于提升机器人操作的泛化性、效率和适应性。核心要点如下： 1. **问题背景**： - 通用策略（如基于视觉-语言-动作的VLA模型）具有跨场景泛化能力，但存在推理延迟高、训练成本大、难以适应新传感器模态的问题。 - 专用策略（如扩散策略）在特定任务中高效精确，但泛化能力有限。 2. **解决方案**： - **双系统协同框架**： - **通用模块**：基于OpenVLA构建，输出离散化动作和潜在表示，提供高层任务理解。 - **专用模块**：轻量级扩散Transformer，通过交叉注意力机制融合通用模块输出和多模态传感器输入（RGB、深度等），实现实时精准控制。 - **异步推理机制**：通用模块的低频输出指导专用模块的高频动作展开，控制频率提升3.8倍。 3. **关键技术**： - **位移窗口条件机制**：解决双系统异步执行时的时序对齐问题。 - **延迟感知训练**：增强对真实世界延迟的鲁棒性。 - **多模态统一编码**：通过感知重采样器压缩多源输入，提升效率。 4. **实验效果**： - **性能提升**： - 在CALVIN基准测试中，任务平均完成长度达3.66（提升12%），连续5任务成功率54.4%（提升13.2%）。 - 实物实验中平均成功率超基准方法20%，多指令任务（如"物体放入篮子"）达80%。 - **泛化性**：在位置偏移、视觉干扰、新背景和新物体场景下，平均成功率70%（超专用策略40%）。 - **效率优势**： - 仅需20M可训练参数和8 GPU小时训练，5%数据下保持高性能。 - 比纯通用策略训练节省1400倍计算资源。 5. **开源信息**：代码和项目页公开于[https://robodual.github.io](https://robodual.github.io)。</details> |
| 2024-10-07 | LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation | http://arxiv.org/abs/2410.05191v1 | <details><summary>展开</summary>本文提出LADEV，一种专为机器人操作中的视觉-语言-动作（VLA）模型设计的语言驱动测试与评估平台。VLA模型虽能整合视觉和语言输入生成控制动作，但其数据驱动特性与低可解释性导致可靠性和鲁棒性难以保障。LADEV通过以下核心机制解决该问题： 1. **语言驱动的模拟环境生成**：利用大型语言模型（LLM）将自然语言描述自动转换为模拟环境配置（如物体布局、光照和相机位姿），避免人工调整，提升测试效率。 2. **自然语言任务指令释义**：通过LLM生成任务指令的多样化变体（如“拾取苹果”改写为“拿起苹果”），结合语义相似性验证，评估VLA模型对语言输入的敏感性。 3. **批量式评估**：支持单指令批量生成多场景和指令变体，实现大规模高效测试（如4000+场景）。 实验验证中，LADEV在四类机器人操作任务（拾取、移动、放置、放入）上测试了7种VLA模型（如RT-1、OpenVLA），结果表明： - 物体数量增加或使用未见物体（如YCB数据集）会显著降低模型性能。 - 指令变体导致任务成功率平均下降10%以上，凸显语言输入对VLA鲁棒性的影响。 - 环境变化（如光照调整）对部分模型性能影响较大。 LADEV为VLA模型提供了自动化、可扩展的评估基准，推动更智能机器人系统的发展。</details> |
| 2024-10-02 | Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust | http://arxiv.org/abs/2410.01971v1 | <details><summary>展开</summary>本文提出BYO-VLA方法，旨在提升视觉-语言-动作（VLA）模型对视觉干扰的鲁棒性，核心要点如下： 1. **问题背景**：现有VLA模型（如Octo、OpenVLA）在任务无关的视觉干扰（如干扰物体、背景颜色变化）下性能显著下降（成功率降低40%），且微调大规模VLA模型成本高昂。 2. **核心方法**：BYO-VLA是一种运行时干预方案： - **动态识别敏感区域**：利用视觉语言模型（VLM）定位任务无关区域，通过敏感度探针（扰动图像区域并监测动作变化）量化模型敏感性。 - **最小化图像编辑**：仅对高敏感性且任务无关的区域进行编辑（如使用Inpaint Anything移除物体干扰或修改背景颜色）。 - **无需模型修改**：兼容任意预训练VLA模型，无需微调或访问模型权重。 3. **实验结果**： - 在语言指令操纵任务中，BYO-VLA使VLA模型在干扰环境下成功率恢复20-40%，接近无干扰时的性能。 - 优于基线方法（如GradCAM敏感度评估），且编辑后的图像保持视觉合理性。 4. **创新点**： - 首次提出纯运行时干预方案，结合任务相关性判断与模型敏感性检测。 - 通过自动化图像编辑工具实现轻量级处理，降低对大规模数据训练的依赖。 5. **应用价值**：为部署通用机器人策略提供实用工具，提升复杂环境中的视觉泛化能力。代码和示例详见项目网站：https://aasherh.github.io/byovla/。</details> |
| 2024-09-29 | RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model | http://arxiv.org/abs/2409.19590v1 | <details><summary>展开</summary>本文提出 **RoboNurse-VLA**，一种基于视觉-语言-动作（VLA）模型的机器人洗手护士系统，用于精准传递手术器械。核心要点如下： 1. **核心创新** - 首次将 **Segment Anything Model 2 (SAM 2)** 与 **Llama 2 语言模型** 集成到 VLA 框架中，实现手术器械的实时抓取与交接。 - 系统通过外科医生的**语音指令**操作，自动识别目标器械并优化抓取/交接姿态。 2. **关键技术** - **视觉模块**：基于 YOLOv8 检测器械和手部，SAM 2 生成分割掩码，结合 MLP 投影器将视觉特征映射到语言空间。 - **动作生成**：Llama 2 根据语音指令和视觉输入生成机器人控制动作（离散化为 256 个动作 token）。 - **训练效率**：冻结视觉模块，使用 **LoRA 微调 Llama 2**，单块 A100 GPU 仅需 20 小时。 3. **实验成果** - **零样本任务**：在复杂场景（多相似器械）中，成功率（65%）显著优于 Octo、RT-2-X 和 OpenVLA（≤10%）。 - **微调后性能**： - 标准器械交接成功率 **100%**（基线模型 ≤45%）。 - 手部位置/姿态变化时保持 **95%** 成功率（基线 ≤40%）。 - **泛化能力**： - 未见过的器械：**90%** 成功率。 - 难抓取物品（如乒乓球）：**95%** 成功率（精准控制抓取力至 4N）。 4. **优势与贡献** - 解决相似器械区分、抓取姿态优化、动态环境适应等挑战。 - 超越现有模型（如 RT-2-X、OpenVLA），且模型规模更小。 - 提供开源数据集（700 张标注手术器械图像）和系统设计细节。 5. **未来方向** - 临床环境验证、扩展数据集、开发避障功能及优化人机协作安全性。 **总结**：RoboNurse-VLA 通过融合 SAM 2 的精准视觉分割与 Llama 2 的语义理解能力，实现了高鲁棒性的手术器械自主交接系统，为手术室自动化提供了新方案。</details> |
| 2024-09-19 | TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2409.12514v5 | <details><summary>展开</summary>本文提出TinyVLA，一种快速、数据高效的视觉-语言-动作（VLA）模型，用于机器人操作。现有VLA模型（如OpenVLA）存在推理速度慢、依赖大规模机器人数据预训练的问题。TinyVLA通过两个关键创新解决这些问题： 1. **轻量级架构**：采用参数少于10亿的紧凑视觉语言模型（VLM）作为主干，结合扩散策略解码器直接输出机器人动作，避免自回归生成动作令牌的开销。 2. **数据高效训练**：使用参数高效微调技术（LoRA），仅需更新5%参数，无需大规模机器人数据集预训练。 实验结果表明： - **性能优势**：在真实机器人任务中，TinyVLA-H比OpenVLA平均成功率提高25.7%（94% vs 68.3%），参数减少5.5倍，推理速度快20倍（14ms vs 292ms）。 - **泛化能力**：在未见过的指令、物体、背景、光照和空间布局下表现出色，尤其在双手机器人任务中，TinyVLA-H成功率达44.5%，而OpenVLA完全失败。 - **多任务学习**：在模拟环境（MetaWorld）和真实单臂/双臂任务中均超越扩散策略等基线模型。 核心贡献在于提供了一种高效VLA框架，在保持性能的同时显著降低计算开销，为机器人部署提供新思路。</details> |
| 2024-09-12 | HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers | http://arxiv.org/abs/2410.05273v3 | <details><summary>展开</summary>HiRT提出了一种分层机器人变换器框架，用于解决大型视觉-语言-动作（VLA）模型在机器人控制中的计算延迟问题。核心创新点包括： 1. **分层架构设计** - **低频语义模块**：基于预训练视觉语言模型（如InstructBLIP），以1-2Hz频率处理多模态输入（图像+语言指令），生成包含任务语义的潜特征。 - **高频执行模块**：轻量级视觉策略（如EfficientNet/ViT）以30Hz运行，通过FiLM或交叉注意力机制融合潜特征与实时观测，输出低层级动作。 2. **关键技术改进** - **异步特征缓存**：语义模块的潜特征持续更新并缓存，供执行模块随时调用。 - **条件机制**：提出跨模块的三种融合方式：FiLM调制卷积层、交叉注意力增强Transformer层、前缀调优优化动作头。 3. **性能优势** - **静态任务**：控制频率提升至2倍（10Hz→20Hz），成功率与基线相当。 - **动态任务**：真实场景动态操作成功率从48%提升至75%（如抓取移动物体）。 - **效率提升**：通过解耦计算负载，推理延迟降低50%以上。 4. **理论基础** 受人类认知双系统理论启发：语义模块对应慢速决策系统（System 2），执行模块对应快速反应系统（System 1）。 该方法在仿真和实物实验中均验证了有效性，为资源受限场景下的实时机器人控制提供了新思路。 --- *关键词：分层学习，视觉语言模型，实时控制，机器人操作*</details> |
| 2024-09-05 | OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving | http://arxiv.org/abs/2409.03272v1 | <details><summary>展开</summary>OccLLaMA 是一种面向自动驾驶的生成式世界模型，其核心创新点如下： 1. **统一多模态框架** - 提出 occupancy-language-action 三模态统一架构，以语义占据（occupancy）作为通用 3D 视觉表示 - 构建包含场景/语言/动作的联合词汇表，支持多任务统一处理 2. **高效场景分词器** - 设计基于 VQVAE 的稀疏编码器，解决占据栅格的稀疏性和类别不平衡问题 - 通过伪点云表示和支柱嵌入优化特征提取 - 采用解耦解码器分别重建几何与语义信息 3. **生成式世界建模** - 扩展 LLaMA 架构实现 "next token/scene" 双预测机制 - 引入空间注意力模块捕捉场景空间关系 - 添加特殊功能 token（如 <occ>、<act>）标识模态边界 4. **三阶段训练策略** - 场景分词器预训练：优化重建损失（交叉熵 + Lovasz） - 3D 占据-语言-动作预训练：多模态联合建模 - 指令微调：适配下游任务 5. **多任务性能验证** - 4D 占据预测：在 OccBench 上超越 OccWorld（MIOU 75.20%） - 运动规划：动作离散化实现可解释决策 - 视觉问答：融合场景理解与推理能力 该模型通过统一多模态表示和生成式预测，为自动驾驶提供可模拟环境演化的世界模型基础。 --- *注：总结基于论文标题、摘要、方法架构（图2）、核心贡献章节（Section 3）及实验结果（表1）提炼，保留关键技术术语（如 "场景分词器"、"next token/scene预测"）以确保专业性*</details> |
| 2024-08-19 | CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving | http://arxiv.org/abs/2408.10845v2 | <details><summary>展开</summary>这篇论文提出了CoVLA（Comprehensive Vision-Language-Action）数据集，用于自动驾驶研究，主要贡献如下： 1. **数据集创新** - 构建了大规模多模态数据集CoVLA-Dataset，包含10,000个真实驾驶场景视频（总计80+小时），涵盖复杂城市道路、高速公路等多种环境。 - 提供**三模态标注**：视觉数据（前视摄像头视频）、语言数据（帧级场景描述）、动作数据（未来3秒轨迹坐标）。 2. **自动化标注方法** - **轨迹标注**：通过GNSS/IMU传感器融合与卡尔曼滤波生成高精度未来轨迹。 - **语言标注**：结合规则模板与视频语言模型（VideoLLaMA 2）生成场景描述，采用幻觉抑制技术提升准确性。 - **物体标注**：利用深度学习模型自动检测交通灯状态与前车信息。 3. **模型开发** - 基于数据集提出CoVLA-Agent模型，整合CLIP视觉编码器与Llama-2语言模型，实现端到端轨迹预测和场景描述生成。 - 模型在多样化场景（如弯道、路口、交通管制）中展现一致性输出能力，轨迹预测误差（ADE=0.955，FDE=2.239）。 4. **技术优势** - 突破现有数据集规模限制（6百万帧数据），通过加权采样保证场景多样性。 - 验证语言-动作关联性：使用真实描述时轨迹预测精度显著提升（ADE↓17.8%）。 5. **开源贡献** 数据集已公开供学术研究使用，为可解释自动驾驶系统提供新基准。 该研究解决了自动驾驶中多模态数据缺失的瓶颈，推动视觉-语言-动作模型在复杂决策任务中的应用。</details> |
| 2024-07-25 | Unified Lexical Representation for Interpretable Visual-Language Alignment | http://arxiv.org/abs/2407.17827v2 | <details><summary>展开</summary>该论文提出LexVLA框架，旨在解决视觉-语言对齐（VLA）中的可解释性问题。核心创新点包括： 1. **统一词汇表示** 通过共享词汇表（17,149个token）但独立模态码本，为图像和文本建立可解释的稀疏向量表示。文本码本冻结预训练的Llama 2嵌入，视觉码本则进行微调以适应图像特征。 2. **模态专用编码器** - **视觉**：采用DINOv2提取局部特征，通过适配器（含自注意力层和MLP）映射到词汇空间 - **文本**：利用Llama 2的上下文预测能力，通过提示工程（"The focus lies on important words..."）激活词汇预测任务 3. **过激活惩罚机制** 提出新正则化项抑制无关词汇的频繁激活，相比传统稀疏约束更有效解决语义漂移问题。 4. **轻量训练策略** 仅需单阶段微调（CC-12M数据集），冻结主干网络参数，通过对比学习目标对齐跨模态表示。 5. **性能优势** 在零样本跨模态检索任务中，使用更少训练数据（CC-12M）超越基于更大数据集（YFCC15M/1.1B）的基线模型，同时通过PatchDis指标验证了局部区域对齐的可解释性。 代码已开源：https://github.com/Clementine24/LexVLA</details> |
| 2024-07-11 | Robotic Control via Embodied Chain-of-Thought Reasoning | http://arxiv.org/abs/2407.08693v3 | <details><summary>展开</summary>论文提出了一种名为“具身思维链推理（ECoT）”的新方法，用于提升视觉-语言-动作模型（VLA）在机器人控制中的性能。核心要点如下： 1. **问题背景**：现有VLA模型直接从感知映射到动作，缺乏推理过程，导致在复杂场景中泛化能力不足（如新物体、新指令或新视角）。传统语言模型的思维链（CoT）方法因缺乏物理环境感知而效果有限。 2. **解决方案（ECoT）**： - **多步推理设计**：在预测动作前，模型需生成包含语义和物理感知的推理链： - **语义层面**：任务分解（TASK）、计划（PLAN）、子任务（SUBTASK）、动作指令（MOVE）。 - **具身层面**：机器人夹爪位置（GRIPPER）、物体检测框（OBJECTS），强制模型关注视觉细节。 - **自动化数据生成**：利用预训练模型（Prismatic VLM生成场景描述、Grounding DINO检测物体框、OWLv2/SAM定位夹爪）构建大规模合成训练数据，无需人工标注。 3. **关键结果**： - **性能提升**：在OpenVLA模型上集成ECoT后，跨任务绝对成功率提升28%（例如"将蘑菇放入锅"任务成功率从71%→100%），超越RT-2-X等基准模型。 - **泛化能力**：在物体/空间关系/指令的OOD任务中表现优异（如"将可食用物体放入碗"成功率88%）。 - **可解释性与交互**：推理链帮助诊断失败原因（如物体识别错误），并支持人类用自然语言实时修正策略（干预后成功率提升48%）。 - **高效推理**：通过异步执行或固定高层推理（如每5步更新一次计划），在保持性能的同时提升推理速度40%。 4. **应用扩展**： - 微调预训练VLA模型可快速迁移ECoT能力到其他机器人平台。 - 冻结物体框预测等优化策略平衡了速度与可解释性。 5. **局限**：推理链结构固定，未动态适配任务；实时控制频率仍有提升空间。 **总结**：ECoT通过融合语义规划和物理感知的推理机制，显著提升机器人策略的泛化能力和可解释性，为VLA模型提供了一种"先思考再行动"的通用框架。</details> |
| 2024-07-10 | Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs | http://arxiv.org/abs/2407.07775v2 | <details><summary>展开</summary>本文提出了一种多模态指令导航新范式MINT，其核心贡献如下： 1. **任务创新**：提出多模态演示游览导航任务（MINT），机器人通过预录的环境游览视频理解多模态用户指令（文本+图像），实现复杂导航（如“手持塑料箱时该放回哪里？”）。 2. **方法设计**： - **分层架构**：结合长上下文视觉语言模型（Gemini 1.5 Pro）与拓扑图。 - 高层策略：VLM解析游览视频和用户指令，定位目标帧 - 底层策略：基于离线构建的拓扑图生成机器人动作 - **拓扑图构建**：通过运动恢复结构（COLMAP）从游览视频生成环境拓扑图，支持鲁棒路径规划 3. **性能验证**： - 在836m²真实办公环境中达到86%端到端成功率（复杂推理任务提升26%） - 多模态指令任务成功率90%，显著优于CLIP检索（20%）和纯文本方法（30%） - 支持智能手机录制游览视频，家庭环境实验成功率100% 4. **关键优势**： - 长上下文VLM解决环境理解瓶颈（1FPS全视频输入） - 拓扑图桥接VLM推理与机器人动作执行，避免零样本动作生成失败（对比实验成功率0%） 该方法显著提升了人机交互的自然性，代码与视频见：youtu.be/-Tof__Q8_5s。</details> |
| 2024-06-28 | LLaRA: Supercharging Robot Learning Data for Vision-Language Policy | http://arxiv.org/abs/2406.20095v3 | <details><summary>展开</summary>本文提出LLaRA框架，通过创新数据生成方法提升视觉语言模型（VLM）在机器人策略学习中的性能。核心要点如下： 1. **指令调优数据生成** - 将行为克隆（BC）数据集转化为对话式指令数据（inBC），将机器人动作与图像像素坐标对齐，用自然语言描述动作（如抓取位置坐标和旋转角度）。 - 提出𝒟-inBC变体：通过目标检测将参考图像转化为文本描述，增强多模态任务理解。 2. **自监督辅助数据集** 从相同轨迹生成6类自监督辅助数据（无需额外标注）： - 目标定位与检测：增强空间感知能力 - 动作与未来预测：提升动态理解 - 时空关系推理：强化物体间交互理解 3. **实验验证** - **仿真任务（VIMA-Bench）**：在80k数据上训练时，LLaRA超过VIMA基准模型（平均成功率90.0% vs 80.7%），且仅需12%的数据量。 - **真实机器人任务**：在零样本场景下，LLaRA在移动/旋转/放置任务中平均成功率36.6%，优于GPT-4o（31.6%）和RT-2风格基线（0%）。 - 关键优势：在有限数据下表现优异，辅助数据集对小规模数据提升显著（如VIMA-0.8k数据上性能提升15-20%）。 4. **框架贡献** - 高效利用预训练VLM知识，通过指令调优实现机器人策略快速迁移 - 开源代码、数据集与模型：https://github.com/LostXine/LLaRA 该方法通过数据重构和自监督增强，解决了VLM在机器人控制中数据效率低的问题，为视觉-语言-动作（VLA）模型提供了新范式。</details> |
| 2024-06-27 | OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents | http://arxiv.org/abs/2407.00114v2 | <details><summary>展开</summary>本文提出OmniJARVIS，一种通过统一视觉-语言-动作标记化实现开放世界指令跟随的智能体模型。核心创新点包括： 1. **统一标记化框架** - 提出自监督行为编码器，将行为轨迹（动作序列）离散化为语义丰富的行为标记 - 扩充多模态语言模型词汇表，实现视觉（观察）、语言（指令/记忆/思考）和动作的统一序列建模 2. **关键技术** - **行为标记器**：基于改进的有限标量量化（FSQ）将128步行为轨迹编码为离散标记 - **策略解码器**：将行为标记解码为具体控制命令 - **多模态交互数据**：整合任务指令、记忆、观察、思维链和行为轨迹形成统一序列 3. **模型优势** - 支持思维链推理、任务规划、问答和行为生成 - 解决开放世界任务中文本指令描述不精确和长时程控制效率低的问题 - 行为标记压缩动作空间，提升决策效率 4. **实验验证** - 在Minecraft开放世界中测试原子任务、程序化任务和开放式任务 - 结果显示在复杂任务和长时程任务中优于传统文本目标传递或直接控制方法 - 分析验证了数据构建、标记化设计和模型扩展性的有效性 5. **资源承诺** 作者将开源数据集、模型和代码（项目页：https://craftjarvis.org/OmniJARVIS/）</details> |
| 2024-06-21 | Learning Efficient and Robust Language-conditioned Manipulation using Textual-Visual Relevancy and Equivariant Language Mapping | http://arxiv.org/abs/2406.15677v2 | <details><summary>展开</summary>本文提出了一种高效且鲁棒的语言条件操作框架GEM，核心创新点如下： 1. **文本-视觉相关性映射** - 利用预训练CLIP模型提取像素级语义相关性图，实现开放词汇的物体识别 - 创新性结合视觉数据库检索机制，通过演示数据中的图像块查询增强语义鲁棒性 - 多视图融合策略将2D相关性映射转换为3D点云投影，提升空间感知能力 2. **语言等变映射技术** - 提出语言可转向卷积核：将语言指令映射为动态卷积核，通过旋转生成等变核组 - 实现SE(2)空间等变性：当目标物体发生旋转/平移时，操作动作自动保持几何一致性 - 离散化SO(2)空间（72个旋转角度），确保模型对物体姿态变化的泛化能力 3. **高效学习框架** - 仅需10-20%的演示数据：相比CLIPort节省10倍数据量，相比VIMA仅需0.1%数据 - 双阶段操作架构：语言条件抓取模块实现等变操作，放置模块采用图像裁剪条件卷积 - 开环动作基元：通过空间动作图表示动作，降低策略学习复杂度 4. **实验验证优势** - 仿真/实物实验表明：在未见物体姿态上比OpenVLA提升53%鲁棒性 - 成功案例：剪刀握柄抓取、字母块精准插入等精细操作 - 支持多任务：开抽屉、物体整理等复杂指令的泛化执行 GEM通过融合视觉语言模型的零样本能力与几何等变特性，在保持数据效率的同时显著提升了对物体姿态和语言指令的泛化能力，为机器人自然语言交互提供了新范式。</details> |
| 2024-06-13 | OpenVLA: An Open-Source Vision-Language-Action Model | http://arxiv.org/abs/2406.09246v3 | <details><summary>展开</summary>本文介绍了OpenVLA，一个开源的视觉-语言-动作（VLA）模型，其核心要点如下： 1. **模型架构与训练** - 基于7B参数的Llama-2语言模型，融合DINOv2（空间特征）和SigLIP（语义特征）的双视觉编码器。 - 在Open X-Embodiment数据集（97万条机器人演示）上训练，覆盖多机器人平台、多样化任务和场景。 2. **性能优势** - 在29项跨机器人平台任务中，以7B参数超越55B参数的闭源模型RT-2-X（绝对成功率提升16.5%）。 - 在语言驱动的多物体场景中表现出色，比扩散策略（Diffusion Policy）高20.4%成功率。 3. **高效微调与部署** - 支持低秩自适应（LoRA）微调：仅需消费级GPU（如RTX 4090），训练参数量降至1.4%，性能与全参数微调相当。 - 量化推理：4-bit量化后模型内存降至7GB，推理速度达6Hz，性能无损。 4. **开源贡献** - 公开模型权重、代码库（支持大规模VLA训练）、微调示例和远程推理服务。 - 提供模块化PyTorch代码，集成FlashAttention、FSDP等技术。 5. **局限性** - 当前仅支持单图像输入，未扩展至多模态传感器或历史观测。 - 高频控制场景（如50Hz）的推理速度需优化。 OpenVLA为机器人泛化控制提供了高性能开源基础，并通过高效微调和部署降低了使用门槛。</details> |
| 2024-06-06 | RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation | http://arxiv.org/abs/2406.04339v2 | <details><summary>展开</summary>本文提出RoboMamba，一种高效的视觉-语言-动作（VLA）模型，用于机器人推理与操作。主要贡献如下： 1. **高效架构设计** 整合CLIP视觉编码器与线性复杂度的Mamba语言模型，通过两阶段训练赋予模型： - **通用视觉常识**（对齐预训练：冻结参数，对齐视觉-语言嵌入） - **机器人相关推理能力**（指令协同训练：混合通用指令与RoboVQA机器人数据集，微调投影层和Mamba） 2. **极简动作微调策略** - 引入仅占模型参数量0.1%（约7MB）的轻量策略头（MLP）预测机械臂末端位姿（位置+旋转） - 实验表明：当模型具备充分推理能力后，仅需单卡A100 GPU训练数十分钟即可获得操作技能 - 推理速度达现有VLA模型的3倍（无需量化/加速技术） 3. **性能优势** - **推理能力**：在3.2B参数量下，RoboMamba在RoboVQA基准达到42.8 BLEU-4，通用MMBench等基准表现竞争性 - **操作性能**： - SAPIEN仿真任务中取得SOTA成功率 - 真实世界实验可生成长时程任务规划并预测原子动作位姿 - **效率**：模型参数量与计算成本显著低于传统Transformer-based VLA模型 项目页面：https://sites.google.com/view/robomamba-web</details> |
| 2024-05-31 | Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations | http://arxiv.org/abs/2406.00121v1 | <details><summary>展开</summary>本文提出“图像编辑推荐”（IER）任务，旨在解决用户因初始编辑意图模糊导致的创意鸿沟问题。核心贡献包括： 1. **任务定义**：针对用户仅提供简单编辑提示（如主题/风格）的场景，要求系统自动生成多样化的具体编辑指令（如“给狗添加钻石项链”），弥合创意构思与可执行操作间的差距。 2. **创新数据集**：基于Chain-of-Thought（CoT）框架，结合GPT-4与人工筛选构建16,000条高质量{图像, 编辑提示, 编辑指令, 编辑区域}四元组数据集，涵盖全局/局部编辑场景。 3. **Creativity-VLA模型**： - 以LLaVA为基座，引入**定位令牌**（⟨EDIT⟩）机制，可同时输出文本指令与编辑区域坐标（通过跨注意力机制解码），支持全局（如风格迁移）和局部（如饰品添加）编辑。 - 训练时融合文本生成损失（CE）与定位损失（L1+GIoU），冻结视觉编码器，微调投影层与3层Transformer定位解码器。 4. **验证效果**： - 在用户偏好评估中，相比MagicBrush/InstructDiffusion基线，在提示相关性、视觉质量、多样性等指标显著领先（如多样性得分1.07 vs 2.43）。 - CLIP分数验证生成指令与图像/用户意图的高对齐性，消融实验证实定位模块的有效性。 该方法通过自动化创意构思过程，降低用户专业门槛，提升图像编辑效率。代码与数据集已开源。</details> |
| 2024-05-27 | A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation | http://arxiv.org/abs/2405.17418v2 | <details><summary>展开</summary>这篇论文提出了一种自校正视觉-语言-动作模型（SC-VLA），用于机器人操作系统的快速与慢速协同控制。核心要点如下： 1. **双系统框架设计** - **快速系统**：基于多模态大语言模型（MLLM），通过参数高效微调实现端到端的末端执行器位姿预测（SE(3)），保留模型的推理能力 - **慢速系统**：通过链式思维（CoT）训练策略实现故障检测与校正，利用终态图像和机器人状态识别位姿错误类型（位置/旋转/混合） 2. **创新校正机制** - 自适应专家反馈：根据错误类型动态调用专家模块（Where2Act位置专家/Anygrasp旋转专家/GPT-4V推理专家） - 迭代校正流程：识别错误→获取专家提示→生成校正动作，模拟人类反思过程 - 连续策略学习（CPL）：将成功校正样本通过指数移动平均技术注入适配器，提升快速系统稳定性 3. **关键实验结果** - **仿真测试**（SAPIEN）： - 相比SOTA方法（ManipLLM），在已见任务成功率从66%→87%，未见任务从30%→68% - 慢速系统使校正准确率提升21-30%，CPL进一步减少45%专家干预频率 - **实物测试**： - 通过少量样本微调实现跨域迁移 - 在拔插充电器等任务上达到90%成功率（表2） 4. **技术优势** - 首次在单一VLA模型中集成位姿预测与底层动作校正能力 - 校正过程效率提升40%（图5b），CPL学习使系统适应性提升39% - 支持开放环境中的连续学习，减少对外部专家的依赖 该方法在模拟和真实场景中均显著提升了操作鲁棒性，为应对复杂任务中的不确定性提供了新范式。</details> |
| 2024-05-23 | A Survey on Vision-Language-Action Models for Embodied AI | http://arxiv.org/abs/2405.14093v5 | <details><summary>展开</summary>本文综述了具身人工智能中的视觉-语言-动作模型（VLA），主要内容如下： 1. **VLA模型定义与架构** VLA是一种多模态模型，通过整合视觉输入（环境状态）、语言指令和动作输出，控制具身代理完成物理世界任务。核心架构包括： - **视觉编码器**：提取环境特征（如CLIP、DINOv2） - **语言编码器**：处理指令（如LLM） - **动作解码器**：生成低维动作（如机器人关节控制） 2. **研究分类** - **组件级研究**：优化预训练视觉表示（PVR）、动力学学习（前向/逆向预测）、世界模型（物理状态预测）和推理能力 - **低级控制策略**：直接生成动作（如CLIPort的SE(2)动作预测，RT系列的Transformer架构） - **高级任务规划器**：分解长视野任务为子任务序列（如模块化语言/代码规划器） 3. **关键资源** - **数据集**：涵盖真实机器人操作（CALVIN）、模拟环境（Meta-World）、人体演示及自动采集数据 - **基准测试**：任务规划（VIMA-Bench）、具身问答（Habitat）等评估体系 4. **挑战与方向** - 安全性保障、数据稀缺性、多模态融合 - 长视野任务框架、实时响应优化 - 伦理问题及多智能体协同 5. **贡献** 首次系统化VLA模型分类，提供开源项目资源（GitHub链接），并指出基础模型泛化、3D视觉融合等未来方向。</details> |
| 2024-05-09 | Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations | http://arxiv.org/abs/2405.06039v2 | <details><summary>展开</summary>这篇论文提出了一种名为Bi-VLA（视觉-语言-动作）的新型双手机器人操作系统，核心贡献和要点如下： 1. **系统架构** - 提出三模块集成框架：视觉模块（Qwen-VL模型）负责场景理解和物体检测（成功率96.06%）；语言模块（Starling-LM-7B）将人类指令转化为语义计划和可执行代码（代码生成成功率100%）；动作模块通过预设API控制双机械臂协同操作。 - 创新性地将语言指令、视觉感知与双手机器人动作生成无缝衔接，实现端到端任务执行。 2. **核心功能** - 支持复杂家庭任务（如按需制作沙拉）：通过视觉问答(VQA)确认食材可用性，语言模块生成动作序列（抓取、切割、投放等），双机械臂分工协作（一臂操作工具，另一臂处理食材）。 - 开发专用API函数库（如`grasp()`, `cut()`, `move_to_object()`），实现精细化动作控制。 3. **技术突破** - 提出像素坐标到3D空间的映射算法（结合布朗-康拉迪畸变校正模型），解决物体定位问题。 - 无需额外训练数据，通过语义规划和代码生成实现动态任务适应。 4. **实验验证** - 在沙拉制备任务中达成83.4%的总体成功率。 - 验证系统对多样化食谱和用户偏好的适应性，证明其在家庭服务场景的应用潜力。 该系统为双手机器人操作提供了可扩展框架，显著提升了人机协作的效率和自然性。</details> |
