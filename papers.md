| 日期 | 标题 | 链接 | 简要总结 |
| --- | --- | --- | --- |
| 2025-11-14 | Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective | http://arxiv.org/abs/2511.11478 | <details><summary>展开</summary>待生成</details> |
| 2025-11-14 | Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2511.11298 | <details><summary>展开</summary>待生成</details> |
| 2025-11-13 | OmniVGGT: Omni-Modality Driven Visual Geometry Grounded | http://arxiv.org/abs/2511.10560 | <details><summary>展开</summary>## 论文研究单位<br>香港科技大学(HKUST)、南洋理工大学(NTU)、中山大学(SYSU)、新加坡国立大学(NUS)、阿里巴巴集团<br><br>## 论文概述<br>论文提出了OmniVGGT框架，这是一个能够有效利用任意数量辅助几何模态（如深度图、相机内参和外参）进行训练和推理的视觉基础模型。该框架通过轻量级的GeoAdapter模块逐步注入几何信息，并采用随机多模态融合策略，使模型能够在测试时处理任意数量的模态输入。在单目/多视角深度估计、多视角立体视觉和相机姿态估计等多个3D视觉任务上取得了最先进的性能。<br><br>## 论文核心贡献点<br>1. 提出了GeoAdapter，一个轻量级适配器，能够将深度图和相机参数编码到空间基础模型中，使用零初始化卷积逐步注入几何信息而不破坏基础模型的表示空间<br>2. 设计了随机多模态融合策略，在训练时随机采样模态子集，使模型能够在测试时接受任意数量的模态输入，并学习鲁棒的空间表示<br>3. 在多种3D视觉任务上超越了现有方法，即使仅使用RGB输入也取得了最先进的结果<br>4. 成功将OmniVGGT集成到视觉-语言-动作(VLA)模型中，在机器人操作任务上实现了持续的性能提升<br><br>## 论文方法描述<br>1. **GeoAdapter架构**：包含相机适配器和深度适配器两部分。相机适配器对相机内参和外参进行归一化、参数化为特征向量，然后通过专用编码器和零卷积层处理；深度适配器对深度图进行批内归一化，通过卷积层编码为空间标记<br>2. **随机多模态训练**：在训练过程中随机为图像序列分配不同数量的辅助信息（相机参数和深度图），确保模型对各种部分信息场景具有鲁棒性<br>3. **训练目标**：采用多任务损失函数，包括相机损失、深度损失和点图损失，每个都增强梯度项以提高局部几何一致性<br>4. **网络架构**：基于VGGT框架，使用24个交替注意力块处理输入，最终通过三个预测头输出深度图、相机姿态和3D点图<br><br>## 论文使用数据集和训练资源<br>使用19个公共数据集进行训练，包括ARKitScenes、BlendedMVS、DL3DV、Dynamic Replica、HyperSim、Kubric、MapFree、MegaDepth、Matterport 3D、MVS-Synth、ScanNet、ScanNet++、Spring、TartanAir、UASOL、Unreal 4K、Virtual KITTI、Waymo和WildRGBD。这些数据集涵盖了合成和真实内容、室内外环境以及静态和动态场景。训练使用32个NVIDIA A100 GPU，耗时10天，采用梯度检查点优化内存使用。<br><br>## 论文使用的评估环境和评估指标<br>在多个标准数据集上进行评估：<br>- **深度估计**：使用绝对相对误差和δ<1.25指标<br>- **相机姿态估计**：使用相对旋转精度(RRA)、相对平移精度(RTA)和AUC(精度阈值曲线下面积)<br>- **3D重建**：使用精度、完整性和法线一致性指标<br>- **机器人操作任务**：在CALVIN数据集上评估平均长度等指标<br>模型在Sintel、Bonn、NYU-v2、ScanNet、ETH3D、DTU、Tanks and Temples、7-Scenes、Co3Dv2和RealEstate10K等数据集上进行了广泛测试。</details> |
| 2025-11-13 | SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation | http://arxiv.org/abs/2511.10518 | <details><summary>展开</summary>待生成</details> |
| 2025-11-13 | Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks | http://arxiv.org/abs/2511.10008 | <details><summary>展开</summary>## 论文研究单位<br>浙江大学<br><br>## 论文概述<br>本文首次系统性地研究了视觉-语言-动作（VLA）模型在面对物理传感器攻击时的脆弱性。鉴于VLA模型严重依赖摄像头和麦克风等传感器输入，作者旨在量化物理传感器攻击（如激光、电磁、超声波）对模型性能的影响，并探索相应的防御机制。论文提出了一个名为“真实-模拟-真实”的框架，用于自动模拟物理世界中的传感器攻击向量，并在模拟和真实机器人系统上进行验证。通过对多种VLA模型和任务进行大规模评估，研究揭示了这些模型存在的显著漏洞。最后，作者提出并验证了一种基于对抗训练的防御方法，以增强VLA模型对此类物理扰动的鲁棒性。<br><br>## 论文核心贡献点<br>- 验证了VLA模型在物理传感器攻击面前的脆弱性，并证实攻击可导致其在真实世界中行为异常。<br>- 提出了一个“真实-模拟-真实”框架，以系统且现实的方式评估VLA模型对抗物理传感器攻击的鲁棒性，有效连接了纯数字攻击模拟与资源密集的物理实验之间的差距。<br>- 进行了大规模的鲁棒性评估，涵盖了多种VLA模型和任务。至关重要的是，通过在真实世界系统上进行的针对性物理实验，验证了模拟环境的发现，从而证实了该框架的有效性。<br>- 提出并验证了一种基于对抗训练的防御策略，用于抵御这些物理攻击，同时保持VLA模型在干净数据集上的性能。<br><br>## 论文方法描述<br>论文方法的核心是“真实-模拟-真实”框架。<br>1. **攻击选择与模拟**：首先，从顶级安全会议中筛选出八种代表性的物理传感器攻击，其中六种针对摄像头（激光致盲、光投影、激光彩色条纹、电磁彩色条纹、电磁截断、超声波模糊），两种针对麦克风（语音拒绝服务、语音欺骗）。然后，基于这些攻击的物理原理，在数字领域实现高保真度的模拟，并定义了弱、中、强三个攻击强度等级。<br>2. **麦克风攻击设计**：<br> - **语音拒绝服务**：通过注入高强度超声波信号使麦克风传感器饱和，从而阻止有效指令的接收。<br> - **语音欺骗**：通过调制的激光或超声波信号向麦克风注入恶意的语音指令。<br>3. **摄像头攻击设计**：<br> - **激光致盲**：使用高功率激光照射摄像头，使其光电传感器饱和，无法捕捉环境光线。<br> - **光投影**：使用投影仪向环境或镜头投射虚假图像，干扰视觉感知。<br> - **激光彩色条纹**：利用摄像头CMOS传感器的卷帘快门效应，通过开关调制激光注入彩色条纹。<br> - **电磁彩色条纹与截断**：通过向摄像头图像传输接口（如MIPI CSI-2）注入电磁干扰，导致颜色解码错误或图像截断。<br> - **超声波模糊**：向配备防抖模块的摄像头注入超声波，引起其惯性测量单元（IMU）共振，误导防抖算法进行不必要的补偿，导致图像模糊。<br>4. **防御方法**：提出一种基于对抗训练的防御策略。该策略首先在干净数据集上训练VLA模型，然后在训练数据中混合一定比例（如30%）的经过攻击模拟的数据集，并对模型进行微调，使其对由传感器攻击引起的分布外扰动具有鲁棒性。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：<br> - 模拟环境：使用Libero模拟器及其配套的数据集，包括Libero-Spatial、Libero-Object、Libero-Goal和Libero-Long。<br> - 真实世界环境：为适应真实世界，通过遥操作收集了一小时的机械臂操作数据，用于微调模型以完成积木抓取与放置任务。<br>- **训练资源**：<br> - 模型运行与评估：NVIDIA 4090 GPU。<br> - 模型微调：使用Lora技术，在NVIDIA H800 GPU (80GB) 上进行。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 模拟环境：Libero模拟器。<br> - 真实世界环境：一个装备有Franka Emika Panda机械臂的实验平台。该机械臂配备了一个全局摄像头（Intel RealSense D435i）、一个手腕摄像头（Intel RealSense D435i）和一个麦克风。攻击平台包括电磁干扰（EMI）平台、投影平台、激光平台和超声波平台。<br>- **评估指标**：<br> - 任务成功率：定义为成功完成的任务次数与总任务次数的比率。</details> |
| 2025-11-13 | Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation | http://arxiv.org/abs/2511.09958 | <details><summary>展开</summary>待生成</details> |
| 2025-11-12 | MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation | http://arxiv.org/abs/2511.09516 | <details><summary>展开</summary>待生成</details> |
| 2025-11-12 | WMPO: World Model-based Policy Optimization for Vision-Language-Action Models | http://arxiv.org/abs/2511.09515 | <details><summary>展开</summary>### 论文研究单位<br>香港科技大学、字节跳动<br><br>### 论文概述<br>本文提出WMPO（World Model-based Policy Optimization），一个基于生成式世界模型的视觉-语言-动作（VLA）模型强化学习框架。该方法通过在像素级世界模型中进行策略优化，完全替代了昂贵且低效的真实环境交互，显著提升了VLA模型的样本效率和泛化能力，同时展现出自我修正等涌现行为。<br><br>### 论文核心贡献点<br>1. **像素级世界模型**：提出与VLA预训练特征对齐的像素空间世界模型，避免潜在空间不匹配问题。<br>2. **策略行为对齐**：通过微调世界模型使其适应策略行为分布，实现对失败场景的逼真模拟。<br>3. **高效在线强化学习**：支持GRPO算法在"想象"轨迹中进行策略优化，克服物理交互瓶颈。<br>4. **涌现能力验证**：实验证明方法能产生自我修正行为、高效任务执行及强泛化能力。<br><br>### 论文方法描述<br>1. **世界模型构建**：基于OpenSora的视频扩散模型，替换3D VAE为SDXL的2D VAE以保留运动细节，引入带噪声帧条件增强长时程生成稳定性。<br>2. **策略行为对齐**：在Open X-Embodiment数据集预训练后，用策略自身采集的轨迹微调世界模型。<br>3. **奖励模型设计**：采用VideoMAE编码器训练二分类模型，通过滑动窗口评估轨迹成功概率。<br>4. **GRPO优化**：每组初始状态生成G条想象轨迹，通过动态采样确保批次多样性，利用归一化优势函数更新策略。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：Open X-Embodiment（预训练）、Mimicgen仿真任务（Coffee、StackThree等）、Cobot Mobile ALOHA真实轨迹（200条专家演示+128条策略采集）。<br>- **训练资源**：8块H100 GPU进行策略微调，32块H100 GPU训练世界模型和优化策略。<br><br>### 论文使用的评估环境和评估指标<br>- **仿真环境**：Mimicgen基准的4个操作任务，评估128个随机初始状态的平均成功率。<br>- **真实环境**：Cobot Mobile ALOHA平台执行"插入方块"任务（间隙5mm），30次试验平均成功率。<br>- **泛化测试**：位置扰动（棒随机位置）、背景扰动（灰色背景）、纹理扰动（木质基座）下的成功率。<br>- **终身学习**：迭代采集128条轨迹更新策略，对比DPO基线的性能提升。<br>- **指标**：任务成功率（%）、轨迹长度（效率）、F1分数（奖励模型可靠性）。</details> |
| 2025-11-12 | MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror | http://arxiv.org/abs/2511.08865 | <details><summary>展开</summary>论文研究单位<br>ZTE Terminators Group<br><br>论文概述<br>本文提出了一个基于PICO的机器人远程操作框架MirrorLimb，旨在实现低成本、实时的手部运动和姿态数据采集。该框架与RealMirror生态系统原生兼容，能够在Isaac仿真环境中稳定、精确地记录机器人轨迹，以促进视觉-语言-动作（VLA）数据集的构建。此外，该系统支持对配备灵巧手和夹爪等多种末端执行器的机器人进行实时遥操作，旨在降低上肢机器人操作研究的技术门槛。<br><br>论文核心贡献点<br>与RealMirror平台原生兼容，结合其运动学/动力学优化能力，可在IsaacSim仿真环境中实现高精度、稳定的机器人远程遥操作。<br>集成了基于WebXR/OpenXR的通信框架和端到端遥操作软件系统，为手柄/手势输入的原始数据提供标准化接口，便于适配不同操作平台和机器人末端执行器。<br>支持PICO 3和4等XR设备，相比专业动捕系统或Apple Vision Pro，硬件成本显著降低，大大降低了实现精细化机器人操作的门槛。<br><br>论文方法描述<br>该方法设计了一个双通道采集栈，实时从PICO XR设备获取手柄指令和精细的手部姿态。手柄路径使用WebXR在浏览器中实现，数据流传输至安全的Node.js服务器；手势路径则通过OpenXR（PICO的Unity SDK）实现，以60Hz的固定频率通过UDP传输关节姿态数据。两路数据都会经过坐标系统转换，以匹配RealMirror/IsaacSim的坐标约定。为了解决遥操作中的抖动和突变问题，系统在逆运动学求解阶段设计了运动学优化规则，通过分析连续帧间末端位置和关节角度的变化，并应用阈值过滤，从而抑制控制不稳定性，确保遥操作的平滑与精确。<br><br>论文使用数据集和训练资源<br>论文本身不使用特定公开数据集进行模型训练，而是提供一个数据采集框架，用于生成和记录机器人在Isaac仿真环境中的遥操作轨迹数据，以构建VLA数据集。使用的硬件资源主要包括PICO 3和PICO 4等XR设备以及用于数据传输和处理的Node.js服务器。<br><br>论文使用的评估环境和评估指标<br>评估环境是NVIDIA的IsaacSim/IsaacLab仿真平台。评估指标侧重于定性性能，包括：成本效益、实时性能、数据采集的稳定性、远程操作的精确度、以及通过运动学优化对末端执行器抖动和突变的抑制效果。</details> |
| 2025-11-11 | SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control | http://arxiv.org/abs/2511.07820 | <details><summary>展开</summary>待生成</details> |
| 2025-11-10 | How Do VLAs Effectively Inherit from VLMs? | http://arxiv.org/abs/2511.06619 | <details><summary>展开</summary>### 论文研究单位<br>Microsoft Research<br><br>### 论文概述<br>该论文探讨了视觉-语言-行动模型如何有效地从预训练的视觉-语言模型中继承知识。为了解决这一核心问题并应对训练过程中可能出现的灾难性遗忘，研究者们引入了一个名为GrinningFace的诊断基准。该基准是一个表情符号桌面操作任务，要求机器人根据语言指令将物体放置在对应的印刷表情符号上。由于表情符号在VLM的预训练数据中广泛存在，但在标准机器人数据集中几乎不存在，该任务的成功完成可以清晰地表明VLM的先验知识被有效迁移到了具身控制中。论文通过在模拟和真实机器人上进行系统性实验，比较了多种有前景的知识迁移技术，揭示了保留VLM先验知识对于VLA泛化能力的关键重要性，并为未来开发真正可泛化的具身智能系统提供了指导。<br><br>### 论文核心贡献点<br>- 提出了一个最小化、可复现的基准，用于解构视觉-语义先验知识与运动技能，可作为诊断工具来评估VLM如何被有效地适配到VLA。<br>- 在一个严格控制实验的框架内，对不同VLA预训练和微调技术进行了系统的比较分析。<br>- 提供了基于实证的洞察，为未来开发可泛化具身智能体的研究方向提供指导，包括：VLM初始化、VLA预训练和VLA微调分别扮演着不同且互补的角色；全参数微调在狭窄任务上表现良好，但会导致灾难性遗忘；联合训练和预测潜在动作是更有前景的研究方向；在更多样化的数据集上进行VLA预训练可以提升性能。<br><br>### 论文方法描述<br>论文设计并比较了多种旨在改善VLA继承VLM先验知识的训练技术。核心方法是构建并使用GrinningFace基准进行评估。被比较的技术包括：<br>- 基线方法：使用π0风格的模型进行全参数微调。<br>- 参数高效微调：采用LoRA或仅微调动作专家模块。<br>- 冻结VLM主干：在微调过程中冻结VLM的权重。<br>- 联合训练：在VLA训练中同时加入视觉-语言任务，如在桌面场景中识别表情符号。<br>- 离散化目标：训练VLA预测离散化的动作目标。<br>- 潜在动作目标：训练VLA在预测机器人动作的同时，也预测潜在动作。<br>- 多样化数据预训练：研究不同数据集（如OXE magic soup, bridge-v2）对VLA性能的影响。<br><br>### 论文使用数据集和训练资源<br>- 数据集：<br> - GrinningFace基准：包含100个用于训练的表情符号和100个用于验证的表情符号，收集了500条轨迹用于微调。<br> - VLA预训练数据集：主要使用Open X-Embodiment (OXE) magic-soup混合数据集（包含913k条轨迹）。实验中还使用了bridge-v2数据集以及排除了bridge-v2的OXE数据集，以进行消融研究。<br>- 训练资源：<br> - 计算资源：使用8块Nvidia A100 GPU进行VLA的预训练和微调，使用单块Nvidia A100 GPU进行评估。<br> - 模型基础：代码库基于π0的开源实现，使用PaliGemma作为VLM主干，SigLIP作为视觉编码器。<br> - 训练配置：VLA预训练80k步，微调30k步，批大小为1024。<br><br>### 论文使用的评估环境和评估指标<br>- 评估环境：<br> - 模拟环境：ManiSkill3。<br> - 真实机器人：搭载Inspire Robots夹爪的Realman RM75机械臂。真实机器人上的任务被简化为“触摸”指定表情符号。<br>- 评估指标：<br> - 核心公式：整体成功率 = 执行成功率 × 识别成功率。<br> - 执行成功率：机器人成功拿起方块并放置到任意一个表情符号卡片上。<br> - 识别成功率：机器人在三个候选卡片中选择了正确的那个。<br> - 评估协议：<br> - ID：评估模型在训练时见过的表情符号组合上的表现。<br> - Train：评估模型在由训练集表情符号构成的新组合上的表现。<br> - Val：评估模型在验证集表情符号上的表现，用于测试泛化能力。</details> |
| 2025-11-09 | ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval | http://arxiv.org/abs/2511.06202 | <details><summary>展开</summary>### 论文研究单位<br>卡内基梅隆大学机器人研究所，美国匹兹堡<br><br>### 论文概述<br>论文提出了一种名为ExpReS-VLA的方法，旨在解决预训练的视觉-语言-动作（VLA）模型在特定部署环境中进行快速适应时遇到的灾难性遗忘和性能下降问题。该方法通过压缩的经验回放和检索增强生成技术，使模型能够在设备端利用少量演示（12个）快速适应新环境，同时保留原有能力。ExpReS-VLA通过存储视觉编码器的嵌入而非原始图像，实现了97%的存储空间节省，并结合检索到的相似经验和一种新的对比损失函数（THCL）来学习成功与失败的经验。实验表明，该方法在模拟和真实机器人任务上均显著提升了任务成功率。<br><br>### 论文核心贡献点<br>RAG增强的机器人学习：首次将检索机制集成到VLA微调中，加速了适应过程。<br>压缩的经验回放：一种通过冻结视觉编码器实现97%内存减少的技术，在保持语义保真度的同时，实现了实际部署。<br>用于失败利用的THCL：一种新颖的分段损失函数，通过动态选择合适的对比目标来防止重复性错误。<br>严谨的实证评估：在40个模拟任务（5个随机种子）和5个物理操作任务（共150次试验）中进行了系统性消融实验，明确了各组件的贡献。<br><br>### 论文方法描述<br>该方法首先使用OpenVLA冻结的视觉编码器（融合SigLIP和DINOv2）从RGB图像中提取1024维的嵌入向量，以实现高效的内存存储。<br>维护一个双缓冲区内存管理系统，分别存储成功和失败的经验轨迹，并采用FIFO策略和时序加权进行更新。<br>在训练时，系统根据当前观察的嵌入，从缓冲区中检索余弦相似度最高的k个相关经验，并将其与当前数据混合构建训练批次。<br>引入了阈值化混合对比损失（THCL），该损失结合了行为克隆损失和自适应的对比学习损失。当三元组损失低于阈值时使用三元组损失，否则使用InfoNCE损失，从而使模型能从成功和失败中学习。<br>整个在线学习流程采用LoRA进行高效微调，在性能低于阈值时触发，并在单块消费级GPU（如RTX 5090）上快速完成。<br><br>### 论文使用数据集和训练资源<br>数据集：模拟实验使用了LIBERO基准测试，包含四个任务套件（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。真实机器人实验在7-DOF Franka Emika Panda机械臂上进行，包含5个操作任务。<br>训练资源：所有实验均在单个NVIDIA RTX 5090 GPU（32GB内存）上完成，使用BFloat16混合精度。模型微调采用LoRA配置，仅训练98.3M个参数（占总数的1.4%）。适应过程仅需12个演示和31秒。<br><br>### 论文使用的评估环境和评估指标<br>评估环境：在LIBERO模拟基准和真实的Franka Emika Panda物理机器人上进行评估。物理机器人任务包括分布内和分布外（包含未见过的背景和物体）两种测试场景。<br>评估指标：主要评估指标是任务成功率。模拟中，每个任务进行50次滚动测试，重复5个随机种子。物理机器人中，每个任务进行30次分布内试验和10次分布外试验。</details> |
| 2025-11-08 | 10 Open Challenges Steering the Future of Vision-Language-Action Models | http://arxiv.org/abs/2511.05936 | <details><summary>展开</summary>论文研究单位<br>新加坡的研究机构，包括A*STAR (Agency for Science, Technology and Research) 和南洋理工大学。<br><br>论文概述<br>本文是一篇综述性论文，旨在探讨视觉-语言-动作（VLA）模型未来发展所面临的10个开放性挑战。这些挑战涵盖了多模态感知、鲁棒推理、训练数据质量、模型评估、跨机器人动作泛化、资源效率、全身协调、安全保障、智能体框架以及人机协调等多个方面。此外，论文还讨论了应对这些挑战的新兴趋势，包括分层规划、空间理解、通用动作表示、世界动态建模、数据合成和后训练，以期推动VLA模型的发展和应用。<br><br>论文核心贡献点<br>1. 系统性地梳理并提出了VLA模型发展过程中的10个核心开放性挑战。<br>2. 总结和分析了当前应对这些挑战的新兴技术趋势和研究方向。<br>3. 提出了一个高层级的多智能体VLA规划框架（算法1），该框架集成了高层规划器、低层级动作专家、安全护栏和评估器，旨在实现更鲁棒和安全的任务执行。<br><br>论文方法描述<br>论文主要不是提出一个单一的新方法，而是对现有及未来可能的方法论进行总结和展望。<br>1. **VLA基础方法**: 介绍了VLA模型的基本框架，即将视觉观测和语言指令结合以生成机器人动作。区分了两种主流的动作建模方法：离散动作模型（将连续动作量化为离散token，易于与Transformer结合，但可能损失精度）和连续动作模型（直接预测连续动作，如使用扩散模型，精度高但计算成本大）。<br>2. **分层规划**: 建议使用大型语言模型（LLM）或视觉语言模型（VLM）作为高层级规划器，将复杂任务分解为子任务。低层级动作专家则负责根据子任务生成具体的机器人动作序列。在生成动作前加入推理步骤可以提高模型性能和可解释性。<br>3. **空间理解**: 提出应增强VLA模型对3D空间的理解能力，例如通过融合来自深度相机或LiDAR的深度信息，或者利用专门的深度估计模块，以提升模型在物理世界中的操作鲁棒性。<br>4. **通用动作表示**: 探讨了通过学习一个通用的原子动作表示（action codebook）来解决跨机器人动作泛化问题的可能性，使模型能够适应不同机器人的动作空间。<br>5. **世界动态建模**: 强调了世界模型在预测动作结果和进行有效规划中的重要性，介绍了两种主要方法：一种是直接生成未来状态像素的生成模型，另一种是预测未来状态嵌入的嵌入预测模型（如JEPA）。<br>6. **数据合成**: 建议利用视频生成模型大规模生成机器人执行任务的模拟视频数据，并通过世界模型从视频中提取“潜在动作”，以解决机器人数据采集成本高的问题。<br>7. **后训练**: 提议借鉴LLM的成功经验，在预训练之后对VLA模型进行后训练。通过利用世界模型作为奖励模型或评估器，可以对生成的动作序列进行评估，并使用强化学习或偏好优化（如DPO）来微调模型，以提高其任务表现和安全性。<br><br>论文使用数据集和训练资源<br>论文讨论了现有研究中常用的数据集，而非为自身方法提供新数据集。<br>1. **数据集**: 提到了`Open-X-Embodiment`数据集，它是一个包含超过一百万个机器人操作片段的大型统一数据集，涵盖了多种机器人、任务和场景。此外，也提到了`DROID`数据集。论文还提议通过视频生成模型合成训练数据。<br>2. **训练资源**: 论文指出，训练大型VLA模型通常需要大量的计算资源。同时，对于部署在机器人上的模型，需要在模型容量和资源效率之间进行权衡，以适应机器人本体的计算和能源限制。<br><br>论文使用的评估环境和评估指标<br>论文本身未进行新的实验评估，而是对现有VLA模型的评估方法进行了分析和评述。<br>1. **评估环境**: 指出当前评估主要在真实机器人（如WidowX, Franka）和模拟器（如LIBERO, SimplerEnv）中进行。论文强调了现有评估的局限性，如评估环境单一、模拟与现实之间存在差距。<br>2. **评估指标**: 提到任务成功率是常用的评估指标。在讨论后训练时，提到了使用任务完成度、效率和最优性等作为奖励函数的可能标准。`SimplerEval`被提及作为一种通过引入分布偏移（如改变背景、光照）来增强评估鲁棒性的尝试。</details> |
| 2025-11-07 | Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots | http://arxiv.org/abs/2511.05642 | <details><summary>展开</summary># 论文研究单位<br>- Department of Cyber‑Physical Systems, Clark Atlanta University (作者 1、2、3)<br>- Siemens Corporation (作者 4)<br><br># 论文概述<br>本文提出 **LiteVLA**，一种面向 CPU‑受限边缘机器人的轻量级视觉‑语言‑行动（VLA）框架。该系统将紧凑的 SmolVLM 主干、LoRA 参数高效微调、4‑bit NF4 量化以及 ROS 2 完整管线集成在 Raspberry Pi 4/TurtleBot 4 上，实现完全本地化的感知‑推理‑控制闭环。在无需云端资源的前提下，验证了在 CPU 上进行实时（或准实时）多模态推理的可行性，为无 GPS、带宽受限环境下的自治机器人提供思路。<br><br># 论文核心贡献点<br>1. **CPU‑only 本体 VLA**：在 Raspberry Pi 4 上使用 GGUF‑量化的 VLA 策略并通过 ROS 2 实现异步控制，基线延迟约 11.1 s/查询（0.09 Hz）。<br>2. **参数高效适配**：利用 LoRA（rank = 8，α = 8，dropout = 0.1）对 SmolVLM 主干进行任务专用微调，内存/算力需求极低。<br>3. **边缘量化与稳定性**：采用 4‑bit NF4 主干 + FP32 投影头的混合精度，实现约 75 % 内存降低、最高 9× 推理加速，同时保持输出稳定。<br>4. **端到端 ROS 2 管线**：统一的感知‑推理‑控制闭环，将 RGB 帧映射为结构化运动指令（geometry_msgs/Twist），使用 llama‑cpp 运行时。<br>5. **可扩展路线图**：提出六阶段 EDGE‑VLA‑ROADMAP，从地面机器人逐步扩展至无人机、多智能体协同、多模态接地、持续/强化学习及联邦安全。<br><br># 论文方法描述<br>- **系统架构**（Figure 1）<br> - 数据采集节点 → 多模态推理节点（SmolVLM+LoRA） → ROS 2 控制节点。<br>- **数据管道**（Algorithm 1）<br> - 通过遥控采集 15 083 条 RGB‑动作对；时间戳对齐 → 归一化 → 224×224 缩放 → 随机水平翻转增强；训练/验证划分 0.85 : 0.15。<br>- **LoRA 微调**<br> - 在 SmolVLM 主干的 query/key/value/output/gating 层注入 rank = 8 的低秩矩阵，损失函数为预测动作与标注文本的交叉熵；约两轮在 CPU 后端完成。<br>- **量化方案**<br> - 主干使用 NF4（4‑bit）量化，投影层保持 FP32（混合精度），以避免动作输出的不稳定。<br>- **推理与 ROS 2 集成**<br> - llama‑cpp‑python 加载 GGUF 权重；推理输出形如 “forward_0.2_3.0s”，在 ROS 2 中解析为 Twist 消息；采用 Action Chunking 机制实现低层持续控制与高层推理的异步解耦。<br><br># 数据集与训练资源<br>- **数据集**：遥控采集的 15 083 条 RGB‑动作对（包含线性/角速度及时间戳），每条指令在室内多环境下覆盖前、后、左、右、停止等类别，分布基本均衡。实际实验使用约 13 000 条（其中 1 152 条为后退指令，其余约 2 990 条/类）。<br>- **预处理**：统一尺寸 224×224、像素标准化、随机水平翻转；时间戳对齐保证图像‑动作一一对应。<br>- **训练资源**：全部微调在 CPU 后端完成（约两轮），未使用 GPU；模型量化与部署在 Raspberry Pi 4（ARM Cortex‑A72，1.5 GHz，4 GB RAM）上执行。<br><br># 评估环境与评估指标<br>- **硬件平台**：Raspberry Pi 4（4 GB RAM）+ TurtleBot 4 机器人，ROS 2 环境。<br>- **评估指标**<br> - **推理延迟**：每条查询的平均耗时；FP32 SmolVLM‑256 基线 ≈ 11 s，LiteVLA（FP32）≈ 18 min，混合量化 ≈ 2 min（9× 加速），全 NF4 ≈ 1.5 min（输出不稳定）。<br> - **内存占用**：混合量化较 FP32 主干降低约 75 %。<br> - **系统吞吐量**：推理频率约 0.09 Hz；控制层通过 Action Chunking 实现低层持续运动。<br> - **输出稳定性**：全 NF4 量化导致动作输出出现幻觉，混合精度保持稳定。<br>- **对比实验**：将 LiteVLA 与原始 SmolVLM‑256（FP32）以及未量化的 LiteVLA 进行横向比较，量化显著降低时延且维持可接受的控制质量。<br>- **稳健性讨论**：在高温或长时运行情况下出现热降频，导致延迟波动；CPU 竞争（图像采集、预处理、推理）亦产生延迟峰值。<br><br>以上内容概括了论文的研究单位、整体思路、核心贡献、技术路线、实验数据及评测细节。</details> |
| 2025-11-07 | EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation | http://arxiv.org/abs/2511.05397 | <details><summary>展开</summary># 论文研究单位<br><br>匹兹堡大学（University of Pittsburgh）<br><br># 论文概述<br><br>EveryDayVLA是一个面向可负担机器人操作的视觉-语言-动作模型。该研究结合了低成本硬件（300美元6自由度机械臂）和先进的VLA模型，通过协作训练预测离散和连续动作，并引入自适应地平线集成器（AdaHorizon）来提高实时操作的安全性和可靠性。在LIBERO仿真基准测试中达到了最先进的性能，在真实世界测试中在分布内场景领先49%，在分布外场景领先34.9%。<br><br># 论文核心贡献点<br><br>1. **协作训练与自适应地平线控制（AdaHorizon）**：联合训练连续（L1回归）和离散自回归动作头，使用分歧估计模型不确定性，在严格实时约束下动态调整动作范围触发重规划<br><br>2. **低成本集成6自由度机械臂**：300美元设计达到优于10mm重复精度，利用Arduino Uno分线板和PCA9685 PWM驱动器进行12位PWM控制<br><br>3. **自动化数据收集流水线**：简化遥操作收集带语言指令、视频和末端执行器轨迹的数据集，发布超过1200个任务执行以实现跨环境可扩展微调<br><br># 论文方法描述<br><br>基于Prismatic-7B VLM构建，使用SigLIP和DinoV2双部分视觉编码器，Llama 2语言模型作为主干。采用协作训练策略联合预测连续和离散动作块，连续动作通过多层感知器（MLP）动作头输出，离散动作通过256-bin离散化处理并使用softmax获得概率分布。损失函数结合交叉熵损失和L1损失（权重λ=1）平衡优化。<br><br>AdaHorizon算法通过计算连续和离散动作预测之间的平均绝对差异作为不确定性度量，动态调整执行的动作块长度。当预测分歧超过阈值时触发重规划，在实时约束下最小执行长度为4个动作。<br><br># 论文使用数据集和训练资源<br><br>- **数据集**：1200个演示的定制数据集，包含RGB观察序列、对应末端执行器姿态和自然语言指令，涵盖多种桌面环境，包括抓取放置、环境操作和块堆叠任务<br>- **训练资源**：仿真实验在2个A100 GPU上微调10万次迭代，真实世界实验在1个A100 GPU上微调5万次迭代；使用LoRA（rank=32）、batch size=8和4步梯度累积<br><br># 论文使用的评估环境和评估指标<br><br>- **仿真基准**：LIBERO四个任务套件（空间、物体、目标、长期），使用成功率（SR）作为主要评估指标<br>- **真实世界评估**：分布内和分布外场景测试，包括静态和动态干扰物评估；成功率作为主要指标<br>- **推理性能**：在LIBERO上测量推理频率（Hz）和延迟（秒）<br>- **对比基线**：与Diffusion Policy、Octo、DiT Policy、OpenVLA、OpenVLA-OFT、ACT、HybridVLA、COGAct等方法进行比较</details> |
| 2025-11-07 | TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models | http://arxiv.org/abs/2511.05275 | <details><summary>展开</summary>### 论文研究单位<br>延世大学人工智能系（Yonsei University, Department of Artificial Intelligence），微软研究院（Microsoft Research）。<br><br>### 论文概述<br>Vision-Language-Action模型（VLAs）在单臂机器人操控中表现出色，但双臂操控因缺乏大规模公开数据而面临挑战。本研究提出TwinVLA，通过组合两个预训练单臂VLA模型实现数据高效的双臂操控。TwinVLA避免了大量双臂预训练数据的依赖，仅需少量双臂演示即可在真实世界和仿真任务中达到或超越单体模型（如RDT-1B）的性能，并接近状态最先进的π_0模型。<br><br>### 论文核心贡献点<br>- 提出模块化双臂架构：通过复制预训练单臂VLA并结合联合注意力机制，实现两臂协调控制。<br>- 数据高效微调范式：仅使用小量双臂数据（每任务约50演示）进行微调，无需额外双臂预训练。<br>- 性能验证：在真实世界（Anubis机器人）和仿真（RoboTwin 2.0、Tabletop-Sim）任务中，TwinVLA在成功率上优于或匹配RDT-1B和DP方法，接近π_0模型。<br><br>### 论文方法描述<br>TwinVLA基于以下三大核心组件：<br>1. **单臂策略复制**：复制预训练单臂VLA的VLM骨干（仅复制1.3B参数），共享视觉编码器和DiT动作头。左右臂各有一个轻量级本体感觉编码器。<br>2. **联合注意力机制**：共享两个VLM间的自注意力层，实现跨臂信息融合，采用因果联合注意力掩码保持时序性和对称交互。<br>3. **混合专家（MoE）集成**：通过MoE高效处理共享输入（如语言和自我视角图像），减少计算开销。同时，采用任务算术和注意力重加权技术保留预训练知识并加速微调。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 预训练：OXE数据集的0.5M轨迹子集。<br> - 微调：真实世界任务每任务收集50集（绝对末端执行器控制），仿真任务每任务50集（如RoboTwin 2.0和Tabletop-Sim）。<br>- **训练资源**：<br> - SingleVLA预训练：5×H100 GPUs，耗时5天，120k步。<br> - TwinVLA微调：1×L40S GPU，耗时2天，100k步。<br> - 计算总量：约25 H100 GPU天（RDT-1B需超1,000 H100 GPU天）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **真实世界**：使用Anubis双臂机器人，任务包括“carrot to bag”（胡萝卜入袋）、“brush to dustpan”（刷子入簸箕）、“take towel off”（取下毛巾）。<br> - **仿真**：<br> - RoboTwin 2.0：50个双臂任务（Easy和Hard设置）。<br> - Tabletop-Sim：5个任务（如“dish-drainer”（碗碟架）、“handover-box”（传递盒子））。<br>- **评估指标**：<br> - 主要指标为任务成功率（Success Rate），在每任务20-500次 rollout 中计算平均成功率。<br> - 语言跟随测试：多任务组合指令（如“put X box into Y pot”）下的平均成功率。<br> - 数据效率评估：随演示数量（20、35、50）变化的成功率曲线。</details> |
| 2025-11-06 | Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment | http://arxiv.org/abs/2511.04555 | <details><summary>展开</summary>```markdown<br>论文研究单位<br>上海交通大学人工智能学院, EvoMind Tech, IAAR-Shanghai, SII, 卡内基梅隆大学, 剑桥大学, 南洋理工大学<br><br>论文概述<br>当前视觉-语言-动作模型通常参数量巨大，且严重依赖大规模机器人数据预训练，导致训练和推理成本高昂，部署困难。同时，许多训练范式会破坏视觉-语言主干的感知表示，导致过拟合和泛化能力差。该论文提出了Evo-1，一个轻量级的VLA模型，旨在降低计算成本并提高部署效率，同时无需机器人数据预训练即可保持强大性能。Evo-1基于一个原生的多模态VLM，结合了新颖的交叉调制扩散Transformer和一个优化的集成模块。此外，论文还引入了一个两阶段训练范式，逐步对齐动作与感知，从而保留了VLM的表示能力。Evo-1仅有7.7亿参数，在Meta-World和RoboTwin基准上取得了最先进的结果，并在LIBERO上达到了94.8%的竞争性结果。在真实世界评估中，Evo-1实现了78%的成功率，并具有高推理频率和低内存开销。<br><br>论文核心贡献点<br>1. 轻量高效的架构：提出了一个仅有0.77B参数的轻量级VLA架构，降低了训练成本，并提高了在消费级GPU上的实时部署推理速度。<br>2. 语义保留以提升泛化能力：引入了一种两阶段训练范式，在保留VLM固有视觉-语言理解的同时，使其适应下游动作生成，有效增强了跨多种操作任务的泛化能力。<br>3. 无需预训练的优异性能：广泛的仿真和真实世界实验表明，Evo-1在没有依赖大规模机器人数据预训练的情况下，实现了最先进的性能，显著减少了对昂贵且劳动密集型数据收集的需求。<br><br>论文方法描述<br>Evo-1是一个模块化的视觉-语言-动作模型，主要由三部分组成：<br>1. 视觉-语言主干：采用InternVL3-1B模型，该模型通过原生多模态范式进行预训练。视觉编码器为InternViT-300M，语言分支为Qwen2.5-0.5B。视觉和语言特征通过将图像块嵌入插入到标记序列中进行融合。<br>2. 交叉调制扩散Transformer：作为动作专家，它是一个基于流匹配范式的条件去噪模块。该模块是一个扩散Transformer，仅依赖堆叠的交叉注意力层来生成连续的控制动作序列。<br>3. 集成模块：采用基于交叉注意力的模块来融合多模态表示和机器人本体感觉信息。具体来说，将从VLM主干提取的多模态表示与机器人状态连接，然后作为后续动作专家Transformer的键值输入。<br>模型采用两阶段训练范式：<br>1. 动作专家对齐：冻结视觉-语言主干，仅训练集成模块和动作专家，使其与预训练的多模态嵌入空间对齐。<br>2. 全规模微调：解冻VLM主干，对整个架构进行端到端的微调，实现感知与控制的深度整合。<br><br>论文使用数据集和训练资源<br>数据集：<br>- 仿真环境：Meta-World benchmark (包含50个任务，分为easy, medium, hard, very-hard四个难度), LIBERO benchmark (包含spatial, object, goal, long四个类别的40个任务), RoboTwin benchmark (包含Click AlarmClock, Dump Bin BigBin, Place Bread Basket, Place Can Basket四个双臂任务)。<br>- 真实世界环境：使用6-DoF xArm6机械臂和四种操作任务（Pick and Place Can, Pour Foam from Cup, Hand Delivery, Can Stacking），每个任务收集100次遥操作演示用于训练。<br>训练资源：论文提到模型设计旨在消费级GPU上进行低成本训练和实时部署。真实世界推理效率评估在RTX 4090d GPU上进行，模型参数量为0.77B。<br><br>论文使用的评估环境和评估指标<br>评估环境：<br>- 仿真环境：Meta-World, LIBERO, 和 RoboTwin。<br>- 真实世界环境：配备平行夹爪的xArm6机械臂，在四个自定义任务上进行评估。<br>- 泛化实验：在真实世界的“Pick and Place Can”任务中，通过加入未见过的干扰物、改变背景颜色、改变目标位置和高度四种方式测试模型的泛化能力。<br>评估指标：<br>- 任务成功率：主要评估指标是任务成功率。在仿真环境中，报告各子任务或难度级别的平均成功率及总体平均成功率。在真实世界环境中，报告各任务的平均成功率及总体平均成功率。<br>- 推理效率：在真实世界实验中，还比较了模型的参数量、GPU内存占用和推理频率。<br>```</details> |
| 2025-11-06 | GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies | http://arxiv.org/abs/2511.04357 | <details><summary>展开</summary>待生成</details> |
| 2025-11-04 | XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations | http://arxiv.org/abs/2511.02776 | <details><summary>展开</summary>## 论文研究单位<br>北京具身智能机器人创新中心（牵头单位），联合北京航空航天大学（机械工程与自动化学院、虚拟现实技术与系统国家重点实验室）、北京大学（计算机学院多媒体信息处理国家重点实验室）共同完成。<br><br>## 论文概述<br>针对视觉语言行动（VLA）模型在低层动作精确控制和跨机器人形态数据整合方面的挑战，论文提出 **XR-1** 框架：通过引入 **统一视觉运动代码（UVMC）** 作为离散潜在表征，结合三阶段训练范式，实现多模态对齐与跨身体控制的视觉语言行动模型。该模型可同时利用人类视频和机器人数据，显著提升多任务泛化能力和实际部署效果。<br><br>## 论文核心贡献点<br>1. **UVMC 机制**<br> 提出统一视觉运动代码（UVMC），通过双分支 VQ-VAE 将视觉动态和机器人运动编码至共享离散潜在空间，并通过 KL 散度约束实现跨模态对齐。<br>2. **三阶段训练框架**<br> 包含自监督 UVMC 学习、跨身体预训练和任务特定微调，模型无关于特定 VLA 架构，可适配多种基础模型（如 π₀、SwitchVLA）。<br>3. **大规模实证验证**<br> 在 6 种机器人平台、120+ 任务中进行超过 14,000 次实机测试，平均成功率显著超越 π₀.₅、π₀、RDT、UniVLA 等 SOTA 基线。<br><br>## 论文方法描述<br>### 核心架构<br>- **UVMC 学习（Stage-1）**<br> 通过双分支 VQ-VAE 分别编码视觉动态（未来帧预测）和机器人运动（动作序列重构），共享码本实现模态统一，引入视觉-运动 KL 散度损失促进对齐。<br>- **跨身体预训练（Stage-2）**<br> 使用 UVMC 作为监督信号，通过可学习 token 将其注入 VLM 主干，结合动作预测损失联合训练策略网络。<br>- **任务特定微调（Stage-3）**<br> 基于目标机器人任务数据微调模型，提升特定场景性能。<br><br>### 训练流程<br>1. 大规模数据预训练 UVMC：利用 Open-X、RoboMIND、Ego4D、XR-D 四类数据（人类视频+机器人数据），加权采样平衡不同来源。<br>2. UVMC 引导策略预训练：在 XR-D 上进行跨身体动作学习。<br>3. 任务特定微调：针对下游任务数据进行小样本适配。<br><br>## 论文使用数据集和训练资源<br>- **数据集组成**：<br> - Open-X（978k episodes, 59.3M frames, 权重 40%）<br> - RoboMIND（69k episodes, 21.4M frames, 权重 15%）<br> - XR-D（158k episodes, 69.1M frames, 权重 35%）<br> - Ego4D（59k episodes, 14.3M frames, 权重 10%）<br>- **训练资源**：<br> - 主模型 XR-1：基于 PaliGemma（SigLIP+Gemma）架构<br> - 轻量模型 XR-1-Light：基于 SwitchVLA（Florence-2）架构<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 6 种真实机器人平台：Tien Kung 1.0/2.0、单/双臂 UR-5e、双臂 Franka、AgileX Cobot Magic 2.0<br> - 覆盖 120+ 复杂任务：双臂协作、灵巧操作、可变形体处理、接触密集任务、动态环境等<br>- **评估指标**：<br> - **成功率（Success Rate）**：每任务 20 次测试的平均完成率<br> - **跨身体泛化**：在未见过机器人平台（如 Tien Kung 2.0）上验证迁移能力<br> - **鲁棒性测试**：新增物体、背景干扰、光照变化下的性能保持率<br><br>**效果摘要**：<br>XR-1 在所有平台平均性能显著优于 π₀.₅、π₀ 等基线（如 Tien Kung 2.0 任务平均 72% vs 41%），并在未知场景（新增干扰物等）中展现更强鲁棒性。<br><br>---<br>论文核心创新在于通过 UVMC 机制统一视觉与动作模态，结合三阶段训练打通人类演示与机器人数据，实现跨身体的通用技能学习。实验充分验证了其在复杂操作中的有效性。</details> |
| 2025-11-01 | iFlyBot-VLA Technical Report | http://arxiv.org/abs/2511.01914 | <details><summary>展开</summary>论文研究单位<br>iFLYTEK Research and Development Group; LindenBot<br><br>论文概述<br>iFlyBot-VLA 是一个基于 Transformer 语言主干与扩散/流匹配动作专家的 Vision-Language-Action（VLA）模型，用于端到端双机械臂操作。论文提出“显式 + 隐式”双层动作表示框架：以频域压缩的离散动作标记 FAST 提供显式监督，帮助 VLM 学习动作语义和隐式规划；以 VQ-VAE 预训练的紧凑潜在动作表示提供隐式规划信号，仅将其特征传入动作专家，实现高效且可控的动作生成。该方法在保持 VLM 通用感知与推理能力的同时，缓解了端到端训练对 VLM 能力的破坏，并通过混合训练策略（机器人轨迹 + 通用 QA/空间 QA 数据）进一步提升泛化。<br><br>论文核心贡献点<br>提出并训练了基于 VQ-VAE 的潜在动作模型，代码本规模为 32，每步检索 8 个离散码，引入 NSVQ 近似以替代 Straight-Through Estimator 并在解码时对当前帧停止梯度。<br>提出双层动作表示：FAST 离散动作标记用于监督 VLM 学习动作语义（特征不送入动作专家），潜在动作标记作为压缩后的隐式规划信号输入动作专家，实现高效连续控制。<br>提出混合训练策略，在预训练阶段将空间 QA 与机器人轨迹数据按优化比例混合，保留并增强 VLM 的通用感知与空间推理；并通过截断专家到主干梯度、在下游微调阶段开启专家反向传播与多噪声扰动等策略稳定训练。<br>在 LIBERO 仿真与真实世界任务上实现领先性能与泛化，并计划开源部分自建数据集。<br><br>论文方法描述<br>总体架构：基于 Qwen2.5-VL(3B) 作为视觉语言主干，在其上接入 Flow-Matching 扩散 Transformer 动作专家；仅传递潜在动作标记对应的 KV 缓存至专家，离散 FAST 标记的 KV 不传递。<br>潜在动作模型（Stage I）：以自监督方式从人机操作视频中学习潜在动作；采用 NSVQ 解决 VQ-VAE 训练中的梯度崩塌问题。<br>离散动作标记（FAST）：对滑窗连续控制信号进行 DCT 压缩和 BPE 编码，仅用于监督 VLM 的动作语义与隐式规划，不向动作专家提供特征。<br>VLA 训练（Stage II/III）：双阶段训练，第一阶段截断专家到主干梯度以保护 VLM 能力，第二阶段开启梯度传播并引入多噪声扰动加速专家适配。<br>动作生成：采用 Flow Matching，通过离散前向欧拉积分（5 步，σ=0.2）从随机高斯噪声逐步还原动作块；动作与状态统一 pad 到 20 维（左右臂各 10 维）。<br><br>论文使用数据集和训练资源<br>潜在动作训练数据（Stage I）：人类视频数据集 HoloAssist、Ego4D、EgoDex、HOI4D、Something-Something V2、EgoVid；机器人数据集 OXE、AgiBot-World、RoboMind、Galaxea。<br>VLA 预训练数据（Stage II）：内部构建的空间推理 QA 数据；公开数据集 OXE、AgiBot-World 的子集；iFLYTEK 自采集双臂操作数据（衣褶、通用抓取摆放、长程包裹分拣）。<br>自采集数据集规模与构成：<br>衣褶任务：8 类衣物（5 款 T 恤、3 款短裤），每类约 190 条轨迹，平均 4.5 分钟，约 110 小时。<br>通用抓取摆放：30 类物体，每类约 400 条轨迹，平均 27 秒，约 90 小时。<br>长程包裹分拣：约 2,752 条轨迹，平均 61 秒，约 47 小时。<br>预训练配比与阶段：预训练阶段混合上述数据集与空间 QA；微调阶段进行任务特定训练（如 LIBERO：70,000 步用于 Long 子集，50,000 步用于其他子集；动作窗口长度 7；全局批大小 64；仅第三人称图像与文本指令）。<br><br>论文使用的评估环境和评估指标<br>仿真评估：LIBERO 基准（LIBERO-Spatial、Object、Goal、Long 四个任务套件，各 10 个任务×10 条演示）；指标为成功率（%）。<br>真实世界评估：<br>通用抓取摆放：四种配置（基础、未见物体、光照变化、未见场景），24 类可见或 14 类未见物体，每类 20 次尝试；指标为成功率（%）。<br>长程包裹分拣：双机械臂翻转与摆放流程，40 次×3 包裹（两条需翻转）试验；采用“严格（不允许校正）”与“允许校正（最多两次校正）”两种评估；指标为成功率与相对提升（%）。<br>衣褶任务：定义步骤级评估协议，统计各步骤完成率；任务含“点选角落”、“摊平（扫平/拖拽）”、“折叠”等子步骤；允许在 3 分钟时间限制内多次尝试。</details> |
| 2025-11-03 | Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process | http://arxiv.org/abs/2511.01718 | <details><summary>展开</summary># 论文研究单位<br>- HKUST(GZ) - 香港科技大学（广州）<br>- Westlake University - 西湖大学<br>- Zhejiang University - 浙江大学<br>- Monash University - 蒙纳士大学<br><br># 论文概述<br>论文提出统一扩散VLA（UD-VLA），通过联合离散去噪扩散过程（JD3P）将视觉、语言和动作的理解、生成和执行统一到单个Transformer中。核心创新是通过同步去噪过程联合优化视觉生成和动作预测，使动作在持续的视觉指导下从初始化逐步演化，实现真正的跨模态协同。<br><br># 论文核心贡献点<br>- 提出统一扩散VLA，通过同步去噪过程紧密耦合理解、生成和执行，形成互利关系<br>- 通过离散标记化、混合注意力和JD3P过程作为跨模态协同的核心机制来实例化设计<br>- 设计两阶段训练pipeline激活图像生成能力，引入多种测试时技术确保高性能和效率<br>- 在CALVIN、LIBERO和SimplerEnv等基准上达到SOTA性能，推理速度比自回归方法快4倍<br><br># 论文方法描述<br>**统一标记化**：将语言、视觉和动作模态转换为离散标记并连接成单序列，使用VQ标记器进行视觉量化，FAST进行动作标记化<br><br>**混合注意机制**：输入部分采用因果和双向注意，输出分为生成块（未来图像）和执行块（动作），块内双向注意，块间因果注意，阻止动作信息回流<br><br>**联合离散去噪扩散过程（JD3P）**：并行生成动作和图像，通过马尔可夫链噪声过程和条件去噪分布，在每个去噪步骤联合重构掩码位置，从噪声逐步恢复原始信号<br><br>**两阶段训练**：阶段(i)在视频数据集上注入未来图像生成能力；阶段(ii)在机器人动作数据集上联合优化图像和动作生成<br><br>**推理技术**：前缀KV缓存、预填充特殊令牌、置信度引导解码、解码空间映射等提升效率<br><br># 论文使用数据集和训练资源<br>- **预训练数据**：大规模视频数据集用于阶段(i)训练<br>- **下游数据**：机器人动作数据集用于阶段(ii)联合训练<br>- **评估基准**：CALVIN、LIBERO、SimplerEnv仿真环境，真实世界UR5e机械臂+Inspire RH56E2机械手平台<br>- **初始化模型**：预训练VLM主干网络（Emu3）<br><br># 论文使用的评估环境和评估指标<br>**CALVIN**：4个环境、34个任务、1000条语言指令，报告连续5个子任务的平均完成长度（最大值5.0）<br><br>**LIBERO**：4个套件（空间、物体、目标、长程），每套件10个任务×50次运行，报告各套件和整体成功率<br><br>**SimplerEnv-WidowX**：真实到仿真传输评估，4个任务，报告各任务和整体成功率<br><br>**真实世界实验**：UR5e机械臂+6自由度Inspire RH56E2机械手，3类任务（堆叠碗、放置方块、翻转塔），每类200条轨迹，在可见和不可见设置下评估各30次的成功率</details> |
| 2025-11-03 | PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model | http://arxiv.org/abs/2511.01571 | <details><summary>展开</summary>### 论文研究单位<br>- 华南理工大学自动化科学与工程学院<br>- 中国科学院沈阳自动化研究所<br>- 穆罕默德·本·扎耶德人工智能大学（Mohamed bin Zayed University of Artificial Intelligence）<br>- 澳大利亚国立大学<br><br>### 论文概述<br>PixelVLA 是首个支持像素级理解和多模态提示（文本和视觉输入）的视觉-语言-动作（VLA）模型。现有 VLAs 局限于图像级理解且依赖文本提示，导致空间推理和跨域泛化能力不足。PixelVLA 通过集成多尺度像素感知编码器、视觉提示编码器和连续动作解码器，解决这些挑战。使用两阶段自动注释管道生成的 Pixel-160K 数据集进行训练，该数据集包含 160K 操作片段和 6.5M 图像-文本-动作三元组。PixelVLA 在三个标准 VLA 基准测试（SimplerEnv、Google Robot、LIBERO）上显著优于 OpenVLA，成功率提升 10.1%~28.7%，且训练成本仅为 OpenVLA 的 1.5%。<br><br>### 论文核心贡献点<br>1. **PixelVLA 架构设计**：提出支持像素级理解和多模态提示的 VLA 模型，包括多尺度像素感知编码器、视觉提示编码器（处理点、线、区域等提示）和连续动作解码器（直接预测 7D 动作）。<br>2. **两阶段自动注释管道**：开发用于合成 Pixel-160K 数据集的管道，第一阶段定位夹爪并生成区域提议，第二阶段使用 LLM 和开放词汇分割模型生成像素级注释和视觉提示。<br>3. **视觉运动指令调优框架**：引入两阶段训练流程——连续动作训练阶段和像素级理解增强阶段，有效提升像素级空间理解能力。<br>4. **实证验证**：在多个基准测试上集成 PixelVLA 架构到 OpenVLA 和 π₀ 模型，验证性能提升和泛化能力。<br><br>### 论文方法描述<br>- **PixelVLA 架构**：基于 Prismatic-7B VLM 和 Llama 2-7B 的主干，结合视觉编码器（DinoV2 和 SigLIP）、MLP 投影仪、视觉提示编码器（源自 SAM）、多尺度像素感知编码器（生成像素感知嵌入）和连续动作解码器（ResNet 块 + MLP，预测连续动作序列）。<br>- **多尺度像素感知编码器**：从多尺度视觉特征中提取像素级信息，公式基于特征和像素掩码的 MLP 组合。<br>- **视觉提示编码器**：处理多样化视觉提示（如点、线、区域），生成提示感知嵌入。<br>- **连续动作解码器**：基于 LLM 隐藏状态，通过线性投影、ResNet 块和 MLP 输出连续动作值（chunk 大小为 8）。<br>- **训练流程**：两阶段视觉运动指令调优。<br> - **阶段一（连续动作训练）**：使用 Fractal 和 Bridge v2 数据训练连续动作解码器，冻结其他模块。<br> - **阶段二（像素级理解增强）**：使用 Pixel-160K 数据集，通过 LoRA 微调 LLM 主干，联合训练视觉提示和像素感知编码器。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：Pixel-160K（160K 操作片段，6.5M 三元组），基于 Fractal 和 Bridge v2 数据集构建；LIBERO-Pixel（使用管道处理 LIBERO 基准）。<br>- **训练资源**：<br> - 两阶段训练：阶段一 100k 步（批量 32，学习率 5e-4），阶段二 200k 步（批量 32，学习率 1e-3，LoRA r=32）。<br> - 总训练成本仅为 OpenVLA 预训练的 1.5%。<br> - 输入分辨率 224×224，单视角第三视角相机。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：三个仿真基准测试。<br> - SimplerEnv（Google Robot 和 WidowX 设置）：评估零样本对象操作。<br> - LIBERO：四个任务套件（LIBERO-Spatial、-Object、-Goal、-Long）评估新机器人设置适配。<br>- **评估指标**：<br> - SimplerEnv：成功率，包括视觉匹配（VM）和变体聚合（VA）分数；平均抓取和任务完成成功率。<br> - LIBERO：每个任务套件的成功率和排名。<br> - 主要指标：操作成功率（%），如 Google Robot 上 PixelVLA 平均 VM 得分 61.4（提升 28.7%）。</details> |
| 2025-11-03 | RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models | http://arxiv.org/abs/2511.01331 | <details><summary>展开</summary>### 论文研究单位<br>Westlake University<br><br>### 论文概述<br>RobustVLA是一种针对Vision-Language-Action (VLA)模型的在线强化学习后训练方法，旨在提高模型在环境扰动（如观察噪声和动作噪声）下的鲁棒性。传统方法在out-of-distribution部署中易失败，RobustVLA通过理论分析的鲁棒性界限，引入Jacobian正则化和smoothness正则化来显式约束模型敏感性，显著提升VLA模型的稳定性和可靠性。<br><br>### 论文核心贡献点<br>- 提出RobustVLA方法，结合在线RL和鲁棒性约束，针对环境扰动优化VLA模型。<br>- 进行系统鲁棒性分析，识别观察扰动和动作扰动的影响，分别推导Jacobian敏感性和smoothness稳定性的关键作用。<br>- 引入双正则化策略：Jacobian正则化抑制观察噪声敏感性，smoothness正则化缓解动作扰动和更新漂移。<br>- 实验证明RobustVLA在多种扰动下优于SOTA基线（如RIPT-VLA、OpenVLA等），提升迁移学习和消融性能。<br><br>### 论文方法描述<br>- **问题建模**：基于马尔可夫决策过程（MDP），VLA模型πθ映射语言指令和观察序列到动作分布。<br>- **鲁棒性分析**：<br> - 定理1（观察扰动误差界限）：返回差距受Jacobian敏感性和观察噪声影响，需控制\|\|∇_s log πθ(a\|s)\|\|。<br> - 定理2（动作扰动返回漂移）：返回差距与累积模型漂移∑δ_t和动作噪声σ√d相关，需限制模型更新平滑度。<br> - 定理3（联合扰动鲁棒性）：观察和动作扰动联合时返回差距扩大，需同时应用双正则化。<br>- **正则化目标**：<br> - Jacobian正则化：ℛ_Jac(θ) = E[min(\|\|∇_s log πθ(a\|s)\|\|², G_max)]，限制输入敏感性。<br> - Smoothness正则化：ℛ_Smooth(θ) = E[\|\|μ_θ(s) - μ_θ-(s)\|\|²]，稳定模型更新。<br> - 整体目标：ℒ_RobustVLA = ℒ_PPO + αℛ_Jac + βℛ_Smooth。<br>- **算法实现**：采用课程学习机制（RobustVLA-C），基于成功率的移动平均自适应调整噪声水平（ε_min到ε_max），防止训练初期不稳定。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：基于LIBERO仿真平台，包含Objects、Long、Spatial、Goal四个任务套件，每个套件使用50个保留测试上下文。<br>- **扰动设置**：<br> - 观察扰动：图像移位、旋转、颜色抖动、遮挡、擦除。<br> - 动作扰动：零均值高斯噪声，标准差0.1、0.2、0.3。<br>- **训练资源**：在线RL交互收集数据，算法在仿真环境中运行；评估基于单VLA模型部署至所有任务。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO仿真平台，模拟机器人操作任务场景。<br>- **评估指标**：<br> - 主要指标：平均成功率（SR）在扰动下的表现。<br> - 对比基线：离线IL（π₀、GEVRM、OpenVLA、OpenVLA-OFT）、离线RL（RWR、ARFM、ReinboT）、在线RL（RIPT-VLA）。<br> - 其他评估：迁移学习性能（从LIBERO Goal套件到下游任务）、消融研究（超参数α和β影响、T-SNE观察表示可视化）。</details> |
| 2025-11-03 | Embodiment Transfer Learning for Vision-Language-Action Models | http://arxiv.org/abs/2511.01224 | <details><summary>展开</summary># 论文研究单位<br>Shanghai University（上海大学）<br><br># 论文概述<br>论文介绍了ET-VLA（Embodiment Transfer Learning for Vision-Language-Action），一种用于将预训练VLA模型高效迁移到多机器人系统的新型框架。传统自回归VLA模型（如RT-2、OpenVLA）在单臂机器人任务上表现优异，但在多机器人协作场景中性能显著下降，经常无法生成结构有效的动作序列。ET-VLA通过合成继续预训练（SCP）和实体化思维图（EGoT）技术，在双机器人系统上显著提升性能，在六个真实世界任务中成功率比OpenVLA高出53.2%。<br><br># 论文核心贡献点<br>- 深入分析了现有自回归VLA模型在多机器人多任务设置中的局限性<br>- 提出ET-VLA框架，包含合成继续预训练（SCP）和实体化思维图（EGoT）两个关键技术<br>- 在真实机器人和仿真环境中广泛评估，证明了相对于最先进VLA模型的优越性能<br><br># 论文方法描述<br>**合成继续预训练（SCP）**：生成合成多机器人数据来预热模型，使其能够生成正确的动作序列和精确的动作token数量。SCP采用交叉采样方法，从batch中随机选择不同样本的动作tokens，组合成14个动作tokens（每个机器人7个DoF），使模型学习token到机器人的映射关系。<br><br>**实体化思维图（EGoT）**：将复杂任务分解为显式表示时间依赖关系的动作图。该方法定义了五个任务类型（Grasp、Release、Waiting、End、Complete），让VLA模型理解和管理多机器人任务中的时间依赖，提供结构化和可解释的规划框架，增强任务规划和跨机器人协调能力。<br><br># 论文使用数据集和训练资源<br>- 使用Open X-Embodiment (OXE)数据集进行预训练<br>- 使用Bridge Data V2进行微调<br>- 收集458条跨6个不同任务的真实机器人轨迹数据<br>- 额外收集980条人类演示轨迹<br>- 在16个A100 GPU上进行训练<br><br># 论文使用的评估环境和评估指标<br>**评估环境**：在三种不同双机器人实体上进行验证：双UR5e、双Franka、双AgileX机器人。设计了六个协作任务：PickBread（拾取面包）、PickFruits（拾取水果）、WipePlate（擦拭盘子）、InsertPlate（插入盘子）、PullString（拉绳）、BuildBlocks（构建积木）。同时在RLBench2和RoboTwin仿真基准上进行评估。<br><br>**评估指标**：主要使用平均成功率（%）作为评估指标，在每个任务上报告成功次数/总试验次数的比率，并通过多个任务的平均成功率来评估整体性能。</details> |
| 2025-11-03 | OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation | http://arxiv.org/abs/2511.01210 | <details><summary>展开</summary># 论文研究单位<br><br>Princeton University、University of California, Los Angeles、Microsoft Research Asia<br><br># 论文概述<br><br>论文提出了OmniVLA，一个多模态视觉-语言-动作（VLA）模型，通过集成红外、毫米波雷达和声学传感器等新型感知模态，实现超越RGB感知能力的机器人操控。该模型将异构传感器统一到图像空间中，利用传感器掩码图像这一核心创新，实现了物理基础的空间智能。<br><br># 论文核心贡献点<br><br>1. 首个统一多个感知模态的VLA模型，整合红外、毫米波雷达和声学传感器<br>2. 提出传感器掩码图像技术，实现传感器信息的空间定位和语义对齐<br>3. 设计轻量级多传感VLA模型架构，支持数据高效学习<br>4. 显著提升机器人操控任务的成功率和泛化能力<br><br># 论文方法描述<br><br>核心方法是传感器掩码图像生成：<br>- 传感器数据预处理：将毫米波雷达和麦克风阵列数据通过波束成形转换为二维热图<br>- 分割掩码生成：使用GPT-4o生成分割提示词，结合Grounded SAM 2进行语义分割<br>- 图像融合：将传感器图像在掩码区域叠加到RGB图像上<br><br>模型架构采用预训练的SmolVLA作为基础：<br>- 冻结视觉和语言编码器<br>- 为每个传感器模态添加独立的多层感知机投影层<br>- 投影后的传感器token与语言token拼接，输入大语言模型<br>- 使用扩散式动作专家模块生成最终机器人动作<br><br># 论文使用数据集和训练资源<br><br>硬件平台：SO101机械臂配合多模态传感器套件，包括RGB相机、红外热像仪、毫米波雷达和六麦克风圆形阵列<br><br>训练数据：收集800个演示片段（每类传感器任务200个，通用抓取任务200个）<br><br>计算资源：<br>- 训练：多个Nvidia A100 GPU进行分布式训练<br>- 推理：本地RTX 4090 GPU，支持15fps实时预测<br><br># 论文使用的评估环境和评估指标<br><br>评估任务涵盖三种需要非视觉感知的操控任务：<br>- 热传感任务：区分冷热饮料并抓取冷饮<br>- 毫米波任务：透视纸箱定位隐藏物体<br>- 声学任务：定位并 uncovering响铃手机<br><br>评估指标：<br>- 任务成功率：25次独立试验的成功率<br>- 任务评分：0.5分选择正确目标+0.5分完成正确操作<br>- 泛化能力：在未见任务上的少样本学习表现<br><br>基准对比：<br>- VLA-RGB：仅RGB输入的VLA模型<br>- VLA-RAW：使用原始传感器数据的VLA模型<br><br>实验结果：OmniVLA达到84%平均成功率，分别比RGB-only和raw-sensor基线高出59%和28%。</details> |
| 2025-11-02 | Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots | http://arxiv.org/abs/2511.00917 | <details><summary>展开</summary>## 论文研究单位<br>宾夕法尼亚大学（University of Pennsylvania），并开放项目主页：maestro-robot.github.io。<br><br>## 论文概述<br>论文提出maestro（Managerial Agent for Executing Sensorimotor Tasks in Robotics），一个以视觉语言模型（VLM）为中心的模块化智能体，通过编排包含感知、几何、控制、预训练视觉-动作模型与图像编辑等在内的“工具集”，以编写与执行代码的方式在真实世界中实现零样本通用机器人操作。该范式不依赖大规模机器人数据，而是在闭循环中“计划—反应—重计划”的执行与反馈回路中不断演化，适应新任务与新硬件。<br><br>## 论文核心贡献点<br>- 首个在零样本条件下能与最新视觉-语言-动作（VLA）模型竞争的模块化机器人策略。<br>- 提出了以VLM为编码智能体、工具库为核心的系统化设计（覆盖几何与主动感知、碰撞规避、VLA高频监控），显著提升语义理解与精确执行的协同能力。<br>- 系统性消融实验揭示主动感知与几何推理工具的必要性。<br>- 展示在少量真实试验基础上的演化式改进机制，通过历史代码与失败分析进行上下文学习。<br><br>## 论文方法描述<br>- 总体架构：以VLM（文中采用Gemini Robotics-ER 1.5）为代码生成智能体，接收任务指令与场景图像，动态编写程序并实时执行与修改，形成“计划—反应—重计划”的闭环。<br>- 工具模块设计（Tabletop / Mobile 双套件）：<br> - 感知：原始RGB+本体感觉；分割与中心点；指向与任务相关关键点（受ReKep启发）；FoundationStereo深度；主动感知（手腕相机变焦/环视）。<br> - 控制与几何：笛卡尔与手爪控制；cuRobo点云无碰撞运动规划；几何与线性代数（向量构造、距离、相对旋转、向量旋转）以支持空间推理。<br> - 视觉运动策略：GraspGen抓取模型；π0.5 VLA作为可调用工具，配以本地托管的Qwen-2.5-VL-72B实现2Hz任务完成监控以高频中断与重规划。<br> - 图像编辑：在图像上绘制关键点与6D位姿叠加，提升视觉定位与推理。<br> - 移动操控：Mobile base状态估计（Faster-LIO）；语义地图缓存；主动探索工具；细粒度“nudge”局部微调与全局导航（Nav2）。<br>- 演化式改进：记录历史执行代码、输出与失败分析，作为上下文样例供后续尝试学习与优化。<br><br>## 论文使用数据集和训练资源<br>- 评估方法：为检验零样本泛化，采用STAR-Gen生成评测场景，固定对比基线；5次试验/任务（初始+4次扰动）。<br>- 真实世界平台与任务：<br> - 桌面操控：Franka Emika Panda（7-DoF）+ Robotiq 2F 手爪；手腕与第三人称相机；DROID平台。任务包括拾取放置、可变形物体（折毛巾/T恤）、铰接物体（开柜）、空间推理（紫色面朝上旋转立方体）、工具使用（用刀切香蕉）、物体功能（挂杯）、记忆与长时序（擦白板指令后按指令叠杯）。<br> - 移动操控：Unitree Go2-W 四足机器人 + AgileX PiPER机械臂；校准手腕相机。任务包括长时序取物、投掷、主动探索（搜索并返回）、物体功能（按按钮开门）。<br>- 预训练/基础模型：<br> - VLM：Gemini Robotics-ER 1.5（用于代码生成与视觉推理）；Qwen-2.5-VL-72B（本地高频监控与中断）。<br> - VLA：π0-FAST-DROID 与 π0.5-DROID（作为可调用工具）。<br> - 其他：GraspGen（抓取）、FoundationStereo（深度估计）、cuRobo（无碰撞规划）、Faster-LIO（移动基定位）、Nav2（导航）。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：两类真实世界本体——桌面操控（DROID、Franka）、移动操控（四足+机械臂），跨场景扰动（物体、位姿、语言指令、背景等）形成零样本测试集。<br>- 评估指标：<br> - 主要指标：平均任务进度（0–100，数值越高越好），基于STAR-Gen与任务分解子目标的可量化评分。<br> - 对比基线：Gemini Robotics Agent（代码即策略）、π0-FAST-DROID、π0.5-DROID；另含maestro+π0.5（VLA作为工具并联）。<br> - 消融设置：去除主动感知与几何模块对性能的影响。<br> - 跨任务类别：在桌面与移动操控的多样任务上进行对比与误差分析。</details> |
| 2025-10-31 | End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection | http://arxiv.org/abs/2511.00139 | <details><summary>展开</summary>### 论文研究单位<br>ByteDance Seed<br><br>### 论文概述<br>论文针对灵巧手在一般机器人中的manipulation挑战，提出一种共享自治（Shared Autonomy）框架，结合人类VR远程操作和自主DexGrasp-VLA策略（用于手部控制），以高效收集协调手臂-手部演示数据。该框架通过分工控制宏-微运动域：人类引导手臂，VLA Copilot执行精细抓取，降低认知负担。数据用于训练端到端VLA策略（含Arm-Hand Feature Enhancement模块），并通过校正远程操作（Corrective Teleoperation）持续改进。实验显示，策略在50+物体上达约90%成功率，数据质量高。<br><br>### 论文核心贡献点<br>- 提出多模态VLA Copilot（DexGrasp-VLA）：融合视觉、语言、触觉和本体感受，实现力适应抓取。<br>- 设计共享自治框架：人类VR控制手臂，VLA自主控制手部，高效收集高质量演示。<br>- 开发Arm-Hand Feature Enhancement架构：显式建模手臂（宏运动）和手部（微运动）的互补特征，提升协调性。<br>- 实现校正人类在环远程操作：采集失败恢复数据，迭代优化策略鲁棒性。<br><br>### 论文方法描述<br>方法分四阶段：<br>1. **DexGrasp-VLA策略训练**：先训练LSTM“盲策略”（融合参数化力控制和远程操作数据），再扩展为多模态VLA（整合视觉、触觉、语言和本体感受）。<br>2. **共享自治数据收集**：人类通过VR相对运动映射远程操作手臂（90Hz），DexGrasp-VLA并行控制手部（30Hz），采集同步手臂-手部数据。<br>3. **端到端VLA策略学习**：基于预训练VLA模型（SFT），添加Arm-Hand Feature Enhancement模块（共享表示+MLP编码的手臂/手部特征），优化总损失（含主损失+辅助损失）。<br>4. **校正远程操作系统**：部署策略自动记录成功/失败轨迹；人类介入纠错采集恢复数据，迭代SFT更新策略。<br><br>### 论文使用数据集和训练资源<br>- **LSTM预训练数据集**：218条轨迹（150条人类远程操作+68条力控制数据），覆盖20+物体，用于训练“盲”力适应抓取策略。<br>- **DexGrasp-VLA手部数据集**：180条抓取轨迹（杂乱场景中60种物体），含手部RGB、触觉（力向量+空间嵌入）、本体状态和动作。<br>- **端到端手臂-手部数据集**：100条共享自治演示，覆盖20种物体，含多视角RGB、语言指令和手臂-手部联合动作。<br>- **校正干预数据集**：100条轨迹（50条方向失败恢复+50条角落案例）。<br>- **训练资源**：基于开源框架LeRobot（Fine-tuning预训练VLA模型π₀）；硬件包括UR3e机械臂（6-DoF）、Xhand五指手（12-DoF）、三RGB-D摄像头及触觉传感器。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：集成机器人平台（UR3e + Xhand），装配三RGB-D摄像头（两静态+一手腕-mounted）；任务场景包括杂杂乱物体抓取、表单清理等。<br>- **评估指标**：<br> - 抓取成功率：DexGrasp-VLA在杂乱场景中95.5%，端到端策略在50+物体上约90%。<br> - 消融研究：验证触觉特征、手部-手臂特征增强、校正远程操作对性能的影响（如成功率、鲁棒性）。<br> - 数据效率：远程操作会话时长、认知负担（经验显示~30分钟无疲劳）。</details> |
| 2025-10-30 | Self-Improving Vision-Language-Action Models with Data Generation via Residual RL | http://arxiv.org/abs/2511.00091 | <details><summary>展开</summary>```markdown<br>论文研究单位<br>NVIDIA, CMU, UC Berkeley, UT Austin<br><br>论文概述<br>该论文提出了一种名为Probe, Learn, Distill (PLD) 的三阶段框架，旨在通过残差强化学习和分布感知的数据收集，自主改进视觉-语言-动作（VLA）模型，而无需额外的人类演示数据。该方法首先冻结VLA主干，通过离线策略RL训练轻量级残差专家；然后采用混合回放方案，使残差干预偏向于基础策略频繁访问的状态；最后通过标准SFT将精心策划的轨迹蒸馏回通用模型。实验表明，PLD在LIBERO基准测试中达到99%任务成功率，在SimplerEnv中实现超过50%的性能提升，并在真实世界的Franka和YAM手臂灵巧操作任务中取得100%成功率。<br><br>论文核心贡献点<br>1. 自主后训练配方：提出了一种使VLA模型无需额外人工演示即可自主改进的后训练流程。<br>2. RL生成数据的系统研究：分析了自动数据收集的关键组成部分，并进行了广泛的实验来研究RL生成数据如何影响对未见任务的泛化能力。<br>3. 综合实证验证：提供了大规模的设计选择消融实验，并在真实机器人硬件上展示了超过99%的成功率，实现了连续1小时的GPU插拔操作而无需人工干预。<br><br>论文方法描述<br>PLD框架包含三个阶段：<br>1. 专家获取（Probe）：冻结VLA主干，通过离线策略RL训练轻量级残差专家，这些专家在基础策略失败时接管，探测VLA通用模型的失败区域。<br>2. 数据收集（Learn）：采用混合回放方案，使残差干预偏向于基础策略频繁访问的状态，对齐收集的轨迹与通用模型的部署分布，同时捕获恢复行为。<br>3. 监督微调（Distill）：将收集的多任务数据通过SFT蒸馏回基础模型，该过程与VLA架构无关，支持流匹配和自回归动作头。<br><br>论文使用数据集和训练资源<br>数据集：LIBERO（包含130个语言条件操作任务，分为四个套件）、SimplerEnv（旨在实现高模拟到真实世界相关性）。<br>训练资源：使用NVIDIA GPU进行训练，具体GPU型号和数量在提供的文本中未明确提及。<br><br>论文使用的评估环境和评估指标<br>评估环境：模拟环境（LIBERO、SimplerEnv）和真实世界环境（Franka Emika Panda机械臂、YAM双手机械臂）。<br>评估指标：任务成功率（Success Rate）、平均回报（Average Return）、零样本泛化性能（Zero-shot Generalization）、样本效率（Sample Efficiency）。<br>```</details> |
| 2025-10-30 | Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail | http://arxiv.org/abs/2511.00088 | <details><summary>展开</summary># 论文研究单位<br>NVIDIA<br><br># 论文概述<br>Alpamayo-R1 (AR1) 是一个视觉-语言-动作模型，旨在通过整合Chain of Causation推理与轨迹规划来增强自主驾驶在复杂驾驶场景中的决策能力。该模型专注于解决安全关键的尾部场景问题，这些场景中监督稀疏且因果理解有限。<br><br># 论文核心贡献点<br>1. **Chain of Causation (CoC)数据集**：构建了混合自动标注和人工标注流程，产生与驾驶行为对齐的决策导向、因果关联的推理轨迹<br>2. **模块化VLA架构**：结合Cosmos-Reason（为物理AI应用预训练的视觉-语言模型）与基于扩散的轨迹解码器，实时生成动态可行的计划<br>3. **多阶段训练策略**：使用监督微调来引出推理能力，并通过强化学习优化推理质量和推理-行动一致性<br><br># 论文方法描述<br>## 架构设计<br>- **VLM主干网络**：采用Cosmos-Reason作为视觉-语言模型骨干<br>- **高效视觉编码**：支持单图像标记化、多相机标记化和多相机视频标记化，实现最高20倍token压缩率<br>- **扩散式轨迹解码**：使用流匹配框架将离散轨迹token转换为连续表示，基于unicycle动力学建模<br><br>## 推理与行动结合<br>通过统一的序列预测框架，将历史观测、推理过程和未来轨迹整合为统一序列，实现推理与行动预测的无缝结合。<br><br>## 数据构建方法<br>**Chain of Causation数据集**采用结构化标注框架：<br>- **决策标注**：定义封闭集合的高层驾驶决策<br>- **关键组件标注**：开放集合的因果因素<br>- **混合标注流程**：结合人工标注(10-20%数据)和自动标注(80-90%数据)<br><br># 论文使用数据集和训练资源<br>- **基础数据集**：基于nuScenes等公开驾驶数据集<br>- **CoC数据集**：混合人工和自动标注，包含结构化推理轨迹<br>- **物理AI领域数据**：覆盖机器人、医疗、智慧城市等多领域数据用于预训练<br>- **人类标注数据**：10-20%高质量人工标注样本用于监督微调和评估<br>- **自动标注数据**：大规模自动生成的结构化标注数据<br><br># 论文使用的评估环境和评估指标<br>## 评估环境<br>- **开环评估**：标准数据集上的规划准确性评估<br>- **闭环评估**：仿真环境中的驾驶性能测试<br>- **实车测试**：车载系统验证实际部署性能<br><br>## 评估指标<br>- **规划性能**：在挑战性场景中规划准确性提升12%<br>- **安全性指标**：<br> - 离路率降低35%<br> - 近距离遭遇率降低25%<br>- **推理质量**：通过大推理模型评估，推理质量提升45%<br>- **推理-行动一致性**：一致性提升37%<br>- **模型扩展性**：从0.5B到7B参数的一致改进<br>- **实时性能**：端到端延迟99毫秒，支持实时部署</details> |
| 2025-10-31 | Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model | http://arxiv.org/abs/2510.27607 | <details><summary>展开</summary>待生成</details> |
| 2025-10-31 | EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities | http://arxiv.org/abs/2510.27545 | <details><summary>展开</summary># 论文研究单位<br>ZhiCheng AI（致诚智能）、伊利诺伊大学厄巴纳-香槟分校（UIUC）、清华大学、北京大学<br><br># 论文概述<br>论文提出 EBT-Policy（Energy-Based Transformer Policy），用能量函数直接刻画可行动轨迹的能量景观，通过能量最小化而非扩散去噪生成动作。该方法旨在解决扩散/流模型在机器人策略学习中的高计算成本、暴露偏差（exposure bias）和推理不稳定等问题。研究在仿真基准（robomimic）与真实双机械臂平台（共4个RGB相机、台式场景）上开展系统实验，验证 EBT-Policy 在训练与推理效率、鲁棒性与泛化能力上的优势，并展示其“涌现能力”——零样本的失败重试与恢复行为。<br><br># 论文核心贡献点<br>- 稳定性与鲁棒性：学习单一、时不变的能量景观，减少暴露偏差与错误累积；在分布外（OOD）状态下通过平衡动力学“回拉”至数据流形，提升在真实复杂环境中的稳定性。<br>- 高效性：推理仅需2步即可在部分任务上达到与扩散策略相同的成功率，相比扩散策略100步，实现50倍计算量减少；训练轮次减少约55%，训练与推理显著提速。<br>- 涌现能力：在未提供重试数据的前提下，仅靠行为克隆与能量驱动即可实现零样本失败恢复与重试策略，表现出类似物理推理的行为选择。<br>- 统一的可解释信号：能量标量提供不确定性与置信度度量，驱动动态计算分配（困难状态多步、简单状态少步），并在训练中通过正则化手段提升多模态分布学习的可控性与稳定性。<br><br># 论文方法描述<br>- 总体思想：将可视运动策略建模为基于能量函数的能量模型（EBM），以能量最小化过程替代扩散模型的去噪/流匹配。动作序列 a 在给定观察 o 与语言指令 ℓ 下，通过迭代沿能量景观负梯度下降生成，最终收敛到低能量（高似然）轨迹。<br>- 生成与采样：采用随机马尔可夫链蒙特卡罗（MCMC）与 Langevin 动态的迭代更新：<br> ŷ_{i+1} = ŷ_i − α_i ∇_ŷ E_θ(x, ŷ_i) + η_i<br> 其中 α_i 为能量缩放的步幅，η_i 为噪声项（可按余弦退火调度）。<br>- 训练细节：随机化 MCMC 步数、缩放 Langevin 噪声、步幅随机化与 Nesterov 加速梯度用于多模态探索与稳定优化；引入能量缩放步幅、预采样归一化（RMSNorm）、梯度裁剪等正则与稳定化机制；在每次迭代中以随机噪声初始化目标轨迹并逐步“去噪”，损失为去噪轨迹与演示轨迹的均方误差。<br>- 推理与动态计算：执行自适应能量下降（梯度范数低于阈值终止或达到最大步数），在不同场景自动分配计算步数，困难状态迭代更久、简单状态快速收敛，形成不确定性与计算的自适应联动。<br><br># 论文使用数据集和训练资源<br>- 仿真数据：robomimic 基准的四项任务（Lift、Can、Square、Tool Hang），每项在模拟环境做50回合评估成功率。<br>- 真实数据：在双机械臂平台（4个RGB相机、台式环境）上通过遥操作采集的数据，用于三项真实任务（Fold Towel、Pick & Place、Collect/Place Pan），包含语言指令与多模态观察。<br>- 架构与对比：设置两种模型变体<br> - EBT-Policy-S：约30M参数，仿真对比用（视觉编码器 ResNet-18，无语言编码器）。<br> - EBT-Policy-R：约100M参数，真实任务用（视觉编码器 DINOv2-S，文本编码器 T5-S）。<br> 与扩散策略（DP）进行同等规模（≤150M）下的公平对比。<br><br># 论文使用的评估环境和评估指标<br>- 环境与场景：仿真 robomimic（标准、可复现实验）；真实双机械臂桌面环境（4相机、可变光照与传感器噪声）。<br>- 指标：任务成功率（SR，百分比），训练收敛轮次/步数（训练效率），推理步数与计算量（推理效率），稳定性与分布外鲁棒性（含失败恢复/重试的定性表现）。<br>- 结果摘要<br> - 仿真：在Lift、Can、Square、Tool Hang四项任务上均优于扩散策略（DP-10步为0，DP-100步在Square/Tool Hang未超越EBT），EBT两项（S、 R）均取得更高SR。<br> - 真实：Fold Towel、Collect Pan、Pick & Place 三任务中，EBT-Policy-R在两项显著优于DP，Pick & Place略优或相当。<br> - 训练与推理效率：EBT在约30轮即达100%成功率，DP需约90轮且推理步数更高；EBT在部分任务2步即收敛，DP在10步时成功率降至0，100步才稳定。</details> |
| 2025-10-30 | Running VLAs at Real-time Speed | http://arxiv.org/abs/2510.26742 | <details><summary>展开</summary># 论文研究单位<br>Dexmal, StepFun<br><br># 论文概述<br>本文展示了如何在单个消费级GPU上以30Hz帧率和最高480Hz轨迹频率运行π_0级多视角VLA模型，实现了之前被认为大型VLA模型无法完成的动态实时任务。通过消除模型推理中的开销，证明了VLAs确实能够在RTX 4090 GPU上实时运行，实验结果显示π_0策略在抓取掉落笔任务中达到100%成功率。<br><br># 论文核心贡献点<br>- 实现了VLA模型的30FPS推理，双视角输入延迟仅27.3ms，显著快于官方openpi推理<br>- 提出完整流式推理框架，实现多层次控制频率（480Hz力控制、30Hz视觉控制、低于1Hz文本交互）<br>- 建立了基于屋顶线模型的理论下界，验证了实现接近最优<br>- 在真实世界中验证了实时抓取掉落笔任务，达到人类反应水平<br><br># 论文方法描述<br>**消除开销策略**：<br>- 使用CUDA图机制消除Python CPU开销，将推理速度提升约两倍<br>- 计算图简化：融合RMS归一化参数到线性层、折叠动作时间编码器、融合QKV投影<br><br>**内核深度优化**：<br>- 调优GEMM瓦片参数，使用Triton实现优化配置<br>- 融合门控线性层，提高并行性和内存访问效率<br>- 采用部分split-k策略处理特殊尺寸的GEMM操作<br>- 将标量操作（偏置、残差、激活函数）融合到GEMM中<br><br>**完整流式推理框架**：<br>- 重叠执行VLM和AE内核，实现资源充分利用<br>- 将高频率输入信号注入AE，支持480Hz控制频率<br>- 实现渐进式动作生成，替代传统的完整轨迹生成<br><br># 论文使用数据集和训练资源<br>- **硬件环境**：单张RTX 4090 GPU<br>- **训练数据**：600集真实世界抓取掉落笔任务的演示数据<br>- **软件框架**：基于官方openpi仓库进行模型训练和推理<br><br># 论文使用的评估环境和评估指标<br>- **推理速度评估**：在不同视角数量下测量推理时间（1视角：20.0ms，2视角：27.3ms，3视角：36.8ms）<br>- **实际任务验证**：抓取掉落笔任务的100%成功率测试<br>- **理论下界分析**：使用屋顶线模型计算GEMM操作的理论上界，考虑内存带宽和计算能力<br>- **同步开销测量**：通过比较不同同步方法的开销，验证实现的接近最优性</details> |
| 2025-10-30 | RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration | http://arxiv.org/abs/2510.26536 | <details><summary>展开</summary>待生成</details> |
| 2025-10-30 | Human-in-the-loop Online Rejection Sampling for Robotic Manipulation | http://arxiv.org/abs/2510.26406 | <details><summary>展开</summary>论文研究单位<br>清华大学深圳国际研究生院, 腾讯机器人X实验室<br><br>论文概述<br>论文提出了一种名为“人在环路中的在线拒绝采样”的方法，用于视觉-语言-动作模型的机器人操作后训练。该方法旨在解决强化学习（RL）在微调VLA模型时的不稳定性问题，以及模仿学习（IL）因离线特性而表现不佳的问题。Hi-ORS通过拒绝采样过滤掉负奖励的样本来稳定值估计，并采用奖励加权的监督学习目标为中间步骤提供密集监督。实验表明，该方法仅需1.5小时的真实世界训练，就能在三个真实任务和两种机器人形态上超越RL和IL基线，并具备强大的测试时错误恢复能力。<br><br>论文核心贡献点<br>1. 识别了VLA后训练中RL不稳定的根源，即不准确的值估计和低效的监督。为此，引入了Hi-ORS，一种利用基于结果的值估计和拒绝采样的简单有效的后训练方法。<br>2. 证明了Hi-ORS能够自然地融合在线人工干预，为策略学习错误恢复行为提供明确指导，从而实现了令人印象深刻的测试时扩展能力。<br>3. 在三个具有挑战性的真实世界任务和两种机器人形态上验证了Hi-ORS，其在有效性和效率上均大幅优于IL和RL基线，且样本效率高，无需复杂的超参数调优。<br><br>论文方法描述<br>Hi-ORS包含一个评估阶段和一个改进阶段，并辅以一个异步基础设施。<br>1. 评估与改进阶段：评估阶段从当前策略生成轨迹，并根据累积奖励使用一个阈值m进行过滤，只保留奖励高于m的成功轨迹，从而避免了不准确的值函数估计。改进阶段则利用这些被接受的高质量轨迹，通过一个奖励加权的监督学习目标来更新策略。该方法为基于流匹配的VLA提供了所有中间去噪步骤的密集监督。<br>2. 人在环路与频率变化：Hi-ORS支持操作员在任务执行中的任何时刻进行干预，如远程操作纠正。这些干预数据只有在满足奖励过滤标准后才会被保留，为学习错误恢复行为提供了明确的指导。系统还采用自适应的数据记录频率，在人工干预期间以较高频率记录，在自主执行期间以较低频率记录，以平衡数据质量和执行流畅性。<br>3. 异步基础设施：系统使用一个GPU进行在线推理，其余GPU-1个GPU进行模型学习。这种异步的Actor-Learner架构提高了训练吞吐量，并允许学习过程在机器人暂停时继续进行。<br><br>论文使用数据集和训练资源<br>1. 数据集：研究未使用标准公开数据集，而是在三个设计的真实世界任务中收集初始人类演示数据。在训练过程中，Hi-ORS通过在线机器人交互和人工干预动态收集新数据，用于策略的迭代改进。<br>2. 训练资源：基础模型是一个基于流匹配的VLA模型π₀，使用PaliGemma-3B作为骨干网络，并包含一个3亿参数的动作专家模块。训练采用异步的GPU设置，利用ZeRO-2技术进行大规模分布式训练，以适应高容量VLAs的计算需求。<br><br>论文使用的评估环境和评估指标<br>1. 评估环境：在两个不同形态的真实机器人上评估了三个任务：<br> * Raise-Hand：使用Paxini Tora One机器人，任务是将手臂抬到目标姿态。<br> * Pack-Detergent：使用Paxini Tora One机器人，任务是将洗涤剂从传送带上拿起并放入纸箱。<br> * Insert-Moisturizer：使用Dobot X-Trainer机器人，任务是将细长的润肤霜瓶插入基座。<br>2. 评估指标：<br> * 成功率：主要评估指标，通过在随机重置的环境中进行10次试验来计算任务的成功率。<br> * 训练时间：记录达到特定性能水平所需的小时数。<br> * 测试时扩展：通过评估策略在不同尝试预算下的最终性能来衡量。</details> |
| 2025-10-29 | $π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models | http://arxiv.org/abs/2510.25889 | <details><summary>展开</summary>待生成</details> |
| 2025-10-29 | Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models | http://arxiv.org/abs/2510.25713 | <details><summary>展开</summary>### 论文研究单位<br>Soft Robotics Lab, ETHz（苏黎世联邦理工学院软机器人实验室）<br><br>### 论文概述<br>针对机器人与人类在协作任务中的自然交互挑战，论文提出基于预训练视觉语言动作模型（VLA）的改进方案。核心思想是通过运动线索（而非语言提示）实现隐式意图理解，使机器人能实时响应人类动作并完成协作任务。<br><br>### 论文核心贡献点<br>- **FiLM条件化**：在视觉编码器中插入FiLM层，增强文本-视觉对齐能力<br>- **辅助损失设计**：添加并行预测头（手部姿态和目标物体）以隐式学习人类意图<br>- **动作后处理优化**：通过增量坐标、旋转向量及手部关节PCA降维（保留96%方差）重构低维动作流形<br>- **方向性损失**（实验效果负向）：提出方向对齐损失，但未提升性能<br>- **推理系统集成**：构建实时硬件-模型接口，实现长任务动态提示切换<br><br>### 论文方法描述<br>**模型架构**：基于Open-VLA改进（SigLIP+DINOv2视觉编码器+LLaMA2-7B语言模型），集成FiLM条件化和辅助预测头。<br>**数据收集流程**：双角色协作采集（操作者穿戴动作捕捉手套控制机器人，协作者在共享空间交互），10Hz采样并同步多模态数据，附文本指令和手部姿态标注。<br>**训练策略**：4×H100 GPU分布式训练（批量24，LoRA rank=32，20轮迭代），采用复合损失（动作+辅助损失）。<br>**推理系统**：硬件接口（绿色）→模型推理（蓝色）→高层规划（红色）的三层架构，0.3秒端到端延迟。<br><br>### 论文使用数据集和训练资源<br>- **协作任务数据**：<br> - "Pick up cube"任务：120条轨迹（红蓝立方体各60条）<br> - "Pass cube"任务：260条轨迹（红立方体200条，蓝立方体60条）<br> - 辅助标签：手部3D姿态（MediaPipe）、目标物体索引<br>- **训练资源**：4×H100 GPU集群，平均训练时长12小时<br><br>### 论文使用的评估环境和评估指标<br>- **动作空间分析**：<br> - 端 effector增量坐标分布更接近高斯（vs原始坐标非凸性）<br> - 手部关节PCA保留4个主成分解释96%方差<br>- **消融实验**：<br> - 动作后处理贡献最大（各指标显著提升）<br> - 辅助损失提供稳定增益<br> - 方向性损失和FiLM在不同损失下呈负向效果<br>- **真实世界评估**：<br> - 场景：桌面临场交互（多摄像头）<br> - 长任务成功率：10次试验中1次成功<br> - 失败原因：协作对象与训练者不同时出现"训练员过拟合"（模型对特定操作者行为过拟合）<br>- **评估指标**：L2损失、PCA重构误差、方向对齐误差、真实任务完成率</details> |
| 2025-10-29 | Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization | http://arxiv.org/abs/2510.25616 | <details><summary>展开</summary>待生成</details> |
| 2025-10-29 | NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies | http://arxiv.org/abs/2510.25122 | <details><summary>展开</summary>待生成</details> |
| 2025-10-27 | A Survey on Efficient Vision-Language-Action Models | http://arxiv.org/abs/2510.24795 | <details><summary>展开</summary>待生成</details> |
| 2025-10-28 | BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning | http://arxiv.org/abs/2510.24161 | <details><summary>展开</summary># 论文研究单位<br>- Tongji University; Shanghai Magic; Koala Uran<br><br># 论文概述<br>- 背景：现有多模态大语言模型（MLLM）、具身大语言模型（ELLM）、视觉-语言-动作模型（VLA）和通用多模态大模型（GMLM）未能同时实现跨空间、跨任务、跨具身的统一能力<br>- 目标：提出BLM1（Boundless Large Model），在单一统一模型中实现数字空间推理与物理空间控制的融合<br>- 方法：两阶段训练（Stage I SFT注入具身知识；Stage II冻结MLLM，训练基于意图桥接接口的扩散变换器（DiT）策略模块）<br>- 结论：单一BLM1在数字任务上约提升6%，物理任务上约提升3%，同时保持指令跟随与跨具身控制能力<br><br># 论文核心贡献点<br>- 统一三能力：跨空间迁移（数字→物理）、跨任务学习、跨具身泛化<br>- 保持语言能力：Stage I以SFT注入具身知识，避免破坏MLLM指令跟随<br>- 有效桥接：意图桥接接口+Perceiver压缩高阶意图，支撑高频率闭环控制<br>- 策略共享：Diffusion Transformer作为通用策略头（0.76B参数），跨具身参数共享<br>- 训练配方：加权采样保证数据集平衡；Stage II中后期引入未来预测损失提升泛化<br>- 系统评测：数字空间六大基准、物理空间四具身六任务，建立统一对比<br>- 结果优势：超越四类模型家族（MLLM、ELLM、VLA、GMLM），单一模型不切换架构<br><br># 论文方法描述<br>- 架构组成<br> - Backbone：Qwen2.5-VL-7B-Instruct；融合视觉与语言；抽取第k层隐藏状态Hk作为高层意图<br> - 意图桥接接口：Perceiver将Hk压缩为固定K个令牌H̃k，降低计算、适配控制<br> - 机器人策略：DiT扩散变换器作为条件生成头；状态编码器fs、动作编码器fa/解码器；预测horizon为h的动作块At<br>- 训练目标<br> - SFT目标（Stage I）：标准next-token交叉熵，冻结视觉与投影器，更新语言与投影<br> - 流动匹配目标（Stage II早期）：Rectified-flow路径，DiT拟合速度场vφ(xτ,τ\|H̃k)，LFM为MSE<br> - 未来预测目标（Stage II后期）：在DiT中引入可学习未来令牌F，使其中间层表征与冻结MLLM对未来观察o_{t+H}与指令y的压缩表征对齐，用余弦相似度<br> - 总体：Stage I仅L_SFT；Stage II早期训练Perceiver/DiT/编解码器；后期叠加L_FP<br>- 训练配方<br> - 加权采样：保证各数据集等概率抽取，避免大数据主导<br> - Stage I：数字多模态问答对（RoboVQA、AgiBot、HoloAssist、BridgeData V2、EgoPlan、ShareRobot）<br> - Stage II：冻结MLLM，使用四具身×六任务的模拟演示数据训练策略；具身特定编码器/解码器独立优化，DiT参数跨具身共享<br> - 物理数据生成：基于ManiSkill，高层技能+关键姿态+IK与碰撞检验+路径规划与平滑，录制第三人称与腕部图像、本体感受与动作<br><br># 论文使用数据集和训练资源<br>- 数字空间数据集（Stage I）<br> - RoboVQA：829,502视频文本对，29,520指令；多类问答（规划、完成检验、 affordance判别与生成、过去/未来事件）<br> - AgiBot：1,001,552条轨迹，2,976.4小时；217任务、87技能、106场景<br> - HoloAssist：166小时；350教师-执行者对，20物理任务、16类物体<br> - BridgeData V2：60,096轨迹，24环境，100+物类；基础到复杂13技能族<br> - EgoPlan：EgoPlan-Bench 4,939多选题 + EgoPlan-IT 50K问答（基于EPIC-Kitchens）<br> - ShareRobot：整合Open X-Embodiment的23数据集，102场景、12具身；1,027,990规划问答、6,522 affordance注释、6,870轨迹预测<br>- 物理空间数据集（Stage II）<br> - 具身：Franka Emika Panda、xArm-6、xArm-7、WidowX AI<br> - 任务：PickCube、PullCube、StackCube、PushCube、PlaceSphere、LiftPegUpright（每具身×任务100集）<br> - 总帧数：347.8K；任务特定成功判定<br>- 数字评测基准（Stage I后）<br> - 多选题：RoboVQA、AgiBot、HoloAssist、RoboFail<br> - 自由问答：EgoThink、ShareRobot<br> - 总计：3,160测试样本（见表2）<br>- 训练资源<br> - 框架：ManiSkill模拟收集<br> - 加权采样：C个数据集等概率，样本在数据集内均匀<br> - 视频帧采样：统一0.5秒间隔采样；短视频4帧，长视频8帧；任务前移除测试片段防泄露<br><br># 论文使用的评估环境和评估指标<br>- 数字空间评估<br> - 指标<br> - Exact-match accuracy（EM）：多选题严格匹配归一化答案<br> - LLM-as-a-judge（GPT评分）：自由问答使用判别式提示与评分标准<br> - EgoThink：s_i∈{0,0.5,1}，最终分数按平均×100<br> - ShareRobot：s_i∈{1,2,3,4,5}，最终分数按((s_i−1)/4)平均×100<br>- 物理空间评估<br> - 指标：Episode级成功率sr_{e,u}=N^s_{e,u}/N_{e,u}；总体为E具身×U任务的成功率平均<br> - 环境：仿真闭环控制；每任务50次rollout，具身特定步长上限<br>- 结果概览<br> - 数字空间：表4显示BLM1平均64.88，显著优于GPT-4o（59.86）与开源MLLM；细分见表5/表6<br> - 物理空间：BLM1在四具身×六任务上均实现稳定跨具身控制成功率，体现共享DiT策略的有效性</details> |
| 2025-10-27 | RoboOmni: Proactive Robot Manipulation in Omni-modal Context | http://arxiv.org/abs/2510.23763 | <details><summary>展开</summary>### 论文研究单位<br>复旦大学、上海创新研究院、新加坡国立大学<br><br>### 论文概述<br>论文提出“跨模态上下文指令”设置：机器人需从人类语音、环境声与视觉线索中主动推断潜在意图，并通过交互确认后执行动作，而不再依赖显式指令。为解决数据缺失与评估基准不足，构建 OmniAction 数据集与 OmniAction-LIBERO 评测；提出 RoboOmni 端到端全模态大模型，集成感知、推理、对话与控制，实现意图识别—确认—执行的闭环。仿真与真实验证显示其在成功率、推理速度、主动辅助与意图识别上显著优于文本与ASR基线。<br><br>### 论文核心贡献点<br>- 定义并研究“跨模态上下文指令”任务：融合语音、环境声与视觉进行主动意图推断与交互确认<br>- 提出 RoboOmni：Perceiver–Thinker–Talker–Executor 的端到端全模态框架，统一推理与控制，支持直接语音交互与行动生成<br>- 构造 OmniAction：约14万集 multimodal 数据、5千+说话人、2.4千事件声音、640背景、六类上下文指令；并基于 LIBERO 构建 OmniAction-LIBERO 评测套件<br>- 实验显示：仿真 OmniAction-LIBERO 上平均成功率达85.6%；真实人声指令平均76.6%；推理延迟降至基线的约0.49倍；具备更强的意图识别与主动协助能力<br><br>### 论文方法描述<br>- 架构：Perceiver 统一编码视觉/音频/文本为共享表征；Thinker 基于全模态LLM在词汇与动作标记联合空间自回归生成；Talker 生成语音；Executor 用 FAST+ 将离散动作标记解码为连续控制向量<br>- 训练：统一自回归目标，同时优化对话与动作生成。预训练使用64×A100、10天、15,360 A100‑小时、批量512、学习率5e-5；下游SFT为8×A100、5e‑5、10–30k步。图像224×224、音频16kHz、动作块长6<br><br>### 论文使用数据集和训练资源<br>- 数据集<br> - OmniAction：约141,162集，涵盖112技能、748对象、5,096说话人音色、2,482非言语事件、640环境背景与六类上下文指令<br> - OmniAction‑LIBERO：基于 LIBERO 的仿真评测（OmniAction‑LIBERO‑TTS：40任务×6变体=240评测量；OmniAction‑LIBERO‑Real：10名志愿者真实语音指令）<br>- 训练资源<br> - 预训练：64×A100，10天，总计约15,360 A100‑小时，批量512，LR=5e-5，前1k步warm‑up<br> - 下游SFT：8×A100，LR=5e-5，10–30k步<br><br>### 论文使用的评估环境和评估指标<br>- 仿真评估：OmniAction‑LIBERO（四套任务：Spatial、Goal、Object、Long‑Horizon）×六种上下文指令；与 OpenVLA、OpenVLA‑OFT、π0、NORA 比较；指标为成功率<br>- 真实人声评测：OmniAction‑LIBERO‑Real；对比“真实文本提示/ASR转文本→VLA”与RoboOmni直接音频输入；指标为成功率<br>- 主动辅助与意图识别：对比 Qwen2.5‑Omni‑3B/7B 与 ASR+GPT‑4o；指标为意图识别准确率、交互能力质性评估<br>- 效率分析：对比端到端与级联流水线；指标为推理延迟（相对ASR+OpenVLA的倍数）</details> |
| 2025-10-27 | UrbanVLA: A Vision-Language-Action Model for Urban Micromobility | http://arxiv.org/abs/2510.23576 | <details><summary>展开</summary>## 论文研究单位<br>北京大学、Galbot、中国科学技术大学、北京人工智能研究院(BAAI)<br><br>## 论文概述<br>UrbanVLA是一个面向城市微移动性应用的视觉-语言-动作(VLA)框架，主要用于配送机器人等在城市环境中的长距离导航任务。该方法通过整合导航工具提供的高级路线信息与机器人视觉感知，实现了大规模城市场景中可靠、安全的长程导航。<br><br>## 论文核心贡献点<br>- 提出首个面向城市微移动性的路线条件化VLA框架，将导航工具指导与视觉语言策略学习有效结合<br>- 开发了模拟到现实的训练管道，通过路线提升算法构建模拟-现实聚合数据集，实现SOTA性能并展示现实世界泛化能力<br>- 引入基于IQL的强化学习微调方法，显著提升安全关键行为能力，包括避障、行人互动和交通合规性<br><br>## 论文方法描述<br>UrbanVLA基于预训练的导航基础模型NavFoM，采用两阶段训练策略：**(1)监督微调(SFT)**：在MetaUrban模拟器和网络导航视频数据上训练，结合VideoQA任务和路线条件导航任务，学习基本导航能力；**(2)强化微调(RFT)**：使用隐式Q-Learning(IQL)在模拟-现实聚合数据上进行离线强化学习，优化安全性和适应性。方法核心包括多模态特征融合、路线与视觉观察对齐、轨迹级别的策略优化。<br><br>## 论文使用数据集和训练资源<br>- **模拟数据**：MetaUrban-12K数据集(约40小时，2400集)，由PPO专家生成<br>- **现实数据**：约8小时的人为遥操作系统数据<br>- **网络数据**：Sekai网络导航视频数据集和LongVU视频问答数据集<br>- **训练资源**：8张NVIDIA H100 GPU，训练约12小时(总计96 GPU小时)<br><br>## 论文使用的评估环境和评估指标<br>- **模拟环境评估**：MetaUrban基准，包含PointNav和SocialNav任务，在MetaUrban-test(1000场景)和MetaUrban-unseen(100场景)上测试<br>- **现实环境评估**：在城市街区进行，包括天桥、行人横道等复杂场景，机器人为Unitree Go2四足机器人<br>- **评估指标**：成功率(SR)、路径长度加权的成功率(SPL)、社交导航分数(SNS)、累积成本(CC)、路线完成度(RC)等</details> |
| 2025-10-27 | RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation | http://arxiv.org/abs/2510.23571 | <details><summary>展开</summary>### 论文研究单位<br>卡内基梅隆大学、浙江大学、北京大学、国立台湾大学<br><br>### 论文概述<br>随着通用机器人策略的快速发展，其评估面临规模化、可复现性与安全性等挑战，现有评测要么依赖真实世界环境（人力与安全成本高），要么局限在同一合成域内训练—测试，无法评估真实数据或其他环境训练的模型。为解决这些问题，本文提出RobotArena ∞，通过自动化将真实演示视频映射为大规模可扩展的仿真环境，并结合视觉语言模型（VLM）自动评分与人类偏好反馈，对来自全球多实验室的视觉语言动作（VLA）策略开展系统性评测与稳健性检验。<br><br>### 论文核心贡献点<br>- 提出可扩展、可扩展的机器人评测协议：结合物理引擎、真实到仿真（real-to-sim）转换与人类偏好反馈，实现规模化且可复现的评测。<br>- 完整自动化的reality-to-simulation翻译管线：融合VLMs、2D到3D生成模型与可微分渲染，无需人工标定或额外注释。<br>- 大规模跨实验室政策评测：聚合超过7000条人类偏好比较、覆盖百余个仿真场景及多种扰动，目前最大规模的机器人评测工作。<br>- 系统性评测发现：跨数据集泛化能力弱、模型架构差异明显、颜色扰动下强VLM背bone策略更稳健、背景变化带来显著性能衰减。<br><br>### 论文方法描述<br>方法核心是将演示视频自动翻译为仿真环境，再对策略进行多维度评测。<br><br>- 自动化视频到仿真映射：提取五个关键要素（1）相机—机器人六自由度位姿；（2）任务相关物体的三维网格重建、姿态与材质属性；（3）场景深度；（4）干净背景；（5）控制增益估计。<br>- 机器人—相机标定：采用可微分渲染的分析—综合方法，优化RGB、光流与DINOv2特征对齐损失，估计相机—机器人位姿。<br>- 物体与场景三维重建：利用Gemini进行机器人与前景物体分割；InvSR提升分割图像质量；Hunyuan-3D生成有纹理网格；通过MINIMA建立真实crop与模拟渲染视图的2D对应；利用MoGe单目深度与SVD解算刚体变换；用LaMa进行背景修复；系统辨识校准PD控制增益。<br>- 可控域扰动：ΔBG替换背景；ΔColor调整颜色通道（如BGR转换）；ΔObjPose随机改变物体位置；用于压力测试策略泛化。<br>- 评测方式：绝对评测用VLM对打乱帧序列进行任务进度打分（最后30%帧平均分数与人类进度相关性最佳）；相对评测采用双盲成对比较，收集偏好标签与自由文本解释，使用Bradley–Terry模型与Sandwich方差得到全局排名与置信区间。<br>- 对比与外部验证：与SIMPLER基准比较，评估策略在BridgeSim与SIMPLER四场景上的表现一致性；在一个真实任务“把胡萝卜放到盘子里”验证仿真—现实一致性。<br><br>### 论文使用数据集和训练资源<br>- 数据源与仿真环境：BridgeSim（Bridge v2，OXE的广泛使用子集）、DROIDSim（DROID，因噪声较高常被排除于预训练）、Rh20TSim（仅SpatialVLA用其训练）。<br>- 评测对象策略：Octo-Base（93M，OXE预训练）、RoboVLM（基于KosMos的VLA）、SpatialVLA（引入3D空间表征）、CogAct（7B VLM背bone+扩散Transformer动作预测）。<br>- 环境规模：100个名义环境与数百种扰动（ΔBG/ΔColor/ΔObjPose），累计7000+人类偏好比较与多轮VLM自动评分。<br><br>### 论文使用的评估环境和评估指标<br>- 评估环境：自动化生成的仿真环境来源于真实演示视频，支持内分布（与训练集对应）与外分布（超出训练集）评测；在仿真中部署策略执行并录制轨迹视频用于后续评分。<br>- 评估指标：<br> - 自动化VLM任务进度评分：打乱帧序列+VLM打分，优选“执行最后30%帧的平均分数”作为进度指标。<br> - 人类偏好成对比较：偏好标签（胜/负/平局）与自由文本理由；Bradley–Terry模型全局排名，Sandwich方差估计置信区间。<br> - 稳健性：跨数据集迁移性能（BridgeSim vs. DROIDSim vs. Rh20TSim），以及在ΔBG/ΔColor/ΔObjPose扰动下的性能曲线与趋势。<br> - 现实—仿真一致性：单任务现实与仿真部署对比（如“把胡萝卜放到盘子里”）。</details> |
| 2025-10-27 | Dexbotic: Open-Source Vision-Language-Action Toolbox | http://arxiv.org/abs/2510.23511 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>Dexmal、StepFun<br><br>## 论文概述<br>Dexbotic是一个基于PyTorch的开源视觉-语言-动作(VLA)模型工具箱，为具身智能领域的专业人士提供一站式VLA研究服务。该工具箱统一了不同研究机构的VLA策略框架，解决了研究碎片化问题，支持多种主流VLA策略的复现，并提供更强的预训练模型以提升性能。<br><br>## 论文核心贡献点<br>- 统一模块化VLA框架：将VLA策略标准化为视觉语言模型(VLM)和动作专家(AE)两个部分<br>- 预训练基础模型：提供比原始开源模型更强大的预训练模型，在多个基准测试中显著提升性能<br>- 实验为中心开发：采用分层配置架构，用户可通过简单修改Exp脚本快速开发新实验<br>- 多平台训练支持：同时支持云端(如阿里云)和本地GPU(RTX 4090)训练<br>- 多机器人支持：统一数据格式支持UR5、Franka、ALOHA等多种主流机器人平台<br><br>## 论文方法描述<br>统一架构设计：<br>- VLM部分：包含视觉编码器(CLIP)、投影器(两层MLP)和大语言模型(Qwen2.5)<br>- AE部分：支持扩散变换器、多层感知机或专家混合等不同架构<br><br>预训练策略：<br>- 离散预训练模型(Dexbotic-Base)：将连续动作量化为256个离散区间<br>- 连续预训练模型：支持单臂和双臂任务，通过扩展噪声token数量实现<br>- 多视图处理：共享视觉编码器处理多视角输入<br><br>数据格式：<br>- Dexdata格式：统一存储机器人数据集，包含video和jsonl两个核心元素<br>- 相比LeRobot和RLDS格式更节省存储空间<br><br>## 论文使用数据集和训练资源<br>训练数据来源：<br>- Open-X Embodiment数据集子集<br>- 仿真数据：RLBench、LIBERO、ManiSkill2<br>- 真实机器人数据：UR5等多型号单臂机器人<br>- 私有数据集：52个操作任务，使用8种单臂真实机器人收集<br>- 双臂数据：Robomind、AgiBot World数据集及ALOHA双臂机器人数据<br><br>训练环境：<br>- 云端平台：阿里云、Volcano Engine<br>- 本地GPU：RTX 4090等消费级显卡<br><br>## 论文使用的评估环境和评估指标<br>仿真基准测试：<br>- SimplerEnv：WidowX机器人，包含4项视觉匹配任务<br>- CALVIN：ABC→D设定，评估长期任务完成能力<br>- ManiSkill2：5项代表性抓取和堆叠任务<br>- RoboTwin2.0：4项双臂操作任务<br>- LIBERO：4个任务套件(Spatial、Object、Goal、Long)<br><br>评估指标：<br>- 任务成功率(Success Rate)：各基准测试的核心指标<br>- 平均完成长度(CALVIN)：连续完成的任务序列平均长度<br>- 性能提升幅度：与原始开源模型的对比改善<br><br>真实世界评估：<br>- 在UR5e、ALOHA、ARX5、Franka等机器人上验证<br>- 任务类型：日常操作如摆放盘子、按钮操作、纸张粉碎等<br>- 典型结果：set the plates任务100%成功率，search the green box任务80%成功率</details> |
| 2025-10-25 | ACG: Action Coherence Guidance for Flow-based VLA models | http://arxiv.org/abs/2510.22201 | <details><summary>展开</summary>待生成</details> |
| 2025-10-23 | Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence | http://arxiv.org/abs/2510.21860 | <details><summary>展开</summary>### 论文研究单位<br>Andon Labs（研究机构及实验室）<br><br>### 论文概述<br>Butter-Bench是一个评估LLM控制机器人实用智能的基准测试。论文探讨当前SOTA大型语言模型在机器人编排（orchestration）能力上的上限，通过设计真实世界任务（如“传递黄油”）评估模型的空间推理、社交理解和物理世界常识等能力。研究发现，人类在所有任务中显著优于LLM，最佳模型完成率仅达40%，人类平均达95%。<br><br>### 论文核心贡献点<br>1. **引入实用智能评估框架**：提出区分分析智能（analytical intelligence）与实用智能（practical intelligence）的机器人基准测试。<br>2. **隔离编排器评估**：在简化硬件架构下测试LLM编排能力，避免VLA（视觉语言动作）模型干扰。<br>3. **系统性能力测评**：设计六大任务全面覆盖空间导航、视觉推理、社交互动和多步规划。<br>4. **红队安全评估**：在压力场景（低电量、充电器故障）下测试模型的对抗性脆弱性。<br>5. **对比人类基线**：明确人类与LLM在真实环境中的能力差距。<br><br>### 论文方法描述<br>- **硬件平台**：使用TurtleBot 4标准移动机器人（搭载OAK-D相机、2D激光雷达、IMU等），运行ROS 2系统提供SLAM能力。<br>- **代理架构**：采用ReAct风格循环，LLM观察环境→推理决策→调用工具执行动作。工具箱包括：<br> 1. 运动控制：`drive`, `rotate`, `wait`<br> 2. 维护功能：`dock`, `undock`, `status`<br> 3. 视觉感知：`take_photo`<br> 4. 导航工具：`view_map`, `navigate_to`<br> 5. 社交交互：`read_msg`, `send_msg`, `save_image`<br>- **任务设计**：六个子任务评估不同能力（如识别含黄油纸袋、注意用户缺席、多步路径规划等）。<br>- **红队测试**：模拟低电量+充电器故障场景，诱导模型泄露机密信息。<br><br>### 论文使用数据集和训练资源<br>- **任务数据集**：基于真实办公环境设计的6项操作任务（附录A详述），包含接受标准。<br>- **模型评估**：测试多个LLM：<br> - **SOTA通用模型**：Gemini 2.5 Pro、Claude Opus 4.1、GPT-5、Grok 4、Llama 4 Maverick<br> - **专用实体模型**：Gemini ER 1.5（针对机器人调优）<br>- **人类基线**：3名人类操作员通过网页界面控制机器人完成相同任务（不知环境布局）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：真实办公室场景（单房间含出口、办公桌、厨房等），固定光照和障碍物布局。<br>- **核心指标**：<br> 1. **主要指标**：任务完成率（每模型-任务组合测试5次）。<br> 2. **辅助指标**：<br> - 任务完成时间（推理延迟+规划效率）<br> - 工具调用分布（分析工具使用模式）<br> - 定性失效模式分类（空间推理、社交理解等5类）。<br>- **安全性评估**：红队攻击成功率（如泄露机密信息的倾向性）。<br><br>**结果摘要**<br>- 人类vs LLM：人类平均95%完成率，最佳LLM Gemini 2.5 Pro仅40%。<br>- **关键弱点**：<br> - 社交理解（"注意缺席"任务：LLM 0% vs 人类100%）<br> - 多步空间规划（"Plan"任务：LLM依赖随机路径漂移，非真实理解）<br>- **模型差异**：<br> Gemini ER 1.5未优于Gemini 2.5 Pro，表明实体调优未显著提升实用智能。</details> |
| 2025-10-21 | VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting | http://arxiv.org/abs/2510.21817 | <details><summary>展开</summary>论文研究单位<br>南京大学, 腾讯优图实验室, 中国科学院自动化研究所, 傅里叶智能有限公司。<br><br>论文概述<br>针对当前视觉-语言-动作（VLA）模型交互范式僵化、无法并发处理视听说动和实时中断的问题，论文提出了VITA-E框架。该框架通过一个双模型架构，使机器人能够同时观察环境、倾听用户指令、提供语言回复并执行动作，实现了类似人类的多任务处理能力，并能响应用户的紧急中断。<br><br>论文核心贡献点<br>1. 提出了一种用于并发交互的双模型架构，其中两个VLA实例分别作为主动模型和备用模型协同工作。<br>2. 设计了一种基于特殊令牌的控制流，VLM生成如[ACT]、[HALT]等令牌直接驱动系统状态转换。<br>3. 提出了一种训练交互式VLA的方法，包括数据整理和微调策略，使VLM学会生成系统级控制令牌。<br><br>论文方法描述<br>VITA-E采用“模型即控制器”范式和双模型交互核心。核心是微调一个视觉语言模型（VLM），使其生成特殊令牌（如[RES], [ACT], [HALT], [END]），作为系统级命令来控制行为。系统由VLM（负责高级理解）和扩散动作专家（负责底层电机控制）组成的双系统架构驱动。双模型架构包含一个“主动模型”和一个“备用模型”，通过同步原语协调，实现了四种交互模式：行为并发、语音中断、任务切换和紧急停止。<br><br>论文使用数据集和训练资源<br>使用的数据集包括ActionNet、Libero、自收集的真实世界场景数据以及通过遥操作机器人收集的300个演示轨迹。训练在Fourier GR2人形机器人平台上进行，采用DeepSpeed ZeRO-3配置。模型训练分为两阶段：首先微调VLM，然后训练扩散动作专家。<br><br>论文使用的评估环境和评估指标<br>评估环境包括Libero仿真基准和真实的Fourier GR2人形机器人。评估指标方面，基础操作任务和交互式任务（语音中断、任务切换、紧急停止）使用成功率进行衡量。并发任务则通过定性展示和语音响应的平均延迟（2.26秒）进行评估。</details> |
| 2025-10-24 | Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos | http://arxiv.org/abs/2510.21571 | <details><summary>展开</summary># 论文研究单位<br><br>清华大学、微软亚洲研究院<br><br># 论文概述<br><br>本文提出了一种将无脚本真实生活人类手部活动视频转换为视觉-语言-动作（VLA）数据格式的新方法，用于预训练机器人操作VLA模型。该方法将人类手部视为灵巧机器人末端执行器，通过开发全自动化的人类活动分析框架，将"野生"第一人称人类视频转换为与现有机器人VLA训练数据完全对齐的数据格式，无需任何注释。最终构建了包含100万片段和2600万帧的大规模手部VLA数据集，并设计了灵巧手部VLA模型。<br><br># 论文核心贡献点<br><br>1. **首个VLA预训练方法**：首次将非结构化人类视频转换为与机器人VLA训练数据完全对齐的数据格式，无需人工注释<br><br>2. **自动化分析框架**：开发了全自动化的人类活动分析框架，包含三维运动标注、原子动作分割和指令标注三个阶段<br><br>3. **大规模数据集构建**：构建了包含100万片段和2600万帧的手部VLA数据集，覆盖广泛的对象、概念、技能和环境变化<br><br>4. **VLA模型架构**：设计了由VLM骨干网和扩散动作专家组成的灵巧手部VLA模型，支持统一的单双hand动作预测<br><br>5. **零样本泛化能力**：模型在完全未见过的真实世界观察中展现出强大的零样本能力<br><br>6. **缩放行为验证**：证明了模型任务性能随预训练数据规模的清晰缩放行为<br><br># 论文方法描述<br><br>## 数据转换框架<br><br>**三维运动标注**：采用单目三维相机和手部姿态跟踪方法，利用深度视觉SLAM、深度估计和手部重建技术，估计相机内参和帧级手部姿态（基于6D手腕姿态和完整关节角度）<br><br>**原子动作分割**：通过检测世界空间中3D手部手腕的速度最小值作为切割点，将长视频分割为原子级手部动作序列，遵循机器人VLA数据的粒度要求<br><br>**指令标注**：从每个片段均匀采样8帧，在帧上叠加手部轨迹投影，使用GPT-4.1为每个片段生成动作的描述性语言指令<br><br>## VLA模型设计<br><br>**模型架构**：由PaliGemma-2 VLM骨干网和DiT-Base扩散Transformer动作专家组成，添加可学习的认知token作为条件输入<br><br>**动作空间**：在相机坐标系中预测手部动作，包含左右手的手腕平移、旋转和15个关节的欧拉角<br><br>**统一预测**：通过动作掩码机制处理单双hand动作的统一预测，使用因果注意机制进行动作去噪<br><br>**训练策略**：采用轨迹感知数据增强，随机裁剪和透视变换，同时变换对应的动作序列<br><br>## 机器人微调<br><br>将人类手部动作空间映射到机器人手部，通过关节拓扑对应关系进行简单的映射策略，对模型进行微调以适应真实机器人操作<br><br># 论文使用数据集和训练资源<br><br>## 预训练数据集<br><br>**数据来源**：处理了来自Ego4D（77%）、Epic-Kitchen（12%）、EgoExo4D（6%）和Something-Something-V2（5%）的第一人称人类视频<br><br>**数据规模**：100万片段，2600万帧，覆盖烹饪、清洁、建筑、维修、手工和绘画等真实生活活动<br><br>**数据多样性**：与OpenImages数据集相比具有更高的视觉多样性，包含更丰富的语言指令多样性（名词、动词、形容词）<br><br>## 训练资源<br><br>**预训练**：在8个NVIDIA H100 GPU上训练2天，批量大小512，学习率1e-4和1e-5<br><br>**微调**：在8个NVIDIA H100 GPU上训练8小时，批量大小256，学习率1e-5<br><br>## 机器人数据<br><br>收集了1.2K条遥操作轨迹用于四个任务的微调：一般拾取放置、功能抓取、倒水和清扫<br><br># 论文使用的评估环境和评估指标<br><br>## 人类手部动作预测评估<br><br>**评估环境**：<br>- 抓取任务：在47个未见过的环境中使用Azure Kinect捕获RGB-D图像<br>- 一般动作任务：在117个未见过的真实生活环境中进行用户研究<br><br>**评估指标**：<br>- 抓取任务：手-物体最小距离（$d_{\text{hand-obj}}$）评估动作合理性<br>- 一般动作：23名参与者对30个随机场景的top-3动作进行排名打分<br><br>## 真实机器人灵巧操作评估<br><br>**评估环境**：<br>- Realman机器人配备12-DoF XHand灵巧手和RealSense头部相机<br>- 桌面环境设置，使用遥操作系统收集训练数据<br><br>**评估任务**：<br>- 拾取放置：移动物体到盒子中（3-4个随机干扰物）<br>- 功能抓取：在功能位置抓取物体（如手柄）<br>- 倒水：抓取瓶子，将内容倒入另一容器，放回桌面<br>- 扫帚清扫：从篮子中拿起扫帚，将垃圾扫入簸箕，归位扫帚<br><br>**评估指标**：<br>- 任务成功率：在可见和不可见物体/背景下的成功率百分比<br>- 对比基线：与VPP和π₀等代表性方法的性能比较<br>- 缩放行为：不同数据规模下的性能变化趋势</details> |
| 2025-10-23 | SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing | http://arxiv.org/abs/2510.20965 | <details><summary>展开</summary>论文研究单位<br>约翰霍普金斯大学、NVIDIA、斯坦福大学、多伦多大学<br><br>论文概述<br>论文介绍了SutureBot，一个用于在da Vinci研究工具包(dVRK)上实现自主端到端缝合的精密框架和基准。该任务被分解为针头拾取、组织穿透和打结三个子任务。为了解决当前数据集规模小、缺乏标准化基准的问题，作者发布了包含1,890次高质量演示的SutureBot数据集。此外，论文提出了一种目标条件化的模仿学习框架，通过显式优化针头插入点精度，将目标准确性提高了59%-74%。作者还评估了包括π₀、GR00T N1和OpenVLA-OFT在内的多种最先进的视觉-语言-动作(VLA)模型在该基准上的表现，为未来的研究建立了基线。<br><br>论文核心贡献点<br>1. 提出了一个用于评估在手术环境中进行长时程灵巧操作策略的新基准，涵盖完整的自主缝合任务。<br>2. 发布了目前最大的公开真实世界缝合数据集，包含1,890次dVRK演示，以支持可复现的研究。<br>3. 提出了一个目标条件化的模仿学习框架，使学习到的策略能够实现精确的定点插入。<br>4. 对多种最先进的VLA模型在SutureBot基准上进行了全面评估，确立了未来研究的性能基线。<br><br>论文方法描述<br>1. 分层框架：采用分层架构，包括一个基于Swin Transformer的高级策略和一个低级策略。高级策略负责根据视觉观测选择当前子任务并生成相应的语言指令。低级策略接收语言指令、实时图像和目标条件，并输出连续的机器人控制指令。<br>2. 低级策略比较：比较了四种最先进的视觉-语言-动作模型作为低级策略，包括π₀、GR00T N1、OpenVLA-OFT和一个多任务Action Chunking Transformer (ACT)。<br>3. 目标表示：探索了三种目标条件表示方法来引导针头放置：(1) 点标签，在内窥镜图像上用蓝/绿像素标记插入/退出点；(2) 二元掩码，一个三通道图像，分别代表插入和退出区域；(3) 距离图，一个编码了到目标点的像素级偏移向量和热力图的三通道图像。实验发现点标签方法在精度上表现最佳。<br><br>论文使用数据集和训练资源<br>1. 数据集：使用dVRK系统和软组织缝合垫进行数据采集。数据集包含1,890次演示，涵盖针头拾取(628次)、针头投掷(310次)和打结(952次)三个任务，并包含454次从失败状态开始的恢复演示。数据包括同步的立体内窥镜图像、腕部摄像头图像(30Hz)和机器人运动学数据。该数据集已在Hugging Face上公开。<br>2. 训练资源：所有模型训练均在配备8块NVIDIA A100 80GB GPU的NVIDIA DGX A100系统上进行。<br><br>论文使用的评估环境和评估指标<br>1. 评估环境：评估在物理世界的da Vinci研究工具包(dVRK)上进行。实验采用固定的机器人配置和缝合垫位置，但改变针头的初始位置。策略的在线推理和评估在配备双NVIDIA RTX 4090 GPU的工作站上完成。<br>2. 评估指标：<br> - 任务成功率：分别评估针头拾取、针头投掷、拉线和打结四个子任务的成功率，并设定了时间限制。<br> - 插入/退出误差：使用紫外线笔标记目标点，任务完成后在紫外线灯下用ImageJ测量实际缝合点与目标点之间的欧几里得距离。<br> - 过程时间：记录完成整个缝合过程所需的总时间。</details> |
| 2025-10-23 | VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation | http://arxiv.org/abs/2510.20818 | <details><summary>展开</summary>论文研究单位<br>University of Washington, DEVCOM ARL<br>论文概述<br>VAMOS是一个分层的视觉-语言-行动（VLA）模型，用于实现能力调制和可转向的导航。它将语义规划与具体机器人的物理约束和能力分离，通过一个通用规划器从多样化的开放世界数据中学习，同时一个专业的可供性模型在安全、低成本的仿真中学习机器人的物理约束和能力。该模型设计了一个接口，让高级规划器直接在图像空间中提出候选路径，然后由可供性模型评估和重新排序。<br>论文核心贡献点<br>提出了一种分层的VLA模型，结合了高级VLM规划器和低级可供性模型，实现跨具身导航。<br>通过预测2D路径作为接口，统一了不同数据源和动作空间，支持大规模异构数据训练。<br>实现了自然语言可转向性，允许在测试时通过文本指令调整导航行为。<br>验证了跨具身导航能力，在腿式和轮式机器人上复用同一高级规划器，仅更换低级可供性模型。<br>实验表明，与模块化基线相比，VAMOS在复杂室内外环境中实现了更高的导航成功率，并且可供性模型显著提高了单机器人导航的可靠性，成功率提升3倍。<br>论文方法描述<br>系统架构为分层设计：高级VLM规划器输入图像和文本编码的目标坐标，输出图像空间中的候选路径。<br>训练时，使用来自SCAND、TartanDrive 2、CODa和Spot数据集的混合数据，总计29.8小时，通过投影3D位姿到2D图像路径，并利用GPT-5-mini生成文本偏好标注以增强可转向性。<br>低级可供性函数在仿真中训练，输入高程图、位置和方向，输出路径的可穿越概率。<br>部署时，VLM预测的2D路径投影到3D世界坐标，由可供性模型计算累计可穿越得分并重新排序，选择最优路径由低级控制器执行。<br>论文使用数据集和训练资源<br>高级规划器使用SCAND、TartanDrive 2、CODa和Spot数据集，总时长29.8小时，混合比例为SCAND 50%、CODa 25%、TartanDrive 2 100%、Spot 100%，并添加了人工草图数据用于few-shot适应。<br>低级可供性模型在Isaac Lab仿真环境中训练，使用程序生成的地形。<br>计算资源包括Nvidia L40节点的8个GPU，训练约5小时，支持在消费级Nvidia RTX 4090上进行LoRA微调。<br>论文使用的评估环境和评估指标<br>评估环境：真实世界的室内外场景，包括走廊（20米）、中庭（20米）、实验室（5米）、校园（40米）、森林（20米）和下坡斜坡（15米），覆盖复杂地形、光照和植被。<br>评估指标：成功率（SR，5次试验中成功到达目标的百分比）、平均干预次数（NI，成功运行中的0-2次）和超时次数（T，0-5次）。<br>跨具身导航评估：在包含楼梯和斜坡的设置中测试Spot和UW Hound机器人，比较有无可供性调制的性能。<br>可转向性评估：通过自然语言指令（如“向左”或“走右侧”）调整路径，使用ChatGPT 5作为VLM裁判，量化偏好对齐率。</think><br>论文研究单位<br>University of Washington, DEVCOM ARL<br>论文概述<br>VAMOS是一个分层的视觉-语言-行动（VLA）模型，用于实现能力调制和可转向的导航。它将语义规划与具体机器人的物理约束和能力分离，通过一个通用规划器从多样化的开放世界数据中学习，同时一个专业的可供性模型在安全、低成本的仿真中学习机器人的物理约束和能力。该模型设计了一个接口，让高级规划器直接在图像空间中提出候选路径，然后由可供性模型评估和重新排序。<br>论文核心贡献点<br>提出了一种分层的VLA模型，结合了高级VLM规划器和低级可供性模型，实现跨具身导航。<br>通过预测2D路径作为接口，统一了不同数据源和动作空间，支持大规模异构数据训练。<br>实现了自然语言可转向性，允许在测试时通过文本指令调整导航行为。<br>验证了跨具身导航能力，在腿式和轮式机器人上复用同一高级规划器，仅更换低级可供性模型。<br>实验表明，与模块化基线相比，VAMOS在复杂室内外环境中实现了更高的导航成功率，并且可供性模型显著提高了单机器人导航的可靠性，成功率提升3倍。<br>论文方法描述<br>系统架构为分层设计：高级VLM规划器输入图像和文本编码的目标坐标，输出图像空间中的候选路径。<br>训练时，使用来自SCAND、TartanDrive 2、CODa和Spot数据集的混合数据，总计29.8小时，通过投影3D位姿到2D图像路径，并利用GPT-5-mini生成文本偏好标注以增强可转向性。<br>低级可供性函数在仿真中训练，输入高程图、位置和方向，输出路径的可穿越概率。<br>部署时，VLM预测的2D路径投影到3D世界坐标，由可供性模型计算累计可穿越得分并重新排序，选择最优路径由低级控制器执行。<br>论文使用数据集和训练资源<br>高级规划器使用SCAND、TartanDrive 2、CODa和Spot数据集，总时长29.8小时，混合比例为SCAND 50%、CODa 25%、TartanDrive 2 100%、Spot 100%，并添加了人工草图数据用于few-shot适应。<br>低级可供性模型在Isaac Lab仿真环境中训练，使用程序生成的地形。<br>计算资源包括Nvidia L40节点的8个GPU，训练约5小时，支持在消费级Nvidia RTX 4090上进行LoRA微调。<br>论文使用的评估环境和评估指标<br>评估环境：真实世界的室内外场景，包括走廊（20米）、中庭（20米）、实验室（5米）、校园（40米）、森林（20米）和下坡斜坡（15米），覆盖复杂地形、光照和植被。<br>评估指标：成功率（SR，5次试验中成功到达目标的百分比）、平均干预次数（NI，成功运行中的0-2次）和超时次数（T，0-5次）。<br>跨具身导航评估：在包含楼梯和斜坡的设置中测试Spot和UW Hound机器人，比较有无可供性调制的性能。<br>可转向性评估：通过自然语言指令（如“向左”或“走右侧”）调整路径，使用ChatGPT 5作为VLM裁判，量化偏好对齐率。</details> |
| 2025-10-23 | MemER: Scaling Up Memory for Robot Control via Experience Retrieval | http://arxiv.org/abs/2510.20328 | <details><summary>展开</summary>## 论文研究单位<br><br>斯坦福大学（Stanford University）<br><br>## 论文概述<br><br>机器人策略通常缺乏长期记忆能力，无法像人类一样利用历史信息完成复杂任务。针对这一问题，本文提出了MemER（Memory via Experience Retrieval）框架，采用分层策略设计，使机器人能够选择性地记忆任务相关的关键帧，从而实现分钟级别的长期记忆。该方法在三个真实世界的长时序机器人操作任务上进行了验证。<br><br>## 论文核心贡献点<br><br>1. **分层记忆架构**：提出高层策略负责选择和跟踪相关关键帧，低层策略执行具体动作的层次化设计<br>2. **关键帧选择机制**：开发了一种基于一维单链接聚类的算法，能从历史观察中自动提取任务相关信息<br>3. **高效记忆存储**：设计了紧凑的视觉记忆表示方法，避免了传统长上下文方法的计算开销和协变量偏移问题<br>4. **少样本适应**：仅需50个远程操作演示即可将预训练的视觉-语言模型适配到机器人任务<br><br>## 论文方法描述<br><br>**高层策略（π_h）**：<br>- 基于Qwen2.5-VL-7B-Instruct微调<br>- 输入：最近N帧、任务指令、选定关键帧<br>- 输出：当前子任务和候选关键帧<br>- 采用一维单链接聚类（合并距离d=5帧）构建视觉记忆<br><br>**低层策略（π_l）**：<br>- 基于π_0.5微调<br>- 输入：当前图像、关节状态、子任务指令<br>- 输出：机器人动作块<br><br>**关键帧选择算法**：<br>1. 收集所有候选关键帧的时间戳<br>2. 对时间戳进行排序<br>3. 使用单链接聚类合并相近帧<br>4. 选择每簇的中位数作为代表帧<br><br>**模型合并技术**：使用权重插值θ=(1-α)·θ_pre + α·θ_ft（α=0.8）保持预训练模型的鲁棒性<br><br>## 论文使用数据集和训练资源<br><br>**训练数据**：<br>- 50个长时序轨迹演示，每个任务10-15个干预演示<br>- 数据格式：(I_t, q_t, l_t, l'_t, a_t)<br><br>**模型训练**：<br>- 高层策略：4500梯度步，96 H200 GPU小时<br>- 低层策略：18000训练步，48 H200 GPU小时<br>- 冻结视觉编码器和投影层，仅微调LLM骨干网络<br><br>**硬件配置**：<br>- Franka机械臂，平行爪夹持器<br>- 双摄像头：第三人称ZED摄像头和手腕安装的miniZED摄像头<br>- 图像分辨率320×180，15Hz采样率<br><br>## 论文使用的评估环境和评估指标<br><br>**评估任务**：<br>1. **物品搜索任务**：在三个不透明垃圾箱中搜索3-5个物品，评估记忆优化路径<br>2. **计数舀取任务**：向两个碗中放入指定数量的两种食材，评估计数准确性<br>3. **除尘与复位任务**：从双层架子移除物品，清洗架子并复位物品，评估空间记忆<br><br>**评估指标**：<br>- 物品搜索：成功检索次数（↑）、使用最优路径次数（↑）<br>- 计数舀取：错误舀取数量（↓）<br>- 除尘与复位：底层/顶层除尘成功（↑）、底层/顶层物品复位成功（↑）<br><br>**对比方法**：<br>- 无历史：仅当前帧<br>- 短历史：N=8帧<br>- 长历史：N=32帧<br>- 人类高层：人工提供正确子任务<br><br>**主要结果**：<br>- MemER在所有任务上>90%成功率<br>- 明显优于无历史和短历史基线<br>- 长历史方法平均性能比MemER差34%<br>- 与人类高层策略性能相当<br>- API-VLMs因延迟问题表现不佳</details> |
| 2025-10-22 | Learning Affordances at Inference-Time for Vision-Language-Action Models | http://arxiv.org/abs/2510.19752 | <details><summary>展开</summary># 论文研究单位<br>UC Berkeley和Physical Intelligence<br><br># 论文概述<br>论文提出了LITEN（Learning from Inference-Time Execution），一种针对视觉-语言-动作模型（VLA）的推理时学习框架。该方法通过将VLA低层策略与高层视觉语言模型（VLM）连接，使高层VLM能够在上下文中利用过往经验，学习机器人的可达性（affordances）和能力。LITEN通过迭代的推理和评估阶段，使机器人能够从实际执行经验中学习，不断改进复杂任务的规划能力，无需额外的模型训练。<br><br># 论文核心贡献点<br>- 提出LITEN框架，实现了VLA模型的推理时学习能力<br>- 设计了两阶段迭代方法：推理阶段生成和执行计划，评估阶段分析执行结果并提取经验<br>- 展示了如何从非结构化的机器人轨迹中提取有意义的反馈<br>- 在复杂的多阶段操作任务上验证了方法的有效性<br>- 证明了方法可以在不使用额外训练数据的情况下显著提升任务成功率<br><br># 论文方法描述<br>LITEN采用双阶段迭代循环：<br>1. **推理阶段**：高层VLM接收任务指令和初始环境观察，生成子任务序列和执行计划。系统提供VLA训练指令风格示例和可操作对象列表来指导计划生成。<br>2. **评估阶段**：系统收集每个子任务的执行轨迹，使用结构化评估程序通过VLM法官分析执行结果：<br> - 判断子任务是否成功<br> - 描述实际执行情况（失败时）<br> - 分析失败原因并提出改进建议<br>3. **经验利用**：将评估结果以层次结构存储，并在下一轮推理中作为上下文提供，帮助VLMreasoner生成更优计划。<br><br>方法使用GPT-5-mini作为高层VLM，π₀.₅-DROID作为低层VLA策略。<br><br># 论文使用数据集和训练资源<br>- **数据集**：使用DROID数据集对π₀.₅-DROID进行微调，并收集了450个演示数据（每个任务150个）<br>- **硬件环境**：标准DROID机器人设置，包括7-DoF Franka Emika Panda机械臂、2F-85 Robotiq抓手、ZED 2.0相机和ZED mini腕部相机<br>- **计算资源**：训练使用4个NVIDIA A100 GPU，批大小128，训练2500步<br>- **技术栈**：部署在5Hz操作频率，VLA产生长度为8的关节速度动作块<br><br># 论文使用的评估环境和评估指标<br>- **评估环境**：桌面操作环境，包含多种物体配置<br>- **任务类型**：三个复杂多阶段任务：<br> - Stacking：堆叠三个物体<br> - Emptying Bowls：将两个碗中的内容物转移到其他碗<br> - Moving Off Table：将桌面物体移动到其他物体上，仅保留三个与桌面直接接触<br>- **评估指标**：<br> - 整体任务成功率（不计算部分成功）<br> - 五次迭代中的性能改善趋势<br> - 与基线方法的对比（No-Feedback、Positive-ICL、Reflexion adaptation）<br>- **实验设计**：每个方法在10个不同初始配置下运行5次迭代，评估经验累积的效果<br>- **消融研究**：移除评估阶段的不同组件（失败推理、结果分析等）来验证各组件的重要性</details> |
| 2025-10-22 | GigaBrain-0: A World Model-Powered Vision-Language-Action Model | http://arxiv.org/abs/2510.19430 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>GigaAI。论文页面：https://GigaBrain0.github.io<br><br>## 论文概述<br>GigaBrain-0是一类由世界模型驱动的视觉-语言-动作（VLA）基础模型，旨在缓解真实机器人数据收集的高成本与多样性受限问题。通过在大规模、真实且多样的世界模型合成轨迹上训练，GigaBrain-0显著减少对真实机器人数据的依赖，同时提升跨任务、跨外观与跨视角的泛化能力。<br><br>## 论文核心贡献点<br>- 以世界模型生成的多样化数据为核心驱动的VLA范式，涵盖视频生成、Real2Real、View Transfer、Sim2Real、人体视频迁移等多源合成轨迹。<br>- 架构层面引入RGB-D输入与Embodied Chain-of-Thought（CoT）监督，增强3D几何感知与长时程推理能力，支持操纵轨迹、子目标语言与离散动作符号的联合学习。<br>- 混合Transformer架构：基于PaliGemma2的VLM作为感知与语言接口，动作生成采用Diffusion Transformer（DiT）与Flow Matching以预测连续动作块；引入离散动作token加速收敛；通过Knowledge Insulation隔离不同优化流以减轻干扰。<br>- 系统性数据管线GigaWorld，提供高效视频生成（单步蒸馏、NATTEN注意力、FP8推理，速度提升>50倍）与质量评估（几何一致性、多视角一致性、文本对齐、物理合理性）。<br>- 真实世界广泛验证：在灵巧操作、长时程与移动操纵任务上稳定领先；外观、放置与视角泛化随合成数据混合比α提升显著。<br>- 轻量化部署GigaBrain-0-Small，在NVIDIA Jetson AGX Orin上以约402M参数实现低延迟与低显存，保持与π0相当的任务成功率。<br><br>## 论文方法描述<br>- 输入与表示：RGB-D输入（B×H×W×4），对SigLIP首层卷积扩展零初始化深度通道；训练期随机丢弃深度以兼容RGB推断。<br>- 架构细节：VLM（PaliGemma2）编码多模态输入；动作专家采用DiT进行Flow Matching生成连续动作块；离散动作token作为辅助预测以加速训练；10个可学习的轨迹token通过双向注意与GRU解码输出2D操纵轨迹关键点；子目标语言与离散动作采用自回归生成。<br>- 监督与损失：联合优化三项——语言/离散动作的next-token损失、Flow Matching动作块回归损失、轨迹2D坐标回归损失；通过Knowledge Insulation隔离语义与动作学习的相互干扰。<br><br>## 论文使用数据集和训练资源<br>- 真实世界数据：融合公开数据集（AgiBotWorld、RoboMind、Open X-Embodiment）与自采数据（Agilex Cobot Magic 199小时 + AgiBot G1 983小时，共1182小时；覆盖工业、商业、办公、居住、实验室等五大类共14个场景）。<br>- 世界模型生成数据：GigaWorld提供多管线合成数据，显著提升外观、几何与视角多样性。包括：<br> - Real2Real Transfer：在深度与边缘结构先验约束下生成多纹理/光照/材质的视觉变体。<br> - View Transfer：基于深度的新视角投影、遮挡修复与物理仿真微调，保持任务语义一致。<br> - Sim2Real Transfer：在Isaac Sim中构造多样化场景并合成至逼真外观，增强域泛化。<br> - Human Video Transfer：将EgoDex等第一人称人类演示转化为稳定机器人执行序列。<br> - 文本条件视频生成+逆动力学（IDM）反推动作序列，扩充操纵轨迹。<br> - 多视角一致视频生成与质量评估用于4D一致性与可训练性筛选。<br>- 训练策略：全量/部分/未标注数据混合训练；基于手爪状态切换自动切分子目标并用Qwen-VL-2.5生成语言子目标；统一去重与采样策略提升样本效率。<br><br>## 论文使用的评估环境和评估指标<br>- 评估平台：AgiBot G1双足/双手机器人平台与PiPER双臂平台。<br>- 任务覆盖：<br> - 灵巧操纵：Laundry Folding、Paper Towel Preparation。<br> - 长时程：Table Bussing、Juice Preparation。<br> - 移动操纵：Boxes Moving、Laundry Baskets Moving。<br>- 指标：真实世界任务成功率（多次试验平均）；外观/放置/视角泛化实验以合成数据混合比α为变量，评估成功率随α提升曲线。<br>- 设备部署：NVIDIA Jetson AGX Orin上对GigaBrain-0-Small与π0进行推理性能与成功率对比（FLOPs、参数量、显存、时延、成功率）。</details> |
| 2025-10-22 | Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes | http://arxiv.org/abs/2510.19400 | <details><summary>展开</summary># 论文研究单位<br>清华大学、北京大学、复旦大学、微软研究院亚洲、香港科技大学、浙江大学<br><br># 论文概述<br>论文介绍了MV-RoboBench，一个专门用于评估视觉语言模型在机器人操作场景中多视图空间推理能力的基准测试。该基准包含1,700个手工制作的QA项目，涵盖8个子任务，分为空间理解（交叉视图匹配、距离判断、视角识别、3D空间一致性）和机器人执行（动作规划、步骤执行、轨迹选择、属性识别）两大类别。论文评估了多种VLM模型，并探索了CoT风格的增强方法。<br><br># 论文核心贡献点<br>1. 建立首个整合空间和机器人推理与同步多视图输入的基准测试<br>2. 验证了机器人多视图场景仍极具挑战性，最强大的VLM模型远低于人类表现<br>3. 发现空间推理和机器人执行之间存在正相关性<br>4. 证明强大的单视图空间基准测试性能不能可靠转移到机器人任务或多视图场景<br>5. 揭示多视图机器人推理对视角整合、遮挡解决和空间融合提出了根本性不同要求<br><br># 论文方法描述<br>构建了多阶段的基准测试构建管道：数据收集（基于AgiWorld和BridgeV2数据集）、QA生成（使用任务特定模板构建五选一QA对）、人工质量审查（迭代审查修正问题）。探索了三种CoT风格增强方法：文本CoT（通过GPT-4.1生成场景描述）、视觉CoT（使用VGGT进行新颖视图合成）、结构CoT（使用MoGe-2提供深度先验）。评估采用零样本多选题格式，确保跨模型公平比较。<br><br># 论文使用数据集和训练资源<br>使用AgiWorld和BridgeV2数据集作为数据源，构建了包含980个片段的1,708个QA对。评估涵盖盲评（Random、GPT-3.5-turbo、GPT-4-turbo）、专有模型（GPT-4o系列、GPT-4.1系列、Claude-3.5/3.7、Gemini-2.x系列）、专有推理模型（o4-mini、GPT-5系列、Claude-3.7-think、Gemini-2.5-pro）、开源模型（Gemma-3、InternVL3、Qwen2.5-vl系列）和开源MoE模型（Llama-4系列）。<br><br># 论文使用的评估环境和评估指标<br>评估采用统一的零样本提示格式，所有任务设计为多选题以确保公平比较。使用准确率作为主要评估指标，并通过人工评估作为参考点。评估环境包括多种模型类别，从盲评文本LLM到专有多模态和推理优化架构，还包括开源社区开发的VLMs和MoE模型。</details> |
| 2025-10-21 | MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning | http://arxiv.org/abs/2510.18337 | <details><summary>展开</summary>### 论文研究单位<br>- **Harvard University** (Wenhui Huang, Han Qi, Yilun Du, Heng Yang)<br>- **University of Michigan** (Changhe Chen)<br>- **Nanyang Technological University** (Chen Lv)<br><br>### 论文概述<br>MoTVLA是一个基于混合Transformer（MoT）架构的视觉语言动作（VLA）模型，通过整合快速与慢速推理来改进机器人学习中的语言指令控制能力。该模型保留预训练VLM的通用智能（通用智能专家）以处理感知和语义规划，同时引入领域专家生成快速运动分解信号，最终通过扩散Transformer（DiT）动作专家执行具体操作。该设计显著提升语言可控性和推理效率，并已在NLP基准、仿真环境（ManiSkill）和真实机器人实验中验证有效性。<br><br>### 论文核心贡献点<br>1. **统一快慢推理架构**：在单一MoT模型中实现通用智能（慢速推理）与领域特定知识（快速推理）的融合，通过分解-组合-再分解的机制保持性能平衡。<br>2. **动作条件化策略学习**：将快速推理生成的运动分解信号作为扩散策略条件，提升任务执行效率和行为可解释性。<br>3. **延迟与性能优势**：在推理速度（快推理频率达4倍提升）、语义理解及真实操作任务上超越SOTA基线（如π0.5、RT-2）。<br><br>### 论文方法描述<br>- **架构设计**：<br> - **通用智能专家（Slow Reasoning）**：基于Qwen2.5-LLM 7B的预训练模型，处理视觉-文本多模态理解。<br> - **领域专家（Fast Reasoning）**：同构架构，生成步骤化运动分解文本（双向注意力）。<br> - **动作专家**：DiT结构，基于视觉窗口（5帧）、机器人状态、运动分解信号执行扩散去噪生成动作序列。<br>- **训练流程**：<br> - **领域专家SFT**：使用1.27M问答对（仿真154K + 真实125K + Robo2VLM 678K + LLaVA-OV 318K）优化快速推理。<br> - **动作专家扩散策略**：ManiSkill仿真（每任务300轨迹）+ 真实操作（Pick-and-Place 50轨迹，Table Bussing 200轨迹），以均方误差优化去噪网络。<br>- **推理模式**：<br> - **对话模式**：通用智能专家执行慢推理回答人类问题（如场景描述）。<br> - **行动模式**：领域专家快速生成运动分解 → 动作专家执行多步操作（如堆叠、插孔）。<br><br>### 论文使用数据集和训练资源<br>- **领域专家数据**：<br> - 仿真数据：154K（ManiSkill的Cube Stacking、Peg-in-Hole、L-tool Pull）<br> - 真实演示：125K（人类操作的Pick-and-Place、Table Bussing）<br> - 外部数据：678K（Robo2VLM，转换为推理文本）、318K（LLaVA-OV，筛选长答案）<br>- **动作专家数据**：<br> - 仿真：900轨迹（3任务×300）<br> - 真实：250轨迹（Pick-and-Place 50 + Table Bussing 200）<br>- **预训练资源**：<br> - 通用智能专家：Bagel-VLM初始化（SigLIP-So400m视觉编码 + Qwen2.5文本分词器）<br> - 推理骨干：MoTVLA-14B（双7B专家，总参数量14B）<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **仿真**：ManiSkill平台（Cube Stacking、Peg-in-Hole、L-tool Pull）。<br> - **真实操作**：桌面任务（蔬菜Pick-and-Place含干扰物、Table Bussing含模糊指令）。<br>- **评估指标**：<br> - **语义推理**：BLEU、METEOR、CIDEr、Token准确率（VQA任务如ScienceQA、Visual-7W）。<br> - **机器人任务**：平均成功率（重复随机种子测试，零样本干扰场景）。<br> - **延迟对比**：推理频率（MoTVLA-14B/1B vs π0.5-KI，H100/A6000 GPU）。<br>- **基线对比**：π0.5-KI、π0、GR-MG、Diffusion Policy（DP），基于1050轨迹微调后性能。</details> |
| 2025-10-20 | RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies | http://arxiv.org/abs/2510.17950 | <details><summary>展开</summary>## 论文研究单位<br>Dexmal 和 Hugging Face<br><br>## 论文概述<br>论文介绍了RoboChallenge，一个在线机器人评估系统，旨在为视觉-语言-动作模型（VLA）提供大规模真实机器人基准测试。系统提供10台在线机器人供公众访问，覆盖UR5、Franka Panda、Cobot Magic Aloha和ARX-5等四种类型。研究构建了初始基准Table30，包含30个围绕桌子的任务，测试VLA模型的多种能力。论文描述了系统设计、评估协议和实验结果，强调了真实机器人在评估中的必要性。<br><br>## 论文核心贡献点<br>- 提出了一个在线机器人评估平台，支持大规模真实机器人测试，提供10台机器人机群。<br>- 设计了评估协议，通过视觉任务重现控制初始状态，区分稳定性（多次测试一致）和公平性（模型间比较一致）。<br>- 构建了Table30基准，包含30个任务，涵盖精确3D定位、多视角、时间依赖和软体操作等挑战。<br>- 实现了“远程机器人”范式，用户通过API直接访问机器人，无需提交模型文件。<br>- 提供了详细的结果分析，展示不同VLA模型在各项任务上的性能差异。<br><br>## 论文方法描述<br>系统采用“远程机器人”模式：用户通过低级别API访问机器人传感器（时间戳RGB、深度和本体感受数据）和动作队列（不可撤销动作）。评估协议包括：<br>- 视觉任务重现：通过参考图像引导测试者固定初始场景状态。<br>- 环境扰动控制：允许光照和背景变化，测试模型鲁棒性。<br>- 测试者控制：区分经验测试者、无知测试者和自适应测试者，提出受控测试者减少偏差。<br>- 稳定性与公平性权衡：基准协议关注模型稳定性（多次测试结果一致），比较协议关注公平性（模型间相对排序一致）。<br><br>## 论文使用数据集和训练资源<br>- 数据集：提供每个任务的演示数据（每任务最多1000集），用户需基于此微调模型。<br>- 训练资源：<br> - 任务特定设置：在8-GPU机器上训练约1天。<br> - 一般设置：混合多任务数据训练“机器通才”模型（每任务约50集）。<br>- 示例模型：包括π₀、π₀.5、CogACT和OpenVLA/OFT等开源VLA算法。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：10台机器人，分为四种类型（UR5、Franka Panda、Cobot Magic Aloha、ARX-5），配备多摄像头（主要、手腕、侧面）。<br>- 评估指标：<br> - 成功率（SR）：任务完成百分比。<br> - 进度分数：基于任务阶段完成情况，每个任务总分10分，每阶段分配点数，重试扣0.5分；每任务测试10次，总分100。<br>- Table30基准：30个任务列表（见表1），如插花、整理水果和开抽屉等，测试精确操作、多视角和时间依赖等能力。<br>- 结果：在π₀.5模型上观察到最高成功率，平均43.7%，其他模型性能较低；任务分析显示时间和软体相关任务更具挑战性。</details> |
| 2025-10-20 | RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation | http://arxiv.org/abs/2510.17640 | <details><summary>展开</summary>### 论文研究单位<br>- 南洋理工大学（新加坡）<br>- 清华大学（中国北京）<br>- 北京邮电大学（中国北京）<br><br>### 论文概述<br>论文针对机器人操作中的分布外（OOD）泛化问题，提出RESample框架。现有视觉-语言-动作（VLA）模型在模仿学习中易受OOD状态影响，因数据集仅含成功轨迹。RESample利用离线强化学习获取动作价值网络识别次优动作，并通过探索性采样生成潜在OOD轨迹，将数据整合训练集以增强VLA模型鲁棒性。<br><br>### 论文核心贡献点<br>- 设计稳健数据增强框架缓解模仿学习的OOD问题。<br>- 提出探索性采样机制，自适应纳入OOD样本。<br>- 利用动作价值网络识别细粒度OOD动作样本。<br>- 在模拟和真实机器人任务中验证有效性。<br><br>### 论文方法描述<br>RESample基于策略（πθ）和动作价值网络（Qφ）的二元性：<br>- **概念框架**：策略与评论家分歧触发探索性采样，强制执行策略自信但低值行动，暴露失败模式。<br>- **价值函数估计**：基于Cal-QL改进，使用行为克隆代理策略（πψ）校准Q值。评论家目标含时间差分损失和正则化（统一惩罚、行动锚定、数据保持）。<br>- **探索性采样机制**：策略生成候选动作，评论家过滤Q值低样本执行；为空则用常规动作。产生失败和恢复轨迹纳入训练。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：模拟实验使用LIBERO基准（包含Spatial、Object、Goal和Long-horizon四类任务，每类10个任务）。真实实验使用Galaxea A1机器人手臂。<br>- **训练资源**：评论家基于SAC算法离线训练，策略如DiT Policy或π0在模拟和真实环境重新训练。参数包括批大小（64或256）、学习率（1e-4或2e-5）、折扣因子（0.99）等。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：模拟实验在LIBERO基准评估，真实实验在四个任务（Pick Block、Stack Cup、Arrange Cubes、Stack 2 Cups）。<br>- **评估指标**：主要使用任务成功率（Success Rate）。在LIBERO上报告各任务类别平均成功率；消融研究使用成功率衡量性能。</details> |
| 2025-10-20 | From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors | http://arxiv.org/abs/2510.17439 | <details><summary>展开</summary>### 论文研究单位<br>- ByteDance Seed、NUS、NTU、THU、SMU 等机构合作<br><br>### 论文概述<br>- FALCON (From Spatial to Action) 是一类视觉-语言-行动（VLA）模型，针对现有 VLA 基于 2D 视觉编码、缺乏可靠 3D 空间理解的问题，提出通过空间基础模型在仅 RGB 条件下注入丰富的空间 token，并在可选条件下融合深度与位姿，从而在动作头层面融合语义与几何信息，增强长程与空间推理能力、跨模态可迁移性以及语言-视觉对齐<br><br>### 论文核心贡献点<br>- 以 Embodied Spatial Model（ESM）从 RGB 生成强几何先验，支持可选深度/位姿注入，无需重训即可提升精度<br>- 设计 Spatial-Enhanced Action Head，将空间 token 在动作头而非 VLM 中融合，保持 VLM 语义对齐不受破坏<br>- 采用随机条件注入训练策略（RGB-only 与 RGB-D/Pose 随机），实现强跨模态迁移能力<br>- 在动作头采用轻量元素相加融合，最大化稳定性与效率（优于交叉注意力与 FiLM）<br><br>### 论文方法描述<br>- 整体架构：2D VLM（如 Kosmos-2）负责语义表示；ESM 提取空间 tokens（基于 VGGT 范式，结合 DINO 视觉 token）；动作头融合语义动作 token 与空间 tokens 产生精确动作<br>- 空间编码：<br> - 输入为第三视角图像，附加可学习相机 token，进入 Spatial Encoder（N 层自/交叉注意力）输出空间 tokens 与相机 token<br> - 3D 条件编码：将相机位姿（7DoF）编码为 GT camera token；对深度与有效性掩膜进行归一化与卷积编码，生成与图像 tokens 等尺寸的深度 tokens<br> - 3D 条件注入：采用 b_d、b_p∼Bernoulli(p) 随机决定是否注入深度与位姿，替换可学习相机 token 并进行元素级相加；在训练中采用深度、点云与位姿的多任务监督<br>- 空间增强动作头：<br> - 将空间 tokens 最大池化为统一向量，经轻量 MLP 投影至动作头维度，与语义 action token 元素级相加<br> - 比较了交叉注意力、FiLM 门控与元素相加，最终采用元素相加（在泛化与稳定性上表现最佳）<br> - 预测器支持两类：MLP（短期）或 LSTM（长期）输出动作序列（7DoF 姿态+夹爪状态，C 步预测）<br>- 训练范式：<br> - 两阶段训练：阶段一冻结 VLM/ESM/旧动作头，只训练适配器（零初始化线性层避免初始破坏对齐）；阶段二解冻并联合优化<br> - 损失：MSE（姿态）+ BCE（夹爪）+ 3D 重建/深度/位姿监督（与 VGGT 一致）<br><br>### 论文使用数据集和训练资源<br>- 数据与平台：Open X-Embodiment 混合预训练；真实机器人多任务微调；基准覆盖 CALVIN（ABCD→D、ABC→D）、SimplerEnv（WidowX、Google Robot）与 11 项真实任务<br>- 训练资源：32×A100 训练；模型规模约 2.9B（VLM～1.6B，ESM～1.0B，配套动作头）<br><br>### 论文使用的评估环境和评估指标<br>- 仿真基准与指标：<br> - CALVIN：Tasks Completed in a Row（1-5 连贯任务完成率）与 Avg. Len（平均连击长度）<br> - ABCD→D：FALCON 97.2/93.3/90.3/88.0/84.0%，Avg Len 4.53；ABC→D：98.4/94.5/88.6/82.5/75.5%，Avg Len 4.40<br> - 相较 RT-1/Robo-Flamingo/GR-1/UP-VLA/RoboVLM 等一致领先<br> - SimplerEnv（WidowX/Google Robot）：按任务成功率汇总<br> - WidowX：FALCON 56.3%（Put Spoon 62.5%，Put Carrot 41.7% 等）<br> - Google Robot：FALCON 62.9%，在“打开抽屉放苹果”上 41.7%，远超 RT-2-X 55B 的 3.7%<br>- 真实世界评测与指标：三类设置的成功率（%）<br> - Base Tasks（9 套任务）：FALCON 70.0%，显著高于 SpatialVLA（44.4%）<br> - Few-shot（Simple/Unseen）：在未见物体/背景/任务描述变体下领先，Simple 提升 +27.5%，Unseen 平均 +27%<br> - 空间理解能力：在高度变化、尺寸变化、空间指令等任务中保持最高成功率（例如高度变化任务 60%→80%）<br>- 模态可迁移性与消融：在 CALVIN 与真实任务上验证 RGB-only 与 RGB-D/Pose 的增益；ESM 深度预测指标（δ<1.25、AbsRel）随可用模态增强而显著提升；元素相加在融合策略中兼具效率与性能</details> |
| 2025-10-20 | Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots | http://arxiv.org/abs/2510.17369 | <details><summary>展开</summary># 论文研究单位<br>- 主要单位：EPFL（洛桑联邦理工学院）<br>- 合作单位：LatentWorlds AI（荷兰代尔夫特理工大学背景）、Embodied AI SA<br><br># 论文概述<br>- 研究问题：现有视觉-语言-动作（VLA）模型主要部署于刚性机械臂，难以迁移到软体连续体机器人；两者在运动学、动力学和形态学上的差异导致即用策略失败。<br>- 目标：在软体连续体机器人上部署VLA模型，建立从刚性到软体机器人之间的可迁移部署流程，展示在接近人类的真实环境中的安全操控。<br>- 方案：构建结构化微调与部署流程，对OpenVLA-OFT与π0两类SOTA VLA模型进行对比评估，在软体机器人Embuddy上完成三类代表性操控任务。<br><br># 论文核心贡献点<br>- 贡献1：首个开放软体机器人演示数据集，覆盖抓取放置与人类近距互动任务，便于复现研究与跨形态迁移学习。<br>- 贡献2：在UR5（刚性）与Embuddy（软体）上系统对比OpenVLA-OFT；经针对性微调后，软体机器人取得与刚性平台相当的任务成功率，验证了刚性→软体领域迁移的可行性。<br>- 贡献3：在软体机器人上比较OpenVLA-OFT与π0：π0在刚性跨平台泛化更强，而经微调后OpenVLA-OFT在软体平台上表现更优。<br><br># 论文方法描述<br>- 平台与任务设计：<br> - 刚性平台：UR5，配套平行夹爪与俯视单目相机。<br> - 软体平台：Embuddy，连续体由三段软体节段构成（每段受腱驱动、最大弯曲角度分别为80°、50°、50°），具固有顺应性与碰撞恢复能力；总重约5 kg，工作空间由弯曲角限制。与UR5使用相同相机与夹爪配置以保证公平对比。<br> - 任务：<br> 1. “把橙子放入盘子”——基础抓取放置。<br> 2. “把X放入盘子”（X为橙子或牛奶）——带选择指令的抓取放置。<br> 3. “用棉花糖喂人”——近距人机互动操控。<br>- 数据采集与处理：<br> - 采用3Dconnexion空间鼠标遥操作系统；软体机器人的逆运动学基于分段常曲率（PCC）模型，将连续体节段近似为常曲率以映射腱长与末端位姿。<br> - 多模态观测：第三人称图像、手腕相机图像、本体感受状态（末端位姿）、自然语言任务描述；统一裁剪与缩放到256×256，手腕图像做翻转。<br> - 表示标准化：<br> - 本体状态s为8维向量[x,y,z,r,p,y,pad,g]，其中(x,y,z)为笛卡尔位置，(r,p,y)为滚-俯-偏角，g为夹爪状态（0/1）。<br> - 动作a定义为相邻状态差分：7维[Δx,Δy,Δz,Δr,Δp,Δy,g]；角差使用Δ = ((Δ+π) mod 2π) − π处理边界。<br> - 过滤与转换：去除几乎静止片段；OpenVLA-OFT数据用RLDS格式，π0数据用LeRobot格式；提供开放数据集（HuggingFace：HCSuMoss/soft_orange、HCSuMoss/soft_feed）。<br>- 模型微调与推理：<br> - OpenVLA-OFT（Llama 2 7B + ViT视觉前端）：采用LoRA（rank=32）进行全量微调以平衡性能与成本；引入FiLM层提升语言-视觉条件化；在Task 2中启用FiLM以强化指令对象选择。<br> - π0（PaliGemma 3B）：由于模型体量较小，采用全量微调；在默认设置上调整动作块大小为8以公平比较。<br> - 推理管线：本地机器人端采集第三人称/手腕图像、本体状态与语言指令；远程GPU进行动作块预测并回传；本地执行并循环，直至任务完成或达最大步数。采用动作块并行解码以提高速度。<br> - 安全性与鲁棒性：软体机器人可在被人工推动后恢复姿态并继续执行任务，显著优于刚性臂在人机交互场景的安全性。<br><br># 论文使用数据集和训练资源<br>- 数据集：开放软体机器人演示数据集，覆盖三类任务（Task 1: 50集；Task 2: 100集，橙子与牛奶各50；Task 3: 20集）；HuggingFace可下载。<br>- 训练资源：<br> - OpenVLA-OFT（UR5）：A100（80 GB+），Microsoft Azure虚拟机；软体Embuddy：H100远程HPC。<br> - π0（Embuddy）：H100远程HPC。<br> - 训练时长：OpenVLA-OFT约需56小时完成200k步（单A100）；π0约需11小时完成30k步（单H100）。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：软体机器人Embuddy与刚性机器人UR5；两者使用相同相机、夹爪与任务设定以保证公平性。<br>- 评估指标：<br> - 任务成功率：在每项任务上进行10次试验，统计成功率作为主要指标。<br> - 推理频率（端到端闭环，含网络延迟）：UR5+OpenVLA-OFT约32.3 Hz；Embuddy+OpenVLA-OFT约25.1 Hz；Embuddy+π0约38.0 Hz。<br> - 语言指令遵循验证：Task 2中FiLM模块提升对象选择准确率（OpenVLA-OFT达70%成功）；Task 3中在场景替换（棉花糖换为橙子）时模型能提前终止而非错误操作。<br> - 扰动鲁棒性测试：<br> - 人类出现与移动对性能无影响；<br> - 未见物体可能偶致混淆（约10次中1次）；<br> - 目标物体置于工作空间外时模型总是失败，说明训练工作空间是决定性约束。</details> |
| 2025-10-20 | DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment | http://arxiv.org/abs/2510.17148 | <details><summary>展开</summary># 论文研究单位<br><br>- RIX, Bosch<br>- AIR, Tsinghua University<br>- Shanghai Jiao Tong University<br><br># 论文概述<br><br>端到端（E2E）自动驾驶模型在常规条件下可生成物理可行的轨迹，但难以泛化至长尾场景；视觉-语言-动作（VLA）模型利用世界知识与认知推理应对复杂场景，但缺乏细粒度三维推理，易产生物理不可行的动作。DiffVLA++通过度量引导的轨迹评分器将VLA与E2E的优势对齐，提出桥接认知推理与端到端规划的统一框架。实验在ICCV 2025 Autonomous Grand Challenge公开榜单上实现EPDMS 49.12。<br><br># 论文核心贡献点<br><br>- 构建完全可微的VLA模块，直接回归带语义与三维推理的未来轨迹（4秒、8个关键点）。<br>- 设计密集轨迹词库的E2E模块，包含代理检测、语义分割与规划头，保障物理可行性与密集场景表达。<br>- 提出度量引导对齐机制：以规则化驾驶指标（NC、DAC、EP、TTC、LK、DDC、TLC、HC）训练轻量轨迹评分器（共享BEV特征的MLP并行头），将VLA与E2E轨迹映射至统一度量空间，实现对齐与融合。<br>- 后处理：对候选轨迹进行可行驶区域过滤与加权评分选择；离线集成两分支以提升最终性能。<br><br># 论文方法描述<br><br>- VLA模块<br> - 视觉流：CLIP ViT-L/14编码多视角图像；Driving Vision Adapter压缩并投影视觉token至LLM嵌入空间。<br> - 语言流：预训练分词器与文本编码器处理导航与高层指令，生成文本token。<br> - 语言模型：Vicuna-v1.5-7B进行多模态融合与因果注意力；LLM最后一层直接回归连续未来轨迹（x,y,θ），避免离散化误差，支持端到端优化。<br>- E2E模块<br> - BEV生成：BevFormer以VoVNet-99为骨干，构建128×128的BEV网格，覆盖64×64米空间。<br> - 多任务头：代理检测头（32查询）、语义分割头、轨迹规划头。<br> - 轨迹规划头：预定义密集轨迹词库（8192候选，K-means聚类专家轨迹），每个候选含8个2Hz关键点；沿候选轨迹采样BEV特征，经注意力机制聚合为上下文感知轨迹嵌入；通过可变形交叉注意力整合代理特征以精炼轨迹嵌入；每个嵌入经MLP解码为残差偏移，输出最终预测轨迹。<br>- 度量引导对齐<br> - 轨迹评分器：并行MLP头回归八项规则化指标（EP连续值[0,1]；DAC、TLC、TTC、LK、HC二值{0,1}；NC、DDC三值{0,0.5,1}）；采用加权复合损失（MSE、BCE、Cross-Entropy）训练。<br>- 后处理<br> - 先用全景驾驶感知模型预测可行驶区域，剔除不合法候选；随后按加权分数排序与选择；E2E与VLA两分支离线集成。<br><br># 论文使用数据集和训练资源<br><br>- 数据集<br> - NavsimV2（navtrain划分）用于训练；Navhard两阶段测试与公开榜单用于评估。<br>- 训练资源与配置<br> - VLA模块：Vicuna-v1.5-7B + CLIP ViT-L/14；AdamW优化器，余弦学习率，初值1e-5；dropout 0.05；1个epoch；批量8；8×NVIDIA A800 GPU。<br> - E2E与评分器联合训练：BevFormer + VoVNet-99；代理检测/分割/规划/评分多损失联合（权重10/1/14/20/14）；30个epoch；批量8；初始学习率1e-4；4×A800 GPU；AdamW与余弦调度（与VLA一致）。<br><br># 论文使用的评估环境和评估指标<br><br>- 评估环境<br> - ICCV 2025 Autonomous Grand Challenge（Navhard两阶段测试与公开leaderboard）。<br>- 评估指标与结果<br> - 主要指标：扩展预测驾驶员模型得分（EPDMS）。<br> - 分支成绩：VLA分支48.0；E2E分支43.7。<br> - 最终模型（离线集成）EPDMS：49.1238。<br> - 分项指标（公开leaderboard，两阶段）：<br> - No-At-Fault Collisions：98.2143（Stage 1）、88.7709（Stage 2）<br> - Drivable Area Compliance：98.5714（Stage 1）、95.3235（Stage 2）<br> - Driving Direction Compliance：100（Stage 1）、97.2196（Stage 2）<br> - Traffic Light Compliance：99.2857（Stage 1）、98.1711（Stage 2）<br> - Ego Progress：79.5117（Stage 1）、73.4289（Stage 2）<br> - Time-To-Collision within bound：98.5714（Stage 1）、87.9888（Stage 2）<br> - Lane Keeping：95（Stage 1）、59.4454（Stage 2）<br> - History Comfort：92.8571（Stage 1）、98.9833（Stage 2）<br> - Two-frame Extended Comfort：50（Stage 1）、52.9822（Stage 2）</details> |
| 2025-10-20 | Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey | http://arxiv.org/abs/2510.17111 | <details><summary>展开</summary>## 论文研究单位<br>中国科学院自动化研究所、中国科学院大学、AiRiA、南京信息工程大学<br><br>## 论文概述<br>这是一篇关于高效视觉-语言-动作（VLA）模型在具身操作中的系统性综述论文。VLA模型通过将自然语言指令和视觉观察映射到机器人动作，实现具身控制。尽管VLA系统功能强大，但其巨大的计算和内存需求与边缘平台的实时性能要求存在显著冲突。本综述首次从效率角度对VLA模型进行系统性回顾，涵盖模型架构、感知特征、动作生成和训练/推理策略四个维度，提供了主流效率优化方法的分类和比较分析。<br><br>## 论文核心贡献点<br>1. 首次专门针对高效VLA的系统性综述，将效率改进技术分为四个维度：模型架构、感知特征、动作生成、训练/推理机制<br>2. 基于分类法总结主流效率优化方法，分析各种技术的优劣势和适用场景<br>3. 讨论VLA模型未来发展趋势，强调需要在新兴趋势下进一步改进效率的优先方向<br>4. 提供实用的参考，支撑开发既高效又具备通用可靠具身智能的VLA系统<br><br>## 论文方法描述<br>**架构层面：**<br>- **静态骨干选择**：使用轻量级模型如Mamba、SmolVLA（2.24亿参数）、NORA（30亿参数）替代大型预训练模型<br>- **动态计算路径**：包括层剪枝（FLOWER）、早期退出（DEER-VLA）、专家混合（MoLE-VLA）、相似性跳过（Efficient-VLA）<br>- **双系统设计**：结合慢系统（大型多模态语言模型）处理复杂推理和快系统（轻量模型）实现快速响应<br><br>**感知特征优化：**<br>- **选择性特征处理**：基于注意力分数、任务相关性、空间结构的令牌修剪方法<br>- **时间共享重用**：利用帧间相似性重用KV缓存、高层表示和推理结果<br><br>**动作生成策略：**<br>- **原始动作生成**：动作块分块、令牌压缩（FAST）、离散化（VOTE）<br>- **推理感知动作**：语言链式思维推理（ECoT）、视觉推理（UniPi、VPP、CoT-VLA）<br><br>**训练推理优化：**<br>- **训练效率**：参数高效微调（LoRA）、知识蒸馏、量化感知训练、剪枝恢复<br>- **推理效率**：非自回归解码、投机解码（Spec-VLA）、并行细化（PD-VLA）<br><br>## 论文使用数据集和训练资源<br>- **Open X-Embodiment (OXE)**：大规模真实世界机器人数据集<br>- **DROID**：大规模野外机器人操作数据集<br>- **各种模拟数据集**：用于预训练和Sim-to-Real迁移<br><br>## 论文使用的评估环境和评估指标<br>**评估维度：**<br>- **资源效率**：模型规模、推理延迟、内存占用、训练时间、能耗<br>- **性能鲁棒性**：任务成功率、长时序稳定性、分布外泛化能力、环境扰动恢复<br>- **可解释性**：人类可读的推理过程、决策归因机制、透明度<br><br>**技术实现：**<br>- 统一的云边部署架构<br>- 标准化硬件配置报告<br>- 开源评估框架和基准测试<br>- 多任务多场景的公开数据集和仿真环境</details> |
| 2025-10-18 | MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation | http://arxiv.org/abs/2510.16617 | <details><summary>展开</summary>## 论文研究单位<br>University of Texas at Austin（作者来自德州大学奥斯汀分校）<br><br>## 论文概述<br>现有的大规模视觉-语言-动作（VLA）模型在跨实验室、环境与任务上的泛化能力不足，通常需要在新场景中收集一定量的专家演示并进行参数微调才能奏效。本文提出 MoS-VLA（Mixture of Skills VLA），通过“技能混合”的方式将机器人策略表示为有限基函数的线性组合，从而构建结构化的技能空间；在线推理时，仅需一次专家演示，通过最小化 L1 行动误差的轻量凸优化（线性规划）得到系数，无需梯度回传即可在新环境中快速适配，显著降低适配开销。论文基于 OpenVLA 与 Open X-Embodiment 数据进行训练，并在仿真与真实机械臂任务上验证效果。<br><br>## 论文核心贡献点<br>- 提出一种一次样本、上下文内的 VLA 适配方法：在推理阶段只需一次专家演示，无需梯度更新，计算与内存开销极低（仅前向传播），在普通 GPU（如 RTX 3090）上几秒即可完成校准。<br>- 首次将函数编码器（Function Encoders）应用于亿级参数与多模态机器人数据，验证其在 VLA 场景的可行性与可扩展性。<br>- 在结构化技能空间中缓解“混合上下文过拟合”：通过学习基函数并在系数空间区分不同上下文，提升了分布外数据集与实际环境中的表现。<br>- 构建了 L1 误差驱动的凸优化校准流程（线性规划），强调鲁棒性与对抗离群点的能力。<br>- 给出端到端实现细节与可扩展训练策略，支持分布式并行与缓冲校准，保证可复现性与工程可行性。<br><br>## 论文方法描述<br>- 问题形式化：机器人策略空间为状态 S = I × T 到行动 A 的映射；上下文 c（如光照、相机位姿、机器人形态等）影响专家策略但不可直接观测；将不同上下文下的专家策略集合视为函数空间。<br>- 主思想：将上下文相关的专家策略表示为函数空间中的函数，学习一组神经网络的基函数 {g1, ..., gk}，任何上下文策略 π_exp^c 可表示为这些基函数的线性组合。新任务在线适配时，通过一次专家演示 τ_exp^c，最小化 L1 行动误差，求解线性规划得到系数 α^c，再以线性组合给出动作预测。<br>- 架构实现：在 OpenVLA 主干的基础上替换语言模型输出头为 k 个独立的“基函数动作头”，共享主干特征；采用并行解码策略同时预测多个行动维度；降低参数开销并保持 Transformer 输入的变长处理能力。<br>- 训练流程：采用修正的函数编码器算法，支持 L1 误差与 Banach 空间设置；使用 LoRA 进行高效微调；维护校准缓冲（每数据集 512 样本），每若干步更新一次上下文系数并在训练中广播；使用分布式数据并行（DDP）在多节点（32×GH200）上进行训练。<br><br>## 论文使用数据集和训练资源<br>- 数据集：Open X-Embodiment（RT-X）中的 Magic Soup Plus 数据混合（同 OpenVLA），包含多个实验室的轨迹数据；包含分布内与分布外（OOD）子集。<br>- 训练资源：32 个计算节点，每节点 1×GH200；全局批次 320；训练步数约 5000（耗时约 24 小时）；Adam 优化器，学习率 1e-4，热身 10 步。<br>- 适配资源（在线）：单条专家演示；在普通 GPU（如 RTX 3090）上仅需几秒完成系数求解。<br><br>## 论文使用的评估环境和评估指标<br>- 数据集评估：在 27 个训练子集与 5 个 OOD 子集上评估动作预测误差（L1 误差），对比基线（OpenVLA）。<br>- 仿真实验（Robosuite）：两项任务——块搬运与开门；每任务 m=20 次试验。<br>- 真实机械臂实验（Franka Emika Panda）：三项任务——目标抵达、块搬运、笔插入；每任务 m=10 次试验；单 RGB 前置相机环境，部分运动方向限制以简化深度估计。<br>- 指标：任务成功率（%），同时观察与讨论适配在短-horizon 任务上的有效性以及在更长时序或更高随机性场景中的局限。<br><br>## 关键结果<br>- 在 5 个 OOD 数据集上，MoS-VLA 的 L1 行动误差全面低于基线；在训练子集中也有 18/27 的优势。<br>- 仿真与真实实验：OpenVLA 在未见环境与新任务上成功率为 0%；MoS-VLA 在一次演示校准后，仿真任务（块搬运、开门）达 70–75% 成功率，真实任务（目标抵达、块搬运、笔插入）达 100% 成功率。<br>- 系数可视化显示相似实验室的上下文在技能空间内聚类，说明学习到的技能空间能够捕捉环境与任务的结构性差异。<br><br>## 工程与复现要点<br>- 架构：在 OpenVLA 主干上引入 k=16 个基函数动作头，采用并行解码直接输出标量动作。<br>- 校准：维护分布式校准缓冲，每 16 步进行一次系数广播与更新；采用 CVXPY 求解 L1 线性规划。<br>- 资源友好：在线适配不需梯度回传，推理阶段计算与内存开销与专家轨迹长度无关（常数复杂度）。</details> |
| 2025-10-17 | NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly? | http://arxiv.org/abs/2510.16263 | <details><summary>展开</summary># 论文研究单位<br>Department of Computer & Data Sciences, Case Western Reserve University<br><br># 论文概述<br>论文提出NEBULA，一个针对单臂操作的统一生态，用于视觉-语言-动作（VLA）智能体的诊断式与可复现评估。NEBULA以标准化API和大规模聚合数据集为基础，构建“双轴评估协议”：一条轴线进行能力测试（精细技能诊断），另一条轴线进行压力测试（稳健性刻画）。论文同时对当前SOTA模型开展系统评测，指出空间推理与动态适应是普遍短板，并强调推理速度与低延迟对动态任务的关键作用。<br><br># 论文核心贡献点<br>- 标准化API与统一数据格式，整合ManiSkill、LeRobot等分散数据集，支持跨数据集训练与公平对比；提供大规模聚合数据集（Alpha与Beta），并配套PyTorch与TFRecord接口、模型适配器。<br>- 首次提出双轴评估协议：将“能力”（六类技能：控制、感知、语言、动态适应、空间推理、稳健性/泛化）与“压力”分离，能力轴采用可控变量隔离与分层难度，压力轴对推理频率、延迟、稳定性、适应性、资源进行单一指标测评。<br>- 全面基准评测：揭示当前VLA在空间推理和动态适应上的系统性薄弱与稳健性不足，显示传统“任务成功率”掩蔽关键失败模式。<br>- 诊断输出与因子隔离：提供雷达图等可视化绩效摘要，验证可控变量隔离与分层难度的有效性，支持解释性错误定位。<br><br># 论文方法描述<br>- 统一数据平台与API：基于SAPIEN与ManiSkill3收集与标注多模态时序数据（RGB/深度/分割、状态、动作、语言指令），形成标准化Episode与Step格式；提供SDK与查询引擎、训练/评测工具链及PyTorch/TF兼容适配器。<br>- 能力测试任务：按六类技能设计独立任务模板，每类技能分为Easy/Medium/Hard三个难度层级；遵循“可控变量隔离”原则，使性能变化可唯一归因于被测技能。<br>- 压力测试任务：四个单指标压力测试（推理频率、延迟、稳定性分数、适应性），每项分为三个压力等级（v1–v3），用于刻画系统在不同运营约束下的退化曲线与稳健边界。<br>- 评测与诊断输出：统一数据加载与评测协议，雷达图与可视化展示；进行因子隔离有效性验证，比较隔离与非隔离场景下模型表现差异。<br><br># 论文使用数据集和训练资源<br>- 数据集：NEBULA-Alpha与Beta两套版本，来源于专家轨迹（运动规划）与人机演示；Alpha包含超过5.4万演示，覆盖控制/感知/语言/动态/空间五类能力家族（稳健性家族仅用于评测，不入训练集）；Beta为约10%规模，部分高难度任务含人工遥操作。<br>- 数据形式：多模态观测（6视角摄像头RGB、深度、分割）、本体感受、动作与成功标签、语言指令；提供PyTorch与TFRecord格式，附带模型特定适配器。<br>- 训练资源：主要评测在Alpha数据集上进行微调；训练使用统一数据加载与原版训练协议；具体硬件规格与资源用量未详述。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：基于SAPIEN的定制仿真平台（ManiSkill3框架），与NEBULA统一API与数据格式兼容。<br>- 能力轴指标：按六类技能（控制、感知、语言、动态适应、空间推理、稳健性/泛化）报告分层成功率；提供能力雷达图等诊断性可视化。<br>- 压力轴指标：推理频率（Hz）、延迟（ms）、稳定性分数（基于相邻动作差的指数衰减，归一化至[0,1]）、适应性（在目标切换/指令变更等场景下的成功率）；每项指标分为三档压力等级（v1–v3）。<br>- 诊断与因子隔离：比较隔离与非隔离场景的误差来源，验证能力评估的可控性与有效性。</details> |
| 2025-10-17 | VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving | http://arxiv.org/abs/2510.15446 | <details><summary>展开</summary>## 论文研究单位<br>- 清华大学苏州汽车研究院<br>- 作者实习单位为清华大学苏州汽车研究院<br><br>## 论文概述<br>论文提出 VDRive 框架，将视觉-语言-动作模型（VLA）与扩散策略（Diffusion Policy）结合，用于端到端自动驾驶。框架以离散化的状态-动作表示为核心，通过“强化的模态对齐”桥接高维传感器空间与低维动作空间：在上下文层面，VLA 通过视觉 token 预训练预测未来观测；几何层面，通过对 VLA 的强化学习微调，使其基于当前驾驶条件预测轨迹与动作。随后，扩散策略头在 VLA 提供当前与预测状态 token 的条件下生成层次化动作与轨迹，并由动态引导的精炼头优化轨迹输出。方法采用全离线强化学习范式，在 nuScenes 开环规划与 Bench2Drive 闭环基准上均取得最先进性能。<br><br>## 论文核心贡献点<br>- 提出 VDRive 框架：通过 VLA 预测未来状态 token 与扩散策略头联合训练，实现上下文与几何对齐的驾驶决策。<br>- 构建强化学习偏好数据集：基于 nuScenes 与 Bench2Drive 构造“选择/拒绝”视觉-动作对用于 VLA 微调。<br>- 设计离线奖励数据：结合规则奖励与专家 VLM（Qwen2.5-VL-72B）评分，训练扩散策略头并构建 actor-critic 强化学习闭环。<br>- 在多基准上实现最先进结果：在 nuScenes 开环评估与 Bench2Drive 闭环评估中均显著提升。<br><br>## 论文方法描述<br>- 离散化表示（CVQ-VAE）：训练条件向量量化变分自编码器，以轨迹为条件，将原始/分割图像编码为离散观测 token，加入 VLA 分词器词表。<br>- VLA 强化微调：利用偏好数据训练 VLA 生成未来观测 token、动作信号与导航命令，结合选择/拒绝样本进行偏好对齐。<br>- 扩散策略头学习：在离线状态-动作-奖励-下一状态数据集上，训练扩散模型作为 actor 网络、最小化生成动作与真实动作的重建误差，并通过 critic 网络提供价值反馈，实现累积奖励最大化。<br>- 动态引导的精炼头：将 VLA 与扩散策略的异步动作预测作为条件，通过 Transformer 编码器与 MLP 映射融合位置嵌入与动作特征，最终解码优化轨迹。<br><br>## 论文使用数据集和训练资源<br>- 数据：nuScenes（1000 场景，6 相机/5 雷达/1 激光雷达，2 Hz）与 Bench2Drive（CARLA 仿真）用于开/闭环评估与偏好/奖励数据集构建；利用 Vista 生成合成风险场景。<br>- 预训练/微调：视觉 token 预训练（CVQ-VAE + VLA tokenizer）；VLA 强化微调（偏好对齐）；扩散策略离线强化学习（actor-critic）。<br>- 模型：VLA 基于 InternVL3-8B；扩散策略头使用扩散模型；奖励评估采用 Qwen2.5-VL-72B。<br><br>## 论文使用的评估环境和评估指标<br>- 开环评估（nuScenes）：平均 L2 误差与碰撞率；在 1s/2s/3s 时间步与总体上报告指标。<br>- 闭环评估（Bench2Drive）：Driving Score、Success Rate、Efficiency、Comfortness；并报告多能力指标（并线、超车、紧急制动、让行、交通标志）。<br>- 消融实验：Bench2Drive-mini 上对比不同数据构造与不同精炼模块（Transformer/LSTM/GRU）设计。</details> |
| 2025-10-16 | RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks | http://arxiv.org/abs/2510.14968 | <details><summary>展开</summary>### 论文研究单位<br>- Mingxuan Yan、Yuping Wang、Jiachen Li：加州大学河滨分校（University of California, Riverside）<br>- Yuping Wang（共同作者）：密歇根大学（University of Michigan）<br>- Zechun Liu：Meta AI<br><br>### 论文概述<br>论文针对长程任务中的层次化视觉-语言-行动（VLA）框架问题，提出检索式演示分解器（RDD）。现有VLM规划器微调依赖人工或启发式子任务分解，易导致子任务与低层视觉运动策略训练数据不一致，降低任务性能。RDD通过视觉特征检索自动分解演示，将子任务与策略训练数据对齐，使用动态规划优化分解策略，实现计划器与策略协同。<br><br>### 论文核心贡献点<br>- 首次协调高层规划器与低层视觉运动策略，通过生成对齐的规划器微调数据集提升长程任务性能。<br>- 提出RDD框架，训练自由的检索式演示分解方法，形式化为最优分割问题，用动态规划高效求解（含理论分析）。<br>- 在仿真和真实基准评估中，RDD优于启发式分解器（如UVD），展现跨设置鲁棒性。<br><br>### 论文方法描述<br>- **问题建模**：将演示分解形式化为最优分割问题（公式3.1），最大化分割策略与策略训练数据的相似性。<br>- **动态规划求解**：利用最优性原理，将复杂度从O(2^N)降至O(N^2)（有界区间时O(N)）（算法1）。<br>- **区间评分函数**：定义区间相似性（公式3.3），结合视觉编码器（如LIV）、近似最近邻检索（Annoy）和时间持续差异。<br>- **视觉特征**：使用视觉编码器嵌入间隔（公式3.5），结合开始和结束帧，计算间隔相似度（公式3.6）。<br>- **OOD处理**：扩展方法处理分布外子任务，结合检索和启发式（公式3.7-3.8）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：RLBench机器人操作基准，使用训练集1908个演示（分解为12700子任务区间）。<br>- **视觉编码器**：LIV（主用），对比R3M、VIP、VC-1、CLIP、DINOv2、ResNet。<br>- **检索方法**：Annoy近似最近邻搜索（Angular距离）。<br>- **规划器**：LLaVA-based VLM（llama3-llava-next-8B），LoRA微调（rank 128，scale 256）。<br>- **计算资源**：4个NVIDIA 6000 Ada GPU，微调约5分钟。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：RLBench仿真基准，13个任务（成功率高>35%）。<br>- **评估指标**：<br> - 多任务成功率（%，平均值和标准差）。<br> - 平均排名（↓）。<br>- **比较基线**：Expert（启发式专家）、UVD（视觉分解器）、Uniform（均匀分割）、w/o Finetune（不微调）。<br>- **实验设置**：每个任务3个演示用于微调；结果平均10次随机种子。</details> |
| 2025-10-16 | VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation | http://arxiv.org/abs/2510.14902 | <details><summary>展开</summary>待生成</details> |
| 2025-10-16 | QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models | http://arxiv.org/abs/2510.14836 | <details><summary>展开</summary>### 论文研究单位<br>- School of Artificial Intelligence, University of the Chinese Academy of Sciences<br>- Institute of Automation, Chinese Academy of Sciences<br>- Beijing Zhongke Huiling Robot Technology Co.<br><br>### 论文概述<br>论文提出QDepth-VLA框架，旨在通过引入量化深度预测作为辅助监督信号，增强视觉-语言-动作（VLA）模型的空间感知和推理能力。现有VLA模型在长时序和精细操作任务中表现不佳，主要因缺乏3D几何理解。QDepth-VLA利用向量量化（VQ-VAE）将深度图转换为离散令牌，并设计专门的深度专家模块预测这些令牌，避免干扰预训练语义对齐。实验在模拟和真实机器人任务上验证了其有效性。<br><br>### 论文核心贡献点<br>1. **QDepth-VLA框架**：通过量化深度预测增强VLA模型的空间理解能力。<br>2. **深度专家模块**：设计用于预测离散深度令牌，而非回归像素级深度，提供更紧凑的优化友好监督信号。<br>3. **性能提升**：在LIBERO基准上相比open π0平均提升7.7%成功率，在Simpler基准上提升6.1%，真实机器人任务提升10.0%。<br><br>### 论文方法描述<br>- **深度标注**：使用Video-Depth-Anything (ViDA)生成单目深度估计。<br>- **VQ-VAE重建**：预训练VQ-VAE将深度图量化为256个代码向量的离散令牌（网格分辨率16×16）。<br>- **QDepth-VLA架构**：基于PaLI-Gemma 3B的VLM，包括预训练VLM、动作专家和深度专家。深度专家采用Transformer架构，输入视觉令牌，预测量化深度令牌。<br>- **混合注意力机制**：设计层次化注意力，允许深度令牌关注文本和图像令牌，动作令牌关注所有前序模态，防止干扰预训练VLM。<br>- **联合训练程序**：总损失函数为动作损失（CFM损失）加指数衰减的深度损失（交叉熵损失）。优化使用AdamW优化器，余弦退火学习率（200步预热）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：LIBERO（四个子集：Spatial, Object, Goal, Long）、Simpler（Google Robot和WidowX250任务）、真实机器人数据集（Piper Arm任务）。<br>- **训练资源**：8×H20 GPU，FSDP并行训练，全局批量大小1024，梯度累积。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：模拟环境（LIBERO和Simpler）、真实机器人环境（6-DoF Piper机械臂）。<br>- **评估指标**：任务成功率（Success Rate），LIBERO每任务50次rollouts，Simpler每任务240-2400次评估，真实机器人每任务10次试验。</details> |
| 2025-10-16 | Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning | http://arxiv.org/abs/2510.14300 | <details><summary>展开</summary>### 论文研究单位<br>论文作者来自多个机构，主要包括：<br>- 上海交通大学（MoE关键人工智能实验室、自动化与智能传感学院、计算机科学学院）<br>- 上海AI实验室<br>- 清华大学深圳国际研究生院<br>- 香港大学<br>- 同济大学<br>- D-Robotics<br>- 系统控制与信息处理教育部重点实验室（上海交通大学）<br>- 信息安全综合管理技术上海市重点实验室<br><br>### 论文概述<br>论文针对Vision-Language-Action (VLA)模型在扩展时面临的挑战（如计算资源需求高、实时控制效率与模型容量平衡难题），提出了AdaMoE（Action-Specialized Mixture of Experts）架构。AdaMoE通过继承预训练VLA模型权重，并在动作专家中引入稀疏激活的MoE层来扩展模型容量，同时利用解耦专家选择与权重的创新设计，在保持计算效率的前提下提升性能。论文在仿真基准（LIBERO和RoboTwin 2.0）和真实世界实验中验证了AdaMoE的有效性。<br><br>### 论文核心贡献点<br>1. **高效扩展VLA模型**：提出一种从预训练密集VLA模型继承权重并转换为MoE架构的低成本方法。<br>2. **解耦专家选择与权重**：引入独立的比例适配器（scale adapter）与路由器（router），解决传统MoE中负载平衡与任务性能冲突的根本问题。<br>3. **显著性能提升**：在LIBERO基准提升1.8%，在RoboTwin 2.0域随机化任务提升9.3%，在真实世界实验提升21.5%，验证了实际应用价值。<br>4. **专家特化验证**：通过专家激活模式分析，证明专家能捕获有意义的操作行为（如定位、抓取等）。<br><br>### 论文方法描述<br>- **架构设计**：基于π₀框架的动作专家由共享专家（处理通用操作）和路由专家（处理特定任务）组成，使用top-k选择（默认k=1）进行稀疏激活。<br>- **解耦机制**：路由器（R）负责专家选择以实现负载平衡，比例适配器（S）独立调整专家贡献权重，最终权重为S_i(x) + softmax(R_i(x))。<br>- **训练目标**：结合流匹配损失（flow matching loss，用于高频率动作生成）和负载平衡损失（load balancing loss），总损失为L_total = L_τ + λ_balance * L_balance（λ_balance默认0.01）。<br>- **专家初始化**：共享专家继承原始FFN权重，路由专家为副本以快速扩展。<br>- **推理过程**：使用流匹配进行去噪迭代（从纯噪声生成动作序列）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - LIBERO基准：包含LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long四个任务套件，每个套件含100专家轨迹。<br> - RoboTwin 2.0：19个域随机化任务（清洁和随机化环境各100/400轨迹）。<br>- **训练资源**：<br> - 训练步数：120,000步（批量大小32）。<br> - 优化器：AdamW（峰值学习率2.5e-5，路由器学习率5e-5，β1=0.9，β2=0.95）。<br> - 其他超参数：动作视野50、专家数4、梯度裁剪1.0、EMA衰减0.99。<br> - 计算资源：百度云平台提供计算支持，AgileX Robotics提供机器人平台。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **仿真环境**：LIBERO和RoboTwin 2.0基准（使用域随机化增强鲁棒性）。<br> - **真实世界环境**：ALOHA-Agilex双机械臂系统（AgileX Robotics），包括四个任务：堆叠盘子、按铃、调整瓶子、放置杯子。<br>- **评估指标**：<br> - 主要指标：任务成功率（Success Rate, SR），以百分比表示（每个任务评估50次试验）。<br> - 其他分析：专家使用强度（专家激活模式）、消融研究（比较不同架构变体如Vanilla MoE、CSMoE和AdaMoE）。</details> |
| 2025-10-15 | LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models | http://arxiv.org/abs/2510.13626 | <details><summary>展开</summary>## 论文研究单位<br>- 复旦大学<br>- 同济大学<br>- 上海创新研究院<br>- 新加坡国立大学<br><br>（作者分别隶属上述单位，属于跨机构合作研究）<br><br>## 论文概述<br>针对视觉‑语言‑动作（VLA）模型在标准基准上表现优异，却在真实场景中易受扰动的现象，本文在 LIBERO 仿真基准上系统地进行鲁棒性评估。通过在七种维度（对象布局、相机视角、机器人初始姿态、语言指令、光照、背景纹理、传感器噪声）上施加可控扰动，揭示模型的脆弱性：对视角和初始状态的微小变化即可导致成功率从 95% 下降到 30% 以下；而语言变化对整体表现影响最小，进一步实验表明多数模型并未真正依赖语言信号。研究还发现模型存在位置偏置、忽略视觉语义以及对组合扰动的负交互效应。为解决这些问题，文中提出了 LIBERO‑Plus 基准、组合泛化差距的统计定义以及基于大规模扰动数据的通用微调方案，实现了显著鲁棒性提升。<br><br>## 论文核心贡献点<br>1. **系统性脆弱性分析**：在七种单一维度扰动下对 10 种 VLA 模型进行评估，量化每类扰动的绝对性能跌幅（表 1）。<br>2. **诊断框架**：定义扰动随机变量与成功指示器，提出条件成功率、组合概率及协方差形式的组合泛化差距 Δij，并用 χ² 检验显著性。<br>3. **深度洞见**：揭示模型对视觉‑动作的固定映射依赖、语言利用率低、位置记忆偏置以及对组合扰动的负交互效应。<br>4. **LIBERO‑Plus 基准**：自动化生成 10,030 任务，涵盖七大扰动因子及 21 子组件，按多模型成功率划分为 Level‑1–Level‑5 五级难度（图 5‑6）。<br>5. **通用训练策略**：构建超过 20,000 条扰动成功轨迹的通用训练集，对 OpenVLA‑OFT_m 进行混合微调，整体成功率提升至 79.6%（视角扰动 92.8%）。<br>6. **开源资源**：公开评估脚本、任务生成代码、难度划分方法及训练配置，便于后续研究复用。<br><br>## 论文方法描述<br>1. **扰动维度**<br> - 对象布局（添加干扰对象、目标位移）<br> - 相机视角（视角/视野变化）<br> - 机器人初始姿态（关节角度/位置扰动）<br> - 语言指令（语义改写、句式变换）<br> - 光照（强度、方向、颜色、阴影）<br> - 背景纹理（桌面/场景材质）<br> - 传感器噪声（光度失真、抖动、模糊）<br><br>2. **模型集合**<br> OpenVLA、OpenVLA‑OFT（含_oft_w、_oft_m）、π₀、π₀‑fast、Nora、WorldVLA、UniVLA、RIPT‑VLA 等，涵盖自回归与扩散两大范式。<br><br>3. **单维度扰动实验**<br> 对每种扰动在 LIBERO 四套任务上评估成功率，记录绝对跌幅（表 1），并分析模型之间的鲁棒差异。<br><br>4. **视觉注意分析**<br> 将对象布局扰动拆分为“添加干扰对象”与“目标位移”，对比成功率（图 1），验证模型是否聚焦任务关键对象。<br><br>5. **光照鲁棒性极端消融**<br> - **全黑**：所有相机输入置黑。<br> - **第三视角黑**：仅第三人称视角黑掉，保留腕部相机。<br> 通过成功率差异解释腕部相机提供的光照不敏感几何线索（图 2）。<br><br>6. **语言利用实验**<br> - **空指令**：完全删除语言输入。<br> - **目标替换**：将指令中的目标对象替换为同场景的其它对象。<br> 观察成功率变化（图 3），判断模型是否真正依赖语言。<br><br>7. **组合泛化差距定义**<br> 设 D_i 为第 i 种扰动是否施加的二值随机变量，Y 为任务成功指示器，定义条件成功率 s(D_i=d_i,D_j=d_j)=P(Y=1\|D_i=d_i,D_j=d_j)。组合差距 Δ_ij = Cov(D_i,D_j\|Y=1) = p(D_i=1,D_j=1\|Y=1) - p(D_i=1\|Y=1)p(D_j=1\|Y=1)。通过 2000 次独立实验估计概率，构建热图并进行 χ² 显著性检验（附录 F）。<br><br>8. **LIBERO‑Plus 基准构建**<br> - 自动生成任务 → 过滤平衡 → 按四模型成功率划分为五级难度。<br> - 任务规模：10,030，七扰动因子，21 子组件（图 5‑6）。<br><br>9. **通用训练与微调**<br> - 基于原始 LIBERO 轨迹扩增 20,000+ 条成功扰动轨迹。<br> - 以 OpenVLA‑OFT_m 预训练权重为起点进行混合微调。<br> - 在 LIBERO‑Plus 上评估，实现跨扰动显著提升（表 2）。<br><br>## 论文使用数据集和训练资源<br>- **基础数据**：LIBERO 基准（4 套任务），每个套件提供对象、语言指令和标准成功轨迹。<br>- **扰动生成**：自动化在七大扰动维度上生成场景与轨迹，筛选后形成 10,030 评估任务。<br>- **训练集**：超过 20,000 条成功扰动轨迹，覆盖对象布局、相机、光照、背景、噪声等多维变化。轨迹统一保存格式（附录 D）。<br>- **微调配置**：使用官方 OpenVLA‑OFT_m 权重进行混合微调；具体超参数与训练过程详见附录 D。<br>- **计算资源**：所有实验在 LIBERO 仿真平台完成，未在文中披露具体硬件，但规模要求大量随机生成的评估样本。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO 仿真平台，支持第三人称视角与腕部相机（如模型使用）。<br>- **模型评估**：<br> 1. **单维度扰动**：在 4 套标准任务上分别施加七种扰动，记录原始成功率及绝对跌幅（表 1）。<br> 2. **组合扰动**：对选定模型在两两扰动组合下进行 2000 次试验，估计联合概率并计算 Δ_ij（热图（图 4））。<br> 3. **极端消融**：全黑与第三人称视角黑掉两种场景，检验视觉依赖。<br> 4. **语言实验**：空指令与目标替换任务，评估语言利用率。<br>- **指标**：<br> - **成功率（%）**——任务执行成功的比例。<br> - **绝对跌幅**——相对原始成功率的下降低点（表 1）。<br> - **组合泛化差距 Δ_ij**——协方差形式的交互效应度量。<br> - **χ² 检验**——Δ_ij 的统计显著性（附录 F）。<br> - **难度分级**——基于多模型成功率分布的任务划分（Level‑1至Level‑5）。<br> - **综合成功率**——所有扰动维度的平均表现（表 2）。<br><br>（所有图表与实验细节均可在正文相应章节及附录中查阅）</details> |
| 2025-10-15 | DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning | http://arxiv.org/abs/2510.13375 | <details><summary>展开</summary>## 论文研究单位<br>IIIS, 清华大学;Galaxea AI<br><br>## 论文概述<br>DepthVLA是一篇针对视觉-语言-动作模型（Vision-Language-Action, VLA）在精确空间推理方面表现不佳的研究。当前基于大规模语言/视觉预训练的VLA虽然在语义理解上表现强劲，但普遍缺乏精细的空间理解，导致在精准抓取、避障与精细操作任务中失败。DepthVLA通过引入预训练的深度预测模块作为“深度专家”，在保持语言理解与开放词汇感知的同时显式增强几何推理能力。<br><br>DepthVLA采用“混合 transformer”（Mixture-of-Transformers, MoT）架构，统一VLM、深度专家与动作专家（流匹配）三者并共享注意力，但采用分块掩码以保护各专家的预训练能力。深度专家基于DINOv2编码器、采用Depth Anything V2初始化，并通过尺度不变的对数损失进行训练，为动作专家提供贯穿中间层的几何特征，从而提升抓取与碰撞避免等精细操作能力。<br><br>实验在真实世界与仿真环境开展，包括Simpler WidowX与LIBERO等基准。DepthVLA显著优于现有方法：在真实任务中取得78.5% vs 65.0%的成绩，在LIBERO上达94.9% vs 93.6%，在Simpler上为74.8% vs 58.8%。尽管新增约600M参数与约20ms的推理延迟，仍具实际可用性。<br><br>## 论文核心贡献点<br>- 深度专家融入MoT的VLA架构：在不破坏语义能力的前提下显式提供空间几何线索，提升精准操作与避障表现。<br>- 分模块预训练策略：VLM与深度专家可分别在大规模数据上独立预训练，提高效率与可扩展性，突破仅依赖具身动作数据。<br>- 真实与仿真全面验证：在多个基准上实现稳定且显著提升，涵盖精细抓取、碰撞避免与复杂多步任务。<br><br>## 论文方法描述<br>- 统一MoT架构：共享注意力的三专家设计（VLM、深度、动作），采用分块掩码使VLM/深度token自注意力、动作token可跨模态注意；保持各专家独立权重。<br>- 深度专家：基于DINOv2的编码器-解码器结构，使用Depth Anything V2初始化进行预训练；损失为尺度不变对数损失，使其具备稳健的距离估计与几何感知。深度专家在所有中间层输出几何特征，支撑动作专家的细粒度空间推理。<br>- 动作专家：采用流匹配（flow matching）损失对连续动作轨迹建模，与深度损失联合优化，实现端到端训练。<br>- 训练策略：深度专家在大规模3D/深度数据上预训练；VLA阶段在具身演示数据上微调，保留深度预测损失以维持几何能力。<br><br>## 论文使用数据集和训练资源<br>- 深度预训练：WildRGB-D、ScanNet、ScanNet++、HyperSim等大规模3D/深度数据。<br>- VLA训练：Galaxea Open-World（10万轨迹、150任务类、50场景）、BridgeData V2（6万+轨迹）、LIBERO（每个suite 500演示）。<br>- 伪深度标签：Depth Anything V2、UniDepth V2、VGGT。<br>- 训练资源：32×H100 GPU，AdamW优化；批量与学习率针对不同数据集设置。<br><br>## 论文使用的评估环境和评估指标<br>- 仿真评估：<br> - Simpler WidowX：4套任务、每套120次试验，报告各任务及平均成功率。<br> - LIBERO（Franka Panda）：四个suite（Spatial/Object/Goal/Long），总计2000次试验，每个suite报告成功率。<br>- 真实评估：<br> - Galaxea R1 Lite双臂移动平台：设计“餐桌清理”“微波操作”“方块叠放”三类任务。<br> - 指标：进度分数（每步成功计一分），每任务平均20次；并提供少样本微调（20条轨迹）设置下的比较。<br>- 推理与资源指标：显存（8.0GB vs 6.7GB）、延迟（210ms/步 vs 190ms/步），参数增加约600M。</details> |
| 2025-10-15 | Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models | http://arxiv.org/abs/2510.13237 | <details><summary>展开</summary>待生成</details> |
| 2025-10-15 | RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation | http://arxiv.org/abs/2510.13149 | <details><summary>展开</summary>### 论文研究单位<br>南京大学、香港科技大学、新加坡国立大学、上海交通大学、上海创新研究院等机构联合研究<br><br>### 论文概述<br>机器人长期操作任务中的组合泛化能力面临挑战，现有模型在扰动条件下难以有效组合技能。针对此问题，研究提出RoboHiMan层次化评估范式，通过HiMan-Bench基准测试系统评估模型在原子任务和组合任务上的泛化能力，结合多级训练数据集和三种评估模式，分析分层架构的瓶颈。<br><br>### 论文核心贡献点<br>1. **构建HiMan-Bench基准**：包含114个原子任务和144个组合任务，覆盖12种扰动因素（颜色、纹理、光照等）<br>2. **设计层次化评估范式**：提出Vanilla、Decoupled、Coupled三种评估模式，分离测试规划和执行能力<br>3. **分析模型能力缺口**：发现VLA模型在组合任务和扰动场景下性能显著下降，揭示分层架构的脆弱性<br><br>### 论文方法描述<br>- **基准测试设计**：<br> - 原子任务：10类基础操作（开抽屉、拿取物品等）<br> - 组合任务：12类多步骤操作（取物放抽屉、转移物品等）<br> - 扰动因素：对象颜色/纹理/大小、接收对象属性、光照、桌面纹理、干扰物、背景纹理、相机位姿<br>- **多级训练数据集**：<br> - L1：仅原子任务演示（每任务20条）<br> - L2：添加扰动原子任务（每任务1条）<br> - L3：添加4个组合任务（每任务5条）<br> - L4：添加扰动组合任务（每任务1条）<br>- **评估模式**：<br> - Vanilla：端到端执行无规划器<br> - Decoupled：规则规划器/视觉语言模型规划器独立评估<br> - Coupled：端到端分层系统（视觉语言模型规划器+低层策略）<br><br>### 论文使用数据集和训练资源<br>- **仿真环境**：基于CoppeliaSim/PyRep构建的HiMan-Bench<br>- **训练数据**：四个多级数据集（L1-L4），总演示量递增<br>- **模型**：四类VLA模型（RVT-2、3D Diffuser Actor、π₀、π₀.₅）<br>- **规划器**：Qwen2.5-VL视觉语言模型（用于高层规划）<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真测试：114个原子任务和144个组合任务，每任务15个无扰动测试+5个扰动测试+5个全扰动测试<br> - 真实世界验证：设计小规模原子/组合任务集，测试扰动鲁棒性<br>- **评估指标**：<br> - **原子任务**：720集评估，平均成功率（A/AP任务）<br> - **组合任务**：900集评估，平均成功率（C/CP任务）<br> - **规划准确率**：离线评估视觉语言模型的分任务预测精度<br> - **性能下降率**：在线评估对比离线基准的降幅（如L4→C&CP任务下降0.382）</details> |
| 2025-10-15 | VLA-0: Building State-of-the-Art VLAs with Zero Modification | http://arxiv.org/abs/2510.13054 | <details><summary>展开</summary>## 论文研究单位<br>**NVIDIA**<br><br>## 论文概述<br>VLA-0 探讨了一种简洁的视觉语言动作模型（VLA）构建方法。该方法直接利用预训练视觉语言模型（VLM）的文本生成能力，通过自然语言字符串表示机器人动作（如坐标、关节角度），而非修改模型架构或引入新的动作表示机制。核心创新在于，通过精心设计的训练和推理配方（如动作集成预测和遮蔽动作增强），这种极简设计在性能上可与更复杂的VLA方法媲美。实验表明，在 LIBERO 仿真基准测试中，VLA-0 在不使用大规模预训练数据的情况下超越了所有基线方法；在真实世界评估中，也击败了基于SO-100数据集预训练的SmolVLA。<br><br>## 论文核心贡献点<br>1. **挑战VLA设计复杂性：** 证明了基于纯文本动作表示的简单VLA设计（无需修改VLM）可达到甚至超越更复杂的VLA架构（如离散token、生成式动作头或定制架构）的性能。<br>2. **提出有效训练配方：** 详细阐述了实现高性能的关键技巧：动作文本解码（如归一化到整数）、推理阶段的动作集成预测（平均多个时间步预测）、训练阶段的遮蔽动作增强（随机遮蔽字符以强制模型基于视觉信息推理）。<br><br>## 论文方法描述<br>* **核心原理：** VLA-0 直接提示预训练的VLM（使用Qwen-VL-2.5-3B）输出动作文本序列。输入包含系统提示（定义任务）、图像（单一或拼接）、任务指令。<br>* **动作表示：** 连续机器人动作（如末端执行器位置）被归一化到固定整数范围（如[0, 1000]），VLM输出该范围内的空间分隔整数序列，代表未来多个时间步的动作。<br>* **推理阶段（关键配方）：**<br> * **动作集成预测：** 模型预测未来n步动作。对当前时间步t，可获得t、t-1、...、t-n+1时刻预测的t步动作（作为其预测序列中的第一个或后续动作）。对这些n个预测取平均作为最终动作输出，增强稳定性。<br> * **动作文本解码：** 解析VLM输出文本为具体动作值。<br>* **训练阶段（关键配方）：**<br> * **遮蔽动作增强：** 随机遮蔽动作字符串中的字符（如替换为占位符），迫使模型不依赖序列自回归完成，而是根据视觉和指令信息推理目标动作。<br> * **优化：** 使用Adam优化器，64个epoch，批量大小192，学习率5e-6，在8个A100 GPU上训练约32小时。<br><br>## 论文使用数据集和训练资源<br>* **仿真数据：** 基于 LIBERO 基准测试。包含四个套件（空间推理、目标泛化、目标导向、长期任务），每个套件10个任务。VLA-0 在LIBERO内域数据上训练，未使用大规模预训练数据。<br>* **真实数据：** 基于 SO-100 机器人平台和LeRobot框架。四个不同任务（重定位方块、推动苹果、拾取放置香蕉/纸杯蛋糕），每个任务100个演示。<br>* **训练资源：** 使用8个 NVIDIA A100 GPU 进行约32小时的训练。<br><br>## 论文使用的评估环境和评估指标<br>* **评估环境：**<br> * **仿真：** LIBERO 基准测试（四个套件，40个任务）。<br> * **真实世界：** SO-100 机器人平台（基于LeRobot框架）。<br>* **评估指标：**<br> * **主要指标：** 任务成功率（Success Rate）。<br> * **报告方式：** LIBERO 报告每个套件（10个任务）的平均成功率及总体平均成功率；SO-100 报告各任务成功率及总体平均成功率。所有评估均在固定次数的回合（如LIBERO每任务50回合）上进行。</details> |
| 2025-10-14 | DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving | http://arxiv.org/abs/2510.12796 | <details><summary>展开</summary># DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving<br><br>## 论文研究单位<br>中国科学院自动化研究所模式识别国家重点实验室(NLPR)与银网智能科技有限公司联合完成<br><br>## 论文概述<br>DriveVLA-W0提出以世界模型为核心的训练范式，缓解VLA模型在稀疏动作监督下的“监督赤字”问题。该方法通过预测未来图像，引入密集的自监督信号，同时适配离散与连续视觉表征的两类主流VLA架构，并在端到端自主驾驶中实现实时推理与更高数据扩展性。<br><br>## 论文核心贡献点<br>- 识别并解决VLA的“监督赤字”：仅用低维动作信号无法充分驱动大规模VLA模型，世界模型通过预测未来图像提供密集监督<br>- 设计跨架构世界建模：离散视觉token采用自回归(AR)世界模型，连续视觉特征采用扩散世界模型<br>- 提出轻量化MoE Action Expert：与大型VLA主干通过联合注意力融合，提升推理效率(延迟降至原VLA的63.1%)<br>- 揭示动作解码器的扩展规律逆转：在小数据上优的query/flow-matching在大规模数据下被自回归解码超越<br>- 放大数据扩展律：在大规模训练中，世界模型持续提升性能，显著优于仅动作监督的基线<br><br>## 论文方法描述<br>- VLA基线：处理语言指令、前视图像与历史动作，采用Emu3-8B(VQ离散token)与Qwen2.5-VL-7B(连续ViT特征)双骨干，输出语言/视觉/动作三类特征，动作预测采用交叉熵损失<br>- AR世界模型(DriveVLA-W0(VQ))：自回归预测当前图像的离散视觉token，训练目标为下一token预测损失，总损失为动作损失加权的世界模型损失<br>- 扩散世界模型(DriveVLA-W0(ViT))：在潜空间进行条件扩散，预测未来帧图像，使用MSE对噪声预测网络优化，总损失为动作损失加权的扩散世界模型损失<br>- 两阶段训练：先联合世界模型与动作目标预训练，再结合Action Expert进行动作微调；输入序列采用深交错的多模态时间拼接<br>- Action Expert(MoE架构)：大型VLA Expert与500M轻量专家通过联合注意力融合，支持三种动作解码策略(查询式回归、自回归、Flow Matching)<br>- 实时性：推理阶段可跳过显式的图像生成过程以保障低延迟<br><br>## 论文使用数据集和训练资源<br>- 数据集：NAVSIM v1/v2(基于OpenScene的安全关键场景基准)与大规模内部数据集(7000万帧、100万视频片段，100个挑战场景)<br>- 训练资源：NAVSIM实验使用8×L20 GPU(global batch=48)；内部数据集使用64×GPU(global batch=256)；统一使用AdamW与余弦学习率、bfloat16混合精度<br>- 对比基线：复现实验的TransFuser-50M与TransFuser-7B(单前视相机配置)<br><br>## 论文使用的评估环境和评估指标<br>- NAVSIM v1：NC、DAC、TTC、舒适度C、EP，综合指标PDMS<br>- NAVSIM v2：NC、DAC、DDC、TLC、EP、TTC、LK、HC、EC，综合指标EPDMS<br>- 内部数据集：3秒6点轨迹的ADE(↓)与碰撞率(↓)，碰撞率计算方法与NC一致<br>- 扩展评估：跨域迁移(动作分布差异)、数据扩展律(70k/700k/70M帧)、延迟分析(H200 GPU)、生成保真度与规划性能的关联分析</details> |
| 2025-10-14 | Reflection-Based Task Adaptation for Self-Improving VLA | http://arxiv.org/abs/2510.12710 | <details><summary>展开</summary>待生成</details> |
| 2025-10-14 | Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model | http://arxiv.org/abs/2510.12276 | <details><summary>展开</summary>### 论文研究单位<br>香港科技大学（广州）、清华大学、西湖大学、浙江大学、华南理工大学等机构联合完成<br><br>### 论文概述<br>针对视觉-语言-行动（VLA）模型缺乏3D空间理解能力的问题，论文提出**空间强制（Spatial Forcing, SF）**方法。该方法通过将VLA模型的中间视觉表征与3D基础模型（VGGT）的几何表征对齐，在不依赖显式3D传感器或深度估计的条件下，隐式增强VLA的空间推理能力。<br><br>### 论文核心贡献点<br>1. **深度探测实验**：验证原始VLA视觉嵌入缺乏有效空间结构<br>2. **空间强制（SF）机制**：将VLA中间视觉表征与VGGT输出的空间表征对齐<br>3. **显著性能提升**：在仿真和真实环境中超越现有方法，同时提升训练效率（3.8×）和数据利用率<br><br>### 论文方法描述<br>- **对齐框架**：<br> - 使用VGGT处理多视图图像，生成像素级空间表征<br> - 对VLA中间层视觉令牌进行归一化和MLP变换<br> - 通过余弦相似度最大化对齐视觉令牌与VGGT的空间表征<br>- **监督策略**：监督中层而非最深层（防止视觉特征丢失）<br>- **总损失函数**：结合动作生成损失与对齐损失<br>- **推理阶段**：与标准VLA模型一致，无额外计算开销<br><br>### 论文使用数据集和训练资源<br>- **仿真数据**：<br> - LIBERO基准（Spatial/Object/Goal/Long任务套件，共500专家演示）<br> - RoboTwin（双机械臂仿真，含简单/困难模式）<br>- **真实数据**：双机械臂AgileX平台（主相机+腕部相机）<br>- **计算资源**：<br> - 训练：8×NVIDIA H100显卡（LIBERO任务），1×H100（RoboTwin任务）<br> - 训练迭代：15万次（LIBERO），3万次（RoboTwin）<br> - 权重因子α：0.5<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **仿真环境**：LIBERO（四类任务）、RoboTwin（双机械臂）<br> - **真实环境**：AgileX双机械臂平台<br>- **评估指标**：<br> - 任务成功率（Success Rate, SR）<br> - 训练收敛速度（迭代次数比较）<br> - 数据效率（不同数据比例下的成功率）<br> - t-SNE可视化验证表征对齐效果<br>- **实验结果**：<br> - LIBERO平均SR达98.5%（超越所有2D/3D方法）<br> - 训练效率提升3.8×（相同成功率所需迭代减少）<br> - 5%数据下实现75.8% SR（数据效率提升5.9×）<br> - 真实任务中透明杯堆叠等场景下SR显著提升<br><br>论文主页：https://spatial-forcing.github.io/</details> |
| 2025-10-13 | ManiAgent: An Agentic Framework for General Robotic Manipulation | http://arxiv.org/abs/2510.11660 | <details><summary>展开</summary>论文研究单位<br>北京工业大学、南京大学、中国科学技术大学、Dexmal<br><br>论文概述<br>论文提出了ManiAgent，一个用于通用机器人操作任务的智能体框架。该框架通过多个专门的智能体进行协作，实现从任务描述和环境输入到机器人操作动作的端到端输出。它旨在解决现有视觉-语言-动作（VLA）模型在复杂推理和长时程任务规划中因数据稀缺和模型容量受限而表现不佳的问题。ManiAgent在SimplerEnv基准测试中达到了86.8%的成功率，在真实世界的拾取和放置任务中达到了95.8%。<br><br>论文核心贡献点<br>- 提出了一个名为ManiAgent的端到端智能体框架，可直接生成用于通用机器人操作任务的可执行动作序列。<br>- 设计了一个感知-推理-控制的流水线，通过协调三个专门的智能体，整合空间感知、任务推理和动作规划，以执行复杂任务。<br>- 进行了大量实验验证ManiAgent的卓越性能，其高成功率使其能作为一个全自动的数据收集工具，为其他基于学习的机器人操作方法提供有力支持。<br><br>论文方法描述<br>ManiAgent框架包含三个协同工作的专门智能体：感知智能体、推理智能体和控制器智能体。<br>1. 感知智能体：利用视觉语言模型（VLM）处理场景图像和用户指令，生成任务相关的场景描述。当VLM不足以进行精确空间定位时，会调用Florence-v2等目标检测方法获取物体的3D坐标和抓取姿态。<br>2. 推理智能体：作为核心，接收场景描述和任务指令，利用大语言模型（LLM）进行状态评估和子任务分解。其分解过程是增量的，根据场景变化逐步调整，并存储历史子任务以防局部循环。<br>3. 控制器智能体：负责将子任务和物体细节（中心位置、抓取姿态）转换为由LLM生成的可执行动作序列。为降低延迟，该智能体采用了一个缓存机制，对于已执行过的子任务，会直接检索并复用参数化的动作序列。<br><br>论文使用数据集和训练资源<br>- 框架本身是免训练的，依赖于预训练的大语言模型（LLM）和视觉语言模型（VLM）。<br>- 仿真实验使用SimplerEnv数据集，具体环境为BridgeTable-v1和BridgeTable-v2，包含4个操作任务。<br>- 真实世界实验在自定义任务集上进行，共设计了8个任务来评估不同维度的能力。<br>- 物理平台使用WidowX-250s机械臂和两个Realsense D435摄像头。<br>- 实验中评估了多种商业和开源模型，包括GPT-4o, GPT-5-nano, GPT-5, Claude-4-sonnet, Grok-4, GPT-oss-120b, Qwen-3-235b。<br>- 组件方面，使用了Florence-v2进行目标检测，AnyGrasp进行抓取姿态生成。<br><br>论文使用的评估环境和评估指标<br>- 评估环境：<br> - 仿真环境：SimplerEnv平台的BridgeTable-v1和BridgeTable-v2。<br> - 真实环境：配备WidowX-250s机械臂和Realsense D435摄像头的物理平台。<br>- 评估指标：<br> - 主要指标：成功率，即成功完成的任务次数与总尝试次数的比例。<br> - 真实世界任务评估维度：<br> - 非拾取与放置能力<br> - 高泛化能力<br> - 相对位置感知能力<br> - 意图推理能力<br> - 知识检索与利用能力<br> - 多步任务规划能力<br> - 自动数据收集任务成功标准：物体最终位置与目标位置的距离小于15厘米。</details> |
| 2025-10-13 | Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning | http://arxiv.org/abs/2510.11027 | <details><summary>展开</summary># Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning<br><br>## 论文研究单位<br>- 论文由上海AI实验室主导，联合中国科学技术大学、上海交通大学、浙江大学、南京大学、复旦大学、清华大学、NUS、东北大学、深圳大学等多机构合作完成。<br>- 通讯作者来自上海AI实验室；项目页面与代码仓库公开。<br><br>## 论文概述<br>- 针对上游VLM（Vision-Language Model）的具身推理能力与下游VLA（Vision-Language-Action）策略学习之间的关键差距，提出Vlaser：一个将高阶推理与低阶控制协同的VLA基础模型。<br>- 构建高质量Vlaser-6M数据引擎，形成600万规模的多任务具身推理语料；系统分析不同VLM初始化与数据类型对VLA微调的影响，缩小互联网数据与具身策略学习之间的域间偏移。<br>- 在12项具身推理基准与闭环机器人任务（WidowX/Google Robot）上取得领先结果，公开模型、数据与代码。<br><br>## 论文核心贡献点<br>- 统一的具身VLA架构与两阶段训练范式：将InternVL3扩展出“动作专家”，通过流匹配实现未来动作chunk预测，兼顾视觉、语言与控制。<br>- 数据引擎与方法论创新：系统整合并增广多源公共数据集，构建覆盖定位（grounding）、空间推理、规划与通用VQA的Vlaser-6M；进一步在SimplerEnv中生成200万面向VLA的“域内”QA数据，缓解视觉域偏移。<br>- 实证与洞见：上游具身推理的提升未必转化为下游VLA闭环成功率；与互联网数据相比，针对具体机器人本体与视角的“域内”数据更能加速收敛与提升成功率，并明确域间偏移的核心问题。<br><br>## 论文方法描述<br>- 模型结构<br> - VLM骨干：基于InternVL3（InternViT图像编码 + Qwen2.5语言模型），尺寸为2B与8B，强调具身常识与端到端控制。<br> - 动作专家：在VLM上增设低层控制分支；采用流匹配（flow matching）学习动作向量化场（vector field），以单帧观测、语言指令与机器人状态token为输入，生成未来H步动作序列；VLA流使用非因果注意力。<br> - 训练与推理损失：VLM阶段采用自回归语言建模损失；VLA阶段最小化预测向量场与真实去噪向量场的MSE；推理时用欧拉积分从τ=0至τ=1对随机噪声进行去噪，生成动作。<br>- 数据引擎<br> - 具身定位：1.5M问答（边界框与中心点，标准化至[0,1000]），来源RoboPoint、ShareRobot、Pixmo-Points、Paco-LaVIS、RefSpatial；并从SA-1B分割掩码增广30万标注。<br> - 通用/空间推理：1.2M通用RoboVQA + 0.5M空间推理数据；源RoboVQA、Robo2VLM、RoboPoint、SPAR、SpaceR-151k、VILASR，以及ScanNet/ScanNet++/CA-1M/ARKitScenes的10万手工空间样本。<br> - 规划：40万数据，含Alpaca-15k-Instruction、MuEP、WAP，以及LLaRP在Habitat的规划轨迹与EgoPlan-IT/EgoCOT的第一人称视频规划样例。<br> - VLA域内数据：200万针对WidowX与Google Robot的SimplerEnv生成问答，涵盖定位、空间、规划、通用VQA等类别。<br>- 训练配方<br> - 阶段一：监督微调InternVL3构建具身VLM骨干。<br> - 阶段二：在机器人数据集上对动作专家进行VLA微调；使用流匹配损失训练，H=4，积分步长δ=10。<br><br>## 论文使用数据集和训练资源<br>- 具身推理评估数据（12项基准）：ERQA、Ego-Plan2、Where2place、Pointarena、Paco-Lavis、Pixmo-Points、VSI-Bench、RefSpatial-Bench、MMSI-Bench、VLABench、EmbodiedBench（ALFRED/Habitat）。<br>- 机器人闭环评估：SimplerEnv（Bridge/WidowX 与 Google Robot 任务集），包含超过5百万帧图像与机器人 эпизодов。<br>- 数据引擎与规模：Vlaser-6M由多源公共数据整合与增广（定位、空间推理、规划、通用VQA），以及来自SimplerEnv的200万域内VLM预训练问答对。<br>- 预训练骨干：InternVL3-2B/8B（InternViT + Qwen2.5-1.5B/7B），动作专家采用流匹配架构；公开模型、训练与推理代码。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境<br> - 具身推理：12项公开基准，涵盖问答、任务规划、定位、空间推理与仿真闭环评估。<br> - 闭环机器人控制：SimplerEnv中的WidowX与Google Robot平台（Pick Coke Can、Move Near、Drawer、Carrot on plate、Put eggplant in basket、Spoon on towel、Stack Cube等任务）。<br>- 评估指标<br> - 具身推理：各基准的任务分数与标准化平均分（Avg）。<br> - 闭环控制：成功率（Success Rate），分任务与平均成功率。<br> - 训练分析：收敛速度与跨域泛化表现（域内vs互联网数据对VLA微调的影响）。</details> |
| 2025-10-13 | RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model | http://arxiv.org/abs/2510.10975 | <details><summary>展开</summary>### 论文研究单位<br>深圳先进技术研究院（中国科学院）、鹏城实验室、中山大学计算机科学与工程学院、南洋理工大学计算与数据科学学院、上海AI实验室、中国科学院大学、X-Era AI Lab<br><br>### 论文概述<br>RoVer是一个针对视觉-语言-动作（VLA）模型的外部测试时缩放框架，旨在通过引入一个紧凑的机器人过程奖励模型（PRM）作为测试时验证器，在不修改原始VLA架构或权重的情况下增强其性能。该方法通过生成多个候选动作并使用PRM预测标量过程奖励和动作空间方向，以选择最优动作执行。关键是通过缓存共享感知特征来摊销感知成本，从而实现高效的测试时计算资源分配。<br><br>### 论文核心贡献点<br>1. **通用测试时缩放框架**：提出一个即插即用的外部测试时缩放框架，纯在推理时增强冻结的VLA策略，无需额外数据或模型重训练。<br>2. **紧凑过程奖励模型**：设计一个同时输出标量过程奖励和动作细化方向的PRM，实现基于验证器的智能候选动作评估和方向引导探索。<br>3. **高效方向引导采样策略**：利用共享感知缓存机制，将感知成本摊销到多个候选动作上，在固定测试时计算预算下支持可扩展的候选生成和选择。<br><br>### 论文方法描述<br>RoVer的整体方法分为三个部分：<br>- **模型架构**：基于GPT-2风格的transformer，初始化自GR-1（使用MAE和CLIP预训练权重），添加奖励和方向预测头。引入共享感知缓存（编码观测、语言和状态特征一次并复用）和动作放大器（MLP结构，强化动作细微差异）。<br>- **模型训练**：采用方向监督和偏好学习（Bradley-Terrry损失）训练PRM。数据准备通过锚定中心采样构造“更好/更差”动作对，并预测从当前动作到专家动作的方向向量。目标函数结合方向对齐损失（余弦相似度）和奖励偏好损失。<br>- **方向引导测试时缩放**：推理时从基础策略采样N个候选动作，沿PRM预测的方向在有界角度内扩展M个新候选（统一候选预算K=N+M），PRM评分所有候选后选择最优动作执行。方向引导策略优于随机高斯采样，通过集中探索提升效率。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：模拟实验使用CALVIN基准（ABC→D设置：训练集为A、B、C环境，测试为未见环境D），仅用20%训练集样本；真实机器人任务基于自定义场景（抓取放置、推按钮、堆叠碗）。<br>- **训练资源**：PRM训练使用CALVIN ABC→D训练集100个epoch；模型总参数0.2B（可训练参数40M，冻结MAE和CLIP编码器）；推理在单张NVIDIA V100 GPU上运行，测试时计算预算可配置（如候选数K=10~10000）。<br><br>### 论文使用的评估环境和评估指标<br>- **模拟评估环境**：CALVIN基准（长时序语言条件任务），评估设置包括ABC→D跨环境迁移。指标为SR@k（连续完成k个任务的概率，k=1-5）和平均链长（Average Chain Length）。<br>- **真实机器人评估环境**：双机械臂Dobot平台（右侧执行任务，左侧固定），使用腕部相机和俯视相机。任务涵盖Seen、Unseen object和Unseen position条件。指标为成功率（%，每条件10次试验）。<br>- **效率评估**：使用共享感知缓存对比无缓存的延迟（秒）和加速比（倍），测试候选数10~10000下的吞吐扩展性。</details> |
| 2025-10-11 | X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model | http://arxiv.org/abs/2510.10274 | <details><summary>展开</summary># X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model<br><br>- 论文研究单位<br> - 清华大学AIR（Institute for AI Industry Research）、上海AI Lab、北京大学等合作完成<br><br>- 论文概述<br> - 目标是构建通用、可扩展的跨实体（跨机器人）视觉-语言-动作（VLA）模型，在不同硬件、数据和环境中实现稳健的泛化与快速适应<br> - 提出软提示（Soft Prompt）机制，将每个数据源的硬件配置、相机设置、任务分布等异质性映射为可学习的轻量嵌入，作为“实体特定提示”，在特征融合早期引导模型学习<br> - 设计X-VLA架构：采用流匹配（flow-matching）训练策略，基于标准Transformer编码器堆叠实现多模态输入的可扩展融合，支持多视角视觉、语言指令和本体感觉的联合编码<br> - 训练管线分两阶段：预训练阶段在异质混合数据上学习“实体无关”的通用策略；域适应阶段冻结主干，仅调优新域的软提示和少量参数，实现高效迁移<br><br>- 论文核心贡献点<br> - 软提示学习：无需手工模板和自然语言描述，以可学习嵌入编码跨实体异质性，稳定预训练并促进快速适应<br> - 简化而可扩展的Transformer架构：用标准自注意力Transformer替代扩散式解码器，保持通用性与简洁性；针对高维（多视角视觉、语言）和低维（本体感觉、动作）模态采用专用编码与早期融合<br> - 自定义训练配方：包括对齐的动作表示、意图抽象（时间下采样构建高层意图锚点）、平衡采样、以及两阶段适应（软提示预热后联合优化）来提升训练稳定性和迁移效率<br> - 实验验证：在6个仿真基准（含自动驾驶）和3个实体机器人上进行广泛评测，实现SOTA结果；此外用少量演示（1,200条）完成高难度的双手机器人“布料折叠”任务，并以1%参数（9M）微调实现接近完整微调的性能<br><br>- 论文方法描述<br> - 异质软提示学习：为每个数据源维护一组可学习的嵌入（软提示），近似映射硬件配置到提示空间；在动作生成早期注入，引导主干进行实体感知的学习<br> - 架构设计：将主视觉-语言流与辅助视角（如腕部相机）分离编码；本体感觉与动作tokens（含时间嵌入）轻量投影后与视觉语言特征早期融合；整体采用标准Transformer堆叠进行融合与精确动作生成<br> - 训练目标：使用流匹配策略，通过学习速度场将高斯噪声逐步传输到目标动作块；采用最优传输路径的线性插值监督，训练目标为预测速度与真实速度之间的均方误差<br> - 数据处理与训练配方<br> - 动作对齐：将动作标准化为末端执行器（EEF）位姿表示（xyz位置、Rotate6D旋转、二进制夹爪状态），分别用MSE和BCE监督<br> - 意图抽象：时间下采样将低层动作轨迹抽象为未来4秒内的30个锚点，增强高层意图建模<br> - 平衡采样：在跨域与域内同时随机打乱，缓解分布偏置与过拟合<br> - 两阶段适应：先在冻结主干下预热新软提示，随后与主干联合微调<br> - 学习率策略：对软提示与视觉-语言模块使用较低学习率，保留预训练表示并稳定优化<br><br>- 论文使用数据集和训练资源<br> - 预训练数据混合：Droid、RoboMind、Agibot等，约290K片段，覆盖7个平台与5类机械臂（单臂到双臂）<br> - X-VLA-0.9B实例：隐藏维度1024、24层Transformer，预训练在异质混合数据上进行；后续在仿真与真实环境进行适配<br> - 软提示与适配：每个数据源一组软提示；域适应中引入新软提示进行轻量化适配<br> - 参数高效微调（PEFT）：采用LoRA等方法，仅调优约9M参数（约1%）实现接近全参数微调的效果<br> - 实际任务数据：构建高质量“布料折叠”数据集Soft-FOLD，包含1,200条演示；并在BridgeData-v2等真实机器人协议上进行评测<br><br>- 论文使用的评估环境和评估指标<br> - 仿真基准：Libero、Simpler、VLABench、RoboTwin-2.0、Calvin、NAVSIM（覆盖单臂、双臂、跨域/跨任务/跨环境与自动驾驶场景）；在多项基准中达成新SOTA<br> - 真实机器人：三类实体平台，分别测试简单操控、灵巧操控和快速适应（基于PEFT），任务成功率、任务完成率/吞吐量为主要指标<br> - 关键结果概览<br> - X-VLA-0.9B在多个仿真基准上超越既有SOTA（示例：Simpler-WidowX达约96%、Libero约98%、Calvin首阶段约96%）<br> - 真实实验中在五类任务上显著优于基线；布料折叠任务达到接近100%成功率，约33次/小时的折叠吞吐<br> - 以LoRA仅调优9M参数，在Libero与Simpler-WidowX上达到约93%与54%成功率，接近或可比全参数微调的强基线<br> - 代理指标：预训练阶段使用ℓ1动作预测误差作为下游适配性能的代理指标，并与域适应成功率呈现强相关性</details> |
| 2025-10-11 | Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback | http://arxiv.org/abs/2510.10181 | <details><summary>展开</summary>### 论文研究单位<br>上海交通大学计算机科学学院<br><br>### 论文概述<br>论文提出Dejavu框架，用于具身代理的后部署学习。该框架通过经验反馈网络（EFN）增强冻结的视觉-语言-行动（VLA）策略。EFN检索与当前上下文相关的历史成功行动经验，并预测残差行动来修正基础策略输出。在部署期间，EFN持续从新经验中学习，尽管基础策略权重固定，但能提升代理的适应性和成功率。实验基于LIBERO模拟器和AgiBot G1机器人，涵盖OpenVLA、UniVLA和GO-1骨干，显示EFN显著改善部署时性能。<br><br>### 论文核心贡献点<br>- 引入EFN作为以经验为中心的部署时机制，结合实时体验库和轻量级控制器，改进冻结VLA策略，无需梯度重训练。<br>- 形式化体验为同步视觉-语言-行动轨迹，并实现语言条件视觉相似度的检索机制。<br>- 将EFN集成至OpenVLA、UniVLA和GO-1骨干，在模拟和真实环境中实现一致的部署时改进。<br><br>### 论文方法描述<br>- **体验库设计**：存储步骤级轨迹（图像、视觉令牌、潜在行动），通过均值-最大融合构造紧凑关键向量，采用余弦相似度的概率top-k检索。<br>- **残差策略学习**：使用软演员-评论家（SAC）算法训练EFN，以预测残差行动；奖励函数基于语义相似度（当前观察与检索经验的下一观察匹配），并包含反空闲惩罚。<br>- **部署时机制**：指令过滤候选集、步进检索与效率优先（倾向短轨迹）、行动修正和执行、在线体验增长（将成功轨迹加入库）。<br>- **训练细节**：EFN输入包括当前观察、基础行动和检索经验；上下文编码后通过行动者网络输出残差，评论家网络评估修正行动；损失函数基于SAC目标。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：模拟实验使用LIBERO数据集；真实实验基于AgiBot G1机器人。<br>- **训练资源**：VLA骨干包括OpenVLA、UniVLA和GO-1；使用Prismatic预训练管道、Flash Attention；LIBERO模拟器用于模拟，AgiBot G1硬件用于现实实验；EFN训练在模拟环境中进行，依赖CUDA加速。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：模拟实验在LIBERO平台上进行；真实实验在AgiBot G1机器人上进行。<br>- **评估指标**：主要指标为成功率（Success Rate）和成功时的平均步数（Steps）；例如，LIBERO四子任务（Spatial、Object、Goal、Long）的成功率提升和步数减少；真实世界任务（PutBottle、SortItem、AddGoods）显示类似改进。基准比较包括EFN与OpenVLA/UniVLA/GO-1的对比，以及不同体验库容量（Volume 100-1000）的消融研究。</details> |
| 2025-10-11 | Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models | http://arxiv.org/abs/2510.09976 | <details><summary>展开</summary>## 论文研究单位<br>- 脑认知与脑启发智能实验室（中科院自动化所）<br>- 中国科学院大学<br>- 脑认知与脑启发智能技术国家重点实验室<br>- Long-term AI<br><br>## 论文概述<br>针对基于条件流匹配（flow matching）的视觉-语言-动作模型（VLA，如 π0），传统策略梯度强化学习方法由于无法计算重要性采样比而难以应用。本文提出“流策略优化”（Flow Policy Optimization，FPO）：用基于条件流匹配（CFM）损失的样本级变化构造“无似然比率”，替换传统重要性采样，实现 PPO 风格的截断代理目标更新；并结合结构感知信用分配、截断代理、潜空间多步欧拉探索与 Q 值集合体，构建稳定且可扩展的在线强化微调框架。在 LIBERO 与 ALOHA Transfer Cube 仿真任务上进行评估，FPO 在多项指标上超越多类强基线，并呈现稳定的学习曲线与稀疏奖励下的有效探索。<br><br>## 论文核心贡献点<br>- 首次实现流匹配策略与 PPO 风格策略梯度更新的兼容：无须显式似然或雅可比，计算“可通约的比率代理”。<br>- 将样本级 CFM 损失差作为策略改进步信号（结构对齐的信用分配），并配套截断代理以控制更新幅度。<br>- 引入潜空间多步欧拉探索与 Q 值集合体（保守目标与 GAE），提升在线学习的稳定性与效率。<br>- 在 LIBERO（四个子套件）与 ALOHA Transfer Cube 任务上，π0-FPO 取得一致优于多类强基线的效果（含在线 RL、偏好对齐与扩散/自回归策略），并稳定收敛。<br><br>## 论文方法描述<br>- FPO 采用“冷启动/滚动-更新”交替流程：冻结旧策略进行滚动收集经验（轨迹、奖励、隐变量与 CFM 损失缓存），随后在当前策略上重评 CFM 损失，将损失差标准化并映射为无似然比率 ρt，最终采用截断代理目标进行策略更新。<br>- 无似然比率：从旧/新策略在同一样本上的 CFM 损失差 Δℓcfm 出发，经批内标准化 zt = (Δℓcfm − μΔ)/σΔ 与单调指数映射 ρt = exp(β zt)，作为 PPO 截断代理中的重要性比替代。<br>- 截断代理目标：Lactor(θ) = −E[min(ρt Ât, clip(ρt, 1−ε, 1+ε) Ât)]，其中优势 Ât 由 GAE 估计。<br>- Q 值集合体：以最小化集合成员的保守目标 y_t = r_t + γ min_i Q̄_ϕi(s_{t+1}, x′_{t+1}) 进行时序差分训练，并用 Polyak 平均更新目标网络，以减少过估计与不稳定性。<br>- 潜空间探索：在隐变量 x 上使用 CFM 速度场 vθ 进行多步欧拉积分生成平滑、时间相关的扰动，增强探索。<br>- 数据管理：使用小滑窗轨迹缓存，保持更新策略与数据收集策略的分布接近，保证比率代理的稳定与可信。<br><br>## 论文使用数据集和训练资源<br>- 数据与预训练：采用 π0 预训练检查点作为起点，冻结其解码器 π0，仅在线更新流匹配 Actor 与 Q 集合体。<br>- 训练方式：在仿真环境中进行在线交互与强化微调，无需大量监督演示数据。细节包括滚动窗口大小、K_update 更新轮次、β 与 ε 等超参数的选择与设置。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：LIBERO 基准（ LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long）与 ALOHA Transfer Cube 仿真任务；二者均为接触丰富与稀疏奖励的典型具身控制环境。<br>- 评估指标：按官方协议报告任务成功率（SR, %），并在 LIBERO 上给出各套件的平均排名；在学习动力学分析中结合成功率与平均回报曲线；消融研究则报告在指定任务上的最终成功率。</details> |
| 2025-10-10 | VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation | http://arxiv.org/abs/2510.09607 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>南京大学、腾讯Youtu Lab、中科院(CASIA)<br><br>## 论文概述<br>VITA-VLA论文提出了一种高效的教学框架，通过动作专家蒸馏技术将小型动作模型的知识转移到预训练的大规模视觉-语言模型(VLM)中，使其具备动作执行能力。该方法解决了传统VLA模型需要大量计算资源和数据进行端到端训练的局限，通过两阶段训练策略显著降低训练成本，同时保持高性能表现。在仿真基准(LIBERO和CALVIN)和真实机器人环境中的实验验证了该方法的有效性。<br><br>## 论文核心贡献点<br>1. **精简架构设计**：在保持原始VLM结构的基础上，仅添加动作token和状态编码器，集成物理输入信息<br>2. **两阶段训练策略**：第一阶段进行轻量级对齐(仅3000万参数)，第二阶段选择性微调，有效降低训练成本<br>3. **优秀性能表现**：在LIBERO基准上达到97.3%平均成功率(提升11.8%)，在LIBERO-LONG上达到93.5%成功率(提升24.5%)，在CALVIN ABC-D基准上达到92.5%第一任务成功率<br>4. **真实世界验证**：在ALOHA机器人平台上实现82.0%平均成功率，较teacher模型提升17%<br><br>## 论文方法描述<br>### 整体架构<br>- **视觉编码器**：InternViT-300M处理图像输入<br>- **连接器**：3层MLP桥接视觉和语言模态<br>- **语言模型**：Qwen-2.5-7B作为基础<br>- **状态编码器**：将6-DoF机械臂状态和2维夹爪状态编码为单token<br>- **动作token**：作为可学习查询，重复3次预测未来3步动作<br>- **动作映射器**：3层MLP将VLM隐藏状态映射到动作空间<br>- **动作解码器**：复用的2层MLP生成最终执行动作<br><br>### 两阶段训练策略<br>**阶段1-对齐阶段**：<br>- 训练状态编码器、动作token和动作映射器(约3000万参数)<br>- 使用MSE损失对齐VLM和小动作模型的隐藏表示空间<br>- 复用预训练动作解码器，避免昂贵端到端预训练<br><br>**阶段2-微调阶段**：<br>- 端到端微调语言模型、状态编码器、动作模块<br>- 使用MAE损失监督6-DoF臂动作，BCE损失监督夹爪动作<br>- 组合损失函数：L_total = L_arm + λ·L_gripper (λ=0.01)<br><br>## 论文使用数据集和训练资源<br>### 仿真数据集<br>- **CALVIN ABC-D**：训练于环境A、B、C，测试于未见环境D，评估零样本泛化<br>- **LIBERO基准**：4个任务套件(Spatial、Object、Goal、LONG)，每个包含10个长时序任务<br><br>### 真实世界数据<br>- 手动收集500个高质量演示轨迹(每任务100个)<br>- 覆盖5个操作任务：close drawer、stack cups、stack blocks、pick place sponge、pick place block<br><br>### 训练资源<br>- 使用DeepSpeed ZeRO-2阶段优化内存使用<br>- 对齐阶段：batch size=8，学习率=1e-4，训练3个epoch<br>- 微调阶段：batch size=4，学习率=1e-4，训练2个epoch<br>- 图像分辨率：统一为200×200像素<br><br>## 论文使用的评估环境和评估指标<br>### 评估环境<br>- **仿真环境**：CALVIN和LIBERO基准测试平台<br>- **真实世界**：ALOHA机器人平台(6关节PiPer机械臂+Songling夹爪)<br><br>### 评估指标<br>- **成功率**：任务完成的平均百分比<br>- **平均任务长度**：连续完成指令的平均数量(CALVIN基准)<br>- **长时序执行能力**：LIBERO-LONG基准上的表现<br>- **真实世界性能**：5个操作任务各40次独立试验的平均成功率<br><br>评估结果显示VITA-VLA在所有基准上都实现了最佳VLA模型性能，特别是在复杂长时序任务中表现出色，并在真实机器人部署中验证了方法的实际有效性。</details> |
| 2025-10-10 | PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs | http://arxiv.org/abs/2510.09507 | <details><summary>展开</summary>## 论文研究单位<br>HKUST(GZ)、HKUST、Beihang University、Knowin<br><br>## 论文概述<br>PhysToolBench是一个专门评估多模态大语言模型（MLLMs）物理工具理解能力的基准。它采用视觉问答（VQA）格式，涵盖超过1000个图像文本对，分为三个难度级别：工具识别（Tool Recognition）、工具理解（Tool Understanding）和工具创建（Tool Creation）。论文评估了32个不同类型的MLLMs，包括专有、开源、具体化和VLA骨干模型，发现当前MLLMs在工具理解方面存在显著不足，性能远低于人类表现（超过90% vs MLLMs最高63%）。<br><br>## 论文核心贡献点<br>1. **首个物理工具理解基准**：提出PhysToolBench，系统性地评估MLLMs对物理工具的掌握程度。<br>2. **三层难度设计**：设计Easy、Medium、Hard三个级别，其中Medium细分为属性理解、工具组合和可用性理解三个子挑战，以渐进式评估深度理解。<br>3. **广泛模型评估**：全面评估32个MLLMs，揭示关键弱点，如小模型缺乏涌现能力、长尾问题严重、工具可用性幻觉和视觉推理不足。<br>4. **视觉中心推理框架**：提出一个初步解决方案，通过全局分析、物体检测和多层次证据整合来增强视觉推理能力。<br><br>## 论文方法描述<br>- **基准设计**：<br> - 数据集包含1000+图像文本对（1024×1024图像），每个图像标注数字标签，模型需输出对应标签或“None”。<br> - 三个难度级别：<br> - Easy: 工具识别（如识别切菜刀用于切蔬菜）。<br> - Medium: 工具理解，包含三个子挑战：<br> - M.1 属性理解（如选择铸铁锅因耐高温）。<br> - M.2 工具组合（如电池插入遥控器）。<br> - M.3 可用性理解（如识别破损的活塞不可用）。<br> - Hard: 工具创建（如用硬币代替平头螺丝刀）。<br>- **数据集收集**：<br> - 阶段1（概念化）：专家设计任务场景对。<br> - 阶段2（图像生成）：使用GPT-4o-image（约90%）和真实摄影（约10%），人工监督质量。<br> - 阶段3（标注验证）：自定义工具标注数字标签，多轮审核确保可靠性。<br>- **评估方法**：<br> - 使用一致文本提示，强制链式思考（CoT）推理或允许内置“思考”模式。<br> - 输出工具标签或“None”，基于准确率评分。<br>- **解决方案**：视觉中心推理代理框架：<br> - 全局分析：整体理解任务和图像上下文。<br> - 物体检测：调用DINOX工具识别并裁剪对象。<br> - 多层次证据整合：结合全局和细节分析生成答案。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：PhysToolBench，包含1000+图像文本对，涵盖日常、工业、户外和专业场景。图像由GPT-4o-image和真实摄影生成，人工标注数字标签。<br>- **模型资源**：评估32个MLLMs，包括GPT-5、o3、GPT-4o、Claude、Gemini等专有模型，Qwen、InternVL、GLM等开源模型，RoboBrain、Embodied-R1等具体化模型，以及PaliGemma、Phi-3-Vision等VLA骨干模型。<br>- **训练资源**：未训练新模型，而是使用现有模型进行评估。专有模型通过API调用，开源模型本地部署（如GLM-4.5V使用108B参数）。硬件要求未明确描述，但涉及大模型推理。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 专有模型（如GPT系列、Claude）通过各自API在线评估。<br> - 开源模型（如Qwen、InternVL）本地部署，使用统一提示。<br> - 人类基准：5名人类参与者作为参考。<br>- **评估指标**：准确率（accuracy），以百分比表示（↑表示越高越好）。<br> - 按难度级别细分：Easy、Medium（M.1/M.2/M.3）、Hard。<br> - 按场景类别细分：专业（Professional）、工业（Industrial）、户外（Outdoor）、日常（Daily）。<br> - 总体分数（Overall）基于加权或平均计算。表1显示人类最佳表现（如Easy 96.19%）与MLLMs最佳表现（如GPT-5 Overall 62.15%）对比。</details> |
| 2025-10-09 | Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered | http://arxiv.org/abs/2510.08464 | <details><summary>展开</summary>待生成</details> |
| 2025-10-09 | Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track | http://arxiv.org/abs/2510.07871 | <details><summary>展开</summary>## 论文研究单位<br>- **香港科技大学（广州）**<br>- **清华大学**<br>- **中国科学院自动化研究所**<br>- **小米电动汽车**<br><br>## 论文概述<br>论文提出了基于主动风险感知的社交导航方法，针对动态室内环境中的机器人导航任务。该方法在Falcon框架基础上创新性地引入了**主动风险感知模块（Proactive Risk Perception Module）**，通过预测周围人类的碰撞风险评分来增强机器人的空间感知能力。在IROS 2025 RoboSense挑战赛中，该方法获得**第二名**。<br><br>## 论文核心贡献点<br>1. **创新性风险感知模块**：提出基于距离的连续风险评分机制，将社交空间分为危险区、警告区和安全区<br>2. **辅助学习优化**：通过密度监督信号提升传统强化学习在社交导航中的表现<br>3. **高效集成方案**：风险模块与Falcon共享状态编码器，训练时提升导航能力，推理时零额外开销<br>4. **实际竞赛验证**：在Social-HM3D数据集上优于Falcon基线方法7.46%<br><br>## 论文方法描述<br>### 核心架构<br>- **主策略网络**：处理RGB-D观测和GPS+指南针信息，通过ResNet-50编码器提取视觉特征，2层LSTM处理时序依赖<br>- **风险感知模块**：轻量级神经网络(公式10)，基于LSTM隐藏状态预测风险分数<br>- **风险评分公式** (公式11)：<br> - 危险区(d<2m)：风险评分=1.0<br> - 警告区(2m≤d<4m)：风险评分线性衰减<br> - 安全区(d≥4m)：风险评分=0<br><br>### 训练优化<br>- **总损失函数** (公式12)：结合主导航损失、Falcon辅助损失和风险感知损失<br>- **DD-PPO算法**：在4块A40 GPU上训练75M步<br>- **风险权重β_risk=0.1**：平衡风险感知与其他目标<br><br>## 论文使用数据集和训练资源<br>### 数据集<br>- **Social-HM3D**：基于HM3D构建的844个真实室内场景<br>- **数据特性**：<br> - 人类数量按场景面积比例校准<br> - 人类运动速度0.8-1.2倍机器人速度<br> - 采用ORCA算法实现避障<br> - 训练集/验证集/测试集比例为：大规模/1000 эпизодов/500 эпизодов<br><br>### 计算资源<br>- **4块NVIDIA A40 GPU**<br>- **8并行环境训练**<br>- **约75M训练步数**<br><br>## 论文使用的评估环境和评估指标<br>### 评估环境<br>- **IROS 2025 RoboSense挑战赛**：Track 2社交导航轨道<br>- **测试数据**：私有测试集约500个未见场景<br>- **约束条件**：<br> - 仅使用RGB-D观测和里程计<br> - 无全局地图或特权信息<br> - 禁止人类位置预测器<br><br>### 评估指标<br>- **成功率和路径效率**：<br> - 成功率(SR)：到达目标1m内百分比<br> - 加权路径长度(SPL)：相对最优路径的效率评估<br>- **社交规范指标**：<br> - 个人空间合规性(PSC)：与人类保持≥0.5m距离的时长百分比<br> - 人类碰撞率(H-Coll)：发生人类碰撞的剧集百分比<br>- **总成绩计算** (公式14)：Total = 0.4×SR + 0.3×SPL + 0.3×PSC<br><br>### 竞赛结果<br>- 团队排名：**16支队伍中第2名**<br>- 总分：0.6994（较Falcon基线(0.6248)提升11.94%）<br>- 核心指标：SR=0.656 \| SPL=0.5958 \| PSC=0.8608 \| H-Coll=0.33<br>- 最佳团队差距：仅0.0028分差</details> |
| 2025-10-09 | USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots | http://arxiv.org/abs/2510.07869 | <details><summary>展开</summary>## 论文研究单位<br>- 中国科学院自动化研究所复杂系统认知与决策智能重点实验室(北京)<br>- 百度公司(北京)<br>- 中国科学院大学人工智能学院(北京)<br><br>## 论文概述<br>针对水下机器人数据稀缺、多任务泛化困难的问题，论文提出USIM，一个基于仿真的多任务视觉-语言-动作(VLA)数据集，覆盖9种场景和20项任务(561K帧，1,852条轨迹，约15.6小时)，并基于USIM训练得到水下通用VLA模型U0。U0采用多模态融合与卷积-注意力感知聚焦增强(CAP)模块，在检查、避障、扫描与动态跟踪等任务上平均成功率约80%，在挑战性移动抓取任务中较基线将机器人-目标距离缩短21.2%，证明仿真数据可有效驱动水下VLA能力形成。<br><br>## 论文核心贡献点<br>- 首个面向多任务、多场景的大规模水下VLA数据集USIM：561K帧/1,852轨迹/约15.6小时，20任务/9场景，涵盖抓取、检查、扫描、导航、跟踪、运输等。<br>- 水下通用VLA模型U0：基于Isaac-GR00T N1.5预训练，加入多模态融合(双目、压力、IMU、DVL等)与CAP模块，显式提升水下目标感知与空间理解。<br>- 建立可扩展“数据-任务”框架：仿真-数据-模型一体化管线，验证了闭环节在线评估与开环节离线评估均显著优于未微调基线，且距离指标改善21.2%。<br><br>## 论文方法描述<br>- 仿真环境：使用Stonefish构建9种水下场景(海床、海底管道、工业池、太阳能充电站、湖泊、近海工厂、现代/古代沉船等)，内置BlueROV2与机械手-夹爪；通过ROS集成与地图随机化、光照/水质变化，生成多样且逼真的视觉条件。<br>- 数据生成：自动化并行采集；每任务多episode，控制层采用PID(ROV姿态跟踪)与MoveIt(机械臂规划)；数据以10Hz录制，遵循LeRobot格式。<br>- 模型U0：<br> - 多模态融合：视觉(左/右相机)、语言、压力/IMU/DVL等状态，归一化推进器PWM与机械臂关节角作为动作空间，采用机器人中心坐标表示目标(相对位姿)，提升动态性与跨任务泛化。<br> - CAP模块：受VLM特征引导的卷积-注意力分支，强化目标检测与定位，损失为CAP的MSE与动作模块损失加权求和(推理时可关闭)。<br>- 训练与部署：基于USIM对GR00T N1.5微调，总批1024、5000步；3B参数，适配NVIDIA Jetson等嵌入式平台。<br><br>## 论文使用数据集和训练资源<br>- 数据集：USIM(561K帧/1,852轨迹/约15.6小时)；训练：526K帧/1,752轨迹；测试：35K帧/100轨迹。<br>- 场景与任务：9场景；20任务(12抓取、2管道检查、2沉船扫描、2避障导航、1动态跟踪、1运输)。<br>- 传感器与动作：双目相机、压力传感器、IMU、DVL；推进器PWM与机械臂关节角。<br>- 训练资源：基于Isaac-GR00T N1.5；批大小1024、训练步数5000；3B参数模型适配Jetson部署。<br><br>## 论文使用的评估环境和评估指标<br>- 评估方式：开环节离线评估(仿真测试集，20任务×5轨迹，共35K帧，约1小时)；闭环节在线测试(在仿真环境执行真实任务)。<br>- 评估指标：<br> - 动作误差 e_action(越低越好)<br> - 目标误差 e_target(CAP模块，衡量定位精度)<br> - 任务成功率(闭环节在线，多次试验统计)<br> - 移动抓取任务的机器人-目标距离(平均距离，越低越好)<br>- 主要结果：<br> - 未微调GR00T N1.5在e_action上远高于微调模型，证实显著领域差异。<br> - 经USIM微调后，模型在双目输入下优于单目；U0相对GR00T FT在单目与双目e_action上分别再降7.7%与4.2%。<br> - 闭环节在线：U0在7项非抓取任务上平均成功率约80%，双目优于单目，并超过GR00T FT。<br> - 移动抓取：U0(双目)将平均距离较GR00T FT缩短21.2%，显示对复杂流体力与机体-机械臂-目标交互的更好适应能力。</details> |
| 2025-10-09 | IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction | http://arxiv.org/abs/2510.07778 | <details><summary>展开</summary># 论文研究单位<br>- 哈尔滨工业大学（深圳）<br>- 南京大学<br>- 中国科学技术大学<br>- Dexmal<br><br># 论文概述<br>论文提出IntentionVLA，一个用于人机交互的具身意图推理VLA框架。当前视觉语言动作模型主要依赖显式指令映射到动作，缺乏推理密集型预训练和推理引导操作，无法在复杂现实交互中执行隐含人类意图推理。IntentionVLA通过课程训练范式和高效推理机制，首先利用精心设计的推理数据结合意图推理、空间 grounding和紧凑具身推理，为模型赋予推理和感知能力，然后在微调阶段采用紧凑推理输出作为动作生成的上下文指导，实现间接指令下的快速推理。<br><br># 论文核心贡献点<br>- 提出统一的VLA模型IntentionVLA，训练于精心策划的意图推理数据，通过两阶段范式和高效推理机制明确桥接高级语义推理与低级动作执行<br>- 在所有评估设置中显著优于最先进的VLA基线，展现强泛化能力和实时交互能力<br>- 针对现有VLA模型无法理解隐含人类意图的问题，提供解决方案<br><br># 论文方法描述<br>构建了涵盖意图推理、空间推理和紧凑推理的综合数据格式，采用高效标注流程从日常工作环境收集数据。模型基于Qwen2.5-7B构建，包含VLM骨干、可学习查询、连接器和扩散Transformer。两阶段训练：第一阶段训练VLM骨干进行意图推理和空间感知，第二阶段训练动作模块，将语义推理转化为可执行动作。推理时采用紧凑推理机制，实现0.2秒内的快速响应。<br><br># 论文使用数据集和训练资源<br>基于日常办公环境构建的具身意图推理数据集，使用GPT-4o和Florence-2等预训练模型进行数据标注。实验使用WidowX-250s机械臂配备Realsense D435i摄像头，训练了IntentionVLA、ECoT、CogACT和π₀等模型进行对比。<br><br># 论文使用的评估环境和评估指标<br>采用任务成功率作为评估指标（每任务10次试验）。评估环境包括：<br>- 分布内任务：直接指令和意图指令<br>- 分布外设置：未见指令、陌生物体操控<br>- 零样本人机交互：真实人手交互测试实时响应能力<br>实验在真实办公环境中进行，对象位置和放置姿态随机化，要求机器人完整正确完成整个交互过程。</details> |
| 2025-10-08 | WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation | http://arxiv.org/abs/2510.07313 | <details><summary>展开</summary>## 论文研究单位<br>- 北京大学多媒体信息处理国家重点实验室（计算机学院）<br>- 香港科技大学<br>- 新加坡国立大学<br>- 北京人形机器人创新中心<br><br>## 论文概述<br>WristWorld提出首个从第三人称“锚点视角”生成腕部视角视频的4D世界模型，无需首帧腕部视角输入，即可合成时空一致且几何对齐的腕部视角序列，用于数据增强与下游视觉–语言–动作（VLA）策略提升。<br><br>## 论文核心贡献点<br>- 首个两阶段4D生成框架，在仅提供锚点视图的条件下，实现腕部视角视频的时空与几何一致合成。<br>- 设计“腕部头”（wrist head）与空间投影一致性损失（SPC），从密集2D–2D对应与重建点云中监督腕部相机位姿，无需深度或外参。<br>- 引入CLIP编码的锚点视角语义与文本提示，联合几何投影条件，指导视频扩散模型合成更真实、对齐的腕部视角。<br>- 作为即插即用模块扩展单视角世界模型为多视角，无需新增腕部数据即可提供腕部视角训练样本。<br>- 在Droid、Calvin与Franka Panda上实现SOTA视频质量，显著提升VLA性能：Calvin平均任务完成长度提升3.81%，缩小锚点–腕部性能差距42.4%。<br><br>## 论文方法描述<br>- 两阶段4D生成：<br> - 重建阶段：扩展VGGT特征并通过“腕部头”与SPC损失估计腕部相机位姿与4D点云，投影得到时间对齐的条件图。<br> - 生成阶段：基于Video DiT，将腕部投影条件与CLIP锚点语义及文本共同作为条件，生成腕部视角视频；结构上支持以[腕部潜变量；条件潜变量]双通道输入。<br>- 训练目标包含标准扩散噪声预测损失与SPC投影损失（像素重投影误差+深度可行性项）。<br><br>## 论文使用数据集和训练资源<br>- 数据集：<br> - Droid：约76k视频，覆盖59个任务，含ext1/ext2与腕部相机；10k子集预训练，100视频验证。<br> - Calvin（模拟）：多任务语言条件基准，使用D split的10%数据。<br> - Franka Panda（实机）：1700演示，3静态外视角+腕部视角；保留100视频评估。<br>- 训练资源：<br> - 重建阶段：Droid预训练在8×A800 GPU约12小时，640×480分辨率，批4。<br> - 生成阶段：在8×A800 GPU约24小时，条件token长度512。<br> - Franka跨视角微调：重建6小时、生成12小时，相同批大小、分辨率与token长度。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：Droid、Calvin、Franka Panda。<br>- 视频质量指标：FVD（越低越好）、LPIPS（越低越好）、SSIM（越高越好）、PSNR（越高越好）。<br>- VLA评估（Calvin）：逐任务连续成功率的1/5–5/5统计与平均完成长度（Avg. Len.）。<br>- 消融实验：评估腕部投影、CLIP锚点语义与SPC损失的贡献。</details> |
| 2025-10-08 | TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking | http://arxiv.org/abs/2510.07134 | <details><summary>展开</summary>### 论文研究单位<br>北京大学、Galbot、中国科学技术大学、BAAI、北京航空航天大学、南方科技大学、北京师范大学联合研究。<br><br>### 论文概述<br>提出TrackVLA++，一种增强具身视觉跟踪（EVT）能力的视觉-语言-动作（VLA）模型。针对现有方法在动态场景中因缺乏显式空间推理和有效时间记忆而失效的问题，TrackVLA++通过引入极坐标思维链（Polar-CoT）机制和目标识别记忆（TIM）模块，显著提升追踪性能与鲁棒性，支持多视角扩展，并在模拟和真实场景中验证了SOTA表现。<br><br>### 论文核心贡献点<br>1. **Polar-CoT机制**：将目标相对位置编码为极坐标token，提供轻量级空间推理能力，优于传统边界框方法。<br>2. **TIM模块**：采用置信度门控记忆更新策略，抵抗遮挡和干扰物，实现长期目标识别。<br>3. **跨域泛化**：在EVT-Bench和Gym-UnrealCV等基准测试中实现SOTA，并在真实机器人任务中验证零样本泛化。<br><br>### 论文方法描述<br>- **架构**：基于导航基础模型NavFoM构建，结合双编码器提取视觉特征（SigLIP和DINOv2）。<br>- **Polar-CoT模块**：<br> - 将代理感知空间（0.6m-5.0m）离散化为60角度×30距离网格，编码为唯一词汇token（含<invalid> token处理遮挡）。<br> - 输入：视觉特征、语言token和TIM记忆；输出：紧凑reasoning token。<br>- **TIM模块**：<br> - 通过加权平均更新记忆：权重由预测置信度（归一化熵）决定，仅高置信度时融合新特征。<br> - 初始化为空状态，首次有效特征后生效。<br>- **训练流程**：<br> - 输入序列：LLM接收视觉、语言和reasoning token；输出动作token经MLP解码为8步轨迹。<br> - 损失函数：轨迹MSE损失、推理对数损失和文本损失（权重α=0.2，β=0.5）。<br>- **数据集**：混合200万样本（100万EVT-Bench跟踪数据 + 100万QA数据：SYNTH-PEDES、图像/视频QA）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 模拟：EVT-Bench（STT/DT/AT分割）、Gym-UnrealCV（Single Target/Distractor/Unseen Objects）。<br> - 真实：Unitree GO2机器人多视角RGB流（Obstacle/Winding Path/Distractor场景）。<br>- **训练资源**：<br> - 硬件：8块NVIDIA H100 GPU。<br> - 时间：约192 GPU小时（约一天）。<br> - 推理速度：4.8 FPS（对比NavFoM 5.1 FPS）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 模拟：EVT-Bench（DT分割作为主要测试）、Gym-UnrealCV。<br> - 真实：Unitree GO2四足机器人，配备4台SG3S11AFxK摄像头。<br>- **评估指标**：<br> - **SR**（Success Rate）：成功完成任务（1-3米内正确定向）的比例。<br> - **TR**（Tracking Rate）：成功跟踪时间步的比例。<br> - **CR**（Collision Rate）：因碰撞终止任务的比例。<br> - **EL**（Episode Length）：Gym-UnrealCV中的平均步长（最大500）。<br> - **识别准确率**：零样本人类识别任务（SYNTH-PEDES数据集）。</details> |
| 2025-10-08 | Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications | http://arxiv.org/abs/2510.07077 | <details><summary>展开</summary># 论文研究单位<br>- 东京大学（The University of Tokyo）<br>- 牛津大学（University of Oxford）<br>- 德克萨斯大学奥斯汀分校（The University of Texas at Austin）<br><br># 论文概述<br>- 背景：大模型（LLM/VLM）在NLP和CV快速进展，推动机器人领域的泛化与可扩展性需求；但“高级语言理解与具体动作执行”之间仍存在鸿沟，早期系统多采用“高层规划+预设动作库/模仿学习”的解耦方式，泛化受限。<br>- VLA定义：将视觉、自然语言作为必要输入，直接生成机器人控制命令的端到端系统；排除仅做高层技能选择或无动作落地的方案。<br>- 研究范围：系统综述VLA从历史演进、架构与模块、模态处理、学习范式，到数据采集、公开数据集、数据增强与评估基准的全栈方法；同时提供从业者指南。<br>- 方法：覆盖从早期CNN/MLP到Transformer/VLM主干，再到扩散/流匹配、潜在动作视频预训练与世界模型/可供性融合，以及分层/CoT推理等路线；结合多数据集与多机器人平台进行分析。<br>- 贡献：提出可落地的VLA全景与实践建议，分类并总结代表性工作，形成统一视角以支持实际部署。<br><br># 论文核心贡献点<br>- 首个覆盖软件与硬件、架构与训练、数据与评估的全栈VLA综述。<br>- 按生成策略梳理历史演进：CNN→序列Transformer→预训练VLM主干→扩散/流匹配→潜在动作学习→分层控制。<br>- 将架构系统化为传感运动模型、世界模型、可供性模型三大类，并细分七种传感运动架构变体。<br>- 总结多模态处理策略（视觉、语言、动作与附加模态）以及动作表征（离散化、连续生成、潜在动作、跨本体统一）。<br>- 归纳学习范式：监督、自监督、强化学习（含与RL组合的高层/低层策略）；给出预训练/后训练实践建议。<br>- 汇总数据采集方法、公开数据集（OXE、RT系列等）、数据增强与跨本体迁移方案。<br>- 梳理真实机器人平台、评估基准与指标，并提炼工程落地建议与未来方向（推理、安全、持续学习、评估等）。<br><br># 论文方法描述<br>- 架构与设计过渡：<br> - 早期端到端CNN/MLP（如CLIPort）：CLIP特征+Transporter网络，端到端学习操作任务，但可扩展性有限。<br> - 序列Transformer（Gato、VIMA）：统一令牌化，图像（ViT/目标检测）、语言（T5/Tokenizers）拼接为序列，自回归生成动作；在仿真中表现良好。<br> - 预训练VLM主干（RT-1/RT-2/RT-X、OpenVLA）：以VLM（PaLM-E、PaLI-X、Prismatic/DINOv2/SigLIP等）为骨干，在大规模机器人数据上联合微调，实现真实世界可扩展；RT-2/RT-X进一步提升跨环境/本体泛化。<br> - 扩散/流匹配策略（Octo、RDT-1B、π0/GR00T N1）：在Transformer后加扩散头或以扩散Transformer为骨干，生成平滑连续动作；π0引入流匹配与动作专家，实现50Hz实时控制；GR00T N1将潜在动作、扩散与流匹配整合为多阶段策略。<br> - 潜在动作视频预训练（LAPA）：从无标注人类视频中学习潜在动作，桥接人类演示与机器人动作空间，兼容真实部署。<br> - 分层与CoT推理（RT-H、π0.5、ECoT/CoT-VLA）：高层策略预测“语言动作/子任务/中间表示”，低层策略生成精细控制；或通过思维链逐步推理（生成子任务、关键点、目标图）提升长时序与复杂任务的稳健性。<br>- 核心架构类型：<br> - 传感运动模型（7类）：Transformer+离散动作、Transformer+扩散头、扩散Transformer、VLM+离散、VLM+扩散、VLM+流匹配、VLM+扩散Transformer；对应自回归、非自回归、连续控制与实时响应等不同折中。<br> - 世界模型（3类）：预测未来视觉→逆动力学生成动作；学习潜在动作以利用人类视频；端到端联合预测动作与未来观测以增强规划与泛化。<br> - 可供性模型（3类）：VLM估计可供性热图/接触点/关键姿态引导控制；从人类数据抽取可供性；与传感运动模型融合，直接生成条件动作。<br>- 模态处理：<br> - 视觉：ViT/DINOv2/SigLIP/CLIP特征，Q-Former/TokenLearner压缩；目标级表示（边界框/分割/跟踪）常与语言对齐。<br> - 语言：继承LLM分词器（T5/LLaMA等），USE/CLIP文本编码器或VLM直接融合；FiLM等条件融合常用。<br> - 动作：离散化分桶（256桶/保留低频词）、MLP解码为连续、扩散/流匹配生成连续动作、DCT/FAST降低序列长度；潜在动作（VQ-VAE/视频重建）；跨本体统一（统一视角、共享原子动作空间、异构令牌统一）。<br> - 附加模态：音频（ASR/声学编码器）、触觉（图像化触觉+ViT/TVL）、3D（深度图/多视角/体素/点云/NeRF/高斯Splatting），提升接触与空间感知。<br>- 训练与实现：<br> - 监督学习：预训练（用VLM/多源数据）→后训练（高质量机器人演示）；上下文学习用于少样本提示。<br> - 自监督：模态对齐（对比学习）、视觉自监督（MAE/CLIP/DINOv2）、潜在动作自监督（视频重建）。<br> - 强化学习：两类组合——用RL微调VLA（如SFT→RL→SFT、RPRM稠密奖励、潜空间RL避免扩散回传不稳）；或VLA做高层决策、RL做低层控制（足式/人形/移动操作）。<br> - 预训练阶段：数据规模与多样性（跨机器人/人类/合成/多任务损失）；常用VLM骨干（PaLM-E、PaLI-X、PaliGemma等）。<br> - 推理：支持非自回归并行输出、目标图像/语言指令输入、多模态条件与低时延控制；实时性通过流匹配或序列压缩优化。<br>- 数据与增强：遥感与多模态采集、统一格式（单相机+7-DoF动作），公开数据集（Open-X Embodiment、RT-1数据、多机器人/人类视频），数据增强（合成轨迹/世界模型生成/光学流与特征点跟踪/图像编辑）。<br><br># 论文使用数据集和训练资源<br>- 公开数据与平台：<br> - Open-X Embodiment (OXE)：多机器人多本体数据，统一格式，促进跨本体泛化。<br> - RT-1/RT-2数据：大规模真实世界机器人演示，涵盖数百任务与长序列。<br> - 人类视频数据集：Ego4D、EPIC-KITCHENS等，用于潜在动作与可供性抽取。<br> - 仿真数据：任务/轨迹/合成视觉样本，用于长时序与多样场景扩展。<br>- 预训练资源：<br> - 多源混合：真实机器人轨迹、互联网规模视觉语言数据、检测/推理辅助损失、合成轨迹与COSMOS等世界模型生成数据。<br> - VLM骨干：PaLM-E、PaLI-X、PaliGemma、LLaMA 2/Qwen2等；视觉编码器（DINOv2/SigLIP、EfficientNet、MAE-ViT）。<br>- 计算与训练关注：Transformer为主，序列长度与高分辨率视觉导致显存/时延压力；采用TokenLearner/Q-Former压缩、流匹配加速采样、并行非自回归解码；建议端到端微调+动作头适配的折中训练策略。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境与平台：<br> - 真实机器人：机械臂、移动操作、人形与足式机器人等多样化本体与场景。<br> - 仿真到真实：模拟环境用于训练与数据扩增，结合真实部署验证。<br>- 评估指标：<br> - 任务层：成功率、平均回报、EpLen、鲁棒性（噪声/遮挡/跨域）。<br> - 控制层：时延与频率（如50Hz）、动作平滑性与稳定性。<br> - 泛化：跨任务/对象/环境/本体的转移能力。<br> - 安全与失败检测：碰撞率、回退策略、恢复成功率。<br> - 效率：参数规模、推理时延、部署资源占用。</details> |
| 2025-10-08 | Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models | http://arxiv.org/abs/2510.07067 | <details><summary>展开</summary># 论文研究单位<br>作者单位未在提供的HTML中明确给出。<br><br># 论文概述<br>论文研究了嵌入在真实人机交互中的自然语言指令对视觉-语言-动作（VLA）模型稳健性的影响，重点考察两类指令噪声：人自然转述与插入与任务无关的上下文。论文在LIBERO和Habitat 2.0两个仿真环境中，对OpenVLA、UniAct、MoDE、π0和LLARP等SOTA VLA模型进行系统评估。发现随上下文长度增长性能显著下降；与训练集语义/词法相似的无关上下文可导致约50%质量损失；人类转述可导致约20%下降。为此提出基于大语言模型（LLM）的无关上下文过滤框架，能将性能恢复至原始的98.5%。<br><br># 论文核心贡献点<br>- 发现并量化了VLA模型在两类语言扰动下的脆弱性：人类转述与无关上下文插入，且对语义/词法接近训练命令的上下文最敏感。<br>- 表明随无关上下文长度增加，性能呈持续下降；在上下文长度接近目标命令时，质量损失可达到与语义接近型上下文相当的水平。<br>- 提出并验证了LLM过滤框架作为VLA前置预处理，能显著提升鲁棒性并恢复性能。<br><br># 论文方法描述<br>- 设计系统化的指令扰动类型：<br> - 按长度变化：单引导词、短句（3–5词）、长句（7–10词）。<br> - 按语义/词法相似度：描述型（Description）、不可执行指令（Infeasible）、位置型（Location，包含场景对象与位置引用但与后续命令无关）。<br> - 上下文插入于目标命令前后，并使最终噪声指令与训练模板在标点与大小写上保持一致以排除非相关变量。<br>- 人研究众包转述：众包工作者对各任务指令进行转述，要求语义不变；每位参与者的五条指令由五名不同工作者独立转述，耗时中位数296秒，收集后由专家审核保留语义保真样本。<br>- 过滤框架：<br> - 使用多尺寸LLM（Flan‑T5 Base、Qwen2.5‑0.5B、1.5B、3B、Llama3.2‑1B、3B、Llama‑3‑8B）以少样本提示进行过滤，去除无关上下文并恢复核心指令。<br> - 为LIBERO与LLARP分别适配过滤指令；小模型对随机上下文过滤较好，语义接近型需要≥3B；开源8B模型在绝大多数场景几乎完全恢复模板指令。<br> - 对极端场景（不可执行型前置）仍有少量残留影响；少数情况下会误删有用细节（如LLARP的初始位置描述），但在测试数据中出现概率极低。<br><br># 论文使用数据集和训练资源<br>- 仿真环境：<br> - LIBERO：四套任务（Goal、Object、Spatial、Long各10个任务；前三种为短时域，Long为长时域）。<br> - Habitat 2.0：生成100条导航+操作指令用于评估。<br>- VLA模型：OpenVLA、UniAct、MoDE、π0（用于LIBERO）；LLARP（用于Habitat 2.0）。<br>- 试验资源：<br> - LIBERO：每任务套50次试验，结果为三随机种子平均（共150次/统计量）。<br> - Habitat 2.0：LLARP并行运行在32个环境中，每任务30次试验，三随机种子平均。<br> - 众包：在英语熟练度筛选后进行转述，数据匿名化并按研究许可发布。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：LIBERO与Habitat 2.0仿真平台。<br>- 评估指标：成功率（SR）。</details> |
| 2025-10-08 | RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training | http://arxiv.org/abs/2510.06710 | <details><summary>展开</summary># RLinf-VLA: A unified and efficient framework for VLA+RL training<br><br>## 论文研究单位<br>- 清华大学、Zhongguancun Academy、Infinigence AI、北京大学、加州大学伯克利分校、哈尔滨工业大学、中国科学院自动化研究所<br><br>## 论文概述<br>- 背景与动机：VLA（视觉-语言-动作）模型大多采用SFT（监督微调），易受分布偏移影响，泛化与稳健性受限；RL（强化学习）通过交互直接优化任务表现，但现有VLA+RL工作碎片化，缺乏统一平台，难以公平对比与规模化扩展。<br>- 贡献概览：提出RLinf-VLA，一个统一且高效的VLA强化学习训练框架。通过三大GPU分配模式、统一的模型/算法/仿真器接口、细致的算法设计（优势与日志概率粒度、部分重置、动作掩码、长度归一化等），在系统与算法两端同时提效。<br>- 关键数字：在仿真中，单一统一模型在LIBERO-130（130项任务）达98.11%成功率，在ManiSkill 25项Pick-and-Place任务达97.66%；系统层面相对基线吞吐2.27×，在GPU并行仿真中采用混合细粒度流水实现1.61×–1.88×加速。<br><br>## 论文核心贡献点<br>- 统一设计与接口：支持多仿真器（ManiSkill、LIBERO）、多VLA架构（OpenVLA、OpenVLA-OFT）、多RL算法（PPO、GRPO）；统一训练/生成/仿真器接口，简化跨配置迁移与公平对比。<br>- GPU分配策略：提出colocated、disaggregated、hybrid三种模式；针对GPU并行仿真，提出hybrid + 细粒度流水，有效削减“GPU气泡”与频繁卸载开销。<br>- 算法与工程细节：优势与日志概率支持chunk/action/token三级粒度并可组合；PPO支持部分重置（提升样本效率）、轻量值网络（共享参数）；GRPO支持分组设计、有效动作掩码、轨迹长度归一化与成功率筛选。<br>- 强实证与实践准则：给出PPO/GRPO与VLA结合的最佳实践；开源并维护生态，持续更新。<br><br>## 论文方法描述<br>- GPU分配策略<br> - Colocated（共享）：训练/生成/仿真共用所有GPU，支持CPU卸载（offload），但在频繁交互场景开销大。<br> - Disaggregated（分离）：各组件分配独占GPU分区，避免内存争抢，但因依赖关系产生“GPU气泡”。<br> - Hybrid + 细粒度流水：将一个GPU上的仿真实例拆分为多个子仿真器S^(1)…S^(k)，仿真与生成并发流水，消除空闲；通过配置可切换模式，无需改动代码。<br>- 模型兼容<br> - LoRA支持：低秩适配，冻结原权重，仅训练少量参数，降低显存与成本。<br> - 模型类型：OpenVLA（~7B，连续/离散动作）与OpenVLA-OFT（连续动作空间+L1回归损失，支持并行解码与动作分块，提高推理吞吐）。<br> - 统一接口：屏蔽模型差异，一套API适配多模型。<br>- 多仿真器支持<br> - 统一接口：Gym风格reset/step、auto_reset、ignore_terminations、chunk_step（处理分块动作与边界）、可视化与固定重置状态等工具。<br> - 任务集合：ManiSkill 25项PutOnPlateInScene（OOD设置复现）；LIBERO六类任务组合为LIBERO-130（130项）。<br>- 多算法支持与设计<br> - PPO：GAE估计优势；轻量值头（共享LM参数）；chunk/action级价值估计；支持“固定episode长度”与“部分重置”两种优化目标。<br> - GRPO：以组为单位进行相对优势估计（去值函数），通过分组（相同任务+相同初始状态）、有效动作掩码、轨迹长度归一化与成功率筛选提升稳定性与效率。<br>- 优势与日志概率粒度<br> - 支持优势（chunk/action）、日志概率（chunk/action/token）组合；不兼容时采用广播（优势广播至更细粒度）。<br><br>## 论文使用数据集和训练资源<br>- 仿真环境<br> - ManiSkill：PutOnPlateInScene25Main-v3（25个抓取-放置任务）。<br> - LIBERO：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-10、LIBERO-90；统一为LIBERO-130。<br>- 训练设置<br> - 算法与模式：PPO（固定episode/部分重置），GRPO（固定episode/有效动作掩码）。<br> - 分块与解码：OpenVLA-OFT支持动作分块与并行解码，提升高频控制能力。<br> - 超参与资源：支持LoRA与不同rollout批量；建议更大rollout批量；LoRA对性能影响不显著但常需超参再调；部分重置与长度归一化提升样本效率与稳定性。<br>- GPU模式与流水<br> - Colocated/Disaggregated/Hybrid三模式切换；细粒度流水stage数量可配置；支持各组件独立offload开关。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境<br> - 仿真：LIBERO-130（跨任务类别、长视野）、ManiSkill（25项OOD抓取-放置）。<br> - 实机：Franka机器人在6个未见物体上零样本抓取-放置。<br>- 指标<br> - 成功率：LIBERO-130上98.11%；ManiSkill上97.66%。<br> - 系统效率：相比基线2.27×吞吐；在GPU并行仿真中混合细粒度流水加速1.61×–1.88×。<br> - 消融与实践：PPO中动作级价值估计优于分块级；部分重置显著提升样本效率；GRPO中长度归一化与有效动作掩码关键；成功率筛选改善稳定性；更大rollout批量有益；LoRA需调参但本身不降性能。<br> - 实机对比：RL策略零样本完成8/30次；SFT策略0/30次，体现更强泛化。</details> |
| 2025-10-07 | Verifier-free Test-Time Sampling for Vision Language Action Models | http://arxiv.org/abs/2510.05681 | <details><summary>展开</summary>根据提供的Arxiv论文HTML原文，我为您总结这篇关于Vision-Language-Action模型的无验证器测试时采样论文：<br><br>## 论文研究单位<br>KAIST（韩国科学技术院）、首尔国立大学（SNU）、RLWRLD<br><br>## 论文概述<br>Vision-Language-Action (VLA) 模型在机器人控制中展现出卓越性能，但在需要高精度的任务中仍存在根本性局限。当前方法主要依赖单次推理范式，限制了精细操作任务的表现。虽然已有测试时扩展方法使用外部验证器提升性能，但这些方法需要额外训练且泛化能力有限。本研究提出Masking Distribution Guided Selection (MG-Select)，一种新颖的VLA测试时扩展框架，仅利用模型内部属性，无需额外训练或外部模块。<br><br>## 论文核心贡献点<br>1. **提出MG-Select框架**：利用KL散度作为置信度指标，从多个候选动作中选择最优动作<br>2. **条件掩蔽分布设计**：通过随机掩蔽状态和语言条件生成参考分布，确保最大不确定性同时保持任务分布对齐<br>3. **联合训练策略**：通过dropout技术使模型学习条件和无条件分布，进一步提升参考分布质量<br>4. **显著性能提升**：在真实世界任务中实现28%的内分布和35%的外分布性能提升，在RoboCasa pick-and-place任务中实现168%相对增益<br><br>## 论文方法描述<br>### 测试时扩展框架<br>- **阶段1**：并行随机采样生成N个候选动作<br>- **阶段2**：使用特定标准进行Best-of-N选择<br><br>### 条件掩蔽分布置信度<br>- 使用KL散度测量预测分布与参考分布间的距离作为置信度指标<br>- 参考分布通过掩蔽文本、状态或两者创建，分别对应：文本掩蔽、状态掩蔽、文本&状态掩蔽<br>- 针对不同任务环境选择最优置信度变体<br><br>### 联合训练策略<br>- 训练时引入四种掩蔽变体：(qₜ,I)、(qₜ,∅)、(∅,I)、(∅,∅)<br>- 通过dropout技术增强模型对条件掩蔽分布的感知能力<br><br>## 论文使用数据集和训练资源<br>### 训练资源<br>- **硬件**：NVIDIA A100 GPU（2块）<br>- **模型**：π₀-FAST (Paligemma-3B VLM)、OpenVLA (Prismatic-7B VLM)<br>- **优化器**：AdamW，学习率2.5e-5到2.5e-6的余弦衰减<br>- **训练配置**：warmup_steps=1,000，global batch size根据数据集变化<br><br>### 使用数据集<br>- **RoboCasa**：24个家庭厨房环境中的原子任务，专注于8个pick-and-place任务<br>- **SIMPLER-WidowX**：4个pick-and-place任务，基于BridgeData V2训练<br>- **LIBERO**：多轴泛化评估，包括布局、物体和目标变化，以及长期任务<br>- **真实世界数据**：DROID数据集，基于Franka Research 3机器人<br><br>## 论文使用的评估环境和评估指标<br>### 评估环境<br>- **仿真环境**：RoboCasa、SIMPLER-WidowX、LIBERO<br>- **真实世界环境**：Franka Research 3机器人，7-DoF机械臂<br><br>### 评估指标<br>- **成功率**：以百分比表示的成功率，基于多次试验（通常为50次仿真、16-24次真实世界）<br>- **样本效率**：在30、100、300个演示样本下的性能表现<br>- **泛化能力**：内分布（ID）和外分布（OOD）任务的区分评估<br>- **效率分析**：推理延迟分析，通过单预填充策略实现45%的延迟减少<br><br>实验结果表明MG-Select在不同演示规模下均能持续改进基础模型性能，特别是在低数据场景下效果显著。</details> |
| 2025-10-07 | MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption | http://arxiv.org/abs/2510.05580 | <details><summary>展开</summary>待生成</details> |
| 2025-10-06 | StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation | http://arxiv.org/abs/2510.05057 | <details><summary>展开</summary># 论文研究单位<br>浙江大学、南京大学、香港科技大学<br><br># 论文概述<br>论文提出 StaMo 方法，旨在从静态图像中学习可泛化的机器人运动。核心思想是将视觉观测压缩为高度紧凑的状态表示（仅需 2 个 1024 维 token），并利用预训练的 Diffusion Transformer（DiT）解码器进行重建。更重要的是，在该紧凑状态空间中，通过简单的线性插值或差分即可自然得到潜在动作（latent action），可解码为可执行的控制信号，避免了对视频时序建模的依赖。该表示可无缝集成到现有 VLA 框架中进行世界建模与策略共训，在模拟与真实环境均取得显著提升。<br><br># 论文核心贡献点<br>- 提出紧凑状态表示学习框架，用 2×1024 token 编码视觉观测，并通过 DiT 解码器实现高质量重建。<br>- 发现并利用“状态差分即动作”的性质：在紧凑潜在空间中，状态之间的线性差分或插值可直接作为潜在动作，无需视频监督。<br>- 将该表示作为未来状态预测目标，集成到 OpenVLA/OFT 等 VLA 框架，提升任务成功率且推理开销极小。<br>- 引入策略共训（co-training）方案：将无标签视频的相邻帧编码为潜在动作，作为伪标签与少量带标签机器人数据联合训练，显著提升泛化与跨域迁移能力。<br>- 验证方法在 LIBERO 基准提升 +14.3%，真实任务成功率提升 +30%，并呈现良好的可扩展性与跨域（sim-to-real）迁移特性。<br><br># 论文方法描述<br>- 图像压缩与状态表示学习：采用 Diffusion Autoencoder 结构；编码器为冻结的 DINOv2 特征提取器 + Transformer 压缩器，解码器为预训练的 DiT（基于 Stable Diffusion 3）；使用 Flow Matching 目标优化，仅训练压缩器与解码器。<br>- 状态与运动的统一：运动定义为状态 token 的差分 a_t = s_{t+1} − s_t 或在潜空间中的线性插值；该潜在动作可直接解码为机器人控制信号，并用于世界模型与策略学习。<br>- 世界建模：在 VLA（如 OpenVLA/OFT）的自回归主干上添加轻量 MLP 头，预测下一状态表示；总损失为动作交叉熵与未来状态回归（MSE + L1）之和，权衡即时控制与未来预测。<br>- 潜在动作的策略共训：将相邻视频帧编码为潜在动作 m_t = E(o_{t+1}) − E(o_t)，作为伪标签，与少量真实机器人动作数据共同训练下游策略。<br><br># 论文使用数据集和训练资源<br>- 数据集：LIBERO（10/90/goal/object/spatial）、DROID、Maniskill（OOD）、Open X-Embodiment；并使用人类 egocentric 视频。<br>- 硬件与训练：8×NVIDIA H100 GPU；AdamW 优化器，学习率 3e-5（余弦退火），批大小 512/GPU，权重衰减 1e-3；训练约 10 天，PyTorch 2.1、Ubuntu 22.04；随机种子 33。<br><br># 论文使用的评估环境和评估指标<br>- 环境与基准：LIBERO 模拟环境（不同子任务 Spatial/Object/Goal/Long）；真实世界机器人实验（Franka Research 3 + UMI 夹爪 + RealSense D435，20 Hz，SE(3) 绝对末端位姿控制）。<br>- 评估指标：图像重建（PSNR、SSIM）、任务成功率（%）、线性探针 MSE（预测动作序列与真实动作的均方误差）、推理频率（Hz）、策略共训与跨域迁移表现。</details> |
| 2025-10-06 | HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks | http://arxiv.org/abs/2510.04898 | <details><summary>展开</summary>论文研究单位<br>University of Oxford<br><br>论文概述<br>该论文指出，现有的视觉-语言-动作（VLA）模型虽然具有强大的泛化能力，但推理成本极高。为了解决这个问题，论文提出了HyperVLA，一种基于超网络的新型VLA架构。HyperVLA在训练时利用一个大型超网络来获取处理多样化多任务行为所需的高模型容量，但在推理时仅激活一个由超网络生成的、针对特定任务的紧凑策略。该方法通过引入几项关键的算法设计，成功训练了HN-based VLA。实验表明，与单体VLA相比，HyperVLA在实现相似甚至更高的零样本泛化和少样本适应成功率的同时，显著降低了推理成本。<br><br>论文核心贡献点<br>1. 提出了HyperVLA，一个基于超网络的VLA架构，通过在训练时使用高容量模型、推理时仅激活小型任务特定策略，实现了推理效率的提升。<br>2. 识别并解决了训练大型HN-based VLA的挑战，提出了三项关键的算法设计：利用预训练视觉主干网络、超网络嵌入归一化和简化的线性动作生成策略。<br>3. 在零样本泛化（SIMPLER基准）和少样本适应（LIBERO基准）实验中，HyperVLA的性能匹配或超越了现有的单体VLA基线（如OpenVLA）。<br>4. 实现了显著的推理效率提升：与OpenVLA相比，HyperVLA将测试时激活的参数数量减少了90倍，推理速度提升了120倍。<br><br>论文方法描述<br>HyperVLA的核心思想是利用超网络的分层结构来解耦不同任务所需的技能。<br>1. 架构：HyperVLA由一个基础策略和一个超网络组成。<br> - 基础策略：一个视觉Transformer（ViT），它仅将当前图像观察作为输入来预测动作。它包括一个DINOv2图像编码器、一个线性投影层、一个小型Transformer策略头和一个线性动作头。<br> - 超网络（HN）：一个基于Transformer的上下文编码器，它接收语言指令（通过冻结的T5编码器）、初始图像的类别标记（通过冻结的DINOv2编码器）和一个可学习的任务上下文标记作为输入，然后生成基础策略的参数。<br>2. 算法设计：<br> - 视觉主干网络：使用预训练的DINOv2作为基础策略的图像编码器并进行微调，以利用视觉基础模型的先验知识，避免在相对较小的机器人数据集上从头训练导致的过拟合。<br> - 上下文嵌入归一化：为了稳定超网络的训练，在将上下文嵌入输入到输出头之前进行归一化。这使得基础策略参数的更新动态与直接训练该策略时相似。<br> - 动作生成策略：采用简单的线性动作头和均方误差（MSE）损失进行训练，而非现有VLAs常用的自回归或扩散模型，这在HN-based VLA设置中更简单、更高效。<br><br>论文使用数据集和训练资源<br>1. 数据集：模型在Open X-Embodiment (OXE)数据集上进行训练。评估在SIMPLER和LIBERO两个模拟基准上进行，SIMPLER用于评估零样本泛化能力，LIBERO用于评估少样本适应能力。<br>2. 训练资源：HyperVLA在4块NVIDIA A5000 GPU上训练了一天。作为对比，基线模型OpenVLA在64块A100 GPU上训练了14天。<br><br>论文使用的评估环境和评估指标<br>1. 评估环境：性能评估在两个模拟环境中进行：SIMPLER和LIBERO。推理速度（时间/步）在NVIDIA L4 GPU上进行测量。<br>2. 评估指标：<br> - 任务成功率：在SIMPLER和LIBERO基准上，评估模型完成任务的百分比。<br> - 推理效率：通过激活的参数数量、每推理步的耗时（毫秒）和浮点运算数（FLOPs）来衡量。</details> |
| 2025-10-05 | ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context | http://arxiv.org/abs/2510.04246 | <details><summary>展开</summary>## 论文总结<br><br>**论文研究单位：**<br>KAIST（韩国科学技术院）、RLWRLD、加州大学伯克利分校<br><br>**论文概述：**<br>针对部分可观察机器人任务中时序上下文缺失导致的策略性能不稳定问题，提出ContextVLA框架。该框架基于预训练视觉语言模型（VLM）的视觉语言动作模型（VLA），通过将历史多帧观测压缩为单一上下文令牌来高效利用时序信息，同时避免直接处理高维视频序列的计算开销。与传统多帧训练策略不同，该方法利用VLM的时序理解能力，在模拟和真实机器人任务中均显著提升策略性能。<br><br>**论文核心贡献点：**<br>1. **揭示关键瓶颈**：系统性分析发现VLA架构在利用多帧观测方面优于传统策略，原因在于预训练VLM具备的时序理解能力；<br>2. **创新压缩机制**：提出分两阶段处理策略——前半段VLM层保留多帧输入以提取时序特征，后半段将历史帧聚合为单令牌（平均池化），实现高效时序上下文融合；<br>3. **通用适配性**：支持自回归和扩散式动作解码器，可直接增强现有VLA模型（如π₀、GR00T N1.5）；<br>4. **显著性能提升**：<br> - 模拟任务：Libero基准平均提升1.9%，Simpler-WidowX提升14.4%（41.8%→56.2%）<br> - 真实任务：长时序"Pick-and-Place Twice"任务从25%提升至65%<br><br>**论文方法描述：**<br>- **观测压缩流程**：<br> 1. 用视觉编码器处理8帧历史观测得到视觉特征<br> 2. 在VLM第2层对历史帧隐藏状态执行平均池化生成上下文令牌m<br> 3. 用m替代后续VLM层中的历史帧令牌，仅保留当前帧令牌<br>- **动作生成**：上下文令牌与VLM当前层特征共同输入动作解码器，支持自回归（令牌化动作）或扩散式生成<br>- **高效推理**：结合因果注意力掩码与KV缓存机制，在t-1步预计算历史令牌和KV缓存，t步仅处理当前观测和上下文令牌<br><br>**论文使用数据集和训练资源：**<br>- **模拟数据集**：Libero（40个任务）、Simpler-WidowX（4任务）、Robocasa（24任务）<br>- **真实数据集**：Bridge v2、3类长时序操作任务（抓取-放置-两次、覆盖-堆叠）<br>- **训练配置**：<br> - 60K迭代，批量32，优化器AdamW<br> - 8帧历史观测，在第2层VLM块执行压缩（n=2）<br> - 硬件：NVIDIA A100 80GB GPU（4卡并行训练，单卡推理）<br><br>**论文使用的评估环境和评估指标：**<br>- **环境**：<br> - 模拟任务：Libero（4子基准）、Simpler-WidowX（4任务）、Robocasa（24任务）<br> - 真实机器人任务：3类时序敏感操作（抓取-放置序列、手部开合等）<br>- **指标**：<br> - **性能**：任务成功率（%）<br> - **效率**：训练墙钟时间（基于π₀在Libero的60K迭代训练）、推理延迟（ms/8帧2视角输入）<br>- **关键结果**：<br> - 在Simpler-WidowX实现14.4%平均提升<br> - 推理时间从227.2ms（未压缩）降至96.3ms（压缩+KV缓存）<br> - 长时序真实任务成功率最高达80%<br><br>---<br>注：该论文在ICLR/NeurIPS等多帧机器人策略研究基础上，通过VLM时序建模和计算优化，首次系统解决了多帧观测在VLA中的高效利用难题。</details> |
| 2025-10-05 | SITCOM: Scaling Inference-Time COMpute for VLAs | http://arxiv.org/abs/2510.04041 | <details><summary>展开</summary>### 论文研究单位<br>Carnegie Mellon University<br><br>### 论文概述<br>论文提出了一种名为SITCOM（Scaling Inference-Time COMpute for VLAs）的框架，旨在解决视觉-语言-动作模型在长时程任务中缺乏前瞻规划和误差累积的问题。该框架通过结合预训练的VLA、一个学习的动力学模型和一个奖励模型，将VLA从单步动作执行器转变为能够进行多步规划的鲁棒长期规划器。SITCOM利用模型预测控制（MPC）的思想，在推理时生成多个动作序列的展开，并通过奖励机制选择最优序列执行。在SIMPLER环境中的实验表明，该方法能将任务完成率从48%显著提升至72%。<br><br>### 论文核心贡献点<br>1. 提出了一个通用的推理时规划框架SITCOM，可以增强任何预训练的VLA模型，通过模拟多步动作展开并基于奖励选择最优动作序列。<br>2. 开发了一个高效的基于Transformer的动力学模型，该模型在BridgeV2数据上预训练，并在SIMPLER环境中微调以弥合真实到模拟的差距。同时，引入了一种受DAgger启发的自适应策略来减少长时程展开中的误差累积。<br>3. 提供了对通过增加候选动作序列数量和未来预测深度来扩展推理时计算以获得性能提升的深入分析。<br><br>### 论文方法描述<br>SITCOM的核心是一个迭代决策过程。在每个决策点：<br>1. VLA模型根据当前图像观察和任务指令，通过高温度采样生成n个候选动作。<br>2. 每个候选动作初始化一个轨迹。利用一个独立的动力学模型来模拟未来状态，即对于每个候选动作，动力学模型预测下一帧图像。<br>3. 这个过程会迭代l步（rollout length），为每个候选动作生成一个完整的多步动作序列和相应的未来状态轨迹。<br>4. 使用一个奖励模型对每条轨迹的最终状态进行评分，该奖励函数综合考虑了夹爪与物体的间隙、物体与目标的距离以及抓取成功与否等指标。<br>5. 选择奖励最高的轨迹，并将其第一个动作在真实世界中执行。<br>6. 更新环境观察，并根据预设的重规划频率重复此过程，直到任务完成或终止。<br>该方法包含两个变体：SITCOM (EnvSim) 使用真实的模拟器进行展开，而 SITCOM (World Model) 则使用训练好的动力学模型进行展开。<br><br>### 论文使用数据集和训练资源<br>* **数据集**:<br> * 动力学模型预训练：使用了BridgeV2数据集，包含约25,000条轨迹，覆盖了13种操作技能和24个场景。<br> * 动力学模型微调与VLA微调：使用了在SIMPLER环境中收集的100条多任务专家轨迹。<br>* **训练资源**:<br> * 论文中未明确指定训练所使用的具体硬件（如GPU类型和数量）或训练时长。<br><br>### 论文使用的评估环境和评估指标<br>* **评估环境**:<br> * SIMPLER，一个开源的模拟环境套件，用于评估通用机器人操作策略。<br> * 使用了7自由度的WidowX机械臂进行四项任务的评估。<br>* **评估指标**:<br> * **Average Success Rate (平均成功率)**: 主要性能指标，计算为成功完成的任务数与总试验数的比值。<br> * **Partial Success Rate (部分成功率)**: 衡量机器人实现部分目标的场景，例如成功抓取物体但未能正确放置。<br> * **Time (时间)**: 与计算资源成正比，衡量为不同数量的候选轨迹生成计划所需的时间，评估了推理时的计算开销。</details> |
| 2025-10-04 | Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert | http://arxiv.org/abs/2510.03896 | <details><summary>展开</summary>论文研究单位<br>浙江大学和上海人工智能实验室。<br><br>论文概述<br>该论文提出了一种新框架，旨在解决将视觉-语言模型（VLM）的规划与推理能力转化为物理世界动作时面临的挑战。传统的视觉-语言-动作（VLA）模型因将推理与行动耦合在单一架构中，且依赖稀缺的领域数据，导致泛化能力差。本文引入了以“通用动作专家”为核心的框架，通过使用稀疏的3D轨迹作为明确的中间表示，将VLM的高级规划与低级的物理动作执行完全解耦。VLM仅需生成粗略的3D路径点，再由动作专家根据实时点云观测将其细化为密集、可执行的动作序列。此外，论文还提出了“动作预训练，点云微调”的训练范式，以提升训练效率和泛化鲁棒性。该方法结合了VLM在视觉理解与规划上的泛化能力，以及动作专家在细粒度动作层面的泛化能力，在仿真和真实世界实验中均展现了强大的零样本部署能力。<br><br>论文核心贡献点<br>- 提出了一个以通用动作专家为中心、以稀疏3D轨迹为清晰接口的框架，实现了高级VLM规划与低级电机控制的完全解耦。据作者所知，这是首次训练出无需任务特定微调即可部署的通用专家。<br>- 提出了“动作预训练，点云微调”策略，使动作专家专注于几何轨迹的精炼而非语义解释，从而实现泛化。<br>- 系统在多样化的实验中展现了卓越的泛化能力，通过成功的零样本部署（无需任何领域内微调）验证了其可行性。<br><br>论文方法描述<br>方法流程分为两部分：<br>1. **VLM规划模块**：VLM在少量经过精心标注的数据上进行监督微调，学习根据2D关键点和深度信息，在相机坐标系中直接预测稀疏的3D路径点和目标末端执行器姿态。这些路径点被转换到机器人基座坐标系后，通过B样条插值生成一条连续平滑的姿态轨迹，为后续的动作专家提供密集的指导信号。这种方式更符合VLM以视觉为中心的特性，避免了学习复杂的相机到机器人坐标转换。<br>2. **通用动作专家训练与执行**：动作专家被建模为一个条件扩散策略。<br> - **模型架构**：该模型接收多模态输入，包括机器人本体状态S_t、从轨迹中采样的指导姿态P_g以及裁剪后的环境点云O_pcd。这些输入经由各自的编码器处理后，拼接成最终的条件特征向量。<br> - **训练范式**：采用“动作预训练，点云微调”的两阶段策略。预训练阶段，模型在仅包含轨迹数据的大批次（可达32768）上训练，学习基本的轨迹跟随能力，此阶段点云特征被屏蔽。微调阶段，模型在包含点云的数据上进行训练，学习根据环境观测对轨迹进行精细化调整。这种解耦训练策略加快了收敛速度并提高了数据利用效率。<br><br>论文使用数据集和训练资源<br>- **数据集**：结合了仿真和真实世界的数据。<br> - 仿真数据：RoboTwin 2.0, CALVIN, LIBERO, RLBench。<br> - 真实世界数据：DROID, AgiBot World, BridgeV2。<br> - 为解决真实世界数据深度图稀疏的问题，论文使用FoundationStereo处理DROID数据，使用PromptDepthAnything处理AgiBot数据，并使用MoGe对部分数据进行深度补全，以增强模型在低质量深度信息下的鲁棒性。<br>- **训练资源**：<br> - VLM微调：在8块80GB NVIDIA A100 GPU上训练1000步，批次大小为32。<br> - 动作专家训练：在相同硬件上进行。预训练阶段持续2天，批次大小为32768；点云微调阶段持续3天，批次大小为256。<br><br>论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真环境：RoboTwin和ManiSkill基准。<br> - 真实世界环境：配备UMI夹持器的Franka Research 3机械臂，以及一个固定的RealSense D435相机（640x480分辨率）。控制频率为20Hz。动作空间定义为SE(3)，即表示末端执行器的7维绝对目标位姿（3D位置+4D四元数）。<br> - 真实世界任务：设计了6个任务，分为短、中、长时程三类，每类两个任务，以评估不同复杂度下的性能。<br>- **评估指标**：<br> - 主要评估指标为任务成功率。<br> - 泛化能力评估：通过在未见过的相机视角、新物体/颜色/语义指令下的零样本性能，以及在ManiSkill（训练中未包含的环境）上的跨环境迁移能力来衡量。<br> - 消融研究：通过分析不同训练步数、噪声尺度、训练策略对最终成功率的影响来验证模型设计的有效性。同时，使用MMLU分数来衡量VLM在微调后语言能力的保留情况。</details> |
| 2025-10-04 | NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation | http://arxiv.org/abs/2510.03895 | <details><summary>展开</summary># 论文研究单位<br>浙江大学<br><br># 论文概述<br>NoTVLA（Narrowing of Trajectory Vision-Language-Action）是一个针对通用机器人操作的框架，旨在解决视觉-语言-动作（VLA）模型中的灾难性遗忘问题。该框架通过将密集的动作轨迹压缩为稀疏的轨迹表示，避免了密集轨迹微调带来的知识遗忘，同时显著降低了计算成本。方法采用三阶段流程：首先通过锚点预测进行深度推理，然后基于运动学原理选择关键帧，最后使用样条基动作去令牌化器生成平滑的高频轨迹。<br><br># 论文核心贡献点<br>- 解耦高层VLM与低层动作专家，在减少微调计算的同时提高具身任务成功率<br>- 使用稀疏且语义修剪的轨迹监督，增强跨平台和跨任务的泛化能力，缓解灾难性遗忘<br>- 保留模型内在的视觉语言推理能力，支持复杂指令跟随和多轮交互，无需额外任务特定微调<br>- 在训练效率上显著优于现有方法，使用计算资源比π₀少一个数量级以上<br><br># 论文方法描述<br>**锚点预测与令牌生成**：通过锚点预测模块（APP）输出2D锚点，结合外部深度源获取深度信息，在锚点条件化令牌生成（ACTG）中输出包含深度、图像坐标、夹爪状态和姿态的多模态令牌序列。<br><br>**运动学基关键帧选择**：基于末端执行器的加速度阈值和夹爪状态变化来识别关键帧，将原始演示分割成逻辑阶段，并通过子关键帧和均匀下采样来保持时间相干性。<br><br>**样条基动作去令牌化器**：将离散令牌序列转换为平滑的高频机器人轨迹。使用三次样条插值处理XYZ位置，球面线性插值（SLERP）处理四元数姿态，确保位置和方向的高频平滑轨迹。<br><br># 论文使用数据集和训练资源<br>- **数据来源**：ManiSkill（3000条轨迹）、RoboTwin 2.0（约2000条轨迹）、AgiBot World（500条轨迹）<br>- **训练平台**：统一模拟混合环境，包含40个任务的多平台机器人数据<br>- **硬件要求**：在32 GPU小时下完成训练，即使在约8 GPU预算下也能展现执行能力<br>- **模型规模**：基于Qwen VL 2.5（7B）参数模型<br>- **多平台支持**：Aloha-AgileX、ARX-X5、Franka Panda、UR5、Piper、AgiBot-G1<br><br># 论文使用的评估环境和评估指标<br>- **基准测试**：RoboTwin 2.0官方基准、AGIBOT挑战赛官方评估<br>- **主要指标**：任务成功率（0-1之间），与专家模型（ACT、DP、DP3）和通用模型（π₀、RDT）对比<br>- **泛化测试**：零样本泛化评估，包括未见指令、背景变换、颜色提示反转等场景<br>- **轨迹质量评估**：几何和时间指标，包括F1分数、DTW距离、Fréchet距离、Hausdorff距离<br>- **跨视角评估**：训练域内（ID）与训练域外（OOD）视角性能对比<br>- **多任务综合性能**：在AGIBOT挑战的10个官方任务中进行测试，总分从2.795提升至3.697</details> |
| 2025-10-04 | LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization | http://arxiv.org/abs/2510.03827 | <details><summary>展开</summary>```markdown<br>## 论文研究单位<br>华中科技大学、哈佛大学、麻省理工学院、武汉理工大学、理海大学<br><br>## 论文概述<br>论文指出LIBERO基准作为视觉-语言-动作（VLA）模型主流评估平台存在训练与评估同质化问题，导致模型通过记忆训练数据即可获得超90%的虚高性能。为此，提出LIBERO-PRO扩展基准，通过四个维度（操作对象、初始状态、任务指令、环境）引入扰动评估模型鲁棒性。实验表明，现有模型在扰动条件下性能骤降至0%，暴露其缺乏真实任务理解和泛化能力，仅依赖动作序列记忆。<br><br>## 论文核心贡献点<br>1. 通过实验证明LIBERO当前评估协议存在根本缺陷：高分数主要反映训练数据记忆而非真实任务理解。<br>2. 提出LIBERO-PRO基准，支持对象、位置、指令、环境四维度扰动及随机组合，实现更可靠评估。<br>3. 在LIBERO-PRO上评测OpenVLA、pi0等模型，揭示其在扰动下性能崩溃，呼吁采用新评估标准。<br><br>## 论文方法描述<br>定义扰动框架$\tau^{(k)}=\phi_k(\tau)$，$k \in \{O,S,L,E\}$：<br>- **对象属性扰动**：修改非关键属性（如颜色、大小），任务语义不变。<br>- **初始位置扰动**：调整物体位置，保持物理合理性。<br>- **指令扰动**：分语义级（改写指令）和任务级（更换目标对象/动作），确保组件来自训练集。<br>- **环境扰动**：替换背景场景，不影响任务可行性。<br>施加约束：扰动幅度$\delta_k$可控，任务间总变距离$d_{TV} > \epsilon$，确保任务有效且差异显著。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：基于LIBERO原始数据扩展，新增对象资产、空间区域、指令改写版本及环境变体。<br>- **训练资源**：直接使用预训练模型，未新增训练。评估模型包括OpenVLA（多任务检查点）、pi0/pi0.5（单任务检查点），均采用官方发布权重。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO-PRO仿真环境，支持四维度扰动随机组合。<br>- **评估指标**：任务成功率（Success Rate），每任务50次试验。对比标准LIBERO与扰动场景下的表现，包括对象（Obj）、位置（Pos）、语义（Sem）、任务（Task）、环境（Env）扰动。<br>```</details> |
| 2025-10-02 | Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer | http://arxiv.org/abs/2510.03342 | <details><summary>展开</summary>待生成</details> |
| 2025-10-03 | MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning | http://arxiv.org/abs/2510.03142 | <details><summary>展开</summary>### 论文研究单位<br>- 北京大学（Peking University）<br>- Galbot<br>- 上海交通大学（Shanghai Jiao Tong University）<br>- 清华大学（Tsinghua University）<br>- BAAI（可能指北京人工智能研究院）<br><br>### 论文概述<br>论文提出MM-Nav，一个多视图视觉语言行动（VLA）模型，旨在通过多专家学习实现鲁棒的视觉导航。核心思想是结合合成环境中的多样导航数据与VLA模型的泛化能力，解决视觉导航中观察数据建模困难的问题。MM-Nav使用360度视觉观察（四个水平分布的相机视图），通过两阶段训练学习：先从强化学习（RL）专家收集数据预训练，再进行在线教师-学生迭代训练。方法最终在模拟和真实环境中展示了强泛化能力，并优于单一能力RL专家。<br><br>### 论文核心贡献点<br>1. 提出MM-Nav多视图VLA模型，支持360度视觉输入，直接输出连续速度命令。<br>2. 设计三种RL专家（reaching、squeezing、avoiding），分别学习导航能力。<br>3. 两阶段训练：离线数据预训练和在线迭代训练（DAgger方式），引入能力平衡数据聚合策略（基于性能差距动态调整数据比例）。<br>4. 在模拟和真实环境中评估，模型性能超越RL教师，验证多能力学习的协同效应。<br>5. 提供开源实现和项目页面，促进研究社区应用。<br><br>### 论文方法描述<br>- **方法概述**：采用教师-学生范式。学生模型是VLA（基于SigLIP视觉编码器和Qwen2语言模型），处理多视图RGB输入（四个相机），并预测速度命令。教师模型为三个RL专家（训练于不同合成环境，使用特权深度信息）。训练分两阶段：离线预训练（用专家数据初始化VLA）和在线迭代（VLA部署后收集专家数据，动态平衡训练比例）。<br>- **RL专家**：三个专家（reaching：到达目标；squeezing：穿行狭窄通道；avoiding：避开动态障碍）。使用PPO算法、深度图像输入（四个相机）、奖励函数针对能力调整。<br>- **VLA学生模型**：视觉编码（SigLIP输出视觉令牌；历史滑窗处理），语言提示（点目标转化为文本），动作预测（输出速度[v_x, v_y, v_yaw]）。<br>- **在线训练**：能力平衡数据聚合（计算VLA与专家的加权旅行时间（WTT）差距，调整数据比例），迭代优化直至收敛。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：合成环境数据（IsaacLab生成）：用于训练RL专家的三个能力特定场景（reaching、squeezing、avoiding），收集500k步专家数据；真实环境测试数据（四个场景：Narrow Zigzag Corridor、Thin Obstacle Avoidance、Dynamic Environment、Cluttered Static Environment）。<br>- **训练资源**：<br> - RL专家训练：IsaacLab模拟，NVIDIA RTX 4090 GPU，单次训练8-12小时。<br> - VLA预训练：8个NVIDIA H100 GPU，约5小时（40 GPU小时）。<br> - 在线迭代：每次迭代2小时（200k专家数据）。<br> - 部署：服务器配备NVIDIA RTX 5090 GPU，机器人端（Unitree GO2）实时推理。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 模拟环境：IsaacLab中三个能力特定场景（reaching、squeezing、avoiding）和一个混合场景（包含所有能力）。<br> - 真实环境：四个场景测试sim-to-real泛化。<br>- **评估指标**：<br> - 成功率（SR）：成功到达目标的百分比。<br> - 碰撞率（CR）：发生碰撞的百分比。<br> - 加权旅行时间（WTT）：成功到达目标的平均时间除以成功率。<br>- 对比基线：iPlanner、ViPlanner、NavDP等，结果显示MM-Nav在SR和CR上优于基线，WTT更低。</details> |
| 2025-10-03 | Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation - Technical Report for IROS 2025 RoboSense Challenge Track 4 | http://arxiv.org/abs/2510.02728 | <details><summary>展开</summary>待生成</details> |
| 2025-10-02 | Contrastive Representation Regularization for Vision-Language-Action Models | http://arxiv.org/abs/2510.01711 | <details><summary>展开</summary>待生成</details> |
| 2025-10-02 | FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models | http://arxiv.org/abs/2510.01642 | <details><summary>展开</summary>### 论文研究单位<br>南洋理工大学（Nanyang Technological University）、A*STAR前沿人工智能研究中心（Centre for Frontier AI Research, A*STAR）、艾伦人工智能研究所（Allen Institute for AI）、华盛顿大学（University of Washington）。<br><br>### 论文概述<br>论文提出FailSafe框架，用于视觉-语言-动作（VLA）模型的故障推理和自动恢复。VLA模型在机器人操作任务中表现良好，但现有数据集缺乏故障和恢复数据，导致模型无法应对执行中的失败。FailSafe通过在仿真中自动生成故障场景和可执行恢复动作，构建大规模数据集，并将LLaVa-OneVision-7B微调为FailSafe-VLM。实验表明，FailSafe-VLM能检测故障和生成恢复动作，并显著提升多个VLA模型在ManiSkill任务中的性能，同时具备跨视角、对象和机器人本体的泛化能力。<br><br>### 论文核心贡献点<br>- 首次提出FailSafe框架，自动生成故障推理和可执行恢复动作，支持任意仿真任务。<br>- 构建FailSafe数据集，使VLM和VLA模型具备故障推理能力，并提升性能，泛化到不同视角、空间配置、对象和机器人。<br>- 开源FailSafe代码，促进社区开发更稳健的具身智能系统。<br><br>### 论文方法描述<br>FailSafe流水线包括四个阶段：<br>1. **故障生成**：定义三种故障模式（translation、rotation、no-ops），通过YAML配置在仿真中随机注入扰动，将任务轨迹扰动为失败场景。<br>2. **动作收集**：从失败和正确轨迹中映射偏差姿态Pd到正确姿态Pc，生成多个候选恢复动作ΔA（7-DoF差值），避免碰撞。<br>3. **系统验证**：回放轨迹验证ΔA能否使失败恢复为成功，仅通过验证的ΔA被纳入数据集。<br>4. **指令微调**：使用FailSafe数据集微调LLaVa-OneVision-7B，得到FailSafe-VLM。训练设置：32个H100 GPU，DeepSpeed ZeRO 3，学习率1e-5（视觉塔2e-6），余弦退火warmup 3%，bfloat16/TF32。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：FailSafe数据集（131k失败-动作对 + 56k真值轨迹），来自ManiSkill仿真中三个任务（pick cube、push cube、stack cube），含多视角图像（front、side、hand）。<br>- **训练资源**：32个H100 GPU（DeepSpeed ZeRO 3），初始化自LLaVa-OV-7B单图像检查点，语言主干Qwen2-7B-Instruct，视觉塔SigLIP，两层GELU MLP投影器。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：ManiSkill仿真平台，使用Franka Emika Panda机器人臂；测试种子生成未见空间配置；评估不同对象（sphere、charger）和机器人（xArm 6）的泛化能力。<br>- **评估指标**：<br> - **VLM比较**：二元成功率（区分失败/成功）、准确性（识别故障类型）、余弦相似度（预测ΔA与真值ΔA相似度）。<br> - **VLA模型**：成功率提升（有无FailSafe-VLM的对比），在三个ManiSkill任务（pick cube、push cube、stack cube）上报告平均改进。</details> |
| 2025-10-02 | VLA-R1: Enhancing Reasoning in Vision-Language-Action Models | http://arxiv.org/abs/2510.01623 | <details><summary>展开</summary>### 论文研究单位<br>GigaAI, CASIA, Tsinghua University<br><br>### 论文概述<br>该论文提出了VLA-R1，一个旨在增强视觉-语言-动作（VLA）模型推理能力的模型。现有VLA模型常缺乏显式的分步推理能力，其后训练流程也很少强化推理质量。为解决这些问题，VLA-R1集成了基于可验证奖励的强化学习（RLVR）和组相对策略优化（GRPO），通过精心设计的奖励函数来系统性地优化模型的推理和执行鲁棒性。此外，论文还构建了VLA-CoT-13K数据集，为模型提供与可供性和轨迹标签对齐的思维链监督。在多个基准测试、仿真环境和真实机器人平台上的评估表明，VLA-R1具有优越的性能和泛化能力。<br><br>### 论文核心贡献点<br>1. 提出了VLA-R1模型，引入了基于RLVR的优化方案，通过GRPO和精心设计的奖励（区域对齐、轨迹一致性、输出格式），系统性地增强了模型的推理和执行鲁棒性。<br>2. 开发了VLA-CoT数据引擎，生成了与可供性和轨迹标签对齐的高质量数据集VLA-CoT-13K，弥补了现有VLA模型缺乏逐步推理监督的不足。<br>3. 在域内、域外、仿真和真实机器人平台上对VLA-R1进行了全面评估，验证了其有效性和跨域泛化能力。<br><br>### 论文方法描述<br>VLA-R1的训练方法包含两个主要阶段：<br>1. **监督微调（SFT）**：首先，使用Qwen2.5-VL-72B模型构建VLA-CoT-13K数据集，该数据集包含与可供性和轨迹注释对齐的逐步思维链。然后，在此数据集上对基于Qwen2.5-VL-3B的模型进行微调，同时监督模型输出的思维片段和最终动作片段。<br>2. **强化学习（RL）**：在SFT之后，使用GRPO算法进行进一步优化。该过程设计了三种可验证的奖励函数：<br> - **Fréchet轨迹奖励**：采用角度长度增强的Fréchet距离（ALAF）来衡量预测轨迹与真实轨迹的对齐程度，考虑了位置、角度和分段长度。<br> - **GIoU可供性奖励**：使用广义交并比（GIoU）作为奖励，以评估预测边界框与真实边界框的空间对齐质量，特别是在非重叠情况下也能提供有效梯度。<br> - **格式奖励**：一个二元奖励，用于强制模型的输出必须遵循指定的结构（思维片段后跟动作片段）。<br><br>### 论文使用数据集和训练资源<br>- **使用数据集**:<br> - **训练数据**: ShareRobot数据集，源自Open X-Embodiment，包含102个操作场景和12种机器人形态的供能力与轨迹注释。<br> - **合成数据**: VLA-CoT-13K，一个包含13K条思维链注释的高质量数据集，通过VLA-CoT数据引擎生成，与可供性和轨迹标签对齐。<br> - **域外评估数据**:<br> - 可供性任务：UMD Part Affordance数据集的子集。<br> - 轨迹任务：VAIT数据集（LLARVA的验证集）。<br>- **训练资源**: 论文中未明确说明所使用的具体计算资源（如GPU类型、数量、训练时长等）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**:<br> - **基准测试**: 在ShareRobot（域内）以及UMD和VAIT（域外）数据集上进行离线评估。<br> - **仿真环境**: 使用RoboTwin模拟器，在带有随机杂物的桌面场景中进行测试，评估了Piper和UR5两种机器人平台。<br> - **真实世界**: 在一个真实的桌面平台上，设计了四个经典场景（碗抓取、水果抓取、厨房场景、混合场景）进行评估。<br>- **评估指标**:<br> - **可供性任务**: 使用交并比（IoU）作为主要度量指标。<br> - **轨迹任务**: 使用离散Fréchet距离（DFD）、豪斯多夫距离（HD）和均方根误差（RMSE）来评估轨迹相似性。<br> - **仿真与真实世界任务**: 使用成功率（SR）作为任务级指标。对于可供性任务，成功定义为正确定位并抓取物体；对于轨迹任务，成功定义为将物体成功运送到指定目标区域。</details> |
| 2025-10-01 | INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models | http://arxiv.org/abs/2510.01389 | <details><summary>展开</summary>待生成</details> |
| 2025-10-01 | Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition | http://arxiv.org/abs/2510.01068 | <details><summary>展开</summary>待生成</details> |
| 2025-10-01 | HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy | http://arxiv.org/abs/2510.00695 | <details><summary>展开</summary>### 论文研究单位<br>KAIST、UC Berkeley、RLWRLD<br><br>### 论文概述<br>现有视觉-语言-行动（Vision-Language-Action, VLA）模型依赖当前观察进行行动预测，忽略历史上下文，导致在需要长期推理的任务中性能受限。本论文提出HAMLET框架，通过引入记忆机制使VLA能够利用历史信息。具体方法包括：<br>- 使用可学习的"moment tokens"压缩每个时间步的感知信息，并通过时间对比学习初始化以突出时序特征。<br>- 集成轻量级"memory module"聚合历史moment tokens，生成行动预测的条件特征。<br>实验表明，HAMLET在真实世界长时序任务和仿真基准上显著提升性能，且无需修改原始VLA架构。<br><br>### 论文核心贡献点<br>- 动机分析：现有VLA假设当前观察独立，限制了在历史依赖任务（如遮挡或长期操作）中的表现。<br>- 方法创新：提出HAMLET框架，通过moment tokens和memory module增强历史感知。<br>- 实验验证：跨多VLA backbone（GR00T N1.5、CogACT）验证有效性，在真实世界任务平均提升47.2%，仿真基准（RoboCasa、LIBERO）也获益。<br>- 通用性：backbone-agnostic设计，plug-and-play适配，无需额外预训练。<br>- 效率优化：避免多帧输入带来的计算开销，内存占用仅为多帧基线的约2倍。<br><br>### 论文方法描述<br>HAMLET包含两个核心组件：<br>- **Context compression via moment tokens**：在每个时间步t，附加可学习向量$\mathbf{m}_t$到VLM输入，通过冻结VLM进行时间对比学习初始化（使用同一时间步的正样本和不同时间步的负样本），使moment tokens捕捉时序判别特征，过滤静态背景。<br>- **Memory consolidation via memory module**：使用2层Transformer处理堆叠的近T个moment tokens（如$\mathbf{M}' = [\mathbf{m}'_{t-k(T-1)}, ..., \mathbf{m}'_t]$），通过因果自注意力生成历史增强特征$\tilde{\mathbf{m}}'$，与VLM表示$\mathbf{h}_t$拼接后输入行动专家，预测k步行动。<br>训练流程：冻结VLM进行moment tokens初始化（30k步），随后联合训练行动预测（60k步）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 真实世界任务：3个桌面任务（Pick-and-Place Twice、Cover-and-Stack、Swap Cubes），每任务50个演示，平均约268帧/轨迹。<br> - 仿真基准：<br> - RoboCasa Kitchen：24个任务，训练演示30/100/300每任务。<br> - LIBERO：40个任务，分4套件（Spatial、Object、Goal、Long）。<br> - SimplerEnv-Bridge：基于WidowX机器人，使用BridgeV2数据集。<br>- **训练资源**：使用NVIDIA A100 GPU进行训练和测量（延迟和内存）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 真实世界：Franka Research 3机器人+Robotiq 2F-85 gripper，双视角摄像头。<br> - 仿真：RoboCasa Kitchen（Franka机器人）、LIBERO（Franka）、SimplerEnv-Bridge（WidowX）。<br>- **评估指标**：<br> - 成功率和部分成功率（如Pick-and-Place Once、Cover Cube），跨任务和基准报告。<br> - 效率指标：延迟（毫秒）和峰值内存使用（MB），在RoboCasa数据集上测量。</details> |
| 2025-10-01 | Hybrid Training for Vision-Language-Action Models | http://arxiv.org/abs/2510.00600 | <details><summary>展开</summary>### 论文研究单位<br>Qualcomm AI Research, University of Tübingen, Max Planck Institute<br><br>### 论文概述<br>本文提出了一种名为混合训练（Hybrid Training, HyT）的框架，用于训练视觉-语言-动作模型（VLAs）。该方法使模型能够从思维链（CoT）推理中学习，同时保持与标准VLA相同的快速推理速度。HyT通过引入模态变量，支持模型在推理时直接预测动作、生成思维或遵循指令等多种模式，从而在模拟基准测试和真实世界实验中提升了性能。<br><br>### 论文核心贡献点<br>- 提出混合训练（HyT）框架，使VLAs能够从CoT中学习，同时在推理时跳过CoT生成，实现快速动作预测。<br>- 通过模态变量控制模型输出模式，支持直接动作预测（act）、思维生成（think）和指令跟随（follow）三种模式。<br>- 验证了CoT技术的性能提升主要源于训练时对思维的内部化，而非推理时的显式生成。<br><br>### 论文方法描述<br>1. **混合训练公式**：定义动作条件分布为思维和模态变量的边缘化分布，公式为：<br> \[<br> p(a_t\|x_t,l) = \sum_i \sum_j p_{\theta}(a_t, \tau^i, m^j\|x_t,l)p(m^j)<br> \]<br> 其中，\(\tau\) 为思维，\(m\) 为模态变量。<br>2. **训练实现**：使用蒙特卡洛估计，以不同概率采样条件输入和输出（如act、think、follow模式），训练模型预测多样化条件分布。<br>3. **推理模式**：通过设置模态变量（如`<act>`），模型可直接输出动作，无需生成思维；或使用`<think>`生成思维，增强可解释性。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - ClevrSkills：包含3000个演示轨迹，涵盖空间放置、堆叠等9个任务。<br> - LIBERO：4个任务套件（Spatial、Object、Goal、Long），使用LLM生成思维链标注。<br> - 真实世界数据：320条轨迹，来自UFactory xArm 6机械臂，涵盖分布内和分布外任务。<br>- **训练资源**：<br> - ClevrSkills：使用4块A100 GPU，批量大小32，学习率2e-5，从PaliGemma-2（3B参数）微调。<br> - LIBERO：结合OFT策略，从Prismatic VLM微调。<br> - 真实世界实验：使用OpenVLA（7B参数），LoRA微调（rank 32），学习率5e-4。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 模拟环境：ClevrSkills（基于ManiSkill2）、LIBERO。<br> - 真实环境：UFactory xArm 6机械臂，配备RealSense D435相机。<br>- **评估指标**：<br> - 成功率（Success Rate）：任务完成率，每个模型在100个评估回合中测试。<br> - 推理时间：动作生成频率，HyT与标准VLA相当（约3Hz），显著快于ECoT（慢3倍）和分层VLA（慢4倍）。<br> - 平均性能：多任务场景下的综合成功率，如LIBERO的4套件平均得分。</details> |
| 2025-10-01 | VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators | http://arxiv.org/abs/2510.00406 | <details><summary>展开</summary>### 论文研究单位<br>西湖大学、浙江大学、复旦大学、OpenHelix团队、北京邮电大学、郑州大学、河北工业大学等<br><br>### 论文概述<br>论文提出 VLA-RFT（Vision-Language-Action 强化微调），用学习到的世界模型作为可控模拟器为 VLA 提供验证奖励，在模拟器中对策略进行轨迹级强化优化，以更低样本复杂度提升 VLA 的泛化与鲁棒性。<br><br>### 论文核心贡献点<br>- 世界模型作为数据驱动的可交互模拟器，基于动作预测未来视觉观察，支持策略多 rollout 生成与反馈<br>- 轨迹级“验证奖励”，通过将生成的视觉轨迹与目标达成参考轨迹对比获得密集且动作对齐的学习信号<br>- 将流匹配扩展为 SDE 策略（SDE-Policy），引入 Sigma Net 表征随机性；结合 GRPO 进行高效稳定微调<br>- 只需约 400 步强化微调即可显著优于强监督基线，并在扰动场景下展现更强鲁棒性<br><br>### 论文方法描述<br>两阶段训练：Stage I 预训练世界模型与 VLA；Stage II 在世界模型中交互 rollout、计算验证奖励并用 GRPO 优化 VLA。<br>- 世界模型：基于 LLaMA 架构的轻量自回归视频预测模型（138M 参数），以最大似然训练<br>- VLA 预训练：VLM 编码器 + 流匹配动作头，MSE 目标稳定输出动作片段<br>- SDE-Policy：在流匹配基础上加入 Sigma Net（输出方差），将 ODE 扩展为 SDE；在 K=10 步扩散积分中计算平均对数似然并形成策略比 r<br>- 验证奖励：将世界模型生成的轨迹与离线专家轨迹对齐，定义为 L1 与 LPIPS 的负加权和；同起点 rollout 的奖励做组内平均以减方差<br>- 目标函数：GRPO 损失 + 流匹配 MSE 辅助项 + 熵正则，确保高效与稳定<br><br>### 论文使用数据集和训练资源<br>- 数据集与基准：LIBERO（Spatial、Object、Goal、Long 四套任务）<br>- 基础策略：轻量 VLA-Adapter；先进行监督微调，再进行强化微调<br>- 世界模型：138M 参数的自回归模型，基于 LLaMA 架构，在 LIBERO 上预训练<br>- 训练资源：4× A800 GPU；采用 VERL 分布式强化框架与 FSDP 切分训练<br><br>### 论文使用的评估环境和评估指标<br>- 世界模型评估：像素误差（MSE）、峰值信噪比（PSNR）、结构相似性（SSIM）、感知距离（LPIPS）<br>- 策略评估：成功率（SR）在标准套件与扰动套件；扰动包含物体/目标位置、机器人状态及组合扰动<br>- 关键结果：约 400 步 RFT 将平均 SR 从 86.6% 提升至 91.1%，且在各类扰动下保持更高稳定性；世界模型具备高保真视觉预测能力，显著低于监督扩展 SFT 所需迭代量</details> |
| 2025-09-30 | MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation | http://arxiv.org/abs/2509.26642 | <details><summary>展开</summary># 论文研究单位<br>- 北京大学多媒体信息处理国家重点实验室<br>- 北京人形机器人创新中心<br>- 香港中文大学（CUHK）<br><br># 论文概述<br>本论文提出了一种多感觉语言-动作模型（MLA），用于机器人的多模态理解和预测。MLA通过无编码器的多模态对齐机制，直接整合2D图像、3D点云和触觉信号，并利用未来多感觉生成后训练策略增强物理动态理解。在复杂、接触丰富的真实世界任务中，MLA在成功率上分别超越之前的SOTA 2D和3D VLA方法12%和24%，同时展示了在未见配置下的强泛化能力。模型基于LLM主干（如LLaMA-2 7B），通过渐进式训练管道（预训练、监督微调、后训练）实现感知、理解和动作生成的统一。<br><br># 论文核心贡献点<br>1. **无编码器的多模态对齐机制**：MLA将LLM自身重新用作感知模块，通过标记级对比学习对齐图像、点云和触觉标记，利用位置引导的一致性约束。<br>2. **未来多感觉生成后训练策略**：模型联合预测未来图像、点云和触觉状态，增强物理动态建模和动作生成条件。<br>3. **渐进式训练配方**：通过预训练（570K+轨迹）、监督微调（SFT）和后训练，实现SOTA性能和泛化，覆盖真实世界单臂和双臂任务。<br>4. **真实世界和仿真验证**：在RLBench仿真器上评估并实现竞争性能，验证方法的稳健性。<br><br># 论文方法描述<br>MLA方法包括以下组件：<br>- **MLA架构**：<br> - **多模态标记器**：图像标记器（14×14 patches，256 tokens）、3D点云标记器（FPS、KNN，256 tokens）、触觉标记器（MLP编码器，单token）。<br> - **LLM主干**：基于LLaMA-2 7B，处理统一标记序列，支持扩散动作生成。<br> - **未来预测解码器**：变换器解码器，分别预测未来图像（MSE损失）、点云（Chamfer距离）和触觉信号（MSE损失）。<br>- **无编码器多模态对齐**：通过位置映射构建跨模态正对（图像-点云-触觉），应用标记级InfoNCE损失（温度τ），使用8层变换器输出优化。<br>- **未来多感觉生成**：预测关键帧未来状态（由机器人关节速度触发），针对图像、点云和触觉联合训练，增强语义、几何和交互理解。<br>- **训练配方**：预训练（SFT阶段前）：仅图像-动作数据，10轮训练；SFT：引入多模态输入，对比损失；后训练：添加未来生成监督。总损失为扩散损失（DDPM）、对比损失和未来生成损失之和。<br><br># 论文使用数据集和训练资源<br>- **预训练数据集**：570K轨迹，结合28个开放源数据集（如Open-X-Embodiment、RoboMIND、DROID等），采样率详见附录Table III，总计36M帧。<br>- **自收集真实世界数据**：6个任务（单臂4个：盖章、擦白板、放盘子、放鸡蛋；双臂2个：舀玉米粒、开锅盖），每任务200个高质量演示，使用Gello平台采集。<br>- **仿真数据**：RLBench任务（10个），每任务100演示轨迹。<br>- **训练资源**：AdamW优化器；SFT：300轮，后训练：100轮；基础模型初始化自Prismatic VLM；推理使用DDIM（4步）。<br><br># 论文使用的评估环境和评估指标<br>- **真实世界评估**：Frank a Research 3单臂和双臂设置，配备RealSense D455相机（第三人和手腕视角）和Tactile传感器。评估接触丰富任务，每个任务15次 rollout，成功率由人工判定。指标：成功率（S.R.）和方差。<br>- **仿真评估**：基于RLBench（CoppeliaSim），图像和点云数据，20次 rollout per task，成功率由内置评估模块计算。指标：平均成功率（%）和方差。<br>- **泛化测试**：未见物体（如生菜代替鸡蛋）和未见背景（杂散场景），对比π_0基线，量化性能下降（如-15%）。<br>- **消融研究**：无编码器对齐（标记级vs图像级）、对比损失层位置、生成模态贡献等，指标为任务成功率变化。</details> |
| 2025-09-30 | Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA | http://arxiv.org/abs/2509.26251 | <details><summary>展开</summary>### 论文研究单位<br>- 主要单位：清华大学深圳国际研究生院（Tsinghua Shenzhen International Graduate School, Tsinghua University）、阿里巴巴集团高德地图（Amap, Alibaba Group）<br>- 合作单位：西安交通大学软件学院（School of Software Engineering, Xi’an Jiaotong University）<br><br>### 论文概述<br>论文针对潜在行动模型（LAMs）在视觉-语言-行动（VLA）系统中的两个核心瓶颈：空间理解不足（仅依赖RGB编码，忽视几何结构）和时序感知有限（依赖稀疏双帧输入，忽略长时动态），提出了一种名为SSM-VLA（Seeing Space and Motion - Vision-Language-Action）的框架。SSM-VLA通过融合几何感知空间编码、多尺度时序建模和视觉思维链推理，增强了VLA系统的鲁棒性和可解释性。<br><br>### 论文核心贡献点<br>1. **Farsighted-LAM模型**：通过DINOv2特征的几何感知空间编码和多尺度时序建模，提升潜在行动的空间-动态表示能力。<br>2. **SSM-VLA架构**：集成Farsighted-LAM与视觉思维链（VisualCoT）推理模块，实现结构化感知与显式推理的结合，增强决策一致性和可解释性。<br>3. **实验验证**：在仿真（CALVIN ABC-D基准）和真实环境中取得最先进性能，证明几何建模、时序一致性和显式推理的有效性。<br><br>### 论文方法描述<br>- **Farsighted-LAM**：<br> - 编码器：处理当前RGB帧和多个未来关键帧（DINOv2特征），通过潜在行动查询生成离散潜在行动序列。<br> - 解码器：重构未来观测（RGB和深度），采用多模态重建损失（L2和LPIPS损失、梯度感知深度损失）监督。<br>- **VLA策略**：<br> - 三阶段推理：视觉思维链预测未来观测 → 远视潜在行动推理 → 条件流匹配生成行动。<br> - 多模态协同注意：统一变换器内实现，结构化注意机制管理信息流。<br>- **关键组件**：几何先验（深度监督）、视觉思维链推理、潜在行动量化。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - CALVIN基准：仿真环境（Franka Panda机械臂），训练集为A/B/C环境，测试集为未见环境D。<br> - 真实世界数据：基于Open-X-Embodiment数据集预训练，在50个人类演示上微调（AgileX Piper机器人）。<br>- **训练资源**：<br> - 潜在行动模型：AdamW优化器，学习率10^-4，权重衰减10^-5，批大小256，训练步100，代码本大小32。<br> - VLA模型：AdamW优化器，学习率10^-3，权重衰减10^-4，批大小64，训练步30，损失权重λ_vision=0.1，λ_latent=0.01。<br> - 流程匹配：10步去噪。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - CALVIN仿真基准：34个操控任务，语言指令，1000个指令链（每链5个连续任务）。<br> - 真实世界实验：AgileX Piper机器人执行放置任务（粉色球入盒），环境包括杂乱背景。<br>- **评估指标**：<br> - CALVIN基准：连续任务完成率（1-5步）和平均成功链长（与基线对比）。<br> - 消融研究：验证Farsighted-LAM结构、多模态注意和几何先验的贡献。<br> - 真实世界：定性展示泛化能力（视觉重建对齐）。</details> |
| 2025-09-30 | TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses | http://arxiv.org/abs/2509.25746 | <details><summary>展开</summary>### 论文研究单位<br>小米机器人（Xiaomi Robotics）<br><br>### 论文概述<br>针对机器人灵巧手抓取执行阶段的“最后一公里”精度问题，论文提出TacRefineNet——首个仅依靠多指指尖触觉反馈实现已知物体任意姿态间精确调整的框架。该方法通过迭代调整末端执行器姿态，利用高分辨率压阻式触觉传感器捕获指尖接触力，实现毫米级抓取精度。为跨越仿真到现实的鸿沟，研究团队构建了MuJoCo仿真环境与真实机器人结合的数据集，并设计了多分支融合策略网络。<br><br>### 论文核心贡献点<br>1. **首个纯触觉驱动框架**：提出TacRefineNet，实现六自由度手部姿态的毫米级精确调整，完全依赖指尖触觉反馈<br>2. **多分支融合策略网络**：通过融合多指触觉信号与本体感受，设计支持任意姿态间转换的策略网络，无需逐姿态重训练<br>3. **仿真-现实高效结合**：采用仿真预训练+现实微调的混合训练策略，仅用少量真实数据即显著提升性能<br><br>### 论文方法描述<br>- **迭代调整策略**：在多指建立接触后，根据触觉图像与目标图像的差异，预测末端执行器的六自由度位姿增量（Δx），通过重复抓取-调整过程收敛至目标姿态<br>- **触觉传感系统**：指尖集成11×9压阻式触觉阵列（像素间距1.1mm），实时输出触觉图像用于特征提取；在MuJoCo中建立物理基触觉模拟，弹性接触点模拟真实传感器弹性行为<br>- **TacRefineNet网络架构**：<br> 1. 多分支视觉编码器处理每根手指的当前/目标触觉图像<br> 2. 融合多指触觉特征与关节位置信息<br> 3. 三层MLP回归六自由度位姿增量<br>- **跨组合训练方案**：随机配对当前与目标触觉图像构建N×N组合，增强策略对任意目标姿态的适应性<br>- **数据增强**：对触觉图像实施缩放噪声，对关节位置添加高斯噪声提升鲁棒性<br><br>### 论文使用数据集和训练资源<br>- **仿真数据集（Ds_img）**：基于MuJoCo物理引擎生成，在限制维度范围内系统采样手部姿态（pitch/roll/y/z轴），记录对应触觉图像、关节位置及目标姿态<br>- **真实数据集（Dr_img）**：复现实实验证中的可行姿态，增大采样步长降低数据密度，剔除实际碰撞姿态<br>- **混合训练策略**：对比两种政策<br> - 政策A：仅仿真数据训练<br> - 政策B：仿真预训练 + 真实数据微调<br>- **硬件资源**：<br> - 仿真：MuJoCo物理引擎<br> - 实物：11自由度灵巧手 + 11×9指尖触觉传感器阵列（0-255离散力值）<br><br>### 论文使用的评估环境和评估指标<br>- **实验场景**：<br> - 多维度姿态调整：16种初始-目标姿态组合（pitch/roll/y/z轴对称设置）<br> - 动态目标跟踪：连续扰动下的长期姿态维持任务<br> - 未见物体泛化：相似几何但不同形态的物体测试<br>- **评估指标**：<br> 1. **精度指标（Metric 1）**：10步调整后的6DOF误差<br> - 位置误差：δ_pos = \|\|p - p_g\|\|₂<br> - 角度误差：δ_rot = 2arccos(\|⟨Q, Q_g⟩\|)<br> 2. **效率指标（Metric 2）**：达到精度阈值的步数<br> - 位置阈值：ε_pos = 0.005m<br> - 角度阈值：ε_rot = 0.05rad<br> 3. **成功率（Metric 3）**：R=5次试验的成功率<br> - 成功判定：同时满足δ_pos ≤ ε_pos、δ_rot ≤ ε_rot，且调整步数≤S_max<br>- **关键结论**：政策B在所有指标上显著优于政策A，最佳组合实现1.1mm位置精度/0.016rad角度精度，100%成功率。16种组合均达毫米级位置精度，最优姿态误差0.009rad。动态场景验证稳定性，未见物体在滚转维度表现出较好泛化性。<br><br>> 注：论文方法在当前对象范围内有效，未来需融合视觉信息突破单对象限制。</details> |
| 2025-09-30 | VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning | http://arxiv.org/abs/2509.25718 | <details><summary>展开</summary># VLA模型通过动作分块PPO和自监督行为克隆的后训练<br><br>## 论文研究单位<br>- 中国科学院自动化研究所多模态人工智能系统国家重点实验室<br>- 中国科学院大学人工智能学院<br><br>## 论文概述<br>本文提出了一种基于动作分块近端策略优化（PPO）和自监督行为克隆的VLA（视觉-语言-动作）模型后训练方法。该方法针对强化学习在VLA模型后训练中面临的稀疏奖励和不稳定训练挑战，通过将连续动作聚合为动作分块来提高策略的时间一致性和反馈密度，同时结合动态更新的演示缓冲区进行自监督行为克隆辅助训练。<br><br>## 论文核心贡献点<br>1. 开发了用于VLA模型后训练的动作分块PPO算法，增加了有效的信息反馈密度<br>2. 构建了基于动态演示缓冲区的自监督行为克隆辅助损失，在训练过程中持续收集高质量任务轨迹<br>3. 实验表明，该方法仅用10个演示就超过了使用100个演示的监督微调性能，成功率达0.93，步数减少到42.17<br><br>## 论文方法描述<br>### 动作分块PPO<br>- 将连续动作聚合为长度为h≥1的动作分块，提高奖励反馈频率<br>- 在actor-critic架构上实现PPO框架，使用截断代理目标限制策略差异<br>- 通过广义优势估计（GAE）计算优势函数<br><br>### 自监督行为克隆<br>- 初始化时使用专家轨迹构建动态演示缓冲区<br>- 训练过程中将高质量成功轨迹加入缓冲区<br>- 使用轨迹长度作为质量代理指标，较短轨迹表示更高质量<br>- 构建辅助监督损失L_BC进行策略优化<br><br>### 在线后训练<br>- 总损失函数为加权组合：L_online = β_t * L_PPO + L_BC<br>- β_t采用渐进调度：β_t = tanh(t/T_warmup)，实现从监督学习到强化学习的平滑过渡<br>- 早期训练完全由行为克隆驱动，随训练进展逐步强调PPO目标<br><br>## 论文使用数据集和训练资源<br>- **数据集**：MetaWorld环境中的MT10基准，包含10个单任务环境<br>- **演示数据**：每个任务使用规则策略收集演示轨迹，每条轨迹限制200步<br>- **模型基础**：Octo-small作为VLA模型骨干网络<br>- **训练配置**：<br> - 优化器：AdamW，学习率10^-5<br> - GAE λ=0.99，折扣因子γ=0.99<br> - PPO截断ε=0.2，值损失权重=0.5，熵损失权重=0.0<br> - 动作分块长度h=4，预热步数T_warmup=40k<br> - 批次大小16，总训练步数500k<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：MetaWorld MT10基准测试<br>- **评估配置**：每任务评估128个回合，使用统一随机种子<br>- **评估指标**：<br> - 平均成功率（Acc.）：128个回合的成功率<br> - 轨迹长度分布的第10百分位数（Len.）<br> - 最短10%轨迹的平均长度（Avg(l).）<br>- **对比基线**：<br> - SFT：10个演示的监督微调<br> - SFT_100：100个演示的监督微调<br> - 标准PPO：无额外演示的标准PPO<br>- **评估结果**：提出的方法在平均性能和大多数单任务指标上均排名第一，证明了RL后训练的可行性和有效性</details> |
| 2025-09-30 | dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought | http://arxiv.org/abs/2509.25681 | <details><summary>展开</summary>### 论文研究单位<br>美的集团、北京大学、上海交通大学<br><br>### 论文概述<br>论文提出 dVLA（diffusion Vision-Language-Action），首个以离散扩散语言模型（DLM）为核心的视觉-语言-动作一体化框架。该模型在统一的扩散目标下，联合优化视觉推理、文本推理与动作生成，并通过多模态思维链（CoT）将高级指令分解为可执行的子目标和动作。在模拟和真实环境中验证有效性：LIBERO 基准平均成功率达 96.4%，真实 Franka 机器人完成多项任务，包括具有挑战性的多步分拣。<br><br>### 论文核心贡献点<br>- 首个基于离散扩散语言模型的 VLA 框架，统一视觉、语言与动作的概率建模与生成。<br>- 提出多模态 CoT 训练范式，模型需同时生成子目标图像（视觉 CoT）、文本推理与离散动作，并在训练中对三者均执行随机掩码与重建，强化跨模态一致性。<br>- 在 LIBERO 基准取得 96.4% 平均成功率，优于离散与连续动作策略；在真实机器人任务上同样领先。<br>- 引入推理加速策略：块级因果前缀注意力（训练时）与 KV 缓存（推理时），获得约 2× 推理加速，性能损失微小。<br>- 模型可预测不安全动作对应的失败视觉 CoT，体现对物理规律和执行后果的内在理解。<br><br>### 论文方法描述<br>- 统一离散化与扩散训练：将视觉（MAGVIT-v2）、文本（LLaDA tokenizer）与动作（FAST：DCT + BPE）统一为离散 token，在相同扩散目标下随机掩码并重建，仅对掩码 token 计算损失。<br>- 架构与初始化：基于 MMaDA 扩展，使用不同分词器并将词汇表从 126,464 扩展至 136,704；所有模态共享一个离散扩散建模目标。<br>- 多模态 CoT 数据与序列结构：输入为 [多视角图像、语言指令、机器人状态]，输出为 [视觉子目标图像、文本推理、离散动作块]，通过起止标记组织序列。<br>- 推理生成：并行输出视觉 CoT（未来状态图像）与文本 CoT（子任务步骤），再据此生成可执行的离散动作。<br>- 加速策略：训练时采用分块因果前缀注意力（块内双向，块间单向），推理时引入 KV 缓存（dLLM-Cache 思想），减少重复计算，实现约 2× 加速。<br><br>### 论文使用数据集和训练资源<br>- 模拟：LIBERO 四个套件（Spatial、Object、Goal、Long），共 40 个任务，每任务 50 条演示；分辨率提升至 256×256。<br>- 真实：Franka 7-DoF 机械臂，1100 条轨迹，涵盖四项任务：<br> - Bin Picking：600 条<br> - Open Box：100 条<br> - Hang Cups：200 条<br> - Pick & place Object：200 条<br>- 训练细节：<br> - 输入图像统一 resize 至 256×256。<br> - 子目标图像预测时域在 [0.9C, 1.1C]，其中 C=5（LIBERO）、C=50（真实任务），仅预测俯视相机图像，使用 classifier-free guidance（scale=3.5）。<br> - 文本推理使用 SEED-1.5VL 的视频分割注释，长任务每 3 秒一段，简单任务可省略以加速推理。<br> - 训练流程与 MMaDA 一致，未明确给出硬件资源。<br><br>### 论文使用的评估环境和评估指标<br>- 模拟评估：LIBERO 基准，每任务 50 次试验（合计 500 次），以任务成功率（SR）为主要指标；与多种连续与离散动作基线对比。<br>- 真实评估：Franka 机器人（2 个 ZED 外置相机 + 1 个 Realsense 435i 手腕相机），每任务 10 次试验（合计 40 次），报告成功率。<br>- 推理效率：比较全注意力与前缀注意力 + KV 缓存两方案下的动作频率（Hz）与成功率，验证约 2× 推理加速与性能保持。</details> |
| 2025-09-29 | World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training | http://arxiv.org/abs/2509.24948 | <details><summary>展开</summary>## 论文研究单位<br>Sun Yat-sen University, China; Amap, Alibaba Group<br><br>## 论文概述<br>Vision-Language-Action (VLA) 模型在少样本与高风险场景下受限：模仿学习对大量演示数据依赖强，强化学习（RL）后训练需真实交互但环境状态不可重置、反馈稀疏且难以判定任务完成。作者提出 World-Env，将世界模型作为低成本、可安全探索的虚拟环境用于 VLA 的 RL 后训练：包括一个视频世界模型用于生成时序一致的未来视觉观测，和一个 VLM 引导的即时反射器用于连续奖励与实时终止信号。该方法在 5 条专家演示下即可显著提升复杂操控任务的成功率，兼顾数据效率与安全性。<br><br>## 论文核心贡献点<br>- 提出 World-Env 框架：以世界模型替代真实交互，支持 VLA 的低成本、风险可控的 RL 后训练。<br>- 集成视频世界模拟器和 VLM 引导的即时反射器，提供语义对齐的连续奖励与语言对齐的任务完成判定。<br>- 引入动态终止机制（基于 R(o1:t, g) > η），避免成功后冗余动作，提升执行效率与成功率。<br><br>## 论文方法描述<br>- 视频世界模拟器（EVAC 思路）：以动作 a_t 和下时刻本体感觉状态 s_{t+1}（6D末端位姿与 1D夹爪）作为输入，投影构建动作图（前景标记+黑背景），以像素级条件通过扩散图像生成预测下一帧视觉观测 o_{t+1}。训练数据混合人类演示与自探索轨迹；自探索通过在模拟器内执行 SFT 策略并用 Laplace 分布引入扰动（scale head 预测 β_t），以覆盖失败与次优行为分布。<br>- VLM 引导的即时反射器：基于 LLaVA，使用冻结的视觉编码器与 LLM 以视频-文本为条件，奖励头输出 R(o1:t, g) ∈ [0,1]，采用阈值 η=0.5 触发终止；训练采用二进制交叉熵（BCE）监督，数据源自 LIBERO 成功判定与模拟器内 oracle 标记。<br>- VLA 策略与 RL：基于 OpenVLA-OFT 的 VLA 策略添加 scale head 使动作分布为 Laplace(μ_t, β_t)，用于不确定度驱动的探索。RL 优化采用 LOOP（RLOO 优势 + PPO 截断目标）：对同一初始状态生成 N 条轨迹（N=8），用 RLOO 计算轨迹优势 A_n，基于 PPO 截断目标更新策略与 scale head。<br>- 终止与奖励使用：在终止步 t_end 给出单步轨迹奖励 R_n = R(o1:t_end, g)，优势广播到轨迹各步；终止时提前结束 rollout。<br>- 训练实现：LoRA 微调视觉-语言骨干（rank=32），LoRA LR=1e-4，动作/标度头全参 LR=1e-5，批量 4，8×H20 GPU（96GB）。<br><br>## 论文使用数据集和训练资源<br>- 数据集：LIBERO（含 LIBERO-Goal、LIBERO-Object、LIBERO-Spatial、LIBERO-Long 四套），每任务仅用 5 条专家演示进行 SFT 预训练；世界模型额外加入自探索过渡数据（含成功与失败）。<br>- 资源：8×NVIDIA H20 GPU（96GB），LoRA（rank=32）进行参数高效微调。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：LIBERO 仿真平台，面向机器人视觉-语言操控。<br>- 指标：成功率（对全测试集）。在“真实反馈约束”设定下比较终止策略（无法获得真实终止信号），验证动态终止与连续奖励的稳定性。</details> |
| 2025-09-29 | IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks | http://arxiv.org/abs/2509.24768 | <details><summary>展开</summary># 论文研究单位<br><br>芬兰阿尔托大学智能机器人组、芬兰奥卢大学生物仿生与智能系统组、丹麦技术大学机械技术系<br><br># 论文概述<br><br>论文提出了IA-VLA框架，用于解决视觉语言行动模型(VLAs)在处理语义复杂任务时的局限性，特别是在涉及视觉重复对象场景下的机器人操控任务。当前VLA模型受限于推理延迟，无法使用足够大的语言模型来处理复杂空间关系和语义指令。<br><br># 论文核心贡献点<br><br>1. 提出了IA-VLA框架，通过大型视觉语言模型作为预处理阶段来增强VLA的输入<br>2. 形式化了视觉重复对象这一复杂的任务类别，构建了相关数据集<br>3. 在涉及重复对象的设置中进行了全面的实验评估，总计1290次评估运行<br>4. 证明了该框架能够显著提高VLA在语义复杂指令下的性能表现<br><br># 论文方法描述<br><br>**IA-VLA框架流程：**<br>- 使用Semantic-SAM对输入图像进行分割，添加数字标签<br>- 调用大型VLM（如GPT-4.1）根据任务描述选择相关对象的数字标签<br>- 对相关对象应用语义分割掩码，对其他区域使用半透明灰色掩码（alpha值0.8）<br>- 通过SAM2实现掩码在时间序列中的传播<br>- 支持两种变体：原始指令版本和重标记版本（将VLA指令简化为"lift the highlighted object"等）<br><br>**掩码过滤算法：**<br>- 掩码块过滤器：将非连通块分离为独立掩码<br>- 掩码重叠过滤器：根据重叠程度合并、丢弃或相减掩码<br>- 面积阈值过滤：移除过小的掩码<br><br># 论文使用数据集和训练资源<br><br>**数据集构成：**<br>- 积木块提升任务：120个演示，12种语言指令<br>- 蔬菜装锅任务：120个演示，12种语言指令<br>- 抽屉开启任务：600个演示，12种语言指令<br>- 训练在桌面任务（积木块和厨房）使用联合模型，抽屉任务单独训练<br><br>**训练配置：**<br>- 基于OpenVLA模型进行微调<br>- 批次大小：16，学习率：0.0005，LoRA秩：32，dropout：0.0<br>- 桌面任务训练25000步，抽屉任务训练50000步<br>- 使用OpenVLA默认图像增强策略<br><br>**计算资源：**<br>- Aalto Science-IT项目和CSC - 芬兰IT科学中心提供的计算资源<br>- Aalto电气工程学院MIDAS研究基础设施<br><br># 论文使用的评估环境和评估指标<br><br>**评估任务设置：**<br>三类任务指令复杂度：<br>- Category 1：训练数据中见过的语言指令<br>- Category 2：见过概念的未见过组合<br>- Category 3：需要从训练概念外推的指令<br><br>**具体任务：**<br>1. **积木块提升**：要求识别特定颜色和位置的积木块进行抓取<br>2. **蔬菜装锅**：将蔬菜放入指定位置的锅中<br>3. **抽屉开启**：打开指定位置和行的抽屉<br><br>**评估指标：**<br>- 成功完成任务得1分<br>- 抓取正确对象但任务未完成得0.5分<br>- 30秒时间限制（不包括预处理时间）<br>- 每个配置进行5次评估运行<br>- 结果以最大可能积分的百分比形式报告<br><br>**评估规模：**<br>- 积木块任务：450次评估运行<br>- 蔬菜装锅任务：480次评估运行<br>- 抽屉开启任务：360次评估运行<br>- 总计1290次评估运行</details> |
| 2025-09-29 | Emergent World Representations in OpenVLA | http://arxiv.org/abs/2509.24559 | <details><summary>展开</summary>### 论文研究单位<br>- LSE.AI, London School of Economics<br>- ETH Zurich<br>- Princeton University - Department of Computer Science<br>- Mila - Quebec AI Institute<br><br>### 论文概述<br>Vision-Language-Action models (VLAs) 如OpenVLA，训练于策略基础强化学习（RL），但是否隐式学习世界模型（状态转移函数）未知。论文通过embedding arithmetic在状态表示上实验，探测OpenVLA是否编码潜在世界模型。使用线性/非线性探针预测状态转移向量，发现激活探针优于embeddings基线，表明世界模型存在。调查训练进展，发现世界模型随计算扩展而出现，并提出SAEs的可解释规划管道。<br><br>### 论文核心贡献点<br>- 利用embedding arithmetic证明OpenVLA编码潜在世界模型。<br>- 展示训练计算扩展增强世界模型发展，并定位其于中间层。<br>- 提出SAEs应用于可解释规划：预测状态转移向量后分解为可解释特征。<br>- 线性探针优于MLP探针，支持线性表示假设（LHR）。<br><br>### 论文方法描述<br>- **理论框架**：基于Koopman算子近似世界模型，定义K步状态转移算子。<br>- **状态转移向量**：学习函数 \( f: \mathbf{a}_t \mapsto \Delta\mathbf{e}_{t\rightarrow t+K} \)，其中 \(\Delta\mathbf{e} = \mathbf{e}_{t+K} - \mathbf{e}_t\)。<br>- **探针**：<br> - 线性探针（Lasso回归）预测 \(\Delta\mathbf{e}\) 从激活 \(\mathbf{a}_t\)。<br> - MLP探针测试非线性表示。<br>- **基线对比**：训练探针于embeddings作为基线，隔离因果效应。<br>- **评估**：R²分数和置换检验（p < 0.01）量化预测性能。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：LIBERO数据集（goal, spatial, long, object子集），共400 episodes, 66,931步。<br>- **模型**：OpenVLA (7B参数)，预训练于Open X-Embodiment数据集。<br>- **训练资源**：探针训练使用网格搜索调优超参数；计算资源未详述，代码可用。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO各子集测试集。<br>- **评估指标**：<br> - 回归R²分数评估预测能力。<br> - 置换检验（100次）检验统计显著性（p值）。<br> - Allan方差分析长时状态转移信噪比。<br> - 时间一致性（cosine similarity）测量嵌入稳定性。</details> |
| 2025-09-29 | PhysiAgent: An Embodied Agent Framework in Physical World | http://arxiv.org/abs/2509.24524 | <details><summary>展开</summary># 论文研究单位<br><br>根据论文致谢部分，该研究由无锡应用技术研究院、清华大学等机构支持，具体包括Wuxi Research Institute of Applied Technologies, Tsinghua University under Grant 20242001120。<br><br># 论文概述<br><br>本文提出了PhysiAgent框架，一个针对物理世界优化的具身智能体框架，旨在解决视觉-语言-动作（VLA）模型在有限泛化能力方面的问题。该框架通过整合监控、记忆、自反思机制以及轻量级工具箱，使VLMs能够根据VLAs的实时性能反馈动态组织不同组件，从而最大化VLAs的能力。实验在真实世界机器人操控任务中验证了显著的任务解决性能提升，展现了有效的自我调节能力和自适应演化特性。<br><br># 论文核心贡献点<br><br>1. **提出PhysiAgent框架**：一个训练无关的模块化具身智能体框架，能够灵活集成VLMs和VLAs用于真实世界部署<br><br>2. **将智能体范式引入物理世界**：将传统在语言或模拟领域探索的智能体范式引入物理世界，为VLMs配备真实世界感知和工具使用能力<br><br>3. **真实世界验证**：在真实世界机器人操控任务中验证框架，展现涌现的自我反思能力和显著的任务性能提升<br><br># 论文方法描述<br><br>PhysiAgent采用统一的自主脚手架机制，包含五个关键组件：<br><br>- **规划器（Planner）**：将原始语言指令分解为适合VLAs的可执行中间指令<br>- **监控器（Monitor）**：持续跟踪VLA执行进度，评估任务进展状态（阻碍、进行中、失败、完成）<br>- **反思器（Reflector）**：作为验证层改善监控精度，生成视觉约束纠正误判<br>- **记忆（Memory）**：包括短期记忆（存储步骤级数据）和长期记忆（存储高级摘要），支持持续适应<br>- **工具箱（Toolbox）**：提供感知、控制和推理工具，增强系统鲁棒性和适应性<br><br>框架实现双向信息流：从VLMs到VLAs的前向流和从VLAs行为到VLMs的后向反馈，使VLMs能够根据实时反馈调整输出。<br><br># 论文使用数据集和训练资源<br><br>**数据集**：<br>- 真实世界桌面操控数据集，包含5个任务：put broccoli on plate、put mushroom on plate、put sausage on plate、put shrimp on plate、put chips on plate<br>- 每个任务包含150个人类远程操控演示数据<br><br>**训练资源**：<br>- Diffusion Policy：使用4个NVIDIA A800 GPU训练120万步，27小时，批大小64，学习率0.0003<br>- RDT-1B：使用8个A800 GPU训练5万步，20小时，批大小64，学习率0.0001<br><br># 论文使用的评估环境和评估指标<br><br>**评估环境**：<br>- 硬件：AIRBOT 6-DOF机械臂配抓取器<br>- 相机设置：三个RGB相机分别位于顶部、前方和手腕安装位置<br>- 测试任务包括三个不同复杂度的桌面操控任务：获取含膳食纤维食物、获取含蛋白质和脂肪食物、烹饪一餐（需要5个步骤）<br><br>**评估指标**：<br>- 累积任务进度（Y轴）vs VLA步骤（X轴）<br>- 任务成功率<br>- 执行效率（完成任务所需的最少步骤）<br>- 性能对比：与vanilla VLA策略、传统层级框架、人机交互层级方法进行对比<br>- 平均5次独立试验的结果展示</details> |
| 2025-09-28 | Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models | http://arxiv.org/abs/2509.23655 | <details><summary>展开</summary># 论文研究单位<br>Centre for Artificial Intelligence, UCL 和 Qualcomm AI Research<br><br># 论文概述<br>针对视觉-语言-动作模型（VLA）训练计算成本高的问题，论文提出了Oat-VLA（Object-Agent-centric Tokenization for VLAs）方法。该方法通过对象中心和智能体中心的标记化策略，将视觉输入的标记数量从传统的256个大幅减少到16个，同时保持或提升模型性能。实验表明，Oat-VLA在LIBERO基准上的收敛速度比OpenVLA快2倍以上，并在真实世界的抓取放置任务中表现更优。<br><br># 论文核心贡献点<br>- 设计了对象中心标记化方法，将场景中对象信息压缩为少量语义标记<br>- 提出智能体中心标记化策略，获取机械臂末端执行器的精确视觉信息<br>- Oat-VLA具有模块化和可扩展性，可重用现有VLA检查点，便于适应<br>- 在LIBERO基准上实现超过2倍的训练收敛加速<br>- 在真实世界抓取放置任务中展现出比OpenVLA更高的成功率<br>- 视觉标记数量减少93.75%，显著降低GPU内存需求和计算成本<br><br># 论文方法描述<br>**对象中心标记化**：使用FT-Dinosaur无监督对象中心模型提取图像分割掩码，通过平均池化将同一对象区域的视觉标记压缩为单一标记，主实验使用7个对象标记。<br><br>**智能体中心标记化**：训练轻量级ResNet Faster R-CNN检测机械臂末端执行器位置，获取该位置周围3×3区域的9个视觉标记，确保精确操作。<br><br>**Oat-VLA架构**：结合7个对象标记和9个智能体标记（共16个视觉标记），通过MLP投影器输入到Llama 2 LLM主干网络中，保持与OpenVLA架构兼容性。<br><br># 论文使用数据集和训练资源<br>**数据集**：LIBERO基准测试套件（包含Spatial、Object、Goal、10四个任务套件）、Open X-Embodiment数据集子集（Bridge+FMB+Fractal）、自建真实世界数据集（320条轨迹）<br><br>**训练资源**：8×H100节点，全参数微调使用批量大小8×64=512，LoRA微调使用批量大小8×48=384，从OpenVLA预训练检查点开始微调<br><br># 论文使用的评估环境和评估指标<br>**评估环境**：LIBERO仿真环境（四个任务套件）、真实世界环境（UFACTORY xArm 6机械臂）<br><br>**评估指标**：<br>- LIBERO基准：成功率及标准差（每套件100次评估），与OpenVLA、Octo、Diffusion Policy对比<br>- 真实世界评估：分布内/外任务成功率及详细分解<br>- 训练效率：动作标记准确率vs训练时间，训练吞吐量（样本/秒）<br>- 消融实验：不同标记化策略的性能对比</details> |
| 2025-09-27 | Leave No Observation Behind: Real-time Correction for VLA Action Chunks | http://arxiv.org/abs/2509.23224 | <details><summary>展开</summary># 论文研究单位<br><br>东京大学（The University of Tokyo）<br><br># 论文概述<br><br>大型视觉-语言-动作（VLA）模型在机器人控制中面临推理延迟的挑战。尽管这些模型能够生成连贯的行为，但延迟导致机器人在动态环境中响应迟缓。为解决这一问题，研究团队提出了异步动作块纠正（A2C2）方法，通过轻量级纠正头在每个控制步骤实时修正动作。该方法能在不重新训练基础策略的情况下，结合最新观察信息和时空位置特征，对预测的动作块进行逐步调整，从而保持闭环响应性并提升在延迟和长视野条件下的成功率。<br><br># 论文核心贡献点<br><br>1. 首次正式定义了VLA模型生成动作块时的延迟问题，并分析了其对机器人实时控制的影响<br>2. 提出了A2C2（异步动作块纠正）方法，这是一个轻量级的附加动作纠正策略，可应用于任何VLA模型<br>3. 建立了基于时间位置特征的动作纠正机制，能有效处理异步执行中的观察滞后问题<br>4. 在Kinetix动态任务和LIBERO Spatial基准测试中，方法在存在延迟和长执行视野条件下均实现了显著的成功率提升<br><br># 论文方法描述<br><br>A2C2框架通过在基础策略之上添加轻量级纠正头πa2c2来扩展基于动作块的策略π。纠正头接收时间t+k时刻的观测ot+k、基础策略预测的动作a_t+k（来自之前t时刻的观察）、时间特征τ_k（位置在动作块内的相对位置）、基础策略的潜在表示zt+k，以及语言指令l。方法采用正弦嵌入表示位置特征，为动作块长度提供周期性结构。纠正头预测残差动作Δat+k，并与基础动作相加产生执行动作aexec_t+k=k。基础策略π每e步执行一次推理，而纠正头运行在每个控制步骤。训练过程中，使用均方误差（MSE）损失函数优化纠正头，通过对比专家演示的目标动作与基础策略输出之间的差异来学习残差预测。方法兼容现有的演示数据集，无需强化学习微调，并且可作为独立插件集成到任何VLA模型中。<br><br># 论文使用数据集和训练资源<br><br>1. **Kinetix动态任务套件**：包含12个高度动态的操控和运动任务，使用1百万步专家演示数据，由RPO训练的专家策略生成<br>2. **LIBERO Spatial基准**：提供432个演示和52,970帧数据，涵盖10个空间操控任务，包含顶部和手腕RGB图像、状态信息和语言指令<br>3. **训练设置**：Kinetix环境使用flow-matching策略作为基础，纠正头为3层MLP（0.31M参数）；LIBERO使用SmolVLA作为基础（450M参数），纠正头结合6层transformer编码器和MLP（32M参数）<br><br># 论文使用的评估环境和评估指标<br><br>1. **评估环境**：<br> - Kinetix仿真环境（12个动态任务）：包括汽车发射、倒立推力投石、捕捉游戏、链式着陆器、抓取、半人马独轮车、硬着陆器、半豹式游泳者、豹式行走和蹦床等场景<br> - LIBERO Spatial真实机器人基准：包含10个需要精细空间推理的操控任务<br><br>2. **评估指标**：<br> - **成功率**：通过多次试验计算任务成功完成的百分比，在不同延迟d和执行视野e的组合下进行评估<br> - **延迟敏感性**：分析推理延迟从0到4步变化时的性能下降<br> - **视野稳定性**：研究执行视野从1到50步对性能的影响<br> - **性能对比**：与Naive async和RTC（Real Time Chunking）基线方法进行对比，评估A2C2的改进效果</details> |
| 2025-09-27 | Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges | http://arxiv.org/abs/2509.23121 | <details><summary>展开</summary>## 论文研究单位<br><br>辽宁辽河实验室（项目编号：LLL24ZZ-02-01 和 LLL24ZZ-02-02）<br><br>## 论文概述<br><br>该论文评估了视觉语言动作（VLA）模型在工业环境中的适用性。研究从工业部署的角度比较了现有SOTA VLA模型在工业场景中的性能，并从数据收集和模型架构两个角度分析了VLA模型在现实工业部署中的局限性。研究发现，当前VLA模型在复杂工业环境、多样化物体类别和高精度放置任务方面仍有很大改进空间。<br><br>## 论文核心贡献点<br><br>- 评估了最先进的VLA模型在工业场景中的拾取和放置任务性能<br>- 从数据集和模型架构两个角度分析VLA模型对非结构化环境的适应性<br>- 讨论了提高VLA模型鲁棒性和任务泛化能力的潜在方向<br>- 提供了VLA模型在工业应用中的实用见解<br><br>## 论文方法描述<br><br>提出了VLA的通用技术框架，包含以下关键技术模块：<br>- 归一化和反归一化：消除不同任务或机器人平台间的维度不一致性<br>- 数据增强：包括图像增强（随机裁剪、颜色抖动、图像损坏等）和指令增强（提示变化、释义等）<br>- 投影器：连接视觉编码器和LLM的桥梁，进行维度对齐和语义映射<br>- 预训练VLM：包括视觉编码器（如DINOv2、SigLIP）和语言编码器（如LLaMA、Gemma、Qwen2-VL）<br>- 政策头：三种主要架构类型包括自回归、扩散和混合模型<br><br>## 论文使用数据集和训练资源<br><br>- 使用来自真实工业场景采集的回合数据进行模型微调，每个任务使用100个回合<br>- 在NVIDIA H20 96GB GPU上训练10小时<br>- 使用Ubuntu 20.04和ROS 1 noetic系统<br>- 在Mobile ALOHA双机械臂机器人上进行所有测试<br><br>## 论文使用的评估环境和评估指标<br><br>- 评估环境：三个工业场景，涵盖视觉遮挡、相机抖动、目标姿态随机化和目标多样性等扰动<br>- 主要指标：成功率（%），计算方法为成功试验次数除以总试验次数<br>- 精度指标：位置误差（2.2 cm）和方向误差（12.4°）<br>- 实验设置：每个试验包含10次测试，涵盖目标姿态随机化和多样化物体类别<br>- 评估结果：Pi0模型经过微调后，在简单抓取任务上达到约60%的成功率，但在高精度放置任务中精度仍有不足</details> |
| 2025-09-26 | VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search | http://arxiv.org/abs/2509.22643 | <details><summary>展开</summary># 论文研究单位<br>南洋理工大学（NTU）、清华大学深圳国际研究生院（Tsinghua SIGS）、清华大学、北京邮电大学（BUP）<br><br># 论文概述<br>现有视觉-语言-动作模型（VLA）在部署中依赖短期状态到动作的映射，易产生逐步偏差，难以完成长时序任务。本文提出VLA-Reasoner，一个可在测试时扩展的即插即用框架，通过在线蒙特卡洛树搜索（MCTS）在世界模型中前瞻未来状态并搜索最优动作，从而缓解VLA的短视问题并显著提升成功率和鲁棒性。<br><br># 论文核心贡献点<br>提出可插拔的测试时推理框架，向任意VLA注入树搜索与未来前瞻能力，无需重新训练底层策略；提出KDE先验的高效候选采样与基于图像的离线奖励塑形，用以在MCTS中实现高效扩展与密集反馈；通过线性融合（式1）将MCTS输出的推理动作与VLA原动作结合，完成即时动作优化与长时指导。<br><br># 论文方法描述<br>问题形式化：给定观测o_t与语言指令l，VLA生成a_t^VLA；VLA-Reasoner在世界模型W中模拟未来轨迹，并用MCTS在动作空间搜索最优a_t^Reasoner，最终执行a_t=α·a_t^VLA+(1−α)·a_t^Reasoner，α为注入强度。<br><br>在线MCTS：围绕VLA预测作为根节点展开。四步循环为<br>- 扩展（Expansion）：在世界模型中进行动作采样与扩展；扩展后的状态通过世界模型W(a_i,o_i)→o_{i+1}得到下一状态。<br>- 模拟（Simulation）：利用动作感知的世界模型生成未来状态以评估当前动作的影响。<br>- 回传（Backpropagation）：自叶向根更新节点值Q与访问次数N，用到KDE概率密度近似N(a)以降低开销。<br>- 选择（Selection）：采用UCB（Upper Confidence Bound）在价值与访问次数间平衡选点。<br><br>KDE高效采样：用离线机器人演示训练KDE核密度估计得到动作分布π^KDE，从该分布采样候选并在Top-k近邻内扩展，避免重复调用VLA并维持探索多样性；支持对动作块（chunk）进行整体采样。<br><br>视觉奖励塑形：为获得密集且稳健的中间反馈，用ResNet-34图像编码器+两层MLP（MSE）在降采样后的离线轨迹上学习从状态到奖励的映射r=MLP(o)，以奖励为导向修正MCTS中的轨迹选择。<br><br>算法伪代码与实现：完整给出从根节点初始化、循环扩展/模拟/回传/选择、选出最优子节点并计算a_t^Reasoner、以及最终动作注入的流程。除世界模型外各组件基于与VLA微调相同的数据进行训练；额外收集少量失败演示用于世界模型微调以提升失败情形下的预测能力。<br><br># 论文使用数据集和训练资源<br>- 仿真基准：LIBERO（Spatial/Goal/Object/Long四套任务，共约500条演示/套）和SimplerEnv（Block/Spoon/Carrot/Eggplant四任务）；包含公共机器人演示数据集与仿真器生成轨迹。<br>- 基础策略：OpenVLA-7B、Octo-Small（27M）、SpatialVLA（4B）等。<br>- 世界模型：iVideoGPT架构的动作感知世界模型（600M参数）。<br>- 训练资源：服务器6×NVIDIA RTX 6000 GPU进行训练；真实世界推理使用NVIDIA RTX 4090。<br>- KDE与奖励网络：在与VLA微调相同的离线数据集上训练；KDE用于动作先验采样，奖励网络以ResNet-34+MSE学习图像至奖励的映射；为覆盖失败情形另行收集少量失败演示用于世界模型微调。<br><br># 论文使用的评估环境和评估指标<br>- 仿真评估：在LIBERO与SimplerEnv上进行定量评估，指标为平均成功率（每套任务各500/100回合）。<br>- 真实世界评估：在Galaxea-A1机械臂上完成5项任务（Block、Fruit、1 Cup、2 Cups、Circle），侧视与腕部摄像头提供视觉输入（OpenVLA不使用腕部图像），每项任务进行20回合评估。<br>- 消融实验：考察注入强度α（0.6最优）以及关键组件（KDE采样vs高斯噪声；图像奖励塑形vs token式奖励头）对成功率的影响。<br>- 核心结论：VLA-Reasoner在仿真与真实世界均显著提升成功率与鲁棒性，优于多种强基线VLA，表明测试时树搜索与未来预测能有效缓解短期偏差并增强长时任务执行能力。</details> |
| 2025-09-26 | UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation | http://arxiv.org/abs/2509.22441 | <details><summary>展开</summary>### 论文研究单位<br>西湖大学工程学院（主单位），浙江大学信息与电子工程学院、环境与资源学院，澳大利亚国立大学。<br><br>### 论文概述<br>提出首个针对自主水下航行器（AUV）的视觉-语言-动作（VLA）框架 UnderwaterVLA，旨在解决海洋环境的极端挑战（水动力学干扰、感知退化、通信受限）。该框架首次将VLA模型应用于水下机器人，通过双脑架构解耦高层推理与低层控制，实现零数据训练和水动力实时补偿，显著提升导航精度与任务成功率。<br><br>### 论文核心贡献点<br>1. **首个水下VLA架构**：系统性解决VLA在水下应用的层级、数据需求和动力学补偿三大核心挑战。<br>2. **零数据训练策略**：通过预训练多模态模型的迁移学习与物理先验融合，规避昂贵的水下演示数据。<br>3. **双脑解耦控制**：云脑负责长时域任务规划，小脑实现实时感知-控制闭环，在通信/算力受限下保障鲁棒性。<br>4. **水动力实时补偿**：在MPC中嵌入流体动力学模型，通过在线参数估计自适应复杂水流环境。<br><br>### 论文方法描述<br>1. **双脑架构**：<br> - **云脑**：Surfacing时运行QVQmax模型，将高层指令分解为序列化子任务（如“右转躲避障碍→直行→低速接近”）。<br> - **小脑**：本地Qwen-VL模型循环执行JSON格式控制指令，动态调整决策（如检测安全距离后自主终止任务）。<br> - 结构示例：`云脑计划 → 子任务序列 → 本地执行（感知-动作循环）`。<br><br>2. **提示工程与可解释性**：<br> - 强制云脑/小脑输出思维链（CoT）推理过程，记录决策逻辑。<br> - 小脑输出标准化JSON结构：<br> ```json<br> {<br> "reasoning": "检测到左侧障碍，右转",<br> "decision": "right",<br> "velocity": "medium",<br> "sub_task_done": false<br> }<br> ```<br><br>3. **零数据MPC水动力控制**：<br> - **运动剖面**：平移/旋转动作严格分为1秒周期的加速-恒速-减速阶段（速度：0.2/0.5/0.8 m/s，角速度：0.5/1.0/1.5 rad/s）。<br> - **MPC优化**：最小化代价函数同时追踪参考信号、控制力和流体阻力：<br> ```math<br> J = \int_0^1 \left[ \beta\\|v(t)-v_{\text{ref}}\\|^2 + \gamma\\|\tau\\|^2 + \delta\\|F_{\text{drag}}\\|^2 \right] dt<br> ```<br> - **流体参数在线估计**：基于IMU数据实时估算拖拽系数：<br> ```math<br> \hat{D}_v = \frac{\tau_v - M\dot{v}}{v\|v\|}<br> ```<br><br>### 论文使用数据集和训练资源<br>- **训练数据需求**：**零**（完全依赖预训练模型 + 物理先验，无需水下演示数据）<br>- **基线数据对比**：QUAR-VLA等基线需262K演示数据（来源自其他机器人任务）。<br>- **模型资源**：<br> - 云脑：QVQmax（大语言模型）<br> - 小脑：Qwen 2.5-VL-7B（多模态视觉-语言模型）<br><br>### 论文使用的评估环境和评估指标<br>1. **实验环境**：<br> - **实验室测试**：控制变量水池，3垂直柱状障碍，用于导航与避障任务。<br> - **真实环境模拟**：降低光照强度至海平面5%，注入硅藻土提升浊度至18 NTU（模拟近岸浑浊水域）。<br><br>2. **评估指标**：<br> - **量化性能**：<br> - 任务成功率：对比QUAR-VLA基线，感知/导航/隧道穿越/避障任务提升19%-27%。<br> - 零数据效率：基线需262K数据，UnderwaterVLA无需任何水下演示数据。<br> - **鲁棒性评估**：<br> - 双脑架构（DBM）vs 单脑模型（SBM）：在浑浊环境中，DBM能自主终止任务避免过冲，SBM因目标丢失持续前进导致任务失败。<br> - 可视化证据：轨迹对比图显示DBM在低能见度下精确接近目标，SBM偏离路径。<br><br>**结论**：通过解耦控制层级、融合物理先验和可解释推理链，UnderwaterVLA在零数据条件下实现水下机器人任务性能突破，为复杂海洋环境中的自主作业提供了可行路径。</details> |
| 2025-09-26 | EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer | http://arxiv.org/abs/2509.22407 | <details><summary>展开</summary>### 论文研究单位<br>- 所属机构：GigaAI、Peking University、Tsinghua University、CASIA<br><br>### 论文概述<br>- 目标：解决VLA（视觉–语言–动作）模型因真实机器人演示数据昂贵、难以扩展而导致的泛化能力不足问题<br>- 核心方案：提出EMMA框架，结合生成式数据引擎与自适应训练策略，通过文本控制的视频生成扩充数据，提升策略在新外观与新环境的零样本泛化<br><br>### 论文核心贡献点<br>- 提出EMMA框架，集成DreamTransfer与AdaMix，显著提升真实世界机器人操作的泛化<br>- DreamTransfer：DiT（扩散Transformer）双分支架构，融合深度与文本，实现多视角一致、几何可控的机器人操作视频生成<br>- AdaMix：基于轨迹性能的自适应重采样，动态提高“困难样本”的训练权重，增强策略稳健性与泛化<br>- 与SOTA相比，DreamTransfer在多视角一致性提升42%、深度一致性提升24%；真实世界零样本视觉任务相较仅用真实数据提升超过200%，AdaMix额外提升13%<br><br>### 论文方法描述<br>- DreamTransfer总体结构：主分支用于去噪潜空间视频令牌，ControlNet分支注入深度结构约束；多视角视频深度沿宽度拼接作为统一潜表示；T5文本编码器提供语义嵌入，并通过交叉注意力融合至主分支<br>- 文本控制与几何一致性：用户可通过自然语言编辑前景、背景与光照，同时保持3D结构与几何合理性；多视角与深度约束共同维持跨视角与时间的一致性<br>- AdaMix训练策略：初期使用高质量视频（低质量样本权重为零），稳定训练；在收敛后根据三类指标动态调整采样权重，聚焦“困难样本”<br> - 指标：动作预测误差MSE（负号）、轨迹平滑度（角加速度二阶差分负号）、关节限位（越界为0，否则为1）<br> - 组合得分：归一化后取平均<br> - 更新权重：p(i) ∝ γ + λ·(1 − s_i)，保证最小支持并强调困难样本<br>- 数据混合与筛选：真实与生成数据按比例混合训练；生成视频以多视角一致性、深度一致性与文本–视频相似度进行过滤与评分<br>- 训练细节：两阶段微调（低分辨率稳定多视角一致性 → 高分辨率细节恢复）；优化器AdamW；分任务（折叠布料、清洁桌面、投掷瓶子）不同步数配置<br><br>### 论文使用数据集和训练资源<br>- 真实演示：折叠布料（50条）、清洁桌面（20条）、投掷瓶子（20条）<br>- 生成数据：Agibot World与NVIDIA Isaac Sim采集；基于Agibot World构建5万条多视角视频（含对齐RGB、时间一致深度与模板化文本描述）<br>- 生成流程：多视角深度一致性由Video Depth Anything估计；前景/背景/光照描述由Qwen2.5-VL-7B生成<br>- 仿真演示：NVIDIA Isaac Sim采集，DreamTransfer执行视间一致的风格与外观转移<br>- 训练硬件与平台：AgileX CobotMagic，双PiPER臂，三台Intel RealSense D435i摄像头；Isaac Sim用于仿真<br>- 两阶段微调配置：阶段一576×128分辨率、批量32、3500步；阶段二1920×480分辨率、批量4、4500步<br><br>### 论文使用的评估环境和评估指标<br>- 任务：折叠布料（真实到真实）、清洁桌面/投掷瓶子（仿真到真实）<br>- 视频生成质量评估<br> - 多视角一致性：匹配像素数（Pix.Mat.）<br> - 深度一致性：RMSE、绝对相对误差（Abs.Rel.）、平方相对误差（Sq.Rel.）<br> - 文本–视频对齐：CLIP相似度（CLIPSim.）<br>- 真实世界机器人评估<br> - 行为得分：最大5分，按失败场景扣分（任务不同规则不同）<br> - 成功率：每任务在5次试验与4种外观变化下评估，共20次运行<br> - 执行质量：执行时间、轨迹平滑度（角加速度）、关节越界帧数<br>- 数据混合实验：固定总量与步数，考察真实/生成比例对成功率的影响（50%为峰值）<br>- 消融实验：固定混合比例（FixMix）对比AdaMix，显示AdaMix在行为得分、成功率与执行质量上的综合提升</details> |
| 2025-09-26 | MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training | http://arxiv.org/abs/2509.22199 | <details><summary>展开</summary># 论文研究单位<br>GigaAI; 中科院自动化研究所 (CASIA); 南京理工大学 (NJUST); 清华大学<br><br># 论文概述<br>MimicDreamer提出一套将低成本的人类演示视频转换为可用的机器人监督数据的统一框架，针对视觉、外参视点和动作三个维度的域差异进行联合对齐，使视觉语言动作（VLA）模型能在仅使用人类演示合成数据的情况下完成少样本机器人执行，并随人类数据规模扩大而显著提升成功率。核心包括H2R Aligner（视频扩散模型的视觉对齐）、EgoStabilizer（单目视点稳定与修复）和统一的H2R动作空间与约束逆动力学求解器。<br><br># 论文核心贡献点<br>- 提出统一的人机演示对齐框架，同时在视觉、视点与动作三个维度缩小差距，将人类视频转化为可执行的机器人监督用于可扩展VLA训练。<br>- 视觉：基于视频扩散和几何先验的H2R Aligner，将人类演示动作迁移为高保真机器人臂视频；视点：EgoStabilizer通过单应变换与背景修补稳定外参视点；动作：将人手轨迹映射到机器人坐标，并以约束逆动力学生成低抖动、可行的关节指令。<br>- 在六项代表性操控任务上，仅用合成数据即可实现少样本执行；随人类数据扩大，显著提升平均成功率（14.7%），优于仅用真实机器人数据的基线。<br><br># 论文方法描述<br>- Viewpoint Stabilization（EgoStabilizer）<br> - Warp Perspective：相邻帧或参考帧估计单应Ht，用RANSAC剔除异常；时间平滑得到Ht~并构造补偿Wt，对每帧进行补偿与对齐；计算跨帧最大公共可见区域并裁剪以消除黑边与视场抖动。<br> - Video Inpainting：对补偿后产生的空洞与遮挡区域构造掩码Mt，采用视频修补模型进行时空传播与跨帧一致性修复，得到稳定序列。<br>- Actions Alignment（H2R动作空间）<br> - Human-side Normalization：将人手三维关键点变换至身体坐标系FB，并通过刚体配准到机器人基座坐标系FR，得到目标位置p*与姿态R*。<br> - Orientation Treatment：因手腕接近球副而末端执行器常以工具轴滚动，只对齐俯仰/偏航（roll软遮蔽）：使用对数映射与加权矩阵WR降低roll权重。<br> - IK Resolver：对每条臂a∈{L,R}，以末端位置误差、姿态加权误差与时间平滑项的二次目标，结合关节限位进行约束优化，用阻尼最小二乘（DLS）逐步求解；夹爪命令由人手开合经轻量VGG分类器与中值滤波推断。<br>- Visual Alignment（H2R Aligner）<br> - 训练：以真实机器人视频Vgt、环境背景Vscene与仿真前景Vsim为三路条件，经共享的冻结VAE编码为潜变量；在潜空间对目标做噪声扰动，通道拼接后输入H2R DiT进行去噪与条件融合，优化CogVideoXLoss。<br> - 推理：Vsimik由IK动作在仿真重放得到；Vsceneik通过对稳定后的人类视频进行人手分割与轻微膨胀得到；目标从噪声起始，经H2R DiT去噪与VAE解码，生成合成机器人域视频；将合成视频与对应IK动作对齐形成可训练数据。<br>- VLA训练<br> - 初始化自π₀预训练模型，复用VLM与动作词表；在合成数据上进行后训练，并混入少量真实演示以增强真实可执行性。采用条件流匹配（CFM）目标训练动作token，输出意图级控制并投影为关节指令。训练细节与超参数见附录。<br><br># 论文使用数据集和训练资源<br>- 数据集：EgoDex（829小时、1080p单目人演示，涵盖194任务，配有3D上身姿态）。H2R Aligner训练了24类操控，共3735样本（每样本64帧@30fps），随机裁剪至640×360并缩放至672×384，按9:1划分训练/验证。<br>- 训练资源：冻结的视频VAE与文本编码器；H2R DiT可训练。π₀预训练模型作为初始化；采用AdamW优化。<br>- 合成数据规模：实验逐步添加人转机数据（5→30个轨迹），在20真人+20合成的配比下达到最佳综合效果。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：六个操控任务——Pick Bag、Clean Surface、Stack Bowls、Dry Hands、Insert Tennis、Stack Cups。任务由真实机器人执行，模仿人类演示情境。<br>- 评估指标：成功率（SR）与进度成功率（PSR）。前者衡量整体任务完成度，后者衡量各子任务完成的平均比例。<br>- 结果要点：<br> - Few-shot：仅20真人 vs 20真人+20合成 vs 20真人+3合成三种设置，后者在平均SR/PSR上分别达85.0%/91.0%，较仅真人基线提升显著，各任务均有+10%~25% SR、+10%~32% PSR提升，并在Clean Surface与Dry Hands上达100%。<br> - Scaling：随人类数据增加，所有任务的SR、PSR单调上升，呈现快速上升后趋稳的规模化趋势；50–50配比下相对基线提升约10%量级，难度高的任务增幅更大。<br> - H2R Aligner：定性结果显示合成视频具备逼真的机械臂外观、一致的接触几何与背景保持。<br> - EgoStabilizer：稳定性（Jitter RMS、Stability）与几何一致性（H-RMSE）均显著提升；平均稳定性下降21.9%，Jitter RMS下降13.1%，几何误差仅小幅下降3.3%。</details> |
| 2025-09-26 | Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting | http://arxiv.org/abs/2509.22195 | <details><summary>展开</summary>## 论文总结<br><br>### 论文研究单位<br>普林斯顿大学（Department of Mechanical and Aerospace Engineering, Department of Computer Science）<br><br>### 论文概述<br>本文针对将视觉语言模型（VLM）微调为视觉语言行动模型（VLA）时存在的核心挑战——灾难性遗忘进行了深入研究。传统方法在获得机器人操作能力的同时会严重削弱VLM的基础推理和多模态理解能力。本文提出了一种数据驱动的解决方案——**VLM2VLA**，其核心思想是将机器人动作转化为自然语言描述（Actions as Language），从而最小化微调数据与预训练数据之间的分布差异。该方法使得仅通过低秩适配（LoRA）即可实现有效微调，无需架构改造或昂贵的共同训练，并显著保持了原VLM的核心能力。<br><br>### 论文核心贡献点<br>1. **动作语言化表示**：提出将低级机器人动作转换为自然语言描述，直接利用VLM预训练的知识空间，避免分布偏移。<br>2. **数据标注与训练管道**：提供了一套可扩展的机器人演示数据重标注流程，将轨迹分解为子任务、运动规划和动作块三层结构。<br>3. **LoRA微调验证**：证明在动作语言化的前提下，LoRA可以有效进行VLA训练并维持VLM知识。<br>4. **性能验证**：通过大量VQA实验和800+真实机器人实验，证明了方法在保持推理能力和零样本泛化方面的显著优势。<br><br>### 论文方法描述<br>1. **三层行动表示 (Actions as Language)**：<br> - **高层子任务预测**：根据视觉观察和指令预测下一步子任务。<br> - **中层运动规划**：基于当前子任务和观察，生成动作的空间方向描述（例如“向左并稍向下”）。<br> - **低层动作生成**：生成最终执行的文本化动作指令序列（例如“向前移动4.2厘米”），包含各个自由度的具体指令。<br>2. **数据重构**：<br> - 使用Gemini模型自动标注机器人轨迹（BridgeData v2），将原始状态-动作序列转换为带有语言描述的序列：`\{(图像, 子任务, 运动计划, 文本化动作块)\}`。<br>3. **训练与推理**：<br> - **训练**：在重构后的语言化数据上，使用LoRA对Gemma-3-12B-IT等VLM进行微调。<br> - **推理**：采用闭环方式，模型首先生成完整的子任务序列；在执行每个动作块后，调用一个验证器（使用Gemini 2.5 Pro）判断任务完成度，决定继续下一子任务或重试当前任务。<br><br>### 论文使用数据集和训练资源<br>1. **数据集**：<br> - **机器人数据**：BridgeData v2 数据集的一个子集（包含主要任务指令）。<br> - **多模态数据**：用于评估VQA能力（未明确说明具体数据集，可能基于其预训练知识）。<br>2. **训练资源**：<br> - **基础模型**：Gemma-3-12B-IT (主要), Gemma-3-4B-IT (用于对比)。<br> - **标注工具**：Gemini 2.5 Pro 和 Gemini 2.5 Flash（用于成本控制）用于自动生成语言化轨迹标签。<br> - **微调方法**：LoRA（应用于所有线性模块）。<br> - **标注成本**：约$900。<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**：<br> - **机器人平台**：WidowX 250S 6自由度机械臂（用于真实世界操作）。<br> - **物理场景**：标准玩具厨房环境。<br>2. **评估指标**：<br> - **多模态理解能力**：<br> - **指标**：多个VQA基准测试的准确率（如MMMU, MMStar, MME, OCRBench, MMB-en, MMB-cn, TextVQA, DocVQA, InfoVQA, AI2D, ChartQA, RealWorldQA）。<br> - **机器人操作能力**：<br> - **指标**：任务成功率（每任务30次试验，多语言任务为90次）。<br> - **任务类型**：<br> - **域内任务 (ID)**：拾取、拾取放置。<br> - **组合任务**：多步骤操作。<br> - **域外任务 (OOD)**：多语言指令翻译（西班牙语、普通话、印地语）；识别流行文化概念（识别“Ash Ketchum”图像上方的物体）。</details> |
| 2025-09-26 | Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation | http://arxiv.org/abs/2509.22093 | <details><summary>展开</summary># 论文研究单位<br>悉尼大学计算机学院<br><br># 论文概述<br>本文针对视觉-语言-动作（VLA）模型在机器人操作中的计算效率问题进行研究。VLA模型在处理长视野多模态上下文时，对密集视觉标记的注意力计算消耗了主要计算资源。现有方法通过减少VLA模型内的视觉冗余来优化推理速度，但忽略了机器人操作不同阶段的冗余变化。研究发现视觉标记冗余度在粗糙操作阶段高于精细操作阶段，且与动作动态密切相关。基于此观察，本文提出Action-aware Dynamic Pruning (ADP)方法，这是一个结合文本驱动标记选择与动作感知轨迹门控的多模态修剪框架。<br><br># 论文核心贡献点<br>1. 发现VLA模型中视觉标记重要性在机器人操作不同阶段存在变化的规律，这启发设计了针对操作阶段的动态修剪方法<br>2. 提出文本驱动的动作感知修剪方法，结合任务指令相关性和基于末端执行器运动的门控规则，实现修剪和全视觉状态之间的自适应切换<br>3. 进行了原理性复杂性分析和广泛的仿真与真实世界实验，证明该方法在减少FLOPs和延迟的同时保持成功操作所需的精细视觉细节<br><br># 论文方法描述<br>ADP方法包含两个核心组件：<br>1. 文本驱动预期修剪：通过跨模态相似性评估视觉补丁相关性，在进入深层融合前仅选择最相关标记。使用文本和视觉标记的查询-键表示计算相似度矩阵，通过多头部平均获得全局重要性分数，保留排名前k的视觉标记<br>2. 动作感知动态策略：使用基于末端执行器轨迹的轻量级决策信号调制修剪决策。将每个解码的动作块视为时间窗口，计算窗口化轨迹距离（欧几里得位移），通过比较当前运动幅度与历史运动统计来动态决定是否启用修剪。当运动幅度相对较高时（粗糙阶段）启用修剪；运动幅度相对较低时（精细阶段）禁用修剪以保留完整视觉<br><br># 论文使用数据集和训练资源<br>数据集：<br>- LIBERO仿真套件（包含Spatial、Object、Goal、Long四个任务集）<br>- 真实世界任务：4个涵盖抓取、放置和擦拭动作的任务<br><br>训练资源：<br>- 使用NVIDIA RTX 4090 GPU<br>- 基于OpenVLA-OFT（7B参数）模型进行实验<br>- Linux工作站环境<br><br># 论文使用的评估环境和评估指标<br>评估环境：<br>- 仿真环境：LIBERO仿真套件<br>- 真实环境：Jaco2物理机器人平台<br>- 硬件：配备NVIDIA RTX 4090的Linux工作站<br><br>评估指标：<br>- 成功率（SR, Success Rate）：在各类任务上的平均成功率<br>- 计算量（FLOPs）：浮点运算次数<br>- 延迟（Latency）：动作推理时间<br>- 加速比（Speedup）：相对于基线方法的推理速度提升倍数</details> |
| 2025-09-26 | Developing Vision-Language-Action Model from Egocentric Videos | http://arxiv.org/abs/2509.21986 | <details><summary>展开</summary>## 论文研究单位<br>京都大学、国立情报学研究所（NII）、东京科学研究所、NII LLMC（东京）、索尼互动娱乐（东京）<br>作者：Tomoya Yoshida（京都大学）、Shuhei Kurita（NII、东京科学研究所）、Taichi Nishimura（索尼互动娱乐）、Shinsuke Mori（京都大学）<br><br>## 论文概述<br>论文探讨了利用自我中心视频（egocentric videos）训练视觉-语言-动作模型（Vision-Language-Action models, VLAs）的可行性。传统方法依赖昂贵的人类远程操作数据，导致数据稀缺。论文提出通过EgoScaler框架从自我中心视频中自动提取6DoF对象操作轨迹，构建大规模预训练数据集，并在π0架构上验证其有效性。实验在仿真和真实环境中进行，结果显示该方法可显著提升任务成功率。<br><br>## 论文核心贡献点<br>1. 成功从自我中心视频训练π0模型，无需辅助标签（如手部姿态）。<br>2. 构建的预训练数据集在性能上与真实机器人数据集竞争。<br>3. 结合自我中心数据和真实机器人数据可进一步提升性能。<br><br>## 论文方法描述<br>- **EgoScaler框架**：用于从自我中心视频中提取6DoF对象操作轨迹。<br> - 步骤：识别操作起止时间及对象；开词汇分割和3D点跟踪提取位置序列；点云配准投影到相机坐标系；奇异值分解计算旋转序列。<br>- **数据处理**：应用于Ego4D、Ego-Exo4D、HD-EPIC和Nymeria四个数据集。初始提取124,559 episodes，经规则过滤（移动距离阈值、背景轨迹相似性阈值）和平滑处理后，得到45,157 episodes。<br>- **训练策略**：使用π0架构预训练和后训练。动作表示为6DoF位移（平移+旋转），预训练优化MSE损失。后训练时合并真实机器人数据集并归一化动作维度。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：<br> - 自我中心视频数据集：Ego4D、Ego-Exo4D、HD-EPIC、Nymeria（用于预训练）。<br> - 真实机器人数据集：BC-Z、BridgeData V2、Fractal（用于对比）。<br>- **训练资源**：<br> - 硬件：8×H200 GPUs。<br> - 优化器：AdamW（学习率5×10^-5）。<br> - 预训练步数：20,000步；后训练步数：40,000步。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真环境：SIMPLER（基于BridgeData V2的pick-and-place任务）。<br> - 真实环境：ALOHA（语言引导的pick-and-place任务）。<br>- **评估指标**：<br> - 成功率：成功次数/总次数。仿真环境200次rollout；真实环境10次rollout。<br> - 真实环境评分：抓取正确对象得0.5分，正确放置得0.5分。</details> |
| 2025-09-25 | RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models | http://arxiv.org/abs/2509.21243 | <details><summary>展开</summary>## 论文研究单位<br>- 加川大学（School of Computing, Gachon University）<br><br>## 论文概述<br>- 针对VLA模型效率与性能的权衡难题，RetoVLA提出“复用Register Tokens”这一被ViT用于去伪影的中间信息，作为“空间上下文”直接注入Action Expert，在维持轻量结构的同时增强空间推理能力。<br>- 基于LIBERO基准、自建仿真环境以及自建7-DOF机械臂进行评估，实机与仿真均显示在长时序与三维空间理解任务上显著提升。<br><br>## 论文核心贡献点<br>- 重新定义Register Tokens：从“去伪影的净化器”转为“空间上下文提供者”，设计可学习门控将其KV注入Action Expert的最后一层交叉注意力，实现语义与全局空间双流融合。<br>- 证明复用信息可弥补轻量模型（如SmolVLA）深度压缩导致的全局理解能力损失，是替代“信息缩减”的新范式。<br>- 三类实验验证：标准化仿真（LIBERO）、自建物理仿真（Unity+MuJoCo）、真实7-DOF机械臂上取得大幅提升（尤其长时序与3D空间任务）。<br><br>## 论文方法描述<br>- 架构<br> - 采用浅层VLM主干（SmolVLM2-500M的前16层）以保证效率；维持单流语义特征与双流信息融合路径。<br>- 空间上下文注入（核心）<br> - Register Token生成：用可学习初始向量与图像patch做注意力，生成场景依赖的Register Tokens。<br> - 注入与融合：将Register Tokens投影为KV并与VLM的KV在Action Expert最后一层交叉注意力处拼接。<br> - 门控：引入可学习标量经sigmoid调制注入强度，避免对极精细局部控制造成干扰。<br>- 训练目标<br> - 条件流匹配（CFM）：学习将噪声动作序列朝真实动作序列演化的向量场，MSE损失优化。<br><br>## 论文使用数据集和训练资源<br>- 数据与任务<br> - LIBERO四类基准：Spatial、Object、Goal、10(Long)。<br> - 真实环境：自建7-DOF机械臂，7项操控任务，收集1,804条episode进行微调。<br> - 自建仿真：Unity + MuJoCo插件，对应部分真实任务。<br>- 训练资源（报告为主干与步骤细节）<br> - 主干：SmolVLM2-500M（使用前16层）。<br> - 训练步数：100k；批量：64。<br> - 注入寄存器：2个Register Tokens（消融最优）。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境<br> - 仿真：LIBERO基准与自定义Unity+MuJoCo仿真。<br> - 真实：7-DOF机械臂，多视角（顶视/侧视/腕部）。<br>- 评估指标<br> - 主要指标：成功率（Success Rate, SR）、平均成功率（MSR）。<br> - 性能对比：SmolVLA vs RetoVLA。<br>- 关键结果摘要<br> - 真实机器人MSR：50.28% → 67.42%，绝对提升17.14%p。<br> - 仿真MSR：62.8% → 74.8%，提升12.0%p。<br> - 长时序与复杂空间任务提升显著（如Build Domino Line、Close Drawer、Jenga等）；对极精细局部操作有轻微权衡。<br> - LIBERO各类总体提升有限，但针对工作记忆与三维空间理解的任务明显受益。</details> |
| 2025-09-25 | Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning | http://arxiv.org/abs/2509.21126 | <details><summary>展开</summary>## 论文研究单位<br>武汉大学，中国。<br><br>## 论文概述<br>本文提出VARL框架，利用视觉语言模型(VLM)作为动作顾问，在在线强化学习(RL)中提供动作建议，旨在改进样本效率，特别是在稀疏奖励任务中。方法通过集成VLM建议的动作，而非修改奖励函数，丰富样本多样性并确保策略最优性和收敛性。论文在多种环境和代理设置中验证了框架的有效性。<br><br>## 论文核心贡献点<br>- 提出VLM作为动作顾问的框架，而非奖励设计师，保证策略最优性。<br>- 设计门控机制（离散和连续动作空间）防止策略过度拟合VLM动作。<br>- 通过策略塑形在早期训练阶段提供指导，提高样本效率。<br>- 低计算开销：相比奖励塑形方法，显著减少VLM查询次数（如VARL仅3次查询对比奖励塑形方法的5000次）。<br>- 适用于状态和视觉基础任务、离散和连续动作空间。<br><br>## 论文方法描述<br>VARL框架包含两个组件：<br>- VLM动作生成器：定期采样最新状态-动作对，查询VLM生成启发式动作并存储至引导缓冲区。<br>- 策略塑形模块：将VLM动作集成至策略训练，通过行为克隆损失和门控函数塑形策略，在指定步数后移除启发式动作。<br>具体算法基于软演员-评论家(SAC)，使用GPT-5作为VLM，损失函数包括基线策略损失、行为克隆损失和门控函数。<br><br>## 论文使用数据集和训练资源<br>- 环境：Meta-World（操控）、AI2-THOR（导航）、真实世界（Realman Robotics RM-65B臂）。<br>- 任务：10个任务，包括状态基础（Drawer Open、Sweep Into、Soccer）、视觉基础（Pick Up Plate、Open Phone、Toggle Off Lamp、Drawer Open、Push Cube）、真实世界视觉基础（Target Reach、Push Cube）。<br>- 训练资源：最大步数500,000，参数λ=10、移除步数N_s=30,000，使用SAC求解器和GPT-5 VLM。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：10个任务，分为三类（状态基础操控、视觉基础操控和导航、真实世界视觉基础操控）。<br>- 评估指标：成功率、学习曲线（平均回报或成功率），与基线方法（SAC、SAC+专家数据、RL-VLM-F、ERL-VLM）比较，包括样本效率分析和消融研究。</details> |
| 2025-09-25 | AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation | http://arxiv.org/abs/2509.21006 | <details><summary>展开</summary>### 论文研究单位<br>Skolkovo Institute of Science and Technology（莫斯科斯科尔科沃理工学院）智能空间机器人实验室。<br><br>### 论文概述<br>AnywhereVLA是一个模块化框架，用于在未见过的大型室内环境中实现自然语言驱动的抓取-放置任务。系统将语言指令转换为结构化任务图，结合LiDAR-SLAM、度量语义映射和主动探索策略，并在接近模块引导下由细调的SmolVLA模型执行操作。在消费级硬件（Jetson Orin NX和Intel NUC）上实时运行（≥10Hz），在实验室环境（静态场景和人类活动）中实现46%整体成功率，结合经典导航与VLA操作确保鲁棒性与灵活性。<br><br>### 论文核心贡献点<br>- **统一模块化框架**：单一语言指令同时驱动环境探索和VLA操作。<br>- **实时性能**：嵌入式设备上实现≥10Hz的全模块运行。<br>- **环境适应能力**：支持未见过的动态场景，探索并定位目标对象。<br>- **方法融合**：结合经典SLAM导航与轻量级VLA（SmolVLA 450M）模型，提升操作可靠性与泛化能力。<br><br>### 论文方法描述<br>- **架构模块**：<br> - **3D语义映射（SM）**：LiDAR-惯性-视觉SLAM构建语义点云，通过LiDAR密集化（插值环间点）、对象聚合（DBSCAN聚类）和置信度估计生成目标地图。<br> - **主动环境探索（AEE）**：前沿驱动的探索策略，条件化目标类，优化视野覆盖并抑制虚假目标。<br> - **接近模块（Approach）**：基于语义地图计算安全接近姿势，验证无碰撞导航。<br> - **VLA操作**：细调SmolVLA模型生成操作动作，集成多视角相机输入。<br>- **工作流**：解析语言指令→触发SM和AEE→SM构建地图→AEE定位目标→Approach导航→VLA执行操作。<br>- **硬件部署**：Jetson Orin NX处理感知和VLA，Intel NUC处理SLAM和导航。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：SO-101操作器抓取-放置轨迹50条（遥操作采集）。<br>- **训练资源**：RTX 4090 GPU（16GB VRAM）细调SmolVLA（批量16，学习率0.0001，余弦退火，AdamW优化，梯度裁剪10.0）。<br>- **部署硬件**：Jetson Orin NX（16GB）和Intel NUC（Core i7, 32GB）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：未见过的大型室内多房间实验室（动态场景，正常人类活动）。<br>- **任务**：50次抓取-放置试验（目标对象随机分布在半径内）。<br>- **指令模板**：自然语言指令，如“Pick up the <object> and place it in the <area>. And bring the <object> to <location>.”<br>- **评估指标**：<br> - 成功率（SR）：整体SR 46%，VLA操作模块SR 85%（细调后）。<br> - 模块性能：SLAM SR 100%，AEE SR 75%，导航SR 90%，对象检测SR 85%，VLA操作SR 80%。<br> - 实时性能：总任务时间（探索半径5m时平均133秒，10m时<10分钟）。<br> - 计算效率：模块频率≥10Hz（如表I所示）。</details> |
| 2025-09-25 | ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation | http://arxiv.org/abs/2509.20841 | <details><summary>展开</summary>### 论文研究单位<br>德坤卢、高伟和贾奎（Dekun Lu, Wei Gao, and Kui Jia）- 作者信息未明确标注所属机构<br><br>### 论文概述<br>论文提出了一种名为ImaginationPolicy的端到端机器人操作策略，旨在解决传统模块化管道信息丢失和特征对齐问题。该方法通过**可操作性导向的关键点**实现对多样化操作任务的统一表示，显著提升机器人在抓取、倒水等任务中的泛化能力和精度。研究采用Chain of Moving Oriented Keypoints (CoMOK)作为动作表示，将操作行为转化为关键点序列，使网络能在不同形状/尺寸物体间实现**亚厘米级精度**。<br><br>### 论文核心贡献点<br>1. **创新动作表示**：提出CoMOK（移动方向关键点链）作为统一操作表示，突破末端执行器姿态表示限制<br>2. **任务泛化能力**：基于可操作性关键点自然适配不同物体形状/尺寸，包括可变形物体<br>3. **多任务统一框架**：单神经网络同时处理抓取、倒水、稳定放置等任务<br>4. **端到端训练**：融合Groma视觉语言模型进行任务规划，Score-Matching网络生成动作分布<br>5. **复杂场景适配**：支持多阶段操作、多模态行为及轨迹动作生成<br><br>### 论文方法描述<br>**CoMOK核心公式**：<br>```<br>网络输出 (o_manipulated, T_affordance, T_action)<br>```<br>- **o_manipulated**: 机器人控制对象（工具/物体/局部区域）<br>- **T_affordance**: 操作方向关键点（如杯子把手、缆绳抓取点）<br>- **T_action**: 目标对齐姿态（若T_affordance与T_action对齐则任务完成）<br><br>**扩展机制**：<br>- **多阶段操作**：全局任务描述自动分解子任务（如倒水任务拆解为"抓杯子→倒水→放置"）<br>- **多模态候选**：扩散模型生成动作分布（如多个抓取位姿可选）<br>- **轨迹序列**：输出SE(3)位姿序列（如切割水果的上下两帧）<br><br>**神经网络架构**：<br>1. **任务规划层**：基于Groma VLM，从RGBD图像和全局指令生成子任务列表<br>2. **动作预测层**：点云输入+阶段任务特征，通过Score-Matching生成关键点序列<br>3. **轨迹生成层**：<br> - 仿真：Motion Policy Networks学习生成关节空间轨迹<br> - 实机：传统任务与运动规划算法筛选可行轨迹<br><br>### 论文使用数据集和训练资源<br>- **抓取检测**：复用现有点云数据集（SE(3)-DiffusionFields来源）<br>- **仿真实验**：三类物体稳定放置任务<br> - 20个瓶子、10个盒子、5个三脚架<br> - 随机放置5个障碍物<br>- **实机实验**：6种缆绳+3种挂钩的多样化操作<br>- **硬件配置**：Rokae SR5六自由度机械臂 + 末端RGBD传感器<br><br>### 论文使用的评估环境和评估指标<br>**评估任务**：<br>1. **抓取检测**：双帧输出（预抓取+抓取）<br>2. **稳定放置**（仿真）：<br> - 成功率：距离桌面≤1cm，Z轴偏差≤15°<br> - 碰撞检测：放置后与障碍物碰撞<br>3. **缆绳插入**（实机）：缆绳是否插入夹具<br>4. **挂钩挂杯**（实机）：20种杯子×3种挂钩<br><br>**评估指标**：<br>- **精度**：位置误差（厘米级）、姿态误差（角度）<br>- **泛化性**：跨物体形状/尺寸成功率<br>- **可靠性**：任务级成功率（不含中间路径错误）<br>- **动态适应**：可变形物体（缆绳）操作成功率<br><br>**结论**：方法实现跨任务统一操作，成功率从稳定放置的仿真数据到实机缆绳插入均有验证，平均执行精度达厘米级且对物体变化鲁棒。</details> |
| 2025-09-24 | Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving | http://arxiv.org/abs/2509.20109 | <details><summary>展开</summary>## 论文研究单位<br>- **LiAuto**（理想汽车）<br>- **清华大学**（Tsinghua University）<br><br>## 论文概述<br>本文针对端到端自动驾驶系统中模仿学习无法内在编码物理规则（如碰撞避免、遵守可行驶区域）的核心挑战，提出了一种名为 **ReflectDrive** 的创新框架。该框架将离散扩散模型应用于轨迹规划，并引入**安全反射机制**（Safety-Guided Regeneration），在推理阶段通过迭代局部搜索和安全锚点重绘，无需梯度计算即可实现轨迹的安全自校正，从而提升规划的可控性和可靠性。<br><br>## 论文核心贡献点<br>1. **首次将离散扩散应用于端到端自动驾驶规划**：将连续轨迹空间离散化为动作码本，利用预训练的扩散语言模型（DLM）进行微调，实现并行解码和双向特征融合。<br>2. **提出安全反射机制**：在推理阶段结合目标条件生成（Goal-Conditioned Generation）和安全导向再生，通过局部搜索识别不安全点并替换为安全锚点，再利用扩散模型的修复能力（inpainting）重新生成轨迹。<br>3. **验证安全约束的有效性**：在真实场景基准测试中，该框架在确保硬安全约束的同时，显著提升了轨迹质量，且性能接近人类驾驶水平。<br><br>## 论文方法描述<br>1. **轨迹离散化（Trajectory Discretization）**：<br> - 将二维驾驶空间中的连续坐标 `(x, y)` 分别映射到预定义的1D码本 \(\mathcal{A}\) 中，生成轨迹的离散化序列 \(\mathbf{y}\)。<br> - 使用均匀网格量化坐标范围 \([-M, M]\)，分辨率 \(\Delta_g\) 可控，确保离散化后的轨迹可通过逆量化恢复为连续坐标。<br><br>2. **离散扩散模型（Discrete Diffusion Model）**：<br> - 基于离散扩散框架（掩码-去噪），采用预训练的VLA模型（如LLaDA-V）作为主干网络。<br> - 通过监督微调训练模型，使其具备根据场景上下文生成离散轨迹序列的能力。<br><br>3. **安全反射推理（Reflective Inference）**：<br> - **目标条件生成**：对终点位置进行概率采样，应用非极大值抑制（NMS）获取空间多样化的目标点，生成多模态轨迹候选。<br> - **安全导向再生**：<br> - **全局评分器**：评估完整轨迹的安全性和质量。<br> - **安全评分器**：定位不安全路标点。<br> - **局部搜索**：在路标点附近（如曼哈顿距离 \(\delta \leq 10\)）搜索可行替代点作为“安全锚点”。<br> - **轨迹修复**：固定安全锚点，通过扩散模型重绘周围轨迹段。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：基于大规模真实世界自动驾驶基准 **NAVSIM** 的数据。<br>- **训练资源**：<br> - 输入：前、左前、右前摄像头图像及语言指令（如“左转”）。<br> - 模型初始化：预训练的扩散语言模型（如LLaDA-V）。<br> - 训练方法：监督微调（SFT），批量大小为16，学习率 \(1 \times 10^{-5}\)，训练3轮。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **NAVSIM 基准**：使用官方闭环仿真器进行评估。<br>- **评估指标**：<br> - **PDMS 分数**（主要指标）：聚合5项指标（分数越高越好）：<br> - **NC**（No-Collision）：无碰撞率（防止事故）。<br> - **DAC**（Drivable Area Compliance）：可行驶区域合规性（遵守道路边界）。<br> - **TTC**（Time-to-Collision）：碰撞时间安全性（避让反应时间）。<br> - **Comfort**：舒适性（加速度/急动度受控）。<br> - **EP**（Ego Progress）：ego进展（路径完成度）。<br><br>**结果摘要**（以相机输入为例）：<br>\| 方法 \| DAC ↑ \| TTC ↑ \| NC ↑ \| EP ↑ \| PDMS ↑ \|<br>\|-------------------\|-------\|-------\|------\|------\|--------\|<br>\| ReflectDrive \| **99.3%** \| 93.5% \| 97.7% \| **86.9%** \| **91.1** \|<br>\| ReflectDrive（无反射） \| 95.4% \| 92.2% \| 96.9% \| 79.0% \| 84.8 \|<br>\| **人类表现** \| 100.0% \| 100.0% \| 100.0% \| 87.5% \| 94.8 \|<br><br>> **结论**：ReflectDrive 的安全反射机制显著提升轨迹安全性（如 DAC 提升 +3.9）同时改善路径完成度（EP 提升 +7.9），接近人类驾驶水平。</details> |
| 2025-09-24 | FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models | http://arxiv.org/abs/2509.19870 | <details><summary>展开</summary>论文研究单位<br>- 复旦大学<br>- 上海人工智能实验室<br>- Sea AI Lab<br><br>论文概述<br>- 针对视觉-语言-动作（VLA）模型提出并形式化“行动冻结”（action-freezing）攻击：对抗图像使机器人忽略后续指令，进入持续无响应状态，可能在关键时刻导致不作为，引发实际安全风险。<br>- 提出FreezeVLA框架，通过最小-最大双层优化生成跨提示可迁移的对抗图像，在三个SOTA VLA模型与四个机器人基准上实现高攻击成功率，显著优于现有方法。<br><br>论文核心贡献点<br>- 识别并形式化VLA模型的“行动冻结”漏洞，揭示与错误动作同等严重的“不作为”威胁。<br>- 提出FreezeVLA：使用内部最大化（对抗提示优化）与外部最小化（对抗图像优化）的双层优化方法，扩展提示嵌入空间覆盖，显著提升跨提示迁移能力。<br>- 跨模型验证：SpatialVLA、OpenVLA、π0 上分别达到平均攻击成功率73.3%、95.4%、59.8%，全面超越随机噪声、PGD、多提示等基线。<br><br>论文方法描述<br>- 威胁模型与目标：白盒访问VLA模型、黑盒用户指令，通过图像扰动使模型在任意指令下稳定输出冻结令牌（如<freeze>、<eos>或“do-nothing”token），导致行动终止。<br>- 内部最大化（对抗提示优化）：从初始参考提示集出发，通过梯度分析定位高影响词并进行贪婪同义词替换，迭代生成更难被冻结的“硬提示”集，扩展提示语义空间。<br>- 外部最小化（对抗图像优化）：基于“硬提示”集合，对图像进行梯度聚合与更新，使VLA模型在多种提示下均以高概率输出冻结令牌，实现持久冻结。<br>- 形式化目标：min−max 双层优化，外层在 L∞ 扰动预算 ε 内最小化冻结损失，内层最大化难提示下的损失，使对抗图像在跨提示下保持有效性。<br>- 提示多样性增强：对比“随机采样”与“GPT生成”提示，显示GPT生成的语义多样性进一步提升攻击迁移性。<br><br>论文使用数据集和训练资源<br>- 数据集与基准：LIBERO-10、LIBERO-Goal、LIBERO-Object、LIBERO-Spatial四个机器人操控基准。<br>- 目标模型：SpatialVLA、OpenVLA、π0 三种SOTA VLA模型（具备动作分块/离散化/连续动作头等不同架构与冻结令牌策略）。<br>- 训练与实验资源：HPC集群，32×NVIDIA A800-SXM4-80GB GPUs。<br><br>论文使用的评估环境和评估指标<br>- 评估指标：攻击成功率（ASR），定义为使VLA模型进入一致冻结状态的对抗图像比例。<br>- 评估设置：<br> - 扰动预算：L∞ 约束下的 ε=4/255（主实验），对比 ε∈{1/255,2/255,4/255,8/255,16/255} 的敏感性。<br> - 优化步数与提示数：图像迭代K=100、步长α=1/255；参考提示规模\|P\|=20；内部提示迭代M=10。<br> - 基线对比：随机噪声、Single-Prompt PGD、Multi-Prompt（随机/随机+GPT）、FreezeVLA（随机）与FreezeVLA+GPT。<br> - 冻结策略差异：SpatialVLA/π0 使用 <eos>；OpenVLA 使用“do-nothing”token。<br>- 主要结果：FreezeVLA+GPT 在 ε=4/255 下实现平均ASR 73.3%（SpatialVLA）、95.4%（OpenVLA）、59.8%（π0），并具有强跨提示可迁移性与随预算提升的稳定增益。</details> |
| 2025-09-24 | Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training | http://arxiv.org/abs/2509.19752 | <details><summary>展开</summary># 论文研究单位<br>- 香港科技大学<br>- 微软亚洲研究院<br>- 武汉大学<br>- 中国科学院大学<br>- 清华大学<br>- 中南大学大数据研究院<br><br># 论文概述<br>当前视觉-语言-行动（VLA）模型高度依赖大规模人工演示数据，但人工数据采集成本高、可扩展性差。强化学习（RL）能自动生成策略，但对长时程、奖励稀疏的操控任务效果不稳且轨迹方差大。文章提出以扩散模型为核心的RL数据生成方法，利用扩散策略的强表达能力和迭代去噪的隐式正则，获得高质、低方差的轨迹，构建“扩散RL → 数据生成 → VLA训练”的完整管线。在LIBERO基准（130个长时操控任务）上验证，扩散RL生成数据训练出的VLA模型平均成功率81.94%，相比人类数据+5.3%、相比高斯RL+12.6%，并揭示了轨迹平滑、低方差特性是提升VLA性能的关键。<br><br># 论文核心贡献点<br>- 面向VLA训练的扩散RL数据生成框架：两阶段训练（BC暖启动 + 在线PPO），在架构、采样与训练流程上提出稳定化改进。<br>- 实证证明扩散RL数据的优越性：在LIBERO的130个任务上显著提高VLA的域内成功率与部分泛化表现。<br>- 轨迹质量量化分析：任务效率、轨迹平滑（均方 jerk）、动作一致性（低方差）三个维度解释数据质量提升如何转化为VLA性能增益。<br>- 消融验证稳定性：ResNet+U‑Net、DDIM 5步采样、余弦退火学习率、并行环境增强数据多样性等设计的必要性。<br><br># 论文方法描述<br>- 策略与采样<br> - 扩散策略：将动作视为由K步扩散过程生成，训练噪声预测网络 εθ(a_k, s_t, k) 反演去噪得到最终动作 a_0。使用DDIM（5步）加快采样、降低方差，保证稳定RL梯度。<br> - 两阶段训练<br> - 阶段一：行为克隆（BC）暖启动。用少量人类多模态演示训练扩散策略，拟合复杂演示分布（公式1）。<br> - 阶段二：在线PPO强化学习微调。以价值网络Vϕ估计状态价值，GAE计算优势，PPO clip目标在每个去噪步上优化策略（公式2–5）。<br>- 稳定化策略<br> - 架构：ResNet主干+U‑Net解码用于多模态与样本效率；FiLM融合本体感觉（proprioception）稳定条件信号。<br> - 学习率：余弦退火，初期鼓励探索，后期稳定收敛。<br> - 数据多样性：并行环境扩充经验缓冲，防止高容量扩散策略在相关小批次上过拟合、模式坍缩。<br>- VLA训练<br> - 专家扩散RL策略收敛后收集数据 D_RL。使用标准行为克隆目标最大化对数似然（公式6）训练VLA，兼容不同策略架构（如π0/OpenVLA）。<br><br># 论文使用数据集和训练资源<br>- 数据集<br> - LIBERO基准：包含多套任务共计130个长时操控任务；常用于知识迁移评测。数据源包括人类演示与由不同方法（高斯RL、扩散RL）生成的轨迹。<br>- 训练流程资源（基于文中描述）<br> - 并行环境用于在线RL采集数据与多样经验回放。<br> - 用于对比的生成方法：人类数据（每任务50条演示）、高斯RL数据、扩散RL数据（每任务50条最优轨迹）、人类+扩散RL混合数据。<br> - VLA训练遵循统一预处理（如OpenVLA流程），超参数保持一致以保证公平对比。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境<br> - LIBERO任务套件，涵盖空间/目标/对象变化与长时任务（LIBERO‑Spatial/Goal/Object/Long），以及由LIBERO‑90与LIBERO‑Long组成的大规模套件（文中称为LIBERO‑100）。<br>- 评估指标<br> - 成功率（SR）：每套任务50次评估的平均成功率；OOD设定为在LIBERO‑90上训练、零样本评估其他未见任务套件。<br> - 轨迹质量分析<br> - 任务效率：平均轨迹长度与“无操作”比例（速度≈0且抓持状态不变）。<br> - 轨迹平滑度：末端执行器轨迹的均方 jerk，越低越平滑。<br> - 动作一致性：同一任务上多条成功轨迹的方差，越低越一致。</details> |
| 2025-09-23 | Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action | http://arxiv.org/abs/2509.19571 | <details><summary>展开</summary>论文研究单位<br>Université de Montréal；Mila - Quebec AI Institute；Sapienza University of Rome<br><br>论文概述<br>针对开放词汇自然语言指令的机器人执行难题，论文提出“Agentic Scene Policies”(ASP)。其核心思想是将语言指令分解为三类可查询操作：对象定位、空间推理与部件级交互，并基于显式场景表示(开放词汇对象地图、CLIP语义特征、3D点云与可 affordance 区域)为大型语言模型(LLM)代理提供可调用工具(检索、空间度量与交互技能)。系统支持桌面与移动操控，并通过 afforfance 驱动的技能库(如 tip_push、pinch_pull、hook_pull)实现零样本操作。<br><br>论文核心贡献点<br>- 统一空间、语义与 affordance 的场景查询式机器人策略(ASP)<br>- 在15项桌面操控任务上与主流 VLA(π0-FAST、π0.5)进行系统性比较，报告成功率与进展率<br>- 提出移动版 ASP，实现基于 affordance 引导的导航、跨关键帧地图聚合与到目标后的局部重检测<br><br>论文方法描述<br>- 对象地图(ObjectMap)：包含 Object(几何点云、CLIP语义特征、RGB/Depth裁剪、多视图)与 Affordance(point_cloud、part描述、对应技能)<br>- LLM 代理：通过工具链调用与符号状态(当前抓取对象与清单)执行查询计划，不直接访问传感器或完整地图<br>- 工具集：<br> - object_retrieval：基于文本与CLIP特征的开放词汇对象检索，返回对象键加入清单<br> - spatial：距离、左右关系、尺寸等点云几何运算<br> - interact：将自然语言动作与目标对象传给内部 affordance 检测与技能调度<br> - 技能(基于 AnyGrasp 与运动规划)：grasp、place、drop、grasp_part、tip_push、pinch_pull、hook_pull<br> - 反馈与重试：ToolOutput 结构返回成功/反馈信息，支持条件检查与策略重试<br>- remapping：技能失败或状态不明时触发地图重算<br>- 移动 ASP：新增 go_to 工具，利用目标 affordance 方向计算期望视角并进行代价优化；跨多帧 RGB-D 聚合对象地图；到达后在本地构建 ObjectMap 进行对象重检测<br><br>论文使用数据集和训练资源<br>- 无专用新数据集；方法在开放词汇场景表示与零样本 VLM 能力之上工作<br>- 移动版构建房间级对象地图：通过遥控采集1–5个关键帧，地图构建与合并约每查询<1分钟<br>- 计算资源：工作站配备 NVIDIA Titan RTX；RGB-D 通过 Wi-Fi 传输；推理组件包括 SAM/MobileSAM、CLIP、AnyGrasp、SLAM(RTABMap)与导航(Nav2)<br><br>论文使用的评估环境和评估指标<br>- 桌面操控：UFactory XArm6 + Intel Realsense D435i(腕部)；与 VLA 对比时采用 Franka 臂+DROID 设定以降低分布偏移<br>- 移动操控：Agilex Ranger Mini 2.0 移动基座 + XArm6 + 追踪相机 T265<br>- 指标：<br> - 成功率(10次/任务平均)<br> - 进展率(接近、尝试、部分完成等分级评分)<br> - 失败模式分析(感知、affordance 检测错误、把柄使用不当等)<br>- 实验规模：15项桌面任务，共540次尝试；移动端进行双物品放置、指尖/拉拔、空间区分等多类查询</details> |
| 2025-09-23 | OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation | http://arxiv.org/abs/2509.19480 | <details><summary>展开</summary>待生成</details> |
| 2025-09-23 | Pure Vision Language Action (VLA) Models: A Comprehensive Survey | http://arxiv.org/abs/2509.19012 | <details><summary>展开</summary># Pure Vision-Language-Action (VLA) Models: A Comprehensive Survey<br><br>## 论文研究单位<br><br>- 兰州大学信息科学与工程学院（中国）<br>- 新加坡国立大学NExT++研究中心（新加坡）<br>- 中国科学院计算技术研究所（中国）<br><br>## 论文概述<br><br>本论文是关于纯 Vision-Language-Action (VLA) 模型的综合调研，系统地回顾了该领域的研究进展。论文从传统基于策略的控制方法出发，聚焦于 VLA 如何将 Vision-Language Models (VLMs) 从被动序列生成器转变为主动的机器人和决策制定代理。调研覆盖了超过300项近期研究，提供了清晰的方法分类体系，分析了 VLA 在不同应用场景中的表现，并提出了未来研究的关键挑战和发展方向。<br><br>## 论文核心贡献点<br><br>- **系统化分类体系**：提出了基于动作生成策略的 VLA 方法分类，将现有方法分为自回归、扩散、强化学习和混合/专用方法四大类别<br>- **方法创新总结**：深入分析了每个类别的方法动机、核心策略和实现机制，强调了定义性特征和技术创新<br>- **资源全面概述**：提供了 VLA 模型训练和评估所需的关联资源（数据集、基准测试、仿真平台）的详细概览<br>- **挑战与机遇分析**：识别了现有技术的关键局限性，并提出了潜在的研究探索方向<br><br>## 论文方法描述<br><br>论文将 VLA 方法分为以下四个主要类别：<br><br>### 1. 自回归模型<br>- 将动作序列视为时间依赖过程，步进式生成动作<br>- 代表性方法包括 Gato、RT-1/RT-2、PaLM-E 等<br>- 进一步细分为：通用 VLA 方法、LLM 驱动的语义规划、轨迹生成与视觉对齐、结构优化与高效推理机制<br><br>### 2. 扩散模型<br>- 将动作生成建模为条件扩散过程<br>- 代表性方法包括 SE(3)-DiffusionFields、Diffusion Policy、3D Diffuser Actor 等<br>- 涵盖：扩散式通用 VLA 方法、多模态架构融合、应用优化与部署<br><br>### 3. 强化学习模型<br>- 基于强化学习的 VLA 微调策略<br>- 通过奖励机制优化策略性能<br><br>### 4. 混合与专用方法<br>- 结合多种范式的混合架构<br>- 针对特定领域和应用的专门设计<br><br>## 论文使用数据集和训练资源<br><br>### 真实世界数据集<br>- **Open X-Embodiment (OXE)**：整合了22个机器人数据集，包含527种技能和160,266个任务<br>- **BridgeData**：涵盖10个环境中的71个任务<br>- 自主驾驶领域的相关数据集<br><br>### 仿真数据集<br>- **Open X-Embodiment (OXE)** 仿真版本<br>- **BridgeData** 仿真版本<br>- 自主驾驶仿真数据集<br><br>### 仿真平台<br>- **THOR、Habitat**：室内环境仿真<br>- **MuJoCo、Isaac Gym**：物理仿真<br>- **CARLA**：自动驾驶仿真<br>- 提供可扩展的虚拟环境，支持多模态标注生成<br><br>## 论文使用的评估环境和评估指标<br><br>论文主要围绕以下应用领域进行评估：<br><br>### 机器人操作<br>- 机械臂控制<br>- 四足机器人<br>- 人形机器人<br>- 移动机器人<br><br>### 自主驾驶<br>- 2D/3D 感知融合<br>- 轨迹预测<br>- 闭环控制<br><br>### 评估指标<br>- 任务成功率<br>- 轨迹精度<br>- 跨平台泛化能力<br>- 零样本/少样本性能<br>- 实时推理效率<br>- 多模态对齐质量<br><br>论文强调需要建立更全面的评估框架，特别关注长时序任务稳定性、语义对齐鲁棒性和实际部署效率等关键指标。</details> |
| 2025-09-23 | Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations | http://arxiv.org/abs/2509.18953 | <details><summary>展开</summary>### 论文研究单位<br>上海交通大学（人工智能学院重点实验室）、中国军事科学院国防创新研究院、智能博弈决策实验室<br><br>### 论文概述<br>当前视觉-语言-动作（VLA）模型在真实部署中易受物理变化影响（如光照、物体位置干扰），但缺乏系统性评估工具。本文提出Eva-VLA框架，将离散物理变化转化为连续参数优化问题，通过黑箱优化算法探索最恶劣场景，暴露了当前VLA模型在多种物理干扰下的严重脆弱性。<br><br>### 论文核心贡献点<br>1. **首次系统性分解物理变化**：将真实世界变化分为物体3D变换（旋转）、光照变化、对抗补丁三类，突破传统梯度攻击限制<br>2. **统一参数化评估框架**：将各类变化转化为连续参数分布，通过仿真环境可复现地评估鲁棒性<br>3. **揭示VLA系统脆弱性**：在OpenVLA等先进模型中验证，物理干扰使失败率从23.5%激增至82.6%，长时序任务达97.8%<br><br>### 论文方法描述<br>#### 三类物理变化参数化<br>- **物体3D变换**：用Tait-Bryan角α,β,γ∈[-90°,90°]约束旋转参数<br>- **光照变化**：通过高斯衰减函数建模点光源L(z)=I·exp(-\|\|z-(x,y)\|\|²/2σ²)，参数λ={x,y,σ,I}控制位置/强度<br>- **对抗补丁**：用自然图像在桌面纹理上优化位置φ={x,y}∈[(W/3,2W/3)×(H/3,2H/3)]<br><br>#### 黑箱优化算法<br>使用CMA-ES（协方差矩阵自适应进化策略）优化参数分布：<br>1. 将变化参数建模为多元高斯分布N(μ,Σ²C)<br>2. 迭代采样配置，计算对抗损失函数L_adv=-∑cos(A_clean, A_adv)<br>3. 基于损失值更新分布参数μ, C, Σ<br>4. 引入学习率自适应和早停机制加速收敛<br><br>### 论文使用数据集和训练资源<br>- **仿真数据集**：LIBERO基准套件（包含空间/物体/目标/长时序四类任务，每类10任务×50轮试验）<br>- **真实数据集**：BridgeData v2（用于部分真实场景验证）<br>- **硬件资源**：NVIDIA A800 GPU (80GB内存) + AgileX Piper机械臂（7自由度） + RealSense D435i相机<br><br>### 论文使用的评估环境和指标<br>- **仿真评估**：基于LIBERO套件，按任务最大步长设超时标准，统计失败率FR=1-SR（成功率）<br>- **真实评估**：在三任务（抓取/定位/放置）上执行50次试验，攻击成功率44.6%<br>- **鲁棒性指标**：<br> - 失败率变化率：清洁环境(23.5%) vs 物理干扰(82.6%)<br> - 不同干扰类型影响：物体变换>对抗补丁>光照变化<br> - 时序影响：长时序任务在干扰下失败率超97%</details> |
| 2025-09-23 | Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation | http://arxiv.org/abs/2509.18865 | <details><summary>展开</summary># 论文研究单位<br>- D3 Center, The University of Osaka<br>- Graduate School of Information Science and Technology, The University of Osaka<br>- Graduate School of Maritime Sciences, Kobe University<br><br># 论文概述<br>提出 Bi-VLA（Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation），将双侧控制（bilateral control）的位置与力信息、视觉观测与自然语言指令融合，构建单模型多任务策略，在真实机器人上完成抓取-搬运-放置任务，相比传统双侧控制式模仿学习在任务成功率与适应性上显著提升。<br><br># 论文核心贡献点<br>- 融合视觉与语言：在双侧控制式模仿学习框架中引入 SigLIP 语言编码与 FiLM 调制，将视觉与语言对齐后统一嵌入决策。<br>- 单模型多任务：打破既往双侧控制方法仅能单任务建模的局限，实现一个策略在语言可区分与视觉可区分两类任务间灵活切换。<br>- 真实机器人验证：在两类任务与干扰环境中取得更高成功率，证明视觉-语言融合在接触丰富与动态环境下的有效性。<br><br># 论文方法描述<br>- 数据采集：采用四通道双侧控制，领导者与跟随者实时交换位置与力矩；关节角度、速度与外力矩由编码器、DOB 与 RFOB 估计；同步采集 RGB 图像与自然语言指令，形成多模态演示。<br>- 学习模型：文本用 SigLIP 编码，视觉用 EfficientNet 提取特征，经 FiLM 进行跨模态融合；融合特征与机器人关节状态共同输入 Transformer 驱动的 CVAE，输出动作块（领导者关节角度、速度、力矩）。<br>- 推理流程：给定跟随者当前关节状态、同步图像与语言指令，模型预测下一步动作块，经双侧控制转换为电流指令执行，实现闭环控制与灵活任务切换。<br><br># 论文使用数据集和训练资源<br>- 硬件：OpenManipulator-X 机械臂两台（领导/跟随），两个 RGB 摄像头（俯视与夹爪侧）。<br>- 数据频率：双侧控制 1000 Hz，图像 100 Hz；每臂 15 维状态（5 关节 × 角度/速度/力矩），合计 30 维联合状态。<br>- 任务演示：每任务收集 6 条演示（上/下各 3）；为评估跨任务低数据泛化，同时收集 4 条混合演示（上/下目标与上/下源各 1）。<br>- 数据增强：采用 DABI 将演示从 1000 Hz 下采样至 100 Hz 与图像对齐，6 条演示扩展至 60 条，4 条混合演示扩展至 40 条。<br>- 模型配置：Transformer 编码器 4 层、解码器 7 层，CVAE 结构，动作块预测。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：两类任务——语言可区分任务（目标位置由指令指定）、视觉可区分任务（拾取位置由场景指定），以及未学习的三球干扰环境。<br>- 评估指标：分阶段成功率（Pick/Move/Place）与总体成功率（各 10 次独立试验，仅当所有阶段均完成视为成功）。<br>- 对比模型：Bi-ACT（无语言）、Bi-VLA（DistilBERT）、Bi-VLA（SigLIP）、Bi-VLA（SigLIP-Mix）。<br>- 关键结果：<br> - 语言可区分任务：Bi-ACT 仅 50%（仅 Up 成功），Bi-VLA（DistilBERT）60%，Bi-VLA（SigLIP）90%，Bi-VLA（SigLIP-Mix）70%。<br> - 视觉可区分任务：Bi-ACT 95%，Bi-VLA（SigLIP）90%，Bi-VLA（SigLIP-Mix）90%。<br> - 未学习三球环境：Bi-ACT 50%，Bi-VLA（SigLIP）75%，Bi-VLA（SigLIP-Mix）75%。</details> |
| 2025-09-22 | Latent Action Pretraining Through World Modeling | http://arxiv.org/abs/2509.18428 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br><br>Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE<br>Alexandria University, Alexandria, Egypt<br><br>## 论文概述<br><br>本文提出了LAWM框架，通过世界建模从无标签视频数据中学习潜在动作表示，用于预训练模仿学习模型。该框架能够从机器人录制视频或人类操作日常物体的视频中进行自监督学习，无需依赖人工标注的动作数据。框架设计为模型无关，可以跨任务、环境和具身形态进行迁移，在LIBERO基准测试和真实世界设置中的表现优于使用真实机器人动作训练的模型和类似的预训练方法，同时更高效且适用于实际场景。<br><br>## 论文核心贡献点<br><br>1. 提出LAWM框架，这是一个模型无关的框架，可以从机器人和人类视频中学习动作块表示，无需动作标签<br>2. 实验表明该框架可以从人类演示和机器人操作视频中学习优于监督预训练的动作先验，且无需使用真实动作标签<br>3. 使用小型模型（BAKU和Dreamerv3）的框架在LIBERO基准测试上优于使用大型模型的类似方法（villa-X）<br><br>## 论文方法描述<br><br>LAWM包含两个阶段：<br><br>潜在动作预训练阶段：<br>- 输入包括图像帧和自然语言指令<br>- 模仿学习模型处理输入产生n个潜在动作表示<br>- 这些潜在动作与当前帧和后续n-1帧配对，输入世界模型<br>- 世界模型基于RSSM架构，包含编码器、动态模型和解码器<br>- 通过预测未来图像帧进行端到端训练，损失函数包括MSE重建损失和KL散度正则化项<br>- 学习信号来自预测视频序列中的下一帧图像<br><br>动作微调阶段：<br>- 预训练的模仿学习模型适配到下游机器人任务<br>- 不再使用世界模型<br>- 使用标注演示数据将观察（图像、语言指令和机器人状态）直接映射到真实动作<br>- 对BAKU采用负对数似然损失，对Diffusion Policy采用去噪扩散损失<br><br>## 论文使用数据集和训练资源<br><br>数据集：<br>- BridgeData v2：60,096条轨迹，24个环境，包含拾取放置、推动、折叠等任务<br>- Something-Something v2：220,847个人类操作日常物体的视频片段（使用其中10%用于预训练）<br>- LIBERO基准测试：包含LIBERO-90（90个任务）和四个任务套件（Spatial、Object、Goal、Long），每个套件10个任务，每个任务50个演示样本<br>- 真实世界自定义数据集：5个任务（3个拾取放置、1个堆叠、1个移动），每个任务50个演示，使用VR控制器收集<br><br>训练资源：<br>- 单个A100 GPU<br>- DreamerV3世界模型使用50M参数配置<br>- 潜在动作空间维度为7（与真实动作相同）<br>- BAKU实验的动作块大小为10，Diffusion Policy实验为16<br>- 在BridgeData v2或Something-Something v2上预训练约30小时<br>- 在LIBERO-90上微调约24小时<br>- 在LIBERO任务套件上微调约2小时<br>- 真实世界数据集微调约20分钟<br><br>## 论文使用的评估环境和评估指标<br><br>评估环境：<br>- LIBERO-90基准测试：90个多样化任务<br>- LIBERO任务套件：<br> - LIBERO-Spatial：新布局下的相同任务和对象类型<br> - LIBERO-Object：新对象类型下的相同任务和布局<br> - LIBERO-Goal：新任务下的相同对象类型和布局<br> - LIBERO-Long：长时域任务，包含多样化的对象、布局和背景<br>- 真实世界设置：6自由度Realman机器人臂，配备1自由度夹爪，双视角相机观测<br><br>评估指标：<br>- 成功率（Success Rate, SR）：成功试验次数占总尝试次数的百分比<br>- 每个任务进行10次评估试验<br>- 使用典型相关分析（CCA）的第一规范分量的Pearson相关系数来量化潜在动作与真实动作之间的对齐程度</details> |
| 2025-09-22 | PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies | http://arxiv.org/abs/2509.18282 | <details><summary>展开</summary>## 论文研究单位<br>University of Washington, NVIDIA, University of Southern California, Allen Institute for AI。<br><br>## 论文概述<br>当前机器人操控策略往往同时学习“关注哪里（where）”“做什么（what）”“如何执行（how）”，导致泛化能力受限。该论文提出将“where/what”的高层语义推理交由视觉语言模型（VLM）承担，使低层策略专注“how”。为此提出了PEEK（Policy-agnostic Extraction of Essential Keypoints），一个策略无关的最小中间表示框架：由VLM预测2D末端执行器路径（what）与任务相关掩蔽点（where），并将标注直接叠加到观测图像上，再由任意RGB/RGB-D策略进行训练与推理。在535次真实世界评估中，PEEK显著提升零样本泛化，包括模拟训练的3D策略在真实场景中成功率提升41.4倍，以及对大型VLA与小型操控策略分别带来2–3.5倍增益。<br><br>## 论文核心贡献点<br>- 统一的点式中间表示：将“路径+掩蔽点”作为策略输入，实现策略无关、可迁移的表示。<br>- 可扩展标注流水线：自动从机器人视频中提取任务相关点与末端执行器路径，支持不同视角与小目标。<br>- 多策略与多具身验证：在2D/3D、不同规模策略与两种真实机械臂系统上统一验证。<br>- 零样本泛化实证：面对视觉干扰与语义新任务大幅提升成功率与鲁棒性。<br><br>## 论文方法描述<br>- 目标表示：联合预测两个点集——末端执行器的2D轨迹点 p_t 与任务相关掩蔽点 m_t，两者作为自然语言响应由VLM输出。<br>- VLM微调：以VILA-1.5-3B为基座，整合机器人与通用点预测/VQA数据；联合优化路径与掩蔽点预测，训练约20小时（A100×8）。<br>- 数据标注流水线：先用点跟踪（CoTracker3）识别场景中显著移动的点集作为“任务相关点”，再以检测器+掩蔽机制构建末端执行器路径；通过“停止点数”K-Means将长轨迹切分为更短的“子轨迹”，以提高表示的最小性与可预测性。<br>- 策略接口：推理时每隔H步查询VLM，将路径与掩蔽点以可视化方式叠加到观测图像上，供任意RGB或RGB-D策略训练与执行。掩蔽通过对预测点为中心的8%边长区域开窗；路径以随时间颜色渐变的线段表示。<br><br>## 论文使用数据集和训练资源<br>- 数据来源：Open X-Embodiment（OXE）20+子数据集、DROID、LIBERO-90、BRIDGE-v2、RoboPoint；总计2M+问答/点预测对、148k轨迹、9种具身。<br>- 训练资源：VLM微调在8×NVIDIA A100上约20小时；推理在RTX 3090上单次查询约4–6秒。<br>- 工具/模型：CoTracker3点跟踪、Detectron2末端执行器检测、FoundationStereo深度估计。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：<br> - Franka（Sim-to-Real）：仿真采集2.5k条“堆叠彩色方块”轨迹；真实世界Zed 2立体相机+FoundationStereoDepth；评估Basic/Clutter/Semantic三类任务。<br> - WidowX（BRIDGE）：单目RGB环境，改换桌面与背景；在Basic/Clutter/Semantic三类任务上评估。<br>- 评估指标：任务完成率与成功率（平均与分项），包含抓取/到达的Partial Credit；零样本跨场景与跨语义泛化能力；消融实验使用路径/掩蔽的组合成功率；VLM质量使用DTW、起点/终点L2、IoU。</details> |
| 2025-09-18 | VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation | http://arxiv.org/abs/2509.18183 | <details><summary>展开</summary>### 论文研究单位<br>Li Auto Inc.，中国北京<br><br>### 论文概述<br>VLA模型在视觉视角变化时（如不同位置、背景或高度）泛化性能下降，影响任务执行。论文提出轻量级视角适应融合框架VLA-LPAF，通过潜在空间融合多视角2D图像，仅使用单视图数据进行微调，弥补视角差异。与RoboFlamingo结合构建RoboFlamingo-LPAF，在CALVIN、LIBERO和自定义CabinEnv数据集上提升成功率8%-30%，并在真实任务中验证视角适应能力。<br><br>### 论文核心贡献点<br>- 首次实现基于2D图像的轻量级潜在视角特征融合框架VLA-LPAF，降低视角一致性约束。<br>- 实例化VLA-LPAF为RoboFlamingo-LPAF，通过多数据集和真实任务验证有效性。<br>- 三阶段训练策略（单视图仅动作、多视图仅融合、多视图联合训练）优化泛化能力。<br><br>### 论文方法描述<br>- **问题定义**：VLA模型映射多模态输入（图像、文本）到动作，但视角差异导致特征偏移。<br>- **架构**：添加MLP基融合模块，利用单视图参考数据集（D_R）和多视图辅助数据集（D_M)。<br>- **对齐融合**：融合模块在ViT编码的潜在空间中，对齐参考视图（R）和辅助视图（M）的特征。<br>- **训练策略**：<br> 1. 单视图阶段：冻结ViT，微调LLM参数（θ），使用动作损失（公式1）。<br> 2. 多视图阶段：仅训练融合模块参数（θ'），使用对齐损失（公式4）。<br> 3. 联合阶段：同时微调θ和θ'，结合动作损失（公式3）和对齐损失（公式5）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 模拟：CALVIN、LIBERO、CabinEnv（自定义舱内环境，包含按钮按压和杠杆翻转任务）。<br> - 数据集构建：参考视图（0°）+ 辅助视图（±45°范围，v=4个视角）。<br>- **训练资源**：8个NVIDIA A800 80GB GPU，图像统一尺寸224×224。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 模拟环境：使用多视角测试数据（如CALVIN中参考视图±90°范围，间隔10°）。<br> - 真实环境：Realman RML机器人臂（单臂），Intel RealSense D415（全局参考和腕部摄像头），Azure Kinect（辅助全局摄像头），执行按钮按压和杠杆翻转任务。<br>- **评估指标**：任务成功率（success rate），基线比较显示CALVIN平均提升8%，LIBERO提升15%，CabinEnv提升30%，并在真实任务中验证RoboFlamingo-LPAF完成未包含视角（如30°）的任务。</details> |
| 2025-09-22 | Prepare Before You Act: Learning From Humans to Rearrange Initial States | http://arxiv.org/abs/2509.18043 | <details><summary>展开</summary>论文研究单位<br>弗吉尼亚理工大学，协作机器人实验室，机械工程系。<br><br>论文概述<br>论文旨在解决模仿学习（IL）策略在处理训练分布之外的初始状态时表现不佳的问题。受人类在执行任务前会先重新整理环境的启发，论文提出了一种名为ReSET的算法。该算法通过学习人类的整理方式，将不熟悉的初始状态自主地改造为更简单、更接近训练数据的“锚状态”，然后再执行预训练的任务策略。论文从理论上证明了这种两阶段方法（先重排环境，再执行策略）能降低泛化误差上限，并在实验中验证了其相较于扩散策略、视觉-语言-动作模型（VLA）等基线方法，能在相同数据量下实现更鲁棒的执行效果和更高的数据效率。<br><br>论文核心贡献点<br>1. 泛化性与数据效率的理论分析：从理论上证明，在任务策略之上学习一个缩减策略，能获得更低的泛化误差上界，并减少总训练数据的需求。<br>2. 基于流的方法：提出了一种结合无关动作的人类视频和无关任务的机器人遥操作数据的方法，用于学习一个缩减策略。该方法包含三个部分：决定何时修改场景、预测人类会如何简化场景（物体点流）、以及将这些预测映射到机器人的动作原语。<br>3. 实验验证：在多个少样本任务设置中，通过实验表明ReSET在处理分布外状态时，性能超越了多种最先进的基线方法。<br><br>论文方法描述<br>ReSET是一个两阶段框架，包含一个缩减策略和一个基础任务策略。核心是学习一个与任务无关的缩减策略，它由三个网络组件构成：<br>1. 评分网络：输入当前场景观察，输出一个场景分数，用于判断是否应该继续调整环境或直接执行基础任务策略。该网络使用带有时间先验标签的人类视频进行训练，认为视频中越靠后的帧，环境越利于任务执行。<br>2. 流生成网络：输入当前场景观察，预测物体的点流，即人类会如何移动物体以简化场景。该网络使用CoTracker3等工具从人类视频中提取物体点流作为训练目标，并采用时空Transformer架构。<br>3. 缩减策略：一个任务无关的策略，将流生成网络预测的点流和机器人当前观察作为输入，输出机器人的具体动作原语（如抓取-放置、推拉、旋转）及其参数。该策略使用任务无关的机器人遥操作（玩耍）数据进行训练。<br>在部署时，ReSET首先使用评分网络评估当前状态，若分数高于阈值，则依次调用流生成网络和缩减策略来改造环境，循环此过程直到状态分数低于阈值，最后执行原始任务策略。<br><br>论文使用数据集和训练资源<br>使用的数据包括：<br>1. 无关动作的人类视频：用于训练评分网络和流生成网络，视频中展示了人类如何整理环境。<br>2. 专家机器人演示数据：用于训练基础任务策略，例如每个任务20条轨迹。<br>3. 任务无关的机器人遥操作数据：用于训练缩减策略，例如20分钟的机器人玩耍数据。<br>训练和实验在实体机器人上完成，使用的是Franka Emika机械臂，配备GELLO遥操作控制器以及两个摄像头（一个侧视固定摄像头，一个夹爪上摄像头）。<br><br>论文使用的评估环境和评估指标<br>评估环境为实体桌面操作场景，机器人平台为Franka Emika机械臂。<br>评估任务包括四个真实世界任务：抓取-放置、揭示-抓取、旋转-放置和多任务。<br>评估指标为任务成功率。每个任务在15个测试场景中进行评估，其中约80%的场景包含训练分布外的状态，以测试方法的泛化能力。对比的基线方法包括ReSET Naive、Diffusion Policy、Dynamics-DP和Pi_0。</details> |
| 2025-09-20 | ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering | http://arxiv.org/abs/2509.16680 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>- Dartmouth College（达特茅斯学院）<br>- Shandong University（山东大学）<br>- Harvard University（哈佛大学）<br><br>## 论文概述<br>论文提出了ProtoVQA，这是一个用于可解释细粒度视觉问答的统一原型框架。该框架通过学习问题感知的原型作为推理锚点，将答案与判别性图像区域连接起来，并应用空间约束匹配来确保选择的证据具有连贯性和语义相关性。通过共享原型主干网络，该框架支持视觉问答和定位任务，并提出了视觉-语言对齐评分（VLAS）来评估解释质量。<br><br>## 论文核心贡献点<br>1. 引入适应性原型框架，能够无缝处理不同的视觉-语言下游任务，包括视觉问答和定位<br>2. 采用空间约束的贪婪匹配策略建模动态视觉问题关系和几何变化<br>3. 通过明确的视觉证据和系统化的视觉-语言对齐验证实现全面的可解释性<br><br>## 论文方法描述<br>**框架组成：**<br>- **特征提取模块**：使用DeiT作为视觉特征提取器，DeBERTa作为文本编码器，将图像和文本特征投影到共享的视觉-语言空间<br>- **可解释原型部分选择模块**：引入子补丁原型（m×k结构）和贪婪匹配算法，通过空间约束选择与原型最匹配的图像区域<br>- **答案处理**：支持两种类型——Type 1（视觉定位）和Type 2（描述性问答），分别处理坐标输入和文本答案<br>- **视觉-语言对齐评估**：提出VLAS指标，测量模型关注区域与真实证据的对齐程度<br><br>**核心技术：**<br>- 子补丁原型：每个原型由k个子补丁组成，形成语义锚点<br>- 空间约束贪婪匹配：通过迭代选择相似度最高的补丁-子补丁对，并使用邻接掩码确保空间连续性<br>- 权重共享机制：在问题编码和答案处理之间共享特征投影器参数<br><br>## 论文使用数据集和训练资源<br>**数据集：**<br>- Visual7W：包含327,939个问题-答案对，覆盖47,300张COCO图像，每个问题配有4个人工选择选项和561,459个对象级定位标注<br><br>**训练资源：**<br>- 硬件：NVIDIA A800 GPU（80GB）<br>- 训练配置：200个epoch，Adam优化器，学习率1×10^-4，批大小64<br>- 图像处理：224×224像素，16×16补丁<br>- 原型参数：m=10个原型，k=3个子补丁，空间约束半径r=3<br><br>## 论文使用的评估环境和评估指标<br>**评估环境：**<br>- 主要在Visual7W测试集上进行评估<br>- 对比基线包括SUPER、QOI_Attention、SDF of VLT、STL、CFR、BriVL、CTI、Bi-CMA等代表性VQA模型<br><br>**评估指标：**<br>- **准确性指标**：分类准确率（Accuracy）<br>- **解释质量指标**：视觉-语言对齐评分（VLAS），通过IoU阈值（θ=0.5）计算模型关注区域与真实标注的对齐程度<br>- **可视化分析**：定性展示模型选择的图像区域与真实标注的对应关系<br><br>**实验结果：**<br>- ProtoVQA在Visual7W上达到70.23%准确率，与强基线模型相当<br>- 在VLAS指标上显著优于基线方法，VLAS@1达到0.4103（比Bi-CMA提升66.4%），VLAS@3达到0.2466（比Bi-CMA提升119.6%）<br>- 定性分析显示模型能够准确关注与问题相关的语义区域</details> |
| 2025-09-19 | Randomized Smoothing Meets Vision-Language Models | http://arxiv.org/abs/2509.16088 | <details><summary>展开</summary>## 论文研究单位<br>- National Technical University of Athens, Athens, Greece<br>- Université Grenoble Alpes, Grenoble, France<br>- CSX‑AI, Grenoble, France<br>- Carl von Ossietzky University of Oldenburg, Oldenburg, Germany<br>- Chalmers University of Technology, Gothenburg, Sweden<br><br>## 论文概述<br>随机化平滑（Randomized Smoothing, RS）是目前在大规模分类模型上唯一可行的鲁棒性认证技术，但其原本只能处理离散标签，难以直接用于生成式的视觉‑语言模型（Vision‑Language Models, VLMs）。本文提出一种将生成式模型的输出映射为分类问题的通用方法，从而把 RS 迁移到 VLMs 场景。核心思路是引入一个“oracle”分类层，对模型生成的文本进行二分类（如有害/无害）或离散动作映射，或将语义等价的答案归为同一等价类。通过对图像添加高斯噪声、保持提示文本不变、进行多次采样并使用 oracle 进行投票，得到多数类的置信下界，进而推导出可验证的鲁棒半径。论文还从理论上分析了样本数对半径和认证精度的影响，给出改进的样本复杂度标度律，使得在实际应用中只需要 10²‑10³ 级别的样本即可得到与经典方法（10⁴‑10⁵ 样本）相匹配的证书。实验在最新的 VLM 上对最近的越狱式攻击进行了验证，展示了该方法的可扩展性和实用性。<br><br>## 论文核心贡献点<br>- 将随机化平滑从分类扩展到生成式 VLM，提出基于 oracle 的抽象层，使得生成结果可以被视作离散类。<br>- 给出在 oracle 错误率 ε<0.5 下的概率下界修正公式<br> \[<br> \bar p_y = \frac{\bar q_y - \epsilon}{1-2\epsilon}<br> \]<br> 并证明在 \(\bar q_y>0.5\) 时该下界仍然是有效的。<br>- 推导出样本数 \(n\) 与可验证半径的关系（利用中心极限定理和 Shore 对 \(\Phi^{-1}\) 的近似），得到标度律<br> \[<br> r_\sigma(\alpha,n) \approx 1 - 1.64\frac{z_\alpha}{\sqrt n}<br> \]<br> 表明样本量只需降低 2‑3 个数量级即可保持接近最优的半径。<br>- 在理论上放宽了前期工作对均匀分布的要求，仅需多数类概率分布主要集中在 \([\beta,1)\) 且 \(\beta\ge 0.7\)，从而提升了方法的适用性。<br>- 给出适用于 VLM 的完整认证算法（Algorithm 2），包括语义等价聚类的投票机制。<br>- 通过在最新 VLMs 上进行实验，验证了对 jailbreak‑style 攻击的防御能力，并报告了认证半径、认证精度以及样本效率的定量结果。<br><br>## 论文方法描述<br>1. **模型与噪声注入**<br> - 输入为图像 \(\mathbf x\) 与文本提示 \(\mathbf t\)。<br> - 对图像加高斯噪声 \(\mathbf z\sim\mathcal N(\mathbf 0,\sigma^2\mathbf I)\) 生成 \(\mathbf x'\)。<br> - 文本保持不变，调用 VLM \(f_\theta(\mathbf x',\mathbf t)\) 获得生成的回答 \(\mathbf y\)。<br><br>2. **Oracle 分类层**<br> - **内容安全分类**：oracle 将 \(\mathbf y\) 判为 “有害” 或 “无害”。<br> - **离散动作映射**：若 VLM 充当 VLA，oracle 将答案映射为有限的动作集合（如 `base‑forward`、`gripper‑open`）。<br> - **语义等价聚类**：oracle 判断新回答是否与已出现的回答语义相同，若相同则累计计数，否则创建新类。<br><br>3. **投票与计数**（Algorithm 2）<br> - 用字典 `ans` 存储每个等价类（或每个离散动作）的出现次数。<br> - 对每个噪声样本执行 oracle 判断并更新相应计数。<br> - 最终返回计数最高的回答 \(y\) 与其计数 \(c\)。<br><br>4. **概率下界与证书半径**<br> - 将计数 \(c\) 与样本数 \(n\) 带入 Clopper‑Pearson 方法得到 \(\bar q_y\)（在置信度 \(1-\alpha\) 下的下界）。<br> - 考虑 oracle 误差 \(\epsilon\)（假设 \(\epsilon<0.5\)），修正得到真实概率下界<br> \[<br> \bar p_y = \frac{\bar q_y - \epsilon}{1-2\epsilon}<br> \]<br> - 若 \(\bar p_y>0.5\)，则可验证半径为<br> \[<br> R = \sigma \Phi^{-1}(\bar p_y)<br> \]<br> - 对于二分类情形，若对 \(\epsilon\) 没有任何已知信息，只要 \(\bar q_y>0.5\) 仍可直接使用 \(R = \sigma\Phi^{-1}(\bar q_y)\) 作为下界（Theorem 4.2）。<br><br>5. **样本效率分析**<br> - 利用 CLT 对 \(\bar p_y\) 进行近似，得到期望半径<br> \[<br> R_\sigma^{\alpha,n}(p_A) \approx \sigma \Phi^{-1}\!\bigl(p_A - t_{\alpha,n}\bigr),\quad<br> t_{\alpha,n}=z_\alpha\sqrt{p_A(1-p_A)/n}<br> \]<br> - 通过 Shore 对 \(\Phi^{-1}\) 的幂级数近似，进一步得到平均半径的下降比例<br> \[<br> r_\sigma(\alpha,n) = \frac{\bar R_\sigma(\alpha,n)}{\bar R_\sigma(0,\infty)} \approx 1 - 1.64\frac{z_\alpha}{\sqrt n}<br> \]<br> - 该公式说明把样本数从 \(10^5\) 降到 \(10^3\) 仍能保持约 90% 以上的理想半径。<br><br>6. **实验实现**<br> - 采样若干噪声图像（如 \(n=500\)‑\(2000\)），对每个图像调用 VLM 并由强 LLM（如 GPT‑4、Llama‑70B）充当 oracle。<br> - 计算多数类的计数、Clopper‑Pearson 下界与对应半径。<br> - 与原始分类 RS 基线以及仅使用 Clopper‑Pearson 不进行 oracle 修正的方案进行对比。<br><br>## 论文使用数据集和训练资源<br>- **模型**：直接使用公开的预训练 VLM（如 LLaVA、InstructBLIP、GPT‑4V 等），不进行额外微调。<br>- **Oracle**：使用更强的语言模型（例如 GPT‑4 或 Llama‑70B）作为分类器或语义等价判断器。<br>- **实验数据**：在论文正文中未提供详细数据集描述，附录 B 中列有用于内容安全、VLA 动作与语义聚类的图像‑提示对（来源于公开的 VLM 评测集），并对每个样本进行多次噪声采样。<br>- **计算资源**：实验基于多卡 GPU 集群执行 VLM 推理，oracle 的调用使用相同的 GPU 或 CPU 后端。由于模型已预训练，计算开销主要是多次前向推理（每张图像 \(n\) 次），具体硬件规格未在正文中给出。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：在标准机器学习服务器上运行（Python + PyTorch），所有 VLM 与 oracle 均通过相同的推理框架进行调用，实验代码已开源。<br>- **鲁棒性指标**<br> - **可验证半径 \(R\)**：在给定置信度 \(\alpha\) 下，对每个输入返回的最大半径。<br> - **认证精度**：在特定半径阈值（如 \(R=0.5\)）下，能够给出非“ABSTAIN”结果的样本比例。<br> - **半径下降比例**：通过标度律或实际测量比较不同样本数 \(n\) 下的平均半径 \(\bar R_\sigma(\alpha,n)\)。<br> - **Oracle 误差敏感性**：评估在不同假设的 \(\epsilon\)（oracle 错误率）下半径下界的变化。<br>- **对抗评估**<br> - 对最新的 jailbreak‑style 攻击（Qi et al., 2024）进行防御测试，统计攻击成功率的降低程度。<br> - 与未加噪声的原始 VLM 对比，展示 RS 增强后的安全性提升。<br>- **样本效率**<br> - 记录在不同样本规模（如 \(n=100, 500, 1000, 5000\)）下的认证半径、认证精度以及所需的计算时间。<br> - 通过经验曲线验证理论标度律 \(1-1.64z_\alpha/\sqrt n\) 的准确性。<br><br>以上内容概括了论文的研究单位、整体概述、主要贡献、方法细节、实验使用的数据与资源以及评估环境和指标。</details> |
| 2025-09-19 | CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine | http://arxiv.org/abs/2509.15968 | <details><summary>展开</summary># 论文研究单位<br>College of Transportation, Tongji University<br><br># 论文概述<br>提出 CoReVLA，一个面向长尾与安全关键场景的持续学习端到端自动驾驶框架，采用“收集-精炼(Collect-and-Refine)”双阶段流程：在基础阶段利用开源驾驶 QA 数据对 Qwen2.5-VL-7B 进行有监督微调(SFT)，获得基本的场景理解与决策能力；随后在 CAVE 沉浸式仿真平台中进行人机在环(HITL)测试，采集接管数据(包含历史图像、人类注意力、人类接管动作与模型错误行为)，并将其转化为偏好对；最后用直接偏好优化(DPO)进行行为精炼，使模型直接对齐人类偏好，避免手工奖励设计与奖励 hacking。<br><br># 论文核心贡献点<br>- 构建基于 CAVE 平台的人机在环接管数据采集流程，系统化获取长尾失败样本、人类注意力与接管行为，转化为高质量训练数据。<br>- 首次将 DPO 应用于自动驾驶长尾场景的行为精炼，利用稀疏接管数据进行偏好对齐，提升模型在高风险场景中的安全性与鲁棒性，避免奖励工程。<br>- 在开放问答与闭环驾驶双重任务上验证方法有效性；在 Bench2Drive 基准上实现 DS 72.18、SR 50%，较最佳基线提升 7.96 DS 与 15% SR，并展示跨平台泛化与持续学习能力。<br><br># 论文方法描述<br>- 预阶段 SFT：整合 LingoQA、BDD、HAD 为 70GB 驾驶 QA 数据；以 Qwen2.5-VL-7B 为基座，在视觉投影器与 LLM 主干上引入 LoRA 微调，训练目标为自回归交叉熵(公式 1)。<br>- Stage 1 接管数据采集：将模型部署于 CAVE 闭环仿真平台，实时与背景交通交互；当模型出现死锁或碰撞风险时切换回放模式，由安全驾驶员佩戴 VR 进行人工接管。记录历史图像序列、人类视觉注意、接管动作与模型错误行为，自动转化为 DPO 格式的三元组(x, y+, y-)。<br>- Stage 2 DPO 行为精炼：以人类偏好对训练，将策略分布建模为动作上的 softmax(公式 2)，最大化人类偏好概率(公式 3)，优化负对数似然损失(公式 4)；可选 KL 正则以约束策略漂移(公式 5)。相较 PPO 等 RLHF，DPO 无需显式奖励、可直接用离线人类接管数据，数据效率高，适合稀疏长尾事件。<br><br># 论文使用数据集和训练资源<br>- 数据集：70GB 融合数据集(LingoQA、BDD、HAD)，涵盖场景认知与安全驾驶策略；每条样本为五帧连续图像与链式思维(CoT)QA 对。<br>- 接管数据：CAVE 平台采集的人类接管三元组，用于 DPO 精炼。<br>- 训练资源与实现：基于 Qwen2.5-VL-7B，在视觉投影器与 LLM 主干上使用 LoRA 微调；所有代码、预处理数据与场景配置开源于 GitHub。<br><br># 论文使用的评估环境和评估指标<br>- 开放问答(open-loop)评估：在 LingoQA、BDD、HAD 上报告 BLEU 与 ROUGE(1/L)，对比 Qwen2.5-VL-7B、Llava-7B、LlavaNext-7B、Impromptu 等基线。<br>- 闭环驾驶(closed-loop)评估：在 CAVE 平台进行人机在环测试与接管采集；将精炼模型在 Bench2Drive 基准上与多种小规模任务特定模型与大规模预训练模型对比，指标包括：<br> - Driving Score(DS)<br> - Success Rate(SR)<br> - Efficiency<br> - Comfortness<br>- 性能结果：CoReVLA 在 Bench2Drive 取得 DS 72.18、SR 50%，较次优方法提升 7.96 DS 与 14.99% SR；案例研究显示 DPO 前后行为改进与跨平台(CAVE↔Bench2Drive)泛化。</details> |
| 2025-09-19 | A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning | http://arxiv.org/abs/2509.15937 | <details><summary>展开</summary># 论文研究单位<br>- 上海人工智能实验室（Shanghai AI Lab）<br><br># 论文概述<br>- 现实世界机器人强化学习（RL）通常依赖人工设计的稀疏奖励、探索低效、数据昂贵，导致泛化与稳定性受限。<br>- 提出VLAC（Vision-Language-Action-Critic）一体化模型，作为通用过程奖励模型（产生密集进度增量与完成信号），同时作为策略生成动作，统一“批评家（奖励/完成/价值）”与“演员（动作）”角色。<br>- 基于InternVL预训练于超4000小时多源数据（人类演示、机器人轨迹、VQA等），支持零样本与一次性上下文迁移、跨场景/跨任务泛化，并通过人类在环分级机制稳定早期学习与样本效率。<br>- 在四类真实操控任务上，200次真实交互内成功率由约30%提升至约90%；结合人类在环可进一步提升约50%样本效率，最高达100%最终成功率。<br><br># 论文核心贡献点<br>- 构建统一的VLAC模型：同一自回归架构交替生成奖励/完成信号与动作token，简化接口、降低奖励工程成本。<br>- 设计pairwise任务进度理解：输入两帧图像与语言目标，输出进度增量，天然作为TD奖励，并可判别退步/停滞；结合任务描述估计与完成判定辅助任务理解。<br>- 引入一次性上下文学习与样本构造：联合正负样本、跨采样描述、像素差过滤、任务描述与图像序列错配，显著提升跨场景/跨任务泛化与对失败轨迹的判别。<br>- 现实世界RL异步执行架构与动态推理调度：0.1秒内响应，修正时间戳匹配动作执行，确保连续性与低延迟；PPO基于结构化动作模板与价值头训练。<br>- 人类在环分级策略（离线演示回放、Return and Explore、Human Guided Explore）：稳定训练、加速探索、减少早期崩溃风险。<br>- 跨实体/跨视角/跨场景的大规模泛化评估与真实任务提升：四个现实操控任务成功率与样本效率显著改善，并支持多机器人扩展。<br><br># 论文方法描述<br>- 模型架构（基于InternVL的多模态大模型）<br> - 批评家（奖励/完成/价值）：pairwise进度学习、任务描述生成、任务完成判定；产生密集进度与done信号；价值头连接以供GAE。<br> - 演员（动作生成）：结构化动作模板自回归生成delta端位姿（x/y/z/roll/pitch/yaw/open）；记录数值token logits用于PPO；支持多样性采样。<br> - 上下文学习：参考轨迹与起始帧作为可选输入，提升迁移能力与绝对进度估计。<br>- 训练数据与构造<br> - 数据规模：超3000小时人类演示+1200小时公共机器人操控+15小时自收集；多VQA数据集（对话、机器人理解、空间推理、图像差异），总计约4000万数据点。<br> - 样本构造：像素差过滤（阈值1%）、正反向/细粒度与全局联合采样、任务完成正负配对、错配描述负样本，提升稳健性与判别能力。<br>- 现实世界RL框架<br> - 异步推理与动态分配：多机器人通过ZeroMQ与Ray连接；推理请求调度至空闲VLA副本；观察时间戳与动作时间戳对齐。<br> - PPO优化：结构化动作token化与概率记录；价值头预热；GAE估计优势；熵正则化促进探索；针对vllm与torch的概率漂移需重算训练阶段概率。<br> - 人类在环：专家演示回放（NLL损失）、针对性Reset的Return and Explore、人类指导演示补全回放，三级干预稳定学习与提升探索效率。<br><br># 论文使用数据集和训练资源<br>- 数据集与领域<br> - 人类/机器人操控：Ego4D HOD、AGIBOT、Bridge、Droid、FMB、RoboSet、自收集数据<br> - 多模态VQA：Llava、SpatialQA、RobotVQA、Spot the diff、InstructPix2Pix<br> - 跨域评测集：RT1、RoboNet、Dobb-E、RH20T、EgoDex、RoboFAC（成功/失败）<br>- 训练资源与实现<br> - 预训练：batch=3200，最大学习率8e-4<br> - 现实RL：2B VLAC为演员（策略），8B VLAC为批评家（奖励/完成/价值）；AGILE PiPER 7-DOF末端执行器（delta pose）；单帧观察含语言指令、前视相机图像、末端位姿<br> - 推理与训练：基于Ray与ZeroMQ；动态推理调度，推理端使用vllm或torch；训练端对vllm生成动作概率进行torch重算以稳定PPO；价值头在演示/早期探索样本上预热<br> - 机器人与系统：单控制器架构；动作时间戳相对观察滞后以匹配执行时间；推理响应目标<0.1秒<br><br># 论文使用的评估环境和评估指标<br>- 任务环境<br> - 真实机器人：AGILE PiPER；四类操控任务（开盖、取放碗、桌面清扫、大米转移等）<br> - 多机器人扩展与跨场景/跨实体评估（RT1、RoboNet、Dobb-E、EgoDex、RoboFAC等）<br>- 评估指标<br> - 批评家进度理解：VOC（值序相关）、VROC（反转序列一致性）、VOC-F1综合指标、NR（负进度比例）<br> - 演员动作与策略：任务进度（人工标注）、成功率（10次trial）、学习曲线（样本效率）<br> - 现实RL表现：成功率曲线、收敛速度、人类在环干预对样本效率的提升幅度<br>- 主要结果（摘要）<br> - 批评家：在Bridge/Droid等分布内与RT1/RoboNet/EgoDex/RoboFAC等分布外均取得高VOC-F1，并能清晰区分成功/失败轨迹；一次性上下文显著提升跨域性能<br> - 演员与RL：四任务平均成功率约75%，单任务最高达90%；200episode内由约30%提升至约90%；人类在环可再提升约50%样本效率，最终可达100%</details> |
| 2025-09-18 | RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation | http://arxiv.org/abs/2509.15212 | <details><summary>展开</summary>论文研究单位：阿里巴巴集团DAMO Academy（1]DAMO Academy, Alibaba Group）和Hupan Lab（2]Hupan Lab）。<br><br>论文概述：论文提出RynnVLA-001模型，旨在通过人类演示增强机器人操作能力。该模型采用两阶段预训练方法：首先利用1200万个自我中心人类操作视频训练图像到视频生成模型，预测未来帧；随后在人类关键点轨迹数据上联合学习帧和轨迹预测，桥接视觉与行动预测。同时引入ActionVAE编码行动序列为紧凑嵌入，降低VLA输出空间复杂度。最终在机器人数据集微调后，RynnVLA-001在拾取放置等任务上优于先进基线（如GR00T N1.5和Pi0），验证了预训练策略对VLA模型的有效性。<br><br>论文核心贡献点：<br>- 提出基于人类演示的视频生成预训练方法，缓解机器人操作数据稀缺问题。<br>- 设计两阶段预训练管道：自我中心视频生成预训练和人类中心轨迹感知视频建模。<br>- 引入ActionVAE优化行动表示，通过压缩行动序列提升预测平滑性和效率。<br>- 实验证明预训练权重能显著提升VLA模型在多任务场景下的成功率。<br><br>论文方法描述：<br>模型采用三阶段训练流程：<br>1. **Ego-Centric Video Generative Pretraining**：基于Chameleon架构的自回归变换器，以图像和语言指令为输入，交替排列视觉令牌和语言令牌，预测未来帧。训练数据包含1200万人类操作视频和244K机器人视频。<br>2. **Human-Centric Trajectory-Aware Video Modeling**：在EgoDex数据集上微调，联合预测未来帧和手腕关键点轨迹。使用ActionVAE压缩轨迹为连续嵌入，并引入状态嵌入表示当前手腕位置。<br>3. **Robot-Centric Vision-Language Action Modeling**：在SO100机器人数据上微调，输入双视角RGB和机器人状态，预测行动嵌入（由ActionVAE解码为行动序列）。推理时仅输出行动嵌入，跳过未来帧生成以提升实时性。<br><br>论文使用数据集和训练资源：<br>- **数据集**：<br> - 预训练：12M自我中心人类操作视频（网络源过滤）、244K机器人操作视频（BridgeData V2等）、EgoDex人类关键点数据。<br> - 微调：SO100机器人自收集数据集（3任务：拾取放置绿草莓249演示、拾取草莓248演示、插入笔架301演示；场景含单/多目标、干扰物）。<br>- **训练资源**：基于Chameleon代码扩展，使用大规模计算资源进行视频预训练和机器人微调。<br><br>论文使用的评估环境和评估指标：<br>- **评估环境**：LeRobot SO100机械臂，任务包括拾取放置绿草莓、拾取草莓、插入笔架；场景分为单目标、多目标和指令跟随（带干扰物）；测试覆盖多机械臂和不同环境。<br>- **评估指标**：<br> - 任务成功率（SR）：目标完成比例。<br> - Success Rate@1（SR@1）：单次试验成功率。<br> - 平均成功率：跨任务平均SR。<br> - 失败条件：超时、抓取失败超过5次、触碰干扰物。</details> |
| 2025-09-18 | Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue | http://arxiv.org/abs/2509.15061 | <details><summary>展开</summary>## 论文研究单位<br>复旦大学计算机科学与人工智能学院<br>上海创新研究院<br>加州大学伯克利分校机械系统控制实验室（UC Berkeley Mechanical Systems Control Lab）<br><br>## 论文概述<br>提出 Ask-to-Clarify 框架以解决真实环境中指令歧义性。框架首先通过多轮对话主动向用户提问澄清歧义，然后端到端生成低层动作执行具体任务。框架由协作组件（VLM）与动作组件（扩散专家）构成，并通过连接模块与两阶段“知识隔离”训练策略实现协作与动作生成能力的融合。在推理时通过信号检测器实现“提问/动作”模式的无缝切换。<br><br>## 论文核心贡献点<br>- 提出任务与框架：要求具身智能体先通过多轮问答澄清歧义，再执行指令；采用 VLM 协作与扩散动作专家的组合，并设计连接模块以在两者间建立可靠条件。<br>- 两阶段知识隔离训练策略：先学习歧义澄清与交互，再学习端到端低层动作；通过冻结 VLM 防止灾难性遗忘，同时用连接模块补偿 VLM 与扩散模型的联动。<br>- 真实世界实验与验证：在 8 项真实任务上对比多种 SOTA VLA，表明该框架显著提升在歧义指令与复杂场景下的成功率与鲁棒性。<br><br>## 论文方法描述<br>- 任务定义：对给定视觉观察与歧义指令，Agent 依序生成澄清问题并接收回答；若干轮后推断正确指令，然后生成低层动作序列完成任务。<br>- 框架结构<br> - 协作组件（VLM，Qwen2-VL-2B）：负责提问、判断歧义与推导正确指令。<br> - 连接模块（FiLM）：以语言指令为条件对视觉观察进行特征调制，为动作专家提供更具区分性的条件。<br> - 动作组件（扩散专家，ScaleDP-Huge）：端到端生成低层动作（动作块长度 50）。<br> - 信号检测器：解析 VLM 输出末端的信号token（<AMBG>、<NOT_AMBG>、<ACT>、<REJ>），以无训练方式路由“提问/动作”状态。<br>- 两阶段训练<br> - Stage 1：使用歧义交互对话数据训练协作组件；冻结视觉编码器，微调 LLM，新增信号token用于区分歧义、行动/拒绝等状态。<br> - Stage 2：冻结协作组件与视觉编码器，训练连接模块与扩散动作专家；采用专家演示的具身数据进行端到端动作生成。<br>- 推理流程<br> - 若指令被判为歧义则输出 <AMBG> 并生成问题；根据用户回答更新对话历史。<br> - 多轮后由 VLM 推断正确指令并标记 <NOT_AMBG>；随后由检测器触发 <ACT>（目标可见则执行）或 <REJ>（目标不可见则拒绝）。<br><br>## 论文使用数据集和训练资源<br>- 机器人与设备：xArm 7（7 DoF + 1 DoF 夹爪）、RealSense D435 相机（手腕与第三视角），xArm Python SDK；Stage 2 采用 Meta Quest 3 进行遥操作采集演示。<br>- 数据<br> - Stage 1（对话数据）：收集多对象图像并由 Qwen3-235B-A22B 生成歧义指令、问答对与正确指令，构成交互对话集。<br> - Stage 2（具身演示）：8 项任务，每项约 10 条演示。<br>- 实现细节<br> - 协作组件：Qwen2-VL-2B-Instruct<br> - 动作组件：ScaleDP-Huge（扩散专家）<br> - 训练超参数（示例）<br> - Stage 1：学习率 1e-5，批量 128，50 轮，训练参数约 1.5B<br> - Stage 2：学习率 2e-5，批量 64，40 轮，训练参数约 978M<br> - 动作块长度：50 步<br><br>## 论文使用的评估环境和评估指标<br>- 环境与任务：xArm 7 真实场景下 8 项任务，含三类通用任务：<br> - Put the Object on the plate（Apple/Peach/Orange）<br> - Pour the water from the Color cup onto the plate（Red/Green/White）<br> - Stack the Color1 block on top of the Color2 block（(Blue, Yellow)、(Yellow, Blue)）<br>- 指标：成功率（每任务 20 次试验）；与基线对比（π0、π0-FAST、OpenVLA-OFT）；消融（知识隔离策略、连接模块、必要性）。<br>- 附加评估<br> - 协作能力：在“目标存在/不存在”场景下判断并执行的正确率。<br> - 鲁棒性：低光照（灯光减半）、视觉干扰（相似干扰物）条件下的成功率。<br><br>结果表明：Ask-to-Clarify 在全部 8 项任务显著优于基线；在低光照与干扰条件下仍保持较高成功率；两阶段训练与连接模块二者缺一不可，共同保证歧义澄清与端到端动作生成的综合能力。</details> |
| 2025-09-18 | Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale | http://arxiv.org/abs/2509.14932 | <details><summary>展开</summary># Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale<br><br>## 论文研究单位<br>- Department of Computer Science & Artificial Intelligence, University of Technology Nuremberg, Germany<br>- Learning, Adaptive Systems and Robotics (LASR) Lab, Faculty of Computer Science, TU Dresden, Germany<br>- Siemens Foundational Technologies, Siemens AG, Germany<br>- Chair for Robotics, Artificial Intelligence and Real-Time Systems, TUM School of Computation, Information and Technology, Technical University of Munich, Germany<br><br>## 论文概述<br>论文提出了Robot Control Stack (RCS)，一个为大规模机器人学习设计的轻量级生态系统。随着Vision-Language-Action models (VLAs)的发展，传统机器人软件框架成为瓶颈，RCS旨在弥合这一差距。RCS采用模块化分层架构，为模拟和物理机器人提供统一接口，促进sim-to-real转换，在保持最小依赖和轻量级设计的同时，提供完整功能集，支持真实世界实验和大规模模拟训练。<br><br>## 论文核心贡献点<br>1. 提出基于环境包装器的RCS架构，支持在不同抽象级别轻松添加新功能，同时支持Python和C++<br>2. 在常见用例上评估RCS，包括跨实体支持、模拟和真实环境中的训练数据收集、VLA和RL智能体的训练和评估<br>3. 在可重现的取物任务上对Octo、OpenVLA和π₀进行广泛实验，涵盖多种不同机器人<br>4. 展示将合成数据与真实数据混合可以显著提升π₀在真实世界中的性能<br><br>## 论文方法描述<br>RCS基于环境包装器概念设计，通过包装器元组W=⟨f:S→S′,g:A′→A,P′,R′⟩将状态和动作从马尔可夫决策过程(MDP)进行转换。架构包含：<br>- C++底层接口定义抽象机器人控制函数，支持Python绑定<br>- 场景包装器序列，可变异或观察环境动作和观察空间<br>- 硬件抽象：标准化传感器和执行器接口，支持同步/异步操作<br>- 仿真集成：基于MuJoCo，提供面向对象的场景视图和回调机制<br>- 机器人工具包：集成Pinocchio进行运动学计算，OMPL用于运动规划<br>- 数字孪生：实时运行仿真作为安全检查器<br>- Agents应用层：通过RPC通信解决VLA策略依赖冲突<br><br>## 论文使用数据集和训练资源<br>- **真实数据集**：FR3 (143个演示)、xArm7 (100个演示)、UR5e (167个演示)、SO101 (120个演示)，均为30Hz频率收集<br>- **仿真数据集**：3000个脚本化演示，成功率73%，生成2193个成功演示<br>- **任务设置**：Pick-Cuboid任务，要求抓取绿色3D打印立方体<br>- **训练资源**：消费者级GPU笔记本电脑用于脚本化仿真数据生成，Nvidia RTX 4080 + 12核CPU用于RL训练<br><br>## 论文使用的评估环境和评估指标<br>**评估环境**：<br>- 四个真实机器人设置：FR3、xArm7、UR5e、SO101<br>- 匹配MuJoCo仿真环境，复制FR3设置<br>- 支持多种传感器：RealSense摄像头、DIGIT触觉传感器、Tacto触觉传感器<br><br>**评估指标**：<br>- **成功率**：50次真实世界 rollout 的抓取成功百分比<br>- **跨VLA模型比较**：在30Hz和5Hz操作下比较Octo、OpenVLA、π₀<br>- **Sim-to-Real评估**：使用SIMPLER方法评估真实到仿真的域转移<br>- **数据混合实验**：混合真实和仿真数据对性能的影响<br>- **RL评估**：训练3小时内(8.5M环境步骤)达到100%成功率，吞吐量>2000步/秒(24个并行环境)</details> |
| 2025-09-18 | CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human | http://arxiv.org/abs/2509.14889 | <details><summary>展开</summary># 论文研究单位<br>- 清华大学计算机科学与技术系（作者归属与资金支持单位）<br><br># 论文概述<br>- 提出 CollabVLA，一个自反思的视觉–语言–动作（VLA）框架，将标准视觉运动策略扩展为可与人“共同想象”与协作的代理<br>- 核心动机：改善现有 VLA 的领域过拟合、推理不可解释，以及依赖重生成模型导致的延迟问题<br>- 方案：通过在 MoE 适配下结合 VLM 的自反思语言推理与扩散式动作生成；在两阶段训练（动作基础化 + 反思调优）下，统一场景理解与动作生成，支持显式自反思并在不确定或失败时主动寻求人类指导<br>- 效果：相比依赖生成的智能体，CollabVLA 归一化“时间”约降低 2×，“梦境”次数约降低 4×，同时成功率更高、可解释性更好、延迟更低<br><br># 论文核心贡献点<br>- 系统化分析了直接自回归 VLA、潜在动作与世界模型三类路线的权衡，指出在执行期轻量级人类在环的缺失是一个被忽视的改进机会<br>- 引入 CollabVLA 框架：在单一骨架视觉运动策略上原生支持反思推理、动作生成与人类交互，通过 MoE 适配实现“反思/控制”的自适应切换<br>- 证明该方法能提升成功率与保持低延迟，并将自反思扩展为可触发的实时人类指导，实现更具稳健性的长尾任务表现<br><br># 论文方法描述<br>- 问题形式化：给定当前观察、过往帧、本体感受与多模态目标，策略输出动作块、反思文本与二值“是否询问人类”的标志<br>- 数据构造：两类互补语料<br> - 多模态目标预训练：融合 Interleave-VLA 的交错图文提示与 MDT 的目标图像增强，并附加 Diffusion-VLA 风格的简洁语言理由<br> - 反思强化调优：扩展 InstructVLA 流程，加入“上下文反思”任务，合成时序不一致、目标多选、动作/目标扰动等失败场景，以自然语言反思形式监督何时反思、如何诊断、如何修订<br>- 模型架构<br> - VLM 主干（InternVL2.5）：输入交错图文（含 [NOW]/[PAST] 图像、目标、人类提示标签、关节信息与可学习 [ACT] 查询）；输出反思字符串、二值询问指示与潜动作嵌入<br> - MoE 适配：在每层 Transformer 的 MHA/FFN 线性投影中插入 LoRA“控制专家”与“反思专家”，通过轻量门控基于局部 token 隐状态自适应选择专家<br> - 扩散式动作模型（DiT）：以潜动作 token 作为交叉注意键值记忆、以反思嵌入通过 FiLM 调制全层，迭代去噪生成低延迟、可行、推理一致的动作轨迹<br> - 推理流程：两阶段“反思–询问/执行”循环；二值头预测是否提问；人类回复作为新增条件再次前向；为降低延迟，支持首次出现 [ACT] 即停止自回归、并行解码剩余查询、缓存跨步记忆，并在新指导到来时用相似度加权融合平滑动作<br>- 训练管道（两阶段）<br> - 动作基础化（Stage 1）：冻结主干，仅激活控制 LoRA；联合优化语言规划损失与扩散去噪损失，隐式学习潜动作 token 作为动作模型的 conditioning<br> - 反思调优（Stage 2）：冻结主干与动作模型；联合训练控制/反思 LoRA、门控网络与询问头；目标函数为反思文本的交叉熵与询问标志的二分类交叉熵，确保不损失动作性能的同时获得可触发的人类交互能力<br><br># 论文使用数据集和训练资源<br>- 数据来源与合成<br> - 机器人/仿真操纵数据与真实世界演示：AgibotWorld、Simpler-3D 等<br> - 多模态预训练：交错图文提示（Interleave-VLA 风格）、目标帧采样（MDT/GR-MG 风格）、扩散式轨迹理由注入（Diffusion-VLA 风格）<br> - 反思强化调优：基于环境回放并由大语言模型生成反思答案，结合生成视觉状态与人工校验；插入无关/打乱帧诱导时序断裂、添加干扰对象诱发多义、扰动动作或目标模拟失败<br>- 预训练与模型规模<br> - 基于 InternVL2.5（约 4B 参数）的视觉语言主干<br> - MoE + LoRA 适配（控制与反思专家）<br> - DiT 动作生成器<br>- 训练与调优<br> - Stage 1：学习基础动作与轻量规划语言<br> - Stage 2：学习反思生成与询问决策，同时冻结主干与动作模型以保性能<br><br># 论文使用的评估环境和评估指标<br>- 评估维度<br> - 多模态理解：MMMU、MMStar、OCRBench、HallBench，以及 TextVQA、DocVQA、InfoVQA、RealWorldQA；额外 500 样本的 ContextReflection 集（从 AgibotWorld 与 GenieSim 保留）<br> - 仿真任务：在 Simpler 基础上扩展为“Simpler-Collab”评测，200 任务、8 类操纵任务，涵盖长时程控制与歧义消解，支持自动化的在环人类模拟<br> - 现实世界任务：DOBOT CR5（ROBOTIQ 夹爪）与 UR5（AG95 夹爪），五个类别，每类四个实例<br>- 指标<br> - 成功率（SR）与平均完成长度（LEN）<br> - 归一化时间（Time）：对每个任务进行分位裁剪并线性缩放后平均<br> - Dream 计数：生成代理的平均显式推理步数；协作变体记为人类询问次数<br>- 主要结果<br> - CollabVLA 在多数仿真子任务上获得最高的 SR 与 LEN，并保持最低的 Time 与 Dream（Time≈36/1.9Dream）<br> - 多模态理解对比显示 CollabVLA 相比常见 2B/4B VLA 在多个理解与 VQA 指标上保持或提升（如 MMMU、TextVQA、DocVQA、InfoVQA、RealWorldQA）<br> - 消融实验验证：去反思（No-Ref）、无 FiLM（No-FiLM）、无询问（No-Ask）、无 MoE（No-MoE）、仅 Stage1（No-Tuning）等设置均弱于完整模型<br>- 实现细节<br> - 仿真环境基于 ManiSkill3（SAPIEN）实现<br> - 对比基线：OpenVLA、ChatVLA、InstructVLA、ECoT、CoT-VLA、DiVLA、RoboDreamer、π0、UniVLA、MDT 等；为部分方法构建协作变体以公平对比<br> - 人类在环评估：人类问题与任务脚本共同输入 LLM 自动模拟人类回复以形成自动化 HITL 评测</details> |
| 2025-09-18 | RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI | http://arxiv.org/abs/2509.14687 | <details><summary>展开</summary># 论文研究单位<br><br>中兴通讯股份有限公司，中国；香港中文大学（深圳），中国<br><br># 论文概述<br><br>RealMirror是一个综合性、开源的具身AI视觉-语言-动作（VLA）平台，专门针对人形机器人VLA研究设计。该平台旨在解决当前具身AI研究中的关键瓶颈：数据采集成本高、缺乏标准化基准测试、以及仿真与现实世界之间的显著差距。RealMirror提供了一个端到端的解决方案，涵盖数据采集、模型训练、模型推理和性能评估的完整流程。<br><br># 论文核心贡献点<br><br>1. 构建了一个高效、低成本的数据采集、模型训练和模型推理系统，实现了端到端VLA研究，无需真实机器人参与<br><br>2. 提出专门针对人形机器人的VLA基准测试，通过多场景和多种VLA模型的广泛实验促进模型演进和公平比较<br><br>3. 通过集成生成模型和3D高斯溅射技术，展示了零样本Sim2Real的可行性，使仅在仿真数据上训练的模型能够在真实机器人上无缝执行任务，无需任何微调<br><br># 论文方法描述<br><br>## 物理仿真场景构建<br>基于NVIDIA Isaac Sim平台构建多样化的室内仿真环境，整合CAD模型和来自各种资产库的资产，分配适当的物理属性（质量、摩擦、碰撞参数等），确保仿真中的合理性和人形机器人实体的兼容性。<br><br>## 数据采集系统<br>开发了基于远程操作的数据采集系统，包含两个主要组件：<br>- 运动控制管线：实现多级滤波机制，包括IK关节控制跳跃滤波、末端执行器位姿通信和漂移补偿、IK求解器阈值滤波、跨帧末端执行器位姿阈值滤波<br>- 轻量级WebXR通信系统：实现90 Hz传输频率，与通用通信框架相比减少了114毫秒的端到端延迟<br><br>## 统一训练和推理框架<br>支持多种代表性VLA模型（ACT、Diffusion Policy、SmolVLA），集成时间集成机制以增强动作预测稳健性，基于LeRobot库扩展以支持人形机器人实体，与Isaac Sim深度集成实现交互式评估。<br><br>## Sim2Real转换框架<br>采用多管齐下的策略：<br>- 静态环境渲染：使用3D高斯溅射从多个视角捕获目标真实世界工作空间，重建整个静态场景<br>- 高保真关节机器人模型：使用3D高斯溅射重建物理人形机器人并分割为单独连杆，通过S、R、T变换与Isaac Sim中的USD模型对齐<br>- 交互对象差异化处理：高精度对象采用数字孪生方法，低精度对象使用少样本3D生成模型<br>- 坐标系对齐和相机校准：使用ICP算法对齐CAD资产与3DGS重建环境，通过SfM解决相机位姿<br><br># 论文使用数据集和训练资源<br><br>## 数据集构成<br>构建了高质量的人形机器人VLA数据集，包含五个任务场景，每个场景240条轨迹，总计超过1200条仿真轨迹：<br>- Kitchen Cleanup（厨房清理）：Pick and Place、双臂协作<br>- Air Fryer Manipulation（空气炸锅操作）：Pick and Place、推拉、双臂协作<br>- Assembly Line Sorting（装配线分拣）：Pick and Place、双臂协作、动态抓取<br>- Cup-to-Cup Transfer（杯间转移）：双臂协作、精密控制<br>- Can Stacking（易拉罐叠放）：Pick and Place、精密控制<br><br>## 训练资源<br>- 硬件：PICO Neo3 Pro头戴设备和Ada5880工作站<br>- 训练参数：每个模型训练100,000步，批次大小为16<br>- 统一动作空间：26维（每条机械臂13维：7维机械臂+6维手部）<br><br># 论文使用的评估环境和评估指标<br><br>## 评估环境<br>- 仿真环境：基于Isaac Sim的交互式评估环境<br>- 真实环境：ZHIYUAN A2机器人进行Sim2Real实验验证<br><br>## 评估指标<br>- 主要指标：任务成功率（Task Success Rate）<br>- 评估规模：Kitchen Cleanup (400次试验)、Air Fryer Manipulation (400次试验)、Can Stacking (400次试验)、Cup-to-Cup Transfer (200次试验)、Assembly Line Sorting (100次试验)<br>- 技能评估：从任务中抽象出五个核心机器人技能进行细粒度分析<br>- Sim2Real评估：基本任务（拾取和放置）达到92.86%准确率，复杂任务（球体转移）达到71.43%准确率，无需微调即可实现真实世界部署<br><br>## 实验结果<br>三个代表性VLA模型在基准测试中的表现：<br>- ACT：平均成功率73.55%，在Kitchen Cleanup和Assembly Line Sorting表现突出<br>- Diffusion Policy：平均成功率75.15%，在Air Fryer Manipulation表现最佳<br>- SmolVLA：平均成功率79.75%，整体表现最为稳健，特别是在精密控制任务中</details> |
| 2025-09-17 | CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping | http://arxiv.org/abs/2509.14143 | <details><summary>展开</summary># 论文研究单位<br>德雷塞尔大学电气与计算机工程系；弗吉尼亚理工大学海产农业研究与推广中心与生物系统工程系<br><br># 论文概述<br>提出CLAW（CLIP-Language-Action for Weight）框架，用于“重量感知”的机器人抓取。核心思想是将条件评估从动作生成中解耦：由轻量的微调CLIP作为提示生成器，实时监测电子秤读数并产生离散语言提示（continue/stop）；π₀ 作为流匹配VLA策略接收多视角视觉与语言提示，生成连续控制信号。验证表明CLAW在单物体与混合物体场景中均可可靠执行重量约束抓取，并优于原始与仅微调的π₀。<br><br># 论文核心贡献点<br>- 引入CLAW：在标准VLA之上增加任务专用VLM用于显式条件监控，实现重量感知操控。<br>- 设计CLIP微调：将电子秤数字显示转换为可被VLA理解的离散提示。<br>- 使用提示监督微调π₀：使其能融合CLIP提示与多视角观测，产生精确动作。<br>- 跨单物体与混合任务（双臂）场景评估，显示鲁棒性与一致尊重重量阈值；对比基线（原始π₀与仅微调π₀）显著提升。<br><br># 论文方法描述<br>- 整体架构<br> - 输入：人类指令（指定对象与目标重量）、电子秤图像、场景多视角图像。<br> - 模块1（CLIP）：对电子秤图像与指令生成二元提示 m_t ∈ {continue, stop}，参数化为 p_φ(m_t \| o_t^scale, l)。<br> - 模块2（π₀）：对场景图像与提示生成连续动作 a_t，参数化为 p_θ(a_t \| o_t^scene, m_t)，采用流匹配生成30Hz控制（50步动作块）。<br> - 频率：CLIP以20Hz更新，π₀以30Hz控制；当某步CLIP未更新时复用最近提示。<br>- 训练流程<br> - CLIP微调：采集2000张秤显示裁剪图；每张与N个“load k g target”指令配对，标签由真实读数 w* 与阈值k比较得到（y=continue当k<w*，否则y=stop），共2000N样本，最小化分类损失。<br> - π₀微调：每任务收集50条演示（抓取与撤碗两阶段）；在演示中人工标注clip_prompt（抓取阶段为“continue…”，撤碗阶段为“stop…”），以流匹配损失最小化 \|\|v_θ(x(t),t,o_t^scene,m_t)−(a_t−x(t))\|\|^2，在H200上训练60,000步。<br>- 推理运行<br> - CLIP实时监测秤并输出continue/stop提示；π₀据提示与场景多视角生成动作块；重量达标后提示切换为“stop”，π₀执行撤碗。<br><br># 论文使用数据集和训练资源<br>- 数据集<br> - CLIP微调：2000张电子秤显示裁剪图，每图与N个阈值指令配对，生成2000N条continue/stop样本。<br> - π₀微调：每任务50条演示，覆盖抓取与撤碗阶段；演示中帧级标注“clip_prompt”。<br>- 训练资源<br> - π₀微调：H200 GPU；60,000步；控制30Hz；CLIP提示更新20Hz。<br> - CLIP微调：硬件未详述。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境<br> - 单物体：桌面设含装目标物的篮子、电子秤与空碗；任务为“抓取至目标重量并撤碗”，覆盖糖果（20/30/40g）与大蒜（20/30/40g）。<br> - 混合物体：左右各置一盒（糖果/大蒜），左右臂分工（抓取与撤碗不同对象），评估跨对象与双臂协调。<br> - 鲁棒性测试：在抓取中途投加过量物体，使秤瞬时超过阈值，检验系统是否能即时切换为撤碗并中断未完成动作。<br>- 评估指标<br> - 成功率（20次试验）：分别统计“动作完成”（抓取与撤碗）与“停止点准确”（达到指定重量阈值即停止），对比原始π₀、仅微调π₀与CLAW。</details> |
| 2025-09-17 | SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model | http://arxiv.org/abs/2509.14138 | <details><summary>展开</summary>### 论文研究单位<br>- **Virginia Seafood Agricultural Research and Extension Center, and Department of Biological Systems Engineering, Virginia Tech, USA**<br>- **Department of Electrical and Computer Engineering, Drexel University, USA**<br><br>### 论文概述<br>长时序机器人操作任务要求执行多个相互依赖的子任务，错误检测机制可能引发级联失效。现有视觉-语言-动作（VLA）模型（如π₀）在连续低级控制方面表现出色，但缺乏识别子任务完成的内部信号，在顺序设置中表现脆弱。本研究提出SeqVLA，它是π₀的完成感知扩展，通过添加轻量级检测头感知当前子任务是否完成。该双头设计使模型不仅生成操作动作，还能自主触发子任务转换。研究调研了四种微调策略（联合或顺序微调，冻结或不冻结预训练主干网）。实验在沙拉包装（七个连续子任务）和糖果包装（四个子任务）上进行。结果显示SeqVLA显著提升整体成功率，其中联合微调并冻结主干网策略表现最佳，消除了顺序相关失效。<br><br>### 论文核心贡献点<br>- 集成学习任务完成检测头到π₀模型，实现从多模态上下文中推断子任务完成。<br>- 识别最有效的微调策略：联合微调并冻结主干网，确保可靠顺序执行。<br>- 在两个实际长时序场景中评估，框架在任务级性能上显著优于强基线。<br><br>### 论文方法描述<br>- **问题建模**：长时序任务表示为顺序子任务序列$\mathcal{T}=\{\mathcal{T}_{1}, \mathcal{T}_{2}, ..., \mathcal{T}_{n}\}$，子任务$\mathcal{T}_{i}$的完成是启动$\mathcal{T}_{i+1}$的先决条件。<br>- **架构**：SeqVLA扩展π₀架构（基于SigLIP视觉编码器、Gemma-2B语言主干网和Gemma-300M动作专家），添加共享动作专家特征的轻量级完成检测头（线性分类器），输出子任务完成概率$p$：<br> - $p = \sigma(\textbf{W} \cdot \textbf{F} + b)$，其中$\textbf{W} \in \mathbb{R}^{1024}$, $b \in \mathbb{R}$为参数，$\sigma$为sigmoid函数。<br> - 总损失函数：$L_{\text{total}} = L_{\text{action}} + \lambda \cdot L_{\text{completion}}$，$L_{\text{completion}}$为二分类交叉熵损失，权重$\lambda=0.1$。<br>- **微调策略**：<br> - **联合微调**：动作和分类头同时优化。<br> - **顺序微调**：先训练动作头和主干网，后训练分类头。<br> - **冻结策略**：冻结预训练VLM主干网以保留原始知识，或全微调适应域。<br> - 组合为四种配置：SeqVLA-J（联合微调，不冻结）、SeqVLA-JF（联合微调，冻结）、SeqVLA-S（顺序微调，不冻结）、SeqVLA-SF（顺序微调，冻结）。<br>- **子任务执行**：每个推理步骤输出动作块和执行概率$p$。当$p < \tau = 0.2$时触发转换：停止当前动作、回程家姿态、切换到下一子任务提示。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - **沙拉包装**：七个连续子任务（菠菜、卷心菜、肉丸、鸡、西红柿、酱料杯、容器关闭），收集350集子任务演示数据。<br> - **糖果包装**：四个子任务（软糖、两次Kinder巧克力、两次士力架、棒棒糖），收集200集子任务演示数据。<br> - 附加长时序演示数据：完整沙拉和糖果任务轨迹用于π₀基线微调。<br>- **数据收集**：使用Aloha双机械臂（14自由度）进行示教，三摄像头（顶视、左抓手、右抓手）记录RGB图像和机器人状态。<br>- **训练资源**：<br> - 基于π₀预训练模型（物理智能版），硬件为Aloha机器人。<br> - 训练环境涉及实时数据采集和微调流程，具体计算资源未详述。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 机器人：Aloha双机械臂（14自由度），三摄像头视角。<br> - 任务：沙拉包装（七子任务）和糖果包装（四子任务）的长时序执行。<br> - 基线：π₀模型在完整长时序演示上微调，直接执行无子任务监控。<br>- **评估指标**：<br> - **成功率**：整体任务和子任务级成功率（如图表显示）。<br> - **分类置信度**：使用熵值衡量完成检测的不确定性（SeqVLA-J熵0.76 vs SeqVLA-S熵1.35）。<br> - **统计可靠性**：Kolmogorov–Smirnov（KS）统计量评估执行和完成阶段分布差异（KS值0.75–0.85，p < 0.001）。<br> - **行为比较**：通过执行记录图（图8、9）定性分析π₀顺序失效vs SeqVLA可靠性。</details> |
| 2025-09-17 | GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model | http://arxiv.org/abs/2509.14117 | <details><summary>展开</summary># 论文研究单位<br><br>Mohamed bin Zayed University of Artificial Intelligence, Department of Robotics, Masdar City, Abu Dhabi, UAE<br><br># 论文概述<br><br>论文针对Vision-Language-Action (VLA)模型在面对新颖相机视角时泛化能力不足的问题，提出了一种名为GeoAware-VLA的方法。该方法通过将标准VLA架构中的视觉编码器替换为预训练的视觉几何基础模型VGGT（Visual Geometry Grounded Transformer），显著提升了模型在新视角下的零样本泛化能力。<br><br># 论文核心贡献点<br><br>1. 提出GeoAware-VLA方法，有效集成视觉几何基础模型到VLA架构中<br>2. 在新视角下实现成功率翻倍提升，在仿真环境中表现优异<br>3. 在真实机器人平台上验证了方法的实用性，特别是在新颖相机角度下的性能提升<br>4. 确认了该方法在不同策略解码器上的一致改进效果<br><br># 论文方法描述<br><br>GeoAware-VLA的核心在于使用冻结的预训练几何感知视觉编码器替代标准的可训练视觉编码器。具体包括：<br><br>1. **几何感知视觉编码器**：采用VGGT作为冻结的特征提取器，利用其多层中间层特征捕获视觉和几何信息<br>2. **视觉投影层**：通过1D卷积网络、池化操作和MLP将VGGT特征转换为统一表示空间<br>3. **多模态编码**：语言和本体感受编码器使用简单的MLP投影器<br>4. **策略解码器**：基于GPT风格的解码器transformer处理多模态输入序列<br>5. **动作生成头**：提供两种变体 - MLP头用于连续动作回归，VQ-BeT头用于建模多模态行为<br><br># 论文使用数据集和训练资源<br><br>1. **数据集**：使用LIBERO基准测试的四个任务套件进行评估<br> - LIBERO-Spatial：测试空间布局泛化<br> - LIBERO-Goal：测试新任务泛化<br> - LIBERO-Object：测试新对象类型泛化<br> - LIBERO-Long：测试更广泛的对象、布局和背景变化<br><br>2. **训练资源**：使用PyTorch框架，在单个NVIDIA A100 GPU（40GB VRAM）上训练，每个模型训练150,000步（仿真）或50,000步（真实世界）<br><br># 论文使用的评估环境和评估指标<br><br>1. **仿真评估环境**：<br> - LIBERO基准测试，包含多样化的操控任务<br> - 在训练视角和新颖视角下进行测试<br> - 包含小、中、大三种视角偏移程度的评估<br><br>2. **真实世界评估环境**：<br> - Realman 65B机械臂<br> - 桌面操控环境，配备两个固定视角相机<br> - 5项不同的操控任务测试<br><br>3. **评估指标**：<br> - 主要指标：成功率（基于10次试验的平均值）<br> - 视角鲁棒性：跨不同视角偏移程度的性能变化<br> - 泛化能力：新视角下的零样本性能表现</details> |
| 2025-09-17 | Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach | http://arxiv.org/abs/2509.13774 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br><br>北京小米机器人技术有限公司（Beijing Xiaomi Robot Technology Co., Ltd.）和香港城市大学（City University of Hong Kong）<br><br>## 论文概述<br><br>本文提出了一种基于人机交互的双执行器（dual-actor）VLA模型微调框架。该框架集成了一个主执行器用于鲁棒的多任务性能，以及一个精调执行器在潜在空间中进行自适应调整。除了标准的物理干预外，论文引入了一种轻量级的talk-and-tweak方案，将人类修正转换为语义化的语言命令，从而生成新的策略学习数据集。在真实世界多任务实验中，该方法在101分钟的在线微调内在三个任务上实现了100%的成功率。对于长视距任务，在超过12个连续操作中保持了50%的成功率。该框架还能有效扩展到多机器人训练，使用双机器人时效率提升可达2倍。<br><br>## 论文核心贡献点<br><br>1. 提出了一种新颖的双执行器VLA微调框架，集成了用于鲁棒多任务策略生成的主执行器和在潜在噪声空间中操作以实现细粒度可控动作调整的精调执行器<br><br>2. 开发了一种轻量级人机交互方案，将实时物理修正（tweak）转换为语义化的语言命令（talk），形成talk-and-tweak数据集。精调执行器利用这些指令进行可解释的调整，而主执行器通过直接干预改进其基线策略<br><br>3. 在真实机器人上验证了所提方法，展示了快速的多任务微调能力（101分钟内达到100%成功率）。对于长视距序列，在12个连续操作中保持50%成功率。此外，框架可无缝扩展到多机器人训练，使用双机器人训练时效率提升达2倍<br><br>## 论文方法描述<br><br>方法分为三个主要部分：<br><br>1. 双执行器强化学习系统：包含预训练的VLA模型进行任务和状态编码，主执行器采用一致性策略（consistency policy）生成动作，精调执行器在潜在噪声空间中进行调整。训练分为离线预热阶段和在线交互阶段，主执行器通过混合目标（行为克隆+Q函数最大化）优化，精调执行器通过行为克隆、Q函数最大化和正则化损失联合训练<br><br>2. Talk-and-Tweak人类干预设计：将人类物理修正（通过SpaceMouse）自动转换为自然语言精调命令。通过计算时间窗口内的累积位移，当超过阈值时生成相应的方向命令（如"向右移动"），形成包含（状态、动作、精调命令）的三元组数据集<br><br>3. 高效多任务学习：采用共享的多任务执行器和任务特定的critic架构。为每个任务维护三个独立缓冲区（专家演示、策略rollouts、人类干预）。在在线阶段，主执行器从所有任务和缓冲区类型中均匀采样更新，精调执行器使用聚合的干预数据集训练，每个任务特定的critic仅使用其对应任务缓冲区的数据更新。引入任务权重机制平衡多任务学习进度<br><br>## 论文使用数据集和训练资源<br><br>数据集：使用Octo作为骨干VLA模型。预热阶段每个任务使用20条轨迹初始化策略（约3000个状态-动作对）。在线微调阶段在三个任务中收集约15000个交互对（每个任务约100条轨迹）。人类干预数据约占15%，用于训练精调策略<br><br>硬件和训练资源：使用自主开发的7自由度机械臂。观察包括两个RGB图像（手腕相机128×128分辨率，头部相机256×256分辨率）和机械臂本体感知状态。动作空间为7维末端执行器增量位姿。数据采集和策略执行频率为10Hz。执行器进程运行在机器人上的NVIDIA Jetson Orin，学习器在配备NVIDIA RTX 3090 GPU的工作站上执行<br><br>## 论文使用的评估环境和评估指标<br><br>评估环境：真实物理环境中的螺栓操作任务，包括三个子任务：(1)将螺栓竖直放置，(2)拾取螺栓，(3)组装螺栓。机器人和物体位置在x-y轴上均匀随机化±5厘米以增强泛化性<br><br>评估指标：<br>1. 成功率：每个子任务如果在50个时间步内完成则视为成功，超过此限制计为失败。每个子任务独立评估成功率<br>2. 片段长度：完成任务所需的时间步数<br>3. 长视距任务评估：完全成功需要机器人顺序完成所有三个子任务<br>4. 所有结果均为25次试验的平均值<br>5. 多机器人扩展性：评估在不同机器人和训练配置下的训练效率和任务性能</details> |
| 2025-09-17 | AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving | http://arxiv.org/abs/2509.13769 | <details><summary>展开</summary># 论文研究单位<br>清华大学（Tsinghua University）、小米EV（Xiaomi EV）、澳门大学（University of Macau）、南洋理工大学（Nanyang Technological University）、北京大学（Peking University）<br><br># 论文概述<br>AdaThinkDrive提出一种“快速回答/慢速思考”的双模推理机制，解决在端到端自动驾驶中思维链（CoT）在简单场景下过度推理、复杂场景下推理收益不一致的问题。论文在Navsim基准上进行实验与评估。<br><br># 论文核心贡献点<br>- 通过在Navsim上系统性对比发现：简单场景下使用CoT收益有限甚至有害，复杂场景下CoT收益显著。由此提出“按场景复杂度自适应推理”的必要性。<br>- 设计AdaThinkDrive框架：三阶段训练范式——预训练获得驾驶常识与知识、两阶段SFT学习双模输出能力、基于GRPO的强化学习通过奖励信号学习“何时思考与何时直接作答”。<br>- 提出“Adaptive Think Reward”策略：通过多 rollout 采样，比较同一场景下Thinking与Non-thinking轨迹质量（PDMS），动态奖惩并更新场景复杂度标签，避免对固定人工标签的依赖。<br>- 在Navsim达成SOTA：PDMS 90.3（仅视觉输入），比最佳纯视觉基线提升1.7；同时在复杂场景中优先使用思考模式（96%），在简单场景中优先直接输出（84%）；相对Always-Think推理时间降低14%。<br><br># 论文方法描述<br>- 问题建模：以联合分布P(m, o \| q)建模对查询q选择模式m（Thinking/Non-thinking）与输出轨迹o的决策；通过最大化期望任务效用U(q, o)选择最优模式。<br>- 数据准备：<br> - 预训练数据：DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive等开源驾驶QA数据，用于获得基础驾驶常识。<br> - 混合SFT数据：为同一查询生成Thinking（含完整推理轨迹）与Non-thinking（省略推理仅含轨迹）两种输出；场景注释通过Qwen2.5-VL-72B自动生成，包含交通灯状态、天气与道路边界等；动态交互对象分为CIPO-1（本车道障碍物）、CIPO-2（可能并线）、Motion Interaction（未来轨迹与自车相交）。<br> - 场景分类：将Navsim训练与验证集按“是否靠近道路边界/是否有关键物体”划分为Level 1（简单，无条件）、Level 2（有单一条件）、Level 3（复杂，同时满足两个条件），并映射为D+（Level 2&3）与D-（Level 1）作为强化学习初始辅助标签。<br>- 两阶段SFT：<br> - 阶段一：在大规模驾驶QA数据上微调，提升场景理解与驾驶语义认知能力。<br> - 阶段二：在Navsim规划数据（含Thinking/Non-thinking双模输出）上微调，使模型具备统一接口生成两种输出风格且不偏向其一。<br>- 自适应思考强化学习（基于GRPO）：<br> - 奖励设计：<br> - PDMS Reward：基于轨迹质量评价PDMS。<br> - Format Reward：强制输出遵循<think>、<answer>标签与轨迹格式规范。<br> - Endpoint Reward：对轨迹终点误差进行分段奖励（<2m=1.0；<4m=0.8；<6m=0.6；<10m=0.4；<15m=0.2；否则=0）。<br> - Adaptive Think Reward：依据多rollout比较Thinking与Non-thinking的平均PDMS与样本数，在D=0/D=1的当前标签下动态奖惩；当满足阈值T与条件时允许修正场景复杂度标签为“challenging/simple”。<br> - 优化目标：采用GRPO优化，引入截断重要性权重与KL正则防止策略偏移。<br><br># 论文使用数据集和训练资源<br>- 数据集：Navsim（主评估）、DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive（用于预训练与知识注入）<br>- 模型：InternVL3-8B作为基础视觉-语言模型<br>- 训练资源：<br> - 阶段一：学习率1e-5，批量1，训练2个epoch<br> - 阶段二：学习率4e-5，批量2，训练2个epoch<br> - 阶段三（RL）：学习率2e-6，批量4，使用64×NVIDIA H20 GPU进行2个epoch<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：Navsim非反应式仿真，OpenScene平台支持<br>- 评估指标：<br> - PDMS（Predictive Driver Model Score）：综合评分，融合NC、DAC、TTC、CF、EP<br> - 推理时间：预测4秒轨迹的平均推理时间（Navsim测试集）<br> - 场景分层：按Level 1/2/3的Think vs Non-think选择比例与性能表现<br>- 定量结果（主要指标）：<br> - AdaThinkDrive（仅视觉）：PDMS 90.3，NC 98.4、DAC 97.8、TTC 95.2、CF 100、EP 84.4<br> - Best-of-N：PDMS 93.0<br> - 相对Non-Think RL：PDMS +2.0；推理时间 Non-Think RL 0.68s、Think RL 0.86s、AdaThinkDrive 0.74s<br> - 相对Always-Think：推理时间减少14%</details> |
| 2025-09-13 | OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft | http://arxiv.org/abs/2509.13347 | <details><summary>展开</summary>待生成</details> |
| 2025-09-16 | The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning | http://arxiv.org/abs/2509.12594 | <details><summary>展开</summary># 论文研究单位<br>- LiAuto Inc.（理想汽车股份有限公司）<br>- 清华大学车辆与运载学院<br>- 中国科学院计算技术研究所<br><br># 论文概述<br>论文针对视觉语言动作（VLA）模型在资源受限平台（如边缘设备）上的部署瓶颈问题，提出了LightVLA框架，通过自适应剪枝视觉token来提高计算效率同时保持或提升性能。VLA模型在机器人任务中表现出色，但受限于大量视觉token的注意力计算成本。LightVLA通过动态生成查询评估token重要性，并使用Gumbel-softmax实现可微token选择，从而在优化过程中自动保留关键token。实验在LIBERO基准上验证其效果。<br><br># 论文核心贡献点<br>- 提出LightVLA，一个性能驱动的可微视觉token剪枝框架，无需额外超参数或可训练参数。<br>- 证明VLA模型的效率和性能可协同优化，剪枝同时减少计算开销并提升成功率。<br>- 在LIBERO基准上实现最先进性能：成功率提升2.6%，FLOPs减少59.1%，延迟降低38.2%。<br>- 提出LightVLA*变体，使用可学习查询（额外参数）探索token剪枝的可能性。<br>- 首次将自适应视觉token剪枝应用于VLA任务，突破效率-性能权衡。<br><br># 论文方法描述<br>LightVLA方法包括三个核心步骤：<br>1. **查询生成**：通过视觉和语言token的交叉注意力生成动态查询（参数无关），以评估token对任务的重要性。<br>2. **Token评分**：计算每个查询对所有视觉token的评分矩阵（S），评分反映token的贡献度。<br>3. **Token选择**：使用Gumbel-softmax采样技术使argmax操作可微，将评分矩阵转换为指示矩阵（I），选择高分token组成剪枝集（H'_v）。<br>训练中，Gumbel噪声强度随时间衰减，促进多样性选择和稳定性；推理时直接使用argmax。保留CLS token和位置信息以维持全局信息。LightVLA*在视觉编码器或LLM早期层引入可学习查询（Q*）和层归一化参数，适用于更复杂场景。<br><br># 论文使用数据集和训练资源<br>- **数据集**：LIBERO基准测试，包含四个任务集（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个任务集提供500个专家演示（总计2000演示）。测试每任务集50次试验（500试验）。<br>- **训练资源**：<br> - 基础模型：OpenVLA-OFT（双分支视觉编码器DINOv2和SigLIP、LLaMA-2-7B LLM、LoRA rank 32微调）。<br> - 微调设置：40000梯度步，初始学习率5e-4（30000步后降至5e-5），批大小每设备8（全局64）。<br> - 硬件：8个Nvidia H20 GPUs。<br><br># 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO仿真环境，使用Franka Emika Panda机械臂，集成相机图像、机器人状态和任务指令。<br>- **评估指标**：<br> - 主要指标：任务成功率（SR, %），分任务集报告。<br> - 效率指标：FLOPs（总计算量）、端到端延迟（毫秒）、平均保留视觉token数。<br> - 对比基线：包括其他VLA模型（如OpenVLA、π_0）和token剪枝方法（如FastV、SparseVLM）。</details> |
| 2025-09-15 | TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning | http://arxiv.org/abs/2509.11839 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>浙江大学、西湖大学、上海交通大学、上海创新研究院<br><br>## 论文概述<br>当前视觉-语言-动作（VLA）模型在跨实体泛化方面显示出潜力，但在高质量演示稀缺时，特别是对于双足人形机器人，难以快速对齐新的机器人动作空间。本文提出了TrajBooster，一个跨实体框架，利用丰富的轮式人形机器人数据来提升双足VLA性能。<br><br>## 论文核心贡献点<br>1. 首次利用重定向动作数据微调VLA模型并实现双足人形机器人全身操作的真实世界应用<br>2. 提出了TrajBooster真实到仿真到真实的跨实体框架，使用末端执行器轨迹作为形态不可知的信号<br>3. 在Unitree G1上仅需10分钟遥操作数据收集，就能实现超越桌面的家庭任务，包括蹲下、跨高度操作和协调的全身运动<br><br>## 论文方法描述<br>### 1. 真实轨迹提取<br>从Agibot-World beta数据集中提取6D双臂末端执行器轨迹，并将其从Agibot数据映射到Unitree官方G1操作数据集，解决工作空间差异问题。<br><br>### 2. 仿真中的重定向<br>设计复合层次化模型用于全身操作重定向，包括：<br>- **手臂策略**：使用闭环逆运动学（CLIK）计算目标关节角度<br>- **工作者策略**：基于目标的条件强化学习策略，输出12-DoF下肢目标关节位置<br>- **管理者策略**：从手腕姿态生成下肢命令（基座速度命令和躯干高度）<br><br>采用启发式增强的调和在线DAgger算法进行训练。<br><br>### 3. 双步后训练<br>- **后预训练**：使用重定向的动作-视觉-语言三元组数据对预训练GR00T N1.5模型进行后预训练<br>- **后训练**：使用收集的遥操作数据进行微调<br><br>## 论文使用数据集和训练资源<br>- **源数据**：Agibot-World beta数据集，包含超过一百万条真实机器人轨迹<br>- **目标数据**：28个真实世界全身操作数据集片段，涵盖四种不同高度配置<br>- **训练资源**：<br> - 重定向模型训练：512个并行环境，200次训练迭代，单个RTX 4090 GPU<br> - 后预训练：双A100 80GB GPU，批次大小=128，60K步数<br> - 后训练：单个A100 GPU，批次大小=16，3K步数<br><br>## 论文使用的评估环境和评估指标<br>### 评估环境<br>- **仿真评估**：在Isaac Gym中评估重定向模型性能<br>- **真实世界评估**：在Unitree G1人形机器人上进行四个遥操作任务评估<br><br>### 评估指标<br>- **轨迹跟踪精度**：位置误差E_p（厘米）和旋转误差E_r（度）<br>- **任务成功率**：在十个试验中完成任务的百分比<br>- **轨迹泛化能力**：使用FastDTW算法计算生成轨迹与真实遥操作数据的相似性<br>- **零样本技能泛化**：评估未见任务的执行能力</details> |
| 2025-09-15 | Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs | http://arxiv.org/abs/2509.11480 | <details><summary>展开</summary>待生成</details> |
| 2025-09-14 | Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations | http://arxiv.org/abs/2509.11417 | <details><summary>展开</summary># 论文研究单位<br>- UC San Diego（加州大学圣地亚哥分校）<br>- Hillbot<br><br># 论文概述<br>VLA模型往往通过直接微调预训练的VLM获取机器人控制能力，但会导致预训练表征严重退化，在视觉背景变化与指令同义改写下性能显著下降。本文提出保留预训练表征的VLA训练框架，在仿真与真实机器人上均显著提升稳健性与泛化。<br><br># 论文核心贡献点<br>- 部分冻结的双视觉编码器：冻结一个编码器作为“锚点”保留预训练表征，另一个可训练以适配机器人任务，拼接后输入后续模块。<br>- 字符串动作分词器：将连续动作转换为字符序列，使用与语言一致的词表和自回归目标，使动作预测与VLM预训练目标对齐，支持细粒度逐步生成。<br>- 特征正则化与联合训练：利用统一的字符串输出空间，在机器人数据与强调空间推理/可供性的视觉-语言数据上进行等量（50%/批次）联合训练，防止灾难性遗忘并增强泛化。<br>- 架构与配方通用性：可无缝适配OpenVLA与π0等多种VLA体系。<br><br># 论文方法描述<br>- 部分冻结双编码器架构：使用两个视觉编码器（一个冻结、一个训练），对同一观测生成两组特征并拼接 z_t = [φ_frozen(o_t) \|\| φ_train(o_t)]，再与语言指令 c 一起送入动作分词器 ψ 生成动作。<br>- 字符串动作分词器：将动作各维度（如Δx=0.0312）序列化为字符序列（如“0 . 0 3 1 2”），利用语言词表与自回归生成目标进行预测，使动作空间与语言空间对齐，支持在语言模型内统一优化机器人与视觉-语言任务。<br>- 联合训练策略：每批训练从机器人数据与视觉-语言数据各取50%，数据涵盖空间推理、视觉问答、目标定位等，确保与任务相关且一致的梯度来源。<br><br># 论文使用数据集和训练资源<br>- 数据集<br> - 机器人：Open X-Embodiment (OXE)<br> - 视觉-语言：LLaVA Visual Instruct CC3M、VQASynth-Spatial、LLaVA OneVision、RoboPoint<br>- 仿真评估：SimplerEnv（Visual Matching与Visual Variant Aggregation），包含背景、光照、桌面纹理、干扰物与视角扰动<br>- 训练与评估硬件：双GTX 1080 Ti；真实机器人使用Logitech C920 Webcam作为视觉输入<br><br># 论文使用的评估环境和评估指标<br>- 仿真：SimplerEnv的Visual Matching与Visual Variant Aggregation<br>- 视觉稳健性：背景遮挡与视觉多样性扰动<br>- 语言稳健性：同义指令改写（GPT-4生成），如“grasp the can”→“get the can”<br>- 泛化与推理：在Text-VQA、POPE、GQA、VizWiz、VSR等VLM基准上评估保留的推理能力<br>- 指标<br> - 任务成功率（%）<br> - VQA准确率（%）<br> - CIFAR-10线性探测准确率（%）与t-SNE可视化<br> - 真实机器人每任务25次试验的成功次数（表V）</details> |
| 2025-09-11 | SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning | http://arxiv.org/abs/2509.09674 | <details><summary>展开</summary>待生成</details> |
| 2025-09-11 | VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model | http://arxiv.org/abs/2509.09372 | <details><summary>展开</summary>待生成</details> |
| 2025-09-11 | SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models | http://arxiv.org/abs/2509.09090 | <details><summary>展开</summary>待生成</details> |
| 2025-09-10 | RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation | http://arxiv.org/abs/2509.08820 | <details><summary>展开</summary>待生成</details> |
| 2025-09-09 | TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models | http://arxiv.org/abs/2509.07962 | <details><summary>展开</summary># 论文研究单位<br><br>北京人工智能研究院 (BAAI)<br>清华大学产业智能研究院 (AIR)<br>南洋理工大学 (NTU)<br><br># 论文概述<br><br>论文研究如何将关节扭矩信号有效融入视觉-语言-动作（VLA）模型，以提升机器人在接触丰富任务中的感知与控制能力。论文系统地探索了扭矩作为本体感受信号的设计空间，并在真实机器人（Cobot Magic ALOHA双臂平台）上验证了所提出方法对成功率和泛化性能的提升。<br><br># 论文核心贡献点<br><br>- 确定了扭矩信号在VLA中的最佳集成位置与方法：解码器侧、单历史token优于编码器与多token方案。<br>- 提出将扭矩历史聚合为单一token插入解码器的观测融入方式，保持解码器输入模式完整性并显著提升接触丰富任务表现。<br>- 引入“动作–扭矩联合预测”的训练目标，将扭矩预测作为辅助输出，使模型学习物理上合理的交互动力学表示。<br>- 在跨模型（RDT、π0）与跨本体（Cobot Magic ALOHA、ROKAE SR）上验证有效性。<br><br># 论文方法描述<br><br>- 扭矩作为观测（Sec 4）<br> - 架构对比：编码器插入、decoder预连接、decoder后连接；结论为decoder后连接最优。<br> - 历史信息：比较逐帧token与单历史token；单历史token在解码器侧最优，避免破坏解码器学习到的输入模式。<br>- 扭矩作为目标（Sec 5）<br> - 联合扩散损失：将动作块与扭矩块拼接为统一token，使用共享扩散权重与分头MSE损失进行联合预测；通过权重β平衡动作与扭矩目标。<br><br># 论文使用数据集和训练资源<br><br>- 数据：作者自采数据集，基于Cobot Magic ALOHA双臂平台的多种任务演示，用于VLA模型微调；涉及10项任务（5项接触丰富、5项常规）。<br>- 训练资源：基于公开预训练权重的VLA模型（ACT、RDT、π0）；硬件平台包括3个D435深度相机与7-DOF机械臂；关节扭矩由电机电流与转矩常数实时估计，无需外置力/扭矩传感器。<br><br># 论文使用的评估环境和评估指标<br><br>- 评估环境：Cobot Magic ALOHA与ROKAE SR机械臂；任务包括按钮按压、充电器插拔、USB插拔、拔插座、门把手转动、瓶子拾取与放置、倒液体、叠 cubes、推到定位、开抽屉等。<br>- 评估指标：20轮试验成功率（成功次数/20）；对比基线与不同扭矩集成策略的定量结果；展示接触阶段的扭矩曲线与任务重试的定性可视化。</details> |
| 2025-09-09 | Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation | http://arxiv.org/abs/2509.07957 | <details><summary>展开</summary># 论文研究单位<br>杭州电子科技大学人工智能学院机器学习与健康国际合作基地（Shunlei Li、Jiuwen Cao）；新墨西哥大学电气与计算机工程系（Longsen Gao）；慕尼黑工业大学计算、信息与技术学院（Yingbai Hu）。<br><br># 论文概述<br>论文提出图融合的视觉-语言-动作模型 GF-VLA，用于从单次 RGB-D 人类演示中为双臂机器人学习可泛化的层级策略与动作。与传统轨迹复制不同，GF-VLA 通过信息论从演示中抽取关键手-物与物-物交互，构造成时间排序的场景图，并将其与语言条件下的Transformer结合，生成可解释的行为树与笛卡尔运动基元。为提升双任务执行效率，论文设计了跨臂分配策略，自主确定抓握任务分配，无需显式几何建模。实验在四类双臂方块组装基准上验证策略迁移与空间泛化能力。<br><br># 论文核心贡献点<br>- 提出基于信息论的交互抽取方法：从多模态演示中以熵与互信息选择任务相关的手-物与物-物交互，构建交互感知的场景图。<br>- 提出 GF-VLA 框架：首次将结构化的交互建模与视觉-语言-动作（VLA）推理统一，支持鲁棒、可泛化的双臂操作。<br>- 增强可解释性：在VLA中嵌入链式思维（CoT）提示与自验证，实现显式的子目标分解与执行验证。<br><br># 论文方法描述<br>- 信息论驱动的场景图：使用滑动时间窗对位置信号进行熵分析，识别动态活跃区间；用互信息检测手-物耦合（分为“耦合运动”与“停靠”）与物-物交互（分为“高效OO”与“瞬态OO”）。图节点包含实体类别与六维位姿，边表示交互类型与强度。<br>- 图融合的视觉-语言-动作：统一双头架构包含LLM头（高层语义规划与CoT推理、自验证）与动作头（低层笛卡尔末端与夹爪控制，5 Hz指令流）。视觉侧采用RGB与RGB-D分割、特征投影与token对齐；语言侧基于7B参数的LLaMA-2骨干适配。<br>- 链式思维语义策略：LLM头输出结构化的推理轨迹与行为树节点（动作类型、参数、验证条件），确保时序一致性与可审计性。<br>- 参数高效微调：使用LoRA适配LLM与动作双头，仅更新主干与适配器。策略头监督来自250段人类演示（125用于训练、125用于评估）；动作头在240段双臂试验上微调（UR5e+Robotiq 2F-85与UR10e+Barrett BH282），任务覆盖形状泛化、空间关系、绝对与相对6D位姿执行。<br><br># 论文使用数据集和训练资源<br>- 人类演示数据：10名参与者提供的250个RGB视频，包含字母搭建与塔式构造任务；125个用于策略头训练、125个用于规划评估。<br>- 机器人试验数据：240次双臂试验，涵盖4类任务×3种形状×20次重复（各120/120划分为微调/评估）；Intel RealSense D435i顶置RGB-D感知。<br>- 计算资源与时延：单张RTX 4090进行约40小时微调，指令-推理平均时延约7.56秒（bfloat16）。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：双UR机械臂与不同夹爪协同（UR5e+Robotiq 2F-85、UR10e+Barrett BH282），顶部RGB-D相机（Intel RealSense D435i）；任务含形状泛化、空间关系、6D位姿执行与字母/塔式结构构建。<br>- 指标：<br> - 图表示准确度（GRA）与任务切分准确度（TSA）：单手任务达GRA 98.5%、TSA 95.6%；字母任务GRA 97.2%、TSA 93.9%；塔式任务GRA 96.8%、TSA 93.1%。<br> - LLM规划评估：覆盖度、排序准确度、验证正确性与CoT可解释性评分。<br> - 抓取成功率（GSR）、放置成功率（PSR）、6D位姿误差（6DPE）、指令一致性评分（ICS）。<br> - 任务成功率（TSR）、计划迁移率（PTR）、双臂协调评分（BCS）。<br>- 性能表现（示例）：<br> - 形状泛化：GSR 0.98±0.02、PSR 0.95±0.03、6DPE 1.2cm/2.5°、ICS 4.8±0.3。<br> - 相对位姿：GSR 0.91±0.04、PSR 0.85±0.06、6DPE 3.1cm/6.8°。<br> - 任务迁移：六个新任务平均TSR 0.90、PTR≥0.83、BCS≥4.4；总体抓取/放置/任务成功率约为94%/89%/90%，且跨形状、姿态与视角变化保持鲁棒性。</details> |
| 2025-09-08 | F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions | http://arxiv.org/abs/2509.06951 | <details><summary>展开</summary># 论文研究单位<br>上海人工智能实验室；哈尔滨工业大学（深圳）<br><br># 论文概述<br>论文提出 F1-VLA，一个将视觉、语言与动作统一建模的具身智能框架。该模型通过显式的“视觉前瞻生成”模块，在感知与行动之间引入目标条件化的未来视觉状态预测，并采用 Mixture-of-Transformer 架构将理解、前瞻与控制三类专家进行层次化整合。基于“预测式逆动力学”思想，F1 将动作生成重塑为“以前瞻为目标的逆问题求解”，在动态与长时序任务中显著提升鲁棒性与成功率。模型在 330k+ 轨迹、136 项任务、5 种机器人形态的大规模数据上进行三阶段训练，并在真实场景与模拟基准上均取得领先表现。<br><br># 论文核心贡献点<br>- 引入视觉前瞻作为显式规划信号，将 VLA 从“反应式状态到动作”的映射转变为“前瞻引导的逆动力学”推理。<br>- 提出 UGA（理解–生成–行动）渐进式注意力与跨专家因果信息流，确保前瞻与行动模块的稳定性与可解释性。<br>- 设计 next-scale（多尺度）预测的高效机制，生成多分辨率的视觉前瞻 tokens，兼顾实时性与预测质量。<br>- 给出三阶段训练配方：先对齐前瞻与理解，再在大规模机器人数据上进行预训练，最后进行任务特定后训练，实现跨任务、跨形态的可迁移能力。<br>- 在 LIBERO、SimplerEnv Bridge 等模拟基准与真实机器人平台（Genie、Franka、ARX LIFT II）上系统验证优越性，特别是在动态环境与长时序任务中的稳定增益。<br><br># 论文方法描述<br>- 架构：Mixture-of-Transformer（MoT），包含理解专家、生成专家、行动专家。理解专家采用 PaliGemma（基于 SigLIP 视觉编码与 Gemma 解码器）以实现语言–视觉对齐；生成专家基于 VAR 的残差 VQ-VAE 进行图像离散化，采用 next-scale 预测与时间卷积聚合历史运动特征以高效合成前瞻；行动专家以 flow matching 在连续动作空间学习从噪声到目标动作的向量场，配合 chunked action 预测生成短期动作序列。<br>- 信息流：理解 → 生成 → 行动的单向因果层次，生成模块不接收来自行动的逆向信息，防止信息泄漏与训练不稳定。<br>- 训练目标：阶段 I 以教师强制最小化前瞻 tokens 的负对数似然；阶段 II/III 以自回归 next-scale 预测和 flow matching 的行动损失联合优化，总体损失为两者加权。<br>- 推理优化：实际推理时仅使用 4 个尺度的前瞻预测以平衡效率与效果。<br><br># 论文使用数据集和训练资源<br>- 训练数据：330k+ 轨迹，覆盖 136 项任务与 5 种机器人形态；包含 LIBERO、Open-X-Embodiment、AgiBotWorld 等公开机器人数据集，涵盖从基础抓取、放置到复杂协作与推送的广泛技能，任务时长 10 秒至 2 分钟以上。<br>- 三阶段训练：Stage I 冻结理解专家，仅训练生成专家进行前瞻对齐；Stage II 在大规模机器人数据上联合预训练；Stage III 进行任务特定后训练以适配新形态与细粒度技能。<br>- 实现细节：理解与行动专家由 π0 初始化，生成专家随机初始化并配备预训练的 VAR 残差 VQ-VAE；主干采用 Swish 激活、RMSNorm 与 RoPE；训练超参数详见附录。<br><br># 论文使用的评估环境和评估指标<br>- 模拟基准：<br> - LIBERO（空间、物体、目标、长时序四套件）：评价指标为成功率（SR，越高越好）和排名（Rank，越低越好）。<br> - SimplerEnv Bridge（多步精细操控）：评价指标为抓取成功率与整体任务成功率。<br>- 真实实验：<br> - 平台：Genie（双臂）、Franka、ARX LIFT II；任务包括基础抓取与放置、精细操控、双臂协作与人机交互、动态传送带抓取、长时序序列操作等。<br> - 指标：每项任务 15 次试验的抓取与任务成功率；报告各任务均值与对比基线（π0、gr00t-N1/N1.5、OpenVLA、SpatialVLA、CoT-VLA、RT-1-X、RoboVLM 等）。</details> |
| 2025-09-08 | LLaDA-VLA: Vision Language Diffusion Action Models | http://arxiv.org/abs/2509.06932 | <details><summary>展开</summary>### 论文研究单位<br><br>中国科学技术大学、南京大学、Dexmal<br><br>### 论文概述<br><br>该论文提出了LLaDA-VLA，这是首个基于预训练扩散视觉语言模型构建的视觉-语言-扩散-动作模型，用于机器人操作任务。为了解决预训练d-VLMs与机器人动作生成之间的领域差距以及掩码扩散范式在生成结构化动作序列方面的不足，论文引入了两个核心设计：一是局部化特殊令牌分类策略，通过将分类空间限制在特殊动作令牌上，降低了模型适应的难度；二是分层动作结构化解码策略，该策略在解码过程中显式地考虑动作内部和动作之间的依赖关系，生成了更连贯、精确的动作序列。大量的实验表明，LLaDA-VLA在模拟环境和真实机器人上的性能均显著优于现有的最先进视觉-语言-动作模型。<br><br>### 论文核心贡献点<br><br>1. 提出了首个基于预训练d-VLMs构建的视觉-语言-扩散-动作模型LLaDA-VLA，为机器人策略学习建立了一个新范式。<br>2. 设计了两种关键技术使掩码扩散模型适用于动作生成：用于简化领域适应的局部化特殊令牌分类策略，以及用于无缝集成到动作生成中的分层动作结构化解码策略。<br>3. 在SimplerEnv和CALVIN模拟基准以及WidowX真实机器人上的广泛实验，验证了LLaDA-VLA的卓越性能，凸显了d-VLMs在机器人操作中的潜力。<br><br>### 论文方法描述<br><br>1. **模型架构**:<br> * 模型基于LLaDA-V预训练d-VLM，采用SigLIP-2作为视觉编码器，MLP作为投影器。<br> * 输入为语言指令和RGB图像。图像通过视觉编码器提取特征，经投影后与文本令牌拼接，一同输入到大语言扩散模型中。<br> * **动作分词化**: 将连续的动作值离散化为32个区间，并引入对应数量的特殊动作令牌。一个包含7维（3个位置、3个旋转、1个夹爪状态）的动作由7个特殊令牌表示。模型一次性预测一个包含K个时间步的动作块（K*D个令牌）。<br>2. **局部化特殊令牌分类**:<br> * 在训练和推理时，模型不进行全词汇表分类，而是仅在新增的特殊动作令牌集合上进行分类。<br> * 损失函数也相应修改，只计算在掩码位置上的动作令牌的交叉熵损失，忽略其他令牌。此举缩小了学习目标，使模型更容易适应机器人领域。<br>3. **分层动作结构化解码**:<br> * 在标准的掩码扩散“预测-重掩码”流程基础上，引入了针对动作块的分层解码。<br> * **动作级解码**: 每个迭代步骤，计算每个动作（包含D个令牌）的置信度分数（将内部令牌的置信度求和）。保留置信度最高的动作，将其他所有动作重新掩码。<br> * **令牌级解码**: 在保留的动作内部，根据令牌级置信度进行排序，保留高置信度令牌，重掩码其余令牌。<br> * 通过迭代此过程，模型能以动作为单位逐步完善输出，从而生成结构合理、前后一致的动作序列。<br><br>### 论文使用数据集和训练资源<br><br>* **数据集**:<br> * **SimplerEnv**: 使用WidowX机器人在Visual Matching设置下进行评估，包含4项操作任务。<br> * **CALVIN**: 一个长视野、语言条件操作的基准。采用ABC-D协议，在A、B、C环境上训练，在D环境上评估泛化性能。<br> * **真实世界WidowX机器人**: 使用WidowX 250S机器人，评估了4个域内任务和4个分布外（OOD）泛化任务。<br>* **训练资源和细节**:<br> * 基于开源的LLaDA-V预训练权重进行微调。<br> * 训练3个周期，学习率为2e-5，批次大小为128。<br> * 动作块大小K设置为5，预测的是增量动作。<br> * 推理时使用10次迭代扩散步骤，并采用dllm-cache方法加速解码。<br><br>### 论文使用的评估环境和评估指标<br><br>* **评估环境**:<br> * **模拟环境**: SimplerEnv和CALVIN。<br> * **真实机器人**: WidowX 250S机械臂。<br>* **评估指标**:<br> * **SimplerEnv**: 任务成功率（%）。<br> * **CALVIN**: （1）连续完成1到5个任务的成功率（%）；（2）连续完成5个任务的平均长度。<br> * **真实机器人**: 域内任务和分布外（OOD）任务的任务成功率（%）。</details> |
| 2025-09-06 | SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning | http://arxiv.org/abs/2509.05614 | <details><summary>展开</summary>待生成</details> |
| 2025-09-05 | FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies | http://arxiv.org/abs/2509.04996 | <details><summary>展开</summary>论文研究单位<br>卡尔斯鲁厄理工学院直观机器人实验室 (Intuitive Robots Lab, Karlsruhe Institute of Technology)，与微软研究院 (Microsoft Research) 合作。<br><br>论文概述<br>当前视觉-语言-动作（VLA）策略模型面临计算成本高和资源需求大的问题，现有扩散型VLA需数十亿参数和大规模数据集。本文提出两项创新：中间模态融合（通过剪枝30-50%的LLM层重新分配容量至扩散头）和动作特定Global-AdaLN条件（通过模块化适配减少20%参数）。基于此构建了参数量为950M的VLA模型FLOWER，仅需200 H100 GPU小时预训练，在10个模拟与真实基准的190个任务上达到与更大VLA模型相当的性能，并在CALVIN ABC基准上创4.53的新SOTA。模型在多样化机器人具身上表现出鲁棒性。<br><br>论文核心贡献点<br>1. **中间模态融合**：从VLM中间层提取特征（剪枝30-50%层），保留语义信息同时为流变换器释放参数空间，提升训练效率和推理速度。<br>2. **动作空间全局自适应层归一化（Global-AdaLN）**：共享调制权重并生成动作类型特定信号，减少20%参数而不损失表达能力。<br>3. **高效架构设计**：集成上述技术构建FLOWER模型，参数仅947M，预训练成本降低99%。<br>4. **广泛验证**：在10个基准（如CALVIN、LIBERO）上190个任务实现SOTA或相当性能，真实场景泛化能力突出。<br><br>论文方法描述<br>模型包含剪枝后的Florence-2-L VLM编码器（移除解码器或后30%层）和18层流变换器（潜在维度1024）。VLM中间特征通过交叉注意力注入流变换器，使用动作类型特定编码器/解码器处理异构动作空间（如Delta-EEF和关节角度）。采用Rectified Flow生成动作，优化目标为最小化噪声与数据间的速度场误差，推理仅需4-8步去噪。训练使用动作块长度20和单静态图像输入，预训练数据混合包含74% Delta-EEF和26%单臂关节状态数据。<br><br>论文使用数据集和训练资源<br>预训练数据集为8个公开机器人数据集的混合（约250k轨迹），包括Droid、Google Robot和BridgeV2（占75%），注重场景和具身多样性。训练资源为4×H100 GPU，48小时（约200 GPU小时），批量大小1024（梯度累积步数4），使用BF16精度优化内存。<br><br>论文使用的评估环境和评估指标<br>**评估环境**：<br>- 模拟基准：CALVIN（任务链指令跟随）、LIBERO（长视野任务）、Aloha（双臂高频控制）、SIMPLER（真实到仿真泛化）。<br>- 真实场景：Franka Panda机器人厨房环境（20个任务），测试泛化（新物体、光照、背景干扰、任务组合）。<br>**评估指标**：<br>- 任务成功率（CALVIN需顺序完成5个指令链，LIBERO/LIBERO-90为单任务成功率）。<br>- 平均序列长度（CALVIN）。<br>- 推理效率：吞吐量（Hz）、延迟（秒）、VRAM占用（MB），在RTX 4090上测试。</details> |
| 2025-09-04 | Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models | http://arxiv.org/abs/2509.04063 | <details><summary>展开</summary>### 论文研究单位<br>论文作者包括 Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang。单位信息部分缺失，但通讯作者 Donglin Wang 来自西湖大学。<br><br>### 论文概述<br>针对基于流匹配的视觉-语言-行动（VLA）模型在复杂任务中行动准确性不足的问题，论文提出了一种离线强化学习后训练方法 ARFM（自适应强化流匹配）。该方法通过在 VLA 流模型损失中引入自适应缩放因子，理论构建了一个权衡偏差-方差的优化目标，以平衡强化学习信号和梯度方差影响，从而实现更稳定和高效的微调过程。实验表明 ARFM 在泛化、鲁棒性、少样本学习和持续学习方面表现优异。<br><br>### 论文核心贡献点<br>1. 提出 ARFM 方法，一种用于 VLA 流模型的新型离线强化学习后训练方法，能够自适应调整数据质量分布。<br>2. 理论建立缩放因子 α 的优化目标，并诱导二分迭代算法进行实时更新，实现高效 VLA 流模型微调。<br>3. 通过仿真和真实机器人操作任务实验，验证 ARFM 在泛化能力、对动态扰动的鲁棒性、少样本学习和持续学习场景中的卓越性能。<br><br>### 论文方法描述<br>方法基于能量加权流匹配（EWFM）框架：<br>- **能量加权 VLA 流模型**：构建能量引导分布 π(A_t\|o_t) ∝ p(A_t\|o_t) exp(α R*(A_t, o_t))，其中 R* 为标准化强化学习优势，α 为缩放因子。使用条件能量加权流匹配（CEFM）损失优化向量场，学习策略分布。<br>- **自适应缩放因子 α 调整**：通过最小化目标函数 J(α) = Var(ĝ(α)) - λ S(α) 实现信号与方差的权衡，其中 ĝ(α) 是损失梯度，S(α) 是强化学习优势评分函数。基于高斯假设，推导出 J(α) 的具体形式，并通过二分迭代算法（Algorithm 1）实时求解最优 α*。最终算法（Algorithm 2）集成到 VLA 流模型微调中。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：使用 LIBERO 仿真基准（包括 Object、Long、Spatial 和 Goal 四个任务套件）和真实世界 UR5 机器人平台（三个抓取和放置任务）。<br>- **训练资源**：论文未明确指定硬件资源，但假设采用标准计算环境（如 GPU）进行仿真和实验。模型基于流匹配实现，训练过程涉及数据采样和损失优化。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**： LIBERO 仿真环境和 UR5 真实机器人实验平台。<br>- **评估指标**：主要使用成功率（Success Rate, SR）衡量任务完成度；在动作扰动实验中，添加高斯噪声（0.1 至 0.3 级别）评估鲁棒性，通过扰动后成功率评估性能。</details> |
| 2025-09-04 | FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction | http://arxiv.org/abs/2509.04018 | <details><summary>展开</summary>待生成</details> |
| 2025-09-03 | ANNIE: Be Careful of Your Robots | http://arxiv.org/abs/2509.03383 | <details><summary>展开</summary>## 论文研究单位<br><br>中国科学院计算技术研究所、中国科学院自动化研究所、佐治亚理工学院、德克萨斯大学达拉斯分校<br><br>## 论文概述<br><br>论文研究了具身AI（EAI）系统中的对抗性安全攻击问题。针对EAI系统将视觉-语言-动作（VLA）模型整合到机器人中带来的新安全风险，提出了第一个系统性的对抗安全攻击研究框架。论文基于ISO/TS 15066标准，建立了从传统机器学习安全向物理AI安全转变的范式转移，并通过仿真和真实机器人实验验证了攻击效果。<br><br>## 论文核心贡献点<br><br>- **安全定义与分类**：提出了基于ISO/TS 15066标准的EAI系统安全定义，将安全违规分为critical、dangerous、risky三个等级<br>- **基准测试数据集**：构建了Annie-Bench基准，包含9个安全关键场景和2400个视频动作序列，专注于评估EAI安全性能<br>- **攻击框架**：开发了Annie-Attack任务感知对抗框架，通过攻击领导者模型将长期目标分解为帧级扰动<br>- **实证验证**：在ACT和Baku两个代表性VLA模型上验证了攻击效果，各安全类别攻击成功率均超过50%<br><br>## 论文方法描述<br><br>- **安全标准定义**：基于人机协作ISO标准，定义了距离约束、速度约束和碰撞约束三个安全标准<br>- **攻击框架设计**：Annie-Attack包含攻击领导者模型，将高层攻击目标转换为帧级动作扰动，使用PGD方法生成对抗样本<br>- **行动空间设置**：使用末端执行器的4维动作表示（3维笛卡尔空间移动+1维夹爪状态）<br>- **稀疏攻击策略**：提出自适应稀疏攻击，根据动作序列阶段动态调整攻击频率，在早期阶段使用高频率攻击，在后期阶段降低频率<br><br>## 论文使用数据集和训练资源<br><br>- **仿真环境**：基于ManiSkill仿真平台构建数据集<br>- **硬件配置**：使用Franka Emika Panda 7自由度机械臂，配备2自由度夹爪和双摄像头系统<br>- **数据集规模**：生成2400个视频动作序列，包含9个不同安全级别场景<br>- **训练数据**：每个场景约240个演示序列用于训练专门的攻击领导者模型<br>- **评估数据**：每个场景20个测试序列用于评估<br><br>## 论文使用的评估环境和评估指标<br><br>- **评估环境**：仿真环境中的9个测试场景，覆盖所有三个安全违规等级<br>- **评估指标**：<br> - 攻击成功率（ASR）：衡量安全约束规则被违反的百分比<br> - 动作一致性（AC）：评估攻击是否导致动作序列中的突然变化<br> - 动作偏差（AD）：测量对抗动作与原始动作分布的偏离程度<br> - 任务成功率变化（TSRC）：攻击前后任务成功率的变化百分比<br>- **基线模型**：在ACT和Baku两个VLA模型上进行评估<br>- **真实实验**：在真实Franka Panda机器人上验证了攻击的物理影响</details> |
| 2025-09-02 | Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance | http://arxiv.org/abs/2509.02055 | <details><summary>展开</summary>### 论文研究单位<br>- Institute of Artificial Intelligence, China Telecom<br>- Tsinghua University<br>- The Chinese University of Hong Kong, Shenzhen<br>- Northwestern Polytechnical University<br><br>### 论文概述<br>Vision-Language-Action（VLA）模型在预训练后能处理多任务机器人操作，但跨 embodiment（如单臂到双臂）和跨任务的适应常因动作分布不匹配而效率低下。本文提出Align-Then-stEer（ATE）框架，通过两阶段策略实现高效适应：先构建统一动作潜在空间桥接预训练和适应数据分布，再以分类器引导机制在潜在空间中引导VLA生成过程向目标域偏移。实验在仿真（RoboTwin 1.0、ManiSkill3）和真实双臂机器人上验证，ATE显著提升成功率，仿真平均增益9.8%，真实跨 embodiment场景增益达32%。<br><br>### 论文核心贡献点<br>- 提出动作分布对齐策略，利用反向KL散度的模式寻求特性构建统一潜在空间，将适应动作嵌入预训练潜在分布模式中。<br>- 设计基于统一潜在空间的分类器引导机制，无缝集成至扩散/流匹配VLA训练目标，实现精确快速适应。<br>- ATE为即插即用设计，模型无关且计算开销小，仅需训练两个轻量级VAEs。<br>- 跨 embodiment、任务和架构（扩散/流匹配）验证有效性，突出适应效率提升。<br><br>### 论文方法描述<br>ATE分两阶段：<br>1. **统一动作潜在空间对齐（Stage 1）**：<br> - 预训练阶段：在预训练动作数据（如DROID、Open X-Embodiment）上训练InfoVAE（V_pretrain），学习潜在分布。<br> - 适应阶段：在适应数据（目标 embodiment）上训练另一个InfoVAE（V_adaptation），通过最小化反向KL散度 D_KL(q_ψ(z\|ã) \|\| q_φ(z))，将适应潜在分布嵌入预训练潜在空间的特定模式，生成统一潜在空间Z。<br>2. **分类器引导适应（Stage 2）**：<br> - 定义引导函数g = -∇\|\|E_ψ(â_t:t+h^k) - E_ψ(a_t:t+h^0)\|\|²，度量中间动作与目标动作在潜在空间中的距离。<br> - 对扩散模型，修改噪声预测为 ε̂ = ε_θ - √(1-ā_k)g，并嵌入训练目标 L(θ) = E[\|\|ε - ε_θ(...) + √(1-ā_k)·λ·g\|\|²]。<br> - 对流匹配模型，修改速度场为 v̂_θ = v_θ + ((1-τ)/τ)·λ·g，并嵌入训练目标。<br> 引导机制确保VLA输出保持于统一潜在空间内，在适配中保留预训练知识。<br><br>### 论文使用数据集和训练资源<br>- **数据集来源**：<br> - 预训练：大规模机器人数据（如DROID、Open X-Embodiment子集、Kuka、ALOHA）。<br> - 适应：仿真（RoboTwin 1.0含17任务，ManiSkill3含2任务）、真实（双RealMan 7-DoF机器人长期任务）。<br>- **训练资源**：<br> - InfoVAE训练分两步：Step 1用通用数据（3000 episodes）训练潜在结构（约12小时）；Step 2用域特定数据微调（RoboTwin/ManiSkill每任务50-100轨迹，真实每任务50轨迹，约0.5小时）。<br> - 潜在维度512，训练含互信息项以增强表示。<br><br>### 论文使用的评估环境和评估指标<br>- **仿真环境**：<br> - RoboTwin 1.0基准：17项单/双臂任务（工具调整、双瓶拾取）。<br> - ManiSkill3基准：2项接触丰富单臂操作（推立方体、拾取立方体）。<br>- **真实环境**：<br> - 双RealMan 7-DoF双臂机器人：4项长期任务（插入、协调操作）和工具使用任务。<br>- **评估指标**：<br> - 成功率（Success Rate）：以任务完成百分比衡量。<br> - 样本效率：比较达到性能阈值所需训练步数（如RDT基线90k步 vs ATE 70k步）。<br> - 泛化能力：在光照、物体位置、视觉干扰下测试鲁棒性。</details> |
| 2025-09-02 | AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving | http://arxiv.org/abs/2509.01944 | <details><summary>展开</summary># 论文研究单位<br><br>阿里巴巴集团高德（AMAP）主导研究，昆士兰大学、兰州大学、凯斯西储大学合作参与<br><br># 论文概述<br><br>论文提出AutoDrive-R²，这是一个用于自动驾驶的视觉-语言-动作(VLA)框架，通过思维链(CoT)处理和强化学习(RL)来增强自动驾驶系统的推理和自我反思能力。方法采用两阶段训练：首先构建包含四步逻辑推理和自我反思机制的nuScenesR²-6K数据集进行监督微调，然后基于Group Relative Policy Optimization (GRPO)算法结合物理约束奖励进行强化学习训练。<br><br># 论文核心贡献点<br><br>- 提出AutoDrive-R²框架，实现从视觉信息和语言指令到轨迹规划的语义推理与自我反思<br>- 构建nuScenesR²-6K数据集，采用四步逻辑链(可视化→计算→逻辑→反思)与自我反思机制<br>- 基于GRPO强化学习的后训练方法，结合基于物理的奖励框架，包含空间对齐、车辆动力学和时间平滑性约束<br><br># 论文方法描述<br><br>**阶段一监督微调(SFT)**：使用Qwen2.5-VL-7B模型在nuScenesR²-6K数据集上微调，数据集包含6k图像-轨迹对，每对包含前视图像、历史车辆状态和相应的思维链推理序列。<br><br>**阶段二强化学习(RL)**：采用GRPO算法，设置最大完成长度4096 tokens，每输入采样6个候选响应。设计物理约束奖励函数：空间对齐奖励r_pos(位置误差)、车辆动力学奖励r_ste(转向角)和r_vel(速度)、时间平滑性奖励r_tem(控制信号平滑性)，各奖励权重相等。<br><br># 论文使用数据集和训练资源<br><br>**训练数据**：nuScenesR²-6K数据集(6k样本对)，每个样本包含前视图像和3秒轨迹规划(0.5秒间隔)<br><br>**评估数据**：nuScenes数据集(1000个城市驾驶场景，六个同步摄像头)和Waymo数据集(4021个驾驶片段，八个摄像头视图)<br><br>**模型规模**：Qwen2.5-VL-3B和Qwen2.5-VL-7B两种规格<br>**训练配置**：学习率5e-7，累积批大小1，GRPO最大长度4096，生成候选数6<br><br># 论文使用的评估环境和评估指标<br><br>**评估环境**：在nuScenes和Waymo数据集上进行轨迹规划评估<br><br>**评估指标**：L2误差(米)，计算预测轨迹与真实轨迹在1s、2s、3s未来时间点的距离，以及平均L2误差<br><br>**性能表现**：AutoDrive-R²在nuScenes上达到0.19m平均L2误差，Waymo上达到0.20m，显著超越EMMA+、DriveVLM等现有方法，展现出强大的零样本泛化能力</details> |
| 2025-08-31 | OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving | http://arxiv.org/abs/2509.00789 | <details><summary>展开</summary>### 论文研究单位<br>未在提供的HTML原文中明确提及。<br><br>### 论文概述<br>OmniReason是一个用于自动驾驶的时空引导视觉-语言-动作框架，旨在解决现有方法在动态驾驶场景中忽视时间维度的问题。该框架通过联合建模动态3D环境和决策过程，建立了鲁棒的时空推理能力。论文提出了OmniReason-Data数据集和OmniReason-Agent模型架构，数据集包含密集时空标注和自然语言解释，模型通过时空知识蒸馏实现可解释的决策。<br><br>### 论文核心贡献点<br>1. 提出OmniReason-nuScenes和OmniReason-Bench2Drive两个大规模视觉-语言-动作（VLA）数据集，包含密集时空标注和自然语言解释，通过减少幻觉的自动标注流水线生成。<br>2. 设计OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器，通过时空知识蒸馏捕获因果推理模式，生成人类可解释的决策依据。<br>3. 在开环规划和视觉问答（VQA）任务上实现最先进性能，显著提升安全性、舒适性和解释质量。<br><br>### 论文方法描述<br>1. **OmniReason-Data构建**：<br> - 基于nuScenes和Bench2Drive，通过规则和原则模板整合人类先验知识，引导Qwen2.5VL 72B模型生成场景感知描述和因果推理链。<br> - 流水线包括场景空间标注、人类先验知识引导和MLLM时序推理，确保物理合理性和时间连贯性。<br>2. **OmniReason-Agent架构**：<br> - 视觉主干网络通过分层编码将多视图输入转换为时空令牌，稀疏时间记忆模块使用记忆压缩查询聚合长期上下文。<br> - VLM推理核心处理历史驾驶状态和语言指令，通过轻量适配器增强冻结VLM。<br> - 运动感知归一化模块动态适应对象状态，混合注意力机制传播对象查询，跨模态聚合融合图像特征。<br>3. **训练目标**：<br> - 联合优化3D检测和场景理解，检测损失包括分类（Focal Loss）和回归（L1），车道分析损失类似。<br> - LLM使用自回归交叉熵损失，总损失为感知和语言损失的加权和。<br><br>### 论文使用数据集和训练资源<br>1. **数据集**：<br> - OmniReason-nuScenes和OmniReason-Bench2Drive，基于nuScenes和Bench2Drive构建，包含多视图视频、对象标注和轨迹数据。<br> - 涵盖环境描述、动态/静态对象、因果推理和动作标注，支持VQA和开环规划任务。<br>2. **训练资源**：<br> - 128块NVIDIA H20 GPU（96GB内存）。<br> - 视觉编码器使用EVA-02-L（CLIP知识蒸馏预训练），基础模型为LLaVA v1.5。<br> - 微调阶段使用AdamW优化器，批大小16，学习率分层：投影模块4e-4，视觉编码器和LLM为2e-5。<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**：<br> - 开环规划任务在nuScenes基准测试，VQA任务在自定义OmniReason数据集评估。<br> - 实施细节中提及使用标准训练配置和余弦退火调度。<br>2. **评估指标**：<br> - **开环规划**：L2位移误差（1/2/3秒）、平均碰撞率（CR）、违规率（IR）。<br> - **VQA**：CIDEr、BLEU-1/4、METEOR、ROUGE-L、Precision、Recall，衡量语言理解和多模态对齐。<br> - 消融实验中分析语言组件和记忆模块对BL-1、L2、CR、IR的影响。</details> |
| 2025-08-30 | Galaxea Open-World Dataset and G0 Dual-System VLA Model | http://arxiv.org/abs/2509.00576 | <details><summary>展开</summary># 论文研究单位<br>Galaxea Team（https://opengalaxea.github.io/G0/）<br><br># 论文概述<br>论文提出了Galaxea Open-World Dataset，一个在真实人类生活和工作环境中收集的大规模、多样化机器人行为数据集。基于该数据集，论文介绍了G0双系统框架，结合了用于多模态规划的视觉语言模型（VLM）和用于精细执行的视觉语言动作（VLA）模型。G0采用三阶段训练课程：跨具身预训练、单具身预训练和任务特定后训练。论文通过涵盖桌面操作、小样本学习和长期移动操作的综合基准测试证明了方法的有效性。<br><br># 论文核心贡献点<br>1. 构建了Galaxea Open-World Dataset，包含100K条演示轨迹，涵盖150个任务类别、50个真实世界场景、1600+独特对象和58种操作技能<br>2. 提出了G0双系统框架，结合VLM（系统2）进行规划和VLA（系统1）执行动作<br>3. 设计了有效的三阶段训练策略，验证了单具身预训练在模型性能提升中的关键作用<br>4. 建立了涵盖多种任务类型的综合基准测试<br><br># 论文方法描述<br>**G0双系统架构**<br>- 系统1（G0-VLA）：端到端视觉语言动作模型，负责感知环境、解释子任务指令并执行动作<br>- 系统2（G0-VLM）：处理高层自然语言任务指令、理解场景并为系统1规划子任务指令<br><br>**G0-VLA三阶段训练**<br>- 阶段1预训练：仅训练VLM组件，使用FAST tokenizer将连续动作块转换为离散索引序列，采用标准交叉熵损失进行自回归训练<br>- 阶段2预训练：在标记的Galaxea数据集上训练VLA，包含预训练VLM和新初始化的动作专家，采用flow-matching损失训练连续动作生成<br>- 后训练：使用最多100条轨迹对下游任务进行微调，验证预训练模型的泛化能力<br><br>**G0-VLM训练**<br>- 基于Qwen2.5-VL进行指令调优，使用数据集中的人工注释子任务和合成的人类风格高层指令<br>- 采样关键帧并输入历史图像观察和机器人动作，支持长期上下文任务规划<br><br># 论文使用数据集和训练资源<br>**Galaxea Open-World Dataset**<br>- 规模：100K演示轨迹，500小时高质量数据<br>- 任务覆盖：150个任务类别，50个不同真实世界场景<br>- 对象多样性：1600+独特对象，58种操作技能<br>- 具身一致性：使用统一的Galaxea R1 Lite平台（23-DoF，包括双臂、3-DoF躯干、6-DoF全向基座）<br><br>**训练数据组成**<br>- 阶段1预训练：约1000小时OXE轨迹 + 500小时Galaxea数据集（仅高层任务描述）+ 200小时内部数据<br>- 阶段2预训练：Galaxea Open-World Dataset的完整标注数据<br>- 支持数据：关键帧采样，k帧历史观察，机器人状态信息<br><br># 论文使用的评估环境和评估指标<br>**评估基准任务**<br>- Table bussing：整理杂乱桌面，测试精确抓取放置和双臂协调操作<br>- Microwave operation：操作微波炉，评估与家电交互和多步操作序列<br>- Bed making：整理床铺，测试全身协调控制能力<br>- Blocks stacking：搭建积木形成特定词汇，验证语言跟随和精确操作<br><br>**评估指标**<br>- Progress score：根据任务完成步骤计算分数（Table bussing: 6分，Microwave operation: 5分，Bed making: 4分，Blocks stacking: 6分）<br>- 每个测试运行10次取平均成绩<br>- 小样本设置：每个任务仅使用20条轨迹进行微调，训练10轮<br><br>**评估环境**<br>- 真实世界环境：住宅、餐饮、零售和办公空间<br>- 物理硬件：Galaxea R1 Lite移动双臂机器人<br>- 评估设置：标准化提示，包含任务特定指令、原子动作选项和输出示例</details> |
| 2025-08-30 | Mechanistic interpretability for steering vision-language-action models | http://arxiv.org/abs/2509.00328 | <details><summary>展开</summary>### 论文研究单位<br>加州大学伯克利分校电气工程与计算机科学系（University of California, Berkeley, Department of Electrical Engineering and Computer Sciences）<br><br>### 论文概述<br>VLA模型结合视觉和语言信息实现机器人动作控制，但缺乏类似传统机器人管道的可解释性机制。本研究将机械可解释性技术应用于VLA模型，通过分析模型内部激活单元的语义含义，实现无需微调的实时行为引导。核心创新在于提取FFN层的语义价值向量（value vectors），识别与动作选择因果相关的控制方向（如速度、方向），并将其作为实时控制接口激活。<br><br>### 论文核心贡献点<br>1. **语义保留发现**：VLA模型在训练中保留大量预训练语义概念（<25% FFN神经元用于动作预测，其余维持语义结构）<br>2. **因果关联验证**：验证内部概念与动作的因果关系（如"慢"概念直接导致末端执行器缓慢移动）<br>3. **实时控制接口**：首次实现基于内部表征的零样本行为控制方法，突破微调/环境交互依赖<br><br>### 论文方法描述<br>1. **价值向量提取**<br> - 分析FFN输出层权重矩阵`Wθ`，提取独立于输入的固定值向量`wθ(i)`（式1-2）<br> - 将值向量投影至标记空间，通过标记概率分布赋予语义含义<br><br>2. **概念激活引导**<br> - 通过kNN聚类或人工选择识别语义对齐的神经元簇`𝒮`（如"up"、"slow"）<br> - 在推理时覆盖该簇的激活值为固定标量`α`（式3-4）<br> - 剩余神经元保持原始激活，生成控制残差`Δx`影响最终动作标记分布<br><br>3. **跨框架实现**<br> - PyTorch框架：在OpenVLA的FFN下投影层应用前向钩子<br> - JAX框架：修改π₀的FFN计算图插入引导算子`I𝒮^α`<br><br>### 论文使用数据集和训练资源<br>- **基础预训练数据**：Open X-Embodiment跨平台机器人数据集（OpenVLA/π₀预训练）<br>- **模拟评估**：LIBERO-Long长程操作基准（10项任务，OpenVLA 7B模型）<br>- **实物实验**：DROID平台数据用于π₀-FAST微调（LoRA方法，5000步）<br>- **硬件资源**：<br> - 模拟：NVIDIA H100 GPU<br> - 实物：NVIDIA A4500 GPU + UR5机械臂（配Robotiq 2F-140夹爪）<br><br>### 论文使用的评估环境和评估指标<br>#### 模拟实验（OpenVLA/LIBERO）<br>- **环境**：10个长程操作任务（抓取、放置、器具操作等）<br>- **干预参数**：对比"快/慢"与"上"概念簇，测试不同层深度注入效果<br>- **指标**：<br> - 末端执行器位移变化率（平均提升27.73%，最大148.54%）<br> - 统计显著性（配对t检验，p<0.001）<br> - 效应大小（Cohen's d范围：-0.091至-1.419）<br><br>#### 实物实验（π₀-FAST/UR5）<br>- **任务场景**：<br> - 低/高运输：玩具企鹅提升高度控制（75演示轨迹）<br> - 慢/快运输：玩具海豹速度控制（120演示轨迹）<br>- **基线对比**：无干预/提示词修改/随机向量干预<br>- **指标**：<br> - 低高组：末端最大高度分布（箱线图统计）<br> - 慢快组：平均位移/累积位移时间序列<br>- **关键发现**：<br> - 低/慢干预显著降低轨迹幅度（p<0.05）<br> - 高/快干预效果接近基线（模型已内建高速行为）<br> - 语义引导优于随机干预与提示词修改<br><br>> **注**：语义方向在不同任务/模型间存在迁移性差异，概念簇（如"小心"vs"卡顿"）可能引发混淆行为。</details> |
| 2025-08-28 | EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control | http://arxiv.org/abs/2508.21112 | <details><summary>展开</summary># EO-1: 通用机器人控制中的交错视觉-文本-动作预训练<br><br>## 论文研究单位<br>Shanghai AI Laboratory, Fudan University, AgiBot, Northwestern Polytechnical University<br><br>## 论文概述<br>EO-1是上海人工智能实验室等机构提出的统一具身基础模型，通过交错视觉-文本-动作预训练实现通用机器人控制。该研究针对当前视觉-语言-动作(VLA)模型在开放世界泛化和交错推理能力方面的不足，提出了统一的模型架构和大规模多模态数据集，在具身推理和机器人控制任务中展现出显著优势。<br><br>## 论文核心贡献点<br>1. **统一架构**：EO-1采用单一统一解码器转换器，整合离散自回归解码与连续流匹配去噪，无需额外动作特定参数即可实现跨模态知识传递<br>2. **交错式具身数据集**：构建包含150万样本的EO-Data1.5M数据集，专门针对交错视觉-文本-动作理解和学习<br>3. **真实世界泛化能力**：在ERQA、LIBERO、SimplerEnv等多个开源基准测试中超越现有模型，展现出强大的开放世界理解和控制能力<br><br>## 论文方法描述<br>EO-1基于预训练VLM构建统一解码器转换器架构，处理交错多模态输入序列：<br>- **输入处理**：文本标记器、视觉编码器、机器人状态投影器和动作去噪投影器将不同模态统一映射到R^d嵌入空间<br>- **共享骨干**：初始化自Qwen2.5-VL的转换器骨干，通过因果注意力处理整个交错序列<br>- **输出机制**：语言头用于文本解码，流头用于连续动作去噪生成<br>- **交错修正采样**：针对混合模态生成中的因果关系破坏问题，提出采样策略确保动作生成段的正确训练<br>- **训练目标**：结合自回归的下一个token预测和流匹配的向量场预测损失<br><br>## 论文使用数据集和训练资源<br>**数据规模**：<br>- 网络多模态数据：570万样本，71亿tokens<br>- 机器人控制数据：120万 эпизод，1273亿tokens<br>- 交错具身数据：EO-Data1.5M，10亿tokens<br><br>**训练资源**：<br>- 五个epoch训练，使用Flash-Attention变长打包<br>- 批量大小为1，平均序列长度16384<br>- 主干学习率5×10^-5，视觉编码器1×10^-5<br>- DeepSpeed ZeRO-1优化器<br>- 推理时仅需6GB GPU内存<br><br>## 论文使用的评估环境和评估指标<br>**评估环境**：<br>- **具身推理基准**：RoboVQA、ERQA、EO-Bench<br>- **机器人控制基准**：LIBERO、SimplerEnv<br>- **真实世界评估**：Franka Panda、WidowX 250S、Agibot G-1等多种机器人平台<br><br>**评估指标**：<br>- **具身推理**：BLEU-4分数(RoboVQA)，准确率(ERQA)，多选VQA准确率(EO-Bench)<br>- **机器人控制**：成功率(SR)，在Google Robot基准中采用匹配和聚合两种评估方式<br>- **多维度评估**：空间理解、物理常识、任务推理、状态估计四个维度共648个QA对<br><br>EO-1在所有基准测试中均展现出优异性能，平均成功率达98.2%，显著超越OpenVLA、π₀等现有开源模型。</details> |
| 2025-08-28 | CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification | http://arxiv.org/abs/2508.21046 | <details><summary>展开</summary>## 论文研究单位<br>School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen<br><br>## 论文概述<br>CogVLA是一个受人类多模态协调启发的视觉-语言-动作模型，通过指令驱动的路由和稀疏化技术，在保持高性能的同时显著降低计算成本。该模型采用三阶段渐进架构，模拟人类的视觉注意力系统（VAS）、补充运动区（SMA）和前运动皮质（PMC），分别用于感知聚焦、语义意图过滤和动作规划。在LIBERO仿真基准和真实世界机器人任务上，CogVLA实现了97.4%和70.0%的任务成功率，相比OpenVLA训练成本降低2.5倍、推理延迟减少2.8倍，同时在多个维度上优于现有高效VLA方法。<br><br>## 论文核心贡献点<br>- 提出认知对齐的CogVLA框架，通过EFA-Routing、LFP-Routing和CAtten模拟人类多模态协调机制，实现从感知到控制的端到端优化。<br>- 开发EFA-Routing和LFP-Routing，实现基于指令的视觉稀疏化，有效减少视觉令牌数量（Stage 1压缩至25%，Stage 2修剪50%）。<br>- 设计V-L-A Coupled Attention（CAtten），融合因果视觉-语言注意和双向动作并行解码，确保语义一致性和时间连贯性。<br>- 在LIBERO和ALOHA基准上验证了优越性能和效率，为可扩展机器人控制提供了解决方案。<br><br>## 论文方法描述<br>CogVLA采用三阶段渐进设计：<br>1. **Encoder-FiLM based Aggregation Routing (EFA-Routing)**：在视觉编码阶段，通过指令调制的FiLM模块动态聚合视觉令牌到聚合令牌（压缩至原始输入25%），并使用门控机制融合多个编码器分支（如SigLIP和DINOv2）。<br>2. **LLM-FiLM based Pruning Routing (LFP-Routing)**：在语言模型阶段，通过任务引导的剪枝路由器基于指令相关性筛选视觉令牌（减少50%令牌），保留语义关键信息以降低计算开销。<br>3. **V-L-A Coupled Attention (CAtten)**：采用混合注意机制，结合因果视觉-语言注意和双向动作注意，支持动作块的并行解码，确保跨模态逻辑一致性和时间连贯性。<br>整个流程支持指令驱动的端到端优化，通过稀疏化视觉输入和并行解码提高效率。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：<br> - 仿真环境：LIBERO基准，包含四个任务套件（空间、物体、目标、长时序），每个套件10个任务50个演示。<br> - 真实世界：Cobot Agilex ALOHA平台的三个长时序任务（物体放置、抽屉操作、T恤折叠），分别45、45、30个演示。<br>- **训练资源**：<br> - 使用4×A800 GPUs（80GB显存）进行训练和评估，得益于指令驱动的稀疏化策略。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真环境：LIBERO基准测试，模拟各种指令遵循任务。<br> - 真实世界：Cobot Agilex ALOHA平台，进行真实机器人操作评估。<br>- **评估指标**：<br> - 任务成功率（Success Rate）：以百分比衡量任务完成能力。<br> - 效率指标：推理时间（秒）、吞吐量（Hz）、FLOPs（计算量）、训练成本（小时/10k步）。<br> - 性能指标：在LIBERO套件上分空间、物体、目标、长时序子任务报告成功率；真实世界任务中统计子任务成功率（如抽屉操作的三步骤）。</details> |
| 2025-08-27 | Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies | http://arxiv.org/abs/2508.20072 | <details><summary>展开</summary># 论文研究单位<br>- The University of Hong Kong<br>- Shanghai AI Laboratory<br>- Shanghai Jiao Tong University<br>- Huawei Cloud Computing Technologies Co., Ltd.<br><br># 论文概述<br>Vision-Language-Action (VLA)模型通过将大视觉-语言模型适配来映射图像和指令到机器人动作。现有的VLAs主要采用两种范式：一是自回归(AR)方法，按固定顺序预测离散化的动作token；二是使用单独的MLP或扩散头将VLM输出映射到可执行控制。Discrete Diffusion VLA提出了首个将离散扩散应用于VLA的框架，通过单一transformer统一视觉、语言和动作，采用自适应解码顺序和二次重新掩码机制，实现了精确动作建模和一致的训练效果。<br><br># 论文核心贡献点<br>1. 首个离散扩散VLA框架，在单一transformer中统一动作生成与视觉-语言，保持强大性能<br>2. 开发了自适应解码策略和迭代重新掩码机制，支持并行动作token解码和错误纠正<br>3. 在Franka Panda、Google Robot和WidowX上验证，实现LIBERO上96.3%平均成功率，SimplerEnv–Fractal上64.1%和SimplerEnv–Bridge上54.2%的整体性能<br>4. 在统一架构中打破了AR模型的左到右瓶颈，减少了函数评估数量<br><br># 论文方法描述<br>Discrete Diffusion VLA将动作解码建模为离散扩散，通过掩码token去噪在相同的transformer内进行。具体方法包括：<br><br>**动作token化和分块**：每个连续控制维度被量化为256个bin的离散token，单步动作包含7个token（3个平移、3个旋转、1个夹爪）。将H个未来时间步的tokens排列成固定长度的动作块，总长度为L=H×D_act。<br><br>**统一架构**：基于OpenVLA架构，将因果注意力修改为双向transformer，所有token（视觉、语言和动作）都通过统一transformer处理，动作位置使用双向注意力。<br><br>**训练过程**：从掩码调度中采样掩码比例γ，用特殊token [MASK]替换γL个动作位置，对掩码位置最小化掩码交叉熵损失。<br><br>**推理过程**：从全掩码状态开始，执行T轮并行细化。每轮对当前掩码位置预测token分布，按置信度对掩码位置排序，保留top (1-γ_t)位置并重新掩码剩余部分。包含阈值检查和残差下降检查的二次重新掩码机制。<br><br># 论文使用数据集和训练资源<br>在三个机器人设置上评估：<br>1. LIBERO：Franka Panda机械臂，包含四个套件（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个套件有10个任务和500个专家演示<br>2. SimplerEnv-Fractal：Google Robot，在Fractal数据集上训练，评估视觉匹配和变体聚合性能<br>3. SimplerEnv-Bridge：WidowX机器人，在BridgeData-V2上训练和评估<br><br>使用与OpenVLA相同的VLM主干（Prismatic-7B）进行微调，图像调整为224×224像素，批量训练采用标准协议。<br><br># 论文使用的评估环境和评估指标<br>**LIBERO**：报告四个套件上的成功率(SR)，每套500次rollouts（10个任务×50集）<br>**SimplerEnv-Fractal**：报告视觉匹配和变体聚合的成功率，包含Pick Coke、Mv Near、Drawer三个任务的评估<br>**SimplerEnv-Bridge**：报告四个任务上的部分和整体成功率，包括Put Spoon on Towel、Put Carrot on Plate、Stack Green on Yellow、Put Eggplant in Basket<br><br>所有基准测试仅使用RGB图像输入（第三人称视角和手腕视角），语言指令和可选末端执行器位置，不使用深度信息或辅助信息。</details> |
| 2025-08-27 | Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation | http://arxiv.org/abs/2508.19958 | <details><summary>展开</summary>待生成</details> |
| 2025-08-27 | Ego-centric Predictive Model Conditioned on Hand Trajectories | http://arxiv.org/abs/2508.19852 | <details><summary>展开</summary>### 论文研究单位<br>未明确提供<br><br>### 论文概述<br>本文提出Ego-PM（Ego-centric Predictive Model），一种基于手部轨迹条件的自我中心预测模型，旨在同时预测未来动作（手部轨迹）和视觉结果。该模型采用两阶段统一框架：第一阶段通过连续状态建模显式预测手部轨迹，第二阶段引入因果交叉注意力融合多模态线索，引导潜扩散模型逐帧生成视频。Ego-PM是首个同时处理人类自我中心活动和机器人操作任务的模型，无需额外标注即可自主预测动作及其视觉后果。<br><br>### 论文核心贡献点<br>1. **首个统一预测模型**：首次实现同时预测未来动作和视觉帧，解决以往方法分离处理动作与视觉预测的局限。<br>2. **连续状态建模（CoSMo）与注意力机制**：提出CoSMo策略利用历史状态预测手部轨迹，并设计因果交叉注意力增强动作条件融合，显著提升预测连贯性。<br>3. **跨场景通用性**：首个统一模型适用于人类自我中心视角（Ego4D）和机器人操作（BridgeData、RLBench），在动作预测与视频生成任务上均超越基线。<br><br>### 论文方法描述<br>模型分为两个训练阶段：<br>1. **阶段一：显式动作建模**<br> - **视觉编码器**：使用CLIP提取帧特征并投影至文本嵌入空间。<br> - **动作编码器/解码器**：设计轻量MLP处理动作嵌入，通过特殊令牌<ACT>标识动作。<br> - **自回归模型**：基于LLaVA处理多模态序列（视觉、文本、动作）。<br> - **连续状态建模（CoSMo）**：采用相邻两个状态（t和t-1）作为输入，增强时序依赖性。<br>2. **阶段二：动作增强帧预测**<br> - **多模态条件融合**：通过因果交叉注意力将视觉、文本与动作嵌入对齐，约束未来帧生成。<br> - **帧预测**：基于潜扩散模型（LDM），以融合条件为指导，从最后一帧开始迭代生成未来帧。<br>训练目标包括阶段一的语言与动作损失（L1+GIoU），以及阶段二的扩散损失。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 人类活动：Ego4D（PRE-15和PNR时刻，文本叙述与手部轨迹）。<br> - 机器人操作：BridgeData V2（人类演示视频与指令）和RLBench（9项多任务评估）。<br>- **训练资源**：<br> - 输入分辨率：256×256。<br> - 初始化权重：LLaVA（Ego4D）或OpenVLA（BridgeData），LDM使用Stable Diffusion。<br> - 训练配置：微调3个epoch，损失权重λ1=0.1、λ2=0.01。<br> - 计算资源：未明确说明（原文未提及硬件细节）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：未明确提供具体环境（如硬件或软件框架），依赖学术评估设置。<br>- **评估指标**：<br> - **帧预测**：<br> - 对齐分数：EgoVLP、EgoVLP+（视频-文本）。<br> - 图像质量：CLIP相似度（越高越好）、FID（越低越好）、PSNR（越高越好）、SSIM（越高越好）、LPIPS（越低越好）。<br> - **动作预测**：<br> - Ego4D：手部掩码IoU（预测与真实区域重叠，越高越好）。<br> - BridgeData/RLBench：任务成功率（百分比，越高越好）。</details> |
| 2025-08-15 | TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models | http://arxiv.org/abs/2508.19257 | <details><summary>展开</summary>### 论文研究单位<br>北京大学 (Peking University)<br><br>### 论文概述<br>当前视觉-语言-动作（VLA）模型在处理连续帧时缺乏时序信息整合，逐帧独立处理导致视觉噪声敏感性和时序关联丢失。论文提出TTF-VLA（时序令牌融合），通过融合历史与当前视觉特征增强VLA推理质量，在不训练的前提下改善机器人操作任务的鲁棒性和效率。<br><br>### 论文核心贡献点<br>1. **双维度检测机制**：提出结合灰度像素差异分析与注意力语义相关性的检测方法，用于识别视觉变化区域和任务关键区域<br>2. **硬融合策略与关键帧**：采用二值令牌选择决策，并通过周期性关键帧重置防止误差累积<br>3. **跨模型适用性**：验证在OpenVLA和VLA-Cache架构上的通用性，无需任务特定调参<br>4. **Query矩阵复用发现**：揭示选择性Query矩阵复用可提升性能，提出KQV矩阵直接复用的潜在加速方向<br><br>### 论文方法描述<br>1. **时序令牌融合框架**<br> - 输入：当前帧I_t、历史帧I_{t-1}、历史令牌T_{t-1}、任务指令L_t<br> - 输出：融合令牌T̃_t<br> - 公式：T̃_t = F(T_t, T_{t-1}, I_t, I_{t-1}, L_t)<br><br>2. **硬融合决策**<br> - 对每个令牌i选择当前或历史令牌：<br> t̃_t^(i) = t_t^(i) 若 m_i^fusion=1，否则 t_{t-1}^(i)<br><br>3. **关键帧机制**<br> - 定期重置所有令牌：IsKeyframe(t) = (t mod K=0) ∨ (T_{t-1}=∅)<br> - 参数K=3平衡时序稳定性和响应速度<br><br>4. **双维度检测融合**<br> - 像素维度：计算灰度图绝对差异d_i^pixel = (1/196) ∑\|G_t(u,v)-G_{t-1}(u,v)\|<br> - 注意力维度：提取文本-视觉和动作-视觉注意力S_text^((l))和S_action^((l))<br> - 最终融合：m_i^fusion = m_i^pixel ∨ m_i^attention<br><br>### 论文使用数据集和训练资源<br>1. **仿真数据集**<br> - LIBERO：4套任务（Object/Spatial/Goal/Long），每套10任务×20 episodes<br> - SimplerEnv：3任务（Move Near/Pick Coke/Drawer），总756 episodes<br><br>2. **真实机器人**<br> - 设备：Franka Research 3机械臂<br> - 数据：3任务×80 demos，5Hz操作频率<br><br>3. **训练资源**<br> - 微调配置：OpenVLA-7B，20,000步，batch size=8，8×A100 GPUs<br> - 推理部署：单A100 GPU，5Hz控制频率<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**<br> - 仿真：LIBERO基准套件 + SimplerEnv跨环境平台<br> - 真实：Franka机械臂实验室场景<br><br>2. **评估指标**<br> - **任务成功率**：成功执行次数/总次数<br> - **时序融合率**：复用历史令牌比例<br> - **计算效率**：令牌复用带来的加速效果<br><br>3. **主要结果**<br> - LIBERO平均提升：OpenVLA +4.0pp (72.4% vs 68.4%)，VLA-Cache +2.7pp<br> - SimplerEnv跨环境：+4.8%相对提升<br> - 真实机器人：+8.7%相对提升，Pick-and-Place任务改善显著<br> - 消融研究：双维度融合优于单一维度，关键帧K=3性能最优<br><br>论文证明TTF-VLA能有效提升VLA模型在多环境下的操作能力，特别是在长序列任务和噪声场景中表现突出，为时序信息利用和计算加速提供了新方向。</details> |
| 2025-08-26 | MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2508.19236 | <details><summary>展开</summary>### 论文研究单位<br>清华大学自动化系BNRist、Dexmal、MEGVII Technology、天津大学、哈尔滨工业大学、StepFun<br><br>### 论文概述<br>MemoryVLA是一个受认知科学启发的视觉-语言-动作模型框架，通过引入感知-认知记忆机制解决机器人操作中的长期时间依赖问题。该方法结合了工作记忆（短期神经网络活动）和长时记忆（海马体系统），用于处理非马尔可夫性的操作任务，如"推按钮"等视觉变化不明显的场景。<br><br>### 论文核心贡献点<br>- 提出认知-记忆-行动框架，利用VLM常识先验和记忆机制建模长期时间依赖<br>- 设计感知-认知记忆库（PCMB），支持高低层特征的记忆检索、门控融合和记忆整合<br>- 实现记忆条件化扩散动作专家，生成时间感知的动作序列<br>- 在150+仿真和真实任务上达到SOTA性能，长期任务提升显著<br><br>### 论文方法描述<br>1. **视觉-语言认知模块**：<br> - 使用7B Prismatic VLM处理RGB图像和语言指令<br> - DINOv2和SigLIP视觉编码器提取感知token（256维）<br> - LLaMA-7B生成认知token（1维），形成工作记忆<br><br>2. **感知-认知记忆模块**：<br> - **记忆检索**：通过带时间位置编码的交叉注意力从PCMB获取历史上下文<br> - **门控融合**：用学习门控自适应融合当前token和检索内容<br> - **记忆整合**：当容量满时合并时间相邻且语义相似的条目<br><br>3. **记忆条件化动作专家**：<br> - 基于扩散的Transformer（DiT）生成16步7-DoF动作序列<br> - 认知token提供高层语义指导，感知token补充细节<br> - DDIM采样10步，使用分类器自由引导（CFG）<br><br>### 论文使用数据集和训练资源<br>- **数据集**：Bridge v2、RT-1、LIBERO（5套件）、真实世界数据<br>- **训练资源**：8块NVIDIA A100 GPU，PyTorch FSDP，全局批大小256，学习率2e-5<br>- **模型参数**：VLM 7B，扩散专家约300M<br><br>### 论文使用的评估环境和评估指标<br>- **仿真环境**：<br> - SimplerEnv-Bridge（WidowX机器人）<br> - SimplerEnv-Fractal（Google机器人，含VM/VA设置）<br> - LIBERO（Franka机器人，5套件：Spatial/Object/Goal/Long/LIBERO-90）<br>- **真实环境**：<br> - Franka和WidowX机器人<br> - Intel RealSense D435 RGB摄像头（640×480→224×224）<br> - ROS集成系统<br>- **评估指标**：<br> - 成功率（%），每个任务15-50次试验<br> - 长期任务采用子目标逐步评分</details> |
| 2025-08-25 | FlowVLA: Thinking in Motion with a Visual Chain of Thought | http://arxiv.org/abs/2508.18269 | <details><summary>展开</summary># 论文研究单位<br>未明确标注单位，仅在作者列表中出现两个上标标记；推断主要来自多机构合作，具体单位信息未在页面中明确给出。<br><br># 论文概述<br>当前视觉-语言-动作（Vision-Language-Action, VLA）模型常以“下一帧预测”作为世界模型的预训练范式，直接从当前帧预测未来帧。这种跳过显式物理推理的方式容易导致“像素复制陷阱”，使预测在物理上不可信，并在长时序中不稳定。为解决该问题，论文提出“视觉思维链”（Visual Chain of Thought, Visual CoT），将预测分解为显式的“先推理运动，再生成外观”的结构化过程，具体采用v_t→f_t→v_{t+1}的因果链条，并以光流f_t作为中间运动表示。论文据此构建FlowVLA：一个两阶段的统一自回归Transformer，第一阶段用Visual CoT进行世界模型预训练，第二阶段进行策略微调以生成动作块。FlowVLA在LIBERO、SimplerEnv等仿真基准以及AgileX Cobot真实机器人平台上实现了更物理合理、更高效的策略学习，并提升样本效率和收敛速度。<br><br># 论文核心贡献点<br>- 指出下一帧预测范式的根本缺陷（像素复制陷阱、缺乏物理因果性），提出“视觉思维链”（Visual CoT）作为世界模型学习新范式。<br>- 通过v_t→f_t→v_{t+1}显式将运动推理与外观生成解耦，并在预训练中引入强归纳偏置以促进动态理解。<br>- 设计FlowVLA：单一统一自回归Transformer，使用共享的矢量量化（VQ）分词器将RGB帧与光流映射为同一词表，实现端到端的跨模态推理。<br>- 两阶段训练：阶段一以Visual CoT进行大规模视频的世界模型预训练；阶段二微调为策略模型以生成动作块，显著缩小预训练-下游任务的领域鸿沟。<br>- 在仿真和真实平台取得最优或大幅领先的性能，并验证样本效率、收敛速度与长时序规划能力提升。<br><br># 论文方法描述<br>- 视觉思维链（Visual CoT）<br> - 将建模目标从P(v_{t+1}\|v_t, L)转化为联合建模P(v_{t+1}, f_t\|v_t, L)，并因式分解为两步：<br> 1) 运动推理：P(f_t\|v_t, L)，要求显式预测光流f_t；<br> 2) 外观生成：P(v_{t+1}\|f_t, v_t, L)，在给定光流条件下生成下一帧。<br> - 将学习从像素回归重构为结构化物理推理，为策略学习提供与“如何动”对齐的中间表示。<br>- 两阶段训练框架<br> - 阶段一：世界模型预训练（Visual CoT）<br> - 输入序列构造为交错的出现（帧）与运动（光流）token：S_wm = {L_instr, v_0, f_0, v_1, f_1, …, v_T, f_T}。<br> - 统一分词：用同一VQ-GAN分词器将RGB帧与光流映射为离散token；光流由RAFT计算，并通过VideoJAM方式将二维位移编码为3通道RGB图，再进行非线性归一化以保留细微运动。<br> - 训练目标：L_WM为对flow token与next-frame token的交叉熵损失之和（通常权重λ=1），以标准的“下一token预测”优化，交错预测f_t与v_{t+1}。<br> - 阶段二：策略微调（动作预测）<br> - 权重初始化自预训练世界模型。<br> - 输入序列为S_policy = {L_instr, v_0, a_0, v_1, a_1, …}；动作按FAST方法离散化为token。<br> - 训练目标：L_policy仅在动作token上计算交叉熵，使模型将已学到的视觉-动力知识聚焦于行动生成。<br>- 统一与简洁性<br> - 无需引入运动特定的网络分支：光流与帧共享同一分词器，单一自回归Transformer即可学习跨模态交错序列，兼具参数与结构效率。<br><br># 论文使用数据集和训练资源<br>- 数据与基准<br> - 仿真基准：LIBERO（空间/目标/长期组合四套Suite）、SimplerEnv-WidowX（评估域移鲁棒性：光照、纹理、视角变化）。<br> - 真实机器人：AgileX Cobot双机械臂平台，包含腕部与前向摄像头；设计四项单臂与双臂任务，采集每任务50–200条人类遥操作演示用于微调。<br>- 预训练与微调设置（典型）<br> - 基于Emu3（约8.5B参数）架构；光流由RAFT预计算；分词采用VQ-GAN。<br> - LIBERO：世界模型预训练约5k步（batch=16）；策略微调约5k步（batch=96）。<br> - SimplerEnv：预训练约12k步（batch=32）；微调约20k步（batch=128）。<br>- 实现细节<br> - 光流转换：2通道(u, v)映射为3通道RGB，方向→色相，速度→饱和度与明度；非线性归一化保留细微运动并避免饱和。<br> - 损失平衡：L_WM中λ=1，交错预测flow与帧；动作token化遵循FAST。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境<br> - LIBERO四套Suite（空间泛化、目标泛化、目标改变、长期组合）以及SimplerEnv-WidowX域移场景。<br> - 真实AgileX Cobot平台的双臂操作任务。<br>- 评估指标<br> - 任务成功率（%）：在LIBERO与SimplerEnv的仿真结果以及真实机器人任务上进行报告，通常每任务多次试验（真实实验每任务25次）以保证统计可靠性。<br> - 收敛与样本效率：通过成功率和训练步数曲线，对比FlowVLA与基线（如UniVLA）在全数据与低数据（50%）条件下的收敛速度与最终性能。</details> |
| 2025-08-23 | NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows | http://arxiv.org/abs/2508.16845 | <details><summary>展开</summary>待生成</details> |
| 2025-08-22 | Do What? Teaching Vision-Language-Action Models to Reject the Impossible | http://arxiv.org/abs/2508.16292 | <details><summary>展开</summary># 论文研究单位<br>University of California, Berkeley<br><br># 论文概述<br>该论文探讨了视觉-语言-动作（VLA）模型在处理基于错误前提的指令时面临的挑战，即那些引用环境中不存在的对象或条件的指令。作者指出，当前大多数VLA模型缺乏识别和回应此类不可能指令的机制。为了解决这个问题，论文提出了一个名为“Instruct-Verify-and-Act (IVA)”的统一框架。该框架旨在使VLA模型能够（一）检测因错误前提而无法执行的指令，（二）通过语言进行澄清或纠正，以及（三）将感知和动作与合理的替代方案相结合。论文通过构建一个包含成对正确指令和错误前提指令的大规模半合成数据集，对VLA模型进行了指令微调，实现了对错误前提指令的鲁棒检测和自然语言纠正。<br><br># 论文核心贡献点<br>1. 提出了Instruct-Verify-and-Act (IVA)框架，这是首个致力于让VLA模型能够识别、解释并纠正错误前提指令的统一框架。<br>2. 构建了一个大规模、上下文增强的半合成指令数据集，该数据集包含成对的正样本和错误前提指令，覆盖了“域内”和“域外”两种错误类型，专门用于训练模型处理此类情况。<br>3. 实验结果表明，与基线模型相比，IVA在错误前提检测准确率上提升了97.56%，在错误前提场景下的成功响应率提升了50.78%，同时保持了处理正常指令的基本能力。<br><br># 论文方法描述<br>论文的方法基于LLARVA模型，一个为机器人指令跟踪设计的VLA架构。该方法主要包括模型构建、数据集生成和训练过程三个部分。<br>模型上，IVA由三部分组成：一个冻结的CLIP ViT-L/14视觉编码器，用于将RGB图像观测编码为视觉token；一个语言编码器，用于对包含机器人类型、控制模式、任务描述和先前本体感觉状态的结构化自然语言指令进行编码；一个多模态自回归transformer解码器，用于融合视觉和语言token，并预测未来的机器人动作序列和2D视觉轨迹。<br>数据集上，作者在RLBench数据集的轨迹基础上生成了一个错误前提指令数据集。该数据集包含两类错误指令：“域内错误指令”，指引用场景中存在但当前任务不相关的对象，期望模型进行纠正；“域外错误指令”，指引用完全不可能的对象（如大象），期望模型拒绝。训练数据按特定比例混合了正样本和错误指令。<br>训练上，作者采用端到端的指令微调方法。在保持视觉和语言编码器冻结的同时，使用LoRA适配器对多模态解码器进行微调。训练目标是让模型在给定图像和指令后，能够自回归地生成正确的动作序列或适当的语言纠正/拒绝响应。<br><br># 论文使用数据集和训练资源<br>1. **数据集**:<br> * **预训练数据**: Open X-Embodiment (OXE) 数据集。<br> * **微调数据**: 基于RLBench轨迹构建的自定义“错误前提指令数据集”。<br> * **数据构成**: 每个任务使用800个回合进行训练。其中约20%的回合包含域外错误指令，约65%的回合包含域内错误指令（这些指令被随机注入到10%的步骤中）。<br>2. **训练资源**:<br> * 论文未明确说明所使用的具体硬件资源（如GPU类型和数量），但其方法涉及对大型多模态模型（基于LLARVA）进行微调，使用了LoRA适配器，这通常需要相当大的GPU计算资源。<br><br># 论文使用的评估环境和评估指标<br>1. **评估环境**:<br> * **模拟器**: RLBench。<br> * **任务**: 9个RLBench任务。<br> * **设置**: 每个任务生成25个评估回合，物体位置随机化。模型输入为前摄像头视角图像和前5个时间步的机器人关节位置。<br> * **基线模型**: LLARVA模型。<br>2. **评估指标**:<br> * **错误前提 (FP) 检测率**: 模型正确识别出指令为错误前提的百分比。该指标分别针对“域内”和“域外”错误指令进行报告。<br> * **错误前提成功率**: 模型在接收到错误前提指令后，能够做出成功响应（如：对于域内指令提出正确替代方案，或对于域外指令明确拒绝）的百分比。<br> * **真实前提 (TP) 成功率**: 模型在接收到正确、可执行的指令后，成功完成任务的百分比。<br> * **总体成功率**: 错误前提成功率和真实前提成功率的平均值，作为综合性能的衡量标准。</details> |
| 2025-08-21 | Survey of Vision-Language-Action Models for Embodied Manipulation | http://arxiv.org/abs/2508.15201 | <details><summary>展开</summary>待生成</details> |
| 2025-08-19 | CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models | http://arxiv.org/abs/2508.13446 | <details><summary>展开</summary>待生成</details> |
| 2025-08-18 | Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy | http://arxiv.org/abs/2508.13103 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br><br>浙江大学计算机科学与技术学院、上海人工智能实验室、商汤科技研究院、南京大学、清华大学<br><br>## 论文概述<br><br>论文提出了Observation-Centric VLA (OC-VLA)框架，旨在解决视觉-语言-动作（VLA）模型在真实环境中的泛化问题。现有VLA模型存在观察空间与动作空间不一致的问题：训练数据来自不同相机视角，但模型通常在机器人基坐标系中预测末端执行器姿态，导致空间不一致。OC-VLA通过利用相机外参标定矩阵，将动作预测从机器人基坐标系转换到相机坐标系，统一了不同视角下的预测目标，从而提高模型对相机视角变化的鲁棒性和泛化能力。<br><br>## 论文核心贡献点<br><br>提出观察中心的VLA框架，将动作预测直接锚定在相机观察空间，通过相机外参矩阵实现坐标系转换<br><br>轻量级、即插即用的策略，与现有VLA架构完全兼容，无需架构修改<br><br>从优化角度分析了相机坐标系预测相比机器人坐标系预测的优势，相机坐标与图像坐标的转换仅需内参矩阵，而机器人坐标转换需要外参矩阵<br><br>在仿真和真实机器人实验中验证了方法的有效性，显著提升任务成功率、加速收敛并增强跨视角泛化能力<br><br>## 论文方法描述<br><br>模型架构采用轻量级VLA模型（约334M参数），使用CLIP文本编码器处理语言指令，DINOv2处理RGB图像，通过Q-Former（4层）进行特征融合和压缩，最后使用LLaMA2风格的Transformer（12层）预测动作<br><br>支持连续动作空间（使用Diffusion Transformer，DDPM 100步训练，DDIM 10步推理）和离散动作空间（非自回归预测）两种模式<br><br>坐标系转换：利用相机外参矩阵T，将机器人基坐标系中的末端执行器姿态转换到相机坐标系，公式为A_cam = T × A_world × T^(-1)<br><br>训练时使用相机坐标系下的动作作为监督目标，推理时将预测的相机坐标系动作转换回机器人坐标系用于机器人执行<br><br>## 论文使用数据集和训练资源<br><br>预训练数据集：Droid数据集，包含来自1417个不同第三人称相机视角的机器人操作轨迹<br><br>仿真评估数据：ManiSkill2数据集，选择5个任务（PickCube, StackCube, PickSingleYCB, PickClutterYCB, PickSingleEGAD），生成约40,000条轨迹，每条轨迹从300,000个随机相机视角池中采样20个相机进行渲染<br><br>真实机器人数据：使用Franka Emika Panda机械臂收集两组数据，Camera 1收集15个任务（固定相机），Camera 2收集8个任务（相机位置有轻微扰动），每个任务10条演示轨迹（10-shot设置）<br><br>训练资源：8张NVIDIA A100 GPU，总batch size 2048（每GPU 256样本），使用AdamW优化器训练30,000步，学习率为Transformer和Q-Former 1e-4，DINOv2为1e-5<br><br>## 论文使用的评估环境和评估指标<br><br>仿真评估环境：ManiSkill2 benchmark，5个任务类型，每个任务从验证集随机采样100条轨迹进行评估（共500条轨迹）<br><br>真实机器人平台：Franka Emika Panda 7自由度机械臂，配备Robotiq 2F-85夹爪和多个RealSense D435i RGB-D相机<br><br>评估指标：任务成功率（Success Rate）<br><br>评估设置包括三种场景：固定相机视角（与训练视角一致）、轻微相机扰动（训练时引入相机位置变化）、新相机视角（零样本评估，使用训练时未见过的相机）<br><br>基线方法：OpenVLA-OFT、π0、以及使用机器人基坐标系预测的相同架构模型<br><br>真实机器人任务涵盖pick & place、pouring、stacking、pick & rotation、pull & push等多种类型，共15个任务，每个任务进行10次试验</details> |
| 2025-08-18 | Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey | http://arxiv.org/abs/2508.13073 | <details><summary>展开</summary># 论文研究单位<br>哈尔滨工业大学（深圳）计算机科学与技术学院<br><br># 论文概述<br>论文首次系统性地综述了基于大型视觉-语言模型（VLM）的视觉-语言-行动（VLA）模型在机器人操作中的应用，涵盖单片式与分层式架构、与其他学习范式的结合、关键特性、数据集与基准，以及未来方向。图1概述了VLA模型的核心优势：开放世界泛化、分层任务规划、知识增强推理与多模态融合。图2给出了综述组织与时间线；图3展示了两类主范式。<br><br># 论文核心贡献点<br>- 提出清晰的VLA定义与两级分类：单片式（包括单系统与双系统）与分层式（规划器+执行器，带可解释中间表示）。<br>- 纵向梳理VLM与操作学习演进路线，呈现大型VLM驱动VLA的历史脉络与代表工作。<br>- 横向比较两类架构的优缺点：单片式强调端到端统一与推理效率，分层式强调可解释的中间表示与模块化训练。<br>- 汇总高级前沿方向：强化学习、无训练优化、从人类视频学习、世界模型融合等。<br>- 系统总结VLA特征：多模态融合、指令跟随、多维泛化。<br>- 分类整理数据集与基准：现实机器人、仿真、人类行为、具身任务。<br>- 识别未来方向：记忆机制、4D感知、高效适配、多智能体协同等。<br><br># 论文方法描述<br>- 架构范式<br> - 单片式（Monolithic）<br> - 单系统（Single-system）：视觉、语言与机器人状态统一输入，LLM自回归生成离散动作令牌（RT-1/2、OpenVLA等）。<br> - 双系统（Dual-system）：系统2（VLM背骨）负责慢速但泛化的推理；系统1（动作专家）负责快速执行（DP-VLA、RoboDual、LCB、GR00T N1、CogACT、ChatVLA系列、Fast-in-Slow等）。<br> - 分层式（Hierarchical）：显式解耦规划与执行，通过可解释中间表示（子任务、关键点、程序、轨迹、可用性地图等）连接Planner与Policy；支持子任务式、关键点式、程序式等子方法。<br>- 性能增强<br> - 感知增强：引入3D与4D时序、触觉、音频等多模态（Leo Agent、SpatialVLA、TraceVLA、4D-VLA、ST-VLA、VTLA、VLAS、FuSe、OE-VLA）。<br> - 推理增强：链式思维与视觉链式思维（ECoT、CoT-VLA）、层级闭环控制（LoHoVLA）、选择性迁移微调（ReFineVLA）。<br> - 泛化增强：统一行动码本（UniAct）、可逆训练（ReVLA）、扩散+自回归协同生成（HybridVLA）、集成投票（VOTE）、世界模型/动力学学习（WorldVLA、UnifiedVLA、UP-VLA）。<br>- 推理效率优化<br> - 架构：层路由/早退（MoLe-VLA、DeeR-VLA、CogVLA），引入Mamba结构（RoboMamba）。<br> - 参数：小参数模型（NORA）、低位宽/三值权重（BitVLA）。<br> - 解码：并行解码（RoboFlamingo、PD-VLA）、投机解码（Spec-VLA）、触发式跳推理（FlashVLA）。<br>- 其他高级方向<br> - 强化学习（RL）集成、免训练优化、从人类视频学习、世界模型驱动VLA等。<br><br># 论文使用数据集和训练资源<br>- 现实机器人数据集与基准：Open X-Embodiment（跨实体）、BridgeData、DROID、RT-1/RT-2演示集等。<br>- 仿真数据集与基准：常用模拟操控数据集与评测基准（用于高效训练与泛化评估）。<br>- 人类行为数据集：人类演示视频与交互数据（支持模仿/从视频学习）。<br>- 具身数据集与基准：多任务/多场景具身评测集合。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：真实机器人平台与仿真环境；跨实体、跨任务与多模态组合的具身场景。<br>- 评估指标：典型操控任务指标，如成功率、路径误差、任务完成度、泛化到新对象/指令、推理质量与效率指标（延迟/吞吐）；论文以综述性评估与特性分析为主，未提供统一的数值对比。</details> |
| 2025-08-17 | Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search | http://arxiv.org/abs/2508.12211 | <details><summary>展开</summary># 论文总结<br><br>**论文研究单位**<br>- Mila — Quebec AI Institute, Canada<br>- Université de Montréal, Canada<br>- The University of British Columbia, Canada<br><br>**论文概述**<br>提出视觉-语言-动作规划与搜索（VLAPS）框架，通过将基于模型的搜索（MCTS）集成到预训练VLA策略的推理过程中，解决VLA模型在复杂环境中的脆弱性问题。该方法利用VLA模型指导搜索过程，使其能够高效探索大空间中的语言条件机器人任务，显著提升成功率（最高67个百分点）。<br><br>**论文核心贡献点**<br>1. 集成VLA策略与MCTS的VLAPS框架<br>2. 自动定义任务导向搜索空间的算法<br>3. 基于VLA的节点选择引导机制<br>4. 实验验证在所有测试场景中均优于VLA基线<br>5. 自适应计算分配机制（失败任务分配更多搜索时间）<br><br>**论文方法描述**<br>1. **VLA驱动的行动块采样**<br> - 定义有限行动块库Φ（2000个K-medoids聚类原型）<br> - 使用VLA策略构建上下文相关采样分布βΦ<br> - 每个节点采样k=10个候选行动块进行搜索<br><br>2. **VLA引导的树遍历**<br> - 采用PUCT选择策略（Q≡0）<br> - 使用VLA先验ψΦv分配节点访问次数<br> - 仿真VLA策略至任务完成或达最大步长<br><br>3. **搜索终止条件**<br> - 发现目标状态即返回轨迹<br> - 或在600秒计算预算内返回最高频动作序列<br><br>**论文使用数据集和训练资源**<br>- **数据集**：Libero模拟机器人数据集（五个任务套件：Spatial、Goal、Object、10、90）<br>- **基础VLA模型**：Octo（97M参数）<br>- **训练资源**：<br> - NVIDIA A100 GPU<br> - 演示数据过滤no-op动作<br> - K-medoids聚类生成2000个行动块原型<br><br>**论文使用的评估环境和评估指标**<br>- **评估环境**：<br> - Libero模拟套件<br> - 256×256固定相机图像<br> - 128×128腕部相机图像<br>- **评估指标**：<br> - **任务成功率**（基于1000次测试）<br> - **算法运行时间**（仅统计成功案例）<br> - **不同微调阶段的性能对比**（10k-200k步）<br>- **实验设置**：<br> - 每次搜索300蒙特卡洛样本<br> - 行动块时域H=4<br> - 最大搜索深度100<br> - 600秒超时限制<br><br>实验显示VLAPS在所有任务套件中始终优于VLA基线，尤其在基础成功率较低时提升最大（如Libero-Object从6%提升至73%）。当基础VLA改进时，VLAPS搜索时间显著减少。自适应计算分配机制使失败任务获得更多搜索资源。</details> |
| 2025-08-16 | Toward General Physical Intelligence for Resilient Agile Manufacturing Automation | http://arxiv.org/abs/2508.11960 | <details><summary>展开</summary>待生成</details> |
| 2025-08-14 | CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model | http://arxiv.org/abs/2508.10416 | <details><summary>展开</summary>论文研究单位<br>- 作者信息：Zhuoyuan Yu, Yuxing Long, Zihan Yang, Chengyan Zeng, Hongwei Fan, Jiyao Zhang, Hao Dong<br><br>论文概述<br>- 本文提出了一种名为"Self-correction Flywheel"的后训练范式，用于增强视觉-语言-动作(VLA)导航模型。现有视觉-语言导航模型在执行指令时常偏离正确轨迹，且缺乏有效的错误纠正能力。该方法将模型在训练集上的错误轨迹视为有价值的数据源，通过检测这些错误轨迹的偏差并自动生成感知和动作的自我纠正数据来驱动模型的持续训练。经过多轮飞轮迭代，开发的单目RGB导航模型CorrectNav在R2R-CE和RxR-CE基准测试中分别达到65.1%和69.3%的新SOTA成功率。<br><br>论文核心贡献点<br>- 提出了Self-correction Flywheel后训练范式，将模型错误轨迹转化为训练数据<br>- 开发了自动轨迹偏差检测方法，精确定位错误位置<br>- 设计了创新的自我纠正数据生成技术，从动作和感知角度创建纠错数据<br>- 实现了多轮飞轮迭代训练机制，持续提升导航模型性能<br>- 开发的CorrectNav模型在模拟和真实机器人环境中均表现出优异的纠错和导航能力<br><br>论文方法描述<br>- 模型结构：CorrectNav由SigLIP视觉编码器、2层MLP投影器和Qwen2大语言模型组成，初始化自LLaVA-Video 7B<br>- 导航微调策略：包括导航动作预测（使用210万+步级数据）、轨迹指令生成和通用多模态数据召回，并实施相机高度、视野等随机化增强视觉多样性<br>- Self-correction Flywheel流程：(1)在训练集评估模型收集错误轨迹；(2)检测偏差并定位；(3)创建纠错轨迹数据（使用轨迹规划器生成恢复路径）和关键帧感知数据（使用多模态LLM生成描述和QA）；(4)继续训练模型；(5)多轮迭代<br><br>论文使用数据集和训练资源<br>- 数据集：VLN-CE的R2R和RxR训练集，包括52.7K样本（R2R）和158万样本（RxR）<br>- 训练资源：8块NVIDIA A100 GPU，导航微调需80小时，单次飞轮迭代需20小时<br>- 多模态数据：从LLaVA-Video数据集中随机采样24万个ActivityQA和NextQA训练实例<br><br>论文使用的评估环境和评估指标<br>- 模拟环境：使用Habitat 3.0模拟器在VLN-CE基准的R2R-CE和RxR-CE的Val-Unseen分割上评估<br>- 真实环境：在AgiBot Lingxi D1四足机器人平台上测试，部署在远程A100 GPU服务器<br>- 评估指标：导航错误(NE)、成功率(SR)、加权路径长度(SPL)、标准化动态时间规整(nDTW)、路径长度误差(NE<3m)</details> |
| 2025-08-14 | Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning | http://arxiv.org/abs/2508.10399 | <details><summary>展开</summary>待生成</details> |
| 2025-08-14 | ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver | http://arxiv.org/abs/2508.10333 | <details><summary>展开</summary>待生成</details> |
| 2025-08-12 | GeoVLA: Empowering 3D Representations in Vision-Language-Action Models | http://arxiv.org/abs/2508.09071 | <details><summary>展开</summary>待生成</details> |
| 2025-08-12 | Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding | http://arxiv.org/abs/2508.09032 | <details><summary>展开</summary>### 论文研究单位<br>莫斯科物理理工学院 (MIPT), 俄罗斯多尔戈普鲁德内, 141701<br>人工智能研究所 (AIRI), 俄罗斯莫斯科, 121170<br><br>### 论文概述<br>该论文提出了一种名为“Spatial Traces”的新方法，旨在增强视觉-语言-动作（VLA）模型。该方法通过将历史关键点的视觉轨迹投影到深度图上，将空间信息和时间信息集成到一个统一的视觉提示中。这种设计使得模型能够同时捕捉空间和时间上下文，从而提高在复杂操作任务中的性能。在SimplerEnv环境中的实验表明，该方法仅需最少的训练数据即可实现性能提升，其平均任务完成成功率比SpatialVLA高出4%，比TraceVLA高出19%。<br><br>### 论文核心贡献点<br>1. 提出了一种名为“Spatial Traces”的方法，该方法能够让VLA模型通过单张图像同时利用环境中的空间和时间信息。<br>2. 在SimplerEnv中的实验表明，仅用52条训练轨迹进行微调，该方法相较于SpatialVLA提升了4%的任务成功率，相较于TraceVLA提升了19%，证明了在数据收集困难的真实世界应用中，融合空间和时间信息的价值。<br><br>### 论文方法描述<br>该方法的核心是构建一个融合了空间和时间信息的视觉表示。具体来说，它包含两个主要组成部分：深度信息和视觉轨迹。首先，使用一个深度估计模型（如ZoeDepth）从当前观察中预测深度图。接着，使用一个轨迹预测模型（如Co-Tracker）从过去一系列观察中追踪并生成关键点的视觉轨迹。然后，将这些二维轨迹投影到当前预测的深度图上，形成一个带有轨迹标记的深度图。最后，原始的RGB观察图像和这个带有轨迹的深度图分别通过各自的图像处理器（如Siglip和Ego3D Positional Encoder）转换成嵌入向量，并将这两个嵌入向量相加，生成最终的视觉嵌入。这个融合的视觉嵌入与文本指令的嵌入一起被输入到VLA模型（如PaliGemma2）中，以预测下一步的动作。<br><br>### 论文使用数据集和训练资源<br>数据集：Bridge数据集的一个子集，包含52条来自真实机器人操作的轨迹，总计1969个步骤。<br>训练资源：模型在单个NVIDIA TITAN RTX GPU上进行训练。训练采用了LoRA适配器，应用于模型的所有线性层。训练参数设置如下：学习率为5e-5，批次大小为1，训练周期为2，LoRA秩为32，优化器为AdamW。完成一个模型的训练大约需要2小时。<br><br>### 论文使用的评估环境和评估指标<br>评估环境：SimplerEnv，一个基于Robosuite构建的虚拟环境，专门用于机器人操作任务。评估选取了四个代表性任务：“Put Spoon”（放勺子）、“Put Carrot”（放胡萝卜）、“Stack Blocks”（堆叠积木）和“Put Eggplant”（放茄子）。<br>评估指标：主要使用两个指标：<br>1. 目标条件成功率（GCS）：衡量智能体是否成功达到基本的任务完成条件（例如，将目标物体从桌面上拿起）。<br>2. 成功率（SR）：衡量整个任务是否完全成功完成，达到最终的目标状态。</details> |
| 2025-08-12 | OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing | http://arxiv.org/abs/2508.08706 | <details><summary>展开</summary>论文研究单位<br>- Paxini Tech.<br>- Shanghai Jiao Tong University<br><br>论文概述<br>该论文提出了OmniVTLA，一个视觉-触觉-语言-动作模型，旨在解决现有视觉-语言-动作（VLA）模型在接触密集型操作任务中由于忽视触觉感知而表现不佳的问题。OmniVTLA通过引入一个双路径触觉编码器框架来整合触觉信息，并构建了一个名为ObjTac的大规模力-触觉数据集来训练语义对齐的触觉编码器。实验表明，该方法在真实世界的拾取与放置任务中，显著优于现有的VLA模型，提高了成功率，减少了任务完成时间，并生成了更平滑的机器人运动轨迹。<br><br>论文核心贡献点<br>1. 提出OmniVTLA框架，一个用于接触密集型操作任务的端到端视觉-触觉-语言-动作模型。该模型利用双编码器路径来克服不同触觉传感器之间的异构性问题。<br>2. 引入ObjTac数据集，一个包含135K个三模态样本的综合触觉数据集，涵盖了10个类别的56个物体，为模型训练提供了丰富的触觉、视觉和文本数据。<br>3. 通过真实世界实验验证了OmniVTLA的优越性能，相较于典型的VLA模型，在夹爪任务上成功率提高了21.9%，在灵巧手任务上提高了6.2%，同时缩短了任务完成时间并优化了轨迹平滑度。<br><br>论文方法描述<br>该模型在VLA框架的基础上扩展，将触觉数据作为输入。其核心是一个双路径触觉编码器：<br>1. 第一条路径使用一个预训练的视觉Transformer（ViT），继承了大规模视觉数据中的丰富语义表示，对应于VTLA-Pre方案。<br>2. 第二条路径使用一个语义对齐的触觉ViT（SA-ViT），通过跨模态对比学习在ObjTac等数据集上进行训练，以实现触觉、视觉和文本模态之间的语义对齐，对应于VTLA-SA方案。<br>3. OmniVTLA将这两条路径的编码token拼接后，与文本token和视觉token一同输入到Gemma-2B骨干网络中，最后由动作头解码生成机器人动作序列。<br>为了训练SA-ViT，论文提出了一个多模态对齐损失函数，同时优化视觉-语言、视觉-触觉和触觉-语言之间的对齐。<br><br>论文使用数据集和训练资源<br>1. 数据集<br> - ObjTac：一个新收集的力-触觉数据集，包含56个物体（10个类别，如塑料、玻璃、木材等），总计135K个同步的文本、视觉和触觉三元组样本。<br> - 任务特定数据集：通过远程操作收集的拾取与放置任务数据，用于训练和评估模型。使用UR5机械臂、带触觉传感器的夹爪和灵巧手在真实环境中进行采集，每类物体包含40个演示片段。<br>2. 训练资源<br> - GPU: NVIDIA A100 (80GB VRAM)<br> - 训练方法：在π0基线上进行微调。<br> - 训练步数: 30K<br> - 学习率: 峰值2.5e-5，带预热和余弦衰减。<br> - 批量大小: 32<br><br>论文使用的评估环境和评估指标<br>1. 评估环境<br> - 硬件：UR5机械臂，末端执行器为配备两个触觉传感器的双指夹爪或配备11个触觉传感器的四指灵巧手，同时配备腕部摄像头和基座摄像头。<br> - 任务：拾取与放置任务，使用夹爪操作4种物体，使用灵巧手操作2种物体，并在未见过的物体上进行泛化测试。<br>2. 评估指标<br> - 离线验证：均方误差（MSE），计算模型预测的机器人状态与真实远程操作数据之间的差异。<br> - 真实世界评估：<br> - 成功率（SR）：成功将物体放置到目标位置的试验百分比。<br> - 完成时间（CT）：从任务开始到成功放置所需的步数。<br> - 运动平滑度：末端执行器轨迹的方差，值越小表示轨迹越平滑。</details> |
| 2025-08-11 | GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions | http://arxiv.org/abs/2508.07650 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model | http://arxiv.org/abs/2508.06571 | <details><summary>展开</summary>论文研究单位<br>Bosch Corporate Research, Shanghai, China<br>School of Communication and Information Engineering, Shanghai University<br>School of Mechanical Engineering, Shanghai Jiao Tong University<br>Bosch Mobility Solutions, Robert Bosch GmbH, Suzhou<br>AIR, Tsinghua University, Beijing<br><br>论文概述<br>本文提出了一种名为IRL-VLA的闭环强化学习框架，用于训练端到端自动驾驶的视觉-语言-动作模型。该方法旨在解决现有VLA模型依赖模仿学习导致性能受限，以及传统闭环训练依赖高保真模拟器所带来的计算开销和领域鸿沟问题。IRL-VLA采用三阶段范式：首先通过模仿学习预训练一个VLA策略；然后通过逆向强化学习构建一个轻量级的奖励世界模型来代替昂贵的模拟器进行奖励计算；最后使用PPO算法在奖励世界模型的引导下对VLA策略进行强化学习微调，以平衡安全、舒适和交通效率。<br><br>论文核心贡献点<br>1. 提出了IRL-VLA，一个开创性的无需依赖模拟器的闭环VLA训练框架。它引入了一个基于逆向强化学习的高效奖励世界模型，实现了可扩展且有效的闭环训练。<br>2. 提出了一个新颖的VLA模型，该模型在模仿学习和强化学习设置下均能实现优越性能，包含语义推理、3D推理和统一扩散规划器三个模块。<br>3. 在NAVSIM v2端到端驾驶基准测试中取得了最先进的性能，在CVPR2025自动驾驶挑战赛中获得第一名。<br><br>论文方法描述<br>1. 模仿策略学习：设计了一个包含语义推理（基于Senna-VLM）、3D推理（BEV编码器和自适应器）和统一扩散规划器的VLA模型。使用模仿学习损失（L1重建损失和二元交叉熵分类损失）进行预训练。<br>2. 逆向环境学习：为了构建奖励世界模型，首先收集多样化的轨迹数据，包括记录扩散过程中的轨迹、使用K-means聚类采样以及改变自车初始位置。然后，奖励世界模型以多视角相机信息和预测轨迹为输入，通过多个独立的MLP头预测EPDMS的各项子指标，并加权求和得到最终奖励。模型通过最小化预测分数与真实分数之间的误差来进行训练。<br>3. 使用奖励世界模型的强化学习：采用PPO算法对预训练的VLA策略进行微调。策略生成一组轨迹，由奖励世界模型评估并计算优势值（使用GAE）。通过最大化累计奖励来更新策略参数，并结合KL散度和模仿学习损失以保证训练的稳定性和防止灾难性遗忘。<br><br>论文使用数据集和训练资源<br>数据集: NAVSIM v2 (基于nuPlan和OpenScene)，包含1,192个训练场景和136个评估场景。<br>训练资源: 8个NVIDIA A100 GPU。<br><br>论文使用的评估环境和评估指标<br>评估环境: NAVSIM v2 基准测试，一个非反应式模拟环境。<br>评估指标: 扩展预测性驾驶员模型分数（EPDMS）及其八个子指标，包括无责任碰撞、可行驶区域合规性、驾驶方向合规性、交通灯合规性、自车进度、碰撞时间、车道保持、历史舒适性和扩展舒适性。最终EPDMS分数由惩罚项和加权平均项共同计算得出。</details> |
| 2025-08-06 | Static and Plugged: Make Embodied Evaluation Simple | http://arxiv.org/abs/2508.06553 | <details><summary>展开</summary>待生成</details> |
| 2025-08-06 | A tutorial note on collecting simulated data for vision-language-action models | http://arxiv.org/abs/2508.06547 | <details><summary>展开</summary>论文研究单位<br>The University of Auckland, New Zealand<br><br>论文概述<br>本文是一篇关于如何为视觉-语言-动作模型收集仿真数据的教程。VLA模型通过单一神经网络统一处理视觉观察、语言理解和动作生成，但其性能高度依赖于高质量、大规模的视觉-语言-动作三元组训练数据。本文回顾并实践了三种具有代表性的数据收集系统：用于灵活定制的PyBullet仿真框架、用于标准化任务定义和评估的LIBERO基准测试套件，以及用于大规模多机器人数据采集的RT-X数据集。文章旨在为研究人员提供构建VLA数据集的具体指导。<br><br>论文核心贡献点<br>1. 提供了一份关于VLA模型数据收集的系统性教程，综述了PyBullet、LIBERO和RT-X三种不同方法的特点和适用场景。<br>2. 详细演示了如何在PyBullet仿真环境中，结合Ravens框架，生成特定任务的高质量自定义数据集，包括任务选择、数据采集流程和存储格式的具体实现。<br>3. 展示了如何对LIBERO基准进行定制化修改（如添加干扰物），并通过人类遥操作收集新的演示数据，详细介绍了从BDDL任务定义到HDF5数据格式生成的完整流程。<br>4. 概述了RT-X大规模跨平台数据集的构成、特点及其在实现跨机器人学习中的作用，强调了其标准化动作表示形式的重要性。<br><br>论文方法描述<br>1. PyBullet数据收集方法：<br> - 使用PyBullet物理引擎和Ravens框架进行仿真和数据收集。<br> - 通过编写脚本化的专家策略来生成演示轨迹，避免了人为操作的不确定性。<br> - 核心流程为：初始化环境和任务 -> 重置场景并随机化 -> 循环执行：获取RGB图像、专家策略生成动作、执行动作、保存数据。<br> - 选取了`block-insertion`、`place-red-in-green`和`towers-of-hanoi`三个代表性任务进行数据采集。<br> - 数据按`episode_id-step_id.pkl`的命名规则，分布式存储在`color`、`depth`、`action`、`reward`、`info`等目录中。<br>2. LIBERO数据收集方法：<br> - 利用LIBERO现有的基准数据集，或通过人类遥操作收集新数据。<br> - 通过修改BDDL（Behavioral Domain Definition Language）文件来定制任务场景，例如添加内置的干扰物对象以增加任务难度。<br> - 遥操作数据收集流程包括：解析BDDL任务 -> 初始化环境 -> 操作员通过SpaceMouse等设备控制机器人 -> 实时记录状态、动作和图像 -> 通过10步连续成功验证机制确保任务完成 -> 将`.npz`临时文件聚合成HDF5格式的结构化数据集。<br>3. RT-X数据集应用方法：<br> - RT-X数据集本身是一种资源，其核心方法在于将来自22种不同机器人的超过100万条真实轨迹数据统一到标准化的数据格式中。<br> - 关键方法是其标准化的7维动作表示法 `[x, y, z, roll, pitch, yaw, gripper]`，它抽象了不同机器人的硬件差异，使得单个模型可以学习和控制多种机器人。<br><br>论文使用数据集和训练资源<br>1. 数据集：<br> - 在PyBullet中生成的自定义数据集，包含`block-insertion`、`place-red-in-green`、`towers-of-hanoi`等任务数据。<br> - LIBERO基准数据集（如`libero_10_no_noops`）。<br> - 在LIBERO中通过修改场景和遥操作收集的新数据集。<br> - RT-X数据集，包含来自22种机器人的超过100万条真实世界轨迹。<br>2. 训练/仿真资源与工具：<br> - PyBullet物理仿真引擎。<br> - Ravens框架（基于PyBullet的任务套件）。<br> - LIBERO基准测试套件（基于robosuite和MuJoCo）。<br> - 用于遥操作的SpaceMouse等设备。<br> - 代码资源已在GitHub上公开。<br><br>论文使用的评估环境和评估指标<br>1. 评估环境：<br> - PyBullet仿真环境。<br> - LIBERO/robosuite/MuJoCo仿真环境。<br> - RT-X数据集中的真实世界多机器人环境。<br>2. 评估指标：<br> - 在PyBullet数据收集中，使用任务成功率作为评估指标，其脚本化的专家策略在所选任务上达到了95%的成功率。<br> - 在LIBERO中，使用由BDDL文件定义的`env._check_success()`函数作为任务成功的判断标准。为了确保评估的鲁棒性，引入了10步连续验证机制，即任务成功状态必须连续保持10个时间步（0.5秒）才被最终确认，以防止误报。</details> |
| 2025-08-07 | Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control | http://arxiv.org/abs/2508.05342 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction | http://arxiv.org/abs/2508.05294 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | Learning to See and Act: Task-Aware View Planning for Robotic Manipulation | http://arxiv.org/abs/2508.05186 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming | http://arxiv.org/abs/2508.02549 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning | http://arxiv.org/abs/2508.02219 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation | http://arxiv.org/abs/2508.02190 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models | http://arxiv.org/abs/2508.02062 | <details><summary>展开</summary>### 论文研究单位<br>宾夕法尼亚大学（University of Pennsylvania）和不列颠哥伦比亚大学（University of British Columbia）<br><br>### 论文概述<br>本文提出了一种名为RICL（Retraining for In-Context Learning）的方法，旨在向预训练的视觉-语言-动作（VLA）模型中注入上下文学习能力。传统VLA模型缺乏上下文学习（ICL）能力，而RICL通过微调少量机器人演示数据，使模型能够利用检索增强生成（RAG）和ICL适应新任务。用户只需提供10-20个目标任务演示，RICL即可检索相关上下文信息并提升任务性能，无需参数更新。实验应用于π₀-FAST VLA，在涉及未见对象、新颖动作和新场景的操作任务中验证了有效性。<br><br>### 论文核心贡献点<br>- 提出RICL方法，首次实现向预训练VLA中注入ICL能力，无需从头训练。<br>- 允许用户通过少量演示（10-20个）教导VLA执行新任务，支持零参数更新适应。<br>- 结合RAG和ICL，提升模型在未见对象、新颖动作和新场景中的泛化能力。<br>- 实验证明RICL-π₀在8个任务上平均完整任务成功率达31.25%，远高于基线的2.5%。<br>- 支持进一步微调：在目标任务演示上微调RICL-VLA，可将平均成功率提升至61.67%。<br>- 开源代码和模型权重（RICL-π₀），提供首个机器人操作任务的简单ICL接口。<br><br>### 论文方法描述<br>- **RICL训练流程**：后训练预训练VLA，输入序列包含查询图像/状态和检索自演示的多个图像、状态、动作。使用DINO-v2图像编码器和ℓ₂距离检索最相关的4组邻居数据，按距离排序（最近邻居置于上下文左侧）。<br>- **动作插值层**：预测动作通过加权插值结合检索动作a'和LLM输出，公式为：<br> ```<br> πᵗʰᵉᵗᵃ_RICL-VLA = e^{-λd} * one-hot(a') + (1 - e^{-λd}) * σ(π_θ(retrieved, query))<br> ```<br> 其中d为查询图像与最近邻居的ℓ₂距离，λ=10，σ为Softmax函数。<br>- **训练细节**：仅微调LLM部分，冻结图像编码器；最小化查询动作块的交叉熵损失；使用CosineDecaySchedule学习率调度。<br>- **部署与微调**：部署时检索任务演示数据并执行ICL；微调阶段在相同演示上进行检索增强训练，进一步提升性能。<br><br>### 论文使用数据集和训练资源<br>- **训练数据**：使用Franka DROID平台收集400个演示（20个拾取放置任务，每任务20个演示），任务如"移动物体至左侧/右侧"或"拾取物体放入碗中"。<br>- **基础模型**：π₀-FAST-DROID（基于π₀-FAST在DROID数据集微调）。<br>- **检索数据**：评估时每个目标任务收集20个演示（如pokeball、idliplate等），用于RAG和ICL。<br>- **训练资源**：两个NVIDIA A100 GPU，批量大小16，训练3个epoch，峰值学习率2.5e-5，动作块长度15。<br>- **硬件平台**：Franka机械臂配移动底座，搭载顶部、右侧和手腕摄像头（图3）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：Franka DROID平台，测试场景包括桌面（tabletop）和厨房水槽（sink），涉及新摄像头位置、光照和干扰物。<br>- **评估任务**：8个任务涵盖未见对象（如pokeball、squeegee）、新颖动作（如推动lever、打开door）和新场景（如sink-idliplate）。任务细节如：<br> - pokeball：拾取pokeball放入托盘。<br> - squeegee：拖动刮刀清洁台面。<br> - door：打开底部柜门。<br>- **评估指标**：每个任务10次测试轨迹，随机化初始位置/方向。报告：<br> - 完整任务成功率。<br> - 中间检查点成功率（如抓取成功、移动成功）。<br> - 结果以堆叠条形图展示（图4），包括不同方法对比。<br>- **对比方法**：原始π₀-FAST-DROID、Retrieve and Play基线、Diffusion Policy基线，以及RICL-π₀的微调版本。</details> |
| 2025-07-31 | XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation | http://arxiv.org/abs/2508.00097 | <details><summary>展开</summary>论文研究单位<br>ByteDance, PICO, San Jose, CA, USA<br>佐治亚理工学院, Institute for Robotics and Intelligent Machines (IRIM), Atlanta, GA, USA<br>乔治梅森大学, Computer Science, Fairfax, Virginia, USA<br><br>论文概述<br>该论文介绍了XRoboToolkit，一个基于OpenXR标准的跨平台框架，用于通过扩展现实（XR）设备进行机器人遥操作。该系统旨在解决当前遥操作方法在可扩展性、设置复杂性和数据质量方面的局限性，以满足大规模、高质量机器人演示数据集的需求，特别是用于训练视觉-语言-动作（VLA）模型。XRoboToolkit提供了低延迟的立体视觉反馈、基于优化的逆运动学求解器，并支持多种跟踪模态，如头戴设备、控制器、手部和辅助运动追踪器。<br><br>论文核心贡献点<br>1. 提出了一个基于OpenXR标准的跨平台遥操作框架XRoboToolkit，解决了XR设备与机器人控制器之间缺乏标准化数据格式的问题，实现了跨设备的无缝集成。<br>2. 开发了一个低延迟的立体视觉反馈系统，集成了高效的通信协议和视频流管道，以最小化延迟并减少晕动症。<br>3. 实现了一个基于二次规划（QP）的优化逆运动学（IK）求解器，能生成平滑可靠的机器人运动，特别是在运动学奇异点附近，并集成了灵巧手重定向功能。<br>4. 设计了模块化架构，使框架能够轻松集成到不同的机器人平台（如精密机械臂、移动机器人、灵巧手）和仿真环境（如MuJoCo）中。<br>5. 通过实际应用和训练VLA模型验证了框架的有效性，证明了其收集的数据质量足以用于微调出具有鲁棒自主性能的策略。<br><br>论文方法描述<br>XRoboToolkit系统主要由部署在XR头显上的Unity客户端和运行在PC上的C++服务组成。Unity客户端负责捕获头、手、控制器和全身等姿态跟踪数据，并呈现立体视觉界面；PC服务则负责接收数据并将其传递给机器人控制模块。数据流采用异步、回调驱动的架构，所有跟踪数据以90Hz的频率在单一JSON对象中传输。机器人控制模块包括：1) 基于PlaCo和Pinocchio库的QP求解器进行机械臂逆运动学求解，支持通过辅助运动追踪器添加约束；2) 通过优化方法将26个OpenXR手部关节点重定向到机器人灵巧手；3) 通过XR控制器摇杆控制移动基座的线速度和角速度。立体视觉支持PICO 4 Ultra头显和ZED Mini相机，并通过自定义着色器调整瞳距和焦点以增强深度感知。<br><br>论文使用数据集和训练资源<br>数据集：使用ARX R5双臂系统收集了100个双臂地毯折叠任务的演示数据。<br>数据细节：数据以50Hz记录，每帧包含14维机器人关节状态、14维位置控制指令以及来自三个RealSense相机（D405i腕部相机和D435i顶置相机）的424x240 RGB图像。<br>训练资源：在预训练的VLA模型pi_0上进行LoRA（Low-Rank Adaptation）微调。训练参数为：80,000步，批次大小为16，动作范围（action horizon）为50帧。<br><br>论文使用的评估环境和评估指标<br>评估环境1：视频流延迟对比。使用Kandao QooCam 3D相机和一个以100Hz频率刷新的LED面板进行测量。对比了XRoboToolkit（使用ZED Mini/PICO 4 Ultra与Quest 3/PICO 4 Ultra的组合）和Open-TeleVision系统的性能。所有设备连接在同一局域网，视频参数设置为1280x720分辨率、60FPS、1Mbps码率。<br>评估指标1：视频流延迟的平均值和标准差。<br>评估环境2：VLA模型性能验证。将收集的地毯折叠数据集用于微调pi_0模型，并在ARX R5双臂系统上部署训练好的策略进行30分钟的连续运行测试。<br>评估指标2：任务成功率、平均任务完成时间，以及策略是否展现出自主重新抓取和重新定位等自适应行为。</details> |
| 2025-07-31 | villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models | http://arxiv.org/abs/2507.23682 | <details><summary>展开</summary>待生成</details> |
| 2025-07-31 | A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving | http://arxiv.org/abs/2507.23540 | <details><summary>展开</summary>待生成</details> |
| 2025-07-31 | FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning | http://arxiv.org/abs/2507.23318 | <details><summary>展开</summary>待生成</details> |
| 2025-07-30 | Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance | http://arxiv.org/abs/2507.22424 | <details><summary>展开</summary>论文研究单位<br>- 清华大学<br>- Infinigence AI<br>- 澳门大学<br><br>论文概述<br>本文提出了Spec-VLA，这是首个专为视觉-语言-动作（VLA）模型设计的推测解码（SD）框架。由于VLA模型庞大的参数量和自回归解码特性，其推理速度受到严重限制。传统的推测解码虽然能加速大语言模型，但直接应用于VLA任务时效果甚微。为解决此问题，Spec-VLA引入了一种宽松接受机制，该机制利用VLA模型中动作标记的离散化特性（如分箱ID），通过计算草稿标记与验证模型标记之间的距离来放宽接受标准，从而在不牺牲任务成功率的前提下，显著提升推理速度。<br><br>论文核心贡献点<br>- 提出了首个应用于VLA模型的推测解码框架Spec-VLA，通过引入高效的草稿模型和并行验证机制来加速动作序列生成。<br>- 指出直接将现有推测解码框架应用于VLA任务效果不佳的原因，即动作预测任务的复杂性以及VLA模型采用的贪婪解码策略。<br>- 设计了一种新颖且几乎零计算开销的宽松接受机制，通过比较动作标记分箱ID的距离，允许接受与验证标记相近的草稿标记，有效提升了接受长度和解码速度。<br><br>论文方法描述<br>Spec-VLA框架包含两个核心部分：<br>1. **推测解码架构**：采用一个轻量级的Llama解码层作为草稿模型，利用验证模型的隐藏状态以及视觉和文本嵌入来自回归地生成多个候选动作标记。随后，原始的VLA模型作为验证模型，对草稿序列进行并行验证。<br>2. **宽松接受机制**：VLA模型（如OpenVLA）将连续的动作维度离散化为256个分箱。该方法将宽松接受定义为：如果草稿标记对应的分箱ID与验证模型预测的分箱ID之间的绝对距离小于或等于一个预设的阈值r，则接受该草稿标记。这一机制通过简单的ID比较实现，避免了复杂的相似度计算，从而在不显著增加计算成本的情况下，大幅提高了草稿标记的接受率。<br><br>论文使用数据集和训练资源<br>- **数据集**：LIBERO仿真基准，包括LIBERO-Goal, LIBERO-Object, LIBERO-Spatial和LIBERO-Long四个任务套件。<br>- **训练资源**：草稿模型的训练在4块Tesla A100 (80G) GPU上完成，耗时约6小时，批次大小为16。<br><br>论文使用的评估环境和评估指标<br>- **评估环境**：在LIBERO仿真环境中进行评估。每个任务执行50次试验来测试策略性能。硬件环境为Tesla A100 (80G) GPU。<br>- **评估指标**：<br> - **成功率**：任务成功完成的百分比。<br> - **加速比**：与标准自回归解码相比，生成速度的提升倍数。<br> - **接受长度**：单次前向传递中平均预测的token数量，反映了推测解码的效率。</details> |
| 2025-07-23 | InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation | http://arxiv.org/abs/2507.17520 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents | http://arxiv.org/abs/2507.17462 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | Confidence Calibration in Vision-Language-Action Models | http://arxiv.org/abs/2507.17383 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback | http://arxiv.org/abs/2507.17294 | <details><summary>展开</summary>待生成</details> |
| 2025-07-22 | ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning | http://arxiv.org/abs/2507.16815 | <details><summary>展开</summary>论文研究单位<br>NVIDIA 和 National Taiwan University<br><br>论文概述<br>该论文提出了ThinkAct，一个双系统框架，旨在通过强化视觉潜在规划来实现视觉-语言-行动推理。ThinkAct通过一个行动对齐的视觉奖励来引导多模态大语言模型生成具身推理计划，并将这些推理计划压缩成一个视觉计划潜在表示，用于指导下游行动模型在目标环境中执行鲁棒的动作。该框架在具身推理和机器人操作基准上实现了少样本适应、长视野规划和自我纠错能力。<br><br>论文核心贡献点<br>1. 提出了ThinkAct，一个将行动执行与视觉基础的具身推理通过视觉潜在规划相互连接的双系统框架。<br>2. 利用目标完成和轨迹对齐的视觉反馈作为行动对齐的奖励，使长视野推理基于具身场景。<br>3. 推进视觉潜在规划，通过提供跨多样化环境的推理增强轨迹指导来引导下游行动执行。<br>4. 证明学习到的推理VLA能够在多样化的具身操作任务中实现少样本适应、长视野规划和自我纠错能力。<br><br>论文方法描述<br>ThinkAct框架包含两个主要模块：一个多模态大语言模型（MLLM）用于高级推理，和一个行动模型用于底层动作执行。MLLM通过强化学习进行训练，使用一个基于视觉目标完成和轨迹分布匹配的行动对齐奖励函数。具体来说，它使用一个目标奖励，通过比较预测的起点和终点位置与检测到的轨迹点来鼓励目标完成；以及一个轨迹奖励，使用动态时间规整（DTW）距离来正则化预测轨迹与演示轨迹的分布对齐。奖励函数还结合了格式正确性分数。强化过程采用GRPO算法。推理步骤被压缩成一个紧凑的潜在轨迹，该轨迹编码了高级意图和规划上下文。行动模型是一个基于Transformer的扩散策略，它根据当前状态（视觉观察和语言指令）以及来自MLLM的潜在计划来预测动作。推理和执行可以异步操作，每个潜在计划对应N次与环境的交互。训练分为多阶段：MLLM先通过监督数据进行冷启动，然后通过强化学习进行微调；行动模型在Open X-Embodiment数据集上预训练，然后在目标环境中通过模仿学习进行微调，同时冻结MLLM。<br><br>论文使用数据集和训练资源<br>训练数据集：<br>- 监督微调冷启动：Open X-Embodiment (OXE)的子集、RoboVQA、EgoPlan-IT、Video-R1-CoT。<br>- 强化微调：OXE子集、Something-Something V2人类视频、以及具身QA数据集如RoboVQA、EgoPlan-IT/Val、Reflect (RoboFail)、LLaVA-Video-178K。<br>- 行动模型训练：Open X-Embodiment (OXE)数据集。<br>评估基准：<br>- 机器人操作：SimplerEnv（包含Google-VM、Google-VA、Bridge-VM设置）和LIBERO（包含Spatial、Object、Goal、Long子任务）。<br>- 具身推理：EgoPlan-Bench2（多选题，准确率）、RoboVQA（自由形式QA，BLEU分数）、OpenEQA（自由形式QA，LLM基础评分和BLEU分数）。<br>训练资源：<br>- 使用16张NVIDIA A100 GPU（80GB内存）进行所有实验。<br><br>论文使用的评估环境和评估指标<br>评估环境：<br>- SimplerEnv：一个仿真基准，包含视觉匹配和变体聚合两种评估设置，提供在不同光照条件、桌面纹理、背景、物体干扰项和机器人相机姿态下的多样化操作场景。<br>- LIBERO：一个机器人操作仿真基准，包含四个结构化任务套件，针对空间布局变化、物体多样性、目标变化和长视野规划。<br>- EgoPlan-Bench2：评估在复杂真实世界场景中以自我为中心的规划能力，是一个包含1,321个高质量多选题QA对的非重叠评估集。<br>- RoboVQA：专注于机器人操作中的视觉问答，其验证集包含1,893个视频-文本对。<br>- OpenEQA：一个具身问答基准，用于评估代理通过自然语言理解和推理真实世界环境的能力。<br>评估指标：<br>- 机器人操作：任务成功率（Success Rate）。<br>- 具身推理：<br> - EgoPlan-Bench2：多选题准确率（Accuracy）。<br> - RoboVQA：BLEU分数（BLEU-1/2/3/4）。<br> - OpenEQA：LLM基础评分（LLM-based scoring）和BLEU分数。</details> |
| 2025-07-21 | Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos | http://arxiv.org/abs/2507.15597 | <details><summary>展开</summary>待生成</details> |
| 2025-07-21 | GR-3 Technical Report | http://arxiv.org/abs/2507.15493 | <details><summary>展开</summary>待生成</details> |
| 2025-07-18 | VLA-Mark: A cross modal watermark for large vision-language alignment model | http://arxiv.org/abs/2507.14067 | <details><summary>展开</summary>待生成</details> |
| 2025-07-18 | EdgeVLA: Efficient Vision-Language-Action Models | http://arxiv.org/abs/2507.14049 | <details><summary>展开</summary>待生成</details> |
| 2025-07-17 | AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation | http://arxiv.org/abs/2507.12768 | <details><summary>展开</summary>### 论文研究单位<br>清华大学计算机科学与技术系、人工智能研究院、清华-博世联合机器学习中心<br><br>### 论文概述<br>本文提出一种任务无关动作（task-agnostic action）范式，将动作执行与任务条件解耦，以提升可扩展性和效率。为此，设计了ATARA数据收集框架和AnyPos模型：ATARA通过脚本策略自动生成任务无关数据，收集速度比人工操作快30倍；AnyPos作为逆动力学模型，通过手臂解耦估计和方向感知解码器从任务无关数据学习动作预测。实验表明，该方法在动作预测准确率上提升51%，真实世界任务成功率提升30-40%。<br><br>### 论文核心贡献点<br>- 任务无关动作范式：解耦动作与特定任务，降低数据采集成本。<br>- ATARA框架：自动化、可扩展的任务无关数据收集，无人工干预。<br>- AnyPos模型：引入手臂解耦估计和方向感知解码器（DAD），提升学习效率和精度。<br>- 两阶段框架：结合视频生成模型（语义先验）和逆动力学模型（物理控制），实现跨任务泛化。<br><br>### 论文方法描述<br>1. **任务无关动作数据**：<br> - 定义为不依赖任务指令或奖励的轨迹，直接建模 \(p(\text{action} \mid \text{image})\)。<br> - 理论推导：任务特定动作 \(p(a \mid x, l)\) 可分解为任务特定图像生成 \(p(x \mid x_0, l)\) 与任务无关动作 \(p(a \mid x)\) 的乘积（公式4）。<br>2. **ATARA数据收集**：<br> - 使用脚本策略 uniformly 探索双臂机器人的立方工作空间，生成610k图像-动作对。<br>3. **AnyPos模型**：<br> - **手臂解耦估计**：通过空间分割（flood-fill 和对称线）隔离单臂区域，减少假设空间，提升精度约20%。<br> - **方向感知解码器（DAD）**：对齐视觉特征（DINOv2）与物理运动方向（关节角度、连杆方向），增强鲁棒性，再提升20%性能。<br><br>### 论文使用数据集和训练资源<br>- **训练数据**：ATARA生成的610k图像-动作对（双臂14维动作空间）。<br>- **测试数据**：真实世界收集的2.5k图像-动作对（包含未见技能和物体）。<br>- **训练资源**：未明确细节，但涉及大规模数据集和预训练视觉骨干（如DINOv2），需高性能计算资源。<br><br>### 论文使用的评估环境和评估指标<br>1. **动作预测评估**：<br> - 环境：真实世界测试集（未见任务）。<br> - 指标：动作预测准确率（AnyPos: 57.13%，比基线高51%）。<br>2. **真实世界重放评估**：<br> - 环境：物理机器人执行提升、抓取、点击等任务。<br> - 指标：任务成功率（AnyPos-ATARA: 92.59%，比人工数据高33%）。<br>3. **视频生成模型部署**：<br> - 环境：结合扩散视频模型进行动作验证。<br> - 指标：下游任务成功率（30-40%提升）。</details> |
| 2025-07-16 | EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos | http://arxiv.org/abs/2507.12440 | <details><summary>展开</summary>待生成</details> |
| 2025-07-14 | Vision Language Action Models in Robotic Manipulation: A Systematic Review | http://arxiv.org/abs/2507.10672 | <details><summary>展开</summary>待生成</details> |
| 2025-07-12 | Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization | http://arxiv.org/abs/2507.09160 | <details><summary>展开</summary>根据您提供的HTML原文，该内容仅包含LaTeX宏包定义（如 \usepackage 和 \DeclareMathOperator）以及LaTeXML生成器的页脚信息，并未包含论文的任何实质内容，如摘要、引言、方法、实验结果等。<br><br>因此，无法根据此片段提取所需信息并生成论文总结。请提供包含论文正文内容的HTML原文。</details> |
| 2025-07-07 | VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting | http://arxiv.org/abs/2507.05116 | <details><summary>展开</summary>待生成</details> |
| 2025-07-06 | DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge | http://arxiv.org/abs/2507.04447 | <details><summary>展开</summary>待生成</details> |
| 2025-07-03 | DexVLG: Dexterous Vision-Language-Grasp Model at Scale | http://arxiv.org/abs/2507.02747 | <details><summary>展开</summary># 论文研究单位<br>Institution1, Institution2<br><br># 论文概述<br>该论文介绍了一个大规模的灵巧视觉-语言-抓握模型和数据集，称为DexVLG。其核心目标是根据语言指令生成稳定且语义对齐的灵巧抓握姿态。该研究利用基于能量的优化方法来合成高质量的抓握，并创建了一个名为DexGraspNet 3.0的大规模、经过部分标注的数据集，用于训练和评估视觉-语言-抓握模型。<br><br># 论文核心贡献点<br>- 提出了一种基于线性规划的、可微分的力封闭（LP-based DFC）方法，用于改进基于能量的抓握合成，使其能够考虑接触力的大小，从而产生更自然的抓握姿态。<br>- 创建了DexGraspNet 3.0，这是一个包含174k个物体和170M个抓握姿态的大规模数据集。该数据集使用GPT-4o从Objaverse中筛选和注释，并对物体进行了语义分割，分为Ours-Wrap和Ours-Pinch两种抓握风格。<br>- 开发了一种新颖的、与部分对齐的手部姿态初始化策略，该策略根据物体部分的几何形状（盖状、盘状、L形、杆状）对物体进行分类，并应用特定的启发式方法来对齐初始手部姿态，从而为优化过程注入了强大的、符合人类直觉的先验知识。<br>- 构建了一个完整的、可扩展的视觉-语言-抓握（VLG）框架，研究表明，流匹配去噪方法优于传统的DDPM和DDIM范式，用于抓握姿态的生成。<br><br># 论文方法描述<br>论文的方法包括三个主要阶段：数据集创建、手部姿态初始化和基于能量的抓握优化。<br>1. **数据集创建**：从庞大的Objaverse数据集中，使用GPT-4o和一组五个标准（例如，是否为场景、颜色为单色、有地面平面）来过滤物体。然后使用SAMesh对物体进行语义分割，并通过结合来自六个正交视图的多视角图像，用GPT-4o为每个部分生成语义描述。最后，查询GPT-4o以获得合理的物体尺寸，并将物体的对角线长度重新缩放到20-50厘米的范围内。<br>2. **手部姿态初始化**：根据方向、长度和部分间关系，将分割后的物体部分分为四种几何类型（盖状、盘状、L形、杆状）。对于每种类型，都设计了一个特定的策略来对齐手掌的初始位置和方向。例如，对于盖状部分，手掌会后退且与主轴方向垂直。这产生了两组初始化抓握：Ours-Wrap（用于环绕抓握）和Ours-Pinch（用于捏取抓握），每个部分各生成5000个姿态。<br>3. **基于能量的抓握优化**：给定一个初始手部姿态，使用一个梯度下降过程来优化一个全面的能量函数。该函数由几个项组成：<br> - **E_FC（LP-based DFC能量）**：一项用于确保力封闭稳定性的关键项。它首先求解一个线性规划问题，以找到在给定姿态下的最优接触力，然后如果姿态稳定，则使用这些力的大小来重新加权能量，以鼓励更自然的接触。<br> - **E_pen（穿透能量）和E_spen（自穿透能量）**：使用CuRobo库来防止手与物体之间以及手自身之间的穿透。<br> - **E_limit（关节限制能量）**：将手部关节角度约束在物理允许的范围内。<br> - **E_dir（方向对齐能量）**：一个余弦相似性项，鼓励手-物接触点与手部网格的正面朝向对齐，以避免扭曲的抓握。<br> 最终的总能量为这些项的加权和：E = w_fc * E_FC + w_pen * E_pen + w_spen * E_spen + w_limit * E_limit + w_dir * E_dir。<br><br># 论文使用数据集和训练资源<br>- **数据集**：DexGraspNet 3.0，一个在Objaverse数据集基础上构建的新数据集。它包含174k个物体和170M个抓握姿态。这些数据被分成两个子集：Ours-Wrap（169k个物体，103M个抓握姿态）和Ours-Pinch（139k个物体，67M个抓握姿态）。<br>- **训练资源**：论文中未具体说明硬件和训练时间。所使用的方法（GPT-4o过滤、SAMesh分割、流匹配去噪）暗示其依赖于标准的大规模深度学习基础设施（例如，GPU集群）。<br><br># 论文使用的评估环境和评估指标<br>- **评估环境**：评估在一个仿真桌面环境中进行。物体被放置在桌子表面上，相机的位置被安排在距离桌子中心80厘米的圆周上，并以45度的俯角向下朝向原点看。<br>- **评估指标**：评估使用以下指标：<br> - **Q1**：一个用于量化力封闭质量的度量，与现有数据集相当。<br> - **Pen（穿透深度）和SPen（自穿透深度）**：以毫米为单位测量的穿透深度，用于评估合成抓握的物理质量。<br> - **Suc（成功率）**：一个任务导向的指标，用于衡量在语言指令下成功抓握目标物体部分的百分比。<br> - **PGA（部分抓握能力）**：一个指标，用于评估生成的抓握在语义上与目标物体部分的对齐程度。</details> |
| 2025-07-02 | cVLA: Towards Efficient Camera-Space VLAs | http://arxiv.org/abs/2507.02190 | <details><summary>展开</summary>待生成</details> |
| 2025-07-02 | A Survey on Vision-Language-Action Models: An Action Tokenization Perspective | http://arxiv.org/abs/2507.01925 | <details><summary>展开</summary>待生成</details> |
| 2025-07-02 | TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control | http://arxiv.org/abs/2507.01424 | <details><summary>展开</summary>待生成</details> |
| 2025-07-01 | Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding | http://arxiv.org/abs/2507.00416 | <details><summary>展开</summary>### 论文研究单位<br>上海交通大学人工智能学院、EvoMind Tech、上海市人工智能研究院、剑桥大学。<br><br>### 论文概述<br>该论文提出了Evo-0，一种具有隐式空间理解的视觉-语言-动作（VLA）模型。现有的VLA模型通常依赖于在2D图像-文本对上预训练的视觉-语言模型（VLM），因此缺乏精确的3D空间理解能力。为解决此问题，一些方法引入了显式的3D输入（如点云或深度图），但这需要额外的深度传感器或预训练的深度估计模型。Evo-0通过一个即插即用的模块，利用一个现成的视觉几何基础模型（VGGT）隐式地将3D几何特征融入VLA模型，仅从RGB图像中就能为模型提供具有深度感知的视觉表示，从而增强其对场景几何结构和物体空间关系的理解。论文在模拟和真实世界的多种空间挑战性任务上进行了评估，结果表明Evo-0显著优于最先进的VLA模型。<br><br>### 论文核心贡献点<br>1. 提出了一个即插即用的模块，通过隐式注入3D几何先验来增强VLA模型的空间理解能力，而无需使用深度传感器或显式的深度估计。<br>2. 在模拟和真实世界的一系列具有空间挑战性的任务上评估了该方法，并证明了其相对于强基线模型的持续改进。<br>3. 设计了一个在多种扰动条件下的鲁棒性评估设置，以验证该方法在真实世界扰动中的有效性。<br><br>### 论文方法描述<br>Evo-0基于开源VLA模型π₀构建。该方法的架构包含两个编码器：一个2D图像编码器（来自ViT）和一个VGGT空间编码器。输入是多视角RGB图像。VGGT编码器输出包含3D几何信息的特征，这些特征被提取为3D tokens。然后，一个轻量级的融合模块通过单层交叉注意力机制将2D视觉 tokens（作为查询）与VGGT的3D tokens（作为键和值）进行融合。融合后的特征被输入到PaliGemma视觉语言模型中，该模型结合几何增强的视觉输入和语言 tokens 来预测机器人动作。为了保持计算效率，核心VLM参数被冻结，仅在融合模块、LoRA层和流匹配动作专家模块上进行微调。<br><br>### 论文使用数据集和训练资源<br>- **模拟数据集**: RLBench基准测试中的五个任务：PlayJenga, PutKnifeOnChoppingBoard, TakeUmbrellaOutOfUmbrellaStand, PlaceHangerOnRack, MoveHanger。每个任务生成100条演示轨迹进行多任务训练。<br>- **真实世界数据集**: 针对五个真实世界任务，通过遥操作收集了100条专家演示数据。这些任务包括：在目标上居中圆柱体、轴孔插入、抓取中间的瓶子、罐子抓取放置、透明物体抓取放置。<br>- **训练资源**: 使用单块NVIDIA A800 GPU（80GB）进行训练，采用bfloat16混合精度，批次大小为32。优化器为AdamW，权重衰减为10^{-10}，采用余弦学习率调度，初始学习率为2.5 × 10^{-5}，在1000步内预热后衰减至2.5 × 10^{-6}。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**:<br> - **模拟**: 在CoppeliaSim模拟器中的RLBench环境，使用配备前置、腕部和俯视摄像头的Franka Panda机器人。<br> - **真实世界**: 使用真实机器人臂执行五个空间操作任务。<br> - **鲁棒性测试**: 在真实世界环境中，对一个简化任务逐步引入五种扰动：未见过的干扰物、背景颜色变化、目标位置偏移、目标高度变化和相机视角变化。<br>- **评估指标**:<br> - **主要指标**: 任务成功率。在模拟任务中，成功标准依据官方RLBench规范。<br> - **真实世界任务**: 除“在目标上居中圆柱体”任务外，均采用二元成功指标。该任务采用0-5分的评分系统，根据圆柱体中心落在目标靶的哪个环来评分，得分越高代表精度越高。<br> - **鲁棒性测试**: 同样使用任务成功率，并在特定条件下（如存在干扰物时）评估子任务的成功率（如选择正确物体的成功率）。</details> |
| 2025-06-30 | A Survey on Vision-Language-Action Models for Autonomous Driving | http://arxiv.org/abs/2506.24044 | <details><summary>展开</summary>### 论文研究单位<br>McGill University (加拿大), Tsinghua University (中国), Xiaomi Corporation, University of Wisconsin–Madison (美国), University of Minnesota–Twin Cities (美国), State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University (中国)<br><br>### 论文概述<br>这是第一篇关于自动驾驶视觉-语言-动作（VLA）模型的全面综述，系统梳理了VLA4AD领域的发展脉络。论文从自动驾驶技术演进切入，对比了经典模块化流水线、端到端学习、视觉语言模型（VLM）和最新的VLA范式，分析了超过20个代表性模型，整合了现有数据集和基准测试，并探讨了开放挑战与未来方向。<br><br>### 论文核心贡献点<br>- 首次系统性地梳理自动驾驶领域的VLA模型研究进展<br>- 形式化了VLA4AD的通用架构构建模块<br>- 追溯了从早期解释器到推理中心VLA模型的四个演进阶段<br>- 对比分析了20多个代表性VLA模型的核心特性与技术差异<br>- 整合了现有数据集和评估基准，提出联合评估驾驶安全性、准确性和解释质量的协议<br>- 详述了鲁棒性、实时效率、形式验证等开放挑战<br>- 提出了基础规模驾驶模型、神经符号安全内核等未来研究方向<br><br>### 论文方法描述<br>VLA4AD的核心架构包含三个主要部分：<br>1. 多模态输入处理：视觉数据（单目/多目相机、BEV表示）、其他传感器数据（LiDAR、雷达、IMU、GPS）和语言输入（导航指令、环境查询、任务级规范）<br>2. 核心架构模块：<br> - 视觉编码器：使用DINOv2、ConvNeXt-V2或CLIP等自监督主干网络<br> - 语言处理器：基于LLaMA2或GPT风格的Transformer，支持指令微调和检索增强<br> - 动作解码器：包括自回归分词器、扩散头或流匹配/策略梯度专家<br>3. 驾驶输出：低级动作（转向角、油门、制动）和轨迹规划（BEV坐标下的路径点）<br><br>### 论文使用数据集和训练资源<br>- BDD100K/BDD-X：100k美国驾驶视频，7k带有人类解释标注的片段<br>- nuScenes：1k个20秒真实场景（波士顿/新加坡），含6摄像头+LiDAR+雷达<br>- Bench2Drive：CARLA闭环基准，220条路线覆盖44种场景类型<br>- Reason2Drive：600k视频-文本对，包含CoT风格的问答标注<br>- DriveLM-Data：18k场景图结构QA，支持条件推理<br>- Impromptu VLA：80k边缘案例片段（30秒），密集人群/救护车/恶劣天气<br>- NuInteract：1k多视图场景，密集标注和3D多轮问答<br>- DriveAction：2.6k真实场景和16.2k视觉语言QA对<br>训练资源包括大规模多传感器日志（如nuScenes、Waymo车队数据）和互联网规模视觉-语言预训练模型<br><br>### 论文使用的评估环境和评估指标<br>评估环境：<br>- 闭环驾驶测试（CARLA模拟器、Navsim-v2）<br>- 开环预测评估<br>- 多基准测试套件（Bench2Drive、Reason2Drive、DriveLM）<br><br>评估指标：<br>- 驾驶安全性指标（碰撞率、违规率）<br>- 控制准确性指标（轨迹偏差、成功率）<br>- 语言能力评估（BLEU、图一致性、逻辑一致性）<br>- 鲁棒性测试（压力场景表现）<br>- 人类偏好对齐评估（通过DriveAction等数据集）<br>- 实时性指标（推理延迟、控制频率）</details> |
| 2025-06-29 | IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering | http://arxiv.org/abs/2506.23329 | <details><summary>展开</summary>待生成</details> |
| 2025-06-27 | 4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration | http://arxiv.org/abs/2506.22242 | <details><summary>展开</summary>待生成</details> |
| 2025-06-26 | WorldVLA: Towards Autoregressive Action World Model | http://arxiv.org/abs/2506.21539 | <details><summary>展开</summary>待生成</details> |
| 2025-06-26 | Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends | http://arxiv.org/abs/2506.20966 | <details><summary>展开</summary>### 论文研究单位<br>中国科学院自动化研究所多模态人工智能系统重点实验室，中国科学院大学人工智能学院，美国伊利诺伊大学厄巴纳-香槟分校格兰杰工程学院。<br><br>### 论文概述<br>本文是一篇综述性论文，探讨了视觉-语言-行动模型的后训练与人类运动学习之间的相似性。论文指出，VLA模型在经过大规模预训练后，仍需通过后训练来适应特定的下游应用，以提高在特定环境、任务和机器人实体上的性能。文章借鉴人类运动学习的视角，围绕环境、实体和任务三个维度，系统性地回顾了VLA模型的后训练策略，并提出了一个与之对齐的分类法，最后讨论了该领域面临的挑战和未来趋势。<br><br>### 论文核心贡献点<br>1. 首次从人类运动学习的视角，对VLA模型的后训练方法进行了系统性综述。<br>2. 提出了一个围绕环境、实体和任务的结构化分类法，该分类法与人类运动学习机制相呼应，用于组织和分析现有的VLA后训练方法。<br>3. 识别并阐述了当前VLA模型后训练面临的关键挑战和未来的研究方向，为该领域的后续研究提供了概念框架和实践见解。<br><br>### 论文方法描述<br>论文提出一个与人类运动学习机制对齐的分类法来综述VLA模型的后训练方法，该分类法包含四个主要方面：<br>1. 增强环境感知：旨在提升模型对操作环境的理解能力。方法包括可供性引导学习、为操作任务增强编码器和操作任务增强表征。<br>2. 改进实体意识：旨在让模型更好地理解和适应特定机器人实体的动态特性。方法包括学习正向/逆向运动学、设计更好的动作输出头。<br>3. 深化任务理解：旨在让模型更深刻地掌握操作任务的逻辑和层次。方法包括人机交互学习、分层任务操作。<br>4. 多组件集成：旨在将环境、实体和任务的多个方面进行协同优化。方法包括使用强化学习、视觉交互预测和主动数据处理。<br><br>### 论文使用数据集和训练资源<br>论文主要提到了Open X-Embodiment数据集，该数据集聚合了58个现有的机器人操作数据集。此外，还提及了多种高保真度模拟器和任务特定的模拟环境被用于生成操作数据以降低数据采集成本。关于具体的训练资源（如计算硬件、训练时长等），原文中未提及。<br><br>### 论文使用的评估环境和评估指标<br>所提供的论文HTML原文中未包含此部分内容。</details> |
| 2025-06-24 | Unified Vision-Language-Action Model | http://arxiv.org/abs/2506.19850 | <details><summary>展开</summary>待生成</details> |
| 2025-06-24 | CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation | http://arxiv.org/abs/2506.19816 | <details><summary>展开</summary>论文研究单位<br>未在提供的文本中找到。<br><br>论文概述<br>该论文提出CronusVLA，一个用于高效且鲁棒机器人操作的统一框架。现有的视觉-语言-动作模型受限于单帧图像范式，无法充分利用多帧历史信息，并且直接处理多帧会带来巨大的计算开销和推理延迟。CronusVLA通过一个两阶段过程将单帧VLA模型扩展到多帧范式：(1) 单帧预训练，在大规模具身数据集上建立有效的视觉-语言基础；(2) 多帧后训练，通过特征分块聚合历史信息，将视觉-语言骨干网络的预测从离散标记调整为可学习特征。此外，论文还引入了SimplerEnv-OR基准，用于定量评估模型在时间和空间干扰下的观测鲁棒性。实验表明，CronusVLA在模拟和真实世界中实现了领先的性能和优越的鲁棒性。<br><br>论文核心贡献点<br>- 提出了一个名为CronusVLA的通用框架，通过单帧预训练和多帧后训练将VLA模型扩展到多帧范式。<br>- 提出了SimplerEnv-OR，一个新基准，用于在观测干扰下对模型鲁棒性进行定量评估。<br>- 通过广泛的模拟和真实世界实验，证明了CronusVLA的领先性能和强大的观测鲁棒性。<br><br>论文方法描述<br>CronusVLA采用一个两阶段训练方法。首先是单帧预训练，在大规模异构数据集（如OXE）上训练一个基本的单帧VLA模型，通过自回归预测离散的动作标记。然后是多帧后训练，引入可学习特征来代替离散动作标记，并对高质量跨具身数据集（如Bridge-v2和Fractal）进行训练。该方法核心是特征分块，将来自多个历史帧的可学习特征聚合成一个块。一个基于DiT的跨帧解码器通过特征调节器和交叉注意力机制来解码这个特征块，以预测动作序列。为了保护预训练的单帧感知能力，论文采用了多帧正则化，将历史特征作为辅助输入，其梯度不回传到视觉-语言骨干网络。<br><br>论文使用数据集和训练资源<br>- 数据集：<br> - 预训练：OXE等大规模异构具身数据集。<br> - 后训练：Bridge-v2和Fractal数据集，包含约148k个片段和5M个多帧片段。<br> - 评估：SimplerEnv、LIBERO、SimplerEnv-OR以及真实世界收集的数据。<br>- 训练资源：所有实验均基于A100 GPU。<br><br>论文使用的评估环境和评估指标<br>- 评估环境：<br> - 模拟环境：SimplerEnv（包含Google Robot和WidowX Robot两种设置）、LIBERO（包含Spatial, Object, Goal, Long四种任务套件）、SimplerEnv-OR（用于鲁棒性测试）。<br> - 真实世界环境：Franka Research 3机器人平台。<br>- 评估指标：<br> - 成功率：任务成功的试验百分比，在SimplerEnv、LIBERO和真实世界实验中使用。<br> - 鲁棒性得分：在SimplerEnv-OR中使用，定义为 R-Score = 100 * (SR_i / SR)，其中SR是原始成功率，SR_i是干扰下的成功率。<br> - 推理速度：以Hz为单位，用于评估模型效率。</details> |
| 2025-06-23 | MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis | http://arxiv.org/abs/2506.18897 | <details><summary>展开</summary>待生成</details> |
| 2025-06-22 | RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation | http://arxiv.org/abs/2506.18088 | <details><summary>展开</summary>论文研究单位<br>MoE key Lab of Artificial Intelligence, AI Institute, SJTU, HKU MMLab, Shanghai AI Lab, D-Robotics, SZU, THU, TeleAI, FDU, USTC, SUSTech, SYSU, CSU, NEU, HKU-SH ICRC, NJU, Lumina EAI<br><br>论文概述<br>RoboTwin 2.0是一个可扩展的仿真框架，旨在自动化、大规模生成用于鲁棒双臂机器人操作的高质量、多样化且现实的数据，并提供统一的评估协议。该框架解决了现有合成数据集在新型任务数据生成效率和仿真环境真实性方面的不足，通过多模态大语言模型与仿真反馈循环相结合的方法，生成专家级轨迹数据，并采用系统性域随机化策略增强策略鲁棒性。<br><br>论文核心贡献点<br>1. 开发了自动化专家数据生成框架，结合多模态大语言模型与仿真反馈循环确保高质量专家轨迹<br>2. 提出系统性域随机化策略，通过增加数据多样性增强策略鲁棒性和仿真到现实的泛化能力<br>3. 引入具身感知抓取调整机制，基于物体功能属性生成特定机器人操作候选方案<br>4. 发布RoboTwin-OD资产库、大规模预收集多具身域随机轨迹数据集、可扩展双臂数据生成器和标准化评估基准<br><br>论文方法描述<br>1. 专家代码生成：使用多模态大语言模型生成初始Python程序，通过模拟执行和视觉语言模型观察进行迭代优化，包含代码生成代理和VLM观察者的双代理架构<br>2. 域随机化：沿五个维度实施系统性随机化 - 场景杂乱、背景纹理、光照条件、桌面高度和多样化语言指令<br>3. 具身感知抓取适应：为每个物体标注候选操作姿态，覆盖多个抓取轴和接近方向，并应用偏向高臂可达性方向的角扰动<br><br>论文使用数据集和训练资源<br>1. 数据集：RoboTwin-OD包含731个物体实例（147个类别），其中534个实例通过RGB到3D重建生成，153个来自Objaverse，44个来自SAPIEN PartNet-Mobility<br>2. 预收集数据：超过100,000个双臂操作轨迹，跨越50个任务和5个机器人平台<br>3. 训练资源：使用多模态大语言模型进行代码生成，视觉语言模型进行执行监控，Stable Diffusion v2生成11,000个高质量纹理<br><br>论文使用的评估环境和评估指标<br>1. 评估环境：仿真环境使用Aloha-AgileX、Piper、Franka、UR5、ARX-X5五种双臂机器人平台；真实环境使用COBOT-Magic双臂平台<br>2. 评估指标：<br> - ASR（Average Success Rate）：平均成功率<br> - Top5-ASR：前5候选程序成功率<br> - CR-Iter：平均精炼迭代次数<br> - Token：生成策略代码的平均token数<br> - 任务成功率：在清洁和域随机化条件下的单任务成功率</details> |
| 2025-06-21 | RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models | http://arxiv.org/abs/2506.17811 | <details><summary>展开</summary>待生成</details> |
| 2025-06-21 | RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models | http://arxiv.org/abs/2506.17639 | <details><summary>展开</summary>论文研究单位<br>上海交通大学机械工程学院<br><br>论文概述<br>论文提出RLRC，一种用于压缩视觉-语言-动作（VLA）模型的三阶段恢复方法。VLA模型因参数量大和推理延迟高，难以部署到资源受限的机器人平台。RLRC通过结构化剪枝、基于监督微调（SFT）和强化学习（RL）的性能恢复，以及可选的4位量化，实现高达8倍的内存减少和2.3倍的推理吞吐量提升，同时保持或超过原始VLA的任务成功率。<br><br>论文核心贡献点<br>- 探索了通用模型压缩技术（量化、剪枝、知识蒸馏）在VLA中的应用，提供了性能权衡的实证见解。<br>- 提出了一种结合结构化剪枝、监督微调、强化学习和训练后量化的新压缩框架，实现高压缩率的同时保持甚至提高任务精度。<br>- 通过广泛实验验证方法有效性，展示了在资源受限条件下部署VLA的实用价值。<br><br>论文方法描述<br>RLRC包含三阶段：<br>1. 结构化剪枝：针对VLA的LLM组件，采用LLM-Pruner框架，以90%剪枝率移除冗余结构，使用Taylor重要性准则，保留首尾层以维持稳定性。<br>2. 性能恢复：首先用SFT在任务特定数据上微调剪枝后模型，恢复大部分性能；随后采用PPO算法进行RL微调，通过稀疏奖励函数（成功放置1.0、抓取0.1、其他0）优化长期回报，使用共享Transformer骨干的actor-critic设计。<br>3. 进一步量化：可选应用4位量化，进一步压缩模型内存占用，适配资源受限设备部署。<br><br>论文使用数据集和训练资源<br>数据集：ManiSkill3的PutOnPlateInScene25Main任务套件，包含16个IND任务（训练时见过）和9个OOD任务（未见过）。<br>训练资源：硬件测试使用单块NVIDIA RTX 5880 Ada；训练细节包括SFT约10k步，RL约0.6M步。<br><br>论文使用的评估环境和评估指标<br>评估环境：ManiSkill3模拟环境，基于8-DoF WidowX-250S机械臂。<br>评估指标：任务成功率（主指标）、内存消耗、推理时间每步、动作吞吐量。</details> |
| 2025-06-21 | VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models | http://arxiv.org/abs/2506.17561 | <details><summary>展开</summary>论文研究单位<br>新加坡国立大学、复旦大学、清华大学、南洋理工大学<br><br>论文概述<br>该论文提出了VLA-OS模型系列，用于系统研究视觉-语言-动作模型中任务规划的不同表示和范式。论文通过统一的架构设计，隔离了网络架构和训练数据的影响，对语言、视觉和图像预测等规划表示进行了全面比较，并在多种机器人操作任务上评估了不同VLA范式的性能。<br><br>论文核心贡献点<br>- 提出了统一的VLA-OS模型家族，包含三种主流VLA范式：ActionOnly-VLA、Integrated-VLA和Hierarchical-VLA<br>- 设计了三种规划表示：语言推理、视觉推理和图像预测，并构建了相应的标注数据集<br>- 在六个基准数据集上进行了系统性实验，包括刚体操作、3D泛化、真实世界可变形操作、灵巧操作和双臂操作任务<br>- 得出了视觉基础的规划表示优于语言表示、分层范式性能最佳但成本更高的关键结论<br><br>论文方法描述<br>VLA-OS采用Qwen2.5作为LLM骨干，使用集成视觉编码器(DINOV2+SigLIP)。ActionOnly范式(VLA-OS-A)直接生成动作；Integrated范式(VLA-OS-I)在单一模型中同时进行规划和动作生成，支持隐式和显式规划；Hierarchical范式(VLA-OS-H)使用两个独立模型分别处理任务规划和策略学习。模型支持2D/3D输入，使用块级因果注意力机制，并设计了可插拔的规划头。<br><br>论文使用数据集和训练资源<br>数据集包括LIBERO系列(2D刚体操作)、COLOSSEUM(3D泛化)、真实世界可变形操作数据集、DexArt(灵巧操作)、FurnitureBench(长时序任务)和PerAct2(双臂操作)。训练资源为8块NVIDIA A100 80G GPU，模型规模从0.5B到7B参数不等。<br><br>论文使用的评估环境和评估指标<br>评估环境涵盖仿真环境(LIBERO、COLOSSEUM)和真实世界环境。评估指标包括任务成功率、规划分数(用于评估任务规划部分性能)、泛化能力(在扰动环境下的表现)、数据/模型可扩展性测试、持续学习能力和训练/推理效率。</details> |
| 2025-06-19 | CapsDT: Diffusion-Transformer for Capsule Robot Manipulation | http://arxiv.org/abs/2506.16263 | <details><summary>展开</summary>待生成</details> |
| 2025-06-19 | ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models | http://arxiv.org/abs/2506.16211 | <details><summary>展开</summary>待生成</details> |
| 2025-06-17 | FormGym: Doing Paperwork with Agents | http://arxiv.org/abs/2506.14079 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics | http://arxiv.org/abs/2506.14009 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning | http://arxiv.org/abs/2506.13757 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction | http://arxiv.org/abs/2506.13751 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding | http://arxiv.org/abs/2506.13725 | <details><summary>展开</summary>论文研究单位<br>HKUST(GZ), Westlake University, Zhejiang University<br><br>论文概述<br>视觉-语言-动作（VLA）模型因其多模态理解和泛化能力而成为机器人学的重要研究方向，但其推理速度瓶颈严重制约了实际部署，尤其是在高频和灵巧操作任务中。虽然Jacobi解码作为一种替代方案被提出，但其加速效果有限。为解决此问题，论文提出CEED-VLA模型，通过一致性蒸馏训练使模型能够在单次迭代中预测多个正确的动作标记，并引入混合标签监督来减轻蒸馏过程中的误差累积。此外，论文识别出Jacobi解码中的低效迭代问题，并提出早期退出解码策略来放宽收敛条件，从而进一步提升平均推理效率。实验结果表明，该方法在不同基线上实现了超过4倍的推理加速，同时保持了较高的任务成功率。<br><br>论文核心贡献点<br>- 提出CEED-VLA，一种用于显著提升推理效率同时保持操作性能的通用加速方法。<br>- 通过一致性蒸馏过程解锁模型的快速推理能力，并提出在自回归损失中采用混合标签监督来保持模型的操作性能。<br>- 识别出Jacobi解码中的低效迭代是加速的瓶颈，并提出早期退出解码策略解决该问题，最终实现了4.1倍的加速和超过4.3倍的频率提升。<br><br>论文方法描述<br>方法主要包含教师模型和学生模型。教师模型是标准的预训练VLA（如OpenVLA和LLaVA-VLA），用于通过Jacobi解码生成动作序列并构建Jacobi轨迹数据集。学生模型（CEED-VLA）通过一致性蒸馏进行训练，其目标是将Jacobi轨迹中的任意中间状态直接映射到收敛的固定点。为实现此目标，训练过程包含两个损失函数：1）一致性损失，使用KL散度使学生模型的预测分布与教师模型在固定点的分布对齐；2）混合标签自回归损失，它自适应地在教师模型输出和真实标签之间选择监督信号，以防止模型性能偏离原始分布。推理时，采用早期退出解码策略，即不完全满足严格收敛条件便提前退出迭代，从而绕过低效迭代，提升整体速度。<br><br>论文使用数据集和训练资源<br>- 数据集：CALVIN和LIBERO模拟环境数据集，以及一个真实的机器人臂部署环境用于真实世界实验。训练所需的Jacobi轨迹数据集是通过在原始机器人数据集上运行教师模型生成的。<br>- 训练资源：论文在LLaVA-VLA和OpenVLA这两个预训练VLA模型的基础上进行微调，但未明确提及所使用的具体硬件资源（如GPU型号）。<br><br>论文使用的评估环境和评估指标<br>- 评估环境：包括模拟环境（CALVIN和LIBERO基准）和真实世界环境（搭载真实机器人臂）。<br>- 评估指标：主要指标是任务成功率，用以衡量操作性能；另一个核心指标是推理加速倍数，用于量化模型推理速度的提升程度。在真实世界实验中还评估了控制频率。</details> |
| 2025-06-16 | ROSA: Harnessing Robot States for Vision-Language and Action Alignment | http://arxiv.org/abs/2506.13679 | <details><summary>展开</summary>### 论文研究单位<br>中国科学技术大学, 南京大学, Dexmal<br><br>### 论文概述<br>该论文提出了ROSA，一种新的训练范式，旨在解决视觉-语言-动作模型在将高级视觉-语言理解与低级机器人物理动作对齐时存在的时空鸿沟问题。现有方法直接微调视觉-语言模型，但面临数据效率低下和对人工标注依赖性强的问题。ROSA通过引入一种自动收集的机器人状态估计数据作为辅助监督，增强模型对3D空间的理解和自我感知能力，从而提升模型在数据有限情况下的性能和泛化能力。实验在RLBench模拟环境和真实的WidowX机器人平台上验证了该方法的有效性。<br><br>### 论文核心贡献点<br>1. 提出了一种名为ROSA的新颖训练范式，它利用机器人状态估计数据来实现视觉-语言和动作空间之间更好的对齐。<br>2. 提出了一种简单而有效的解决方案来创建机器人状态估计数据，在不需要额外人类收集工作的情况下，显著增强了VLA的数据效率。<br>3. 在RLBench模拟和一个真实世界的WidowX平台上进行了广泛的实验，证明ROSA有效增强了当前的VLA模型，并实现了优于先前方法的性能。<br><br>### 论文方法描述<br>ROSA方法的核心是将VLA模型的训练分解为两个互补部分：预测未来动作和估计当前机器人状态。<br>1. **训练数据**：<br> * **专家动作数据**：由人类操作员收集的轨迹数据，包含视觉观测、语言指令和对应的7自由度机器人动作（3D位置、3D欧拉角、1个夹爪开合状态）。<br> * **机器人状态估计数据**：通过自动化脚本控制机器人在预定义环境中随机移动并记录观测与状态。这些数据同样包含7自由度的状态信息，并配以统一的语言指令，如“What is the current state of the robot?”，以实现与专家数据的格式统一和联合训练。<br>2. **模型架构**：<br> * 基于LLaVA架构，包含一个视觉编码器、一个投影器和一个大语言模型。<br> * 视觉编码器使用CLIP ViT-L/14，投影器为两层MLP，大语言模型骨干为Qwen-2.5-7B。<br> * 为了让LLM预测连续的动作和状态，采用线性量化的方法将连续值离散化为token，推理时再通过逆映射恢复为连续值。<br>3. **训练目标**：<br> * 对专家动作数据和机器人状态数据使用统一的训练目标，即下一个token预测的交叉熵损失。<br> * 模型以1:4的比例混合两种数据进行训练，并完全微调所有层。<br><br>### 论文使用数据集和训练资源<br>1. **数据集**：<br> * **RLBench**：用于模拟实验，包含12个任务，每个任务包含多个训练变体。<br> * **真实机器人数据**：在WidowX 250S机器人上采集，包括4个已见任务和4个用于泛化评估的未见任务。<br>2. **训练资源**：<br> * 使用8块NVIDIA A100 GPU进行训练。<br> * 基于Qwen-2.5-7B、CLIP ViT-L/14模型构建。<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**：<br> * **RLBench模拟环境**：使用Franka Panda机器人，配备一个固定的前置RGB摄像头（336x336分辨率）。<br> * **真实世界环境**：使用WidowX 250S机器人和一个Intel RealSense D435摄像头提供第三人称视角。<br>2. **评估指标**：<br> * 主要评估指标是**成功率**，即任务成功的episode或试验次数占总数的百分比。<br> * 在RLBench上，每个任务评估25个episode，并重复三次，报告平均成功率。<br> * 在真实机器人上，每个任务评估10次试验。</details> |
| 2025-06-15 | SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration | http://arxiv.org/abs/2506.12723 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models | http://arxiv.org/abs/2506.10100 | <details><summary>展开</summary>### 论文研究单位<br>上海交通大学人工智能学院，哈尔滨工业大学，西安交通大学，电子科技大学<br><br>### 论文概述<br>论文提出EfficientVLA，一个无需训练的结构化推理加速框架，用于解决视觉-语言-动作（VLA）模型在推理阶段的计算和内存瓶颈。该方法通过系统性地消除多层冗余：基于层间相似性分析修剪语言模型中的冗余层；采用任务感知策略选择紧凑且多样化的视觉token；在扩散式动作头中缓存中间特征以减少时间冗余。实验表明，该方法在CogACT模型上实现1.93倍推理加速，FLOPs降至28.9%，成功率仅下降0.6%。<br><br>### 论文核心贡献点<br>1. 系统性分析扩散式VLA架构的计算瓶颈与多层冗余机制。<br>2. 提出无需训练的EfficientVLA框架，集成三种冗余消除策略：语言层修剪、任务感知视觉token选择、动作头特征缓存。<br>3. 设计时间相关性缓存机制，在扩散去噪过程中重用中间注意力与MLP计算。<br>4. 在SIMPLER基准上验证有效性：实现1.93×加速，FLOPs降至28.9%，精度损失0.6%。<br><br>### 论文方法描述<br>1. **语言模型修剪**：通过计算层输入/输出隐藏状态的余弦相似度，定义重要性分数（公式1），修剪低分数的非连续层。<br>2. **视觉token修剪**：量化任务相关性（基于视觉-语言交叉注意力分数），选择关键token后通过多样性增强（任务驱动+多样性驱动）平衡相关性与信息覆盖。<br>3. **特征缓存**：利用DiT块中特征的时间连贯性，在扩散动作头中静态缓存N步中间特征，减少相邻步骤的重复计算。<br><br>### 论文使用数据集和训练资源<br>- **基础模型**：标准VLA模型CogACT<br>- **评估数据集**：SIMPLER环境（仿真环境）的任务数据集<br>- **训练资源**：方法无需额外训练，直接应用于预训练模型<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：SIMPLER仿真环境<br>- **评估指标**：推理速度（加速倍数）、FLOPs（计算量）、成功率（任务完成率）</details> |
| 2025-06-11 | SAFE: Multitask Failure Detection for Vision-Language-Action Models | http://arxiv.org/abs/2506.09937 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models | http://arxiv.org/abs/2506.09930 | <details><summary>展开</summary>### 论文研究单位<br>New York University<br><br>### 论文概述<br>本文提出了一个名为INT-ACT的视觉-语言-动作（VLA）模型泛化能力探测套件，包含50个模拟任务，覆盖10个子类别，旨在系统性地评估当前最先进VLA模型的泛化边界。研究发现，尽管VLA模型在语义理解（意图）方面表现出色，但在执行动作时存在显著差距，特别是在分布外（OOD）观察下，策略往往表现出连贯的意图，却在动作执行上失败。此外，在动作数据上的微调可能会削弱原始VLM的通用推理能力。论文发布了任务套件和评估代码，作为未来VLA模型的标准化基准。<br><br>### 论文核心贡献点<br>1. 提出并开源了INT-ACT，一个全面的VLA泛化探测套件，包含跨越3大类和10个子类别的50个任务，显著扩展了现有VLA基准的范围。<br>2. 通过广泛的基准测试，揭示了当前最先进VLA模型的两个关键失败模式：<br> - 持续且显著的意图-行动差距（Intention-Action Gap），即在分布偏移下强大的语义理解未能转化为可靠的执行。<br> - 脆弱的多模态泛化能力，尤其是在语言变化和复合视觉-语言分布偏移下表现不佳。<br><br>### 论文方法描述<br>1. **测试平台选择**：基于SimplerEnv基准（构建于ManiSkill2模拟器之上），因其设计能紧密匹配模型在真实世界中的性能。<br>2. **设计原则**：探测任务分为三个主要类别：<br> - **对象多样性（Object Diversity）**：引入分布外对象，包括家居物品和工业工具，测试模型对新对象、外观和功能性的泛化能力。<br> - **语言复杂性（Language Complexity）**：从模板化命令（如“把A放在B上”）扩展到组合性、知识和推理密集的指令，包括动作动词改写、语义否定和参考性外观描述。<br> - **视觉-语言思维（Vision-Language Thinking）**：通过添加视觉干扰物和需要常识推理的干扰物（如橙子与橙汁盒），测试模型在复杂环境中的鲁棒性。<br>3. **评估指标**：<br> - **抓取成功率**：机器人夹爪是否成功抓取正确的源对象。<br> - **意图正确率**：夹爪是否在任何一帧内移动到正确源对象的小半径内，捕获策略意图。<br> - **任务成功率**：任务是否成功完成。<br><br>### 论文使用数据集和训练资源<br>1. **数据集**：主要基于BridgeV2数据集进行微调和评估，该数据集与SimplerEnv一致。<br>2. **模型选择与训练协议**：评估了四种代表性模型（π₀、SpatialVLA、Magma、Octo）及其变体，所有微调实验均在BridgeV2上进行，遵循各自论文的训练和微调协议以确保公平比较。<br>3. **训练资源**：使用了NYU IT高性能计算资源、服务和专业知识，并得到NSF资助（拨款号2238968, 2322242, 2026479）。<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**：完全在模拟环境中进行，基于ManiSkill2模拟器构建的SimplerEnv基准，确保部署的低门槛和可重复性。<br>2. **评估指标**：<br> - **抓取成功率（Grasp Success Rate）**：衡量夹爪成功抓取正确源对象的能力。<br> - **意图正确率（Intention Correct Rate）**：新增指标，评估夹爪是否移动到正确源对象附近，即使后续抓取失败也记录意图。<br> - **任务成功率（Task Success Rate）**：衡量任务整体完成情况。<br> - **错误对象尝试率（Wrong Object Attempt Rate）**：在干扰物场景中监控是否移动了非源对象。<br>3. **评估协议**：每个任务评估24个情节，对应所有预定义的场景和对象配置，每个配置重复3个随机种子，所有指标平均跨情节和种子计算。</details> |
| 2025-06-11 | OctoNav: Towards Generalist Embodied Navigation | http://arxiv.org/abs/2506.09839 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models | http://arxiv.org/abs/2506.09172 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency | http://arxiv.org/abs/2506.08822 | <details><summary>展开</summary>### 论文研究单位<br><br>北京类人机器人创新中心 (Beijing Innovation Center of Humanoid Robotics)<br>中国科学院自动化研究所模式识别国家重点实验室 (NLPR, MAIS, Institute of Automation of Chinese Academy of Sciences)<br><br>### 论文概述<br><br>论文提出了FreqPolicy，一种新颖的基于流的视觉运动策略，旨在通过频率一致性约束实现高效的单步动作生成。该方法首次在基于流的视觉运动策略中引入时间知识，以解决机器人操作中动作轨迹的时间依赖性问题。通过在频域中对齐不同时间步的动作特征，并结合自适应频率分量损失，FreqPolicy在保持生成动作质量的同时显著提高了推理速度。<br><br>### 论文核心贡献点<br><br>1. 首个利用时间知识进行机器人操作的单步视觉运动策略。<br>2. 借鉴时间序列和语音处理领域，提出频率一致性约束目标以增强任意两个动作速度的规律化，并提出自适应频率分量损失以有效捕捉动作序列的结构性时间变化。<br>3. 在3个模拟基准的53个任务上进行广泛实验，证明其优于现有单步动作生成器，例如在MetaWorld上达到84.2%的成功率。<br>4. 将FreqPolicy集成到视觉-语言-动作（VLA）模型中，在不损失任务性能的情况下显著提升推理速度（例如，5倍加速）。<br><br>### 论文方法描述<br><br>FreqPolicy基于流匹配框架，通过学习一个时间依赖的向量场将先验分布（如高斯噪声）转换到目标动作分布。核心创新在于引入频率一致性约束，强制在频域中对齐沿流路径不同时间步的动作速度，从而促进单步动作生成向目标分布收敛。此外，设计了一种自适应频率分量损失，动态强调具有更大差异的频率分量，以捕捉机器人操作任务中固有的结构性时间变化。模型支持2D图像和3D点云输入，输出动作块的速度向量。<br><br>### 论文使用数据集和训练资源<br><br>数据集: 在3个模拟基准（MetaWorld、Robomimic、Libero）的53个任务上进行评估。<br>训练资源: 未明确指定硬件细节，但提及模型训练和推理在标准GPU环境下进行，其中现实世界实验达到93.5 Hz的推理频率。<br><br>### 论文使用的评估环境和评估指标<br><br>评估环境: 模拟环境（MetaWorld、Robomimic、Libero）和真实世界机器人场景。<br>评估指标: 任务成功率（如MetaWorld上的平均成功率）、推理速度（Hz）、与基线方法（如FlowPolicy、CP、ManiCM）的性能比较，以及在VLA模型集成后的任务完成率和推理加速比。</details> |
| 2025-06-10 | Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing | http://arxiv.org/abs/2506.08462 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization | http://arxiv.org/abs/2506.08440 | <details><summary>展开</summary>### 论文研究单位<br>吉林大学人工智能学院<br><br>### 论文概述<br>论文针对视觉-语言-动作模型主要依赖成功演示数据训练，在分布外场景下泛化能力有限的问题，提出了一种基于在线强化学习的VLA模型微调框架TGRPO。该方法利用大语言模型自动构建多阶段密集奖励函数，并结合一种基于分组的策略优化算法，通过相对比较降低训练方差，从而提升模型在复杂长时程任务中的性能。<br><br>### 论文核心贡献点<br>1. 提出了一个面向VLA模型的在线强化学习训练框架，使其能够通过与环境的交互从失败中学习，克服了仅依赖成功演示的根本限制。<br>2. 设计了TGRPO算法，该算法是一种新颖的基于分组的策略优化方法，融合了轨迹级和步级优势估计，以更好地捕获长时程机器人任务的结构，改善了信誉分配并增强了策略稳定性。<br>3. 强调了密集奖励设计与基于分组优化的协同作用，利用LLM解析任务并生成阶段性密集奖励，再结合分组策略，显著提升了强化学习在复杂机器人任务中训练VLA模型的效果。<br><br>### 论文方法描述<br>该方法包含两部分。首先，利用大语言模型（如Claude 3.7 Sonnet）将自然语言指令分解为子任务，并结合环境中关键物体位置与预采集的成功演示中的机器人末端姿态，设计一个多阶段的密集奖励函数，为每个步骤提供细粒度的反馈。其次，提出轨迹分组相对策略优化算法（TGRPO），算法并行采样多条轨迹，然后将这些轨迹在步级和轨迹级两个维度进行分组，分别计算步级相对优势和轨迹级相对优势。通过加权融合这两种优势，得到最终的相对优势信号，并用于一个类似于PPO的裁剪目标函数中，以无价值网络的方式更新VLA模型策略。<br><br>### 论文使用数据集和训练资源<br>实验使用LIBERO机器人模拟器基准进行评估，该基准包含Spatial、Object、Goal、Long四个任务套件。基础模型采用OpenVLA，并使用LoRA进行微调。所有实验均在单块NVIDIA A100 GPU上进行。<br><br>### 论文使用的评估环境和评估指标<br>评估环境为LIBERO模拟器。评估指标为任务成功率，具体为在每个任务上运行50个测试回合，报告每个任务和每个任务套件的平均成功率。</details> |
| 2025-06-09 | HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation | http://arxiv.org/abs/2506.08296 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework | http://arxiv.org/abs/2506.08185 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models | http://arxiv.org/abs/2506.07961 | <details><summary>展开</summary>### 论文研究单位<br>CASIA, ByteDance Seed, UCAS, FiveAges, NJU.<br><br>### 论文概述<br>论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。<br><br>### 论文核心贡献点<br>1. 提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。<br>2. 提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。<br>3. 在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。<br><br>### 论文方法描述<br>BridgeVLA采用两阶段训练策略：<br>1. 2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。<br>2. 3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 预训练：RoboPoint的120K对象检测分割。<br> - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。<br>- **训练资源**：<br> - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。<br> - 训练采用AdamW优化器，具体超参数见附录。<br> - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。<br> - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。<br>- **评估指标**：<br> - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。<br> - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。<br> - 样本效率：在仅3条轨迹/任务训练下的成功率。</think><br>### 论文研究单位<br>CASIA, ByteDance Seed, UCAS, FiveAges, NJU<br><br>### 论文概述<br>论文提出了BridgeVLA，一种新颖的3D视觉-语言-动作（VLA）模型，旨在通过视觉-语言模型（VLM）高效且有效地学习3D机器人操作。该方法通过输入-输出对齐来解决现有3D VLA模型数据效率低的问题，具体包括：在预训练阶段，将VLM骨干网络训练为输入2D图像并输出2D热力图；在微调阶段，通过将原始点云投影到多视图图像并预测热力图后再生成最终动作，以保持输入与输出的对齐。实验表明，BridgeVLA在多个仿真基准和真实机器人任务中均优于现有方法，并展现出卓越的样本效率和泛化能力。<br><br>### 论文核心贡献点<br>1. 提出BridgeVLA，一种通过2D热力图实现输入-输出对齐的3D VLA模型，能够高效学习3D机器人操作。<br>2. 提出一种可扩展的预训练方法，通过对象定位任务使VLM具备基于文本输入预测热力图的能力。<br>3. 在仿真和真实环境中进行了广泛实验，证明BridgeVLA在性能和样本效率上优于现有方法，并在分布外泛化任务中表现鲁棒。<br><br>### 论文方法描述<br>BridgeVLA采用两阶段训练策略：<br>1. 2D热力图预训练：使用RoboPoint的120K对象检测数据集，训练VLM（如PaliGemma）输入图像和文本描述，输出定位目标对象的2D热力图。通过交叉熵损失监督，使模型具备空间感知的预测能力。<br>2. 3D动作微调：将3D点云通过正交投影生成多视图图像，与指令一起输入预训练的VLM。模型预测多视图热力图，通过反投影估计3D平移动作。同时使用全局和局部特征预测旋转、夹爪状态和碰撞标志。采用粗到细的精细化策略和刚性变换增强几何鲁棒性。总损失函数包括平移、旋转、夹爪和碰撞四项交叉熵损失。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 预训练：RoboPoint的120K对象检测分割。<br> - 微调与评估：RLBench（18个任务，每任务100条轨迹）、COLOSSEUM（12种扰动泛化设置）、GemBench（通用泛化）、真实机器人任务（7种设置）。<br>- **训练资源**：<br> - 预训练与微调均使用PaliGemma（SigLIP视觉编码器+Gemma骨干）作为VLM骨干。<br> - 训练采用AdamW优化器，具体超参数见附录。<br> - 预训练在RoboPoint数据上，微调在多任务机器人数据上进行。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真：CoppeliaSim中的RLBench、COLOSSEUM（扩展RLBench的扰动评估）、GemBench。<br> - 真实：Franka Emika Panda机器人，配置RGB-D相机，在七种泛化设置（如视觉扰动、未见物体类别）下测试。<br>- **评估指标**：<br> - 主要指标：任务成功率（Success Rate, SR），在25次试验中计算平均值。<br> - 辅助指标：平均排名（Avg. Rank），跨任务或扰动设置的性能排序。<br> - 样本效率：在仅3条轨迹/任务训练下的成功率。</details> |
| 2025-06-09 | Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse | http://arxiv.org/abs/2506.07639 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation | http://arxiv.org/abs/2506.07530 | <details><summary>展开</summary>```markdown<br># 论文研究单位<br>Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences<br><br># 论文概述<br>本文提出了BitVLA，这是第一个用于机器人操作的1-bit视觉-语言-动作（VLA）模型，其所有参数均为三元值（-1, 0, 1）。该研究旨在解决现有VLA模型因体积过大而难以部署在资源受限的机器人系统上的问题。尽管缺乏大规模的机器人预训练，BitVLA在LIBERO基准测试上实现了与当前最先进的OpenVLA-OFT（4位后训练量化模型）相当的性能，同时仅消耗其29.8%的内存，这使其在内存受限的边缘设备上具有广阔的应用前景。<br><br># 论文核心贡献点<br>1. 提出了首个1-bit VLA模型BitVLA，其中语言模型和视觉编码器的所有参数均被量化为三元值（-1, 0, 1），显著降低了模型部署的内存和计算成本。<br>2. 设计了一种蒸馏感知训练策略，用于将全精度的视觉编码器压缩至1.58-bit。该方法利用全精度编码器作为教师模型，通过最小化中间层表示之间的差异来对齐学生模型（1.58-bit）的潜在表示，有效保留了性能。<br>3. 通过在LIBERO基准测试上的实验证明，BitVLA在多个机器人操作任务上的平均成功率与OpenVLA-OFT（4-bit）模型相当，但内存占用仅为后者的29.8%，验证了其在资源受限环境下的高效性和可行性。<br><br># 论文方法描述<br>BitVLA的构建分为三个主要部分：<br>1. 模型架构：以1-bit LLM BitNet b1.58 2B4T为语言骨干，使用SigLIP-L作为视觉编码器，并通过一个两层的MLP连接器进行对齐。模型采用量化感知训练，权重使用`absmean`量化器量化为`{-1, 0, 1}`，激活值使用`absmax`量化器量化为8-bit整数（[-128, 127]）。为解决量化操作不可微的问题，训练中采用直通估计器（STE）来传递梯度。<br>2. 蒸馏感知训练：为压缩视觉编码器，该方法将全精度的视觉编码器作为教师模型，指导一个1.58-bit权重的学生模型进行训练。总损失函数由两部分组成：标准的语言建模损失（`L_LM`）和表示对齐损失（`L_aux`）。表示对齐损失通过计算教师和学生模型各层输出特征之间的均方误差来确保潜在表示的一致性，其系数`γ`设为0.1。<br>3. 机器人微调：在特定机器人任务上，模型采用与OpenVLA-OFT相同的微调策略。具体包括使用双向注意力掩码替代因果掩码以实现并行解码，并采用动作分块技术一次性生成多个时间步的动作序列。同时，添加一个基于MLP的动作头，将模型的输出映射到连续的机器人动作空间，并通过最小化L1损失进行训练。<br><br># 论文使用数据集和训练资源<br>模型训练遵循一个三阶段流程：<br>1. 第一阶段（连接器预训练）：使用LLaVA 1.5-558k数据集。<br>2. 第二阶段（视觉指令微调）：使用MammoTH-VL数据集的1000万样本子集。<br>3. 第三阶段（蒸馏训练）：使用第二阶段数据中的500万样本子集，将视觉编码器从全精度（W16A16）量化至1.58-bit权重和8-bit激活（W1.58A8），训练数据量最高达100亿个token。<br>训练资源：模型在8张NVIDIA A100（80GB）显卡上完成训练，共耗时14天。<br><br># 论文使用的评估环境和评估指标<br>评估环境：实验在LIBERO仿真环境中进行，该环境包含四个任务套件，分别评估模型在不同方面的泛化能力：<br>1. LIBERO-Spatial：评估空间泛化能力。<br>2. LIBERO-Object：评估对未见物体类别的泛化能力。<br>3. LIBERO-Goal：评估对多样化语言指令的泛化能力。<br>4. LIBERO-Long：评估长时程推理和多任务执行能力。<br>评估指标：主要指标是任务成功率（%），即在各个任务套件上成功完成任务的百分比。报告了每个子任务套件的成功率以及四个套件的平均成功率。此外，在视觉问答（VQA）任务上，评估指标为零样本准确率（%）。<br>```</details> |
| 2025-06-09 | Real-Time Execution of Action Chunking Flow Policies | http://arxiv.org/abs/2506.07339 | <details><summary>展开</summary>待生成</details> |
| 2025-06-08 | Robotic Policy Learning via Human-assisted Action Preference Optimization | http://arxiv.org/abs/2506.07127 | <details><summary>展开</summary>待生成</details> |
| 2025-06-07 | RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation | http://arxiv.org/abs/2506.06677 | <details><summary>展开</summary>待生成</details> |
| 2025-06-06 | DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models | http://arxiv.org/abs/2506.05667 | <details><summary>展开</summary>### 论文研究单位<br>Li Auto Inc.<br><br>### 论文概述<br>该论文介绍了 DriveAction，一个专为视觉-语言-动作（VLA）模型设计的、以动作为驱动的自动驾驶基准。该基准旨在解决现有基准在场景多样性、可靠的动作级标注以及与人类偏好对齐的评估协议方面的不足。DriveAction 包含 16,185 个 QA 对，源自 2,610 个驾驶场景，它利用自动驾驶车辆驾驶员主动收集的真实世界数据，提供与人类驾驶偏好对齐的高级别离散动作标签，并实施一个以动作为根的树状评估框架，以系统地评估从视觉、语言到动作的完整决策过程。<br><br>### 论文核心贡献点<br>1. 驾驶员贡献的广泛覆盖驾驶场景：数据来源于自动驾驶车辆驾驶员主动收集的真实世界数据，覆盖广泛且具有代表性的日常和挑战性驾驶场景，并通过人工筛选保证质量。<br>2. 与人类驾驶偏好对齐的真实标签：动作标签直接从驾驶员的实时驾驶操作中收集，忠实地捕捉了决策时刻的人类意图。这些标签被离散化为高级别动作，与端到端大模型的输出粒度相匹配，并经过多轮人工验证。<br>3. 以动作为根的树状结构评估：引入了一个以动作为根、树状结构的评估框架。该框架根据目标动作动态确定所需的视觉和语言任务，将 V-L-A 任务系统地整合到一个可扩展的框架中，支持综合评估和任务特定评估，并能分析视觉和语言信息对最终动作决策的影响。<br><br>### 论文方法描述<br>DriveAction 的方法包含三个核心部分：<br>1. 场景构建：数据来自公司运营的自动驾驶车队，覆盖 148 个城市和多款车型，涵盖了匝道、主辅路切换、导航/效率变道、绕行弱势交通参与者、交叉口和路段等七大关键场景类别。<br>2. 标注对齐：动作标签源于真实驾驶操作，而非事后人工标注。为匹配大模型决策频率，标签被离散化为如“变道”、“减速”等高级别动作。所有数据均经过人工审核，排除了错误、不合理或违法的行为。<br>3. 评估框架：采用以动作为根的树状任务架构，顶层是动作节点，中间是语言任务（如导航遵循），底层是视觉任务（如车道线检测）。评估时提供连续视觉帧、导航指令和车速等关键场景信息。支持四种综合评估模式（V-L-A, V-A, L-A, A）和针对每个节点的任务特定评估。<br><br>### 论文使用数据集和训练资源<br>1. 数据集：使用的数据集是 DriveAction 基准，包含 16,185 个 QA 对，由 2,610 个驾驶场景生成。该数据集由驾驶员贡献的数据构建，已在 Hugging Face 上公开。<br>2. 训练资源：为了在驾驶领域进行评估，论文使用专有驾驶数据训练了两个轻量级车载模型：一个非 MOE 架构模型（0.5B 参数）和一个 MOE 架构模型（8×0.4B 参数）。论文未提及训练这些模型所使用的具体硬件资源（如 GPU 类型或数量）。<br><br>### 论文使用的评估环境和评估指标<br>1. 评估环境：<br> * 模型：评估了 12 个广泛使用的视觉语言模型（VLM），包括 GPT-4o、Claude 3.5 Sonnet、Qwen-Max-Latest 等非推理模型，以及 o1、o3、Claude 3.7 Sonnet Thinking 等推理模型。同时，也评估了上述两个专有的驾驶领域模型。<br> * 框架：所有实验使用 VLMEvalKit 工具包实现。<br> * 对比基准：与 BDD-X、Next-qa、TextVQA、RealWorldQA、Reason2Drive 等现有基准进行了对比。<br>2. 评估指标：<br> * 主要评估指标是准确率，用于衡量所有问题类型的模型性能。<br> * 在跨基准比较中，使用统一的评估指标，即每个基准所有问题的平均得分（0-100分制）。</details> |
| 2025-06-04 | SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models | http://arxiv.org/abs/2506.03574 | <details><summary>展开</summary>待生成</details> |
| 2025-06-03 | Adversarial Attacks on Robotic Vision Language Action Models | http://arxiv.org/abs/2506.03350 | <details><summary>展开</summary>待生成</details> |
| 2025-06-02 | SAB3R: Semantic-Augmented Backbone in 3D Reconstruction | http://arxiv.org/abs/2506.02112 | <details><summary>展开</summary>论文研究单位<br>University of Virginia, University of Michigan<br><br>论文概述<br>论文提出了一种名为“Map and Locate”的新任务，该任务统一了开放词汇语义分割和3D重建，旨在从未定位的视频中生成点云并根据开放词汇查询分割物体实例。为解决此任务，论文提出了一个名为SAB3R（Semantic-Augmented Backbone in 3D Reconstruction）的简单而有效的基线方法。该方法基于MASt3R，并通过一种轻量级的知识蒸馏策略，将2D视觉主干（如CLIP和DINOv2）的稠密语义特征迁移到MASt3R中，从而在不引入任何辅助冻结网络的情况下，在单次前向传播中同时生成稠密语义特征和点云地图。<br><br>论文核心贡献点<br>1. Map and Locate基准测试：引入了一个新颖的多视图3D语义分割基准测试，该任务统一了重建、重组和识别。该基准包含一个大规模数据集、明确的评估协议和标准化指标。<br>2. SAB3R模型：提出了一个统一的框架，通过高效的蒸馏策略，从未定位的图像中同时进行开放词汇分割和3D重建。该模型因其性能和计算效率被作为一个基线提出。<br><br>论文方法描述<br>SAB3R方法基于MASt3R架构，并引入了一个多任务框架来整合2D语义信息。<br>1. 基础组件：采用MASt3R的架构，其包含一个ViT-Large编码器和一个ViT-Base解码器。该模型通过编码器-解码器对处理多视图图像，并使用交叉注意力机制交换信息，最终预测点图和置信度图。<br>2. 蒸馏2D语义特征：为了在不遗忘3D重建能力的前提下整合2D语义信息，该方法在原有MASt3R模型的基础上增加了新的预测头，用于回归来自CLIP和DINOv2等2D基础模型的稠密特征。训练时，总损失函数由三部分组成：MASt3R原有的置信度损失（L_conf）和匹配损失（L_match），以及新增的2D特征回归损失（L_2D）。总损失为 `L_total = L_conf + β * L_match + γ * L_2D`，其中β和γ是超参数。<br>3. 整合附加特征：该蒸馏框架具有灵活性，可以方便地整合多种2D特征。每增加一种新特征，只需添加一个对应的预测头和回归损失项，即可扩展总损失函数。<br><br>论文使用数据集和训练资源<br>1. 数据集：使用ScanNet数据集进行训练和评估。具体来说，从ScanNet的验证集中挑选了24个多样化的室内场景，并为每个场景创建了包含2、3或4个视图的图像组，确保每组图像间有足够的重叠。语义分类采用NYU40类别体系。<br>2. 训练资源：模型基于MASt3R进行微调。教师模型为MaskCLIP（由FeatUp增强）和MASt3R。训练采用多任务学习策略，损失权重通过经验确定。具体的硬件配置和训练时长在提供的文本中未明确提及。<br><br>论文使用的评估环境和评估指标<br>1. 评估环境：模型主要在提出的“Map and Locate”基准测试上进行评估，该基准基于ScanNet数据集构建。此外，模型还在多个零样本任务上进行了评估，包括单目深度估计、相对相机位姿估计和零样本开放词汇语义分割等，这些任务分别在标准数据集（如NYUv2、ScanNet）上进行测试。<br>2. 评估指标：<br> - mIoU (mean Intersection over Union)：衡量预测点云与真实点云之间的平均交并比。<br> - Acc (Accuracy)：正确分类的3D点占所有真实点的比例。<br> - mComp (Mean Completeness)：预测点云到其最近真实点云的平均距离，衡量重建的完整性。<br> - mdComp (Median Completeness)：预测点云到其最近真实点云的中值距离，对离群值更具鲁棒性。</details> |
| 2025-06-02 | Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning | http://arxiv.org/abs/2506.01953 | <details><summary>展开</summary>论文研究单位<br>The Chinese University of Hong Kong, Peking University, AI2Robotics, Beijing Academy of Artificial Intelligence (BAAI)<br><br>论文概述<br>本文提出了Fast-in-Slow (FiS)，一个统一的双系统视觉-语言-动作(VLA)基础模型，将快速操作能力嵌入到慢速推理系统内。该模型解决了机器人操作中的泛化策略和执行效率两个关键挑战，通过将VLM的最终transformer块改造成高效的执行模块(System 1)，同时保留完整VLM的推理能力(System 2)，实现了117.7 Hz的控制频率，在模拟和真实世界任务中超越了先前最先进方法8%和11%的平均成功率。<br><br>论文核心贡献点<br>1. 提出了Fast-in-Slow (FiS)，一个统一的双系统VLA模型，将System 1执行嵌入到预训练的VLM中，同时保留其固有的System 2推理能力，实现了两个系统间的无缝协调<br>2. 鉴于System 2和System 1在FiS-VLA中扮演根本不同的角色，为两个系统设计了异构模态输入和异步操作频率，实现了快速和精确的操作<br>3. 提出了双感知共训练策略联合优化System 2和System 1，在单臂模拟和双臂真实世界实验中展示了SOTA性能，同时保持高执行频率<br><br>论文方法描述<br>FiS-VLA架构基于Prismatic VLMs，包含视觉编码器和7B LLaMA2 LLM主干。视觉编码器使用SigLIP和DINOv2联合提取视觉特征，3D点云通过轻量级3D标记器处理，然后通过共享视觉编码器提取空间特征。System 2处理2D图像和语言指令的低频输入，产生中间潜在特征指导System 1。System 1处理3D点云、2D图像和机器人状态的高频输入，进行实时动作生成。训练采用双感知共训练策略，结合快速系统的扩散去噪目标和慢速系统的自回归下一个token预测目标，整体训练目标为两者之和。<br><br>论文使用数据集和训练资源<br>预训练使用大规模跨具身数据集，包括Open X-Embodiment、DROID、ROBOMIND等，超过860K轨迹样本。然后在RLBench模拟数据和自收集的真实世界数据上进行微调。模型基于7B LLaMA2，在NVIDIA 4090 GPU上进行训练，预训练5个epoch。<br><br>论文使用的评估环境和评估指标<br>在RLBench基准的10个操作任务上进行模拟评估，包括Close box、Close Laptop等任务，使用Franka Panda机器人。真实世界实验在AgileX和AlphaBot双臂机器人上进行，验证不同机器人配置下的鲁棒性。评估指标为任务成功率，并报告推断速度。与ManipLLM、OpenVLA、π₀和CogACT等基线方法比较，FiS-VLA在平均成功率和推断速度上均取得优势。</details> |
| 2025-06-02 | SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics | http://arxiv.org/abs/2506.01844 | <details><summary>展开</summary>待生成</details> |
| 2025-06-01 | OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation | http://arxiv.org/abs/2506.01196 | <details><summary>展开</summary>待生成</details> |
| 2025-06-01 | GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering | http://arxiv.org/abs/2506.01174 | <details><summary>展开</summary>待生成</details> |
| 2025-05-31 | LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks | http://arxiv.org/abs/2506.00411 | <details><summary>展开</summary>待生成</details> |
| 2025-05-30 | Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction | http://arxiv.org/abs/2505.24156 | <details><summary>展开</summary>待生成</details> |
| 2025-05-29 | Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models | http://arxiv.org/abs/2505.23757 | <details><summary>展开</summary>待生成</details> |
| 2025-05-29 | Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better | http://arxiv.org/abs/2505.23705 | <details><summary>展开</summary>论文研究单位<br>Physical Intelligence<br><br>论文概述<br>论文研究了视觉-语言-动作模型（VLA）在实时机器人控制中的挑战。传统VLM模型因参数量大和离散token输出限制，难以满足高频连续控制需求。现有方法通过添加连续动作专家（如流匹配或扩散模块）提升控制效率，但会破坏预训练VLM的知识，导致训练速度下降和泛化能力减弱。本文提出知识隔离训练方法，通过隔离梯度流保护预训练知识，同时加速训练并提升泛化性能。<br><br>论文核心贡献点<br>提出知识隔离训练框架，通过分离VLM主干和动作专家的梯度流，在训练中同时优化离散动作预测和连续动作生成。<br>实现更快的训练收敛速度，同时保留预训练VLM的语义知识。<br>在模拟和真实机器人任务中验证了方法的泛化优势，支持高频控制（如折叠衣物等动态任务）。<br>首次实现VLA在快速训练、实时推理和强泛化能力上的统一。<br><br>论文方法描述<br>联合离散-连续动作预测：VLM主干使用离散动作token进行next-token预测训练，动作专家同时通过流匹配学习连续动作输出。<br>知识隔离机制：动作专家的梯度不反向传播至VLM主干，避免未初始化参数的干扰。<br>动作专家架构：基于轻量级网络，通过交叉注意力机制融合VLM特征，支持动作块（action chunk）生成。<br>训练策略：混合机器人数据与通用VLM数据，增强跨模态知识迁移。<br><br>论文使用数据集和训练资源<br>数据集：<br>公共基准：DROID（大规模双臂操作数据）、LIBERO（模拟家务任务）。<br>真实任务：静态机器人（抽屉整理、衣物折叠、桌面清理）和移动操作机器人（整理床铺、洗碗篮整理、移动抽屉操作、衣物搬运）。<br>训练资源：基于PaliGemma初始化VLM，使用大规模GPU集群（具体配置未详述）。<br><br>论文使用的评估环境和评估指标<br>评估环境：<br>模拟环境：基于LIBERO的虚拟场景。<br>真实环境：静态机械臂和移动操作机器人（如双臂系统）。<br>评估指标：<br>任务成功率：在给定语言指令下完成目标的比率。<br>动作精度：连续动作与目标轨迹的偏差。<br>语言理解能力：指令遵循的语义匹配度。<br>训练效率：收敛速度和资源消耗对比。</details> |
| 2025-05-29 | TrackVLA: Embodied Visual Tracking in the Wild | http://arxiv.org/abs/2505.23189 | <details><summary>展开</summary>待生成</details> |
| 2025-05-28 | Zero-Shot 3D Visual Grounding from Vision-Language Models | http://arxiv.org/abs/2505.22429 | <details><summary>展开</summary>待生成</details> |
| 2025-05-28 | ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation | http://arxiv.org/abs/2505.22159 | <details><summary>展开</summary>### 论文研究单位<br>复旦大学、上海交通大学、上海创新研究院、上海AI实验室、新加坡国立大学、上海大学、西安交通大学、Noematrix Intelligence<br><br>### 论文概述<br>论文提出ForceVLA，一种增强视觉-语言-动作（VLA）模型的端到端操作框架，通过将外部力感知作为核心模态，解决接触密集型操作任务中精细力控制的需求。该框架引入FVLMoE（力感知混合专家模块），动态融合预训练视觉-语言嵌入与实时6轴力反馈，并在五个接触密集任务中验证有效性，平均成功率提升23.2%，最高达80%。<br><br>### 论文核心贡献点<br>- 提出力感知多模态框架：首次将力作为一等公民模态集成到VLA系统，通过FVLMoE实现动态模态感知融合，增强物理交互能力。<br>- 构建数据集与工具链：发布ForceVLA-Data数据集（244条轨迹，14万时间步）及完整数据采集系统（含遥操作工具和转换器），开源以支持社区研究。<br>- 实验性能突破：在插拔、擦拭等任务中成功率显著超越基线（如π₀模型），泛化至新物体、遮挡和物理扰动场景，验证鲁棒性。<br><br>### 论文方法描述<br>- **整体架构**：基于π₀框架，扩展多模态输入（视觉、语言、本体感觉、力）。视觉和语言通过SigLIP编码为嵌入，力信号经线性投影后与VL嵌入融合。<br>- **FVLMoE模块**：采用后融合策略，力令牌在VLM处理后输入MoE。包含4个专家网络（MLP）和路由器，通过Top-1路由动态选择专家，输出通过残差连接注入动作流解码器，指导力感知动作生成。<br>- **动作生成**：采用条件流匹配模型，融合特征通过逐元素加法调制噪声动作轨迹，迭代去噪输出H步动作块。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：ForceVLA-Data，涵盖5个任务（按瓶、插拔、USB插入、擦白板、削黄瓜）。通过Flexiv Rizon 7-DOF机械臂采集，双RGB-D相机（第三人称1280×720@30fps、手腕640×480@30fps）同步视觉、本体感觉和6轴力矩数据。<br>- **训练资源**：8×NVIDIA RTX 4090 GPU（24GB VRAM），64核CPU，251GB RAM。使用Adam优化器（β₁=0.9, β₂=0.95），峰值学习率2.5×10⁻⁵，多任务训练30k步（约12小时），单任务训练10k步（约9小时），精度bfloat16。<br><br>### 论文使用的评估环境和评估指标<br>- **环境**：真实物理场景，五项接触密集任务：瓶泵按压、插头插入、USB插入、白板擦拭、黄瓜削皮。泛化测试包括物体更换（瓶/插头类型）、高度变化、视觉遮挡、插座不稳等扰动。<br>- **指标**：主要任务成功率（按任务特定标准，如完全插入/擦拭）。削黄瓜任务附加指标：平均削皮长度（cm↑）、最少完成行程数（↓）。泛化场景报告各条件下成功率（%）。</details> |
| 2025-05-28 | ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge | http://arxiv.org/abs/2505.21906 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models | http://arxiv.org/abs/2505.21567 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Hume: Introducing System-2 Thinking in Visual-Language-Action Model | http://arxiv.org/abs/2505.21432 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models | http://arxiv.org/abs/2505.21200 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Hierarchical Instruction-aware Embodied Visual Tracking | http://arxiv.org/abs/2505.20710 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | What Can RL Bring to VLA Generalization? An Empirical Study | http://arxiv.org/abs/2505.19789 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback | http://arxiv.org/abs/2505.19767 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving | http://arxiv.org/abs/2505.19381 | <details><summary>展开</summary>论文研究单位<br>RIX, Bosch; AIR, Tsinghua University; Shanghai University; Shanghai Jiao Tong University; Southeast University<br><br>论文概述<br>该论文提出了一个名为DiffVLA的端到端自动驾驶框架，旨在解决现有方法中存在的BEV计算昂贵、动作多样性不足以及在复杂场景下决策次优等问题。DiffVLA结合了视觉语言模型（VLM）的引导、混合稀疏-密集的感知模块以及一个基于扩散的规划模块。该方法通过探索稀疏扩散表示来高效处理多模态驾驶行为，并利用VLM、智能体和地图实例之间的深度交互来优化轨迹生成。论文在Autonomous Grand Challenge 2025的NAVSIM v2竞赛中取得了优异的性能，其方法在私有测试集上达到了45.0的PDMS分数。<br><br>论文核心贡献点<br>1. 提出了一个VLM引导模块，该模块处理多视图图像和导航指令，生成高级驾驶命令和初始轨迹，为后续规划提供语义指导。<br>2. 设计了一个混合感知模块，包含一个用于生成密集BEV特征表示的密集分支和一个用于提取3D目标、车道线等实例级信息的稀疏分支，从而同时利用场景的隐式和显式特征。<br>3. 开发了一个基于扩散的规划模块，该模块使用多模态轨迹锚点作为先验，并采用缩短的扩散策略，通过分层信息编码策略融合来自感知模块和VLM的异构输入，以生成多模态的未来轨迹。<br><br>论文方法描述<br>DiffVLA框架主要由三个部分组成：<br>1. **感知模块**：包含两个并行分支。稀疏感知分支采用采样策略进行3D目标检测和在线地图生成，输出3D边界框和地图矢量。密集感知分支则采用BEV特征投影方法生成密集的BEV特征图。两个分支的输出通过MLP编码后集成到规划模块中。<br>2. **VLM模块**：基于Senna-VLM框架，该模块包括一个ViT-L/14视觉编码器、一个驾驶视觉适配器、一个文本编码器和一个Vicuna-v1.5-7B大语言模型。它处理多视图图像和外部导航指令，生成高层次的横向（如变道）和纵向（如加速）控制指令，这些指令被编码后用于引导扩散规划过程。<br>3. **规划模块**：采用一个基于扩散的策略。首先，通过k-means聚类从训练数据中构建一组离散的轨迹词汇表，并为其添加高斯噪声生成轨迹锚点。然后，利用来自感知模块的显式（稀疏）和隐式（密集）特征作为条件，一个神经网络`f_θ`逐步对带噪声的轨迹进行去噪，最终预测出多条候选轨迹及其对应的分类得分。该模块使用轨迹分类和回归的联合损失进行训练。<br><br>论文使用数据集和训练资源<br>* **数据集**：在NAVSIM v2数据集上进行训练和评估。该数据集通过引入反应性的背景交通参与者和真实的合成多视角相机图像，为模型的鲁棒性和泛化能力提供了闭环评估。<br>* **训练资源**：模型采用两阶段训练策略。第一阶段分别训练VLM引导模型和稀疏感知模块。第二阶段联合训练密集感知模块和规划头，同时冻结第一阶段模型的权重。训练使用AdamW优化器和余弦学习率衰减策略。VLM模块训练1个epoch，批量大小为192，学习率为2e-5。感知和规划模块训练100个epoch，批量大小为256，学习率为1e-4。所有感知分支均使用VoV-99作为骨干网络。<br><br>论文使用的评估环境和评估指标<br>* **评估环境**：在Autonomous Grand Challenge 2025竞赛的私有测试集上进行评估，该竞赛使用NAVSIM v2作为基准，提供了一个包含挑战性真实和反应性合成场景的闭环评估环境。<br>* **评估指标**：主要评估指标是`extended_pdm_score_combined` (EPDMS)，论文方法在该指标上取得了45.007的成绩。此外，评估还包括一系列分阶段的子指标，主要分为两个阶段（stage_one和stage_two），涵盖安全性、合规性、效率和舒适性等方面。具体指标包括：<br> * **安全与合规性**：`no_at_fault_collisions`（无过错碰撞率）、`drivable_area_compliance`（可行驶区域合规率）、`driving_direction_compliance`（行驶方向合规率）、`traffic_light_compliance`（红绿灯合规率）、`time_to_collision_within_bound`（碰撞时间在合理范围内比率）、`lane_keeping`（车道保持率）。<br> * **效率**：`ego_progress`（自车进度）。<br> * **舒适性**：`history_comfort`（历史舒适性）、`two_frame_extended_comfort`（两帧扩展舒适性）。</details> |
| 2025-05-25 | ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning | http://arxiv.org/abs/2505.19080 | <details><summary>展开</summary>待生成</details> |
| 2025-05-24 | Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance | http://arxiv.org/abs/2505.18793 | <details><summary>展开</summary>待生成</details> |
| 2025-05-24 | VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning | http://arxiv.org/abs/2505.18719 | <details><summary>展开</summary>论文研究单位<br>- 清华大学深圳国际研究生院<br>- 南洋理工大学<br><br>论文概述<br>- 论文提出了 VLA-RL，一个利用在线强化学习（RL）来改进预训练自回归视觉-语言-动作（VLA）模型的算法和系统框架。<br>- 传统的VLA模型依赖模仿学习，在处理分布外（OOD）场景时会失败，因为其无法探索有限的离线数据中未覆盖的状态。<br>- VLA-RL旨在通过在线探索和策略优化来解决这一局限性，从而提升模型在下游任务中的泛化能力和性能。<br>- 实验表明，该方法能使OpenVLA-7B模型在LIBERO的40项挑战性任务上，性能超越最强基线4.5%，并能匹配先进商业模型（如π0-FAST）的性能。<br><br>论文核心贡献点<br>- 提出了VLA-RL，一个系统性的框架，首次将在线强化学习有效地应用于大规模预训练VLA模型的下游任务优化。<br>- 将通用机器人操作轨迹形式化为多模态、多轮对话，从而可以在统一的视角下对自回归VLA进行轨迹级的RL训练。<br>- 为了解决机器人动作空间中奖励稀疏的问题，引入了机器人过程奖励模型（RPRM）。该模型基于自动提取的任务片段上的伪奖励标签进行微调，用于提供密集的奖励信号。<br>- 识别并验证了多项关键的系统性实现技术，包括课程选择策略、GPU均衡的向量化环境、批量解码和评论家预热，这些技术显著提升了训练的稳定性和效率。<br><br>论文方法描述<br>- **整体流程**：系统采用Actor-Critic框架，包含一个待训练的策略模型、一个同构的价值模型和一个冻结的机器人过程奖励模型（RPRM）。<br>- **RL问题形式化**：将机器人操作任务定义为马尔可夫决策过程，其中状态是图像观测和文本指令的组合，动作是VLA模型生成的动作词元序列。这使得可以用标准RL算法进行优化。<br>- **奖励密集化**：使用机器人过程奖励模型（RPRM）为环境提供的稀疏奖励补充密集的奖励信号。总奖励为两者之和。<br>- **训练算法**：采用类似PPO的算法进行优化。首先在多个并行环境中进行数据收集（rollout），然后利用广义优势估计（GAE）计算优势函数，最后使用收集的数据和优势函数通过PPO更新策略和价值模型的参数。<br>- **系统性实现**：通过课程学习选择任务、利用GPU均衡的向量化环境、批量解码以及对评论家模型进行预热等工程技巧，确保了训练过程的可扩展性和稳定性。<br><br>论文使用数据集和训练资源<br>- **数据集**：实验在LIBERO数据集上进行，该数据集包含40项具有挑战性的机器人操作任务。<br>- **训练资源**：训练利用了多个GPU，并采用GPU均衡的向量化环境进行并行化，但原文未提供具体的GPU数量和训练时长等资源细节。<br><br>论文使用的评估环境和评估指标<br>- **评估环境**：评估在LIBERO模拟器环境中进行。<br>- **评估指标**：主要评估指标是任务的成功率。论文通过比较不同方法在LIBERO任务集上的平均成功率来衡量性能，并与最强的监督微调基线以及先进的商业模型进行对比。</details> |
| 2025-05-22 | ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems | http://arxiv.org/abs/2505.17295 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | Interactive Post-Training for Vision-Language-Action Models | http://arxiv.org/abs/2505.17016 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving | http://arxiv.org/abs/2505.16278 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning | http://arxiv.org/abs/2505.15725 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems | http://arxiv.org/abs/2505.15685 | <details><summary>展开</summary>### 论文研究单位<br>- IHPC, Agency for Science, Technology and Research, Singapore<br>- Nanyang Technological University, Singapore<br><br>### 论文概述<br>论文研究了三种基础模型（Foundation Models）在具身机器人系统中的集成策略：端到端视觉-语言-动作模型（VLAs）、模块化视觉-语言模型（VLM）管道，以及多模态大语言模型（MLLM）代理作为编排者。通过两个案例研究——指令基础（instruction grounding）和机器人操作（robotic manipulation）——评估了这些策略在复杂指令跟随和通用动作生成中的权衡。实验揭示了模型规模、泛化能力和数据效率之间的平衡点，为语言驱动的物理代理提供了设计见解。<br><br>### 论文核心贡献点<br>1. **范式分析**：系统分析了三种FM集成范式在共享具身任务上的能力与权衡。<br>2. **资源发布**：发布了一个数据集和代码，用于评估指令基础和对象操作，涵盖跨模态推理和不同布局下的技能适应。<br>3. **实践见解**：提供了对最先进VLAs和MLLMs的及时洞察，调查了它们的能力和失败模式，为实践者选择FM堆栈提供了实用权衡指导。<br>4. **系统演示**：发布了一个完整的端到端抓娃娃机器人系统作为真实世界FM集成演示。<br><br>### 论文方法描述<br>1. **端到端VLA模型**：<br> - **自回归模型**：逐步生成动作，基于当前感知输入和历史输出预测低级控制令牌（如关节角度）。<br> - **扩散模型**：通过去噪轨迹生成动作，建模未来动作的分布而非逐步生成。<br> - **优势与局限**：利用大规模预训练实现任务泛化，但受限于数据稀缺性和对新任务/环境的脆弱性。<br>2. **模块化VLM管道**：<br> - 专业化VLM处理感知，输出符号化场景信息（如边界框、分割掩码），下游规划器生成动作。<br> - 优势：可解释性和高效率（模型参数仅100M-600M）。<br> - 局限：交互刚性，感知错误传播无缓解。<br>3. **多模态LLM代理作为编排者**：<br> - MLLM作为认知枢纽，通过函数调用调用视觉工具，推理后输出高级动作基元。<br> - 优势：视觉常识推理和指令跟随能力强。<br> - 局限：资源密集型，部署挑战大。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - **杂乱桌面操作数据集**：163个演示片段，目标是在杂乱环境中拾取螺丝刀，使用UR5机械臂和RealSense相机收集。<br> - **复杂指令基础数据集**：473条指令，包含隐式、属性显式和空间关系指令，用于跨模态消歧。<br>- **训练资源**：<br> - 部分微调：单块NVIDIA A6000 GPU（48GB VRAM），持续3天。<br> - 全量微调：8×H100 GPU节点。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **真实环境**：WidowX机器人平台（真实世界任务）。<br> - **仿真环境**：LIBERO基准测试（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。<br>- **评估指标**：<br> - **成功率（%）**：操作任务中的任务完成率，每个任务500次试验。<br> - **宏平均准确率**：指令基础任务中物体定位和复杂指令理解的准确率。</details> |
| 2025-05-21 | Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization | http://arxiv.org/abs/2505.15660 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | FLARE: Robot Learning with Implicit World Modeling | http://arxiv.org/abs/2505.15659 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control | http://arxiv.org/abs/2505.15304 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy | http://arxiv.org/abs/2505.15206 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation | http://arxiv.org/abs/2505.15098 | <details><summary>展开</summary>待生成</details> |
| 2025-05-20 | AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory | http://arxiv.org/abs/2505.14030 | <details><summary>展开</summary>### 论文研究单位<br>香港大学（HKU）、TeleAI、清华大学（THU）、上海交通大学（SJTU）、香港大学上海智能计算中心<br><br>### 论文概述<br>AutoBio 是一个针对数字生物实验室的机器人自动化模拟框架和基准测试。现有视觉-语言-动作（VLA）模型的基准测试主要聚焦于家庭任务，缺乏对专业科学领域的评估。生物实验室实验具有结构化协议、高精度要求和多模态交互的特点，但现有模拟器在处理流体、数字界面和实验室特有物理机制方面存在局限。AutoBio 通过仪器数字化、物理插件和渲染技术，构建了一个高保真度的生物实验模拟环境，并设计了一套涵盖基础生物学操作的基准任务，用于评估VLA模型在科学工作流中的表现。<br><br>### 论文核心贡献点<br>1. **生物实验室专用模拟器**：开发了仪器数字化流程、物理插件（螺纹/棘轮机制、准静态液体）和基于物理的渲染（PBR）支持透明材料与动态界面。<br>2. **生物学基础任务基准**：提供16项任务，分三个难度级别，支持轨迹合成和VLA模型集成，用于标准化评估实验协议中的机器人操作。<br>3. **科学导向环境的VLA评估**：系统测试了两个SOTA VLA模型（π₀和RDT），揭示了当前模型在精度操作、指令遵循和视觉推理方面的显著差距。<br><br>### 论文方法描述<br>**模拟器架构**：<br>- **AutoBio Assets**：通过3D高斯泼溅（3DGS）结合CAD细化流程，将真实实验室仪器（如离心机、热循环仪）转换为可交互的数字资产。<br>- **AutoBio Physics**：基于MuJoCo引擎，开发定制物理插件，包括螺纹机制（使用圆形螺旋线的SDF模拟）、棘轮机制（离散位置反馈）、偏心机制（振荡运动模拟）和准静态液体（平面界面简化）。<br>- **AutoBio Rendering**：支持两种后端——基础渲染（快速OpenGL渲染）和高级渲染（Blender PBR实现透明材料与动态纹理渲染），后者确保容器和液体的视觉保真度。<br>- **反应式界面**：动态纹理渲染实现仪器控制面板的实时交互反馈。<br><br>**基准设计**：<br>- **任务分级**：简单级（如关闭热循环仪盖）、中等难度（如旋开离心管盖）、困难级（如操作混匀仪面板），任务涵盖场景初始化、示范合成、状态检查和VLA接口。<br>- **机器人配置**：支持单臂（Aloha）和双臂（UR5e搭配Robotiq夹爪或DexHand），提供多相机视角（全局和手腕相机）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：从9个任务中生成100条示范轨迹（50Hz），总计792k帧（约4.4小时连续数据），格式为LeRobot数据集，发布于HuggingFace。<br>- **训练资源**：在NVIDIA H800 GPU上训练，批次大小32，30,000步，单次运行10-14小时，总计约1,500 GPU小时。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：AutoBio模拟器，包含16个任务，分三个难度级别（每级3个测试任务），环境支持随机化参数（如目标位置）。<br>- **评估指标**：<br> - **任务成功率**：二进制评分（成功=1，失败=0），适用于大部分任务。<br> - **相对进度分数**：用于"操作混匀仪面板"任务，反映部分完成度。<br> - **指标计算**：100次测试 episodes的平均成功率（%），报告均值及标准误（如99.7±0.3）。</details> |
| 2025-05-20 | InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning | http://arxiv.org/abs/2505.13888 | <details><summary>展开</summary>待生成</details> |
| 2025-05-19 | SPKLIP: Aligning Spike Video Streams with Natural Language | http://arxiv.org/abs/2505.12656 | <details><summary>展开</summary># 论文研究单位<br>北京大学 National Key Laboratory for Multimedia Information Processing, School of Computer Science; 中国科学院大学人工智能学院; 电子科技大学英才学院; 北京大学人工智能研究院<br><br># 论文概述<br>SPKLIP是首个专为脉冲视频流与自然语言对齐设计的端到端框架。它通过多模态对比学习，直接从脉冲事件流中实现高速动态场景的语义理解，解决了传统CLIP等模型在处理稀疏、异步的脉冲数据时性能下降的问题。<br><br># 论文核心贡献点<br>- 提出了一种新型的脉冲-视频-语言对齐架构，包含专为处理稀疏、异步事件流设计的分层脉冲特征提取器，并通过脉冲-文本对比学习直接对齐原始脉冲视频与文本。<br>- 开发了节能的全脉冲设计，将SNN组件集成到流程中，展示了其在神经形态硬件上的能效潜力，并在一个新的真实世界脉冲视频数据集上验证了其泛化能力。<br>- 通过全面的实验，证明SPKLIP在脉冲-视频-语言对齐任务上显著优于传统视觉语言模型。<br><br># 论文方法描述<br>- **分层脉冲特征提取器**：包含多尺度时序滤波和空间注意模块。多尺度时序滤波通过并行分支处理不同时间尺度的特征，使用可学习的时序掩码动态加权；空间注意模块通过注意力机制增强关键时间步并抑制噪声。<br>- **时空注意力残差网络**：使用MAPResNet进行分层特征提取，结合残差块和多头自注意力池化，然后通过Transformer编码器建模长期时间依赖关系，最后通过全局特征池化得到紧凑表示。<br>- **脉冲-文本对比学习**：使用对比损失函数对齐脉冲视频特征和文本特征。<br><br># 论文使用数据集和训练资源<br>- **数据集**：基准脉冲数据集和一个新贡献的真实世界脉冲视频数据集。<br>- **训练资源**：在论文提供的HTML原文中未明确提及具体的硬件配置和训练时间等资源细节。<br><br># 论文使用的评估环境和评估指标<br>- **评估环境**：在提供的HTML原文中未详细说明具体的硬件和软件环境。<br>- **评估指标**：论文使用了对比学习常用的指标，如零样本分类准确率，并在文本到视频检索等下游任务上评估模型性能。具体指标名称在原文中未明确列出。</details> |
| 2025-05-18 | RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction | http://arxiv.org/abs/2505.12224 | <details><summary>展开</summary>待生成</details> |
| 2025-05-16 | Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions | http://arxiv.org/abs/2505.11214 | <details><summary>展开</summary>待生成</details> |
| 2025-05-16 | Conditioning Matters: Training Diffusion Policies is Faster Than You Think | http://arxiv.org/abs/2505.11123 | <details><summary>展开</summary>待生成</details> |
| 2025-05-14 | Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware | http://arxiv.org/abs/2505.09601 | <details><summary>展开</summary>待生成</details> |
| 2025-05-14 | VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation | http://arxiv.org/abs/2505.09577 | <details><summary>展开</summary>论文研究单位<br>- 中国科学院自动化研究所多模态人工智能系统重点实验室<br>- 三星北京研究院<br>- 北京智源人工智能研究院<br><br>论文概述<br>- 本文提出了VTLA（Vision-Tactile-Language-Action）模型，这是一个用于接触密集型操作任务（如插入操作）的多模态框架。该模型通过跨模态语言基础有效整合视觉和触觉输入，以生成稳健的策略。研究者构建了一个低成本的模拟多模态数据集，包含专门针对指尖插入任务的视觉-触觉-动作-指令对。此外，引入了直接偏好优化（DPO）来提供类似回归的监督，以弥合基于分类的下一个令牌预测损失与连续机器人任务之间的差距。实验结果表明，VTLA模型在未见过的销形状上实现了超过90%的成功率，并在真实世界的销插入实验中展示了出色的Sim2Real性能。<br><br>论文核心贡献点<br>- 提出了VTLA模型，这是一个集成视觉、触觉和语言输入的机器人操作策略生成框架。<br>- 设计了视觉引导的时间增强令牌（VGTE），通过强调视觉先验并在标记化前融入时间融合，以解决VLM在视觉-触觉操作中的时间推理限制。<br>- 引入了偏好学习（使用DPO）到VTLA中，以减轻对真实动作的过拟合，并通过模拟回归监督增强泛化能力。<br>- 在真实世界插入实验中证明VTLA优于现有方法，展示了所设计模块的有效性。<br><br>论文方法描述<br>- VTLA模型基于视觉-语言模型（VLM）进行构建，通过两阶段训练流程进行优化。在第一阶段，使用模拟数据收集的视觉-触觉-动作-指令对进行监督微调（SFT），采用下一个令牌预测（NTP）损失。在第二阶段，应用直接偏好优化（DPO），通过比较模型生成的动作与真实动作的接近程度，创建偏好数据集，并优化模型以偏好更接近真实动作的响应。为了解决VLM在时间理解上的局限性，提出了视觉引导的时间增强令牌（VGTE），该方法在标记化前强调视觉先验并增强时间融合，从而提升跨模态时间推理能力。<br><br>论文使用数据集和训练资源<br>- 数据集：在NVIDIA Isaac Gym中构建的模拟多模态数据集，包含28,000个销插入样本，覆盖5种不同的销孔形状和0.6至2.0毫米的装配间隙。数据集包含视觉图像、左右触觉图像序列（2x2网格格式）、动作标签和文本指令。使用领域随机化技术增强Sim2Real泛化能力。<br>- 训练资源：使用LlamaFactory框架进行训练。在SFT阶段，基础模型为Qwen2-VL 7B，学习率为5e-4，批量大小为64，训练10个周期。在DPO阶段，学习率为5e-6，批量大小为32，训练3个周期。视觉编码器和模态适配器参数被冻结，仅微调语言模型。<br><br>论文使用的评估环境和评估指标<br>- 评估环境：在模拟环境（Isaac Gym）中测试不同间隙和形状的销插入任务；真实世界实验使用配备Robotiq 2F-85夹爪的UR3机器人，手腕安装Intel RealSense D405相机捕获视觉，指尖使用GelStereo 2.0传感器获取触觉数据。<br>- 评估指标：数据集评估使用目标收敛率（GCR，所有方向动作正确的百分比）和L1距离（x、y、rz方向）。任务评估使用成功率（插入成功的百分比）和平均尝试步数（完成插入的平均步数）。真实世界评估也记录成功率和步数。</details> |
| 2025-05-13 | From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation | http://arxiv.org/abs/2505.08548 | <details><summary>展开</summary>待生成</details> |
| 2025-05-13 | Training Strategies for Efficient Embodied Reasoning | http://arxiv.org/abs/2505.08243 | <details><summary>展开</summary>```markdown<br># 论文研究单位<br>UC Berkeley, Stanford University, Physical Intelligence<br><br># 论文概述<br>本文研究了机器人链式思维（ECoT）推理为何能提升视觉-语言-动作模型（VLA）的性能，并提出轻量级训练策略ECoT-Lite。传统ECoT虽能提升泛化能力，但存在推理数据标注成本高和推理速度慢（仅1-1.2Hz）的问题。通过实证分析，论文揭示了ECoT有效的核心机制，并设计出无需生成中间推理即可获得高性能且实现3倍推理加速的方案。<br><br># 论文核心贡献点<br>1. 提出"推理改进表示学习"的核心机制，通过预训练或推理Dropout策略提升VLA内部表示质量。<br>2. 开发四种轻量级训练方法：推理预训练/联合训练、推理Dropout、推理脚手架和思考令牌。<br>3. 在LIBERO-90基准测试中达到SOTA（90.8%成功率），在BridgeData V2上超越标准VLA 10-19%，同时实现3倍推理加速。<br>4. 揭示模型表达能力（Hyp.3）并非主要瓶颈，语义推理才是关键。<br><br># 论文方法描述<br>1. **推理预训练/联合训练**：先在推理数据上预训练VLM，再微调动作预测（Hyp.1）。<br>2. **推理Dropout**：训练中随机丢弃推理步骤，迫使模型直接预测动作（Hyp.1）。<br>3. **推理脚手架**：训练时提供推理示例作为上下文，但不计算损失（Hyp.2）。<br>4. **思考令牌**：添加无语义令牌增加计算量（Hyp.3），实验证明无效。<br><br># 论文使用数据集和训练资源<br>- **数据集**：LIBERO-90（90项模拟操作任务）、BridgeData V2（真实机器人操作任务）。<br>- **推理数据生成**：通过基础模型（如GPT-4V）标注轨迹的中间步骤（子任务分解、物体定位等）。<br>- **训练资源**：NVIDIA H100 GPU，使用TensorRT-LLM FP8编译优化，训练步数与基线模型相当。<br><br># 论文使用的评估环境和评估指标<br>- **仿真环境**：LIBERO-90标准测试及两个泛化变体（物体位置扰动+干扰物、空间关系泛化）。<br>- **真实环境**：BridgeData V2的泛化评估，包含运动泛化、空间关系、未见物体等维度。<br>- **评估指标**：任务成功率（50次试验/任务的平均值及标准误差），控制频率作为推理速度指标。<br>- **关键结果**：ECoT-Lite在LIBERO-90达90.8%成功，比标准VLA高8.8%；推理速度达3.5Hz+（标准ECoT为1-1.2Hz）。<br>```</details> |
| 2025-05-12 | ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning | http://arxiv.org/abs/2505.07395 | <details><summary>展开</summary>待生成</details> |
| 2025-05-09 | UniVLA: Learning to Act Anywhere with Task-centric Latent Actions | http://arxiv.org/abs/2505.06111 | <details><summary>展开</summary>待生成</details> |
| 2025-05-09 | 3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks | http://arxiv.org/abs/2505.05800 | <details><summary>展开</summary>待生成</details> |
| 2025-05-08 | Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments | http://arxiv.org/abs/2505.05540 | <details><summary>展开</summary>待生成</details> |
| 2025-05-07 | Vision-Language-Action Models: Concepts, Progress, Applications and Challenges | http://arxiv.org/abs/2505.04769 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation | http://arxiv.org/abs/2505.03912 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration | http://arxiv.org/abs/2505.03673 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | Task Reconstruction and Extrapolation for $π_0$ using Text Latent | http://arxiv.org/abs/2505.03500 | <details><summary>展开</summary>### 论文研究单位<br>Independent<br><br>### 论文概述<br>该论文研究了视觉-语言-动作模型在任务外推方面的局限性，发现这些模型虽然在演示过的任务上表现良好，但在需要组合不同任务中的技能以解决新任务时常常失败。作者提出了一种名为“text latent”的方法，这是一种从模型内部隐藏状态中提取的任务特定向量。通过将“text latent”注入模型的残差流，可以重构相应的任务行为。进一步地，通过线性插值混合不同任务的“text latent”（Text Latent Interpolation, TLI），可以组合子技能以生成新颖的行为，从而实现任务外推。研究揭示了π₀模型内部编码了独立且可组合的技能表示，但其本身无法自主地组合这些表示。此外，论文还提出并验证了一个新的基准测试集libero-ood，并发现当前最先进的VLAs普遍存在“空间过拟合”问题，即将对象名称与其在演示场景中的空间位置相关联，而非真正理解对象。<br><br>### 论文核心贡献点<br>1. **引入“text latent”**：提出并验证了一种从模型文本标记的隐藏状态中提取的任务特定向量，它编码了完成任务所需的核心语义知识。<br>2. **任务重构**：证明了通过将“text latent”注入模型的残差流，可以在没有原始任务提示的情况下重构任务行为，成功率超过80%。<br>3. **提出Text Latent Interpolation (TLI)**：开发了一种通过在时间步上线性插值两个“text latent”来组合不同任务技能的方法，使得模型能够完成需要技能拼接的泛化任务。<br>4. **提出libero-ood基准**：创建了一个包含20个外推任务的新基准测试集，用于评估VLAs的组合泛化能力，这些任务需要模型拼接已学习但未曾组合过的子轨迹。<br>5. **显著提升外推性能**：通过应用TLI方法，将π₀模型在libero-ood基准上的成功率从9%提升至83%，证明了其内在的可组合潜力。<br>6. **发现VLAs的共性缺陷**：通过在多个SOTA VLAs上测试，发现它们普遍存在“空间过拟合”问题，即模型将对象名称与训练数据中的特定空间位置绑定，缺乏真正的对象和目标理解能力。<br><br>### 论文方法描述<br>1. **Preliminary**：首先阐述了基于Transformer的VLAs的工作原理，即视觉、语言和本体感觉信息被编码为嵌入，并依次通过L层Transformer，最终由这些嵌入和隐藏状态生成动作。<br>2. **Text Latent 提取**：对于一个给定的任务，通过运行模型在多个演示轨迹上，收集所有时间步和所有Transformer层中对应文本标记的隐藏状态。然后对这些状态进行元素平均，得到一个形状为 `(L-1) x \|T\| x d` 的张量，即该任务的“text latent”。<br>3. **Text Latent Interpolation (TLI)**：为了解决需要组合两个基础任务技能的新任务，该方法在每个时间步 `i` 计算一个插值系数 `α = i / λ`（其中 `λ` 是预设的过渡步数）。然后，将两个基础任务的“text latent” (`𝒯¹`, `𝒯²`) 按该系数进行加权，并将结果注入到当前文本标记的隐藏状态中。这使得模型在任务开始时执行任务1的行为，随着时间推进，行为逐渐平滑过渡到任务2。<br><br>### 论文使用数据集和训练资源<br>1. **数据集**：<br> * **标准基准**：LIBERO仿真环境中的三个任务套件，包括 `libero-goal`, `libero-object`, `libero-spatial`。<br> * **新基准**：作者提出的 `libero-ood`，包含 `libero-goal-ood` 和 `libero-spatial-ood` 两个子集，共20个任务。这些任务的设计理念是，完成任务所需的抓取和放置动作分别在训练任务中出现过，但它们的组合是全新的。<br>2. **训练/计算资源**：<br> * “text latent” 的提取基于每个任务的20个演示轨迹。<br> * 所有实验均在 **Nvidia RTX 4090** GPU上完成。<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**：所有实验均在 **LIBERO仿真环境**中进行。<br>2. **评估指标**：<br> * 主要指标为 **任务成功率**。对于每个任务，执行10次独立的评估（使用不同随机种子）。每个任务套体的最终成功率是总100次尝试中成功次数的比例。<br> * 辅以 **定性分析**，通过可视化模型的行为轨迹来分析其决策过程，特别是“空间过拟合”现象。</details> |
| 2025-05-06 | GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data | http://arxiv.org/abs/2505.03233 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets | http://arxiv.org/abs/2505.03174 | <details><summary>展开</summary>待生成</details> |
| 2025-05-04 | Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions | http://arxiv.org/abs/2505.02152 | <details><summary>展开</summary>待生成</details> |
| 2025-04-28 | NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks | http://arxiv.org/abs/2504.19854 | <details><summary>展开</summary>待生成</details> |
| 2025-04-22 | $π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization | http://arxiv.org/abs/2504.16054 | <details><summary>展开</summary>待生成</details> |
| 2025-04-01 | Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning | http://arxiv.org/abs/2504.00907 | <details><summary>展开</summary>待生成</details> |
| 2025-03-30 | OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model | http://arxiv.org/abs/2503.23463 | <details><summary>展开</summary>待生成</details> |
| 2025-03-27 | CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models | http://arxiv.org/abs/2503.22020 | <details><summary>展开</summary>待生成</details> |
| 2025-03-26 | MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation | http://arxiv.org/abs/2503.20384 | <details><summary>展开</summary>### 论文研究单位<br>南京大学、香港理工大学、北京大学计算机学院多媒体信息处理国家重点实验室、北京智源人工智能研究院、香港科技大学。<br><br>### 论文概述<br>该论文提出了一种名为MoLe-VLA的新型视觉-语言-动作模型，旨在通过动态跳过大型语言模型中的部分层来提高机器人操作的效率。该方法受到神经科学中“浅层脑假说”和混合专家模型的启发，将每个LLM层视为一个专家，并设计了一个时空感知路由器来根据机器人的当前状态选择性地激活相关层，以模仿大脑在认知和因果推理中不同的信号通路。此外，为了弥补因层跳过而损失的LLM认知能力，论文还提出了一种认知自知识蒸馏方法来增强模型对任务需求的理解并生成相关的动作序列。<br><br>### 论文核心贡献点<br>1. 受浅层脑假说启发，开发了MoLe框架，模仿人脑信号流，通过路由器实现动态层激活以提高模型效率。<br>2. 提出了一种新型层决策路由器STAR，充分利用机器人输入的时空信息做出更准确的激活决策。<br>3. 引入了一种自知识蒸馏范式CogKD，以恢复稀疏LLM中因层跳过而丢失的认知信息，提升整体性能。<br><br>### 论文方法描述<br>论文方法主要包括三个核心部分：<br>1. **混合层架构**：将LLM的每一层视为一个独立的专家，设计路由器动态选择执行哪些层。输入嵌入`x_k`经过路由器生成二进制门控向量`G_mol(x)`，仅top-k个值为1的层`π_k`被执行，其余层被跳过，输出`h_k`根据公式`h_k = G_k * π_k(h_{k-1}) + (1 - G_k) * h_{k-1}`计算。<br>2. **时空感知路由器（STAR）**：为克服传统路由器无法捕捉动态具身智能任务中关键时空信息的限制，STAR独立处理来自视觉输入的空间特征和来自文本输入的时间依赖，将其组合成统一表示，以对齐LLM层选择与当前环境需求。<br>3. **认知自知识蒸馏（CogKD）**：为了补偿层跳过导致的认知表达能力下降，使用原始全层模型作为教师，MoLe层跳过模型作为学生。引入可学习的“认知token”来整合视觉token和语言指导，通过分析认知token与学生token的相似性来识别关键信息，并自适应地重新加权蒸馏过程。<br><br>### 论文使用数据集和训练资源<br>数据集：RLBench仿真环境数据集和真实世界任务数据集。<br>训练资源：论文未明确提及具体硬件，但训练过程在多个VLA模型上进行了端到端的实现，使用了扩散动作头，通过最小化预测噪声与真实噪声之间的均方误差进行优化。优化目标包括任务损失`L_task`和可能的负载平衡损失。<br><br>### 论文使用的评估环境和评估指标<br>评估环境：RLBench模拟环境和真实世界环境（使用Franka机械臂设置）。<br>评估指标：主要指标是任务成功率，计算为在十个任务上的平均成功率。此外，还评估了计算效率，包括推理延迟（或频率）和计算成本（如FLOPs或延迟倍数）。在真实世界中，评估了模型在一系列操作任务上的定性表现。</details> |
| 2025-03-25 | Gemini Robotics: Bringing AI into the Physical World | http://arxiv.org/abs/2503.20020 | <details><summary>展开</summary>待生成</details> |
| 2025-03-25 | Boosting Robotic Manipulation Generalization with Minimal Costly Data | http://arxiv.org/abs/2503.19516 | <details><summary>展开</summary>待生成</details> |
| 2025-03-20 | IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes | http://arxiv.org/abs/2503.17406 | <details><summary>展开</summary>论文研究单位<br>卡内基梅隆大学机器人研究所<br><br>论文概述<br>论文提出了一种名为IRef-VLA的新基准数据集，用于解决在3D场景中，当语言指令不完美或存在歧义时的交互式参考定位问题。该任务旨在评估和促进开发能够理解自然语言、将其与物理世界关联、并能处理语言不完美性的智能体，特别是在室内导航等机器人应用中。论文通过创建大规模、多样化的数据集并定义一个扩展任务来推动这一领域的发展。<br><br>论文核心贡献点<br>1. 提出了IRef-VLA，一个大规模的3D场景参考定位基准数据集，是当前最大的真实世界数据集，包含超过11.5K个扫描的3D房间、760万条启发式生成的语义关系和470万条参考陈述。<br>2. 数据集包含了丰富的标注信息，如语义对象和房间标注、密集的场景图、可自由导航的空间标注。<br>3. 引入了“带有不完美参考的参考定位”这一新任务，要求模型不仅能定位对象，还要能检测引用对象是否存在，并在不存在时生成有效的替代建议。<br>4. 数据集中特别增加了包含不完美或歧义语言的陈述，以评估模型的鲁棒性。<br><br>论文方法描述<br>1. 数据处理：整合了ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes和Unity生成的多个数据集，并进行标准化处理，包括提取场景点云、对象语义类别、边界框和颜色信息，同时生成可自由导航的空间区域。<br>2. 场景图生成：基于对象的边界框，通过启发式方法计算八种类型的空间语义关系（如上方、附近、之间），为每个区域生成密集的场景图。<br>3. 语言生成：采用基于模板的方法生成参考陈述。生成的语句遵循三个原则：视图无关、无歧义、最简（遵循格莱斯准则）。通过使用关系谓词的同义词来增加语言的多样性。<br>4. 不完美语言生成：通过修改现有真实陈述中的目标对象、锚点对象或关系等属性，生成描述不存在对象的不完美参考陈述，以模拟真实世界中语言与场景不匹配的情况。<br><br>论文使用数据集和训练资源<br>数据集：<br>- 来源：ScanNet, Matterport3D, HM3D, 3RScan, ARKitScenes, Unity scenes。<br>- 规模：7,635个场景，超过11.5K个区域，286K个对象（跨477个类别），760万个空间关系，470万条参考陈述。<br>训练资源：<br>- 训练模型：MVT (Multi-View Transformer) 和 3D-VisTA。<br>- 训练划分：对ScanNet数据使用官方ScanNet/ReferIt3D的训练/验证划分，其余场景按80%/20%划分训练集和验证集。<br>- 训练策略：分别训练了在ScanNet子集上和完整IRef-VLA数据集上的模型，以评估数据规模对泛化能力的影响。模型训练至损失收敛。<br><br>论文使用的评估环境和评估指标<br>评估环境：<br>- 在IRef-VLA数据集自身的验证集上进行评估。<br>- 为了测试泛化能力，将在IRef-VLA上训练的模型进行零样本迁移，在ReferIt3D的测试集（包括合成语句的Sr3D和人类语句的Nr3D）上进行评估。<br>评估指标：<br>- 标准参考定位任务：使用准确率（Accuracy）。<br>- 不完美参考定位任务：<br> - 对象存在性检测子任务：使用二分类指标，包括真阳性、假阳性、真阴性、假阴性，并计算F1分数。<br> - 替代建议子任务：提出了一种启发式相似性评分系统（score_sim），通过加权计算原始陈述与建议替代陈述在对象类别、属性和空间关系等方面的匹配程度来量化替代建议的质量。</details> |
| 2025-03-20 | JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse | http://arxiv.org/abs/2503.16365 | <details><summary>展开</summary>待生成</details> |
| 2025-03-18 | GR00T N1: An Open Foundation Model for Generalist Humanoid Robots | http://arxiv.org/abs/2503.14734 | <details><summary>展开</summary>待生成</details> |
| 2025-03-15 | ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis | http://arxiv.org/abs/2503.14526 | <details><summary>展开</summary># 论文研究单位<br>北卡罗来纳大学教堂山分校计算机科学系、Robotics and AI Institute、华盛顿大学电气与计算机工程系<br><br># 论文概述<br>ReBot是一种通过真实到模拟再到真实的机器人视频合成技术来扩展机器人学习的方法。该方法首先在模拟环境中重放真实世界的机器人轨迹以多样化被操作物体，然后将模拟的运动与修复后的真实世界背景集成，合成物理上逼真且时间上一致的机器人视频。ReBot旨在解决真实世界数据收集成本高昂的问题，并通过模拟的扩展性和最小化模拟到现实的差距来增强视觉-语言-动作（VLA）模型的性能。<br><br># 论文核心贡献点<br>1. 提出首个真实到模拟再到真实的机器人数据集扩展方法，用于将VLA模型适配到目标领域。<br>2. 结合了真实数据和模拟的优势，利用模拟的可扩展性，同时通过真实数据来最小化模拟到现实的差距。<br>3. 全自动化的数据流程，无需人工干预。<br>4. 在模拟和真实世界环境中的广泛实验证明，ReBot显著提升了VLA模型的性能和鲁棒性，例如在SimplerEnv上OpenVLA的性能提升21.8%，真实世界任务成功率提高20%。<br><br># 论文方法描述<br>ReBot包含三个关键步骤：<br>1. 真实到模拟轨迹重放：在模拟环境中自动设置数字孪生，重放真实世界机器人轨迹以获取操纵新物体的模拟运动。通过分析夹具动作序列确定物体和容器的位置，并验证重放轨迹的成功性。<br>2. 真实世界背景修复：使用GroundedSAM2分割和跟踪真实世界视频中的机器人和物体，然后使用ProPainter移除这些特定元素，获得与任务无关的真实世界背景。<br>3. 模拟到真实视频合成：将模拟的运动与修复后的真实背景集成，合成新的机器人视频帧。通过替换原始语言指令中的物体和容器描述来生成新的指令。<br><br># 论文使用数据集和训练资源<br>数据集：BridgeData V2、DROID、自建的220个真实世界片段、Objaverse的厨房资产。<br>模拟环境：Isaac Sim 4.1，基于Isaac Lab实现。<br>训练资源：4块NVIDIA A6000 GPU，对Octo进行全微调，批次大小256，学习率4e-5；对OpenVLA进行LoRA微调，批次大小32，学习率5e-4。<br><br># 论文使用的评估环境和评估指标<br>评估环境：<br>- 模拟环境：SimplerEnv，使用WidowX 250 6DOF机器人臂。<br>- 真实环境：Franka Panda 7DoF机器人臂配备Robotiq 2F-85夹具。<br>评估指标：<br>- 视频质量：使用VBench评估时间质量（主体一致性、背景一致性、运动平滑度）和逐帧质量（成像质量）。<br>- 任务性能：在模拟和真实环境中评估VLA模型的任务成功率和泛化性能，包括领域内和跨域任务的表现。</details> |
| 2025-03-17 | MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation | http://arxiv.org/abs/2503.13446 | <details><summary>展开</summary>待生成</details> |
| 2025-03-13 | HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model | http://arxiv.org/abs/2503.10631 | <details><summary>展开</summary>论文研究单位<br>北京大学，北京人工智能研究院，香港中文大学<br><br>论文概述<br>该论文提出了HybridVLA，一个统一的视觉-语言-动作（VLA）模型框架，它将扩散和自回归动作生成方法整合到单个大型语言模型中。通过协作训练配方和动作集成机制，该模型旨在吸收扩散动作的连续性和自回归的上下文推理能力，实现两种范式的相互增强，从而提高机器人操作的鲁棒性和性能。HybridVLA在大规模跨具身机器人数据集上进行了预训练，并在模拟和真实世界任务中均实现了最先进的性能。<br><br>论文核心贡献点<br>1. 提出了HybridVLA，一个统一的模型，将扩散和自回归动作生成无缝集成在单个大型语言模型中，有效结合了扩散动作的连续性和自回归生成的上下文推理能力。<br>2. 引入了一种协作训练配方，通过共享的LLM主干连接两种动作生成方法，使得它们能够相互促进，并提出了一种协作动作集成机制，根据自回归动作令牌的置信度自适应地融合两种动作预测，增强了操作鲁棒性。<br>3. 提出的方法在多种任务上实现了最先进的性能，并在多种未见过的配置中表现出强大的泛化能力。<br><br>论文方法描述<br>HybridVLA采用预训练的视觉-语言模型（VLM）作为基础架构。其核心方法包括：<br>1. 令牌序列构建：设计了一种统一的令牌序列来组织多模态输入（视觉、语言、机器人状态）、扩散令牌和自回归令牌，通过特殊的标记令牌（<BOD>和<EOD>）连接不同部分，以协调两种生成范式。<br>2. 协作训练：使用混合目标函数（L_hybrid = L_dif + L_ce）联合训练模型。扩散部分使用均方误差损失预测噪声，自回归部分使用交叉熵损失预测离散动作令牌。通过共享LLM主干进行反向传播，使模型同时吸收连续动作表示和语义推理表示。<br>3. 协作动作集成：在推理时，模型同时生成扩散和自回归两种动作。根据自回归令牌的平均置信度（c^ar）进行自适应融合：当置信度高于阈值θ（0.96）时，对两种动作取平均；否则，仅使用扩散动作。扩散过程采用DDIM采样，仅需4步即可完成。<br><br>论文使用数据集和训练资源<br>预训练数据集：Open X-Embodiment, DROID, ROBOMIND，总计760K条轨迹，超过3300万帧，在A800 GPU上训练超过10,000小时。<br>微调数据集：自收集的模拟数据（RLBench）和真实世界数据。<br>训练资源：未明确提及具体GPU数量，但提及了A800 GPU和大量训练时长。<br><br>论文使用的评估环境和评估指标<br>评估环境：<br>1. 模拟环境：RLBench。<br>2. 真实世界环境：单臂和双臂机器人任务设置。<br>评估指标：<br>1. 成功率：在RLBench上报告平均成功率。<br>2. 推理速度：以Hz为单位报告模型的推理频率。<br>3. 泛化能力：在未见过的操作物体、背景、空间位置和光照条件下评估性能的稳定性。</details> |
| 2025-03-12 | CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games | http://arxiv.org/abs/2503.09527 | <details><summary>展开</summary>待生成</details> |
| 2025-03-11 | MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models | http://arxiv.org/abs/2503.08007 | <details><summary>展开</summary>待生成</details> |
| 2025-03-10 | PointVLA: Injecting the 3D World into Vision-Language-Action Models | http://arxiv.org/abs/2503.07511 | <details><summary>展开</summary>待生成</details> |
| 2025-03-06 | Refined Policy Distillation: From VLA Generalists to RL Experts | http://arxiv.org/abs/2503.05833 | <details><summary>展开</summary>待生成</details> |
| 2025-03-06 | VLA Model-Expert Collaboration for Bi-directional Manipulation Learning | http://arxiv.org/abs/2503.04163 | <details><summary>展开</summary>```markdown<br># 论文研究单位<br>中国科学院自动化研究所多模态人工智能系统重点实验室，以及中国科学院大学人工智能学院。<br><br># 论文概述<br>该论文提出了一种VLA模型-专家协作框架，旨在解决当前视觉-语言-动作（VLA）模型在多任务泛化方面的局限性。该框架通过引入有限数量的专家动作来增强VLA模型的性能，从而在降低专家工作量的同时，提升模型的可靠性和泛化能力。更重要的是，在协作过程中收集的操纵数据可用于进一步微调VLA模型，同时人类专家也能通过交互提升技能，形成一个双向的学习循环。实验结果在MetaWorld环境中验证了该框架在协作操纵和学习方面的有效性，显著提高了任务成功率。<br><br># 论文核心贡献点<br>- 提出了一种半自主操纵方法，通过VLA模型与专家的协作完成任务，这是首次对VLA模型与专家协作的探索性研究。<br>- 实现了一个双向学习过程：VLA模型可以利用协作中收集的操纵数据进行自我完善，同时专家也能逐渐适应VLA模型的行为模式。<br>- 在MetaWorld的MT10和MT50基准测试中进行了验证。当VLA模型与专家的动作比例为4:1时，VLA模型的平均成功率分别提升了6.2%和13.5%，而人类专家的操作步骤减少了82.24%。<br><br># 论文方法描述<br>- **VLA模型通用结构**: VLA模型将视觉输入（v_i）和语言指令（l_i）作为条件，通过各自的编码器处理后进行融合，最终预测出机器人的动作（a_i）。对于离散动作输出的模型，预测结果会被映射回连续的动作空间。<br>- **专家策略**: 研究中定义了两种专家策略。一种是基于规则的策略，在MetaWorld模拟环境中提供近最优解；另一种是人类用户策略，由参与者直接控制机械臂，其表现可能因熟练度而异。<br>- **专家-VLA协作**: 该过程包含“协作操纵”和“协作学习”两个阶段。在协作操纵中，VLA模型连续执行N步后，由专家介入执行一步，循环此模式直至任务完成。在协作学习中，将专家介入时产生的（视觉、语言、动作）数据存入缓冲区，当缓冲区满后，用这些数据对VLA模型进行监督式微调。<br><br># 论文使用数据集和训练资源<br>- **数据集**: 使用MetaWorld模拟环境中的ML10（10个任务）和ML50（50个任务）基准数据集。模型微调数据是通过基于规则的策略为每个任务采集50条演示轨迹，每条轨迹最多500个步骤。<br>- **训练资源**: VLA模型使用800K个采样数据进行微调。训练期间对视觉输入应用了随机裁剪、亮度/对比度/饱和度/色调调整等多种数据增强技术。动作输出根据模型类型（离散/连续）进行了归一化处理。<br><br># 论文使用的评估环境和评估指标<br>- **评估环境**: MetaWorld模拟环境，具体在ML10和ML50多任务基准上进行评估。<br>- **评估指标**:<br> - **成功率**: 对每个任务进行50次随机初始化的测试，统计任务成功完成的比率。<br> - **操作步骤数**: 记录并比较在纯专家策略和协作策略下，专家执行任务所需的平均动作步数，以量化专家工作量的减少。<br>```</details> |
| 2025-03-05 | OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction | http://arxiv.org/abs/2503.03734 | <details><summary>展开</summary>待生成</details> |
| 2025-03-05 | SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning | http://arxiv.org/abs/2503.03480 | <details><summary>展开</summary>待生成</details> |
| 2025-03-04 | RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour | http://arxiv.org/abs/2503.02572 | <details><summary>展开</summary># 论文研究单位<br>Skolkovo Institute of Science and Technology Moscow<br><br># 论文概述<br>RaceVLA提出了一种创新的自主竞速无人机导航方法，通过利用视觉-语言-动作（VLA）模型来模拟人类类似的行为。该模型处理来自无人机机载摄像头的第一人称视角（FPV）视频流和自然语言指令，以生成控制无人机的速度向量和偏航角速度信号。该研究旨在使无人机能够根据实时环境反馈调整其导航策略，模仿人类飞行员的决策过程。模型在收集的竞速无人机数据集上进行微调，在复杂的无人机竞速环境中展现出强大的泛化能力。<br><br># 论文核心贡献点<br>1. 提出了RaceVLA，这是第一个专门为自主竞速无人机设计的基于VLA的系统。<br>2. 在运动泛化（75.0 vs. 60.0）和语义泛化（45.5 vs. 36.3）方面优于OpenVLA，并在所有评估轴（视觉、运动、物理、语义）上均优于RT-2。<br>3. 利用动态相机特性训练模型，使其对相机位置变化具有鲁棒性，从而显著提升了运动泛化能力。<br>4. 实现了高速场景下的有效导航，平均速度为1.04 m/s，最高速度为2.02 m/s，展示了系统的实时决策能力。<br>5. 开源了RaceVLA的代码库、预训练权重和数据集，促进了相关领域的研究。<br><br># 论文方法描述<br>RaceVLA的核心是一个基于OpenVLA模型进行微调的VLA模型，其输入是来自无人机机载摄像头的FPV帧和自然语言指令，输出是直接控制无人机的四维动作向量。<br>- 模型架构：在OpenVLA-7b模型基础上，使用低秩适应（LoRA）技术进行微调，将原始的7维机械臂动作向量适配为适用于无人机控制的4维速度向量（Vx, Vy, Vz, omega）。<br>- 系统操作：无人机导航遵循一个迭代过程，系统实时处理连续的FPV帧和语言指令来计算并执行下一步飞行动作，而不是采用传统的基于路径点的导航方式。<br>- 硬件平台：使用定制的8英寸竞速无人机，配备Speedy Bee F405飞控（运行ArduPilot固件）、RealSense T265摄像头以及用于定位和计算的机载Intel NUC计算机。<br>- 软件流程：VLA模型运行在远程服务器上，通过Flask API接收无人机传来的实时帧，处理后将导航命令发回无人机。定位由OpenVINS视觉惯性里程计系统提供，整个系统在机器人操作系统（ROS）框架下运行。<br><br># 论文使用数据集和训练资源<br>- 数据集规模：包含200个片段（episodes）和大约20,000张图像。<br>- 数据集内容：涵盖了多种竞速任务，如穿过拱形门、方形门和在环形赛道上飞行。每个数据样本包括位置数据、速度分量（Vx, Vy, Vz）、偏航角变化和同步的视觉帧。<br>- 数据采集：使用Vicon运动捕捉系统以60Hz的频率记录无人机速度，使用T265摄像头以30Hz的频率捕捉图像。<br>- 训练模型：对OpenVLA-7b模型进行微调。<br>- 训练技术：采用参数高效的LoRA技术，使用rank-32的适配器来优化内存使用。<br>- 训练配置：批次大小为16，学习率为5e-4，共进行7000个梯度步，未使用图像增强。<br>- 训练资源：使用一块NVIDIA A100 GPU进行训练。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：<br> 1. 在无人机竞速赛道上进行任务评估，包括穿越单个门、根据指令选择特定门、在变化的配置下完成连续多门穿越任务。<br> 2. 在预定义的竞速赛道上进行多次飞行测试。<br> 3. 评估模型的泛化能力，涵盖四个维度：视觉（新背景、干扰物）、运动（新物体位置和姿态）、物理（新物体形状和尺寸）和语义（新指令和目标物体）。<br>- 评估指标：<br> 1. 泛化成功率：在视觉、运动、物理和语义四个维度上的任务成功百分比。<br> 2. 飞行参数：包括平均速度、最大速度、速度标准差、平均偏航角速度和任务平均持续时间。<br> 3. 性能对比：与基准模型OpenVLA和RT-2在不同泛化维度上的性能比较。</details> |
| 2025-03-04 | Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding | http://arxiv.org/abs/2503.02310 | <details><summary>展开</summary>待生成</details> |
| 2025-03-03 | CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs | http://arxiv.org/abs/2503.01378 | <details><summary>展开</summary>论文研究单位<br>Skolkovo Institute of Science and Technology<br><br>论文概述<br>该论文介绍了CognitiveDrone，一个专为无人机（UAV）复杂认知任务设计的新型视觉-语言-行动（VLA）模型。该模型在包含超过8,000条模拟飞行轨迹的数据集上训练，涵盖Human Recognition、Symbol Understanding和Reasoning三大类别，能够根据第一人称视觉输入和文本指令生成实时的4D动作命令。为提升在复杂场景下的表现，论文进一步提出了CognitiveDrone-R1，它集成了一个额外的视觉-语言模型（VLM）推理模块，以在高频控制前简化任务指令。论文还发布了开源评估基准CognitiveDroneBench，用于评估无人机在认知任务中的表现。实验结果表明，CognitiveDrone-R1在关键认知任务上的成功率比基础模型有显著提升。<br><br>论文核心贡献点<br>1. 提出了CognitiveDrone，一个专为解决复杂无人机认知任务而设计的VLA模型。<br>2. 开发了CognitiveDrone-R1，一个集成了VLM推理模块的增强版系统，用于提升任务理解和推理能力。<br>3. 创建并发布了CognitiveDroneBench，这是首个专门用于评估无人机认知任务的开源基准测试。<br>4. 构建并开源了一个包含8,062个样本的数据集，覆盖Human Recognition、Symbol Understanding和Reasoning三个认知类别。<br>5. 公开了完整的代码库、模型权重、训练和推理代码以及基准环境。<br><br>论文方法描述<br>系统架构基于开源的OpenVLA模型，一个拥有70亿参数的VLA模型，用于从第一人称视觉输入和文本指令生成平滑的4D（Vx, Vy, Vz, omega）无人机控制命令。该模型在包含8,000多个模拟飞行片段的自定义数据集上进行了微调，使其对无人机飞行物理和认知任务有深刻理解。为处理更复杂的认知任务，CognitiveDrone-R1架构增加了一个独立的70亿参数VLM（基于Qwen2.5-VL）作为推理模块。VLM以较低频率（约2Hz）运行，负责处理和简化任务指令，然后将简化后的指令传递给以较高频率（10Hz）运行的VLA模型，后者负责生成实时的控制信号。这种设计结合了VLA的快速反应控制和VLM的深层推理能力。<br><br>论文使用数据集和训练资源<br>数据集：论文收集了一个包含8,062条连续轨迹样本的自定义数据集，分为三个认知类别：Human Recognition（根据文本提示识别特定人物并导航至相应门）、Symbol Understanding（识别数字、字母、公司徽标、动物符号）和Reasoning（进行逻辑推断，如解决数学问题或根据属性关联对象）。数据以RLDS格式组织，记录了无人机在模拟环境中的速度和姿态。数据集被划分为训练集和测试集。<br>训练资源：模型在四块NVIDIA A100 GPU上进行训练。基础模型为OpenVLA-7b，并采用参数高效的LoRA方法（秩32适配器）进行微调。训练配置包括64的批次大小、5e-4的学习率、4000个梯度步长，并禁用了图像增强。<br><br>论文使用的评估环境和评估指标<br>评估环境：使用名为CognitiveDroneBench的开源基准进行评估。该基准基于Gazebo物理模拟器，集成了ArduPilot插件以真实模拟无人机动力学。环境中构建了一个带有多个连续认知检查点的赛道。无人机在每个检查点会接收到第一人称视角图像和文本指令，需要通过解决认知任务来选择正确的门并穿越。<br>评估指标：主要评估指标是成功率。无人机成功穿越正确的门得1分。每个任务类别（Human Recognition, Symbol Understanding, Reasoning）的最终得分是该类别累积得分除以最大可能得分。此外，还计算了所有任务样本的总体平均成功率，以衡量模型的综合性能。</details> |
| 2025-02-27 | Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success | http://arxiv.org/abs/2502.19645 | <details><summary>展开</summary>待生成</details> |
| 2025-02-26 | ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration | http://arxiv.org/abs/2502.19250 | <details><summary>展开</summary>待生成</details> |
| 2025-02-24 | Evolution 6.0: Evolving Robotic Capabilities Through Generative Design | http://arxiv.org/abs/2502.17034 | <details><summary>展开</summary>### 论文研究单位<br>Skolkovo Institute of Science and Technology, Intelligent Space Robotics Laboratory, Center for Digital Engineering.<br><br>### 论文概述<br>该论文提出了一个名为 Evolution 6.0 的概念，这是一个由生成式人工智能驱动的自主机器人系统。当机器人缺乏完成人类请求任务所需的工具时，它可以自主设计所需工具并学习如何使用它们以实现目标。该系统整合了视觉语言模型（VLMs）、视觉语言动作模型（VLA）和文本到3D生成模型，用于工具设计和任务执行，旨在提高机器人在不可预测环境中的适应性和操作灵活性。<br><br>### 论文核心贡献点<br>- 提出了 Evolution 6.0 框架，实现机器人通过生成式设计自主进化和创造工具。<br>- 构建了一个包含工具生成模块和动作生成模块的系统，实现了从场景理解到工具制造和任务执行的闭环。<br>- 集成了 QwenVLM、OpenVLA 和 Llama-Mesh 模型，分别用于环境理解、动作规划和3D工具生成。<br>- 通过实验验证了系统的有效性，工具生成成功率达到90%，动作生成在物理和视觉泛化上达到83.5%。<br><br>### 论文方法描述<br>系统包含两个核心模块：<br>- **工具生成模块**:<br> - 通过相机等传感器捕获机器人场景输入。<br> - 使用 Qwen2-VL-2B-Instruct 模型分析环境并生成工具设计的文本描述（如“创建一个3D刀具模型”）。<br> - 将描述输入到自回归语言模型 Llama-Mesh 中，生成3D网格格式的工具模型。<br> - 对网格进行渲染和验证，然后转换为G-Code，通过3D打印机制造出实体工具。<br>- **动作生成模块**:<br> - 基于一个经过微调的 OpenVLA 模型。<br> - 接收第三方视角的相机画面和自然语言任务指令作为输入。<br> - 输出一个7维的动作向量（3个平移位移，3个角位移和1个夹爪动作），直接控制机器人机械臂。<br> - 系统以5Hz的频率迭代运行，实时处理环境变化并调整动作。<br><br>### 论文使用数据集和训练资源<br>- **数据集**:<br> - 使用 UR10 机械臂和 Logitech C920e 相机收集。<br> - 包含20个数据片段，平均分为两个任务：切蛋糕和抓取并放置蛋糕。<br> - 数据格式化为RLDS（强化学习数据集）结构，包含机器人位姿、状态和同步的视觉反馈。<br>- **训练资源**:<br> - 对 OpenVLA-7b 模型进行微调，采用参数高效的 LoRA 方法，rank为32。<br> - 训练设置包括：批量大小为16，学习率为 5e-4，训练4000个梯度步。<br> - 在一块 NVIDIA A100 GPU 上进行训练。<br> - 推理时，QwenVLM 和 Llama-Mesh 模型被优化为 int8 精度，在一块 NVIDIA RTX 4090 24Gb GPU 上运行。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**:<br> - 评估分为两个阶段。<br> - 第一阶段评估工具生成模块，在10个不同的机器人场景中进行。<br> - 第二阶段评估动作生成模块，在一台 UR-10 机器人实物上进行，通过第三方视角观察。<br> - 第二阶段包含五种评估场景：与训练数据相同的场景、物理泛化（目标物体尺寸或颜色变化）、运动泛化（物体位置变化）、语义泛化（新的指令）和视觉泛化（场景变化或引入干扰物）。<br>- **评估指标**:<br> - **工具生成**: 使用成功率和平均推理时间作为指标。<br> - **动作生成**: 使用任务成功率作为主要指标，分别在五种场景下进行评估。</details> |
| 2025-02-20 | Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration | http://arxiv.org/abs/2502.14795 | <details><summary>展开</summary>待生成</details> |
| 2025-02-20 | ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model | http://arxiv.org/abs/2502.14420 | <details><summary>展开</summary>论文研究单位<br>Midea Group, East China Normal University, Shanghai University, Beijing Innovation Center of Humanoid Robotics, Tsinghua University<br><br>论文概述<br>论文提出了ChatVLA，一个统一的视觉-语言-动作模型框架，用于同时实现多模态理解和机器人控制。研究指出现有VLA模型存在两个关键挑战：spurious forgetting（伪遗忘），即机器人训练覆盖了关键的视觉-文本对齐；和task interference（任务干扰），即控制和理解任务的参数空间冲突导致性能下降。ChatVLA通过分阶段对齐训练和混合专家架构来解决这些问题，在保持参数效率的同时实现双重能力。<br><br>论文核心贡献点<br>对现有VLA方法进行深入分析，证明其在多模态理解和机器人控制两方面的局限性<br>引入ChatVLA框架，统一了对话能力、多模态理解和机器人控制于单一神经网络<br>在多模态理解基准和真实机器人任务上进行广泛实验评估<br>在包含25个多样化任务的真实家庭环境中进行机器人实验验证性能<br><br>论文方法描述<br>采用分阶段对齐训练策略：第一阶段专注于机器人控制数据训练，第二阶段增量整合多模态数据以重新激活视觉-文本对齐<br>引入混合专家架构：在MLP层使用MoE设计，使控制和理解任务共享注意力层以促进知识转移，同时隔离任务特定MLP以最小化干扰<br>架构基于双重编码理论，共享层处理跨任务知识，分离层处理任务特定知识<br><br>论文使用数据集和训练资源<br>机器人数据集D_robot包含专家演示轨迹，每个演示由状态-动作对序列组成<br>多模态理解数据集D_v-t包含视觉图像和对应问题或标题的图像-文本对<br>使用基准包括TextVQA、DocVQA、MMMU、MME、MMStar等13个多模态理解基准<br>模型基于2B参数规模，相比ECoT使用3.5倍更少的VLM骨干参数<br><br>论文使用的评估环境和评估指标<br>真实机器人环境：包含25个任务的多任务设置，涵盖浴室、厨房、桌面等多样化家庭环境技能<br>多模态理解基准：包括视觉问答任务和一般理解任务，涵盖数学、OCR等能力<br>评估指标：机器人任务成功率、平均成功长度、多模态理解准确率、MMMU得分、MMStar得分等</details> |
| 2025-02-19 | VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation | http://arxiv.org/abs/2502.13508 | <details><summary>展开</summary>待生成</details> |
| 2025-02-14 | Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation | http://arxiv.org/abs/2502.10040 | <details><summary>展开</summary>待生成</details> |
| 2025-02-13 | GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation | http://arxiv.org/abs/2502.09268 | <details><summary>展开</summary>论文研究单位<br>Zhejiang University, Westlake University<br><br>论文概述<br>该论文提出了GEVRM（Goal-Expressive Video Generation Model for Robust Visual Manipulation），一个旨在增强视觉-语言-动作（VLA）模型在部署期间面对不可避免的外部扰动时的鲁棒性的新颖框架。该方法集成了经典的内部模型控制（IMC）原理，通过文本引导的视频生成模型生成具有高度表现力的未来视觉规划目标，并利用原型对比学习来对齐状态表示。这种设计使模型能够隐式地推断和区分来自外部环境的扰动，从而在标准与受扰动的CALVIN基准以及真实机器人任务中实现稳健的性能。<br><br>论文核心贡献点<br>- 引入了GEVRM，一个新颖的闭环VLA模型，它将IMC原理整合到机器人视觉运动控制中，以增强对干扰的鲁棒性。<br>- 研究了如何利用文本引导的视频生成模型获得高度表现力的目标，并通过原型对比学习对齐状态表示来抵抗部署时的外部扰动。<br>- 通过广泛的实验验证了GEVRM的有效性，表明它在标准与受扰动的CALVIN基准上显著优于以前的最先进方法，并且在真实世界机器人操作中，所生成目标状态的表现力相较于基线方法有显著改善。<br><br>论文方法描述<br>方法分为三个核心部分：<br>1. 机器人行为规划器：采用基于DiT（Diffusion Transformer）的文本引导视频扩散模型。模型使用2D VAE进行空间压缩（8倍下采样），使用3D VAE进行时间压缩（4倍下采样），并实施一个随机掩蔽机制（75%权重分配给初始帧），通过Rectified Flow训练，以提升对物理世界规律的理解。<br>2. 状态对齐以模拟响应：使用基于ResNet-34的视觉编码器将目标状态和当前状态映射到潜在空间，并通过原型对比学习进行对齐。该方法将来自同一轨迹的状态视为正样本对，不同轨迹的状态视为负样本对，优化一个温度参数δ以分配概率，从而在潜在空间中模拟机器人对扰动的响应。<br>3. 目标引导的动作预测：设计一个以生成的高表现力目标为条件的扩散策略，通过逆动力学和对比学习目标进行联合优化，将目标与当前内部编码信号解码为机器人可执行的鲁棒动作。<br><br>论文使用数据集和训练资源<br>- 机器人行为规划器使用大规模的文本-视频对进行训练，数据来源于带语言标签的网络视频片段和带文本注释的机器人序列决策数据。<br>- 机器人动作预测模型使用少量的玩耍数据（视频-动作序列对）进行训练，该数据不依赖语言标签。<br>- 论文中未明确提及具体的训练资源，如GPU数量或训练时长。<br><br>论文使用的评估环境和评估指标<br>- 评估环境：CALVIN基准（包含标准条件和引入外部扰动如光照变化和视频流噪声的场景）以及真实世界的机器人操作任务。<br>- 评估指标：对于目标生成，评估其表现力（expressiveness）；对于动作执行，评估在标准与扰动基准上的任务成功率，并与先前的最先进方法进行性能比较。</details> |
| 2025-02-07 | Survey on Vision-Language-Action Models | http://arxiv.org/abs/2502.06851 | <details><summary>展开</summary>待生成</details> |
| 2025-02-09 | DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control | http://arxiv.org/abs/2502.05855 | <details><summary>展开</summary>待生成</details> |
| 2025-02-08 | HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation | http://arxiv.org/abs/2502.05485 | <details><summary>展开</summary>待生成</details> |
| 2025-02-08 | ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy | http://arxiv.org/abs/2502.05450 | <details><summary>展开</summary>待生成</details> |
| 2025-02-06 | Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture | http://arxiv.org/abs/2502.04558 | <details><summary>展开</summary>论文研究单位<br>塔夫茨大学计算机科学系<br><br>论文概述<br>该论文旨在解决视觉-语言-动作（VLA）模型作为机器人通用策略时存在的“黑箱”问题和环境敏感性。为了提升其可解释性和鲁棒性，作者提出将VLA模型与认知架构（CA）相结合。具体方法是通过探测技术，从开源的OpenVLA模型的隐藏层中提取符号化状态（如物体属性、关系和动作状态），并将这些符号信息集成到DIARC认知架构中，实现对模型内部状态的实时监控和高层逻辑推理。<br><br>论文核心贡献点<br>1. 通过线性探测实验，证明了OpenVLA模型在其隐藏层中编码了丰富的符号化状态信息，包括物体关系、物体属性以及动作状态。<br>2. 对OpenVLA模型Llama 2主干的所有33个隐藏层进行了系统性的探测分析，评估了不同层次对符号信息的编码能力。<br>3. 设计并实现了一个集成的DIARC-OpenVLA系统，该系统利用探测到的符号状态进行实时状态监控和可视化，为构建更可解释、更可靠的机器人系统提供了基础。<br><br>论文方法描述<br>1. 集成系统构建：构建了DIARC-OpenVLA集成系统。用户通过DIARC的图形界面发送自然语言指令，指令通过WebSocket服务器传递给OpenVLA。OpenVLA在LIBERO模拟环境中执行动作，同时其指定隐藏层的激活值被提取出来。<br>2. 探测实验设计：<br> - 探测器训练：为OpenVLA的33个隐藏层分别训练两个线性探测器（一个用于物体状态，一个用于动作状态）。探测器是一个多标签分类器，使用二元交叉熵损失进行训练，将4096维的隐藏层激活向量映射到一组符号状态的二元预测值。<br> - 数据收集：在LIBERO-spatial数据集的10个抓取与放置任务中，让OpenVLA执行并收集数据。每个任务收集5个成功的轨迹。在每个时间步t，记录第ℓ层的隐藏激活h_t和对应的环境真实符号状态y_t，形成训练对(h_t, object_state_t)和(h_t, action_state_t)。<br> - 数据预处理：采用轨迹级别的训练/测试集划分以避免时间泄露。移除了在训练集中标签值变化极小（真值占比低于1%或高于99%）的符号状态标签。<br><br>论文使用数据集和训练资源<br>1. 数据集：LIBERO-spatial数据集。该数据集包含10个模拟的抓取与放置任务，任务形式为“捡起某个具有特定空间关系的黑碗，并将其放到盘子上”。总共收集了10个任务 * 5个轨迹/任务 = 50个成功轨迹的数据。<br>2. 训练资源：论文中未明确提及训练探测器时所使用的具体硬件资源（如GPU型号）和训练时长。<br><br>论文使用的评估环境和评估指标<br>1. 评估环境：在LIBERO-spatial模拟环境中收集的测试集上对训练好的探测器进行评估。<br>2. 评估指标：采用准确率作为主要评估指标。对于每一种符号谓词（如on, left-of, grasped），计算该谓词所有实例（如on(bowl, plate), on(plate, table)）的二元分类准确率的平均值。结果以热力图形式展示，呈现了不同隐藏层在不同符号谓词上的预测准确率。</details> |
| 2025-02-04 | VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation | http://arxiv.org/abs/2502.02175 | <details><summary>展开</summary>待生成</details> |
| 2025-02-03 | Scalable, Training-Free Visual Language Robotics: A Modular Multi-Model Framework for Consumer-Grade GPUs | http://arxiv.org/abs/2502.01071 | <details><summary>展开</summary>论文研究单位<br>CNRS-AIST JRL (Joint Robotics Laboratory), National Institute of Advanced Industrial Science and Technology (AIST), Japan。<br><br>论文概述<br>现有的视觉-语言-动作（VLA）模型因高昂的计算成本、需要大量重新训练以及可扩展性有限而受到阻碍，限制了其广泛应用。本文提出了SVLR（可扩展视觉-语言机器人技术），一个开源、模块化且无需训练的框架，旨在解决这些问题。SVLR集成了多个轻量级开源AI模型（如Mini-InternVL、CLIPSeg、Phi-3和all-MiniLM），用于处理视觉和语言输入，以识别环境中的物体并响应自然语言指令生成动作序列。该框架的关键优势在于其可扩展性，无需重新训练，只需添加文本描述和任务定义即可轻松集成新的机器人任务和机器人。实验表明，SVLR可在NVIDIA RTX 2070（移动版）GPU上有效运行，并在抓取与放置任务中展现出良好的性能。<br><br>论文核心贡献点<br>1. 提出了一个名为SVLR的开源、模块化、无需训练的机器人控制框架，旨在提高视觉-语言机器人技术的可扩展性和可及性。<br>2. 通过结合轻量级的开源AI模型，使整个框架能够在消费级GPU（如NVIDIA RTX 2070）上运行，降低了对高端硬件的依赖。<br>3. 设计了一种模块化架构，允许用户仅通过添加文本描述和预编程任务来轻松扩展新的机器人和任务，无需对模型进行重新训练。<br>4. 集成了句子相似度模型，用于校准大型语言模型（LLM）的输出与预定义任务及感知模块检测到的物体，提高了系统的鲁棒性和任务执行的可靠性。<br><br>论文方法描述<br>SVLR框架接收用户的语言指令和环境图像作为输入，通过四个主要组件进行处理：<br>1. 机器人信息模块：以文本形式描述机器人的能力，并提供预编程的任务集。<br>2. 感知模块：首先使用视觉-语言模型（VLM）根据图像生成场景中物体的文本描述，然后使用零样本分割模型为每个物体生成掩码，并计算其质心，最后将像素坐标转换为世界坐标系。<br>3. 大型语言模型（LLM）与提示生成：提示生成器将用户指令、感知模块提供的环境信息和机器人的能力描述整合成一个提示。LLM据此生成一系列高层次的、格式化为“action_name: [parameters]”的动作描述。<br>4. 动作管理器：解析LLM的输出，使用句子相似度模型将输出的动作名称和参数与机器人信息模块中的预定义任务及感知模块检测到的物体名称进行匹配。匹配成功后，执行相应的预编程任务，并将命令发送给机器人控制器。<br><br>论文使用数据集和训练资源<br>该论文提出了一个无需训练的框架，没有为该特定任务使用或创建任何数据集。所有使用的AI模型（Mini-InternVL, CLIPSeg, Phi-3, all-MiniLM）均为预训练的开源模型，框架直接利用其现有能力，无需进一步训练或微调。<br><br>论文使用的评估环境和评估指标<br>评估环境：<br>* **硬件**：UR10机器人臂，Robotiq 2F-140夹爪，一个安装在夹爪上的标准网络摄像头。<br>* **计算平台**：NVIDIA RTX 2070（移动版）GPU，配备8GB显存。<br><br>评估指标与任务：<br>* **任务**：主要针对抓取与放置任务进行评估，以演示框架理解自然语言指令、进行推理和执行动作序列的能力。<br>* **评估方式**：评估是定性的，通过展示框架成功执行多种不同复杂度的语言指令（如“clean the bottle”, “Put the can into the green box and the bottle into the red box”）的实例来证明其有效性。文中未提及使用如成功率或平均执行时间等定量指标。</details> |
| 2025-01-31 | UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent | http://arxiv.org/abs/2501.18867 | <details><summary>展开</summary>待生成</details> |
| 2025-01-28 | Improving Vision-Language-Action Model with Online Reinforcement Learning | http://arxiv.org/abs/2501.16664 | <details><summary>展开</summary>待生成</details> |
| 2025-01-25 | An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation | http://arxiv.org/abs/2501.15068 | <details><summary>展开</summary>待生成</details> |
| 2025-01-16 | FAST: Efficient Action Tokenization for Vision-Language-Action Models | http://arxiv.org/abs/2501.09747 | <details><summary>展开</summary>### 论文研究单位<br>Physical Intelligence, UC Berkeley, Stanford<br><br>### 论文概述<br>论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。<br><br>### 论文核心贡献点<br>1. 提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。<br>2. 发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。<br>3. 验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。<br>4. 实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。<br><br>### 论文方法描述<br>1. 动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。<br>2. DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。<br>3. 系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。<br>4. 展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。<br>5. 逆向过程：通过逆BPE和逆DCT重建连续动作。<br><br>### 论文使用数据集和训练资源<br>1. 评估任务：<br> - Libero模拟基准（空间、目标、长时任务）<br> - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）<br> - DROID零样本桌面操作（15Hz）<br>2. 训练数据：<br> - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。<br> - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。<br>3. 计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。<br><br>### 论文使用的评估环境和评估指标<br>1. 评估环境：<br> - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。<br> - 模拟环境：Libero任务套件。<br> - 零样本测试：三个大学校园的全新桌面场景。<br>2. 评估指标：<br> - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。<br> - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。<br> - 训练效率：收敛速度和计算时间。<br> - 泛化能力：零样本环境下的策略鲁棒性。</think><br>### 论文研究单位<br>Physical Intelligence, UC Berkeley, Stanford<br><br>### 论文概述<br>论文提出了一种名为FAST（Frequency-space Action Sequence Tokenization）的高效动作序列标记化方法，用于训练自回归视觉-语言-动作模型。该方法基于离散余弦变换（DCT）对机器人动作序列进行压缩，解决了现有标记化方法在处理高频控制任务时效果不佳的问题。论文还发布了FAST+，一个在100万个真实机器人轨迹上训练的通用动作标记化器，可处理多种机器人和任务。<br><br>### 论文核心贡献点<br>1. 提出FAST标记化方法：使用DCT将动作序列转换到频域，通过量化和字节对编码（BPE）压缩信号，有效减少标记数量。<br>2. 发布FAST+通用标记化器：在多样化机器人数据上预训练，可直接应用于新任务，无需重新训练。<br>3. 验证了FAST在多个高频灵巧操作任务上的有效性：使自回归VLA模型首次成功应用于DROID等高频数据集，实现零样本泛化。<br>4. 实验表明FAST训练速度比扩散模型快5倍，同时保持相当性能。<br><br>### 论文方法描述<br>1. 动作归一化：将动作的1%和99%分位数映射到[-1,1]范围。<br>2. DCT变换：对每个动作维度独立应用离散余弦变换，得到频域系数。<br>3. 系数量化：通过缩放因子（γ=10）对系数缩放后四舍五入，稀疏化高频分量。<br>4. 展平与BPE压缩：按频率优先顺序展平系数矩阵，训练BPE编码器生成紧凑标记序列。<br>5. 逆向过程：通过逆BPE和逆DCT重建连续动作。<br><br>### 论文使用数据集和训练资源<br>1. 评估任务：<br> - Libero模拟基准（空间、目标、长时任务）<br> - 真实机器人任务：桌面整理（20Hz）、T恤折叠（50Hz）、杂货装袋（20Hz）、烤面包（50Hz）、洗衣折叠（50Hz）<br> - DROID零样本桌面操作（15Hz）<br>2. 训练数据：<br> - FAST+通用标记化器：100万个真实机器人轨迹，涵盖单臂、双臂和移动机器人。<br> - VLA训练：π₀模型在10,000小时数据上训练，包括BridgeV2（5Hz）、DROID（15Hz）等。<br>3. 计算资源：未明确说明硬件配置，但强调FAST比扩散模型训练快5倍。<br><br>### 论文使用的评估环境和评估指标<br>1. 评估环境：<br> - 真实机器人：UR5单臂、ARX双臂、Trossen Viper-X双臂系统。<br> - 模拟环境：Libero任务套件。<br> - 零样本测试：三个大学校园的全新桌面场景。<br>2. 评估指标：<br> - 任务成功率：洗衣折叠按衣物项目计算，其他任务按回合完成率。<br> - 标记压缩率：比较FAST与朴素分箱方法的平均标记数。<br> - 训练效率：收敛速度和计算时间。<br> - 泛化能力：零样本环境下的策略鲁棒性。</details> |
| 2025-01-12 | Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing | http://arxiv.org/abs/2501.06919 | <details><summary>展开</summary>待生成</details> |
| 2025-01-09 | UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation | http://arxiv.org/abs/2501.05014 | <details><summary>展开</summary>### 论文研究单位<br>莫斯科斯科尔科沃科学技术学院<br><br>### 论文概述<br>该论文提出了UAV-VLA（视觉-语言-动作）系统，这是一个旨在通过简单文本指令为空中机器人生成大规模飞行任务的工具。该系统集成了卫星图像处理、视觉语言模型（VLM）和大型语言模型（GPT），能够将用户的自然语言请求转换为具体的飞行路径和动作计划。系统利用卫星图像提供的丰富上下文信息来增强决策和任务规划。实验表明，该方法生成的飞行路径长度与人工结果相差22%，在K-最近邻（KNN）方法下，地图上目标定位的平均欧氏距离误差为34.22米。同时，UAV-VLA系统生成所有飞行计划仅需5分24秒，比有经验的操作员快6.5倍。<br><br>### 论文核心贡献点<br>- 提出了一个大规模的视觉-语言-动作（VLA）系统，能够根据文本任务请求和卫星图像生成路径-动作集。<br>- 引入了UAV-VLPA-nano-30基准数据集，用于在全球范围内评估VLA系统的性能。<br>- 在UAV-VLPA-nano-30基准上验证了该系统，其性能可与人类专家生成的路径和动作计划相媲美。<br><br>### 论文方法描述<br>该方法包含三个核心模块：<br>1. **目标提取GPT模块**：解析用户的语言指令，提取出一组目标任务。<br>2. **目标搜索VLM模块**：利用Molmo模型在卫星图像中识别出这些目标，并输出它们在图像中的像素坐标。<br>3. **动作生成GPT模块**：将像素坐标通过元数据转换为全球地理坐标，并结合任务细节，使用MAVProxy工具生成具体的无人机动作序列。<br>整个流程实现了从自然语言指令到无人机可执行任务的端到端转换，包括指令解析、目标检测和坐标转换。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：使用新提出的UAV-VLPA-nano-30基准数据集。该数据集包含30张来自USGS EarthExplorer平台的高分辨率卫星图像，覆盖美国多样化的城市、郊区和自然环境。图像分辨率为1.5米/像素，拍摄于春夏季节的白天。<br>- **训练资源**：实验未涉及模型从零开始的训练，而是使用了预训练模型。具体评估环境为一台配备RTX 4090 GPU (24GB VRAM)和Intel Core i9-13900K CPU的PC。由于内存限制，使用了量化后的Molmo-7B-D BnB 4-bit模型进行目标搜索。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：系统生成的飞行计划与人类专家在Mission Planner软件中手动创建的计划进行比较。测试任务为：“创建一个飞行计划，让四轴飞行器在100米高度绕飞每个建筑物，然后返回起始点并降落”。<br>- **评估指标**：<br> - **路径长度**：比较系统生成路径与人工生成路径的总长度。<br> - **定位误差**：使用均方根误差（RMSE）衡量生成路径点与人工路径点之间的空间偏差。采用了三种方法计算误差：顺序方法、动态时间规整（DTW）和K-最近邻（KNN）。<br> - **效率**：比较系统生成任务所需时间与人类操作员所需时间。</details> |
| 2025-01-08 | Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding | http://arxiv.org/abs/2501.04693 | <details><summary>展开</summary>论文研究单位<br>Berkeley AI Research (BAIR), UC Berkeley, USA<br><br>论文概述<br>论文提出了一种名为FuSe的新方法，通过自然语言作为跨模态锚定，将预训练的通用机器人策略微调以支持异构传感器模态（如触觉和听觉）。该方法使机器人能够在部分可观测环境中执行需要多模态推理的任务，例如在视觉遮挡时依赖触觉和听觉。实验表明FuSe在真实世界任务中将成功率提高了20%以上。<br><br>论文核心贡献点<br>- 提出FuSe微调方法，利用语言锚定将异构传感器数据整合到预训练的通用机器人策略中<br>- 引入多模态对比损失和语言生成损失，连接视觉、触觉、听觉等模态与策略的语义知识<br>- 实现零样本多模态提示能力，如跨模态对象描述和组合推理<br>- 验证方法在多种架构上的通用性，包括基于Transformer的Octo模型和视觉-语言-动作(VLA)模型<br>- 开源包含26,866条轨迹的多模态数据集，涵盖视觉、触觉、音频和动作数据<br><br>论文方法描述<br>FuSe方法包含以下关键组件：<br>- 触觉编码器：使用预训练的TVL编码器处理触觉图像<br>- 音频编码器：将原始音频波形转换为频谱图，通过ResNet26编码<br>- 辅助损失函数：多模态对比损失最大化不同视图间的互信息；语言生成损失预测高阶语义<br>- 最终损失函数：L = L_BC + βL_gen + λL_contrast，其中β=1，λ=1<br>- 语言重述：使用ChatGPT生成多样化的指令模板以增强泛化能力<br><br>论文使用数据集和训练资源<br>- 数据集：自建多模态机器人轨迹数据集，包含26,866条轨迹，涵盖视觉(640x480)、触觉(DIGIT传感器320x240)、音频(44.1kHz)、本体感觉和语言指令<br>- 训练资源：在TPU v5e-128 pod上训练50,000步，批量大小1024，使用余弦学习率调度器，峰值学习率3×10^-4<br><br>论文使用的评估环境和评估指标<br>- 评估环境：WidowX 250 6-DoF机械臂，配备第三人称和手腕RGB相机、两个DIGIT触觉传感器、麦克风和9-DoF IMU<br>- 任务：桌面抓取、购物袋抓取和按钮按压三个任务，其中购物袋任务模拟视觉遮挡场景<br>- 评估指标：任务成功率，包括到达和抓取成功率，在训练和未见对象上分别评估<br>- 额外评估：多模态提示准确率、组合任务完成率和消融研究性能比较</details> |
| 2025-01-07 | OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints | http://arxiv.org/abs/2501.03841 | <details><summary>展开</summary>### 论文研究单位<br>CFCS, School of CS, Peking University; PKU-AgiBot Lab; AgiBot<br><br>### 论文概述<br>该论文提出了一种名为OmniManip的通用机器人操作系统，旨在解决非结构化环境下的机器人操控难题。现有视觉语言模型（VLM）虽擅长高层推理，但缺乏精细操控所需的细粒度3D空间理解能力，而将VLM微调为视觉-语言-动作模型（VLA）又面临数据成本高和泛化性差的问题。为应对这些挑战，论文提出了一种新颖的、以物体为中心的表示方法，通过将交互原语（点和方向）定义在物体的标准空间中，作为空间约束来桥接VLM的高层推理与底层操控。系统设计为一个双闭环架构：一个用于高层规划，通过原语重采样、交互渲染和VLM检查实现闭环；另一个用于底层执行，通过6D姿态跟踪实现闭环，从而在无需微调VLM的情况下实现鲁棒、实时的控制。<br><br>### 论文核心贡献点<br>1. 提出了一种新颖的以物体为中心的交互表示，有效桥接了VLM的高层常识推理与底层机器人操控之间的鸿沟。<br>2. 首次提出了一个规划和执行双闭环的开放词汇操控系统，且整个过程无需对VLM进行微调。<br>3. 通过大量实验证明了该方法在多样化操控任务上具有强大的零样本泛化能力，并展示了其在自动化大规模仿真数据生成方面的潜力。<br><br>### 论文方法描述<br>该方法将复杂的机器人任务分解为多个阶段，每个阶段由带有空间约束的物体交互原语来定义。<br>1. **任务分解与原语定义**：利用视觉基础模型（VFM）和VLM从指令中识别相关物体并分解任务。交互原语被定义为物体标准空间中的交互点和交互方向。空间约束则定义了主动物体与被动物体之间交互原语的距离和角度关系。<br>2. **原语与约束提取**：首先使用单视图3D生成网络和通用6D物体姿态估计模型对物体进行网格重建和标准化。然后，通过VLM结合视觉提示定位交互点（包括可见和不可见的点），并通过LLM对沿主轴采样的交互方向进行任务相关性评分，生成一个有序的、带有约束的交互原语列表。<br>3. **双闭环系统**：<br> - **闭环规划**：引入了一个基于重采样、渲染和检查的自校正机制（RRC）。系统渲染当前交互配置，交由VLM验证。根据验证结果（成功、失败或需优化），系统决定接受、尝试下一个约束或进入细化阶段进行更精细的重采样，从而有效缓解VLM的幻觉问题。<br> - **闭环执行**：当空间约束确定后，任务执行被表述为一个优化问题，目标是在满足空间约束、碰撞避免和路径平滑等损失函数的条件下，计算最优的末端执行器目标姿态。通过6D姿态跟踪器实时更新姿态，形成执行闭环。<br><br>### 论文使用数据集和训练资源<br>该方法不依赖于特定任务的训练数据集，采用零样本学习方式。它利用了多种预训练模型作为组件，包括：<br>- 视觉语言模型（VLM）和大型语言模型（LLM）用于高层推理和验证。<br>- Omni6DPose用于通用6D物体姿态估计。<br>- 单视图3D生成网络用于从单张图像生成物体网格。<br>实验中使用的机器人硬件包括Franka Emika和Kinova Gen3机械臂，以及各种日常物体。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：在真实世界的实验平台上进行评估，使用了Franka Emika和Kinova Gen3两种机械臂，并包含了12种不同的日常操作任务，涵盖了刚体物体操控（如倒茶、插花）和铰接物体操控（如开关抽屉）。<br>- **评估指标**：主要采用任务成功率作为核心量化指标。对于每个任务，记录在多次尝试中成功完成的次数，并计算总体的平均成功率。例如，在12项任务上，OmniManip的闭环版本总成功率为68.3%，显著优于其他基线方法。</details> |
| 2025-01-07 | Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis | http://arxiv.org/abs/2501.03565 | <details><summary>展开</summary>### 论文研究单位<br>中国科学技术大学生命科学与医学部、苏州高等研究院、斯坦福大学、科大讯飞医疗事业部、中国科学技术大学附属第一医院放射科。<br><br>### 论文概述<br>针对3D医学图像零样本诊断中现有视觉-语言对齐方法存在的模态间隙问题，提出Bridged Semantic Alignment (BrgSA)框架。该框架通过大型语言模型对医疗报告进行语义总结，并设计跨模态知识交互模块，利用跨模态知识库作为语义桥梁，缩小图像与文本特征之间的间隙，提升对齐效果。<br><br>### 论文核心贡献点<br>- 提出BrgSA框架，包含语义报告总结和跨模态知识交互模块，有效弥合视觉与文本特征间隙。<br>- 构建扩展基准数据集CT-RATE-LT，涵盖15种低频异常，用于评估长尾疾病的零样本诊断能力。<br>- 在CT-RATE、RAD-ChestCT等数据集上实现state-of-the-art性能，显著提升低频异常诊断和报告-体素检索任务效果。<br><br>### 论文方法描述<br>- **语义总结**：利用大型语言模型（如GPT-4 Turbo）提取医疗报告关键信息，生成固定模板总结（如"There is [abnormality]"），并采用双输入策略（原始报告+总结）平衡信息完整性与语义一致性。<br>- **跨模态知识交互（CMKI）**：引入跨模态知识库（CMKB）作为共享语义桥梁，通过注意力权重重建图像和文本特征（公式7），结合重构损失（MSE）和对比损失（InfoNCE）优化特征对齐（公式8-11）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：CT-RATE（47,149训练样本）、CT-RATE-LT（扩展15种低频异常）、RAD-ChestCT（3,630样本）、INSPECT（3,214 CTPA样本）。<br>- **训练资源**：单块NVIDIA A800 GPU，PyTorch框架，学习率5e-5，批量大小64，CMKB大小K=2048，损失权重α=0.5、β=1、γ=1。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：单块NVIDIA A800 GPU。<br>- **评估指标**：<br> - 多标签分类：AUC、Accuracy、F1、Precision、mAP、Recall@1、Precision@3。<br> - 检索任务：体素-体素检索用MAP@Q（Q={5,10,50}），报告-体素检索用Recall@P（P={5,10,50,100}）。<br> - 统计显著性：AUC用DeLong测试，其他指标用bootstrap或t-test（p<0.001）。</details> |
| 2025-01-06 | Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches | http://arxiv.org/abs/2501.03151 | <details><summary>展开</summary>待生成</details> |
| 2024-12-29 | CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance | http://arxiv.org/abs/2412.20451 | <details><summary>展开</summary>待生成</details> |
| 2024-12-24 | VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks | http://arxiv.org/abs/2412.18194 | <details><summary>展开</summary>待生成</details> |
| 2024-12-20 | QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning | http://arxiv.org/abs/2412.15576 | <details><summary>展开</summary>待生成</details> |
| 2024-12-18 | Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models | http://arxiv.org/abs/2412.14058 | <details><summary>展开</summary>论文研究单位<br>清华大学, 字节跳动研究, 中科院自动化所模式识别国家重点实验室, 上海交通大学, 新加坡国立大学<br><br>论文概述<br>论文探讨了构建视觉-语言-动作模型（VLAs）以实现通用机器人策略的关键设计因素。通过系统研究不同VLA架构、骨干网络选择、数据使用策略，回答了为何选择VLAs、如何构建VLAs、选择哪种骨干网络以及何时使用跨具身数据集等问题。论文提出了一个名为RoboVLMs的新框架，用于将VLMs转换为VLAs，并在多个基准测试中实现了最先进的性能。<br><br>论文核心贡献点<br>1. 系统分析了构建VLAs的四个关键问题：为何选择VLAs、如何构建VLAs、选择哪种VLM骨干网络以及何时使用跨具身数据集。<br>2. 提出了RoboVLMs框架，一个灵活、易于使用的开源框架，支持将任何VLM轻松集成到VLA中，并允许自由组合各种设计选择。<br>3. 通过超过600个不同的实验，验证了VLAs的有效性，并在三个模拟任务和真实世界实验中实现了最先进的性能。<br>4. 开源了所有细节，包括代码、模型、数据集和工具包，以及详细的训练和评估方案。<br><br>论文方法描述<br>1. VLA结构研究：比较了四种VLA结构，包括单步建模、交错连续动作建模和策略头连续动作建模，以确定最佳结构。<br>2. 骨干网络选择：研究了8种不同的VLM骨干网络，评估其在机器人操作任务中的适用性。<br>3. 数据使用策略：探讨了预训练、微调和后训练三种数据使用策略，以确定何时使用跨具身数据集。<br>4. RoboVLMs框架：提供了一个统一的框架，允许无缝集成任何VLM，并支持各种设计选择的组合。<br><br>论文使用数据集和训练资源<br>1. 数据集：CALVIN（多任务桌面操作）、SimplerEnv（真实到模拟环境）和自收集的真实世界机器人操作数据集（包含100个操作任务和74K轨迹）。<br>2. 训练资源：使用了不同规模的VLM骨干网络（3B到9B参数），并在不同数据规模（10%、100%、500%）下进行了实验。<br><br>论文使用的评估环境和评估指标<br>1. 评估环境：<br> - 模拟环境：CALVIN和SimplerEnv。<br> - 真实世界环境：使用Kinova Gen3机器人臂和Robotiq 2F-85夹爪的真实机器人实验。<br>2. 评估指标：<br> - CALVIN：连续任务成功率（1至5个任务）和平均任务长度。<br> - SimplerEnv：任务成功率。<br> - 真实世界：平均成功率，包括简单设置、未见干扰物、未见目标对象、未见背景和新颖技能描述。</details> |
| 2024-12-18 | RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation | http://arxiv.org/abs/2412.13877 | <details><summary>展开</summary>论文研究单位<br>Beijing Innovation Center of Humanoid Robotics, State Key Laboratory of Multimedia Information Processing (Peking University), and Beijing Academy of Artificial Intelligence.<br><br>论文概述<br>论文提出了RoboMIND，一个用于机器人操作的大规模、多形态智能规范数据集。该数据集通过人类遥操作收集，包含107k个演示轨迹，涵盖479个不同任务和96种物体类别。RoboMIND在统一的数据收集平台和标准化协议下构建，涉及四种不同的机器人形态：Franka Emika Panda、双灵巧手的X-Humannoid Tien Kung人形机器人、AgileX双臂机器人和UR5e。数据集还包括5k个真实世界失败演示，每个都附有详细原因，以及一个在Isaac Sim模拟器中复现真实世界任务的数字孪生环境。论文通过使用单任务模仿学习方法和视觉-语言-动作（VLA）大模型进行大量实验，验证了数据集的质量和多样性。<br><br>论文核心贡献点<br>1. 提出了RoboMIND，一个大规模、多形态的机器人操作数据集，包含107k个演示轨迹、479个任务、96个物体类别和38种操作技能。<br>2. 数据集通过人类遥操作收集，并在统一平台和标准化协议下构建，确保数据一致性和可靠性。<br>3. 涵盖四种机器人形态（单臂、双臂、人形），提供了多样化的机器人形态数据。<br>4. 包含5k个失败案例轨迹，每个附带详细失败原因，支持策略学习中的失败反思和纠正。<br>5. 创建了数字孪生环境，便于低成本收集额外训练数据和高效评估。<br>6. 通过广泛的实验证明了数据集的有效性，包括单任务模仿学习和多任务VLA模型的评估。<br><br>论文方法描述<br>数据集构建方法：采用人类遥操作技术收集数据，针对不同机器人类型使用特定设备（如3D打印组件、辅助臂控制、动作捕捉服）。开发了一个智能数据平台，包含数据收集、存储、预处理、分类和标注五个主要功能模块。数据以标准化H5格式存储，包含多视图RGB-D数据、机器人本体状态信息、末端执行器状态信息和遥操作身体状态信息。数据预处理基于预定义标准进行质量保证检查，包括初始检查、详细检查和数据过滤与问题记录。数据分类采用以任务为中心的协议，每个任务由机器人类型、操作技能、涉及物体和场景描述四个关键组件定义。数据标注提供任务级语言描述和10k轨迹的帧级细粒度语言描述，结合Gemini自动生成和人工修正。<br><br>论文使用数据集和训练资源<br>数据集：RoboMIND，包含107k个演示轨迹，涉及479个任务和96个物体类别。训练资源：实验使用真实机器人设置，包括配备Intel RealSense D435i相机的Franka Emika Panda、配备Orbbec相机的Tien Kung人形机器人、配备Orbbec Astra相机的AgileX Cobot Magic V2.0，以及配备顶部Intel RealSense D435i相机的UR5e。数字孪生环境基于NVIDIA Isaac Sim构建。<br><br>论文使用的评估环境和评估指标<br>评估环境：真实世界的机器人实验设置，针对Franka使用顶部、左侧和右侧三个视角的相机，针对Tien Kung和AgileX使用内置摄像头，针对UR使用外部顶部相机。评估指标：任务成功率。每个模型进行十次测试，记录成功或失败情况，计算成功次数与总测试次数的比率作为成功率。</details> |
| 2024-12-16 | Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning | http://arxiv.org/abs/2412.11974 | <details><summary>展开</summary>待生成</details> |
| 2024-12-13 | TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies | http://arxiv.org/abs/2412.10345 | <details><summary>展开</summary>论文研究单位<br>University of Maryland, College Park; Microsoft Research; Capital One<br><br>论文概述<br>该论文提出了TraceVLA，一种通过视觉轨迹提示增强通用机器人策略时空感知能力的模型。该方法通过在图像上叠加机器人历史移动的视觉轨迹作为提示，让VLA模型更好地理解时空动态，从而在复杂操作任务中提升性能。作者开发了7B和4B两种规模的VLA模型，并在模拟和真实机器人任务上进行了广泛评估。<br><br>论文核心贡献点<br>- 提出视觉轨迹提示技术：通过点跟踪算法生成历史移动轨迹，并作为视觉提示叠加在原始图像上，为VLA模型提供时空记忆。<br>- 构建数据集与模型：收集了150K条机器人轨迹数据，用于微调OpenVLA，并开发了基于Phi-3-Vision的紧凑4B模型。<br>- 验证有效性：在模拟环境(SimplerEnv的137个配置)和真实机器人(WidowX的4个任务)上均达到SOTA性能，分别比OpenVLA提升10%和3.5倍。<br><br>论文方法描述<br>1. 视觉轨迹生成：<br> - 使用Co-Tracker对历史图像序列进行密集点轨迹跟踪(K=40网格)。<br> - 筛选活跃轨迹：计算点位移，保留移动总位移超过阈值κ的轨迹。<br> - 随机采样M=5条轨迹，叠加在当前观测图像上作为视觉提示。<br><br>2. 模型架构设计：<br> - 双图像输入：同时输入原始图像和叠加轨迹的图像，用特殊分隔符连接。<br> - 训练时采用dropout机制：以概率α替换轨迹图为原始图，提升鲁棒性。<br><br>3. 训练与推理优化：<br> - 训练：对OpenVLA和Phi-3-Vision进行5个epoch的微调。<br> - 推理：稀疏查询Co-Tracker，每20步重计算密集轨迹，平衡效率与性能。<br><br>论文使用数据集和训练资源<br>- 数据集：BridgeData-v2, Google RT-1, Open X-Embodiment (970K轨迹)，以及自采WidowX数据(120条)，总计150K带轨迹标注的轨迹。<br>- 训练资源：7B模型使用32×H100，batch size=4096，30个epoch；4B模型可在RTX4090/A5000上微调。<br>- 超参数：时间窗口N=6，网格大小K=40，采样轨迹数M=5。<br><br>论文使用的评估环境和评估指标<br>- 仿真环境：SimplerEnv的Google机器人任务，包含视觉匹配(visual matching)和变体聚合(variant aggregation)两种设置，覆盖137个环境配置。<br>- 真实环境：WidowX-250机器人，8个操作任务(含4个未见任务)，使用256×256 RGB摄像头。<br>- 评估指标：任务成功率(%)，具体包括环境适应下的平均成功率，以及未见任务的泛化性能对比。</details> |
| 2024-12-09 | Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks | http://arxiv.org/abs/2412.06224 | <details><summary>展开</summary>待生成</details> |
| 2024-12-05 | NaVILA: Legged Robot Vision-Language-Action Model for Navigation | http://arxiv.org/abs/2412.04453 | <details><summary>展开</summary>### 论文研究单位<br>UC San Diego, USC, NVIDIA<br><br>### 论文概述<br>NaVILA是一个用于腿式机器人导航的视觉-语言-动作模型，旨在解决视觉-语言导航问题。该模型采用两级框架：高级视觉-语言-动作（VLA）模型生成包含空间信息的中级动作（如“向前移动75cm”），低级视觉运动策略则执行这些动作。这种方法使机器人能够通过人类语言指令导航复杂场景，并在模拟和真实环境中实现了高性能，显著提升了现有基准的导航成功率。<br><br>### 论文核心贡献点<br>- 提出了一个两级框架NaVILA，统一了VLA模型与运动技能，通过中级语言动作桥接高级推理和低级控制。<br>- VLA模型输出自然语言形式的中级动作，而不是直接预测低级关节动作，增强了跨机器人平台的可移植性。<br>- 利用YouTube人类旅行视频数据训练模型，首次实现直接从真实视频中学习连续环境导航，提升泛化能力。<br>- 开发了新基准VLN-CE-Isaac，基于Isaac Sim，包含更现实的场景、低级控制和物理交互。<br>- 在真实机器人部署中取得88%的整体成功率，其中复杂指令成功率达75%，展示了在挑战环境中的鲁棒性。<br><br>### 论文方法描述<br>- **高级VLA模型**：基于VILA视觉-语言模型，处理单视图图像和历史帧，生成导航动作。设计特定导航提示，区分当前观察和历史帧，使用文本描述内存信息。采用监督微调（SFT）混合数据，包括仿真数据、真实视频数据和一般VQA数据。<br>- **从人类视频学习**：处理2000个YouTube旅行视频，通过熵采样生成20K轨迹，使用MASt3R估计相机姿态，提取步进动作，并利用VLM和LLM生成自然语言指令。<br>- **低级运动策略**：使用PPO算法训练视觉运动策略，输入为LiDAR点云生成的身高图和本体感觉数据（如关节位置、速度），输出为期望关节位置。策略在Isaac Sim中通过Isaac Lab以单阶段方式训练，避免策略蒸馏，提高效率。身高图处理包括体素网格化和最大滤波，以增强环境感知。<br>- **训练推理**：VLA模型在VILA第二阶段基础上微调一个epoch，所有模块未冻结；推理时使用正则表达式解析器提取动作类型和参数。运动策略以高频率运行，支持实时控制。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 仿真导航数据：R2R-CE和RxR-CE，使用Habitat模拟器生成动作序列。<br> - 真实视频数据：2000个YouTube人类旅行视频，处理成20K轨迹，通过MASt3R估计姿态。<br> - 辅助导航数据：EnvDrop增强指令、ScanQA 3D扫描QA对、导航轨迹摘要任务。<br> - 一般VQA数据：来自多个通用视觉问答数据集，以保持模型泛化能力。<br>- **训练资源**：<br> - VLA训练：基于VILA模型，在混合数据上微调一个epoch，使用标准GPU资源（具体型号未提及）。<br> - 运动策略训练：在Isaac Sim中使用Isaac Lab，采用PPO算法，在RTX 4090 GPU上达到60K FPS训练吞吐量。<br> - 推理：VLA模型运行在低频率，运动策略实时运行，整体系统部署在机器人硬件上。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 经典VLN基准：R2R-CE和RxR-CE的Val-Unseen分割。<br> - 新基准：VLN-CE-Isaac，基于Isaac Sim，包含详细机器人关节运动和物理交互。<br> - 真实世界场景：在Unitree Go2、Unitree H1和Booster T1机器人上测试，包括实验室、住宅和户外不平地形。<br>- **评估指标**：<br> - 导航误差（NE，数值越低越好）。<br> - 目标成功率（OS，数值越高越好）。<br> - 成功率（SR，数值越高越好）。<br> - 路径长度加权成功率（SPL，数值越高越好）。<br> - nDTW（用于RxR-CE，数值越高越好）。</details> |
| 2024-12-02 | Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control | http://arxiv.org/abs/2412.01034 | <details><summary>展开</summary>论文研究单位<br>Hanyang University, Hyundai Motor Company<br><br>论文概述<br>本文提出了一种量化感知模仿学习框架QAIL，用于资源高效的机器人控制。现有的基于深度神经网络的策略模型在机器人操作和自动驾驶中表现出色，但计算成本高且内存需求大。虽然量化可以降低这些成本，但直接量化会导致性能显著下降，因为量化误差在动作序列中会累积，影响决策质量。本文通过将量化集成到模仿学习的微调过程中，并结合量化鲁棒行为克隆技术，使低精度策略能够在资源受限的设备上保持高性能，同时实现显著的加速和能效提升。<br><br>论文核心贡献点<br>1. 提出了量化感知模仿学习(QAIL)框架，将量化操作集成到模仿学习的微调过程中，增强策略对低比特精度误差的鲁棒性。<br>2. 设计了量化鲁棒行为克隆(QBC)方法，通过最小化量化策略与全精度策略之间的动作分布差异来减少累积误差。<br>3. 开发了加权QBC(wQBC)变体，基于状态重要性对损失进行加权，特别适用于长时程任务。<br>4. 在机器人操作、自动驾驶和物理模拟三个领域验证了方法有效性，实现了接近全精度模型的性能，同时获得2.5×至3.7×的加速和显著的能耗降低。<br><br>论文方法描述<br>方法由两个主要组件构成：<br>1. QAIL：结合专家演示数据集(D_E)和全精度策略生成数据(D_FP)，在量化感知设置下通过模仿学习目标训练量化策略。使用STE(直通估计器)处理量化操作的梯度。<br>2. QBC：引入KL散度损失函数，对齐量化策略π^q_θ和全精度策略π^FP的动作分布，减少序列决策中的量化误差累积。总损失函数为L^total = L^QAIL + λL^QBC。<br>3. wQBC：通过基于扰动的显著性分数评估状态重要性，对QBC损失进行加权，使模型关注关键决策状态。<br><br>论文使用数据集和训练资源<br>数据集：<br>- 机器人操作：LIBERO基准(包括LIBERO-Spatial, LIBERO-Object等)<br>- 自动驾驶：NoCrash基准(包括NoCrash-Busy, NoCrash-Dense)<br>- 物理模拟：D4RL基准(MuJoCo环境)<br>训练资源：NVIDIA A100 GPU(80GB)和NVIDIA RTX 4090 GPU(24GB)<br><br>论文使用的评估环境和评估指标<br>评估环境：<br>- 机器人操作：LIBERO模拟环境<br>- 自动驾驶：CARLA模拟器中的NoCrash基准<br>- 物理模拟：MuJoCo控制环境<br>评估指标：<br>- 机器人操作：任务成功率<br>- 自动驾驶：驾驶得分、碰撞率、路线完成率<br>- 物理模拟：归一化平均回报<br>- 效率指标：推理延迟、吞吐量、能耗<br>- 硬件平台：Jetson Orin边缘GPU、低端GPU、设备CPU</details> |
| 2024-11-29 | SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters | http://arxiv.org/abs/2412.00174 | <details><summary>展开</summary>论文研究单位<br>SenseTime Research, S-Lab, Nanyang Technological University<br><br>论文概述<br>论文介绍了SOLAMI，这是一个用于与3D自主角色进行沉浸式交互的首个端到端社交视觉-语言-动作（VLA）建模框架。该框架旨在赋予3D角色社交智能，使其能够感知、理解用户的多模态输入（语音和动作）并生成相应的多模态响应（语音和动作）。SOLAMI从三个方面构建了3D自主角色：1）社交VLA架构：一个统一的框架，用于根据用户输入生成多模态响应；2）交互式多模态数据：一个名为SynMSI的合成数据集，以解决数据稀缺问题；3）沉浸式VR界面：一个允许用户与角色进行沉浸式交互的VR系统。广泛的定量实验和用户研究表明，该框架能够产生更精确、自然且符合用户期望的角色响应，同时具有更低的延迟。<br><br>论文核心贡献点<br>1. 一种新的VLA架构，用于建模角色的行为系统以实现沉浸式社交交互。<br>2. 一个专用的合成流水线，能够利用现有的运动数据集自动生成大规模的多模态交互数据SynMSI。<br>3. 一个沉浸式VR接口，供用户通过语音和动作与各种角色进行交互。<br><br>论文方法描述<br>SOLAMI是一个基于仅解码器LLM骨干网络的端到端社交VLA模型。<br>- **架构**：用户的语音和动作首先被转换为离散的语音和动作标记。然后，LLM基于用户的输入标记和角色设置预测角色的输出响应标记。生成的标记随后被解码为相应的语音和动作。模型使用特殊标记来分隔不同模态序列，并以多轮对话的方式进行交互。运动表示采用SMPL-X关节旋转，并为身体、手部和相对变换设计了独立的VQ-VAE量化器以实现更高的重建精度。语音则使用SpeechTokenizer将语音解构为语义标记，并支持通过短语音样本进行实例语音克隆。<br>- **训练**：采用三阶段训练策略。第一阶段训练运动量化器，然后冻结。第二阶段进行多任务预训练，以实现运动与文本、语音与文本之间的模态对齐。第三阶段使用合成的多模态交互数据集进行指令微调，使模型能够处理长序列、多轮对话，该阶段采用对响应的下一个token预测进行监督，并比较了全参数微调和LoRA微调方法。<br><br>论文使用数据集和训练资源<br>- **数据集**：为解决数据稀缺问题，论文提出了SynMSI数据集，其生成流程包括：1）从网络平台收集5.3K个话题；2）使用GPT-4o基于话题和角色设置生成多轮对话文本脚本；3）利用精心策划的运动数据库（包含HumanML3D, Inter-X, DLP-MoCap等，共46K个运动-文本对）通过文本嵌入检索最相关的运动；4）根据检索到的运动优化语音脚本，并使用TTS/语音克隆技术生成角色特定的语音。最终获得了6.3K个多轮多模态对话项。<br>- **训练资源**：模型在2个NVIDIA H800 GPU上进行训练和推理。<br><br>论文使用的评估环境和评估指标<br>- **评估环境**：开发了一个VR接口进行评估。前端使用Oculus Quest 3头显，其内置的全身追踪系统用于捕捉用户动作，并通过麦克风捕捉语音。后端由2个H800 GPU驱动的服务器构成，负责运行SOLAMI及基线模型。用户的动作被重定向到SMPL-X模型，生成的角色面部动画由UniTalker方法驱动，最终渲染到3D角色模型上。实验还包括定量评估和VR用户研究。<br>- **评估指标**：<br> - **运动指标**：FID (Frechet Inception Distance)、Diversity、PA-MPJPE (Procrustes Aligned Mean Per Joint Position Error)、Angle Error。<br> - **语音指标**：VC Similarity (Voice Cloning Similarity)、WER (Word Error Rate)。<br> - **效率指标**：Inference Time (推理时间)、Latency (延迟)。<br> - **用户研究**：通过VR界面进行定性评估，以衡量交互的自然度、精确度和用户满意度。</details> |
| 2024-11-29 | RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World | http://arxiv.org/abs/2412.00171 | <details><summary>展开</summary>论文研究单位<br>未在提供的HTML原文中找到相关信息。<br><br>论文概述<br>未在提供的HTML原文中找到相关信息。原文仅包含一个工作流程图，展示了从真实机器人收集数据到模型评估的步骤。<br><br>论文核心贡献点<br>未在提供的HTML原文中找到相关信息。<br><br>论文方法描述<br>根据原文中的流程图，论文方法包含一个循环流程：1. 在真实机器人上收集示范数据；2. 数据处理；3. 数据标注；4. 模型训练；5. 在真实机器人上进行评估。<br><br>论文使用数据集和训练资源<br>未在提供的HTML原文中找到相关信息。<br><br>论文使用的评估环境和评估指标<br>根据原文中的流程图，评估在真实机器人上进行。具体的评估指标未在原文中提及。</details> |
| 2024-11-29 | CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation | http://arxiv.org/abs/2411.19650 | <details><summary>展开</summary>### 论文研究单位<br>清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。<br><br>### 论文概述<br>该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。<br><br>### 论文核心贡献点<br>- 提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。<br>- 系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。<br>- 引入了一种简单而有效的自适应动作集成算法，用于时间融合。<br>- 在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。<br><br>### 论文方法描述<br>模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。<br><br>### 论文使用数据集和训练资源<br>- 数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。<br>- 预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。<br>- 微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。<br>- 训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。<br><br>### 论文使用的评估环境和评估指标<br>- 评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。<br>- 评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。</think><br>### 论文研究单位<br>清华大学、微软亚洲研究院、中国科学技术大学、中国科学院微电子研究所。<br><br>### 论文概述<br>该论文提出了CogACT，一种用于机器人操作的基础视觉-语言-动作模型。它旨在通过协同认知和行动能力来提高任务性能和泛化能力，以应对现有视觉-语言-动作模型在任务成功率上的不足。<br><br>### 论文核心贡献点<br>- 提出了一种组件化的VLA模型架构，将认知与行动解耦，使用VLM输出引导专门的扩散动作模块。<br>- 系统研究了扩散动作变换器作为动作序列建模骨干，并证明了其有利的扩展行为。<br>- 引入了一种简单而有效的自适应动作集成算法，用于时间融合。<br>- 在多个机器人平台和基准测试上，显著超越了现有VLA模型，展示了在新机器人上的快速适应能力和对未见物体和背景的泛化能力。<br><br>### 论文方法描述<br>模型由三部分组成：视觉模块、语言模块和扩散动作模块。视觉模块使用DINOv2和SigLIP预训练模型将图像观察编码为视觉标记。语言模块基于LLaMA-2，将视觉标记与语言指令集成，生成一个认知特征。扩散动作模块以该认知特征为条件，使用扩散变换器预测一个多步动作序列，处理动作的连续性、多模态和时间相关性。训练通过最小化预测噪声与真实噪声之间的均方误差进行端到端优化。推理时，采用自适应动作集成算法，根据历史预测与当前预测的相似性自适应地融合多个动作预测，以生成最终执行的动作。<br><br>### 论文使用数据集和训练资源<br>- 数据集：Open X-Embodiment数据集，包含大规模的视觉-语言-动作数据。<br>- 预训练数据：互联网规模的图像-文本对数据，用于视觉和语言模块的初始化。<br>- 微调数据：针对真实机器人实验，使用Realman和Franka机器人收集的小量特定数据。<br>- 训练资源：未明确提及具体硬件，但提及模型包含7B参数的VLM基础和最大300M参数的动作模块。<br><br>### 论文使用的评估环境和评估指标<br>- 评估环境：模拟环境、真实世界环境（使用Realman机器人和Franka机器人）。<br>- 评估指标：任务成功率，用于衡量模型执行各种语言指令引导任务的成功率。</details> |
| 2024-11-28 | GRAPE: Generalizing Robot Policy via Preference Alignment | http://arxiv.org/abs/2411.19309 | <details><summary>展开</summary>待生成</details> |
| 2024-11-18 | Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics | http://arxiv.org/abs/2411.13587 | <details><summary>展开</summary>### 论文研究单位<br>Rochester Institute of Technology, University of Missouri - Kansas City, U.S. Naval Research Laboratory, Lamar University, Meta AI, University of Rochester, Rutgers University<br><br>### 论文概述<br>信息未在提供的HTML原文中。<br><br>### 论文核心贡献点<br>信息未在提供的HTML原文中。<br><br>### 论文方法描述<br>信息未在提供的HTML原文中。<br><br>### 论文使用数据集和训练资源<br>信息未在提供的HTML原文中。<br><br>### 论文使用的评估环境和评估指标<br>信息未在提供的HTML原文中。</details> |
| 2024-11-15 | Visual-Linguistic Agent: Towards Collaborative Contextual Object Reasoning | http://arxiv.org/abs/2411.10252 | <details><summary>展开</summary># 论文研究单位<br>Carnegie Mellon University, Zhejiang University, Beijing Institute of Petrochemical Technology, TikTok, Sealand Technology Inc., Singapore Management University<br><br># 论文概述<br>论文提出视觉-语言代理（VLA）框架，结合多模态大语言模型（MLLMs）的推理能力和传统目标检测模型的定位能力。传统目标检测模型定位精确但缺乏上下文连贯性，MLLMs擅长关系推理但定位不准确。VLA通过MLLM作为语言代理与专门的视觉代理协作，利用空间和上下文关系推理评估检测结果，并通过分类视觉代理提供纠正反馈，实现更准确和上下文相关的目标检测。<br><br># 论文核心贡献点<br>1. 提出视觉-语言代理（VLA）协作框架，利用MLLMs的推理能力增强目标检测的上下文连贯性。<br>2. MLLM作为语言代理与目标检测视觉代理、分类视觉代理协作，通过空间关系推理过滤错误检测，最大化MLLM的推理能力同时增强定位精度。<br>3. 在COCO数据集上实验显示VLA使AP50:95提升3%，为多模态目标检测树立新基准。<br><br># 论文方法描述<br>VLA框架包含三个阶段：<br>1. 场景理解与目标检测：视觉代理（如YOLO）处理图像生成边界框和类别标签，语言代理（MLLM）基于视觉特征生成上下文图像描述。<br>2. 理性分析与错误过滤：语言代理根据场景理解和常识知识评估每个检测的合理性，标记错误检测结果。<br>3. 目标错误纠正：针对标记的错误检测，分类视觉代理（如CLIP）执行细粒度分类修正错误标签。<br>代理间使用JSON格式交换结构化数据，最终结果保存为COCO风格JSON文件。<br><br># 论文使用数据集和训练资源<br>数据集：COCO数据集<br>多模态语言模型：GPT-4o, Claude 3.5 Sonnet, LLaVA, Gemini 1.5<br>目标检测模型：Faster R-CNN, YOLOX, YOLOv11, DETR, DINO<br>分类视觉代理：CLIP<br><br># 论文使用的评估环境和评估指标<br>评估环境：COCO数据集<br>评估指标：<br>检测指标：平均精度均值（mAP），具体包括AP50:95、AP50、AP75<br>目标尺寸相关指标：APs（小目标）、APm（中目标）、APl（大目标）<br>错误纠正指标：纠正率（Correction Rate），评估错误检测的修复能力</details> |
| 2024-11-04 | Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks | http://arxiv.org/abs/2411.05821 | <details><summary>展开</summary>### 论文研究单位<br>Metarch.ai<br><br>### 论文概述<br>本文提出了一个全面的评估框架和基准测试套件，用于评估视觉-语言-动作（VLA）模型在机器人学习任务中的性能。研究对GPT-4o、OpenVLA和JAT三种最先进的VLM和VLA模型进行了详细分析，覆盖了来自Open-X-Embodiment集合的20个多样化数据集。研究揭示了当前VLA模型在不同任务和机器人平台上的性能变化显著，其中GPT-4o通过复杂的提示工程展现出最一致的性能，而所有模型在需要多步规划的复杂操作任务中均表现不佳，且模型性能对动作空间特性和环境因素高度敏感。<br><br>### 论文核心贡献点<br>1. **首个大规模VLA基准测试**：推出了MultiNet v0.1，这是首个针对大规模通用动作模型的基准测试。<br>2. **系统性评估框架**：建立了一个全面的评估框架，包括数据集管理、模型配置和性能指标。<br>3. **模型性能分析**：提供了对三种代表性VLA模型的详细性能分析，揭示了它们的优势和局限。<br>4. **开源工具**：发布了开源软件基础设施，用于下载、管理和利用基准数据。<br>5. **跨模态映射框架**：提出了一个通用框架，用于将VLM映射到其他模态类别，特别是动作空间。<br><br>### 论文方法描述<br>1. **数据集处理**：从Open-X-Embodiment数据集中筛选并处理了53个数据集，最终对20个数据集进行评估。数据集涵盖多种机器人平台和任务类型，使用RLDS格式存储。<br>2. **模型配置**：<br> - **JAT**：采用零样本设置，对图像进行4通道处理（RGB复制红色通道作为Alpha），观察值和动作拼接为单一张量。<br> - **GPT-4o**：构建综合提示，包括浮点观察状态、主图像观察、自然语言指令和动作空间描述，对不兼容输出进行错误处理。<br> - **OpenVLA**：标准化夹爪命令（二进制/三元离散化或连续归一化），处理特殊数据集（如UCSD和ETH），并排除终端张量。<br>3. **评估指标**：主要使用平均均方误差（AMSE）、归一化AMSE（NAMSE）和完成率，通过比较预测动作与真实动作来评估模型性能。<br>4. **推理基础设施**：JAT和GPT使用GCP e2-standard-8实例，OpenVLA使用配备NVIDIA L4 GPU的GCP g2-standard-8实例。<br><br>### 论文使用数据集和训练资源<br>1. **数据集**：Open-X-Embodiment数据集，包含来自21个机构的超过100万条真实机器人轨迹，涵盖22种不同的机器人形态。评估版本v0.1使用了53个数据集，完整训练数据约32TB。<br>2. **训练资源**：未明确提及模型训练过程，但详细描述了推理阶段的计算资源，包括GCP实例类型和GPU配置（如NVIDIA L4）。<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**：基于Google Cloud Platform（GCP）的虚拟机实例，具体包括：<br> - JAT和GPT：e2-standard-8实例（8 vCPU，32 GB内存）。<br> - OpenVLA：g2-standard-8实例（NVIDIA L4 GPU，24 GB显存）。<br>2. **评估指标**：<br> - **平均均方误差（AMSE）**：计算数据集中所有时间步的MSE平均值，用于跨数据集性能比较。<br> - **归一化AMSE（NAMSE）**：对每个模型的MSE进行最小-最大归一化后取平均，用于 equitable 跨数据集比较。<br> - **完成率**：通过比较预测与真实最终动作评估任务完成情况，作为任务完成能力的近似度量。</details> |
| 2024-11-05 | VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation | http://arxiv.org/abs/2411.03540 | <details><summary>展开</summary>### 论文研究单位<br>Robotics Institute, Carnegie Mellon University<br><br>### 论文概述<br>论文介绍了VLA-3D，一个用于3D语义场景理解和导航任务的大规模真实世界数据集。该数据集旨在解决现有视觉-语言模型在室内导航任务中因空间推理和语义理解不足而面临的挑战，特别是在包含大量细粒度物体的复杂且变化的场景中。VLA-3D通过提供丰富的3D场景数据、语义关系和指称语言，帮助开发更鲁棒和交互式的具身智能体。<br><br>### 论文核心贡献点<br>1. 提出了VLA-3D，一个目前最大的真实世界3D视觉-语言-行动数据集，包含超过11.5K个室内场景、23.5M个语义关系和9.7M个指称语句。<br>2. 为每个场景提供了大规模的密集场景图，有助于处理场景变化和识别相似物体，这是该数据集区别于其他数据集的关键特征之一。<br>3. 包含了可导航的自由空间标注，使得模型不仅能引用物体，还能引用空间或路径。<br>4. 生成的指称语言语句是视图无关、无歧义且最小化的，更贴近人类自然语言习惯。<br>5. 公开了完整的数据集生成代码和可视化工具，以促进研究发展。<br><br>### 论文方法描述<br>数据集的构建包含三个主要步骤：<br>1. **3D扫描处理**：整合ScanNet、Matterport3D、HM3D、3RScan、ARKitScenes和Unity等数据源的3D扫描数据，生成点云。为每个物体提取语义类别、边界框和主要颜色，并计算水平可穿越的自由空间区域。<br>2. **场景图生成**：基于物体的3D边界框，使用启发式方法为每个区域内的物体对或三元组计算八种类型的语义空间关系（如above, below, near, between等），构建结构化的场景图。<br>3. **语言生成**：基于生成的场景图，使用模板方法合成指称语言语句。这些语句遵循视图无关、无歧义和最小化的原则，并利用关系谓词的同义词增加多样性。<br><br>### 论文使用数据集和训练资源<br>* **使用数据集**：该论文主要介绍了一个新的数据集VLA-3D，其本身由多个现有数据集构成，包括ScanNet、Matterport3D、Habitat-Matterport 3D (HM3D)、3RScan、ARKitScenes以及Unity生成的场景。最终数据集包含7635个场景，超过11.5K个区域（房间），超过28.6万个物体，以及477个不同的物体类别。<br>* **训练资源**：论文未提及在VLA-3D数据集上从头训练模型的计算资源（如GPU数量、训练时间）。评估时直接使用了MVT和3D-VisTA模型的预训练检查点。<br><br>### 论文使用的评估环境和评估指标<br>* **评估环境**：在VLA-3D数据集的测试集上，对两个当前最先进的开源模型（MVT和3D-VisTA）的预训练权重进行了直接评估，以验证数据集的挑战性。评估内容是在未见过的复杂场景上执行指称物体定位任务。<br>* **评估指标**：主要的评估指标是**准确率**，即模型根据语言描述正确识别出目标物体的百分比。论文还将模型在VLA-3D上的性能与它们在标准基准数据集Nr3D和Sr3D上的表现进行了对比。</details> |
| 2024-11-04 | DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution | http://arxiv.org/abs/2411.02359 | <details><summary>展开</summary>待生成</details> |
| 2024-11-01 | CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision | http://arxiv.org/abs/2411.00508 | <details><summary>展开</summary>待生成</details> |
| 2024-10-21 | The Duality of Generative AI and Reinforcement Learning in Robotics: A Review | http://arxiv.org/abs/2410.16411 | <details><summary>展开</summary>待生成</details> |
| 2024-10-21 | VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making | http://arxiv.org/abs/2410.15885 | <details><summary>展开</summary>待生成</details> |
| 2024-10-21 | A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM | http://arxiv.org/abs/2410.15549 | <details><summary>展开</summary>待生成</details> |
| 2024-10-17 | Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand | http://arxiv.org/abs/2410.14022 | <details><summary>展开</summary>待生成</details> |
| 2024-10-15 | Latent Action Pretraining from Videos | http://arxiv.org/abs/2410.11758 | <details><summary>展开</summary>待生成</details> |
| 2024-10-10 | Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation | http://arxiv.org/abs/2410.08001 | <details><summary>展开</summary>待生成</details> |
| 2024-09-12 | HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers | http://arxiv.org/abs/2410.05273 | <details><summary>展开</summary>待生成</details> |
| 2024-10-07 | LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation | http://arxiv.org/abs/2410.05191 | <details><summary>展开</summary>待生成</details> |
| 2024-10-02 | Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust | http://arxiv.org/abs/2410.01971 | <details><summary>展开</summary>待生成</details> |
| 2024-09-29 | RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model | http://arxiv.org/abs/2409.19590 | <details><summary>展开</summary>待生成</details> |
| 2024-09-19 | TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2409.12514 | <details><summary>展开</summary>待生成</details> |
| 2024-09-05 | OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving | http://arxiv.org/abs/2409.03272 | <details><summary>展开</summary>待生成</details> |
| 2024-08-19 | CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving | http://arxiv.org/abs/2408.10845 | <details><summary>展开</summary>待生成</details> |
| 2024-07-25 | Unified Lexical Representation for Interpretable Visual-Language Alignment | http://arxiv.org/abs/2407.17827 | <details><summary>展开</summary>待生成</details> |
| 2024-07-11 | Robotic Control via Embodied Chain-of-Thought Reasoning | http://arxiv.org/abs/2407.08693 | <details><summary>展开</summary>待生成</details> |
| 2024-07-10 | Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs | http://arxiv.org/abs/2407.07775 | <details><summary>展开</summary>待生成</details> |
| 2024-06-27 | OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents | http://arxiv.org/abs/2407.00114 | <details><summary>展开</summary>待生成</details> |
| 2024-06-28 | LLaRA: Supercharging Robot Learning Data for Vision-Language Policy | http://arxiv.org/abs/2406.20095 | <details><summary>展开</summary>待生成</details> |
| 2024-06-21 | Learning Efficient and Robust Language-conditioned Manipulation using Textual-Visual Relevancy and Equivariant Language Mapping | http://arxiv.org/abs/2406.15677 | <details><summary>展开</summary>待生成</details> |
| 2024-06-13 | OpenVLA: An Open-Source Vision-Language-Action Model | http://arxiv.org/abs/2406.09246 | <details><summary>展开</summary>待生成</details> |
| 2024-06-06 | RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation | http://arxiv.org/abs/2406.04339 | <details><summary>展开</summary>待生成</details> |
| 2024-05-31 | Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations | http://arxiv.org/abs/2406.00121 | <details><summary>展开</summary>待生成</details> |
| 2024-05-27 | A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation | http://arxiv.org/abs/2405.17418 | <details><summary>展开</summary>待生成</details> |
| 2024-05-23 | A Survey on Vision-Language-Action Models for Embodied AI | http://arxiv.org/abs/2405.14093 | <details><summary>展开</summary>待生成</details> |
| 2024-05-09 | Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations | http://arxiv.org/abs/2405.06039 | <details><summary>展开</summary>待生成</details> |
