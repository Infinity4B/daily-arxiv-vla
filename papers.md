| 日期 | 标题 | 链接 | 简要总结 |
| --- | --- | --- | --- |
| 2025-09-25 | RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models | http://arxiv.org/abs/2509.21243v1 | <details><summary>展开</summary>论文提出**RetoVLA**，一种轻量级视觉-语言-动作（VLA）模型，通过**重用Vision Transformer中的Register Tokens增强空间推理能力**，解决传统轻量化方法导致的性能下降问题。核心要点如下： ### 1. **核心创新** - **Register Tokens再利用**：传统方法将Register Tokens视为“净化器”（吸收ViT中的异常信息后丢弃），本文将其重新定义为**空间上下文提供者**。这些tokens包含场景的全局空间信息（如物体3D布局、工作空间结构）。 - **空间上下文注入**：设计新模块，将Register Tokens作为Key-Value对直接注入动作专家（Action Expert）的注意力层，使模型同时利用高层语义特征与全局空间信息生成动作。 - **自适应门控**：引入可学习标量门控（经Sigmoid激活），动态调节Register Tokens的影响强度，避免全局信息干扰需局部精度的任务。 ### 2. **架构设计** - **双流信息流**： （1）VLM主干提取语义特征（仅用前一半层以降低计算量）； （2）Register Tokens通过空间聚合器生成场景相关表示，注入动作专家的交叉注意力层。 - **训练目标**：采用条件流匹配（Conditional Flow Matching），将噪声动作序列逐步优化至真实动作，结合视觉/语言上下文。 ### 3. **关键优势** - **效率与性能平衡**：在轻量化VLM（如SmolVLA）基础上，仅增加少量计算开销即显著提升空间推理能力，避免传统方法的信息损失。 - **任务适应性**：门控机制使模型自动区分需全局理解的任务（如长期规划）与需局部精度的任务。 ### 4. **实验结果** - **真实机器人（7-DOF机械臂）**：在7类复杂操作任务（如堆叠积木、关闭抽屉）上，平均成功率从**50.3%提升至67.4%**（+17.1%），其中长视距任务（如摆放多米诺骨牌）提升最高达**28%**。 - **LIBERO基准测试**：在需工作记忆（+11.5%）和3D空间推理（+9.0%）的任务上表现突出，但局部精确操作任务略有下降。 - **仿真实验**：在自定义Unity/MuJoCo环境中，平均成功率提升**12%**（62.8% → 74.8%），验证方法的泛化性。 ### 5. **结论与意义** - **重新定义信息流**：证明被丢弃的Register Tokens是提升机器人空间智能的关键资源，为轻量化高性能VLA模型提供新范式。 - **开源承诺**：将发布代码、模型权重及机器人硬件规格以促进复现。 > **局限**：局部精细操作任务存在性能权衡，未来需优化门控机制或融合策略。 > **视频演示**：[https://youtu.be/2CseBR-snZg](https://youtu.be/2CseBR-snZg)</details> |
| 2025-09-25 | Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning | http://arxiv.org/abs/2509.21126v1 | <details><summary>展开</summary>本文提出**VARL框架**（VLM作为在线强化学习的动作建议器），通过利用视觉语言模型（VLM）生成动作建议提升强化学习（RL）的样本效率。核心要点如下： ### 1. **问题背景** - 在线RL在复杂任务中样本效率低，需大量交互学习最优策略。 - 现有方法局限：VLM策略在低级控制表现不足；微调需专家演示；VLM奖励设计依赖模型偏好准确性且计算开销大。 ### 2. **VARL框架设计** - **核心创新**：VLM作为**动作建议器**（非奖励设计器），提供启发式动作引导RL探索。 - **双组件结构**： - **VLM动作生成器**：基于当前状态和任务描述生成候选动作（图2），存储至启发式缓冲区。 - **策略塑形**：将VLM建议动作融入策略更新（公式8），通过门控函数（公式4,7）避免过度拟合次优动作，并在训练后期移除启发式信号（固定步数 \(N_s\)）。 - **优势**：保证RL收敛性，避免奖励函数偏差；显著减少VLM查询量（仅需3次，表1）。 ### 3. **关键优势** - **提升样本效率**：在稀疏/稠密奖励任务中，收敛速度均优于SAC及SAC+专家数据（图4），尤其适用于事件驱动型奖励。 - **低计算开销**：相比奖励塑形方法（如RL-VLM-F、ERL-VLM），VLM调用量减少99%以上（表1）。 - **任务通用性**：在10种仿真/现实任务验证（图3），支持离散/连续动作空间及视觉/状态输入（Meta-World、AI2-THOR、真实机械臂任务）。 - **现实可行性**：直接在真实环境（RM-65B机械臂）实现在线RL，无需专家演示或环境建模（目标抓取任务仅需3000次交互）。 ### 4. **实验验证** - **效率对比**：VARL在7种环境中样本效率显著优于基线（图4）。 - **现实部署**：成功学习真实世界推方块（30k–50k次交互）和位置到达任务（3k次交互）。 - **超参数鲁棒性**：对动作权重系数 \(\lambda\) 和启发式移除步数 \(N_s\) 不敏感（图6）。 ### 5. **结论** VARL通过VLM动作建议增强探索多样性，解决在线RL样本效率问题，同时降低计算成本，为无演示真实环境RL提供可行路径。未来将扩展至更复杂任务及视频生成技术融合。</details> |
| 2025-09-25 | AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation | http://arxiv.org/abs/2509.21006v1 | <details><summary>展开</summary>论文提出**AnywhereVLA框架**，用于未知室内环境中的语言驱动移动操作机器人系统。核心要点如下： 1. **模块化架构**： - 将自然语言指令解析为任务图，驱动**SLAM（LiDAR+相机）**、**3D语义建图（带置信度）** 和**主动环境探索（AEE）** 模块。 - 目标检测后，**路径规划模块**选择可达的抓取位姿。 - **轻量化操作头（SmolVLA）** 基于微调的抓取数据集生成操作动作。 2. **关键技术**： - **语义建图**：融合LiDAR点云与目标检测，通过插值解决稀疏性问题（图3→图4），结合多视角数据估计目标置信度（公式2）。 - **主动探索**：基于前沿检测算法（算法2），语言指令驱动机器人搜索未知区域直至定位目标物体。 - **嵌入式部署**：在**Jetson Orin NX**（感知与VLA）和**Intel NUC**（SLAM与控制）上实现全系统实时运行（≥10Hz）。 3. **实验验证**： - 在动态多房间实验室测试50次任务，整体成功率**46%**（VLA操作模块微调后成功率85%）。 - 表II显示各模块成功率：探索（75%）、导航（90%）、目标检测（85%）、VLA操作（80%）。 - 5米半径内平均任务时间**133秒**（图7），10米半径内小于10分钟。 4. **优势与局限**： - 结合**经典导航的鲁棒性**与**VLA的语言泛化能力**，开源代码/模型/数据集。 - 局限：无法处理复杂空间约束（如“桌上的瓶子”）；探索失败率25%（狭窄空间）。 --- **结论**：AnywhereVLA通过模块化设计平衡了导航可靠性与语言操作的灵活性，为大规模室内移动操作提供了可行方案。未来需提升空间语义理解能力。</details> |
| 2025-09-25 | ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation | http://arxiv.org/abs/2509.20841v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-24 | Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving | http://arxiv.org/abs/2509.20109v1 | <details><summary>展开</summary>论文提出了一种名为ReflectDrive的新型端到端自动驾驶框架，其核心创新点包括： 1. **离散扩散模型的应用** - 首次将离散扩散模型引入自动驾驶轨迹生成，通过将二维驾驶空间离散化为动作码本，将连续轨迹表示为离散token序列。 - 利用预训练扩散语言模型（DLMs）进行轨迹规划微调，支持并行解码和双向特征融合。 2. **安全反射机制** - **目标条件生成**：首先生成多模态候选轨迹（通过目标点采样和NMS过滤），再基于全局评分器选择最优轨迹。 - **安全引导再生**：对不安全轨迹点进行局部搜索，找到可行解作为"安全锚点"，通过扩散修复（inpainting）重构轨迹，无需梯度计算。 - 评分函数体系：包含全局安全评分（$S_{\text{global}}$）、局部安全检测（$S_{\text{safe}}$）和轨迹质量评估（$S_{\text{local}}$），确保硬性安全约束（如碰撞避免、可行驶区域合规）。 3. **实验验证** - 在NAVSIM真实驾驶基准测试中，ReflectDrive的PDMS分数达91.1（仅相机输入），显著优于基线模型（+6.3分）。 - 安全指标突出：可行驶区域合规率（DAC）达99.3%，接近人类水平（100%）；配合精确环境信息时，综合性能达人类驾驶的94.7%。 该方法通过离散token空间的高效搜索与修复机制，解决了模仿学习模型难以保障物理安全的问题，为可验证的端到端自动驾驶提供了新范式。</details> |
| 2025-09-24 | FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models | http://arxiv.org/abs/2509.19870v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-24 | Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training | http://arxiv.org/abs/2509.19752v1 | <details><summary>展开</summary>论文提出一种基于扩散强化学习的数据生成方法，用于替代人类演示数据训练视觉-语言-动作（VLA）模型。核心要点如下： 1. **问题背景** VLA模型依赖大规模人类演示数据，但人工收集成本高且数据存在高方差和多模态问题。传统强化学习（RL）在长视野稀疏奖励任务中生成的数据质量低。 2. **方法创新** - **扩散策略优化**：采用扩散模型作为策略表示，通过迭代去噪过程生成平滑、低方差动作序列，结合PPO算法进行在线微调。 - **两阶段训练**： - **阶段1**：多模态行为克隆（BC）预热，利用少量人类数据初始化策略。 - **阶段2**：扩散PPO优化，通过稳定化技术（DDIM采样器、余弦退火学习率、多样化经验回放）提升训练效率。 3. **关键优势** - **高质量数据**：扩散RL生成的数据比人类演示更高效（轨迹长度缩短23%）、更平滑（运动jerk值降低38%）、更一致（动作方差最低）。 - **性能提升**：在LIBERO基准（130个任务）上，仅用扩散RL数据训练的VLA模型达到**81.9%**平均成功率，超越人类数据（+5.3%）和高斯RL数据（+12.6%）。 - **泛化能力**：人类数据与扩散RL数据混合训练，在OOD任务上成功率达**5.2%**（单一数据源最高仅2.06%）。 4. **实验验证** - 架构设计：ResNet+U-Net优于ViT+MLP，防止策略崩溃。 - 采样效率：DDIM（5步）比DDPM快10倍且性能相当。 - 数据多样性：并行环境收集数据避免模式坍塌。 5. **结论** 扩散RL可生成高质量、低方差数据，显著提升VLA模型性能，为减少人类数据依赖提供有效解决方案。</details> |
| 2025-09-23 | Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action | http://arxiv.org/abs/2509.19571v1 | <details><summary>展开</summary>论文提出了一种名为Agentic Scene Policies (ASP) 的机器人策略框架，旨在解决开放词汇自然语言指令的执行问题。其核心创新点包括： 1. **框架设计** ASP 通过结构化场景表示（ObjectMap）统一处理空间、语义和功能特性（affordances）。该框架包含： - **对象地图**：整合物体几何、语义特征（如CLIP嵌入）和交互功能（如把手部位及对应技能） - **LLM智能体**：将用户指令分解为工具调用序列 - **工具系统**：提供物体检索、空间推理和交互功能（如抓取/推动特定功能部位） 2. **关键技术突破** - **零样本交互**：利用基础模型（Gemini）检测物体功能部位（如按钮/把手），并映射到预定义技能（如`tip_push`按压） - **移动扩展**：支持房间级任务，结合导航（基于功能朝向规划路径）与操作，通过多视角建图实现跨场景物体重定位 - **模块化验证**：在15项桌面操作任务中，ASP 成功率显著超过端到端视觉语言动作模型（VLA），尤其在需精细操作的任务（如拔除图钉/开抽屉）优势达40%以上 3. **实验验证** - 对比VLA基线（如π₀-FAST）显示：ASP在复杂指令理解（空间关系/功能交互）和零样本泛化性上更具优势 - 消融实验证明功能检测是关键：移除该模块后任务成功率平均下降21% - 移动场景测试验证了框架可扩展性（如"将鸡蛋放入锅"等需导航+操作的任务） 4. **局限与展望** 当前技能库限于基础操作（抓取/推拉），未来需结合学习策略处理长时序任务（如折叠衣物）。场景表示的动态更新也是重要改进方向。 ASP 的核心价值在于通过显式场景表示弥合语言指令与机器人动作的鸿沟，为开放场景任务提供可解释且可扩展的解决方案。</details> |
| 2025-09-23 | OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation | http://arxiv.org/abs/2509.19480v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-23 | Pure Vision Language Action (VLA) Models: A Comprehensive Survey | http://arxiv.org/abs/2509.19012v2 | <details><summary>展开</summary>这篇综述论文《Pure Vision Language Action (VLA) Models: A Comprehensive Survey》系统性地总结了视觉语言动作模型的研究进展，要点如下： 1. **研究背景与目标** - VLA模型将传统机器人控制范式转变为通用化智能体，整合视觉感知、语言理解和动作生成，实现复杂环境中的主动决策。 - 论文提出首个针对纯VLA方法的分类框架，填补现有研究空白。 2. **核心分类框架** VLA方法分为四大范式： - **自回归模型**：基于Transformer架构，通过序列建模生成动作（如Gato、RT系列）。优势在于跨任务泛化能力，但存在误差累积和计算延迟问题。 - **扩散模型**：将决策建模为条件生成过程（如Diffusion Policy），支持多模态目标条件，但需解决推理效率瓶颈。 - **强化微调模型**：结合RL优化策略，提升策略稳定性和样本效率。 - **混合架构与专业模型**：针对特定场景（如手术机器人、无人机）设计多模态融合方案。 3. **关键资源体系** - **数据集**：Open X-Embodiment（跨21机构527技能）、BridgeData等标准化数据集推动模型训练。 - **仿真平台**：THOR、Habitat、CARLA等提供可扩展的虚拟测试环境。 - **硬件平台**：涵盖机械臂、四足机器人、人形机器人等多形态载体。 4. **挑战与未来方向** - **核心挑战**：机器人数据稀缺性、架构异构性、实时推理成本、人机伪交互、评估体系局限。 - **突破方向**：世界模型与跨模态统一、因果推理突破、虚实数据融合、可信生态系统构建。 5. **贡献总结** - 建立首个VLA方法分类学，系统分析300+研究成果。 - 阐明各范式的技术演进路径（如自回归模型从基础token化→大规模实机训练→语义推理集成）。 - 指出开放性难题：长时程任务稳定性、安全部署机制、多模态对齐鲁棒性。 该综述为通用具身智能的发展提供了方法论基础和技术路线图，强调需在数据生成、计算效率和安全验证三方面取得突破。</details> |
| 2025-09-23 | Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations | http://arxiv.org/abs/2509.18953v1 | <details><summary>展开</summary>该论文提出Eva-VLA框架，首次系统评估视觉-语言-动作（VLA）模型在真实物理变化下的鲁棒性。核心贡献如下： 1. **问题定义**：针对VLA模型在真实部署中面临的物理变化脆弱性问题（如物体位姿变化、光照干扰、对抗性补丁），提出首个统一评估框架。 2. **方法创新**： - 将物理变化分解为三类连续参数化问题： * 物体3D变换（旋转角α/β/γ） * 光照变化（位置x/y、半径σ、强度I） * 对抗补丁位置（Δx/Δy） - 采用黑盒优化算法CMA-ES搜索最坏场景，避免真实数据采集成本 3. **实验结果**： - 在OpenVLA等先进模型上验证，所有物理变化均导致>60%失败率 - 物体3D变换对长时任务造成最高97.8%失败率 - 光照变化和对抗补丁分别导致62.6%和71.2%平均失败率 - 物理实验证实44.6%攻击成功率 4. **揭示问题**：实验暴露VLA模型在实验室环境与真实部署间的严重差距，证明当前模型对物理变化极度敏感，尤其长时任务存在级联失效风险。 该框架为提升VLA模型鲁棒性提供了评估基准和优化路径。</details> |
| 2025-09-23 | Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation | http://arxiv.org/abs/2509.18865v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-22 | Latent Action Pretraining Through World Modeling | http://arxiv.org/abs/2509.18428v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-22 | PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies | http://arxiv.org/abs/2509.18282v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-22 | Prepare Before You Act: Learning From Humans to Rearrange Initial States | http://arxiv.org/abs/2509.18043v1 | <details><summary>展开</summary>这篇论文提出了一种名为ReSET的算法，旨在通过模仿人类行为改善机器人模仿学习（IL）在环境状态超出训练分布时的鲁棒性。核心要点如下： ### 1. **问题背景** - IL策略在目标物体位置异常或被遮挡时容易失败，直接收集大量数据解决该问题效率低下。 - 人类在面临非常规初始状态时，会先**重构环境**（如移开障碍物、调整物体位姿）简化任务，再进行操作。 ### 2. **ReSET算法核心思想** - **分阶段策略**： - **简化策略（Reduction Policy）**：将复杂初始状态（如遮挡场景）转化为**锚状态（Anchor States）**（如移除障碍物后的状态），使环境分布更集中（降低方差）。 - **任务策略（Task Policy）**：在锚状态上执行原任务（如抓取杯子）。 - **理论支撑**：锚状态满足 $\text{tr}(\Sigma_a) < \text{tr}(\Sigma_0)$（状态协方差矩阵迹更小），可降低泛化误差上界（公式6），提升数据效率。 ### 3. **关键技术组件** - **评分网络**： 基于人类视频训练，评估当前状态是否适合直接执行任务策略（低分表示适合执行）。 - **点流生成网络**： 从人类视频中提取物体运动轨迹（点流 $\mathcal{T}$），预测如何重构环境（如移开障碍物的路径）。 - **简化策略**： 将点流映射为机器人可执行的动作基元（如抓取、推动、旋转）。 ### 4. **实验验证** - **任务场景**：抓取遮挡物体、旋转工具、多任务操作等（图4）。 - **结果**： - 在分布外状态上，ReSET成功率（平均85%）显著高于扩散策略（65%）、Dynamics-DP等基线（图5）。 - 仅需 **20分钟人类视频 + 20分钟机器人通用操作数据**，达到扩散策略 **70次专家演示** 的效果，数据效率更高。 ### 5. **贡献总结** - **理论**：证明分阶段策略（环境重构→任务执行）可降低泛化误差。 - **方法**：结合人类视频（动作无关）与机器人数据（任务无关），实现环境重构决策、运动预测与动作生成。 - **效果**：在少量数据下提升任务鲁棒性，适用于长视野、多任务场景。 > 论文链接：[https://reset2025paper.github.io](https://reset2025paper.github.io)</details> |
| 2025-09-20 | ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering | http://arxiv.org/abs/2509.16680v1 | <details><summary>展开</summary>论文提出**ProtoVQA框架**，用于可解释的细粒度视觉问答（VQA），核心要点如下： 1. **研究动机** - VQA在医疗诊断、自动驾驶等安全关键领域应用时，需模型提供**可验证的解释**，而现有方法（如注意力可视化）难以忠实反映决策过程。 - 原型学习在视觉任务中可增强可解释性，但多模态场景下存在**视觉-语言语义鸿沟**、**几何变化建模不足**等问题。 2. **核心框架ProtoVQA** - **问题感知原型**：将问题令牌重塑为 \(m \times k \times D\) 的3D张量（\(m\) 个原型，每个含 \(k\) 个子块），作为语义锚点连接答案与图像区域。 - **空间约束贪婪匹配**：通过迭代选择相似度最高的图像块-子块对（公式2-3），并施加空间连续性约束（半径 \(r=3\)），确保证据区域语义相关且空间连贯。 - **双模态答案处理**： - **Type 1（视觉定位）**：直接投影坐标至特征空间。 - **Type 2（描述性QA）**：冻结的权重共享投影器处理文本答案，避免过拟合。 - **共享骨干网络**：统一原型框架同时支持VQA和定位任务。 3. **评估指标VLAS** - 提出**视觉-语言对齐分数（VLAS）**，衡量模型关注区域与真实证据的重叠度（IoU>0.5），优于传统像素级指标，更符合人类解释评估需求（公式5）。 4. **实验结果** - **数据集**：Visual7W（32.8万QA对，47K图像）。 - **性能**： - 准确率70.23%（表1），与ViT骨干的基线模型（如Bi-CMA 70.53%）相当。 - **VLAS@1达0.4103**，较最佳基线提升66.4%；VLAS@3提升119.6%（表2），证明解释忠实性显著领先。 - **定性可视化**：匹配的局部区域（图2）与真实答案框高度对齐，展示细粒度推理能力（如物体部件、空间关系）。 5. **意义与局限** - 首次将原型学习扩展至多模态VQA，提供**透明推理路径**。 - 局限：未探索生成式VQA；领域迁移（如医疗）需进一步适配。 **总结**：ProtoVQA通过问题驱动原型匹配和空间约束机制，在保持竞争性准确率的同时，显著提升解释可信度，推动可信VQA系统发展。</details> |
| 2025-09-19 | Randomized Smoothing Meets Vision-Language Models | http://arxiv.org/abs/2509.16088v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-19 | CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine | http://arxiv.org/abs/2509.15968v1 | <details><summary>展开</summary>论文提出**CoReVLA框架**，旨在提升自动驾驶在**长尾场景**（罕见但高危场景）中的性能。核心要点如下： 1. **问题背景** - 现有自动驾驶系统在长尾场景（如突发事故、极端天气）中表现不佳，导致安全事故。 - 视觉语言动作模型（VLA）虽具推理能力，但受限于**高质量长尾数据稀缺**和**稀疏数据下训练效率低**。 2. **解决方案：双阶段框架** - **阶段1（Collect）**： - 混合开源驾驶QA数据集（LingoQA、BDD、HAD）对Qwen2.5-VL-7B模型进行监督微调（SFT），构建基础场景理解能力。 - 在CAVE仿真平台进行人机协同测试，收集人类接管失败的场景数据（含视觉输入、接管行为及注意力）。 - **阶段2（Refine）**： - 利用接管数据，通过**直接偏好优化（DPO）** 对齐人类行为偏好：将模型失败行为（次优）与人类接管行为（最优）对比，避免手动设计奖励函数导致的奖励黑客问题。 3. **实验结果** - **开环QA评估**：在LingoQA/BDD/HAD数据集上，BLEU和ROUGE分数显著优于基线（如Llava、Impromptu）。 - **闭环驾驶测试**（Bench2Drive基准）： - 驾驶评分（DS）达 **72.18**，成功率（SR）达 **50%**，较SOTA方法提升 **7.96 DS** 和 **15% SR**。 - 案例验证：模型能通过接管数据持续优化行为，避免相似场景的重复失败（如雨天避让故障车辆、行人突发横穿）。 4. **贡献与资源** - 提出首个结合**CAVE平台接管数据收集**与**DPO行为优化**的端到端框架。 - 开源代码、数据集及场景配置：[https://github.com/FanGShiYuu/CoReVLA](https://github.com/FanGShiYuu/CoReVLA)。 > 总结：CoReVLA通过**数据收集+行为精炼**双阶段设计，高效利用人类接管数据优化长尾场景决策，显著提升自动驾驶安全性与泛化能力。</details> |
| 2025-09-19 | A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning | http://arxiv.org/abs/2509.15937v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-18 | RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation | http://arxiv.org/abs/2509.15212v1 | <details><summary>展开</summary>论文提出RynnVLA-001模型，通过人类演示视频提升机器人操作能力。核心方法包括两阶段预训练： 1. **自我中心视频生成预训练**：在1200万自我中心操作视频上训练图像到视频模型，基于初始帧和语言指令预测未来帧，学习操作动态。 2. **人类中心轨迹感知建模**：扩展模型，联合预测未来帧和关键点轨迹，桥接视觉预测与动作表示。 引入**ActionVAE**（变分自编码器）压缩动作序列为紧凑嵌入，简化输出空间并确保动作连贯性。 在相同机器人数据集上微调后，模型优于最先进基线（如GR00T N1.5和Pi0），验证了预训练策略的有效性。</details> |
| 2025-09-18 | Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue | http://arxiv.org/abs/2509.15061v2 | <details><summary>展开</summary>待生成</details> |
| 2025-09-18 | Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale | http://arxiv.org/abs/2509.14932v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-18 | CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human | http://arxiv.org/abs/2509.14889v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-18 | RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI | http://arxiv.org/abs/2509.14687v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-18 | VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation | http://arxiv.org/abs/2509.18183v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-17 | CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping | http://arxiv.org/abs/2509.14143v1 | <details><summary>展开</summary>本文提出 **CLAW框架**（一种视觉-语言-动作模型），用于解决机器人抓取任务中**重量感知控制**的难题。以下是核心要点： ### 1. **问题背景** - 现有视觉-语言-动作（VLA）模型（如π₀）虽能生成连续动作，但**难以精确满足数值阈值约束**（如按重量停止抓取），因其端到端设计缺乏显式状态监控机制。 ### 2. **CLAW框架设计** - **核心创新**：**解耦状态监控与动作生成**。 - **轻量级监控模块**：微调CLIP模型，实时读取秤盘图像并生成二元提示（`继续`/`停止`），基于任务指定的重量阈值。 - **动作生成模块**：微调后的π₀模型接收CLIP提示+多视角图像，输出连续机器人动作。 - **优势**：结合符号化重量推理与高频运动控制，支持实时响应。 ### 3. **关键技术** - **CLIP微调**：构建含2000张秤盘图像的训练集，通过数值比较任务学习生成精准提示。 - **π₀微调**：收集50组演示数据，人工标注抓取/停止阶段的提示标签，训练π₀响应提示指令。 ### 4. **实验验证** - **任务场景**： - 单物体抓取（糖果/大蒜，目标重量20g/30g/40g）。 - 混合物体抓取（指定物体+重量）。 - **结果**： - CLAW在**动作执行成功率**和**重量停止准确率**均达**100%**（表I）。 - 显著优于基线（原始π₀动作成功率≤35%，停止率≈0%；微调π₀虽动作成功率达100%，但停止准确率仅0-35%）。 - **鲁棒性**：即使重量突发波动（图5），CLAW仍能即时调整动作。 ### 5. **贡献总结** - 提出首个重量感知VLA框架，通过显式监控提升控制精度。 - 验证CLIP作为轻量提示生成器的可行性。 - 实现π₀在提示监督下的高效动作生成。 - 在单/多物体任务中均表现鲁棒。 ### 6. **未来方向** - 增强秤盘定位鲁棒性（无需手动裁剪）。 - 扩展非数值停止条件（如时间、视觉形态）。 - 融合多模态输入（声音、触觉）。 > **关键价值**：CLAW为机器人执行需精确量化约束的任务（如按重配料、药品分装）提供了新范式。</details> |
| 2025-09-17 | SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model | http://arxiv.org/abs/2509.14138v1 | <details><summary>展开</summary>SeqVLA 提出一种基于完成感知的视觉-语言-动作（VLA）模型，用于解决长时程操作任务中的顺序执行问题。核心要点如下： 1. **问题与动机**：现有 VLA 模型（如 π₀）擅长连续控制，但缺乏子任务完成判断能力，导致长时程任务（如多步骤打包）中错误累积和顺序混乱。 2. **模型设计**： - **双头架构**：在 π₀ 基础上增加轻量级**完成检测头**（Completion Detection Head），共享动作专家层的特征。 - **功能**：主头输出机器人动作；检测头实时预测当前子任务完成概率（二元分类），触发自主任务切换。 - **损失函数**：总损失 = 动作损失（π₀ 的流匹配损失） + λ · 完成检测损失（二元交叉熵，λ=0.1）。 3. **微调策略**： - 对比四种策略：**联合 vs. 顺序微调** × **主干网络全微调 vs. 冻结**。 - **最优方案**：联合微调且不冻结主干（SeqVLA-J），其完成检测置信度最高（熵值最低），KS统计量显著（p<0.001）。 4. **实验验证**： - **任务**：沙拉打包（7个子任务）和糖果打包（4个子任务）。 - **结果**： - SeqVLA 显著超越基线 π₀，消除顺序错误（如重复执行或跳步）。 - SeqVLA-J 在子任务成功率（图6）和长时程整体成功率（图10）上均最优。 5. **贡献**： - 首次在 VLA 模型中集成完成检测机制，实现自主子任务切换。 - 明确最佳微调策略（联合+非冻结主干），提升长时程任务鲁棒性。 - 在真实机器人任务中验证有效性，为顺序操作提供新解决方案。 **未来方向**：探索层次化任务分解、人机协作场景及动态任务序列适应能力。</details> |
| 2025-09-17 | GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model | http://arxiv.org/abs/2509.14117v2 | <details><summary>展开</summary>本文提出GeoAware-VLA模型，旨在解决视觉-语言-动作（VLA）模型在摄像机视角变化时泛化能力差的问题。核心方法是用预训练的几何视觉模型VGGT作为冻结的特征提取器，替代传统可训练视觉编码器，并添加轻量级可训练投影层适配策略解码器。关键贡献如下： 1. **几何感知设计**： 引入VGGT模型提取隐含几何特征，使策略无需从零学习3D一致性，显著提升视角不变性。 2. **性能提升**： - 在LIBERO基准测试中，新视角下的零样本成功率提升超2倍（平均82.6% vs 基线37.9%-50.2%）。 - 真实机器人实验验证有效性，尤其在未训练视角下成功率显著高于基线（如任务5提升27%）。 3. **技术优势**： - 兼容连续（MLP）和离散（VQ-BeT）动作空间，模型轻量（仅投影层可训练）。 - 消融实验表明均匀选择VGGT中间层特征最有效，优于仅用末层或全层。 4. **实验验证**： - 模拟环境：跨4类任务集（Spatial/Object/Goal/Long）均优于基线（如Long任务新视角成功率47.3% vs 基线3.7%）。 - 真实场景：5项操作任务成功率高，证明几何先验提升策略鲁棒性。 结论指出，几何感知是提升VLA泛化能力的关键，该方法为构建适应视角变化的通用机器人策略提供有效路径。</details> |
| 2025-09-17 | Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach | http://arxiv.org/abs/2509.13774v1 | <details><summary>展开</summary>本文提出了一种人机协作的双执行器VLA模型微调框架，用于解决复杂机器人任务中的性能瓶颈。核心要点如下： 1. **双执行器架构**： - **主执行器**：基于扩散策略生成鲁棒的多任务动作 - **精炼执行器**：在潜在噪声空间进行细粒度调整，接受语言指令引导 2. **"说调"人机交互机制**： - 将物理干预（如SpaceMouse操作）实时转化为语义化语言指令（如"向右移动"） - 构建"说-调"数据集，同时优化主执行器的基线策略和精炼执行器的指令响应能力 3. **多任务学习策略**： - 采用共享执行器+任务特定评论家架构 - 引入自适应Q值加权机制平衡不同任务的学习进度 4. **实验效果**： - 真实机器人多任务测试：101分钟在线微调后在螺栓直立放置/抓取/装配三个任务均达100%成功率 - 长时程任务：12步连续操作中维持50%成功率 - 多机器人扩展：双机器人并行训练实现2倍效率提升 - 模型泛化性：在Octo和SmolVLA等不同VLA骨干网络上均有效 5. **局限**： - 完全遮挡场景下性能受限 - 长序列任务存在误差累积 - 当前仍需人类干预保障安全性 该方法通过语义化人机交互和分层策略优化，显著提升了VLA模型在复杂机器人任务中的适应效率和执行精度。</details> |
| 2025-09-17 | AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving | http://arxiv.org/abs/2509.13769v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-16 | The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning | http://arxiv.org/abs/2509.12594v2 | <details><summary>展开</summary>待生成</details> |
| 2025-09-15 | TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning | http://arxiv.org/abs/2509.11839v2 | <details><summary>展开</summary>TrajBooster 是一个基于轨迹中心的跨形态框架，旨在利用丰富的轮式人形机器人数据提升双足人形机器人的视觉-语言-动作（VLA）模型性能。其核心创新点包括： 1. **轨迹作为形态无关接口** 以末端执行器的 6D 轨迹为通用媒介，从轮式人形机器人（如 Agibot）提取真实轨迹，经尺度归一化处理适配目标双足机器人（Unitree G1）。 2. **仿真中的分层重定向模型** - **架构**：分解为手臂策略（逆运动学生成关节角度）、管理者策略（输出基座速度/躯干高度指令）和工作者策略（执行指令控制腿部关节）。 - **训练**：采用启发式增强的协调在线 DAgger 算法，利用仿真特权信息生成可行全身动作，高效生成大规模目标机器人兼容动作数据。 3. **两阶段 VLA 微调** - **后预训练**：使用重定向数据构建异构三元组（源视觉/语言 + 目标动作），对齐 VLA 动作分布。 - **后训练**：仅需 10 分钟真实遥操作数据微调，显著降低数据收集成本。 4. **关键成果** - **加速适应**：比仅用真实数据训练快 3 倍以上，实现跨高度操作（如蹲起、抓取）。 - **增强泛化**：在物体位置分布外场景的成功率提升至 80%（基线为 0%）。 - **零样本迁移**：解锁未见于遥操作的任务（如 "传递水杯"）。 - **鲁棒性**：在真实部署中展现协调全身运动与动态平衡能力。 **贡献总结**：首次通过轨迹重定向实现双足人形全身操作的 VLA 模型，减少对昂贵同形态数据的依赖，提升动作空间理解与零样本泛化能力。</details> |
| 2025-09-15 | Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs | http://arxiv.org/abs/2509.11480v1 | <details><summary>展开</summary>本文评估了视觉-语言-动作（VLA）模型在边缘设备（如NVIDIA Jetson AGX Orin）与数据中心GPU（如H100、A100）上的跨平台性能扩展。通过LIBERO基准测试五种代表性VLA模型（包括新提出的VOTE和QwenVLA），测量准确率、延迟、吞吐量和内存占用。主要发现： 1. **架构选择影响性能**：动作标记化（如VOTE的少令牌设计）和主干大小（如QwenVLA的小型主干）显著优化吞吐量和内存占用。 2. **边缘功率约束下的非线性下降**：在低功率模式（如15W），性能下降明显，但高功率边缘配置（如Orin MAX模式）可匹配旧数据中心GPU（如V100）。 3. **高吞吐量可行**：VOTE变体（如VOTE-MLP4）在保持高准确率（VOTE-1T达96.9%）的同时，实现高吞吐量（数据中心达474.78 Hz），无需显著牺牲精度。 这些结果为VLA模型在资源受限场景（如机器人部署）的选择和优化提供了实用指导，挑战了数据中心硬件绝对优越的假设。</details> |
| 2025-09-14 | Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations | http://arxiv.org/abs/2509.11417v2 | <details><summary>展开</summary>待生成</details> |
| 2025-09-13 | OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft | http://arxiv.org/abs/2509.13347v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-11 | SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning | http://arxiv.org/abs/2509.09674v1 | <details><summary>展开</summary>论文提出了一种名为**SimpleVLA-RL**的强化学习框架，用于提升视觉-语言-动作（VLA）模型的训练效率和泛化能力。核心要点如下： 1. **解决VLA模型的关键挑战** - **数据稀缺性**：传统监督微调（SFT）依赖大量人类操作轨迹，成本高昂且难以扩展。 - **泛化能力差**：模型在分布偏移任务（如场景/物体/目标变化）中表现显著下降。 2. **框架设计创新** - **高效在线RL系统**：基于veRL改进，支持并行环境交互、轨迹采样和损失计算优化。 - **探索增强策略**： - **动态采样**：仅保留混合成功/失败的轨迹组，确保非零梯度。 - **高温采样**（T=1.6）和**放宽PPO截断范围**（[0.8, 1.28]），提升策略多样性。 - **简化奖励机制**：仅用二元结果奖励（1/0），避免复杂过程奖励设计。 3. **显著性能提升** - **基准测试结果**： - **LIBERO**：平均成功率从91%提升至99.1%，长时任务（LIBERO-Long）从17.1%升至98.5%。 - **RoboTwin 1.0/2.0**：双臂任务提升30.6%（1.0）和30.5%（2.0），超越π₀等SOTA模型。 - **数据效率**：仅需单任务演示即可大幅提升性能（如LIBERO-Long从17.1%→91.7%）。 4. **关键发现** - **"Pushcut"现象**：RL训练中自发涌现新动作模式，超越监督数据中的策略。 - **泛化能力**：在空间/物体/目标变化任务中表现优异，且仿真策略可直接迁移至真实机器人（sim-to-real）。 5. **贡献总结** - 降低对大规模数据的依赖，提升长时规划与跨场景泛化能力。 - 首次在VLA模型中验证纯结果奖励RL的有效性，为VLA训练提供新范式。 > 论文链接：[SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674) > 代码开源：[PRIME-RL/SimpleVLA-RL](https://github.com/PRIME-RL/SimpleVLA-RL)</details> |
| 2025-09-11 | VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model | http://arxiv.org/abs/2509.09372v2 | <details><summary>展开</summary>待生成</details> |
| 2025-09-11 | SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models | http://arxiv.org/abs/2509.09090v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-10 | RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation | http://arxiv.org/abs/2509.08820v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-09 | TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models | http://arxiv.org/abs/2509.07962v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-09 | Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation | http://arxiv.org/abs/2509.07957v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-08 | F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions | http://arxiv.org/abs/2509.06951v2 | <details><summary>展开</summary>本文提出了一种名为 \(\mathcal{F}_{1}\) 的视觉-语言-动作（VLA）模型，通过集成视觉预见生成机制，弥合感知、预测与执行之间的鸿沟。其核心要点如下： ### 1. **模型架构创新** - **三专家混合架构**：采用 Mixture-of-Transformer 结构，包含三个专用模块： - **理解专家**：基于预训练视觉语言模型（如 PaliGemma），对齐语言指令与视觉观测。 - **生成专家**：通过多尺度残差 VQ-VAE 编码历史观测，生成目标导向的视觉预见图像（\(\hat{o}_{t+1}\)），作为显式规划目标。 - **动作专家**：基于预测逆动力学模型（PIDM），将动作生成转化为视觉目标驱动的逆动力学问题，输出动作序列 \(\hat{a}_{t:t+k}\)。 - **渐进注意力机制**（UGA）：跨专家信息流遵循"理解→生成→动作"的因果层级，避免信息回流，确保稳定性。 ### 2. **训练策略** - **三阶段渐进式训练**： - **阶段 I**：对齐生成专家与预训练的理解专家，通过教师强制学习视觉预见。 - **阶段 II**：在大规模机器人数据集（330k 轨迹，136 任务）上联合优化所有专家，学习通用视觉运动知识。 - **阶段 III**：任务特定数据上微调，适应新机器人本体。 - **损失函数**：结合自回归视觉预测损失（\(\mathcal{L}_{\mathrm{gen}}^{\mathrm{pred}}\)）与流匹配动作损失（\(\mathcal{L}_{\mathrm{action}}\)），强化预测与执行的协同。 ### 3. **关键优势** - **视觉预见引导决策**：通过生成未来观测作为中间目标，将反应式策略转化为基于规划的决策，提升长视野任务鲁棒性。 - **高效多尺度预测**：采用"下一尺度预测"机制，平衡计算效率与生成质量（4 尺度为最优）。 - **强泛化能力**：三阶段训练注入可迁移的视觉预见能力，支持动态环境适应。 ### 4. **实验结果** - **真实任务**：在 9 项 Genie 机器人任务中，平均成功率 **82.2%**（最高任务 **100%**），显著优于 \(\pi_0\)（65.2%）和 gr00t-N1（30.4%）。 - **仿真基准**： - **LIBERO**：平均成功率 **95.7%**，长视野任务提升显著（LIBERO-Long: **91.3%**）。 - **SimplerEnv Bridge**：平均成功率 **72.9%**，精细操作任务（如 Eggplant）达 **100%**。 - **消融实验**：验证生成专家（移除后性能下降 17.2%）和三阶段训练（移除预训练阶段 II 下降 3.7%）的必要性。 ### 5. **应用场景** - **动态环境**：在传送带抓取任务中成功率达 **66.7%**，展现对移动目标的适应能力。 - **跨本体迁移**：仅需 47 条演示数据即可适配新机器人（ARX LIFT II）。 > 总结：\(\mathcal{F}_{1}\) 通过视觉预见重构动作生成范式，在动态与长视野任务中实现显著性能突破，为具身智能提供可解释、可规划的决策框架。</details> |
| 2025-09-08 | LLaDA-VLA: Vision Language Diffusion Action Models | http://arxiv.org/abs/2509.06932v2 | <details><summary>展开</summary>论文提出**LLaDA-VLA**，首个基于预训练扩散视觉语言模型（d-VLM）的**视觉-语言-扩散-动作模型**，用于机器人操作任务。核心要点如下： --- ### **1. 研究动机** - **现有局限**：主流视觉-语言-动作模型（VLA）依赖自回归模型（ARMs），存在生成效率低、单向生成灵活性不足的问题。 - **扩散模型优势**：掩码扩散模型（MDMs）支持并行生成与迭代优化，在文本和视觉任务中表现优异，但尚未应用于机器人动作生成。 --- ### **2. 核心创新** - **局部化特殊令牌分类（Localized Special-token Classification）** 将分类空间从完整词汇表缩减至机器人动作相关的特殊令牌，显著降低预训练d-VLM适配机器人领域的难度。 - **分层动作结构化解码（Hierarchical Action-Structured Decoding）** - **动作级置信排序**：根据动作整体置信度确定解码顺序。 - **令牌级置信排序**：在动作内部按令牌置信度细化解码。 *解决扩散模型难以生成结构化动作序列的问题，显式建模动作内/间依赖关系。* --- ### **3. 模型架构** - **基础框架**：基于预训练d-VLM（LLaDA-V），集成视觉编码器（SigLIP-2）和投影模块（MLP）。 - **输入/输出**： - 输入：语言指令 + 第一视角RGB图像。 - 输出：离散化动作序列（7维/时间步：位移、旋转、夹爪状态）。 - **动作分块**：预测连续K时间步的动作块（默认K=5），提升轨迹连贯性。 --- ### **4. 实验结果** - **仿真基准**： - **CALVIN**：平均任务完成长度提升0.74（优于OpenVLA等）。 - **SimplerEnv**：平均成功率提升51.3%（超越CogACT等）。 - **真实机器人（WidowX）**： - 在4项任务中平均成功率58%（超越π0的35%和CogACT的30%）。 - **泛化能力**：在未见过的物体/容器任务中平均成功率40%（较π0提升25%）。 - **消融实验**： - 局部化分类策略使平均任务长度提升0.79。 - 分层解码策略进一步提升0.58。 --- ### **5. 贡献总结** - 提出首个**扩散式VLA框架**，为机器人策略学习开辟新范式。 - 设计两项关键技术解决d-VLM适配机器人任务的挑战。 - 在仿真与真实场景中均实现**SOTA性能**，验证扩散模型在机器人操作中的潜力。 --- **关键结论**：LLaDA-VLA通过结合扩散模型的高效生成与结构化动作解码，显著提升机器人操作的精度与泛化能力，为未来基于d-VLM的机器人研究奠定基础。</details> |
| 2025-09-06 | SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning | http://arxiv.org/abs/2509.05614v1 | <details><summary>展开</summary>本文提出**SpecPrune-VLA**，一种通过动作感知自推测剪枝加速视觉-语言-动作（VLA）模型的方法。核心贡献如下： 1. **动作级静态令牌剪枝** - 利用历史动作的全局注意力信息识别冗余令牌（如背景），结合当前动作的局部信息（前两层LLM注意力）动态补充关键令牌（如移动物体）。 - 通过速度自适应的帧采样策略（如公式 \( T = \lfloor -\frac{16}{3} \cdot v_t + \frac{22}{3} \rfloor + 4 \) ）精准捕捉动态变化区域。 2. **层级动态令牌剪枝** - 设计重要性评分机制（公式 \( s_i^{(l)} = \omega_{\text{rank},i}^{(l)} \times \omega_{\text{conf}}^{(l)} \) ），结合令牌排名权重和层置信度，在Transformer深层自适应剪枝冗余令牌。 3. **轻量动作感知控制器** - 根据机械臂末端速度（平移 \( v_t \) 和旋转 \( v_r \) ）区分粗粒度/细粒度动作，动态调整剪枝强度（细粒度时保留更多令牌）。 **实验结果**： 在LIBERO仿真基准测试中，相比OpenVLA-OFT： - A800 GPU上平均加速 **1.46×**，任务成功率损失可忽略（<0.7%） - RTX 3090 GPU上加速 **1.57×** - FLOPs减少约 **57%**，同时保持空间推理、物体操作等任务的高成功率（>96%）。 该方法通过融合跨动作的全局信息和局部推测，显著提升VLA模型推理效率，适用于实时机器人控制场景。</details> |
| 2025-09-05 | FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies | http://arxiv.org/abs/2509.04996v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-04 | Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models | http://arxiv.org/abs/2509.04063v1 | <details><summary>展开</summary>待生成</details> |
| 2025-09-04 | FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction | http://arxiv.org/abs/2509.04018v1 | <details><summary>展开</summary>论文提出FPC-VLA框架，通过集成视觉-语言-动作（VLA）模型与故障预测-校正监督器，提升机器人操作的可靠性。核心创新点如下： 1. **双模型架构** - **监督器模块**：基于视觉语言模型（VLM），在关键帧（如夹爪状态变化时）评估动作可行性。通过视觉-语言查询预测故障风险，并生成空间校正指令（如"向左大距离移动"）。 - **自动化数据集生成**：从RLDS格式数据自动构建故障校正数据集，无需人工标注。通过分析夹爪状态变化轨迹，生成包含图像、任务指令和结构化QA的样本（如表II示例）。 2. **动作优化机制** - **相似性引导融合模块**：聚合历史动作预测，通过余弦相似性（公式15）和时间衰减权重（公式17-18）平滑输出，解决动作突变问题。其中姿态与夹爪状态分开处理（公式19-20）。 3. **实验验证** - **跨平台测试**：在WidowX（SIMPLER基准）、Google Robot（表IV）和Franka（LIBERO基准）上均超越SOTA： - WidowX零样本任务成功率64.6%（表III），较次优方法提升22%。 - Google Robot上"打开抽屉放苹果"任务成功率54.5%（表IV）。 - LIBERO长时程任务成功率82.2%（表V），提升12个百分点。 - **实时性**：监督器仅触发≤3次/任务，推理延时增加15%但成功率提升34.6%（任务成功率从58.3%→92.9%）。 4. **应用价值** 成功部署于真实机器人（图4），处理遮挡物体操作等复杂场景，项目页面与代码已开源。 > 该方法解决了传统VLA模型缺乏故障恢复能力的问题，通过轻量化监督机制显著提升系统鲁棒性，为自主机器人提供可扩展的可靠性解决方案。</details> |
| 2025-09-03 | ANNIE: Be Careful of Your Robots | http://arxiv.org/abs/2509.03383v1 | <details><summary>展开</summary>本文《Annie: Be Careful of Your Robots》针对具身AI（EAI）系统的安全风险展开研究，核心贡献如下： 1. **安全定义与分类**：基于ISO/TS 15066标准，首次为EAI系统建立安全框架，将安全违规分为三类： - **关键违规**（Critical）：涉及危险工具（如刀具）与人类距离过近，违反物理隔离约束。 - **危险违规**（Dangerous）：物体速度超限或过早释放，导致潜在伤害。 - **风险违规**（Risky）：机器人与环境物体碰撞，引发间接损害。 2. **安全基准Annie-Bench**：构建首个EAI安全专用数据集，包含9个安全关键场景（每类3个），共2,400个视频-动作序列，覆盖三类违规场景，支持安全攻击与防御评估。 3. **攻击框架Annie-Attack**：提出任务感知对抗攻击框架，通过“攻击领导者模型”将长期安全目标分解为帧级扰动，实现： - **高攻击成功率**：在ACT、Baku等EAI模型上，关键、危险、风险任务的攻击成功率分别达52%、67%、50%。 - **隐蔽性优化**：支持稀疏攻击（如每3帧扰动）和自适应策略，降低攻击频率同时保持高成功率。 4. **评估与影响**：实验验证攻击在模拟和真实机器人场景的有效性，暴露EAI系统在物理交互中的安全漏洞，呼吁加强安全防御机制。 论文强调传统机器学习安全方法不适用于EAI，需重新定义安全指标并设计物理环境下的防御策略。</details> |
| 2025-09-02 | Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance | http://arxiv.org/abs/2509.02055v2 | <details><summary>展开</summary>待生成</details> |
| 2025-09-02 | AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving | http://arxiv.org/abs/2509.01944v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-31 | OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving | http://arxiv.org/abs/2509.00789v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-30 | Galaxea Open-World Dataset and G0 Dual-System VLA Model | http://arxiv.org/abs/2509.00576v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-30 | Mechanistic interpretability for steering vision-language-action models | http://arxiv.org/abs/2509.00328v1 | <details><summary>展开</summary>这篇论文的核心贡献是提出了首个针对视觉-语言-动作模型（VLA）的机制可解释性框架，实现了通过内部表征干预实时控制模型行为。主要要点如下： 1. **核心发现** - VLA 内部存在可解释的语义概念神经元（如“速度”“方向”），其 FFN 层值向量（value vectors）可投影到词嵌入空间展现语义关联。 - 即使模型仅输出动作指令，超过 75% 的神经元仍保留语义概念（如“slow”），且与动作选择存在因果关联。 - VLA 训练未显著改变预训练 VLM 的语义结构，微调主要影响动作令牌分布而非语义概念。 2. **干预方法** - **激活导向（Activation Steering）**：通过识别与特定概念（如“向上”）对齐的神经元集群，在推理时注入固定强度激活（公式 3-4），无需微调或环境交互。 - 实验证明：在仿真（OpenVLA/LIBERO）和实体机器人（π₀/UR5）中，干预可显著改变行为（如“fast”集群使末端位移提升 27.7%）。 3. **关键实验** - **仿真实验**： - 激活“fast/slow”集群可调节末端执行器位移（p<0.001）。 - 晚期层干预对运动控制效果更显著（如“up”集群在晚期层注入时位移提升 12.5%）。 - **机器人实验**： - 在 UR5 上，语义干预（如“low”集群）比提示词修改（如“low place penguin”）更有效降低轨迹高度。 - “slow”干预显著降低平均位移（但“fast”干预与基线相近，表明默认行为已偏快）。 4. **意义与局限** - **贡献**：首次建立 VLA 机制可解释性与实时控制的桥梁，为安全部署提供透明调控接口。 - **局限**：语义方向存在歧义性，微调可能影响可干预性，需扩展至移动平台等复杂场景。 **结论**：该框架证明 VLA 的语义概念可被直接操控以实现零样本行为控制，推动了具身智能模型的可解释性与安全性研究。</details> |
| 2025-08-28 | CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification | http://arxiv.org/abs/2508.21046v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-28 | EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control | http://arxiv.org/abs/2508.21112v3 | <details><summary>展开</summary>待生成</details> |
| 2025-08-27 | Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies | http://arxiv.org/abs/2508.20072v1 | <details><summary>展开</summary>本文提出Discrete Diffusion VLA模型，创新性地将离散扩散引入视觉-语言-动作（VLA）策略的动作解码中。核心贡献包括： 1. **统一架构设计**：在单一Transformer中集成视觉、语言和动作模态，通过离散扩散对分桶化的动作令牌建模，使用与VLM主干相同的交叉熵目标训练，保留预训练多模态先验知识。 2. **自适应解码机制**： - 采用"易先难后"的并行解码策略，根据置信度分数动态选择待预测令牌 - 引入二次重掩码技术，通过阈值检测和置信残差检查修正不确定预测，提升跨步一致性 3. **性能优势**： - 在LIBERO任务上达到96.3%平均成功率（+0.9% vs OpenVLA-OFT） - SimplerEnv-Fractal视觉匹配率71.2%，整体64.1% - SimplerEnv-Bridge整体49.3%（+9.8% vs π₀+FAST） - 相比自回归模型减少4.7倍函数评估次数，突破序列解码瓶颈 该方法通过固定步长的迭代精炼实现动作生成，为大规模VLA模型提供可扩展的统一框架。</details> |
| 2025-08-27 | Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation | http://arxiv.org/abs/2508.19958v2 | <details><summary>展开</summary>待生成</details> |
| 2025-08-27 | Ego-centric Predictive Model Conditioned on Hand Trajectories | http://arxiv.org/abs/2508.19852v2 | <details><summary>展开</summary>待生成</details> |
| 2025-08-26 | MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2508.19236v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-25 | FlowVLA: Thinking in Motion with a Visual Chain of Thought | http://arxiv.org/abs/2508.18269v2 | <details><summary>展开</summary>论文提出FlowVLA框架，解决现有Vision-Language-Action (VLA)模型通过直接下一帧预测（\(v_t \rightarrow v_{t+1}\)）训练世界模型时，混同运动动态与外观渲染的问题，导致物理预测不合理和策略学习低效。核心创新是引入**Visual Chain of Thought (Visual CoT)**，将预测分解为“运动推理→外观生成”的链式过程（\(v_t \rightarrow f_t \rightarrow v_{t+1}\)），其中\(f_t\)为中间光流运动表示。FlowVLA作为自回归Transformer实现，采用统一标记化将光流和图像编码为共享词汇，无需额外架构。实验表明，在LIBERO和SimplerEnv等机器人操作基准上，FlowVLA实现了最先进性能，并显著提升样本效率，验证了显式运动推理对世界建模和策略学习的有效性。</details> |
| 2025-08-23 | NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows | http://arxiv.org/abs/2508.16845v1 | <details><summary>展开</summary>论文提出**NinA模型**，用**标准化流（Normalizing Flows, NFs）替代扩散模型**作为视觉-语言-行动（VLA）模型中的动作解码器，核心要点如下： ### 1. **问题与动机** - **扩散模型的缺陷**：当前VLA模型广泛使用扩散模型作为动作解码器，虽能处理复杂动作分布，但需多次迭代去噪，推理延迟高，难以满足实时机器人控制的高频需求。 - **NFs的优势**：标准化流通过单次可逆变换生成动作，**推理速度显著更快**，且支持精确似然估计，利于强化学习、不确定性建模等下游任务。 ### 2. **NinA方法** - **架构设计**： - 基于FLOWER VLA框架，保留预训练的视觉-语言模型（VLM）编码器，**仅替换动作解码器为NF**。 - 提出两种NF变体：**MLP版（参数量2M）** 和 **Transformer版（参数量38M）**，前者轻量高效，后者性能更强。 - **关键技术**： - **噪声注入**：训练时对动作添加高斯噪声（最佳σ=0.03），提升泛化能力（消融实验证实必要性）。 - **可逆耦合层**：将动作分割为两部分，一部分经网络生成缩放/偏置参数，另一部分线性变换后拼接。 - **PLU可逆线性层**：增强表达能力（消融实验显示对Transformer版有轻微增益）。 ### 3. **实验结果** - **性能匹配**：在LIBERO基准测试中： - NinA Transformer平均成功率0.938，接近扩散模型（0.952），参数量仅为其1/10（38M vs 330M）。 - NinA MLP平均成功率0.909，优于小型扩散模型（31M参数版为0.916）。 - **推理速度**： - NinA比扩散模型**快5-10倍**（如H100 GPU：0.021s vs 0.110s）。 - **关键消融**： - 移除噪声注入导致性能显著下降（Transformer从0.938→0.896）。 - Transformer在深度扩展性上优于MLP（深度增至30层仍稳定）。 ### 4. **意义与展望** - **高效替代方案**：NFs在保持性能的同时大幅提升推理效率，适用于实时机器人控制。 - **潜在价值**：NFs的似然计算能力为强化学习、不确定性估计等任务提供新可能。 - **未来方向**：扩展至更大规模预训练、多场景部署及与其他模态的集成。 --- **总结**：NinA通过标准化流实现**高效单次动作生成**，在推理速度和参数量上显著优于扩散模型，为实时VLA控制提供新路径。</details> |
| 2025-08-22 | Do What? Teaching Vision-Language-Action Models to Reject the Impossible | http://arxiv.org/abs/2508.16292v1 | <details><summary>展开</summary>论文提出**Instruct-Verify-and-Act (IVA) 框架**，旨在提升视觉-语言-动作模型（VLA）处理**错误前提指令**（如指令引用环境中不存在物体）的能力。核心要点如下： 1. **问题背景**： - 现有VLA模型在真实场景中面临用户指令可能基于错误前提（如“拿桌上的红杯子”，但杯子不存在）的挑战，无法有效识别或纠正此类指令。 2. **方法创新**： - **IVA框架**： - **检测（Instruct）**：识别指令是否因错误前提（物体缺失/条件不满足）而无法执行。 - **验证（Verify）**：通过语言交互澄清或纠正用户意图（如“未找到红杯子，是否指蓝色抽屉？”）。 - **执行（Act）**：对可行指令生成动作，或基于感知提出替代方案。 - **数据集构建**： - 利用RLBench环境构建大规模指令调优数据集，包含**真实前提指令**与两类错误前提指令： - **领域内错误**（如任务中相似但缺失的物体）； - **领域外错误**（如完全不存在的物体）。 - 数据分布：65%任务含领域内错误，20%含领域外错误，确保模型全面学习。 3. **实验结果**： - 在9个RLBench任务上，IVA显著优于基线（如LLARVA）： - **错误前提检测率**：领域内达100%，领域外达97.78%（平均提升97.56%）。 - **错误场景响应成功率**：提升50.78%。 - **真实前提任务性能**：保持与基线相当（42.67% vs 38.67%），证明框架不损害基础能力。 4. **局限与展望**： - 数据依赖模拟环境，需验证真实场景泛化性； - 复杂指令（如多轮对话）处理能力有限； - 未来将探索开放环境部署及更灵活的语言交互。 **总结**：IVA通过统一框架使VLA模型能主动拒绝错误指令并引导用户修正，提升人机交互的鲁棒性与安全性。</details> |
| 2025-08-21 | Survey of Vision-Language-Action Models for Embodied Manipulation | http://arxiv.org/abs/2508.15201v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-19 | CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models | http://arxiv.org/abs/2508.13446v1 | <details><summary>展开</summary>论文提出了一种名为**CAST（Counterfactual Augmentation with Synthetic Trajectories）**的新方法，用于提升视觉-语言-动作（VLA）模型在机器人导航任务中的指令跟随能力。核心要点如下： 1. **问题背景** - VLA模型在遵循细粒度语言指令（如“沿左侧墙壁移动”）时表现不佳，主要原因是训练数据缺乏**语义多样性**和**细粒度任务变体**。 - 现有数据集中，相同观察状态通常只对应单一任务（如“打开抽屉”），导致模型忽略语言指令（**后验塌陷**问题）。 2. **核心方法：CAST** - **反事实标签生成**：利用视觉语言模型（VLM）为现有机器人轨迹生成**反事实指令**（如将“直行走廊”改为“沿左侧墙壁导航”）及对应动作。 - **原子策略辅助**：通过训练简单原子策略（如“左转/右转”）生成反事实动作，确保指令与动作的可行性。 - **数据增强**：将生成的反事实轨迹（指令-动作对）加入训练集，强制模型关注语言指令与动作的关联。 3. **理论依据** - 反事实标签通过增加**条件互信息** \(I(\mathbf{a}; \ell \| \mathbf{o})\)，确保模型必须依赖指令预测动作（Lemma IV.1）。 4. **实验验证** - **任务设置**：在3种真实环境（室内走廊、厨房、户外公园）测试27项复杂指令任务，涵盖物体导航（如“移动到椅子”）、关系导航（如“移动到桌子右侧”）和连续导航（如“沿窗户移动”）。 - **结果**： - CAST显著提升VLA性能：**53%** 平均成功率，比未使用CAST的VLA高**27%**。 - 优于基线方法：超过分层规划（\(\pi_a\) + VLM规划）和CoNVOI等SOTA方法**19%**。 - **关键优势**：无需额外数据收集，仅通过数据增强即可提升模型对语言指令的敏感度。 5. **技术贡献** - 开源CAST数据增强代码、数据集及训练好的VLA模型（**CounterfactualVLA**），促进可复现性。 6. **局限性** - 原子行为设计依赖任务领域知识（如导航中的转向指令）。 - VLM标注质量影响生成效果，未来可结合仿真或生成模型优化。 **总结**：CAST通过合成反事实指令-动作对，解决了VLA模型因训练数据单一导致的指令忽略问题，显著提升了机器人对复杂语言指令的理解和执行能力。</details> |
| 2025-08-18 | Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy | http://arxiv.org/abs/2508.13103v1 | <details><summary>展开</summary>本文提出了一种名为“观察中心视觉-语言-动作策略”（OC-VLA）的新框架，旨在解决现有视觉-语言-动作（VLA）模型在泛化到真实环境时面临的核心问题：观察空间（相机坐标系）与动作空间（机器人基坐标系）之间的错位导致模型鲁棒性差。具体要点如下： 1. **问题分析**：现有VLA模型通常在机器人基坐标系中预测末端执行器位姿，但视觉观察基于相机坐标系。这种不一致在训练数据视角多样时加剧了跨视角泛化困难，尤其当机器人基座不在相机视野内时。 2. **核心方法**：OC-VLA利用相机外参标定矩阵，将动作目标从机器人基坐标系转换到相机坐标系进行预测。这统一了观察与动作的空间表示，无需修改模型架构即可实现即插即用： - **训练阶段**：将动作位姿转换至相机坐标系作为监督信号。 - **推理阶段**：预测的相机坐标系动作反向转换回机器人坐标系执行。 3. **关键优势**： - 显著提升模型对相机视角变化的鲁棒性，改善跨视角泛化。 - 作为轻量级模块，兼容现有VLA架构（如连续或离散动作空间）。 4. **实验结果**： - **模拟任务**（ManiSkill2）：OC-VLA在成功率上优于基线（例如离散动作空间提升14%），尤其在视角变化任务中表现更优。 - **真实机器人任务**（Franka Emika Panda）：在10样本微调设置下，OC-VLA平均成功率提高10%，且在未见视角下泛化能力更强（性能下降幅度比基线低6%）。 该方法通过统一观察与动作空间，有效解决了VLA模型的空间错位问题，提升了实际部署的适应性。</details> |
| 2025-08-18 | Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey | http://arxiv.org/abs/2508.13073v2 | <details><summary>展开</summary>本文系统综述了基于大型视觉语言模型（VLA）的视觉-语言-动作模型在机器人操作中的应用，核心要点如下： 1. **模型架构分类**： - **单体模型（Monolithic Models）**： - **单系统模型**：集成环境感知（视觉、语言、机器人状态）与动作生成于统一架构，通过自回归或并行解码生成动作（如RT系列、OpenVLA）。研究聚焦模型性能增强（多模态感知、推理链优化）和推理效率优化（架构精简、参数量化）。 - **双系统模型**：分离高层推理（System 2：VLM主干）与实时控制（System 1：动作专家）。分串联式（如DP-VLA、RoboDual，VLM输出特征驱动动作生成）和并联式（如GR00T N1、ChatVLA，双系统交互协同），提升响应速度与任务适应性。 2. **分层模型（Hierarchical Models）**： - 显式解耦任务规划与策略执行，生成可解释中间表示（如关键点、子任务、程序）。 - **纯规划器（Planner-Only）**：VLM输出结构化指令（如程序代码、关键点坐标），由独立策略模块执行。 - **规划器+策略（Planner+Policy）**：联合优化规划与执行模块（如基于关键点或子任务的端到端训练），提升任务鲁棒性。 3. **关键贡献**： - 统一VLA模型定义，解决术语不一致性，整合机器人学、CV、NLP多领域研究。 - 分析VLA特性：多模态融合、指令跟随、多维度泛化能力，并系统分类数据集（仿真/真实世界/人类行为数据）。 - 指出未来方向：记忆机制、4D感知、高效适配、多智能体协作等。 4. **应用与挑战**： - VLA模型利用VLMs的开放世界泛化、知识推理优势，提升机器人复杂场景操作能力。 - 现存挑战：推理效率与实时控制需求冲突、多模态对齐复杂性、长时序任务规划稳定性。 该综述首次系统化梳理大型VLM-based VLA模型，为机器人操作领域提供架构设计参考与研究路线图。</details> |
| 2025-08-17 | Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search | http://arxiv.org/abs/2508.12211v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-16 | Toward General Physical Intelligence for Resilient Agile Manufacturing Automation | http://arxiv.org/abs/2508.11960v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-15 | TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models | http://arxiv.org/abs/2508.19257v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-14 | CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model | http://arxiv.org/abs/2508.10416v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-14 | Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning | http://arxiv.org/abs/2508.10399v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-14 | ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver | http://arxiv.org/abs/2508.10333v1 | <details><summary>展开</summary>论文提出ReconVLA模型，解决现有视觉-语言-动作（VLA）模型视觉注意力分散的问题，核心贡献如下： 1. **重构式VLA框架** - 引入隐式 grounding 范式：通过扩散transformer重建图像中的"注视区域"（目标操作物体） - 监督机制：以VLA的视觉输出为条件，重建目标区域，迫使模型学习细粒度表征 - 效果：引导视觉注意力精确聚焦目标区域，提升操作精度 2. **大规模预训练数据集** - 构建超过10万条轨迹、200万样本的机器人数据集 - 利用Grounding DINO自动标注目标区域 - 增强视觉重建的泛化能力 3. **关键实验结果** - 隐式grounding优于显式grounding（EG）和思维链grounding（CG） - 注意力可视化显示注意力集中度提升（CALVIN基准成功率提高20.2%） - 在长视野任务（如"堆叠积木"）中表现突出，真实世界多任务成功率超90% - 预训练使模型对未见目标具泛化能力 4. **架构创新** - 双路径设计：动作路径输出离散动作token，重构路径恢复场景token - 冻结视觉tokenizer保留细节信息，实现高保真重建 论文通过仿真和真实实验验证了模型在精确操作和泛化能力上的优势，项目页面见：https://zionchow.github.io/ReconVLA/。</details> |
| 2025-08-12 | GeoVLA: Empowering 3D Representations in Vision-Language-Action Models | http://arxiv.org/abs/2508.09071v2 | <details><summary>展开</summary>GeoVLA提出了一种新颖的视觉-语言-动作（VLA）框架，通过整合3D几何信息增强机器人操作能力。核心创新包括： 1. **双路径架构设计** - **视觉语言分支**：利用预训练VLM（如Prismatic）处理RGB图像和语言指令，生成融合特征 - **几何分支**：通过定制化点嵌入网络（PEN）处理深度图转换的点云数据，提取以机械臂末端为中心的3D特征 2. **关键组件创新** - **PEN模块**：采用双路径Transformer（几何特征路径+旋转位置编码），选择性提取末端执行器锚点特征 - **3DAE动作专家**：基于扩散Transformer，通过静态路由的混合专家机制融合多模态特征，生成连续动作序列 3. **显著性能优势** - 仿真实验：在LIBERO和ManiSkill2基准上达到SOTA（分别超越前最佳2.4%和11%） - 真实世界任务：平均成功率86.3%，在高度适应/尺度感知/视角变化等场景展现强鲁棒性 - 消融实验：验证PEN的几何编码能力和3DAE多模态融合机制的有效性 4. **工程贡献** - 开源项目地址：https://linsun449.github.io/GeoVLA - 避免大规模3D指令微调需求，通过端到端训练保持预训练模型的知识迁移能力 该方法解决了传统VLA模型依赖2D视觉输入导致的几何信息缺失问题，为机器人操作任务提供了更精确的空间感知基础。</details> |
| 2025-08-12 | Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding | http://arxiv.org/abs/2508.09032v1 | <details><summary>展开</summary>本文提出了一种名为“空间轨迹”（Spatial Traces）的新方法，用于增强视觉-语言-动作（VLA）模型对时空信息的理解。核心创新点是通过视觉提示技术，将关键点的运动轨迹投影到深度图上，使模型能同时捕捉空间（深度信息）和时间（历史轨迹）特征。在 SimplerEnv 环境中进行实验，仅使用 52 条训练轨迹微调后，该方法（ST-VLA）的任务成功率比 SpatialVLA 提高 4%，比 TraceVLA 提高 19%，显著提升了复杂操作任务（如物体抓取和放置）的性能。该方法在数据有限的现实场景中具有高效性，未来可结合 3D 重建进一步优化。</details> |
| 2025-08-12 | OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing | http://arxiv.org/abs/2508.08706v2 | <details><summary>展开</summary>本文提出OmniVTLA模型，解决现有Vision-Language-Action（VLA）模型忽视触觉感知的问题，尤其在接触丰富的任务中性能不足。核心要点如下： 1. **问题与目标**： - 现有VLA模型依赖视觉和语言，但忽略触觉感知，导致接触密集型任务（如抓取）失败。 - 目标：整合视觉、触觉、语言和动作模态，提升机器人操作的鲁棒性和泛化能力。 2. **关键创新**： - **OmniVTLA架构**： - 双路径触觉编码器（dual-encoder path）：使用预训练视觉变换器（ViT）处理视觉数据，语义对齐触觉ViT（SA-ViT）处理触觉信号，解决传感器异质性。 - 通过对比学习对齐触觉、视觉和文本语义空间。 - **ObjTac数据集**： - 收集135K三模态样本（文本、视觉、基于力的触觉数据），涵盖56个对象、10个类别，补充现有数据集。 - **语义对齐训练**： - 利用ObjTac训练SA-ViT编码器，学习统一触觉表示，作为OmniVTLA的初始化。 3. **实验结果**： - **性能提升**： - 夹爪拾取任务：成功率96.9%（比VLA基线高21.9%）。 - 灵巧手任务：成功率100%（比基线高6.2%）。 - **效率优势**： - 任务完成时间减少24.2%，轨迹平滑性提升89.6%，符合“非接触快速移动，接触时减速”原则。 - **泛化能力**：在未见对象上表现优异，如塑料瓶和方瓶成功率100%。 4. **意义**： - 为触觉感知提供统一框架，增强机器人对物理交互的理解，推动多模态模型在真实场景的应用。</details> |
| 2025-08-11 | GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions | http://arxiv.org/abs/2508.07650v2 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control | http://arxiv.org/abs/2508.05342v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction | http://arxiv.org/abs/2508.05294v2 | <details><summary>展开</summary>本文综述了大型语言模型（LLM）、视觉语言模型（VLM）及视觉语言动作模型（VLA）在实现具身智能体AI（Embodied Agentic AI）中的研究进展与分类体系，核心要点如下： 1. **核心概念** - **具身智能体AI**：区别于端到端学习或传统符号规划，强调LLM/VLM作为**智能中介**协调机器人系统（如ROS），通过自然语言理解、API调用和任务规划增强自主性，而非直接生成控制策略。 - **优势**：兼容现有机器人软件栈，提升灵活性、可解释性和人机交互能力。 2. **模型集成方法分类**（图1） - **协议集成**：模型作为协议翻译器（如`ros2ai`将自然语言转为ROS命令）。 - **接口集成**：模型作为交互层，动态分解任务并调用工具（如ROSA、RAI框架支持多工具循环调用）。 - **编排导向集成**：模型协调资源或多智能体（如AutoRT管理机器人集群任务分配）。 - **嵌入式集成**：模型直接输出动作或感知（端到端VLAs如RT-2、π0）。 3. **智能体角色分类**（图3） - **规划型**：生成高层任务序列（如SayCan通过LLM+价值函数选择动作）。 - **编排型**：管理多技能/多机器人协作（如LABOR Agent调度数百种操作技能）。 - **任务专用型**：解决特定任务（如BUMBLE处理开放世界移动操作）。 - **模型中心型**：统一架构处理多模态输入（如RoboCat泛化跨任务策略）。 - **通用型**：模块化架构组合技能库（如Voyager自主生成代码工具）。 - **通用系统型**：提供可扩展框架（如ROSA、OpenMind OM1支持工具定制）。 4. **关键趋势** - **工业与社区驱动**：除学术研究外，ROS社区工具（如ROS-MCP）和工业框架（OpenMind）推动实用化。 - **演进方向**：从ChatGPT初期的简单协议交互（2023）→ VLAs与多智能体协调（2024）→ 开放世界通用智能体（2025）。 5. **挑战与展望** - 需解决**环境 grounding**、**长时记忆**、**安全验证**及**评估标准**问题。 - 融合多集成方法和智能体角色是迈向通用具身智能的关键路径。 **总结**：论文提出双维度分类法（集成方法×智能体角色），系统化梳理了LLM/VLM驱动的机器人智能体架构，强调其作为"中间层"协调传统系统的设计范式，为具身智能发展提供技术路线参考。</details> |
| 2025-08-07 | Learning to See and Act: Task-Aware View Planning for Robotic Manipulation | http://arxiv.org/abs/2508.05186v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model | http://arxiv.org/abs/2508.06571v3 | <details><summary>展开</summary>待生成</details> |
| 2025-08-06 | Static and Plugged: Make Embodied Evaluation Simple | http://arxiv.org/abs/2508.06553v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-06 | A tutorial note on collecting simulated data for vision-language-action models | http://arxiv.org/abs/2508.06547v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming | http://arxiv.org/abs/2508.02549v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning | http://arxiv.org/abs/2508.02219v1 | <details><summary>展开</summary>本文提出**CO-RFT**算法，通过**分块离线强化学习**高效微调视觉-语言-动作（VLA）模型，解决现有方法在样本效率、动作分块兼容性和训练稳定性上的不足。核心要点如下： ### 1. **核心方法** - **分块强化学习框架**： - 将时序差分（TD）学习扩展至**动作分块**（一次预测多步动作），提升样本效率与稀疏奖励处理能力。 - **分块Critic网络**：基于Transformer架构，输入当前状态和动作块，预测多步Q值序列，优化动作块决策。 - **两阶段微调（CO-RFT）**： - **阶段1（模仿学习）**：全参数微调VLA模型，适配新机器人平台。 - **阶段2（离线RL）**：结合动作分块的CalQL算法优化策略，引入保守正则化缓解分布偏移问题。 ### 2. **关键优势** - **高效样本利用**：仅需30-60个演示样本。 - **兼容动作分块**：保留VLA模型输出多步动作的特性，提升动作连贯性。 - **奖励上采样技术**：缓解稀疏奖励问题，增强价值学习。 ### 3. **实验结果** - **性能提升**：在真实机械臂任务中，相比监督微调（SFT）： - 平均**成功率提升57%**（部分任务达100%）。 - 平均**周期时间减少22.3%**。 - **泛化能力**：在未知位置测试时成功率**达44.3%**，数据多样性（随机初始化）对泛化影响显著。 ### 4. **意义** - 为VLA模型微调提供样本高效的离线RL方案，避免在线RL的复杂基础设施需求。 - 首次实现动作分块与强化学习的兼容，为长视野、稀疏奖励任务提供新思路。 > 总结：CO-RFT通过分块强化学习和两阶段微调，显著提升VLA模型在真实机器人任务中的性能与泛化能力，同时降低样本需求和推理延迟。</details> |
| 2025-08-04 | FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation | http://arxiv.org/abs/2508.02190v1 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models | http://arxiv.org/abs/2508.02062v1 | <details><summary>展开</summary>论文提出**RICL（Retraining for In-Context Learning）方法**，旨在为预训练的视觉-语言-动作模型（VLA）注入上下文学习（ICL）能力，使其无需参数微调即可适应新任务。核心要点如下： 1. **问题背景**： - 现有VLA模型（如π₀-FAST）虽能处理多任务，但**缺乏ICL能力**，用户需通过微调模型参数来适应新任务，过程复杂。 2. **RICL解决方案**： - **核心思路**：通过特定微调方法，使预训练VLA模型能够利用**检索增强生成（RAG）** 和**少量演示数据**（10-20条）进行上下文学习。 - **实现步骤**： - 收集少量“引导演示数据”（20个任务的400条演示），将查询状态与检索到的相似状态-动作对拼接为输入序列。 - 引入**动作插值层**（公式1），结合检索动作与模型预测结果，提升动作生成质量。 - 仅微调语言模型部分，冻结视觉编码器（图2架构）。 - **部署**：用户提供新任务演示后，RICL-VLA自动检索相关片段注入上下文，实现零参数更新的任务适应。 3. **实验结果**： - 在7项新任务（涉及新物体、新动作、新场景）上，RICL-VLA（π₀-FAST-DROID为基础）显著优于原始模型： - **零参数更新**：任务成功率从基线的2.5%提升至31.25%（图4）。 - **支持微调**：若允许在新任务数据上微调，成功率进一步提升至61.67%，且效果优于直接微调原始模型。 - 案例验证（图1）：原始模型在处理精灵球抓取、刮水器拖动等新任务时失败，而RICL-VLA仅通过20条演示即成功适应。 4. **贡献与意义**： - 首次实现VLA模型的**即插即用式上下文学习接口**，用户仅需提供少量演示即可提升模型表现。 - 开源代码与模型权重，推动机器人任务适配的简易化（官网链接见脚注）。 5. **局限与未来**： - 当前主要适用于拾放类任务，对复杂动作（如网球挥拍）泛化有限。 - 未来方向：扩展引导数据规模、结合人类视频演示、增强动作多样性。 **关键词**：视觉-语言-动作模型（VLA）、上下文学习（ICL）、检索增强生成（RAG）。</details> |
| 2025-07-31 | XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation | http://arxiv.org/abs/2508.00097v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-31 | villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models | http://arxiv.org/abs/2507.23682v3 | <details><summary>展开</summary>论文提出villa-X框架，通过改进视觉-语言-动作（VLA）模型中的潜在动作建模，提升机器人策略的泛化能力。核心贡献包括： 1. **潜在动作模型（LAM）改进** - 引入本体感知前向动力学模型（proprio-FDM），通过预测机器人未来状态和动作，将潜在动作与物理动力学对齐，解决纯视觉方法忽略细微动作（如夹爪旋转）的问题。 - 添加本体上下文（$c_e$）区分异构数据，提升跨具身泛化能力。 2. **执行器模块（ACT）设计** - 提出分层策略架构：VLM编码视觉语言输入，ACT-latent生成潜在动作计划，ACT-robot基于潜在动作生成机器人动作。 - 采用联合扩散建模，通过注意力掩码策略防止动作分支过度依赖潜在动作。 3. **实验验证** - **LAM有效性**：proprio-FDM提升潜在动作质量（图3），在SIMPLER基准测试中平均性能超越基线（表1）。 - **零样本泛化**：潜在动作专家可生成未见具身（Realman机械臂）和开放词汇符号的规划（图4）。 - **整体性能**：在SIMPLER仿真任务中，Google机器人任务平均成功率77.7%，WidowX机器人62.5%（表2）；实物实验显示在夹爪和灵巧手任务中均优于GR00T等基线（表3-4）。 4. **优势** - 通过大规模预训练，实现跨具身和开放场景的零样本泛化。 - 为学习通用机器人策略提供可扩展范式，代码已开源。 局限性与未来工作包括进一步探索潜在动作专家的规划验证能力。</details> |
| 2025-07-31 | A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving | http://arxiv.org/abs/2507.23540v1 | <details><summary>展开</summary>论文提出一个统一的感知-语言-动作（PLA）框架，用于提升自动驾驶系统在复杂环境中的适应性、鲁棒性和可解释性。该框架整合多传感器融合（相机、LiDAR、雷达）与大型语言模型（LLM，如GPT-4.1）增强的视觉-语言-动作（VLA）架构，通过三层结构实现： 1. **感知层**：处理传感器数据生成结构化场景描述（如物体位置、速度）。 2. **语言层**：基于LLM进行上下文推理和风险分析，输出可解释的驾驶命令。 3. **动作层**：执行轨迹规划与控制，并通过数字孪生验证安全性。 在城市交叉口施工区场景（nuScenes数据集）的评估中，框架在速度预测（MAE 0.39 m/s）、轨迹跟踪（ADE 1.013m）方面表现优异，但转向角预测（MAE 2.52°）需优化。结果突显语言增强框架在提升安全性和泛化能力方面的潜力，未来工作将聚焦转向精度改进和场景扩展。</details> |
| 2025-07-31 | FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning | http://arxiv.org/abs/2507.23318v3 | <details><summary>展开</summary>论文提出FastDriveVLA框架，用于高效端到端自动驾驶的视觉语言动作（VLA）模型。核心创新点包括： 1. **基于重建的令牌修剪**：设计即插即用模块ReconPruner，通过MAE风格像素重建优先保留前景信息（如车辆、行人、车道），并引入对抗性前景-背景重建策略防止模型退化。 2. **专用数据集**：构建nuScenes-FG数据集（241K图像-掩码对），标注驾驶场景的前景区域，支撑模型训练。 3. **高效部署**：ReconPruner轻量（0.07B参数），训练后可无缝集成不同VLA模型，无需重训练，显著降低计算开销。 4. **性能优势**：在nuScenes开放环规划基准上，以50%令牌修剪率实现SOTA（L2误差降1.7%，碰撞率降4.2%），推理速度提升3.7倍。 方法解决了现有注意力/相似性修剪在自动驾驶中的局限性，强调前景信息对决策的关键性。</details> |
| 2025-07-30 | Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance | http://arxiv.org/abs/2507.22424v2 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation | http://arxiv.org/abs/2507.17520v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents | http://arxiv.org/abs/2507.17462v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | Confidence Calibration in Vision-Language-Action Models | http://arxiv.org/abs/2507.17383v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback | http://arxiv.org/abs/2507.17294v2 | <details><summary>展开</summary>待生成</details> |
| 2025-07-22 | ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning | http://arxiv.org/abs/2507.16815v2 | <details><summary>展开</summary>待生成</details> |
| 2025-07-21 | Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos | http://arxiv.org/abs/2507.15597v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-21 | GR-3 Technical Report | http://arxiv.org/abs/2507.15493v2 | <details><summary>展开</summary>待生成</details> |
| 2025-07-18 | VLA-Mark: A cross modal watermark for large vision-language alignment model | http://arxiv.org/abs/2507.14067v2 | <details><summary>展开</summary>待生成</details> |
| 2025-07-18 | EdgeVLA: Efficient Vision-Language-Action Models | http://arxiv.org/abs/2507.14049v1 | <details><summary>展开</summary>论文提出EdgeVLA（EVLA），一种高效的视觉-语言-动作模型，旨在解决大型VLA模型在资源受限边缘设备上部署的挑战。核心创新包括：（1）消除末端执行器位置预测的自回归要求，实现7倍推理加速；（2）采用小型语言模型（如Qwen2-0.5B），结合SigLIP和DINOv2视觉编码器，在降低计算需求的同时保持模型性能。实验结果在BridgeData V2和OpenX数据集上显示，EVLA训练性能与OpenVLA相当，但推理速度显著提升（从20ms降至5ms），内存占用减少75%（从16GB降至4GB）。模型和代码已开源以促进边缘机器人应用研究。</details> |
| 2025-07-17 | AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation | http://arxiv.org/abs/2507.12768v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-16 | EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos | http://arxiv.org/abs/2507.12440v3 | <details><summary>展开</summary>这篇论文提出了一种名为EgoVLA的视觉-语言-动作模型，其核心创新点是通过人类第一视角视频学习操作技能，并将其迁移至人形机器人。主要贡献如下： 1. **方法架构** - 提出**EgoVLA模型**：基于NVILA-2B视觉语言模型构建，输入包含视觉历史帧（6帧RGB）、语言指令和本体感觉状态 - 统一动作空间：使用**手腕位姿（3D平移+旋转）**和**MANO手部参数（15维PCA）**作为人类与机器人的共享动作表示 - 训练流程：先在人类视频数据集预训练，再用少量机器人演示微调 2. **数据集与基准** - 构建**大规模人类操作数据集**：整合HOI4D、HOT3D、HoloAssist和TACO四个来源，共50万图像-动作对 - 提出**Ego Humanoid Manipulation Benchmark**：在NVIDIA IsaacSim中设计12项双手操作任务（7项短时程+5项长时程），包含100条演示/任务 3. **关键技术** - **动作转换机制**：通过逆运动学将预测的人体手腕位姿转换为机器人末端位姿，并训练轻量MLP将MANO关键点映射至机器人关节命令 - **跨域适应**：设计几何变换对齐人类与机器人动作空间，实现预训练模型向目标域的迁移 - **多模态处理**：利用动作查询令牌预测未来30步（1秒）的动作序列 4. **实验结果** - 在仿真基准测试中，EgoVLA显著优于基线模型： - 短时程任务平均成功率77.78%（可见场景）→69.11%（新场景） - 长时程任务平均成功率45.93%（可见场景）→28.79%（新场景） - 关键优势：人类视频预训练提升**细粒度操作能力**（如翻转杯子、堆叠罐头）和**跨场景泛化性**，尤其在长时程任务中比无预训练模型高20%成功率 5. **局限性与应用** - 需少量机器人演示微调（零样本迁移失败） - 动作转换引入微小误差（指尖位置误差5×10⁻⁵m） - 为大规模机器人操作学习提供新范式，减少对真实机器人数据的依赖 该方法通过统一动作空间桥接人类与机器人操作技能，为利用丰富人类视频数据训练通用操作策略开辟了新途径。</details> |
| 2025-07-14 | Vision Language Action Models in Robotic Manipulation: A Systematic Review | http://arxiv.org/abs/2507.10672v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-12 | Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization | http://arxiv.org/abs/2507.09160v1 | <details><summary>展开</summary>论文HTML原文截断，仅显示LaTeX命令定义（如argmax、argmin、sign、Tr），未提供正文内容，无法总结要点。</details> |
| 2025-07-07 | VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting | http://arxiv.org/abs/2507.05116v3 | <details><summary>展开</summary>本文提出VOTE方法，解决现有视觉-语言-动作（VLA）模型的两大缺陷： 1. **计算开销过大**：传统VLA模型需生成大量动作令牌或依赖扩散过程，导致高推理延迟（如OpenVLA延迟240ms）和训练成本高（需填充大量空令牌）。 2. **动作利用率不足**：历史预测动作未被充分整合，仅依赖当前观测易产生不稳定轨迹。 **核心方案**： - **训练框架**：引入特殊令牌`<ACT>`表示整个动作块，替代多令牌生成。通过单次前馈预测一个`<ACT>`令牌，经MLP动作头并行解码为连续动作（如7自由度机械臂位姿），显著减少生成令牌数量（原需$N \times D$个令牌降至1个），降低83%训练填充开销。 - **推理优化**：提出投票集成策略，基于历史$K$步与当前动作的余弦相似度（阈值$\tau=0.5$）划分动作集，按多数表决选择最优动作执行，提升动作稳定性。 **实验结果**： - **性能提升**： - SimplerEnv任务成功率58.3%（超CogACT 7%），LIBERO任务平均成功率98%（超OpenVLA 21.5%）。 - 集成策略较平均法提升3.5%成功率（表5）。 - **效率优势**： - 推理速度达205.2 Hz（A6000 GPU），较OpenVLA加速48.8倍； - 边缘设备（Jetson Orin）实现46 Hz吞吐，延迟346ms，加速39倍（表4）。 - 瓶颈式MLP结构较各向同性设计参数量减少26%，内存占用仅14.4GB（表6）。 **贡献总结**： 1. 轻量训练框架大幅降低计算开销； 2. 投票集成机制提升动作利用率； 3. 在精度与速度上均优于主流VLA模型，具备边缘部署实用性。 代码开源：https://github.com/LukeLIN-web/VOTE</details> |
| 2025-07-06 | DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge | http://arxiv.org/abs/2507.04447v3 | <details><summary>展开</summary>待生成</details> |
| 2025-07-03 | DexVLG: Dexterous Vision-Language-Grasp Model at Scale | http://arxiv.org/abs/2507.02747v1 | <details><summary>展开</summary>待生成</details> |
| 2025-07-02 | cVLA: Towards Efficient Camera-Space VLAs | http://arxiv.org/abs/2507.02190v1 | <details><summary>展开</summary>这篇论文提出了一种高效的相机空间视觉-语言-动作（cVLA）模型，核心贡献如下： 1. **轻量级架构设计** - 基于PaliGemma预训练模型微调，仅需优化注意力层参数 - 将机器人轨迹预测转化为图像坐标系中的末端执行器关键位姿预测（预测两个关键点） - 相比传统VLA模型输出底层控制信号，本方法更高效且与机器人本体解耦 2. **关键技术创新** - **相机空间表示**：直接在图像坐标系（归一化宽高+深度）预测位姿，替代传统机器人基坐标系 - **深度信息融合**：将深度图转换为Viridis色彩编码的RGB图像，复用预训练图像编码器 - **动作编码**：利用定位令牌（1024个）预测位置，分割令牌（128个）预测方向 3. **高效训练方案** - 使用ManiSkill模拟器构建多样化训练数据集（CLEVR几何体+Objaverse真实物体） - 引入两级数据增强：简单版（较少相机/场景随机）和困难版（强随机化） - 通过背景替换（概率0.2）和文本模板增强泛化能力 4. **推理优化策略** - **图像裁剪**：提升小物体定位精度（输入224×224分辨率下） - **束搜索-NMS**：改进解码策略，解决位姿令牌分布的多峰特性 - **多预测评估**：提出基于L1距离的mAP评估指标（阈值0.5-50cm） 5. **单次模仿学习** - 扩展模型支持基于示范的one-shot模仿：输入示范图像+轨迹对，输出新场景轨迹 - 在CLEVR-hard模拟数据上达到28%成功率，在DROID真实数据上L1误差11.56 6. **实验验证** - 模拟实验：深度输入提升成功率（CLEVR-hard达54%） - 实物验证：Franka Panda机器人实现零样本迁移（无需真实数据微调） - 开源承诺：将发布代码、数据集和模型 **局限**：目前局限于桌面级准静态操作任务，对复杂抓取和精确旋转的泛化能力有待提升。该方法为高效训练VLAs提供了新思路，显著降低了计算和数据需求。</details> |
| 2025-07-02 | A Survey on Vision-Language-Action Models: An Action Tokenization Perspective | http://arxiv.org/abs/2507.01925v1 | <details><summary>展开</summary>这篇论文提出了一种统一框架，从**动作标记化（Action Tokenization）**的视角系统综述了视觉-语言-动作（VLA）模型。核心要点如下： ### 1. **统一框架与动作标记分类** - VLA模型可抽象为：视觉与语言输入经一系列**VLA模块**处理，生成链式**动作标记**，逐步编码可执行动作信息。 - 动作标记分为8类： - **语言描述**（如任务分解的自然语言计划） - **代码**（如可执行API指令） - **可供性**（如关键点、边界框、分割掩码、空间图） - **轨迹**（如运动路径） - **目标状态**（如目标图像/视频） - **潜在表示**（如压缩的隐空间编码） - **原始动作**（如机器人关节指令） - **推理**（如思维链）。 ### 2. **动作标记的发展趋势** - **综合应用优于单一类型**：未来VLA需融合多种标记（如语言计划+代码控制+轨迹生成）。 - **关键方向**： - **语言描述**：适合任务规划但缺乏精细动作表达能力。 - **代码**：逻辑清晰但依赖预定义API库。 - **可供性+轨迹**：结合语义引导（"做什么"）与路径规划（"如何做"）。 - **目标状态**：通过世界模型预测视觉目标，支持长时序任务。 - **原始动作**：端到端学习的理想形式，但受数据稀缺限制。 - **推理**：作为元标记增强其他类型，向多模态自适应演进。 ### 3. **架构与数据演进** - **分层架构**：高层用语言/代码规划，底层融合目标状态预测、轨迹建模与可供性，最终映射到原始动作。 - **数据可扩展性**：构建三层数据源——人类视频（底层）、仿真数据（中层）、机器人数据（顶层）。 ### 4. **关键挑战与未来方向** - **从模仿学习到强化学习**：引入RL解决模仿学习的局限性，需高效算法降低现实部署成本。 - **从模型到智能体**：VLA需扩展记忆、探索、反思等认知架构，支持主动决策。 - **硬件-模型-数据协同**：通用智能需三者共同进化（如提升机器人灵巧性、多模态感知）。 - **安全对齐**：当前研究侧重能力，未来需加强安全性与人类价值观对齐。 ### 5. **局限性与展望** - **现状局限**：多数实验限于简化场景（如夹爪操作），离开放物理世界需求差距显著。 - **核心挑战**：硬件灵活性不足（如缺乏类人灵巧手）、高质量具身数据稀缺、开放环境泛化能力弱。 > 总结：该框架为VLA研究提供系统性视角，强调**动作标记的设计是模型核心**，未来需在多标记协同、硬件-数据-模型共进化及安全对齐上突破，推动通用具身智能发展。</details> |
| 2025-07-02 | TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control | http://arxiv.org/abs/2507.01424v2 | <details><summary>展开</summary>待生成</details> |
| 2025-07-01 | Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding | http://arxiv.org/abs/2507.00416v2 | <details><summary>展开</summary>论文提出Evo-0模型，通过隐式整合3D几何特征增强视觉-语言-动作（VLA）模型的空间理解能力。核心要点如下： 1. **问题**：现有VLA模型因依赖2D图像-文本预训练而缺乏精确空间理解，显式3D输入（如深度图）需额外传感器且易引入噪声。 2. **方法**：设计轻量融合模块，利用视觉几何基础模型（VGGT）从RGB图像提取几何特征，与VLM视觉标记结合，无需显式3D数据。 3. **实验**： - **模拟任务**（RLBench）：5项空间挑战任务中，Evo-0平均成功率（56%）显著优于基线（OpenVLA-OFT:25%，π₀:41%）。 - **真实任务**：5项操作（如钉孔插入、透明物体抓取），Evo-0平均成功率提升28.88%，验证几何特征的有效性。 - **鲁棒性**：在干扰物、背景变化等5类扰动下，Evo-0表现更稳健（如目标位置偏移时成功率提高10%）。 4. **贡献**：提供即插即用解决方案，提升空间理解；多场景验证性能优势；模型保持实时控制频率（6.94Hz）。</details> |
| 2025-06-30 | A Survey on Vision-Language-Action Models for Autonomous Driving | http://arxiv.org/abs/2506.24044v1 | <details><summary>展开</summary>这篇论文《自动驾驶中的视觉-语言-动作模型综述》首次全面概述了VLA在自动驾驶（VLA4AD）领域的研究进展。核心要点如下： 1. **范式演进**： - 从传统模块化流水线→端到端学习→VLM解释型模型→VLA主动决策模型演进 - VLA4AD核心突破：统一视觉感知、语言理解与控制决策，实现可解释的指令跟随 2. **架构范式**： - **输入**：多模态感知（摄像头/LiDAR）+ 语言指令（导航/交互命令） - **核心模块**：视觉编码器（如BEV特征）、语言处理器（LLM）、动作解码器（轨迹生成/低层控制） - **输出**：低层控制信号（转向/油门）或高层轨迹规划 3. **模型进展**： - **解释型VLM**：被动生成场景描述（如DriveGPT-4） - **模块化VLA**：语言指令转中间表示再生成动作（如CoVLA-Agent） - **端到端VLA**：传感器→语言→动作端到端映射（如EMMA） - **推理增强VLA**：集成CoT推理与轨迹规划（如ORION） 4. **数据集与评测**： - 关键数据集：BDD-X（带人工解释）、nuScenes（多传感器）、Impromptu VLA（长尾场景） - 评测协议：联合评估驾驶安全性、指令准确性和解释质量 5. **挑战与方向**： - 开放挑战：实时性（30Hz控制）、鲁棒性（极端场景）、形式化验证 - 未来方向：神经符号安全核、车队级持续学习、跨模态社会智能 论文系统化梳理了20余个代表性模型（见表1），并指出VLA4AD正推动自动驾驶向可解释、人机协同的方向发展。</details> |
| 2025-06-29 | IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering | http://arxiv.org/abs/2506.23329v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-27 | 4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration | http://arxiv.org/abs/2506.22242v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-26 | WorldVLA: Towards Autoregressive Action World Model | http://arxiv.org/abs/2506.21539v1 | <details><summary>展开</summary>该论文提出WorldVLA，一种自回归动作世界模型，核心贡献如下： 1. **统一框架**：整合视觉-语言-动作（VLA）模型与世界模型，通过共享词表的图像、文本、动作分词器，在单一自回归架构中实现动作生成与未来图像预测的联合学习。动作模型基于观测生成动作，世界模型利用动作预测环境状态，二者相互增强。 2. **注意力掩码策略**：针对自回归生成动作序列时的误差累积问题，提出选择性掩码机制。生成当前动作时屏蔽历史动作依赖，缓解预训练多模态模型在动作域的泛化不足，显著提升动作块生成性能（抓取成功率提升4%-23%）。 3. **实验验证**：在LIBERO基准测试中，WorldVLA超越独立模型： - 动作模型：抓取成功率提升4% - 世界模型：视频生成Fréchet视频距离（FVD）降低10% 验证了动作与世界模型的协同优化效应，证明统一框架在物理理解与决策生成上的优势。 [[论文链接](https://arxiv.org/abs/2506.21539)]</details> |
| 2025-06-26 | Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends | http://arxiv.org/abs/2506.20966v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-24 | Unified Vision-Language-Action Model | http://arxiv.org/abs/2506.19850v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-24 | CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation | http://arxiv.org/abs/2506.19816v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-23 | MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis | http://arxiv.org/abs/2506.18897v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-22 | RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation | http://arxiv.org/abs/2506.18088v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-21 | RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models | http://arxiv.org/abs/2506.17811v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-21 | RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models | http://arxiv.org/abs/2506.17639v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-21 | VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models | http://arxiv.org/abs/2506.17561v1 | <details><summary>展开</summary>本文系统研究了视觉-语言-动作（VLA）模型中任务规划的表示形式与范式，核心发现如下： ### 核心结论 1. **视觉规划表示优于语言表示** - 视觉推理（V）和图像前瞻规划（IF）在任务性能、泛化能力和推理速度上均优于语言规划（L） - 视觉表示的推理速度更快（IF比L快1.7倍），训练成本更低（IF训练时间比L减少30%） 2. **分层范式（Hierarchical-VLA）综合性能最佳** - 在LIBERO等6个基准测试（刚性/可变形物体、2D/3D环境、仿真/现实场景）中： - 任务成功率平均提升8.2%（最高达85.6%） - 泛化能力最强（在COLOSSEUM泛化测试中提升1.3%） - 支持多表示融合（L+V+IF）时性能最优（74.2%） 3. **训练与推理效率的权衡** - 分层范式训练成本增加40%，推理延迟增加50%（因需两阶段计算） - 集成范式（Integrated-VLA）隐式规划（I-I）性能次优（73.3%），但推理速度更快 ### 关键发现 - **规划预训练增益显著**：任务规划头部预训练可使成功率提升5.6-5.8% - **策略学习是瓶颈**：在LIBERO任务中，策略学习模块的失败率（35%）高于任务规划模块（15%） - **显式规划的缺陷**：集成范式的显式规划（I-E）因误差累积导致性能下降（比基准低13.5%） ### 实践意义 分层范式虽需更高算力，但为复杂操作任务提供了最优解决方案。未来工作可聚焦于优化其计算效率。 > 论文视频：https://nus-lins-lab.github.io/vlaos/ > 模型代码：开源在HuggingFace</details> |
| 2025-06-19 | CapsDT: Diffusion-Transformer for Capsule Robot Manipulation | http://arxiv.org/abs/2506.16263v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-19 | ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models | http://arxiv.org/abs/2506.16211v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-17 | FormGym: Doing Paperwork with Agents | http://arxiv.org/abs/2506.14079v1 | <details><summary>展开</summary>论文提出FormGym基准，用于评估代理在纯图像表单填写任务中的能力（无OCR或PDF文本访问）。核心要点如下： 1. **问题背景**：表单填写是耗时挑战，需多模态理解、信息检索和工具使用。纯图像领域（如扫描文档）尤其困难，代理需基于用户配置文件（236个特征）填充字段。 2. **FormGym基准**： - 包含432个字段、55个文档和3个任务（如自然语言输入、文档转移和数据库查询）。 - 评估代理动作（如放置文本、签名）和流程（单次或迭代完成）。 3. **基线表现**： - 视觉语言模型（VLAs）准确率普遍低于1%，主要因定位能力差。 - GUI代理准确率在10.6%–68.0%，但成本高、延迟高。 4. **解决方案FieldFinder**： - 辅助工具，帮助LLMs定位字段输入区域（预测边界框）。 - 集成后，所有模型性能提升：最大提升从2%到56%（如GPT-4o在FUNSD任务），平均提升显著。 5. **贡献与计划**： - 开源基准和FieldFinder工具（计划GitHub发布）。 - 分离语义理解与空间定位，缓解VLAs瓶颈。</details> |
| 2025-06-16 | GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics | http://arxiv.org/abs/2506.14009v1 | <details><summary>展开</summary>这篇论文提出了一种名为GRaD-Nav++的轻量级视觉-语言-动作（VLA）框架，用于实现无人机完全机载的自然语言导航。核心创新点包括： 1. **系统架构** - 采用轻量化设计（基于CLIP的视觉语言模型 + 混合专家策略网络），可在无人机嵌入式硬件上实时运行 - 引入MoE（Mixture-of-Experts）动作头，通过自适应计算路由提升泛化能力并减少遗忘 2. **训练方法** - 在3D高斯泼溅（3DGS）模拟器中构建高保真视觉环境 - 结合可微分强化学习（DiffRL）与可微分无人机动力学模型，实现高效策略优化 - 使用参考轨迹引导的多阶段任务训练范式 3. **核心性能** - **多任务泛化**：在训练任务上达到83%成功率，未见过任务上75%（仿真）；实物部署分别达67%和50% - **多环境适应**：跨不同仿真环境平均成功率81%，实物场景达67% - 相比现有方法（如RaceVLA/CognitiveDrone），在保持精度的同时显著降低计算需求 4. **技术贡献** - 首次实现完全机载的VLA导航，摆脱对外部基础设施依赖 - 建立视觉-语言指令与低层控制间的端到端映射 - 通过上下文估计器网络缓解仿真到现实的迁移差距 该框架为资源受限的无人机平台提供了可靠的语言引导导航新范式，在搜救、物流等场景具有应用潜力。</details> |
| 2025-06-16 | AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning | http://arxiv.org/abs/2506.13757v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction | http://arxiv.org/abs/2506.13751v3 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding | http://arxiv.org/abs/2506.13725v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | ROSA: Harnessing Robot States for Vision-Language and Action Alignment | http://arxiv.org/abs/2506.13679v1 | <details><summary>展开</summary>本文提出了一种名为ROSA（利用机器人状态实现视觉-语言-动作对齐）的新训练范式，核心贡献如下： 1. **问题识别**：现有视觉-语言-动作（VLA）模型在微调视觉-语言模型（VLM）时存在**空间-时间鸿沟**： - 空间上：VLM处理高层语义，而机器人动作需精确实时3D空间定位 - 时间上：VLM关注当前图像理解，而VLA需预测未来动作 2. **解决方案**： - **双模态数据训练**： - **专家动作数据**：人工收集的示教数据（成本高） - **机器人状态数据**：自动化采集末端执行器的3D位姿和夹爪状态（零人工成本） - **统一训练架构**：将状态估计与动作预测整合到同一VLA框架中，通过状态估计增强空间感知能力 3. **关键技术优势**： - **数据高效性**：在低数据场景（如100条专家轨迹）下，RLBench仿真任务成功率提升11.4%，真实机器人任务成功率翻倍 - **泛化能力**：在未见过的物体/容器任务上（如"方块入盒"），成功率较基线提升60% - **自动化扩展**：状态数据可通过机器人生成随机动作自动收集，支持大规模应用 4. **实验结果**： - **仿真（RLBench）**：12项任务平均成功率63.7%（100条专家数据），优于PerACT等非VLA方法 - **真实机器人（WidowX）**： - 已知任务：35%平均成功率提升 - 未知任务：85%平均成功率（基线43%） - **极端场景**：单样本训练在3项任务中实现非零成功率（基线全失败） 5. **核心价值**：通过机器人状态数据弥合语义与物理空间的鸿沟，显著降低VLA模型对人工示教数据的依赖，为高效机器人操控提供新范式。 > 注：ROSA基于标准LLaVA架构（CLIP视觉编码器+Qwen-2.5-7B语言模型），实验表明其在不同数据规模下均优于基线方法。</details> |
| 2025-06-15 | SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration | http://arxiv.org/abs/2506.12723v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models | http://arxiv.org/abs/2506.10100v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | SAFE: Multitask Failure Detection for Vision-Language-Action Models | http://arxiv.org/abs/2506.09937v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models | http://arxiv.org/abs/2506.09930v1 | <details><summary>展开</summary>这篇论文的核心贡献是提出了INT-ACT评测套件，用于系统评估视觉-语言-动作模型（VLA）的泛化能力。主要要点如下： 1. **问题背景** 当前VLA模型评估存在两大局限： - 传统机器人基准缺乏语言指令 - 新兴VLA基准任务单一，无法量化VLM预训练对策略泛化的贡献 - 真实机器人评测成本高，可复现性差 2. **INT-ACT评测套件** - 在SimplerEnv模拟器上构建，包含**50个任务**，覆盖三大维度： - **物体多样性**：引入分布外（OOD）物体（如工业工具）、新外观和非常规物体关系 - **语言复杂性**：涵盖动作改写（"Place A atop B"）、否定指令（"不要拿B"）、指代表述（"紫色物体"） - **视觉语言思维**：添加干扰物（如胡萝卜旁的兔子）、常识推理（"兔子最爱的蔬菜"） 3. **关键发现** - **意图-执行鸿沟**： - VLA在OOD场景下保持高意图正确率（80-100%），但任务成功率平均下降40-70% - 例如：更换目标物体（如键盘→轮子）使π₀模型抓取成功率从77%降至56% - **语言泛化退化**： - 微调后VLA的语言理解能力显著弱于原始VLM - 否定指令使任务成功率平均下降24%，外观描述指令下降18% - **多模态干扰脆弱性**： - 当语言常识与视觉干扰叠加（如"兔子最爱的蔬菜" + 兔子玩偶），错误物体操作率高达65% 4. **模型对比** - π₀-scratch表现最佳（平均任务成功率48.9%），但仍有显著泛化缺口 - 联合训练的Magma对语言变化更鲁棒，但动作执行能力较弱 - 非VLM基线Octo泛化能力最差（成功率<10%） 5. **开源贡献** - 发布评测套件和代码（https://ai4ce.github.io/INT-ACT/） - 呼吁社区关注VLA中感知-动作的转化瓶颈 论文揭示了当前VLA架构的局限：VLM提供强大的意图理解，但动作执行模块难以有效利用该能力，尤其在跨模态分布偏移时。这为未来改进方向提供了实证基础。</details> |
| 2025-06-11 | OctoNav: Towards Generalist Embodied Navigation | http://arxiv.org/abs/2506.09839v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models | http://arxiv.org/abs/2506.09172v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency | http://arxiv.org/abs/2506.08822v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing | http://arxiv.org/abs/2506.08462v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization | http://arxiv.org/abs/2506.08440v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation | http://arxiv.org/abs/2506.08296v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework | http://arxiv.org/abs/2506.08185v2 | <details><summary>展开</summary>论文提出了一种代理性手术AI框架，用于预测外科医生个性化手势序列并量化隐私风险。核心要点如下： 1. **问题与创新**： - 现有手术AI系统忽略外科医生的个性化风格（如训练、经验、运动行为）。 - 提出结合离散扩散模型和视觉-语言-动作（VLA）框架的新方法，将手势预测建模为结构化去噪任务。 2. **方法**： - **多模态输入**：融合手术视频的视觉特征、任务意图的语言描述，以及外科医生身份和技能（GRS分数）的嵌入。 - **隐私保护**：通过第三方大型语言模型（LLM）的自然语言提示（如“Surgeon ID: 3, GRS: 3.75”）生成嵌入，避免直接暴露身份。 - **扩散过程**：前向过程添加离散噪声，反向过程基于条件重建原始序列。 3. **实验与结果**： - 在JIGSAWS数据集上验证，ID+GRS的LLM嵌入策略性能最佳（Top-1准确率83.89%），能学习独特的外科医生运动指纹。 - **隐私风险**：成员推理攻击显示，更丰富的嵌入（如ID+GRS）虽提升任务性能，但显著增加身份泄露风险（AUC达1.0），揭示个性化与隐私的权衡。 4. **贡献与意义**： - 首次在手术AI中量化个性化建模的隐私风险。 - 强调临床部署需平衡个性化收益与隐私保护（如差分隐私）。 - 代码已开源。</details> |
| 2025-06-09 | BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models | http://arxiv.org/abs/2506.07961v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse | http://arxiv.org/abs/2506.07639v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation | http://arxiv.org/abs/2506.07530v1 | <details><summary>展开</summary>本文提出**BitVLA**，首个用于机器人操作的**1-bit视觉-语言-动作（VLA）模型**，核心创新如下： 1. **1-bit三元参数量化** - 所有权重参数压缩为三元值 `{-1, 0, 1}`，视觉编码器采用**1.58-bit权重+8-bit激活**（W1.58A8） - 通过**蒸馏感知训练策略**：以全精度视觉编码器为教师模型，结合语言建模损失（$\mathcal{L}_{\text{LM}}$）和表征对齐损失（$\mathcal{L}_{\text{aux}}$）压缩模型，减少量化误差 2. **高效架构设计** - 基于**1-bit LLM（BitNet b1.58 2B4T）** 和 **SigLIP-L视觉编码器** - 推理时采用**并行解码+动作分块**技术，提升实时控制效率 3. **性能与效率优势** - 在LIBERO机器人操作基准测试中，**性能媲美4-bit后量化的OpenVLA-OFT**（平均成功率94.8% vs 91.9%） - **内存占用仅为全精度模型的29.8%**（1.4GB vs 4.7GB），显著降低边缘设备部署门槛 - **无需大规模机器人预训练**，仅需视觉语言预训练+任务微调 4. **应用潜力** - 在视觉问答（VQA）任务中保持竞争力（零样本准确率51.5%） - 开源代码与模型权重，推动低资源机器人部署 > 论文链接：[https://github.com/ustcwhy/BitVLA](https://github.com/ustcwhy/BitVLA)</details> |
| 2025-06-09 | Real-Time Execution of Action Chunking Flow Policies | http://arxiv.org/abs/2506.07339v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-08 | Robotic Policy Learning via Human-assisted Action Preference Optimization | http://arxiv.org/abs/2506.07127v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-07 | RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation | http://arxiv.org/abs/2506.06677v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-06 | DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models | http://arxiv.org/abs/2506.05667v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-04 | SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models | http://arxiv.org/abs/2506.03574v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-03 | Adversarial Attacks on Robotic Vision Language Action Models | http://arxiv.org/abs/2506.03350v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-02 | SAB3R: Semantic-Augmented Backbone in 3D Reconstruction | http://arxiv.org/abs/2506.02112v2 | <details><summary>展开</summary>待生成</details> |
| 2025-06-02 | Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning | http://arxiv.org/abs/2506.01953v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-02 | SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics | http://arxiv.org/abs/2506.01844v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-01 | OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation | http://arxiv.org/abs/2506.01196v1 | <details><summary>展开</summary>待生成</details> |
| 2025-06-01 | GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering | http://arxiv.org/abs/2506.01174v1 | <details><summary>展开</summary>本文提出GraphPad，一种用于具身问答的可修改3D场景图记忆系统。针对静态场景图无法适应任务变化的缺陷，GraphPad的核心创新在于通过语言驱动的API实现推理时动态更新： 1. **问题背景**：传统预构建的3D场景图常遗漏任务关键信息（如物体/空间关系），导致具身问答（EQA）性能受限 2. **解决方案**： - **结构化场景记忆（SSM）**：包含场景图（带对象轨迹）、图形便签（任务注释）、帧内存（关键帧）和导航日志（帧索引） - **可修改API**：VLM通过`find_objects`（插入新物体）、`analyze_objects`（注释属性）、`analyze_frame`（联合更新）动态修补知识缺口 - **推理循环**：VLM迭代调用API（平均1.9次/查询），整合新信息至场景图 3. **关键结果**： - 在OpenEQA基准测试达**55.3%准确率**（+3.0pp优于同VLM图像基线），且输入帧数减少5倍 - 消融实验显示导航日志贡献最大（+9.6pp），帧级API优于节点级API - 在属性识别（+20.3pp）、功能推理（+5.7pp）等需细粒度分析的类别提升显著 4. **贡献价值**：无需额外训练/数据，通过语言引导的感知更新实现任务自适应记忆，为具身智能体提供高效动态工作空间 > 分析依据：论文摘要（§Abstract）、方法架构（§3）、实验结果（§4.1-4.5）及对比基线（Table 1）。</details> |
| 2025-05-31 | LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks | http://arxiv.org/abs/2506.00411v1 | <details><summary>展开</summary>论文提出了一种统一视觉-语言-动作模型LoHoVLA，用于解决长视野具身任务中的关键挑战。核心要点如下： 1. **问题背景**： - 现有视觉语言动作（VLA）模型在长视野任务（需多步骤规划）中表现不佳，缺乏有效的任务分解能力。 - 分层架构（独立规划器+控制器）存在协调性差和泛化能力弱的问题。 2. **模型创新**： - **统一框架**：LoHoVLA将高层任务规划（生成语言子任务）和低层动作控制（预测动作令牌）集成到单一模型中，共享预训练视觉语言模型（PaliGemma）的骨干网络。 - **分层闭环控制**：引入动态纠错机制——若子任务执行失败超过阈值则重新规划，否则仅更新动作，减少冗余计算。 3. **数据集**： - 构建合成数据集**LoHoSet**（基于Ravens模拟器），包含20个长视野任务，每个任务提供1000条专家演示（含视觉观测、语言目标、子任务和动作）。 4. **实验结果**： - 在Ravens模拟器上显著超越分层架构和标准VLA模型，尤其在未见过任务上展现强泛化能力。 - 消融实验验证了统一架构和闭环控制机制的有效性。 5. **意义**： - 为长视野具身任务提供端到端解决方案，证明统一架构在提升任务规划、动作执行和泛化性能方面的潜力。 总结：LoHoVLA通过统一规划和控制的联合表示学习，结合闭环纠错机制，解决了长视野任务中的协调与泛化问题，为具身智能提供了新思路。</details> |
| 2025-05-30 | Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction | http://arxiv.org/abs/2505.24156v1 | <details><summary>展开</summary>论文提出**CogRobot框架**，用于解决双机械臂操作策略泛化性差的问题。核心创新点包括： 1. **两阶段视频预测方法** - **文本→光流生成**：利用预训练文本-视频模型（CogVideoX），将语言指令转化为光流序列，捕捉物体运动趋势（减少语言歧义）。 - **光流→视频预测**：基于光流生成未来轨迹视频，显式建模机械臂与物体的动态交互（降低数据需求）。 2. **降低数据依赖** - 光流作为中间表示，仅需少量双臂操作数据微调模型，避免直接学习低层动作的高复杂度。 3. **实验验证** - 在仿真和真实双臂机器人平台（Realman）测试，相比基线方法： - 准确预测符合物理规律的运动轨迹 - 提升多步骤任务成功率（如开盖、放置物体） - 对未见场景展现强泛化性。 **关键效果**：通过光流桥接高层指令与底层动作，实现高效、可泛化的双臂协调策略，减少90%以上真实机器人数据需求。</details> |
| 2025-05-29 | Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models | http://arxiv.org/abs/2505.23757v1 | <details><summary>展开</summary>本文提出了Impromptu VLA数据集，旨在解决自动驾驶在非结构化道路（如边界模糊、临时交规变化等场景）的数据稀缺问题。核心贡献包括： 1. **新型数据集构建** - 从8个开源数据集（总计200万视频片段）中精选80,000+片段，聚焦四类挑战性场景：道路边界模糊、临时交规变化、非常规动态障碍物、恶劣路况 - 提供丰富的多任务标注：规划导向的问答对（场景描述、交通信号检测、动态意图预测等）及动作轨迹 2. **创新数据处理流程** - 建立基于VLM的层级分类法（Qwen2.5-VL + 思维链推理） - 通过时序稳定性过滤和人工验证确保标注质量 3. **显著性能提升验证** - **闭环测试**：在NeuroNCAP基准上，3B模型平均安全分提升21.5%（1.77→2.15/5.0），碰撞率降低9.7%（72.5%→65.5%） - **开环预测**：nuScenes轨迹预测L2误差达0.30m，接近SOTA方法（EMMA+的0.29m） - 诊断评估显示VLM在感知/预测/规划能力均有提升 数据集与模型已开源，项目页：http://Impromptu-VLA.c7w.tech/ 代码仓库：https://github.com/ahydchh/Impromptu-VLA</details> |
| 2025-05-29 | Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better | http://arxiv.org/abs/2505.23705v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-29 | TrackVLA: Embodied Visual Tracking in the Wild | http://arxiv.org/abs/2505.23189v1 | <details><summary>展开</summary>这篇论文提出了TrackVLA，一种视觉-语言-动作（VLA）模型，用于解决具身视觉跟踪（EVT）任务。核心创新点如下： 1. **统一架构设计**： - 提出并行预测框架，共享LLM主干（Vicuna-7B），同时处理目标识别（语言建模头）和轨迹规划（基于锚点的扩散模型） - 采用网格池化策略压缩视觉特征（EVA-CLIP编码），细粒度特征用于最新观测，粗粒度用于历史帧 2. **高效动作模型**： - 锚点扩散策略通过K-means聚类预定义轨迹模式，仅需2步去噪（比标准扩散模型快5倍） - 联合优化轨迹回归损失（MSE）和分类损失（BCE） 3. **大规模数据集**： - 构建EVT-Bench数据集（21,771训练/4,215测试场景），包含： - 100+高保真人体化身（SMPL-X模型 + 随机纹理） - 三类难度任务：单目标跟踪/干扰环境跟踪/外观模糊目标跟踪 - 收集855K跟踪样本 + 855K识别样本（362K人体识别 + 493K开放世界VQA） 4. **性能优势**： - 在Gym-UnrealCV等基准上零样本达到SOTA - 10 FPS实时推理，鲁棒应对遮挡和动态场景 - 消融实验验证：1:1的跟踪-识别数据比例最优 5. **开源贡献**： - 公开EVT-Bench数据集和代码（项目页：https://pku-epic.github.io/TrackVLA-web） 该方法通过协同优化识别与规划，解决了传统模块化方法错误累积的瓶颈，在仿真和现实场景均展现强泛化能力。</details> |
| 2025-05-28 | Zero-Shot 3D Visual Grounding from Vision-Language Models | http://arxiv.org/abs/2505.22429v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-28 | ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation | http://arxiv.org/abs/2505.22159v3 | <details><summary>展开</summary>待生成</details> |
| 2025-05-28 | ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge | http://arxiv.org/abs/2505.21906v2 | <details><summary>展开</summary>论文提出ChatVLA-2，一个新颖的视觉-语言-动作（VLA）模型，旨在通过预训练知识实现开放世界具身推理。核心创新点包括： 1. **模型架构**：采用动态混合专家（Mixture-of-Expert）机制，解耦多模态理解与机器人控制的特征空间，同时保留共享表示；引入推理跟随增强模块，确保动作输出与内部推理一致。 2. **训练策略**：两阶段管道——第一阶段联合训练图像-文本数据与机器人数据，保留预训练视觉语言模型（VLM）知识；第二阶段冻结VLM，仅微调动作专家，强化推理-动作对齐。 3. **能力验证**：通过数学匹配（解决未见过方程并选择数字卡片）和玩具放置（处理新物体和空间指令）实验，证明模型在OCR、数学推理和空间理解上的开放世界泛化能力，显著超越OpenVLA、DexVLA等基线。 该方法有效解决了VLA模型微调时知识丢失问题，推动了具身推理的通用化发展。</details> |
| 2025-05-27 | Hume: Introducing System-2 Thinking in Visual-Language-Action Model | http://arxiv.org/abs/2505.21432v4 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models | http://arxiv.org/abs/2505.21200v1 | <details><summary>展开</summary>这篇论文提出了一种名为**FlashVLA**的高效推理框架，用于加速视觉-语言-动作（VLA）模型。核心创新点如下： ### 核心问题 - VLA模型（如OpenVLA）存在**双重冗余**： 1. **动作级冗余**：连续动作步骤的输出高度相似（>90%步骤变化微小）。 2. **视觉令牌冗余**：大量视觉令牌对推理贡献度低（如附录C所示）。 ### 解决方案：FlashVLA框架 1. **令牌感知动作重用（Token-Aware Action Reuse）** - **动态跳过机制**：通过轻量级"FlashTrigger"模块判断当前帧是否满足重用条件： - 计算连续动作向量的夹角变化（公式7） - 结合视觉令牌稳定性分析 - 若满足条件（如动作方向变化<阈值），则直接复用前一帧动作，**跳过完整推理**。 2. **信息导向的视觉令牌压缩（Information-Guided Token Pruning）** - 基于**信息贡献理论**（公式2）： - 对注意力输出矩阵进行SVD分解 - 计算每个令牌的信息贡献得分 \( C(x) = \sum \|u_{xi} \sigma_i\| \) - 保留高贡献令牌（如Top 62.5%），显著降低计算量（FLOPs减少55.7%）。 ### 关键优势 - **无需训练**：即插即用，兼容Flash Attention等现有优化。 - **双路径加速**： - **重用路径**：零计算成本（稳定动作阶段） - **轻量路径**：压缩视觉令牌（动态变化阶段） - **效率提升**（LIBERO基准测试）： - FLOPs降低55.7% - 推理延迟减少36.0% - 任务成功率仅下降0.7%（99.3%→98.6%） ### 实验验证 - 在**LIBERO**四大任务（Spatial/Object/Goal/Long）上验证： - 视觉令牌压缩至62.5%时，计算效率最优。 - 消融实验证明动作重用贡献延迟降低的70%。 - 泛化性：在VLAbench环境保持>98%成功率。 ### 意义 首次实现VLA模型的**训练无关加速**，为实时机器人控制提供轻量级解决方案，符合"三思而后行（Think Twice, Act Once）"的高效执行范式。</details> |
| 2025-05-27 | EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models | http://arxiv.org/abs/2505.21567v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Hierarchical Instruction-aware Embodied Visual Tracking | http://arxiv.org/abs/2505.20710v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | What Can RL Bring to VLA Generalization? An Empirical Study | http://arxiv.org/abs/2505.19789v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback | http://arxiv.org/abs/2505.19767v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving | http://arxiv.org/abs/2505.19381v4 | <details><summary>展开</summary>待生成</details> |
| 2025-05-25 | ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning | http://arxiv.org/abs/2505.19080v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-24 | Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance | http://arxiv.org/abs/2505.18793v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-24 | VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning | http://arxiv.org/abs/2505.18719v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems | http://arxiv.org/abs/2505.17295v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | Interactive Post-Training for Vision-Language-Action Models | http://arxiv.org/abs/2505.17016v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving | http://arxiv.org/abs/2505.16278v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning | http://arxiv.org/abs/2505.15725v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems | http://arxiv.org/abs/2505.15685v1 | <details><summary>展开</summary>这篇论文系统比较了三种基础模型（FMs）在具身机器人系统中的集成范式：端到端视觉-语言-动作（VLA）模型、模块化视觉语言模型（VLM）流水线，以及多模态大语言模型（LLM）代理。核心发现如下： 1. **指令落地任务评估** - **多模态LLM代理**（如Gemini 2.5-Pro）在复杂指令理解（如隐含对象指代和空间关系推理）中表现最优（平均准确率82.1%），但计算成本高且依赖大规模闭源模型。 - **模块化VLM流水线**（如GroundingDINO）参数量仅为LLM的1%-6%，数据效率高且可解释性强，但在复杂指令处理（如“拧螺丝工具”）和错误传播方面存在局限（平均准确率40.8%）。 - **量化影响**：LLM的INT4量化使模型尺寸减小70%，但显著损害关系型指令理解能力（准确率下降14%-17%）。 2. **机器人操作任务评估** - **VLA模型微调**：扩散模型（如π₀）在仿真环境LIBERO任务中表现最佳（平均成功率94.15%），但需大量数据；自回归模型（如OpenVLA）微调收敛慢且方差大。 - **泛化缺陷**：所有VLA模型在未见物体干扰下性能显著下降（平均成功率降幅＞15%），且仿真到实机迁移存在明显性能断层。 - **动作分块技术**（Action Chunking）可提升长时序任务表现（如NORA在LIBERO-Long任务成功率从45%升至74.6%）。 3. **关键约束与启示** - **数据稀缺**：机器人数据获取成本高，需开发仿真到实机的鲁棒迁移方法。 - **泛化瓶颈**：VLA模型对分布外物体和场景变化敏感，需融合空间推理模块。 - **部署效率**：LLM的高计算需求与机器人硬件存在矛盾，建议采用轻量化架构（如GPT-4o-mini）或分层决策系统。 论文通过公开数据集和爪机机器人原型（https://github.com/HRItdy/claw_machine）验证了上述结论，为语言驱动机器人系统的设计提供了实证依据。</details> |
| 2025-05-21 | Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization | http://arxiv.org/abs/2505.15660v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | FLARE: Robot Learning with Implicit World Modeling | http://arxiv.org/abs/2505.15659v1 | <details><summary>展开</summary>论文提出了一种名为**FLARE（Future Latent Representation Alignment）**的机器人学习框架，其核心创新点是通过隐式世界建模提升策略性能。主要要点如下： 1. **方法设计** - FLARE通过**未来潜在表示对齐**将预测性世界建模集成到策略学习中：在扩散变换器（DiT）中引入少量可学习的"未来令牌"，使其隐藏状态能够预测未来观测的紧凑嵌入表示。 - 采用**双目标训练**：结合动作流匹配损失（生成动作）和未来潜在对齐损失（预测未来状态），无需显式重建图像帧。 - 提出**动作感知嵌入模型**：基于Q-Former压缩视觉-语言特征，并通过跨具身数据预训练提升泛化能力。 2. **性能优势** - 在24个RoboCasa单臂操作任务和24个GR1人形机器人桌面操作任务上达到SOTA，平均成功率分别为**70.1%**（优于基线26%）和**55.0%**（优于基线25%）。 - **轻量化设计**：仅需对标准VLA模型添加少量令牌，计算开销极低。 3. **关键应用** - **数据高效迁移**：利用预训练嵌入模型，仅需100条真实机器人轨迹即可实现95%的实际任务成功率。 - **无动作标签学习**：结合人类第一视角视频（无动作标签）和单条机器人演示，成功学习新物体的抓取策略，成功率提升至80%（比纯动作训练高一倍）。 4. **创新意义** 提供了一种可扩展的隐式世界建模范式，显著提升策略的长期推理能力和跨任务泛化性，同时兼容多模态数据源。</details> |
| 2025-05-21 | Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control | http://arxiv.org/abs/2505.15304v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy | http://arxiv.org/abs/2505.15206v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation | http://arxiv.org/abs/2505.15098v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-20 | AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory | http://arxiv.org/abs/2505.14030v3 | <details><summary>展开</summary>待生成</details> |
| 2025-05-20 | InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning | http://arxiv.org/abs/2505.13888v2 | <details><summary>展开</summary>论文提出了一种名为**InSpire**（内禀空间推理）的方法，旨在提升视觉-语言-动作模型（VLAs）的空间推理能力，以解决其因虚假相关性导致的泛化能力不足问题。核心要点如下： 1. **问题背景**： - 现有VLAs在预测动作时易受任务无关视觉特征的干扰（如背景噪声），忽视语言指令和空间关系，导致在训练数据分布外的场景泛化能力差。 2. **方法创新**： - **空间推理问答机制**：在语言指令前添加固定问题 **“目标物体相对于机器人的方位？”**（如“黑色碗在机器人的哪个方向？”），并约束模型从预定义方向（左/右/前/后/抓取中等）生成答案。 - **双阶段推理**： - **空间感知阶段**：模型先回答物体的空间方位问题，生成文本描述（如“碗在右侧”）。 - **动作生成阶段**：将此文本描述与原始指令、视觉观察结合，输出最终动作。 - **无需额外资源**：作为即插即用模块，无需额外训练数据或外部大模型交互，兼容现有自回归VLAs（如miniVLA-VQ、π₀-FAST）。 3. **技术实现**： - 通过**自动化规则标注**物体与机器人的空间关系（利用仿真环境或真实交互的3D位置数据），构建训练所需的方位标签（图3）。 - 训练时对齐模型生成的方位答案与真实空间关系，引导模型聚焦任务相关特征。 4. **实验结果**： - **仿真环境**：miniVLA-VQ的已见/未见任务成功率提升**6.2%**和**10%**。 - **真实环境**：π₀-FAST的已见/未见任务成功率提升**25%**和**26%**。 - 基于LIBERO基准训练的**InspireVLA-1B模型**在性能与计算效率上优于现有推理型VLAs。 5. **贡献总结**： - 提出InSpire方法，通过空间推理缓解虚假相关性。 - 实现即插即用的空间能力增强，无需额外成本。 - 仿真与真实场景实验验证了方法的有效性与灵活性。 该方法通过显式空间推理桥接观察与动作，显著提升了VLAs在复杂任务中的鲁棒性和泛化能力。</details> |
| 2025-05-19 | SPKLIP: Aligning Spike Video Streams with Natural Language | http://arxiv.org/abs/2505.12656v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-18 | RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction | http://arxiv.org/abs/2505.12224v3 | <details><summary>展开</summary>待生成</details> |
| 2025-05-16 | Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions | http://arxiv.org/abs/2505.11214v1 | <details><summary>展开</summary>这篇论文提出了一种新型视觉-语言-动作模型OE-VLA，旨在解决传统VLA模型仅支持语言指令的局限性。核心贡献如下： 1. **模型创新** - 提出OE-VLA架构，首次支持**开放式多模态指令**（如图像目标、手写指令板、演示视频和视觉目标状态） - 基于LLaVA-Next-Interleave基础模型构建，包含视觉编码器（SigLIP-ViT）、投影层和LLM主干（Qwen-1.5） - 采用离散化动作标记输出，通过统一架构处理多模态输入序列 2. **数据与方法** - 设计**自动数据转换方法**：将现有机器人数据集扩展为多模态指令数据集（VOS/OIF/VGR/VDL四类任务） - 提出**两阶段训练策略**： - 阶段1：多图像基础训练（MGrounding数据集）提升空间关系理解 - 阶段2：开放式指令微调（混合多模态指令数据） 3. **评估体系** - 构建新基准**OE-CALVIN**（含base/hard两个版本），基于CALVIN测试集但替换为多模态指令 - hard版引入来自互联网的图像/视频，增加跨域泛化难度 4. **实验结果** - 语言任务：OE-VLA-7B在CALVIN ABC→D基准达2.99平均成功率（SOTA） - 多模态任务： - OE-CALVIN_base：平均成功率2.75（1B模型）→3.48（7B模型） - 视觉目标指定（VOS）表现最优（7B模型达3.46） - 消融实验验证两阶段训练的有效性（尤其跨域场景） 该方法显著扩展了人机交互场景，为机器人处理开放世界指令提供了新范式。</details> |
| 2025-05-16 | Conditioning Matters: Training Diffusion Policies is Faster Than You Think | http://arxiv.org/abs/2505.11123v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-14 | Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware | http://arxiv.org/abs/2505.09601v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-14 | VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation | http://arxiv.org/abs/2505.09577v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-13 | From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation | http://arxiv.org/abs/2505.08548v2 | <details><summary>展开</summary>本文提出了一种名为FSD（From Seeing to Doing）的新型视觉语言模型，旨在解决机器人操作中泛化能力不足的问题。核心创新点是通过空间关系推理生成中间视觉辅助表示，为机器人操作提供细粒度指导。主要贡献包括： 1. **空间关系链式推理（SrCoT）**：建立以物体坐标和空间关系为锚点的多步推理机制，将视觉辅助生成转化为推理过程。通过构建空间关系图（描述阶段）和坐标迭代推导（推理阶段），显著提升空间理解能力。 2. **弱到强能力数据构建**：设计五级分层数据集（区域定位→空间关系→空间推理→空间可供性生成→视觉轨迹生成），结合大规模具身数据集（BridgeDataV2、RT-X等）和常识数据，通过自动化流程生成30万条训练数据。 3. **自对齐机制**：通过双向任务（图像+指令→轨迹 vs. 图像+轨迹→指令）统一空间理解与生成能力，解决坐标空间与视觉信号的对齐问题。 4. **VABench基准**：构建包含300个手动标注样本的视觉辅助生成评测集，涵盖真实场景和仿真任务。 实验验证： - 在8个空间推理基准上排名第一（平均1.3位），尤其在3D深度感知（88.0%）和空间关系（78.3%）任务表现突出 - 物体定位任务（RoboRefIt）准确率56.7%，超越GPT-4o（15.3%） - 零-shot机器人操作：在SimplerEnv仿真环境达到40.6%成功率，真实世界8项任务平均成功率72%，超越基线方法30% - 视觉轨迹生成（VABench）的RMSE降至78.26，GPT评分达6.21分 FSD通过中间视觉辅助表示（空间可供点/框、物体轨迹）有效桥接视觉推理与动作决策，为跨场景、跨平台的通用机器人操作提供新范式。 论文链接：https://embodied-fsd.github.io/ 代码仓库：https://github.com/pickxiguapi/Embodied-FSD</details> |
| 2025-05-13 | Training Strategies for Efficient Embodied Reasoning | http://arxiv.org/abs/2505.08243v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-12 | ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning | http://arxiv.org/abs/2505.07395v1 | <details><summary>展开</summary>论文提出ReinboT（强化机器人GPT），一种融合强化学习（RL）的视觉-语言-动作（VLA）模型，旨在解决机器人操作任务中训练数据质量不均的问题。核心创新点包括： 1. **RL与VLA融合机制** - 引入密集奖励函数（含子目标达成、任务进度、行为平滑度、任务完成度四个加权组件），量化操作任务质量。 - 通过期望回归（expectile regression）预测最大化累积回报（ReturnToGo），使模型学习数据分布中的最优策略。 2. **端到端架构设计** - 基于GPT架构，新增[RTG]令牌预测累积回报，利用历史观测（图像+本体感知）和语言指令联合建模。 - 解码器分支出动作和未来状态预测头，实现多任务学习。 3. **实验性能优势** - 在CALVIN混合质量数据集上达到SOTA，显著超越模仿学习基线。 - 验证了少样本学习能力及真实场景的分布外泛化性，例如在稀疏奖励任务中成功率提升35%以上。 该方法通过奖励稠密化和回报最大化机制，提升了机器人对数据质量分布的感知能力，为复杂操作任务提供了鲁棒决策基础。</details> |
| 2025-05-09 | UniVLA: Learning to Act Anywhere with Task-centric Latent Actions | http://arxiv.org/abs/2505.06111v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-09 | 3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks | http://arxiv.org/abs/2505.05800v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-08 | Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments | http://arxiv.org/abs/2505.05540v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-07 | Vision-Language-Action Models: Concepts, Progress, Applications and Challenges | http://arxiv.org/abs/2505.04769v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation | http://arxiv.org/abs/2505.03912v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration | http://arxiv.org/abs/2505.03673v2 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | Task Reconstruction and Extrapolation for $π_0$ using Text Latent | http://arxiv.org/abs/2505.03500v4 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data | http://arxiv.org/abs/2505.03233v3 | <details><summary>展开</summary>这篇论文提出了GraspVLA，一个基于十亿级合成动作数据预训练的抓取基础模型。核心要点如下： 1. **合成数据集创新** - 构建了**SynGrasp-1B**数据集（全球首个十亿帧规模抓取数据集）： - 通过物理仿真和照片级渲染生成 - 包含10,680个物体（240个类别） - 覆盖光照/背景/视角/高度等多样化场景 - 生成效率提升策略：异步写入、并行渲染、单步运动规划 2. **模型架构突破** - **Progressive Action Generation (PAG)** 机制： - 将感知任务（视觉定位、抓取位姿预测）作为动作生成的中间步骤 - 形成链式推理（Chain-of-Thought）统一框架 - 多模态融合设计： - 视觉语言主干（DINO-v2 + SigLIP + InternLM2） - 基于流匹配的动作生成器 - 支持合成动作数据与互联网语义数据协同训练 3. **关键性能优势** - **零样本泛化能力**： - 真实世界测试：在合成/网络类别物体上均达93%抓取成功率 - 超越RT-2、OpenVLA等基线模型（SPL指标提升35%+） - 透明物体抓取成功率86.6%（显著优于AnyGrasp的10%） - **小样本适应性**： - 仅需100条演示数据即可适应新任务（如避碰抓杯、密集场景顺序抓取） - 边界框标注即可扩展新物体类别（无需动作标注） - 数据规模定律：性能随训练帧数稳定提升（网络类别泛化需更多数据） 4. **工程贡献** - 开源SynGrasp-1B数据集与预训练模型 - 支持新机械臂/相机配置快速适配（5k样本/1天微调） - 项目页面：https://pku-epic.github.io/GraspVLA-web 该方法首次验证了纯合成数据训练VLA模型的可行性，显著降低机器人基础模型对真实动作数据的依赖。</details> |
| 2025-05-06 | Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets | http://arxiv.org/abs/2505.03174v1 | <details><summary>展开</summary>待生成</details> |
| 2025-05-04 | Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions | http://arxiv.org/abs/2505.02152v1 | <details><summary>展开</summary>待生成</details> |
| 2025-04-28 | NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks | http://arxiv.org/abs/2504.19854v1 | <details><summary>展开</summary>待生成</details> |
| 2025-04-22 | $π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization | http://arxiv.org/abs/2504.16054v1 | <details><summary>展开</summary>待生成</details> |
| 2025-04-01 | Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning | http://arxiv.org/abs/2504.00907v2 | <details><summary>展开</summary>本文提出了一种新型任务“Ask-to-Act”，旨在解决具身智能体在模糊指令下的交互问题，核心贡献和创新点如下： ### 核心问题 - **模糊指令处理**：真实环境中（如家庭场景），人类指令常存在歧义（例如“拿杯子”但存在多个杯子）。传统方法需依赖大规模人类演示或手动设计奖励函数，成本高昂且难以推广。 ### 解决方案 1. **Ask-to-Act任务**： - 智能体在部分可观测环境中执行物体抓取任务（如“将杯子放到咖啡桌”）。 - 需通过**最少量的澄清问题**（如“您要红色的杯子吗？”）消除歧义，涉及五类模糊性：物体属性、空间关系、尺寸及其组合。 - 评估指标包括成功率（SR）、歧义解决效率（ARS）和提问比例（QR）。 2. **方法创新**： - **多模态LLM（MLLM）适配**：将LLaVA-OneVision等MLLM改造为视觉-语言-动作（VLA）策略模型。 - **强化学习+LLM奖励**： - 使用LLM（如Llama-3）生成密集奖励函数，替代人工设计奖励。 - 奖励函数涵盖任务成功、子目标达成、有效提问及超预算惩罚（公式1）。 - **语法约束解码**：限制动作预测空间，确保输出有效的技能或问题。 ### 实验结果 - **显著优势**：在Habitat 3.0的83个场景中测试，RL微调的MLLM优于所有基线： - **新场景泛化**：成功率提升40.3%（对比GPT-4o等零样本基线）。 - **新任务泛化**：成功率提升19.1%（处理未见的模糊组合）。 - **关键发现**： - 仅依赖子目标奖励的模型性能接近随机水平，验证密集奖励的必要性。 - 增加提问预算可提升任务成功率及泛化能力。 ### 意义 - **首个端到端框架**：首次实现MLLM同时执行动作和主动提问，且无需人类演示或手动奖励工程。 - **新评估基准**：提供包含63个训练场景/20个测试场景的数据集，推动具身智能的交互研究。 论文通过LLM生成的奖励信号，解决了模糊指令下智能体决策与交互的关键挑战，为可扩展的具身智能训练提供了新范式。</details> |
| 2025-03-30 | OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model | http://arxiv.org/abs/2503.23463v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-27 | CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models | http://arxiv.org/abs/2503.22020v1 | <details><summary>展开</summary>这篇论文提出了一种名为CoT-VLA的创新方法，通过在视觉-语言-动作模型（VLA）中引入视觉思维链（Visual Chain-of-Thought, CoT）推理机制，显著提升了机器人任务的性能。以下是核心要点： ### 1. **核心创新：视觉思维链推理** - **问题背景**：传统VLA模型直接从观察映射到动作，缺乏中间推理步骤，限制了复杂任务的表现。 - **解决方案**：CoT-VLA引入**子目标图像生成**作为中间推理步骤： - 首先生成未来帧图像（子目标），表示任务预期的中间状态（视觉推理）。 - 然后基于当前观察和子目标图像预测动作序列（动作生成）。 - **优势**：子目标图像天然存在于演示数据中，无需额外标注；同时可兼容无动作标注的视频数据（如EPIC-KITCHENS），提升视觉推理能力。 ### 2. **模型架构与技术设计** - **基础模型**：基于多模态生成模型VILA-U构建，支持图像和文本的联合理解与生成。 - **混合注意力机制**： - **因果注意力**：用于自回归生成子目标图像和文本。 - **完全注意力**：并行预测所有动作维度，提升动作生成效率。 - **动作分块预测**：一次性输出多步动作序列（而非单步），增强时序连贯性。 ### 3. **训练与部署策略** - **两阶段训练**： - **预训练**：结合机器人演示数据（Open X-Embodiment）和无动作视频数据（EPIC-KITCHENS/Something-Something V2），优化视觉令牌和动作令牌预测。 - **微调**：在下游任务数据上适配，实现闭环控制。 - **损失函数**：视觉生成损失（$\mathcal{L}_{\text{visual}}$）与动作预测损失（$\mathcal{L}_{\text{action}}$）联合优化。 ### 4. **实验效果** - **显著提升**：在仿真（LIBERO）和真实机器人任务（Bridge-V2、Franka-Tabletop）中全面验证： - **LIBERO基准**：平均成功率81.13%，超越最佳基线（OpenVLA）6%。 - **真实任务**：成功率提升17%，尤其在长时程任务（+15.3%）和空间推理任务（+8.6%）上优势明显。 - **关键优势**：视觉CoT机制显著提升复杂任务的可解释性和动作规划准确性。 ### 5. **贡献总结** - 首创**视觉思维链推理**框架，将子目标图像作为中间表示。 - 提出**混合注意力机制**，平衡生成与预测效率。 - 实验证明**多源数据训练**（含无标注视频）的有效性，为VLA模型提供新范式。 > 论文链接：[https://cot-vla.github.io/](https://cot-vla.github.io/)</details> |
| 2025-03-26 | MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation | http://arxiv.org/abs/2503.20384v2 | <details><summary>展开</summary>本文提出MoLe-VLA框架，通过动态层跳跃机制提升机器人操作中多模态大语言模型（MLLM）的效率与性能。核心要点如下： 1. **问题背景** MLLM在机器人任务中面临高计算开销问题（如7B模型仅5-12Hz推理速度），难以满足实时控制需求（50-1000Hz）。现有稀疏化方法（如早期退出）易忽略关键语义层，导致性能下降。 2. **核心创新** - **混合层机制（MoLe）**：受神经科学"浅脑假说"启发，将每个LLM层视为独立专家，通过路由器动态激活部分层（类似脑信号路径选择），跳过冗余计算。 - **时空感知路由器（STAR）**：融合视觉空间特征与语言时序依赖，精准选择激活层（如优先保留高层语义层），提升决策准确性。 - **认知自蒸馏（CogKD）**：引入可学习的"认知令牌"，用完整模型指导层跳跃模型恢复丢失的语义信息，通过任务相关特征对齐提升鲁棒性。 3. **关键成果** - **效率**：最高减少5.6倍LLM计算量，适用于资源受限的机器人平台。 - **性能**：在RLBench仿真和真实任务中平均成功率提升8%，实现效率与精度共赢。 - **通用性**：可适配多种VLA模型（如OpenVLA），支持动态环境下的实时推理。 4. **验证场景** 在10项RLBench任务（如物体抓取、场景交互）和Franka机械臂真实部署中验证有效性，显著优于传统稀疏化方法（如MoE、早期退出）。 总结：MoLe-VLA通过神经科学启发的动态架构与知识蒸馏技术，解决了机器人任务中MLLM部署的效率瓶颈，为轻量化具身智能系统提供新思路。</details> |
| 2025-03-25 | Gemini Robotics: Bringing AI into the Physical World | http://arxiv.org/abs/2503.20020v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-25 | Boosting Robotic Manipulation Generalization with Minimal Costly Data | http://arxiv.org/abs/2503.19516v2 | <details><summary>展开</summary>待生成</details> |
| 2025-03-20 | JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse | http://arxiv.org/abs/2503.16365v2 | <details><summary>展开</summary>待生成</details> |
| 2025-03-20 | IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes | http://arxiv.org/abs/2503.17406v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-18 | GR00T N1: An Open Foundation Model for Generalist Humanoid Robots | http://arxiv.org/abs/2503.14734v2 | <details><summary>展开</summary>待生成</details> |
| 2025-03-17 | MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation | http://arxiv.org/abs/2503.13446v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-15 | ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis | http://arxiv.org/abs/2503.14526v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-13 | HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model | http://arxiv.org/abs/2503.10631v3 | <details><summary>展开</summary>待生成</details> |
| 2025-03-12 | CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games | http://arxiv.org/abs/2503.09527v1 | <details><summary>展开</summary>本文提出CombatVLA，一种面向3D动作角色扮演游戏（如《黑神话：悟空》）战斗任务的高效视觉-语言-动作模型。核心创新点如下： 1. **问题定位** 针对现有视觉-语言-动作模型（VLA）在复杂3D环境中实时决策的不足（高分辨率感知、动态战术推理、秒级响应需求），尤其战斗场景中的延迟敏感性问题。 2. **关键技术贡献** - **动作追踪器**：轻量级工具，后台录制玩家键盘/鼠标操作与游戏画面帧，通过时间戳对齐生成视频-动作对数据（公式1）。 - **战斗理解基准（CUBench）**：包含三类任务（单帧采集/多帧理解/多帧推理），评估模型对敌人定位、攻击模式识别等战斗智商（图3）。 - **动作思维链（AoT）**：将追踪数据转化为结构化序列（动作+解释），引入截断符`⟨TRUNC⟩`加速推理（图4）。 - **三阶段渐进训练**： - 粗粒度视频级AoT学习战斗环境 - 细粒度帧级AoT学习精确响应 - 截断策略优化实现高效输出 - **自适应动作加权损失**：结合模态对比损失（公式3）和动作对齐损失（公式4），优先学习关键动作类别。 3. **模型性能** - 仅3B参数量，推理速度比GPT-4o等VLM方案快50倍（图1）。 - 在CUBench上超越所有对比模型（包括GPT-4o和Qwen2.5-VL）。 - 实际游戏成功率高于人类玩家。 4. **部署框架** 集成动作执行代理，通过截断AoT策略实现实时控制（图2e），可操作真实PC设备。 5. **开源承诺** 将公开动作追踪器、数据集、基准测试、模型权重及训练代码（项目页：https://combatvla.github.io/）。</details> |
| 2025-03-11 | MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models | http://arxiv.org/abs/2503.08007v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-10 | PointVLA: Injecting the 3D World into Vision-Language-Action Models | http://arxiv.org/abs/2503.07511v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-06 | Refined Policy Distillation: From VLA Generalists to RL Experts | http://arxiv.org/abs/2503.05833v2 | <details><summary>展开</summary>待生成</details> |
| 2025-03-06 | VLA Model-Expert Collaboration for Bi-directional Manipulation Learning | http://arxiv.org/abs/2503.04163v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-05 | OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction | http://arxiv.org/abs/2503.03734v3 | <details><summary>展开</summary>待生成</details> |
| 2025-03-05 | SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning | http://arxiv.org/abs/2503.03480v2 | <details><summary>展开</summary>待生成</details> |
| 2025-03-04 | RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour | http://arxiv.org/abs/2503.02572v1 | <details><summary>展开</summary>论文提出**RaceVLA**，首个基于**视觉-语言-动作（VLA）模型**的竞速无人机导航系统，核心要点如下： 1. **核心创新**： - 利用**第一人称视角（FPV）视频流**和**自然语言指令**，直接生成无人机四维控制信号（`Vx, Vy, Vz, ω`），模拟人类飞行员的实时决策行为。 - 基于OpenVLA-7B模型微调，将机械臂的7维动作输出简化为适应无人机的4维控制（线速度+偏航角速度）。 2. **技术实现**： - **系统架构**：搭载RealSense T265摄像头的8英寸竞速无人机，通过ROS框架与地面服务器通信；模型经**int8量化**优化，推理速度达**4Hz**。 - **数据集**：收集200个飞行片段（约2万张图像），包含拱门/方形门导航、环形赛道等任务，使用RLDS格式存储动作、图像及语言指令。 - **训练**：采用LoRA技术（rank-32）高效微调，在A100 GPU上训练7000步（学习率5e⁻⁴）。 3. **性能优势**： - **泛化能力**：在**运动泛化**（75.0 vs. 60.0）和**语义泛化**（45.5 vs. 36.3）上超越OpenVLA；**全面优于RT-2模型**（视觉79.6 vs. 52.0，运动75.0 vs. 55.0，物理50.0 vs. 26.7，语义45.5 vs. 38.8）。 - **实时表现**：赛道任务平均速度**1.04 m/s**（最高2.02 m/s），平均偏航角速度0.40 rad/s，验证高速场景适应性。 4. **局限与改进**： - **动态环境挑战**：视觉泛化（79.6 vs. OpenVLA 87.0）和物理泛化（50.0 vs. 76.7）稍弱，主因是运动中物体尺寸变化及避障复杂性。 - 未来将提升视觉/物理泛化能力，并优化推理速度以支持更高飞行速度。 5. **开源贡献**： - 公开**代码库、预训练模型及数据集**（项目页：https://racevla.github.io/），推动无人机自主导航研究。 **总结**：RaceVLA首次将VLA模型应用于竞速无人机，实现类人决策的高适应性导航，为动态环境中的自主飞行设立新基准。</details> |
| 2025-03-04 | Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding | http://arxiv.org/abs/2503.02310v1 | <details><summary>展开</summary>待生成</details> |
| 2025-03-03 | CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs | http://arxiv.org/abs/2503.01378v1 | <details><summary>展开</summary>待生成</details> |
| 2025-02-27 | Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success | http://arxiv.org/abs/2502.19645v2 | <details><summary>展开</summary>待生成</details> |
| 2025-02-26 | ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration | http://arxiv.org/abs/2502.19250v2 | <details><summary>展开</summary>待生成</details> |
| 2025-02-24 | Evolution 6.0: Evolving Robotic Capabilities Through Generative Design | http://arxiv.org/abs/2502.17034v4 | <details><summary>展开</summary>待生成</details> |
| 2025-02-20 | Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration | http://arxiv.org/abs/2502.14795v2 | <details><summary>展开</summary>本文是ICML 2025会议的投稿与格式指南，主要要点如下： 1. **提交要求**： - 电子提交（PDF格式），通过会议网站进行。 - 初始提交匿名（双盲评审），不得包含作者信息或致谢。 - 页面限制：正文不超过8页（参考文献和附录不计入页数），总文件大小≤10MB；最终版本可额外增加1页正文。 - 附录必须与正文合并为一个文件。 2. **格式规范**： - 文本分两列，宽度6.75英寸，行距11点，使用10号Times字体。 - 标题居中（14号粗体），摘要为单段落（4–6句话）。 - 图标题置于下方，表标题置于上方；图表需清晰且仅使用矢量或无损格式。 - 参考文献采用APA格式，按时间顺序排列并包含页码。 3. **其他规定**： - 最终版本需添加作者信息和修改脚注（如会议名称）。 - 必须包含“影响声明”，讨论工作的伦理及社会影响。 - 鼓励共享软件和数据（提交时匿名处理）。 - 可添加附录，但需符合总体文件大小限制。</details> |
| 2025-02-20 | ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model | http://arxiv.org/abs/2502.14420v2 | <details><summary>展开</summary>待生成</details> |
| 2025-02-19 | VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation | http://arxiv.org/abs/2502.13508v2 | <details><summary>展开</summary>本文提出了一种新型的端到端视觉-语言-动作模型VLAS，首次将语音指令直接集成到机器人操作策略模型中，无需依赖外部语音识别系统。核心创新点包括： 1. **语音指令融合**：通过Whisper编码器处理原始语音，利用多层感知机将语音特征映射到LLaVA的语言空间，实现语音-文本-视觉的多模态对齐。 2. **三阶段训练范式**： - **语音对齐阶段**：在LibriSpeech数据集上微调MLP层 - **语音问答微调**：结合新构建的SQA语音问答数据集和VQA视觉问答数据 - **机器人操作微调**：在CSI机器人操作数据集上训练动作生成 3. **语音检索增强生成（Voice RAG）**：通过声纹识别检索个性化知识库，解决定制化任务中的个体特定需求（如物品归属识别、用户偏好理解）。 4. **实验验证**： - 在CALVIN基准测试中，VLAS的语音指令性能媲美传统文本指令VLA模型 - 在定制化任务基准上，语音指令成功率提升至86.5%（传统VLA仅19.2%） - 真实UR5机械臂实验验证了实用性和个性化交互能力 VLAS突破了现有VLA模型仅支持文本指令的限制，通过保留语音中的非语义信息（如声纹），为机器人提供了更自然的人机交互方式和个性化任务处理能力。代码、模型及数据集已开源。</details> |
| 2025-02-14 | Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation | http://arxiv.org/abs/2502.10040v1 | <details><summary>展开</summary>待生成</details> |
| 2025-02-13 | GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation | http://arxiv.org/abs/2502.09268v2 | <details><summary>展开</summary>论文提出GEVRM模型，通过结合内部模型控制（IMC）原理增强机器人视觉操作的鲁棒性。核心创新点如下： 1. **目标生成**：采用文本引导的视频扩散模型作为机器人行为规划器，通过时空压缩和随机掩码策略生成高表现力的未来视觉目标帧（参考输入）。该模型利用高效视频编码（2D/3D VAE）和DiT架构，结合Rectified Flow训练范式提升生成效率。 2. **状态对齐**：通过原型对比学习优化内部嵌入（internal embeddings），对齐当前状态与生成的目标状态。该方法模拟系统响应，隐含推断外部扰动（如光照变化、传输噪声），使模型能区分并抵抗环境干扰。 3. **动作预测**：设计基于扩散的目标引导策略，将视觉目标与当前状态联合编码，通过对比学习优化后生成7维机器人动作。该策略在扰动下仍能稳定追踪高表现力目标。 实验表明，GEVRM在CALVIN基准测试（标准/扰动场景）达到SOTA性能，并在真实机器人任务中显著提升目标生成表现力和任务成功率。消融研究验证了各模块的有效性。</details> |
| 2025-02-09 | DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control | http://arxiv.org/abs/2502.05855v3 | <details><summary>展开</summary>DexVLA是一种新型视觉-语言-动作（VLA）模型框架，旨在提升机器人控制的效率和泛化能力，尤其针对复杂长视界任务。其核心要点如下： 1. **问题背景**： - 现有VLA模型存在动作表示瓶颈和数据稀缺问题（依赖大规模人类演示数据），且视觉语言模型（VLM）与动作学习脱节。 2. **核心创新**： - **扩散专家**：引入10亿参数的扩散模型作为动作生成模块，采用多头设计支持跨体现（如单臂、双手、灵巧手）学习。 - **体现课程学习**：三阶段训练策略： - **阶段1**：仅预训练扩散专家于跨体现数据（100小时），学习低级运动技能。 - **阶段2**：对齐VLM与特定机器人体现，将抽象语义映射到物理控制。 - **阶段3**：任务特定微调，结合子步骤推理（如将“折叠衬衫”分解为“抚平褶皱”“对齐袖子”），使模型直接处理复杂任务。 3. **关键优势**： - **高效泛化**：无需任务特定适应即能完成衬衫折叠、物品分拣等任务；仅需≤100个演示即可适应新体现（如灵巧手倒饮料）。 - **长视界任务**：通过直接语言提示（如“折叠洗衣”）完成复杂任务，优于OpenVLA、π₀等模型（如洗衣折叠任务得分0.4 vs. π₀的0.2）。 - **计算高效**：预训练仅需100小时数据，推理速度达60Hz（单A6000 GPU）。 4. **实验验证**： - 在多个体现（Franka机械臂、双手UR5e、灵巧手等）和任务（洗衣折叠、餐桌清理）中，DexVLA均显著优于基线模型，尤其在数据效率和任务分解能力上表现突出。</details> |
| 2025-02-08 | HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation | http://arxiv.org/abs/2502.05485v4 | <details><summary>展开</summary>待生成</details> |
| 2025-02-08 | ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy | http://arxiv.org/abs/2502.05450v2 | <details><summary>展开</summary>待生成</details> |
| 2025-02-07 | Survey on Vision-Language-Action Models | http://arxiv.org/abs/2502.06851v3 | <details><summary>展开</summary>待生成</details> |
| 2025-02-06 | Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture | http://arxiv.org/abs/2502.04558v1 | <details><summary>展开</summary>待生成</details> |
| 2025-02-04 | VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation | http://arxiv.org/abs/2502.02175v1 | <details><summary>展开</summary>待生成</details> |
| 2025-02-03 | Scalable, Training-Free Visual Language Robotics: A Modular Multi-Model Framework for Consumer-Grade GPUs | http://arxiv.org/abs/2502.01071v1 | <details><summary>展开</summary>论文提出SVLR（Scalable Visual Language Robotics）框架，一个可扩展、无需训练的视觉语言机器人系统，专为消费级GPU设计。其核心要点如下： 1. **框架设计**： - SVLR是开源、模块化的多模型框架，整合轻量级AI模型（包括视觉语言模型Mini-InternVL、分割模型CLIPSeg、大语言模型Phi-3和句子相似模型all-MiniLM），处理视觉和语言输入以生成机器人动作序列。 - 无需重新训练，通过预编程任务和参数化对象位置实现机器人控制。 2. **关键优势**： - **可扩展性**：用户可轻松添加新任务或机器人，仅需文本描述和任务定义，无需重新训练。 - **资源高效**：在消费级GPU（如NVIDIA RTX 2070）上运行，降低部署门槛。 3. **核心组件**： - **感知模块**：识别环境对象并输出位置坐标。 - **语言模型**：解析用户指令，生成任务序列（如pick-and-place）。 - **动作管理器**：使用句子相似模型匹配任务参数，确保与预定义任务一致。 4. **实验结果**： - 在UR10机器人上验证，成功执行pick-and-place等任务，展示对复杂指令（如多对象操作）的适应性。 - 模块化设计支持未来模型更新和任务扩展。 5. **局限性与展望**： - 需进一步评估复杂任务性能；未来工作包括集成深度相机、扩展至移动机器人及模型优化。</details> |
| 2025-01-31 | UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent | http://arxiv.org/abs/2501.18867v3 | <details><summary>展开</summary>待生成</details> |
| 2025-01-28 | Improving Vision-Language-Action Model with Online Reinforcement Learning | http://arxiv.org/abs/2501.16664v1 | <details><summary>展开</summary>待生成</details> |
| 2025-01-25 | An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation | http://arxiv.org/abs/2501.15068v3 | <details><summary>展开</summary>本文提出了一种数据高效的三轮回原子技能库构建方法，用于解决具身操作中端到端模型数据需求高、泛化能力差的问题。核心要点如下： 1. **问题背景**： - 现有端到端具身操作模型在真实复杂场景中泛化能力不足，且依赖大量任务特定数据，导致“数据爆炸”。 - 原子技能分解可减少数据需求，但传统方法受限于预定义静态技能集，无法动态扩展。 2. **方法框架（三轮回）**： - **VLP轮**：利用视觉-语言-规划（VLP）代理分解任务为子任务，结合空间感知（如物体检测和空间关系推理）生成执行计划。 - **VLA轮**：通过视觉-语言-动作（VLA）模型微调实现原子技能执行，利用少量数据适应新对象和场景。 - **原子技能轮**：抽象子任务为通用技能定义，动态扩展技能库；遇到新任务时仅需补充缺失技能数据，大幅降低数据成本。 3. **关键优势**： - **数据高效**：相比端到端方法，减少数据需求（实验显示任务成功率提升40%），支持跨任务复用。 - **动态扩展**：技能库随任务更新自然增长，提升对新任务的适应性。 - **高性能**：真实环境实验（如拾取放置、倒水任务）验证了方法在降低数据成本的同时维持高成功率。 4. **实验验证**： - 在ALOHA机器人上测试，相比端到端基线（如Octo、RDT-1B），数据需求减少且任务成功率更高（例如，在物体位置分布外场景成功率提升20–40%）。 - 方法兼容多种VLA模型，证实了泛化性和实用性。</details> |
| 2025-01-16 | FAST: Efficient Action Tokenization for Vision-Language-Action Models | http://arxiv.org/abs/2501.09747v1 | <details><summary>展开</summary>待生成</details> |
| 2025-01-12 | Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing | http://arxiv.org/abs/2501.06919v1 | <details><summary>展开</summary>论文提出Shake-VLA系统，一种基于视觉-语言-动作（VLA）模型的机器人系统，用于双手机器人操作和液体混合（如鸡尾酒制备）。核心要点如下： 1. **系统组成**： - **视觉模块**：使用YOLOV8检测配料瓶和EasyOCR读取标签（91%成功率）。 - **语音处理**：OpenAI Whisper-1实现语音转文本（嘈杂环境93%成功率），Google TTS提供语音反馈。 - **检索增强生成（RAG）**：结合FAISS检索和GPT-4o生成食谱及机器人指令。 - **异常检测**：比对食谱与可用配料，提供替代方案（95%成功率）。 - **动作执行**：语言模块（GPT-4o）生成双手机器人动作指令（如倒液、抓取），并利用力扭矩传感器精确控制液体量。 2. **实验结果**： - 整体系统在鸡尾酒制备中实现100%成功率，涵盖食谱生成到执行。 - 视觉模块在杂乱环境中表现稳健，语音模块抗噪能力强。 3. **创新与价值**： - 集成VLA模型实现人机交互与精确操作，适用于服务场景。 - 未来方向：增强多语言文本识别、噪声鲁棒性，扩展至实验室自动化等任务。</details> |
| 2025-01-09 | UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation | http://arxiv.org/abs/2501.05014v2 | <details><summary>展开</summary>论文提出 **UAV-VLA 系统**，通过整合卫星图像、视觉语言模型（VLM）和 GPT 技术，实现基于自然语言指令的无人机大规模任务自动生成。核心要点如下： ### 一、核心贡献 1. **系统设计** - 开发 **Vision-Language-Action (VLA) 系统**，用户输入文本指令（如“在100米高度环绕所有建筑飞行并返航”），系统自动生成飞行路径和动作序列。 - 流程分三步： - **目标解析**（GPT 提取指令中的任务目标）； - **视觉定位**（Molmo VLM 在卫星图中识别目标物体位置）； - **动作生成**（GPT 结合坐标和飞行协议生成可执行路径）。 - **开源代码**：https://github.com/sautenich/uav-vla 2. **新基准数据集** - 构建 **UAV-VLPA-nano-30** 评估基准，包含 **30 幅美国多地高清卫星图**（分辨率 1.5 米/像素），覆盖城市、郊区、自然区域等场景，用于测试全局路径规划能力。 3. **性能验证** - 系统性能接近人类专家水平，部分任务路径更优。 --- ### 二、实验结果 1. **效率优势** - 生成全部 **30 个任务的飞行计划仅需 5 分 24 秒**，比人类操作员（35 分钟）快 **6.5 倍**。 2. **路径质量** - 系统路径总长 **77.74 km**，较人工规划（63.89 km）长 **21.6%**，但 **23% 的任务路径更短**（见图 5）。 - **定位误差**（KNN 方法）：平均 **34.22 米**（见表 I），优于时序匹配（409.54 米）和动态时间规整（307.27 米）。 --- ### 三、未来方向 1. 构建专用卫星地图路径规划数据集，提升模型精度。 2. 开发端到端模型，实现从高层目标到全自主任务规划的闭环。 ### 总结 UAV-VLA 通过 **“文本指令→视觉定位→路径生成”** 的流水线，显著提升无人机任务规划效率，为自然语言控制无人机及多机协作奠定基础。</details> |
| 2025-01-08 | Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding | http://arxiv.org/abs/2501.04693v3 | <details><summary>展开</summary>本文提出FuSe方法，通过语言引导将异构传感器（触觉、听觉等）融入预训练的通用机器人策略中，解决多模态数据稀缺问题。核心要点如下： 1. **问题背景**：现有通用机器人策略（如Octo/VLA模型）主要依赖视觉和本体感觉，缺乏触觉、听觉等多模态感知能力，在视觉遮挡场景（如袋内抓取）表现受限。 2. **方法创新**： - **语言跨模态对齐**：提出多模态对比损失和语言生成损失，利用自然语言关联异构传感器语义。 - **传感器编码**：预训练触觉编码器（TVL）和音频频谱处理（ResNet），适配小规模数据集。 - **损失函数**：结合行为克隆损失与辅助损失（$\mathcal{L} = \mathcal{L}_{BC} + \beta\mathcal{L}_{gen} + \lambda\mathcal{L}_{contrast}$），避免模型忽略新传感器。 3. **实验验证**： - **数据集**：收集27K条真实机器人轨迹（含视觉/触觉/音频/动作/语言），覆盖桌面抓取、袋内抓取和按钮按压任务。 - **性能提升**：在视觉遮挡的袋内抓取任务中，FuSe比纯视觉基线成功率提高20%以上（图5）。 - **新能力**： - **多模态提示**：实现"抓取红色且触感柔软的发声物体"等指令（表I）。 - **组合推理**：完成"按压与蓝色按钮同声的按钮"等跨模态任务（图6）。 - **语言生成**：交互时生成物体描述（如触觉属性）。 4. **普适性**：方法适配不同架构模型（Octo和3B参数VLA模型PaliGemma），VLA版本在袋内抓取任务表现更优（图8）。 5. **开源贡献**：公开数据集、代码及模型，推动多模态机器人研究。 **限制**：计算资源需求高，历史上下文长度有限（0.4秒）。</details> |
| 2025-01-07 | OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints | http://arxiv.org/abs/2501.03841v1 | <details><summary>展开</summary>待生成</details> |
| 2025-01-07 | Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis | http://arxiv.org/abs/2501.03565v1 | <details><summary>展开</summary>待生成</details> |
| 2025-01-06 | Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches | http://arxiv.org/abs/2501.03151v1 | <details><summary>展开</summary>这篇论文探讨了大型语言模型（LLMs）实现人工通用智能（AGI）的基础原则与方法。核心要点如下： 1. **AGI的核心挑战** - 当前LLMs虽在多领域展现强大能力（如推理、对话），但其认知能力仍存在**表面性**和**脆弱性**，缺乏真正的通用智能。 - 实现AGI需解决四个基础问题：**具身性（Embodiment）**、**符号接地（Symbol Grounding）**、**因果性（Causality）** 和**记忆机制（Memory）**。 2. **关键原则的作用** - **具身性**：通过物理或虚拟载体使LLMs与环境交互（如机器人、模拟环境），赋予其**目标感知**、**情境感知**和**自我感知**能力，支持自主决策（如PaLM-E、EmbodiedGPT模型）。 - **符号接地**：将抽象符号关联到现实实体（如通过知识图谱、本体提示），解决LLMs的**语义鸿沟**和幻觉问题（如KG-LLM集成方法）。 - **因果性**：结合神经符号方法或物理模型（如Theory of Mind），使LLMs理解事件间的因果关系，提升推理鲁棒性。 - **记忆机制**：通过注意力机制、外部存储（如RAG）或分层记忆（感官/工作/长期记忆）实现知识累积与复用，支持持续学习。 3. **实现路径** - **多模态融合**：视觉-语言-动作模型（VLAs）整合多感官数据，支持复杂任务（如导航、物体操作）。 - **模拟环境训练**：利用3D仿真或扩展现实（XR）低成本训练具身代理，再迁移至真实场景。 - **认知架构整合**：将四大原则有机融合，构建**目标驱动**、**环境交互**、**因果推断**和**知识积累**的通用AI框架。 4. **局限与展望** - 现有方法仍面临**场景泛化性不足**、**高成本数据需求**及**复杂行为建模困难**等挑战。 - 未来需探索神经符号结合、世界模型与动态记忆的深度集成，以实现人类级通用智能。 总结：论文系统论证了具身性、符号接地、因果性与记忆是LLMs迈向AGI的基石，并综述了各领域前沿技术路径，为构建真正通用的认知模型提供理论和方法基础。</details> |
| 2024-12-29 | CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance | http://arxiv.org/abs/2412.20451v2 | <details><summary>展开</summary>待生成</details> |
| 2024-12-24 | VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks | http://arxiv.org/abs/2412.18194v1 | <details><summary>展开</summary>待生成</details> |
| 2024-12-20 | QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning | http://arxiv.org/abs/2412.15576v5 | <details><summary>展开</summary>待生成</details> |
| 2024-12-18 | Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models | http://arxiv.org/abs/2412.14058v3 | <details><summary>展开</summary>待生成</details> |
| 2024-12-18 | RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation | http://arxiv.org/abs/2412.13877v3 | <details><summary>展开</summary>待生成</details> |
| 2024-12-16 | Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning | http://arxiv.org/abs/2412.11974v2 | <details><summary>展开</summary>待生成</details> |
| 2024-12-13 | TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies | http://arxiv.org/abs/2412.10345v3 | <details><summary>展开</summary>本文提出了一种名为“视觉轨迹提示”（visual trace prompting）的新方法，用于增强视觉-语言-动作（VLA）模型在机器人操控任务中的时空感知能力。核心要点如下： 1. **方法创新** - 引入**视觉轨迹提示**技术：通过在输入图像上叠加机器人历史运动轨迹（由稠密点跟踪算法生成），显式编码时空信息，解决VLA模型对历史动作感知不足的问题。 - 轨迹生成：使用Co-tracker提取历史图像序列中的关键点轨迹，筛选显著移动的“活跃点”，叠加到当前观测图像上作为视觉提示。 - 模型架构：输入包含原始图像和轨迹提示图像，通过分隔符拼接，并调整文本提示以引导模型关注轨迹信息（图1）。 2. **模型与数据** - 提出**TraceVLA模型**：基于OpenVLA（7B参数）微调，使用自建的**150K机器人操控轨迹数据集**（包含BridgeData-v2、Google RT1等数据）。 - 推出轻量版**TraceVLA-Phi3**：以4B参数Phi-3-Vision为骨干，在Open X-Embodiment数据集上预训练后微调，在保持性能的同时提升推理效率。 3. **性能验证** - **仿真实验**：在SimplerEnv的137种配置中，TraceVLA平均性能超越OpenVLA 10%，尤其在视角变化、干扰物存在等复杂场景下提升显著（图4）。 - **实物机器人实验**：在WidowX机器人上执行8项任务（含4项未见任务），TraceVLA成功率较OpenVLA提高3.5倍，展现出强泛化能力（图6）。 - 轻量模型TraceVLA-Phi3性能接近7B基线，推理速度更快。 4. **关键优势** - **时空感知提升**：视觉轨迹提示使模型能追踪历史动作，减少环境干扰（如光照变化、背景杂乱）的影响。 - **高效性**：相比直接输入多帧历史图像（导致性能下降6%），轨迹提示避免信息冗余；较文本描述轨迹方案提升6.4%性能（图8）。 - **泛化性**：在物体、场景、任务指令变化的未见任务中表现鲁棒（如抓放香蕉任务成功率80% vs 基线10%）。 5. **局限性** - 训练内存成本增加约10GB（H100），推理中轨迹跟踪引入额外0.03秒/步耗时（图10）。 - 轨迹长度需平衡（N=6最优），过长会遮挡关键信息（图9）。 **结论**：视觉轨迹提示通过简单高效的时空信息编码，显著提升VLA模型在机器人操控中的决策能力，为通用机器人策略提供新思路。模型与代码已开源。</details> |
| 2024-12-09 | Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks | http://arxiv.org/abs/2412.06224v2 | <details><summary>展开</summary>待生成</details> |
| 2024-12-05 | NaVILA: Legged Robot Vision-Language-Action Model for Navigation | http://arxiv.org/abs/2412.04453v2 | <details><summary>展开</summary>论文提出NaVILA框架，用于腿式机器人的视觉-语言-动作导航，核心创新点包括： 1. **分层架构设计** - **高层VLA模型**：将视觉语言模型（VILA）微调为视觉语言动作模型，输出自然语言描述的中层动作指令（如"前进75cm"），而非直接生成底层关节控制信号。 - **底层运动策略**：基于强化学习的视觉运动策略，将中层指令转化为关节动作，利用LiDAR点云构建高度图实现避障，通过Isaac Lab仿真训练并直接部署到真实机器人。 2. **创新训练方法** - **多源数据融合**：整合仿真导航数据（R2R-CE/RxR-CE）、人类游览视频（YouTube）、场景理解数据（ScanQA）和通用VQA数据，增强泛化能力。 - **人类视频利用**：首次通过姿态估计和语言重述，从人类游览视频提取连续空间动作标签，解决真实环境数据稀缺问题。 3. **性能优势** - 在VLN-CE基准上超越SOTA方法17%成功率，新提出的VLN-CE-Isaac仿真基准上视觉策略比非视觉策略高14%成功率。 - 真实世界实验在25条指令中达到88%成功率（复杂指令75%），成功部署于Unitree Go2/H1、Booster T1等多机器人平台。 该框架通过解耦高层推理与底层控制，解决了语言指令到关节动作的转换难题，同时兼顾了跨场景泛化性和复杂地形适应性。</details> |
| 2024-12-02 | Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control | http://arxiv.org/abs/2412.01034v1 | <details><summary>展开</summary>该论文提出了一种量化感知模仿学习框架（QAIL），用于实现资源高效的机器人控制策略部署。核心创新点如下： 1. **问题背景** - 基于深度神经网络的策略模型（如视觉-语言-动作模型）在机器人操控和自动驾驶中面临计算开销大、内存需求高的问题 - 传统量化方法会导致动作决策误差累积，尤其在复杂长序列任务中（如物体精细操作、密集交通场景） 2. **解决方案** - **量化感知模仿学习（QAIL）**：在微调阶段引入量化操作，增强模型对低精度误差的鲁棒性 - **量化鲁棒行为克隆（QBC）**：创新性地通过对齐全精度模型与量化模型的行动分布，减少序列决策中的误差累积 - 加权QBC（wQBC）：基于视觉注意力机制动态调整关键状态的优化权重 3. **关键技术指标** \| 应用场景 \| 量化方案 \| 加速比 \| 能效提升 \| 硬件平台 \| \|----------------\|----------\|--------\|----------\|---------------\| \| 机器人操控 \| 4bit权重 \| 2.5× \| 2.5× \| 边缘GPU \| \| 自动驾驶 \| 4bit权重+激活 \| 3.7× \| 3.1× \| 低端GPU \| \| 车载CPU部署 \| 8bit权重+激活 \| 1.7× \| 1.3× \| 车载CPU \| 4. **实验验证** - 在LIBERO机器人操作基准上保持与FP32模型相当的任务成功率 - 在NoCrash自动驾驶基准的密集场景中碰撞率降低47% - 可视化分析显示量化模型保留了与全精度模型相似的注意力机制（图5） 5. **创新价值** 首次实现模仿学习策略模型在资源受限设备的高效部署，为实时机器人控制提供实用解决方案，代码已开源。 该方法显著降低了策略模型的计算需求，同时保持决策可靠性，为边缘设备部署大规模策略模型开辟了新途径。</details> |
| 2024-11-29 | SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters | http://arxiv.org/abs/2412.00174v1 | <details><summary>展开</summary>论文提出了一种名为**SOLAMI**的端到端社交视觉-语言-动作（VLA）建模框架，用于实现用户与3D自主角色的沉浸式交互。核心贡献包括： 1. **统一社交VLA架构** - 基于解码器LLM主干，处理用户语音和动作输入，生成角色语音与动作响应。 - 通过运动分词器（VQ-VAE）和语音分词器（SpeechTokenizer）将多模态输入离散化，实现低延迟端到端生成。 2. **合成多模态数据集SynMSI** - 利用现有文本-动作数据集（如HumanML3D、Inter-X），通过自动化流程生成6.3K轮多模态对话数据。 - 结合GPT-4o生成角色相关对话脚本，并通过文本嵌入检索匹配动作，确保语音与动作对齐。 3. **沉浸式VR交互接口** - 开发VR系统（Oculus Quest 3 + 后端服务），实时捕捉用户动作与语音，驱动角色生成多模态响应。 - 支持面部动画（UniTalker）与角色个性化（3D模型库），提升沉浸感。 4. **三阶段训练策略** - **分词器预训练**：运动VQ-VAE与语音离散化。 - **多任务预训练**：46K动作-文本对与410K语音-文本对对齐多模态。 - **指令微调**：5.7K多轮对话数据优化社交响应生成。 5. **实验结果** - 定量评估显示SOLAMI在动作精度（PA-MPJPE↓ 7.2）、语音相似度（VC↑ 0.81）和延迟（1.4s↓）上优于模块化基线（如LLM-Agent）。 - 用户研究证实其生成响应更自然且符合角色设定。 SOLAMI首次实现社交场景中多模态输入到输出的端到端建模，为3D角色交互提供新范式。</details> |
| 2024-11-29 | RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World | http://arxiv.org/abs/2412.00171v3 | <details><summary>展开</summary>论文提出了一种闭环机器人学习系统框架，要点如下： - 从真实机器人收集演示数据。 - 数据经过处理（如清洗和格式化）后，进行人工或自动标注。 - 标注数据用于训练机器学习模型。 - 模型训练后进行性能评估。 - 评估结果反馈至真实机器人，实现系统优化闭环。</details> |
| 2024-11-29 | CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation | http://arxiv.org/abs/2411.19650v1 | <details><summary>展开</summary>CogACT是一种新型视觉-语言-动作（VLA）基础模型，旨在提升机器人操作的认知与动作协同能力。其核心创新点包括： 1. **组件化架构**：分离视觉模块（处理图像）、语言模块（整合指令与视觉信息）和动作模块（扩散变换器DiT生成动作序列），取代传统VLM直接量化动作的方法。 2. **扩散动作模块**：采用DiT建模连续、多模态的动作序列，通过条件扩散过程提升动作精度和时序相关性，并展示良好的参数缩放性（数百兆参数即可显著提升性能）。 3. **自适应动作集成（AAE）**：在推理时动态融合历史预测与当前动作，避免跨模态冲突，优化轨迹平滑性。 4. **性能优势**：在5种机器人平台（仿真和真实环境）上评估，CogACT显著超越OpenVLA（相似7B模型规模）和RT-2-X（55B模型），平均成功率提升35%（仿真）和55%（真实任务），并展现强泛化能力（适应新机器人、未见物体及背景）。</details> |
| 2024-11-28 | GRAPE: Generalizing Robot Policy via Preference Alignment | http://arxiv.org/abs/2411.19309v2 | <details><summary>展开</summary>待生成</details> |
| 2024-11-18 | Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics | http://arxiv.org/abs/2411.13587v4 | <details><summary>展开</summary>本文探讨了机器人学中视觉-语言-动作模型（VLAMs）的对抗性漏洞，聚焦于这些模型在面临对抗攻击时的脆弱性及其对机器人系统安全性的影响。</details> |
| 2024-11-15 | Visual-Linguistic Agent: Towards Collaborative Contextual Object Reasoning | http://arxiv.org/abs/2411.10252v1 | <details><summary>展开</summary>待生成</details> |
| 2024-11-05 | VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation | http://arxiv.org/abs/2411.03540v1 | <details><summary>展开</summary>待生成</details> |
| 2024-11-04 | DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution | http://arxiv.org/abs/2411.02359v1 | <details><summary>展开</summary>待生成</details> |
| 2024-11-04 | Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks | http://arxiv.org/abs/2411.05821v2 | <details><summary>展开</summary>根据论文内容，中文要点总结如下： 1. **研究目标** 提出首个系统性评估视觉-语言-动作模型（VLAs）在机器人学习任务中的框架，通过20个Open-X数据集评测GPT-4o、OpenVLA和JAT三种先进模型在操作任务中的表现。 2. **核心发现** - **模型差异显著**：GPT-4o通过精细提示工程实现最稳定性能（平均AMSE 0.1-0.5），OpenVLA在训练分布内任务表现优异但跨任务波动大，JAT整体表现较弱（AMSE普遍＞1）。 - **共性缺陷**：所有模型在需多步规划的复杂操作任务（如厨房物品整理）中表现显著下降。 - **敏感因素**：模型性能受动作空间特性（如连续/离散控制）和环境变化（如物体遮挡）影响显著。 3. **方法论创新** - 构建标准化评估流程，包括动作空间转换协议（如夹爪命令三元离散化）和跨平台适配方案。 - 提出归一化AMSE（NAMSE）指标，消除动作空间尺度差异，揭示GPT-4o具有最优任务泛化性（NAMSE＜0.2）。 4. **关键启示** - 提示工程对VLA性能影响大于架构设计（GPT-4o的提示含动作空间统计描述）。 - 专用模型（OpenVLA）与通用模型（GPT-4o）存在性能权衡，需平衡任务专注力与泛化能力。 5. **资源开放** 公开评估框架和53个数据集（32TB）的预处理代码，促进VLA模型的标准化评测。 6. **未来方向** 扩展评估至非机器人领域（如软件环境交互），并探索组合推理、长序列任务等能力评测。</details> |
| 2024-11-01 | CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision | http://arxiv.org/abs/2411.00508v4 | <details><summary>展开</summary>待生成</details> |
| 2024-10-21 | The Duality of Generative AI and Reinforcement Learning in Robotics: A Review | http://arxiv.org/abs/2410.16411v2 | <details><summary>展开</summary>这篇论文探讨了生成式AI（如大型语言模型LLM、视觉语言模型VLM、扩散模型DM）与强化学习（RL）在机器人领域的协同作用，提出了一种新的分类框架。核心要点如下： ### 核心二元性 1. **生成式AI作为RL工具** - 利用预训练的生成模型（LLM/VLM/DM）增强RL训练环节： - **奖励设计**：LLM/VLM根据任务描述自动生成奖励函数（如EUREKA框架） - **状态表示**：VLM从视觉输入中提取语义特征（如ZeST框架） - **规划探索**：LLM生成高层任务分解（如BOSS框架的技能链机制） - 优势：提供先验知识，减少训练样本需求，支持零样本泛化 2. **RL优化生成式策略** - RL用于训练/微调生成式策略（如Transformer/Diffusion策略）： - **预训练**：通过RL训练通用策略骨架（如VLA模型） - **微调**：适应具体任务（如安全约束下的策略调整） - **策略蒸馏**：将大模型知识压缩至轻量策略（专家→通用模型双向传输） ### 关键贡献 - **新分类法**（图2）：基于模型架构（LLM/VLM/DM）、模态（文本/图像/轨迹）、任务角色（奖励/状态/规划）三维度组织169篇文献 - **挑战分析**：指出生成模型与RL融合的瓶颈： - **可扩展性**：大模型计算成本高（如GPT-4实时推理负担） - **安全性**：生成策略的不可解释性（黑箱决策风险） - **数据对齐**：仿真到现实的域适应问题（表2/3显示仅37%研究含实物实验） - **未来方向**：提出三类新研究方向： 1. 基于人类反馈的RL（RLHF） 2. 演员-评论员基础模型架构 3. 结合最优控制的约束感知生成模型 ### 实证发现 - **扩散模型崛起**（图1b）：2023年后DM在RL中应用激增（如规划/探索任务） - **多模态必要性**（图4a）：VLM相比LLM更适应具身任务（避免文本描述的信息损失） - **开源缺口**：仅41%论文公开代码（表2/3），制约复现与改进 论文通过[GitHub仓库](https://github.com/clmoro/Robotics-RL-FMs-Integration)持续更新相关研究，强调生成式AI与RL的互补性将推动自适应机器人系统的演进。</details> |
| 2024-10-21 | VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making | http://arxiv.org/abs/2410.15885v3 | <details><summary>展开</summary>该论文提出了一种名为VLASCD（MIMO-VLA）的新型视觉语言动作模型，旨在解决多输入多输出（MIMO）场景下传统多输入单输出（MISO）模型的根本性缺陷。核心要点如下： 1. **问题发现** - 现有MISO模型（如LLMs和VLAs）在并行多任务场景（如同时对话与决策）中存在任务干扰问题，导致性能下降。 - 任务竞争共享输出通道引发优化不平衡，例如：对话生成与决策动作相互排斥。 2. **解决方案：MIMO-VLA框架** - **并行多任务输出**：通过独立输出通道支持同步生成自然语言对话和连续动作决策。 - **关键技术创新**： - **连续动作映射**：采用MLP直接输出连续动作值（如加速度/转向角），避免离散化误差。 - **图像重建损失**：通过辅助视觉重建任务增强环境特征提取能力。 - **标签平滑策略**：优化文本生成质量，防止过拟合。 - **梯度空间隔离机制**：语言、动作、图像损失分别作用于不同位置编码，消除任务干扰。 3. **实验验证** - **测试平台**：CARLA自动驾驶仿真环境。 - **性能优势**： - 决策指标：驾驶得分（DS）提升至105.25（H=4），远超OpenVLA（-7.84）和Decision Transformer（7.68）。 - 对话能力：GPT-4o评估得分显著高于基线模型，保持流畅人机对话。 - 泛化性：在未训练地图（town04）表现稳定（DS=94.26）。 4. **消融实验结论** - 移除语言损失导致对话质量骤降；图像损失提升决策鲁棒性。 - 连续动作损失比离散化方法（如OpenVLA）效果提升46%。 5. **意义** - 首次实现端到端同步对话与决策的统一框架，为多模态多任务学习开辟新方向。 代码已开源：https://github.com/Mark-zjtang/MIMO-VLA</details> |
| 2024-10-21 | A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM | http://arxiv.org/abs/2410.15549v1 | <details><summary>展开</summary>### 论文要点总结： 1. **问题背景**： Vision-Language-Action (VLA) 模型通过整合视觉和语言输入，使机器人执行复杂任务，但现有模型计算开销大，导致推理速度慢（如1-6Hz），无法满足实时操作需求，且缺乏泛化能力。 2. **核心方法**： 提出 **Dual Process VLA (DP-VLA)** 框架，受双过程理论启发： - **大型系统2模型（L-Sys2）**：基于Vision-Language Models (VLMs)，处理复杂推理和决策（如任务规划），在低频运行（仅任务变更时激活），减少计算负担。 - **小型系统1模型（S-Sys2）**：处理实时运动控制（如传感器输入和状态反馈），高频运行确保快速响应。 两者协同：L-Sys2输出潜在特征（latent features）指导S-Sys1生成精细动作。 3. **关键贡献**： - **高效性**：推理速度提升至0.03秒（RoboCasa实验），任务成功率提高20.4%（平均57.3% vs 基线47.6%）。 - **可扩展性**：L-Sys2可无缝升级新VLMs，无需修改整体系统。 - **实验验证**：在RoboCasa模拟环境中，优于OpenVLA、BC-Transformer等基线。 4. **实验结果**： - **数据集**：RoboCasa（厨房场景，24项原子任务）。 - **配置**：L-Sys2用OpenVLA，S-Sys1用BC-Transformer。 - **性能**：高成功率（如开门任务达84%），低延迟（比OpenVLA快8倍）；消融实验显示解码阶段特征效果最佳，且预训练VLMs优于微调。 5. **结论**： DP-VLA通过分离推理与控制，平衡计算效率与任务性能，为实时机器人操作提供可扩展解决方案。未来工作将优化动态调度和多级推理。</details> |
| 2024-10-17 | Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand | http://arxiv.org/abs/2410.14022v1 | <details><summary>展开</summary>本文提出一种混合控制方法，结合视觉-语言-动作（VLA）模型和扩散模型的优势，实现拟人手（ADAPT Hand 2）的灵巧操作。VLA模型基于语言命令进行高层规划（如手臂运动），提供泛化性；扩散模型处理低层交互（如抓取），提供精度和鲁棒性。通过事件信号触发两者切换，用于拾取-放置任务。 实验表明，该方法成功率超过80%（VLA单独使用仅40%），关键优势包括：VLA准确定位对象、扩散模型支持多模态抓取和错误恢复、拟人手的柔顺性增强交互鲁棒性。这是首次在拟人手上应用VLA模型，为灵巧操作提供新框架。</details> |
| 2024-10-15 | Latent Action Pretraining from Videos | http://arxiv.org/abs/2410.11758v2 | <details><summary>展开</summary>待生成</details> |
| 2024-10-10 | Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation | http://arxiv.org/abs/2410.08001v3 | <details><summary>展开</summary>待生成</details> |
| 2024-10-07 | LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation | http://arxiv.org/abs/2410.05191v1 | <details><summary>展开</summary>本文提出LADEV，一种专为机器人操作中的视觉-语言-动作（VLA）模型设计的语言驱动测试与评估平台。VLA模型虽能整合视觉和语言输入生成控制动作，但其数据驱动特性与低可解释性导致可靠性和鲁棒性难以保障。LADEV通过以下核心机制解决该问题： 1. **语言驱动的模拟环境生成**：利用大型语言模型（LLM）将自然语言描述自动转换为模拟环境配置（如物体布局、光照和相机位姿），避免人工调整，提升测试效率。 2. **自然语言任务指令释义**：通过LLM生成任务指令的多样化变体（如“拾取苹果”改写为“拿起苹果”），结合语义相似性验证，评估VLA模型对语言输入的敏感性。 3. **批量式评估**：支持单指令批量生成多场景和指令变体，实现大规模高效测试（如4000+场景）。 实验验证中，LADEV在四类机器人操作任务（拾取、移动、放置、放入）上测试了7种VLA模型（如RT-1、OpenVLA），结果表明： - 物体数量增加或使用未见物体（如YCB数据集）会显著降低模型性能。 - 指令变体导致任务成功率平均下降10%以上，凸显语言输入对VLA鲁棒性的影响。 - 环境变化（如光照调整）对部分模型性能影响较大。 LADEV为VLA模型提供了自动化、可扩展的评估基准，推动更智能机器人系统的发展。</details> |
| 2024-10-02 | Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust | http://arxiv.org/abs/2410.01971v1 | <details><summary>展开</summary>待生成</details> |
| 2024-09-29 | RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model | http://arxiv.org/abs/2409.19590v1 | <details><summary>展开</summary>本文提出 **RoboNurse-VLA**，一种基于视觉-语言-动作（VLA）模型的机器人洗手护士系统，用于精准传递手术器械。核心要点如下： 1. **核心创新** - 首次将 **Segment Anything Model 2 (SAM 2)** 与 **Llama 2 语言模型** 集成到 VLA 框架中，实现手术器械的实时抓取与交接。 - 系统通过外科医生的**语音指令**操作，自动识别目标器械并优化抓取/交接姿态。 2. **关键技术** - **视觉模块**：基于 YOLOv8 检测器械和手部，SAM 2 生成分割掩码，结合 MLP 投影器将视觉特征映射到语言空间。 - **动作生成**：Llama 2 根据语音指令和视觉输入生成机器人控制动作（离散化为 256 个动作 token）。 - **训练效率**：冻结视觉模块，使用 **LoRA 微调 Llama 2**，单块 A100 GPU 仅需 20 小时。 3. **实验成果** - **零样本任务**：在复杂场景（多相似器械）中，成功率（65%）显著优于 Octo、RT-2-X 和 OpenVLA（≤10%）。 - **微调后性能**： - 标准器械交接成功率 **100%**（基线模型 ≤45%）。 - 手部位置/姿态变化时保持 **95%** 成功率（基线 ≤40%）。 - **泛化能力**： - 未见过的器械：**90%** 成功率。 - 难抓取物品（如乒乓球）：**95%** 成功率（精准控制抓取力至 4N）。 4. **优势与贡献** - 解决相似器械区分、抓取姿态优化、动态环境适应等挑战。 - 超越现有模型（如 RT-2-X、OpenVLA），且模型规模更小。 - 提供开源数据集（700 张标注手术器械图像）和系统设计细节。 5. **未来方向** - 临床环境验证、扩展数据集、开发避障功能及优化人机协作安全性。 **总结**：RoboNurse-VLA 通过融合 SAM 2 的精准视觉分割与 Llama 2 的语义理解能力，实现了高鲁棒性的手术器械自主交接系统，为手术室自动化提供了新方案。</details> |
| 2024-09-19 | TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2409.12514v5 | <details><summary>展开</summary>本文提出TinyVLA，一种快速、数据高效的视觉-语言-动作（VLA）模型，用于机器人操作。现有VLA模型（如OpenVLA）存在推理速度慢、依赖大规模机器人数据预训练的问题。TinyVLA通过两个关键创新解决这些问题： 1. **轻量级架构**：采用参数少于10亿的紧凑视觉语言模型（VLM）作为主干，结合扩散策略解码器直接输出机器人动作，避免自回归生成动作令牌的开销。 2. **数据高效训练**：使用参数高效微调技术（LoRA），仅需更新5%参数，无需大规模机器人数据集预训练。 实验结果表明： - **性能优势**：在真实机器人任务中，TinyVLA-H比OpenVLA平均成功率提高25.7%（94% vs 68.3%），参数减少5.5倍，推理速度快20倍（14ms vs 292ms）。 - **泛化能力**：在未见过的指令、物体、背景、光照和空间布局下表现出色，尤其在双手机器人任务中，TinyVLA-H成功率达44.5%，而OpenVLA完全失败。 - **多任务学习**：在模拟环境（MetaWorld）和真实单臂/双臂任务中均超越扩散策略等基线模型。 核心贡献在于提供了一种高效VLA框架，在保持性能的同时显著降低计算开销，为机器人部署提供新思路。</details> |
| 2024-09-12 | HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers | http://arxiv.org/abs/2410.05273v3 | <details><summary>展开</summary>待生成</details> |
| 2024-09-05 | OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving | http://arxiv.org/abs/2409.03272v1 | <details><summary>展开</summary>待生成</details> |
| 2024-08-19 | CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving | http://arxiv.org/abs/2408.10845v2 | <details><summary>展开</summary>这篇论文提出了CoVLA（Comprehensive Vision-Language-Action）数据集，用于自动驾驶研究，主要贡献如下： 1. **数据集创新** - 构建了大规模多模态数据集CoVLA-Dataset，包含10,000个真实驾驶场景视频（总计80+小时），涵盖复杂城市道路、高速公路等多种环境。 - 提供**三模态标注**：视觉数据（前视摄像头视频）、语言数据（帧级场景描述）、动作数据（未来3秒轨迹坐标）。 2. **自动化标注方法** - **轨迹标注**：通过GNSS/IMU传感器融合与卡尔曼滤波生成高精度未来轨迹。 - **语言标注**：结合规则模板与视频语言模型（VideoLLaMA 2）生成场景描述，采用幻觉抑制技术提升准确性。 - **物体标注**：利用深度学习模型自动检测交通灯状态与前车信息。 3. **模型开发** - 基于数据集提出CoVLA-Agent模型，整合CLIP视觉编码器与Llama-2语言模型，实现端到端轨迹预测和场景描述生成。 - 模型在多样化场景（如弯道、路口、交通管制）中展现一致性输出能力，轨迹预测误差（ADE=0.955，FDE=2.239）。 4. **技术优势** - 突破现有数据集规模限制（6百万帧数据），通过加权采样保证场景多样性。 - 验证语言-动作关联性：使用真实描述时轨迹预测精度显著提升（ADE↓17.8%）。 5. **开源贡献** 数据集已公开供学术研究使用，为可解释自动驾驶系统提供新基准。 该研究解决了自动驾驶中多模态数据缺失的瓶颈，推动视觉-语言-动作模型在复杂决策任务中的应用。</details> |
| 2024-07-25 | Unified Lexical Representation for Interpretable Visual-Language Alignment | http://arxiv.org/abs/2407.17827v2 | <details><summary>展开</summary>待生成</details> |
| 2024-07-11 | Robotic Control via Embodied Chain-of-Thought Reasoning | http://arxiv.org/abs/2407.08693v3 | <details><summary>展开</summary>待生成</details> |
| 2024-07-10 | Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs | http://arxiv.org/abs/2407.07775v2 | <details><summary>展开</summary>本文提出了一种多模态指令导航新范式MINT，其核心贡献如下： 1. **任务创新**：提出多模态演示游览导航任务（MINT），机器人通过预录的环境游览视频理解多模态用户指令（文本+图像），实现复杂导航（如“手持塑料箱时该放回哪里？”）。 2. **方法设计**： - **分层架构**：结合长上下文视觉语言模型（Gemini 1.5 Pro）与拓扑图。 - 高层策略：VLM解析游览视频和用户指令，定位目标帧 - 底层策略：基于离线构建的拓扑图生成机器人动作 - **拓扑图构建**：通过运动恢复结构（COLMAP）从游览视频生成环境拓扑图，支持鲁棒路径规划 3. **性能验证**： - 在836m²真实办公环境中达到86%端到端成功率（复杂推理任务提升26%） - 多模态指令任务成功率90%，显著优于CLIP检索（20%）和纯文本方法（30%） - 支持智能手机录制游览视频，家庭环境实验成功率100% 4. **关键优势**： - 长上下文VLM解决环境理解瓶颈（1FPS全视频输入） - 拓扑图桥接VLM推理与机器人动作执行，避免零样本动作生成失败（对比实验成功率0%） 该方法显著提升了人机交互的自然性，代码与视频见：youtu.be/-Tof__Q8_5s。</details> |
| 2024-06-28 | LLaRA: Supercharging Robot Learning Data for Vision-Language Policy | http://arxiv.org/abs/2406.20095v3 | <details><summary>展开</summary>本文提出LLaRA框架，通过创新数据生成方法提升视觉语言模型（VLM）在机器人策略学习中的性能。核心要点如下： 1. **指令调优数据生成** - 将行为克隆（BC）数据集转化为对话式指令数据（inBC），将机器人动作与图像像素坐标对齐，用自然语言描述动作（如抓取位置坐标和旋转角度）。 - 提出𝒟-inBC变体：通过目标检测将参考图像转化为文本描述，增强多模态任务理解。 2. **自监督辅助数据集** 从相同轨迹生成6类自监督辅助数据（无需额外标注）： - 目标定位与检测：增强空间感知能力 - 动作与未来预测：提升动态理解 - 时空关系推理：强化物体间交互理解 3. **实验验证** - **仿真任务（VIMA-Bench）**：在80k数据上训练时，LLaRA超过VIMA基准模型（平均成功率90.0% vs 80.7%），且仅需12%的数据量。 - **真实机器人任务**：在零样本场景下，LLaRA在移动/旋转/放置任务中平均成功率36.6%，优于GPT-4o（31.6%）和RT-2风格基线（0%）。 - 关键优势：在有限数据下表现优异，辅助数据集对小规模数据提升显著（如VIMA-0.8k数据上性能提升15-20%）。 4. **框架贡献** - 高效利用预训练VLM知识，通过指令调优实现机器人策略快速迁移 - 开源代码、数据集与模型：https://github.com/LostXine/LLaRA 该方法通过数据重构和自监督增强，解决了VLM在机器人控制中数据效率低的问题，为视觉-语言-动作（VLA）模型提供了新范式。</details> |
| 2024-06-27 | OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents | http://arxiv.org/abs/2407.00114v2 | <details><summary>展开</summary>本文提出OmniJARVIS，一种通过统一视觉-语言-动作标记化实现开放世界指令跟随的智能体模型。核心创新点包括： 1. **统一标记化框架** - 提出自监督行为编码器，将行为轨迹（动作序列）离散化为语义丰富的行为标记 - 扩充多模态语言模型词汇表，实现视觉（观察）、语言（指令/记忆/思考）和动作的统一序列建模 2. **关键技术** - **行为标记器**：基于改进的有限标量量化（FSQ）将128步行为轨迹编码为离散标记 - **策略解码器**：将行为标记解码为具体控制命令 - **多模态交互数据**：整合任务指令、记忆、观察、思维链和行为轨迹形成统一序列 3. **模型优势** - 支持思维链推理、任务规划、问答和行为生成 - 解决开放世界任务中文本指令描述不精确和长时程控制效率低的问题 - 行为标记压缩动作空间，提升决策效率 4. **实验验证** - 在Minecraft开放世界中测试原子任务、程序化任务和开放式任务 - 结果显示在复杂任务和长时程任务中优于传统文本目标传递或直接控制方法 - 分析验证了数据构建、标记化设计和模型扩展性的有效性 5. **资源承诺** 作者将开源数据集、模型和代码（项目页：https://craftjarvis.org/OmniJARVIS/）</details> |
| 2024-06-21 | Learning Efficient and Robust Language-conditioned Manipulation using Textual-Visual Relevancy and Equivariant Language Mapping | http://arxiv.org/abs/2406.15677v2 | <details><summary>展开</summary>本文提出了一种高效且鲁棒的语言条件操作框架GEM，核心创新点如下： 1. **文本-视觉相关性映射** - 利用预训练CLIP模型提取像素级语义相关性图，实现开放词汇的物体识别 - 创新性结合视觉数据库检索机制，通过演示数据中的图像块查询增强语义鲁棒性 - 多视图融合策略将2D相关性映射转换为3D点云投影，提升空间感知能力 2. **语言等变映射技术** - 提出语言可转向卷积核：将语言指令映射为动态卷积核，通过旋转生成等变核组 - 实现SE(2)空间等变性：当目标物体发生旋转/平移时，操作动作自动保持几何一致性 - 离散化SO(2)空间（72个旋转角度），确保模型对物体姿态变化的泛化能力 3. **高效学习框架** - 仅需10-20%的演示数据：相比CLIPort节省10倍数据量，相比VIMA仅需0.1%数据 - 双阶段操作架构：语言条件抓取模块实现等变操作，放置模块采用图像裁剪条件卷积 - 开环动作基元：通过空间动作图表示动作，降低策略学习复杂度 4. **实验验证优势** - 仿真/实物实验表明：在未见物体姿态上比OpenVLA提升53%鲁棒性 - 成功案例：剪刀握柄抓取、字母块精准插入等精细操作 - 支持多任务：开抽屉、物体整理等复杂指令的泛化执行 GEM通过融合视觉语言模型的零样本能力与几何等变特性，在保持数据效率的同时显著提升了对物体姿态和语言指令的泛化能力，为机器人自然语言交互提供了新范式。</details> |
| 2024-06-13 | OpenVLA: An Open-Source Vision-Language-Action Model | http://arxiv.org/abs/2406.09246v3 | <details><summary>展开</summary>待生成</details> |
| 2024-06-06 | RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation | http://arxiv.org/abs/2406.04339v2 | <details><summary>展开</summary>待生成</details> |
| 2024-05-31 | Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations | http://arxiv.org/abs/2406.00121v1 | <details><summary>展开</summary>待生成</details> |
| 2024-05-27 | A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation | http://arxiv.org/abs/2405.17418v2 | <details><summary>展开</summary>这篇论文提出了一种自校正视觉-语言-动作模型（SC-VLA），用于机器人操作系统的快速与慢速协同控制。核心要点如下： 1. **双系统框架设计** - **快速系统**：基于多模态大语言模型（MLLM），通过参数高效微调实现端到端的末端执行器位姿预测（SE(3)），保留模型的推理能力 - **慢速系统**：通过链式思维（CoT）训练策略实现故障检测与校正，利用终态图像和机器人状态识别位姿错误类型（位置/旋转/混合） 2. **创新校正机制** - 自适应专家反馈：根据错误类型动态调用专家模块（Where2Act位置专家/Anygrasp旋转专家/GPT-4V推理专家） - 迭代校正流程：识别错误→获取专家提示→生成校正动作，模拟人类反思过程 - 连续策略学习（CPL）：将成功校正样本通过指数移动平均技术注入适配器，提升快速系统稳定性 3. **关键实验结果** - **仿真测试**（SAPIEN）： - 相比SOTA方法（ManipLLM），在已见任务成功率从66%→87%，未见任务从30%→68% - 慢速系统使校正准确率提升21-30%，CPL进一步减少45%专家干预频率 - **实物测试**： - 通过少量样本微调实现跨域迁移 - 在拔插充电器等任务上达到90%成功率（表2） 4. **技术优势** - 首次在单一VLA模型中集成位姿预测与底层动作校正能力 - 校正过程效率提升40%（图5b），CPL学习使系统适应性提升39% - 支持开放环境中的连续学习，减少对外部专家的依赖 该方法在模拟和真实场景中均显著提升了操作鲁棒性，为应对复杂任务中的不确定性提供了新范式。</details> |
| 2024-05-23 | A Survey on Vision-Language-Action Models for Embodied AI | http://arxiv.org/abs/2405.14093v5 | <details><summary>展开</summary>本文综述了具身人工智能中的视觉-语言-动作模型（VLA），主要内容如下： 1. **VLA模型定义与架构** VLA是一种多模态模型，通过整合视觉输入（环境状态）、语言指令和动作输出，控制具身代理完成物理世界任务。核心架构包括： - **视觉编码器**：提取环境特征（如CLIP、DINOv2） - **语言编码器**：处理指令（如LLM） - **动作解码器**：生成低维动作（如机器人关节控制） 2. **研究分类** - **组件级研究**：优化预训练视觉表示（PVR）、动力学学习（前向/逆向预测）、世界模型（物理状态预测）和推理能力 - **低级控制策略**：直接生成动作（如CLIPort的SE(2)动作预测，RT系列的Transformer架构） - **高级任务规划器**：分解长视野任务为子任务序列（如模块化语言/代码规划器） 3. **关键资源** - **数据集**：涵盖真实机器人操作（CALVIN）、模拟环境（Meta-World）、人体演示及自动采集数据 - **基准测试**：任务规划（VIMA-Bench）、具身问答（Habitat）等评估体系 4. **挑战与方向** - 安全性保障、数据稀缺性、多模态融合 - 长视野任务框架、实时响应优化 - 伦理问题及多智能体协同 5. **贡献** 首次系统化VLA模型分类，提供开源项目资源（GitHub链接），并指出基础模型泛化、3D视觉融合等未来方向。</details> |
| 2024-05-09 | Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations | http://arxiv.org/abs/2405.06039v2 | <details><summary>展开</summary>这篇论文提出了一种名为Bi-VLA（视觉-语言-动作）的新型双手机器人操作系统，核心贡献和要点如下： 1. **系统架构** - 提出三模块集成框架：视觉模块（Qwen-VL模型）负责场景理解和物体检测（成功率96.06%）；语言模块（Starling-LM-7B）将人类指令转化为语义计划和可执行代码（代码生成成功率100%）；动作模块通过预设API控制双机械臂协同操作。 - 创新性地将语言指令、视觉感知与双手机器人动作生成无缝衔接，实现端到端任务执行。 2. **核心功能** - 支持复杂家庭任务（如按需制作沙拉）：通过视觉问答(VQA)确认食材可用性，语言模块生成动作序列（抓取、切割、投放等），双机械臂分工协作（一臂操作工具，另一臂处理食材）。 - 开发专用API函数库（如`grasp()`, `cut()`, `move_to_object()`），实现精细化动作控制。 3. **技术突破** - 提出像素坐标到3D空间的映射算法（结合布朗-康拉迪畸变校正模型），解决物体定位问题。 - 无需额外训练数据，通过语义规划和代码生成实现动态任务适应。 4. **实验验证** - 在沙拉制备任务中达成83.4%的总体成功率。 - 验证系统对多样化食谱和用户偏好的适应性，证明其在家庭服务场景的应用潜力。 该系统为双手机器人操作提供了可扩展框架，显著提升了人机协作的效率和自然性。</details> |
