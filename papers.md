| 日期 | 标题 | 链接 | 简要总结 |
| --- | --- | --- | --- |
| 2025-11-14 | Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective | http://arxiv.org/abs/2511.11478 | <details><summary>展开</summary>待生成</details> |
| 2025-11-14 | Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2511.11298 | <details><summary>展开</summary>待生成</details> |
| 2025-11-13 | OmniVGGT: Omni-Modality Driven Visual Geometry Grounded | http://arxiv.org/abs/2511.10560 | <details><summary>展开</summary>待生成</details> |
| 2025-11-13 | SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation | http://arxiv.org/abs/2511.10518 | <details><summary>展开</summary>待生成</details> |
| 2025-11-13 | Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks | http://arxiv.org/abs/2511.10008 | <details><summary>展开</summary>待生成</details> |
| 2025-11-13 | Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation | http://arxiv.org/abs/2511.09958 | <details><summary>展开</summary>待生成</details> |
| 2025-11-12 | MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation | http://arxiv.org/abs/2511.09516 | <details><summary>展开</summary>待生成</details> |
| 2025-11-12 | WMPO: World Model-based Policy Optimization for Vision-Language-Action Models | http://arxiv.org/abs/2511.09515 | <details><summary>展开</summary>待生成</details> |
| 2025-11-12 | MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror | http://arxiv.org/abs/2511.08865 | <details><summary>展开</summary>待生成</details> |
| 2025-11-11 | SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control | http://arxiv.org/abs/2511.07820 | <details><summary>展开</summary>待生成</details> |
| 2025-11-10 | How Do VLAs Effectively Inherit from VLMs? | http://arxiv.org/abs/2511.06619 | <details><summary>展开</summary>待生成</details> |
| 2025-11-09 | ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval | http://arxiv.org/abs/2511.06202 | <details><summary>展开</summary>### 论文研究单位<br>卡内基梅隆大学机器人研究所，美国匹兹堡<br><br>### 论文概述<br>论文提出了一种名为ExpReS-VLA的方法，旨在解决预训练的视觉-语言-动作（VLA）模型在特定部署环境中进行快速适应时遇到的灾难性遗忘和性能下降问题。该方法通过压缩的经验回放和检索增强生成技术，使模型能够在设备端利用少量演示（12个）快速适应新环境，同时保留原有能力。ExpReS-VLA通过存储视觉编码器的嵌入而非原始图像，实现了97%的存储空间节省，并结合检索到的相似经验和一种新的对比损失函数（THCL）来学习成功与失败的经验。实验表明，该方法在模拟和真实机器人任务上均显著提升了任务成功率。<br><br>### 论文核心贡献点<br>RAG增强的机器人学习：首次将检索机制集成到VLA微调中，加速了适应过程。<br>压缩的经验回放：一种通过冻结视觉编码器实现97%内存减少的技术，在保持语义保真度的同时，实现了实际部署。<br>用于失败利用的THCL：一种新颖的分段损失函数，通过动态选择合适的对比目标来防止重复性错误。<br>严谨的实证评估：在40个模拟任务（5个随机种子）和5个物理操作任务（共150次试验）中进行了系统性消融实验，明确了各组件的贡献。<br><br>### 论文方法描述<br>该方法首先使用OpenVLA冻结的视觉编码器（融合SigLIP和DINOv2）从RGB图像中提取1024维的嵌入向量，以实现高效的内存存储。<br>维护一个双缓冲区内存管理系统，分别存储成功和失败的经验轨迹，并采用FIFO策略和时序加权进行更新。<br>在训练时，系统根据当前观察的嵌入，从缓冲区中检索余弦相似度最高的k个相关经验，并将其与当前数据混合构建训练批次。<br>引入了阈值化混合对比损失（THCL），该损失结合了行为克隆损失和自适应的对比学习损失。当三元组损失低于阈值时使用三元组损失，否则使用InfoNCE损失，从而使模型能从成功和失败中学习。<br>整个在线学习流程采用LoRA进行高效微调，在性能低于阈值时触发，并在单块消费级GPU（如RTX 5090）上快速完成。<br><br>### 论文使用数据集和训练资源<br>数据集：模拟实验使用了LIBERO基准测试，包含四个任务套件（LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, LIBERO-Long）。真实机器人实验在7-DOF Franka Emika Panda机械臂上进行，包含5个操作任务。<br>训练资源：所有实验均在单个NVIDIA RTX 5090 GPU（32GB内存）上完成，使用BFloat16混合精度。模型微调采用LoRA配置，仅训练98.3M个参数（占总数的1.4%）。适应过程仅需12个演示和31秒。<br><br>### 论文使用的评估环境和评估指标<br>评估环境：在LIBERO模拟基准和真实的Franka Emika Panda物理机器人上进行评估。物理机器人任务包括分布内和分布外（包含未见过的背景和物体）两种测试场景。<br>评估指标：主要评估指标是任务成功率。模拟中，每个任务进行50次滚动测试，重复5个随机种子。物理机器人中，每个任务进行30次分布内试验和10次分布外试验。</details> |
| 2025-11-08 | 10 Open Challenges Steering the Future of Vision-Language-Action Models | http://arxiv.org/abs/2511.05936 | <details><summary>展开</summary>待生成</details> |
| 2025-11-07 | Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots | http://arxiv.org/abs/2511.05642 | <details><summary>展开</summary># 论文研究单位<br>- Department of Cyber‑Physical Systems, Clark Atlanta University (作者 1、2、3)<br>- Siemens Corporation (作者 4)<br><br># 论文概述<br>本文提出 **LiteVLA**，一种面向 CPU‑受限边缘机器人的轻量级视觉‑语言‑行动（VLA）框架。该系统将紧凑的 SmolVLM 主干、LoRA 参数高效微调、4‑bit NF4 量化以及 ROS 2 完整管线集成在 Raspberry Pi 4/TurtleBot 4 上，实现完全本地化的感知‑推理‑控制闭环。在无需云端资源的前提下，验证了在 CPU 上进行实时（或准实时）多模态推理的可行性，为无 GPS、带宽受限环境下的自治机器人提供思路。<br><br># 论文核心贡献点<br>1. **CPU‑only 本体 VLA**：在 Raspberry Pi 4 上使用 GGUF‑量化的 VLA 策略并通过 ROS 2 实现异步控制，基线延迟约 11.1 s/查询（0.09 Hz）。<br>2. **参数高效适配**：利用 LoRA（rank = 8，α = 8，dropout = 0.1）对 SmolVLM 主干进行任务专用微调，内存/算力需求极低。<br>3. **边缘量化与稳定性**：采用 4‑bit NF4 主干 + FP32 投影头的混合精度，实现约 75 % 内存降低、最高 9× 推理加速，同时保持输出稳定。<br>4. **端到端 ROS 2 管线**：统一的感知‑推理‑控制闭环，将 RGB 帧映射为结构化运动指令（geometry_msgs/Twist），使用 llama‑cpp 运行时。<br>5. **可扩展路线图**：提出六阶段 EDGE‑VLA‑ROADMAP，从地面机器人逐步扩展至无人机、多智能体协同、多模态接地、持续/强化学习及联邦安全。<br><br># 论文方法描述<br>- **系统架构**（Figure 1）<br> - 数据采集节点 → 多模态推理节点（SmolVLM+LoRA） → ROS 2 控制节点。<br>- **数据管道**（Algorithm 1）<br> - 通过遥控采集 15 083 条 RGB‑动作对；时间戳对齐 → 归一化 → 224×224 缩放 → 随机水平翻转增强；训练/验证划分 0.85 : 0.15。<br>- **LoRA 微调**<br> - 在 SmolVLM 主干的 query/key/value/output/gating 层注入 rank = 8 的低秩矩阵，损失函数为预测动作与标注文本的交叉熵；约两轮在 CPU 后端完成。<br>- **量化方案**<br> - 主干使用 NF4（4‑bit）量化，投影层保持 FP32（混合精度），以避免动作输出的不稳定。<br>- **推理与 ROS 2 集成**<br> - llama‑cpp‑python 加载 GGUF 权重；推理输出形如 “forward_0.2_3.0s”，在 ROS 2 中解析为 Twist 消息；采用 Action Chunking 机制实现低层持续控制与高层推理的异步解耦。<br><br># 数据集与训练资源<br>- **数据集**：遥控采集的 15 083 条 RGB‑动作对（包含线性/角速度及时间戳），每条指令在室内多环境下覆盖前、后、左、右、停止等类别，分布基本均衡。实际实验使用约 13 000 条（其中 1 152 条为后退指令，其余约 2 990 条/类）。<br>- **预处理**：统一尺寸 224×224、像素标准化、随机水平翻转；时间戳对齐保证图像‑动作一一对应。<br>- **训练资源**：全部微调在 CPU 后端完成（约两轮），未使用 GPU；模型量化与部署在 Raspberry Pi 4（ARM Cortex‑A72，1.5 GHz，4 GB RAM）上执行。<br><br># 评估环境与评估指标<br>- **硬件平台**：Raspberry Pi 4（4 GB RAM）+ TurtleBot 4 机器人，ROS 2 环境。<br>- **评估指标**<br> - **推理延迟**：每条查询的平均耗时；FP32 SmolVLM‑256 基线 ≈ 11 s，LiteVLA（FP32）≈ 18 min，混合量化 ≈ 2 min（9× 加速），全 NF4 ≈ 1.5 min（输出不稳定）。<br> - **内存占用**：混合量化较 FP32 主干降低约 75 %。<br> - **系统吞吐量**：推理频率约 0.09 Hz；控制层通过 Action Chunking 实现低层持续运动。<br> - **输出稳定性**：全 NF4 量化导致动作输出出现幻觉，混合精度保持稳定。<br>- **对比实验**：将 LiteVLA 与原始 SmolVLM‑256（FP32）以及未量化的 LiteVLA 进行横向比较，量化显著降低时延且维持可接受的控制质量。<br>- **稳健性讨论**：在高温或长时运行情况下出现热降频，导致延迟波动；CPU 竞争（图像采集、预处理、推理）亦产生延迟峰值。<br><br>以上内容概括了论文的研究单位、整体思路、核心贡献、技术路线、实验数据及评测细节。</details> |
| 2025-11-07 | EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation | http://arxiv.org/abs/2511.05397 | <details><summary>展开</summary># 论文研究单位<br><br>匹兹堡大学（University of Pittsburgh）<br><br># 论文概述<br><br>EveryDayVLA是一个面向可负担机器人操作的视觉-语言-动作模型。该研究结合了低成本硬件（300美元6自由度机械臂）和先进的VLA模型，通过协作训练预测离散和连续动作，并引入自适应地平线集成器（AdaHorizon）来提高实时操作的安全性和可靠性。在LIBERO仿真基准测试中达到了最先进的性能，在真实世界测试中在分布内场景领先49%，在分布外场景领先34.9%。<br><br># 论文核心贡献点<br><br>1. **协作训练与自适应地平线控制（AdaHorizon）**：联合训练连续（L1回归）和离散自回归动作头，使用分歧估计模型不确定性，在严格实时约束下动态调整动作范围触发重规划<br><br>2. **低成本集成6自由度机械臂**：300美元设计达到优于10mm重复精度，利用Arduino Uno分线板和PCA9685 PWM驱动器进行12位PWM控制<br><br>3. **自动化数据收集流水线**：简化遥操作收集带语言指令、视频和末端执行器轨迹的数据集，发布超过1200个任务执行以实现跨环境可扩展微调<br><br># 论文方法描述<br><br>基于Prismatic-7B VLM构建，使用SigLIP和DinoV2双部分视觉编码器，Llama 2语言模型作为主干。采用协作训练策略联合预测连续和离散动作块，连续动作通过多层感知器（MLP）动作头输出，离散动作通过256-bin离散化处理并使用softmax获得概率分布。损失函数结合交叉熵损失和L1损失（权重λ=1）平衡优化。<br><br>AdaHorizon算法通过计算连续和离散动作预测之间的平均绝对差异作为不确定性度量，动态调整执行的动作块长度。当预测分歧超过阈值时触发重规划，在实时约束下最小执行长度为4个动作。<br><br># 论文使用数据集和训练资源<br><br>- **数据集**：1200个演示的定制数据集，包含RGB观察序列、对应末端执行器姿态和自然语言指令，涵盖多种桌面环境，包括抓取放置、环境操作和块堆叠任务<br>- **训练资源**：仿真实验在2个A100 GPU上微调10万次迭代，真实世界实验在1个A100 GPU上微调5万次迭代；使用LoRA（rank=32）、batch size=8和4步梯度累积<br><br># 论文使用的评估环境和评估指标<br><br>- **仿真基准**：LIBERO四个任务套件（空间、物体、目标、长期），使用成功率（SR）作为主要评估指标<br>- **真实世界评估**：分布内和分布外场景测试，包括静态和动态干扰物评估；成功率作为主要指标<br>- **推理性能**：在LIBERO上测量推理频率（Hz）和延迟（秒）<br>- **对比基线**：与Diffusion Policy、Octo、DiT Policy、OpenVLA、OpenVLA-OFT、ACT、HybridVLA、COGAct等方法进行比较</details> |
| 2025-11-07 | TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models | http://arxiv.org/abs/2511.05275 | <details><summary>展开</summary>### 论文研究单位<br>延世大学人工智能系（Yonsei University, Department of Artificial Intelligence），微软研究院（Microsoft Research）。<br><br>### 论文概述<br>Vision-Language-Action模型（VLAs）在单臂机器人操控中表现出色，但双臂操控因缺乏大规模公开数据而面临挑战。本研究提出TwinVLA，通过组合两个预训练单臂VLA模型实现数据高效的双臂操控。TwinVLA避免了大量双臂预训练数据的依赖，仅需少量双臂演示即可在真实世界和仿真任务中达到或超越单体模型（如RDT-1B）的性能，并接近状态最先进的π_0模型。<br><br>### 论文核心贡献点<br>- 提出模块化双臂架构：通过复制预训练单臂VLA并结合联合注意力机制，实现两臂协调控制。<br>- 数据高效微调范式：仅使用小量双臂数据（每任务约50演示）进行微调，无需额外双臂预训练。<br>- 性能验证：在真实世界（Anubis机器人）和仿真（RoboTwin 2.0、Tabletop-Sim）任务中，TwinVLA在成功率上优于或匹配RDT-1B和DP方法，接近π_0模型。<br><br>### 论文方法描述<br>TwinVLA基于以下三大核心组件：<br>1. **单臂策略复制**：复制预训练单臂VLA的VLM骨干（仅复制1.3B参数），共享视觉编码器和DiT动作头。左右臂各有一个轻量级本体感觉编码器。<br>2. **联合注意力机制**：共享两个VLM间的自注意力层，实现跨臂信息融合，采用因果联合注意力掩码保持时序性和对称交互。<br>3. **混合专家（MoE）集成**：通过MoE高效处理共享输入（如语言和自我视角图像），减少计算开销。同时，采用任务算术和注意力重加权技术保留预训练知识并加速微调。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 预训练：OXE数据集的0.5M轨迹子集。<br> - 微调：真实世界任务每任务收集50集（绝对末端执行器控制），仿真任务每任务50集（如RoboTwin 2.0和Tabletop-Sim）。<br>- **训练资源**：<br> - SingleVLA预训练：5×H100 GPUs，耗时5天，120k步。<br> - TwinVLA微调：1×L40S GPU，耗时2天，100k步。<br> - 计算总量：约25 H100 GPU天（RDT-1B需超1,000 H100 GPU天）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **真实世界**：使用Anubis双臂机器人，任务包括“carrot to bag”（胡萝卜入袋）、“brush to dustpan”（刷子入簸箕）、“take towel off”（取下毛巾）。<br> - **仿真**：<br> - RoboTwin 2.0：50个双臂任务（Easy和Hard设置）。<br> - Tabletop-Sim：5个任务（如“dish-drainer”（碗碟架）、“handover-box”（传递盒子））。<br>- **评估指标**：<br> - 主要指标为任务成功率（Success Rate），在每任务20-500次 rollout 中计算平均成功率。<br> - 语言跟随测试：多任务组合指令（如“put X box into Y pot”）下的平均成功率。<br> - 数据效率评估：随演示数量（20、35、50）变化的成功率曲线。</details> |
| 2025-11-06 | Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment | http://arxiv.org/abs/2511.04555 | <details><summary>展开</summary>待生成</details> |
| 2025-11-06 | GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies | http://arxiv.org/abs/2511.04357 | <details><summary>展开</summary>待生成</details> |
| 2025-11-04 | XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations | http://arxiv.org/abs/2511.02776 | <details><summary>展开</summary>## 论文研究单位<br>北京具身智能机器人创新中心（牵头单位），联合北京航空航天大学（机械工程与自动化学院、虚拟现实技术与系统国家重点实验室）、北京大学（计算机学院多媒体信息处理国家重点实验室）共同完成。<br><br>## 论文概述<br>针对视觉语言行动（VLA）模型在低层动作精确控制和跨机器人形态数据整合方面的挑战，论文提出 **XR-1** 框架：通过引入 **统一视觉运动代码（UVMC）** 作为离散潜在表征，结合三阶段训练范式，实现多模态对齐与跨身体控制的视觉语言行动模型。该模型可同时利用人类视频和机器人数据，显著提升多任务泛化能力和实际部署效果。<br><br>## 论文核心贡献点<br>1. **UVMC 机制**<br> 提出统一视觉运动代码（UVMC），通过双分支 VQ-VAE 将视觉动态和机器人运动编码至共享离散潜在空间，并通过 KL 散度约束实现跨模态对齐。<br>2. **三阶段训练框架**<br> 包含自监督 UVMC 学习、跨身体预训练和任务特定微调，模型无关于特定 VLA 架构，可适配多种基础模型（如 π₀、SwitchVLA）。<br>3. **大规模实证验证**<br> 在 6 种机器人平台、120+ 任务中进行超过 14,000 次实机测试，平均成功率显著超越 π₀.₅、π₀、RDT、UniVLA 等 SOTA 基线。<br><br>## 论文方法描述<br>### 核心架构<br>- **UVMC 学习（Stage-1）**<br> 通过双分支 VQ-VAE 分别编码视觉动态（未来帧预测）和机器人运动（动作序列重构），共享码本实现模态统一，引入视觉-运动 KL 散度损失促进对齐。<br>- **跨身体预训练（Stage-2）**<br> 使用 UVMC 作为监督信号，通过可学习 token 将其注入 VLM 主干，结合动作预测损失联合训练策略网络。<br>- **任务特定微调（Stage-3）**<br> 基于目标机器人任务数据微调模型，提升特定场景性能。<br><br>### 训练流程<br>1. 大规模数据预训练 UVMC：利用 Open-X、RoboMIND、Ego4D、XR-D 四类数据（人类视频+机器人数据），加权采样平衡不同来源。<br>2. UVMC 引导策略预训练：在 XR-D 上进行跨身体动作学习。<br>3. 任务特定微调：针对下游任务数据进行小样本适配。<br><br>## 论文使用数据集和训练资源<br>- **数据集组成**：<br> - Open-X（978k episodes, 59.3M frames, 权重 40%）<br> - RoboMIND（69k episodes, 21.4M frames, 权重 15%）<br> - XR-D（158k episodes, 69.1M frames, 权重 35%）<br> - Ego4D（59k episodes, 14.3M frames, 权重 10%）<br>- **训练资源**：<br> - 主模型 XR-1：基于 PaliGemma（SigLIP+Gemma）架构<br> - 轻量模型 XR-1-Light：基于 SwitchVLA（Florence-2）架构<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 6 种真实机器人平台：Tien Kung 1.0/2.0、单/双臂 UR-5e、双臂 Franka、AgileX Cobot Magic 2.0<br> - 覆盖 120+ 复杂任务：双臂协作、灵巧操作、可变形体处理、接触密集任务、动态环境等<br>- **评估指标**：<br> - **成功率（Success Rate）**：每任务 20 次测试的平均完成率<br> - **跨身体泛化**：在未见过机器人平台（如 Tien Kung 2.0）上验证迁移能力<br> - **鲁棒性测试**：新增物体、背景干扰、光照变化下的性能保持率<br><br>**效果摘要**：<br>XR-1 在所有平台平均性能显著优于 π₀.₅、π₀ 等基线（如 Tien Kung 2.0 任务平均 72% vs 41%），并在未知场景（新增干扰物等）中展现更强鲁棒性。<br><br>---<br>论文核心创新在于通过 UVMC 机制统一视觉与动作模态，结合三阶段训练打通人类演示与机器人数据，实现跨身体的通用技能学习。实验充分验证了其在复杂操作中的有效性。</details> |
| 2025-11-01 | iFlyBot-VLA Technical Report | http://arxiv.org/abs/2511.01914 | <details><summary>展开</summary>论文研究单位<br>iFLYTEK Research and Development Group; LindenBot<br><br>论文概述<br>iFlyBot-VLA 是一个基于 Transformer 语言主干与扩散/流匹配动作专家的 Vision-Language-Action（VLA）模型，用于端到端双机械臂操作。论文提出“显式 + 隐式”双层动作表示框架：以频域压缩的离散动作标记 FAST 提供显式监督，帮助 VLM 学习动作语义和隐式规划；以 VQ-VAE 预训练的紧凑潜在动作表示提供隐式规划信号，仅将其特征传入动作专家，实现高效且可控的动作生成。该方法在保持 VLM 通用感知与推理能力的同时，缓解了端到端训练对 VLM 能力的破坏，并通过混合训练策略（机器人轨迹 + 通用 QA/空间 QA 数据）进一步提升泛化。<br><br>论文核心贡献点<br>提出并训练了基于 VQ-VAE 的潜在动作模型，代码本规模为 32，每步检索 8 个离散码，引入 NSVQ 近似以替代 Straight-Through Estimator 并在解码时对当前帧停止梯度。<br>提出双层动作表示：FAST 离散动作标记用于监督 VLM 学习动作语义（特征不送入动作专家），潜在动作标记作为压缩后的隐式规划信号输入动作专家，实现高效连续控制。<br>提出混合训练策略，在预训练阶段将空间 QA 与机器人轨迹数据按优化比例混合，保留并增强 VLM 的通用感知与空间推理；并通过截断专家到主干梯度、在下游微调阶段开启专家反向传播与多噪声扰动等策略稳定训练。<br>在 LIBERO 仿真与真实世界任务上实现领先性能与泛化，并计划开源部分自建数据集。<br><br>论文方法描述<br>总体架构：基于 Qwen2.5-VL(3B) 作为视觉语言主干，在其上接入 Flow-Matching 扩散 Transformer 动作专家；仅传递潜在动作标记对应的 KV 缓存至专家，离散 FAST 标记的 KV 不传递。<br>潜在动作模型（Stage I）：以自监督方式从人机操作视频中学习潜在动作；采用 NSVQ 解决 VQ-VAE 训练中的梯度崩塌问题。<br>离散动作标记（FAST）：对滑窗连续控制信号进行 DCT 压缩和 BPE 编码，仅用于监督 VLM 的动作语义与隐式规划，不向动作专家提供特征。<br>VLA 训练（Stage II/III）：双阶段训练，第一阶段截断专家到主干梯度以保护 VLM 能力，第二阶段开启梯度传播并引入多噪声扰动加速专家适配。<br>动作生成：采用 Flow Matching，通过离散前向欧拉积分（5 步，σ=0.2）从随机高斯噪声逐步还原动作块；动作与状态统一 pad 到 20 维（左右臂各 10 维）。<br><br>论文使用数据集和训练资源<br>潜在动作训练数据（Stage I）：人类视频数据集 HoloAssist、Ego4D、EgoDex、HOI4D、Something-Something V2、EgoVid；机器人数据集 OXE、AgiBot-World、RoboMind、Galaxea。<br>VLA 预训练数据（Stage II）：内部构建的空间推理 QA 数据；公开数据集 OXE、AgiBot-World 的子集；iFLYTEK 自采集双臂操作数据（衣褶、通用抓取摆放、长程包裹分拣）。<br>自采集数据集规模与构成：<br>衣褶任务：8 类衣物（5 款 T 恤、3 款短裤），每类约 190 条轨迹，平均 4.5 分钟，约 110 小时。<br>通用抓取摆放：30 类物体，每类约 400 条轨迹，平均 27 秒，约 90 小时。<br>长程包裹分拣：约 2,752 条轨迹，平均 61 秒，约 47 小时。<br>预训练配比与阶段：预训练阶段混合上述数据集与空间 QA；微调阶段进行任务特定训练（如 LIBERO：70,000 步用于 Long 子集，50,000 步用于其他子集；动作窗口长度 7；全局批大小 64；仅第三人称图像与文本指令）。<br><br>论文使用的评估环境和评估指标<br>仿真评估：LIBERO 基准（LIBERO-Spatial、Object、Goal、Long 四个任务套件，各 10 个任务×10 条演示）；指标为成功率（%）。<br>真实世界评估：<br>通用抓取摆放：四种配置（基础、未见物体、光照变化、未见场景），24 类可见或 14 类未见物体，每类 20 次尝试；指标为成功率（%）。<br>长程包裹分拣：双机械臂翻转与摆放流程，40 次×3 包裹（两条需翻转）试验；采用“严格（不允许校正）”与“允许校正（最多两次校正）”两种评估；指标为成功率与相对提升（%）。<br>衣褶任务：定义步骤级评估协议，统计各步骤完成率；任务含“点选角落”、“摊平（扫平/拖拽）”、“折叠”等子步骤；允许在 3 分钟时间限制内多次尝试。</details> |
| 2025-11-03 | Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process | http://arxiv.org/abs/2511.01718 | <details><summary>展开</summary># 论文研究单位<br>- HKUST(GZ) - 香港科技大学（广州）<br>- Westlake University - 西湖大学<br>- Zhejiang University - 浙江大学<br>- Monash University - 蒙纳士大学<br><br># 论文概述<br>论文提出统一扩散VLA（UD-VLA），通过联合离散去噪扩散过程（JD3P）将视觉、语言和动作的理解、生成和执行统一到单个Transformer中。核心创新是通过同步去噪过程联合优化视觉生成和动作预测，使动作在持续的视觉指导下从初始化逐步演化，实现真正的跨模态协同。<br><br># 论文核心贡献点<br>- 提出统一扩散VLA，通过同步去噪过程紧密耦合理解、生成和执行，形成互利关系<br>- 通过离散标记化、混合注意力和JD3P过程作为跨模态协同的核心机制来实例化设计<br>- 设计两阶段训练pipeline激活图像生成能力，引入多种测试时技术确保高性能和效率<br>- 在CALVIN、LIBERO和SimplerEnv等基准上达到SOTA性能，推理速度比自回归方法快4倍<br><br># 论文方法描述<br>**统一标记化**：将语言、视觉和动作模态转换为离散标记并连接成单序列，使用VQ标记器进行视觉量化，FAST进行动作标记化<br><br>**混合注意机制**：输入部分采用因果和双向注意，输出分为生成块（未来图像）和执行块（动作），块内双向注意，块间因果注意，阻止动作信息回流<br><br>**联合离散去噪扩散过程（JD3P）**：并行生成动作和图像，通过马尔可夫链噪声过程和条件去噪分布，在每个去噪步骤联合重构掩码位置，从噪声逐步恢复原始信号<br><br>**两阶段训练**：阶段(i)在视频数据集上注入未来图像生成能力；阶段(ii)在机器人动作数据集上联合优化图像和动作生成<br><br>**推理技术**：前缀KV缓存、预填充特殊令牌、置信度引导解码、解码空间映射等提升效率<br><br># 论文使用数据集和训练资源<br>- **预训练数据**：大规模视频数据集用于阶段(i)训练<br>- **下游数据**：机器人动作数据集用于阶段(ii)联合训练<br>- **评估基准**：CALVIN、LIBERO、SimplerEnv仿真环境，真实世界UR5e机械臂+Inspire RH56E2机械手平台<br>- **初始化模型**：预训练VLM主干网络（Emu3）<br><br># 论文使用的评估环境和评估指标<br>**CALVIN**：4个环境、34个任务、1000条语言指令，报告连续5个子任务的平均完成长度（最大值5.0）<br><br>**LIBERO**：4个套件（空间、物体、目标、长程），每套件10个任务×50次运行，报告各套件和整体成功率<br><br>**SimplerEnv-WidowX**：真实到仿真传输评估，4个任务，报告各任务和整体成功率<br><br>**真实世界实验**：UR5e机械臂+6自由度Inspire RH56E2机械手，3类任务（堆叠碗、放置方块、翻转塔），每类200条轨迹，在可见和不可见设置下评估各30次的成功率</details> |
| 2025-11-03 | PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model | http://arxiv.org/abs/2511.01571 | <details><summary>展开</summary>### 论文研究单位<br>- 华南理工大学自动化科学与工程学院<br>- 中国科学院沈阳自动化研究所<br>- 穆罕默德·本·扎耶德人工智能大学（Mohamed bin Zayed University of Artificial Intelligence）<br>- 澳大利亚国立大学<br><br>### 论文概述<br>PixelVLA 是首个支持像素级理解和多模态提示（文本和视觉输入）的视觉-语言-动作（VLA）模型。现有 VLAs 局限于图像级理解且依赖文本提示，导致空间推理和跨域泛化能力不足。PixelVLA 通过集成多尺度像素感知编码器、视觉提示编码器和连续动作解码器，解决这些挑战。使用两阶段自动注释管道生成的 Pixel-160K 数据集进行训练，该数据集包含 160K 操作片段和 6.5M 图像-文本-动作三元组。PixelVLA 在三个标准 VLA 基准测试（SimplerEnv、Google Robot、LIBERO）上显著优于 OpenVLA，成功率提升 10.1%~28.7%，且训练成本仅为 OpenVLA 的 1.5%。<br><br>### 论文核心贡献点<br>1. **PixelVLA 架构设计**：提出支持像素级理解和多模态提示的 VLA 模型，包括多尺度像素感知编码器、视觉提示编码器（处理点、线、区域等提示）和连续动作解码器（直接预测 7D 动作）。<br>2. **两阶段自动注释管道**：开发用于合成 Pixel-160K 数据集的管道，第一阶段定位夹爪并生成区域提议，第二阶段使用 LLM 和开放词汇分割模型生成像素级注释和视觉提示。<br>3. **视觉运动指令调优框架**：引入两阶段训练流程——连续动作训练阶段和像素级理解增强阶段，有效提升像素级空间理解能力。<br>4. **实证验证**：在多个基准测试上集成 PixelVLA 架构到 OpenVLA 和 π₀ 模型，验证性能提升和泛化能力。<br><br>### 论文方法描述<br>- **PixelVLA 架构**：基于 Prismatic-7B VLM 和 Llama 2-7B 的主干，结合视觉编码器（DinoV2 和 SigLIP）、MLP 投影仪、视觉提示编码器（源自 SAM）、多尺度像素感知编码器（生成像素感知嵌入）和连续动作解码器（ResNet 块 + MLP，预测连续动作序列）。<br>- **多尺度像素感知编码器**：从多尺度视觉特征中提取像素级信息，公式基于特征和像素掩码的 MLP 组合。<br>- **视觉提示编码器**：处理多样化视觉提示（如点、线、区域），生成提示感知嵌入。<br>- **连续动作解码器**：基于 LLM 隐藏状态，通过线性投影、ResNet 块和 MLP 输出连续动作值（chunk 大小为 8）。<br>- **训练流程**：两阶段视觉运动指令调优。<br> - **阶段一（连续动作训练）**：使用 Fractal 和 Bridge v2 数据训练连续动作解码器，冻结其他模块。<br> - **阶段二（像素级理解增强）**：使用 Pixel-160K 数据集，通过 LoRA 微调 LLM 主干，联合训练视觉提示和像素感知编码器。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：Pixel-160K（160K 操作片段，6.5M 三元组），基于 Fractal 和 Bridge v2 数据集构建；LIBERO-Pixel（使用管道处理 LIBERO 基准）。<br>- **训练资源**：<br> - 两阶段训练：阶段一 100k 步（批量 32，学习率 5e-4），阶段二 200k 步（批量 32，学习率 1e-3，LoRA r=32）。<br> - 总训练成本仅为 OpenVLA 预训练的 1.5%。<br> - 输入分辨率 224×224，单视角第三视角相机。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：三个仿真基准测试。<br> - SimplerEnv（Google Robot 和 WidowX 设置）：评估零样本对象操作。<br> - LIBERO：四个任务套件（LIBERO-Spatial、-Object、-Goal、-Long）评估新机器人设置适配。<br>- **评估指标**：<br> - SimplerEnv：成功率，包括视觉匹配（VM）和变体聚合（VA）分数；平均抓取和任务完成成功率。<br> - LIBERO：每个任务套件的成功率和排名。<br> - 主要指标：操作成功率（%），如 Google Robot 上 PixelVLA 平均 VM 得分 61.4（提升 28.7%）。</details> |
| 2025-11-03 | RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models | http://arxiv.org/abs/2511.01331 | <details><summary>展开</summary>### 论文研究单位<br>Westlake University<br><br>### 论文概述<br>RobustVLA是一种针对Vision-Language-Action (VLA)模型的在线强化学习后训练方法，旨在提高模型在环境扰动（如观察噪声和动作噪声）下的鲁棒性。传统方法在out-of-distribution部署中易失败，RobustVLA通过理论分析的鲁棒性界限，引入Jacobian正则化和smoothness正则化来显式约束模型敏感性，显著提升VLA模型的稳定性和可靠性。<br><br>### 论文核心贡献点<br>- 提出RobustVLA方法，结合在线RL和鲁棒性约束，针对环境扰动优化VLA模型。<br>- 进行系统鲁棒性分析，识别观察扰动和动作扰动的影响，分别推导Jacobian敏感性和smoothness稳定性的关键作用。<br>- 引入双正则化策略：Jacobian正则化抑制观察噪声敏感性，smoothness正则化缓解动作扰动和更新漂移。<br>- 实验证明RobustVLA在多种扰动下优于SOTA基线（如RIPT-VLA、OpenVLA等），提升迁移学习和消融性能。<br><br>### 论文方法描述<br>- **问题建模**：基于马尔可夫决策过程（MDP），VLA模型πθ映射语言指令和观察序列到动作分布。<br>- **鲁棒性分析**：<br> - 定理1（观察扰动误差界限）：返回差距受Jacobian敏感性和观察噪声影响，需控制\|\|∇_s log πθ(a\|s)\|\|。<br> - 定理2（动作扰动返回漂移）：返回差距与累积模型漂移∑δ_t和动作噪声σ√d相关，需限制模型更新平滑度。<br> - 定理3（联合扰动鲁棒性）：观察和动作扰动联合时返回差距扩大，需同时应用双正则化。<br>- **正则化目标**：<br> - Jacobian正则化：ℛ_Jac(θ) = E[min(\|\|∇_s log πθ(a\|s)\|\|², G_max)]，限制输入敏感性。<br> - Smoothness正则化：ℛ_Smooth(θ) = E[\|\|μ_θ(s) - μ_θ-(s)\|\|²]，稳定模型更新。<br> - 整体目标：ℒ_RobustVLA = ℒ_PPO + αℛ_Jac + βℛ_Smooth。<br>- **算法实现**：采用课程学习机制（RobustVLA-C），基于成功率的移动平均自适应调整噪声水平（ε_min到ε_max），防止训练初期不稳定。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：基于LIBERO仿真平台，包含Objects、Long、Spatial、Goal四个任务套件，每个套件使用50个保留测试上下文。<br>- **扰动设置**：<br> - 观察扰动：图像移位、旋转、颜色抖动、遮挡、擦除。<br> - 动作扰动：零均值高斯噪声，标准差0.1、0.2、0.3。<br>- **训练资源**：在线RL交互收集数据，算法在仿真环境中运行；评估基于单VLA模型部署至所有任务。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO仿真平台，模拟机器人操作任务场景。<br>- **评估指标**：<br> - 主要指标：平均成功率（SR）在扰动下的表现。<br> - 对比基线：离线IL（π₀、GEVRM、OpenVLA、OpenVLA-OFT）、离线RL（RWR、ARFM、ReinboT）、在线RL（RIPT-VLA）。<br> - 其他评估：迁移学习性能（从LIBERO Goal套件到下游任务）、消融研究（超参数α和β影响、T-SNE观察表示可视化）。</details> |
| 2025-11-03 | Embodiment Transfer Learning for Vision-Language-Action Models | http://arxiv.org/abs/2511.01224 | <details><summary>展开</summary># 论文研究单位<br>Shanghai University（上海大学）<br><br># 论文概述<br>论文介绍了ET-VLA（Embodiment Transfer Learning for Vision-Language-Action），一种用于将预训练VLA模型高效迁移到多机器人系统的新型框架。传统自回归VLA模型（如RT-2、OpenVLA）在单臂机器人任务上表现优异，但在多机器人协作场景中性能显著下降，经常无法生成结构有效的动作序列。ET-VLA通过合成继续预训练（SCP）和实体化思维图（EGoT）技术，在双机器人系统上显著提升性能，在六个真实世界任务中成功率比OpenVLA高出53.2%。<br><br># 论文核心贡献点<br>- 深入分析了现有自回归VLA模型在多机器人多任务设置中的局限性<br>- 提出ET-VLA框架，包含合成继续预训练（SCP）和实体化思维图（EGoT）两个关键技术<br>- 在真实机器人和仿真环境中广泛评估，证明了相对于最先进VLA模型的优越性能<br><br># 论文方法描述<br>**合成继续预训练（SCP）**：生成合成多机器人数据来预热模型，使其能够生成正确的动作序列和精确的动作token数量。SCP采用交叉采样方法，从batch中随机选择不同样本的动作tokens，组合成14个动作tokens（每个机器人7个DoF），使模型学习token到机器人的映射关系。<br><br>**实体化思维图（EGoT）**：将复杂任务分解为显式表示时间依赖关系的动作图。该方法定义了五个任务类型（Grasp、Release、Waiting、End、Complete），让VLA模型理解和管理多机器人任务中的时间依赖，提供结构化和可解释的规划框架，增强任务规划和跨机器人协调能力。<br><br># 论文使用数据集和训练资源<br>- 使用Open X-Embodiment (OXE)数据集进行预训练<br>- 使用Bridge Data V2进行微调<br>- 收集458条跨6个不同任务的真实机器人轨迹数据<br>- 额外收集980条人类演示轨迹<br>- 在16个A100 GPU上进行训练<br><br># 论文使用的评估环境和评估指标<br>**评估环境**：在三种不同双机器人实体上进行验证：双UR5e、双Franka、双AgileX机器人。设计了六个协作任务：PickBread（拾取面包）、PickFruits（拾取水果）、WipePlate（擦拭盘子）、InsertPlate（插入盘子）、PullString（拉绳）、BuildBlocks（构建积木）。同时在RLBench2和RoboTwin仿真基准上进行评估。<br><br>**评估指标**：主要使用平均成功率（%）作为评估指标，在每个任务上报告成功次数/总试验次数的比率，并通过多个任务的平均成功率来评估整体性能。</details> |
| 2025-11-03 | OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation | http://arxiv.org/abs/2511.01210 | <details><summary>展开</summary># 论文研究单位<br><br>Princeton University、University of California, Los Angeles、Microsoft Research Asia<br><br># 论文概述<br><br>论文提出了OmniVLA，一个多模态视觉-语言-动作（VLA）模型，通过集成红外、毫米波雷达和声学传感器等新型感知模态，实现超越RGB感知能力的机器人操控。该模型将异构传感器统一到图像空间中，利用传感器掩码图像这一核心创新，实现了物理基础的空间智能。<br><br># 论文核心贡献点<br><br>1. 首个统一多个感知模态的VLA模型，整合红外、毫米波雷达和声学传感器<br>2. 提出传感器掩码图像技术，实现传感器信息的空间定位和语义对齐<br>3. 设计轻量级多传感VLA模型架构，支持数据高效学习<br>4. 显著提升机器人操控任务的成功率和泛化能力<br><br># 论文方法描述<br><br>核心方法是传感器掩码图像生成：<br>- 传感器数据预处理：将毫米波雷达和麦克风阵列数据通过波束成形转换为二维热图<br>- 分割掩码生成：使用GPT-4o生成分割提示词，结合Grounded SAM 2进行语义分割<br>- 图像融合：将传感器图像在掩码区域叠加到RGB图像上<br><br>模型架构采用预训练的SmolVLA作为基础：<br>- 冻结视觉和语言编码器<br>- 为每个传感器模态添加独立的多层感知机投影层<br>- 投影后的传感器token与语言token拼接，输入大语言模型<br>- 使用扩散式动作专家模块生成最终机器人动作<br><br># 论文使用数据集和训练资源<br><br>硬件平台：SO101机械臂配合多模态传感器套件，包括RGB相机、红外热像仪、毫米波雷达和六麦克风圆形阵列<br><br>训练数据：收集800个演示片段（每类传感器任务200个，通用抓取任务200个）<br><br>计算资源：<br>- 训练：多个Nvidia A100 GPU进行分布式训练<br>- 推理：本地RTX 4090 GPU，支持15fps实时预测<br><br># 论文使用的评估环境和评估指标<br><br>评估任务涵盖三种需要非视觉感知的操控任务：<br>- 热传感任务：区分冷热饮料并抓取冷饮<br>- 毫米波任务：透视纸箱定位隐藏物体<br>- 声学任务：定位并 uncovering响铃手机<br><br>评估指标：<br>- 任务成功率：25次独立试验的成功率<br>- 任务评分：0.5分选择正确目标+0.5分完成正确操作<br>- 泛化能力：在未见任务上的少样本学习表现<br><br>基准对比：<br>- VLA-RGB：仅RGB输入的VLA模型<br>- VLA-RAW：使用原始传感器数据的VLA模型<br><br>实验结果：OmniVLA达到84%平均成功率，分别比RGB-only和raw-sensor基线高出59%和28%。</details> |
| 2025-11-02 | Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots | http://arxiv.org/abs/2511.00917 | <details><summary>展开</summary>## 论文研究单位<br>宾夕法尼亚大学（University of Pennsylvania），并开放项目主页：maestro-robot.github.io。<br><br>## 论文概述<br>论文提出maestro（Managerial Agent for Executing Sensorimotor Tasks in Robotics），一个以视觉语言模型（VLM）为中心的模块化智能体，通过编排包含感知、几何、控制、预训练视觉-动作模型与图像编辑等在内的“工具集”，以编写与执行代码的方式在真实世界中实现零样本通用机器人操作。该范式不依赖大规模机器人数据，而是在闭循环中“计划—反应—重计划”的执行与反馈回路中不断演化，适应新任务与新硬件。<br><br>## 论文核心贡献点<br>- 首个在零样本条件下能与最新视觉-语言-动作（VLA）模型竞争的模块化机器人策略。<br>- 提出了以VLM为编码智能体、工具库为核心的系统化设计（覆盖几何与主动感知、碰撞规避、VLA高频监控），显著提升语义理解与精确执行的协同能力。<br>- 系统性消融实验揭示主动感知与几何推理工具的必要性。<br>- 展示在少量真实试验基础上的演化式改进机制，通过历史代码与失败分析进行上下文学习。<br><br>## 论文方法描述<br>- 总体架构：以VLM（文中采用Gemini Robotics-ER 1.5）为代码生成智能体，接收任务指令与场景图像，动态编写程序并实时执行与修改，形成“计划—反应—重计划”的闭环。<br>- 工具模块设计（Tabletop / Mobile 双套件）：<br> - 感知：原始RGB+本体感觉；分割与中心点；指向与任务相关关键点（受ReKep启发）；FoundationStereo深度；主动感知（手腕相机变焦/环视）。<br> - 控制与几何：笛卡尔与手爪控制；cuRobo点云无碰撞运动规划；几何与线性代数（向量构造、距离、相对旋转、向量旋转）以支持空间推理。<br> - 视觉运动策略：GraspGen抓取模型；π0.5 VLA作为可调用工具，配以本地托管的Qwen-2.5-VL-72B实现2Hz任务完成监控以高频中断与重规划。<br> - 图像编辑：在图像上绘制关键点与6D位姿叠加，提升视觉定位与推理。<br> - 移动操控：Mobile base状态估计（Faster-LIO）；语义地图缓存；主动探索工具；细粒度“nudge”局部微调与全局导航（Nav2）。<br>- 演化式改进：记录历史执行代码、输出与失败分析，作为上下文样例供后续尝试学习与优化。<br><br>## 论文使用数据集和训练资源<br>- 评估方法：为检验零样本泛化，采用STAR-Gen生成评测场景，固定对比基线；5次试验/任务（初始+4次扰动）。<br>- 真实世界平台与任务：<br> - 桌面操控：Franka Emika Panda（7-DoF）+ Robotiq 2F 手爪；手腕与第三人称相机；DROID平台。任务包括拾取放置、可变形物体（折毛巾/T恤）、铰接物体（开柜）、空间推理（紫色面朝上旋转立方体）、工具使用（用刀切香蕉）、物体功能（挂杯）、记忆与长时序（擦白板指令后按指令叠杯）。<br> - 移动操控：Unitree Go2-W 四足机器人 + AgileX PiPER机械臂；校准手腕相机。任务包括长时序取物、投掷、主动探索（搜索并返回）、物体功能（按按钮开门）。<br>- 预训练/基础模型：<br> - VLM：Gemini Robotics-ER 1.5（用于代码生成与视觉推理）；Qwen-2.5-VL-72B（本地高频监控与中断）。<br> - VLA：π0-FAST-DROID 与 π0.5-DROID（作为可调用工具）。<br> - 其他：GraspGen（抓取）、FoundationStereo（深度估计）、cuRobo（无碰撞规划）、Faster-LIO（移动基定位）、Nav2（导航）。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：两类真实世界本体——桌面操控（DROID、Franka）、移动操控（四足+机械臂），跨场景扰动（物体、位姿、语言指令、背景等）形成零样本测试集。<br>- 评估指标：<br> - 主要指标：平均任务进度（0–100，数值越高越好），基于STAR-Gen与任务分解子目标的可量化评分。<br> - 对比基线：Gemini Robotics Agent（代码即策略）、π0-FAST-DROID、π0.5-DROID；另含maestro+π0.5（VLA作为工具并联）。<br> - 消融设置：去除主动感知与几何模块对性能的影响。<br> - 跨任务类别：在桌面与移动操控的多样任务上进行对比与误差分析。</details> |
| 2025-10-31 | End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection | http://arxiv.org/abs/2511.00139 | <details><summary>展开</summary>### 论文研究单位<br>ByteDance Seed<br><br>### 论文概述<br>论文针对灵巧手在一般机器人中的manipulation挑战，提出一种共享自治（Shared Autonomy）框架，结合人类VR远程操作和自主DexGrasp-VLA策略（用于手部控制），以高效收集协调手臂-手部演示数据。该框架通过分工控制宏-微运动域：人类引导手臂，VLA Copilot执行精细抓取，降低认知负担。数据用于训练端到端VLA策略（含Arm-Hand Feature Enhancement模块），并通过校正远程操作（Corrective Teleoperation）持续改进。实验显示，策略在50+物体上达约90%成功率，数据质量高。<br><br>### 论文核心贡献点<br>- 提出多模态VLA Copilot（DexGrasp-VLA）：融合视觉、语言、触觉和本体感受，实现力适应抓取。<br>- 设计共享自治框架：人类VR控制手臂，VLA自主控制手部，高效收集高质量演示。<br>- 开发Arm-Hand Feature Enhancement架构：显式建模手臂（宏运动）和手部（微运动）的互补特征，提升协调性。<br>- 实现校正人类在环远程操作：采集失败恢复数据，迭代优化策略鲁棒性。<br><br>### 论文方法描述<br>方法分四阶段：<br>1. **DexGrasp-VLA策略训练**：先训练LSTM“盲策略”（融合参数化力控制和远程操作数据），再扩展为多模态VLA（整合视觉、触觉、语言和本体感受）。<br>2. **共享自治数据收集**：人类通过VR相对运动映射远程操作手臂（90Hz），DexGrasp-VLA并行控制手部（30Hz），采集同步手臂-手部数据。<br>3. **端到端VLA策略学习**：基于预训练VLA模型（SFT），添加Arm-Hand Feature Enhancement模块（共享表示+MLP编码的手臂/手部特征），优化总损失（含主损失+辅助损失）。<br>4. **校正远程操作系统**：部署策略自动记录成功/失败轨迹；人类介入纠错采集恢复数据，迭代SFT更新策略。<br><br>### 论文使用数据集和训练资源<br>- **LSTM预训练数据集**：218条轨迹（150条人类远程操作+68条力控制数据），覆盖20+物体，用于训练“盲”力适应抓取策略。<br>- **DexGrasp-VLA手部数据集**：180条抓取轨迹（杂乱场景中60种物体），含手部RGB、触觉（力向量+空间嵌入）、本体状态和动作。<br>- **端到端手臂-手部数据集**：100条共享自治演示，覆盖20种物体，含多视角RGB、语言指令和手臂-手部联合动作。<br>- **校正干预数据集**：100条轨迹（50条方向失败恢复+50条角落案例）。<br>- **训练资源**：基于开源框架LeRobot（Fine-tuning预训练VLA模型π₀）；硬件包括UR3e机械臂（6-DoF）、Xhand五指手（12-DoF）、三RGB-D摄像头及触觉传感器。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：集成机器人平台（UR3e + Xhand），装配三RGB-D摄像头（两静态+一手腕-mounted）；任务场景包括杂杂乱物体抓取、表单清理等。<br>- **评估指标**：<br> - 抓取成功率：DexGrasp-VLA在杂乱场景中95.5%，端到端策略在50+物体上约90%。<br> - 消融研究：验证触觉特征、手部-手臂特征增强、校正远程操作对性能的影响（如成功率、鲁棒性）。<br> - 数据效率：远程操作会话时长、认知负担（经验显示~30分钟无疲劳）。</details> |
| 2025-10-30 | Self-Improving Vision-Language-Action Models with Data Generation via Residual RL | http://arxiv.org/abs/2511.00091 | <details><summary>展开</summary>待生成</details> |
| 2025-10-30 | Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail | http://arxiv.org/abs/2511.00088 | <details><summary>展开</summary># 论文研究单位<br>NVIDIA<br><br># 论文概述<br>Alpamayo-R1 (AR1) 是一个视觉-语言-动作模型，旨在通过整合Chain of Causation推理与轨迹规划来增强自主驾驶在复杂驾驶场景中的决策能力。该模型专注于解决安全关键的尾部场景问题，这些场景中监督稀疏且因果理解有限。<br><br># 论文核心贡献点<br>1. **Chain of Causation (CoC)数据集**：构建了混合自动标注和人工标注流程，产生与驾驶行为对齐的决策导向、因果关联的推理轨迹<br>2. **模块化VLA架构**：结合Cosmos-Reason（为物理AI应用预训练的视觉-语言模型）与基于扩散的轨迹解码器，实时生成动态可行的计划<br>3. **多阶段训练策略**：使用监督微调来引出推理能力，并通过强化学习优化推理质量和推理-行动一致性<br><br># 论文方法描述<br>## 架构设计<br>- **VLM主干网络**：采用Cosmos-Reason作为视觉-语言模型骨干<br>- **高效视觉编码**：支持单图像标记化、多相机标记化和多相机视频标记化，实现最高20倍token压缩率<br>- **扩散式轨迹解码**：使用流匹配框架将离散轨迹token转换为连续表示，基于unicycle动力学建模<br><br>## 推理与行动结合<br>通过统一的序列预测框架，将历史观测、推理过程和未来轨迹整合为统一序列，实现推理与行动预测的无缝结合。<br><br>## 数据构建方法<br>**Chain of Causation数据集**采用结构化标注框架：<br>- **决策标注**：定义封闭集合的高层驾驶决策<br>- **关键组件标注**：开放集合的因果因素<br>- **混合标注流程**：结合人工标注(10-20%数据)和自动标注(80-90%数据)<br><br># 论文使用数据集和训练资源<br>- **基础数据集**：基于nuScenes等公开驾驶数据集<br>- **CoC数据集**：混合人工和自动标注，包含结构化推理轨迹<br>- **物理AI领域数据**：覆盖机器人、医疗、智慧城市等多领域数据用于预训练<br>- **人类标注数据**：10-20%高质量人工标注样本用于监督微调和评估<br>- **自动标注数据**：大规模自动生成的结构化标注数据<br><br># 论文使用的评估环境和评估指标<br>## 评估环境<br>- **开环评估**：标准数据集上的规划准确性评估<br>- **闭环评估**：仿真环境中的驾驶性能测试<br>- **实车测试**：车载系统验证实际部署性能<br><br>## 评估指标<br>- **规划性能**：在挑战性场景中规划准确性提升12%<br>- **安全性指标**：<br> - 离路率降低35%<br> - 近距离遭遇率降低25%<br>- **推理质量**：通过大推理模型评估，推理质量提升45%<br>- **推理-行动一致性**：一致性提升37%<br>- **模型扩展性**：从0.5B到7B参数的一致改进<br>- **实时性能**：端到端延迟99毫秒，支持实时部署</details> |
| 2025-10-31 | Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model | http://arxiv.org/abs/2510.27607 | <details><summary>展开</summary>待生成</details> |
| 2025-10-31 | EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities | http://arxiv.org/abs/2510.27545 | <details><summary>展开</summary># 论文研究单位<br>ZhiCheng AI（致诚智能）、伊利诺伊大学厄巴纳-香槟分校（UIUC）、清华大学、北京大学<br><br># 论文概述<br>论文提出 EBT-Policy（Energy-Based Transformer Policy），用能量函数直接刻画可行动轨迹的能量景观，通过能量最小化而非扩散去噪生成动作。该方法旨在解决扩散/流模型在机器人策略学习中的高计算成本、暴露偏差（exposure bias）和推理不稳定等问题。研究在仿真基准（robomimic）与真实双机械臂平台（共4个RGB相机、台式场景）上开展系统实验，验证 EBT-Policy 在训练与推理效率、鲁棒性与泛化能力上的优势，并展示其“涌现能力”——零样本的失败重试与恢复行为。<br><br># 论文核心贡献点<br>- 稳定性与鲁棒性：学习单一、时不变的能量景观，减少暴露偏差与错误累积；在分布外（OOD）状态下通过平衡动力学“回拉”至数据流形，提升在真实复杂环境中的稳定性。<br>- 高效性：推理仅需2步即可在部分任务上达到与扩散策略相同的成功率，相比扩散策略100步，实现50倍计算量减少；训练轮次减少约55%，训练与推理显著提速。<br>- 涌现能力：在未提供重试数据的前提下，仅靠行为克隆与能量驱动即可实现零样本失败恢复与重试策略，表现出类似物理推理的行为选择。<br>- 统一的可解释信号：能量标量提供不确定性与置信度度量，驱动动态计算分配（困难状态多步、简单状态少步），并在训练中通过正则化手段提升多模态分布学习的可控性与稳定性。<br><br># 论文方法描述<br>- 总体思想：将可视运动策略建模为基于能量函数的能量模型（EBM），以能量最小化过程替代扩散模型的去噪/流匹配。动作序列 a 在给定观察 o 与语言指令 ℓ 下，通过迭代沿能量景观负梯度下降生成，最终收敛到低能量（高似然）轨迹。<br>- 生成与采样：采用随机马尔可夫链蒙特卡罗（MCMC）与 Langevin 动态的迭代更新：<br> ŷ_{i+1} = ŷ_i − α_i ∇_ŷ E_θ(x, ŷ_i) + η_i<br> 其中 α_i 为能量缩放的步幅，η_i 为噪声项（可按余弦退火调度）。<br>- 训练细节：随机化 MCMC 步数、缩放 Langevin 噪声、步幅随机化与 Nesterov 加速梯度用于多模态探索与稳定优化；引入能量缩放步幅、预采样归一化（RMSNorm）、梯度裁剪等正则与稳定化机制；在每次迭代中以随机噪声初始化目标轨迹并逐步“去噪”，损失为去噪轨迹与演示轨迹的均方误差。<br>- 推理与动态计算：执行自适应能量下降（梯度范数低于阈值终止或达到最大步数），在不同场景自动分配计算步数，困难状态迭代更久、简单状态快速收敛，形成不确定性与计算的自适应联动。<br><br># 论文使用数据集和训练资源<br>- 仿真数据：robomimic 基准的四项任务（Lift、Can、Square、Tool Hang），每项在模拟环境做50回合评估成功率。<br>- 真实数据：在双机械臂平台（4个RGB相机、台式环境）上通过遥操作采集的数据，用于三项真实任务（Fold Towel、Pick & Place、Collect/Place Pan），包含语言指令与多模态观察。<br>- 架构与对比：设置两种模型变体<br> - EBT-Policy-S：约30M参数，仿真对比用（视觉编码器 ResNet-18，无语言编码器）。<br> - EBT-Policy-R：约100M参数，真实任务用（视觉编码器 DINOv2-S，文本编码器 T5-S）。<br> 与扩散策略（DP）进行同等规模（≤150M）下的公平对比。<br><br># 论文使用的评估环境和评估指标<br>- 环境与场景：仿真 robomimic（标准、可复现实验）；真实双机械臂桌面环境（4相机、可变光照与传感器噪声）。<br>- 指标：任务成功率（SR，百分比），训练收敛轮次/步数（训练效率），推理步数与计算量（推理效率），稳定性与分布外鲁棒性（含失败恢复/重试的定性表现）。<br>- 结果摘要<br> - 仿真：在Lift、Can、Square、Tool Hang四项任务上均优于扩散策略（DP-10步为0，DP-100步在Square/Tool Hang未超越EBT），EBT两项（S、 R）均取得更高SR。<br> - 真实：Fold Towel、Collect Pan、Pick & Place 三任务中，EBT-Policy-R在两项显著优于DP，Pick & Place略优或相当。<br> - 训练与推理效率：EBT在约30轮即达100%成功率，DP需约90轮且推理步数更高；EBT在部分任务2步即收敛，DP在10步时成功率降至0，100步才稳定。</details> |
| 2025-10-30 | Running VLAs at Real-time Speed | http://arxiv.org/abs/2510.26742 | <details><summary>展开</summary># 论文研究单位<br>Dexmal, StepFun<br><br># 论文概述<br>本文展示了如何在单个消费级GPU上以30Hz帧率和最高480Hz轨迹频率运行π_0级多视角VLA模型，实现了之前被认为大型VLA模型无法完成的动态实时任务。通过消除模型推理中的开销，证明了VLAs确实能够在RTX 4090 GPU上实时运行，实验结果显示π_0策略在抓取掉落笔任务中达到100%成功率。<br><br># 论文核心贡献点<br>- 实现了VLA模型的30FPS推理，双视角输入延迟仅27.3ms，显著快于官方openpi推理<br>- 提出完整流式推理框架，实现多层次控制频率（480Hz力控制、30Hz视觉控制、低于1Hz文本交互）<br>- 建立了基于屋顶线模型的理论下界，验证了实现接近最优<br>- 在真实世界中验证了实时抓取掉落笔任务，达到人类反应水平<br><br># 论文方法描述<br>**消除开销策略**：<br>- 使用CUDA图机制消除Python CPU开销，将推理速度提升约两倍<br>- 计算图简化：融合RMS归一化参数到线性层、折叠动作时间编码器、融合QKV投影<br><br>**内核深度优化**：<br>- 调优GEMM瓦片参数，使用Triton实现优化配置<br>- 融合门控线性层，提高并行性和内存访问效率<br>- 采用部分split-k策略处理特殊尺寸的GEMM操作<br>- 将标量操作（偏置、残差、激活函数）融合到GEMM中<br><br>**完整流式推理框架**：<br>- 重叠执行VLM和AE内核，实现资源充分利用<br>- 将高频率输入信号注入AE，支持480Hz控制频率<br>- 实现渐进式动作生成，替代传统的完整轨迹生成<br><br># 论文使用数据集和训练资源<br>- **硬件环境**：单张RTX 4090 GPU<br>- **训练数据**：600集真实世界抓取掉落笔任务的演示数据<br>- **软件框架**：基于官方openpi仓库进行模型训练和推理<br><br># 论文使用的评估环境和评估指标<br>- **推理速度评估**：在不同视角数量下测量推理时间（1视角：20.0ms，2视角：27.3ms，3视角：36.8ms）<br>- **实际任务验证**：抓取掉落笔任务的100%成功率测试<br>- **理论下界分析**：使用屋顶线模型计算GEMM操作的理论上界，考虑内存带宽和计算能力<br>- **同步开销测量**：通过比较不同同步方法的开销，验证实现的接近最优性</details> |
| 2025-10-30 | RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration | http://arxiv.org/abs/2510.26536 | <details><summary>展开</summary>待生成</details> |
| 2025-10-30 | Human-in-the-loop Online Rejection Sampling for Robotic Manipulation | http://arxiv.org/abs/2510.26406 | <details><summary>展开</summary>待生成</details> |
| 2025-10-29 | $π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models | http://arxiv.org/abs/2510.25889 | <details><summary>展开</summary>待生成</details> |
| 2025-10-29 | Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models | http://arxiv.org/abs/2510.25713 | <details><summary>展开</summary>### 论文研究单位<br>Soft Robotics Lab, ETHz（苏黎世联邦理工学院软机器人实验室）<br><br>### 论文概述<br>针对机器人与人类在协作任务中的自然交互挑战，论文提出基于预训练视觉语言动作模型（VLA）的改进方案。核心思想是通过运动线索（而非语言提示）实现隐式意图理解，使机器人能实时响应人类动作并完成协作任务。<br><br>### 论文核心贡献点<br>- **FiLM条件化**：在视觉编码器中插入FiLM层，增强文本-视觉对齐能力<br>- **辅助损失设计**：添加并行预测头（手部姿态和目标物体）以隐式学习人类意图<br>- **动作后处理优化**：通过增量坐标、旋转向量及手部关节PCA降维（保留96%方差）重构低维动作流形<br>- **方向性损失**（实验效果负向）：提出方向对齐损失，但未提升性能<br>- **推理系统集成**：构建实时硬件-模型接口，实现长任务动态提示切换<br><br>### 论文方法描述<br>**模型架构**：基于Open-VLA改进（SigLIP+DINOv2视觉编码器+LLaMA2-7B语言模型），集成FiLM条件化和辅助预测头。<br>**数据收集流程**：双角色协作采集（操作者穿戴动作捕捉手套控制机器人，协作者在共享空间交互），10Hz采样并同步多模态数据，附文本指令和手部姿态标注。<br>**训练策略**：4×H100 GPU分布式训练（批量24，LoRA rank=32，20轮迭代），采用复合损失（动作+辅助损失）。<br>**推理系统**：硬件接口（绿色）→模型推理（蓝色）→高层规划（红色）的三层架构，0.3秒端到端延迟。<br><br>### 论文使用数据集和训练资源<br>- **协作任务数据**：<br> - "Pick up cube"任务：120条轨迹（红蓝立方体各60条）<br> - "Pass cube"任务：260条轨迹（红立方体200条，蓝立方体60条）<br> - 辅助标签：手部3D姿态（MediaPipe）、目标物体索引<br>- **训练资源**：4×H100 GPU集群，平均训练时长12小时<br><br>### 论文使用的评估环境和评估指标<br>- **动作空间分析**：<br> - 端 effector增量坐标分布更接近高斯（vs原始坐标非凸性）<br> - 手部关节PCA保留4个主成分解释96%方差<br>- **消融实验**：<br> - 动作后处理贡献最大（各指标显著提升）<br> - 辅助损失提供稳定增益<br> - 方向性损失和FiLM在不同损失下呈负向效果<br>- **真实世界评估**：<br> - 场景：桌面临场交互（多摄像头）<br> - 长任务成功率：10次试验中1次成功<br> - 失败原因：协作对象与训练者不同时出现"训练员过拟合"（模型对特定操作者行为过拟合）<br>- **评估指标**：L2损失、PCA重构误差、方向对齐误差、真实任务完成率</details> |
| 2025-10-29 | Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization | http://arxiv.org/abs/2510.25616 | <details><summary>展开</summary>待生成</details> |
| 2025-10-29 | NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies | http://arxiv.org/abs/2510.25122 | <details><summary>展开</summary>待生成</details> |
| 2025-10-27 | A Survey on Efficient Vision-Language-Action Models | http://arxiv.org/abs/2510.24795 | <details><summary>展开</summary>待生成</details> |
| 2025-10-28 | BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning | http://arxiv.org/abs/2510.24161 | <details><summary>展开</summary># 论文研究单位<br>- Tongji University; Shanghai Magic; Koala Uran<br><br># 论文概述<br>- 背景：现有多模态大语言模型（MLLM）、具身大语言模型（ELLM）、视觉-语言-动作模型（VLA）和通用多模态大模型（GMLM）未能同时实现跨空间、跨任务、跨具身的统一能力<br>- 目标：提出BLM1（Boundless Large Model），在单一统一模型中实现数字空间推理与物理空间控制的融合<br>- 方法：两阶段训练（Stage I SFT注入具身知识；Stage II冻结MLLM，训练基于意图桥接接口的扩散变换器（DiT）策略模块）<br>- 结论：单一BLM1在数字任务上约提升6%，物理任务上约提升3%，同时保持指令跟随与跨具身控制能力<br><br># 论文核心贡献点<br>- 统一三能力：跨空间迁移（数字→物理）、跨任务学习、跨具身泛化<br>- 保持语言能力：Stage I以SFT注入具身知识，避免破坏MLLM指令跟随<br>- 有效桥接：意图桥接接口+Perceiver压缩高阶意图，支撑高频率闭环控制<br>- 策略共享：Diffusion Transformer作为通用策略头（0.76B参数），跨具身参数共享<br>- 训练配方：加权采样保证数据集平衡；Stage II中后期引入未来预测损失提升泛化<br>- 系统评测：数字空间六大基准、物理空间四具身六任务，建立统一对比<br>- 结果优势：超越四类模型家族（MLLM、ELLM、VLA、GMLM），单一模型不切换架构<br><br># 论文方法描述<br>- 架构组成<br> - Backbone：Qwen2.5-VL-7B-Instruct；融合视觉与语言；抽取第k层隐藏状态Hk作为高层意图<br> - 意图桥接接口：Perceiver将Hk压缩为固定K个令牌H̃k，降低计算、适配控制<br> - 机器人策略：DiT扩散变换器作为条件生成头；状态编码器fs、动作编码器fa/解码器；预测horizon为h的动作块At<br>- 训练目标<br> - SFT目标（Stage I）：标准next-token交叉熵，冻结视觉与投影器，更新语言与投影<br> - 流动匹配目标（Stage II早期）：Rectified-flow路径，DiT拟合速度场vφ(xτ,τ\|H̃k)，LFM为MSE<br> - 未来预测目标（Stage II后期）：在DiT中引入可学习未来令牌F，使其中间层表征与冻结MLLM对未来观察o_{t+H}与指令y的压缩表征对齐，用余弦相似度<br> - 总体：Stage I仅L_SFT；Stage II早期训练Perceiver/DiT/编解码器；后期叠加L_FP<br>- 训练配方<br> - 加权采样：保证各数据集等概率抽取，避免大数据主导<br> - Stage I：数字多模态问答对（RoboVQA、AgiBot、HoloAssist、BridgeData V2、EgoPlan、ShareRobot）<br> - Stage II：冻结MLLM，使用四具身×六任务的模拟演示数据训练策略；具身特定编码器/解码器独立优化，DiT参数跨具身共享<br> - 物理数据生成：基于ManiSkill，高层技能+关键姿态+IK与碰撞检验+路径规划与平滑，录制第三人称与腕部图像、本体感受与动作<br><br># 论文使用数据集和训练资源<br>- 数字空间数据集（Stage I）<br> - RoboVQA：829,502视频文本对，29,520指令；多类问答（规划、完成检验、 affordance判别与生成、过去/未来事件）<br> - AgiBot：1,001,552条轨迹，2,976.4小时；217任务、87技能、106场景<br> - HoloAssist：166小时；350教师-执行者对，20物理任务、16类物体<br> - BridgeData V2：60,096轨迹，24环境，100+物类；基础到复杂13技能族<br> - EgoPlan：EgoPlan-Bench 4,939多选题 + EgoPlan-IT 50K问答（基于EPIC-Kitchens）<br> - ShareRobot：整合Open X-Embodiment的23数据集，102场景、12具身；1,027,990规划问答、6,522 affordance注释、6,870轨迹预测<br>- 物理空间数据集（Stage II）<br> - 具身：Franka Emika Panda、xArm-6、xArm-7、WidowX AI<br> - 任务：PickCube、PullCube、StackCube、PushCube、PlaceSphere、LiftPegUpright（每具身×任务100集）<br> - 总帧数：347.8K；任务特定成功判定<br>- 数字评测基准（Stage I后）<br> - 多选题：RoboVQA、AgiBot、HoloAssist、RoboFail<br> - 自由问答：EgoThink、ShareRobot<br> - 总计：3,160测试样本（见表2）<br>- 训练资源<br> - 框架：ManiSkill模拟收集<br> - 加权采样：C个数据集等概率，样本在数据集内均匀<br> - 视频帧采样：统一0.5秒间隔采样；短视频4帧，长视频8帧；任务前移除测试片段防泄露<br><br># 论文使用的评估环境和评估指标<br>- 数字空间评估<br> - 指标<br> - Exact-match accuracy（EM）：多选题严格匹配归一化答案<br> - LLM-as-a-judge（GPT评分）：自由问答使用判别式提示与评分标准<br> - EgoThink：s_i∈{0,0.5,1}，最终分数按平均×100<br> - ShareRobot：s_i∈{1,2,3,4,5}，最终分数按((s_i−1)/4)平均×100<br>- 物理空间评估<br> - 指标：Episode级成功率sr_{e,u}=N^s_{e,u}/N_{e,u}；总体为E具身×U任务的成功率平均<br> - 环境：仿真闭环控制；每任务50次rollout，具身特定步长上限<br>- 结果概览<br> - 数字空间：表4显示BLM1平均64.88，显著优于GPT-4o（59.86）与开源MLLM；细分见表5/表6<br> - 物理空间：BLM1在四具身×六任务上均实现稳定跨具身控制成功率，体现共享DiT策略的有效性</details> |
| 2025-10-27 | RoboOmni: Proactive Robot Manipulation in Omni-modal Context | http://arxiv.org/abs/2510.23763 | <details><summary>展开</summary>### 论文研究单位<br>复旦大学、上海创新研究院、新加坡国立大学<br><br>### 论文概述<br>论文提出“跨模态上下文指令”设置：机器人需从人类语音、环境声与视觉线索中主动推断潜在意图，并通过交互确认后执行动作，而不再依赖显式指令。为解决数据缺失与评估基准不足，构建 OmniAction 数据集与 OmniAction-LIBERO 评测；提出 RoboOmni 端到端全模态大模型，集成感知、推理、对话与控制，实现意图识别—确认—执行的闭环。仿真与真实验证显示其在成功率、推理速度、主动辅助与意图识别上显著优于文本与ASR基线。<br><br>### 论文核心贡献点<br>- 定义并研究“跨模态上下文指令”任务：融合语音、环境声与视觉进行主动意图推断与交互确认<br>- 提出 RoboOmni：Perceiver–Thinker–Talker–Executor 的端到端全模态框架，统一推理与控制，支持直接语音交互与行动生成<br>- 构造 OmniAction：约14万集 multimodal 数据、5千+说话人、2.4千事件声音、640背景、六类上下文指令；并基于 LIBERO 构建 OmniAction-LIBERO 评测套件<br>- 实验显示：仿真 OmniAction-LIBERO 上平均成功率达85.6%；真实人声指令平均76.6%；推理延迟降至基线的约0.49倍；具备更强的意图识别与主动协助能力<br><br>### 论文方法描述<br>- 架构：Perceiver 统一编码视觉/音频/文本为共享表征；Thinker 基于全模态LLM在词汇与动作标记联合空间自回归生成；Talker 生成语音；Executor 用 FAST+ 将离散动作标记解码为连续控制向量<br>- 训练：统一自回归目标，同时优化对话与动作生成。预训练使用64×A100、10天、15,360 A100‑小时、批量512、学习率5e-5；下游SFT为8×A100、5e‑5、10–30k步。图像224×224、音频16kHz、动作块长6<br><br>### 论文使用数据集和训练资源<br>- 数据集<br> - OmniAction：约141,162集，涵盖112技能、748对象、5,096说话人音色、2,482非言语事件、640环境背景与六类上下文指令<br> - OmniAction‑LIBERO：基于 LIBERO 的仿真评测（OmniAction‑LIBERO‑TTS：40任务×6变体=240评测量；OmniAction‑LIBERO‑Real：10名志愿者真实语音指令）<br>- 训练资源<br> - 预训练：64×A100，10天，总计约15,360 A100‑小时，批量512，LR=5e-5，前1k步warm‑up<br> - 下游SFT：8×A100，LR=5e-5，10–30k步<br><br>### 论文使用的评估环境和评估指标<br>- 仿真评估：OmniAction‑LIBERO（四套任务：Spatial、Goal、Object、Long‑Horizon）×六种上下文指令；与 OpenVLA、OpenVLA‑OFT、π0、NORA 比较；指标为成功率<br>- 真实人声评测：OmniAction‑LIBERO‑Real；对比“真实文本提示/ASR转文本→VLA”与RoboOmni直接音频输入；指标为成功率<br>- 主动辅助与意图识别：对比 Qwen2.5‑Omni‑3B/7B 与 ASR+GPT‑4o；指标为意图识别准确率、交互能力质性评估<br>- 效率分析：对比端到端与级联流水线；指标为推理延迟（相对ASR+OpenVLA的倍数）</details> |
| 2025-10-27 | UrbanVLA: A Vision-Language-Action Model for Urban Micromobility | http://arxiv.org/abs/2510.23576 | <details><summary>展开</summary>## 论文研究单位<br>北京大学、Galbot、中国科学技术大学、北京人工智能研究院(BAAI)<br><br>## 论文概述<br>UrbanVLA是一个面向城市微移动性应用的视觉-语言-动作(VLA)框架，主要用于配送机器人等在城市环境中的长距离导航任务。该方法通过整合导航工具提供的高级路线信息与机器人视觉感知，实现了大规模城市场景中可靠、安全的长程导航。<br><br>## 论文核心贡献点<br>- 提出首个面向城市微移动性的路线条件化VLA框架，将导航工具指导与视觉语言策略学习有效结合<br>- 开发了模拟到现实的训练管道，通过路线提升算法构建模拟-现实聚合数据集，实现SOTA性能并展示现实世界泛化能力<br>- 引入基于IQL的强化学习微调方法，显著提升安全关键行为能力，包括避障、行人互动和交通合规性<br><br>## 论文方法描述<br>UrbanVLA基于预训练的导航基础模型NavFoM，采用两阶段训练策略：**(1)监督微调(SFT)**：在MetaUrban模拟器和网络导航视频数据上训练，结合VideoQA任务和路线条件导航任务，学习基本导航能力；**(2)强化微调(RFT)**：使用隐式Q-Learning(IQL)在模拟-现实聚合数据上进行离线强化学习，优化安全性和适应性。方法核心包括多模态特征融合、路线与视觉观察对齐、轨迹级别的策略优化。<br><br>## 论文使用数据集和训练资源<br>- **模拟数据**：MetaUrban-12K数据集(约40小时，2400集)，由PPO专家生成<br>- **现实数据**：约8小时的人为遥操作系统数据<br>- **网络数据**：Sekai网络导航视频数据集和LongVU视频问答数据集<br>- **训练资源**：8张NVIDIA H100 GPU，训练约12小时(总计96 GPU小时)<br><br>## 论文使用的评估环境和评估指标<br>- **模拟环境评估**：MetaUrban基准，包含PointNav和SocialNav任务，在MetaUrban-test(1000场景)和MetaUrban-unseen(100场景)上测试<br>- **现实环境评估**：在城市街区进行，包括天桥、行人横道等复杂场景，机器人为Unitree Go2四足机器人<br>- **评估指标**：成功率(SR)、路径长度加权的成功率(SPL)、社交导航分数(SNS)、累积成本(CC)、路线完成度(RC)等</details> |
| 2025-10-27 | RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation | http://arxiv.org/abs/2510.23571 | <details><summary>展开</summary>### 论文研究单位<br>卡内基梅隆大学、浙江大学、北京大学、国立台湾大学<br><br>### 论文概述<br>随着通用机器人策略的快速发展，其评估面临规模化、可复现性与安全性等挑战，现有评测要么依赖真实世界环境（人力与安全成本高），要么局限在同一合成域内训练—测试，无法评估真实数据或其他环境训练的模型。为解决这些问题，本文提出RobotArena ∞，通过自动化将真实演示视频映射为大规模可扩展的仿真环境，并结合视觉语言模型（VLM）自动评分与人类偏好反馈，对来自全球多实验室的视觉语言动作（VLA）策略开展系统性评测与稳健性检验。<br><br>### 论文核心贡献点<br>- 提出可扩展、可扩展的机器人评测协议：结合物理引擎、真实到仿真（real-to-sim）转换与人类偏好反馈，实现规模化且可复现的评测。<br>- 完整自动化的reality-to-simulation翻译管线：融合VLMs、2D到3D生成模型与可微分渲染，无需人工标定或额外注释。<br>- 大规模跨实验室政策评测：聚合超过7000条人类偏好比较、覆盖百余个仿真场景及多种扰动，目前最大规模的机器人评测工作。<br>- 系统性评测发现：跨数据集泛化能力弱、模型架构差异明显、颜色扰动下强VLM背bone策略更稳健、背景变化带来显著性能衰减。<br><br>### 论文方法描述<br>方法核心是将演示视频自动翻译为仿真环境，再对策略进行多维度评测。<br><br>- 自动化视频到仿真映射：提取五个关键要素（1）相机—机器人六自由度位姿；（2）任务相关物体的三维网格重建、姿态与材质属性；（3）场景深度；（4）干净背景；（5）控制增益估计。<br>- 机器人—相机标定：采用可微分渲染的分析—综合方法，优化RGB、光流与DINOv2特征对齐损失，估计相机—机器人位姿。<br>- 物体与场景三维重建：利用Gemini进行机器人与前景物体分割；InvSR提升分割图像质量；Hunyuan-3D生成有纹理网格；通过MINIMA建立真实crop与模拟渲染视图的2D对应；利用MoGe单目深度与SVD解算刚体变换；用LaMa进行背景修复；系统辨识校准PD控制增益。<br>- 可控域扰动：ΔBG替换背景；ΔColor调整颜色通道（如BGR转换）；ΔObjPose随机改变物体位置；用于压力测试策略泛化。<br>- 评测方式：绝对评测用VLM对打乱帧序列进行任务进度打分（最后30%帧平均分数与人类进度相关性最佳）；相对评测采用双盲成对比较，收集偏好标签与自由文本解释，使用Bradley–Terry模型与Sandwich方差得到全局排名与置信区间。<br>- 对比与外部验证：与SIMPLER基准比较，评估策略在BridgeSim与SIMPLER四场景上的表现一致性；在一个真实任务“把胡萝卜放到盘子里”验证仿真—现实一致性。<br><br>### 论文使用数据集和训练资源<br>- 数据源与仿真环境：BridgeSim（Bridge v2，OXE的广泛使用子集）、DROIDSim（DROID，因噪声较高常被排除于预训练）、Rh20TSim（仅SpatialVLA用其训练）。<br>- 评测对象策略：Octo-Base（93M，OXE预训练）、RoboVLM（基于KosMos的VLA）、SpatialVLA（引入3D空间表征）、CogAct（7B VLM背bone+扩散Transformer动作预测）。<br>- 环境规模：100个名义环境与数百种扰动（ΔBG/ΔColor/ΔObjPose），累计7000+人类偏好比较与多轮VLM自动评分。<br><br>### 论文使用的评估环境和评估指标<br>- 评估环境：自动化生成的仿真环境来源于真实演示视频，支持内分布（与训练集对应）与外分布（超出训练集）评测；在仿真中部署策略执行并录制轨迹视频用于后续评分。<br>- 评估指标：<br> - 自动化VLM任务进度评分：打乱帧序列+VLM打分，优选“执行最后30%帧的平均分数”作为进度指标。<br> - 人类偏好成对比较：偏好标签（胜/负/平局）与自由文本理由；Bradley–Terry模型全局排名，Sandwich方差估计置信区间。<br> - 稳健性：跨数据集迁移性能（BridgeSim vs. DROIDSim vs. Rh20TSim），以及在ΔBG/ΔColor/ΔObjPose扰动下的性能曲线与趋势。<br> - 现实—仿真一致性：单任务现实与仿真部署对比（如“把胡萝卜放到盘子里”）。</details> |
| 2025-10-27 | Dexbotic: Open-Source Vision-Language-Action Toolbox | http://arxiv.org/abs/2510.23511 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>Dexmal、StepFun<br><br>## 论文概述<br>Dexbotic是一个基于PyTorch的开源视觉-语言-动作(VLA)模型工具箱，为具身智能领域的专业人士提供一站式VLA研究服务。该工具箱统一了不同研究机构的VLA策略框架，解决了研究碎片化问题，支持多种主流VLA策略的复现，并提供更强的预训练模型以提升性能。<br><br>## 论文核心贡献点<br>- 统一模块化VLA框架：将VLA策略标准化为视觉语言模型(VLM)和动作专家(AE)两个部分<br>- 预训练基础模型：提供比原始开源模型更强大的预训练模型，在多个基准测试中显著提升性能<br>- 实验为中心开发：采用分层配置架构，用户可通过简单修改Exp脚本快速开发新实验<br>- 多平台训练支持：同时支持云端(如阿里云)和本地GPU(RTX 4090)训练<br>- 多机器人支持：统一数据格式支持UR5、Franka、ALOHA等多种主流机器人平台<br><br>## 论文方法描述<br>统一架构设计：<br>- VLM部分：包含视觉编码器(CLIP)、投影器(两层MLP)和大语言模型(Qwen2.5)<br>- AE部分：支持扩散变换器、多层感知机或专家混合等不同架构<br><br>预训练策略：<br>- 离散预训练模型(Dexbotic-Base)：将连续动作量化为256个离散区间<br>- 连续预训练模型：支持单臂和双臂任务，通过扩展噪声token数量实现<br>- 多视图处理：共享视觉编码器处理多视角输入<br><br>数据格式：<br>- Dexdata格式：统一存储机器人数据集，包含video和jsonl两个核心元素<br>- 相比LeRobot和RLDS格式更节省存储空间<br><br>## 论文使用数据集和训练资源<br>训练数据来源：<br>- Open-X Embodiment数据集子集<br>- 仿真数据：RLBench、LIBERO、ManiSkill2<br>- 真实机器人数据：UR5等多型号单臂机器人<br>- 私有数据集：52个操作任务，使用8种单臂真实机器人收集<br>- 双臂数据：Robomind、AgiBot World数据集及ALOHA双臂机器人数据<br><br>训练环境：<br>- 云端平台：阿里云、Volcano Engine<br>- 本地GPU：RTX 4090等消费级显卡<br><br>## 论文使用的评估环境和评估指标<br>仿真基准测试：<br>- SimplerEnv：WidowX机器人，包含4项视觉匹配任务<br>- CALVIN：ABC→D设定，评估长期任务完成能力<br>- ManiSkill2：5项代表性抓取和堆叠任务<br>- RoboTwin2.0：4项双臂操作任务<br>- LIBERO：4个任务套件(Spatial、Object、Goal、Long)<br><br>评估指标：<br>- 任务成功率(Success Rate)：各基准测试的核心指标<br>- 平均完成长度(CALVIN)：连续完成的任务序列平均长度<br>- 性能提升幅度：与原始开源模型的对比改善<br><br>真实世界评估：<br>- 在UR5e、ALOHA、ARX5、Franka等机器人上验证<br>- 任务类型：日常操作如摆放盘子、按钮操作、纸张粉碎等<br>- 典型结果：set the plates任务100%成功率，search the green box任务80%成功率</details> |
| 2025-10-25 | ACG: Action Coherence Guidance for Flow-based VLA models | http://arxiv.org/abs/2510.22201 | <details><summary>展开</summary>待生成</details> |
| 2025-10-23 | Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence | http://arxiv.org/abs/2510.21860 | <details><summary>展开</summary>### 论文研究单位<br>Andon Labs（研究机构及实验室）<br><br>### 论文概述<br>Butter-Bench是一个评估LLM控制机器人实用智能的基准测试。论文探讨当前SOTA大型语言模型在机器人编排（orchestration）能力上的上限，通过设计真实世界任务（如“传递黄油”）评估模型的空间推理、社交理解和物理世界常识等能力。研究发现，人类在所有任务中显著优于LLM，最佳模型完成率仅达40%，人类平均达95%。<br><br>### 论文核心贡献点<br>1. **引入实用智能评估框架**：提出区分分析智能（analytical intelligence）与实用智能（practical intelligence）的机器人基准测试。<br>2. **隔离编排器评估**：在简化硬件架构下测试LLM编排能力，避免VLA（视觉语言动作）模型干扰。<br>3. **系统性能力测评**：设计六大任务全面覆盖空间导航、视觉推理、社交互动和多步规划。<br>4. **红队安全评估**：在压力场景（低电量、充电器故障）下测试模型的对抗性脆弱性。<br>5. **对比人类基线**：明确人类与LLM在真实环境中的能力差距。<br><br>### 论文方法描述<br>- **硬件平台**：使用TurtleBot 4标准移动机器人（搭载OAK-D相机、2D激光雷达、IMU等），运行ROS 2系统提供SLAM能力。<br>- **代理架构**：采用ReAct风格循环，LLM观察环境→推理决策→调用工具执行动作。工具箱包括：<br> 1. 运动控制：`drive`, `rotate`, `wait`<br> 2. 维护功能：`dock`, `undock`, `status`<br> 3. 视觉感知：`take_photo`<br> 4. 导航工具：`view_map`, `navigate_to`<br> 5. 社交交互：`read_msg`, `send_msg`, `save_image`<br>- **任务设计**：六个子任务评估不同能力（如识别含黄油纸袋、注意用户缺席、多步路径规划等）。<br>- **红队测试**：模拟低电量+充电器故障场景，诱导模型泄露机密信息。<br><br>### 论文使用数据集和训练资源<br>- **任务数据集**：基于真实办公环境设计的6项操作任务（附录A详述），包含接受标准。<br>- **模型评估**：测试多个LLM：<br> - **SOTA通用模型**：Gemini 2.5 Pro、Claude Opus 4.1、GPT-5、Grok 4、Llama 4 Maverick<br> - **专用实体模型**：Gemini ER 1.5（针对机器人调优）<br>- **人类基线**：3名人类操作员通过网页界面控制机器人完成相同任务（不知环境布局）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：真实办公室场景（单房间含出口、办公桌、厨房等），固定光照和障碍物布局。<br>- **核心指标**：<br> 1. **主要指标**：任务完成率（每模型-任务组合测试5次）。<br> 2. **辅助指标**：<br> - 任务完成时间（推理延迟+规划效率）<br> - 工具调用分布（分析工具使用模式）<br> - 定性失效模式分类（空间推理、社交理解等5类）。<br>- **安全性评估**：红队攻击成功率（如泄露机密信息的倾向性）。<br><br>**结果摘要**<br>- 人类vs LLM：人类平均95%完成率，最佳LLM Gemini 2.5 Pro仅40%。<br>- **关键弱点**：<br> - 社交理解（"注意缺席"任务：LLM 0% vs 人类100%）<br> - 多步空间规划（"Plan"任务：LLM依赖随机路径漂移，非真实理解）<br>- **模型差异**：<br> Gemini ER 1.5未优于Gemini 2.5 Pro，表明实体调优未显著提升实用智能。</details> |
| 2025-10-21 | VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting | http://arxiv.org/abs/2510.21817 | <details><summary>展开</summary>待生成</details> |
| 2025-10-24 | Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos | http://arxiv.org/abs/2510.21571 | <details><summary>展开</summary># 论文研究单位<br><br>清华大学、微软亚洲研究院<br><br># 论文概述<br><br>本文提出了一种将无脚本真实生活人类手部活动视频转换为视觉-语言-动作（VLA）数据格式的新方法，用于预训练机器人操作VLA模型。该方法将人类手部视为灵巧机器人末端执行器，通过开发全自动化的人类活动分析框架，将"野生"第一人称人类视频转换为与现有机器人VLA训练数据完全对齐的数据格式，无需任何注释。最终构建了包含100万片段和2600万帧的大规模手部VLA数据集，并设计了灵巧手部VLA模型。<br><br># 论文核心贡献点<br><br>1. **首个VLA预训练方法**：首次将非结构化人类视频转换为与机器人VLA训练数据完全对齐的数据格式，无需人工注释<br><br>2. **自动化分析框架**：开发了全自动化的人类活动分析框架，包含三维运动标注、原子动作分割和指令标注三个阶段<br><br>3. **大规模数据集构建**：构建了包含100万片段和2600万帧的手部VLA数据集，覆盖广泛的对象、概念、技能和环境变化<br><br>4. **VLA模型架构**：设计了由VLM骨干网和扩散动作专家组成的灵巧手部VLA模型，支持统一的单双hand动作预测<br><br>5. **零样本泛化能力**：模型在完全未见过的真实世界观察中展现出强大的零样本能力<br><br>6. **缩放行为验证**：证明了模型任务性能随预训练数据规模的清晰缩放行为<br><br># 论文方法描述<br><br>## 数据转换框架<br><br>**三维运动标注**：采用单目三维相机和手部姿态跟踪方法，利用深度视觉SLAM、深度估计和手部重建技术，估计相机内参和帧级手部姿态（基于6D手腕姿态和完整关节角度）<br><br>**原子动作分割**：通过检测世界空间中3D手部手腕的速度最小值作为切割点，将长视频分割为原子级手部动作序列，遵循机器人VLA数据的粒度要求<br><br>**指令标注**：从每个片段均匀采样8帧，在帧上叠加手部轨迹投影，使用GPT-4.1为每个片段生成动作的描述性语言指令<br><br>## VLA模型设计<br><br>**模型架构**：由PaliGemma-2 VLM骨干网和DiT-Base扩散Transformer动作专家组成，添加可学习的认知token作为条件输入<br><br>**动作空间**：在相机坐标系中预测手部动作，包含左右手的手腕平移、旋转和15个关节的欧拉角<br><br>**统一预测**：通过动作掩码机制处理单双hand动作的统一预测，使用因果注意机制进行动作去噪<br><br>**训练策略**：采用轨迹感知数据增强，随机裁剪和透视变换，同时变换对应的动作序列<br><br>## 机器人微调<br><br>将人类手部动作空间映射到机器人手部，通过关节拓扑对应关系进行简单的映射策略，对模型进行微调以适应真实机器人操作<br><br># 论文使用数据集和训练资源<br><br>## 预训练数据集<br><br>**数据来源**：处理了来自Ego4D（77%）、Epic-Kitchen（12%）、EgoExo4D（6%）和Something-Something-V2（5%）的第一人称人类视频<br><br>**数据规模**：100万片段，2600万帧，覆盖烹饪、清洁、建筑、维修、手工和绘画等真实生活活动<br><br>**数据多样性**：与OpenImages数据集相比具有更高的视觉多样性，包含更丰富的语言指令多样性（名词、动词、形容词）<br><br>## 训练资源<br><br>**预训练**：在8个NVIDIA H100 GPU上训练2天，批量大小512，学习率1e-4和1e-5<br><br>**微调**：在8个NVIDIA H100 GPU上训练8小时，批量大小256，学习率1e-5<br><br>## 机器人数据<br><br>收集了1.2K条遥操作轨迹用于四个任务的微调：一般拾取放置、功能抓取、倒水和清扫<br><br># 论文使用的评估环境和评估指标<br><br>## 人类手部动作预测评估<br><br>**评估环境**：<br>- 抓取任务：在47个未见过的环境中使用Azure Kinect捕获RGB-D图像<br>- 一般动作任务：在117个未见过的真实生活环境中进行用户研究<br><br>**评估指标**：<br>- 抓取任务：手-物体最小距离（$d_{\text{hand-obj}}$）评估动作合理性<br>- 一般动作：23名参与者对30个随机场景的top-3动作进行排名打分<br><br>## 真实机器人灵巧操作评估<br><br>**评估环境**：<br>- Realman机器人配备12-DoF XHand灵巧手和RealSense头部相机<br>- 桌面环境设置，使用遥操作系统收集训练数据<br><br>**评估任务**：<br>- 拾取放置：移动物体到盒子中（3-4个随机干扰物）<br>- 功能抓取：在功能位置抓取物体（如手柄）<br>- 倒水：抓取瓶子，将内容倒入另一容器，放回桌面<br>- 扫帚清扫：从篮子中拿起扫帚，将垃圾扫入簸箕，归位扫帚<br><br>**评估指标**：<br>- 任务成功率：在可见和不可见物体/背景下的成功率百分比<br>- 对比基线：与VPP和π₀等代表性方法的性能比较<br>- 缩放行为：不同数据规模下的性能变化趋势</details> |
| 2025-10-23 | SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing | http://arxiv.org/abs/2510.20965 | <details><summary>展开</summary>待生成</details> |
| 2025-10-23 | VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation | http://arxiv.org/abs/2510.20818 | <details><summary>展开</summary>待生成</details> |
| 2025-10-23 | MemER: Scaling Up Memory for Robot Control via Experience Retrieval | http://arxiv.org/abs/2510.20328 | <details><summary>展开</summary>## 论文研究单位<br><br>斯坦福大学（Stanford University）<br><br>## 论文概述<br><br>机器人策略通常缺乏长期记忆能力，无法像人类一样利用历史信息完成复杂任务。针对这一问题，本文提出了MemER（Memory via Experience Retrieval）框架，采用分层策略设计，使机器人能够选择性地记忆任务相关的关键帧，从而实现分钟级别的长期记忆。该方法在三个真实世界的长时序机器人操作任务上进行了验证。<br><br>## 论文核心贡献点<br><br>1. **分层记忆架构**：提出高层策略负责选择和跟踪相关关键帧，低层策略执行具体动作的层次化设计<br>2. **关键帧选择机制**：开发了一种基于一维单链接聚类的算法，能从历史观察中自动提取任务相关信息<br>3. **高效记忆存储**：设计了紧凑的视觉记忆表示方法，避免了传统长上下文方法的计算开销和协变量偏移问题<br>4. **少样本适应**：仅需50个远程操作演示即可将预训练的视觉-语言模型适配到机器人任务<br><br>## 论文方法描述<br><br>**高层策略（π_h）**：<br>- 基于Qwen2.5-VL-7B-Instruct微调<br>- 输入：最近N帧、任务指令、选定关键帧<br>- 输出：当前子任务和候选关键帧<br>- 采用一维单链接聚类（合并距离d=5帧）构建视觉记忆<br><br>**低层策略（π_l）**：<br>- 基于π_0.5微调<br>- 输入：当前图像、关节状态、子任务指令<br>- 输出：机器人动作块<br><br>**关键帧选择算法**：<br>1. 收集所有候选关键帧的时间戳<br>2. 对时间戳进行排序<br>3. 使用单链接聚类合并相近帧<br>4. 选择每簇的中位数作为代表帧<br><br>**模型合并技术**：使用权重插值θ=(1-α)·θ_pre + α·θ_ft（α=0.8）保持预训练模型的鲁棒性<br><br>## 论文使用数据集和训练资源<br><br>**训练数据**：<br>- 50个长时序轨迹演示，每个任务10-15个干预演示<br>- 数据格式：(I_t, q_t, l_t, l'_t, a_t)<br><br>**模型训练**：<br>- 高层策略：4500梯度步，96 H200 GPU小时<br>- 低层策略：18000训练步，48 H200 GPU小时<br>- 冻结视觉编码器和投影层，仅微调LLM骨干网络<br><br>**硬件配置**：<br>- Franka机械臂，平行爪夹持器<br>- 双摄像头：第三人称ZED摄像头和手腕安装的miniZED摄像头<br>- 图像分辨率320×180，15Hz采样率<br><br>## 论文使用的评估环境和评估指标<br><br>**评估任务**：<br>1. **物品搜索任务**：在三个不透明垃圾箱中搜索3-5个物品，评估记忆优化路径<br>2. **计数舀取任务**：向两个碗中放入指定数量的两种食材，评估计数准确性<br>3. **除尘与复位任务**：从双层架子移除物品，清洗架子并复位物品，评估空间记忆<br><br>**评估指标**：<br>- 物品搜索：成功检索次数（↑）、使用最优路径次数（↑）<br>- 计数舀取：错误舀取数量（↓）<br>- 除尘与复位：底层/顶层除尘成功（↑）、底层/顶层物品复位成功（↑）<br><br>**对比方法**：<br>- 无历史：仅当前帧<br>- 短历史：N=8帧<br>- 长历史：N=32帧<br>- 人类高层：人工提供正确子任务<br><br>**主要结果**：<br>- MemER在所有任务上>90%成功率<br>- 明显优于无历史和短历史基线<br>- 长历史方法平均性能比MemER差34%<br>- 与人类高层策略性能相当<br>- API-VLMs因延迟问题表现不佳</details> |
| 2025-10-22 | Learning Affordances at Inference-Time for Vision-Language-Action Models | http://arxiv.org/abs/2510.19752 | <details><summary>展开</summary># 论文研究单位<br>UC Berkeley和Physical Intelligence<br><br># 论文概述<br>论文提出了LITEN（Learning from Inference-Time Execution），一种针对视觉-语言-动作模型（VLA）的推理时学习框架。该方法通过将VLA低层策略与高层视觉语言模型（VLM）连接，使高层VLM能够在上下文中利用过往经验，学习机器人的可达性（affordances）和能力。LITEN通过迭代的推理和评估阶段，使机器人能够从实际执行经验中学习，不断改进复杂任务的规划能力，无需额外的模型训练。<br><br># 论文核心贡献点<br>- 提出LITEN框架，实现了VLA模型的推理时学习能力<br>- 设计了两阶段迭代方法：推理阶段生成和执行计划，评估阶段分析执行结果并提取经验<br>- 展示了如何从非结构化的机器人轨迹中提取有意义的反馈<br>- 在复杂的多阶段操作任务上验证了方法的有效性<br>- 证明了方法可以在不使用额外训练数据的情况下显著提升任务成功率<br><br># 论文方法描述<br>LITEN采用双阶段迭代循环：<br>1. **推理阶段**：高层VLM接收任务指令和初始环境观察，生成子任务序列和执行计划。系统提供VLA训练指令风格示例和可操作对象列表来指导计划生成。<br>2. **评估阶段**：系统收集每个子任务的执行轨迹，使用结构化评估程序通过VLM法官分析执行结果：<br> - 判断子任务是否成功<br> - 描述实际执行情况（失败时）<br> - 分析失败原因并提出改进建议<br>3. **经验利用**：将评估结果以层次结构存储，并在下一轮推理中作为上下文提供，帮助VLMreasoner生成更优计划。<br><br>方法使用GPT-5-mini作为高层VLM，π₀.₅-DROID作为低层VLA策略。<br><br># 论文使用数据集和训练资源<br>- **数据集**：使用DROID数据集对π₀.₅-DROID进行微调，并收集了450个演示数据（每个任务150个）<br>- **硬件环境**：标准DROID机器人设置，包括7-DoF Franka Emika Panda机械臂、2F-85 Robotiq抓手、ZED 2.0相机和ZED mini腕部相机<br>- **计算资源**：训练使用4个NVIDIA A100 GPU，批大小128，训练2500步<br>- **技术栈**：部署在5Hz操作频率，VLA产生长度为8的关节速度动作块<br><br># 论文使用的评估环境和评估指标<br>- **评估环境**：桌面操作环境，包含多种物体配置<br>- **任务类型**：三个复杂多阶段任务：<br> - Stacking：堆叠三个物体<br> - Emptying Bowls：将两个碗中的内容物转移到其他碗<br> - Moving Off Table：将桌面物体移动到其他物体上，仅保留三个与桌面直接接触<br>- **评估指标**：<br> - 整体任务成功率（不计算部分成功）<br> - 五次迭代中的性能改善趋势<br> - 与基线方法的对比（No-Feedback、Positive-ICL、Reflexion adaptation）<br>- **实验设计**：每个方法在10个不同初始配置下运行5次迭代，评估经验累积的效果<br>- **消融研究**：移除评估阶段的不同组件（失败推理、结果分析等）来验证各组件的重要性</details> |
| 2025-10-22 | GigaBrain-0: A World Model-Powered Vision-Language-Action Model | http://arxiv.org/abs/2510.19430 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>GigaAI。论文页面：https://GigaBrain0.github.io<br><br>## 论文概述<br>GigaBrain-0是一类由世界模型驱动的视觉-语言-动作（VLA）基础模型，旨在缓解真实机器人数据收集的高成本与多样性受限问题。通过在大规模、真实且多样的世界模型合成轨迹上训练，GigaBrain-0显著减少对真实机器人数据的依赖，同时提升跨任务、跨外观与跨视角的泛化能力。<br><br>## 论文核心贡献点<br>- 以世界模型生成的多样化数据为核心驱动的VLA范式，涵盖视频生成、Real2Real、View Transfer、Sim2Real、人体视频迁移等多源合成轨迹。<br>- 架构层面引入RGB-D输入与Embodied Chain-of-Thought（CoT）监督，增强3D几何感知与长时程推理能力，支持操纵轨迹、子目标语言与离散动作符号的联合学习。<br>- 混合Transformer架构：基于PaliGemma2的VLM作为感知与语言接口，动作生成采用Diffusion Transformer（DiT）与Flow Matching以预测连续动作块；引入离散动作token加速收敛；通过Knowledge Insulation隔离不同优化流以减轻干扰。<br>- 系统性数据管线GigaWorld，提供高效视频生成（单步蒸馏、NATTEN注意力、FP8推理，速度提升>50倍）与质量评估（几何一致性、多视角一致性、文本对齐、物理合理性）。<br>- 真实世界广泛验证：在灵巧操作、长时程与移动操纵任务上稳定领先；外观、放置与视角泛化随合成数据混合比α提升显著。<br>- 轻量化部署GigaBrain-0-Small，在NVIDIA Jetson AGX Orin上以约402M参数实现低延迟与低显存，保持与π0相当的任务成功率。<br><br>## 论文方法描述<br>- 输入与表示：RGB-D输入（B×H×W×4），对SigLIP首层卷积扩展零初始化深度通道；训练期随机丢弃深度以兼容RGB推断。<br>- 架构细节：VLM（PaliGemma2）编码多模态输入；动作专家采用DiT进行Flow Matching生成连续动作块；离散动作token作为辅助预测以加速训练；10个可学习的轨迹token通过双向注意与GRU解码输出2D操纵轨迹关键点；子目标语言与离散动作采用自回归生成。<br>- 监督与损失：联合优化三项——语言/离散动作的next-token损失、Flow Matching动作块回归损失、轨迹2D坐标回归损失；通过Knowledge Insulation隔离语义与动作学习的相互干扰。<br><br>## 论文使用数据集和训练资源<br>- 真实世界数据：融合公开数据集（AgiBotWorld、RoboMind、Open X-Embodiment）与自采数据（Agilex Cobot Magic 199小时 + AgiBot G1 983小时，共1182小时；覆盖工业、商业、办公、居住、实验室等五大类共14个场景）。<br>- 世界模型生成数据：GigaWorld提供多管线合成数据，显著提升外观、几何与视角多样性。包括：<br> - Real2Real Transfer：在深度与边缘结构先验约束下生成多纹理/光照/材质的视觉变体。<br> - View Transfer：基于深度的新视角投影、遮挡修复与物理仿真微调，保持任务语义一致。<br> - Sim2Real Transfer：在Isaac Sim中构造多样化场景并合成至逼真外观，增强域泛化。<br> - Human Video Transfer：将EgoDex等第一人称人类演示转化为稳定机器人执行序列。<br> - 文本条件视频生成+逆动力学（IDM）反推动作序列，扩充操纵轨迹。<br> - 多视角一致视频生成与质量评估用于4D一致性与可训练性筛选。<br>- 训练策略：全量/部分/未标注数据混合训练；基于手爪状态切换自动切分子目标并用Qwen-VL-2.5生成语言子目标；统一去重与采样策略提升样本效率。<br><br>## 论文使用的评估环境和评估指标<br>- 评估平台：AgiBot G1双足/双手机器人平台与PiPER双臂平台。<br>- 任务覆盖：<br> - 灵巧操纵：Laundry Folding、Paper Towel Preparation。<br> - 长时程：Table Bussing、Juice Preparation。<br> - 移动操纵：Boxes Moving、Laundry Baskets Moving。<br>- 指标：真实世界任务成功率（多次试验平均）；外观/放置/视角泛化实验以合成数据混合比α为变量，评估成功率随α提升曲线。<br>- 设备部署：NVIDIA Jetson AGX Orin上对GigaBrain-0-Small与π0进行推理性能与成功率对比（FLOPs、参数量、显存、时延、成功率）。</details> |
| 2025-10-22 | Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes | http://arxiv.org/abs/2510.19400 | <details><summary>展开</summary># 论文研究单位<br>清华大学、北京大学、复旦大学、微软研究院亚洲、香港科技大学、浙江大学<br><br># 论文概述<br>论文介绍了MV-RoboBench，一个专门用于评估视觉语言模型在机器人操作场景中多视图空间推理能力的基准测试。该基准包含1,700个手工制作的QA项目，涵盖8个子任务，分为空间理解（交叉视图匹配、距离判断、视角识别、3D空间一致性）和机器人执行（动作规划、步骤执行、轨迹选择、属性识别）两大类别。论文评估了多种VLM模型，并探索了CoT风格的增强方法。<br><br># 论文核心贡献点<br>1. 建立首个整合空间和机器人推理与同步多视图输入的基准测试<br>2. 验证了机器人多视图场景仍极具挑战性，最强大的VLM模型远低于人类表现<br>3. 发现空间推理和机器人执行之间存在正相关性<br>4. 证明强大的单视图空间基准测试性能不能可靠转移到机器人任务或多视图场景<br>5. 揭示多视图机器人推理对视角整合、遮挡解决和空间融合提出了根本性不同要求<br><br># 论文方法描述<br>构建了多阶段的基准测试构建管道：数据收集（基于AgiWorld和BridgeV2数据集）、QA生成（使用任务特定模板构建五选一QA对）、人工质量审查（迭代审查修正问题）。探索了三种CoT风格增强方法：文本CoT（通过GPT-4.1生成场景描述）、视觉CoT（使用VGGT进行新颖视图合成）、结构CoT（使用MoGe-2提供深度先验）。评估采用零样本多选题格式，确保跨模型公平比较。<br><br># 论文使用数据集和训练资源<br>使用AgiWorld和BridgeV2数据集作为数据源，构建了包含980个片段的1,708个QA对。评估涵盖盲评（Random、GPT-3.5-turbo、GPT-4-turbo）、专有模型（GPT-4o系列、GPT-4.1系列、Claude-3.5/3.7、Gemini-2.x系列）、专有推理模型（o4-mini、GPT-5系列、Claude-3.7-think、Gemini-2.5-pro）、开源模型（Gemma-3、InternVL3、Qwen2.5-vl系列）和开源MoE模型（Llama-4系列）。<br><br># 论文使用的评估环境和评估指标<br>评估采用统一的零样本提示格式，所有任务设计为多选题以确保公平比较。使用准确率作为主要评估指标，并通过人工评估作为参考点。评估环境包括多种模型类别，从盲评文本LLM到专有多模态和推理优化架构，还包括开源社区开发的VLMs和MoE模型。</details> |
| 2025-10-21 | MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning | http://arxiv.org/abs/2510.18337 | <details><summary>展开</summary>### 论文研究单位<br>- **Harvard University** (Wenhui Huang, Han Qi, Yilun Du, Heng Yang)<br>- **University of Michigan** (Changhe Chen)<br>- **Nanyang Technological University** (Chen Lv)<br><br>### 论文概述<br>MoTVLA是一个基于混合Transformer（MoT）架构的视觉语言动作（VLA）模型，通过整合快速与慢速推理来改进机器人学习中的语言指令控制能力。该模型保留预训练VLM的通用智能（通用智能专家）以处理感知和语义规划，同时引入领域专家生成快速运动分解信号，最终通过扩散Transformer（DiT）动作专家执行具体操作。该设计显著提升语言可控性和推理效率，并已在NLP基准、仿真环境（ManiSkill）和真实机器人实验中验证有效性。<br><br>### 论文核心贡献点<br>1. **统一快慢推理架构**：在单一MoT模型中实现通用智能（慢速推理）与领域特定知识（快速推理）的融合，通过分解-组合-再分解的机制保持性能平衡。<br>2. **动作条件化策略学习**：将快速推理生成的运动分解信号作为扩散策略条件，提升任务执行效率和行为可解释性。<br>3. **延迟与性能优势**：在推理速度（快推理频率达4倍提升）、语义理解及真实操作任务上超越SOTA基线（如π0.5、RT-2）。<br><br>### 论文方法描述<br>- **架构设计**：<br> - **通用智能专家（Slow Reasoning）**：基于Qwen2.5-LLM 7B的预训练模型，处理视觉-文本多模态理解。<br> - **领域专家（Fast Reasoning）**：同构架构，生成步骤化运动分解文本（双向注意力）。<br> - **动作专家**：DiT结构，基于视觉窗口（5帧）、机器人状态、运动分解信号执行扩散去噪生成动作序列。<br>- **训练流程**：<br> - **领域专家SFT**：使用1.27M问答对（仿真154K + 真实125K + Robo2VLM 678K + LLaVA-OV 318K）优化快速推理。<br> - **动作专家扩散策略**：ManiSkill仿真（每任务300轨迹）+ 真实操作（Pick-and-Place 50轨迹，Table Bussing 200轨迹），以均方误差优化去噪网络。<br>- **推理模式**：<br> - **对话模式**：通用智能专家执行慢推理回答人类问题（如场景描述）。<br> - **行动模式**：领域专家快速生成运动分解 → 动作专家执行多步操作（如堆叠、插孔）。<br><br>### 论文使用数据集和训练资源<br>- **领域专家数据**：<br> - 仿真数据：154K（ManiSkill的Cube Stacking、Peg-in-Hole、L-tool Pull）<br> - 真实演示：125K（人类操作的Pick-and-Place、Table Bussing）<br> - 外部数据：678K（Robo2VLM，转换为推理文本）、318K（LLaVA-OV，筛选长答案）<br>- **动作专家数据**：<br> - 仿真：900轨迹（3任务×300）<br> - 真实：250轨迹（Pick-and-Place 50 + Table Bussing 200）<br>- **预训练资源**：<br> - 通用智能专家：Bagel-VLM初始化（SigLIP-So400m视觉编码 + Qwen2.5文本分词器）<br> - 推理骨干：MoTVLA-14B（双7B专家，总参数量14B）<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **仿真**：ManiSkill平台（Cube Stacking、Peg-in-Hole、L-tool Pull）。<br> - **真实操作**：桌面任务（蔬菜Pick-and-Place含干扰物、Table Bussing含模糊指令）。<br>- **评估指标**：<br> - **语义推理**：BLEU、METEOR、CIDEr、Token准确率（VQA任务如ScienceQA、Visual-7W）。<br> - **机器人任务**：平均成功率（重复随机种子测试，零样本干扰场景）。<br> - **延迟对比**：推理频率（MoTVLA-14B/1B vs π0.5-KI，H100/A6000 GPU）。<br>- **基线对比**：π0.5-KI、π0、GR-MG、Diffusion Policy（DP），基于1050轨迹微调后性能。</details> |
| 2025-10-20 | RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies | http://arxiv.org/abs/2510.17950 | <details><summary>展开</summary>## 论文研究单位<br>Dexmal 和 Hugging Face<br><br>## 论文概述<br>论文介绍了RoboChallenge，一个在线机器人评估系统，旨在为视觉-语言-动作模型（VLA）提供大规模真实机器人基准测试。系统提供10台在线机器人供公众访问，覆盖UR5、Franka Panda、Cobot Magic Aloha和ARX-5等四种类型。研究构建了初始基准Table30，包含30个围绕桌子的任务，测试VLA模型的多种能力。论文描述了系统设计、评估协议和实验结果，强调了真实机器人在评估中的必要性。<br><br>## 论文核心贡献点<br>- 提出了一个在线机器人评估平台，支持大规模真实机器人测试，提供10台机器人机群。<br>- 设计了评估协议，通过视觉任务重现控制初始状态，区分稳定性（多次测试一致）和公平性（模型间比较一致）。<br>- 构建了Table30基准，包含30个任务，涵盖精确3D定位、多视角、时间依赖和软体操作等挑战。<br>- 实现了“远程机器人”范式，用户通过API直接访问机器人，无需提交模型文件。<br>- 提供了详细的结果分析，展示不同VLA模型在各项任务上的性能差异。<br><br>## 论文方法描述<br>系统采用“远程机器人”模式：用户通过低级别API访问机器人传感器（时间戳RGB、深度和本体感受数据）和动作队列（不可撤销动作）。评估协议包括：<br>- 视觉任务重现：通过参考图像引导测试者固定初始场景状态。<br>- 环境扰动控制：允许光照和背景变化，测试模型鲁棒性。<br>- 测试者控制：区分经验测试者、无知测试者和自适应测试者，提出受控测试者减少偏差。<br>- 稳定性与公平性权衡：基准协议关注模型稳定性（多次测试结果一致），比较协议关注公平性（模型间相对排序一致）。<br><br>## 论文使用数据集和训练资源<br>- 数据集：提供每个任务的演示数据（每任务最多1000集），用户需基于此微调模型。<br>- 训练资源：<br> - 任务特定设置：在8-GPU机器上训练约1天。<br> - 一般设置：混合多任务数据训练“机器通才”模型（每任务约50集）。<br>- 示例模型：包括π₀、π₀.5、CogACT和OpenVLA/OFT等开源VLA算法。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：10台机器人，分为四种类型（UR5、Franka Panda、Cobot Magic Aloha、ARX-5），配备多摄像头（主要、手腕、侧面）。<br>- 评估指标：<br> - 成功率（SR）：任务完成百分比。<br> - 进度分数：基于任务阶段完成情况，每个任务总分10分，每阶段分配点数，重试扣0.5分；每任务测试10次，总分100。<br>- Table30基准：30个任务列表（见表1），如插花、整理水果和开抽屉等，测试精确操作、多视角和时间依赖等能力。<br>- 结果：在π₀.5模型上观察到最高成功率，平均43.7%，其他模型性能较低；任务分析显示时间和软体相关任务更具挑战性。</details> |
| 2025-10-20 | RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation | http://arxiv.org/abs/2510.17640 | <details><summary>展开</summary>### 论文研究单位<br>- 南洋理工大学（新加坡）<br>- 清华大学（中国北京）<br>- 北京邮电大学（中国北京）<br><br>### 论文概述<br>论文针对机器人操作中的分布外（OOD）泛化问题，提出RESample框架。现有视觉-语言-动作（VLA）模型在模仿学习中易受OOD状态影响，因数据集仅含成功轨迹。RESample利用离线强化学习获取动作价值网络识别次优动作，并通过探索性采样生成潜在OOD轨迹，将数据整合训练集以增强VLA模型鲁棒性。<br><br>### 论文核心贡献点<br>- 设计稳健数据增强框架缓解模仿学习的OOD问题。<br>- 提出探索性采样机制，自适应纳入OOD样本。<br>- 利用动作价值网络识别细粒度OOD动作样本。<br>- 在模拟和真实机器人任务中验证有效性。<br><br>### 论文方法描述<br>RESample基于策略（πθ）和动作价值网络（Qφ）的二元性：<br>- **概念框架**：策略与评论家分歧触发探索性采样，强制执行策略自信但低值行动，暴露失败模式。<br>- **价值函数估计**：基于Cal-QL改进，使用行为克隆代理策略（πψ）校准Q值。评论家目标含时间差分损失和正则化（统一惩罚、行动锚定、数据保持）。<br>- **探索性采样机制**：策略生成候选动作，评论家过滤Q值低样本执行；为空则用常规动作。产生失败和恢复轨迹纳入训练。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：模拟实验使用LIBERO基准（包含Spatial、Object、Goal和Long-horizon四类任务，每类10个任务）。真实实验使用Galaxea A1机器人手臂。<br>- **训练资源**：评论家基于SAC算法离线训练，策略如DiT Policy或π0在模拟和真实环境重新训练。参数包括批大小（64或256）、学习率（1e-4或2e-5）、折扣因子（0.99）等。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：模拟实验在LIBERO基准评估，真实实验在四个任务（Pick Block、Stack Cup、Arrange Cubes、Stack 2 Cups）。<br>- **评估指标**：主要使用任务成功率（Success Rate）。在LIBERO上报告各任务类别平均成功率；消融研究使用成功率衡量性能。</details> |
| 2025-10-20 | From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors | http://arxiv.org/abs/2510.17439 | <details><summary>展开</summary>### 论文研究单位<br>- ByteDance Seed、NUS、NTU、THU、SMU 等机构合作<br><br>### 论文概述<br>- FALCON (From Spatial to Action) 是一类视觉-语言-行动（VLA）模型，针对现有 VLA 基于 2D 视觉编码、缺乏可靠 3D 空间理解的问题，提出通过空间基础模型在仅 RGB 条件下注入丰富的空间 token，并在可选条件下融合深度与位姿，从而在动作头层面融合语义与几何信息，增强长程与空间推理能力、跨模态可迁移性以及语言-视觉对齐<br><br>### 论文核心贡献点<br>- 以 Embodied Spatial Model（ESM）从 RGB 生成强几何先验，支持可选深度/位姿注入，无需重训即可提升精度<br>- 设计 Spatial-Enhanced Action Head，将空间 token 在动作头而非 VLM 中融合，保持 VLM 语义对齐不受破坏<br>- 采用随机条件注入训练策略（RGB-only 与 RGB-D/Pose 随机），实现强跨模态迁移能力<br>- 在动作头采用轻量元素相加融合，最大化稳定性与效率（优于交叉注意力与 FiLM）<br><br>### 论文方法描述<br>- 整体架构：2D VLM（如 Kosmos-2）负责语义表示；ESM 提取空间 tokens（基于 VGGT 范式，结合 DINO 视觉 token）；动作头融合语义动作 token 与空间 tokens 产生精确动作<br>- 空间编码：<br> - 输入为第三视角图像，附加可学习相机 token，进入 Spatial Encoder（N 层自/交叉注意力）输出空间 tokens 与相机 token<br> - 3D 条件编码：将相机位姿（7DoF）编码为 GT camera token；对深度与有效性掩膜进行归一化与卷积编码，生成与图像 tokens 等尺寸的深度 tokens<br> - 3D 条件注入：采用 b_d、b_p∼Bernoulli(p) 随机决定是否注入深度与位姿，替换可学习相机 token 并进行元素级相加；在训练中采用深度、点云与位姿的多任务监督<br>- 空间增强动作头：<br> - 将空间 tokens 最大池化为统一向量，经轻量 MLP 投影至动作头维度，与语义 action token 元素级相加<br> - 比较了交叉注意力、FiLM 门控与元素相加，最终采用元素相加（在泛化与稳定性上表现最佳）<br> - 预测器支持两类：MLP（短期）或 LSTM（长期）输出动作序列（7DoF 姿态+夹爪状态，C 步预测）<br>- 训练范式：<br> - 两阶段训练：阶段一冻结 VLM/ESM/旧动作头，只训练适配器（零初始化线性层避免初始破坏对齐）；阶段二解冻并联合优化<br> - 损失：MSE（姿态）+ BCE（夹爪）+ 3D 重建/深度/位姿监督（与 VGGT 一致）<br><br>### 论文使用数据集和训练资源<br>- 数据与平台：Open X-Embodiment 混合预训练；真实机器人多任务微调；基准覆盖 CALVIN（ABCD→D、ABC→D）、SimplerEnv（WidowX、Google Robot）与 11 项真实任务<br>- 训练资源：32×A100 训练；模型规模约 2.9B（VLM～1.6B，ESM～1.0B，配套动作头）<br><br>### 论文使用的评估环境和评估指标<br>- 仿真基准与指标：<br> - CALVIN：Tasks Completed in a Row（1-5 连贯任务完成率）与 Avg. Len（平均连击长度）<br> - ABCD→D：FALCON 97.2/93.3/90.3/88.0/84.0%，Avg Len 4.53；ABC→D：98.4/94.5/88.6/82.5/75.5%，Avg Len 4.40<br> - 相较 RT-1/Robo-Flamingo/GR-1/UP-VLA/RoboVLM 等一致领先<br> - SimplerEnv（WidowX/Google Robot）：按任务成功率汇总<br> - WidowX：FALCON 56.3%（Put Spoon 62.5%，Put Carrot 41.7% 等）<br> - Google Robot：FALCON 62.9%，在“打开抽屉放苹果”上 41.7%，远超 RT-2-X 55B 的 3.7%<br>- 真实世界评测与指标：三类设置的成功率（%）<br> - Base Tasks（9 套任务）：FALCON 70.0%，显著高于 SpatialVLA（44.4%）<br> - Few-shot（Simple/Unseen）：在未见物体/背景/任务描述变体下领先，Simple 提升 +27.5%，Unseen 平均 +27%<br> - 空间理解能力：在高度变化、尺寸变化、空间指令等任务中保持最高成功率（例如高度变化任务 60%→80%）<br>- 模态可迁移性与消融：在 CALVIN 与真实任务上验证 RGB-only 与 RGB-D/Pose 的增益；ESM 深度预测指标（δ<1.25、AbsRel）随可用模态增强而显著提升；元素相加在融合策略中兼具效率与性能</details> |
| 2025-10-20 | Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots | http://arxiv.org/abs/2510.17369 | <details><summary>展开</summary># 论文研究单位<br>- 主要单位：EPFL（洛桑联邦理工学院）<br>- 合作单位：LatentWorlds AI（荷兰代尔夫特理工大学背景）、Embodied AI SA<br><br># 论文概述<br>- 研究问题：现有视觉-语言-动作（VLA）模型主要部署于刚性机械臂，难以迁移到软体连续体机器人；两者在运动学、动力学和形态学上的差异导致即用策略失败。<br>- 目标：在软体连续体机器人上部署VLA模型，建立从刚性到软体机器人之间的可迁移部署流程，展示在接近人类的真实环境中的安全操控。<br>- 方案：构建结构化微调与部署流程，对OpenVLA-OFT与π0两类SOTA VLA模型进行对比评估，在软体机器人Embuddy上完成三类代表性操控任务。<br><br># 论文核心贡献点<br>- 贡献1：首个开放软体机器人演示数据集，覆盖抓取放置与人类近距互动任务，便于复现研究与跨形态迁移学习。<br>- 贡献2：在UR5（刚性）与Embuddy（软体）上系统对比OpenVLA-OFT；经针对性微调后，软体机器人取得与刚性平台相当的任务成功率，验证了刚性→软体领域迁移的可行性。<br>- 贡献3：在软体机器人上比较OpenVLA-OFT与π0：π0在刚性跨平台泛化更强，而经微调后OpenVLA-OFT在软体平台上表现更优。<br><br># 论文方法描述<br>- 平台与任务设计：<br> - 刚性平台：UR5，配套平行夹爪与俯视单目相机。<br> - 软体平台：Embuddy，连续体由三段软体节段构成（每段受腱驱动、最大弯曲角度分别为80°、50°、50°），具固有顺应性与碰撞恢复能力；总重约5 kg，工作空间由弯曲角限制。与UR5使用相同相机与夹爪配置以保证公平对比。<br> - 任务：<br> 1. “把橙子放入盘子”——基础抓取放置。<br> 2. “把X放入盘子”（X为橙子或牛奶）——带选择指令的抓取放置。<br> 3. “用棉花糖喂人”——近距人机互动操控。<br>- 数据采集与处理：<br> - 采用3Dconnexion空间鼠标遥操作系统；软体机器人的逆运动学基于分段常曲率（PCC）模型，将连续体节段近似为常曲率以映射腱长与末端位姿。<br> - 多模态观测：第三人称图像、手腕相机图像、本体感受状态（末端位姿）、自然语言任务描述；统一裁剪与缩放到256×256，手腕图像做翻转。<br> - 表示标准化：<br> - 本体状态s为8维向量[x,y,z,r,p,y,pad,g]，其中(x,y,z)为笛卡尔位置，(r,p,y)为滚-俯-偏角，g为夹爪状态（0/1）。<br> - 动作a定义为相邻状态差分：7维[Δx,Δy,Δz,Δr,Δp,Δy,g]；角差使用Δ = ((Δ+π) mod 2π) − π处理边界。<br> - 过滤与转换：去除几乎静止片段；OpenVLA-OFT数据用RLDS格式，π0数据用LeRobot格式；提供开放数据集（HuggingFace：HCSuMoss/soft_orange、HCSuMoss/soft_feed）。<br>- 模型微调与推理：<br> - OpenVLA-OFT（Llama 2 7B + ViT视觉前端）：采用LoRA（rank=32）进行全量微调以平衡性能与成本；引入FiLM层提升语言-视觉条件化；在Task 2中启用FiLM以强化指令对象选择。<br> - π0（PaliGemma 3B）：由于模型体量较小，采用全量微调；在默认设置上调整动作块大小为8以公平比较。<br> - 推理管线：本地机器人端采集第三人称/手腕图像、本体状态与语言指令；远程GPU进行动作块预测并回传；本地执行并循环，直至任务完成或达最大步数。采用动作块并行解码以提高速度。<br> - 安全性与鲁棒性：软体机器人可在被人工推动后恢复姿态并继续执行任务，显著优于刚性臂在人机交互场景的安全性。<br><br># 论文使用数据集和训练资源<br>- 数据集：开放软体机器人演示数据集，覆盖三类任务（Task 1: 50集；Task 2: 100集，橙子与牛奶各50；Task 3: 20集）；HuggingFace可下载。<br>- 训练资源：<br> - OpenVLA-OFT（UR5）：A100（80 GB+），Microsoft Azure虚拟机；软体Embuddy：H100远程HPC。<br> - π0（Embuddy）：H100远程HPC。<br> - 训练时长：OpenVLA-OFT约需56小时完成200k步（单A100）；π0约需11小时完成30k步（单H100）。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：软体机器人Embuddy与刚性机器人UR5；两者使用相同相机、夹爪与任务设定以保证公平性。<br>- 评估指标：<br> - 任务成功率：在每项任务上进行10次试验，统计成功率作为主要指标。<br> - 推理频率（端到端闭环，含网络延迟）：UR5+OpenVLA-OFT约32.3 Hz；Embuddy+OpenVLA-OFT约25.1 Hz；Embuddy+π0约38.0 Hz。<br> - 语言指令遵循验证：Task 2中FiLM模块提升对象选择准确率（OpenVLA-OFT达70%成功）；Task 3中在场景替换（棉花糖换为橙子）时模型能提前终止而非错误操作。<br> - 扰动鲁棒性测试：<br> - 人类出现与移动对性能无影响；<br> - 未见物体可能偶致混淆（约10次中1次）；<br> - 目标物体置于工作空间外时模型总是失败，说明训练工作空间是决定性约束。</details> |
| 2025-10-20 | DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment | http://arxiv.org/abs/2510.17148 | <details><summary>展开</summary># 论文研究单位<br><br>- RIX, Bosch<br>- AIR, Tsinghua University<br>- Shanghai Jiao Tong University<br><br># 论文概述<br><br>端到端（E2E）自动驾驶模型在常规条件下可生成物理可行的轨迹，但难以泛化至长尾场景；视觉-语言-动作（VLA）模型利用世界知识与认知推理应对复杂场景，但缺乏细粒度三维推理，易产生物理不可行的动作。DiffVLA++通过度量引导的轨迹评分器将VLA与E2E的优势对齐，提出桥接认知推理与端到端规划的统一框架。实验在ICCV 2025 Autonomous Grand Challenge公开榜单上实现EPDMS 49.12。<br><br># 论文核心贡献点<br><br>- 构建完全可微的VLA模块，直接回归带语义与三维推理的未来轨迹（4秒、8个关键点）。<br>- 设计密集轨迹词库的E2E模块，包含代理检测、语义分割与规划头，保障物理可行性与密集场景表达。<br>- 提出度量引导对齐机制：以规则化驾驶指标（NC、DAC、EP、TTC、LK、DDC、TLC、HC）训练轻量轨迹评分器（共享BEV特征的MLP并行头），将VLA与E2E轨迹映射至统一度量空间，实现对齐与融合。<br>- 后处理：对候选轨迹进行可行驶区域过滤与加权评分选择；离线集成两分支以提升最终性能。<br><br># 论文方法描述<br><br>- VLA模块<br> - 视觉流：CLIP ViT-L/14编码多视角图像；Driving Vision Adapter压缩并投影视觉token至LLM嵌入空间。<br> - 语言流：预训练分词器与文本编码器处理导航与高层指令，生成文本token。<br> - 语言模型：Vicuna-v1.5-7B进行多模态融合与因果注意力；LLM最后一层直接回归连续未来轨迹（x,y,θ），避免离散化误差，支持端到端优化。<br>- E2E模块<br> - BEV生成：BevFormer以VoVNet-99为骨干，构建128×128的BEV网格，覆盖64×64米空间。<br> - 多任务头：代理检测头（32查询）、语义分割头、轨迹规划头。<br> - 轨迹规划头：预定义密集轨迹词库（8192候选，K-means聚类专家轨迹），每个候选含8个2Hz关键点；沿候选轨迹采样BEV特征，经注意力机制聚合为上下文感知轨迹嵌入；通过可变形交叉注意力整合代理特征以精炼轨迹嵌入；每个嵌入经MLP解码为残差偏移，输出最终预测轨迹。<br>- 度量引导对齐<br> - 轨迹评分器：并行MLP头回归八项规则化指标（EP连续值[0,1]；DAC、TLC、TTC、LK、HC二值{0,1}；NC、DDC三值{0,0.5,1}）；采用加权复合损失（MSE、BCE、Cross-Entropy）训练。<br>- 后处理<br> - 先用全景驾驶感知模型预测可行驶区域，剔除不合法候选；随后按加权分数排序与选择；E2E与VLA两分支离线集成。<br><br># 论文使用数据集和训练资源<br><br>- 数据集<br> - NavsimV2（navtrain划分）用于训练；Navhard两阶段测试与公开榜单用于评估。<br>- 训练资源与配置<br> - VLA模块：Vicuna-v1.5-7B + CLIP ViT-L/14；AdamW优化器，余弦学习率，初值1e-5；dropout 0.05；1个epoch；批量8；8×NVIDIA A800 GPU。<br> - E2E与评分器联合训练：BevFormer + VoVNet-99；代理检测/分割/规划/评分多损失联合（权重10/1/14/20/14）；30个epoch；批量8；初始学习率1e-4；4×A800 GPU；AdamW与余弦调度（与VLA一致）。<br><br># 论文使用的评估环境和评估指标<br><br>- 评估环境<br> - ICCV 2025 Autonomous Grand Challenge（Navhard两阶段测试与公开leaderboard）。<br>- 评估指标与结果<br> - 主要指标：扩展预测驾驶员模型得分（EPDMS）。<br> - 分支成绩：VLA分支48.0；E2E分支43.7。<br> - 最终模型（离线集成）EPDMS：49.1238。<br> - 分项指标（公开leaderboard，两阶段）：<br> - No-At-Fault Collisions：98.2143（Stage 1）、88.7709（Stage 2）<br> - Drivable Area Compliance：98.5714（Stage 1）、95.3235（Stage 2）<br> - Driving Direction Compliance：100（Stage 1）、97.2196（Stage 2）<br> - Traffic Light Compliance：99.2857（Stage 1）、98.1711（Stage 2）<br> - Ego Progress：79.5117（Stage 1）、73.4289（Stage 2）<br> - Time-To-Collision within bound：98.5714（Stage 1）、87.9888（Stage 2）<br> - Lane Keeping：95（Stage 1）、59.4454（Stage 2）<br> - History Comfort：92.8571（Stage 1）、98.9833（Stage 2）<br> - Two-frame Extended Comfort：50（Stage 1）、52.9822（Stage 2）</details> |
| 2025-10-20 | Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey | http://arxiv.org/abs/2510.17111 | <details><summary>展开</summary>## 论文研究单位<br>中国科学院自动化研究所、中国科学院大学、AiRiA、南京信息工程大学<br><br>## 论文概述<br>这是一篇关于高效视觉-语言-动作（VLA）模型在具身操作中的系统性综述论文。VLA模型通过将自然语言指令和视觉观察映射到机器人动作，实现具身控制。尽管VLA系统功能强大，但其巨大的计算和内存需求与边缘平台的实时性能要求存在显著冲突。本综述首次从效率角度对VLA模型进行系统性回顾，涵盖模型架构、感知特征、动作生成和训练/推理策略四个维度，提供了主流效率优化方法的分类和比较分析。<br><br>## 论文核心贡献点<br>1. 首次专门针对高效VLA的系统性综述，将效率改进技术分为四个维度：模型架构、感知特征、动作生成、训练/推理机制<br>2. 基于分类法总结主流效率优化方法，分析各种技术的优劣势和适用场景<br>3. 讨论VLA模型未来发展趋势，强调需要在新兴趋势下进一步改进效率的优先方向<br>4. 提供实用的参考，支撑开发既高效又具备通用可靠具身智能的VLA系统<br><br>## 论文方法描述<br>**架构层面：**<br>- **静态骨干选择**：使用轻量级模型如Mamba、SmolVLA（2.24亿参数）、NORA（30亿参数）替代大型预训练模型<br>- **动态计算路径**：包括层剪枝（FLOWER）、早期退出（DEER-VLA）、专家混合（MoLE-VLA）、相似性跳过（Efficient-VLA）<br>- **双系统设计**：结合慢系统（大型多模态语言模型）处理复杂推理和快系统（轻量模型）实现快速响应<br><br>**感知特征优化：**<br>- **选择性特征处理**：基于注意力分数、任务相关性、空间结构的令牌修剪方法<br>- **时间共享重用**：利用帧间相似性重用KV缓存、高层表示和推理结果<br><br>**动作生成策略：**<br>- **原始动作生成**：动作块分块、令牌压缩（FAST）、离散化（VOTE）<br>- **推理感知动作**：语言链式思维推理（ECoT）、视觉推理（UniPi、VPP、CoT-VLA）<br><br>**训练推理优化：**<br>- **训练效率**：参数高效微调（LoRA）、知识蒸馏、量化感知训练、剪枝恢复<br>- **推理效率**：非自回归解码、投机解码（Spec-VLA）、并行细化（PD-VLA）<br><br>## 论文使用数据集和训练资源<br>- **Open X-Embodiment (OXE)**：大规模真实世界机器人数据集<br>- **DROID**：大规模野外机器人操作数据集<br>- **各种模拟数据集**：用于预训练和Sim-to-Real迁移<br><br>## 论文使用的评估环境和评估指标<br>**评估维度：**<br>- **资源效率**：模型规模、推理延迟、内存占用、训练时间、能耗<br>- **性能鲁棒性**：任务成功率、长时序稳定性、分布外泛化能力、环境扰动恢复<br>- **可解释性**：人类可读的推理过程、决策归因机制、透明度<br><br>**技术实现：**<br>- 统一的云边部署架构<br>- 标准化硬件配置报告<br>- 开源评估框架和基准测试<br>- 多任务多场景的公开数据集和仿真环境</details> |
| 2025-10-18 | MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation | http://arxiv.org/abs/2510.16617 | <details><summary>展开</summary>## 论文研究单位<br>University of Texas at Austin（作者来自德州大学奥斯汀分校）<br><br>## 论文概述<br>现有的大规模视觉-语言-动作（VLA）模型在跨实验室、环境与任务上的泛化能力不足，通常需要在新场景中收集一定量的专家演示并进行参数微调才能奏效。本文提出 MoS-VLA（Mixture of Skills VLA），通过“技能混合”的方式将机器人策略表示为有限基函数的线性组合，从而构建结构化的技能空间；在线推理时，仅需一次专家演示，通过最小化 L1 行动误差的轻量凸优化（线性规划）得到系数，无需梯度回传即可在新环境中快速适配，显著降低适配开销。论文基于 OpenVLA 与 Open X-Embodiment 数据进行训练，并在仿真与真实机械臂任务上验证效果。<br><br>## 论文核心贡献点<br>- 提出一种一次样本、上下文内的 VLA 适配方法：在推理阶段只需一次专家演示，无需梯度更新，计算与内存开销极低（仅前向传播），在普通 GPU（如 RTX 3090）上几秒即可完成校准。<br>- 首次将函数编码器（Function Encoders）应用于亿级参数与多模态机器人数据，验证其在 VLA 场景的可行性与可扩展性。<br>- 在结构化技能空间中缓解“混合上下文过拟合”：通过学习基函数并在系数空间区分不同上下文，提升了分布外数据集与实际环境中的表现。<br>- 构建了 L1 误差驱动的凸优化校准流程（线性规划），强调鲁棒性与对抗离群点的能力。<br>- 给出端到端实现细节与可扩展训练策略，支持分布式并行与缓冲校准，保证可复现性与工程可行性。<br><br>## 论文方法描述<br>- 问题形式化：机器人策略空间为状态 S = I × T 到行动 A 的映射；上下文 c（如光照、相机位姿、机器人形态等）影响专家策略但不可直接观测；将不同上下文下的专家策略集合视为函数空间。<br>- 主思想：将上下文相关的专家策略表示为函数空间中的函数，学习一组神经网络的基函数 {g1, ..., gk}，任何上下文策略 π_exp^c 可表示为这些基函数的线性组合。新任务在线适配时，通过一次专家演示 τ_exp^c，最小化 L1 行动误差，求解线性规划得到系数 α^c，再以线性组合给出动作预测。<br>- 架构实现：在 OpenVLA 主干的基础上替换语言模型输出头为 k 个独立的“基函数动作头”，共享主干特征；采用并行解码策略同时预测多个行动维度；降低参数开销并保持 Transformer 输入的变长处理能力。<br>- 训练流程：采用修正的函数编码器算法，支持 L1 误差与 Banach 空间设置；使用 LoRA 进行高效微调；维护校准缓冲（每数据集 512 样本），每若干步更新一次上下文系数并在训练中广播；使用分布式数据并行（DDP）在多节点（32×GH200）上进行训练。<br><br>## 论文使用数据集和训练资源<br>- 数据集：Open X-Embodiment（RT-X）中的 Magic Soup Plus 数据混合（同 OpenVLA），包含多个实验室的轨迹数据；包含分布内与分布外（OOD）子集。<br>- 训练资源：32 个计算节点，每节点 1×GH200；全局批次 320；训练步数约 5000（耗时约 24 小时）；Adam 优化器，学习率 1e-4，热身 10 步。<br>- 适配资源（在线）：单条专家演示；在普通 GPU（如 RTX 3090）上仅需几秒完成系数求解。<br><br>## 论文使用的评估环境和评估指标<br>- 数据集评估：在 27 个训练子集与 5 个 OOD 子集上评估动作预测误差（L1 误差），对比基线（OpenVLA）。<br>- 仿真实验（Robosuite）：两项任务——块搬运与开门；每任务 m=20 次试验。<br>- 真实机械臂实验（Franka Emika Panda）：三项任务——目标抵达、块搬运、笔插入；每任务 m=10 次试验；单 RGB 前置相机环境，部分运动方向限制以简化深度估计。<br>- 指标：任务成功率（%），同时观察与讨论适配在短-horizon 任务上的有效性以及在更长时序或更高随机性场景中的局限。<br><br>## 关键结果<br>- 在 5 个 OOD 数据集上，MoS-VLA 的 L1 行动误差全面低于基线；在训练子集中也有 18/27 的优势。<br>- 仿真与真实实验：OpenVLA 在未见环境与新任务上成功率为 0%；MoS-VLA 在一次演示校准后，仿真任务（块搬运、开门）达 70–75% 成功率，真实任务（目标抵达、块搬运、笔插入）达 100% 成功率。<br>- 系数可视化显示相似实验室的上下文在技能空间内聚类，说明学习到的技能空间能够捕捉环境与任务的结构性差异。<br><br>## 工程与复现要点<br>- 架构：在 OpenVLA 主干上引入 k=16 个基函数动作头，采用并行解码直接输出标量动作。<br>- 校准：维护分布式校准缓冲，每 16 步进行一次系数广播与更新；采用 CVXPY 求解 L1 线性规划。<br>- 资源友好：在线适配不需梯度回传，推理阶段计算与内存开销与专家轨迹长度无关（常数复杂度）。</details> |
| 2025-10-17 | NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly? | http://arxiv.org/abs/2510.16263 | <details><summary>展开</summary># 论文研究单位<br>Department of Computer & Data Sciences, Case Western Reserve University<br><br># 论文概述<br>论文提出NEBULA，一个针对单臂操作的统一生态，用于视觉-语言-动作（VLA）智能体的诊断式与可复现评估。NEBULA以标准化API和大规模聚合数据集为基础，构建“双轴评估协议”：一条轴线进行能力测试（精细技能诊断），另一条轴线进行压力测试（稳健性刻画）。论文同时对当前SOTA模型开展系统评测，指出空间推理与动态适应是普遍短板，并强调推理速度与低延迟对动态任务的关键作用。<br><br># 论文核心贡献点<br>- 标准化API与统一数据格式，整合ManiSkill、LeRobot等分散数据集，支持跨数据集训练与公平对比；提供大规模聚合数据集（Alpha与Beta），并配套PyTorch与TFRecord接口、模型适配器。<br>- 首次提出双轴评估协议：将“能力”（六类技能：控制、感知、语言、动态适应、空间推理、稳健性/泛化）与“压力”分离，能力轴采用可控变量隔离与分层难度，压力轴对推理频率、延迟、稳定性、适应性、资源进行单一指标测评。<br>- 全面基准评测：揭示当前VLA在空间推理和动态适应上的系统性薄弱与稳健性不足，显示传统“任务成功率”掩蔽关键失败模式。<br>- 诊断输出与因子隔离：提供雷达图等可视化绩效摘要，验证可控变量隔离与分层难度的有效性，支持解释性错误定位。<br><br># 论文方法描述<br>- 统一数据平台与API：基于SAPIEN与ManiSkill3收集与标注多模态时序数据（RGB/深度/分割、状态、动作、语言指令），形成标准化Episode与Step格式；提供SDK与查询引擎、训练/评测工具链及PyTorch/TF兼容适配器。<br>- 能力测试任务：按六类技能设计独立任务模板，每类技能分为Easy/Medium/Hard三个难度层级；遵循“可控变量隔离”原则，使性能变化可唯一归因于被测技能。<br>- 压力测试任务：四个单指标压力测试（推理频率、延迟、稳定性分数、适应性），每项分为三个压力等级（v1–v3），用于刻画系统在不同运营约束下的退化曲线与稳健边界。<br>- 评测与诊断输出：统一数据加载与评测协议，雷达图与可视化展示；进行因子隔离有效性验证，比较隔离与非隔离场景下模型表现差异。<br><br># 论文使用数据集和训练资源<br>- 数据集：NEBULA-Alpha与Beta两套版本，来源于专家轨迹（运动规划）与人机演示；Alpha包含超过5.4万演示，覆盖控制/感知/语言/动态/空间五类能力家族（稳健性家族仅用于评测，不入训练集）；Beta为约10%规模，部分高难度任务含人工遥操作。<br>- 数据形式：多模态观测（6视角摄像头RGB、深度、分割）、本体感受、动作与成功标签、语言指令；提供PyTorch与TFRecord格式，附带模型特定适配器。<br>- 训练资源：主要评测在Alpha数据集上进行微调；训练使用统一数据加载与原版训练协议；具体硬件规格与资源用量未详述。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：基于SAPIEN的定制仿真平台（ManiSkill3框架），与NEBULA统一API与数据格式兼容。<br>- 能力轴指标：按六类技能（控制、感知、语言、动态适应、空间推理、稳健性/泛化）报告分层成功率；提供能力雷达图等诊断性可视化。<br>- 压力轴指标：推理频率（Hz）、延迟（ms）、稳定性分数（基于相邻动作差的指数衰减，归一化至[0,1]）、适应性（在目标切换/指令变更等场景下的成功率）；每项指标分为三档压力等级（v1–v3）。<br>- 诊断与因子隔离：比较隔离与非隔离场景的误差来源，验证能力评估的可控性与有效性。</details> |
| 2025-10-17 | VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving | http://arxiv.org/abs/2510.15446 | <details><summary>展开</summary>## 论文研究单位<br>- 清华大学苏州汽车研究院<br>- 作者实习单位为清华大学苏州汽车研究院<br><br>## 论文概述<br>论文提出 VDRive 框架，将视觉-语言-动作模型（VLA）与扩散策略（Diffusion Policy）结合，用于端到端自动驾驶。框架以离散化的状态-动作表示为核心，通过“强化的模态对齐”桥接高维传感器空间与低维动作空间：在上下文层面，VLA 通过视觉 token 预训练预测未来观测；几何层面，通过对 VLA 的强化学习微调，使其基于当前驾驶条件预测轨迹与动作。随后，扩散策略头在 VLA 提供当前与预测状态 token 的条件下生成层次化动作与轨迹，并由动态引导的精炼头优化轨迹输出。方法采用全离线强化学习范式，在 nuScenes 开环规划与 Bench2Drive 闭环基准上均取得最先进性能。<br><br>## 论文核心贡献点<br>- 提出 VDRive 框架：通过 VLA 预测未来状态 token 与扩散策略头联合训练，实现上下文与几何对齐的驾驶决策。<br>- 构建强化学习偏好数据集：基于 nuScenes 与 Bench2Drive 构造“选择/拒绝”视觉-动作对用于 VLA 微调。<br>- 设计离线奖励数据：结合规则奖励与专家 VLM（Qwen2.5-VL-72B）评分，训练扩散策略头并构建 actor-critic 强化学习闭环。<br>- 在多基准上实现最先进结果：在 nuScenes 开环评估与 Bench2Drive 闭环评估中均显著提升。<br><br>## 论文方法描述<br>- 离散化表示（CVQ-VAE）：训练条件向量量化变分自编码器，以轨迹为条件，将原始/分割图像编码为离散观测 token，加入 VLA 分词器词表。<br>- VLA 强化微调：利用偏好数据训练 VLA 生成未来观测 token、动作信号与导航命令，结合选择/拒绝样本进行偏好对齐。<br>- 扩散策略头学习：在离线状态-动作-奖励-下一状态数据集上，训练扩散模型作为 actor 网络、最小化生成动作与真实动作的重建误差，并通过 critic 网络提供价值反馈，实现累积奖励最大化。<br>- 动态引导的精炼头：将 VLA 与扩散策略的异步动作预测作为条件，通过 Transformer 编码器与 MLP 映射融合位置嵌入与动作特征，最终解码优化轨迹。<br><br>## 论文使用数据集和训练资源<br>- 数据：nuScenes（1000 场景，6 相机/5 雷达/1 激光雷达，2 Hz）与 Bench2Drive（CARLA 仿真）用于开/闭环评估与偏好/奖励数据集构建；利用 Vista 生成合成风险场景。<br>- 预训练/微调：视觉 token 预训练（CVQ-VAE + VLA tokenizer）；VLA 强化微调（偏好对齐）；扩散策略离线强化学习（actor-critic）。<br>- 模型：VLA 基于 InternVL3-8B；扩散策略头使用扩散模型；奖励评估采用 Qwen2.5-VL-72B。<br><br>## 论文使用的评估环境和评估指标<br>- 开环评估（nuScenes）：平均 L2 误差与碰撞率；在 1s/2s/3s 时间步与总体上报告指标。<br>- 闭环评估（Bench2Drive）：Driving Score、Success Rate、Efficiency、Comfortness；并报告多能力指标（并线、超车、紧急制动、让行、交通标志）。<br>- 消融实验：Bench2Drive-mini 上对比不同数据构造与不同精炼模块（Transformer/LSTM/GRU）设计。</details> |
| 2025-10-16 | RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks | http://arxiv.org/abs/2510.14968 | <details><summary>展开</summary>### 论文研究单位<br>- Mingxuan Yan、Yuping Wang、Jiachen Li：加州大学河滨分校（University of California, Riverside）<br>- Yuping Wang（共同作者）：密歇根大学（University of Michigan）<br>- Zechun Liu：Meta AI<br><br>### 论文概述<br>论文针对长程任务中的层次化视觉-语言-行动（VLA）框架问题，提出检索式演示分解器（RDD）。现有VLM规划器微调依赖人工或启发式子任务分解，易导致子任务与低层视觉运动策略训练数据不一致，降低任务性能。RDD通过视觉特征检索自动分解演示，将子任务与策略训练数据对齐，使用动态规划优化分解策略，实现计划器与策略协同。<br><br>### 论文核心贡献点<br>- 首次协调高层规划器与低层视觉运动策略，通过生成对齐的规划器微调数据集提升长程任务性能。<br>- 提出RDD框架，训练自由的检索式演示分解方法，形式化为最优分割问题，用动态规划高效求解（含理论分析）。<br>- 在仿真和真实基准评估中，RDD优于启发式分解器（如UVD），展现跨设置鲁棒性。<br><br>### 论文方法描述<br>- **问题建模**：将演示分解形式化为最优分割问题（公式3.1），最大化分割策略与策略训练数据的相似性。<br>- **动态规划求解**：利用最优性原理，将复杂度从O(2^N)降至O(N^2)（有界区间时O(N)）（算法1）。<br>- **区间评分函数**：定义区间相似性（公式3.3），结合视觉编码器（如LIV）、近似最近邻检索（Annoy）和时间持续差异。<br>- **视觉特征**：使用视觉编码器嵌入间隔（公式3.5），结合开始和结束帧，计算间隔相似度（公式3.6）。<br>- **OOD处理**：扩展方法处理分布外子任务，结合检索和启发式（公式3.7-3.8）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：RLBench机器人操作基准，使用训练集1908个演示（分解为12700子任务区间）。<br>- **视觉编码器**：LIV（主用），对比R3M、VIP、VC-1、CLIP、DINOv2、ResNet。<br>- **检索方法**：Annoy近似最近邻搜索（Angular距离）。<br>- **规划器**：LLaVA-based VLM（llama3-llava-next-8B），LoRA微调（rank 128，scale 256）。<br>- **计算资源**：4个NVIDIA 6000 Ada GPU，微调约5分钟。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：RLBench仿真基准，13个任务（成功率高>35%）。<br>- **评估指标**：<br> - 多任务成功率（%，平均值和标准差）。<br> - 平均排名（↓）。<br>- **比较基线**：Expert（启发式专家）、UVD（视觉分解器）、Uniform（均匀分割）、w/o Finetune（不微调）。<br>- **实验设置**：每个任务3个演示用于微调；结果平均10次随机种子。</details> |
| 2025-10-16 | VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation | http://arxiv.org/abs/2510.14902 | <details><summary>展开</summary>待生成</details> |
| 2025-10-16 | QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models | http://arxiv.org/abs/2510.14836 | <details><summary>展开</summary>### 论文研究单位<br>- School of Artificial Intelligence, University of the Chinese Academy of Sciences<br>- Institute of Automation, Chinese Academy of Sciences<br>- Beijing Zhongke Huiling Robot Technology Co.<br><br>### 论文概述<br>论文提出QDepth-VLA框架，旨在通过引入量化深度预测作为辅助监督信号，增强视觉-语言-动作（VLA）模型的空间感知和推理能力。现有VLA模型在长时序和精细操作任务中表现不佳，主要因缺乏3D几何理解。QDepth-VLA利用向量量化（VQ-VAE）将深度图转换为离散令牌，并设计专门的深度专家模块预测这些令牌，避免干扰预训练语义对齐。实验在模拟和真实机器人任务上验证了其有效性。<br><br>### 论文核心贡献点<br>1. **QDepth-VLA框架**：通过量化深度预测增强VLA模型的空间理解能力。<br>2. **深度专家模块**：设计用于预测离散深度令牌，而非回归像素级深度，提供更紧凑的优化友好监督信号。<br>3. **性能提升**：在LIBERO基准上相比open π0平均提升7.7%成功率，在Simpler基准上提升6.1%，真实机器人任务提升10.0%。<br><br>### 论文方法描述<br>- **深度标注**：使用Video-Depth-Anything (ViDA)生成单目深度估计。<br>- **VQ-VAE重建**：预训练VQ-VAE将深度图量化为256个代码向量的离散令牌（网格分辨率16×16）。<br>- **QDepth-VLA架构**：基于PaLI-Gemma 3B的VLM，包括预训练VLM、动作专家和深度专家。深度专家采用Transformer架构，输入视觉令牌，预测量化深度令牌。<br>- **混合注意力机制**：设计层次化注意力，允许深度令牌关注文本和图像令牌，动作令牌关注所有前序模态，防止干扰预训练VLM。<br>- **联合训练程序**：总损失函数为动作损失（CFM损失）加指数衰减的深度损失（交叉熵损失）。优化使用AdamW优化器，余弦退火学习率（200步预热）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：LIBERO（四个子集：Spatial, Object, Goal, Long）、Simpler（Google Robot和WidowX250任务）、真实机器人数据集（Piper Arm任务）。<br>- **训练资源**：8×H20 GPU，FSDP并行训练，全局批量大小1024，梯度累积。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：模拟环境（LIBERO和Simpler）、真实机器人环境（6-DoF Piper机械臂）。<br>- **评估指标**：任务成功率（Success Rate），LIBERO每任务50次rollouts，Simpler每任务240-2400次评估，真实机器人每任务10次试验。</details> |
| 2025-10-16 | Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning | http://arxiv.org/abs/2510.14300 | <details><summary>展开</summary>### 论文研究单位<br>论文作者来自多个机构，主要包括：<br>- 上海交通大学（MoE关键人工智能实验室、自动化与智能传感学院、计算机科学学院）<br>- 上海AI实验室<br>- 清华大学深圳国际研究生院<br>- 香港大学<br>- 同济大学<br>- D-Robotics<br>- 系统控制与信息处理教育部重点实验室（上海交通大学）<br>- 信息安全综合管理技术上海市重点实验室<br><br>### 论文概述<br>论文针对Vision-Language-Action (VLA)模型在扩展时面临的挑战（如计算资源需求高、实时控制效率与模型容量平衡难题），提出了AdaMoE（Action-Specialized Mixture of Experts）架构。AdaMoE通过继承预训练VLA模型权重，并在动作专家中引入稀疏激活的MoE层来扩展模型容量，同时利用解耦专家选择与权重的创新设计，在保持计算效率的前提下提升性能。论文在仿真基准（LIBERO和RoboTwin 2.0）和真实世界实验中验证了AdaMoE的有效性。<br><br>### 论文核心贡献点<br>1. **高效扩展VLA模型**：提出一种从预训练密集VLA模型继承权重并转换为MoE架构的低成本方法。<br>2. **解耦专家选择与权重**：引入独立的比例适配器（scale adapter）与路由器（router），解决传统MoE中负载平衡与任务性能冲突的根本问题。<br>3. **显著性能提升**：在LIBERO基准提升1.8%，在RoboTwin 2.0域随机化任务提升9.3%，在真实世界实验提升21.5%，验证了实际应用价值。<br>4. **专家特化验证**：通过专家激活模式分析，证明专家能捕获有意义的操作行为（如定位、抓取等）。<br><br>### 论文方法描述<br>- **架构设计**：基于π₀框架的动作专家由共享专家（处理通用操作）和路由专家（处理特定任务）组成，使用top-k选择（默认k=1）进行稀疏激活。<br>- **解耦机制**：路由器（R）负责专家选择以实现负载平衡，比例适配器（S）独立调整专家贡献权重，最终权重为S_i(x) + softmax(R_i(x))。<br>- **训练目标**：结合流匹配损失（flow matching loss，用于高频率动作生成）和负载平衡损失（load balancing loss），总损失为L_total = L_τ + λ_balance * L_balance（λ_balance默认0.01）。<br>- **专家初始化**：共享专家继承原始FFN权重，路由专家为副本以快速扩展。<br>- **推理过程**：使用流匹配进行去噪迭代（从纯噪声生成动作序列）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - LIBERO基准：包含LIBERO-Spatial、LIBERO-Object、LIBERO-Goal和LIBERO-Long四个任务套件，每个套件含100专家轨迹。<br> - RoboTwin 2.0：19个域随机化任务（清洁和随机化环境各100/400轨迹）。<br>- **训练资源**：<br> - 训练步数：120,000步（批量大小32）。<br> - 优化器：AdamW（峰值学习率2.5e-5，路由器学习率5e-5，β1=0.9，β2=0.95）。<br> - 其他超参数：动作视野50、专家数4、梯度裁剪1.0、EMA衰减0.99。<br> - 计算资源：百度云平台提供计算支持，AgileX Robotics提供机器人平台。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **仿真环境**：LIBERO和RoboTwin 2.0基准（使用域随机化增强鲁棒性）。<br> - **真实世界环境**：ALOHA-Agilex双机械臂系统（AgileX Robotics），包括四个任务：堆叠盘子、按铃、调整瓶子、放置杯子。<br>- **评估指标**：<br> - 主要指标：任务成功率（Success Rate, SR），以百分比表示（每个任务评估50次试验）。<br> - 其他分析：专家使用强度（专家激活模式）、消融研究（比较不同架构变体如Vanilla MoE、CSMoE和AdaMoE）。</details> |
| 2025-10-15 | LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models | http://arxiv.org/abs/2510.13626 | <details><summary>展开</summary>## 论文研究单位<br>- 复旦大学<br>- 同济大学<br>- 上海创新研究院<br>- 新加坡国立大学<br><br>（作者分别隶属上述单位，属于跨机构合作研究）<br><br>## 论文概述<br>针对视觉‑语言‑动作（VLA）模型在标准基准上表现优异，却在真实场景中易受扰动的现象，本文在 LIBERO 仿真基准上系统地进行鲁棒性评估。通过在七种维度（对象布局、相机视角、机器人初始姿态、语言指令、光照、背景纹理、传感器噪声）上施加可控扰动，揭示模型的脆弱性：对视角和初始状态的微小变化即可导致成功率从 95% 下降到 30% 以下；而语言变化对整体表现影响最小，进一步实验表明多数模型并未真正依赖语言信号。研究还发现模型存在位置偏置、忽略视觉语义以及对组合扰动的负交互效应。为解决这些问题，文中提出了 LIBERO‑Plus 基准、组合泛化差距的统计定义以及基于大规模扰动数据的通用微调方案，实现了显著鲁棒性提升。<br><br>## 论文核心贡献点<br>1. **系统性脆弱性分析**：在七种单一维度扰动下对 10 种 VLA 模型进行评估，量化每类扰动的绝对性能跌幅（表 1）。<br>2. **诊断框架**：定义扰动随机变量与成功指示器，提出条件成功率、组合概率及协方差形式的组合泛化差距 Δij，并用 χ² 检验显著性。<br>3. **深度洞见**：揭示模型对视觉‑动作的固定映射依赖、语言利用率低、位置记忆偏置以及对组合扰动的负交互效应。<br>4. **LIBERO‑Plus 基准**：自动化生成 10,030 任务，涵盖七大扰动因子及 21 子组件，按多模型成功率划分为 Level‑1–Level‑5 五级难度（图 5‑6）。<br>5. **通用训练策略**：构建超过 20,000 条扰动成功轨迹的通用训练集，对 OpenVLA‑OFT_m 进行混合微调，整体成功率提升至 79.6%（视角扰动 92.8%）。<br>6. **开源资源**：公开评估脚本、任务生成代码、难度划分方法及训练配置，便于后续研究复用。<br><br>## 论文方法描述<br>1. **扰动维度**<br> - 对象布局（添加干扰对象、目标位移）<br> - 相机视角（视角/视野变化）<br> - 机器人初始姿态（关节角度/位置扰动）<br> - 语言指令（语义改写、句式变换）<br> - 光照（强度、方向、颜色、阴影）<br> - 背景纹理（桌面/场景材质）<br> - 传感器噪声（光度失真、抖动、模糊）<br><br>2. **模型集合**<br> OpenVLA、OpenVLA‑OFT（含_oft_w、_oft_m）、π₀、π₀‑fast、Nora、WorldVLA、UniVLA、RIPT‑VLA 等，涵盖自回归与扩散两大范式。<br><br>3. **单维度扰动实验**<br> 对每种扰动在 LIBERO 四套任务上评估成功率，记录绝对跌幅（表 1），并分析模型之间的鲁棒差异。<br><br>4. **视觉注意分析**<br> 将对象布局扰动拆分为“添加干扰对象”与“目标位移”，对比成功率（图 1），验证模型是否聚焦任务关键对象。<br><br>5. **光照鲁棒性极端消融**<br> - **全黑**：所有相机输入置黑。<br> - **第三视角黑**：仅第三人称视角黑掉，保留腕部相机。<br> 通过成功率差异解释腕部相机提供的光照不敏感几何线索（图 2）。<br><br>6. **语言利用实验**<br> - **空指令**：完全删除语言输入。<br> - **目标替换**：将指令中的目标对象替换为同场景的其它对象。<br> 观察成功率变化（图 3），判断模型是否真正依赖语言。<br><br>7. **组合泛化差距定义**<br> 设 D_i 为第 i 种扰动是否施加的二值随机变量，Y 为任务成功指示器，定义条件成功率 s(D_i=d_i,D_j=d_j)=P(Y=1\|D_i=d_i,D_j=d_j)。组合差距 Δ_ij = Cov(D_i,D_j\|Y=1) = p(D_i=1,D_j=1\|Y=1) - p(D_i=1\|Y=1)p(D_j=1\|Y=1)。通过 2000 次独立实验估计概率，构建热图并进行 χ² 显著性检验（附录 F）。<br><br>8. **LIBERO‑Plus 基准构建**<br> - 自动生成任务 → 过滤平衡 → 按四模型成功率划分为五级难度。<br> - 任务规模：10,030，七扰动因子，21 子组件（图 5‑6）。<br><br>9. **通用训练与微调**<br> - 基于原始 LIBERO 轨迹扩增 20,000+ 条成功扰动轨迹。<br> - 以 OpenVLA‑OFT_m 预训练权重为起点进行混合微调。<br> - 在 LIBERO‑Plus 上评估，实现跨扰动显著提升（表 2）。<br><br>## 论文使用数据集和训练资源<br>- **基础数据**：LIBERO 基准（4 套任务），每个套件提供对象、语言指令和标准成功轨迹。<br>- **扰动生成**：自动化在七大扰动维度上生成场景与轨迹，筛选后形成 10,030 评估任务。<br>- **训练集**：超过 20,000 条成功扰动轨迹，覆盖对象布局、相机、光照、背景、噪声等多维变化。轨迹统一保存格式（附录 D）。<br>- **微调配置**：使用官方 OpenVLA‑OFT_m 权重进行混合微调；具体超参数与训练过程详见附录 D。<br>- **计算资源**：所有实验在 LIBERO 仿真平台完成，未在文中披露具体硬件，但规模要求大量随机生成的评估样本。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO 仿真平台，支持第三人称视角与腕部相机（如模型使用）。<br>- **模型评估**：<br> 1. **单维度扰动**：在 4 套标准任务上分别施加七种扰动，记录原始成功率及绝对跌幅（表 1）。<br> 2. **组合扰动**：对选定模型在两两扰动组合下进行 2000 次试验，估计联合概率并计算 Δ_ij（热图（图 4））。<br> 3. **极端消融**：全黑与第三人称视角黑掉两种场景，检验视觉依赖。<br> 4. **语言实验**：空指令与目标替换任务，评估语言利用率。<br>- **指标**：<br> - **成功率（%）**——任务执行成功的比例。<br> - **绝对跌幅**——相对原始成功率的下降低点（表 1）。<br> - **组合泛化差距 Δ_ij**——协方差形式的交互效应度量。<br> - **χ² 检验**——Δ_ij 的统计显著性（附录 F）。<br> - **难度分级**——基于多模型成功率分布的任务划分（Level‑1至Level‑5）。<br> - **综合成功率**——所有扰动维度的平均表现（表 2）。<br><br>（所有图表与实验细节均可在正文相应章节及附录中查阅）</details> |
| 2025-10-15 | DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning | http://arxiv.org/abs/2510.13375 | <details><summary>展开</summary>## 论文研究单位<br>IIIS, 清华大学;Galaxea AI<br><br>## 论文概述<br>DepthVLA是一篇针对视觉-语言-动作模型（Vision-Language-Action, VLA）在精确空间推理方面表现不佳的研究。当前基于大规模语言/视觉预训练的VLA虽然在语义理解上表现强劲，但普遍缺乏精细的空间理解，导致在精准抓取、避障与精细操作任务中失败。DepthVLA通过引入预训练的深度预测模块作为“深度专家”，在保持语言理解与开放词汇感知的同时显式增强几何推理能力。<br><br>DepthVLA采用“混合 transformer”（Mixture-of-Transformers, MoT）架构，统一VLM、深度专家与动作专家（流匹配）三者并共享注意力，但采用分块掩码以保护各专家的预训练能力。深度专家基于DINOv2编码器、采用Depth Anything V2初始化，并通过尺度不变的对数损失进行训练，为动作专家提供贯穿中间层的几何特征，从而提升抓取与碰撞避免等精细操作能力。<br><br>实验在真实世界与仿真环境开展，包括Simpler WidowX与LIBERO等基准。DepthVLA显著优于现有方法：在真实任务中取得78.5% vs 65.0%的成绩，在LIBERO上达94.9% vs 93.6%，在Simpler上为74.8% vs 58.8%。尽管新增约600M参数与约20ms的推理延迟，仍具实际可用性。<br><br>## 论文核心贡献点<br>- 深度专家融入MoT的VLA架构：在不破坏语义能力的前提下显式提供空间几何线索，提升精准操作与避障表现。<br>- 分模块预训练策略：VLM与深度专家可分别在大规模数据上独立预训练，提高效率与可扩展性，突破仅依赖具身动作数据。<br>- 真实与仿真全面验证：在多个基准上实现稳定且显著提升，涵盖精细抓取、碰撞避免与复杂多步任务。<br><br>## 论文方法描述<br>- 统一MoT架构：共享注意力的三专家设计（VLM、深度、动作），采用分块掩码使VLM/深度token自注意力、动作token可跨模态注意；保持各专家独立权重。<br>- 深度专家：基于DINOv2的编码器-解码器结构，使用Depth Anything V2初始化进行预训练；损失为尺度不变对数损失，使其具备稳健的距离估计与几何感知。深度专家在所有中间层输出几何特征，支撑动作专家的细粒度空间推理。<br>- 动作专家：采用流匹配（flow matching）损失对连续动作轨迹建模，与深度损失联合优化，实现端到端训练。<br>- 训练策略：深度专家在大规模3D/深度数据上预训练；VLA阶段在具身演示数据上微调，保留深度预测损失以维持几何能力。<br><br>## 论文使用数据集和训练资源<br>- 深度预训练：WildRGB-D、ScanNet、ScanNet++、HyperSim等大规模3D/深度数据。<br>- VLA训练：Galaxea Open-World（10万轨迹、150任务类、50场景）、BridgeData V2（6万+轨迹）、LIBERO（每个suite 500演示）。<br>- 伪深度标签：Depth Anything V2、UniDepth V2、VGGT。<br>- 训练资源：32×H100 GPU，AdamW优化；批量与学习率针对不同数据集设置。<br><br>## 论文使用的评估环境和评估指标<br>- 仿真评估：<br> - Simpler WidowX：4套任务、每套120次试验，报告各任务及平均成功率。<br> - LIBERO（Franka Panda）：四个suite（Spatial/Object/Goal/Long），总计2000次试验，每个suite报告成功率。<br>- 真实评估：<br> - Galaxea R1 Lite双臂移动平台：设计“餐桌清理”“微波操作”“方块叠放”三类任务。<br> - 指标：进度分数（每步成功计一分），每任务平均20次；并提供少样本微调（20条轨迹）设置下的比较。<br>- 推理与资源指标：显存（8.0GB vs 6.7GB）、延迟（210ms/步 vs 190ms/步），参数增加约600M。</details> |
| 2025-10-15 | Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models | http://arxiv.org/abs/2510.13237 | <details><summary>展开</summary>待生成</details> |
| 2025-10-15 | RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation | http://arxiv.org/abs/2510.13149 | <details><summary>展开</summary>### 论文研究单位<br>南京大学、香港科技大学、新加坡国立大学、上海交通大学、上海创新研究院等机构联合研究<br><br>### 论文概述<br>机器人长期操作任务中的组合泛化能力面临挑战，现有模型在扰动条件下难以有效组合技能。针对此问题，研究提出RoboHiMan层次化评估范式，通过HiMan-Bench基准测试系统评估模型在原子任务和组合任务上的泛化能力，结合多级训练数据集和三种评估模式，分析分层架构的瓶颈。<br><br>### 论文核心贡献点<br>1. **构建HiMan-Bench基准**：包含114个原子任务和144个组合任务，覆盖12种扰动因素（颜色、纹理、光照等）<br>2. **设计层次化评估范式**：提出Vanilla、Decoupled、Coupled三种评估模式，分离测试规划和执行能力<br>3. **分析模型能力缺口**：发现VLA模型在组合任务和扰动场景下性能显著下降，揭示分层架构的脆弱性<br><br>### 论文方法描述<br>- **基准测试设计**：<br> - 原子任务：10类基础操作（开抽屉、拿取物品等）<br> - 组合任务：12类多步骤操作（取物放抽屉、转移物品等）<br> - 扰动因素：对象颜色/纹理/大小、接收对象属性、光照、桌面纹理、干扰物、背景纹理、相机位姿<br>- **多级训练数据集**：<br> - L1：仅原子任务演示（每任务20条）<br> - L2：添加扰动原子任务（每任务1条）<br> - L3：添加4个组合任务（每任务5条）<br> - L4：添加扰动组合任务（每任务1条）<br>- **评估模式**：<br> - Vanilla：端到端执行无规划器<br> - Decoupled：规则规划器/视觉语言模型规划器独立评估<br> - Coupled：端到端分层系统（视觉语言模型规划器+低层策略）<br><br>### 论文使用数据集和训练资源<br>- **仿真环境**：基于CoppeliaSim/PyRep构建的HiMan-Bench<br>- **训练数据**：四个多级数据集（L1-L4），总演示量递增<br>- **模型**：四类VLA模型（RVT-2、3D Diffuser Actor、π₀、π₀.₅）<br>- **规划器**：Qwen2.5-VL视觉语言模型（用于高层规划）<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真测试：114个原子任务和144个组合任务，每任务15个无扰动测试+5个扰动测试+5个全扰动测试<br> - 真实世界验证：设计小规模原子/组合任务集，测试扰动鲁棒性<br>- **评估指标**：<br> - **原子任务**：720集评估，平均成功率（A/AP任务）<br> - **组合任务**：900集评估，平均成功率（C/CP任务）<br> - **规划准确率**：离线评估视觉语言模型的分任务预测精度<br> - **性能下降率**：在线评估对比离线基准的降幅（如L4→C&CP任务下降0.382）</details> |
| 2025-10-15 | VLA-0: Building State-of-the-Art VLAs with Zero Modification | http://arxiv.org/abs/2510.13054 | <details><summary>展开</summary>## 论文研究单位<br>**NVIDIA**<br><br>## 论文概述<br>VLA-0 探讨了一种简洁的视觉语言动作模型（VLA）构建方法。该方法直接利用预训练视觉语言模型（VLM）的文本生成能力，通过自然语言字符串表示机器人动作（如坐标、关节角度），而非修改模型架构或引入新的动作表示机制。核心创新在于，通过精心设计的训练和推理配方（如动作集成预测和遮蔽动作增强），这种极简设计在性能上可与更复杂的VLA方法媲美。实验表明，在 LIBERO 仿真基准测试中，VLA-0 在不使用大规模预训练数据的情况下超越了所有基线方法；在真实世界评估中，也击败了基于SO-100数据集预训练的SmolVLA。<br><br>## 论文核心贡献点<br>1. **挑战VLA设计复杂性：** 证明了基于纯文本动作表示的简单VLA设计（无需修改VLM）可达到甚至超越更复杂的VLA架构（如离散token、生成式动作头或定制架构）的性能。<br>2. **提出有效训练配方：** 详细阐述了实现高性能的关键技巧：动作文本解码（如归一化到整数）、推理阶段的动作集成预测（平均多个时间步预测）、训练阶段的遮蔽动作增强（随机遮蔽字符以强制模型基于视觉信息推理）。<br><br>## 论文方法描述<br>* **核心原理：** VLA-0 直接提示预训练的VLM（使用Qwen-VL-2.5-3B）输出动作文本序列。输入包含系统提示（定义任务）、图像（单一或拼接）、任务指令。<br>* **动作表示：** 连续机器人动作（如末端执行器位置）被归一化到固定整数范围（如[0, 1000]），VLM输出该范围内的空间分隔整数序列，代表未来多个时间步的动作。<br>* **推理阶段（关键配方）：**<br> * **动作集成预测：** 模型预测未来n步动作。对当前时间步t，可获得t、t-1、...、t-n+1时刻预测的t步动作（作为其预测序列中的第一个或后续动作）。对这些n个预测取平均作为最终动作输出，增强稳定性。<br> * **动作文本解码：** 解析VLM输出文本为具体动作值。<br>* **训练阶段（关键配方）：**<br> * **遮蔽动作增强：** 随机遮蔽动作字符串中的字符（如替换为占位符），迫使模型不依赖序列自回归完成，而是根据视觉和指令信息推理目标动作。<br> * **优化：** 使用Adam优化器，64个epoch，批量大小192，学习率5e-6，在8个A100 GPU上训练约32小时。<br><br>## 论文使用数据集和训练资源<br>* **仿真数据：** 基于 LIBERO 基准测试。包含四个套件（空间推理、目标泛化、目标导向、长期任务），每个套件10个任务。VLA-0 在LIBERO内域数据上训练，未使用大规模预训练数据。<br>* **真实数据：** 基于 SO-100 机器人平台和LeRobot框架。四个不同任务（重定位方块、推动苹果、拾取放置香蕉/纸杯蛋糕），每个任务100个演示。<br>* **训练资源：** 使用8个 NVIDIA A100 GPU 进行约32小时的训练。<br><br>## 论文使用的评估环境和评估指标<br>* **评估环境：**<br> * **仿真：** LIBERO 基准测试（四个套件，40个任务）。<br> * **真实世界：** SO-100 机器人平台（基于LeRobot框架）。<br>* **评估指标：**<br> * **主要指标：** 任务成功率（Success Rate）。<br> * **报告方式：** LIBERO 报告每个套件（10个任务）的平均成功率及总体平均成功率；SO-100 报告各任务成功率及总体平均成功率。所有评估均在固定次数的回合（如LIBERO每任务50回合）上进行。</details> |
| 2025-10-14 | DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving | http://arxiv.org/abs/2510.12796 | <details><summary>展开</summary># DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving<br><br>## 论文研究单位<br>中国科学院自动化研究所模式识别国家重点实验室(NLPR)与银网智能科技有限公司联合完成<br><br>## 论文概述<br>DriveVLA-W0提出以世界模型为核心的训练范式，缓解VLA模型在稀疏动作监督下的“监督赤字”问题。该方法通过预测未来图像，引入密集的自监督信号，同时适配离散与连续视觉表征的两类主流VLA架构，并在端到端自主驾驶中实现实时推理与更高数据扩展性。<br><br>## 论文核心贡献点<br>- 识别并解决VLA的“监督赤字”：仅用低维动作信号无法充分驱动大规模VLA模型，世界模型通过预测未来图像提供密集监督<br>- 设计跨架构世界建模：离散视觉token采用自回归(AR)世界模型，连续视觉特征采用扩散世界模型<br>- 提出轻量化MoE Action Expert：与大型VLA主干通过联合注意力融合，提升推理效率(延迟降至原VLA的63.1%)<br>- 揭示动作解码器的扩展规律逆转：在小数据上优的query/flow-matching在大规模数据下被自回归解码超越<br>- 放大数据扩展律：在大规模训练中，世界模型持续提升性能，显著优于仅动作监督的基线<br><br>## 论文方法描述<br>- VLA基线：处理语言指令、前视图像与历史动作，采用Emu3-8B(VQ离散token)与Qwen2.5-VL-7B(连续ViT特征)双骨干，输出语言/视觉/动作三类特征，动作预测采用交叉熵损失<br>- AR世界模型(DriveVLA-W0(VQ))：自回归预测当前图像的离散视觉token，训练目标为下一token预测损失，总损失为动作损失加权的世界模型损失<br>- 扩散世界模型(DriveVLA-W0(ViT))：在潜空间进行条件扩散，预测未来帧图像，使用MSE对噪声预测网络优化，总损失为动作损失加权的扩散世界模型损失<br>- 两阶段训练：先联合世界模型与动作目标预训练，再结合Action Expert进行动作微调；输入序列采用深交错的多模态时间拼接<br>- Action Expert(MoE架构)：大型VLA Expert与500M轻量专家通过联合注意力融合，支持三种动作解码策略(查询式回归、自回归、Flow Matching)<br>- 实时性：推理阶段可跳过显式的图像生成过程以保障低延迟<br><br>## 论文使用数据集和训练资源<br>- 数据集：NAVSIM v1/v2(基于OpenScene的安全关键场景基准)与大规模内部数据集(7000万帧、100万视频片段，100个挑战场景)<br>- 训练资源：NAVSIM实验使用8×L20 GPU(global batch=48)；内部数据集使用64×GPU(global batch=256)；统一使用AdamW与余弦学习率、bfloat16混合精度<br>- 对比基线：复现实验的TransFuser-50M与TransFuser-7B(单前视相机配置)<br><br>## 论文使用的评估环境和评估指标<br>- NAVSIM v1：NC、DAC、TTC、舒适度C、EP，综合指标PDMS<br>- NAVSIM v2：NC、DAC、DDC、TLC、EP、TTC、LK、HC、EC，综合指标EPDMS<br>- 内部数据集：3秒6点轨迹的ADE(↓)与碰撞率(↓)，碰撞率计算方法与NC一致<br>- 扩展评估：跨域迁移(动作分布差异)、数据扩展律(70k/700k/70M帧)、延迟分析(H200 GPU)、生成保真度与规划性能的关联分析</details> |
| 2025-10-14 | Reflection-Based Task Adaptation for Self-Improving VLA | http://arxiv.org/abs/2510.12710 | <details><summary>展开</summary>待生成</details> |
| 2025-10-14 | Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model | http://arxiv.org/abs/2510.12276 | <details><summary>展开</summary>### 论文研究单位<br>香港科技大学（广州）、清华大学、西湖大学、浙江大学、华南理工大学等机构联合完成<br><br>### 论文概述<br>针对视觉-语言-行动（VLA）模型缺乏3D空间理解能力的问题，论文提出**空间强制（Spatial Forcing, SF）**方法。该方法通过将VLA模型的中间视觉表征与3D基础模型（VGGT）的几何表征对齐，在不依赖显式3D传感器或深度估计的条件下，隐式增强VLA的空间推理能力。<br><br>### 论文核心贡献点<br>1. **深度探测实验**：验证原始VLA视觉嵌入缺乏有效空间结构<br>2. **空间强制（SF）机制**：将VLA中间视觉表征与VGGT输出的空间表征对齐<br>3. **显著性能提升**：在仿真和真实环境中超越现有方法，同时提升训练效率（3.8×）和数据利用率<br><br>### 论文方法描述<br>- **对齐框架**：<br> - 使用VGGT处理多视图图像，生成像素级空间表征<br> - 对VLA中间层视觉令牌进行归一化和MLP变换<br> - 通过余弦相似度最大化对齐视觉令牌与VGGT的空间表征<br>- **监督策略**：监督中层而非最深层（防止视觉特征丢失）<br>- **总损失函数**：结合动作生成损失与对齐损失<br>- **推理阶段**：与标准VLA模型一致，无额外计算开销<br><br>### 论文使用数据集和训练资源<br>- **仿真数据**：<br> - LIBERO基准（Spatial/Object/Goal/Long任务套件，共500专家演示）<br> - RoboTwin（双机械臂仿真，含简单/困难模式）<br>- **真实数据**：双机械臂AgileX平台（主相机+腕部相机）<br>- **计算资源**：<br> - 训练：8×NVIDIA H100显卡（LIBERO任务），1×H100（RoboTwin任务）<br> - 训练迭代：15万次（LIBERO），3万次（RoboTwin）<br> - 权重因子α：0.5<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **仿真环境**：LIBERO（四类任务）、RoboTwin（双机械臂）<br> - **真实环境**：AgileX双机械臂平台<br>- **评估指标**：<br> - 任务成功率（Success Rate, SR）<br> - 训练收敛速度（迭代次数比较）<br> - 数据效率（不同数据比例下的成功率）<br> - t-SNE可视化验证表征对齐效果<br>- **实验结果**：<br> - LIBERO平均SR达98.5%（超越所有2D/3D方法）<br> - 训练效率提升3.8×（相同成功率所需迭代减少）<br> - 5%数据下实现75.8% SR（数据效率提升5.9×）<br> - 真实任务中透明杯堆叠等场景下SR显著提升<br><br>论文主页：https://spatial-forcing.github.io/</details> |
| 2025-10-13 | ManiAgent: An Agentic Framework for General Robotic Manipulation | http://arxiv.org/abs/2510.11660 | <details><summary>展开</summary>待生成</details> |
| 2025-10-13 | Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning | http://arxiv.org/abs/2510.11027 | <details><summary>展开</summary># Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning<br><br>## 论文研究单位<br>- 论文由上海AI实验室主导，联合中国科学技术大学、上海交通大学、浙江大学、南京大学、复旦大学、清华大学、NUS、东北大学、深圳大学等多机构合作完成。<br>- 通讯作者来自上海AI实验室；项目页面与代码仓库公开。<br><br>## 论文概述<br>- 针对上游VLM（Vision-Language Model）的具身推理能力与下游VLA（Vision-Language-Action）策略学习之间的关键差距，提出Vlaser：一个将高阶推理与低阶控制协同的VLA基础模型。<br>- 构建高质量Vlaser-6M数据引擎，形成600万规模的多任务具身推理语料；系统分析不同VLM初始化与数据类型对VLA微调的影响，缩小互联网数据与具身策略学习之间的域间偏移。<br>- 在12项具身推理基准与闭环机器人任务（WidowX/Google Robot）上取得领先结果，公开模型、数据与代码。<br><br>## 论文核心贡献点<br>- 统一的具身VLA架构与两阶段训练范式：将InternVL3扩展出“动作专家”，通过流匹配实现未来动作chunk预测，兼顾视觉、语言与控制。<br>- 数据引擎与方法论创新：系统整合并增广多源公共数据集，构建覆盖定位（grounding）、空间推理、规划与通用VQA的Vlaser-6M；进一步在SimplerEnv中生成200万面向VLA的“域内”QA数据，缓解视觉域偏移。<br>- 实证与洞见：上游具身推理的提升未必转化为下游VLA闭环成功率；与互联网数据相比，针对具体机器人本体与视角的“域内”数据更能加速收敛与提升成功率，并明确域间偏移的核心问题。<br><br>## 论文方法描述<br>- 模型结构<br> - VLM骨干：基于InternVL3（InternViT图像编码 + Qwen2.5语言模型），尺寸为2B与8B，强调具身常识与端到端控制。<br> - 动作专家：在VLM上增设低层控制分支；采用流匹配（flow matching）学习动作向量化场（vector field），以单帧观测、语言指令与机器人状态token为输入，生成未来H步动作序列；VLA流使用非因果注意力。<br> - 训练与推理损失：VLM阶段采用自回归语言建模损失；VLA阶段最小化预测向量场与真实去噪向量场的MSE；推理时用欧拉积分从τ=0至τ=1对随机噪声进行去噪，生成动作。<br>- 数据引擎<br> - 具身定位：1.5M问答（边界框与中心点，标准化至[0,1000]），来源RoboPoint、ShareRobot、Pixmo-Points、Paco-LaVIS、RefSpatial；并从SA-1B分割掩码增广30万标注。<br> - 通用/空间推理：1.2M通用RoboVQA + 0.5M空间推理数据；源RoboVQA、Robo2VLM、RoboPoint、SPAR、SpaceR-151k、VILASR，以及ScanNet/ScanNet++/CA-1M/ARKitScenes的10万手工空间样本。<br> - 规划：40万数据，含Alpaca-15k-Instruction、MuEP、WAP，以及LLaRP在Habitat的规划轨迹与EgoPlan-IT/EgoCOT的第一人称视频规划样例。<br> - VLA域内数据：200万针对WidowX与Google Robot的SimplerEnv生成问答，涵盖定位、空间、规划、通用VQA等类别。<br>- 训练配方<br> - 阶段一：监督微调InternVL3构建具身VLM骨干。<br> - 阶段二：在机器人数据集上对动作专家进行VLA微调；使用流匹配损失训练，H=4，积分步长δ=10。<br><br>## 论文使用数据集和训练资源<br>- 具身推理评估数据（12项基准）：ERQA、Ego-Plan2、Where2place、Pointarena、Paco-Lavis、Pixmo-Points、VSI-Bench、RefSpatial-Bench、MMSI-Bench、VLABench、EmbodiedBench（ALFRED/Habitat）。<br>- 机器人闭环评估：SimplerEnv（Bridge/WidowX 与 Google Robot 任务集），包含超过5百万帧图像与机器人 эпизодов。<br>- 数据引擎与规模：Vlaser-6M由多源公共数据整合与增广（定位、空间推理、规划、通用VQA），以及来自SimplerEnv的200万域内VLM预训练问答对。<br>- 预训练骨干：InternVL3-2B/8B（InternViT + Qwen2.5-1.5B/7B），动作专家采用流匹配架构；公开模型、训练与推理代码。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境<br> - 具身推理：12项公开基准，涵盖问答、任务规划、定位、空间推理与仿真闭环评估。<br> - 闭环机器人控制：SimplerEnv中的WidowX与Google Robot平台（Pick Coke Can、Move Near、Drawer、Carrot on plate、Put eggplant in basket、Spoon on towel、Stack Cube等任务）。<br>- 评估指标<br> - 具身推理：各基准的任务分数与标准化平均分（Avg）。<br> - 闭环控制：成功率（Success Rate），分任务与平均成功率。<br> - 训练分析：收敛速度与跨域泛化表现（域内vs互联网数据对VLA微调的影响）。</details> |
| 2025-10-13 | RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model | http://arxiv.org/abs/2510.10975 | <details><summary>展开</summary>### 论文研究单位<br>深圳先进技术研究院（中国科学院）、鹏城实验室、中山大学计算机科学与工程学院、南洋理工大学计算与数据科学学院、上海AI实验室、中国科学院大学、X-Era AI Lab<br><br>### 论文概述<br>RoVer是一个针对视觉-语言-动作（VLA）模型的外部测试时缩放框架，旨在通过引入一个紧凑的机器人过程奖励模型（PRM）作为测试时验证器，在不修改原始VLA架构或权重的情况下增强其性能。该方法通过生成多个候选动作并使用PRM预测标量过程奖励和动作空间方向，以选择最优动作执行。关键是通过缓存共享感知特征来摊销感知成本，从而实现高效的测试时计算资源分配。<br><br>### 论文核心贡献点<br>1. **通用测试时缩放框架**：提出一个即插即用的外部测试时缩放框架，纯在推理时增强冻结的VLA策略，无需额外数据或模型重训练。<br>2. **紧凑过程奖励模型**：设计一个同时输出标量过程奖励和动作细化方向的PRM，实现基于验证器的智能候选动作评估和方向引导探索。<br>3. **高效方向引导采样策略**：利用共享感知缓存机制，将感知成本摊销到多个候选动作上，在固定测试时计算预算下支持可扩展的候选生成和选择。<br><br>### 论文方法描述<br>RoVer的整体方法分为三个部分：<br>- **模型架构**：基于GPT-2风格的transformer，初始化自GR-1（使用MAE和CLIP预训练权重），添加奖励和方向预测头。引入共享感知缓存（编码观测、语言和状态特征一次并复用）和动作放大器（MLP结构，强化动作细微差异）。<br>- **模型训练**：采用方向监督和偏好学习（Bradley-Terrry损失）训练PRM。数据准备通过锚定中心采样构造“更好/更差”动作对，并预测从当前动作到专家动作的方向向量。目标函数结合方向对齐损失（余弦相似度）和奖励偏好损失。<br>- **方向引导测试时缩放**：推理时从基础策略采样N个候选动作，沿PRM预测的方向在有界角度内扩展M个新候选（统一候选预算K=N+M），PRM评分所有候选后选择最优动作执行。方向引导策略优于随机高斯采样，通过集中探索提升效率。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：模拟实验使用CALVIN基准（ABC→D设置：训练集为A、B、C环境，测试为未见环境D），仅用20%训练集样本；真实机器人任务基于自定义场景（抓取放置、推按钮、堆叠碗）。<br>- **训练资源**：PRM训练使用CALVIN ABC→D训练集100个epoch；模型总参数0.2B（可训练参数40M，冻结MAE和CLIP编码器）；推理在单张NVIDIA V100 GPU上运行，测试时计算预算可配置（如候选数K=10~10000）。<br><br>### 论文使用的评估环境和评估指标<br>- **模拟评估环境**：CALVIN基准（长时序语言条件任务），评估设置包括ABC→D跨环境迁移。指标为SR@k（连续完成k个任务的概率，k=1-5）和平均链长（Average Chain Length）。<br>- **真实机器人评估环境**：双机械臂Dobot平台（右侧执行任务，左侧固定），使用腕部相机和俯视相机。任务涵盖Seen、Unseen object和Unseen position条件。指标为成功率（%，每条件10次试验）。<br>- **效率评估**：使用共享感知缓存对比无缓存的延迟（秒）和加速比（倍），测试候选数10~10000下的吞吐扩展性。</details> |
| 2025-10-11 | X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model | http://arxiv.org/abs/2510.10274 | <details><summary>展开</summary># X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model<br><br>- 论文研究单位<br> - 清华大学AIR（Institute for AI Industry Research）、上海AI Lab、北京大学等合作完成<br><br>- 论文概述<br> - 目标是构建通用、可扩展的跨实体（跨机器人）视觉-语言-动作（VLA）模型，在不同硬件、数据和环境中实现稳健的泛化与快速适应<br> - 提出软提示（Soft Prompt）机制，将每个数据源的硬件配置、相机设置、任务分布等异质性映射为可学习的轻量嵌入，作为“实体特定提示”，在特征融合早期引导模型学习<br> - 设计X-VLA架构：采用流匹配（flow-matching）训练策略，基于标准Transformer编码器堆叠实现多模态输入的可扩展融合，支持多视角视觉、语言指令和本体感觉的联合编码<br> - 训练管线分两阶段：预训练阶段在异质混合数据上学习“实体无关”的通用策略；域适应阶段冻结主干，仅调优新域的软提示和少量参数，实现高效迁移<br><br>- 论文核心贡献点<br> - 软提示学习：无需手工模板和自然语言描述，以可学习嵌入编码跨实体异质性，稳定预训练并促进快速适应<br> - 简化而可扩展的Transformer架构：用标准自注意力Transformer替代扩散式解码器，保持通用性与简洁性；针对高维（多视角视觉、语言）和低维（本体感觉、动作）模态采用专用编码与早期融合<br> - 自定义训练配方：包括对齐的动作表示、意图抽象（时间下采样构建高层意图锚点）、平衡采样、以及两阶段适应（软提示预热后联合优化）来提升训练稳定性和迁移效率<br> - 实验验证：在6个仿真基准（含自动驾驶）和3个实体机器人上进行广泛评测，实现SOTA结果；此外用少量演示（1,200条）完成高难度的双手机器人“布料折叠”任务，并以1%参数（9M）微调实现接近完整微调的性能<br><br>- 论文方法描述<br> - 异质软提示学习：为每个数据源维护一组可学习的嵌入（软提示），近似映射硬件配置到提示空间；在动作生成早期注入，引导主干进行实体感知的学习<br> - 架构设计：将主视觉-语言流与辅助视角（如腕部相机）分离编码；本体感觉与动作tokens（含时间嵌入）轻量投影后与视觉语言特征早期融合；整体采用标准Transformer堆叠进行融合与精确动作生成<br> - 训练目标：使用流匹配策略，通过学习速度场将高斯噪声逐步传输到目标动作块；采用最优传输路径的线性插值监督，训练目标为预测速度与真实速度之间的均方误差<br> - 数据处理与训练配方<br> - 动作对齐：将动作标准化为末端执行器（EEF）位姿表示（xyz位置、Rotate6D旋转、二进制夹爪状态），分别用MSE和BCE监督<br> - 意图抽象：时间下采样将低层动作轨迹抽象为未来4秒内的30个锚点，增强高层意图建模<br> - 平衡采样：在跨域与域内同时随机打乱，缓解分布偏置与过拟合<br> - 两阶段适应：先在冻结主干下预热新软提示，随后与主干联合微调<br> - 学习率策略：对软提示与视觉-语言模块使用较低学习率，保留预训练表示并稳定优化<br><br>- 论文使用数据集和训练资源<br> - 预训练数据混合：Droid、RoboMind、Agibot等，约290K片段，覆盖7个平台与5类机械臂（单臂到双臂）<br> - X-VLA-0.9B实例：隐藏维度1024、24层Transformer，预训练在异质混合数据上进行；后续在仿真与真实环境进行适配<br> - 软提示与适配：每个数据源一组软提示；域适应中引入新软提示进行轻量化适配<br> - 参数高效微调（PEFT）：采用LoRA等方法，仅调优约9M参数（约1%）实现接近全参数微调的效果<br> - 实际任务数据：构建高质量“布料折叠”数据集Soft-FOLD，包含1,200条演示；并在BridgeData-v2等真实机器人协议上进行评测<br><br>- 论文使用的评估环境和评估指标<br> - 仿真基准：Libero、Simpler、VLABench、RoboTwin-2.0、Calvin、NAVSIM（覆盖单臂、双臂、跨域/跨任务/跨环境与自动驾驶场景）；在多项基准中达成新SOTA<br> - 真实机器人：三类实体平台，分别测试简单操控、灵巧操控和快速适应（基于PEFT），任务成功率、任务完成率/吞吐量为主要指标<br> - 关键结果概览<br> - X-VLA-0.9B在多个仿真基准上超越既有SOTA（示例：Simpler-WidowX达约96%、Libero约98%、Calvin首阶段约96%）<br> - 真实实验中在五类任务上显著优于基线；布料折叠任务达到接近100%成功率，约33次/小时的折叠吞吐<br> - 以LoRA仅调优9M参数，在Libero与Simpler-WidowX上达到约93%与54%成功率，接近或可比全参数微调的强基线<br> - 代理指标：预训练阶段使用ℓ1动作预测误差作为下游适配性能的代理指标，并与域适应成功率呈现强相关性</details> |
| 2025-10-11 | Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback | http://arxiv.org/abs/2510.10181 | <details><summary>展开</summary>### 论文研究单位<br>上海交通大学计算机科学学院<br><br>### 论文概述<br>论文提出Dejavu框架，用于具身代理的后部署学习。该框架通过经验反馈网络（EFN）增强冻结的视觉-语言-行动（VLA）策略。EFN检索与当前上下文相关的历史成功行动经验，并预测残差行动来修正基础策略输出。在部署期间，EFN持续从新经验中学习，尽管基础策略权重固定，但能提升代理的适应性和成功率。实验基于LIBERO模拟器和AgiBot G1机器人，涵盖OpenVLA、UniVLA和GO-1骨干，显示EFN显著改善部署时性能。<br><br>### 论文核心贡献点<br>- 引入EFN作为以经验为中心的部署时机制，结合实时体验库和轻量级控制器，改进冻结VLA策略，无需梯度重训练。<br>- 形式化体验为同步视觉-语言-行动轨迹，并实现语言条件视觉相似度的检索机制。<br>- 将EFN集成至OpenVLA、UniVLA和GO-1骨干，在模拟和真实环境中实现一致的部署时改进。<br><br>### 论文方法描述<br>- **体验库设计**：存储步骤级轨迹（图像、视觉令牌、潜在行动），通过均值-最大融合构造紧凑关键向量，采用余弦相似度的概率top-k检索。<br>- **残差策略学习**：使用软演员-评论家（SAC）算法训练EFN，以预测残差行动；奖励函数基于语义相似度（当前观察与检索经验的下一观察匹配），并包含反空闲惩罚。<br>- **部署时机制**：指令过滤候选集、步进检索与效率优先（倾向短轨迹）、行动修正和执行、在线体验增长（将成功轨迹加入库）。<br>- **训练细节**：EFN输入包括当前观察、基础行动和检索经验；上下文编码后通过行动者网络输出残差，评论家网络评估修正行动；损失函数基于SAC目标。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：模拟实验使用LIBERO数据集；真实实验基于AgiBot G1机器人。<br>- **训练资源**：VLA骨干包括OpenVLA、UniVLA和GO-1；使用Prismatic预训练管道、Flash Attention；LIBERO模拟器用于模拟，AgiBot G1硬件用于现实实验；EFN训练在模拟环境中进行，依赖CUDA加速。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：模拟实验在LIBERO平台上进行；真实实验在AgiBot G1机器人上进行。<br>- **评估指标**：主要指标为成功率（Success Rate）和成功时的平均步数（Steps）；例如，LIBERO四子任务（Spatial、Object、Goal、Long）的成功率提升和步数减少；真实世界任务（PutBottle、SortItem、AddGoods）显示类似改进。基准比较包括EFN与OpenVLA/UniVLA/GO-1的对比，以及不同体验库容量（Volume 100-1000）的消融研究。</details> |
| 2025-10-11 | Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models | http://arxiv.org/abs/2510.09976 | <details><summary>展开</summary>## 论文研究单位<br>- 脑认知与脑启发智能实验室（中科院自动化所）<br>- 中国科学院大学<br>- 脑认知与脑启发智能技术国家重点实验室<br>- Long-term AI<br><br>## 论文概述<br>针对基于条件流匹配（flow matching）的视觉-语言-动作模型（VLA，如 π0），传统策略梯度强化学习方法由于无法计算重要性采样比而难以应用。本文提出“流策略优化”（Flow Policy Optimization，FPO）：用基于条件流匹配（CFM）损失的样本级变化构造“无似然比率”，替换传统重要性采样，实现 PPO 风格的截断代理目标更新；并结合结构感知信用分配、截断代理、潜空间多步欧拉探索与 Q 值集合体，构建稳定且可扩展的在线强化微调框架。在 LIBERO 与 ALOHA Transfer Cube 仿真任务上进行评估，FPO 在多项指标上超越多类强基线，并呈现稳定的学习曲线与稀疏奖励下的有效探索。<br><br>## 论文核心贡献点<br>- 首次实现流匹配策略与 PPO 风格策略梯度更新的兼容：无须显式似然或雅可比，计算“可通约的比率代理”。<br>- 将样本级 CFM 损失差作为策略改进步信号（结构对齐的信用分配），并配套截断代理以控制更新幅度。<br>- 引入潜空间多步欧拉探索与 Q 值集合体（保守目标与 GAE），提升在线学习的稳定性与效率。<br>- 在 LIBERO（四个子套件）与 ALOHA Transfer Cube 任务上，π0-FPO 取得一致优于多类强基线的效果（含在线 RL、偏好对齐与扩散/自回归策略），并稳定收敛。<br><br>## 论文方法描述<br>- FPO 采用“冷启动/滚动-更新”交替流程：冻结旧策略进行滚动收集经验（轨迹、奖励、隐变量与 CFM 损失缓存），随后在当前策略上重评 CFM 损失，将损失差标准化并映射为无似然比率 ρt，最终采用截断代理目标进行策略更新。<br>- 无似然比率：从旧/新策略在同一样本上的 CFM 损失差 Δℓcfm 出发，经批内标准化 zt = (Δℓcfm − μΔ)/σΔ 与单调指数映射 ρt = exp(β zt)，作为 PPO 截断代理中的重要性比替代。<br>- 截断代理目标：Lactor(θ) = −E[min(ρt Ât, clip(ρt, 1−ε, 1+ε) Ât)]，其中优势 Ât 由 GAE 估计。<br>- Q 值集合体：以最小化集合成员的保守目标 y_t = r_t + γ min_i Q̄_ϕi(s_{t+1}, x′_{t+1}) 进行时序差分训练，并用 Polyak 平均更新目标网络，以减少过估计与不稳定性。<br>- 潜空间探索：在隐变量 x 上使用 CFM 速度场 vθ 进行多步欧拉积分生成平滑、时间相关的扰动，增强探索。<br>- 数据管理：使用小滑窗轨迹缓存，保持更新策略与数据收集策略的分布接近，保证比率代理的稳定与可信。<br><br>## 论文使用数据集和训练资源<br>- 数据与预训练：采用 π0 预训练检查点作为起点，冻结其解码器 π0，仅在线更新流匹配 Actor 与 Q 集合体。<br>- 训练方式：在仿真环境中进行在线交互与强化微调，无需大量监督演示数据。细节包括滚动窗口大小、K_update 更新轮次、β 与 ε 等超参数的选择与设置。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：LIBERO 基准（ LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long）与 ALOHA Transfer Cube 仿真任务；二者均为接触丰富与稀疏奖励的典型具身控制环境。<br>- 评估指标：按官方协议报告任务成功率（SR, %），并在 LIBERO 上给出各套件的平均排名；在学习动力学分析中结合成功率与平均回报曲线；消融研究则报告在指定任务上的最终成功率。</details> |
| 2025-10-10 | VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation | http://arxiv.org/abs/2510.09607 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>南京大学、腾讯Youtu Lab、中科院(CASIA)<br><br>## 论文概述<br>VITA-VLA论文提出了一种高效的教学框架，通过动作专家蒸馏技术将小型动作模型的知识转移到预训练的大规模视觉-语言模型(VLM)中，使其具备动作执行能力。该方法解决了传统VLA模型需要大量计算资源和数据进行端到端训练的局限，通过两阶段训练策略显著降低训练成本，同时保持高性能表现。在仿真基准(LIBERO和CALVIN)和真实机器人环境中的实验验证了该方法的有效性。<br><br>## 论文核心贡献点<br>1. **精简架构设计**：在保持原始VLM结构的基础上，仅添加动作token和状态编码器，集成物理输入信息<br>2. **两阶段训练策略**：第一阶段进行轻量级对齐(仅3000万参数)，第二阶段选择性微调，有效降低训练成本<br>3. **优秀性能表现**：在LIBERO基准上达到97.3%平均成功率(提升11.8%)，在LIBERO-LONG上达到93.5%成功率(提升24.5%)，在CALVIN ABC-D基准上达到92.5%第一任务成功率<br>4. **真实世界验证**：在ALOHA机器人平台上实现82.0%平均成功率，较teacher模型提升17%<br><br>## 论文方法描述<br>### 整体架构<br>- **视觉编码器**：InternViT-300M处理图像输入<br>- **连接器**：3层MLP桥接视觉和语言模态<br>- **语言模型**：Qwen-2.5-7B作为基础<br>- **状态编码器**：将6-DoF机械臂状态和2维夹爪状态编码为单token<br>- **动作token**：作为可学习查询，重复3次预测未来3步动作<br>- **动作映射器**：3层MLP将VLM隐藏状态映射到动作空间<br>- **动作解码器**：复用的2层MLP生成最终执行动作<br><br>### 两阶段训练策略<br>**阶段1-对齐阶段**：<br>- 训练状态编码器、动作token和动作映射器(约3000万参数)<br>- 使用MSE损失对齐VLM和小动作模型的隐藏表示空间<br>- 复用预训练动作解码器，避免昂贵端到端预训练<br><br>**阶段2-微调阶段**：<br>- 端到端微调语言模型、状态编码器、动作模块<br>- 使用MAE损失监督6-DoF臂动作，BCE损失监督夹爪动作<br>- 组合损失函数：L_total = L_arm + λ·L_gripper (λ=0.01)<br><br>## 论文使用数据集和训练资源<br>### 仿真数据集<br>- **CALVIN ABC-D**：训练于环境A、B、C，测试于未见环境D，评估零样本泛化<br>- **LIBERO基准**：4个任务套件(Spatial、Object、Goal、LONG)，每个包含10个长时序任务<br><br>### 真实世界数据<br>- 手动收集500个高质量演示轨迹(每任务100个)<br>- 覆盖5个操作任务：close drawer、stack cups、stack blocks、pick place sponge、pick place block<br><br>### 训练资源<br>- 使用DeepSpeed ZeRO-2阶段优化内存使用<br>- 对齐阶段：batch size=8，学习率=1e-4，训练3个epoch<br>- 微调阶段：batch size=4，学习率=1e-4，训练2个epoch<br>- 图像分辨率：统一为200×200像素<br><br>## 论文使用的评估环境和评估指标<br>### 评估环境<br>- **仿真环境**：CALVIN和LIBERO基准测试平台<br>- **真实世界**：ALOHA机器人平台(6关节PiPer机械臂+Songling夹爪)<br><br>### 评估指标<br>- **成功率**：任务完成的平均百分比<br>- **平均任务长度**：连续完成指令的平均数量(CALVIN基准)<br>- **长时序执行能力**：LIBERO-LONG基准上的表现<br>- **真实世界性能**：5个操作任务各40次独立试验的平均成功率<br><br>评估结果显示VITA-VLA在所有基准上都实现了最佳VLA模型性能，特别是在复杂长时序任务中表现出色，并在真实机器人部署中验证了方法的实际有效性。</details> |
| 2025-10-10 | PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs | http://arxiv.org/abs/2510.09507 | <details><summary>展开</summary>## 论文研究单位<br>HKUST(GZ)、HKUST、Beihang University、Knowin<br><br>## 论文概述<br>PhysToolBench是一个专门评估多模态大语言模型（MLLMs）物理工具理解能力的基准。它采用视觉问答（VQA）格式，涵盖超过1000个图像文本对，分为三个难度级别：工具识别（Tool Recognition）、工具理解（Tool Understanding）和工具创建（Tool Creation）。论文评估了32个不同类型的MLLMs，包括专有、开源、具体化和VLA骨干模型，发现当前MLLMs在工具理解方面存在显著不足，性能远低于人类表现（超过90% vs MLLMs最高63%）。<br><br>## 论文核心贡献点<br>1. **首个物理工具理解基准**：提出PhysToolBench，系统性地评估MLLMs对物理工具的掌握程度。<br>2. **三层难度设计**：设计Easy、Medium、Hard三个级别，其中Medium细分为属性理解、工具组合和可用性理解三个子挑战，以渐进式评估深度理解。<br>3. **广泛模型评估**：全面评估32个MLLMs，揭示关键弱点，如小模型缺乏涌现能力、长尾问题严重、工具可用性幻觉和视觉推理不足。<br>4. **视觉中心推理框架**：提出一个初步解决方案，通过全局分析、物体检测和多层次证据整合来增强视觉推理能力。<br><br>## 论文方法描述<br>- **基准设计**：<br> - 数据集包含1000+图像文本对（1024×1024图像），每个图像标注数字标签，模型需输出对应标签或“None”。<br> - 三个难度级别：<br> - Easy: 工具识别（如识别切菜刀用于切蔬菜）。<br> - Medium: 工具理解，包含三个子挑战：<br> - M.1 属性理解（如选择铸铁锅因耐高温）。<br> - M.2 工具组合（如电池插入遥控器）。<br> - M.3 可用性理解（如识别破损的活塞不可用）。<br> - Hard: 工具创建（如用硬币代替平头螺丝刀）。<br>- **数据集收集**：<br> - 阶段1（概念化）：专家设计任务场景对。<br> - 阶段2（图像生成）：使用GPT-4o-image（约90%）和真实摄影（约10%），人工监督质量。<br> - 阶段3（标注验证）：自定义工具标注数字标签，多轮审核确保可靠性。<br>- **评估方法**：<br> - 使用一致文本提示，强制链式思考（CoT）推理或允许内置“思考”模式。<br> - 输出工具标签或“None”，基于准确率评分。<br>- **解决方案**：视觉中心推理代理框架：<br> - 全局分析：整体理解任务和图像上下文。<br> - 物体检测：调用DINOX工具识别并裁剪对象。<br> - 多层次证据整合：结合全局和细节分析生成答案。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：PhysToolBench，包含1000+图像文本对，涵盖日常、工业、户外和专业场景。图像由GPT-4o-image和真实摄影生成，人工标注数字标签。<br>- **模型资源**：评估32个MLLMs，包括GPT-5、o3、GPT-4o、Claude、Gemini等专有模型，Qwen、InternVL、GLM等开源模型，RoboBrain、Embodied-R1等具体化模型，以及PaliGemma、Phi-3-Vision等VLA骨干模型。<br>- **训练资源**：未训练新模型，而是使用现有模型进行评估。专有模型通过API调用，开源模型本地部署（如GLM-4.5V使用108B参数）。硬件要求未明确描述，但涉及大模型推理。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 专有模型（如GPT系列、Claude）通过各自API在线评估。<br> - 开源模型（如Qwen、InternVL）本地部署，使用统一提示。<br> - 人类基准：5名人类参与者作为参考。<br>- **评估指标**：准确率（accuracy），以百分比表示（↑表示越高越好）。<br> - 按难度级别细分：Easy、Medium（M.1/M.2/M.3）、Hard。<br> - 按场景类别细分：专业（Professional）、工业（Industrial）、户外（Outdoor）、日常（Daily）。<br> - 总体分数（Overall）基于加权或平均计算。表1显示人类最佳表现（如Easy 96.19%）与MLLMs最佳表现（如GPT-5 Overall 62.15%）对比。</details> |
| 2025-10-09 | Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered | http://arxiv.org/abs/2510.08464 | <details><summary>展开</summary>待生成</details> |
| 2025-10-09 | Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track | http://arxiv.org/abs/2510.07871 | <details><summary>展开</summary>## 论文研究单位<br>- **香港科技大学（广州）**<br>- **清华大学**<br>- **中国科学院自动化研究所**<br>- **小米电动汽车**<br><br>## 论文概述<br>论文提出了基于主动风险感知的社交导航方法，针对动态室内环境中的机器人导航任务。该方法在Falcon框架基础上创新性地引入了**主动风险感知模块（Proactive Risk Perception Module）**，通过预测周围人类的碰撞风险评分来增强机器人的空间感知能力。在IROS 2025 RoboSense挑战赛中，该方法获得**第二名**。<br><br>## 论文核心贡献点<br>1. **创新性风险感知模块**：提出基于距离的连续风险评分机制，将社交空间分为危险区、警告区和安全区<br>2. **辅助学习优化**：通过密度监督信号提升传统强化学习在社交导航中的表现<br>3. **高效集成方案**：风险模块与Falcon共享状态编码器，训练时提升导航能力，推理时零额外开销<br>4. **实际竞赛验证**：在Social-HM3D数据集上优于Falcon基线方法7.46%<br><br>## 论文方法描述<br>### 核心架构<br>- **主策略网络**：处理RGB-D观测和GPS+指南针信息，通过ResNet-50编码器提取视觉特征，2层LSTM处理时序依赖<br>- **风险感知模块**：轻量级神经网络(公式10)，基于LSTM隐藏状态预测风险分数<br>- **风险评分公式** (公式11)：<br> - 危险区(d<2m)：风险评分=1.0<br> - 警告区(2m≤d<4m)：风险评分线性衰减<br> - 安全区(d≥4m)：风险评分=0<br><br>### 训练优化<br>- **总损失函数** (公式12)：结合主导航损失、Falcon辅助损失和风险感知损失<br>- **DD-PPO算法**：在4块A40 GPU上训练75M步<br>- **风险权重β_risk=0.1**：平衡风险感知与其他目标<br><br>## 论文使用数据集和训练资源<br>### 数据集<br>- **Social-HM3D**：基于HM3D构建的844个真实室内场景<br>- **数据特性**：<br> - 人类数量按场景面积比例校准<br> - 人类运动速度0.8-1.2倍机器人速度<br> - 采用ORCA算法实现避障<br> - 训练集/验证集/测试集比例为：大规模/1000 эпизодов/500 эпизодов<br><br>### 计算资源<br>- **4块NVIDIA A40 GPU**<br>- **8并行环境训练**<br>- **约75M训练步数**<br><br>## 论文使用的评估环境和评估指标<br>### 评估环境<br>- **IROS 2025 RoboSense挑战赛**：Track 2社交导航轨道<br>- **测试数据**：私有测试集约500个未见场景<br>- **约束条件**：<br> - 仅使用RGB-D观测和里程计<br> - 无全局地图或特权信息<br> - 禁止人类位置预测器<br><br>### 评估指标<br>- **成功率和路径效率**：<br> - 成功率(SR)：到达目标1m内百分比<br> - 加权路径长度(SPL)：相对最优路径的效率评估<br>- **社交规范指标**：<br> - 个人空间合规性(PSC)：与人类保持≥0.5m距离的时长百分比<br> - 人类碰撞率(H-Coll)：发生人类碰撞的剧集百分比<br>- **总成绩计算** (公式14)：Total = 0.4×SR + 0.3×SPL + 0.3×PSC<br><br>### 竞赛结果<br>- 团队排名：**16支队伍中第2名**<br>- 总分：0.6994（较Falcon基线(0.6248)提升11.94%）<br>- 核心指标：SR=0.656 \| SPL=0.5958 \| PSC=0.8608 \| H-Coll=0.33<br>- 最佳团队差距：仅0.0028分差</details> |
| 2025-10-09 | USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots | http://arxiv.org/abs/2510.07869 | <details><summary>展开</summary>## 论文研究单位<br>- 中国科学院自动化研究所复杂系统认知与决策智能重点实验室(北京)<br>- 百度公司(北京)<br>- 中国科学院大学人工智能学院(北京)<br><br>## 论文概述<br>针对水下机器人数据稀缺、多任务泛化困难的问题，论文提出USIM，一个基于仿真的多任务视觉-语言-动作(VLA)数据集，覆盖9种场景和20项任务(561K帧，1,852条轨迹，约15.6小时)，并基于USIM训练得到水下通用VLA模型U0。U0采用多模态融合与卷积-注意力感知聚焦增强(CAP)模块，在检查、避障、扫描与动态跟踪等任务上平均成功率约80%，在挑战性移动抓取任务中较基线将机器人-目标距离缩短21.2%，证明仿真数据可有效驱动水下VLA能力形成。<br><br>## 论文核心贡献点<br>- 首个面向多任务、多场景的大规模水下VLA数据集USIM：561K帧/1,852轨迹/约15.6小时，20任务/9场景，涵盖抓取、检查、扫描、导航、跟踪、运输等。<br>- 水下通用VLA模型U0：基于Isaac-GR00T N1.5预训练，加入多模态融合(双目、压力、IMU、DVL等)与CAP模块，显式提升水下目标感知与空间理解。<br>- 建立可扩展“数据-任务”框架：仿真-数据-模型一体化管线，验证了闭环节在线评估与开环节离线评估均显著优于未微调基线，且距离指标改善21.2%。<br><br>## 论文方法描述<br>- 仿真环境：使用Stonefish构建9种水下场景(海床、海底管道、工业池、太阳能充电站、湖泊、近海工厂、现代/古代沉船等)，内置BlueROV2与机械手-夹爪；通过ROS集成与地图随机化、光照/水质变化，生成多样且逼真的视觉条件。<br>- 数据生成：自动化并行采集；每任务多episode，控制层采用PID(ROV姿态跟踪)与MoveIt(机械臂规划)；数据以10Hz录制，遵循LeRobot格式。<br>- 模型U0：<br> - 多模态融合：视觉(左/右相机)、语言、压力/IMU/DVL等状态，归一化推进器PWM与机械臂关节角作为动作空间，采用机器人中心坐标表示目标(相对位姿)，提升动态性与跨任务泛化。<br> - CAP模块：受VLM特征引导的卷积-注意力分支，强化目标检测与定位，损失为CAP的MSE与动作模块损失加权求和(推理时可关闭)。<br>- 训练与部署：基于USIM对GR00T N1.5微调，总批1024、5000步；3B参数，适配NVIDIA Jetson等嵌入式平台。<br><br>## 论文使用数据集和训练资源<br>- 数据集：USIM(561K帧/1,852轨迹/约15.6小时)；训练：526K帧/1,752轨迹；测试：35K帧/100轨迹。<br>- 场景与任务：9场景；20任务(12抓取、2管道检查、2沉船扫描、2避障导航、1动态跟踪、1运输)。<br>- 传感器与动作：双目相机、压力传感器、IMU、DVL；推进器PWM与机械臂关节角。<br>- 训练资源：基于Isaac-GR00T N1.5；批大小1024、训练步数5000；3B参数模型适配Jetson部署。<br><br>## 论文使用的评估环境和评估指标<br>- 评估方式：开环节离线评估(仿真测试集，20任务×5轨迹，共35K帧，约1小时)；闭环节在线测试(在仿真环境执行真实任务)。<br>- 评估指标：<br> - 动作误差 e_action(越低越好)<br> - 目标误差 e_target(CAP模块，衡量定位精度)<br> - 任务成功率(闭环节在线，多次试验统计)<br> - 移动抓取任务的机器人-目标距离(平均距离，越低越好)<br>- 主要结果：<br> - 未微调GR00T N1.5在e_action上远高于微调模型，证实显著领域差异。<br> - 经USIM微调后，模型在双目输入下优于单目；U0相对GR00T FT在单目与双目e_action上分别再降7.7%与4.2%。<br> - 闭环节在线：U0在7项非抓取任务上平均成功率约80%，双目优于单目，并超过GR00T FT。<br> - 移动抓取：U0(双目)将平均距离较GR00T FT缩短21.2%，显示对复杂流体力与机体-机械臂-目标交互的更好适应能力。</details> |
| 2025-10-09 | IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction | http://arxiv.org/abs/2510.07778 | <details><summary>展开</summary># 论文研究单位<br>- 哈尔滨工业大学（深圳）<br>- 南京大学<br>- 中国科学技术大学<br>- Dexmal<br><br># 论文概述<br>论文提出IntentionVLA，一个用于人机交互的具身意图推理VLA框架。当前视觉语言动作模型主要依赖显式指令映射到动作，缺乏推理密集型预训练和推理引导操作，无法在复杂现实交互中执行隐含人类意图推理。IntentionVLA通过课程训练范式和高效推理机制，首先利用精心设计的推理数据结合意图推理、空间 grounding和紧凑具身推理，为模型赋予推理和感知能力，然后在微调阶段采用紧凑推理输出作为动作生成的上下文指导，实现间接指令下的快速推理。<br><br># 论文核心贡献点<br>- 提出统一的VLA模型IntentionVLA，训练于精心策划的意图推理数据，通过两阶段范式和高效推理机制明确桥接高级语义推理与低级动作执行<br>- 在所有评估设置中显著优于最先进的VLA基线，展现强泛化能力和实时交互能力<br>- 针对现有VLA模型无法理解隐含人类意图的问题，提供解决方案<br><br># 论文方法描述<br>构建了涵盖意图推理、空间推理和紧凑推理的综合数据格式，采用高效标注流程从日常工作环境收集数据。模型基于Qwen2.5-7B构建，包含VLM骨干、可学习查询、连接器和扩散Transformer。两阶段训练：第一阶段训练VLM骨干进行意图推理和空间感知，第二阶段训练动作模块，将语义推理转化为可执行动作。推理时采用紧凑推理机制，实现0.2秒内的快速响应。<br><br># 论文使用数据集和训练资源<br>基于日常办公环境构建的具身意图推理数据集，使用GPT-4o和Florence-2等预训练模型进行数据标注。实验使用WidowX-250s机械臂配备Realsense D435i摄像头，训练了IntentionVLA、ECoT、CogACT和π₀等模型进行对比。<br><br># 论文使用的评估环境和评估指标<br>采用任务成功率作为评估指标（每任务10次试验）。评估环境包括：<br>- 分布内任务：直接指令和意图指令<br>- 分布外设置：未见指令、陌生物体操控<br>- 零样本人机交互：真实人手交互测试实时响应能力<br>实验在真实办公环境中进行，对象位置和放置姿态随机化，要求机器人完整正确完成整个交互过程。</details> |
| 2025-10-08 | WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation | http://arxiv.org/abs/2510.07313 | <details><summary>展开</summary>## 论文研究单位<br>- 北京大学多媒体信息处理国家重点实验室（计算机学院）<br>- 香港科技大学<br>- 新加坡国立大学<br>- 北京人形机器人创新中心<br><br>## 论文概述<br>WristWorld提出首个从第三人称“锚点视角”生成腕部视角视频的4D世界模型，无需首帧腕部视角输入，即可合成时空一致且几何对齐的腕部视角序列，用于数据增强与下游视觉–语言–动作（VLA）策略提升。<br><br>## 论文核心贡献点<br>- 首个两阶段4D生成框架，在仅提供锚点视图的条件下，实现腕部视角视频的时空与几何一致合成。<br>- 设计“腕部头”（wrist head）与空间投影一致性损失（SPC），从密集2D–2D对应与重建点云中监督腕部相机位姿，无需深度或外参。<br>- 引入CLIP编码的锚点视角语义与文本提示，联合几何投影条件，指导视频扩散模型合成更真实、对齐的腕部视角。<br>- 作为即插即用模块扩展单视角世界模型为多视角，无需新增腕部数据即可提供腕部视角训练样本。<br>- 在Droid、Calvin与Franka Panda上实现SOTA视频质量，显著提升VLA性能：Calvin平均任务完成长度提升3.81%，缩小锚点–腕部性能差距42.4%。<br><br>## 论文方法描述<br>- 两阶段4D生成：<br> - 重建阶段：扩展VGGT特征并通过“腕部头”与SPC损失估计腕部相机位姿与4D点云，投影得到时间对齐的条件图。<br> - 生成阶段：基于Video DiT，将腕部投影条件与CLIP锚点语义及文本共同作为条件，生成腕部视角视频；结构上支持以[腕部潜变量；条件潜变量]双通道输入。<br>- 训练目标包含标准扩散噪声预测损失与SPC投影损失（像素重投影误差+深度可行性项）。<br><br>## 论文使用数据集和训练资源<br>- 数据集：<br> - Droid：约76k视频，覆盖59个任务，含ext1/ext2与腕部相机；10k子集预训练，100视频验证。<br> - Calvin（模拟）：多任务语言条件基准，使用D split的10%数据。<br> - Franka Panda（实机）：1700演示，3静态外视角+腕部视角；保留100视频评估。<br>- 训练资源：<br> - 重建阶段：Droid预训练在8×A800 GPU约12小时，640×480分辨率，批4。<br> - 生成阶段：在8×A800 GPU约24小时，条件token长度512。<br> - Franka跨视角微调：重建6小时、生成12小时，相同批大小、分辨率与token长度。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：Droid、Calvin、Franka Panda。<br>- 视频质量指标：FVD（越低越好）、LPIPS（越低越好）、SSIM（越高越好）、PSNR（越高越好）。<br>- VLA评估（Calvin）：逐任务连续成功率的1/5–5/5统计与平均完成长度（Avg. Len.）。<br>- 消融实验：评估腕部投影、CLIP锚点语义与SPC损失的贡献。</details> |
| 2025-10-08 | TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking | http://arxiv.org/abs/2510.07134 | <details><summary>展开</summary>### 论文研究单位<br>北京大学、Galbot、中国科学技术大学、BAAI、北京航空航天大学、南方科技大学、北京师范大学联合研究。<br><br>### 论文概述<br>提出TrackVLA++，一种增强具身视觉跟踪（EVT）能力的视觉-语言-动作（VLA）模型。针对现有方法在动态场景中因缺乏显式空间推理和有效时间记忆而失效的问题，TrackVLA++通过引入极坐标思维链（Polar-CoT）机制和目标识别记忆（TIM）模块，显著提升追踪性能与鲁棒性，支持多视角扩展，并在模拟和真实场景中验证了SOTA表现。<br><br>### 论文核心贡献点<br>1. **Polar-CoT机制**：将目标相对位置编码为极坐标token，提供轻量级空间推理能力，优于传统边界框方法。<br>2. **TIM模块**：采用置信度门控记忆更新策略，抵抗遮挡和干扰物，实现长期目标识别。<br>3. **跨域泛化**：在EVT-Bench和Gym-UnrealCV等基准测试中实现SOTA，并在真实机器人任务中验证零样本泛化。<br><br>### 论文方法描述<br>- **架构**：基于导航基础模型NavFoM构建，结合双编码器提取视觉特征（SigLIP和DINOv2）。<br>- **Polar-CoT模块**：<br> - 将代理感知空间（0.6m-5.0m）离散化为60角度×30距离网格，编码为唯一词汇token（含<invalid> token处理遮挡）。<br> - 输入：视觉特征、语言token和TIM记忆；输出：紧凑reasoning token。<br>- **TIM模块**：<br> - 通过加权平均更新记忆：权重由预测置信度（归一化熵）决定，仅高置信度时融合新特征。<br> - 初始化为空状态，首次有效特征后生效。<br>- **训练流程**：<br> - 输入序列：LLM接收视觉、语言和reasoning token；输出动作token经MLP解码为8步轨迹。<br> - 损失函数：轨迹MSE损失、推理对数损失和文本损失（权重α=0.2，β=0.5）。<br>- **数据集**：混合200万样本（100万EVT-Bench跟踪数据 + 100万QA数据：SYNTH-PEDES、图像/视频QA）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 模拟：EVT-Bench（STT/DT/AT分割）、Gym-UnrealCV（Single Target/Distractor/Unseen Objects）。<br> - 真实：Unitree GO2机器人多视角RGB流（Obstacle/Winding Path/Distractor场景）。<br>- **训练资源**：<br> - 硬件：8块NVIDIA H100 GPU。<br> - 时间：约192 GPU小时（约一天）。<br> - 推理速度：4.8 FPS（对比NavFoM 5.1 FPS）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 模拟：EVT-Bench（DT分割作为主要测试）、Gym-UnrealCV。<br> - 真实：Unitree GO2四足机器人，配备4台SG3S11AFxK摄像头。<br>- **评估指标**：<br> - **SR**（Success Rate）：成功完成任务（1-3米内正确定向）的比例。<br> - **TR**（Tracking Rate）：成功跟踪时间步的比例。<br> - **CR**（Collision Rate）：因碰撞终止任务的比例。<br> - **EL**（Episode Length）：Gym-UnrealCV中的平均步长（最大500）。<br> - **识别准确率**：零样本人类识别任务（SYNTH-PEDES数据集）。</details> |
| 2025-10-08 | Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications | http://arxiv.org/abs/2510.07077 | <details><summary>展开</summary># 论文研究单位<br>- 东京大学（The University of Tokyo）<br>- 牛津大学（University of Oxford）<br>- 德克萨斯大学奥斯汀分校（The University of Texas at Austin）<br><br># 论文概述<br>- 背景：大模型（LLM/VLM）在NLP和CV快速进展，推动机器人领域的泛化与可扩展性需求；但“高级语言理解与具体动作执行”之间仍存在鸿沟，早期系统多采用“高层规划+预设动作库/模仿学习”的解耦方式，泛化受限。<br>- VLA定义：将视觉、自然语言作为必要输入，直接生成机器人控制命令的端到端系统；排除仅做高层技能选择或无动作落地的方案。<br>- 研究范围：系统综述VLA从历史演进、架构与模块、模态处理、学习范式，到数据采集、公开数据集、数据增强与评估基准的全栈方法；同时提供从业者指南。<br>- 方法：覆盖从早期CNN/MLP到Transformer/VLM主干，再到扩散/流匹配、潜在动作视频预训练与世界模型/可供性融合，以及分层/CoT推理等路线；结合多数据集与多机器人平台进行分析。<br>- 贡献：提出可落地的VLA全景与实践建议，分类并总结代表性工作，形成统一视角以支持实际部署。<br><br># 论文核心贡献点<br>- 首个覆盖软件与硬件、架构与训练、数据与评估的全栈VLA综述。<br>- 按生成策略梳理历史演进：CNN→序列Transformer→预训练VLM主干→扩散/流匹配→潜在动作学习→分层控制。<br>- 将架构系统化为传感运动模型、世界模型、可供性模型三大类，并细分七种传感运动架构变体。<br>- 总结多模态处理策略（视觉、语言、动作与附加模态）以及动作表征（离散化、连续生成、潜在动作、跨本体统一）。<br>- 归纳学习范式：监督、自监督、强化学习（含与RL组合的高层/低层策略）；给出预训练/后训练实践建议。<br>- 汇总数据采集方法、公开数据集（OXE、RT系列等）、数据增强与跨本体迁移方案。<br>- 梳理真实机器人平台、评估基准与指标，并提炼工程落地建议与未来方向（推理、安全、持续学习、评估等）。<br><br># 论文方法描述<br>- 架构与设计过渡：<br> - 早期端到端CNN/MLP（如CLIPort）：CLIP特征+Transporter网络，端到端学习操作任务，但可扩展性有限。<br> - 序列Transformer（Gato、VIMA）：统一令牌化，图像（ViT/目标检测）、语言（T5/Tokenizers）拼接为序列，自回归生成动作；在仿真中表现良好。<br> - 预训练VLM主干（RT-1/RT-2/RT-X、OpenVLA）：以VLM（PaLM-E、PaLI-X、Prismatic/DINOv2/SigLIP等）为骨干，在大规模机器人数据上联合微调，实现真实世界可扩展；RT-2/RT-X进一步提升跨环境/本体泛化。<br> - 扩散/流匹配策略（Octo、RDT-1B、π0/GR00T N1）：在Transformer后加扩散头或以扩散Transformer为骨干，生成平滑连续动作；π0引入流匹配与动作专家，实现50Hz实时控制；GR00T N1将潜在动作、扩散与流匹配整合为多阶段策略。<br> - 潜在动作视频预训练（LAPA）：从无标注人类视频中学习潜在动作，桥接人类演示与机器人动作空间，兼容真实部署。<br> - 分层与CoT推理（RT-H、π0.5、ECoT/CoT-VLA）：高层策略预测“语言动作/子任务/中间表示”，低层策略生成精细控制；或通过思维链逐步推理（生成子任务、关键点、目标图）提升长时序与复杂任务的稳健性。<br>- 核心架构类型：<br> - 传感运动模型（7类）：Transformer+离散动作、Transformer+扩散头、扩散Transformer、VLM+离散、VLM+扩散、VLM+流匹配、VLM+扩散Transformer；对应自回归、非自回归、连续控制与实时响应等不同折中。<br> - 世界模型（3类）：预测未来视觉→逆动力学生成动作；学习潜在动作以利用人类视频；端到端联合预测动作与未来观测以增强规划与泛化。<br> - 可供性模型（3类）：VLM估计可供性热图/接触点/关键姿态引导控制；从人类数据抽取可供性；与传感运动模型融合，直接生成条件动作。<br>- 模态处理：<br> - 视觉：ViT/DINOv2/SigLIP/CLIP特征，Q-Former/TokenLearner压缩；目标级表示（边界框/分割/跟踪）常与语言对齐。<br> - 语言：继承LLM分词器（T5/LLaMA等），USE/CLIP文本编码器或VLM直接融合；FiLM等条件融合常用。<br> - 动作：离散化分桶（256桶/保留低频词）、MLP解码为连续、扩散/流匹配生成连续动作、DCT/FAST降低序列长度；潜在动作（VQ-VAE/视频重建）；跨本体统一（统一视角、共享原子动作空间、异构令牌统一）。<br> - 附加模态：音频（ASR/声学编码器）、触觉（图像化触觉+ViT/TVL）、3D（深度图/多视角/体素/点云/NeRF/高斯Splatting），提升接触与空间感知。<br>- 训练与实现：<br> - 监督学习：预训练（用VLM/多源数据）→后训练（高质量机器人演示）；上下文学习用于少样本提示。<br> - 自监督：模态对齐（对比学习）、视觉自监督（MAE/CLIP/DINOv2）、潜在动作自监督（视频重建）。<br> - 强化学习：两类组合——用RL微调VLA（如SFT→RL→SFT、RPRM稠密奖励、潜空间RL避免扩散回传不稳）；或VLA做高层决策、RL做低层控制（足式/人形/移动操作）。<br> - 预训练阶段：数据规模与多样性（跨机器人/人类/合成/多任务损失）；常用VLM骨干（PaLM-E、PaLI-X、PaliGemma等）。<br> - 推理：支持非自回归并行输出、目标图像/语言指令输入、多模态条件与低时延控制；实时性通过流匹配或序列压缩优化。<br>- 数据与增强：遥感与多模态采集、统一格式（单相机+7-DoF动作），公开数据集（Open-X Embodiment、RT-1数据、多机器人/人类视频），数据增强（合成轨迹/世界模型生成/光学流与特征点跟踪/图像编辑）。<br><br># 论文使用数据集和训练资源<br>- 公开数据与平台：<br> - Open-X Embodiment (OXE)：多机器人多本体数据，统一格式，促进跨本体泛化。<br> - RT-1/RT-2数据：大规模真实世界机器人演示，涵盖数百任务与长序列。<br> - 人类视频数据集：Ego4D、EPIC-KITCHENS等，用于潜在动作与可供性抽取。<br> - 仿真数据：任务/轨迹/合成视觉样本，用于长时序与多样场景扩展。<br>- 预训练资源：<br> - 多源混合：真实机器人轨迹、互联网规模视觉语言数据、检测/推理辅助损失、合成轨迹与COSMOS等世界模型生成数据。<br> - VLM骨干：PaLM-E、PaLI-X、PaliGemma、LLaMA 2/Qwen2等；视觉编码器（DINOv2/SigLIP、EfficientNet、MAE-ViT）。<br>- 计算与训练关注：Transformer为主，序列长度与高分辨率视觉导致显存/时延压力；采用TokenLearner/Q-Former压缩、流匹配加速采样、并行非自回归解码；建议端到端微调+动作头适配的折中训练策略。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境与平台：<br> - 真实机器人：机械臂、移动操作、人形与足式机器人等多样化本体与场景。<br> - 仿真到真实：模拟环境用于训练与数据扩增，结合真实部署验证。<br>- 评估指标：<br> - 任务层：成功率、平均回报、EpLen、鲁棒性（噪声/遮挡/跨域）。<br> - 控制层：时延与频率（如50Hz）、动作平滑性与稳定性。<br> - 泛化：跨任务/对象/环境/本体的转移能力。<br> - 安全与失败检测：碰撞率、回退策略、恢复成功率。<br> - 效率：参数规模、推理时延、部署资源占用。</details> |
| 2025-10-08 | Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models | http://arxiv.org/abs/2510.07067 | <details><summary>展开</summary># 论文研究单位<br>作者单位未在提供的HTML中明确给出。<br><br># 论文概述<br>论文研究了嵌入在真实人机交互中的自然语言指令对视觉-语言-动作（VLA）模型稳健性的影响，重点考察两类指令噪声：人自然转述与插入与任务无关的上下文。论文在LIBERO和Habitat 2.0两个仿真环境中，对OpenVLA、UniAct、MoDE、π0和LLARP等SOTA VLA模型进行系统评估。发现随上下文长度增长性能显著下降；与训练集语义/词法相似的无关上下文可导致约50%质量损失；人类转述可导致约20%下降。为此提出基于大语言模型（LLM）的无关上下文过滤框架，能将性能恢复至原始的98.5%。<br><br># 论文核心贡献点<br>- 发现并量化了VLA模型在两类语言扰动下的脆弱性：人类转述与无关上下文插入，且对语义/词法接近训练命令的上下文最敏感。<br>- 表明随无关上下文长度增加，性能呈持续下降；在上下文长度接近目标命令时，质量损失可达到与语义接近型上下文相当的水平。<br>- 提出并验证了LLM过滤框架作为VLA前置预处理，能显著提升鲁棒性并恢复性能。<br><br># 论文方法描述<br>- 设计系统化的指令扰动类型：<br> - 按长度变化：单引导词、短句（3–5词）、长句（7–10词）。<br> - 按语义/词法相似度：描述型（Description）、不可执行指令（Infeasible）、位置型（Location，包含场景对象与位置引用但与后续命令无关）。<br> - 上下文插入于目标命令前后，并使最终噪声指令与训练模板在标点与大小写上保持一致以排除非相关变量。<br>- 人研究众包转述：众包工作者对各任务指令进行转述，要求语义不变；每位参与者的五条指令由五名不同工作者独立转述，耗时中位数296秒，收集后由专家审核保留语义保真样本。<br>- 过滤框架：<br> - 使用多尺寸LLM（Flan‑T5 Base、Qwen2.5‑0.5B、1.5B、3B、Llama3.2‑1B、3B、Llama‑3‑8B）以少样本提示进行过滤，去除无关上下文并恢复核心指令。<br> - 为LIBERO与LLARP分别适配过滤指令；小模型对随机上下文过滤较好，语义接近型需要≥3B；开源8B模型在绝大多数场景几乎完全恢复模板指令。<br> - 对极端场景（不可执行型前置）仍有少量残留影响；少数情况下会误删有用细节（如LLARP的初始位置描述），但在测试数据中出现概率极低。<br><br># 论文使用数据集和训练资源<br>- 仿真环境：<br> - LIBERO：四套任务（Goal、Object、Spatial、Long各10个任务；前三种为短时域，Long为长时域）。<br> - Habitat 2.0：生成100条导航+操作指令用于评估。<br>- VLA模型：OpenVLA、UniAct、MoDE、π0（用于LIBERO）；LLARP（用于Habitat 2.0）。<br>- 试验资源：<br> - LIBERO：每任务套50次试验，结果为三随机种子平均（共150次/统计量）。<br> - Habitat 2.0：LLARP并行运行在32个环境中，每任务30次试验，三随机种子平均。<br> - 众包：在英语熟练度筛选后进行转述，数据匿名化并按研究许可发布。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：LIBERO与Habitat 2.0仿真平台。<br>- 评估指标：成功率（SR）。</details> |
| 2025-10-08 | RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training | http://arxiv.org/abs/2510.06710 | <details><summary>展开</summary># RLinf-VLA: A unified and efficient framework for VLA+RL training<br><br>## 论文研究单位<br>- 清华大学、Zhongguancun Academy、Infinigence AI、北京大学、加州大学伯克利分校、哈尔滨工业大学、中国科学院自动化研究所<br><br>## 论文概述<br>- 背景与动机：VLA（视觉-语言-动作）模型大多采用SFT（监督微调），易受分布偏移影响，泛化与稳健性受限；RL（强化学习）通过交互直接优化任务表现，但现有VLA+RL工作碎片化，缺乏统一平台，难以公平对比与规模化扩展。<br>- 贡献概览：提出RLinf-VLA，一个统一且高效的VLA强化学习训练框架。通过三大GPU分配模式、统一的模型/算法/仿真器接口、细致的算法设计（优势与日志概率粒度、部分重置、动作掩码、长度归一化等），在系统与算法两端同时提效。<br>- 关键数字：在仿真中，单一统一模型在LIBERO-130（130项任务）达98.11%成功率，在ManiSkill 25项Pick-and-Place任务达97.66%；系统层面相对基线吞吐2.27×，在GPU并行仿真中采用混合细粒度流水实现1.61×–1.88×加速。<br><br>## 论文核心贡献点<br>- 统一设计与接口：支持多仿真器（ManiSkill、LIBERO）、多VLA架构（OpenVLA、OpenVLA-OFT）、多RL算法（PPO、GRPO）；统一训练/生成/仿真器接口，简化跨配置迁移与公平对比。<br>- GPU分配策略：提出colocated、disaggregated、hybrid三种模式；针对GPU并行仿真，提出hybrid + 细粒度流水，有效削减“GPU气泡”与频繁卸载开销。<br>- 算法与工程细节：优势与日志概率支持chunk/action/token三级粒度并可组合；PPO支持部分重置（提升样本效率）、轻量值网络（共享参数）；GRPO支持分组设计、有效动作掩码、轨迹长度归一化与成功率筛选。<br>- 强实证与实践准则：给出PPO/GRPO与VLA结合的最佳实践；开源并维护生态，持续更新。<br><br>## 论文方法描述<br>- GPU分配策略<br> - Colocated（共享）：训练/生成/仿真共用所有GPU，支持CPU卸载（offload），但在频繁交互场景开销大。<br> - Disaggregated（分离）：各组件分配独占GPU分区，避免内存争抢，但因依赖关系产生“GPU气泡”。<br> - Hybrid + 细粒度流水：将一个GPU上的仿真实例拆分为多个子仿真器S^(1)…S^(k)，仿真与生成并发流水，消除空闲；通过配置可切换模式，无需改动代码。<br>- 模型兼容<br> - LoRA支持：低秩适配，冻结原权重，仅训练少量参数，降低显存与成本。<br> - 模型类型：OpenVLA（~7B，连续/离散动作）与OpenVLA-OFT（连续动作空间+L1回归损失，支持并行解码与动作分块，提高推理吞吐）。<br> - 统一接口：屏蔽模型差异，一套API适配多模型。<br>- 多仿真器支持<br> - 统一接口：Gym风格reset/step、auto_reset、ignore_terminations、chunk_step（处理分块动作与边界）、可视化与固定重置状态等工具。<br> - 任务集合：ManiSkill 25项PutOnPlateInScene（OOD设置复现）；LIBERO六类任务组合为LIBERO-130（130项）。<br>- 多算法支持与设计<br> - PPO：GAE估计优势；轻量值头（共享LM参数）；chunk/action级价值估计；支持“固定episode长度”与“部分重置”两种优化目标。<br> - GRPO：以组为单位进行相对优势估计（去值函数），通过分组（相同任务+相同初始状态）、有效动作掩码、轨迹长度归一化与成功率筛选提升稳定性与效率。<br>- 优势与日志概率粒度<br> - 支持优势（chunk/action）、日志概率（chunk/action/token）组合；不兼容时采用广播（优势广播至更细粒度）。<br><br>## 论文使用数据集和训练资源<br>- 仿真环境<br> - ManiSkill：PutOnPlateInScene25Main-v3（25个抓取-放置任务）。<br> - LIBERO：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-10、LIBERO-90；统一为LIBERO-130。<br>- 训练设置<br> - 算法与模式：PPO（固定episode/部分重置），GRPO（固定episode/有效动作掩码）。<br> - 分块与解码：OpenVLA-OFT支持动作分块与并行解码，提升高频控制能力。<br> - 超参与资源：支持LoRA与不同rollout批量；建议更大rollout批量；LoRA对性能影响不显著但常需超参再调；部分重置与长度归一化提升样本效率与稳定性。<br>- GPU模式与流水<br> - Colocated/Disaggregated/Hybrid三模式切换；细粒度流水stage数量可配置；支持各组件独立offload开关。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境<br> - 仿真：LIBERO-130（跨任务类别、长视野）、ManiSkill（25项OOD抓取-放置）。<br> - 实机：Franka机器人在6个未见物体上零样本抓取-放置。<br>- 指标<br> - 成功率：LIBERO-130上98.11%；ManiSkill上97.66%。<br> - 系统效率：相比基线2.27×吞吐；在GPU并行仿真中混合细粒度流水加速1.61×–1.88×。<br> - 消融与实践：PPO中动作级价值估计优于分块级；部分重置显著提升样本效率；GRPO中长度归一化与有效动作掩码关键；成功率筛选改善稳定性；更大rollout批量有益；LoRA需调参但本身不降性能。<br> - 实机对比：RL策略零样本完成8/30次；SFT策略0/30次，体现更强泛化。</details> |
| 2025-10-07 | Verifier-free Test-Time Sampling for Vision Language Action Models | http://arxiv.org/abs/2510.05681 | <details><summary>展开</summary>根据提供的Arxiv论文HTML原文，我为您总结这篇关于Vision-Language-Action模型的无验证器测试时采样论文：<br><br>## 论文研究单位<br>KAIST（韩国科学技术院）、首尔国立大学（SNU）、RLWRLD<br><br>## 论文概述<br>Vision-Language-Action (VLA) 模型在机器人控制中展现出卓越性能，但在需要高精度的任务中仍存在根本性局限。当前方法主要依赖单次推理范式，限制了精细操作任务的表现。虽然已有测试时扩展方法使用外部验证器提升性能，但这些方法需要额外训练且泛化能力有限。本研究提出Masking Distribution Guided Selection (MG-Select)，一种新颖的VLA测试时扩展框架，仅利用模型内部属性，无需额外训练或外部模块。<br><br>## 论文核心贡献点<br>1. **提出MG-Select框架**：利用KL散度作为置信度指标，从多个候选动作中选择最优动作<br>2. **条件掩蔽分布设计**：通过随机掩蔽状态和语言条件生成参考分布，确保最大不确定性同时保持任务分布对齐<br>3. **联合训练策略**：通过dropout技术使模型学习条件和无条件分布，进一步提升参考分布质量<br>4. **显著性能提升**：在真实世界任务中实现28%的内分布和35%的外分布性能提升，在RoboCasa pick-and-place任务中实现168%相对增益<br><br>## 论文方法描述<br>### 测试时扩展框架<br>- **阶段1**：并行随机采样生成N个候选动作<br>- **阶段2**：使用特定标准进行Best-of-N选择<br><br>### 条件掩蔽分布置信度<br>- 使用KL散度测量预测分布与参考分布间的距离作为置信度指标<br>- 参考分布通过掩蔽文本、状态或两者创建，分别对应：文本掩蔽、状态掩蔽、文本&状态掩蔽<br>- 针对不同任务环境选择最优置信度变体<br><br>### 联合训练策略<br>- 训练时引入四种掩蔽变体：(qₜ,I)、(qₜ,∅)、(∅,I)、(∅,∅)<br>- 通过dropout技术增强模型对条件掩蔽分布的感知能力<br><br>## 论文使用数据集和训练资源<br>### 训练资源<br>- **硬件**：NVIDIA A100 GPU（2块）<br>- **模型**：π₀-FAST (Paligemma-3B VLM)、OpenVLA (Prismatic-7B VLM)<br>- **优化器**：AdamW，学习率2.5e-5到2.5e-6的余弦衰减<br>- **训练配置**：warmup_steps=1,000，global batch size根据数据集变化<br><br>### 使用数据集<br>- **RoboCasa**：24个家庭厨房环境中的原子任务，专注于8个pick-and-place任务<br>- **SIMPLER-WidowX**：4个pick-and-place任务，基于BridgeData V2训练<br>- **LIBERO**：多轴泛化评估，包括布局、物体和目标变化，以及长期任务<br>- **真实世界数据**：DROID数据集，基于Franka Research 3机器人<br><br>## 论文使用的评估环境和评估指标<br>### 评估环境<br>- **仿真环境**：RoboCasa、SIMPLER-WidowX、LIBERO<br>- **真实世界环境**：Franka Research 3机器人，7-DoF机械臂<br><br>### 评估指标<br>- **成功率**：以百分比表示的成功率，基于多次试验（通常为50次仿真、16-24次真实世界）<br>- **样本效率**：在30、100、300个演示样本下的性能表现<br>- **泛化能力**：内分布（ID）和外分布（OOD）任务的区分评估<br>- **效率分析**：推理延迟分析，通过单预填充策略实现45%的延迟减少<br><br>实验结果表明MG-Select在不同演示规模下均能持续改进基础模型性能，特别是在低数据场景下效果显著。</details> |
| 2025-10-07 | MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption | http://arxiv.org/abs/2510.05580 | <details><summary>展开</summary>待生成</details> |
| 2025-10-06 | StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation | http://arxiv.org/abs/2510.05057 | <details><summary>展开</summary># 论文研究单位<br>浙江大学、南京大学、香港科技大学<br><br># 论文概述<br>论文提出 StaMo 方法，旨在从静态图像中学习可泛化的机器人运动。核心思想是将视觉观测压缩为高度紧凑的状态表示（仅需 2 个 1024 维 token），并利用预训练的 Diffusion Transformer（DiT）解码器进行重建。更重要的是，在该紧凑状态空间中，通过简单的线性插值或差分即可自然得到潜在动作（latent action），可解码为可执行的控制信号，避免了对视频时序建模的依赖。该表示可无缝集成到现有 VLA 框架中进行世界建模与策略共训，在模拟与真实环境均取得显著提升。<br><br># 论文核心贡献点<br>- 提出紧凑状态表示学习框架，用 2×1024 token 编码视觉观测，并通过 DiT 解码器实现高质量重建。<br>- 发现并利用“状态差分即动作”的性质：在紧凑潜在空间中，状态之间的线性差分或插值可直接作为潜在动作，无需视频监督。<br>- 将该表示作为未来状态预测目标，集成到 OpenVLA/OFT 等 VLA 框架，提升任务成功率且推理开销极小。<br>- 引入策略共训（co-training）方案：将无标签视频的相邻帧编码为潜在动作，作为伪标签与少量带标签机器人数据联合训练，显著提升泛化与跨域迁移能力。<br>- 验证方法在 LIBERO 基准提升 +14.3%，真实任务成功率提升 +30%，并呈现良好的可扩展性与跨域（sim-to-real）迁移特性。<br><br># 论文方法描述<br>- 图像压缩与状态表示学习：采用 Diffusion Autoencoder 结构；编码器为冻结的 DINOv2 特征提取器 + Transformer 压缩器，解码器为预训练的 DiT（基于 Stable Diffusion 3）；使用 Flow Matching 目标优化，仅训练压缩器与解码器。<br>- 状态与运动的统一：运动定义为状态 token 的差分 a_t = s_{t+1} − s_t 或在潜空间中的线性插值；该潜在动作可直接解码为机器人控制信号，并用于世界模型与策略学习。<br>- 世界建模：在 VLA（如 OpenVLA/OFT）的自回归主干上添加轻量 MLP 头，预测下一状态表示；总损失为动作交叉熵与未来状态回归（MSE + L1）之和，权衡即时控制与未来预测。<br>- 潜在动作的策略共训：将相邻视频帧编码为潜在动作 m_t = E(o_{t+1}) − E(o_t)，作为伪标签，与少量真实机器人动作数据共同训练下游策略。<br><br># 论文使用数据集和训练资源<br>- 数据集：LIBERO（10/90/goal/object/spatial）、DROID、Maniskill（OOD）、Open X-Embodiment；并使用人类 egocentric 视频。<br>- 硬件与训练：8×NVIDIA H100 GPU；AdamW 优化器，学习率 3e-5（余弦退火），批大小 512/GPU，权重衰减 1e-3；训练约 10 天，PyTorch 2.1、Ubuntu 22.04；随机种子 33。<br><br># 论文使用的评估环境和评估指标<br>- 环境与基准：LIBERO 模拟环境（不同子任务 Spatial/Object/Goal/Long）；真实世界机器人实验（Franka Research 3 + UMI 夹爪 + RealSense D435，20 Hz，SE(3) 绝对末端位姿控制）。<br>- 评估指标：图像重建（PSNR、SSIM）、任务成功率（%）、线性探针 MSE（预测动作序列与真实动作的均方误差）、推理频率（Hz）、策略共训与跨域迁移表现。</details> |
| 2025-10-06 | HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks | http://arxiv.org/abs/2510.04898 | <details><summary>展开</summary>待生成</details> |
| 2025-10-05 | ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context | http://arxiv.org/abs/2510.04246 | <details><summary>展开</summary>## 论文总结<br><br>**论文研究单位：**<br>KAIST（韩国科学技术院）、RLWRLD、加州大学伯克利分校<br><br>**论文概述：**<br>针对部分可观察机器人任务中时序上下文缺失导致的策略性能不稳定问题，提出ContextVLA框架。该框架基于预训练视觉语言模型（VLM）的视觉语言动作模型（VLA），通过将历史多帧观测压缩为单一上下文令牌来高效利用时序信息，同时避免直接处理高维视频序列的计算开销。与传统多帧训练策略不同，该方法利用VLM的时序理解能力，在模拟和真实机器人任务中均显著提升策略性能。<br><br>**论文核心贡献点：**<br>1. **揭示关键瓶颈**：系统性分析发现VLA架构在利用多帧观测方面优于传统策略，原因在于预训练VLM具备的时序理解能力；<br>2. **创新压缩机制**：提出分两阶段处理策略——前半段VLM层保留多帧输入以提取时序特征，后半段将历史帧聚合为单令牌（平均池化），实现高效时序上下文融合；<br>3. **通用适配性**：支持自回归和扩散式动作解码器，可直接增强现有VLA模型（如π₀、GR00T N1.5）；<br>4. **显著性能提升**：<br> - 模拟任务：Libero基准平均提升1.9%，Simpler-WidowX提升14.4%（41.8%→56.2%）<br> - 真实任务：长时序"Pick-and-Place Twice"任务从25%提升至65%<br><br>**论文方法描述：**<br>- **观测压缩流程**：<br> 1. 用视觉编码器处理8帧历史观测得到视觉特征<br> 2. 在VLM第2层对历史帧隐藏状态执行平均池化生成上下文令牌m<br> 3. 用m替代后续VLM层中的历史帧令牌，仅保留当前帧令牌<br>- **动作生成**：上下文令牌与VLM当前层特征共同输入动作解码器，支持自回归（令牌化动作）或扩散式生成<br>- **高效推理**：结合因果注意力掩码与KV缓存机制，在t-1步预计算历史令牌和KV缓存，t步仅处理当前观测和上下文令牌<br><br>**论文使用数据集和训练资源：**<br>- **模拟数据集**：Libero（40个任务）、Simpler-WidowX（4任务）、Robocasa（24任务）<br>- **真实数据集**：Bridge v2、3类长时序操作任务（抓取-放置-两次、覆盖-堆叠）<br>- **训练配置**：<br> - 60K迭代，批量32，优化器AdamW<br> - 8帧历史观测，在第2层VLM块执行压缩（n=2）<br> - 硬件：NVIDIA A100 80GB GPU（4卡并行训练，单卡推理）<br><br>**论文使用的评估环境和评估指标：**<br>- **环境**：<br> - 模拟任务：Libero（4子基准）、Simpler-WidowX（4任务）、Robocasa（24任务）<br> - 真实机器人任务：3类时序敏感操作（抓取-放置序列、手部开合等）<br>- **指标**：<br> - **性能**：任务成功率（%）<br> - **效率**：训练墙钟时间（基于π₀在Libero的60K迭代训练）、推理延迟（ms/8帧2视角输入）<br>- **关键结果**：<br> - 在Simpler-WidowX实现14.4%平均提升<br> - 推理时间从227.2ms（未压缩）降至96.3ms（压缩+KV缓存）<br> - 长时序真实任务成功率最高达80%<br><br>---<br>注：该论文在ICLR/NeurIPS等多帧机器人策略研究基础上，通过VLM时序建模和计算优化，首次系统解决了多帧观测在VLA中的高效利用难题。</details> |
| 2025-10-05 | SITCOM: Scaling Inference-Time COMpute for VLAs | http://arxiv.org/abs/2510.04041 | <details><summary>展开</summary>待生成</details> |
| 2025-10-04 | Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert | http://arxiv.org/abs/2510.03896 | <details><summary>展开</summary>待生成</details> |
| 2025-10-04 | NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation | http://arxiv.org/abs/2510.03895 | <details><summary>展开</summary># 论文研究单位<br>浙江大学<br><br># 论文概述<br>NoTVLA（Narrowing of Trajectory Vision-Language-Action）是一个针对通用机器人操作的框架，旨在解决视觉-语言-动作（VLA）模型中的灾难性遗忘问题。该框架通过将密集的动作轨迹压缩为稀疏的轨迹表示，避免了密集轨迹微调带来的知识遗忘，同时显著降低了计算成本。方法采用三阶段流程：首先通过锚点预测进行深度推理，然后基于运动学原理选择关键帧，最后使用样条基动作去令牌化器生成平滑的高频轨迹。<br><br># 论文核心贡献点<br>- 解耦高层VLM与低层动作专家，在减少微调计算的同时提高具身任务成功率<br>- 使用稀疏且语义修剪的轨迹监督，增强跨平台和跨任务的泛化能力，缓解灾难性遗忘<br>- 保留模型内在的视觉语言推理能力，支持复杂指令跟随和多轮交互，无需额外任务特定微调<br>- 在训练效率上显著优于现有方法，使用计算资源比π₀少一个数量级以上<br><br># 论文方法描述<br>**锚点预测与令牌生成**：通过锚点预测模块（APP）输出2D锚点，结合外部深度源获取深度信息，在锚点条件化令牌生成（ACTG）中输出包含深度、图像坐标、夹爪状态和姿态的多模态令牌序列。<br><br>**运动学基关键帧选择**：基于末端执行器的加速度阈值和夹爪状态变化来识别关键帧，将原始演示分割成逻辑阶段，并通过子关键帧和均匀下采样来保持时间相干性。<br><br>**样条基动作去令牌化器**：将离散令牌序列转换为平滑的高频机器人轨迹。使用三次样条插值处理XYZ位置，球面线性插值（SLERP）处理四元数姿态，确保位置和方向的高频平滑轨迹。<br><br># 论文使用数据集和训练资源<br>- **数据来源**：ManiSkill（3000条轨迹）、RoboTwin 2.0（约2000条轨迹）、AgiBot World（500条轨迹）<br>- **训练平台**：统一模拟混合环境，包含40个任务的多平台机器人数据<br>- **硬件要求**：在32 GPU小时下完成训练，即使在约8 GPU预算下也能展现执行能力<br>- **模型规模**：基于Qwen VL 2.5（7B）参数模型<br>- **多平台支持**：Aloha-AgileX、ARX-X5、Franka Panda、UR5、Piper、AgiBot-G1<br><br># 论文使用的评估环境和评估指标<br>- **基准测试**：RoboTwin 2.0官方基准、AGIBOT挑战赛官方评估<br>- **主要指标**：任务成功率（0-1之间），与专家模型（ACT、DP、DP3）和通用模型（π₀、RDT）对比<br>- **泛化测试**：零样本泛化评估，包括未见指令、背景变换、颜色提示反转等场景<br>- **轨迹质量评估**：几何和时间指标，包括F1分数、DTW距离、Fréchet距离、Hausdorff距离<br>- **跨视角评估**：训练域内（ID）与训练域外（OOD）视角性能对比<br>- **多任务综合性能**：在AGIBOT挑战的10个官方任务中进行测试，总分从2.795提升至3.697</details> |
| 2025-10-04 | LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization | http://arxiv.org/abs/2510.03827 | <details><summary>展开</summary>待生成</details> |
| 2025-10-02 | Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer | http://arxiv.org/abs/2510.03342 | <details><summary>展开</summary>待生成</details> |
| 2025-10-03 | MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning | http://arxiv.org/abs/2510.03142 | <details><summary>展开</summary>### 论文研究单位<br>- 北京大学（Peking University）<br>- Galbot<br>- 上海交通大学（Shanghai Jiao Tong University）<br>- 清华大学（Tsinghua University）<br>- BAAI（可能指北京人工智能研究院）<br><br>### 论文概述<br>论文提出MM-Nav，一个多视图视觉语言行动（VLA）模型，旨在通过多专家学习实现鲁棒的视觉导航。核心思想是结合合成环境中的多样导航数据与VLA模型的泛化能力，解决视觉导航中观察数据建模困难的问题。MM-Nav使用360度视觉观察（四个水平分布的相机视图），通过两阶段训练学习：先从强化学习（RL）专家收集数据预训练，再进行在线教师-学生迭代训练。方法最终在模拟和真实环境中展示了强泛化能力，并优于单一能力RL专家。<br><br>### 论文核心贡献点<br>1. 提出MM-Nav多视图VLA模型，支持360度视觉输入，直接输出连续速度命令。<br>2. 设计三种RL专家（reaching、squeezing、avoiding），分别学习导航能力。<br>3. 两阶段训练：离线数据预训练和在线迭代训练（DAgger方式），引入能力平衡数据聚合策略（基于性能差距动态调整数据比例）。<br>4. 在模拟和真实环境中评估，模型性能超越RL教师，验证多能力学习的协同效应。<br>5. 提供开源实现和项目页面，促进研究社区应用。<br><br>### 论文方法描述<br>- **方法概述**：采用教师-学生范式。学生模型是VLA（基于SigLIP视觉编码器和Qwen2语言模型），处理多视图RGB输入（四个相机），并预测速度命令。教师模型为三个RL专家（训练于不同合成环境，使用特权深度信息）。训练分两阶段：离线预训练（用专家数据初始化VLA）和在线迭代（VLA部署后收集专家数据，动态平衡训练比例）。<br>- **RL专家**：三个专家（reaching：到达目标；squeezing：穿行狭窄通道；avoiding：避开动态障碍）。使用PPO算法、深度图像输入（四个相机）、奖励函数针对能力调整。<br>- **VLA学生模型**：视觉编码（SigLIP输出视觉令牌；历史滑窗处理），语言提示（点目标转化为文本），动作预测（输出速度[v_x, v_y, v_yaw]）。<br>- **在线训练**：能力平衡数据聚合（计算VLA与专家的加权旅行时间（WTT）差距，调整数据比例），迭代优化直至收敛。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：合成环境数据（IsaacLab生成）：用于训练RL专家的三个能力特定场景（reaching、squeezing、avoiding），收集500k步专家数据；真实环境测试数据（四个场景：Narrow Zigzag Corridor、Thin Obstacle Avoidance、Dynamic Environment、Cluttered Static Environment）。<br>- **训练资源**：<br> - RL专家训练：IsaacLab模拟，NVIDIA RTX 4090 GPU，单次训练8-12小时。<br> - VLA预训练：8个NVIDIA H100 GPU，约5小时（40 GPU小时）。<br> - 在线迭代：每次迭代2小时（200k专家数据）。<br> - 部署：服务器配备NVIDIA RTX 5090 GPU，机器人端（Unitree GO2）实时推理。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 模拟环境：IsaacLab中三个能力特定场景（reaching、squeezing、avoiding）和一个混合场景（包含所有能力）。<br> - 真实环境：四个场景测试sim-to-real泛化。<br>- **评估指标**：<br> - 成功率（SR）：成功到达目标的百分比。<br> - 碰撞率（CR）：发生碰撞的百分比。<br> - 加权旅行时间（WTT）：成功到达目标的平均时间除以成功率。<br>- 对比基线：iPlanner、ViPlanner、NavDP等，结果显示MM-Nav在SR和CR上优于基线，WTT更低。</details> |
| 2025-10-03 | Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation - Technical Report for IROS 2025 RoboSense Challenge Track 4 | http://arxiv.org/abs/2510.02728 | <details><summary>展开</summary>待生成</details> |
| 2025-10-02 | Contrastive Representation Regularization for Vision-Language-Action Models | http://arxiv.org/abs/2510.01711 | <details><summary>展开</summary>待生成</details> |
| 2025-10-02 | FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models | http://arxiv.org/abs/2510.01642 | <details><summary>展开</summary>### 论文研究单位<br>南洋理工大学（Nanyang Technological University）、A*STAR前沿人工智能研究中心（Centre for Frontier AI Research, A*STAR）、艾伦人工智能研究所（Allen Institute for AI）、华盛顿大学（University of Washington）。<br><br>### 论文概述<br>论文提出FailSafe框架，用于视觉-语言-动作（VLA）模型的故障推理和自动恢复。VLA模型在机器人操作任务中表现良好，但现有数据集缺乏故障和恢复数据，导致模型无法应对执行中的失败。FailSafe通过在仿真中自动生成故障场景和可执行恢复动作，构建大规模数据集，并将LLaVa-OneVision-7B微调为FailSafe-VLM。实验表明，FailSafe-VLM能检测故障和生成恢复动作，并显著提升多个VLA模型在ManiSkill任务中的性能，同时具备跨视角、对象和机器人本体的泛化能力。<br><br>### 论文核心贡献点<br>- 首次提出FailSafe框架，自动生成故障推理和可执行恢复动作，支持任意仿真任务。<br>- 构建FailSafe数据集，使VLM和VLA模型具备故障推理能力，并提升性能，泛化到不同视角、空间配置、对象和机器人。<br>- 开源FailSafe代码，促进社区开发更稳健的具身智能系统。<br><br>### 论文方法描述<br>FailSafe流水线包括四个阶段：<br>1. **故障生成**：定义三种故障模式（translation、rotation、no-ops），通过YAML配置在仿真中随机注入扰动，将任务轨迹扰动为失败场景。<br>2. **动作收集**：从失败和正确轨迹中映射偏差姿态Pd到正确姿态Pc，生成多个候选恢复动作ΔA（7-DoF差值），避免碰撞。<br>3. **系统验证**：回放轨迹验证ΔA能否使失败恢复为成功，仅通过验证的ΔA被纳入数据集。<br>4. **指令微调**：使用FailSafe数据集微调LLaVa-OneVision-7B，得到FailSafe-VLM。训练设置：32个H100 GPU，DeepSpeed ZeRO 3，学习率1e-5（视觉塔2e-6），余弦退火warmup 3%，bfloat16/TF32。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：FailSafe数据集（131k失败-动作对 + 56k真值轨迹），来自ManiSkill仿真中三个任务（pick cube、push cube、stack cube），含多视角图像（front、side、hand）。<br>- **训练资源**：32个H100 GPU（DeepSpeed ZeRO 3），初始化自LLaVa-OV-7B单图像检查点，语言主干Qwen2-7B-Instruct，视觉塔SigLIP，两层GELU MLP投影器。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：ManiSkill仿真平台，使用Franka Emika Panda机器人臂；测试种子生成未见空间配置；评估不同对象（sphere、charger）和机器人（xArm 6）的泛化能力。<br>- **评估指标**：<br> - **VLM比较**：二元成功率（区分失败/成功）、准确性（识别故障类型）、余弦相似度（预测ΔA与真值ΔA相似度）。<br> - **VLA模型**：成功率提升（有无FailSafe-VLM的对比），在三个ManiSkill任务（pick cube、push cube、stack cube）上报告平均改进。</details> |
| 2025-10-02 | VLA-R1: Enhancing Reasoning in Vision-Language-Action Models | http://arxiv.org/abs/2510.01623 | <details><summary>展开</summary>待生成</details> |
| 2025-10-01 | INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models | http://arxiv.org/abs/2510.01389 | <details><summary>展开</summary>待生成</details> |
| 2025-10-01 | Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition | http://arxiv.org/abs/2510.01068 | <details><summary>展开</summary>待生成</details> |
| 2025-10-01 | HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy | http://arxiv.org/abs/2510.00695 | <details><summary>展开</summary>### 论文研究单位<br>KAIST、UC Berkeley、RLWRLD<br><br>### 论文概述<br>现有视觉-语言-行动（Vision-Language-Action, VLA）模型依赖当前观察进行行动预测，忽略历史上下文，导致在需要长期推理的任务中性能受限。本论文提出HAMLET框架，通过引入记忆机制使VLA能够利用历史信息。具体方法包括：<br>- 使用可学习的"moment tokens"压缩每个时间步的感知信息，并通过时间对比学习初始化以突出时序特征。<br>- 集成轻量级"memory module"聚合历史moment tokens，生成行动预测的条件特征。<br>实验表明，HAMLET在真实世界长时序任务和仿真基准上显著提升性能，且无需修改原始VLA架构。<br><br>### 论文核心贡献点<br>- 动机分析：现有VLA假设当前观察独立，限制了在历史依赖任务（如遮挡或长期操作）中的表现。<br>- 方法创新：提出HAMLET框架，通过moment tokens和memory module增强历史感知。<br>- 实验验证：跨多VLA backbone（GR00T N1.5、CogACT）验证有效性，在真实世界任务平均提升47.2%，仿真基准（RoboCasa、LIBERO）也获益。<br>- 通用性：backbone-agnostic设计，plug-and-play适配，无需额外预训练。<br>- 效率优化：避免多帧输入带来的计算开销，内存占用仅为多帧基线的约2倍。<br><br>### 论文方法描述<br>HAMLET包含两个核心组件：<br>- **Context compression via moment tokens**：在每个时间步t，附加可学习向量$\mathbf{m}_t$到VLM输入，通过冻结VLM进行时间对比学习初始化（使用同一时间步的正样本和不同时间步的负样本），使moment tokens捕捉时序判别特征，过滤静态背景。<br>- **Memory consolidation via memory module**：使用2层Transformer处理堆叠的近T个moment tokens（如$\mathbf{M}' = [\mathbf{m}'_{t-k(T-1)}, ..., \mathbf{m}'_t]$），通过因果自注意力生成历史增强特征$\tilde{\mathbf{m}}'$，与VLM表示$\mathbf{h}_t$拼接后输入行动专家，预测k步行动。<br>训练流程：冻结VLM进行moment tokens初始化（30k步），随后联合训练行动预测（60k步）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 真实世界任务：3个桌面任务（Pick-and-Place Twice、Cover-and-Stack、Swap Cubes），每任务50个演示，平均约268帧/轨迹。<br> - 仿真基准：<br> - RoboCasa Kitchen：24个任务，训练演示30/100/300每任务。<br> - LIBERO：40个任务，分4套件（Spatial、Object、Goal、Long）。<br> - SimplerEnv-Bridge：基于WidowX机器人，使用BridgeV2数据集。<br>- **训练资源**：使用NVIDIA A100 GPU进行训练和测量（延迟和内存）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 真实世界：Franka Research 3机器人+Robotiq 2F-85 gripper，双视角摄像头。<br> - 仿真：RoboCasa Kitchen（Franka机器人）、LIBERO（Franka）、SimplerEnv-Bridge（WidowX）。<br>- **评估指标**：<br> - 成功率和部分成功率（如Pick-and-Place Once、Cover Cube），跨任务和基准报告。<br> - 效率指标：延迟（毫秒）和峰值内存使用（MB），在RoboCasa数据集上测量。</details> |
| 2025-10-01 | Hybrid Training for Vision-Language-Action Models | http://arxiv.org/abs/2510.00600 | <details><summary>展开</summary>待生成</details> |
| 2025-10-01 | VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators | http://arxiv.org/abs/2510.00406 | <details><summary>展开</summary>### 论文研究单位<br>西湖大学、浙江大学、复旦大学、OpenHelix团队、北京邮电大学、郑州大学、河北工业大学等<br><br>### 论文概述<br>论文提出 VLA-RFT（Vision-Language-Action 强化微调），用学习到的世界模型作为可控模拟器为 VLA 提供验证奖励，在模拟器中对策略进行轨迹级强化优化，以更低样本复杂度提升 VLA 的泛化与鲁棒性。<br><br>### 论文核心贡献点<br>- 世界模型作为数据驱动的可交互模拟器，基于动作预测未来视觉观察，支持策略多 rollout 生成与反馈<br>- 轨迹级“验证奖励”，通过将生成的视觉轨迹与目标达成参考轨迹对比获得密集且动作对齐的学习信号<br>- 将流匹配扩展为 SDE 策略（SDE-Policy），引入 Sigma Net 表征随机性；结合 GRPO 进行高效稳定微调<br>- 只需约 400 步强化微调即可显著优于强监督基线，并在扰动场景下展现更强鲁棒性<br><br>### 论文方法描述<br>两阶段训练：Stage I 预训练世界模型与 VLA；Stage II 在世界模型中交互 rollout、计算验证奖励并用 GRPO 优化 VLA。<br>- 世界模型：基于 LLaMA 架构的轻量自回归视频预测模型（138M 参数），以最大似然训练<br>- VLA 预训练：VLM 编码器 + 流匹配动作头，MSE 目标稳定输出动作片段<br>- SDE-Policy：在流匹配基础上加入 Sigma Net（输出方差），将 ODE 扩展为 SDE；在 K=10 步扩散积分中计算平均对数似然并形成策略比 r<br>- 验证奖励：将世界模型生成的轨迹与离线专家轨迹对齐，定义为 L1 与 LPIPS 的负加权和；同起点 rollout 的奖励做组内平均以减方差<br>- 目标函数：GRPO 损失 + 流匹配 MSE 辅助项 + 熵正则，确保高效与稳定<br><br>### 论文使用数据集和训练资源<br>- 数据集与基准：LIBERO（Spatial、Object、Goal、Long 四套任务）<br>- 基础策略：轻量 VLA-Adapter；先进行监督微调，再进行强化微调<br>- 世界模型：138M 参数的自回归模型，基于 LLaMA 架构，在 LIBERO 上预训练<br>- 训练资源：4× A800 GPU；采用 VERL 分布式强化框架与 FSDP 切分训练<br><br>### 论文使用的评估环境和评估指标<br>- 世界模型评估：像素误差（MSE）、峰值信噪比（PSNR）、结构相似性（SSIM）、感知距离（LPIPS）<br>- 策略评估：成功率（SR）在标准套件与扰动套件；扰动包含物体/目标位置、机器人状态及组合扰动<br>- 关键结果：约 400 步 RFT 将平均 SR 从 86.6% 提升至 91.1%，且在各类扰动下保持更高稳定性；世界模型具备高保真视觉预测能力，显著低于监督扩展 SFT 所需迭代量</details> |
| 2025-09-30 | MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation | http://arxiv.org/abs/2509.26642 | <details><summary>展开</summary># 论文研究单位<br>- 北京大学多媒体信息处理国家重点实验室<br>- 北京人形机器人创新中心<br>- 香港中文大学（CUHK）<br><br># 论文概述<br>本论文提出了一种多感觉语言-动作模型（MLA），用于机器人的多模态理解和预测。MLA通过无编码器的多模态对齐机制，直接整合2D图像、3D点云和触觉信号，并利用未来多感觉生成后训练策略增强物理动态理解。在复杂、接触丰富的真实世界任务中，MLA在成功率上分别超越之前的SOTA 2D和3D VLA方法12%和24%，同时展示了在未见配置下的强泛化能力。模型基于LLM主干（如LLaMA-2 7B），通过渐进式训练管道（预训练、监督微调、后训练）实现感知、理解和动作生成的统一。<br><br># 论文核心贡献点<br>1. **无编码器的多模态对齐机制**：MLA将LLM自身重新用作感知模块，通过标记级对比学习对齐图像、点云和触觉标记，利用位置引导的一致性约束。<br>2. **未来多感觉生成后训练策略**：模型联合预测未来图像、点云和触觉状态，增强物理动态建模和动作生成条件。<br>3. **渐进式训练配方**：通过预训练（570K+轨迹）、监督微调（SFT）和后训练，实现SOTA性能和泛化，覆盖真实世界单臂和双臂任务。<br>4. **真实世界和仿真验证**：在RLBench仿真器上评估并实现竞争性能，验证方法的稳健性。<br><br># 论文方法描述<br>MLA方法包括以下组件：<br>- **MLA架构**：<br> - **多模态标记器**：图像标记器（14×14 patches，256 tokens）、3D点云标记器（FPS、KNN，256 tokens）、触觉标记器（MLP编码器，单token）。<br> - **LLM主干**：基于LLaMA-2 7B，处理统一标记序列，支持扩散动作生成。<br> - **未来预测解码器**：变换器解码器，分别预测未来图像（MSE损失）、点云（Chamfer距离）和触觉信号（MSE损失）。<br>- **无编码器多模态对齐**：通过位置映射构建跨模态正对（图像-点云-触觉），应用标记级InfoNCE损失（温度τ），使用8层变换器输出优化。<br>- **未来多感觉生成**：预测关键帧未来状态（由机器人关节速度触发），针对图像、点云和触觉联合训练，增强语义、几何和交互理解。<br>- **训练配方**：预训练（SFT阶段前）：仅图像-动作数据，10轮训练；SFT：引入多模态输入，对比损失；后训练：添加未来生成监督。总损失为扩散损失（DDPM）、对比损失和未来生成损失之和。<br><br># 论文使用数据集和训练资源<br>- **预训练数据集**：570K轨迹，结合28个开放源数据集（如Open-X-Embodiment、RoboMIND、DROID等），采样率详见附录Table III，总计36M帧。<br>- **自收集真实世界数据**：6个任务（单臂4个：盖章、擦白板、放盘子、放鸡蛋；双臂2个：舀玉米粒、开锅盖），每任务200个高质量演示，使用Gello平台采集。<br>- **仿真数据**：RLBench任务（10个），每任务100演示轨迹。<br>- **训练资源**：AdamW优化器；SFT：300轮，后训练：100轮；基础模型初始化自Prismatic VLM；推理使用DDIM（4步）。<br><br># 论文使用的评估环境和评估指标<br>- **真实世界评估**：Frank a Research 3单臂和双臂设置，配备RealSense D455相机（第三人和手腕视角）和Tactile传感器。评估接触丰富任务，每个任务15次 rollout，成功率由人工判定。指标：成功率（S.R.）和方差。<br>- **仿真评估**：基于RLBench（CoppeliaSim），图像和点云数据，20次 rollout per task，成功率由内置评估模块计算。指标：平均成功率（%）和方差。<br>- **泛化测试**：未见物体（如生菜代替鸡蛋）和未见背景（杂散场景），对比π_0基线，量化性能下降（如-15%）。<br>- **消融研究**：无编码器对齐（标记级vs图像级）、对比损失层位置、生成模态贡献等，指标为任务成功率变化。</details> |
| 2025-09-30 | Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA | http://arxiv.org/abs/2509.26251 | <details><summary>展开</summary>### 论文研究单位<br>- 主要单位：清华大学深圳国际研究生院（Tsinghua Shenzhen International Graduate School, Tsinghua University）、阿里巴巴集团高德地图（Amap, Alibaba Group）<br>- 合作单位：西安交通大学软件学院（School of Software Engineering, Xi’an Jiaotong University）<br><br>### 论文概述<br>论文针对潜在行动模型（LAMs）在视觉-语言-行动（VLA）系统中的两个核心瓶颈：空间理解不足（仅依赖RGB编码，忽视几何结构）和时序感知有限（依赖稀疏双帧输入，忽略长时动态），提出了一种名为SSM-VLA（Seeing Space and Motion - Vision-Language-Action）的框架。SSM-VLA通过融合几何感知空间编码、多尺度时序建模和视觉思维链推理，增强了VLA系统的鲁棒性和可解释性。<br><br>### 论文核心贡献点<br>1. **Farsighted-LAM模型**：通过DINOv2特征的几何感知空间编码和多尺度时序建模，提升潜在行动的空间-动态表示能力。<br>2. **SSM-VLA架构**：集成Farsighted-LAM与视觉思维链（VisualCoT）推理模块，实现结构化感知与显式推理的结合，增强决策一致性和可解释性。<br>3. **实验验证**：在仿真（CALVIN ABC-D基准）和真实环境中取得最先进性能，证明几何建模、时序一致性和显式推理的有效性。<br><br>### 论文方法描述<br>- **Farsighted-LAM**：<br> - 编码器：处理当前RGB帧和多个未来关键帧（DINOv2特征），通过潜在行动查询生成离散潜在行动序列。<br> - 解码器：重构未来观测（RGB和深度），采用多模态重建损失（L2和LPIPS损失、梯度感知深度损失）监督。<br>- **VLA策略**：<br> - 三阶段推理：视觉思维链预测未来观测 → 远视潜在行动推理 → 条件流匹配生成行动。<br> - 多模态协同注意：统一变换器内实现，结构化注意机制管理信息流。<br>- **关键组件**：几何先验（深度监督）、视觉思维链推理、潜在行动量化。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - CALVIN基准：仿真环境（Franka Panda机械臂），训练集为A/B/C环境，测试集为未见环境D。<br> - 真实世界数据：基于Open-X-Embodiment数据集预训练，在50个人类演示上微调（AgileX Piper机器人）。<br>- **训练资源**：<br> - 潜在行动模型：AdamW优化器，学习率10^-4，权重衰减10^-5，批大小256，训练步100，代码本大小32。<br> - VLA模型：AdamW优化器，学习率10^-3，权重衰减10^-4，批大小64，训练步30，损失权重λ_vision=0.1，λ_latent=0.01。<br> - 流程匹配：10步去噪。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - CALVIN仿真基准：34个操控任务，语言指令，1000个指令链（每链5个连续任务）。<br> - 真实世界实验：AgileX Piper机器人执行放置任务（粉色球入盒），环境包括杂乱背景。<br>- **评估指标**：<br> - CALVIN基准：连续任务完成率（1-5步）和平均成功链长（与基线对比）。<br> - 消融研究：验证Farsighted-LAM结构、多模态注意和几何先验的贡献。<br> - 真实世界：定性展示泛化能力（视觉重建对齐）。</details> |
| 2025-09-30 | TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses | http://arxiv.org/abs/2509.25746 | <details><summary>展开</summary>### 论文研究单位<br>小米机器人（Xiaomi Robotics）<br><br>### 论文概述<br>针对机器人灵巧手抓取执行阶段的“最后一公里”精度问题，论文提出TacRefineNet——首个仅依靠多指指尖触觉反馈实现已知物体任意姿态间精确调整的框架。该方法通过迭代调整末端执行器姿态，利用高分辨率压阻式触觉传感器捕获指尖接触力，实现毫米级抓取精度。为跨越仿真到现实的鸿沟，研究团队构建了MuJoCo仿真环境与真实机器人结合的数据集，并设计了多分支融合策略网络。<br><br>### 论文核心贡献点<br>1. **首个纯触觉驱动框架**：提出TacRefineNet，实现六自由度手部姿态的毫米级精确调整，完全依赖指尖触觉反馈<br>2. **多分支融合策略网络**：通过融合多指触觉信号与本体感受，设计支持任意姿态间转换的策略网络，无需逐姿态重训练<br>3. **仿真-现实高效结合**：采用仿真预训练+现实微调的混合训练策略，仅用少量真实数据即显著提升性能<br><br>### 论文方法描述<br>- **迭代调整策略**：在多指建立接触后，根据触觉图像与目标图像的差异，预测末端执行器的六自由度位姿增量（Δx），通过重复抓取-调整过程收敛至目标姿态<br>- **触觉传感系统**：指尖集成11×9压阻式触觉阵列（像素间距1.1mm），实时输出触觉图像用于特征提取；在MuJoCo中建立物理基触觉模拟，弹性接触点模拟真实传感器弹性行为<br>- **TacRefineNet网络架构**：<br> 1. 多分支视觉编码器处理每根手指的当前/目标触觉图像<br> 2. 融合多指触觉特征与关节位置信息<br> 3. 三层MLP回归六自由度位姿增量<br>- **跨组合训练方案**：随机配对当前与目标触觉图像构建N×N组合，增强策略对任意目标姿态的适应性<br>- **数据增强**：对触觉图像实施缩放噪声，对关节位置添加高斯噪声提升鲁棒性<br><br>### 论文使用数据集和训练资源<br>- **仿真数据集（Ds_img）**：基于MuJoCo物理引擎生成，在限制维度范围内系统采样手部姿态（pitch/roll/y/z轴），记录对应触觉图像、关节位置及目标姿态<br>- **真实数据集（Dr_img）**：复现实实验证中的可行姿态，增大采样步长降低数据密度，剔除实际碰撞姿态<br>- **混合训练策略**：对比两种政策<br> - 政策A：仅仿真数据训练<br> - 政策B：仿真预训练 + 真实数据微调<br>- **硬件资源**：<br> - 仿真：MuJoCo物理引擎<br> - 实物：11自由度灵巧手 + 11×9指尖触觉传感器阵列（0-255离散力值）<br><br>### 论文使用的评估环境和评估指标<br>- **实验场景**：<br> - 多维度姿态调整：16种初始-目标姿态组合（pitch/roll/y/z轴对称设置）<br> - 动态目标跟踪：连续扰动下的长期姿态维持任务<br> - 未见物体泛化：相似几何但不同形态的物体测试<br>- **评估指标**：<br> 1. **精度指标（Metric 1）**：10步调整后的6DOF误差<br> - 位置误差：δ_pos = \|\|p - p_g\|\|₂<br> - 角度误差：δ_rot = 2arccos(\|⟨Q, Q_g⟩\|)<br> 2. **效率指标（Metric 2）**：达到精度阈值的步数<br> - 位置阈值：ε_pos = 0.005m<br> - 角度阈值：ε_rot = 0.05rad<br> 3. **成功率（Metric 3）**：R=5次试验的成功率<br> - 成功判定：同时满足δ_pos ≤ ε_pos、δ_rot ≤ ε_rot，且调整步数≤S_max<br>- **关键结论**：政策B在所有指标上显著优于政策A，最佳组合实现1.1mm位置精度/0.016rad角度精度，100%成功率。16种组合均达毫米级位置精度，最优姿态误差0.009rad。动态场景验证稳定性，未见物体在滚转维度表现出较好泛化性。<br><br>> 注：论文方法在当前对象范围内有效，未来需融合视觉信息突破单对象限制。</details> |
| 2025-09-30 | VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning | http://arxiv.org/abs/2509.25718 | <details><summary>展开</summary># VLA模型通过动作分块PPO和自监督行为克隆的后训练<br><br>## 论文研究单位<br>- 中国科学院自动化研究所多模态人工智能系统国家重点实验室<br>- 中国科学院大学人工智能学院<br><br>## 论文概述<br>本文提出了一种基于动作分块近端策略优化（PPO）和自监督行为克隆的VLA（视觉-语言-动作）模型后训练方法。该方法针对强化学习在VLA模型后训练中面临的稀疏奖励和不稳定训练挑战，通过将连续动作聚合为动作分块来提高策略的时间一致性和反馈密度，同时结合动态更新的演示缓冲区进行自监督行为克隆辅助训练。<br><br>## 论文核心贡献点<br>1. 开发了用于VLA模型后训练的动作分块PPO算法，增加了有效的信息反馈密度<br>2. 构建了基于动态演示缓冲区的自监督行为克隆辅助损失，在训练过程中持续收集高质量任务轨迹<br>3. 实验表明，该方法仅用10个演示就超过了使用100个演示的监督微调性能，成功率达0.93，步数减少到42.17<br><br>## 论文方法描述<br>### 动作分块PPO<br>- 将连续动作聚合为长度为h≥1的动作分块，提高奖励反馈频率<br>- 在actor-critic架构上实现PPO框架，使用截断代理目标限制策略差异<br>- 通过广义优势估计（GAE）计算优势函数<br><br>### 自监督行为克隆<br>- 初始化时使用专家轨迹构建动态演示缓冲区<br>- 训练过程中将高质量成功轨迹加入缓冲区<br>- 使用轨迹长度作为质量代理指标，较短轨迹表示更高质量<br>- 构建辅助监督损失L_BC进行策略优化<br><br>### 在线后训练<br>- 总损失函数为加权组合：L_online = β_t * L_PPO + L_BC<br>- β_t采用渐进调度：β_t = tanh(t/T_warmup)，实现从监督学习到强化学习的平滑过渡<br>- 早期训练完全由行为克隆驱动，随训练进展逐步强调PPO目标<br><br>## 论文使用数据集和训练资源<br>- **数据集**：MetaWorld环境中的MT10基准，包含10个单任务环境<br>- **演示数据**：每个任务使用规则策略收集演示轨迹，每条轨迹限制200步<br>- **模型基础**：Octo-small作为VLA模型骨干网络<br>- **训练配置**：<br> - 优化器：AdamW，学习率10^-5<br> - GAE λ=0.99，折扣因子γ=0.99<br> - PPO截断ε=0.2，值损失权重=0.5，熵损失权重=0.0<br> - 动作分块长度h=4，预热步数T_warmup=40k<br> - 批次大小16，总训练步数500k<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：MetaWorld MT10基准测试<br>- **评估配置**：每任务评估128个回合，使用统一随机种子<br>- **评估指标**：<br> - 平均成功率（Acc.）：128个回合的成功率<br> - 轨迹长度分布的第10百分位数（Len.）<br> - 最短10%轨迹的平均长度（Avg(l).）<br>- **对比基线**：<br> - SFT：10个演示的监督微调<br> - SFT_100：100个演示的监督微调<br> - 标准PPO：无额外演示的标准PPO<br>- **评估结果**：提出的方法在平均性能和大多数单任务指标上均排名第一，证明了RL后训练的可行性和有效性</details> |
| 2025-09-30 | dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought | http://arxiv.org/abs/2509.25681 | <details><summary>展开</summary>### 论文研究单位<br>美的集团、北京大学、上海交通大学<br><br>### 论文概述<br>论文提出 dVLA（diffusion Vision-Language-Action），首个以离散扩散语言模型（DLM）为核心的视觉-语言-动作一体化框架。该模型在统一的扩散目标下，联合优化视觉推理、文本推理与动作生成，并通过多模态思维链（CoT）将高级指令分解为可执行的子目标和动作。在模拟和真实环境中验证有效性：LIBERO 基准平均成功率达 96.4%，真实 Franka 机器人完成多项任务，包括具有挑战性的多步分拣。<br><br>### 论文核心贡献点<br>- 首个基于离散扩散语言模型的 VLA 框架，统一视觉、语言与动作的概率建模与生成。<br>- 提出多模态 CoT 训练范式，模型需同时生成子目标图像（视觉 CoT）、文本推理与离散动作，并在训练中对三者均执行随机掩码与重建，强化跨模态一致性。<br>- 在 LIBERO 基准取得 96.4% 平均成功率，优于离散与连续动作策略；在真实机器人任务上同样领先。<br>- 引入推理加速策略：块级因果前缀注意力（训练时）与 KV 缓存（推理时），获得约 2× 推理加速，性能损失微小。<br>- 模型可预测不安全动作对应的失败视觉 CoT，体现对物理规律和执行后果的内在理解。<br><br>### 论文方法描述<br>- 统一离散化与扩散训练：将视觉（MAGVIT-v2）、文本（LLaDA tokenizer）与动作（FAST：DCT + BPE）统一为离散 token，在相同扩散目标下随机掩码并重建，仅对掩码 token 计算损失。<br>- 架构与初始化：基于 MMaDA 扩展，使用不同分词器并将词汇表从 126,464 扩展至 136,704；所有模态共享一个离散扩散建模目标。<br>- 多模态 CoT 数据与序列结构：输入为 [多视角图像、语言指令、机器人状态]，输出为 [视觉子目标图像、文本推理、离散动作块]，通过起止标记组织序列。<br>- 推理生成：并行输出视觉 CoT（未来状态图像）与文本 CoT（子任务步骤），再据此生成可执行的离散动作。<br>- 加速策略：训练时采用分块因果前缀注意力（块内双向，块间单向），推理时引入 KV 缓存（dLLM-Cache 思想），减少重复计算，实现约 2× 加速。<br><br>### 论文使用数据集和训练资源<br>- 模拟：LIBERO 四个套件（Spatial、Object、Goal、Long），共 40 个任务，每任务 50 条演示；分辨率提升至 256×256。<br>- 真实：Franka 7-DoF 机械臂，1100 条轨迹，涵盖四项任务：<br> - Bin Picking：600 条<br> - Open Box：100 条<br> - Hang Cups：200 条<br> - Pick & place Object：200 条<br>- 训练细节：<br> - 输入图像统一 resize 至 256×256。<br> - 子目标图像预测时域在 [0.9C, 1.1C]，其中 C=5（LIBERO）、C=50（真实任务），仅预测俯视相机图像，使用 classifier-free guidance（scale=3.5）。<br> - 文本推理使用 SEED-1.5VL 的视频分割注释，长任务每 3 秒一段，简单任务可省略以加速推理。<br> - 训练流程与 MMaDA 一致，未明确给出硬件资源。<br><br>### 论文使用的评估环境和评估指标<br>- 模拟评估：LIBERO 基准，每任务 50 次试验（合计 500 次），以任务成功率（SR）为主要指标；与多种连续与离散动作基线对比。<br>- 真实评估：Franka 机器人（2 个 ZED 外置相机 + 1 个 Realsense 435i 手腕相机），每任务 10 次试验（合计 40 次），报告成功率。<br>- 推理效率：比较全注意力与前缀注意力 + KV 缓存两方案下的动作频率（Hz）与成功率，验证约 2× 推理加速与性能保持。</details> |
| 2025-09-29 | World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training | http://arxiv.org/abs/2509.24948 | <details><summary>展开</summary>## 论文研究单位<br>Sun Yat-sen University, China; Amap, Alibaba Group<br><br>## 论文概述<br>Vision-Language-Action (VLA) 模型在少样本与高风险场景下受限：模仿学习对大量演示数据依赖强，强化学习（RL）后训练需真实交互但环境状态不可重置、反馈稀疏且难以判定任务完成。作者提出 World-Env，将世界模型作为低成本、可安全探索的虚拟环境用于 VLA 的 RL 后训练：包括一个视频世界模型用于生成时序一致的未来视觉观测，和一个 VLM 引导的即时反射器用于连续奖励与实时终止信号。该方法在 5 条专家演示下即可显著提升复杂操控任务的成功率，兼顾数据效率与安全性。<br><br>## 论文核心贡献点<br>- 提出 World-Env 框架：以世界模型替代真实交互，支持 VLA 的低成本、风险可控的 RL 后训练。<br>- 集成视频世界模拟器和 VLM 引导的即时反射器，提供语义对齐的连续奖励与语言对齐的任务完成判定。<br>- 引入动态终止机制（基于 R(o1:t, g) > η），避免成功后冗余动作，提升执行效率与成功率。<br><br>## 论文方法描述<br>- 视频世界模拟器（EVAC 思路）：以动作 a_t 和下时刻本体感觉状态 s_{t+1}（6D末端位姿与 1D夹爪）作为输入，投影构建动作图（前景标记+黑背景），以像素级条件通过扩散图像生成预测下一帧视觉观测 o_{t+1}。训练数据混合人类演示与自探索轨迹；自探索通过在模拟器内执行 SFT 策略并用 Laplace 分布引入扰动（scale head 预测 β_t），以覆盖失败与次优行为分布。<br>- VLM 引导的即时反射器：基于 LLaVA，使用冻结的视觉编码器与 LLM 以视频-文本为条件，奖励头输出 R(o1:t, g) ∈ [0,1]，采用阈值 η=0.5 触发终止；训练采用二进制交叉熵（BCE）监督，数据源自 LIBERO 成功判定与模拟器内 oracle 标记。<br>- VLA 策略与 RL：基于 OpenVLA-OFT 的 VLA 策略添加 scale head 使动作分布为 Laplace(μ_t, β_t)，用于不确定度驱动的探索。RL 优化采用 LOOP（RLOO 优势 + PPO 截断目标）：对同一初始状态生成 N 条轨迹（N=8），用 RLOO 计算轨迹优势 A_n，基于 PPO 截断目标更新策略与 scale head。<br>- 终止与奖励使用：在终止步 t_end 给出单步轨迹奖励 R_n = R(o1:t_end, g)，优势广播到轨迹各步；终止时提前结束 rollout。<br>- 训练实现：LoRA 微调视觉-语言骨干（rank=32），LoRA LR=1e-4，动作/标度头全参 LR=1e-5，批量 4，8×H20 GPU（96GB）。<br><br>## 论文使用数据集和训练资源<br>- 数据集：LIBERO（含 LIBERO-Goal、LIBERO-Object、LIBERO-Spatial、LIBERO-Long 四套），每任务仅用 5 条专家演示进行 SFT 预训练；世界模型额外加入自探索过渡数据（含成功与失败）。<br>- 资源：8×NVIDIA H20 GPU（96GB），LoRA（rank=32）进行参数高效微调。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：LIBERO 仿真平台，面向机器人视觉-语言操控。<br>- 指标：成功率（对全测试集）。在“真实反馈约束”设定下比较终止策略（无法获得真实终止信号），验证动态终止与连续奖励的稳定性。</details> |
| 2025-09-29 | IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks | http://arxiv.org/abs/2509.24768 | <details><summary>展开</summary># 论文研究单位<br><br>芬兰阿尔托大学智能机器人组、芬兰奥卢大学生物仿生与智能系统组、丹麦技术大学机械技术系<br><br># 论文概述<br><br>论文提出了IA-VLA框架，用于解决视觉语言行动模型(VLAs)在处理语义复杂任务时的局限性，特别是在涉及视觉重复对象场景下的机器人操控任务。当前VLA模型受限于推理延迟，无法使用足够大的语言模型来处理复杂空间关系和语义指令。<br><br># 论文核心贡献点<br><br>1. 提出了IA-VLA框架，通过大型视觉语言模型作为预处理阶段来增强VLA的输入<br>2. 形式化了视觉重复对象这一复杂的任务类别，构建了相关数据集<br>3. 在涉及重复对象的设置中进行了全面的实验评估，总计1290次评估运行<br>4. 证明了该框架能够显著提高VLA在语义复杂指令下的性能表现<br><br># 论文方法描述<br><br>**IA-VLA框架流程：**<br>- 使用Semantic-SAM对输入图像进行分割，添加数字标签<br>- 调用大型VLM（如GPT-4.1）根据任务描述选择相关对象的数字标签<br>- 对相关对象应用语义分割掩码，对其他区域使用半透明灰色掩码（alpha值0.8）<br>- 通过SAM2实现掩码在时间序列中的传播<br>- 支持两种变体：原始指令版本和重标记版本（将VLA指令简化为"lift the highlighted object"等）<br><br>**掩码过滤算法：**<br>- 掩码块过滤器：将非连通块分离为独立掩码<br>- 掩码重叠过滤器：根据重叠程度合并、丢弃或相减掩码<br>- 面积阈值过滤：移除过小的掩码<br><br># 论文使用数据集和训练资源<br><br>**数据集构成：**<br>- 积木块提升任务：120个演示，12种语言指令<br>- 蔬菜装锅任务：120个演示，12种语言指令<br>- 抽屉开启任务：600个演示，12种语言指令<br>- 训练在桌面任务（积木块和厨房）使用联合模型，抽屉任务单独训练<br><br>**训练配置：**<br>- 基于OpenVLA模型进行微调<br>- 批次大小：16，学习率：0.0005，LoRA秩：32，dropout：0.0<br>- 桌面任务训练25000步，抽屉任务训练50000步<br>- 使用OpenVLA默认图像增强策略<br><br>**计算资源：**<br>- Aalto Science-IT项目和CSC - 芬兰IT科学中心提供的计算资源<br>- Aalto电气工程学院MIDAS研究基础设施<br><br># 论文使用的评估环境和评估指标<br><br>**评估任务设置：**<br>三类任务指令复杂度：<br>- Category 1：训练数据中见过的语言指令<br>- Category 2：见过概念的未见过组合<br>- Category 3：需要从训练概念外推的指令<br><br>**具体任务：**<br>1. **积木块提升**：要求识别特定颜色和位置的积木块进行抓取<br>2. **蔬菜装锅**：将蔬菜放入指定位置的锅中<br>3. **抽屉开启**：打开指定位置和行的抽屉<br><br>**评估指标：**<br>- 成功完成任务得1分<br>- 抓取正确对象但任务未完成得0.5分<br>- 30秒时间限制（不包括预处理时间）<br>- 每个配置进行5次评估运行<br>- 结果以最大可能积分的百分比形式报告<br><br>**评估规模：**<br>- 积木块任务：450次评估运行<br>- 蔬菜装锅任务：480次评估运行<br>- 抽屉开启任务：360次评估运行<br>- 总计1290次评估运行</details> |
| 2025-09-29 | Emergent World Representations in OpenVLA | http://arxiv.org/abs/2509.24559 | <details><summary>展开</summary>### 论文研究单位<br>- LSE.AI, London School of Economics<br>- ETH Zurich<br>- Princeton University - Department of Computer Science<br>- Mila - Quebec AI Institute<br><br>### 论文概述<br>Vision-Language-Action models (VLAs) 如OpenVLA，训练于策略基础强化学习（RL），但是否隐式学习世界模型（状态转移函数）未知。论文通过embedding arithmetic在状态表示上实验，探测OpenVLA是否编码潜在世界模型。使用线性/非线性探针预测状态转移向量，发现激活探针优于embeddings基线，表明世界模型存在。调查训练进展，发现世界模型随计算扩展而出现，并提出SAEs的可解释规划管道。<br><br>### 论文核心贡献点<br>- 利用embedding arithmetic证明OpenVLA编码潜在世界模型。<br>- 展示训练计算扩展增强世界模型发展，并定位其于中间层。<br>- 提出SAEs应用于可解释规划：预测状态转移向量后分解为可解释特征。<br>- 线性探针优于MLP探针，支持线性表示假设（LHR）。<br><br>### 论文方法描述<br>- **理论框架**：基于Koopman算子近似世界模型，定义K步状态转移算子。<br>- **状态转移向量**：学习函数 \( f: \mathbf{a}_t \mapsto \Delta\mathbf{e}_{t\rightarrow t+K} \)，其中 \(\Delta\mathbf{e} = \mathbf{e}_{t+K} - \mathbf{e}_t\)。<br>- **探针**：<br> - 线性探针（Lasso回归）预测 \(\Delta\mathbf{e}\) 从激活 \(\mathbf{a}_t\)。<br> - MLP探针测试非线性表示。<br>- **基线对比**：训练探针于embeddings作为基线，隔离因果效应。<br>- **评估**：R²分数和置换检验（p < 0.01）量化预测性能。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：LIBERO数据集（goal, spatial, long, object子集），共400 episodes, 66,931步。<br>- **模型**：OpenVLA (7B参数)，预训练于Open X-Embodiment数据集。<br>- **训练资源**：探针训练使用网格搜索调优超参数；计算资源未详述，代码可用。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO各子集测试集。<br>- **评估指标**：<br> - 回归R²分数评估预测能力。<br> - 置换检验（100次）检验统计显著性（p值）。<br> - Allan方差分析长时状态转移信噪比。<br> - 时间一致性（cosine similarity）测量嵌入稳定性。</details> |
| 2025-09-29 | PhysiAgent: An Embodied Agent Framework in Physical World | http://arxiv.org/abs/2509.24524 | <details><summary>展开</summary># 论文研究单位<br><br>根据论文致谢部分，该研究由无锡应用技术研究院、清华大学等机构支持，具体包括Wuxi Research Institute of Applied Technologies, Tsinghua University under Grant 20242001120。<br><br># 论文概述<br><br>本文提出了PhysiAgent框架，一个针对物理世界优化的具身智能体框架，旨在解决视觉-语言-动作（VLA）模型在有限泛化能力方面的问题。该框架通过整合监控、记忆、自反思机制以及轻量级工具箱，使VLMs能够根据VLAs的实时性能反馈动态组织不同组件，从而最大化VLAs的能力。实验在真实世界机器人操控任务中验证了显著的任务解决性能提升，展现了有效的自我调节能力和自适应演化特性。<br><br># 论文核心贡献点<br><br>1. **提出PhysiAgent框架**：一个训练无关的模块化具身智能体框架，能够灵活集成VLMs和VLAs用于真实世界部署<br><br>2. **将智能体范式引入物理世界**：将传统在语言或模拟领域探索的智能体范式引入物理世界，为VLMs配备真实世界感知和工具使用能力<br><br>3. **真实世界验证**：在真实世界机器人操控任务中验证框架，展现涌现的自我反思能力和显著的任务性能提升<br><br># 论文方法描述<br><br>PhysiAgent采用统一的自主脚手架机制，包含五个关键组件：<br><br>- **规划器（Planner）**：将原始语言指令分解为适合VLAs的可执行中间指令<br>- **监控器（Monitor）**：持续跟踪VLA执行进度，评估任务进展状态（阻碍、进行中、失败、完成）<br>- **反思器（Reflector）**：作为验证层改善监控精度，生成视觉约束纠正误判<br>- **记忆（Memory）**：包括短期记忆（存储步骤级数据）和长期记忆（存储高级摘要），支持持续适应<br>- **工具箱（Toolbox）**：提供感知、控制和推理工具，增强系统鲁棒性和适应性<br><br>框架实现双向信息流：从VLMs到VLAs的前向流和从VLAs行为到VLMs的后向反馈，使VLMs能够根据实时反馈调整输出。<br><br># 论文使用数据集和训练资源<br><br>**数据集**：<br>- 真实世界桌面操控数据集，包含5个任务：put broccoli on plate、put mushroom on plate、put sausage on plate、put shrimp on plate、put chips on plate<br>- 每个任务包含150个人类远程操控演示数据<br><br>**训练资源**：<br>- Diffusion Policy：使用4个NVIDIA A800 GPU训练120万步，27小时，批大小64，学习率0.0003<br>- RDT-1B：使用8个A800 GPU训练5万步，20小时，批大小64，学习率0.0001<br><br># 论文使用的评估环境和评估指标<br><br>**评估环境**：<br>- 硬件：AIRBOT 6-DOF机械臂配抓取器<br>- 相机设置：三个RGB相机分别位于顶部、前方和手腕安装位置<br>- 测试任务包括三个不同复杂度的桌面操控任务：获取含膳食纤维食物、获取含蛋白质和脂肪食物、烹饪一餐（需要5个步骤）<br><br>**评估指标**：<br>- 累积任务进度（Y轴）vs VLA步骤（X轴）<br>- 任务成功率<br>- 执行效率（完成任务所需的最少步骤）<br>- 性能对比：与vanilla VLA策略、传统层级框架、人机交互层级方法进行对比<br>- 平均5次独立试验的结果展示</details> |
| 2025-09-28 | Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models | http://arxiv.org/abs/2509.23655 | <details><summary>展开</summary># 论文研究单位<br>Centre for Artificial Intelligence, UCL 和 Qualcomm AI Research<br><br># 论文概述<br>针对视觉-语言-动作模型（VLA）训练计算成本高的问题，论文提出了Oat-VLA（Object-Agent-centric Tokenization for VLAs）方法。该方法通过对象中心和智能体中心的标记化策略，将视觉输入的标记数量从传统的256个大幅减少到16个，同时保持或提升模型性能。实验表明，Oat-VLA在LIBERO基准上的收敛速度比OpenVLA快2倍以上，并在真实世界的抓取放置任务中表现更优。<br><br># 论文核心贡献点<br>- 设计了对象中心标记化方法，将场景中对象信息压缩为少量语义标记<br>- 提出智能体中心标记化策略，获取机械臂末端执行器的精确视觉信息<br>- Oat-VLA具有模块化和可扩展性，可重用现有VLA检查点，便于适应<br>- 在LIBERO基准上实现超过2倍的训练收敛加速<br>- 在真实世界抓取放置任务中展现出比OpenVLA更高的成功率<br>- 视觉标记数量减少93.75%，显著降低GPU内存需求和计算成本<br><br># 论文方法描述<br>**对象中心标记化**：使用FT-Dinosaur无监督对象中心模型提取图像分割掩码，通过平均池化将同一对象区域的视觉标记压缩为单一标记，主实验使用7个对象标记。<br><br>**智能体中心标记化**：训练轻量级ResNet Faster R-CNN检测机械臂末端执行器位置，获取该位置周围3×3区域的9个视觉标记，确保精确操作。<br><br>**Oat-VLA架构**：结合7个对象标记和9个智能体标记（共16个视觉标记），通过MLP投影器输入到Llama 2 LLM主干网络中，保持与OpenVLA架构兼容性。<br><br># 论文使用数据集和训练资源<br>**数据集**：LIBERO基准测试套件（包含Spatial、Object、Goal、10四个任务套件）、Open X-Embodiment数据集子集（Bridge+FMB+Fractal）、自建真实世界数据集（320条轨迹）<br><br>**训练资源**：8×H100节点，全参数微调使用批量大小8×64=512，LoRA微调使用批量大小8×48=384，从OpenVLA预训练检查点开始微调<br><br># 论文使用的评估环境和评估指标<br>**评估环境**：LIBERO仿真环境（四个任务套件）、真实世界环境（UFACTORY xArm 6机械臂）<br><br>**评估指标**：<br>- LIBERO基准：成功率及标准差（每套件100次评估），与OpenVLA、Octo、Diffusion Policy对比<br>- 真实世界评估：分布内/外任务成功率及详细分解<br>- 训练效率：动作标记准确率vs训练时间，训练吞吐量（样本/秒）<br>- 消融实验：不同标记化策略的性能对比</details> |
| 2025-09-27 | Leave No Observation Behind: Real-time Correction for VLA Action Chunks | http://arxiv.org/abs/2509.23224 | <details><summary>展开</summary># 论文研究单位<br><br>东京大学（The University of Tokyo）<br><br># 论文概述<br><br>大型视觉-语言-动作（VLA）模型在机器人控制中面临推理延迟的挑战。尽管这些模型能够生成连贯的行为，但延迟导致机器人在动态环境中响应迟缓。为解决这一问题，研究团队提出了异步动作块纠正（A2C2）方法，通过轻量级纠正头在每个控制步骤实时修正动作。该方法能在不重新训练基础策略的情况下，结合最新观察信息和时空位置特征，对预测的动作块进行逐步调整，从而保持闭环响应性并提升在延迟和长视野条件下的成功率。<br><br># 论文核心贡献点<br><br>1. 首次正式定义了VLA模型生成动作块时的延迟问题，并分析了其对机器人实时控制的影响<br>2. 提出了A2C2（异步动作块纠正）方法，这是一个轻量级的附加动作纠正策略，可应用于任何VLA模型<br>3. 建立了基于时间位置特征的动作纠正机制，能有效处理异步执行中的观察滞后问题<br>4. 在Kinetix动态任务和LIBERO Spatial基准测试中，方法在存在延迟和长执行视野条件下均实现了显著的成功率提升<br><br># 论文方法描述<br><br>A2C2框架通过在基础策略之上添加轻量级纠正头πa2c2来扩展基于动作块的策略π。纠正头接收时间t+k时刻的观测ot+k、基础策略预测的动作a_t+k（来自之前t时刻的观察）、时间特征τ_k（位置在动作块内的相对位置）、基础策略的潜在表示zt+k，以及语言指令l。方法采用正弦嵌入表示位置特征，为动作块长度提供周期性结构。纠正头预测残差动作Δat+k，并与基础动作相加产生执行动作aexec_t+k=k。基础策略π每e步执行一次推理，而纠正头运行在每个控制步骤。训练过程中，使用均方误差（MSE）损失函数优化纠正头，通过对比专家演示的目标动作与基础策略输出之间的差异来学习残差预测。方法兼容现有的演示数据集，无需强化学习微调，并且可作为独立插件集成到任何VLA模型中。<br><br># 论文使用数据集和训练资源<br><br>1. **Kinetix动态任务套件**：包含12个高度动态的操控和运动任务，使用1百万步专家演示数据，由RPO训练的专家策略生成<br>2. **LIBERO Spatial基准**：提供432个演示和52,970帧数据，涵盖10个空间操控任务，包含顶部和手腕RGB图像、状态信息和语言指令<br>3. **训练设置**：Kinetix环境使用flow-matching策略作为基础，纠正头为3层MLP（0.31M参数）；LIBERO使用SmolVLA作为基础（450M参数），纠正头结合6层transformer编码器和MLP（32M参数）<br><br># 论文使用的评估环境和评估指标<br><br>1. **评估环境**：<br> - Kinetix仿真环境（12个动态任务）：包括汽车发射、倒立推力投石、捕捉游戏、链式着陆器、抓取、半人马独轮车、硬着陆器、半豹式游泳者、豹式行走和蹦床等场景<br> - LIBERO Spatial真实机器人基准：包含10个需要精细空间推理的操控任务<br><br>2. **评估指标**：<br> - **成功率**：通过多次试验计算任务成功完成的百分比，在不同延迟d和执行视野e的组合下进行评估<br> - **延迟敏感性**：分析推理延迟从0到4步变化时的性能下降<br> - **视野稳定性**：研究执行视野从1到50步对性能的影响<br> - **性能对比**：与Naive async和RTC（Real Time Chunking）基线方法进行对比，评估A2C2的改进效果</details> |
| 2025-09-27 | Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges | http://arxiv.org/abs/2509.23121 | <details><summary>展开</summary>## 论文研究单位<br><br>辽宁辽河实验室（项目编号：LLL24ZZ-02-01 和 LLL24ZZ-02-02）<br><br>## 论文概述<br><br>该论文评估了视觉语言动作（VLA）模型在工业环境中的适用性。研究从工业部署的角度比较了现有SOTA VLA模型在工业场景中的性能，并从数据收集和模型架构两个角度分析了VLA模型在现实工业部署中的局限性。研究发现，当前VLA模型在复杂工业环境、多样化物体类别和高精度放置任务方面仍有很大改进空间。<br><br>## 论文核心贡献点<br><br>- 评估了最先进的VLA模型在工业场景中的拾取和放置任务性能<br>- 从数据集和模型架构两个角度分析VLA模型对非结构化环境的适应性<br>- 讨论了提高VLA模型鲁棒性和任务泛化能力的潜在方向<br>- 提供了VLA模型在工业应用中的实用见解<br><br>## 论文方法描述<br><br>提出了VLA的通用技术框架，包含以下关键技术模块：<br>- 归一化和反归一化：消除不同任务或机器人平台间的维度不一致性<br>- 数据增强：包括图像增强（随机裁剪、颜色抖动、图像损坏等）和指令增强（提示变化、释义等）<br>- 投影器：连接视觉编码器和LLM的桥梁，进行维度对齐和语义映射<br>- 预训练VLM：包括视觉编码器（如DINOv2、SigLIP）和语言编码器（如LLaMA、Gemma、Qwen2-VL）<br>- 政策头：三种主要架构类型包括自回归、扩散和混合模型<br><br>## 论文使用数据集和训练资源<br><br>- 使用来自真实工业场景采集的回合数据进行模型微调，每个任务使用100个回合<br>- 在NVIDIA H20 96GB GPU上训练10小时<br>- 使用Ubuntu 20.04和ROS 1 noetic系统<br>- 在Mobile ALOHA双机械臂机器人上进行所有测试<br><br>## 论文使用的评估环境和评估指标<br><br>- 评估环境：三个工业场景，涵盖视觉遮挡、相机抖动、目标姿态随机化和目标多样性等扰动<br>- 主要指标：成功率（%），计算方法为成功试验次数除以总试验次数<br>- 精度指标：位置误差（2.2 cm）和方向误差（12.4°）<br>- 实验设置：每个试验包含10次测试，涵盖目标姿态随机化和多样化物体类别<br>- 评估结果：Pi0模型经过微调后，在简单抓取任务上达到约60%的成功率，但在高精度放置任务中精度仍有不足</details> |
| 2025-09-26 | VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search | http://arxiv.org/abs/2509.22643 | <details><summary>展开</summary># 论文研究单位<br>南洋理工大学（NTU）、清华大学深圳国际研究生院（Tsinghua SIGS）、清华大学、北京邮电大学（BUP）<br><br># 论文概述<br>现有视觉-语言-动作模型（VLA）在部署中依赖短期状态到动作的映射，易产生逐步偏差，难以完成长时序任务。本文提出VLA-Reasoner，一个可在测试时扩展的即插即用框架，通过在线蒙特卡洛树搜索（MCTS）在世界模型中前瞻未来状态并搜索最优动作，从而缓解VLA的短视问题并显著提升成功率和鲁棒性。<br><br># 论文核心贡献点<br>提出可插拔的测试时推理框架，向任意VLA注入树搜索与未来前瞻能力，无需重新训练底层策略；提出KDE先验的高效候选采样与基于图像的离线奖励塑形，用以在MCTS中实现高效扩展与密集反馈；通过线性融合（式1）将MCTS输出的推理动作与VLA原动作结合，完成即时动作优化与长时指导。<br><br># 论文方法描述<br>问题形式化：给定观测o_t与语言指令l，VLA生成a_t^VLA；VLA-Reasoner在世界模型W中模拟未来轨迹，并用MCTS在动作空间搜索最优a_t^Reasoner，最终执行a_t=α·a_t^VLA+(1−α)·a_t^Reasoner，α为注入强度。<br><br>在线MCTS：围绕VLA预测作为根节点展开。四步循环为<br>- 扩展（Expansion）：在世界模型中进行动作采样与扩展；扩展后的状态通过世界模型W(a_i,o_i)→o_{i+1}得到下一状态。<br>- 模拟（Simulation）：利用动作感知的世界模型生成未来状态以评估当前动作的影响。<br>- 回传（Backpropagation）：自叶向根更新节点值Q与访问次数N，用到KDE概率密度近似N(a)以降低开销。<br>- 选择（Selection）：采用UCB（Upper Confidence Bound）在价值与访问次数间平衡选点。<br><br>KDE高效采样：用离线机器人演示训练KDE核密度估计得到动作分布π^KDE，从该分布采样候选并在Top-k近邻内扩展，避免重复调用VLA并维持探索多样性；支持对动作块（chunk）进行整体采样。<br><br>视觉奖励塑形：为获得密集且稳健的中间反馈，用ResNet-34图像编码器+两层MLP（MSE）在降采样后的离线轨迹上学习从状态到奖励的映射r=MLP(o)，以奖励为导向修正MCTS中的轨迹选择。<br><br>算法伪代码与实现：完整给出从根节点初始化、循环扩展/模拟/回传/选择、选出最优子节点并计算a_t^Reasoner、以及最终动作注入的流程。除世界模型外各组件基于与VLA微调相同的数据进行训练；额外收集少量失败演示用于世界模型微调以提升失败情形下的预测能力。<br><br># 论文使用数据集和训练资源<br>- 仿真基准：LIBERO（Spatial/Goal/Object/Long四套任务，共约500条演示/套）和SimplerEnv（Block/Spoon/Carrot/Eggplant四任务）；包含公共机器人演示数据集与仿真器生成轨迹。<br>- 基础策略：OpenVLA-7B、Octo-Small（27M）、SpatialVLA（4B）等。<br>- 世界模型：iVideoGPT架构的动作感知世界模型（600M参数）。<br>- 训练资源：服务器6×NVIDIA RTX 6000 GPU进行训练；真实世界推理使用NVIDIA RTX 4090。<br>- KDE与奖励网络：在与VLA微调相同的离线数据集上训练；KDE用于动作先验采样，奖励网络以ResNet-34+MSE学习图像至奖励的映射；为覆盖失败情形另行收集少量失败演示用于世界模型微调。<br><br># 论文使用的评估环境和评估指标<br>- 仿真评估：在LIBERO与SimplerEnv上进行定量评估，指标为平均成功率（每套任务各500/100回合）。<br>- 真实世界评估：在Galaxea-A1机械臂上完成5项任务（Block、Fruit、1 Cup、2 Cups、Circle），侧视与腕部摄像头提供视觉输入（OpenVLA不使用腕部图像），每项任务进行20回合评估。<br>- 消融实验：考察注入强度α（0.6最优）以及关键组件（KDE采样vs高斯噪声；图像奖励塑形vs token式奖励头）对成功率的影响。<br>- 核心结论：VLA-Reasoner在仿真与真实世界均显著提升成功率与鲁棒性，优于多种强基线VLA，表明测试时树搜索与未来预测能有效缓解短期偏差并增强长时任务执行能力。</details> |
| 2025-09-26 | UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation | http://arxiv.org/abs/2509.22441 | <details><summary>展开</summary>### 论文研究单位<br>西湖大学工程学院（主单位），浙江大学信息与电子工程学院、环境与资源学院，澳大利亚国立大学。<br><br>### 论文概述<br>提出首个针对自主水下航行器（AUV）的视觉-语言-动作（VLA）框架 UnderwaterVLA，旨在解决海洋环境的极端挑战（水动力学干扰、感知退化、通信受限）。该框架首次将VLA模型应用于水下机器人，通过双脑架构解耦高层推理与低层控制，实现零数据训练和水动力实时补偿，显著提升导航精度与任务成功率。<br><br>### 论文核心贡献点<br>1. **首个水下VLA架构**：系统性解决VLA在水下应用的层级、数据需求和动力学补偿三大核心挑战。<br>2. **零数据训练策略**：通过预训练多模态模型的迁移学习与物理先验融合，规避昂贵的水下演示数据。<br>3. **双脑解耦控制**：云脑负责长时域任务规划，小脑实现实时感知-控制闭环，在通信/算力受限下保障鲁棒性。<br>4. **水动力实时补偿**：在MPC中嵌入流体动力学模型，通过在线参数估计自适应复杂水流环境。<br><br>### 论文方法描述<br>1. **双脑架构**：<br> - **云脑**：Surfacing时运行QVQmax模型，将高层指令分解为序列化子任务（如“右转躲避障碍→直行→低速接近”）。<br> - **小脑**：本地Qwen-VL模型循环执行JSON格式控制指令，动态调整决策（如检测安全距离后自主终止任务）。<br> - 结构示例：`云脑计划 → 子任务序列 → 本地执行（感知-动作循环）`。<br><br>2. **提示工程与可解释性**：<br> - 强制云脑/小脑输出思维链（CoT）推理过程，记录决策逻辑。<br> - 小脑输出标准化JSON结构：<br> ```json<br> {<br> "reasoning": "检测到左侧障碍，右转",<br> "decision": "right",<br> "velocity": "medium",<br> "sub_task_done": false<br> }<br> ```<br><br>3. **零数据MPC水动力控制**：<br> - **运动剖面**：平移/旋转动作严格分为1秒周期的加速-恒速-减速阶段（速度：0.2/0.5/0.8 m/s，角速度：0.5/1.0/1.5 rad/s）。<br> - **MPC优化**：最小化代价函数同时追踪参考信号、控制力和流体阻力：<br> ```math<br> J = \int_0^1 \left[ \beta\\|v(t)-v_{\text{ref}}\\|^2 + \gamma\\|\tau\\|^2 + \delta\\|F_{\text{drag}}\\|^2 \right] dt<br> ```<br> - **流体参数在线估计**：基于IMU数据实时估算拖拽系数：<br> ```math<br> \hat{D}_v = \frac{\tau_v - M\dot{v}}{v\|v\|}<br> ```<br><br>### 论文使用数据集和训练资源<br>- **训练数据需求**：**零**（完全依赖预训练模型 + 物理先验，无需水下演示数据）<br>- **基线数据对比**：QUAR-VLA等基线需262K演示数据（来源自其他机器人任务）。<br>- **模型资源**：<br> - 云脑：QVQmax（大语言模型）<br> - 小脑：Qwen 2.5-VL-7B（多模态视觉-语言模型）<br><br>### 论文使用的评估环境和评估指标<br>1. **实验环境**：<br> - **实验室测试**：控制变量水池，3垂直柱状障碍，用于导航与避障任务。<br> - **真实环境模拟**：降低光照强度至海平面5%，注入硅藻土提升浊度至18 NTU（模拟近岸浑浊水域）。<br><br>2. **评估指标**：<br> - **量化性能**：<br> - 任务成功率：对比QUAR-VLA基线，感知/导航/隧道穿越/避障任务提升19%-27%。<br> - 零数据效率：基线需262K数据，UnderwaterVLA无需任何水下演示数据。<br> - **鲁棒性评估**：<br> - 双脑架构（DBM）vs 单脑模型（SBM）：在浑浊环境中，DBM能自主终止任务避免过冲，SBM因目标丢失持续前进导致任务失败。<br> - 可视化证据：轨迹对比图显示DBM在低能见度下精确接近目标，SBM偏离路径。<br><br>**结论**：通过解耦控制层级、融合物理先验和可解释推理链，UnderwaterVLA在零数据条件下实现水下机器人任务性能突破，为复杂海洋环境中的自主作业提供了可行路径。</details> |
| 2025-09-26 | EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer | http://arxiv.org/abs/2509.22407 | <details><summary>展开</summary>### 论文研究单位<br>- 所属机构：GigaAI、Peking University、Tsinghua University、CASIA<br><br>### 论文概述<br>- 目标：解决VLA（视觉–语言–动作）模型因真实机器人演示数据昂贵、难以扩展而导致的泛化能力不足问题<br>- 核心方案：提出EMMA框架，结合生成式数据引擎与自适应训练策略，通过文本控制的视频生成扩充数据，提升策略在新外观与新环境的零样本泛化<br><br>### 论文核心贡献点<br>- 提出EMMA框架，集成DreamTransfer与AdaMix，显著提升真实世界机器人操作的泛化<br>- DreamTransfer：DiT（扩散Transformer）双分支架构，融合深度与文本，实现多视角一致、几何可控的机器人操作视频生成<br>- AdaMix：基于轨迹性能的自适应重采样，动态提高“困难样本”的训练权重，增强策略稳健性与泛化<br>- 与SOTA相比，DreamTransfer在多视角一致性提升42%、深度一致性提升24%；真实世界零样本视觉任务相较仅用真实数据提升超过200%，AdaMix额外提升13%<br><br>### 论文方法描述<br>- DreamTransfer总体结构：主分支用于去噪潜空间视频令牌，ControlNet分支注入深度结构约束；多视角视频深度沿宽度拼接作为统一潜表示；T5文本编码器提供语义嵌入，并通过交叉注意力融合至主分支<br>- 文本控制与几何一致性：用户可通过自然语言编辑前景、背景与光照，同时保持3D结构与几何合理性；多视角与深度约束共同维持跨视角与时间的一致性<br>- AdaMix训练策略：初期使用高质量视频（低质量样本权重为零），稳定训练；在收敛后根据三类指标动态调整采样权重，聚焦“困难样本”<br> - 指标：动作预测误差MSE（负号）、轨迹平滑度（角加速度二阶差分负号）、关节限位（越界为0，否则为1）<br> - 组合得分：归一化后取平均<br> - 更新权重：p(i) ∝ γ + λ·(1 − s_i)，保证最小支持并强调困难样本<br>- 数据混合与筛选：真实与生成数据按比例混合训练；生成视频以多视角一致性、深度一致性与文本–视频相似度进行过滤与评分<br>- 训练细节：两阶段微调（低分辨率稳定多视角一致性 → 高分辨率细节恢复）；优化器AdamW；分任务（折叠布料、清洁桌面、投掷瓶子）不同步数配置<br><br>### 论文使用数据集和训练资源<br>- 真实演示：折叠布料（50条）、清洁桌面（20条）、投掷瓶子（20条）<br>- 生成数据：Agibot World与NVIDIA Isaac Sim采集；基于Agibot World构建5万条多视角视频（含对齐RGB、时间一致深度与模板化文本描述）<br>- 生成流程：多视角深度一致性由Video Depth Anything估计；前景/背景/光照描述由Qwen2.5-VL-7B生成<br>- 仿真演示：NVIDIA Isaac Sim采集，DreamTransfer执行视间一致的风格与外观转移<br>- 训练硬件与平台：AgileX CobotMagic，双PiPER臂，三台Intel RealSense D435i摄像头；Isaac Sim用于仿真<br>- 两阶段微调配置：阶段一576×128分辨率、批量32、3500步；阶段二1920×480分辨率、批量4、4500步<br><br>### 论文使用的评估环境和评估指标<br>- 任务：折叠布料（真实到真实）、清洁桌面/投掷瓶子（仿真到真实）<br>- 视频生成质量评估<br> - 多视角一致性：匹配像素数（Pix.Mat.）<br> - 深度一致性：RMSE、绝对相对误差（Abs.Rel.）、平方相对误差（Sq.Rel.）<br> - 文本–视频对齐：CLIP相似度（CLIPSim.）<br>- 真实世界机器人评估<br> - 行为得分：最大5分，按失败场景扣分（任务不同规则不同）<br> - 成功率：每任务在5次试验与4种外观变化下评估，共20次运行<br> - 执行质量：执行时间、轨迹平滑度（角加速度）、关节越界帧数<br>- 数据混合实验：固定总量与步数，考察真实/生成比例对成功率的影响（50%为峰值）<br>- 消融实验：固定混合比例（FixMix）对比AdaMix，显示AdaMix在行为得分、成功率与执行质量上的综合提升</details> |
| 2025-09-26 | MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training | http://arxiv.org/abs/2509.22199 | <details><summary>展开</summary># 论文研究单位<br>GigaAI; 中科院自动化研究所 (CASIA); 南京理工大学 (NJUST); 清华大学<br><br># 论文概述<br>MimicDreamer提出一套将低成本的人类演示视频转换为可用的机器人监督数据的统一框架，针对视觉、外参视点和动作三个维度的域差异进行联合对齐，使视觉语言动作（VLA）模型能在仅使用人类演示合成数据的情况下完成少样本机器人执行，并随人类数据规模扩大而显著提升成功率。核心包括H2R Aligner（视频扩散模型的视觉对齐）、EgoStabilizer（单目视点稳定与修复）和统一的H2R动作空间与约束逆动力学求解器。<br><br># 论文核心贡献点<br>- 提出统一的人机演示对齐框架，同时在视觉、视点与动作三个维度缩小差距，将人类视频转化为可执行的机器人监督用于可扩展VLA训练。<br>- 视觉：基于视频扩散和几何先验的H2R Aligner，将人类演示动作迁移为高保真机器人臂视频；视点：EgoStabilizer通过单应变换与背景修补稳定外参视点；动作：将人手轨迹映射到机器人坐标，并以约束逆动力学生成低抖动、可行的关节指令。<br>- 在六项代表性操控任务上，仅用合成数据即可实现少样本执行；随人类数据扩大，显著提升平均成功率（14.7%），优于仅用真实机器人数据的基线。<br><br># 论文方法描述<br>- Viewpoint Stabilization（EgoStabilizer）<br> - Warp Perspective：相邻帧或参考帧估计单应Ht，用RANSAC剔除异常；时间平滑得到Ht~并构造补偿Wt，对每帧进行补偿与对齐；计算跨帧最大公共可见区域并裁剪以消除黑边与视场抖动。<br> - Video Inpainting：对补偿后产生的空洞与遮挡区域构造掩码Mt，采用视频修补模型进行时空传播与跨帧一致性修复，得到稳定序列。<br>- Actions Alignment（H2R动作空间）<br> - Human-side Normalization：将人手三维关键点变换至身体坐标系FB，并通过刚体配准到机器人基座坐标系FR，得到目标位置p*与姿态R*。<br> - Orientation Treatment：因手腕接近球副而末端执行器常以工具轴滚动，只对齐俯仰/偏航（roll软遮蔽）：使用对数映射与加权矩阵WR降低roll权重。<br> - IK Resolver：对每条臂a∈{L,R}，以末端位置误差、姿态加权误差与时间平滑项的二次目标，结合关节限位进行约束优化，用阻尼最小二乘（DLS）逐步求解；夹爪命令由人手开合经轻量VGG分类器与中值滤波推断。<br>- Visual Alignment（H2R Aligner）<br> - 训练：以真实机器人视频Vgt、环境背景Vscene与仿真前景Vsim为三路条件，经共享的冻结VAE编码为潜变量；在潜空间对目标做噪声扰动，通道拼接后输入H2R DiT进行去噪与条件融合，优化CogVideoXLoss。<br> - 推理：Vsimik由IK动作在仿真重放得到；Vsceneik通过对稳定后的人类视频进行人手分割与轻微膨胀得到；目标从噪声起始，经H2R DiT去噪与VAE解码，生成合成机器人域视频；将合成视频与对应IK动作对齐形成可训练数据。<br>- VLA训练<br> - 初始化自π₀预训练模型，复用VLM与动作词表；在合成数据上进行后训练，并混入少量真实演示以增强真实可执行性。采用条件流匹配（CFM）目标训练动作token，输出意图级控制并投影为关节指令。训练细节与超参数见附录。<br><br># 论文使用数据集和训练资源<br>- 数据集：EgoDex（829小时、1080p单目人演示，涵盖194任务，配有3D上身姿态）。H2R Aligner训练了24类操控，共3735样本（每样本64帧@30fps），随机裁剪至640×360并缩放至672×384，按9:1划分训练/验证。<br>- 训练资源：冻结的视频VAE与文本编码器；H2R DiT可训练。π₀预训练模型作为初始化；采用AdamW优化。<br>- 合成数据规模：实验逐步添加人转机数据（5→30个轨迹），在20真人+20合成的配比下达到最佳综合效果。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：六个操控任务——Pick Bag、Clean Surface、Stack Bowls、Dry Hands、Insert Tennis、Stack Cups。任务由真实机器人执行，模仿人类演示情境。<br>- 评估指标：成功率（SR）与进度成功率（PSR）。前者衡量整体任务完成度，后者衡量各子任务完成的平均比例。<br>- 结果要点：<br> - Few-shot：仅20真人 vs 20真人+20合成 vs 20真人+3合成三种设置，后者在平均SR/PSR上分别达85.0%/91.0%，较仅真人基线提升显著，各任务均有+10%~25% SR、+10%~32% PSR提升，并在Clean Surface与Dry Hands上达100%。<br> - Scaling：随人类数据增加，所有任务的SR、PSR单调上升，呈现快速上升后趋稳的规模化趋势；50–50配比下相对基线提升约10%量级，难度高的任务增幅更大。<br> - H2R Aligner：定性结果显示合成视频具备逼真的机械臂外观、一致的接触几何与背景保持。<br> - EgoStabilizer：稳定性（Jitter RMS、Stability）与几何一致性（H-RMSE）均显著提升；平均稳定性下降21.9%，Jitter RMS下降13.1%，几何误差仅小幅下降3.3%。</details> |
| 2025-09-26 | Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting | http://arxiv.org/abs/2509.22195 | <details><summary>展开</summary>## 论文总结<br><br>### 论文研究单位<br>普林斯顿大学（Department of Mechanical and Aerospace Engineering, Department of Computer Science）<br><br>### 论文概述<br>本文针对将视觉语言模型（VLM）微调为视觉语言行动模型（VLA）时存在的核心挑战——灾难性遗忘进行了深入研究。传统方法在获得机器人操作能力的同时会严重削弱VLM的基础推理和多模态理解能力。本文提出了一种数据驱动的解决方案——**VLM2VLA**，其核心思想是将机器人动作转化为自然语言描述（Actions as Language），从而最小化微调数据与预训练数据之间的分布差异。该方法使得仅通过低秩适配（LoRA）即可实现有效微调，无需架构改造或昂贵的共同训练，并显著保持了原VLM的核心能力。<br><br>### 论文核心贡献点<br>1. **动作语言化表示**：提出将低级机器人动作转换为自然语言描述，直接利用VLM预训练的知识空间，避免分布偏移。<br>2. **数据标注与训练管道**：提供了一套可扩展的机器人演示数据重标注流程，将轨迹分解为子任务、运动规划和动作块三层结构。<br>3. **LoRA微调验证**：证明在动作语言化的前提下，LoRA可以有效进行VLA训练并维持VLM知识。<br>4. **性能验证**：通过大量VQA实验和800+真实机器人实验，证明了方法在保持推理能力和零样本泛化方面的显著优势。<br><br>### 论文方法描述<br>1. **三层行动表示 (Actions as Language)**：<br> - **高层子任务预测**：根据视觉观察和指令预测下一步子任务。<br> - **中层运动规划**：基于当前子任务和观察，生成动作的空间方向描述（例如“向左并稍向下”）。<br> - **低层动作生成**：生成最终执行的文本化动作指令序列（例如“向前移动4.2厘米”），包含各个自由度的具体指令。<br>2. **数据重构**：<br> - 使用Gemini模型自动标注机器人轨迹（BridgeData v2），将原始状态-动作序列转换为带有语言描述的序列：`\{(图像, 子任务, 运动计划, 文本化动作块)\}`。<br>3. **训练与推理**：<br> - **训练**：在重构后的语言化数据上，使用LoRA对Gemma-3-12B-IT等VLM进行微调。<br> - **推理**：采用闭环方式，模型首先生成完整的子任务序列；在执行每个动作块后，调用一个验证器（使用Gemini 2.5 Pro）判断任务完成度，决定继续下一子任务或重试当前任务。<br><br>### 论文使用数据集和训练资源<br>1. **数据集**：<br> - **机器人数据**：BridgeData v2 数据集的一个子集（包含主要任务指令）。<br> - **多模态数据**：用于评估VQA能力（未明确说明具体数据集，可能基于其预训练知识）。<br>2. **训练资源**：<br> - **基础模型**：Gemma-3-12B-IT (主要), Gemma-3-4B-IT (用于对比)。<br> - **标注工具**：Gemini 2.5 Pro 和 Gemini 2.5 Flash（用于成本控制）用于自动生成语言化轨迹标签。<br> - **微调方法**：LoRA（应用于所有线性模块）。<br> - **标注成本**：约$900。<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**：<br> - **机器人平台**：WidowX 250S 6自由度机械臂（用于真实世界操作）。<br> - **物理场景**：标准玩具厨房环境。<br>2. **评估指标**：<br> - **多模态理解能力**：<br> - **指标**：多个VQA基准测试的准确率（如MMMU, MMStar, MME, OCRBench, MMB-en, MMB-cn, TextVQA, DocVQA, InfoVQA, AI2D, ChartQA, RealWorldQA）。<br> - **机器人操作能力**：<br> - **指标**：任务成功率（每任务30次试验，多语言任务为90次）。<br> - **任务类型**：<br> - **域内任务 (ID)**：拾取、拾取放置。<br> - **组合任务**：多步骤操作。<br> - **域外任务 (OOD)**：多语言指令翻译（西班牙语、普通话、印地语）；识别流行文化概念（识别“Ash Ketchum”图像上方的物体）。</details> |
| 2025-09-26 | Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation | http://arxiv.org/abs/2509.22093 | <details><summary>展开</summary>待生成</details> |
| 2025-09-26 | Developing Vision-Language-Action Model from Egocentric Videos | http://arxiv.org/abs/2509.21986 | <details><summary>展开</summary>## 论文研究单位<br>京都大学、国立情报学研究所（NII）、东京科学研究所、NII LLMC（东京）、索尼互动娱乐（东京）<br>作者：Tomoya Yoshida（京都大学）、Shuhei Kurita（NII、东京科学研究所）、Taichi Nishimura（索尼互动娱乐）、Shinsuke Mori（京都大学）<br><br>## 论文概述<br>论文探讨了利用自我中心视频（egocentric videos）训练视觉-语言-动作模型（Vision-Language-Action models, VLAs）的可行性。传统方法依赖昂贵的人类远程操作数据，导致数据稀缺。论文提出通过EgoScaler框架从自我中心视频中自动提取6DoF对象操作轨迹，构建大规模预训练数据集，并在π0架构上验证其有效性。实验在仿真和真实环境中进行，结果显示该方法可显著提升任务成功率。<br><br>## 论文核心贡献点<br>1. 成功从自我中心视频训练π0模型，无需辅助标签（如手部姿态）。<br>2. 构建的预训练数据集在性能上与真实机器人数据集竞争。<br>3. 结合自我中心数据和真实机器人数据可进一步提升性能。<br><br>## 论文方法描述<br>- **EgoScaler框架**：用于从自我中心视频中提取6DoF对象操作轨迹。<br> - 步骤：识别操作起止时间及对象；开词汇分割和3D点跟踪提取位置序列；点云配准投影到相机坐标系；奇异值分解计算旋转序列。<br>- **数据处理**：应用于Ego4D、Ego-Exo4D、HD-EPIC和Nymeria四个数据集。初始提取124,559 episodes，经规则过滤（移动距离阈值、背景轨迹相似性阈值）和平滑处理后，得到45,157 episodes。<br>- **训练策略**：使用π0架构预训练和后训练。动作表示为6DoF位移（平移+旋转），预训练优化MSE损失。后训练时合并真实机器人数据集并归一化动作维度。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：<br> - 自我中心视频数据集：Ego4D、Ego-Exo4D、HD-EPIC、Nymeria（用于预训练）。<br> - 真实机器人数据集：BC-Z、BridgeData V2、Fractal（用于对比）。<br>- **训练资源**：<br> - 硬件：8×H200 GPUs。<br> - 优化器：AdamW（学习率5×10^-5）。<br> - 预训练步数：20,000步；后训练步数：40,000步。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真环境：SIMPLER（基于BridgeData V2的pick-and-place任务）。<br> - 真实环境：ALOHA（语言引导的pick-and-place任务）。<br>- **评估指标**：<br> - 成功率：成功次数/总次数。仿真环境200次rollout；真实环境10次rollout。<br> - 真实环境评分：抓取正确对象得0.5分，正确放置得0.5分。</details> |
| 2025-09-25 | RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models | http://arxiv.org/abs/2509.21243 | <details><summary>展开</summary>## 论文研究单位<br>- 加川大学（School of Computing, Gachon University）<br><br>## 论文概述<br>- 针对VLA模型效率与性能的权衡难题，RetoVLA提出“复用Register Tokens”这一被ViT用于去伪影的中间信息，作为“空间上下文”直接注入Action Expert，在维持轻量结构的同时增强空间推理能力。<br>- 基于LIBERO基准、自建仿真环境以及自建7-DOF机械臂进行评估，实机与仿真均显示在长时序与三维空间理解任务上显著提升。<br><br>## 论文核心贡献点<br>- 重新定义Register Tokens：从“去伪影的净化器”转为“空间上下文提供者”，设计可学习门控将其KV注入Action Expert的最后一层交叉注意力，实现语义与全局空间双流融合。<br>- 证明复用信息可弥补轻量模型（如SmolVLA）深度压缩导致的全局理解能力损失，是替代“信息缩减”的新范式。<br>- 三类实验验证：标准化仿真（LIBERO）、自建物理仿真（Unity+MuJoCo）、真实7-DOF机械臂上取得大幅提升（尤其长时序与3D空间任务）。<br><br>## 论文方法描述<br>- 架构<br> - 采用浅层VLM主干（SmolVLM2-500M的前16层）以保证效率；维持单流语义特征与双流信息融合路径。<br>- 空间上下文注入（核心）<br> - Register Token生成：用可学习初始向量与图像patch做注意力，生成场景依赖的Register Tokens。<br> - 注入与融合：将Register Tokens投影为KV并与VLM的KV在Action Expert最后一层交叉注意力处拼接。<br> - 门控：引入可学习标量经sigmoid调制注入强度，避免对极精细局部控制造成干扰。<br>- 训练目标<br> - 条件流匹配（CFM）：学习将噪声动作序列朝真实动作序列演化的向量场，MSE损失优化。<br><br>## 论文使用数据集和训练资源<br>- 数据与任务<br> - LIBERO四类基准：Spatial、Object、Goal、10(Long)。<br> - 真实环境：自建7-DOF机械臂，7项操控任务，收集1,804条episode进行微调。<br> - 自建仿真：Unity + MuJoCo插件，对应部分真实任务。<br>- 训练资源（报告为主干与步骤细节）<br> - 主干：SmolVLM2-500M（使用前16层）。<br> - 训练步数：100k；批量：64。<br> - 注入寄存器：2个Register Tokens（消融最优）。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境<br> - 仿真：LIBERO基准与自定义Unity+MuJoCo仿真。<br> - 真实：7-DOF机械臂，多视角（顶视/侧视/腕部）。<br>- 评估指标<br> - 主要指标：成功率（Success Rate, SR）、平均成功率（MSR）。<br> - 性能对比：SmolVLA vs RetoVLA。<br>- 关键结果摘要<br> - 真实机器人MSR：50.28% → 67.42%，绝对提升17.14%p。<br> - 仿真MSR：62.8% → 74.8%，提升12.0%p。<br> - 长时序与复杂空间任务提升显著（如Build Domino Line、Close Drawer、Jenga等）；对极精细局部操作有轻微权衡。<br> - LIBERO各类总体提升有限，但针对工作记忆与三维空间理解的任务明显受益。</details> |
| 2025-09-25 | Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning | http://arxiv.org/abs/2509.21126 | <details><summary>展开</summary>## 论文研究单位<br>武汉大学，中国。<br><br>## 论文概述<br>本文提出VARL框架，利用视觉语言模型(VLM)作为动作顾问，在在线强化学习(RL)中提供动作建议，旨在改进样本效率，特别是在稀疏奖励任务中。方法通过集成VLM建议的动作，而非修改奖励函数，丰富样本多样性并确保策略最优性和收敛性。论文在多种环境和代理设置中验证了框架的有效性。<br><br>## 论文核心贡献点<br>- 提出VLM作为动作顾问的框架，而非奖励设计师，保证策略最优性。<br>- 设计门控机制（离散和连续动作空间）防止策略过度拟合VLM动作。<br>- 通过策略塑形在早期训练阶段提供指导，提高样本效率。<br>- 低计算开销：相比奖励塑形方法，显著减少VLM查询次数（如VARL仅3次查询对比奖励塑形方法的5000次）。<br>- 适用于状态和视觉基础任务、离散和连续动作空间。<br><br>## 论文方法描述<br>VARL框架包含两个组件：<br>- VLM动作生成器：定期采样最新状态-动作对，查询VLM生成启发式动作并存储至引导缓冲区。<br>- 策略塑形模块：将VLM动作集成至策略训练，通过行为克隆损失和门控函数塑形策略，在指定步数后移除启发式动作。<br>具体算法基于软演员-评论家(SAC)，使用GPT-5作为VLM，损失函数包括基线策略损失、行为克隆损失和门控函数。<br><br>## 论文使用数据集和训练资源<br>- 环境：Meta-World（操控）、AI2-THOR（导航）、真实世界（Realman Robotics RM-65B臂）。<br>- 任务：10个任务，包括状态基础（Drawer Open、Sweep Into、Soccer）、视觉基础（Pick Up Plate、Open Phone、Toggle Off Lamp、Drawer Open、Push Cube）、真实世界视觉基础（Target Reach、Push Cube）。<br>- 训练资源：最大步数500,000，参数λ=10、移除步数N_s=30,000，使用SAC求解器和GPT-5 VLM。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：10个任务，分为三类（状态基础操控、视觉基础操控和导航、真实世界视觉基础操控）。<br>- 评估指标：成功率、学习曲线（平均回报或成功率），与基线方法（SAC、SAC+专家数据、RL-VLM-F、ERL-VLM）比较，包括样本效率分析和消融研究。</details> |
| 2025-09-25 | AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation | http://arxiv.org/abs/2509.21006 | <details><summary>展开</summary>### 论文研究单位<br>Skolkovo Institute of Science and Technology（莫斯科斯科尔科沃理工学院）智能空间机器人实验室。<br><br>### 论文概述<br>AnywhereVLA是一个模块化框架，用于在未见过的大型室内环境中实现自然语言驱动的抓取-放置任务。系统将语言指令转换为结构化任务图，结合LiDAR-SLAM、度量语义映射和主动探索策略，并在接近模块引导下由细调的SmolVLA模型执行操作。在消费级硬件（Jetson Orin NX和Intel NUC）上实时运行（≥10Hz），在实验室环境（静态场景和人类活动）中实现46%整体成功率，结合经典导航与VLA操作确保鲁棒性与灵活性。<br><br>### 论文核心贡献点<br>- **统一模块化框架**：单一语言指令同时驱动环境探索和VLA操作。<br>- **实时性能**：嵌入式设备上实现≥10Hz的全模块运行。<br>- **环境适应能力**：支持未见过的动态场景，探索并定位目标对象。<br>- **方法融合**：结合经典SLAM导航与轻量级VLA（SmolVLA 450M）模型，提升操作可靠性与泛化能力。<br><br>### 论文方法描述<br>- **架构模块**：<br> - **3D语义映射（SM）**：LiDAR-惯性-视觉SLAM构建语义点云，通过LiDAR密集化（插值环间点）、对象聚合（DBSCAN聚类）和置信度估计生成目标地图。<br> - **主动环境探索（AEE）**：前沿驱动的探索策略，条件化目标类，优化视野覆盖并抑制虚假目标。<br> - **接近模块（Approach）**：基于语义地图计算安全接近姿势，验证无碰撞导航。<br> - **VLA操作**：细调SmolVLA模型生成操作动作，集成多视角相机输入。<br>- **工作流**：解析语言指令→触发SM和AEE→SM构建地图→AEE定位目标→Approach导航→VLA执行操作。<br>- **硬件部署**：Jetson Orin NX处理感知和VLA，Intel NUC处理SLAM和导航。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：SO-101操作器抓取-放置轨迹50条（遥操作采集）。<br>- **训练资源**：RTX 4090 GPU（16GB VRAM）细调SmolVLA（批量16，学习率0.0001，余弦退火，AdamW优化，梯度裁剪10.0）。<br>- **部署硬件**：Jetson Orin NX（16GB）和Intel NUC（Core i7, 32GB）。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：未见过的大型室内多房间实验室（动态场景，正常人类活动）。<br>- **任务**：50次抓取-放置试验（目标对象随机分布在半径内）。<br>- **指令模板**：自然语言指令，如“Pick up the <object> and place it in the <area>. And bring the <object> to <location>.”<br>- **评估指标**：<br> - 成功率（SR）：整体SR 46%，VLA操作模块SR 85%（细调后）。<br> - 模块性能：SLAM SR 100%，AEE SR 75%，导航SR 90%，对象检测SR 85%，VLA操作SR 80%。<br> - 实时性能：总任务时间（探索半径5m时平均133秒，10m时<10分钟）。<br> - 计算效率：模块频率≥10Hz（如表I所示）。</details> |
| 2025-09-25 | ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation | http://arxiv.org/abs/2509.20841 | <details><summary>展开</summary>### 论文研究单位<br>德坤卢、高伟和贾奎（Dekun Lu, Wei Gao, and Kui Jia）- 作者信息未明确标注所属机构<br><br>### 论文概述<br>论文提出了一种名为ImaginationPolicy的端到端机器人操作策略，旨在解决传统模块化管道信息丢失和特征对齐问题。该方法通过**可操作性导向的关键点**实现对多样化操作任务的统一表示，显著提升机器人在抓取、倒水等任务中的泛化能力和精度。研究采用Chain of Moving Oriented Keypoints (CoMOK)作为动作表示，将操作行为转化为关键点序列，使网络能在不同形状/尺寸物体间实现**亚厘米级精度**。<br><br>### 论文核心贡献点<br>1. **创新动作表示**：提出CoMOK（移动方向关键点链）作为统一操作表示，突破末端执行器姿态表示限制<br>2. **任务泛化能力**：基于可操作性关键点自然适配不同物体形状/尺寸，包括可变形物体<br>3. **多任务统一框架**：单神经网络同时处理抓取、倒水、稳定放置等任务<br>4. **端到端训练**：融合Groma视觉语言模型进行任务规划，Score-Matching网络生成动作分布<br>5. **复杂场景适配**：支持多阶段操作、多模态行为及轨迹动作生成<br><br>### 论文方法描述<br>**CoMOK核心公式**：<br>```<br>网络输出 (o_manipulated, T_affordance, T_action)<br>```<br>- **o_manipulated**: 机器人控制对象（工具/物体/局部区域）<br>- **T_affordance**: 操作方向关键点（如杯子把手、缆绳抓取点）<br>- **T_action**: 目标对齐姿态（若T_affordance与T_action对齐则任务完成）<br><br>**扩展机制**：<br>- **多阶段操作**：全局任务描述自动分解子任务（如倒水任务拆解为"抓杯子→倒水→放置"）<br>- **多模态候选**：扩散模型生成动作分布（如多个抓取位姿可选）<br>- **轨迹序列**：输出SE(3)位姿序列（如切割水果的上下两帧）<br><br>**神经网络架构**：<br>1. **任务规划层**：基于Groma VLM，从RGBD图像和全局指令生成子任务列表<br>2. **动作预测层**：点云输入+阶段任务特征，通过Score-Matching生成关键点序列<br>3. **轨迹生成层**：<br> - 仿真：Motion Policy Networks学习生成关节空间轨迹<br> - 实机：传统任务与运动规划算法筛选可行轨迹<br><br>### 论文使用数据集和训练资源<br>- **抓取检测**：复用现有点云数据集（SE(3)-DiffusionFields来源）<br>- **仿真实验**：三类物体稳定放置任务<br> - 20个瓶子、10个盒子、5个三脚架<br> - 随机放置5个障碍物<br>- **实机实验**：6种缆绳+3种挂钩的多样化操作<br>- **硬件配置**：Rokae SR5六自由度机械臂 + 末端RGBD传感器<br><br>### 论文使用的评估环境和评估指标<br>**评估任务**：<br>1. **抓取检测**：双帧输出（预抓取+抓取）<br>2. **稳定放置**（仿真）：<br> - 成功率：距离桌面≤1cm，Z轴偏差≤15°<br> - 碰撞检测：放置后与障碍物碰撞<br>3. **缆绳插入**（实机）：缆绳是否插入夹具<br>4. **挂钩挂杯**（实机）：20种杯子×3种挂钩<br><br>**评估指标**：<br>- **精度**：位置误差（厘米级）、姿态误差（角度）<br>- **泛化性**：跨物体形状/尺寸成功率<br>- **可靠性**：任务级成功率（不含中间路径错误）<br>- **动态适应**：可变形物体（缆绳）操作成功率<br><br>**结论**：方法实现跨任务统一操作，成功率从稳定放置的仿真数据到实机缆绳插入均有验证，平均执行精度达厘米级且对物体变化鲁棒。</details> |
| 2025-09-24 | Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving | http://arxiv.org/abs/2509.20109 | <details><summary>展开</summary>## 论文研究单位<br>- **LiAuto**（理想汽车）<br>- **清华大学**（Tsinghua University）<br><br>## 论文概述<br>本文针对端到端自动驾驶系统中模仿学习无法内在编码物理规则（如碰撞避免、遵守可行驶区域）的核心挑战，提出了一种名为 **ReflectDrive** 的创新框架。该框架将离散扩散模型应用于轨迹规划，并引入**安全反射机制**（Safety-Guided Regeneration），在推理阶段通过迭代局部搜索和安全锚点重绘，无需梯度计算即可实现轨迹的安全自校正，从而提升规划的可控性和可靠性。<br><br>## 论文核心贡献点<br>1. **首次将离散扩散应用于端到端自动驾驶规划**：将连续轨迹空间离散化为动作码本，利用预训练的扩散语言模型（DLM）进行微调，实现并行解码和双向特征融合。<br>2. **提出安全反射机制**：在推理阶段结合目标条件生成（Goal-Conditioned Generation）和安全导向再生，通过局部搜索识别不安全点并替换为安全锚点，再利用扩散模型的修复能力（inpainting）重新生成轨迹。<br>3. **验证安全约束的有效性**：在真实场景基准测试中，该框架在确保硬安全约束的同时，显著提升了轨迹质量，且性能接近人类驾驶水平。<br><br>## 论文方法描述<br>1. **轨迹离散化（Trajectory Discretization）**：<br> - 将二维驾驶空间中的连续坐标 `(x, y)` 分别映射到预定义的1D码本 \(\mathcal{A}\) 中，生成轨迹的离散化序列 \(\mathbf{y}\)。<br> - 使用均匀网格量化坐标范围 \([-M, M]\)，分辨率 \(\Delta_g\) 可控，确保离散化后的轨迹可通过逆量化恢复为连续坐标。<br><br>2. **离散扩散模型（Discrete Diffusion Model）**：<br> - 基于离散扩散框架（掩码-去噪），采用预训练的VLA模型（如LLaDA-V）作为主干网络。<br> - 通过监督微调训练模型，使其具备根据场景上下文生成离散轨迹序列的能力。<br><br>3. **安全反射推理（Reflective Inference）**：<br> - **目标条件生成**：对终点位置进行概率采样，应用非极大值抑制（NMS）获取空间多样化的目标点，生成多模态轨迹候选。<br> - **安全导向再生**：<br> - **全局评分器**：评估完整轨迹的安全性和质量。<br> - **安全评分器**：定位不安全路标点。<br> - **局部搜索**：在路标点附近（如曼哈顿距离 \(\delta \leq 10\)）搜索可行替代点作为“安全锚点”。<br> - **轨迹修复**：固定安全锚点，通过扩散模型重绘周围轨迹段。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：基于大规模真实世界自动驾驶基准 **NAVSIM** 的数据。<br>- **训练资源**：<br> - 输入：前、左前、右前摄像头图像及语言指令（如“左转”）。<br> - 模型初始化：预训练的扩散语言模型（如LLaDA-V）。<br> - 训练方法：监督微调（SFT），批量大小为16，学习率 \(1 \times 10^{-5}\)，训练3轮。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - **NAVSIM 基准**：使用官方闭环仿真器进行评估。<br>- **评估指标**：<br> - **PDMS 分数**（主要指标）：聚合5项指标（分数越高越好）：<br> - **NC**（No-Collision）：无碰撞率（防止事故）。<br> - **DAC**（Drivable Area Compliance）：可行驶区域合规性（遵守道路边界）。<br> - **TTC**（Time-to-Collision）：碰撞时间安全性（避让反应时间）。<br> - **Comfort**：舒适性（加速度/急动度受控）。<br> - **EP**（Ego Progress）：ego进展（路径完成度）。<br><br>**结果摘要**（以相机输入为例）：<br>\| 方法 \| DAC ↑ \| TTC ↑ \| NC ↑ \| EP ↑ \| PDMS ↑ \|<br>\|-------------------\|-------\|-------\|------\|------\|--------\|<br>\| ReflectDrive \| **99.3%** \| 93.5% \| 97.7% \| **86.9%** \| **91.1** \|<br>\| ReflectDrive（无反射） \| 95.4% \| 92.2% \| 96.9% \| 79.0% \| 84.8 \|<br>\| **人类表现** \| 100.0% \| 100.0% \| 100.0% \| 87.5% \| 94.8 \|<br><br>> **结论**：ReflectDrive 的安全反射机制显著提升轨迹安全性（如 DAC 提升 +3.9）同时改善路径完成度（EP 提升 +7.9），接近人类驾驶水平。</details> |
| 2025-09-24 | FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models | http://arxiv.org/abs/2509.19870 | <details><summary>展开</summary>论文研究单位<br>- 复旦大学<br>- 上海人工智能实验室<br>- Sea AI Lab<br><br>论文概述<br>- 针对视觉-语言-动作（VLA）模型提出并形式化“行动冻结”（action-freezing）攻击：对抗图像使机器人忽略后续指令，进入持续无响应状态，可能在关键时刻导致不作为，引发实际安全风险。<br>- 提出FreezeVLA框架，通过最小-最大双层优化生成跨提示可迁移的对抗图像，在三个SOTA VLA模型与四个机器人基准上实现高攻击成功率，显著优于现有方法。<br><br>论文核心贡献点<br>- 识别并形式化VLA模型的“行动冻结”漏洞，揭示与错误动作同等严重的“不作为”威胁。<br>- 提出FreezeVLA：使用内部最大化（对抗提示优化）与外部最小化（对抗图像优化）的双层优化方法，扩展提示嵌入空间覆盖，显著提升跨提示迁移能力。<br>- 跨模型验证：SpatialVLA、OpenVLA、π0 上分别达到平均攻击成功率73.3%、95.4%、59.8%，全面超越随机噪声、PGD、多提示等基线。<br><br>论文方法描述<br>- 威胁模型与目标：白盒访问VLA模型、黑盒用户指令，通过图像扰动使模型在任意指令下稳定输出冻结令牌（如<freeze>、<eos>或“do-nothing”token），导致行动终止。<br>- 内部最大化（对抗提示优化）：从初始参考提示集出发，通过梯度分析定位高影响词并进行贪婪同义词替换，迭代生成更难被冻结的“硬提示”集，扩展提示语义空间。<br>- 外部最小化（对抗图像优化）：基于“硬提示”集合，对图像进行梯度聚合与更新，使VLA模型在多种提示下均以高概率输出冻结令牌，实现持久冻结。<br>- 形式化目标：min−max 双层优化，外层在 L∞ 扰动预算 ε 内最小化冻结损失，内层最大化难提示下的损失，使对抗图像在跨提示下保持有效性。<br>- 提示多样性增强：对比“随机采样”与“GPT生成”提示，显示GPT生成的语义多样性进一步提升攻击迁移性。<br><br>论文使用数据集和训练资源<br>- 数据集与基准：LIBERO-10、LIBERO-Goal、LIBERO-Object、LIBERO-Spatial四个机器人操控基准。<br>- 目标模型：SpatialVLA、OpenVLA、π0 三种SOTA VLA模型（具备动作分块/离散化/连续动作头等不同架构与冻结令牌策略）。<br>- 训练与实验资源：HPC集群，32×NVIDIA A800-SXM4-80GB GPUs。<br><br>论文使用的评估环境和评估指标<br>- 评估指标：攻击成功率（ASR），定义为使VLA模型进入一致冻结状态的对抗图像比例。<br>- 评估设置：<br> - 扰动预算：L∞ 约束下的 ε=4/255（主实验），对比 ε∈{1/255,2/255,4/255,8/255,16/255} 的敏感性。<br> - 优化步数与提示数：图像迭代K=100、步长α=1/255；参考提示规模\|P\|=20；内部提示迭代M=10。<br> - 基线对比：随机噪声、Single-Prompt PGD、Multi-Prompt（随机/随机+GPT）、FreezeVLA（随机）与FreezeVLA+GPT。<br> - 冻结策略差异：SpatialVLA/π0 使用 <eos>；OpenVLA 使用“do-nothing”token。<br>- 主要结果：FreezeVLA+GPT 在 ε=4/255 下实现平均ASR 73.3%（SpatialVLA）、95.4%（OpenVLA）、59.8%（π0），并具有强跨提示可迁移性与随预算提升的稳定增益。</details> |
| 2025-09-24 | Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training | http://arxiv.org/abs/2509.19752 | <details><summary>展开</summary># 论文研究单位<br>- 香港科技大学<br>- 微软亚洲研究院<br>- 武汉大学<br>- 中国科学院大学<br>- 清华大学<br>- 中南大学大数据研究院<br><br># 论文概述<br>当前视觉-语言-行动（VLA）模型高度依赖大规模人工演示数据，但人工数据采集成本高、可扩展性差。强化学习（RL）能自动生成策略，但对长时程、奖励稀疏的操控任务效果不稳且轨迹方差大。文章提出以扩散模型为核心的RL数据生成方法，利用扩散策略的强表达能力和迭代去噪的隐式正则，获得高质、低方差的轨迹，构建“扩散RL → 数据生成 → VLA训练”的完整管线。在LIBERO基准（130个长时操控任务）上验证，扩散RL生成数据训练出的VLA模型平均成功率81.94%，相比人类数据+5.3%、相比高斯RL+12.6%，并揭示了轨迹平滑、低方差特性是提升VLA性能的关键。<br><br># 论文核心贡献点<br>- 面向VLA训练的扩散RL数据生成框架：两阶段训练（BC暖启动 + 在线PPO），在架构、采样与训练流程上提出稳定化改进。<br>- 实证证明扩散RL数据的优越性：在LIBERO的130个任务上显著提高VLA的域内成功率与部分泛化表现。<br>- 轨迹质量量化分析：任务效率、轨迹平滑（均方 jerk）、动作一致性（低方差）三个维度解释数据质量提升如何转化为VLA性能增益。<br>- 消融验证稳定性：ResNet+U‑Net、DDIM 5步采样、余弦退火学习率、并行环境增强数据多样性等设计的必要性。<br><br># 论文方法描述<br>- 策略与采样<br> - 扩散策略：将动作视为由K步扩散过程生成，训练噪声预测网络 εθ(a_k, s_t, k) 反演去噪得到最终动作 a_0。使用DDIM（5步）加快采样、降低方差，保证稳定RL梯度。<br> - 两阶段训练<br> - 阶段一：行为克隆（BC）暖启动。用少量人类多模态演示训练扩散策略，拟合复杂演示分布（公式1）。<br> - 阶段二：在线PPO强化学习微调。以价值网络Vϕ估计状态价值，GAE计算优势，PPO clip目标在每个去噪步上优化策略（公式2–5）。<br>- 稳定化策略<br> - 架构：ResNet主干+U‑Net解码用于多模态与样本效率；FiLM融合本体感觉（proprioception）稳定条件信号。<br> - 学习率：余弦退火，初期鼓励探索，后期稳定收敛。<br> - 数据多样性：并行环境扩充经验缓冲，防止高容量扩散策略在相关小批次上过拟合、模式坍缩。<br>- VLA训练<br> - 专家扩散RL策略收敛后收集数据 D_RL。使用标准行为克隆目标最大化对数似然（公式6）训练VLA，兼容不同策略架构（如π0/OpenVLA）。<br><br># 论文使用数据集和训练资源<br>- 数据集<br> - LIBERO基准：包含多套任务共计130个长时操控任务；常用于知识迁移评测。数据源包括人类演示与由不同方法（高斯RL、扩散RL）生成的轨迹。<br>- 训练流程资源（基于文中描述）<br> - 并行环境用于在线RL采集数据与多样经验回放。<br> - 用于对比的生成方法：人类数据（每任务50条演示）、高斯RL数据、扩散RL数据（每任务50条最优轨迹）、人类+扩散RL混合数据。<br> - VLA训练遵循统一预处理（如OpenVLA流程），超参数保持一致以保证公平对比。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境<br> - LIBERO任务套件，涵盖空间/目标/对象变化与长时任务（LIBERO‑Spatial/Goal/Object/Long），以及由LIBERO‑90与LIBERO‑Long组成的大规模套件（文中称为LIBERO‑100）。<br>- 评估指标<br> - 成功率（SR）：每套任务50次评估的平均成功率；OOD设定为在LIBERO‑90上训练、零样本评估其他未见任务套件。<br> - 轨迹质量分析<br> - 任务效率：平均轨迹长度与“无操作”比例（速度≈0且抓持状态不变）。<br> - 轨迹平滑度：末端执行器轨迹的均方 jerk，越低越平滑。<br> - 动作一致性：同一任务上多条成功轨迹的方差，越低越一致。</details> |
| 2025-09-23 | Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action | http://arxiv.org/abs/2509.19571 | <details><summary>展开</summary>论文研究单位<br>Université de Montréal；Mila - Quebec AI Institute；Sapienza University of Rome<br><br>论文概述<br>针对开放词汇自然语言指令的机器人执行难题，论文提出“Agentic Scene Policies”(ASP)。其核心思想是将语言指令分解为三类可查询操作：对象定位、空间推理与部件级交互，并基于显式场景表示(开放词汇对象地图、CLIP语义特征、3D点云与可 affordance 区域)为大型语言模型(LLM)代理提供可调用工具(检索、空间度量与交互技能)。系统支持桌面与移动操控，并通过 afforfance 驱动的技能库(如 tip_push、pinch_pull、hook_pull)实现零样本操作。<br><br>论文核心贡献点<br>- 统一空间、语义与 affordance 的场景查询式机器人策略(ASP)<br>- 在15项桌面操控任务上与主流 VLA(π0-FAST、π0.5)进行系统性比较，报告成功率与进展率<br>- 提出移动版 ASP，实现基于 affordance 引导的导航、跨关键帧地图聚合与到目标后的局部重检测<br><br>论文方法描述<br>- 对象地图(ObjectMap)：包含 Object(几何点云、CLIP语义特征、RGB/Depth裁剪、多视图)与 Affordance(point_cloud、part描述、对应技能)<br>- LLM 代理：通过工具链调用与符号状态(当前抓取对象与清单)执行查询计划，不直接访问传感器或完整地图<br>- 工具集：<br> - object_retrieval：基于文本与CLIP特征的开放词汇对象检索，返回对象键加入清单<br> - spatial：距离、左右关系、尺寸等点云几何运算<br> - interact：将自然语言动作与目标对象传给内部 affordance 检测与技能调度<br> - 技能(基于 AnyGrasp 与运动规划)：grasp、place、drop、grasp_part、tip_push、pinch_pull、hook_pull<br> - 反馈与重试：ToolOutput 结构返回成功/反馈信息，支持条件检查与策略重试<br>- remapping：技能失败或状态不明时触发地图重算<br>- 移动 ASP：新增 go_to 工具，利用目标 affordance 方向计算期望视角并进行代价优化；跨多帧 RGB-D 聚合对象地图；到达后在本地构建 ObjectMap 进行对象重检测<br><br>论文使用数据集和训练资源<br>- 无专用新数据集；方法在开放词汇场景表示与零样本 VLM 能力之上工作<br>- 移动版构建房间级对象地图：通过遥控采集1–5个关键帧，地图构建与合并约每查询<1分钟<br>- 计算资源：工作站配备 NVIDIA Titan RTX；RGB-D 通过 Wi-Fi 传输；推理组件包括 SAM/MobileSAM、CLIP、AnyGrasp、SLAM(RTABMap)与导航(Nav2)<br><br>论文使用的评估环境和评估指标<br>- 桌面操控：UFactory XArm6 + Intel Realsense D435i(腕部)；与 VLA 对比时采用 Franka 臂+DROID 设定以降低分布偏移<br>- 移动操控：Agilex Ranger Mini 2.0 移动基座 + XArm6 + 追踪相机 T265<br>- 指标：<br> - 成功率(10次/任务平均)<br> - 进展率(接近、尝试、部分完成等分级评分)<br> - 失败模式分析(感知、affordance 检测错误、把柄使用不当等)<br>- 实验规模：15项桌面任务，共540次尝试；移动端进行双物品放置、指尖/拉拔、空间区分等多类查询</details> |
| 2025-09-23 | OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation | http://arxiv.org/abs/2509.19480 | <details><summary>展开</summary>待生成</details> |
| 2025-09-23 | Pure Vision Language Action (VLA) Models: A Comprehensive Survey | http://arxiv.org/abs/2509.19012 | <details><summary>展开</summary># Pure Vision-Language-Action (VLA) Models: A Comprehensive Survey<br><br>## 论文研究单位<br><br>- 兰州大学信息科学与工程学院（中国）<br>- 新加坡国立大学NExT++研究中心（新加坡）<br>- 中国科学院计算技术研究所（中国）<br><br>## 论文概述<br><br>本论文是关于纯 Vision-Language-Action (VLA) 模型的综合调研，系统地回顾了该领域的研究进展。论文从传统基于策略的控制方法出发，聚焦于 VLA 如何将 Vision-Language Models (VLMs) 从被动序列生成器转变为主动的机器人和决策制定代理。调研覆盖了超过300项近期研究，提供了清晰的方法分类体系，分析了 VLA 在不同应用场景中的表现，并提出了未来研究的关键挑战和发展方向。<br><br>## 论文核心贡献点<br><br>- **系统化分类体系**：提出了基于动作生成策略的 VLA 方法分类，将现有方法分为自回归、扩散、强化学习和混合/专用方法四大类别<br>- **方法创新总结**：深入分析了每个类别的方法动机、核心策略和实现机制，强调了定义性特征和技术创新<br>- **资源全面概述**：提供了 VLA 模型训练和评估所需的关联资源（数据集、基准测试、仿真平台）的详细概览<br>- **挑战与机遇分析**：识别了现有技术的关键局限性，并提出了潜在的研究探索方向<br><br>## 论文方法描述<br><br>论文将 VLA 方法分为以下四个主要类别：<br><br>### 1. 自回归模型<br>- 将动作序列视为时间依赖过程，步进式生成动作<br>- 代表性方法包括 Gato、RT-1/RT-2、PaLM-E 等<br>- 进一步细分为：通用 VLA 方法、LLM 驱动的语义规划、轨迹生成与视觉对齐、结构优化与高效推理机制<br><br>### 2. 扩散模型<br>- 将动作生成建模为条件扩散过程<br>- 代表性方法包括 SE(3)-DiffusionFields、Diffusion Policy、3D Diffuser Actor 等<br>- 涵盖：扩散式通用 VLA 方法、多模态架构融合、应用优化与部署<br><br>### 3. 强化学习模型<br>- 基于强化学习的 VLA 微调策略<br>- 通过奖励机制优化策略性能<br><br>### 4. 混合与专用方法<br>- 结合多种范式的混合架构<br>- 针对特定领域和应用的专门设计<br><br>## 论文使用数据集和训练资源<br><br>### 真实世界数据集<br>- **Open X-Embodiment (OXE)**：整合了22个机器人数据集，包含527种技能和160,266个任务<br>- **BridgeData**：涵盖10个环境中的71个任务<br>- 自主驾驶领域的相关数据集<br><br>### 仿真数据集<br>- **Open X-Embodiment (OXE)** 仿真版本<br>- **BridgeData** 仿真版本<br>- 自主驾驶仿真数据集<br><br>### 仿真平台<br>- **THOR、Habitat**：室内环境仿真<br>- **MuJoCo、Isaac Gym**：物理仿真<br>- **CARLA**：自动驾驶仿真<br>- 提供可扩展的虚拟环境，支持多模态标注生成<br><br>## 论文使用的评估环境和评估指标<br><br>论文主要围绕以下应用领域进行评估：<br><br>### 机器人操作<br>- 机械臂控制<br>- 四足机器人<br>- 人形机器人<br>- 移动机器人<br><br>### 自主驾驶<br>- 2D/3D 感知融合<br>- 轨迹预测<br>- 闭环控制<br><br>### 评估指标<br>- 任务成功率<br>- 轨迹精度<br>- 跨平台泛化能力<br>- 零样本/少样本性能<br>- 实时推理效率<br>- 多模态对齐质量<br><br>论文强调需要建立更全面的评估框架，特别关注长时序任务稳定性、语义对齐鲁棒性和实际部署效率等关键指标。</details> |
| 2025-09-23 | Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations | http://arxiv.org/abs/2509.18953 | <details><summary>展开</summary>### 论文研究单位<br>上海交通大学（人工智能学院重点实验室）、中国军事科学院国防创新研究院、智能博弈决策实验室<br><br>### 论文概述<br>当前视觉-语言-动作（VLA）模型在真实部署中易受物理变化影响（如光照、物体位置干扰），但缺乏系统性评估工具。本文提出Eva-VLA框架，将离散物理变化转化为连续参数优化问题，通过黑箱优化算法探索最恶劣场景，暴露了当前VLA模型在多种物理干扰下的严重脆弱性。<br><br>### 论文核心贡献点<br>1. **首次系统性分解物理变化**：将真实世界变化分为物体3D变换（旋转）、光照变化、对抗补丁三类，突破传统梯度攻击限制<br>2. **统一参数化评估框架**：将各类变化转化为连续参数分布，通过仿真环境可复现地评估鲁棒性<br>3. **揭示VLA系统脆弱性**：在OpenVLA等先进模型中验证，物理干扰使失败率从23.5%激增至82.6%，长时序任务达97.8%<br><br>### 论文方法描述<br>#### 三类物理变化参数化<br>- **物体3D变换**：用Tait-Bryan角α,β,γ∈[-90°,90°]约束旋转参数<br>- **光照变化**：通过高斯衰减函数建模点光源L(z)=I·exp(-\|\|z-(x,y)\|\|²/2σ²)，参数λ={x,y,σ,I}控制位置/强度<br>- **对抗补丁**：用自然图像在桌面纹理上优化位置φ={x,y}∈[(W/3,2W/3)×(H/3,2H/3)]<br><br>#### 黑箱优化算法<br>使用CMA-ES（协方差矩阵自适应进化策略）优化参数分布：<br>1. 将变化参数建模为多元高斯分布N(μ,Σ²C)<br>2. 迭代采样配置，计算对抗损失函数L_adv=-∑cos(A_clean, A_adv)<br>3. 基于损失值更新分布参数μ, C, Σ<br>4. 引入学习率自适应和早停机制加速收敛<br><br>### 论文使用数据集和训练资源<br>- **仿真数据集**：LIBERO基准套件（包含空间/物体/目标/长时序四类任务，每类10任务×50轮试验）<br>- **真实数据集**：BridgeData v2（用于部分真实场景验证）<br>- **硬件资源**：NVIDIA A800 GPU (80GB内存) + AgileX Piper机械臂（7自由度） + RealSense D435i相机<br><br>### 论文使用的评估环境和指标<br>- **仿真评估**：基于LIBERO套件，按任务最大步长设超时标准，统计失败率FR=1-SR（成功率）<br>- **真实评估**：在三任务（抓取/定位/放置）上执行50次试验，攻击成功率44.6%<br>- **鲁棒性指标**：<br> - 失败率变化率：清洁环境(23.5%) vs 物理干扰(82.6%)<br> - 不同干扰类型影响：物体变换>对抗补丁>光照变化<br> - 时序影响：长时序任务在干扰下失败率超97%</details> |
| 2025-09-23 | Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation | http://arxiv.org/abs/2509.18865 | <details><summary>展开</summary># 论文研究单位<br>- D3 Center, The University of Osaka<br>- Graduate School of Information Science and Technology, The University of Osaka<br>- Graduate School of Maritime Sciences, Kobe University<br><br># 论文概述<br>提出 Bi-VLA（Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation），将双侧控制（bilateral control）的位置与力信息、视觉观测与自然语言指令融合，构建单模型多任务策略，在真实机器人上完成抓取-搬运-放置任务，相比传统双侧控制式模仿学习在任务成功率与适应性上显著提升。<br><br># 论文核心贡献点<br>- 融合视觉与语言：在双侧控制式模仿学习框架中引入 SigLIP 语言编码与 FiLM 调制，将视觉与语言对齐后统一嵌入决策。<br>- 单模型多任务：打破既往双侧控制方法仅能单任务建模的局限，实现一个策略在语言可区分与视觉可区分两类任务间灵活切换。<br>- 真实机器人验证：在两类任务与干扰环境中取得更高成功率，证明视觉-语言融合在接触丰富与动态环境下的有效性。<br><br># 论文方法描述<br>- 数据采集：采用四通道双侧控制，领导者与跟随者实时交换位置与力矩；关节角度、速度与外力矩由编码器、DOB 与 RFOB 估计；同步采集 RGB 图像与自然语言指令，形成多模态演示。<br>- 学习模型：文本用 SigLIP 编码，视觉用 EfficientNet 提取特征，经 FiLM 进行跨模态融合；融合特征与机器人关节状态共同输入 Transformer 驱动的 CVAE，输出动作块（领导者关节角度、速度、力矩）。<br>- 推理流程：给定跟随者当前关节状态、同步图像与语言指令，模型预测下一步动作块，经双侧控制转换为电流指令执行，实现闭环控制与灵活任务切换。<br><br># 论文使用数据集和训练资源<br>- 硬件：OpenManipulator-X 机械臂两台（领导/跟随），两个 RGB 摄像头（俯视与夹爪侧）。<br>- 数据频率：双侧控制 1000 Hz，图像 100 Hz；每臂 15 维状态（5 关节 × 角度/速度/力矩），合计 30 维联合状态。<br>- 任务演示：每任务收集 6 条演示（上/下各 3）；为评估跨任务低数据泛化，同时收集 4 条混合演示（上/下目标与上/下源各 1）。<br>- 数据增强：采用 DABI 将演示从 1000 Hz 下采样至 100 Hz 与图像对齐，6 条演示扩展至 60 条，4 条混合演示扩展至 40 条。<br>- 模型配置：Transformer 编码器 4 层、解码器 7 层，CVAE 结构，动作块预测。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：两类任务——语言可区分任务（目标位置由指令指定）、视觉可区分任务（拾取位置由场景指定），以及未学习的三球干扰环境。<br>- 评估指标：分阶段成功率（Pick/Move/Place）与总体成功率（各 10 次独立试验，仅当所有阶段均完成视为成功）。<br>- 对比模型：Bi-ACT（无语言）、Bi-VLA（DistilBERT）、Bi-VLA（SigLIP）、Bi-VLA（SigLIP-Mix）。<br>- 关键结果：<br> - 语言可区分任务：Bi-ACT 仅 50%（仅 Up 成功），Bi-VLA（DistilBERT）60%，Bi-VLA（SigLIP）90%，Bi-VLA（SigLIP-Mix）70%。<br> - 视觉可区分任务：Bi-ACT 95%，Bi-VLA（SigLIP）90%，Bi-VLA（SigLIP-Mix）90%。<br> - 未学习三球环境：Bi-ACT 50%，Bi-VLA（SigLIP）75%，Bi-VLA（SigLIP-Mix）75%。</details> |
| 2025-09-22 | Latent Action Pretraining Through World Modeling | http://arxiv.org/abs/2509.18428 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br><br>Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE<br>Alexandria University, Alexandria, Egypt<br><br>## 论文概述<br><br>本文提出了LAWM框架，通过世界建模从无标签视频数据中学习潜在动作表示，用于预训练模仿学习模型。该框架能够从机器人录制视频或人类操作日常物体的视频中进行自监督学习，无需依赖人工标注的动作数据。框架设计为模型无关，可以跨任务、环境和具身形态进行迁移，在LIBERO基准测试和真实世界设置中的表现优于使用真实机器人动作训练的模型和类似的预训练方法，同时更高效且适用于实际场景。<br><br>## 论文核心贡献点<br><br>1. 提出LAWM框架，这是一个模型无关的框架，可以从机器人和人类视频中学习动作块表示，无需动作标签<br>2. 实验表明该框架可以从人类演示和机器人操作视频中学习优于监督预训练的动作先验，且无需使用真实动作标签<br>3. 使用小型模型（BAKU和Dreamerv3）的框架在LIBERO基准测试上优于使用大型模型的类似方法（villa-X）<br><br>## 论文方法描述<br><br>LAWM包含两个阶段：<br><br>潜在动作预训练阶段：<br>- 输入包括图像帧和自然语言指令<br>- 模仿学习模型处理输入产生n个潜在动作表示<br>- 这些潜在动作与当前帧和后续n-1帧配对，输入世界模型<br>- 世界模型基于RSSM架构，包含编码器、动态模型和解码器<br>- 通过预测未来图像帧进行端到端训练，损失函数包括MSE重建损失和KL散度正则化项<br>- 学习信号来自预测视频序列中的下一帧图像<br><br>动作微调阶段：<br>- 预训练的模仿学习模型适配到下游机器人任务<br>- 不再使用世界模型<br>- 使用标注演示数据将观察（图像、语言指令和机器人状态）直接映射到真实动作<br>- 对BAKU采用负对数似然损失，对Diffusion Policy采用去噪扩散损失<br><br>## 论文使用数据集和训练资源<br><br>数据集：<br>- BridgeData v2：60,096条轨迹，24个环境，包含拾取放置、推动、折叠等任务<br>- Something-Something v2：220,847个人类操作日常物体的视频片段（使用其中10%用于预训练）<br>- LIBERO基准测试：包含LIBERO-90（90个任务）和四个任务套件（Spatial、Object、Goal、Long），每个套件10个任务，每个任务50个演示样本<br>- 真实世界自定义数据集：5个任务（3个拾取放置、1个堆叠、1个移动），每个任务50个演示，使用VR控制器收集<br><br>训练资源：<br>- 单个A100 GPU<br>- DreamerV3世界模型使用50M参数配置<br>- 潜在动作空间维度为7（与真实动作相同）<br>- BAKU实验的动作块大小为10，Diffusion Policy实验为16<br>- 在BridgeData v2或Something-Something v2上预训练约30小时<br>- 在LIBERO-90上微调约24小时<br>- 在LIBERO任务套件上微调约2小时<br>- 真实世界数据集微调约20分钟<br><br>## 论文使用的评估环境和评估指标<br><br>评估环境：<br>- LIBERO-90基准测试：90个多样化任务<br>- LIBERO任务套件：<br> - LIBERO-Spatial：新布局下的相同任务和对象类型<br> - LIBERO-Object：新对象类型下的相同任务和布局<br> - LIBERO-Goal：新任务下的相同对象类型和布局<br> - LIBERO-Long：长时域任务，包含多样化的对象、布局和背景<br>- 真实世界设置：6自由度Realman机器人臂，配备1自由度夹爪，双视角相机观测<br><br>评估指标：<br>- 成功率（Success Rate, SR）：成功试验次数占总尝试次数的百分比<br>- 每个任务进行10次评估试验<br>- 使用典型相关分析（CCA）的第一规范分量的Pearson相关系数来量化潜在动作与真实动作之间的对齐程度</details> |
| 2025-09-22 | PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies | http://arxiv.org/abs/2509.18282 | <details><summary>展开</summary>## 论文研究单位<br>University of Washington, NVIDIA, University of Southern California, Allen Institute for AI。<br><br>## 论文概述<br>当前机器人操控策略往往同时学习“关注哪里（where）”“做什么（what）”“如何执行（how）”，导致泛化能力受限。该论文提出将“where/what”的高层语义推理交由视觉语言模型（VLM）承担，使低层策略专注“how”。为此提出了PEEK（Policy-agnostic Extraction of Essential Keypoints），一个策略无关的最小中间表示框架：由VLM预测2D末端执行器路径（what）与任务相关掩蔽点（where），并将标注直接叠加到观测图像上，再由任意RGB/RGB-D策略进行训练与推理。在535次真实世界评估中，PEEK显著提升零样本泛化，包括模拟训练的3D策略在真实场景中成功率提升41.4倍，以及对大型VLA与小型操控策略分别带来2–3.5倍增益。<br><br>## 论文核心贡献点<br>- 统一的点式中间表示：将“路径+掩蔽点”作为策略输入，实现策略无关、可迁移的表示。<br>- 可扩展标注流水线：自动从机器人视频中提取任务相关点与末端执行器路径，支持不同视角与小目标。<br>- 多策略与多具身验证：在2D/3D、不同规模策略与两种真实机械臂系统上统一验证。<br>- 零样本泛化实证：面对视觉干扰与语义新任务大幅提升成功率与鲁棒性。<br><br>## 论文方法描述<br>- 目标表示：联合预测两个点集——末端执行器的2D轨迹点 p_t 与任务相关掩蔽点 m_t，两者作为自然语言响应由VLM输出。<br>- VLM微调：以VILA-1.5-3B为基座，整合机器人与通用点预测/VQA数据；联合优化路径与掩蔽点预测，训练约20小时（A100×8）。<br>- 数据标注流水线：先用点跟踪（CoTracker3）识别场景中显著移动的点集作为“任务相关点”，再以检测器+掩蔽机制构建末端执行器路径；通过“停止点数”K-Means将长轨迹切分为更短的“子轨迹”，以提高表示的最小性与可预测性。<br>- 策略接口：推理时每隔H步查询VLM，将路径与掩蔽点以可视化方式叠加到观测图像上，供任意RGB或RGB-D策略训练与执行。掩蔽通过对预测点为中心的8%边长区域开窗；路径以随时间颜色渐变的线段表示。<br><br>## 论文使用数据集和训练资源<br>- 数据来源：Open X-Embodiment（OXE）20+子数据集、DROID、LIBERO-90、BRIDGE-v2、RoboPoint；总计2M+问答/点预测对、148k轨迹、9种具身。<br>- 训练资源：VLM微调在8×NVIDIA A100上约20小时；推理在RTX 3090上单次查询约4–6秒。<br>- 工具/模型：CoTracker3点跟踪、Detectron2末端执行器检测、FoundationStereo深度估计。<br><br>## 论文使用的评估环境和评估指标<br>- 评估环境：<br> - Franka（Sim-to-Real）：仿真采集2.5k条“堆叠彩色方块”轨迹；真实世界Zed 2立体相机+FoundationStereoDepth；评估Basic/Clutter/Semantic三类任务。<br> - WidowX（BRIDGE）：单目RGB环境，改换桌面与背景；在Basic/Clutter/Semantic三类任务上评估。<br>- 评估指标：任务完成率与成功率（平均与分项），包含抓取/到达的Partial Credit；零样本跨场景与跨语义泛化能力；消融实验使用路径/掩蔽的组合成功率；VLM质量使用DTW、起点/终点L2、IoU。</details> |
| 2025-09-18 | VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation | http://arxiv.org/abs/2509.18183 | <details><summary>展开</summary>### 论文研究单位<br>Li Auto Inc.，中国北京<br><br>### 论文概述<br>VLA模型在视觉视角变化时（如不同位置、背景或高度）泛化性能下降，影响任务执行。论文提出轻量级视角适应融合框架VLA-LPAF，通过潜在空间融合多视角2D图像，仅使用单视图数据进行微调，弥补视角差异。与RoboFlamingo结合构建RoboFlamingo-LPAF，在CALVIN、LIBERO和自定义CabinEnv数据集上提升成功率8%-30%，并在真实任务中验证视角适应能力。<br><br>### 论文核心贡献点<br>- 首次实现基于2D图像的轻量级潜在视角特征融合框架VLA-LPAF，降低视角一致性约束。<br>- 实例化VLA-LPAF为RoboFlamingo-LPAF，通过多数据集和真实任务验证有效性。<br>- 三阶段训练策略（单视图仅动作、多视图仅融合、多视图联合训练）优化泛化能力。<br><br>### 论文方法描述<br>- **问题定义**：VLA模型映射多模态输入（图像、文本）到动作，但视角差异导致特征偏移。<br>- **架构**：添加MLP基融合模块，利用单视图参考数据集（D_R）和多视图辅助数据集（D_M)。<br>- **对齐融合**：融合模块在ViT编码的潜在空间中，对齐参考视图（R）和辅助视图（M）的特征。<br>- **训练策略**：<br> 1. 单视图阶段：冻结ViT，微调LLM参数（θ），使用动作损失（公式1）。<br> 2. 多视图阶段：仅训练融合模块参数（θ'），使用对齐损失（公式4）。<br> 3. 联合阶段：同时微调θ和θ'，结合动作损失（公式3）和对齐损失（公式5）。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - 模拟：CALVIN、LIBERO、CabinEnv（自定义舱内环境，包含按钮按压和杠杆翻转任务）。<br> - 数据集构建：参考视图（0°）+ 辅助视图（±45°范围，v=4个视角）。<br>- **训练资源**：8个NVIDIA A800 80GB GPU，图像统一尺寸224×224。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 模拟环境：使用多视角测试数据（如CALVIN中参考视图±90°范围，间隔10°）。<br> - 真实环境：Realman RML机器人臂（单臂），Intel RealSense D415（全局参考和腕部摄像头），Azure Kinect（辅助全局摄像头），执行按钮按压和杠杆翻转任务。<br>- **评估指标**：任务成功率（success rate），基线比较显示CALVIN平均提升8%，LIBERO提升15%，CabinEnv提升30%，并在真实任务中验证RoboFlamingo-LPAF完成未包含视角（如30°）的任务。</details> |
| 2025-09-22 | Prepare Before You Act: Learning From Humans to Rearrange Initial States | http://arxiv.org/abs/2509.18043 | <details><summary>展开</summary>待生成</details> |
| 2025-09-20 | ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering | http://arxiv.org/abs/2509.16680 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>- Dartmouth College（达特茅斯学院）<br>- Shandong University（山东大学）<br>- Harvard University（哈佛大学）<br><br>## 论文概述<br>论文提出了ProtoVQA，这是一个用于可解释细粒度视觉问答的统一原型框架。该框架通过学习问题感知的原型作为推理锚点，将答案与判别性图像区域连接起来，并应用空间约束匹配来确保选择的证据具有连贯性和语义相关性。通过共享原型主干网络，该框架支持视觉问答和定位任务，并提出了视觉-语言对齐评分（VLAS）来评估解释质量。<br><br>## 论文核心贡献点<br>1. 引入适应性原型框架，能够无缝处理不同的视觉-语言下游任务，包括视觉问答和定位<br>2. 采用空间约束的贪婪匹配策略建模动态视觉问题关系和几何变化<br>3. 通过明确的视觉证据和系统化的视觉-语言对齐验证实现全面的可解释性<br><br>## 论文方法描述<br>**框架组成：**<br>- **特征提取模块**：使用DeiT作为视觉特征提取器，DeBERTa作为文本编码器，将图像和文本特征投影到共享的视觉-语言空间<br>- **可解释原型部分选择模块**：引入子补丁原型（m×k结构）和贪婪匹配算法，通过空间约束选择与原型最匹配的图像区域<br>- **答案处理**：支持两种类型——Type 1（视觉定位）和Type 2（描述性问答），分别处理坐标输入和文本答案<br>- **视觉-语言对齐评估**：提出VLAS指标，测量模型关注区域与真实证据的对齐程度<br><br>**核心技术：**<br>- 子补丁原型：每个原型由k个子补丁组成，形成语义锚点<br>- 空间约束贪婪匹配：通过迭代选择相似度最高的补丁-子补丁对，并使用邻接掩码确保空间连续性<br>- 权重共享机制：在问题编码和答案处理之间共享特征投影器参数<br><br>## 论文使用数据集和训练资源<br>**数据集：**<br>- Visual7W：包含327,939个问题-答案对，覆盖47,300张COCO图像，每个问题配有4个人工选择选项和561,459个对象级定位标注<br><br>**训练资源：**<br>- 硬件：NVIDIA A800 GPU（80GB）<br>- 训练配置：200个epoch，Adam优化器，学习率1×10^-4，批大小64<br>- 图像处理：224×224像素，16×16补丁<br>- 原型参数：m=10个原型，k=3个子补丁，空间约束半径r=3<br><br>## 论文使用的评估环境和评估指标<br>**评估环境：**<br>- 主要在Visual7W测试集上进行评估<br>- 对比基线包括SUPER、QOI_Attention、SDF of VLT、STL、CFR、BriVL、CTI、Bi-CMA等代表性VQA模型<br><br>**评估指标：**<br>- **准确性指标**：分类准确率（Accuracy）<br>- **解释质量指标**：视觉-语言对齐评分（VLAS），通过IoU阈值（θ=0.5）计算模型关注区域与真实标注的对齐程度<br>- **可视化分析**：定性展示模型选择的图像区域与真实标注的对应关系<br><br>**实验结果：**<br>- ProtoVQA在Visual7W上达到70.23%准确率，与强基线模型相当<br>- 在VLAS指标上显著优于基线方法，VLAS@1达到0.4103（比Bi-CMA提升66.4%），VLAS@3达到0.2466（比Bi-CMA提升119.6%）<br>- 定性分析显示模型能够准确关注与问题相关的语义区域</details> |
| 2025-09-19 | Randomized Smoothing Meets Vision-Language Models | http://arxiv.org/abs/2509.16088 | <details><summary>展开</summary>## 论文研究单位<br>- National Technical University of Athens, Athens, Greece<br>- Université Grenoble Alpes, Grenoble, France<br>- CSX‑AI, Grenoble, France<br>- Carl von Ossietzky University of Oldenburg, Oldenburg, Germany<br>- Chalmers University of Technology, Gothenburg, Sweden<br><br>## 论文概述<br>随机化平滑（Randomized Smoothing, RS）是目前在大规模分类模型上唯一可行的鲁棒性认证技术，但其原本只能处理离散标签，难以直接用于生成式的视觉‑语言模型（Vision‑Language Models, VLMs）。本文提出一种将生成式模型的输出映射为分类问题的通用方法，从而把 RS 迁移到 VLMs 场景。核心思路是引入一个“oracle”分类层，对模型生成的文本进行二分类（如有害/无害）或离散动作映射，或将语义等价的答案归为同一等价类。通过对图像添加高斯噪声、保持提示文本不变、进行多次采样并使用 oracle 进行投票，得到多数类的置信下界，进而推导出可验证的鲁棒半径。论文还从理论上分析了样本数对半径和认证精度的影响，给出改进的样本复杂度标度律，使得在实际应用中只需要 10²‑10³ 级别的样本即可得到与经典方法（10⁴‑10⁵ 样本）相匹配的证书。实验在最新的 VLM 上对最近的越狱式攻击进行了验证，展示了该方法的可扩展性和实用性。<br><br>## 论文核心贡献点<br>- 将随机化平滑从分类扩展到生成式 VLM，提出基于 oracle 的抽象层，使得生成结果可以被视作离散类。<br>- 给出在 oracle 错误率 ε<0.5 下的概率下界修正公式<br> \[<br> \bar p_y = \frac{\bar q_y - \epsilon}{1-2\epsilon}<br> \]<br> 并证明在 \(\bar q_y>0.5\) 时该下界仍然是有效的。<br>- 推导出样本数 \(n\) 与可验证半径的关系（利用中心极限定理和 Shore 对 \(\Phi^{-1}\) 的近似），得到标度律<br> \[<br> r_\sigma(\alpha,n) \approx 1 - 1.64\frac{z_\alpha}{\sqrt n}<br> \]<br> 表明样本量只需降低 2‑3 个数量级即可保持接近最优的半径。<br>- 在理论上放宽了前期工作对均匀分布的要求，仅需多数类概率分布主要集中在 \([\beta,1)\) 且 \(\beta\ge 0.7\)，从而提升了方法的适用性。<br>- 给出适用于 VLM 的完整认证算法（Algorithm 2），包括语义等价聚类的投票机制。<br>- 通过在最新 VLMs 上进行实验，验证了对 jailbreak‑style 攻击的防御能力，并报告了认证半径、认证精度以及样本效率的定量结果。<br><br>## 论文方法描述<br>1. **模型与噪声注入**<br> - 输入为图像 \(\mathbf x\) 与文本提示 \(\mathbf t\)。<br> - 对图像加高斯噪声 \(\mathbf z\sim\mathcal N(\mathbf 0,\sigma^2\mathbf I)\) 生成 \(\mathbf x'\)。<br> - 文本保持不变，调用 VLM \(f_\theta(\mathbf x',\mathbf t)\) 获得生成的回答 \(\mathbf y\)。<br><br>2. **Oracle 分类层**<br> - **内容安全分类**：oracle 将 \(\mathbf y\) 判为 “有害” 或 “无害”。<br> - **离散动作映射**：若 VLM 充当 VLA，oracle 将答案映射为有限的动作集合（如 `base‑forward`、`gripper‑open`）。<br> - **语义等价聚类**：oracle 判断新回答是否与已出现的回答语义相同，若相同则累计计数，否则创建新类。<br><br>3. **投票与计数**（Algorithm 2）<br> - 用字典 `ans` 存储每个等价类（或每个离散动作）的出现次数。<br> - 对每个噪声样本执行 oracle 判断并更新相应计数。<br> - 最终返回计数最高的回答 \(y\) 与其计数 \(c\)。<br><br>4. **概率下界与证书半径**<br> - 将计数 \(c\) 与样本数 \(n\) 带入 Clopper‑Pearson 方法得到 \(\bar q_y\)（在置信度 \(1-\alpha\) 下的下界）。<br> - 考虑 oracle 误差 \(\epsilon\)（假设 \(\epsilon<0.5\)），修正得到真实概率下界<br> \[<br> \bar p_y = \frac{\bar q_y - \epsilon}{1-2\epsilon}<br> \]<br> - 若 \(\bar p_y>0.5\)，则可验证半径为<br> \[<br> R = \sigma \Phi^{-1}(\bar p_y)<br> \]<br> - 对于二分类情形，若对 \(\epsilon\) 没有任何已知信息，只要 \(\bar q_y>0.5\) 仍可直接使用 \(R = \sigma\Phi^{-1}(\bar q_y)\) 作为下界（Theorem 4.2）。<br><br>5. **样本效率分析**<br> - 利用 CLT 对 \(\bar p_y\) 进行近似，得到期望半径<br> \[<br> R_\sigma^{\alpha,n}(p_A) \approx \sigma \Phi^{-1}\!\bigl(p_A - t_{\alpha,n}\bigr),\quad<br> t_{\alpha,n}=z_\alpha\sqrt{p_A(1-p_A)/n}<br> \]<br> - 通过 Shore 对 \(\Phi^{-1}\) 的幂级数近似，进一步得到平均半径的下降比例<br> \[<br> r_\sigma(\alpha,n) = \frac{\bar R_\sigma(\alpha,n)}{\bar R_\sigma(0,\infty)} \approx 1 - 1.64\frac{z_\alpha}{\sqrt n}<br> \]<br> - 该公式说明把样本数从 \(10^5\) 降到 \(10^3\) 仍能保持约 90% 以上的理想半径。<br><br>6. **实验实现**<br> - 采样若干噪声图像（如 \(n=500\)‑\(2000\)），对每个图像调用 VLM 并由强 LLM（如 GPT‑4、Llama‑70B）充当 oracle。<br> - 计算多数类的计数、Clopper‑Pearson 下界与对应半径。<br> - 与原始分类 RS 基线以及仅使用 Clopper‑Pearson 不进行 oracle 修正的方案进行对比。<br><br>## 论文使用数据集和训练资源<br>- **模型**：直接使用公开的预训练 VLM（如 LLaVA、InstructBLIP、GPT‑4V 等），不进行额外微调。<br>- **Oracle**：使用更强的语言模型（例如 GPT‑4 或 Llama‑70B）作为分类器或语义等价判断器。<br>- **实验数据**：在论文正文中未提供详细数据集描述，附录 B 中列有用于内容安全、VLA 动作与语义聚类的图像‑提示对（来源于公开的 VLM 评测集），并对每个样本进行多次噪声采样。<br>- **计算资源**：实验基于多卡 GPU 集群执行 VLM 推理，oracle 的调用使用相同的 GPU 或 CPU 后端。由于模型已预训练，计算开销主要是多次前向推理（每张图像 \(n\) 次），具体硬件规格未在正文中给出。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：在标准机器学习服务器上运行（Python + PyTorch），所有 VLM 与 oracle 均通过相同的推理框架进行调用，实验代码已开源。<br>- **鲁棒性指标**<br> - **可验证半径 \(R\)**：在给定置信度 \(\alpha\) 下，对每个输入返回的最大半径。<br> - **认证精度**：在特定半径阈值（如 \(R=0.5\)）下，能够给出非“ABSTAIN”结果的样本比例。<br> - **半径下降比例**：通过标度律或实际测量比较不同样本数 \(n\) 下的平均半径 \(\bar R_\sigma(\alpha,n)\)。<br> - **Oracle 误差敏感性**：评估在不同假设的 \(\epsilon\)（oracle 错误率）下半径下界的变化。<br>- **对抗评估**<br> - 对最新的 jailbreak‑style 攻击（Qi et al., 2024）进行防御测试，统计攻击成功率的降低程度。<br> - 与未加噪声的原始 VLM 对比，展示 RS 增强后的安全性提升。<br>- **样本效率**<br> - 记录在不同样本规模（如 \(n=100, 500, 1000, 5000\)）下的认证半径、认证精度以及所需的计算时间。<br> - 通过经验曲线验证理论标度律 \(1-1.64z_\alpha/\sqrt n\) 的准确性。<br><br>以上内容概括了论文的研究单位、整体概述、主要贡献、方法细节、实验使用的数据与资源以及评估环境和指标。</details> |
| 2025-09-19 | CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine | http://arxiv.org/abs/2509.15968 | <details><summary>展开</summary># 论文研究单位<br>College of Transportation, Tongji University<br><br># 论文概述<br>提出 CoReVLA，一个面向长尾与安全关键场景的持续学习端到端自动驾驶框架，采用“收集-精炼(Collect-and-Refine)”双阶段流程：在基础阶段利用开源驾驶 QA 数据对 Qwen2.5-VL-7B 进行有监督微调(SFT)，获得基本的场景理解与决策能力；随后在 CAVE 沉浸式仿真平台中进行人机在环(HITL)测试，采集接管数据(包含历史图像、人类注意力、人类接管动作与模型错误行为)，并将其转化为偏好对；最后用直接偏好优化(DPO)进行行为精炼，使模型直接对齐人类偏好，避免手工奖励设计与奖励 hacking。<br><br># 论文核心贡献点<br>- 构建基于 CAVE 平台的人机在环接管数据采集流程，系统化获取长尾失败样本、人类注意力与接管行为，转化为高质量训练数据。<br>- 首次将 DPO 应用于自动驾驶长尾场景的行为精炼，利用稀疏接管数据进行偏好对齐，提升模型在高风险场景中的安全性与鲁棒性，避免奖励工程。<br>- 在开放问答与闭环驾驶双重任务上验证方法有效性；在 Bench2Drive 基准上实现 DS 72.18、SR 50%，较最佳基线提升 7.96 DS 与 15% SR，并展示跨平台泛化与持续学习能力。<br><br># 论文方法描述<br>- 预阶段 SFT：整合 LingoQA、BDD、HAD 为 70GB 驾驶 QA 数据；以 Qwen2.5-VL-7B 为基座，在视觉投影器与 LLM 主干上引入 LoRA 微调，训练目标为自回归交叉熵(公式 1)。<br>- Stage 1 接管数据采集：将模型部署于 CAVE 闭环仿真平台，实时与背景交通交互；当模型出现死锁或碰撞风险时切换回放模式，由安全驾驶员佩戴 VR 进行人工接管。记录历史图像序列、人类视觉注意、接管动作与模型错误行为，自动转化为 DPO 格式的三元组(x, y+, y-)。<br>- Stage 2 DPO 行为精炼：以人类偏好对训练，将策略分布建模为动作上的 softmax(公式 2)，最大化人类偏好概率(公式 3)，优化负对数似然损失(公式 4)；可选 KL 正则以约束策略漂移(公式 5)。相较 PPO 等 RLHF，DPO 无需显式奖励、可直接用离线人类接管数据，数据效率高，适合稀疏长尾事件。<br><br># 论文使用数据集和训练资源<br>- 数据集：70GB 融合数据集(LingoQA、BDD、HAD)，涵盖场景认知与安全驾驶策略；每条样本为五帧连续图像与链式思维(CoT)QA 对。<br>- 接管数据：CAVE 平台采集的人类接管三元组，用于 DPO 精炼。<br>- 训练资源与实现：基于 Qwen2.5-VL-7B，在视觉投影器与 LLM 主干上使用 LoRA 微调；所有代码、预处理数据与场景配置开源于 GitHub。<br><br># 论文使用的评估环境和评估指标<br>- 开放问答(open-loop)评估：在 LingoQA、BDD、HAD 上报告 BLEU 与 ROUGE(1/L)，对比 Qwen2.5-VL-7B、Llava-7B、LlavaNext-7B、Impromptu 等基线。<br>- 闭环驾驶(closed-loop)评估：在 CAVE 平台进行人机在环测试与接管采集；将精炼模型在 Bench2Drive 基准上与多种小规模任务特定模型与大规模预训练模型对比，指标包括：<br> - Driving Score(DS)<br> - Success Rate(SR)<br> - Efficiency<br> - Comfortness<br>- 性能结果：CoReVLA 在 Bench2Drive 取得 DS 72.18、SR 50%，较次优方法提升 7.96 DS 与 14.99% SR；案例研究显示 DPO 前后行为改进与跨平台(CAVE↔Bench2Drive)泛化。</details> |
| 2025-09-19 | A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning | http://arxiv.org/abs/2509.15937 | <details><summary>展开</summary># 论文研究单位<br>- 上海人工智能实验室（Shanghai AI Lab）<br><br># 论文概述<br>- 现实世界机器人强化学习（RL）通常依赖人工设计的稀疏奖励、探索低效、数据昂贵，导致泛化与稳定性受限。<br>- 提出VLAC（Vision-Language-Action-Critic）一体化模型，作为通用过程奖励模型（产生密集进度增量与完成信号），同时作为策略生成动作，统一“批评家（奖励/完成/价值）”与“演员（动作）”角色。<br>- 基于InternVL预训练于超4000小时多源数据（人类演示、机器人轨迹、VQA等），支持零样本与一次性上下文迁移、跨场景/跨任务泛化，并通过人类在环分级机制稳定早期学习与样本效率。<br>- 在四类真实操控任务上，200次真实交互内成功率由约30%提升至约90%；结合人类在环可进一步提升约50%样本效率，最高达100%最终成功率。<br><br># 论文核心贡献点<br>- 构建统一的VLAC模型：同一自回归架构交替生成奖励/完成信号与动作token，简化接口、降低奖励工程成本。<br>- 设计pairwise任务进度理解：输入两帧图像与语言目标，输出进度增量，天然作为TD奖励，并可判别退步/停滞；结合任务描述估计与完成判定辅助任务理解。<br>- 引入一次性上下文学习与样本构造：联合正负样本、跨采样描述、像素差过滤、任务描述与图像序列错配，显著提升跨场景/跨任务泛化与对失败轨迹的判别。<br>- 现实世界RL异步执行架构与动态推理调度：0.1秒内响应，修正时间戳匹配动作执行，确保连续性与低延迟；PPO基于结构化动作模板与价值头训练。<br>- 人类在环分级策略（离线演示回放、Return and Explore、Human Guided Explore）：稳定训练、加速探索、减少早期崩溃风险。<br>- 跨实体/跨视角/跨场景的大规模泛化评估与真实任务提升：四个现实操控任务成功率与样本效率显著改善，并支持多机器人扩展。<br><br># 论文方法描述<br>- 模型架构（基于InternVL的多模态大模型）<br> - 批评家（奖励/完成/价值）：pairwise进度学习、任务描述生成、任务完成判定；产生密集进度与done信号；价值头连接以供GAE。<br> - 演员（动作生成）：结构化动作模板自回归生成delta端位姿（x/y/z/roll/pitch/yaw/open）；记录数值token logits用于PPO；支持多样性采样。<br> - 上下文学习：参考轨迹与起始帧作为可选输入，提升迁移能力与绝对进度估计。<br>- 训练数据与构造<br> - 数据规模：超3000小时人类演示+1200小时公共机器人操控+15小时自收集；多VQA数据集（对话、机器人理解、空间推理、图像差异），总计约4000万数据点。<br> - 样本构造：像素差过滤（阈值1%）、正反向/细粒度与全局联合采样、任务完成正负配对、错配描述负样本，提升稳健性与判别能力。<br>- 现实世界RL框架<br> - 异步推理与动态分配：多机器人通过ZeroMQ与Ray连接；推理请求调度至空闲VLA副本；观察时间戳与动作时间戳对齐。<br> - PPO优化：结构化动作token化与概率记录；价值头预热；GAE估计优势；熵正则化促进探索；针对vllm与torch的概率漂移需重算训练阶段概率。<br> - 人类在环：专家演示回放（NLL损失）、针对性Reset的Return and Explore、人类指导演示补全回放，三级干预稳定学习与提升探索效率。<br><br># 论文使用数据集和训练资源<br>- 数据集与领域<br> - 人类/机器人操控：Ego4D HOD、AGIBOT、Bridge、Droid、FMB、RoboSet、自收集数据<br> - 多模态VQA：Llava、SpatialQA、RobotVQA、Spot the diff、InstructPix2Pix<br> - 跨域评测集：RT1、RoboNet、Dobb-E、RH20T、EgoDex、RoboFAC（成功/失败）<br>- 训练资源与实现<br> - 预训练：batch=3200，最大学习率8e-4<br> - 现实RL：2B VLAC为演员（策略），8B VLAC为批评家（奖励/完成/价值）；AGILE PiPER 7-DOF末端执行器（delta pose）；单帧观察含语言指令、前视相机图像、末端位姿<br> - 推理与训练：基于Ray与ZeroMQ；动态推理调度，推理端使用vllm或torch；训练端对vllm生成动作概率进行torch重算以稳定PPO；价值头在演示/早期探索样本上预热<br> - 机器人与系统：单控制器架构；动作时间戳相对观察滞后以匹配执行时间；推理响应目标<0.1秒<br><br># 论文使用的评估环境和评估指标<br>- 任务环境<br> - 真实机器人：AGILE PiPER；四类操控任务（开盖、取放碗、桌面清扫、大米转移等）<br> - 多机器人扩展与跨场景/跨实体评估（RT1、RoboNet、Dobb-E、EgoDex、RoboFAC等）<br>- 评估指标<br> - 批评家进度理解：VOC（值序相关）、VROC（反转序列一致性）、VOC-F1综合指标、NR（负进度比例）<br> - 演员动作与策略：任务进度（人工标注）、成功率（10次trial）、学习曲线（样本效率）<br> - 现实RL表现：成功率曲线、收敛速度、人类在环干预对样本效率的提升幅度<br>- 主要结果（摘要）<br> - 批评家：在Bridge/Droid等分布内与RT1/RoboNet/EgoDex/RoboFAC等分布外均取得高VOC-F1，并能清晰区分成功/失败轨迹；一次性上下文显著提升跨域性能<br> - 演员与RL：四任务平均成功率约75%，单任务最高达90%；200episode内由约30%提升至约90%；人类在环可再提升约50%样本效率，最终可达100%</details> |
| 2025-09-18 | RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation | http://arxiv.org/abs/2509.15212 | <details><summary>展开</summary>论文研究单位：阿里巴巴集团DAMO Academy（1]DAMO Academy, Alibaba Group）和Hupan Lab（2]Hupan Lab）。<br><br>论文概述：论文提出RynnVLA-001模型，旨在通过人类演示增强机器人操作能力。该模型采用两阶段预训练方法：首先利用1200万个自我中心人类操作视频训练图像到视频生成模型，预测未来帧；随后在人类关键点轨迹数据上联合学习帧和轨迹预测，桥接视觉与行动预测。同时引入ActionVAE编码行动序列为紧凑嵌入，降低VLA输出空间复杂度。最终在机器人数据集微调后，RynnVLA-001在拾取放置等任务上优于先进基线（如GR00T N1.5和Pi0），验证了预训练策略对VLA模型的有效性。<br><br>论文核心贡献点：<br>- 提出基于人类演示的视频生成预训练方法，缓解机器人操作数据稀缺问题。<br>- 设计两阶段预训练管道：自我中心视频生成预训练和人类中心轨迹感知视频建模。<br>- 引入ActionVAE优化行动表示，通过压缩行动序列提升预测平滑性和效率。<br>- 实验证明预训练权重能显著提升VLA模型在多任务场景下的成功率。<br><br>论文方法描述：<br>模型采用三阶段训练流程：<br>1. **Ego-Centric Video Generative Pretraining**：基于Chameleon架构的自回归变换器，以图像和语言指令为输入，交替排列视觉令牌和语言令牌，预测未来帧。训练数据包含1200万人类操作视频和244K机器人视频。<br>2. **Human-Centric Trajectory-Aware Video Modeling**：在EgoDex数据集上微调，联合预测未来帧和手腕关键点轨迹。使用ActionVAE压缩轨迹为连续嵌入，并引入状态嵌入表示当前手腕位置。<br>3. **Robot-Centric Vision-Language Action Modeling**：在SO100机器人数据上微调，输入双视角RGB和机器人状态，预测行动嵌入（由ActionVAE解码为行动序列）。推理时仅输出行动嵌入，跳过未来帧生成以提升实时性。<br><br>论文使用数据集和训练资源：<br>- **数据集**：<br> - 预训练：12M自我中心人类操作视频（网络源过滤）、244K机器人操作视频（BridgeData V2等）、EgoDex人类关键点数据。<br> - 微调：SO100机器人自收集数据集（3任务：拾取放置绿草莓249演示、拾取草莓248演示、插入笔架301演示；场景含单/多目标、干扰物）。<br>- **训练资源**：基于Chameleon代码扩展，使用大规模计算资源进行视频预训练和机器人微调。<br><br>论文使用的评估环境和评估指标：<br>- **评估环境**：LeRobot SO100机械臂，任务包括拾取放置绿草莓、拾取草莓、插入笔架；场景分为单目标、多目标和指令跟随（带干扰物）；测试覆盖多机械臂和不同环境。<br>- **评估指标**：<br> - 任务成功率（SR）：目标完成比例。<br> - Success Rate@1（SR@1）：单次试验成功率。<br> - 平均成功率：跨任务平均SR。<br> - 失败条件：超时、抓取失败超过5次、触碰干扰物。</details> |
| 2025-09-18 | Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue | http://arxiv.org/abs/2509.15061 | <details><summary>展开</summary>## 论文研究单位<br>复旦大学计算机科学与人工智能学院<br>上海创新研究院<br>加州大学伯克利分校机械系统控制实验室（UC Berkeley Mechanical Systems Control Lab）<br><br>## 论文概述<br>提出 Ask-to-Clarify 框架以解决真实环境中指令歧义性。框架首先通过多轮对话主动向用户提问澄清歧义，然后端到端生成低层动作执行具体任务。框架由协作组件（VLM）与动作组件（扩散专家）构成，并通过连接模块与两阶段“知识隔离”训练策略实现协作与动作生成能力的融合。在推理时通过信号检测器实现“提问/动作”模式的无缝切换。<br><br>## 论文核心贡献点<br>- 提出任务与框架：要求具身智能体先通过多轮问答澄清歧义，再执行指令；采用 VLM 协作与扩散动作专家的组合，并设计连接模块以在两者间建立可靠条件。<br>- 两阶段知识隔离训练策略：先学习歧义澄清与交互，再学习端到端低层动作；通过冻结 VLM 防止灾难性遗忘，同时用连接模块补偿 VLM 与扩散模型的联动。<br>- 真实世界实验与验证：在 8 项真实任务上对比多种 SOTA VLA，表明该框架显著提升在歧义指令与复杂场景下的成功率与鲁棒性。<br><br>## 论文方法描述<br>- 任务定义：对给定视觉观察与歧义指令，Agent 依序生成澄清问题并接收回答；若干轮后推断正确指令，然后生成低层动作序列完成任务。<br>- 框架结构<br> - 协作组件（VLM，Qwen2-VL-2B）：负责提问、判断歧义与推导正确指令。<br> - 连接模块（FiLM）：以语言指令为条件对视觉观察进行特征调制，为动作专家提供更具区分性的条件。<br> - 动作组件（扩散专家，ScaleDP-Huge）：端到端生成低层动作（动作块长度 50）。<br> - 信号检测器：解析 VLM 输出末端的信号token（<AMBG>、<NOT_AMBG>、<ACT>、<REJ>），以无训练方式路由“提问/动作”状态。<br>- 两阶段训练<br> - Stage 1：使用歧义交互对话数据训练协作组件；冻结视觉编码器，微调 LLM，新增信号token用于区分歧义、行动/拒绝等状态。<br> - Stage 2：冻结协作组件与视觉编码器，训练连接模块与扩散动作专家；采用专家演示的具身数据进行端到端动作生成。<br>- 推理流程<br> - 若指令被判为歧义则输出 <AMBG> 并生成问题；根据用户回答更新对话历史。<br> - 多轮后由 VLM 推断正确指令并标记 <NOT_AMBG>；随后由检测器触发 <ACT>（目标可见则执行）或 <REJ>（目标不可见则拒绝）。<br><br>## 论文使用数据集和训练资源<br>- 机器人与设备：xArm 7（7 DoF + 1 DoF 夹爪）、RealSense D435 相机（手腕与第三视角），xArm Python SDK；Stage 2 采用 Meta Quest 3 进行遥操作采集演示。<br>- 数据<br> - Stage 1（对话数据）：收集多对象图像并由 Qwen3-235B-A22B 生成歧义指令、问答对与正确指令，构成交互对话集。<br> - Stage 2（具身演示）：8 项任务，每项约 10 条演示。<br>- 实现细节<br> - 协作组件：Qwen2-VL-2B-Instruct<br> - 动作组件：ScaleDP-Huge（扩散专家）<br> - 训练超参数（示例）<br> - Stage 1：学习率 1e-5，批量 128，50 轮，训练参数约 1.5B<br> - Stage 2：学习率 2e-5，批量 64，40 轮，训练参数约 978M<br> - 动作块长度：50 步<br><br>## 论文使用的评估环境和评估指标<br>- 环境与任务：xArm 7 真实场景下 8 项任务，含三类通用任务：<br> - Put the Object on the plate（Apple/Peach/Orange）<br> - Pour the water from the Color cup onto the plate（Red/Green/White）<br> - Stack the Color1 block on top of the Color2 block（(Blue, Yellow)、(Yellow, Blue)）<br>- 指标：成功率（每任务 20 次试验）；与基线对比（π0、π0-FAST、OpenVLA-OFT）；消融（知识隔离策略、连接模块、必要性）。<br>- 附加评估<br> - 协作能力：在“目标存在/不存在”场景下判断并执行的正确率。<br> - 鲁棒性：低光照（灯光减半）、视觉干扰（相似干扰物）条件下的成功率。<br><br>结果表明：Ask-to-Clarify 在全部 8 项任务显著优于基线；在低光照与干扰条件下仍保持较高成功率；两阶段训练与连接模块二者缺一不可，共同保证歧义澄清与端到端动作生成的综合能力。</details> |
| 2025-09-18 | Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale | http://arxiv.org/abs/2509.14932 | <details><summary>展开</summary># Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale<br><br>## 论文研究单位<br>- Department of Computer Science & Artificial Intelligence, University of Technology Nuremberg, Germany<br>- Learning, Adaptive Systems and Robotics (LASR) Lab, Faculty of Computer Science, TU Dresden, Germany<br>- Siemens Foundational Technologies, Siemens AG, Germany<br>- Chair for Robotics, Artificial Intelligence and Real-Time Systems, TUM School of Computation, Information and Technology, Technical University of Munich, Germany<br><br>## 论文概述<br>论文提出了Robot Control Stack (RCS)，一个为大规模机器人学习设计的轻量级生态系统。随着Vision-Language-Action models (VLAs)的发展，传统机器人软件框架成为瓶颈，RCS旨在弥合这一差距。RCS采用模块化分层架构，为模拟和物理机器人提供统一接口，促进sim-to-real转换，在保持最小依赖和轻量级设计的同时，提供完整功能集，支持真实世界实验和大规模模拟训练。<br><br>## 论文核心贡献点<br>1. 提出基于环境包装器的RCS架构，支持在不同抽象级别轻松添加新功能，同时支持Python和C++<br>2. 在常见用例上评估RCS，包括跨实体支持、模拟和真实环境中的训练数据收集、VLA和RL智能体的训练和评估<br>3. 在可重现的取物任务上对Octo、OpenVLA和π₀进行广泛实验，涵盖多种不同机器人<br>4. 展示将合成数据与真实数据混合可以显著提升π₀在真实世界中的性能<br><br>## 论文方法描述<br>RCS基于环境包装器概念设计，通过包装器元组W=⟨f:S→S′,g:A′→A,P′,R′⟩将状态和动作从马尔可夫决策过程(MDP)进行转换。架构包含：<br>- C++底层接口定义抽象机器人控制函数，支持Python绑定<br>- 场景包装器序列，可变异或观察环境动作和观察空间<br>- 硬件抽象：标准化传感器和执行器接口，支持同步/异步操作<br>- 仿真集成：基于MuJoCo，提供面向对象的场景视图和回调机制<br>- 机器人工具包：集成Pinocchio进行运动学计算，OMPL用于运动规划<br>- 数字孪生：实时运行仿真作为安全检查器<br>- Agents应用层：通过RPC通信解决VLA策略依赖冲突<br><br>## 论文使用数据集和训练资源<br>- **真实数据集**：FR3 (143个演示)、xArm7 (100个演示)、UR5e (167个演示)、SO101 (120个演示)，均为30Hz频率收集<br>- **仿真数据集**：3000个脚本化演示，成功率73%，生成2193个成功演示<br>- **任务设置**：Pick-Cuboid任务，要求抓取绿色3D打印立方体<br>- **训练资源**：消费者级GPU笔记本电脑用于脚本化仿真数据生成，Nvidia RTX 4080 + 12核CPU用于RL训练<br><br>## 论文使用的评估环境和评估指标<br>**评估环境**：<br>- 四个真实机器人设置：FR3、xArm7、UR5e、SO101<br>- 匹配MuJoCo仿真环境，复制FR3设置<br>- 支持多种传感器：RealSense摄像头、DIGIT触觉传感器、Tacto触觉传感器<br><br>**评估指标**：<br>- **成功率**：50次真实世界 rollout 的抓取成功百分比<br>- **跨VLA模型比较**：在30Hz和5Hz操作下比较Octo、OpenVLA、π₀<br>- **Sim-to-Real评估**：使用SIMPLER方法评估真实到仿真的域转移<br>- **数据混合实验**：混合真实和仿真数据对性能的影响<br>- **RL评估**：训练3小时内(8.5M环境步骤)达到100%成功率，吞吐量>2000步/秒(24个并行环境)</details> |
| 2025-09-18 | CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human | http://arxiv.org/abs/2509.14889 | <details><summary>展开</summary># 论文研究单位<br>- 清华大学计算机科学与技术系（作者归属与资金支持单位）<br><br># 论文概述<br>- 提出 CollabVLA，一个自反思的视觉–语言–动作（VLA）框架，将标准视觉运动策略扩展为可与人“共同想象”与协作的代理<br>- 核心动机：改善现有 VLA 的领域过拟合、推理不可解释，以及依赖重生成模型导致的延迟问题<br>- 方案：通过在 MoE 适配下结合 VLM 的自反思语言推理与扩散式动作生成；在两阶段训练（动作基础化 + 反思调优）下，统一场景理解与动作生成，支持显式自反思并在不确定或失败时主动寻求人类指导<br>- 效果：相比依赖生成的智能体，CollabVLA 归一化“时间”约降低 2×，“梦境”次数约降低 4×，同时成功率更高、可解释性更好、延迟更低<br><br># 论文核心贡献点<br>- 系统化分析了直接自回归 VLA、潜在动作与世界模型三类路线的权衡，指出在执行期轻量级人类在环的缺失是一个被忽视的改进机会<br>- 引入 CollabVLA 框架：在单一骨架视觉运动策略上原生支持反思推理、动作生成与人类交互，通过 MoE 适配实现“反思/控制”的自适应切换<br>- 证明该方法能提升成功率与保持低延迟，并将自反思扩展为可触发的实时人类指导，实现更具稳健性的长尾任务表现<br><br># 论文方法描述<br>- 问题形式化：给定当前观察、过往帧、本体感受与多模态目标，策略输出动作块、反思文本与二值“是否询问人类”的标志<br>- 数据构造：两类互补语料<br> - 多模态目标预训练：融合 Interleave-VLA 的交错图文提示与 MDT 的目标图像增强，并附加 Diffusion-VLA 风格的简洁语言理由<br> - 反思强化调优：扩展 InstructVLA 流程，加入“上下文反思”任务，合成时序不一致、目标多选、动作/目标扰动等失败场景，以自然语言反思形式监督何时反思、如何诊断、如何修订<br>- 模型架构<br> - VLM 主干（InternVL2.5）：输入交错图文（含 [NOW]/[PAST] 图像、目标、人类提示标签、关节信息与可学习 [ACT] 查询）；输出反思字符串、二值询问指示与潜动作嵌入<br> - MoE 适配：在每层 Transformer 的 MHA/FFN 线性投影中插入 LoRA“控制专家”与“反思专家”，通过轻量门控基于局部 token 隐状态自适应选择专家<br> - 扩散式动作模型（DiT）：以潜动作 token 作为交叉注意键值记忆、以反思嵌入通过 FiLM 调制全层，迭代去噪生成低延迟、可行、推理一致的动作轨迹<br> - 推理流程：两阶段“反思–询问/执行”循环；二值头预测是否提问；人类回复作为新增条件再次前向；为降低延迟，支持首次出现 [ACT] 即停止自回归、并行解码剩余查询、缓存跨步记忆，并在新指导到来时用相似度加权融合平滑动作<br>- 训练管道（两阶段）<br> - 动作基础化（Stage 1）：冻结主干，仅激活控制 LoRA；联合优化语言规划损失与扩散去噪损失，隐式学习潜动作 token 作为动作模型的 conditioning<br> - 反思调优（Stage 2）：冻结主干与动作模型；联合训练控制/反思 LoRA、门控网络与询问头；目标函数为反思文本的交叉熵与询问标志的二分类交叉熵，确保不损失动作性能的同时获得可触发的人类交互能力<br><br># 论文使用数据集和训练资源<br>- 数据来源与合成<br> - 机器人/仿真操纵数据与真实世界演示：AgibotWorld、Simpler-3D 等<br> - 多模态预训练：交错图文提示（Interleave-VLA 风格）、目标帧采样（MDT/GR-MG 风格）、扩散式轨迹理由注入（Diffusion-VLA 风格）<br> - 反思强化调优：基于环境回放并由大语言模型生成反思答案，结合生成视觉状态与人工校验；插入无关/打乱帧诱导时序断裂、添加干扰对象诱发多义、扰动动作或目标模拟失败<br>- 预训练与模型规模<br> - 基于 InternVL2.5（约 4B 参数）的视觉语言主干<br> - MoE + LoRA 适配（控制与反思专家）<br> - DiT 动作生成器<br>- 训练与调优<br> - Stage 1：学习基础动作与轻量规划语言<br> - Stage 2：学习反思生成与询问决策，同时冻结主干与动作模型以保性能<br><br># 论文使用的评估环境和评估指标<br>- 评估维度<br> - 多模态理解：MMMU、MMStar、OCRBench、HallBench，以及 TextVQA、DocVQA、InfoVQA、RealWorldQA；额外 500 样本的 ContextReflection 集（从 AgibotWorld 与 GenieSim 保留）<br> - 仿真任务：在 Simpler 基础上扩展为“Simpler-Collab”评测，200 任务、8 类操纵任务，涵盖长时程控制与歧义消解，支持自动化的在环人类模拟<br> - 现实世界任务：DOBOT CR5（ROBOTIQ 夹爪）与 UR5（AG95 夹爪），五个类别，每类四个实例<br>- 指标<br> - 成功率（SR）与平均完成长度（LEN）<br> - 归一化时间（Time）：对每个任务进行分位裁剪并线性缩放后平均<br> - Dream 计数：生成代理的平均显式推理步数；协作变体记为人类询问次数<br>- 主要结果<br> - CollabVLA 在多数仿真子任务上获得最高的 SR 与 LEN，并保持最低的 Time 与 Dream（Time≈36/1.9Dream）<br> - 多模态理解对比显示 CollabVLA 相比常见 2B/4B VLA 在多个理解与 VQA 指标上保持或提升（如 MMMU、TextVQA、DocVQA、InfoVQA、RealWorldQA）<br> - 消融实验验证：去反思（No-Ref）、无 FiLM（No-FiLM）、无询问（No-Ask）、无 MoE（No-MoE）、仅 Stage1（No-Tuning）等设置均弱于完整模型<br>- 实现细节<br> - 仿真环境基于 ManiSkill3（SAPIEN）实现<br> - 对比基线：OpenVLA、ChatVLA、InstructVLA、ECoT、CoT-VLA、DiVLA、RoboDreamer、π0、UniVLA、MDT 等；为部分方法构建协作变体以公平对比<br> - 人类在环评估：人类问题与任务脚本共同输入 LLM 自动模拟人类回复以形成自动化 HITL 评测</details> |
| 2025-09-18 | RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI | http://arxiv.org/abs/2509.14687 | <details><summary>展开</summary># 论文研究单位<br><br>中兴通讯股份有限公司，中国；香港中文大学（深圳），中国<br><br># 论文概述<br><br>RealMirror是一个综合性、开源的具身AI视觉-语言-动作（VLA）平台，专门针对人形机器人VLA研究设计。该平台旨在解决当前具身AI研究中的关键瓶颈：数据采集成本高、缺乏标准化基准测试、以及仿真与现实世界之间的显著差距。RealMirror提供了一个端到端的解决方案，涵盖数据采集、模型训练、模型推理和性能评估的完整流程。<br><br># 论文核心贡献点<br><br>1. 构建了一个高效、低成本的数据采集、模型训练和模型推理系统，实现了端到端VLA研究，无需真实机器人参与<br><br>2. 提出专门针对人形机器人的VLA基准测试，通过多场景和多种VLA模型的广泛实验促进模型演进和公平比较<br><br>3. 通过集成生成模型和3D高斯溅射技术，展示了零样本Sim2Real的可行性，使仅在仿真数据上训练的模型能够在真实机器人上无缝执行任务，无需任何微调<br><br># 论文方法描述<br><br>## 物理仿真场景构建<br>基于NVIDIA Isaac Sim平台构建多样化的室内仿真环境，整合CAD模型和来自各种资产库的资产，分配适当的物理属性（质量、摩擦、碰撞参数等），确保仿真中的合理性和人形机器人实体的兼容性。<br><br>## 数据采集系统<br>开发了基于远程操作的数据采集系统，包含两个主要组件：<br>- 运动控制管线：实现多级滤波机制，包括IK关节控制跳跃滤波、末端执行器位姿通信和漂移补偿、IK求解器阈值滤波、跨帧末端执行器位姿阈值滤波<br>- 轻量级WebXR通信系统：实现90 Hz传输频率，与通用通信框架相比减少了114毫秒的端到端延迟<br><br>## 统一训练和推理框架<br>支持多种代表性VLA模型（ACT、Diffusion Policy、SmolVLA），集成时间集成机制以增强动作预测稳健性，基于LeRobot库扩展以支持人形机器人实体，与Isaac Sim深度集成实现交互式评估。<br><br>## Sim2Real转换框架<br>采用多管齐下的策略：<br>- 静态环境渲染：使用3D高斯溅射从多个视角捕获目标真实世界工作空间，重建整个静态场景<br>- 高保真关节机器人模型：使用3D高斯溅射重建物理人形机器人并分割为单独连杆，通过S、R、T变换与Isaac Sim中的USD模型对齐<br>- 交互对象差异化处理：高精度对象采用数字孪生方法，低精度对象使用少样本3D生成模型<br>- 坐标系对齐和相机校准：使用ICP算法对齐CAD资产与3DGS重建环境，通过SfM解决相机位姿<br><br># 论文使用数据集和训练资源<br><br>## 数据集构成<br>构建了高质量的人形机器人VLA数据集，包含五个任务场景，每个场景240条轨迹，总计超过1200条仿真轨迹：<br>- Kitchen Cleanup（厨房清理）：Pick and Place、双臂协作<br>- Air Fryer Manipulation（空气炸锅操作）：Pick and Place、推拉、双臂协作<br>- Assembly Line Sorting（装配线分拣）：Pick and Place、双臂协作、动态抓取<br>- Cup-to-Cup Transfer（杯间转移）：双臂协作、精密控制<br>- Can Stacking（易拉罐叠放）：Pick and Place、精密控制<br><br>## 训练资源<br>- 硬件：PICO Neo3 Pro头戴设备和Ada5880工作站<br>- 训练参数：每个模型训练100,000步，批次大小为16<br>- 统一动作空间：26维（每条机械臂13维：7维机械臂+6维手部）<br><br># 论文使用的评估环境和评估指标<br><br>## 评估环境<br>- 仿真环境：基于Isaac Sim的交互式评估环境<br>- 真实环境：ZHIYUAN A2机器人进行Sim2Real实验验证<br><br>## 评估指标<br>- 主要指标：任务成功率（Task Success Rate）<br>- 评估规模：Kitchen Cleanup (400次试验)、Air Fryer Manipulation (400次试验)、Can Stacking (400次试验)、Cup-to-Cup Transfer (200次试验)、Assembly Line Sorting (100次试验)<br>- 技能评估：从任务中抽象出五个核心机器人技能进行细粒度分析<br>- Sim2Real评估：基本任务（拾取和放置）达到92.86%准确率，复杂任务（球体转移）达到71.43%准确率，无需微调即可实现真实世界部署<br><br>## 实验结果<br>三个代表性VLA模型在基准测试中的表现：<br>- ACT：平均成功率73.55%，在Kitchen Cleanup和Assembly Line Sorting表现突出<br>- Diffusion Policy：平均成功率75.15%，在Air Fryer Manipulation表现最佳<br>- SmolVLA：平均成功率79.75%，整体表现最为稳健，特别是在精密控制任务中</details> |
| 2025-09-17 | CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping | http://arxiv.org/abs/2509.14143 | <details><summary>展开</summary># 论文研究单位<br>德雷塞尔大学电气与计算机工程系；弗吉尼亚理工大学海产农业研究与推广中心与生物系统工程系<br><br># 论文概述<br>提出CLAW（CLIP-Language-Action for Weight）框架，用于“重量感知”的机器人抓取。核心思想是将条件评估从动作生成中解耦：由轻量的微调CLIP作为提示生成器，实时监测电子秤读数并产生离散语言提示（continue/stop）；π₀ 作为流匹配VLA策略接收多视角视觉与语言提示，生成连续控制信号。验证表明CLAW在单物体与混合物体场景中均可可靠执行重量约束抓取，并优于原始与仅微调的π₀。<br><br># 论文核心贡献点<br>- 引入CLAW：在标准VLA之上增加任务专用VLM用于显式条件监控，实现重量感知操控。<br>- 设计CLIP微调：将电子秤数字显示转换为可被VLA理解的离散提示。<br>- 使用提示监督微调π₀：使其能融合CLIP提示与多视角观测，产生精确动作。<br>- 跨单物体与混合任务（双臂）场景评估，显示鲁棒性与一致尊重重量阈值；对比基线（原始π₀与仅微调π₀）显著提升。<br><br># 论文方法描述<br>- 整体架构<br> - 输入：人类指令（指定对象与目标重量）、电子秤图像、场景多视角图像。<br> - 模块1（CLIP）：对电子秤图像与指令生成二元提示 m_t ∈ {continue, stop}，参数化为 p_φ(m_t \| o_t^scale, l)。<br> - 模块2（π₀）：对场景图像与提示生成连续动作 a_t，参数化为 p_θ(a_t \| o_t^scene, m_t)，采用流匹配生成30Hz控制（50步动作块）。<br> - 频率：CLIP以20Hz更新，π₀以30Hz控制；当某步CLIP未更新时复用最近提示。<br>- 训练流程<br> - CLIP微调：采集2000张秤显示裁剪图；每张与N个“load k g target”指令配对，标签由真实读数 w* 与阈值k比较得到（y=continue当k<w*，否则y=stop），共2000N样本，最小化分类损失。<br> - π₀微调：每任务收集50条演示（抓取与撤碗两阶段）；在演示中人工标注clip_prompt（抓取阶段为“continue…”，撤碗阶段为“stop…”），以流匹配损失最小化 \|\|v_θ(x(t),t,o_t^scene,m_t)−(a_t−x(t))\|\|^2，在H200上训练60,000步。<br>- 推理运行<br> - CLIP实时监测秤并输出continue/stop提示；π₀据提示与场景多视角生成动作块；重量达标后提示切换为“stop”，π₀执行撤碗。<br><br># 论文使用数据集和训练资源<br>- 数据集<br> - CLIP微调：2000张电子秤显示裁剪图，每图与N个阈值指令配对，生成2000N条continue/stop样本。<br> - π₀微调：每任务50条演示，覆盖抓取与撤碗阶段；演示中帧级标注“clip_prompt”。<br>- 训练资源<br> - π₀微调：H200 GPU；60,000步；控制30Hz；CLIP提示更新20Hz。<br> - CLIP微调：硬件未详述。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境<br> - 单物体：桌面设含装目标物的篮子、电子秤与空碗；任务为“抓取至目标重量并撤碗”，覆盖糖果（20/30/40g）与大蒜（20/30/40g）。<br> - 混合物体：左右各置一盒（糖果/大蒜），左右臂分工（抓取与撤碗不同对象），评估跨对象与双臂协调。<br> - 鲁棒性测试：在抓取中途投加过量物体，使秤瞬时超过阈值，检验系统是否能即时切换为撤碗并中断未完成动作。<br>- 评估指标<br> - 成功率（20次试验）：分别统计“动作完成”（抓取与撤碗）与“停止点准确”（达到指定重量阈值即停止），对比原始π₀、仅微调π₀与CLAW。</details> |
| 2025-09-17 | SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model | http://arxiv.org/abs/2509.14138 | <details><summary>展开</summary>### 论文研究单位<br>- **Virginia Seafood Agricultural Research and Extension Center, and Department of Biological Systems Engineering, Virginia Tech, USA**<br>- **Department of Electrical and Computer Engineering, Drexel University, USA**<br><br>### 论文概述<br>长时序机器人操作任务要求执行多个相互依赖的子任务，错误检测机制可能引发级联失效。现有视觉-语言-动作（VLA）模型（如π₀）在连续低级控制方面表现出色，但缺乏识别子任务完成的内部信号，在顺序设置中表现脆弱。本研究提出SeqVLA，它是π₀的完成感知扩展，通过添加轻量级检测头感知当前子任务是否完成。该双头设计使模型不仅生成操作动作，还能自主触发子任务转换。研究调研了四种微调策略（联合或顺序微调，冻结或不冻结预训练主干网）。实验在沙拉包装（七个连续子任务）和糖果包装（四个子任务）上进行。结果显示SeqVLA显著提升整体成功率，其中联合微调并冻结主干网策略表现最佳，消除了顺序相关失效。<br><br>### 论文核心贡献点<br>- 集成学习任务完成检测头到π₀模型，实现从多模态上下文中推断子任务完成。<br>- 识别最有效的微调策略：联合微调并冻结主干网，确保可靠顺序执行。<br>- 在两个实际长时序场景中评估，框架在任务级性能上显著优于强基线。<br><br>### 论文方法描述<br>- **问题建模**：长时序任务表示为顺序子任务序列$\mathcal{T}=\{\mathcal{T}_{1}, \mathcal{T}_{2}, ..., \mathcal{T}_{n}\}$，子任务$\mathcal{T}_{i}$的完成是启动$\mathcal{T}_{i+1}$的先决条件。<br>- **架构**：SeqVLA扩展π₀架构（基于SigLIP视觉编码器、Gemma-2B语言主干网和Gemma-300M动作专家），添加共享动作专家特征的轻量级完成检测头（线性分类器），输出子任务完成概率$p$：<br> - $p = \sigma(\textbf{W} \cdot \textbf{F} + b)$，其中$\textbf{W} \in \mathbb{R}^{1024}$, $b \in \mathbb{R}$为参数，$\sigma$为sigmoid函数。<br> - 总损失函数：$L_{\text{total}} = L_{\text{action}} + \lambda \cdot L_{\text{completion}}$，$L_{\text{completion}}$为二分类交叉熵损失，权重$\lambda=0.1$。<br>- **微调策略**：<br> - **联合微调**：动作和分类头同时优化。<br> - **顺序微调**：先训练动作头和主干网，后训练分类头。<br> - **冻结策略**：冻结预训练VLM主干网以保留原始知识，或全微调适应域。<br> - 组合为四种配置：SeqVLA-J（联合微调，不冻结）、SeqVLA-JF（联合微调，冻结）、SeqVLA-S（顺序微调，不冻结）、SeqVLA-SF（顺序微调，冻结）。<br>- **子任务执行**：每个推理步骤输出动作块和执行概率$p$。当$p < \tau = 0.2$时触发转换：停止当前动作、回程家姿态、切换到下一子任务提示。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：<br> - **沙拉包装**：七个连续子任务（菠菜、卷心菜、肉丸、鸡、西红柿、酱料杯、容器关闭），收集350集子任务演示数据。<br> - **糖果包装**：四个子任务（软糖、两次Kinder巧克力、两次士力架、棒棒糖），收集200集子任务演示数据。<br> - 附加长时序演示数据：完整沙拉和糖果任务轨迹用于π₀基线微调。<br>- **数据收集**：使用Aloha双机械臂（14自由度）进行示教，三摄像头（顶视、左抓手、右抓手）记录RGB图像和机器人状态。<br>- **训练资源**：<br> - 基于π₀预训练模型（物理智能版），硬件为Aloha机器人。<br> - 训练环境涉及实时数据采集和微调流程，具体计算资源未详述。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 机器人：Aloha双机械臂（14自由度），三摄像头视角。<br> - 任务：沙拉包装（七子任务）和糖果包装（四子任务）的长时序执行。<br> - 基线：π₀模型在完整长时序演示上微调，直接执行无子任务监控。<br>- **评估指标**：<br> - **成功率**：整体任务和子任务级成功率（如图表显示）。<br> - **分类置信度**：使用熵值衡量完成检测的不确定性（SeqVLA-J熵0.76 vs SeqVLA-S熵1.35）。<br> - **统计可靠性**：Kolmogorov–Smirnov（KS）统计量评估执行和完成阶段分布差异（KS值0.75–0.85，p < 0.001）。<br> - **行为比较**：通过执行记录图（图8、9）定性分析π₀顺序失效vs SeqVLA可靠性。</details> |
| 2025-09-17 | GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model | http://arxiv.org/abs/2509.14117 | <details><summary>展开</summary># 论文研究单位<br><br>Mohamed bin Zayed University of Artificial Intelligence, Department of Robotics, Masdar City, Abu Dhabi, UAE<br><br># 论文概述<br><br>论文针对Vision-Language-Action (VLA)模型在面对新颖相机视角时泛化能力不足的问题，提出了一种名为GeoAware-VLA的方法。该方法通过将标准VLA架构中的视觉编码器替换为预训练的视觉几何基础模型VGGT（Visual Geometry Grounded Transformer），显著提升了模型在新视角下的零样本泛化能力。<br><br># 论文核心贡献点<br><br>1. 提出GeoAware-VLA方法，有效集成视觉几何基础模型到VLA架构中<br>2. 在新视角下实现成功率翻倍提升，在仿真环境中表现优异<br>3. 在真实机器人平台上验证了方法的实用性，特别是在新颖相机角度下的性能提升<br>4. 确认了该方法在不同策略解码器上的一致改进效果<br><br># 论文方法描述<br><br>GeoAware-VLA的核心在于使用冻结的预训练几何感知视觉编码器替代标准的可训练视觉编码器。具体包括：<br><br>1. **几何感知视觉编码器**：采用VGGT作为冻结的特征提取器，利用其多层中间层特征捕获视觉和几何信息<br>2. **视觉投影层**：通过1D卷积网络、池化操作和MLP将VGGT特征转换为统一表示空间<br>3. **多模态编码**：语言和本体感受编码器使用简单的MLP投影器<br>4. **策略解码器**：基于GPT风格的解码器transformer处理多模态输入序列<br>5. **动作生成头**：提供两种变体 - MLP头用于连续动作回归，VQ-BeT头用于建模多模态行为<br><br># 论文使用数据集和训练资源<br><br>1. **数据集**：使用LIBERO基准测试的四个任务套件进行评估<br> - LIBERO-Spatial：测试空间布局泛化<br> - LIBERO-Goal：测试新任务泛化<br> - LIBERO-Object：测试新对象类型泛化<br> - LIBERO-Long：测试更广泛的对象、布局和背景变化<br><br>2. **训练资源**：使用PyTorch框架，在单个NVIDIA A100 GPU（40GB VRAM）上训练，每个模型训练150,000步（仿真）或50,000步（真实世界）<br><br># 论文使用的评估环境和评估指标<br><br>1. **仿真评估环境**：<br> - LIBERO基准测试，包含多样化的操控任务<br> - 在训练视角和新颖视角下进行测试<br> - 包含小、中、大三种视角偏移程度的评估<br><br>2. **真实世界评估环境**：<br> - Realman 65B机械臂<br> - 桌面操控环境，配备两个固定视角相机<br> - 5项不同的操控任务测试<br><br>3. **评估指标**：<br> - 主要指标：成功率（基于10次试验的平均值）<br> - 视角鲁棒性：跨不同视角偏移程度的性能变化<br> - 泛化能力：新视角下的零样本性能表现</details> |
| 2025-09-17 | Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach | http://arxiv.org/abs/2509.13774 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br><br>北京小米机器人技术有限公司（Beijing Xiaomi Robot Technology Co., Ltd.）和香港城市大学（City University of Hong Kong）<br><br>## 论文概述<br><br>本文提出了一种基于人机交互的双执行器（dual-actor）VLA模型微调框架。该框架集成了一个主执行器用于鲁棒的多任务性能，以及一个精调执行器在潜在空间中进行自适应调整。除了标准的物理干预外，论文引入了一种轻量级的talk-and-tweak方案，将人类修正转换为语义化的语言命令，从而生成新的策略学习数据集。在真实世界多任务实验中，该方法在101分钟的在线微调内在三个任务上实现了100%的成功率。对于长视距任务，在超过12个连续操作中保持了50%的成功率。该框架还能有效扩展到多机器人训练，使用双机器人时效率提升可达2倍。<br><br>## 论文核心贡献点<br><br>1. 提出了一种新颖的双执行器VLA微调框架，集成了用于鲁棒多任务策略生成的主执行器和在潜在噪声空间中操作以实现细粒度可控动作调整的精调执行器<br><br>2. 开发了一种轻量级人机交互方案，将实时物理修正（tweak）转换为语义化的语言命令（talk），形成talk-and-tweak数据集。精调执行器利用这些指令进行可解释的调整，而主执行器通过直接干预改进其基线策略<br><br>3. 在真实机器人上验证了所提方法，展示了快速的多任务微调能力（101分钟内达到100%成功率）。对于长视距序列，在12个连续操作中保持50%成功率。此外，框架可无缝扩展到多机器人训练，使用双机器人训练时效率提升达2倍<br><br>## 论文方法描述<br><br>方法分为三个主要部分：<br><br>1. 双执行器强化学习系统：包含预训练的VLA模型进行任务和状态编码，主执行器采用一致性策略（consistency policy）生成动作，精调执行器在潜在噪声空间中进行调整。训练分为离线预热阶段和在线交互阶段，主执行器通过混合目标（行为克隆+Q函数最大化）优化，精调执行器通过行为克隆、Q函数最大化和正则化损失联合训练<br><br>2. Talk-and-Tweak人类干预设计：将人类物理修正（通过SpaceMouse）自动转换为自然语言精调命令。通过计算时间窗口内的累积位移，当超过阈值时生成相应的方向命令（如"向右移动"），形成包含（状态、动作、精调命令）的三元组数据集<br><br>3. 高效多任务学习：采用共享的多任务执行器和任务特定的critic架构。为每个任务维护三个独立缓冲区（专家演示、策略rollouts、人类干预）。在在线阶段，主执行器从所有任务和缓冲区类型中均匀采样更新，精调执行器使用聚合的干预数据集训练，每个任务特定的critic仅使用其对应任务缓冲区的数据更新。引入任务权重机制平衡多任务学习进度<br><br>## 论文使用数据集和训练资源<br><br>数据集：使用Octo作为骨干VLA模型。预热阶段每个任务使用20条轨迹初始化策略（约3000个状态-动作对）。在线微调阶段在三个任务中收集约15000个交互对（每个任务约100条轨迹）。人类干预数据约占15%，用于训练精调策略<br><br>硬件和训练资源：使用自主开发的7自由度机械臂。观察包括两个RGB图像（手腕相机128×128分辨率，头部相机256×256分辨率）和机械臂本体感知状态。动作空间为7维末端执行器增量位姿。数据采集和策略执行频率为10Hz。执行器进程运行在机器人上的NVIDIA Jetson Orin，学习器在配备NVIDIA RTX 3090 GPU的工作站上执行<br><br>## 论文使用的评估环境和评估指标<br><br>评估环境：真实物理环境中的螺栓操作任务，包括三个子任务：(1)将螺栓竖直放置，(2)拾取螺栓，(3)组装螺栓。机器人和物体位置在x-y轴上均匀随机化±5厘米以增强泛化性<br><br>评估指标：<br>1. 成功率：每个子任务如果在50个时间步内完成则视为成功，超过此限制计为失败。每个子任务独立评估成功率<br>2. 片段长度：完成任务所需的时间步数<br>3. 长视距任务评估：完全成功需要机器人顺序完成所有三个子任务<br>4. 所有结果均为25次试验的平均值<br>5. 多机器人扩展性：评估在不同机器人和训练配置下的训练效率和任务性能</details> |
| 2025-09-17 | AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving | http://arxiv.org/abs/2509.13769 | <details><summary>展开</summary># 论文研究单位<br>清华大学（Tsinghua University）、小米EV（Xiaomi EV）、澳门大学（University of Macau）、南洋理工大学（Nanyang Technological University）、北京大学（Peking University）<br><br># 论文概述<br>AdaThinkDrive提出一种“快速回答/慢速思考”的双模推理机制，解决在端到端自动驾驶中思维链（CoT）在简单场景下过度推理、复杂场景下推理收益不一致的问题。论文在Navsim基准上进行实验与评估。<br><br># 论文核心贡献点<br>- 通过在Navsim上系统性对比发现：简单场景下使用CoT收益有限甚至有害，复杂场景下CoT收益显著。由此提出“按场景复杂度自适应推理”的必要性。<br>- 设计AdaThinkDrive框架：三阶段训练范式——预训练获得驾驶常识与知识、两阶段SFT学习双模输出能力、基于GRPO的强化学习通过奖励信号学习“何时思考与何时直接作答”。<br>- 提出“Adaptive Think Reward”策略：通过多 rollout 采样，比较同一场景下Thinking与Non-thinking轨迹质量（PDMS），动态奖惩并更新场景复杂度标签，避免对固定人工标签的依赖。<br>- 在Navsim达成SOTA：PDMS 90.3（仅视觉输入），比最佳纯视觉基线提升1.7；同时在复杂场景中优先使用思考模式（96%），在简单场景中优先直接输出（84%）；相对Always-Think推理时间降低14%。<br><br># 论文方法描述<br>- 问题建模：以联合分布P(m, o \| q)建模对查询q选择模式m（Thinking/Non-thinking）与输出轨迹o的决策；通过最大化期望任务效用U(q, o)选择最优模式。<br>- 数据准备：<br> - 预训练数据：DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive等开源驾驶QA数据，用于获得基础驾驶常识。<br> - 混合SFT数据：为同一查询生成Thinking（含完整推理轨迹）与Non-thinking（省略推理仅含轨迹）两种输出；场景注释通过Qwen2.5-VL-72B自动生成，包含交通灯状态、天气与道路边界等；动态交互对象分为CIPO-1（本车道障碍物）、CIPO-2（可能并线）、Motion Interaction（未来轨迹与自车相交）。<br> - 场景分类：将Navsim训练与验证集按“是否靠近道路边界/是否有关键物体”划分为Level 1（简单，无条件）、Level 2（有单一条件）、Level 3（复杂，同时满足两个条件），并映射为D+（Level 2&3）与D-（Level 1）作为强化学习初始辅助标签。<br>- 两阶段SFT：<br> - 阶段一：在大规模驾驶QA数据上微调，提升场景理解与驾驶语义认知能力。<br> - 阶段二：在Navsim规划数据（含Thinking/Non-thinking双模输出）上微调，使模型具备统一接口生成两种输出风格且不偏向其一。<br>- 自适应思考强化学习（基于GRPO）：<br> - 奖励设计：<br> - PDMS Reward：基于轨迹质量评价PDMS。<br> - Format Reward：强制输出遵循<think>、<answer>标签与轨迹格式规范。<br> - Endpoint Reward：对轨迹终点误差进行分段奖励（<2m=1.0；<4m=0.8；<6m=0.6；<10m=0.4；<15m=0.2；否则=0）。<br> - Adaptive Think Reward：依据多rollout比较Thinking与Non-thinking的平均PDMS与样本数，在D=0/D=1的当前标签下动态奖惩；当满足阈值T与条件时允许修正场景复杂度标签为“challenging/simple”。<br> - 优化目标：采用GRPO优化，引入截断重要性权重与KL正则防止策略偏移。<br><br># 论文使用数据集和训练资源<br>- 数据集：Navsim（主评估）、DriveLM、LingoQA、ImpromptuVLA、NuScenes-QA、NuInstruct、OmniDrive（用于预训练与知识注入）<br>- 模型：InternVL3-8B作为基础视觉-语言模型<br>- 训练资源：<br> - 阶段一：学习率1e-5，批量1，训练2个epoch<br> - 阶段二：学习率4e-5，批量2，训练2个epoch<br> - 阶段三（RL）：学习率2e-6，批量4，使用64×NVIDIA H20 GPU进行2个epoch<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：Navsim非反应式仿真，OpenScene平台支持<br>- 评估指标：<br> - PDMS（Predictive Driver Model Score）：综合评分，融合NC、DAC、TTC、CF、EP<br> - 推理时间：预测4秒轨迹的平均推理时间（Navsim测试集）<br> - 场景分层：按Level 1/2/3的Think vs Non-think选择比例与性能表现<br>- 定量结果（主要指标）：<br> - AdaThinkDrive（仅视觉）：PDMS 90.3，NC 98.4、DAC 97.8、TTC 95.2、CF 100、EP 84.4<br> - Best-of-N：PDMS 93.0<br> - 相对Non-Think RL：PDMS +2.0；推理时间 Non-Think RL 0.68s、Think RL 0.86s、AdaThinkDrive 0.74s<br> - 相对Always-Think：推理时间减少14%</details> |
| 2025-09-13 | OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft | http://arxiv.org/abs/2509.13347 | <details><summary>展开</summary>待生成</details> |
| 2025-09-16 | The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning | http://arxiv.org/abs/2509.12594 | <details><summary>展开</summary># 论文研究单位<br>- LiAuto Inc.（理想汽车股份有限公司）<br>- 清华大学车辆与运载学院<br>- 中国科学院计算技术研究所<br><br># 论文概述<br>论文针对视觉语言动作（VLA）模型在资源受限平台（如边缘设备）上的部署瓶颈问题，提出了LightVLA框架，通过自适应剪枝视觉token来提高计算效率同时保持或提升性能。VLA模型在机器人任务中表现出色，但受限于大量视觉token的注意力计算成本。LightVLA通过动态生成查询评估token重要性，并使用Gumbel-softmax实现可微token选择，从而在优化过程中自动保留关键token。实验在LIBERO基准上验证其效果。<br><br># 论文核心贡献点<br>- 提出LightVLA，一个性能驱动的可微视觉token剪枝框架，无需额外超参数或可训练参数。<br>- 证明VLA模型的效率和性能可协同优化，剪枝同时减少计算开销并提升成功率。<br>- 在LIBERO基准上实现最先进性能：成功率提升2.6%，FLOPs减少59.1%，延迟降低38.2%。<br>- 提出LightVLA*变体，使用可学习查询（额外参数）探索token剪枝的可能性。<br>- 首次将自适应视觉token剪枝应用于VLA任务，突破效率-性能权衡。<br><br># 论文方法描述<br>LightVLA方法包括三个核心步骤：<br>1. **查询生成**：通过视觉和语言token的交叉注意力生成动态查询（参数无关），以评估token对任务的重要性。<br>2. **Token评分**：计算每个查询对所有视觉token的评分矩阵（S），评分反映token的贡献度。<br>3. **Token选择**：使用Gumbel-softmax采样技术使argmax操作可微，将评分矩阵转换为指示矩阵（I），选择高分token组成剪枝集（H'_v）。<br>训练中，Gumbel噪声强度随时间衰减，促进多样性选择和稳定性；推理时直接使用argmax。保留CLS token和位置信息以维持全局信息。LightVLA*在视觉编码器或LLM早期层引入可学习查询（Q*）和层归一化参数，适用于更复杂场景。<br><br># 论文使用数据集和训练资源<br>- **数据集**：LIBERO基准测试，包含四个任务集（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个任务集提供500个专家演示（总计2000演示）。测试每任务集50次试验（500试验）。<br>- **训练资源**：<br> - 基础模型：OpenVLA-OFT（双分支视觉编码器DINOv2和SigLIP、LLaMA-2-7B LLM、LoRA rank 32微调）。<br> - 微调设置：40000梯度步，初始学习率5e-4（30000步后降至5e-5），批大小每设备8（全局64）。<br> - 硬件：8个Nvidia H20 GPUs。<br><br># 论文使用的评估环境和评估指标<br>- **评估环境**：LIBERO仿真环境，使用Franka Emika Panda机械臂，集成相机图像、机器人状态和任务指令。<br>- **评估指标**：<br> - 主要指标：任务成功率（SR, %），分任务集报告。<br> - 效率指标：FLOPs（总计算量）、端到端延迟（毫秒）、平均保留视觉token数。<br> - 对比基线：包括其他VLA模型（如OpenVLA、π_0）和token剪枝方法（如FastV、SparseVLM）。</details> |
| 2025-09-15 | TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning | http://arxiv.org/abs/2509.11839 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br>浙江大学、西湖大学、上海交通大学、上海创新研究院<br><br>## 论文概述<br>当前视觉-语言-动作（VLA）模型在跨实体泛化方面显示出潜力，但在高质量演示稀缺时，特别是对于双足人形机器人，难以快速对齐新的机器人动作空间。本文提出了TrajBooster，一个跨实体框架，利用丰富的轮式人形机器人数据来提升双足VLA性能。<br><br>## 论文核心贡献点<br>1. 首次利用重定向动作数据微调VLA模型并实现双足人形机器人全身操作的真实世界应用<br>2. 提出了TrajBooster真实到仿真到真实的跨实体框架，使用末端执行器轨迹作为形态不可知的信号<br>3. 在Unitree G1上仅需10分钟遥操作数据收集，就能实现超越桌面的家庭任务，包括蹲下、跨高度操作和协调的全身运动<br><br>## 论文方法描述<br>### 1. 真实轨迹提取<br>从Agibot-World beta数据集中提取6D双臂末端执行器轨迹，并将其从Agibot数据映射到Unitree官方G1操作数据集，解决工作空间差异问题。<br><br>### 2. 仿真中的重定向<br>设计复合层次化模型用于全身操作重定向，包括：<br>- **手臂策略**：使用闭环逆运动学（CLIK）计算目标关节角度<br>- **工作者策略**：基于目标的条件强化学习策略，输出12-DoF下肢目标关节位置<br>- **管理者策略**：从手腕姿态生成下肢命令（基座速度命令和躯干高度）<br><br>采用启发式增强的调和在线DAgger算法进行训练。<br><br>### 3. 双步后训练<br>- **后预训练**：使用重定向的动作-视觉-语言三元组数据对预训练GR00T N1.5模型进行后预训练<br>- **后训练**：使用收集的遥操作数据进行微调<br><br>## 论文使用数据集和训练资源<br>- **源数据**：Agibot-World beta数据集，包含超过一百万条真实机器人轨迹<br>- **目标数据**：28个真实世界全身操作数据集片段，涵盖四种不同高度配置<br>- **训练资源**：<br> - 重定向模型训练：512个并行环境，200次训练迭代，单个RTX 4090 GPU<br> - 后预训练：双A100 80GB GPU，批次大小=128，60K步数<br> - 后训练：单个A100 GPU，批次大小=16，3K步数<br><br>## 论文使用的评估环境和评估指标<br>### 评估环境<br>- **仿真评估**：在Isaac Gym中评估重定向模型性能<br>- **真实世界评估**：在Unitree G1人形机器人上进行四个遥操作任务评估<br><br>### 评估指标<br>- **轨迹跟踪精度**：位置误差E_p（厘米）和旋转误差E_r（度）<br>- **任务成功率**：在十个试验中完成任务的百分比<br>- **轨迹泛化能力**：使用FastDTW算法计算生成轨迹与真实遥操作数据的相似性<br>- **零样本技能泛化**：评估未见任务的执行能力</details> |
| 2025-09-15 | Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs | http://arxiv.org/abs/2509.11480 | <details><summary>展开</summary>待生成</details> |
| 2025-09-14 | Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations | http://arxiv.org/abs/2509.11417 | <details><summary>展开</summary># 论文研究单位<br>- UC San Diego（加州大学圣地亚哥分校）<br>- Hillbot<br><br># 论文概述<br>VLA模型往往通过直接微调预训练的VLM获取机器人控制能力，但会导致预训练表征严重退化，在视觉背景变化与指令同义改写下性能显著下降。本文提出保留预训练表征的VLA训练框架，在仿真与真实机器人上均显著提升稳健性与泛化。<br><br># 论文核心贡献点<br>- 部分冻结的双视觉编码器：冻结一个编码器作为“锚点”保留预训练表征，另一个可训练以适配机器人任务，拼接后输入后续模块。<br>- 字符串动作分词器：将连续动作转换为字符序列，使用与语言一致的词表和自回归目标，使动作预测与VLM预训练目标对齐，支持细粒度逐步生成。<br>- 特征正则化与联合训练：利用统一的字符串输出空间，在机器人数据与强调空间推理/可供性的视觉-语言数据上进行等量（50%/批次）联合训练，防止灾难性遗忘并增强泛化。<br>- 架构与配方通用性：可无缝适配OpenVLA与π0等多种VLA体系。<br><br># 论文方法描述<br>- 部分冻结双编码器架构：使用两个视觉编码器（一个冻结、一个训练），对同一观测生成两组特征并拼接 z_t = [φ_frozen(o_t) \|\| φ_train(o_t)]，再与语言指令 c 一起送入动作分词器 ψ 生成动作。<br>- 字符串动作分词器：将动作各维度（如Δx=0.0312）序列化为字符序列（如“0 . 0 3 1 2”），利用语言词表与自回归生成目标进行预测，使动作空间与语言空间对齐，支持在语言模型内统一优化机器人与视觉-语言任务。<br>- 联合训练策略：每批训练从机器人数据与视觉-语言数据各取50%，数据涵盖空间推理、视觉问答、目标定位等，确保与任务相关且一致的梯度来源。<br><br># 论文使用数据集和训练资源<br>- 数据集<br> - 机器人：Open X-Embodiment (OXE)<br> - 视觉-语言：LLaVA Visual Instruct CC3M、VQASynth-Spatial、LLaVA OneVision、RoboPoint<br>- 仿真评估：SimplerEnv（Visual Matching与Visual Variant Aggregation），包含背景、光照、桌面纹理、干扰物与视角扰动<br>- 训练与评估硬件：双GTX 1080 Ti；真实机器人使用Logitech C920 Webcam作为视觉输入<br><br># 论文使用的评估环境和评估指标<br>- 仿真：SimplerEnv的Visual Matching与Visual Variant Aggregation<br>- 视觉稳健性：背景遮挡与视觉多样性扰动<br>- 语言稳健性：同义指令改写（GPT-4生成），如“grasp the can”→“get the can”<br>- 泛化与推理：在Text-VQA、POPE、GQA、VizWiz、VSR等VLM基准上评估保留的推理能力<br>- 指标<br> - 任务成功率（%）<br> - VQA准确率（%）<br> - CIFAR-10线性探测准确率（%）与t-SNE可视化<br> - 真实机器人每任务25次试验的成功次数（表V）</details> |
| 2025-09-11 | SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning | http://arxiv.org/abs/2509.09674 | <details><summary>展开</summary>待生成</details> |
| 2025-09-11 | VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model | http://arxiv.org/abs/2509.09372 | <details><summary>展开</summary>待生成</details> |
| 2025-09-11 | SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models | http://arxiv.org/abs/2509.09090 | <details><summary>展开</summary>待生成</details> |
| 2025-09-10 | RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation | http://arxiv.org/abs/2509.08820 | <details><summary>展开</summary>待生成</details> |
| 2025-09-09 | TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models | http://arxiv.org/abs/2509.07962 | <details><summary>展开</summary># 论文研究单位<br><br>北京人工智能研究院 (BAAI)<br>清华大学产业智能研究院 (AIR)<br>南洋理工大学 (NTU)<br><br># 论文概述<br><br>论文研究如何将关节扭矩信号有效融入视觉-语言-动作（VLA）模型，以提升机器人在接触丰富任务中的感知与控制能力。论文系统地探索了扭矩作为本体感受信号的设计空间，并在真实机器人（Cobot Magic ALOHA双臂平台）上验证了所提出方法对成功率和泛化性能的提升。<br><br># 论文核心贡献点<br><br>- 确定了扭矩信号在VLA中的最佳集成位置与方法：解码器侧、单历史token优于编码器与多token方案。<br>- 提出将扭矩历史聚合为单一token插入解码器的观测融入方式，保持解码器输入模式完整性并显著提升接触丰富任务表现。<br>- 引入“动作–扭矩联合预测”的训练目标，将扭矩预测作为辅助输出，使模型学习物理上合理的交互动力学表示。<br>- 在跨模型（RDT、π0）与跨本体（Cobot Magic ALOHA、ROKAE SR）上验证有效性。<br><br># 论文方法描述<br><br>- 扭矩作为观测（Sec 4）<br> - 架构对比：编码器插入、decoder预连接、decoder后连接；结论为decoder后连接最优。<br> - 历史信息：比较逐帧token与单历史token；单历史token在解码器侧最优，避免破坏解码器学习到的输入模式。<br>- 扭矩作为目标（Sec 5）<br> - 联合扩散损失：将动作块与扭矩块拼接为统一token，使用共享扩散权重与分头MSE损失进行联合预测；通过权重β平衡动作与扭矩目标。<br><br># 论文使用数据集和训练资源<br><br>- 数据：作者自采数据集，基于Cobot Magic ALOHA双臂平台的多种任务演示，用于VLA模型微调；涉及10项任务（5项接触丰富、5项常规）。<br>- 训练资源：基于公开预训练权重的VLA模型（ACT、RDT、π0）；硬件平台包括3个D435深度相机与7-DOF机械臂；关节扭矩由电机电流与转矩常数实时估计，无需外置力/扭矩传感器。<br><br># 论文使用的评估环境和评估指标<br><br>- 评估环境：Cobot Magic ALOHA与ROKAE SR机械臂；任务包括按钮按压、充电器插拔、USB插拔、拔插座、门把手转动、瓶子拾取与放置、倒液体、叠 cubes、推到定位、开抽屉等。<br>- 评估指标：20轮试验成功率（成功次数/20）；对比基线与不同扭矩集成策略的定量结果；展示接触阶段的扭矩曲线与任务重试的定性可视化。</details> |
| 2025-09-09 | Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation | http://arxiv.org/abs/2509.07957 | <details><summary>展开</summary># 论文研究单位<br>杭州电子科技大学人工智能学院机器学习与健康国际合作基地（Shunlei Li、Jiuwen Cao）；新墨西哥大学电气与计算机工程系（Longsen Gao）；慕尼黑工业大学计算、信息与技术学院（Yingbai Hu）。<br><br># 论文概述<br>论文提出图融合的视觉-语言-动作模型 GF-VLA，用于从单次 RGB-D 人类演示中为双臂机器人学习可泛化的层级策略与动作。与传统轨迹复制不同，GF-VLA 通过信息论从演示中抽取关键手-物与物-物交互，构造成时间排序的场景图，并将其与语言条件下的Transformer结合，生成可解释的行为树与笛卡尔运动基元。为提升双任务执行效率，论文设计了跨臂分配策略，自主确定抓握任务分配，无需显式几何建模。实验在四类双臂方块组装基准上验证策略迁移与空间泛化能力。<br><br># 论文核心贡献点<br>- 提出基于信息论的交互抽取方法：从多模态演示中以熵与互信息选择任务相关的手-物与物-物交互，构建交互感知的场景图。<br>- 提出 GF-VLA 框架：首次将结构化的交互建模与视觉-语言-动作（VLA）推理统一，支持鲁棒、可泛化的双臂操作。<br>- 增强可解释性：在VLA中嵌入链式思维（CoT）提示与自验证，实现显式的子目标分解与执行验证。<br><br># 论文方法描述<br>- 信息论驱动的场景图：使用滑动时间窗对位置信号进行熵分析，识别动态活跃区间；用互信息检测手-物耦合（分为“耦合运动”与“停靠”）与物-物交互（分为“高效OO”与“瞬态OO”）。图节点包含实体类别与六维位姿，边表示交互类型与强度。<br>- 图融合的视觉-语言-动作：统一双头架构包含LLM头（高层语义规划与CoT推理、自验证）与动作头（低层笛卡尔末端与夹爪控制，5 Hz指令流）。视觉侧采用RGB与RGB-D分割、特征投影与token对齐；语言侧基于7B参数的LLaMA-2骨干适配。<br>- 链式思维语义策略：LLM头输出结构化的推理轨迹与行为树节点（动作类型、参数、验证条件），确保时序一致性与可审计性。<br>- 参数高效微调：使用LoRA适配LLM与动作双头，仅更新主干与适配器。策略头监督来自250段人类演示（125用于训练、125用于评估）；动作头在240段双臂试验上微调（UR5e+Robotiq 2F-85与UR10e+Barrett BH282），任务覆盖形状泛化、空间关系、绝对与相对6D位姿执行。<br><br># 论文使用数据集和训练资源<br>- 人类演示数据：10名参与者提供的250个RGB视频，包含字母搭建与塔式构造任务；125个用于策略头训练、125个用于规划评估。<br>- 机器人试验数据：240次双臂试验，涵盖4类任务×3种形状×20次重复（各120/120划分为微调/评估）；Intel RealSense D435i顶置RGB-D感知。<br>- 计算资源与时延：单张RTX 4090进行约40小时微调，指令-推理平均时延约7.56秒（bfloat16）。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：双UR机械臂与不同夹爪协同（UR5e+Robotiq 2F-85、UR10e+Barrett BH282），顶部RGB-D相机（Intel RealSense D435i）；任务含形状泛化、空间关系、6D位姿执行与字母/塔式结构构建。<br>- 指标：<br> - 图表示准确度（GRA）与任务切分准确度（TSA）：单手任务达GRA 98.5%、TSA 95.6%；字母任务GRA 97.2%、TSA 93.9%；塔式任务GRA 96.8%、TSA 93.1%。<br> - LLM规划评估：覆盖度、排序准确度、验证正确性与CoT可解释性评分。<br> - 抓取成功率（GSR）、放置成功率（PSR）、6D位姿误差（6DPE）、指令一致性评分（ICS）。<br> - 任务成功率（TSR）、计划迁移率（PTR）、双臂协调评分（BCS）。<br>- 性能表现（示例）：<br> - 形状泛化：GSR 0.98±0.02、PSR 0.95±0.03、6DPE 1.2cm/2.5°、ICS 4.8±0.3。<br> - 相对位姿：GSR 0.91±0.04、PSR 0.85±0.06、6DPE 3.1cm/6.8°。<br> - 任务迁移：六个新任务平均TSR 0.90、PTR≥0.83、BCS≥4.4；总体抓取/放置/任务成功率约为94%/89%/90%，且跨形状、姿态与视角变化保持鲁棒性。</details> |
| 2025-09-08 | F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions | http://arxiv.org/abs/2509.06951 | <details><summary>展开</summary># 论文研究单位<br>上海人工智能实验室；哈尔滨工业大学（深圳）<br><br># 论文概述<br>论文提出 F1-VLA，一个将视觉、语言与动作统一建模的具身智能框架。该模型通过显式的“视觉前瞻生成”模块，在感知与行动之间引入目标条件化的未来视觉状态预测，并采用 Mixture-of-Transformer 架构将理解、前瞻与控制三类专家进行层次化整合。基于“预测式逆动力学”思想，F1 将动作生成重塑为“以前瞻为目标的逆问题求解”，在动态与长时序任务中显著提升鲁棒性与成功率。模型在 330k+ 轨迹、136 项任务、5 种机器人形态的大规模数据上进行三阶段训练，并在真实场景与模拟基准上均取得领先表现。<br><br># 论文核心贡献点<br>- 引入视觉前瞻作为显式规划信号，将 VLA 从“反应式状态到动作”的映射转变为“前瞻引导的逆动力学”推理。<br>- 提出 UGA（理解–生成–行动）渐进式注意力与跨专家因果信息流，确保前瞻与行动模块的稳定性与可解释性。<br>- 设计 next-scale（多尺度）预测的高效机制，生成多分辨率的视觉前瞻 tokens，兼顾实时性与预测质量。<br>- 给出三阶段训练配方：先对齐前瞻与理解，再在大规模机器人数据上进行预训练，最后进行任务特定后训练，实现跨任务、跨形态的可迁移能力。<br>- 在 LIBERO、SimplerEnv Bridge 等模拟基准与真实机器人平台（Genie、Franka、ARX LIFT II）上系统验证优越性，特别是在动态环境与长时序任务中的稳定增益。<br><br># 论文方法描述<br>- 架构：Mixture-of-Transformer（MoT），包含理解专家、生成专家、行动专家。理解专家采用 PaliGemma（基于 SigLIP 视觉编码与 Gemma 解码器）以实现语言–视觉对齐；生成专家基于 VAR 的残差 VQ-VAE 进行图像离散化，采用 next-scale 预测与时间卷积聚合历史运动特征以高效合成前瞻；行动专家以 flow matching 在连续动作空间学习从噪声到目标动作的向量场，配合 chunked action 预测生成短期动作序列。<br>- 信息流：理解 → 生成 → 行动的单向因果层次，生成模块不接收来自行动的逆向信息，防止信息泄漏与训练不稳定。<br>- 训练目标：阶段 I 以教师强制最小化前瞻 tokens 的负对数似然；阶段 II/III 以自回归 next-scale 预测和 flow matching 的行动损失联合优化，总体损失为两者加权。<br>- 推理优化：实际推理时仅使用 4 个尺度的前瞻预测以平衡效率与效果。<br><br># 论文使用数据集和训练资源<br>- 训练数据：330k+ 轨迹，覆盖 136 项任务与 5 种机器人形态；包含 LIBERO、Open-X-Embodiment、AgiBotWorld 等公开机器人数据集，涵盖从基础抓取、放置到复杂协作与推送的广泛技能，任务时长 10 秒至 2 分钟以上。<br>- 三阶段训练：Stage I 冻结理解专家，仅训练生成专家进行前瞻对齐；Stage II 在大规模机器人数据上联合预训练；Stage III 进行任务特定后训练以适配新形态与细粒度技能。<br>- 实现细节：理解与行动专家由 π0 初始化，生成专家随机初始化并配备预训练的 VAR 残差 VQ-VAE；主干采用 Swish 激活、RMSNorm 与 RoPE；训练超参数详见附录。<br><br># 论文使用的评估环境和评估指标<br>- 模拟基准：<br> - LIBERO（空间、物体、目标、长时序四套件）：评价指标为成功率（SR，越高越好）和排名（Rank，越低越好）。<br> - SimplerEnv Bridge（多步精细操控）：评价指标为抓取成功率与整体任务成功率。<br>- 真实实验：<br> - 平台：Genie（双臂）、Franka、ARX LIFT II；任务包括基础抓取与放置、精细操控、双臂协作与人机交互、动态传送带抓取、长时序序列操作等。<br> - 指标：每项任务 15 次试验的抓取与任务成功率；报告各任务均值与对比基线（π0、gr00t-N1/N1.5、OpenVLA、SpatialVLA、CoT-VLA、RT-1-X、RoboVLM 等）。</details> |
| 2025-09-08 | LLaDA-VLA: Vision Language Diffusion Action Models | http://arxiv.org/abs/2509.06932 | <details><summary>展开</summary>待生成</details> |
| 2025-09-06 | SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning | http://arxiv.org/abs/2509.05614 | <details><summary>展开</summary>待生成</details> |
| 2025-09-05 | FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies | http://arxiv.org/abs/2509.04996 | <details><summary>展开</summary>待生成</details> |
| 2025-09-04 | Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models | http://arxiv.org/abs/2509.04063 | <details><summary>展开</summary>### 论文研究单位<br>论文作者包括 Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang。单位信息部分缺失，但通讯作者 Donglin Wang 来自西湖大学。<br><br>### 论文概述<br>针对基于流匹配的视觉-语言-行动（VLA）模型在复杂任务中行动准确性不足的问题，论文提出了一种离线强化学习后训练方法 ARFM（自适应强化流匹配）。该方法通过在 VLA 流模型损失中引入自适应缩放因子，理论构建了一个权衡偏差-方差的优化目标，以平衡强化学习信号和梯度方差影响，从而实现更稳定和高效的微调过程。实验表明 ARFM 在泛化、鲁棒性、少样本学习和持续学习方面表现优异。<br><br>### 论文核心贡献点<br>1. 提出 ARFM 方法，一种用于 VLA 流模型的新型离线强化学习后训练方法，能够自适应调整数据质量分布。<br>2. 理论建立缩放因子 α 的优化目标，并诱导二分迭代算法进行实时更新，实现高效 VLA 流模型微调。<br>3. 通过仿真和真实机器人操作任务实验，验证 ARFM 在泛化能力、对动态扰动的鲁棒性、少样本学习和持续学习场景中的卓越性能。<br><br>### 论文方法描述<br>方法基于能量加权流匹配（EWFM）框架：<br>- **能量加权 VLA 流模型**：构建能量引导分布 π(A_t\|o_t) ∝ p(A_t\|o_t) exp(α R*(A_t, o_t))，其中 R* 为标准化强化学习优势，α 为缩放因子。使用条件能量加权流匹配（CEFM）损失优化向量场，学习策略分布。<br>- **自适应缩放因子 α 调整**：通过最小化目标函数 J(α) = Var(ĝ(α)) - λ S(α) 实现信号与方差的权衡，其中 ĝ(α) 是损失梯度，S(α) 是强化学习优势评分函数。基于高斯假设，推导出 J(α) 的具体形式，并通过二分迭代算法（Algorithm 1）实时求解最优 α*。最终算法（Algorithm 2）集成到 VLA 流模型微调中。<br><br>### 论文使用数据集和训练资源<br>- **数据集**：使用 LIBERO 仿真基准（包括 Object、Long、Spatial 和 Goal 四个任务套件）和真实世界 UR5 机器人平台（三个抓取和放置任务）。<br>- **训练资源**：论文未明确指定硬件资源，但假设采用标准计算环境（如 GPU）进行仿真和实验。模型基于流匹配实现，训练过程涉及数据采样和损失优化。<br><br>### 论文使用的评估环境和评估指标<br>- **评估环境**： LIBERO 仿真环境和 UR5 真实机器人实验平台。<br>- **评估指标**：主要使用成功率（Success Rate, SR）衡量任务完成度；在动作扰动实验中，添加高斯噪声（0.1 至 0.3 级别）评估鲁棒性，通过扰动后成功率评估性能。</details> |
| 2025-09-04 | FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction | http://arxiv.org/abs/2509.04018 | <details><summary>展开</summary>待生成</details> |
| 2025-09-03 | ANNIE: Be Careful of Your Robots | http://arxiv.org/abs/2509.03383 | <details><summary>展开</summary>## 论文研究单位<br><br>中国科学院计算技术研究所、中国科学院自动化研究所、佐治亚理工学院、德克萨斯大学达拉斯分校<br><br>## 论文概述<br><br>论文研究了具身AI（EAI）系统中的对抗性安全攻击问题。针对EAI系统将视觉-语言-动作（VLA）模型整合到机器人中带来的新安全风险，提出了第一个系统性的对抗安全攻击研究框架。论文基于ISO/TS 15066标准，建立了从传统机器学习安全向物理AI安全转变的范式转移，并通过仿真和真实机器人实验验证了攻击效果。<br><br>## 论文核心贡献点<br><br>- **安全定义与分类**：提出了基于ISO/TS 15066标准的EAI系统安全定义，将安全违规分为critical、dangerous、risky三个等级<br>- **基准测试数据集**：构建了Annie-Bench基准，包含9个安全关键场景和2400个视频动作序列，专注于评估EAI安全性能<br>- **攻击框架**：开发了Annie-Attack任务感知对抗框架，通过攻击领导者模型将长期目标分解为帧级扰动<br>- **实证验证**：在ACT和Baku两个代表性VLA模型上验证了攻击效果，各安全类别攻击成功率均超过50%<br><br>## 论文方法描述<br><br>- **安全标准定义**：基于人机协作ISO标准，定义了距离约束、速度约束和碰撞约束三个安全标准<br>- **攻击框架设计**：Annie-Attack包含攻击领导者模型，将高层攻击目标转换为帧级动作扰动，使用PGD方法生成对抗样本<br>- **行动空间设置**：使用末端执行器的4维动作表示（3维笛卡尔空间移动+1维夹爪状态）<br>- **稀疏攻击策略**：提出自适应稀疏攻击，根据动作序列阶段动态调整攻击频率，在早期阶段使用高频率攻击，在后期阶段降低频率<br><br>## 论文使用数据集和训练资源<br><br>- **仿真环境**：基于ManiSkill仿真平台构建数据集<br>- **硬件配置**：使用Franka Emika Panda 7自由度机械臂，配备2自由度夹爪和双摄像头系统<br>- **数据集规模**：生成2400个视频动作序列，包含9个不同安全级别场景<br>- **训练数据**：每个场景约240个演示序列用于训练专门的攻击领导者模型<br>- **评估数据**：每个场景20个测试序列用于评估<br><br>## 论文使用的评估环境和评估指标<br><br>- **评估环境**：仿真环境中的9个测试场景，覆盖所有三个安全违规等级<br>- **评估指标**：<br> - 攻击成功率（ASR）：衡量安全约束规则被违反的百分比<br> - 动作一致性（AC）：评估攻击是否导致动作序列中的突然变化<br> - 动作偏差（AD）：测量对抗动作与原始动作分布的偏离程度<br> - 任务成功率变化（TSRC）：攻击前后任务成功率的变化百分比<br>- **基线模型**：在ACT和Baku两个VLA模型上进行评估<br>- **真实实验**：在真实Franka Panda机器人上验证了攻击的物理影响</details> |
| 2025-09-02 | Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance | http://arxiv.org/abs/2509.02055 | <details><summary>展开</summary>### 论文研究单位<br>- Institute of Artificial Intelligence, China Telecom<br>- Tsinghua University<br>- The Chinese University of Hong Kong, Shenzhen<br>- Northwestern Polytechnical University<br><br>### 论文概述<br>Vision-Language-Action（VLA）模型在预训练后能处理多任务机器人操作，但跨 embodiment（如单臂到双臂）和跨任务的适应常因动作分布不匹配而效率低下。本文提出Align-Then-stEer（ATE）框架，通过两阶段策略实现高效适应：先构建统一动作潜在空间桥接预训练和适应数据分布，再以分类器引导机制在潜在空间中引导VLA生成过程向目标域偏移。实验在仿真（RoboTwin 1.0、ManiSkill3）和真实双臂机器人上验证，ATE显著提升成功率，仿真平均增益9.8%，真实跨 embodiment场景增益达32%。<br><br>### 论文核心贡献点<br>- 提出动作分布对齐策略，利用反向KL散度的模式寻求特性构建统一潜在空间，将适应动作嵌入预训练潜在分布模式中。<br>- 设计基于统一潜在空间的分类器引导机制，无缝集成至扩散/流匹配VLA训练目标，实现精确快速适应。<br>- ATE为即插即用设计，模型无关且计算开销小，仅需训练两个轻量级VAEs。<br>- 跨 embodiment、任务和架构（扩散/流匹配）验证有效性，突出适应效率提升。<br><br>### 论文方法描述<br>ATE分两阶段：<br>1. **统一动作潜在空间对齐（Stage 1）**：<br> - 预训练阶段：在预训练动作数据（如DROID、Open X-Embodiment）上训练InfoVAE（V_pretrain），学习潜在分布。<br> - 适应阶段：在适应数据（目标 embodiment）上训练另一个InfoVAE（V_adaptation），通过最小化反向KL散度 D_KL(q_ψ(z\|ã) \|\| q_φ(z))，将适应潜在分布嵌入预训练潜在空间的特定模式，生成统一潜在空间Z。<br>2. **分类器引导适应（Stage 2）**：<br> - 定义引导函数g = -∇\|\|E_ψ(â_t:t+h^k) - E_ψ(a_t:t+h^0)\|\|²，度量中间动作与目标动作在潜在空间中的距离。<br> - 对扩散模型，修改噪声预测为 ε̂ = ε_θ - √(1-ā_k)g，并嵌入训练目标 L(θ) = E[\|\|ε - ε_θ(...) + √(1-ā_k)·λ·g\|\|²]。<br> - 对流匹配模型，修改速度场为 v̂_θ = v_θ + ((1-τ)/τ)·λ·g，并嵌入训练目标。<br> 引导机制确保VLA输出保持于统一潜在空间内，在适配中保留预训练知识。<br><br>### 论文使用数据集和训练资源<br>- **数据集来源**：<br> - 预训练：大规模机器人数据（如DROID、Open X-Embodiment子集、Kuka、ALOHA）。<br> - 适应：仿真（RoboTwin 1.0含17任务，ManiSkill3含2任务）、真实（双RealMan 7-DoF机器人长期任务）。<br>- **训练资源**：<br> - InfoVAE训练分两步：Step 1用通用数据（3000 episodes）训练潜在结构（约12小时）；Step 2用域特定数据微调（RoboTwin/ManiSkill每任务50-100轨迹，真实每任务50轨迹，约0.5小时）。<br> - 潜在维度512，训练含互信息项以增强表示。<br><br>### 论文使用的评估环境和评估指标<br>- **仿真环境**：<br> - RoboTwin 1.0基准：17项单/双臂任务（工具调整、双瓶拾取）。<br> - ManiSkill3基准：2项接触丰富单臂操作（推立方体、拾取立方体）。<br>- **真实环境**：<br> - 双RealMan 7-DoF双臂机器人：4项长期任务（插入、协调操作）和工具使用任务。<br>- **评估指标**：<br> - 成功率（Success Rate）：以任务完成百分比衡量。<br> - 样本效率：比较达到性能阈值所需训练步数（如RDT基线90k步 vs ATE 70k步）。<br> - 泛化能力：在光照、物体位置、视觉干扰下测试鲁棒性。</details> |
| 2025-09-02 | AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving | http://arxiv.org/abs/2509.01944 | <details><summary>展开</summary># 论文研究单位<br><br>阿里巴巴集团高德（AMAP）主导研究，昆士兰大学、兰州大学、凯斯西储大学合作参与<br><br># 论文概述<br><br>论文提出AutoDrive-R²，这是一个用于自动驾驶的视觉-语言-动作(VLA)框架，通过思维链(CoT)处理和强化学习(RL)来增强自动驾驶系统的推理和自我反思能力。方法采用两阶段训练：首先构建包含四步逻辑推理和自我反思机制的nuScenesR²-6K数据集进行监督微调，然后基于Group Relative Policy Optimization (GRPO)算法结合物理约束奖励进行强化学习训练。<br><br># 论文核心贡献点<br><br>- 提出AutoDrive-R²框架，实现从视觉信息和语言指令到轨迹规划的语义推理与自我反思<br>- 构建nuScenesR²-6K数据集，采用四步逻辑链(可视化→计算→逻辑→反思)与自我反思机制<br>- 基于GRPO强化学习的后训练方法，结合基于物理的奖励框架，包含空间对齐、车辆动力学和时间平滑性约束<br><br># 论文方法描述<br><br>**阶段一监督微调(SFT)**：使用Qwen2.5-VL-7B模型在nuScenesR²-6K数据集上微调，数据集包含6k图像-轨迹对，每对包含前视图像、历史车辆状态和相应的思维链推理序列。<br><br>**阶段二强化学习(RL)**：采用GRPO算法，设置最大完成长度4096 tokens，每输入采样6个候选响应。设计物理约束奖励函数：空间对齐奖励r_pos(位置误差)、车辆动力学奖励r_ste(转向角)和r_vel(速度)、时间平滑性奖励r_tem(控制信号平滑性)，各奖励权重相等。<br><br># 论文使用数据集和训练资源<br><br>**训练数据**：nuScenesR²-6K数据集(6k样本对)，每个样本包含前视图像和3秒轨迹规划(0.5秒间隔)<br><br>**评估数据**：nuScenes数据集(1000个城市驾驶场景，六个同步摄像头)和Waymo数据集(4021个驾驶片段，八个摄像头视图)<br><br>**模型规模**：Qwen2.5-VL-3B和Qwen2.5-VL-7B两种规格<br>**训练配置**：学习率5e-7，累积批大小1，GRPO最大长度4096，生成候选数6<br><br># 论文使用的评估环境和评估指标<br><br>**评估环境**：在nuScenes和Waymo数据集上进行轨迹规划评估<br><br>**评估指标**：L2误差(米)，计算预测轨迹与真实轨迹在1s、2s、3s未来时间点的距离，以及平均L2误差<br><br>**性能表现**：AutoDrive-R²在nuScenes上达到0.19m平均L2误差，Waymo上达到0.20m，显著超越EMMA+、DriveVLM等现有方法，展现出强大的零样本泛化能力</details> |
| 2025-08-31 | OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving | http://arxiv.org/abs/2509.00789 | <details><summary>展开</summary>待生成</details> |
| 2025-08-30 | Galaxea Open-World Dataset and G0 Dual-System VLA Model | http://arxiv.org/abs/2509.00576 | <details><summary>展开</summary># 论文研究单位<br>Galaxea Team（https://opengalaxea.github.io/G0/）<br><br># 论文概述<br>论文提出了Galaxea Open-World Dataset，一个在真实人类生活和工作环境中收集的大规模、多样化机器人行为数据集。基于该数据集，论文介绍了G0双系统框架，结合了用于多模态规划的视觉语言模型（VLM）和用于精细执行的视觉语言动作（VLA）模型。G0采用三阶段训练课程：跨具身预训练、单具身预训练和任务特定后训练。论文通过涵盖桌面操作、小样本学习和长期移动操作的综合基准测试证明了方法的有效性。<br><br># 论文核心贡献点<br>1. 构建了Galaxea Open-World Dataset，包含100K条演示轨迹，涵盖150个任务类别、50个真实世界场景、1600+独特对象和58种操作技能<br>2. 提出了G0双系统框架，结合VLM（系统2）进行规划和VLA（系统1）执行动作<br>3. 设计了有效的三阶段训练策略，验证了单具身预训练在模型性能提升中的关键作用<br>4. 建立了涵盖多种任务类型的综合基准测试<br><br># 论文方法描述<br>**G0双系统架构**<br>- 系统1（G0-VLA）：端到端视觉语言动作模型，负责感知环境、解释子任务指令并执行动作<br>- 系统2（G0-VLM）：处理高层自然语言任务指令、理解场景并为系统1规划子任务指令<br><br>**G0-VLA三阶段训练**<br>- 阶段1预训练：仅训练VLM组件，使用FAST tokenizer将连续动作块转换为离散索引序列，采用标准交叉熵损失进行自回归训练<br>- 阶段2预训练：在标记的Galaxea数据集上训练VLA，包含预训练VLM和新初始化的动作专家，采用flow-matching损失训练连续动作生成<br>- 后训练：使用最多100条轨迹对下游任务进行微调，验证预训练模型的泛化能力<br><br>**G0-VLM训练**<br>- 基于Qwen2.5-VL进行指令调优，使用数据集中的人工注释子任务和合成的人类风格高层指令<br>- 采样关键帧并输入历史图像观察和机器人动作，支持长期上下文任务规划<br><br># 论文使用数据集和训练资源<br>**Galaxea Open-World Dataset**<br>- 规模：100K演示轨迹，500小时高质量数据<br>- 任务覆盖：150个任务类别，50个不同真实世界场景<br>- 对象多样性：1600+独特对象，58种操作技能<br>- 具身一致性：使用统一的Galaxea R1 Lite平台（23-DoF，包括双臂、3-DoF躯干、6-DoF全向基座）<br><br>**训练数据组成**<br>- 阶段1预训练：约1000小时OXE轨迹 + 500小时Galaxea数据集（仅高层任务描述）+ 200小时内部数据<br>- 阶段2预训练：Galaxea Open-World Dataset的完整标注数据<br>- 支持数据：关键帧采样，k帧历史观察，机器人状态信息<br><br># 论文使用的评估环境和评估指标<br>**评估基准任务**<br>- Table bussing：整理杂乱桌面，测试精确抓取放置和双臂协调操作<br>- Microwave operation：操作微波炉，评估与家电交互和多步操作序列<br>- Bed making：整理床铺，测试全身协调控制能力<br>- Blocks stacking：搭建积木形成特定词汇，验证语言跟随和精确操作<br><br>**评估指标**<br>- Progress score：根据任务完成步骤计算分数（Table bussing: 6分，Microwave operation: 5分，Bed making: 4分，Blocks stacking: 6分）<br>- 每个测试运行10次取平均成绩<br>- 小样本设置：每个任务仅使用20条轨迹进行微调，训练10轮<br><br>**评估环境**<br>- 真实世界环境：住宅、餐饮、零售和办公空间<br>- 物理硬件：Galaxea R1 Lite移动双臂机器人<br>- 评估设置：标准化提示，包含任务特定指令、原子动作选项和输出示例</details> |
| 2025-08-30 | Mechanistic interpretability for steering vision-language-action models | http://arxiv.org/abs/2509.00328 | <details><summary>展开</summary>### 论文研究单位<br>加州大学伯克利分校电气工程与计算机科学系（University of California, Berkeley, Department of Electrical Engineering and Computer Sciences）<br><br>### 论文概述<br>VLA模型结合视觉和语言信息实现机器人动作控制，但缺乏类似传统机器人管道的可解释性机制。本研究将机械可解释性技术应用于VLA模型，通过分析模型内部激活单元的语义含义，实现无需微调的实时行为引导。核心创新在于提取FFN层的语义价值向量（value vectors），识别与动作选择因果相关的控制方向（如速度、方向），并将其作为实时控制接口激活。<br><br>### 论文核心贡献点<br>1. **语义保留发现**：VLA模型在训练中保留大量预训练语义概念（<25% FFN神经元用于动作预测，其余维持语义结构）<br>2. **因果关联验证**：验证内部概念与动作的因果关系（如"慢"概念直接导致末端执行器缓慢移动）<br>3. **实时控制接口**：首次实现基于内部表征的零样本行为控制方法，突破微调/环境交互依赖<br><br>### 论文方法描述<br>1. **价值向量提取**<br> - 分析FFN输出层权重矩阵`Wθ`，提取独立于输入的固定值向量`wθ(i)`（式1-2）<br> - 将值向量投影至标记空间，通过标记概率分布赋予语义含义<br><br>2. **概念激活引导**<br> - 通过kNN聚类或人工选择识别语义对齐的神经元簇`𝒮`（如"up"、"slow"）<br> - 在推理时覆盖该簇的激活值为固定标量`α`（式3-4）<br> - 剩余神经元保持原始激活，生成控制残差`Δx`影响最终动作标记分布<br><br>3. **跨框架实现**<br> - PyTorch框架：在OpenVLA的FFN下投影层应用前向钩子<br> - JAX框架：修改π₀的FFN计算图插入引导算子`I𝒮^α`<br><br>### 论文使用数据集和训练资源<br>- **基础预训练数据**：Open X-Embodiment跨平台机器人数据集（OpenVLA/π₀预训练）<br>- **模拟评估**：LIBERO-Long长程操作基准（10项任务，OpenVLA 7B模型）<br>- **实物实验**：DROID平台数据用于π₀-FAST微调（LoRA方法，5000步）<br>- **硬件资源**：<br> - 模拟：NVIDIA H100 GPU<br> - 实物：NVIDIA A4500 GPU + UR5机械臂（配Robotiq 2F-140夹爪）<br><br>### 论文使用的评估环境和评估指标<br>#### 模拟实验（OpenVLA/LIBERO）<br>- **环境**：10个长程操作任务（抓取、放置、器具操作等）<br>- **干预参数**：对比"快/慢"与"上"概念簇，测试不同层深度注入效果<br>- **指标**：<br> - 末端执行器位移变化率（平均提升27.73%，最大148.54%）<br> - 统计显著性（配对t检验，p<0.001）<br> - 效应大小（Cohen's d范围：-0.091至-1.419）<br><br>#### 实物实验（π₀-FAST/UR5）<br>- **任务场景**：<br> - 低/高运输：玩具企鹅提升高度控制（75演示轨迹）<br> - 慢/快运输：玩具海豹速度控制（120演示轨迹）<br>- **基线对比**：无干预/提示词修改/随机向量干预<br>- **指标**：<br> - 低高组：末端最大高度分布（箱线图统计）<br> - 慢快组：平均位移/累积位移时间序列<br>- **关键发现**：<br> - 低/慢干预显著降低轨迹幅度（p<0.05）<br> - 高/快干预效果接近基线（模型已内建高速行为）<br> - 语义引导优于随机干预与提示词修改<br><br>> **注**：语义方向在不同任务/模型间存在迁移性差异，概念簇（如"小心"vs"卡顿"）可能引发混淆行为。</details> |
| 2025-08-28 | EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control | http://arxiv.org/abs/2508.21112 | <details><summary>展开</summary># EO-1: 通用机器人控制中的交错视觉-文本-动作预训练<br><br>## 论文研究单位<br>Shanghai AI Laboratory, Fudan University, AgiBot, Northwestern Polytechnical University<br><br>## 论文概述<br>EO-1是上海人工智能实验室等机构提出的统一具身基础模型，通过交错视觉-文本-动作预训练实现通用机器人控制。该研究针对当前视觉-语言-动作(VLA)模型在开放世界泛化和交错推理能力方面的不足，提出了统一的模型架构和大规模多模态数据集，在具身推理和机器人控制任务中展现出显著优势。<br><br>## 论文核心贡献点<br>1. **统一架构**：EO-1采用单一统一解码器转换器，整合离散自回归解码与连续流匹配去噪，无需额外动作特定参数即可实现跨模态知识传递<br>2. **交错式具身数据集**：构建包含150万样本的EO-Data1.5M数据集，专门针对交错视觉-文本-动作理解和学习<br>3. **真实世界泛化能力**：在ERQA、LIBERO、SimplerEnv等多个开源基准测试中超越现有模型，展现出强大的开放世界理解和控制能力<br><br>## 论文方法描述<br>EO-1基于预训练VLM构建统一解码器转换器架构，处理交错多模态输入序列：<br>- **输入处理**：文本标记器、视觉编码器、机器人状态投影器和动作去噪投影器将不同模态统一映射到R^d嵌入空间<br>- **共享骨干**：初始化自Qwen2.5-VL的转换器骨干，通过因果注意力处理整个交错序列<br>- **输出机制**：语言头用于文本解码，流头用于连续动作去噪生成<br>- **交错修正采样**：针对混合模态生成中的因果关系破坏问题，提出采样策略确保动作生成段的正确训练<br>- **训练目标**：结合自回归的下一个token预测和流匹配的向量场预测损失<br><br>## 论文使用数据集和训练资源<br>**数据规模**：<br>- 网络多模态数据：570万样本，71亿tokens<br>- 机器人控制数据：120万 эпизод，1273亿tokens<br>- 交错具身数据：EO-Data1.5M，10亿tokens<br><br>**训练资源**：<br>- 五个epoch训练，使用Flash-Attention变长打包<br>- 批量大小为1，平均序列长度16384<br>- 主干学习率5×10^-5，视觉编码器1×10^-5<br>- DeepSpeed ZeRO-1优化器<br>- 推理时仅需6GB GPU内存<br><br>## 论文使用的评估环境和评估指标<br>**评估环境**：<br>- **具身推理基准**：RoboVQA、ERQA、EO-Bench<br>- **机器人控制基准**：LIBERO、SimplerEnv<br>- **真实世界评估**：Franka Panda、WidowX 250S、Agibot G-1等多种机器人平台<br><br>**评估指标**：<br>- **具身推理**：BLEU-4分数(RoboVQA)，准确率(ERQA)，多选VQA准确率(EO-Bench)<br>- **机器人控制**：成功率(SR)，在Google Robot基准中采用匹配和聚合两种评估方式<br>- **多维度评估**：空间理解、物理常识、任务推理、状态估计四个维度共648个QA对<br><br>EO-1在所有基准测试中均展现出优异性能，平均成功率达98.2%，显著超越OpenVLA、π₀等现有开源模型。</details> |
| 2025-08-28 | CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification | http://arxiv.org/abs/2508.21046 | <details><summary>展开</summary>## 论文研究单位<br>School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen<br><br>## 论文概述<br>CogVLA是一个受人类多模态协调启发的视觉-语言-动作模型，通过指令驱动的路由和稀疏化技术，在保持高性能的同时显著降低计算成本。该模型采用三阶段渐进架构，模拟人类的视觉注意力系统（VAS）、补充运动区（SMA）和前运动皮质（PMC），分别用于感知聚焦、语义意图过滤和动作规划。在LIBERO仿真基准和真实世界机器人任务上，CogVLA实现了97.4%和70.0%的任务成功率，相比OpenVLA训练成本降低2.5倍、推理延迟减少2.8倍，同时在多个维度上优于现有高效VLA方法。<br><br>## 论文核心贡献点<br>- 提出认知对齐的CogVLA框架，通过EFA-Routing、LFP-Routing和CAtten模拟人类多模态协调机制，实现从感知到控制的端到端优化。<br>- 开发EFA-Routing和LFP-Routing，实现基于指令的视觉稀疏化，有效减少视觉令牌数量（Stage 1压缩至25%，Stage 2修剪50%）。<br>- 设计V-L-A Coupled Attention（CAtten），融合因果视觉-语言注意和双向动作并行解码，确保语义一致性和时间连贯性。<br>- 在LIBERO和ALOHA基准上验证了优越性能和效率，为可扩展机器人控制提供了解决方案。<br><br>## 论文方法描述<br>CogVLA采用三阶段渐进设计：<br>1. **Encoder-FiLM based Aggregation Routing (EFA-Routing)**：在视觉编码阶段，通过指令调制的FiLM模块动态聚合视觉令牌到聚合令牌（压缩至原始输入25%），并使用门控机制融合多个编码器分支（如SigLIP和DINOv2）。<br>2. **LLM-FiLM based Pruning Routing (LFP-Routing)**：在语言模型阶段，通过任务引导的剪枝路由器基于指令相关性筛选视觉令牌（减少50%令牌），保留语义关键信息以降低计算开销。<br>3. **V-L-A Coupled Attention (CAtten)**：采用混合注意机制，结合因果视觉-语言注意和双向动作注意，支持动作块的并行解码，确保跨模态逻辑一致性和时间连贯性。<br>整个流程支持指令驱动的端到端优化，通过稀疏化视觉输入和并行解码提高效率。<br><br>## 论文使用数据集和训练资源<br>- **数据集**：<br> - 仿真环境：LIBERO基准，包含四个任务套件（空间、物体、目标、长时序），每个套件10个任务50个演示。<br> - 真实世界：Cobot Agilex ALOHA平台的三个长时序任务（物体放置、抽屉操作、T恤折叠），分别45、45、30个演示。<br>- **训练资源**：<br> - 使用4×A800 GPUs（80GB显存）进行训练和评估，得益于指令驱动的稀疏化策略。<br><br>## 论文使用的评估环境和评估指标<br>- **评估环境**：<br> - 仿真环境：LIBERO基准测试，模拟各种指令遵循任务。<br> - 真实世界：Cobot Agilex ALOHA平台，进行真实机器人操作评估。<br>- **评估指标**：<br> - 任务成功率（Success Rate）：以百分比衡量任务完成能力。<br> - 效率指标：推理时间（秒）、吞吐量（Hz）、FLOPs（计算量）、训练成本（小时/10k步）。<br> - 性能指标：在LIBERO套件上分空间、物体、目标、长时序子任务报告成功率；真实世界任务中统计子任务成功率（如抽屉操作的三步骤）。</details> |
| 2025-08-27 | Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies | http://arxiv.org/abs/2508.20072 | <details><summary>展开</summary># 论文研究单位<br>- The University of Hong Kong<br>- Shanghai AI Laboratory<br>- Shanghai Jiao Tong University<br>- Huawei Cloud Computing Technologies Co., Ltd.<br><br># 论文概述<br>Vision-Language-Action (VLA)模型通过将大视觉-语言模型适配来映射图像和指令到机器人动作。现有的VLAs主要采用两种范式：一是自回归(AR)方法，按固定顺序预测离散化的动作token；二是使用单独的MLP或扩散头将VLM输出映射到可执行控制。Discrete Diffusion VLA提出了首个将离散扩散应用于VLA的框架，通过单一transformer统一视觉、语言和动作，采用自适应解码顺序和二次重新掩码机制，实现了精确动作建模和一致的训练效果。<br><br># 论文核心贡献点<br>1. 首个离散扩散VLA框架，在单一transformer中统一动作生成与视觉-语言，保持强大性能<br>2. 开发了自适应解码策略和迭代重新掩码机制，支持并行动作token解码和错误纠正<br>3. 在Franka Panda、Google Robot和WidowX上验证，实现LIBERO上96.3%平均成功率，SimplerEnv–Fractal上64.1%和SimplerEnv–Bridge上54.2%的整体性能<br>4. 在统一架构中打破了AR模型的左到右瓶颈，减少了函数评估数量<br><br># 论文方法描述<br>Discrete Diffusion VLA将动作解码建模为离散扩散，通过掩码token去噪在相同的transformer内进行。具体方法包括：<br><br>**动作token化和分块**：每个连续控制维度被量化为256个bin的离散token，单步动作包含7个token（3个平移、3个旋转、1个夹爪）。将H个未来时间步的tokens排列成固定长度的动作块，总长度为L=H×D_act。<br><br>**统一架构**：基于OpenVLA架构，将因果注意力修改为双向transformer，所有token（视觉、语言和动作）都通过统一transformer处理，动作位置使用双向注意力。<br><br>**训练过程**：从掩码调度中采样掩码比例γ，用特殊token [MASK]替换γL个动作位置，对掩码位置最小化掩码交叉熵损失。<br><br>**推理过程**：从全掩码状态开始，执行T轮并行细化。每轮对当前掩码位置预测token分布，按置信度对掩码位置排序，保留top (1-γ_t)位置并重新掩码剩余部分。包含阈值检查和残差下降检查的二次重新掩码机制。<br><br># 论文使用数据集和训练资源<br>在三个机器人设置上评估：<br>1. LIBERO：Franka Panda机械臂，包含四个套件（LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long），每个套件有10个任务和500个专家演示<br>2. SimplerEnv-Fractal：Google Robot，在Fractal数据集上训练，评估视觉匹配和变体聚合性能<br>3. SimplerEnv-Bridge：WidowX机器人，在BridgeData-V2上训练和评估<br><br>使用与OpenVLA相同的VLM主干（Prismatic-7B）进行微调，图像调整为224×224像素，批量训练采用标准协议。<br><br># 论文使用的评估环境和评估指标<br>**LIBERO**：报告四个套件上的成功率(SR)，每套500次rollouts（10个任务×50集）<br>**SimplerEnv-Fractal**：报告视觉匹配和变体聚合的成功率，包含Pick Coke、Mv Near、Drawer三个任务的评估<br>**SimplerEnv-Bridge**：报告四个任务上的部分和整体成功率，包括Put Spoon on Towel、Put Carrot on Plate、Stack Green on Yellow、Put Eggplant in Basket<br><br>所有基准测试仅使用RGB图像输入（第三人称视角和手腕视角），语言指令和可选末端执行器位置，不使用深度信息或辅助信息。</details> |
| 2025-08-27 | Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation | http://arxiv.org/abs/2508.19958 | <details><summary>展开</summary>待生成</details> |
| 2025-08-27 | Ego-centric Predictive Model Conditioned on Hand Trajectories | http://arxiv.org/abs/2508.19852 | <details><summary>展开</summary>待生成</details> |
| 2025-08-15 | TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models | http://arxiv.org/abs/2508.19257 | <details><summary>展开</summary>### 论文研究单位<br>北京大学 (Peking University)<br><br>### 论文概述<br>当前视觉-语言-动作（VLA）模型在处理连续帧时缺乏时序信息整合，逐帧独立处理导致视觉噪声敏感性和时序关联丢失。论文提出TTF-VLA（时序令牌融合），通过融合历史与当前视觉特征增强VLA推理质量，在不训练的前提下改善机器人操作任务的鲁棒性和效率。<br><br>### 论文核心贡献点<br>1. **双维度检测机制**：提出结合灰度像素差异分析与注意力语义相关性的检测方法，用于识别视觉变化区域和任务关键区域<br>2. **硬融合策略与关键帧**：采用二值令牌选择决策，并通过周期性关键帧重置防止误差累积<br>3. **跨模型适用性**：验证在OpenVLA和VLA-Cache架构上的通用性，无需任务特定调参<br>4. **Query矩阵复用发现**：揭示选择性Query矩阵复用可提升性能，提出KQV矩阵直接复用的潜在加速方向<br><br>### 论文方法描述<br>1. **时序令牌融合框架**<br> - 输入：当前帧I_t、历史帧I_{t-1}、历史令牌T_{t-1}、任务指令L_t<br> - 输出：融合令牌T̃_t<br> - 公式：T̃_t = F(T_t, T_{t-1}, I_t, I_{t-1}, L_t)<br><br>2. **硬融合决策**<br> - 对每个令牌i选择当前或历史令牌：<br> t̃_t^(i) = t_t^(i) 若 m_i^fusion=1，否则 t_{t-1}^(i)<br><br>3. **关键帧机制**<br> - 定期重置所有令牌：IsKeyframe(t) = (t mod K=0) ∨ (T_{t-1}=∅)<br> - 参数K=3平衡时序稳定性和响应速度<br><br>4. **双维度检测融合**<br> - 像素维度：计算灰度图绝对差异d_i^pixel = (1/196) ∑\|G_t(u,v)-G_{t-1}(u,v)\|<br> - 注意力维度：提取文本-视觉和动作-视觉注意力S_text^((l))和S_action^((l))<br> - 最终融合：m_i^fusion = m_i^pixel ∨ m_i^attention<br><br>### 论文使用数据集和训练资源<br>1. **仿真数据集**<br> - LIBERO：4套任务（Object/Spatial/Goal/Long），每套10任务×20 episodes<br> - SimplerEnv：3任务（Move Near/Pick Coke/Drawer），总756 episodes<br><br>2. **真实机器人**<br> - 设备：Franka Research 3机械臂<br> - 数据：3任务×80 demos，5Hz操作频率<br><br>3. **训练资源**<br> - 微调配置：OpenVLA-7B，20,000步，batch size=8，8×A100 GPUs<br> - 推理部署：单A100 GPU，5Hz控制频率<br><br>### 论文使用的评估环境和评估指标<br>1. **评估环境**<br> - 仿真：LIBERO基准套件 + SimplerEnv跨环境平台<br> - 真实：Franka机械臂实验室场景<br><br>2. **评估指标**<br> - **任务成功率**：成功执行次数/总次数<br> - **时序融合率**：复用历史令牌比例<br> - **计算效率**：令牌复用带来的加速效果<br><br>3. **主要结果**<br> - LIBERO平均提升：OpenVLA +4.0pp (72.4% vs 68.4%)，VLA-Cache +2.7pp<br> - SimplerEnv跨环境：+4.8%相对提升<br> - 真实机器人：+8.7%相对提升，Pick-and-Place任务改善显著<br> - 消融研究：双维度融合优于单一维度，关键帧K=3性能最优<br><br>论文证明TTF-VLA能有效提升VLA模型在多环境下的操作能力，特别是在长序列任务和噪声场景中表现突出，为时序信息利用和计算加速提供了新方向。</details> |
| 2025-08-26 | MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2508.19236 | <details><summary>展开</summary>待生成</details> |
| 2025-08-25 | FlowVLA: Thinking in Motion with a Visual Chain of Thought | http://arxiv.org/abs/2508.18269 | <details><summary>展开</summary># 论文研究单位<br>未明确标注单位，仅在作者列表中出现两个上标标记；推断主要来自多机构合作，具体单位信息未在页面中明确给出。<br><br># 论文概述<br>当前视觉-语言-动作（Vision-Language-Action, VLA）模型常以“下一帧预测”作为世界模型的预训练范式，直接从当前帧预测未来帧。这种跳过显式物理推理的方式容易导致“像素复制陷阱”，使预测在物理上不可信，并在长时序中不稳定。为解决该问题，论文提出“视觉思维链”（Visual Chain of Thought, Visual CoT），将预测分解为显式的“先推理运动，再生成外观”的结构化过程，具体采用v_t→f_t→v_{t+1}的因果链条，并以光流f_t作为中间运动表示。论文据此构建FlowVLA：一个两阶段的统一自回归Transformer，第一阶段用Visual CoT进行世界模型预训练，第二阶段进行策略微调以生成动作块。FlowVLA在LIBERO、SimplerEnv等仿真基准以及AgileX Cobot真实机器人平台上实现了更物理合理、更高效的策略学习，并提升样本效率和收敛速度。<br><br># 论文核心贡献点<br>- 指出下一帧预测范式的根本缺陷（像素复制陷阱、缺乏物理因果性），提出“视觉思维链”（Visual CoT）作为世界模型学习新范式。<br>- 通过v_t→f_t→v_{t+1}显式将运动推理与外观生成解耦，并在预训练中引入强归纳偏置以促进动态理解。<br>- 设计FlowVLA：单一统一自回归Transformer，使用共享的矢量量化（VQ）分词器将RGB帧与光流映射为同一词表，实现端到端的跨模态推理。<br>- 两阶段训练：阶段一以Visual CoT进行大规模视频的世界模型预训练；阶段二微调为策略模型以生成动作块，显著缩小预训练-下游任务的领域鸿沟。<br>- 在仿真和真实平台取得最优或大幅领先的性能，并验证样本效率、收敛速度与长时序规划能力提升。<br><br># 论文方法描述<br>- 视觉思维链（Visual CoT）<br> - 将建模目标从P(v_{t+1}\|v_t, L)转化为联合建模P(v_{t+1}, f_t\|v_t, L)，并因式分解为两步：<br> 1) 运动推理：P(f_t\|v_t, L)，要求显式预测光流f_t；<br> 2) 外观生成：P(v_{t+1}\|f_t, v_t, L)，在给定光流条件下生成下一帧。<br> - 将学习从像素回归重构为结构化物理推理，为策略学习提供与“如何动”对齐的中间表示。<br>- 两阶段训练框架<br> - 阶段一：世界模型预训练（Visual CoT）<br> - 输入序列构造为交错的出现（帧）与运动（光流）token：S_wm = {L_instr, v_0, f_0, v_1, f_1, …, v_T, f_T}。<br> - 统一分词：用同一VQ-GAN分词器将RGB帧与光流映射为离散token；光流由RAFT计算，并通过VideoJAM方式将二维位移编码为3通道RGB图，再进行非线性归一化以保留细微运动。<br> - 训练目标：L_WM为对flow token与next-frame token的交叉熵损失之和（通常权重λ=1），以标准的“下一token预测”优化，交错预测f_t与v_{t+1}。<br> - 阶段二：策略微调（动作预测）<br> - 权重初始化自预训练世界模型。<br> - 输入序列为S_policy = {L_instr, v_0, a_0, v_1, a_1, …}；动作按FAST方法离散化为token。<br> - 训练目标：L_policy仅在动作token上计算交叉熵，使模型将已学到的视觉-动力知识聚焦于行动生成。<br>- 统一与简洁性<br> - 无需引入运动特定的网络分支：光流与帧共享同一分词器，单一自回归Transformer即可学习跨模态交错序列，兼具参数与结构效率。<br><br># 论文使用数据集和训练资源<br>- 数据与基准<br> - 仿真基准：LIBERO（空间/目标/长期组合四套Suite）、SimplerEnv-WidowX（评估域移鲁棒性：光照、纹理、视角变化）。<br> - 真实机器人：AgileX Cobot双机械臂平台，包含腕部与前向摄像头；设计四项单臂与双臂任务，采集每任务50–200条人类遥操作演示用于微调。<br>- 预训练与微调设置（典型）<br> - 基于Emu3（约8.5B参数）架构；光流由RAFT预计算；分词采用VQ-GAN。<br> - LIBERO：世界模型预训练约5k步（batch=16）；策略微调约5k步（batch=96）。<br> - SimplerEnv：预训练约12k步（batch=32）；微调约20k步（batch=128）。<br>- 实现细节<br> - 光流转换：2通道(u, v)映射为3通道RGB，方向→色相，速度→饱和度与明度；非线性归一化保留细微运动并避免饱和。<br> - 损失平衡：L_WM中λ=1，交错预测flow与帧；动作token化遵循FAST。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境<br> - LIBERO四套Suite（空间泛化、目标泛化、目标改变、长期组合）以及SimplerEnv-WidowX域移场景。<br> - 真实AgileX Cobot平台的双臂操作任务。<br>- 评估指标<br> - 任务成功率（%）：在LIBERO与SimplerEnv的仿真结果以及真实机器人任务上进行报告，通常每任务多次试验（真实实验每任务25次）以保证统计可靠性。<br> - 收敛与样本效率：通过成功率和训练步数曲线，对比FlowVLA与基线（如UniVLA）在全数据与低数据（50%）条件下的收敛速度与最终性能。</details> |
| 2025-08-23 | NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows | http://arxiv.org/abs/2508.16845 | <details><summary>展开</summary>待生成</details> |
| 2025-08-22 | Do What? Teaching Vision-Language-Action Models to Reject the Impossible | http://arxiv.org/abs/2508.16292 | <details><summary>展开</summary>待生成</details> |
| 2025-08-21 | Survey of Vision-Language-Action Models for Embodied Manipulation | http://arxiv.org/abs/2508.15201 | <details><summary>展开</summary>待生成</details> |
| 2025-08-19 | CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models | http://arxiv.org/abs/2508.13446 | <details><summary>展开</summary>待生成</details> |
| 2025-08-18 | Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy | http://arxiv.org/abs/2508.13103 | <details><summary>展开</summary># 论文总结<br><br>## 论文研究单位<br><br>浙江大学计算机科学与技术学院、上海人工智能实验室、商汤科技研究院、南京大学、清华大学<br><br>## 论文概述<br><br>论文提出了Observation-Centric VLA (OC-VLA)框架，旨在解决视觉-语言-动作（VLA）模型在真实环境中的泛化问题。现有VLA模型存在观察空间与动作空间不一致的问题：训练数据来自不同相机视角，但模型通常在机器人基坐标系中预测末端执行器姿态，导致空间不一致。OC-VLA通过利用相机外参标定矩阵，将动作预测从机器人基坐标系转换到相机坐标系，统一了不同视角下的预测目标，从而提高模型对相机视角变化的鲁棒性和泛化能力。<br><br>## 论文核心贡献点<br><br>提出观察中心的VLA框架，将动作预测直接锚定在相机观察空间，通过相机外参矩阵实现坐标系转换<br><br>轻量级、即插即用的策略，与现有VLA架构完全兼容，无需架构修改<br><br>从优化角度分析了相机坐标系预测相比机器人坐标系预测的优势，相机坐标与图像坐标的转换仅需内参矩阵，而机器人坐标转换需要外参矩阵<br><br>在仿真和真实机器人实验中验证了方法的有效性，显著提升任务成功率、加速收敛并增强跨视角泛化能力<br><br>## 论文方法描述<br><br>模型架构采用轻量级VLA模型（约334M参数），使用CLIP文本编码器处理语言指令，DINOv2处理RGB图像，通过Q-Former（4层）进行特征融合和压缩，最后使用LLaMA2风格的Transformer（12层）预测动作<br><br>支持连续动作空间（使用Diffusion Transformer，DDPM 100步训练，DDIM 10步推理）和离散动作空间（非自回归预测）两种模式<br><br>坐标系转换：利用相机外参矩阵T，将机器人基坐标系中的末端执行器姿态转换到相机坐标系，公式为A_cam = T × A_world × T^(-1)<br><br>训练时使用相机坐标系下的动作作为监督目标，推理时将预测的相机坐标系动作转换回机器人坐标系用于机器人执行<br><br>## 论文使用数据集和训练资源<br><br>预训练数据集：Droid数据集，包含来自1417个不同第三人称相机视角的机器人操作轨迹<br><br>仿真评估数据：ManiSkill2数据集，选择5个任务（PickCube, StackCube, PickSingleYCB, PickClutterYCB, PickSingleEGAD），生成约40,000条轨迹，每条轨迹从300,000个随机相机视角池中采样20个相机进行渲染<br><br>真实机器人数据：使用Franka Emika Panda机械臂收集两组数据，Camera 1收集15个任务（固定相机），Camera 2收集8个任务（相机位置有轻微扰动），每个任务10条演示轨迹（10-shot设置）<br><br>训练资源：8张NVIDIA A100 GPU，总batch size 2048（每GPU 256样本），使用AdamW优化器训练30,000步，学习率为Transformer和Q-Former 1e-4，DINOv2为1e-5<br><br>## 论文使用的评估环境和评估指标<br><br>仿真评估环境：ManiSkill2 benchmark，5个任务类型，每个任务从验证集随机采样100条轨迹进行评估（共500条轨迹）<br><br>真实机器人平台：Franka Emika Panda 7自由度机械臂，配备Robotiq 2F-85夹爪和多个RealSense D435i RGB-D相机<br><br>评估指标：任务成功率（Success Rate）<br><br>评估设置包括三种场景：固定相机视角（与训练视角一致）、轻微相机扰动（训练时引入相机位置变化）、新相机视角（零样本评估，使用训练时未见过的相机）<br><br>基线方法：OpenVLA-OFT、π0、以及使用机器人基坐标系预测的相同架构模型<br><br>真实机器人任务涵盖pick & place、pouring、stacking、pick & rotation、pull & push等多种类型，共15个任务，每个任务进行10次试验</details> |
| 2025-08-18 | Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey | http://arxiv.org/abs/2508.13073 | <details><summary>展开</summary># 论文研究单位<br>哈尔滨工业大学（深圳）计算机科学与技术学院<br><br># 论文概述<br>论文首次系统性地综述了基于大型视觉-语言模型（VLM）的视觉-语言-行动（VLA）模型在机器人操作中的应用，涵盖单片式与分层式架构、与其他学习范式的结合、关键特性、数据集与基准，以及未来方向。图1概述了VLA模型的核心优势：开放世界泛化、分层任务规划、知识增强推理与多模态融合。图2给出了综述组织与时间线；图3展示了两类主范式。<br><br># 论文核心贡献点<br>- 提出清晰的VLA定义与两级分类：单片式（包括单系统与双系统）与分层式（规划器+执行器，带可解释中间表示）。<br>- 纵向梳理VLM与操作学习演进路线，呈现大型VLM驱动VLA的历史脉络与代表工作。<br>- 横向比较两类架构的优缺点：单片式强调端到端统一与推理效率，分层式强调可解释的中间表示与模块化训练。<br>- 汇总高级前沿方向：强化学习、无训练优化、从人类视频学习、世界模型融合等。<br>- 系统总结VLA特征：多模态融合、指令跟随、多维泛化。<br>- 分类整理数据集与基准：现实机器人、仿真、人类行为、具身任务。<br>- 识别未来方向：记忆机制、4D感知、高效适配、多智能体协同等。<br><br># 论文方法描述<br>- 架构范式<br> - 单片式（Monolithic）<br> - 单系统（Single-system）：视觉、语言与机器人状态统一输入，LLM自回归生成离散动作令牌（RT-1/2、OpenVLA等）。<br> - 双系统（Dual-system）：系统2（VLM背骨）负责慢速但泛化的推理；系统1（动作专家）负责快速执行（DP-VLA、RoboDual、LCB、GR00T N1、CogACT、ChatVLA系列、Fast-in-Slow等）。<br> - 分层式（Hierarchical）：显式解耦规划与执行，通过可解释中间表示（子任务、关键点、程序、轨迹、可用性地图等）连接Planner与Policy；支持子任务式、关键点式、程序式等子方法。<br>- 性能增强<br> - 感知增强：引入3D与4D时序、触觉、音频等多模态（Leo Agent、SpatialVLA、TraceVLA、4D-VLA、ST-VLA、VTLA、VLAS、FuSe、OE-VLA）。<br> - 推理增强：链式思维与视觉链式思维（ECoT、CoT-VLA）、层级闭环控制（LoHoVLA）、选择性迁移微调（ReFineVLA）。<br> - 泛化增强：统一行动码本（UniAct）、可逆训练（ReVLA）、扩散+自回归协同生成（HybridVLA）、集成投票（VOTE）、世界模型/动力学学习（WorldVLA、UnifiedVLA、UP-VLA）。<br>- 推理效率优化<br> - 架构：层路由/早退（MoLe-VLA、DeeR-VLA、CogVLA），引入Mamba结构（RoboMamba）。<br> - 参数：小参数模型（NORA）、低位宽/三值权重（BitVLA）。<br> - 解码：并行解码（RoboFlamingo、PD-VLA）、投机解码（Spec-VLA）、触发式跳推理（FlashVLA）。<br>- 其他高级方向<br> - 强化学习（RL）集成、免训练优化、从人类视频学习、世界模型驱动VLA等。<br><br># 论文使用数据集和训练资源<br>- 现实机器人数据集与基准：Open X-Embodiment（跨实体）、BridgeData、DROID、RT-1/RT-2演示集等。<br>- 仿真数据集与基准：常用模拟操控数据集与评测基准（用于高效训练与泛化评估）。<br>- 人类行为数据集：人类演示视频与交互数据（支持模仿/从视频学习）。<br>- 具身数据集与基准：多任务/多场景具身评测集合。<br><br># 论文使用的评估环境和评估指标<br>- 评估环境：真实机器人平台与仿真环境；跨实体、跨任务与多模态组合的具身场景。<br>- 评估指标：典型操控任务指标，如成功率、路径误差、任务完成度、泛化到新对象/指令、推理质量与效率指标（延迟/吞吐）；论文以综述性评估与特性分析为主，未提供统一的数值对比。</details> |
| 2025-08-17 | Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search | http://arxiv.org/abs/2508.12211 | <details><summary>展开</summary># 论文总结<br><br>**论文研究单位**<br>- Mila — Quebec AI Institute, Canada<br>- Université de Montréal, Canada<br>- The University of British Columbia, Canada<br><br>**论文概述**<br>提出视觉-语言-动作规划与搜索（VLAPS）框架，通过将基于模型的搜索（MCTS）集成到预训练VLA策略的推理过程中，解决VLA模型在复杂环境中的脆弱性问题。该方法利用VLA模型指导搜索过程，使其能够高效探索大空间中的语言条件机器人任务，显著提升成功率（最高67个百分点）。<br><br>**论文核心贡献点**<br>1. 集成VLA策略与MCTS的VLAPS框架<br>2. 自动定义任务导向搜索空间的算法<br>3. 基于VLA的节点选择引导机制<br>4. 实验验证在所有测试场景中均优于VLA基线<br>5. 自适应计算分配机制（失败任务分配更多搜索时间）<br><br>**论文方法描述**<br>1. **VLA驱动的行动块采样**<br> - 定义有限行动块库Φ（2000个K-medoids聚类原型）<br> - 使用VLA策略构建上下文相关采样分布βΦ<br> - 每个节点采样k=10个候选行动块进行搜索<br><br>2. **VLA引导的树遍历**<br> - 采用PUCT选择策略（Q≡0）<br> - 使用VLA先验ψΦv分配节点访问次数<br> - 仿真VLA策略至任务完成或达最大步长<br><br>3. **搜索终止条件**<br> - 发现目标状态即返回轨迹<br> - 或在600秒计算预算内返回最高频动作序列<br><br>**论文使用数据集和训练资源**<br>- **数据集**：Libero模拟机器人数据集（五个任务套件：Spatial、Goal、Object、10、90）<br>- **基础VLA模型**：Octo（97M参数）<br>- **训练资源**：<br> - NVIDIA A100 GPU<br> - 演示数据过滤no-op动作<br> - K-medoids聚类生成2000个行动块原型<br><br>**论文使用的评估环境和评估指标**<br>- **评估环境**：<br> - Libero模拟套件<br> - 256×256固定相机图像<br> - 128×128腕部相机图像<br>- **评估指标**：<br> - **任务成功率**（基于1000次测试）<br> - **算法运行时间**（仅统计成功案例）<br> - **不同微调阶段的性能对比**（10k-200k步）<br>- **实验设置**：<br> - 每次搜索300蒙特卡洛样本<br> - 行动块时域H=4<br> - 最大搜索深度100<br> - 600秒超时限制<br><br>实验显示VLAPS在所有任务套件中始终优于VLA基线，尤其在基础成功率较低时提升最大（如Libero-Object从6%提升至73%）。当基础VLA改进时，VLAPS搜索时间显著减少。自适应计算分配机制使失败任务获得更多搜索资源。</details> |
| 2025-08-16 | Toward General Physical Intelligence for Resilient Agile Manufacturing Automation | http://arxiv.org/abs/2508.11960 | <details><summary>展开</summary>待生成</details> |
| 2025-08-14 | CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model | http://arxiv.org/abs/2508.10416 | <details><summary>展开</summary>待生成</details> |
| 2025-08-14 | Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning | http://arxiv.org/abs/2508.10399 | <details><summary>展开</summary>待生成</details> |
| 2025-08-14 | ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver | http://arxiv.org/abs/2508.10333 | <details><summary>展开</summary>待生成</details> |
| 2025-08-12 | GeoVLA: Empowering 3D Representations in Vision-Language-Action Models | http://arxiv.org/abs/2508.09071 | <details><summary>展开</summary>待生成</details> |
| 2025-08-12 | Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding | http://arxiv.org/abs/2508.09032 | <details><summary>展开</summary>待生成</details> |
| 2025-08-12 | OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing | http://arxiv.org/abs/2508.08706 | <details><summary>展开</summary>待生成</details> |
| 2025-08-11 | GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions | http://arxiv.org/abs/2508.07650 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model | http://arxiv.org/abs/2508.06571 | <details><summary>展开</summary>待生成</details> |
| 2025-08-06 | Static and Plugged: Make Embodied Evaluation Simple | http://arxiv.org/abs/2508.06553 | <details><summary>展开</summary>待生成</details> |
| 2025-08-06 | A tutorial note on collecting simulated data for vision-language-action models | http://arxiv.org/abs/2508.06547 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control | http://arxiv.org/abs/2508.05342 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction | http://arxiv.org/abs/2508.05294 | <details><summary>展开</summary>待生成</details> |
| 2025-08-07 | Learning to See and Act: Task-Aware View Planning for Robotic Manipulation | http://arxiv.org/abs/2508.05186 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming | http://arxiv.org/abs/2508.02549 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning | http://arxiv.org/abs/2508.02219 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation | http://arxiv.org/abs/2508.02190 | <details><summary>展开</summary>待生成</details> |
| 2025-08-04 | RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models | http://arxiv.org/abs/2508.02062 | <details><summary>展开</summary>待生成</details> |
| 2025-07-31 | XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation | http://arxiv.org/abs/2508.00097 | <details><summary>展开</summary>待生成</details> |
| 2025-07-31 | villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models | http://arxiv.org/abs/2507.23682 | <details><summary>展开</summary>待生成</details> |
| 2025-07-31 | A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving | http://arxiv.org/abs/2507.23540 | <details><summary>展开</summary>待生成</details> |
| 2025-07-31 | FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning | http://arxiv.org/abs/2507.23318 | <details><summary>展开</summary>待生成</details> |
| 2025-07-30 | Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance | http://arxiv.org/abs/2507.22424 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation | http://arxiv.org/abs/2507.17520 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents | http://arxiv.org/abs/2507.17462 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | Confidence Calibration in Vision-Language-Action Models | http://arxiv.org/abs/2507.17383 | <details><summary>展开</summary>待生成</details> |
| 2025-07-23 | VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback | http://arxiv.org/abs/2507.17294 | <details><summary>展开</summary>待生成</details> |
| 2025-07-22 | ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning | http://arxiv.org/abs/2507.16815 | <details><summary>展开</summary>待生成</details> |
| 2025-07-21 | Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos | http://arxiv.org/abs/2507.15597 | <details><summary>展开</summary>待生成</details> |
| 2025-07-21 | GR-3 Technical Report | http://arxiv.org/abs/2507.15493 | <details><summary>展开</summary>待生成</details> |
| 2025-07-18 | VLA-Mark: A cross modal watermark for large vision-language alignment model | http://arxiv.org/abs/2507.14067 | <details><summary>展开</summary>待生成</details> |
| 2025-07-18 | EdgeVLA: Efficient Vision-Language-Action Models | http://arxiv.org/abs/2507.14049 | <details><summary>展开</summary>待生成</details> |
| 2025-07-17 | AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation | http://arxiv.org/abs/2507.12768 | <details><summary>展开</summary>待生成</details> |
| 2025-07-16 | EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos | http://arxiv.org/abs/2507.12440 | <details><summary>展开</summary>待生成</details> |
| 2025-07-14 | Vision Language Action Models in Robotic Manipulation: A Systematic Review | http://arxiv.org/abs/2507.10672 | <details><summary>展开</summary>待生成</details> |
| 2025-07-12 | Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization | http://arxiv.org/abs/2507.09160 | <details><summary>展开</summary>待生成</details> |
| 2025-07-07 | VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting | http://arxiv.org/abs/2507.05116 | <details><summary>展开</summary>待生成</details> |
| 2025-07-06 | DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge | http://arxiv.org/abs/2507.04447 | <details><summary>展开</summary>待生成</details> |
| 2025-07-03 | DexVLG: Dexterous Vision-Language-Grasp Model at Scale | http://arxiv.org/abs/2507.02747 | <details><summary>展开</summary>待生成</details> |
| 2025-07-02 | cVLA: Towards Efficient Camera-Space VLAs | http://arxiv.org/abs/2507.02190 | <details><summary>展开</summary>待生成</details> |
| 2025-07-02 | A Survey on Vision-Language-Action Models: An Action Tokenization Perspective | http://arxiv.org/abs/2507.01925 | <details><summary>展开</summary>待生成</details> |
| 2025-07-02 | TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control | http://arxiv.org/abs/2507.01424 | <details><summary>展开</summary>待生成</details> |
| 2025-07-01 | Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding | http://arxiv.org/abs/2507.00416 | <details><summary>展开</summary>待生成</details> |
| 2025-06-30 | A Survey on Vision-Language-Action Models for Autonomous Driving | http://arxiv.org/abs/2506.24044 | <details><summary>展开</summary>待生成</details> |
| 2025-06-29 | IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering | http://arxiv.org/abs/2506.23329 | <details><summary>展开</summary>待生成</details> |
| 2025-06-27 | 4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration | http://arxiv.org/abs/2506.22242 | <details><summary>展开</summary>待生成</details> |
| 2025-06-26 | WorldVLA: Towards Autoregressive Action World Model | http://arxiv.org/abs/2506.21539 | <details><summary>展开</summary>待生成</details> |
| 2025-06-26 | Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends | http://arxiv.org/abs/2506.20966 | <details><summary>展开</summary>待生成</details> |
| 2025-06-24 | Unified Vision-Language-Action Model | http://arxiv.org/abs/2506.19850 | <details><summary>展开</summary>待生成</details> |
| 2025-06-24 | CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation | http://arxiv.org/abs/2506.19816 | <details><summary>展开</summary>待生成</details> |
| 2025-06-23 | MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis | http://arxiv.org/abs/2506.18897 | <details><summary>展开</summary>待生成</details> |
| 2025-06-22 | RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation | http://arxiv.org/abs/2506.18088 | <details><summary>展开</summary>待生成</details> |
| 2025-06-21 | RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models | http://arxiv.org/abs/2506.17811 | <details><summary>展开</summary>待生成</details> |
| 2025-06-21 | RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models | http://arxiv.org/abs/2506.17639 | <details><summary>展开</summary>待生成</details> |
| 2025-06-21 | VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models | http://arxiv.org/abs/2506.17561 | <details><summary>展开</summary>待生成</details> |
| 2025-06-19 | CapsDT: Diffusion-Transformer for Capsule Robot Manipulation | http://arxiv.org/abs/2506.16263 | <details><summary>展开</summary>待生成</details> |
| 2025-06-19 | ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models | http://arxiv.org/abs/2506.16211 | <details><summary>展开</summary>待生成</details> |
| 2025-06-17 | FormGym: Doing Paperwork with Agents | http://arxiv.org/abs/2506.14079 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics | http://arxiv.org/abs/2506.14009 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning | http://arxiv.org/abs/2506.13757 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction | http://arxiv.org/abs/2506.13751 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding | http://arxiv.org/abs/2506.13725 | <details><summary>展开</summary>待生成</details> |
| 2025-06-16 | ROSA: Harnessing Robot States for Vision-Language and Action Alignment | http://arxiv.org/abs/2506.13679 | <details><summary>展开</summary>待生成</details> |
| 2025-06-15 | SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration | http://arxiv.org/abs/2506.12723 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models | http://arxiv.org/abs/2506.10100 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | SAFE: Multitask Failure Detection for Vision-Language-Action Models | http://arxiv.org/abs/2506.09937 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models | http://arxiv.org/abs/2506.09930 | <details><summary>展开</summary>待生成</details> |
| 2025-06-11 | OctoNav: Towards Generalist Embodied Navigation | http://arxiv.org/abs/2506.09839 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models | http://arxiv.org/abs/2506.09172 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency | http://arxiv.org/abs/2506.08822 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing | http://arxiv.org/abs/2506.08462 | <details><summary>展开</summary>待生成</details> |
| 2025-06-10 | TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization | http://arxiv.org/abs/2506.08440 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation | http://arxiv.org/abs/2506.08296 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework | http://arxiv.org/abs/2506.08185 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models | http://arxiv.org/abs/2506.07961 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse | http://arxiv.org/abs/2506.07639 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation | http://arxiv.org/abs/2506.07530 | <details><summary>展开</summary>待生成</details> |
| 2025-06-09 | Real-Time Execution of Action Chunking Flow Policies | http://arxiv.org/abs/2506.07339 | <details><summary>展开</summary>待生成</details> |
| 2025-06-08 | Robotic Policy Learning via Human-assisted Action Preference Optimization | http://arxiv.org/abs/2506.07127 | <details><summary>展开</summary>待生成</details> |
| 2025-06-07 | RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation | http://arxiv.org/abs/2506.06677 | <details><summary>展开</summary>待生成</details> |
| 2025-06-06 | DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models | http://arxiv.org/abs/2506.05667 | <details><summary>展开</summary>待生成</details> |
| 2025-06-04 | SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models | http://arxiv.org/abs/2506.03574 | <details><summary>展开</summary>待生成</details> |
| 2025-06-03 | Adversarial Attacks on Robotic Vision Language Action Models | http://arxiv.org/abs/2506.03350 | <details><summary>展开</summary>待生成</details> |
| 2025-06-02 | SAB3R: Semantic-Augmented Backbone in 3D Reconstruction | http://arxiv.org/abs/2506.02112 | <details><summary>展开</summary>待生成</details> |
| 2025-06-02 | Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning | http://arxiv.org/abs/2506.01953 | <details><summary>展开</summary>待生成</details> |
| 2025-06-02 | SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics | http://arxiv.org/abs/2506.01844 | <details><summary>展开</summary>待生成</details> |
| 2025-06-01 | OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation | http://arxiv.org/abs/2506.01196 | <details><summary>展开</summary>待生成</details> |
| 2025-06-01 | GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering | http://arxiv.org/abs/2506.01174 | <details><summary>展开</summary>待生成</details> |
| 2025-05-31 | LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks | http://arxiv.org/abs/2506.00411 | <details><summary>展开</summary>待生成</details> |
| 2025-05-30 | Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction | http://arxiv.org/abs/2505.24156 | <details><summary>展开</summary>待生成</details> |
| 2025-05-29 | Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models | http://arxiv.org/abs/2505.23757 | <details><summary>展开</summary>待生成</details> |
| 2025-05-29 | Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better | http://arxiv.org/abs/2505.23705 | <details><summary>展开</summary>待生成</details> |
| 2025-05-29 | TrackVLA: Embodied Visual Tracking in the Wild | http://arxiv.org/abs/2505.23189 | <details><summary>展开</summary>待生成</details> |
| 2025-05-28 | Zero-Shot 3D Visual Grounding from Vision-Language Models | http://arxiv.org/abs/2505.22429 | <details><summary>展开</summary>待生成</details> |
| 2025-05-28 | ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation | http://arxiv.org/abs/2505.22159 | <details><summary>展开</summary>待生成</details> |
| 2025-05-28 | ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge | http://arxiv.org/abs/2505.21906 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models | http://arxiv.org/abs/2505.21567 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Hume: Introducing System-2 Thinking in Visual-Language-Action Model | http://arxiv.org/abs/2505.21432 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models | http://arxiv.org/abs/2505.21200 | <details><summary>展开</summary>待生成</details> |
| 2025-05-27 | Hierarchical Instruction-aware Embodied Visual Tracking | http://arxiv.org/abs/2505.20710 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | What Can RL Bring to VLA Generalization? An Empirical Study | http://arxiv.org/abs/2505.19789 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback | http://arxiv.org/abs/2505.19767 | <details><summary>展开</summary>待生成</details> |
| 2025-05-26 | DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving | http://arxiv.org/abs/2505.19381 | <details><summary>展开</summary>待生成</details> |
| 2025-05-25 | ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning | http://arxiv.org/abs/2505.19080 | <details><summary>展开</summary>待生成</details> |
| 2025-05-24 | Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance | http://arxiv.org/abs/2505.18793 | <details><summary>展开</summary>待生成</details> |
| 2025-05-24 | VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning | http://arxiv.org/abs/2505.18719 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems | http://arxiv.org/abs/2505.17295 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | Interactive Post-Training for Vision-Language-Action Models | http://arxiv.org/abs/2505.17016 | <details><summary>展开</summary>待生成</details> |
| 2025-05-22 | DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving | http://arxiv.org/abs/2505.16278 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning | http://arxiv.org/abs/2505.15725 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems | http://arxiv.org/abs/2505.15685 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization | http://arxiv.org/abs/2505.15660 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | FLARE: Robot Learning with Implicit World Modeling | http://arxiv.org/abs/2505.15659 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control | http://arxiv.org/abs/2505.15304 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy | http://arxiv.org/abs/2505.15206 | <details><summary>展开</summary>待生成</details> |
| 2025-05-21 | Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation | http://arxiv.org/abs/2505.15098 | <details><summary>展开</summary>待生成</details> |
| 2025-05-20 | AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory | http://arxiv.org/abs/2505.14030 | <details><summary>展开</summary>待生成</details> |
| 2025-05-20 | InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning | http://arxiv.org/abs/2505.13888 | <details><summary>展开</summary>待生成</details> |
| 2025-05-19 | SPKLIP: Aligning Spike Video Streams with Natural Language | http://arxiv.org/abs/2505.12656 | <details><summary>展开</summary>待生成</details> |
| 2025-05-18 | RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction | http://arxiv.org/abs/2505.12224 | <details><summary>展开</summary>待生成</details> |
| 2025-05-16 | Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions | http://arxiv.org/abs/2505.11214 | <details><summary>展开</summary>待生成</details> |
| 2025-05-16 | Conditioning Matters: Training Diffusion Policies is Faster Than You Think | http://arxiv.org/abs/2505.11123 | <details><summary>展开</summary>待生成</details> |
| 2025-05-14 | Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware | http://arxiv.org/abs/2505.09601 | <details><summary>展开</summary>待生成</details> |
| 2025-05-14 | VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation | http://arxiv.org/abs/2505.09577 | <details><summary>展开</summary>待生成</details> |
| 2025-05-13 | From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation | http://arxiv.org/abs/2505.08548 | <details><summary>展开</summary>待生成</details> |
| 2025-05-13 | Training Strategies for Efficient Embodied Reasoning | http://arxiv.org/abs/2505.08243 | <details><summary>展开</summary>待生成</details> |
| 2025-05-12 | ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning | http://arxiv.org/abs/2505.07395 | <details><summary>展开</summary>待生成</details> |
| 2025-05-09 | UniVLA: Learning to Act Anywhere with Task-centric Latent Actions | http://arxiv.org/abs/2505.06111 | <details><summary>展开</summary>待生成</details> |
| 2025-05-09 | 3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks | http://arxiv.org/abs/2505.05800 | <details><summary>展开</summary>待生成</details> |
| 2025-05-08 | Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments | http://arxiv.org/abs/2505.05540 | <details><summary>展开</summary>待生成</details> |
| 2025-05-07 | Vision-Language-Action Models: Concepts, Progress, Applications and Challenges | http://arxiv.org/abs/2505.04769 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation | http://arxiv.org/abs/2505.03912 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration | http://arxiv.org/abs/2505.03673 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | Task Reconstruction and Extrapolation for $π_0$ using Text Latent | http://arxiv.org/abs/2505.03500 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data | http://arxiv.org/abs/2505.03233 | <details><summary>展开</summary>待生成</details> |
| 2025-05-06 | Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets | http://arxiv.org/abs/2505.03174 | <details><summary>展开</summary>待生成</details> |
| 2025-05-04 | Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions | http://arxiv.org/abs/2505.02152 | <details><summary>展开</summary>待生成</details> |
| 2025-04-28 | NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks | http://arxiv.org/abs/2504.19854 | <details><summary>展开</summary>待生成</details> |
| 2025-04-22 | $π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization | http://arxiv.org/abs/2504.16054 | <details><summary>展开</summary>待生成</details> |
| 2025-04-01 | Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning | http://arxiv.org/abs/2504.00907 | <details><summary>展开</summary>待生成</details> |
| 2025-03-30 | OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model | http://arxiv.org/abs/2503.23463 | <details><summary>展开</summary>待生成</details> |
| 2025-03-27 | CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models | http://arxiv.org/abs/2503.22020 | <details><summary>展开</summary>待生成</details> |
| 2025-03-26 | MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation | http://arxiv.org/abs/2503.20384 | <details><summary>展开</summary>待生成</details> |
| 2025-03-25 | Gemini Robotics: Bringing AI into the Physical World | http://arxiv.org/abs/2503.20020 | <details><summary>展开</summary>待生成</details> |
| 2025-03-25 | Boosting Robotic Manipulation Generalization with Minimal Costly Data | http://arxiv.org/abs/2503.19516 | <details><summary>展开</summary>待生成</details> |
| 2025-03-20 | IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes | http://arxiv.org/abs/2503.17406 | <details><summary>展开</summary>待生成</details> |
| 2025-03-20 | JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse | http://arxiv.org/abs/2503.16365 | <details><summary>展开</summary>待生成</details> |
| 2025-03-18 | GR00T N1: An Open Foundation Model for Generalist Humanoid Robots | http://arxiv.org/abs/2503.14734 | <details><summary>展开</summary>待生成</details> |
| 2025-03-15 | ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis | http://arxiv.org/abs/2503.14526 | <details><summary>展开</summary>待生成</details> |
| 2025-03-17 | MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation | http://arxiv.org/abs/2503.13446 | <details><summary>展开</summary>待生成</details> |
| 2025-03-13 | HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model | http://arxiv.org/abs/2503.10631 | <details><summary>展开</summary>待生成</details> |
| 2025-03-12 | CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games | http://arxiv.org/abs/2503.09527 | <details><summary>展开</summary>待生成</details> |
| 2025-03-11 | MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models | http://arxiv.org/abs/2503.08007 | <details><summary>展开</summary>待生成</details> |
| 2025-03-10 | PointVLA: Injecting the 3D World into Vision-Language-Action Models | http://arxiv.org/abs/2503.07511 | <details><summary>展开</summary>待生成</details> |
| 2025-03-06 | Refined Policy Distillation: From VLA Generalists to RL Experts | http://arxiv.org/abs/2503.05833 | <details><summary>展开</summary>待生成</details> |
| 2025-03-06 | VLA Model-Expert Collaboration for Bi-directional Manipulation Learning | http://arxiv.org/abs/2503.04163 | <details><summary>展开</summary>待生成</details> |
| 2025-03-05 | OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction | http://arxiv.org/abs/2503.03734 | <details><summary>展开</summary>待生成</details> |
| 2025-03-05 | SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning | http://arxiv.org/abs/2503.03480 | <details><summary>展开</summary>待生成</details> |
| 2025-03-04 | RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour | http://arxiv.org/abs/2503.02572 | <details><summary>展开</summary>待生成</details> |
| 2025-03-04 | Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding | http://arxiv.org/abs/2503.02310 | <details><summary>展开</summary>待生成</details> |
| 2025-03-03 | CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs | http://arxiv.org/abs/2503.01378 | <details><summary>展开</summary>待生成</details> |
| 2025-02-27 | Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success | http://arxiv.org/abs/2502.19645 | <details><summary>展开</summary>待生成</details> |
| 2025-02-26 | ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration | http://arxiv.org/abs/2502.19250 | <details><summary>展开</summary>待生成</details> |
| 2025-02-24 | Evolution 6.0: Evolving Robotic Capabilities Through Generative Design | http://arxiv.org/abs/2502.17034 | <details><summary>展开</summary>待生成</details> |
| 2025-02-20 | Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration | http://arxiv.org/abs/2502.14795 | <details><summary>展开</summary>待生成</details> |
| 2025-02-20 | ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model | http://arxiv.org/abs/2502.14420 | <details><summary>展开</summary>待生成</details> |
| 2025-02-19 | VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation | http://arxiv.org/abs/2502.13508 | <details><summary>展开</summary>待生成</details> |
| 2025-02-14 | Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation | http://arxiv.org/abs/2502.10040 | <details><summary>展开</summary>待生成</details> |
| 2025-02-13 | GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation | http://arxiv.org/abs/2502.09268 | <details><summary>展开</summary>待生成</details> |
| 2025-02-07 | Survey on Vision-Language-Action Models | http://arxiv.org/abs/2502.06851 | <details><summary>展开</summary>待生成</details> |
| 2025-02-09 | DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control | http://arxiv.org/abs/2502.05855 | <details><summary>展开</summary>待生成</details> |
| 2025-02-08 | HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation | http://arxiv.org/abs/2502.05485 | <details><summary>展开</summary>待生成</details> |
| 2025-02-08 | ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy | http://arxiv.org/abs/2502.05450 | <details><summary>展开</summary>待生成</details> |
| 2025-02-06 | Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture | http://arxiv.org/abs/2502.04558 | <details><summary>展开</summary>待生成</details> |
| 2025-02-04 | VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation | http://arxiv.org/abs/2502.02175 | <details><summary>展开</summary>待生成</details> |
| 2025-02-03 | Scalable, Training-Free Visual Language Robotics: A Modular Multi-Model Framework for Consumer-Grade GPUs | http://arxiv.org/abs/2502.01071 | <details><summary>展开</summary>待生成</details> |
| 2025-01-31 | UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent | http://arxiv.org/abs/2501.18867 | <details><summary>展开</summary>待生成</details> |
| 2025-01-28 | Improving Vision-Language-Action Model with Online Reinforcement Learning | http://arxiv.org/abs/2501.16664 | <details><summary>展开</summary>待生成</details> |
| 2025-01-25 | An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation | http://arxiv.org/abs/2501.15068 | <details><summary>展开</summary>待生成</details> |
| 2025-01-16 | FAST: Efficient Action Tokenization for Vision-Language-Action Models | http://arxiv.org/abs/2501.09747 | <details><summary>展开</summary>待生成</details> |
| 2025-01-12 | Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing | http://arxiv.org/abs/2501.06919 | <details><summary>展开</summary>待生成</details> |
| 2025-01-09 | UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation | http://arxiv.org/abs/2501.05014 | <details><summary>展开</summary>待生成</details> |
| 2025-01-08 | Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding | http://arxiv.org/abs/2501.04693 | <details><summary>展开</summary>待生成</details> |
| 2025-01-07 | OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints | http://arxiv.org/abs/2501.03841 | <details><summary>展开</summary>待生成</details> |
| 2025-01-07 | Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis | http://arxiv.org/abs/2501.03565 | <details><summary>展开</summary>待生成</details> |
| 2025-01-06 | Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches | http://arxiv.org/abs/2501.03151 | <details><summary>展开</summary>待生成</details> |
| 2024-12-29 | CoA-VLA: Improving Vision-Language-Action Models via Visual-Textual Chain-of-Affordance | http://arxiv.org/abs/2412.20451 | <details><summary>展开</summary>待生成</details> |
| 2024-12-24 | VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks | http://arxiv.org/abs/2412.18194 | <details><summary>展开</summary>待生成</details> |
| 2024-12-20 | QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning | http://arxiv.org/abs/2412.15576 | <details><summary>展开</summary>待生成</details> |
| 2024-12-18 | Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models | http://arxiv.org/abs/2412.14058 | <details><summary>展开</summary>待生成</details> |
| 2024-12-18 | RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation | http://arxiv.org/abs/2412.13877 | <details><summary>展开</summary>待生成</details> |
| 2024-12-16 | Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning | http://arxiv.org/abs/2412.11974 | <details><summary>展开</summary>待生成</details> |
| 2024-12-13 | TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies | http://arxiv.org/abs/2412.10345 | <details><summary>展开</summary>待生成</details> |
| 2024-12-09 | Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks | http://arxiv.org/abs/2412.06224 | <details><summary>展开</summary>待生成</details> |
| 2024-12-05 | NaVILA: Legged Robot Vision-Language-Action Model for Navigation | http://arxiv.org/abs/2412.04453 | <details><summary>展开</summary>待生成</details> |
| 2024-12-02 | Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control | http://arxiv.org/abs/2412.01034 | <details><summary>展开</summary>待生成</details> |
| 2024-11-29 | SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters | http://arxiv.org/abs/2412.00174 | <details><summary>展开</summary>待生成</details> |
| 2024-11-29 | RoboMatrix: A Skill-centric Hierarchical Framework for Scalable Robot Task Planning and Execution in Open-World | http://arxiv.org/abs/2412.00171 | <details><summary>展开</summary>待生成</details> |
| 2024-11-29 | CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation | http://arxiv.org/abs/2411.19650 | <details><summary>展开</summary>待生成</details> |
| 2024-11-28 | GRAPE: Generalizing Robot Policy via Preference Alignment | http://arxiv.org/abs/2411.19309 | <details><summary>展开</summary>待生成</details> |
| 2024-11-18 | Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics | http://arxiv.org/abs/2411.13587 | <details><summary>展开</summary>待生成</details> |
| 2024-11-15 | Visual-Linguistic Agent: Towards Collaborative Contextual Object Reasoning | http://arxiv.org/abs/2411.10252 | <details><summary>展开</summary>待生成</details> |
| 2024-11-04 | Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks | http://arxiv.org/abs/2411.05821 | <details><summary>展开</summary>待生成</details> |
| 2024-11-05 | VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation | http://arxiv.org/abs/2411.03540 | <details><summary>展开</summary>待生成</details> |
| 2024-11-04 | DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution | http://arxiv.org/abs/2411.02359 | <details><summary>展开</summary>待生成</details> |
| 2024-11-01 | CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision | http://arxiv.org/abs/2411.00508 | <details><summary>展开</summary>待生成</details> |
| 2024-10-21 | The Duality of Generative AI and Reinforcement Learning in Robotics: A Review | http://arxiv.org/abs/2410.16411 | <details><summary>展开</summary>待生成</details> |
| 2024-10-21 | VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making | http://arxiv.org/abs/2410.15885 | <details><summary>展开</summary>待生成</details> |
| 2024-10-21 | A Dual Process VLA: Efficient Robotic Manipulation Leveraging VLM | http://arxiv.org/abs/2410.15549 | <details><summary>展开</summary>待生成</details> |
| 2024-10-17 | Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand | http://arxiv.org/abs/2410.14022 | <details><summary>展开</summary>待生成</details> |
| 2024-10-15 | Latent Action Pretraining from Videos | http://arxiv.org/abs/2410.11758 | <details><summary>展开</summary>待生成</details> |
| 2024-10-10 | Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation | http://arxiv.org/abs/2410.08001 | <details><summary>展开</summary>待生成</details> |
| 2024-09-12 | HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers | http://arxiv.org/abs/2410.05273 | <details><summary>展开</summary>待生成</details> |
| 2024-10-07 | LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation | http://arxiv.org/abs/2410.05191 | <details><summary>展开</summary>待生成</details> |
| 2024-10-02 | Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust | http://arxiv.org/abs/2410.01971 | <details><summary>展开</summary>待生成</details> |
| 2024-09-29 | RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model | http://arxiv.org/abs/2409.19590 | <details><summary>展开</summary>待生成</details> |
| 2024-09-19 | TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation | http://arxiv.org/abs/2409.12514 | <details><summary>展开</summary>待生成</details> |
| 2024-09-05 | OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving | http://arxiv.org/abs/2409.03272 | <details><summary>展开</summary>待生成</details> |
| 2024-08-19 | CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving | http://arxiv.org/abs/2408.10845 | <details><summary>展开</summary>待生成</details> |
| 2024-07-25 | Unified Lexical Representation for Interpretable Visual-Language Alignment | http://arxiv.org/abs/2407.17827 | <details><summary>展开</summary>待生成</details> |
| 2024-07-11 | Robotic Control via Embodied Chain-of-Thought Reasoning | http://arxiv.org/abs/2407.08693 | <details><summary>展开</summary>待生成</details> |
| 2024-07-10 | Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs | http://arxiv.org/abs/2407.07775 | <details><summary>展开</summary>待生成</details> |
| 2024-06-27 | OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents | http://arxiv.org/abs/2407.00114 | <details><summary>展开</summary>待生成</details> |
| 2024-06-28 | LLaRA: Supercharging Robot Learning Data for Vision-Language Policy | http://arxiv.org/abs/2406.20095 | <details><summary>展开</summary>待生成</details> |
| 2024-06-21 | Learning Efficient and Robust Language-conditioned Manipulation using Textual-Visual Relevancy and Equivariant Language Mapping | http://arxiv.org/abs/2406.15677 | <details><summary>展开</summary>待生成</details> |
| 2024-06-13 | OpenVLA: An Open-Source Vision-Language-Action Model | http://arxiv.org/abs/2406.09246 | <details><summary>展开</summary>待生成</details> |
| 2024-06-06 | RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation | http://arxiv.org/abs/2406.04339 | <details><summary>展开</summary>待生成</details> |
| 2024-05-31 | Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations | http://arxiv.org/abs/2406.00121 | <details><summary>展开</summary>待生成</details> |
| 2024-05-27 | A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation | http://arxiv.org/abs/2405.17418 | <details><summary>展开</summary>待生成</details> |
| 2024-05-23 | A Survey on Vision-Language-Action Models for Embodied AI | http://arxiv.org/abs/2405.14093 | <details><summary>展开</summary>待生成</details> |
| 2024-05-09 | Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations | http://arxiv.org/abs/2405.06039 | <details><summary>展开</summary>待生成</details> |
